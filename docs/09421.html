<html>
<head>
<title>When your Neural Net doesn’t know: a bayesian approach with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当你的神经网络不知道:一个贝叶斯方法与 Keras</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-your-neural-net-doesnt-know-a-bayesian-approach-with-keras-4782c0818624?source=collection_archive---------11-----------------------#2019-12-12">https://towardsdatascience.com/when-your-neural-net-doesnt-know-a-bayesian-approach-with-keras-4782c0818624?source=collection_archive---------11-----------------------#2019-12-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="00f1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度学习中模型不确定性的表示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3cf06ac07406160435db76d6f9034e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KecBbw-S2QgFRn7d"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@robschreckhise?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Rob Schreckhise</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="fe31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个试图拟合分类模型并检查其性能的人都面临着这样的问题:不仅要验证 KPI(如准确度、精确度和召回率)，还要验证<em class="lv">模型对它所说的内容有多有信心</em>。用来验证预测可靠性的最直观的工具是寻找各种类别的概率。</p><p id="2a53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概率越高，可信度越高。这并不意味着更高的准确性。很有可能出现错误分类。</p><p id="31c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在所做的是从我们拟合的模型中提取最佳结果，研究概率分布，并试图在我们的神经网络被迫做出决定时限制错误。我们以两种方式完成这项任务:</p><ul class=""><li id="6406" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">验证数据概率的经典研究，目的是建立阈值以避免错误分类。</li><li id="5137" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">最近的一种方法是基于贝叶斯理论的概率推断，在神经网络框架内有一个“<em class="lv">排列</em>，称为<strong class="lb iu">蒙特卡洛退出，它几乎免费提供不确定性估计</strong>，如本文中的<a class="ae ky" href="https://arxiv.org/abs/1506.02142" rel="noopener ugc nofollow" target="_blank">所示。</a></li></ul><h1 id="a5dd" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">数据</h1><p id="5353" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我在 Kaggle 上找到了这个实验的数据。我选择了一个有趣的数据集，其中包含了 10 种猴子的图片。<em class="lv">数据集由两个文件组成，训练和验证。每个文件夹包含 10 个子文件夹，标记为 n0~n9，每个对应一个物种形成</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Monkey" rel="noopener ugc nofollow" target="_blank"> <em class="lv">维基百科的猴子进化树</em> </a> <em class="lv">。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/35f1affbd271da9ac3c031a6b78df88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQCJlB4oOzKFR6_kgAQySw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image distribution of monkey species among train and validation folders</figcaption></figure><p id="92c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些图像质量很好，并且在各个类别之间保持平衡。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/c89ff77a11607291f1623212916f08e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmLgsZdndnZqT_UICJ5c1A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Samples of monkey images from each species</figcaption></figure><p id="4d30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用 Keras 'ImageDataGenerator '装载它们，在火车上执行数据扩充。在此过程中，我们存储 10%的训练集作为验证，这将有助于我们尝试按照标准方法建立概率阈值。根据本帖的范围，我们限定了目标类，只考虑前五种猴子。</p><h1 id="8c75" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">模型</h1><p id="2cb5" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们想要使用的神经网络结构是由简单的卷积层、最大池块和漏失构成的。最后一个是正则化训练的基础，稍后当我们用贝叶斯过程解释神经网络的不确定性时会派上用场。</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="5bd3" class="no ml it nk b gy np nq l nr ns">inp = Input(shape=SHAPE+tuple([3]))</span><span id="99de" class="no ml it nk b gy nt nq l nr ns">x = Conv2D(32, (3, 3), activation='relu')(inp)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/>x = Conv2D(32, (3, 3), activation='relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/>x = Dropout(0.5)(x, training = True)</span><span id="fbb9" class="no ml it nk b gy nt nq l nr ns">x = Conv2D(64, (3, 3), activation='relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/>x = Conv2D(64, (3, 3), activation='relu')(x)<br/>x = MaxPooling2D(pool_size=(2, 2))(x)<br/>x = Dropout(0.5)(x, training = True)</span><span id="a93d" class="no ml it nk b gy nt nq l nr ns">x = Flatten()(x)<br/>x = Dense(512, activation='relu')(x)<br/>x = Dropout(0.3)(x, training = True)</span><span id="292a" class="no ml it nk b gy nt nq l nr ns">out = Dense(5, activation='softmax')(x)</span><span id="830c" class="no ml it nk b gy nt nq l nr ns">model = Model(inp, out)<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="d656" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练之后，测试的准确度在 0.79 左右，迫使我们的模型对所有进行分类。</p><h2 id="13c1" class="no ml it bd mm nu nv dn mq nw nx dp mu li ny nz mw lm oa ob my lq oc od na oe bi translated">标准阈值的不确定性</h2><p id="4e33" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">我们介绍的第一种方法是基于对验证集上计算的概率的简单研究。这个过程使我们能够知道我们的神经网络何时失败，以及每类错误的置信度。通过这种方式，我们创建了与模型的最终预测结合使用的阈值:如果预测的标签低于相关类别的阈值，则我们拒绝进行预测。</p><p id="4005" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的验证由 10%的训练图像组成。为了有足够的概率分布来建立有意义的阈值，我们在验证时适当地操作数据扩充:在预测阶段，每个图像被扩充 100 倍，即每个样本多 100 个概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/0278be738a668bcc05f5454c1ccf395b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NS1rq73q4BKj7KwlgscEmg.png"/></div></div></figure><p id="d2b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在使用我们的扩充数据的预测步骤结束时，我们有 3 种不同的分数分布:每个类别的概率分数、错误分类样本的概率分数(在每个类别中)、正确分类样本的概率分数(在每个类别中)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/057c099bdfe154a552f4d105d9b0bb77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ovO3PyaUUuNrBQoN6Q9sxQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Probability Distributions on validation data with augmentation</figcaption></figure><p id="6dc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将每个类别的三个引用分布中的第一个作为第 10 个百分位数来计算阈值。预测概率低于能力阈值的测试图像被标记为“<em class="lv">未分类为</em>”。抑制未分类的<em class="lv"/>图像(共 20 张)，精度从 0.79 提高到 0.82。</p><h2 id="d5f8" class="no ml it bd mm nu nv dn mq nw nx dp mu li ny nz mw lm oa ob my lq oc od na oe bi translated">贝叶斯理论的不确定性</h2><p id="ca0e" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated"><em class="lv">贝叶斯概率理论提供了基于数学的工具来推理模型的不确定性，但这些通常伴随着令人望而却步的计算成本</em>。<em class="lv">令人惊讶的是，可以在不改变任何东西的情况下，将最近的深度学习工具转换为贝叶斯模型！</em>解决方案是使用神经网络中的漏失作为贝叶斯近似。</p><p id="7f60" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，在每个隐藏层之后应用随机漏失，因此模型输出可以近似地视为从后验预测分布生成的随机样本。因此，模型的不确定性可以通过位置指数或其他从几次重复的预测中获得的统计数据来估计。这个过程特别吸引人，因为它易于实现，并且可以直接应用于任何现有的神经网络而不会损失性能。</p><p id="9460" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个新的输入图像，我们激活 dropout，将其设置为 0.5(在训练结束时由 Keras 关闭)并计算预测。当我们重新激活辍学生时，我们改变了我们的神经网络结构，结果也是随机的。很明显，如果我们对每个测试样本重复预测 100 次，我们将能够<strong class="lb iu">建立每个类别中每个样本的概率分布</strong>。都可以用一些丰富多彩的情节来澄清。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/098834f71a06ee50fe43dbb291975609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwpmmManLI9ChSgOzd_M1Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">examples of Probability Distributions on test images</figcaption></figure><p id="0abe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一点上，评估很容易…如果在多个概率样本中，该图像的中值概率同时高于其他中值(上图中的红色虚线)并且至少为 0.5(上图中的绿色虚线)，我们希望神经网络输出一个猴子物种作为推荐。否则，我们将该图像标记为未分类为的<em class="lv">。抑制未分类的<em class="lv"/>图像(共 16 张)，精度从 0.79 提高到 0.83</em></p><h1 id="d395" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">摘要</h1><p id="4b73" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">在这篇文章中，我们评估了两种不同的评估神经网络可信度的方法。一种标准的方法是保留部分数据作为验证，以便研究概率分布和设置阈值。这是每种型号的通用程序。另一种方法建议应用随机退出，以建立概率分布并研究它们的差异。这两种技术都有助于避免错误分类，放松我们的神经网络，在没有太多信心的时候做出预测。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="856e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的 GITHUB 回购</strong> </a></p><p id="a79d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="6afe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="a6cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">贝叶斯近似下的辍学:表现深度学习中的模型不确定性:亚林·加尔，邹斌·格拉马尼</p></div></div>    
</body>
</html>