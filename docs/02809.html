<html>
<head>
<title>Why Linear Regression is not suitable for Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么线性回归不适合分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28?source=collection_archive---------7-----------------------#2019-05-07">https://towardsdatascience.com/why-linear-regression-is-not-suitable-for-binary-classification-c64457be8e28?source=collection_archive---------7-----------------------#2019-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="094a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">分类任务的线性回归与逻辑回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/44c1dacbc813da897cd16197ac00aa1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e2CayjIMploOYd8vOqYZLQ.png"/></div></div></figure><p id="642d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文解释了为什么在分类问题上逻辑回归比线性回归表现得更好，以及线性回归不适合的两个原因:</p><ul class=""><li id="7ac2" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">预测值是连续的，而不是概率性的</li><li id="0602" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">使用线性回归进行分类时，对不平衡数据敏感</li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="77e4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae ml" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank">监督学习</a>是机器学习必不可少的一部分。这是通过将输入变量映射到结果标签，从训练数据集中的示例中学习的任务，然后结果标签可用于预测新观察的结果。监督学习分类任务的示例有:</p><ol class=""><li id="3b7a" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp mm lw lx ly bi translated">给出泰坦尼克号沉没时幸存和未幸存的乘客名单，预测是否有人能在灾难中幸存</li><li id="4f3f" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp mm lw lx ly bi translated">给定一组猫和狗的图像，识别下一个图像是否包含一只狗或一只猫</li><li id="dc72" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp mm lw lx ly bi translated">给定一组带有情感标签的电影评论，确定一个新评论的情感(<a class="ae ml" href="https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-only/overview" rel="noopener ugc nofollow" target="_blank"> from Kaggle </a>)</li><li id="22dd" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp mm lw lx ly bi translated">给定从 0 到 9 的手绘数字图像，识别手绘数字图像上的数字</li></ol><p id="f385" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">示例 1 和 2 是<a class="ae ml" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank">二元分类</a>问题的示例，其中只有两种可能的结果(或类)。例 3 和例 4 是有两个以上结果的<a class="ae ml" href="https://en.wikipedia.org/wiki/Multiclass_classification" rel="noopener ugc nofollow" target="_blank">多类分类</a>问题的例子。</p><h1 id="907f" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">使用线性回归可以解决分类问题吗？</h1><p id="1c97" class="pw-post-body-paragraph ku kv it kw b kx nf ju kz la ng jx lc ld nh lf lg lh ni lj lk ll nj ln lo lp im bi translated">假设我们创建了一个完美平衡的数据集(所有事情都应该如此)，其中包含一个客户列表和一个标签，以确定客户是否购买了产品。在数据集中，有 20 个客户。10 名年龄在 10 至 19 岁之间的顾客购买了产品，10 名年龄在 20 至 29 岁之间的顾客没有购买产品。“已购买”是由 0 和 1 表示二进制标签，其中 0 表示“客户没有购买”，1 表示“客户购买”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c30dcb52a60b2c4f4185ddb9d0f88a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*n6wG6BdkXJegoFo9G7rg8g.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Our sample training dataset of 20 customers and their purchase label</figcaption></figure><p id="997b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae ml" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型的目的是找到输入变量和目标变量之间的关系。下面是我们使用上述数据集训练的线性回归模型。<strong class="kw iu">红线</strong>是训练数据集的最佳拟合线，旨在最小化预测值和实际值之间的距离。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/5644812d72d587a3067cdd00b92bb60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*lBc9kCcWfZxlvX-gdrVoUA.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Linear regression model, showing best fit line for the training dataset</figcaption></figure><p id="7793" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用这个模型进行预测是非常简单的。给定任何年龄，我们都能够预测 Y 轴上的值。如果 Y 大于 0.5(高于绿线)，则预测该客户会购买，否则不会购买。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="58db" class="nv mo it nr b gy nw nx l ny nz">+-----+-------------------+<br/>| Age | Predicted Y Value |<br/>+-----+-------------------+<br/>|  10 |        1.21428571 |<br/>|  15 |        0.83834586 |<br/>|  19 |        0.53759398 |<br/>|  20 |        0.46240602 |<br/>|  25 |        0.08646617 |<br/>|  30 |       -0.28947368 |<br/>+-----+-------------------+</span></pre><h1 id="ffc4" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">问题#1:预测值是连续的，不是概率性的</h1><p id="3070" class="pw-post-body-paragraph ku kv it kw b kx nf ju kz la ng jx lc ld nh lf lg lh ni lj lk ll nj ln lo lp im bi translated">在二元分类问题中，我们感兴趣的是结果发生的概率。概率的范围在 0 到 1 之间，其中某件事情一定会发生的概率是 1，0 是某件事情不太可能发生。但是在线性回归中，我们预测的是一个绝对数字，其范围可以在 0 和 1 之外。</p><p id="8235" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用我们的线性回归模型，任何 30 岁以上的人都有一个负“购买”价值的预测，这实际上没有意义。当然，我们可以将任何大于 1 的值限定为 1，将小于 0 的值限定为 0。线性回归还是可以的吧？</p><p id="3252" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是的，这可能行得通，但是<a class="ae ml" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>更适合分类任务，我们想证明逻辑回归比线性回归产生更好的结果。让我们看看逻辑回归是如何对数据集进行分类的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/70b024e53f6fa866a6c9d26ee4a416ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*S_uL234q-Frnkag5tuBnig.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Logistic regression model, a sigmoid curve that fit the training dataset</figcaption></figure><p id="3504" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们在同一个数据集上训练了两个模型，一个通过线性回归，另一个通过逻辑回归。我们可以通过使用均方根误差(RMSE)和决定系数(R 得分)来比较这两种模型的性能。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="98cf" class="nv mo it nr b gy nw nx l ny nz">+---------------------+--------------------+----------------------+<br/>|                     | R2 (higher better) | RMSE (lower better)  |<br/>+---------------------+--------------------+----------------------+<br/>| Linear regression   | 0.7518796992481203 | 0.062030075187969935 |<br/>| Logistic regression | 0.9404089597242656 | 0.014897760068933596 |<br/>+---------------------+--------------------+----------------------+</span></pre><p id="f660" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">r 是观察数据点与拟合回归线接近程度的量度，通常越高越好。但是仅有 R 是不够的，所以我们也要看看 RMSE。RMSE 衡量观察数据点与模型预测值的距离，越低越好。</p><p id="a659" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从度量标准来看，在分类任务中，逻辑回归比线性回归表现得好得多。就像<a class="oa ob ep" href="https://medium.com/u/2fccb851bb5e?source=post_page-----c64457be8e28--------------------------------" rel="noopener" target="_blank">凯西·科兹尔科夫</a> <a class="ae ml" href="https://hackernoon.com/machine-learning-is-the-emperor-wearing-clothes-59933d12a3cc" rel="noopener ugc nofollow" target="_blank">引用它</a>:</p><blockquote class="oc"><p id="1481" class="od oe it bd of og oh oi oj ok ol lp dk translated">神经网络也可以被称为“瑜伽网络”——它们的特殊能力给了你一个非常灵活的边界。</p></blockquote><h1 id="4683" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz om ka mz kc on kd nb kf oo kg nd ne bi translated">问题#2:对不平衡数据敏感</h1><p id="5365" class="pw-post-body-paragraph ku kv it kw b kx nf ju kz la ng jx lc ld nh lf lg lh ni lj lk ll nj ln lo lp im bi translated">让我们再添加 10 个年龄在 60 到 70 岁之间的客户，并训练我们的线性回归模型，找到最佳拟合线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e9794bdef35404ca1911785b11e94167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*HFrHo41Fe9Zqo32v4_Sbcw.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Linear regression model on 30 customers</figcaption></figure><p id="d9b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的线性回归模型成功地拟合了一条新的线，但是如果你仔细观察，一些客户(年龄 20 到 22 岁)的结果被错误地预测了。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="3b91" class="nv mo it nr b gy nw nx l ny nz">+-----+-------------------+<br/>| Age | Predicted Y Value |<br/>+-----+-------------------+<br/>|  18 |        0.56495292 |<br/>|  19 |        0.55091537 |<br/>|  20 |        0.53687781 |<br/>|  21 |        0.52284026 |<br/>|  22 |        0.50880271 |<br/>|  23 |        0.49476516 |<br/>|  24 |        0.48072761 |<br/>|  25 |        0.46669006 |<br/>+-----+-------------------+</span></pre><p id="78e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">线性回归试图通过最小化预测误差来拟合回归线，以便最小化年龄在 60 至 70 岁之间的客户的预测值和实际值之间的距离。让我们用相同的数据集训练一个逻辑回归模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fdbdacdcb72e66ab2cdd134fca0d569c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*GgUYnIinlmTUnEc8CEesjQ.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Logistic regression model on the same training dataset</figcaption></figure><p id="4074" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">是啊！在这个非常简单的数据集中，逻辑回归成功地对所有数据点进行了完美的分类。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="aa45" class="nv mo it nr b gy nw nx l ny nz">+-----+-------------------+<br/>| Age | Predicted Y Value |<br/>+-----+-------------------+<br/>|  18 |        0.85713668 |<br/>|  19 |        0.64502441 |<br/>|  20 |        0.35497751 |<br/>|  21 |        0.14286435 |<br/>|  22 |        0.04805457 |<br/>+-----+-------------------+</span></pre><p id="25e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们再次比较两个模型的 R 和 RMSE，你会发现逻辑回归比线性回归做得更好。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b7f9" class="nv mo it nr b gy nw nx l ny nz">+---------------------+---------------------+----------------------+<br/>|                     | R2 (higher better)  | RMSE (lower better)  |<br/>+---------------------+---------------------+----------------------+<br/>| Linear regression   |  0.4211265134234073 |  0.12863855257257611 |<br/>| Logistic regression |  0.9553066567250715 |  0.00993185406109522 |<br/>+---------------------+---------------------+----------------------+</span></pre><h1 id="3512" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结论</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/044f170c7729f11099599bba6db4a0da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*WdV3CRY4JWTG4fUmTTMF1w.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Both the linear and the logistic regression line</figcaption></figure><p id="4db4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">线性回归适用于预测连续值的输出，例如预测房地产价格。其预测输出可以是任何实数，范围从负无穷大到无穷大。回归线是一条直线。</p><p id="1f74" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">而逻辑回归用于分类问题，预测概率范围在 0 到 1 之间。例如，预测客户是否会购买。回归线是一条 s 形曲线。</p><h1 id="187a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">笔记本</h1><p id="76fb" class="pw-post-body-paragraph ku kv it kw b kx nf ju kz la ng jx lc ld nh lf lg lh ni lj lk ll nj ln lo lp im bi translated">在这个<a class="ae ml" href="https://gist.github.com/jinglescode/c0a3065dfb0fdc03287938cc600489a3" rel="noopener ugc nofollow" target="_blank">笔记本</a>中查看本文中使用的代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><div class="kj kk kl km gt or"><a rel="noopener follow" target="_blank" href="/data-scientist-the-dirtiest-job-of-the-21st-century-7f0c8215e845"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">数据科学家:21 世纪最肮脏的工作</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">40%的吸尘器，40%的看门人，20%的算命师。</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ks or"/></div></div></a></div><div class="kj kk kl km gt ab cb"><figure class="pg kn ph pi pj pk pl paragraph-image"><a href="https://www.linkedin.com/in/jingles/"><img src="../Images/7820823f18c088b934fefc4fcbe5e6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*fPTPd_WxZ4Ey7iOVElxwJQ.png"/></a></figure><figure class="pg kn pm pi pj pk pl paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/ed2857d42868ce52ed8f717376bc4cc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*i2NzU4j49rZ36Mxz4gp4Sg.png"/></a></figure><figure class="pg kn pm pi pj pk pl paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/c6faf13786230940c1756ff46938c471.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*oENDSDMTwXi2CJdO1gryug.png"/></a></figure></div></div></div>    
</body>
</html>