<html>
<head>
<title>Beginner’s Guide to K-Nearest Neighbors in R: from Zero to Hero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R 中 K-最近邻初学者指南:从零到英雄</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb?source=collection_archive---------2-----------------------#2019-12-15">https://towardsdatascience.com/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb?source=collection_archive---------2-----------------------#2019-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8eca" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习:监督学习</h2><div class=""/><div class=""><h2 id="f812" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在 R 中建立具有各种性能指标的 KNN 模型的管道</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f9806e635a5d570f81b173096490e055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKA0SUXJ3ypntg7zLuQ6Dw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@mathyaskurmann?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Mathyas Kurmann</a> on <a class="ae lh" href="https://unsplash.com/s/photos/neighbors?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f0bd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"><em class="me">2021 年 1 月 10 日更新</em> </strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mf mg l"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">KNN in audio</figcaption></figure><blockquote class="mh"><p id="2e32" class="mi mj it bd mk ml mm mn mo mp mq md dk translated">"如果你住的地方离比尔·盖茨只有 5 分钟的路程，我敢打赌你一定很有钱."</p></blockquote><h1 id="90c6" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated">介绍</h1><p id="7313" class="pw-post-body-paragraph li lj it lk b ll nj kd ln lo nk kg lq lr nl lt lu lv nm lx ly lz nn mb mc md im bi translated">在机器学习领域，K 近邻 KNN 最直观，因此很容易被希望进入该领域的数据科学爱好者所接受。为了决定观察的分类标签，KNN 会查看其邻居并将邻居的标签分配给感兴趣的观察。这是 KNN 方法的基本思想，就像一开始用的比尔游戏的比喻一样。不需要更高维度的计算来理解算法是如何工作的。</p><p id="bb94" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是有一个问题。查看一个邻居可能会给模型带来偏差和不准确性，我们必须为该方法设置几个“参与规则”。例如，顾名思义，KNN 可以采用其“k”个邻居的多数情况。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3818111d24159dfcf7334cb0f43ab2f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*OG5xIIalKr8q4mdRtLdN1Q.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">A quick look at how KNN works, by <a class="ae lh" href="https://en.m.wikipedia.org/wiki/File:ReducedDataSet.png" rel="noopener ugc nofollow" target="_blank">Agor153</a></figcaption></figure><blockquote class="mh"><p id="67ac" class="mi mj it bd mk ml mm mn mo mp mq md dk translated">为了决定新观察的标签，我们查看最近的邻居。</p></blockquote><h1 id="51b9" class="mr ms it bd mt mu mv mw mx my mz na nb ki nc kj nd kl ne km nf ko ng kp nh ni bi translated"><strong class="ak">测量距离</strong></h1><p id="fe76" class="pw-post-body-paragraph li lj it lk b ll nj kd ln lo nk kg lq lr nl lt lu lv nm lx ly lz nn mb mc md im bi translated">为了选择邻居的数量，我们需要采用一个数字来量化邻居之间的相似性或不相似性(<a class="ae lh" href="https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/1491952962/ref=sr_1_2?crid=2DHSXJNLD87ON&amp;dchild=1&amp;keywords=practical+statistics+for+data+scientists&amp;qid=1610331584&amp;sprefix=practical+statistics%2Caps%2C110&amp;sr=8-2" rel="noopener ugc nofollow" target="_blank">数据科学家实用统计</a>)。为此，KNN 根据数据类型有两套距离度量标准。</p><p id="74f6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于离散变量，KNN 采用海明距离。它测量使两个字符串相似所需的最少替换数(<a class="ae lh" href="https://en.m.wikipedia.org/wiki/Hamming_distance" rel="noopener ugc nofollow" target="_blank"> Wikipedia </a>)。</p><p id="d674" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于连续变量，我们使用欧几里德距离。为了计算两个向量<em class="me"> (x1，x2，…，xp) </em>和<em class="me"> (μ1，μ2，…，μp) </em>之间的距离，我们取它们的个体差，平方，求和，然后平方根，如下所示:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/1889c14cdf9ca13d8d535504afa479ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JPCEMFoRPBR-zdKXvXh-Iw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">screenshot of <a class="ae lh" href="https://www.amazon.com/Practical-Statistics-Data-Scientists-Essential/dp/1491952962/ref=sr_1_2?crid=2DHSXJNLD87ON&amp;dchild=1&amp;keywords=practical+statistics+for+data+scientists&amp;qid=1610331584&amp;sprefix=practical+statistics%2Caps%2C110&amp;sr=8-2" rel="noopener ugc nofollow" target="_blank">Practical Statistics for Data Scientists</a></figcaption></figure><p id="5d6d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">顺便提一下，在数据科学访谈中，为 KNN 选择距离指标是一个备受考验的话题。他们通常会询问选择一个指标的理由，以及他们的权衡。我在一篇相关的文章中阐述了如何处理这类问题:</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/crack-data-science-interviews-essential-machine-learning-concepts-afd6a0a6d1aa"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">破解数据科学访谈:基本的机器学习概念</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">赢在 2021 年:数据科学家/工程师的必读之作，第 1 部分</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh lb nt"/></div></div></a></div></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="56e0" class="mr ms it bd mt mu op mw mx my oq na nb ki or kj nd kl os km nf ko ot kp nh ni bi translated"><strong class="ak">什么是 K 倍交叉验证？</strong></h1><p id="4f1e" class="pw-post-body-paragraph li lj it lk b ll nj kd ln lo nk kg lq lr nl lt lu lv nm lx ly lz nn mb mc md im bi translated">如上所述，KNN 的关键是设置邻居的数量，我们求助于交叉验证(CV)来决定额外的 K 个邻居。</p><p id="214a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">交叉验证可以简要描述为以下步骤:</p><ol class=""><li id="a2db" class="ou ov it lk b ll lm lo lp lr ow lv ox lz oy md oz pa pb pc bi translated"><em class="me">将数据分成 K 个均匀分布的块/折叠</em></li><li id="ae9e" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated"><em class="me">选择 1 个组块/折叠作为测试集，其余 K-1 个作为训练集</em></li><li id="7e27" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated"><em class="me">基于训练集开发 KNN 模型</em></li><li id="8c4f" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated"><em class="me">仅比较测试集上的预测值和实际值</em></li><li id="8af5" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated"><em class="me">将 ML 模型应用于测试集，并使用每个组块重复 K 次</em></li><li id="5c67" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated"><em class="me">将模型的指标分数相加，并在 K 倍上求平均值</em></li></ol></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="7b65" class="mr ms it bd mt mu op mw mx my oq na nb ki or kj nd kl os km nf ko ot kp nh ni bi translated">怎么选 K？</h1><p id="82bd" class="pw-post-body-paragraph li lj it lk b ll nj kd ln lo nk kg lq lr nl lt lu lv nm lx ly lz nn mb mc md im bi translated">从技术上讲，我们可以将 K 设置为 1 和样本大小 n 之间的任何值。设置 K = n，CV 将 1 个观察值作为训练集，其余 n-1 个案例作为测试集，并对整个数据集重复该过程。这种简历被称为<strong class="lk jd">“留一法交叉验证”(LOOCV) </strong>。</p><p id="1aeb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，LOOCV 有直观的意义，但需要大量的计算能力。对于非常大的数据集，它会永远运行。</p><p id="8b60" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了选择最佳的 K 倍，我们必须在偏差和方差之间进行权衡。对于一个小的 K，该模型有一个高偏差，但估计测试误差的方差低。对于一个大 K，我们有一个低偏差但高方差(<strong class="lk jd">破解数据科学访谈</strong>)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/afa8be6a3d2d2530bb19d6e6c7734848.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BDQZ00vJYeYktgztE7G7kQ.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@jontyson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jon Tyson</a> on <a class="ae lh" href="https://unsplash.com/s/photos/neighbors?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="21ab" class="mr ms it bd mt mu mv mw mx my mz na nb ki pj kj nd kl pk km nf ko pl kp nh ni bi translated">R 中的代码实现</h1><h2 id="6747" class="pm ms it bd mt pn po dn mx pp pq dp nb lr pr ps nd lv pt pu nf lz pv pw nh iz bi translated">1.软件准备</h2><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="a37b" class="pm ms it py b gy qc qd l qe qf"><em class="me"># install.packages(“ISLR”)<br/># install.packages(“ggplot2”) # install.packages(“plyr”)<br/># install.packages(“dplyr”) # install.packages(“class”)</em></span><span id="290f" class="pm ms it py b gy qg qd l qe qf"><em class="me"># Load libraries<br/></em><strong class="py jd">library</strong>(ISLR) <br/><strong class="py jd">library</strong>(ggplot2) <br/><strong class="py jd">library</strong>(reshape2) <br/><strong class="py jd">library</strong>(plyr) <br/><strong class="py jd">library</strong>(dplyr) <br/><strong class="py jd">library</strong>(class)</span><span id="eda5" class="pm ms it py b gy qg qd l qe qf"># load data and clean the dataset<br/>banking=read.csv(“bank-additional-full.csv”,sep =”;”,header=T)</span><span id="4da8" class="pm ms it py b gy qg qd l qe qf">##check for missing data and make sure no missing data<br/>banking[!complete.cases(banking),]</span><span id="56c9" class="pm ms it py b gy qg qd l qe qf">#re-code qualitative (factor) variables into numeric<br/>banking$job= recode(banking$job, “‘admin.’=1;’blue-collar’=2;’entrepreneur’=3;’housemaid’=4;’management’=5;’retired’=6;’self-employed’=7;’services’=8;’student’=9;’technician’=10;’unemployed’=11;’unknown’=12”)</span><span id="62f0" class="pm ms it py b gy qg qd l qe qf">#recode variable again<br/>banking$marital = recode(banking$marital, “‘divorced’=1;’married’=2;’single’=3;’unknown’=4”)</span><span id="cbb2" class="pm ms it py b gy qg qd l qe qf">banking$education = recode(banking$education, “‘basic.4y’=1;’basic.6y’=2;’basic.9y’=3;’high.school’=4;’illiterate’=5;’professional.course’=6;’university.degree’=7;’unknown’=8”)</span><span id="7cb1" class="pm ms it py b gy qg qd l qe qf">banking$default = recode(banking$default, “‘no’=1;’yes’=2;’unknown’=3”)</span><span id="5a18" class="pm ms it py b gy qg qd l qe qf">banking$housing = recode(banking$housing, “‘no’=1;’yes’=2;’unknown’=3”)</span><span id="7d60" class="pm ms it py b gy qg qd l qe qf">banking$loan = recode(banking$loan, “‘no’=1;’yes’=2;’unknown’=3”)</span><span id="c729" class="pm ms it py b gy qg qd l qe qf">banking$contact = recode(banking$loan, “‘cellular’=1;’telephone’=2;”)</span><span id="19df" class="pm ms it py b gy qg qd l qe qf">banking$month = recode(banking$month, “‘mar’=1;’apr’=2;’may’=3;’jun’=4;’jul’=5;’aug’=6;’sep’=7;’oct’=8;’nov’=9;’dec’=10”)</span><span id="046c" class="pm ms it py b gy qg qd l qe qf">banking$day_of_week = recode(banking$day_of_week, “‘mon’=1;’tue’=2;’wed’=3;’thu’=4;’fri’=5;”)</span><span id="4d35" class="pm ms it py b gy qg qd l qe qf">banking$poutcome = recode(banking$poutcome, “‘failure’=1;’nonexistent’=2;’success’=3;”)</span><span id="4609" class="pm ms it py b gy qg qd l qe qf">#remove variable “pdays”, b/c it has no variation<br/>banking$pdays=NULL </span><span id="b3af" class="pm ms it py b gy qg qd l qe qf">#remove variable “duration”, b/c itis collinear with the DV<br/>banking$duration=NULL</span></pre><p id="ee06" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">加载和清理原始数据集后，通常的做法是直观地检查变量的分布，检查季节性、模式、异常值等。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="bf70" class="pm ms it py b gy qc qd l qe qf">#EDA of the DV<br/>plot(banking$y,main="Plot 1: Distribution of Dependent Variable")</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/4cde7a73ff25a9a53b9cb627a6915bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXhvl-RtHBiO6awEU9qykg.png"/></div></div></figure><p id="8258" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可以看出，结果变量(<em class="me">银行服务订阅</em>)并不是均衡分布的，“否”比“是”多得多，这给机器学习分类问题带来不便。假阳性率很高，因为许多少数病例会被归类为多数病例。对于这种分布不均匀的罕见事件，非参数分类方法是首选方法。</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/classifying-rare-events-using-five-machine-learning-techniques-fab464573233"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">使用 5 种机器学习算法对罕见事件进行分类</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">哪一种最适合不平衡数据？有什么权衡吗？</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="qi l oe of og oc oh lb nt"/></div></div></a></div><h2 id="9a28" class="pm ms it bd mt pn po dn mx pp pq dp nb lr pr ps nd lv pt pu nf lz pv pw nh iz bi translated"><strong class="ak"> 2。数据分割</strong></h2><p id="0733" class="pw-post-body-paragraph li lj it lk b ll nj kd ln lo nk kg lq lr nl lt lu lv nm lx ly lz nn mb mc md im bi translated">我们将数据集分为训练集和测试集。根据经验，我们坚持“80–20”划分，即 80%的数据作为训练集，20%作为测试集。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="6978" class="pm ms it py b gy qc qd l qe qf">#split the dataset into training and test sets randomly, but we need to set seed so as to generate the same value each time we run the code</span><span id="3c54" class="pm ms it py b gy qg qd l qe qf">set.seed(1)</span><span id="d4c7" class="pm ms it py b gy qg qd l qe qf"><strong class="py jd">#create an index to split the data: 80% training and 20% test<br/></strong>index = round(nrow(banking)*0.2,digits=0)</span><span id="e870" class="pm ms it py b gy qg qd l qe qf">#sample randomly throughout the dataset and keep the total number equal to the value of index<br/>test.indices = sample(1:nrow(banking), index)</span><span id="1833" class="pm ms it py b gy qg qd l qe qf">#80% training set<br/>banking.train=banking[-test.indices,] </span><span id="94eb" class="pm ms it py b gy qg qd l qe qf">#20% test set<br/>banking.test=banking[test.indices,] </span><span id="89ba" class="pm ms it py b gy qg qd l qe qf">#Select the training set except the DV<br/>YTrain = banking.train$y<br/>XTrain = banking.train %&gt;% select(-y)</span><span id="a105" class="pm ms it py b gy qg qd l qe qf"># Select the test set except the DV<br/>YTest = banking.test$y<br/>XTest = banking.test %&gt;% select(-y)</span></pre><h2 id="db80" class="pm ms it bd mt pn po dn mx pp pq dp nb lr pr ps nd lv pt pu nf lz pv pw nh iz bi translated">3.<strong class="ak">火车模型</strong></h2><p id="57e6" class="pw-post-body-paragraph li lj it lk b ll nj kd ln lo nk kg lq lr nl lt lu lv nm lx ly lz nn mb mc md im bi translated">让我们创建一个新函数("<strong class="lk jd"> calc_error_rate </strong>")来记录错误分类率。当使用训练模型预测的标签与实际结果标签不匹配时，该函数计算比率。它衡量分类的准确性。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="462e" class="pm ms it py b gy qc qd l qe qf">#define an error rate function and apply it to obtain test/training errors</span><span id="f1a2" class="pm ms it py b gy qg qd l qe qf">calc_error_rate &lt;- function(predicted.value, true.value){<br/> return(mean(true.value!=predicted.value)) <br/>}</span></pre><p id="7039" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们需要另一个函数“<em class="me"> do.chunk() </em>”，来做 k 重交叉验证。该函数返回可能折叠值的数据框。</p><p id="0020" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该步骤的主要目的是为 KNN 选择最佳 K 值。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="62b8" class="pm ms it py b gy qc qd l qe qf">nfold = 10<br/>set.seed(1)</span><span id="ed9b" class="pm ms it py b gy qg qd l qe qf"># cut() divides the range into several intervals<br/>folds = seq.int(nrow(banking.train)) %&gt;%<br/>     cut(breaks = nfold, labels=FALSE) %&gt;%  <br/>     sample</span><span id="fd9d" class="pm ms it py b gy qg qd l qe qf">do.chunk &lt;- function(chunkid, folddef, Xdat, Ydat, k){ <br/>     train = (folddef!=chunkid)# training index</span><span id="940a" class="pm ms it py b gy qg qd l qe qf">Xtr = Xdat[train,] # training set by the index</span><span id="492a" class="pm ms it py b gy qg qd l qe qf">Ytr = Ydat[train] # true label in training set</span><span id="2a0d" class="pm ms it py b gy qg qd l qe qf">Xvl = Xdat[!train,] # test set</span><span id="37dd" class="pm ms it py b gy qg qd l qe qf">Yvl = Ydat[!train] # true label in test set</span><span id="41ad" class="pm ms it py b gy qg qd l qe qf">predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, k = k) # predict training labels</span><span id="27ea" class="pm ms it py b gy qg qd l qe qf">predYvl = knn(train = Xtr, test = Xvl, cl = Ytr, k = k) # predict test labels</span><span id="ccb9" class="pm ms it py b gy qg qd l qe qf">data.frame(fold =chunkid, # k folds <br/>train.error = calc_error_rate(predYtr, Ytr),#training error per fold <br/> val.error = calc_error_rate(predYvl, Yvl)) # test error per fold<br/> }</span><span id="c67f" class="pm ms it py b gy qg qd l qe qf"># set error.folds to save validation errors<br/>error.folds=NULL</span><span id="0efc" class="pm ms it py b gy qg qd l qe qf"># create a sequence of data with an interval of 10<br/>kvec = c(1, seq(10, 50, length.out=5))</span><span id="0e29" class="pm ms it py b gy qg qd l qe qf">set.seed(1)</span><span id="fd4a" class="pm ms it py b gy qg qd l qe qf">for (j in kvec){<br/> tmp = ldply(1:nfold, do.chunk, # apply do.function to each fold<br/> folddef=folds, Xdat=XTrain, Ydat=YTrain, k=j) # required arguments<br/> tmp$neighbors = j # track each value of neighbors<br/> error.folds = rbind(error.folds, tmp) # combine the results <br/> }</span><span id="3fd1" class="pm ms it py b gy qg qd l qe qf"><strong class="py jd">#melt() in the package reshape2 melts wide-format data into long-format data<br/></strong>errors = melt(error.folds, id.vars=c(“fold”,”neighbors”), value.name= “error”)</span></pre><p id="391c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来的步骤是找到最小化验证误差的 k 的数量</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="8a3e" class="pm ms it py b gy qc qd l qe qf">val.error.means = errors %&gt;%<br/>    #select all rows of validation errors<br/>    filter(variable== “val.error” ) %&gt;% <br/>    #group the selected data by neighbors<br/>    group_by(neighbors, variable) %&gt;%<br/>    #cacluate CV error for each k<br/>    summarise_each(funs(mean), error) %&gt;%<br/>    #remove existing grouping<br/>    ungroup() %&gt;% <br/>    filter(error==min(error))</span><span id="43e6" class="pm ms it py b gy qg qd l qe qf"><em class="me"># Best number of neighbors</em><br/><em class="me"># if there is a tie, pick larger number of neighbors for simpler model</em><br/>numneighbor = max(val.error.means$neighbors)<br/>numneighbor</span><span id="d06a" class="pm ms it py b gy qg qd l qe qf">## [20]</span></pre><p id="45fd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，在使用 10 重交叉验证后，最佳邻居数为 20。</p><h2 id="5d07" class="pm ms it bd mt pn po dn mx pp pq dp nb lr pr ps nd lv pt pu nf lz pv pw nh iz bi translated">4.一些模型指标</h2><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="f773" class="pm ms it py b gy qc qd l qe qf">#training error<br/>set.seed(20)<br/>pred.YTtrain = knn(train=XTrain, test=XTrain, cl=YTrain, k=20)</span><span id="9d21" class="pm ms it py b gy qg qd l qe qf">knn_traing_error &lt;- calc_error_rate(predicted.value=pred.YTtrain, true.value=YTrain)<br/>knn_traing_error</span><span id="5609" class="pm ms it py b gy qg qd l qe qf">[1] 0.101214</span></pre><p id="c1ac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">训练误差为 0.10。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="37de" class="pm ms it py b gy qc qd l qe qf">#test error<br/>set.seed(20)<br/>pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=20)</span><span id="1918" class="pm ms it py b gy qg qd l qe qf">knn_test_error &lt;- calc_error_rate(predicted.value=pred.YTest, true.value=YTest)<br/>knn_test_error</span><span id="f163" class="pm ms it py b gy qg qd l qe qf">[1] 0.1100995</span></pre><p id="85af" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">测试误差为 0.11。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="af04" class="pm ms it py b gy qc qd l qe qf">#confusion matrix</span><span id="d481" class="pm ms it py b gy qg qd l qe qf">conf.matrix = <strong class="py jd">table</strong>(predicted=pred.YTest, true=YTest)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/2e39e6d149445ed79356422590b409ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*HPSM3JJ-Tsj6msdKu0er-w.png"/></div></figure><p id="5598" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据上面的混淆矩阵，我们可以计算出下面的数值，为绘制 ROC 曲线做准备。</p><p id="ebdf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">准确度= </strong> (TP +TN)/(TP+FP+FN+TN)</p><p id="d09e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">TPR/召回/灵敏度</strong> = TP/(TP+FN)</p><p id="d266" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">精度</strong> = TP/(TP+FP)</p><p id="6f0b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">特异性</strong> = TN/(TN+FP)</p><p id="eb44" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> FPR </strong> = 1 —特异性= FP/(TN+FP)</p><p id="9967" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> F1 得分</strong> = 2*TP/(2*TP+FP+FN) =精度*召回/(精度+召回)</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="0f00" class="pm ms it py b gy qc qd l qe qf"># Test accuracy rate</span><span id="7fcf" class="pm ms it py b gy qg qd l qe qf">sum(diag(conf.matrix)/sum(conf.matrix))</span><span id="1b79" class="pm ms it py b gy qg qd l qe qf">[1] 0.8899005</span><span id="1721" class="pm ms it py b gy qg qd l qe qf"><em class="me"># Test error rate</em></span><span id="4e1d" class="pm ms it py b gy qg qd l qe qf">1 - sum(drag(conf.matrix)/sum(conf.matrix))</span><span id="c953" class="pm ms it py b gy qg qd l qe qf">[1] 0.1100995</span></pre><p id="0aef" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如您可能注意到的，测试准确率+测试错误率= 1，我提供了计算每个值的多种方法。</p><pre class="ks kt ku kv gt px py pz qa aw qb bi"><span id="0318" class="pm ms it py b gy qc qd l qe qf"># ROC and AUC<br/>knn_model = knn(train=XTrain, test=XTrain, cl=YTrain, k=20,prob=TRUE)prob &lt;- attr(knn_model, “prob”)</span><span id="3a7d" class="pm ms it py b gy qg qd l qe qf">prob &lt;- 2*ifelse(knn_model == “-1”, prob,1-prob) — 1</span><span id="a779" class="pm ms it py b gy qg qd l qe qf">pred_knn &lt;- prediction(prob, YTrain)</span><span id="bd48" class="pm ms it py b gy qg qd l qe qf">performance_knn &lt;- performance(pred_knn, “tpr”, “fpr”)# AUC</span><span id="0f8a" class="pm ms it py b gy qg qd l qe qf">auc_knn &lt;- performance(pred_knn,”auc”)<a class="ae lh" href="http://twitter.com/y" rel="noopener ugc nofollow" target="_blank">@y</a>.values</span><span id="9bd0" class="pm ms it py b gy qg qd l qe qf">auc_knn</span><span id="8c68" class="pm ms it py b gy qg qd l qe qf">[1] 0.8470583</span><span id="f0b7" class="pm ms it py b gy qg qd l qe qf">plot(performance_knn,col=2,lwd=2,main=”ROC Curves for KNN”)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/5db830ac06121f534dacd452da8128d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YR6-gjLDj4GPLRjvlLYX_Q.png"/></div></div></figure><p id="e358" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">总之，我们已经了解了什么是 KNN，以及在 r 中构建 KNN 模型的流程。此外，我们还掌握了进行 K-Fold 交叉验证的技巧，以及如何在 r 中实现代码</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="c5c2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">我的</em><a class="ae lh" href="https://github.com/LeihuaYe/Machine-Learning-Rare-Event-Classification" rel="noopener ugc nofollow" target="_blank"><em class="me">Github</em></a><em class="me">上有完整的 Python 代码。</em></p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><p id="f7a9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> Medium 最近进化出了它的</em> <a class="ae lh" href="https://blog.medium.com/evolving-the-partner-program-2613708f9f3c" rel="noopener"> <em class="me">作家伙伴计划</em> </a> <em class="me">，支持像我这样的普通作家。如果你还不是订户，通过下面的链接注册，我会收到一部分会员费。</em></p><div class="nq nr gp gr ns nt"><a href="https://leihua-ye.medium.com/membership" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">阅读叶雷华博士研究员(以及其他成千上万的媒体作家)的每一个故事</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">leihua-ye.medium.com</p></div></div><div class="oc l"><div class="ql l oe of og oc oh lb nt"/></div></div></a></div></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="b726" class="mr ms it bd mt mu op mw mx my oq na nb ki or kj nd kl os km nf ko ot kp nh ni bi translated">我的数据科学面试序列</h1><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/essential-sql-skills-for-data-scientists-in-2021-8eb14a38b97f"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">2021 年数据科学家必备的 SQL 技能</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">数据科学家/工程师的四项 SQL 技能</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="qm l oe of og oc oh lb nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/5-python-coding-questions-asked-at-faang-59e6cf5ba2a0"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">FAANG 在 2021 年提出这 5 个 Python 问题</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">数据科学家和数据工程师的必读！</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="qn l oe of og oc oh lb nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/statistical-simulation-in-python-part-2-91f71f474f77"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">FAANG 在 2021 年询问这些 Python 模拟</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">数据科学和数据工程面试的必读材料，第 2 部分</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="qo l oe of og oc oh lb nt"/></div></div></a></div></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h1 id="b1da" class="mr ms it bd mt mu op mw mx my oq na nb ki or kj nd kl os km nf ko ot kp nh ni bi translated">喜欢读这本书吗？</h1><blockquote class="qp qq qr"><p id="99b8" class="li lj me lk b ll lm kd ln lo lp kg lq qs ls lt lu qt lw lx ly qu ma mb mc md im bi translated">请在<a class="ae lh" href="https://www.linkedin.com/in/leihuaye/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://www.youtube.com/channel/UCBBu2nqs6iZPyNSgMjXUGPg" rel="noopener ugc nofollow" target="_blank"> Youtube </a>上找到我。</p><p id="2f98" class="li lj me lk b ll lm kd ln lo lp kg lq qs ls lt lu qt lw lx ly qu ma mb mc md im bi translated">还有，看看我其他关于人工智能和机器学习的帖子。</p></blockquote></div></div>    
</body>
</html>