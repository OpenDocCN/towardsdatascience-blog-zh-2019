<html>
<head>
<title>Understanding RNNs, LSTMs and GRUs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解 rnn、LSTMs 和 gru</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-rnns-lstms-and-grus-ed62eb584d90?source=collection_archive---------16-----------------------#2019-11-10">https://towardsdatascience.com/understanding-rnns-lstms-and-grus-ed62eb584d90?source=collection_archive---------16-----------------------#2019-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="126f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个<strong class="jp ir">递归神经网络</strong> (RNN)是一个基本神经网络的变体。rnn 适用于处理序列数据，如自然语言处理和音频识别。直到最近，他们还饱受短期记忆问题的困扰。在这篇文章中，我将尝试解释什么是(1) RNN，(2)消失梯度问题，(3)这个问题的解决方案被称为<strong class="jp ir">长短期记忆</strong> (LSTM)和<strong class="jp ir">门控循环单元</strong> (GRU)。</p><h1 id="f41f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">什么是 RNN？</h1><p id="912a" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">首先，让我们介绍基本的神经网络架构，神经网络通过 3 个基本步骤进行训练:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/51f82cd606ad577b61e0bbd0833dd1c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/0*PKXZYjUwrVzb7rGk.png"/></div></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/bf03b85071a7a613ccdbf39d211ad924.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*Gx7hpYcv3TgtsGtu.gif"/></div></figure><p id="da38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(1)进行预测的向前传球。</p><p id="1c2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(2)使用<strong class="jp ir">损失函数</strong>将预测与地面真实情况进行比较。损失函数输出一个<strong class="jp ir">误差值</strong>。</p><p id="dcea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(3)使用该误差值，执行<strong class="jp ir">反向传播</strong>，其计算网络中每个节点的梯度。</p><p id="d838" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相比之下，RNN 包含一个<strong class="jp ir">隐藏状态</strong>，它从先前的状态中获取信息:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/d3c0efce6f90ecbb9bf778bded89877d.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*3htVyljT69LGMnml.gif"/></div></figure><p id="921f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">隐藏状态的概念类似于为了做出更准确的预测而对顺序数据进行整合。考虑一下，如果您的数据仍然是运动中的球的照片，预测球的运动会容易得多:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/0db996e225653482e611fb8dfec3d648.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/0*_RHCAM4bfxdqGhdX.png"/></div></figure><p id="a9d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">没有序列信息，就不可能预测它的移动方向，相反，如果你知道以前的位置:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/81ea698e1f05d53a1bb1d850a8d087ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*KTdxK3i-UYojldwy.gif"/></div></figure><p id="5002" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">预测会更准确。同样的逻辑也适用于估计句子中的下一个单词，或者歌曲中的下一段音频。这个信息是隐藏状态，它是先前输入的表示。</p><h1 id="2a5c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">消失梯度问题</h1><p id="02ff" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">然而，这变得有问题，为了训练一个 RNN，你使用一个叫做<strong class="jp ir">通过时间</strong> (BPTT)的反向传播的应用。由于每一层的权重通过<strong class="jp ir">链规则</strong>进行调整，它们的梯度值将随着其在每个时间步长的传播而呈指数缩小，最终“消失”:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/7e025415552127f02be188ec7ad8978e.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*lym0Kq6cJVytuxjD.gif"/></div></figure><p id="6258" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了在 NLP 应用程序中说明这一现象:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/f58cc45d3473b77b15b121123086ab29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*5LuVAMkwooXQoVo8kJf1TQ.png"/></div><figcaption class="ma mb gj gh gi mc md bd b be z dk">Vanishing Gradient</figcaption></figure><p id="9172" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过输出 5，您可以看到来自“什么”和“时间”的信息几乎都消失了，如果没有这些信息，您认为您能多好地预测“是”和“它”之后的内容呢？</p><h1 id="f5a0" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">LSTM 和 GRU 作为解决方案</h1><p id="94f3" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">LSTMs 和 GRUs 是作为消失梯度问题的解决方案而创建的。它们内部有一种叫做<strong class="jp ir">门</strong>的机制，可以调节信息的流动。</p><p id="55aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于 LSTM，有一个主<strong class="jp ir">单元状态</strong>，或传送带，以及几个控制新信息是否可以进入传送带的门:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi me"><img src="../Images/c32efdbe107a1aaa2964e9b32f9342ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DgJ79QVOToF1AywBLqaCnQ.png"/></div></div></figure><p id="14a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上面的问题中，假设我们要确定新句子中说话人的性别。我们将不得不选择性地忘记关于先前状态的某些事情，即，关于鲍勃是谁，他是否喜欢苹果，并记住其他事情，爱丽丝是一个女人，她喜欢桔子。</p><p id="7a32" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">放大来看，LSTM 中的门通过三个步骤来完成这一过程:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mj"><img src="../Images/6a4ce17917a31353df760418e0fc300c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Ul3gv7pe5akx1a_g-gNnwQ.gif"/></div></div></figure><p id="1fea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(1)决定忘记什么(状态)<br/> (2)决定记住什么(状态)<br/> (3)状态的实际“遗忘”和更新<br/> (4)输出的产生</p><p id="176b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总结一下，在 LSTM 中，遗忘门(1)决定哪些相关内容要从前面的步骤中保留。输入(2)门决定从当前步骤添加什么相关信息。输出门(4)决定下一个隐藏状态应该是什么。</p><p id="f50a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于作为新一代 rnn 的 GRU，它与 LSTM 非常相似，只是 GRUs 摆脱了小区状态，使用隐藏状态来传递信息。它也只有两个门，一个<strong class="jp ir">复位门</strong>和<strong class="jp ir">更新门</strong>:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mk"><img src="../Images/eb55dcc18ec2e3d7e36990bae4b2983f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9MQn4ZxwA0As8S8T.png"/></div></div></figure><p id="50e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(1)更新门的作用类似于 LSTM 的遗忘和输入门，它决定保留什么信息和丢弃什么信息，以及添加什么新信息。</p><p id="2e5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">候选隐藏状态的实现方式与 LSTM 相同，不同之处在于，在候选计算内部，它将重置一些先前的门(0-1 之间的向量，由线性变换定义)。</p><p id="e72f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">(2)重置门用于决定忘记多少过去的信息。</p><p id="58f1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">插图摘自<br/> 1。<a class="ae ml" href="https://www.youtube.com/redirect?v=nFTQ7kHQWtc&amp;event=video_description&amp;redir_token=xfOh4nt5k1IP4vmqji3oxaJNatN8MTU3MzQ0Mjk5MUAxNTczMzU2NTkx&amp;q=http%3A%2F%2Fbit.ly%2F2Hc2zhf" rel="noopener ugc nofollow" target="_blank">麻省理工 6。s094:Lex frid man<br/>2 在 2017 年冬季</a>讲授的自动驾驶汽车深度学习。LSTM 和 GRU 的图解指南:一步步的解释</p></div></div>    
</body>
</html>