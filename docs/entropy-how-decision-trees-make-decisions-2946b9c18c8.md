# 熵:决策树如何做决策

> 原文：<https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8?source=collection_archive---------0----------------------->

## 非常有效的机器学习算法背后的简单逻辑和数学

你是一名训练中的数据科学家。从编写第一行 Python 或 R 代码开始，您已经走过了漫长的道路。你对 Scikit 了如指掌。你现在花在卡格尔身上的时间比脸书还多。您对构建令人敬畏的随机森林和其他基于树的集合模型并不陌生，它们可以完成这项工作。然而，如果不彻底，你什么都不是。你想要更深入地挖掘和理解流行的机器学习模型背后的一些错综复杂的东西和概念。我也是。

在这篇博文中，我将介绍熵的概念，作为统计学中的一个一般主题，这将允许我进一步介绍信息增益的概念，并随后解释为什么这两个基本概念形成了决策树如何基于我们提供给它们的数据来构建自己的基础。

没错。那我们继续吧。

熵是什么？用最通俗的术语来说，熵只不过是无序的度量。(你也可以把它当成纯度的衡量标准。你会明白的。我喜欢无序，因为它听起来更酷。)

熵的数学公式如下-

![](img/1a5dea41777914daa35b97077611fea1.png)

Entropy. Sometimes also denoted using the letter ‘H’

其中“Pi”只是我们数据中某个元素/类“I”的频繁出现概率。为了简单起见，假设我们只有两个类，一个正类和一个负类。因此，这里的“我”可以是+或(-)。因此，如果我们的数据集中总共有 100 个数据点，其中 30 个数据点属于正类，70 个数据点属于负类，那么‘P+’将是 3/10，而‘P-’将是 7/10。很简单。

如果我用上面的公式计算这个例子中我的类的熵。这是我会得到的。

![](img/c8bec78d2f537ee326890540060294ee.png)

这里的熵大约是 0.88。这被认为是一种高熵，一种高水平的无序(意味着低水平的纯度)。熵的度量值介于 0 和 1 之间。(根据数据集中类的数量，熵可以大于 1，但这意味着同样的事情，非常高的无序度。为了简单起见，本博客中的例子将具有介于 0 和 1 之间的熵)。

请看下图。

![](img/5d34ee27e31d7c5abe6bc64fe0ab4b88.png)

Provost, Foster; Fawcett, Tom. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking

x 轴测量每个气泡中属于正类的数据点的比例，y 轴测量它们各自的熵。很快，你可以看到图表的倒“U”形。熵在极端情况下是最低的，这时气泡要么不包含正实例，要么只包含正实例。也就是说，当气泡是纯的时，无序度为 0。当气泡在正负两种情况下平均分配时，熵在中间最高。极度无序，因为没有多数。

为什么熵是用以 2 为底的对数来度量的，或者为什么熵是在 0 和 1 之间度量的，而不是其他范围，这有关系吗？不。这只是一个度量。知道它是怎么来的并不重要。知道如何阅读它以及它告诉我们什么是很重要的，这一点我们刚刚在上面做了。熵是对无序或不确定性的度量，机器学习模型和数据科学家的目标通常是减少不确定性。

现在我们知道如何测量无序。接下来，我们需要一个度量标准，在给定额外信息(特征/独立变量)的情况下，测量目标变量/类别中这种无序的减少。这就是信息增益的用武之地。数学上它可以写成:

![](img/f2735a9986d86f660146a305f7ca67ba.png)

Information Gain from X on Y

我们简单地从 Y 的熵中减去给定 X 的 Y 的熵，来计算给定关于 Y 的额外信息 X 的关于 Y 的不确定性的减少，这被称为信息增益。这种不确定性减少得越多，从 x 获得的关于 Y 的信息就越多。

让我用一个简单的列联表来举例说明，然后我将把所有这些放在一起，看决策树如何使用熵和信息增益来决定在数据集上训练时根据什么特征来分割它们的节点。

# **举例:列联表**

![](img/f067d8800438f50249bcd74251cc72a0.png)

Contingency Table

这里，我们的目标变量是负债，可以采用两个值“正常”和“高”，我们只有一个称为信用评级的特征，可以采用值“优秀”、“良好”和“差”。总共有 14 个观察值。其中 7 个属于正常负债类，7 个属于高负债类。所以这是一个平均分配。对顶行进行总结，我们可以看到有 4 个观察值对于功能信用评级具有极佳的价值。此外，我还可以看到我的目标变量是如何划分为“优秀”信用评级的。对于其信用评级值为“优秀”的观察，有 3 个属于正常负债类别，只有 1 个属于高负债类别。我同样可以从列联表中计算出其他信用评级的值。

对于这个示例，我将使用这个列联表来计算我们的目标变量本身的熵，然后在给定关于特征、信用评级的附加信息的情况下计算我们的目标变量的熵。这将允许我计算“信用评级”为我的目标变量“负债”提供了多少额外的信息。

那我们就开始吧。

![](img/dba9e8f18a5cc21004839abd2645a4dd.png)

我们的目标变量的熵是 1，由于类标签“正常”和“高”之间的平均分裂而处于最大无序状态。我们的下一步是给定关于信用评分的额外信息，计算我们的目标可变负债的熵。为此，我们将计算每个信用评分值的负债熵，并使用每个值中结束的观察比例的加权平均值将它们相加。当我们在决策树的上下文中讨论这个问题时，我们为什么使用加权平均值将变得更清楚。

![](img/c01ac9a9656fdc86c36ae16f0970813f.png)

给定特征信用评级，我们得到了目标变量的熵。现在我们可以从信用评级中计算负债的信息增益，看看这个特征的信息量有多大。

![](img/4be64613f17710abdc681b1dbbc56d19.png)

了解信用评级有助于我们减少目标变量负债的不确定性！这难道不是一个好的功能应该做的吗？为我们提供目标变量的信息？这就是决策树如何以及为什么使用熵和信息增益来确定在哪个特征上分裂它们的节点，以更接近预测每次分裂的目标变量，并且还确定何时停止分裂树！(当然除了像最大深度这样的超参数)。让我们看看另一个使用决策树的例子。

# 示例:决策树

考虑一个例子，我们正在构建一个决策树来预测给一个人的贷款是否会导致注销。我们的全部人口由 30 个实例组成。16 个属于注销类，另外 14 个属于非注销类。我们有两个特性，分别是可以取两个值-->“< 50K” or “>50K”的“Balance”和可以取三个值-->“OWN”、“RENT”或“OTHER”的“Residence”。我将向您展示决策树算法如何使用熵和信息增益的概念来决定首先分割哪个属性，哪个特征提供更多信息，或者减少两个目标变量中更多的不确定性。

**特点一:平衡**

![](img/5a862e3e3a93d6af55dc01854805bee0.png)

Provost, Foster; Fawcett, Tom. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking

圆点是分类正确的数据点，星号是未注销的数据点。在属性平衡上分割父节点给我们 2 个子节点。左侧节点获得了总观察值中的 13 个，其中 12/13 ( 0.92 概率)观察值来自注销类，而只有 1/13( 0.08 概率)观察值来自未写入类。右节点得到总观察值的 17，其中 13/17( 0.76 概率)观察值来自非注销类，4/17 ( 0.24 概率)来自注销类。

让我们计算父节点的熵，看看通过平衡分裂，树可以减少多少不确定性。

![](img/b2f6d1e95221724da1fa5366373f0fe8.png)

分割特征，“平衡”导致目标变量的信息增益为 0.37。让我们做同样的事情为功能，“住宅”，看看它如何比较。

**特色二:住宅**

![](img/bf4f23d32f81ffd529a90914f2e40f3b.png)

Provost, Foster; Fawcett, Tom. Data Science for Business: What You Need to Know about Data Mining and Data-Analytic Thinking

在 Residence 上分割树给了我们 3 个子节点。左边的子节点得到总观察值中的 8 个，其中 7/8 (0.88 概率)观察值来自注销类，而只有 1/8 (0.12 概率)观察值来自非注销类。中间的子节点得到总观察值中的 10 个，其中 4/10 (0.4 概率)观察值属于核销类，6/10( 0.6 概率)观察值属于非核销类。右边的子节点得到总观察值中的 12 个，其中 5/12 ( 0.42 概率)观察值来自注销类，7/12 ( 0.58)观察值来自非注销类。我们已经知道了父节点的熵。我们只需要计算分裂后的熵来计算“居住”的信息增益

![](img/942216291531439b2a4a4b3a323de420.png)

从特征、平衡中获得的信息几乎是从住所中获得的信息的 3 倍！如果你回头看一下图表，你可以看到平衡分裂的子节点看起来确实比驻留的子节点更纯粹。然而，最左侧的居住节点也非常纯粹，但这是加权平均值发挥作用的地方。尽管该节点非常纯，但它具有最少的总观测值，并且当我们从驻留分裂计算总熵时，一个结果贡献了其纯度的一小部分。这一点很重要，因为我们寻求的是一个特性的整体信息能力，我们不希望我们的结果被一个特性中罕见的值所扭曲。

就其本身特性而言，Balance 比 Residence 提供了更多关于我们的目标变量的信息。它减少了我们的目标变量中更多的无序。决策树算法将使用此结果，使用 Balance 对我们的数据进行第一次拆分。从现在开始，决策树算法将在每次分割时使用这个过程来决定下一个要分割的特征。在现实世界的场景中，对于两个以上的特征，首先对信息最丰富的特征进行分割，然后在每次分割时，需要重新计算每个附加特征的信息增益，因为它与每个特征本身的信息增益不同。熵和信息增益必须在已经进行了一次或多次分裂之后计算，这将改变结果。决策树会重复这个过程，直到它变得越来越深，直到它达到预定义的深度，或者没有额外的分裂可以导致超过某个阈值的更高的信息增益，该阈值通常也可以被指定为超参数！

你有它！你现在知道什么是熵和信息增益，以及它们是如何计算的。您了解决策树如何独自或在基于树的集合中决定分割要素的最佳顺序，并决定在给定数据上训练自己时何时停止。如果你必须向某人解释决策树是如何错综复杂地工作的，希望你不会做得太糟糕。

我希望你能从这篇文章中获得一些价值。如果我遗漏了什么，或者有什么不准确的地方，或者如果你有任何反馈，请在评论中告诉我。我将非常感激。谢谢你。