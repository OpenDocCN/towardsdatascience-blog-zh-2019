# 我应该开源我的模型吗？

> 原文：<https://towardsdatascience.com/should-i-open-source-my-model-1c109188b164?source=collection_archive---------8----------------------->

## 决定是否开源机器学习模型的最佳实践

我已经研究开源机器学习与敏感性的问题很长时间了，特别是在灾难应对的背景下:什么时候公开发布数据或模型是对的/错的？本文列出了一些常见问题、当今最佳实践的答案，以及我在哪里遇到过这些问题的一些例子。

![](img/b04c7164335ce3f8f547ac5bbfd97dc0.png)

What are the risks of sharing your Machine Learning model?

OpenAI 本周引发了一场风暴，当时他们发布了新的研究，显示了许多人工智能任务的最新成果，但他们决定不发布他们的机器学习模型，这违背了研究界的普遍做法:

> “由于我们担心该技术的恶意应用，我们不会发布经过训练的模型。”

对 OpenAI 决定的批评包括它如何限制了研究社区复制结果的能力，以及这一行动本身如何加剧了媒体对 AI 的恐惧，这种恐惧现在被夸大了。

正是这条推文首先引起了我的注意。Anima Anandkumar 在弥合机器学习的研究和实际应用之间的差距方面有很多经验。我们是 AWS 的同事，最近一起讨论了机器学习从博士到产品的问题([https://vimeo.com/272703924](https://vimeo.com/272703924))。

Stephen Merity 很好地总结了社交媒体的反应，他感叹说，机器学习社区在这方面几乎没有经验:

我在机器学习和灾难响应方面的经验很少。在来美国完成斯坦福大学自然语言处理博士学位之前，我在塞拉利昂和利比里亚从事冲突后发展工作，从那以后，我一直在工业和灾难应对领域工作。公平地说，与我一起工作的大多数人以前从未考虑过数据和模型的敏感性，因此本文希望能够弥合这一差距。

本文旨在作为一个独立的资源，供提出这些问题的人参考。如果你想看引发它的文章，这里有 OpenAI 的帖子，其中包含了他们的担忧，如生成类似假新闻的内容和在网上假冒他人的能力:

[](https://blog.openai.com/better-language-models/) [## 更好的语言模型及其含义

### 我们已经训练了一个大规模的无监督语言模型，它可以生成连贯的文本段落，实现…

blog.openai.com](https://blog.openai.com/better-language-models/) 

下面是引起如此多讨论的推文——你可以阅读在我写这篇文章的时候仍然收到的回复:

OpenAI 保留他们的模型，而只提前给记者，这是对还是错？我让你来决定。对我来说(正如我下面要讨论的), OpenAI 在两个本可以缓解问题的领域失败了:调查虚假内容是否能被检测出来；并发布多种语言的模型来对抗对英语的偏见。

以下是一些问题，您可以在评估自己是否发布模型或数据集的决策时使用:

## 应该质疑是否开源我的模型？

是的。如果你的模型是建立在私有数据上的，它可以被逆向工程提取私有数据。

## 我的模型 100%来自公开数据，那么我需要质疑是否开源我的模型吗？

是的。如果在新的上下文中重新发布，发布的数据可能会变得敏感，聚合数据(包括机器学习模型)可能会变得比单个数据点更敏感。

在阿拉伯之春期间，我看到许多人在推特上谈论他们当地的情况:道路封锁、难民等。虽然它们是“公开”的推文，但这些推文显然是为少数几个追随者写的，他们没有意识到报道道路封闭也有助于描绘部队移动的画面。作为*不*做什么的一个例子，其中一些推文被复制到联合国控制的网站并重新发布，没有任何机制让原作者将它们从联合国网站上删除。中东和北非的许多参与者将联合国视为负面的外国影响(或入侵者)，因此发推文的人被视为合作者——他们不在乎这些人是否只想与少数追随者分享信息。

所以，你需要问自己:将数据或模型重新文本化，使其现在由我自己或我的组织发布，会有什么效果？

当单个数据点不敏感时，聚合数据被视为敏感也是很常见的。这是许多军事组织的标准做法:当他们从一组来源汇总数据时，他们会重新评估汇总信息的敏感程度。聚合通常是统计或无监督机器学习的结果，但基于该数据构建的监督模型同样适用。关于聚合如何改变军队中数据敏感性的公开示例，请参见最近的一个案例，其中军队中使用 Strava 的跑步者意外泄露了基地的位置，当时它显示了军队中跑步最多的人的热图:

[](https://www.wired.com/story/strava-heat-map-military-bases-fitness-trackers-privacy/) [## 斯特拉瓦热图显示，即使是军队也无法对社会数据保密

### 现代版的二战时期的警告“信口开河会沉船”可能是“FFS 不要分享你的 Fitbit…

www.wired.com](https://www.wired.com/story/strava-heat-map-military-bases-fitness-trackers-privacy/) 

许多组织选择采用类似的政策。Medium 就是其中之一:在写这篇文章时，根据 Medium 的策展指导方针，我不能“暴露某人的身份，包括暴露个人信息或聚集公共信息”。你也应该这样做。

所以，你应该经常问自己:在我的模型中，数据的集合比单个数据点更敏感吗？

## 我如何评估风险？

使用与安全性相同的模型:权衡误用的成本与它为不良行为者提供的价值。

在安全方面，你把每一个策略都视为“易破的”。目标是使破坏某些安全措施的成本高于您所保护的数据的价值。所以，不值得坏演员花时间去破它。

对于那些出于负面原因想要使用你的研究论文中的模型的人来说，复制这些模型的成本值得吗？你应该明确这一点。这是你决定是否开源的一个因素。

在 OpenAI 的案例中，他们可能已经决定发布该决定的风险简介，大致如下:我们认为不开源该模型的决定足以阻止互联网上大多数孤独的巨魔重新创建该模型。但我们承认，一大群科学家(可能是国家资助的)以我们的研究论文为指导来重建模型是不够的。

## 我应该相信记者对风险做出决定吗？

不。或者至少，不是没有问题。记者必须出售内容，而更耸人听闻的内容往往会卖得更多。有意识或无意识地，一个记者可能倾向于开源，因为这样他们更容易写它。另一方面，不开源数据的决定可能会导致耸人听闻的关于该决定所包含的危险的报道(就像 OpenAI 的案例一样)。

我见过很多灾难中糟糕的新闻报道导致的可预防的死亡案例。在 2014 年西非埃博拉疫情中，我预测:

> 每有一个人感染埃博拉病毒，就会有十个人死于其他可预防的疾病。

[](https://medium.com/@robert.munro/the-silent-victims-of-ebola-e1a8f83185a9) [## 埃博拉的沉默受害者

medium.com](https://medium.com/@robert.munro/the-silent-victims-of-ebola-e1a8f83185a9) 

我是应对疫情的大多数主要政府和援助组织的顾问，因为我之前曾在该地区生活过，并分别在东非从事埃博拉疫情追踪工作。我警告过几乎每一家新闻机构，无论是本地的还是国际的，关于围绕报道的炒作和它可能导致的死亡。

一小群记者听了，但大多数没有。疫情结束后，当塞拉利昂卫生部副部长在旧金山的一次会议上发言时，她报告了这些完全相同的令人悲伤的数字:他们估计，每有一个人直接死于埃博拉，就有 10 人因为不去诊所而死于其他疾病。

所以，在和记者谈论你的研究时要非常小心。

## 我如何确保记者负责任地谈论我的机器学习研究？

接受媒体培训！许多组织都有媒体培训，即使只有几个小时也会有所帮助。我不能在这篇文章中总结你需要知道的所有事情，但是这里是我发现的通常有效的最重要的事情:

问记者他们的故事是关于什么的。

如果他们写的是机器学习研究的进展，那么你可能没问题。如果他们写的是“人工智能的危险”，或“假新闻”，或“干涉选举”，那么你应该越来越小心你的采访可能会被歪曲以适应他们的叙述。

问一个故事是关于什么的策略对我来说很有效，但我记得只有一个例外:一名 BBC 记者正在写一篇关于英语如何主宰互联网的文章。我在接受采访时说，不，英语在互联网上的份额一直在稳步下降:人们更喜欢他们的主要语言，英语正在成为“互联网的第二语言”。但是他们报道我说“英语正在成为互联网的语言”。如果发生这种情况，你也无能为力 BBC 的影响力比我否认我说过这话的推特更大。你可以要求媒体机构修改文章，或者至少公开声明你被错误引用了。

## 我应该相信政府会做出风险决策吗？

不。很明显，你不应该违反法律。但不能因为合法就说可以。政府是一群像其他人一样的人，试图让他们的头脑面对机器学习的真实和不那么真实的威胁。

政府也容易受到媒体的影响。为应对埃博拉疫情，利比里亚政府关闭了边境。在边境工作过之后，我知道这完全是一场闹剧。边界是一系列相连的村庄、河流、小溪和森林小径，早在今天的官方边界之前就存在了。“关闭”边境和共享边境感染数据让政府看起来很果断，但最终几乎没有人能免受埃博拉病毒的影响，而是让他们远离诊所去治疗其他可治疗的疾病，让情况变得更加糟糕。

像记者一样，将政府视为重要的合作伙伴，但要认识到你们都有不同的议程，其中许多议程将是一致的，但不是全部。

## 我应该从我的模型中调查负面用例的解决方案吗？

是啊！这是 OpenAI 的失败之一。如果他们认为他们模型的输出可以用来制造假新闻，那么这是可以检验的:

你能创建一个文本分类任务来区分人类书写的内容和 OpenAI 模型的输出吗？

这是一个 OpenAI 可以在几天内运行的实验，它将让我们更好地了解这到底是一个多大的问题。

最近，我与脸书进行了长时间的会谈，讨论加入他们的角色，负责发现假新闻。从一个正在解决这个问题的人的角度来看这个问题，这是我想知道的第一件事:我能通过编程来检测这种模型输出，以便解决它吗？我最终决定担任目前的职位，原因与这个角色本身无关——我认为打击脸书的假新闻是任何人现在都可以做的最重要的事情之一——open ai 的这项额外研究将会有所帮助。更好的是，如果您可以创建一个可以识别生成内容的模型池，那么创建击败所有模型并通过自动检测系统的生成内容将变得更加困难。

如果您可以定量地证明，数据的负面用例更容易或更难应对，那么这将是您决策过程中的一个因素。

## 这是机器学习的新问题吗？

不，而且你可以从过去的经验中学到很多。

2014—2015 年，沙特阿拉伯政府曾三次找我帮忙监控社交媒体上的异见人士。当时我是 Idibon 的首席执行官，这是一家大约 40 人的人工智能公司，拥有大量语言中最准确的自然语言处理技术，因此我们自然被视为最适合他们用例的技术。沙特阿拉伯的一个部门首先直接接触了我们，然后是间接接触，一次是通过一家精品咨询公司，一次是通过世界上五大咨询公司之一。在每一个案例中，公开的目标都是帮助那些抱怨政府的人。经过与沙特阿拉伯和机器学习专家的仔细磋商，我们决定使用一个识别投诉的系统来识别持不同政见者。由于沙特阿拉伯是一个不经审判就迫害持不同政见者的国家，而且经常是暴力迫害，我们拒绝提供帮助。

如果你正面临类似的困境，请寻找有知识深度的人来谈论最受影响的社区(理想情况下是该社区内的人)以及过去面临类似机器学习问题的人。

## 假新闻是新问题吗？

不。宣传可能和语言本身一样古老。

2007 年，当我护送记者报道塞拉利昂选举时，我们不断听到暴力报道。我们将跟踪这些报道，以发现没有实际的暴力行为。原来是一家盗版电台在播放假新闻，其中一些是由合法电台接收的，假新闻的意图是将一个或多个政党的支持者描绘成暴力分子，并可能吓得人们根本不去投票。

在塞拉利昂最近的选举中，我看到社交媒体上流传着类似的关于暴力和选举舞弊的假新闻。大型社交媒体公司负责假新闻的人都悄悄地向我承认，他们无法识别塞拉利昂和其他许多国家的大多数语言中的假新闻。

所以，宣传已经在这里很长一段时间了，它使用了一切可用的技术来扩大信息的传播。最大的差距是在对抗宣传的方式上，这意味着在大多数情况下英语之外的更好的人工智能。

## 我应该专注于平衡机器学习的坏用例与更明显好的用例吗？

是的。通过发布主要具有积极应用领域的模型，很容易对世界产生积极的影响。限制一个有很多负面应用领域的模型发布，很难对世界产生正面影响。

这是 OpenAI 的另一个失败，他们缺乏多样性。与其他任何研究小组相比，OpenAI 发布了仅适用于英语和(很少)少数其他高特权语言的模型和研究。英语只占世界日常对话的 5%。在句子中的词序需要有多严格，在标准化拼写中，以及在“单词”作为机器学习功能的原子单位如何有用方面，英语都是一个异数。OpenAI 的研究依赖于这三个方面:词序、词作为特征、拼写一致。它能适用于世界上大多数的语言吗？我们不知道，因为他们没有测试过。OpenAI 的研究告诉我，我们需要担心英语的这种内容生成，但它没有告诉我今天假新闻流通的 100 多种其他语言的风险。

坦率地说，OpenAI 的多样性问题根深蒂固。当我和几十个人一起注意到一次人工智能会议有 30 多位全是男性的演讲者，并且 OpenAI 的首席科学家是第一位演讲者时，OpenAI 忽视了这些抱怨。

尽管不同的人公开或私下发了一些信息，但我不知道 OpenAI 采取了什么行动来解决这个多样性表现的问题。

我个人拒绝所有我认为会议阵容在机器学习社区中延续偏见的演讲邀请，我知道许多人也这样做。很可能 OpenAI 对多样性的更宽松的态度导致了不多样化的研究。在实践中，我一般不相信世界上 95%的语言只有英语的结果。OpenAI 有很多很好的基础研究，比如如何让任何模型更轻量级，从而在更多的上下文中可用，但他们的英语语言重点限制了积极的用例。

如果你不想踏入假新闻这类应用的灰色地带，那么就选择一个天生更有影响力的研究领域，比如低资源语言中健康相关文本的语言模型。

## 我需要多深入地考虑用例的敏感性？

一直到各个领域。当我为 AWS 的命名实体解析服务运行产品时，我们必须考虑是否要将街道级地址标识为一个显式字段，并潜在地将坐标映射到该地址。我们认为这是固有的敏感信息，不应该在通用解决方案中产品化。

在任何研究项目中考虑这一点:您是否在您的模型中明确或隐含地识别敏感信息？

## 我是否应该开源我的模型，仅仅因为其他人都这样做？

不。你应该总是质疑你自己的影响。

无论你是否同意 OpenAI 的决定，他们在做出明智的决定而不是盲目跟风发布完整模型方面是正确的。

## 我还应该担心什么？

可能有很多东西我今天没有讲到！我写这篇文章是作为对 OpenAI 昨天宣布的快速回应。如果有需求，我会花更多时间分享最佳实践！