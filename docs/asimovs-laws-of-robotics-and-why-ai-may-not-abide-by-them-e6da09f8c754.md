# 阿西莫夫的机器人定律，以及为什么人工智能可能不遵守它们

> 原文：<https://towardsdatascience.com/asimovs-laws-of-robotics-and-why-ai-may-not-abide-by-them-e6da09f8c754?source=collection_archive---------10----------------------->

"但是如果我们最终陷入终结者的场景呢？"问这样的问题无可厚非，因为通过电影和科幻故事，机器人接管的情况几乎无处不在，因此构成了我们对人工智能(AI)未来的印象。然而，既然人类能够通过法律生活和合作，为什么不为人工智能应用法律呢？进入阿西莫夫机器人定律！一如既往，为了更好地了解未来，让我们来回顾一下过去。

## 三大定律

艾萨克·阿西莫夫(1920-1992)除了是生物化学教授之外，还被认为是他那个时代“三大”科幻作家之一。在 1900 年代中期，他假设了三条定律，如果遵守的话，将会阻止机器人起义。它们如下:

> 法则一:机器人不得伤害人类，也不得坐视人类受到伤害。
> 
> 法则二:机器人必须服从人类给它的命令，除非这些命令与第一法则相冲突。
> 
> 定律三:机器人必须保护自己的存在，只要这种保护不与第一或第二定律相冲突。

现在，如果你熟悉编程，你就会知道机器是从 0 开始计数的，而不是从 1 开始计数的(MATLAB 除外，不过我们先不去说这个)。因此，也有一个由[电脑爱好者](https://www.youtube.com/watch?v=7PKx3kS7f4A)提出的第 0 定律，指的是集体而不是个人，它是这样的:

> 法则 0:机器人不能伤害人类，或者，通过不作为，让人类受到伤害。

那么，如果法律早在 20 世纪 50 年代就已经制定出来了，为什么会有这么多恐惧呢？是什么促使埃隆·马斯克甚至斯蒂芬·霍金将人工智能标榜为人类“[最大的生存威胁](https://www.bbc.co.uk/news/technology-30290540)”？因为，长话短说，阿西莫夫定律不起作用。

## 计划中的缺陷

回到现在。为了与阿西莫夫的想法保持一致，让我们假设我们确实有足够复杂的人工智能代理来应用这些法律。为了便于讨论，让我们也假设，尽管是叙事机制的法则，它们也适用于现实世界。

技术问题:如果法律是英文的，而一个代理只处理中文，怎么办？或者，即使代理是在美国制造的，我们怎么知道它了解法律？因此，我们需要一种方法来(I)翻译法律,( ii)将单词背后的含义翻译成每一种可能的语言(为了涵盖所有可能性，还必须使用像拉丁语这样的死语言以及二进制机器语言)。对于人类来说，这些任务是非常相关的。另一方面，对于机器来说，这是两个非常不同的任务。第一个任务只是指用不同的语言产生相应的句子串，而第二个任务包括理解这些串。只做第一个任务的一个类比是，如果我告诉你用西班牙语唱 Despacito 的歌词。你可能说得很好，但你不知道它们是什么意思(假设你不懂西班牙语)。另一方面，只做第二项任务就像你脑子里有一个想法，但不知道如何表达。

幸运的是，自然语言处理(NLP)领域在过去几年里有了巨大的飞跃。对于第一项任务，具有长短期记忆细胞的神经网络可以用于序列到序列的翻译(对于那些对这个模型感兴趣的人，我会推荐 Jason Brownlee 的[文章](https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/))。此外，上个月(2019 年 5 月)发布了端到端语音到语音翻译模型[translator on](https://ai.googleblog.com/2019/05/introducing-translatotron-end-to-end.html)。对于第二个任务，Word2Vec 模型已经通过将相关的单词标绘在一起，从而推导出句子中的语义，证明了自己是值得使用的。一个这样的图如下所示。

![](img/64e02665371d4d3ca0dada998e8ebe8f.png)

Meanings and relationships derived from words (sourced from: [https://www.tensorflow.org/images/linear-relationships.png](https://www.tensorflow.org/images/linear-relationships.png))

所以是的，现在的机器**可以**理解语言。然而，他们仍然有很多事情不能做。一个例子是理解成语。虽然“泄漏秘密”可能象征性地意味着“揭示一个秘密”，比喻翻译是不可能的。因此，一台机器可以逐字翻译每个单词。假设它按照正确的顺序翻译成法语，那么这个表达就会是“jeter les haricots”，这肯定会听起来断章取义，如果不是非常滑稽的话。

但是，为了便于讨论，让我们大胆假设比喻翻译问题将在未来几年内得到解决。这样做，所有关于让代理人理解法律的技术问题都会得到解决，我们也会因此而安全，对吗？做好准备，因为这是事情变得有趣的地方！

当阿西莫夫提出这些定律时，他不知不觉地将它们建立在另一个假设上:我们人类确切地知道道德底线在哪里。但是我们有吗？

让我们以第一定律中的“伤害”一词为例。让我们也考虑一下同一法律中的“人类”一词。它的定义包括什么？例如，在 14 世纪，奴隶被认为比人更接近牛。如今，胎儿对人类生命的权利是许多讨论的主题。然而，在未来，如果一名孕妇，由于某种情况，在分娩时死亡的可能性很高，她的人工智能医生应该建议流产吗？人们需要记住，虽然从逻辑上讲，妇女堕胎后存活的机会更大，但胎儿一旦出生，就有更多的生存理由。所以，不管怎样，机器人最终伤害了人类。

接下来的决定甚至会让我们人类处于否认的状态。让我们考虑一下丹·布朗的地狱场景，并将第 0 定律应用于它。向代理呈现一个按钮。代理人被告知，如果按下按钮，一半的人类会立刻死去，但是这个物种会再生存几个世纪。如果不这样做(因此，无所作为)，人类将在 50 年内达到人口过剩，我们的物种将崩溃。代理应该怎么做，如果你处在它的位置，你会怎么做？

## 结论

阿西莫夫的法律试图解决人工智能起义的威胁。让机器人遵守法律的技术障碍是我们目前在让它们理解法律方面的限制。真正的障碍，一个哲学和伦理上的障碍，是我们的假设，即给定这样模糊的约束，机器人会完全按照我们想要的方式行事，而我们甚至不知道我们是什么意思。即使我们传达的意思是正确的，法律也可以简单地重新表述为“无论如何，只要做好工作，好吗？”考虑到它们可能造成的影响。

*Hans A. Gunnoo 是一名数据科学家，他的职业生涯始于电子工程，后来专攻机器学习。他还在业余时间为开源人工智能项目和关于数据科学领域最新趋势的博客做出贡献。*