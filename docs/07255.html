<html>
<head>
<title>Practical Coding in TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.0 中的实用编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-coding-in-tensorflow-2-0-1aab32bcfde1?source=collection_archive---------15-----------------------#2019-10-12">https://towardsdatascience.com/practical-coding-in-tensorflow-2-0-1aab32bcfde1?source=collection_archive---------15-----------------------#2019-10-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6d48" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">tf.function、TensorArray 和高级控制流</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/43cdcf7dc81d063503a2ec4a8a52203c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WYAzYAPpkyysiUq3o_25yw.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">An <a class="ae kz" href="https://pixabay.com/illustrations/instructions-user-manual-76729/" rel="noopener ugc nofollow" target="_blank">image</a> by <a class="ae kz" href="https://pixabay.com/users/geralt-9301/" rel="noopener ugc nofollow" target="_blank">Gerd Altmann</a></figcaption></figure><h2 id="d5f8" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">摘要</h2><p id="ab6b" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me lj mf mg mh ln mi mj mk lr ml mm mn mo ij bi translated">与 PyTorch 的激烈竞争给我们带来了 TensorFlow (TF)的新版本。包经历了很多变化，但最关键的是<code class="fe kf kg kh ki b">session.run()</code>的退役。默认情况下，TF 2 使用的是 eager 模式，而不是我们熟悉的构建和执行静态图的模式。这种代码可以用 pythonic 的方式编写，并转换成计算图。为了以静态图的形式执行代码，开发人员必须用<code class="fe kf kg kh ki b">@tf.function</code>修饰想要的函数。</p><p id="2ab1" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">在这篇文章中，我将用例子来解释这些概念。我假设读者了解 Python 和机器学习(ML)的基础知识。如果你是这个领域的新手，欢迎！这个<a class="ae kz" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">课程</a>将会是一个很好的开始。在这里，你可以找到这篇文章的 colab 版本。</p><h2 id="2c82" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">装置</h2><p id="cf37" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me lj mf mg mh ln mi mj mk lr ml mm mn mo ij bi translated">要安装 TF 2.x，请访问这个<a class="ae kz" href="https://www.tensorflow.org/install" rel="noopener ugc nofollow" target="_blank">页面</a>。</p><p id="2abd" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">要检查您的当前版本:</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="aa32" class="la lb iq ki b gy my mz l na nb">import tensorflow as tf<br/>print(tf.__version__)</span></pre><h2 id="094b" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">急切的执行</h2><p id="f79e" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me lj mf mg mh ln mi mj mk lr ml mm mn mo ij bi translated">代码在 TF 2 中急切地执行。这意味着您可以在不调用<code class="fe kf kg kh ki b">session.run()</code>或使用占位符的情况下向计算图提供数据。计算图是定义一系列操作的结构。这种结构允许我们通过沿着图形向后移动来自动计算导数。观看此<a class="ae kz" href="https://www.youtube.com/watch?v=hCP1vGoCdYU" rel="noopener ugc nofollow" target="_blank">视频</a>了解更多细节。在 TF 1 中，开发人员必须创建一个图形，然后执行它。现在图形是动态构建的，执行类似于函数调用。</p><p id="111c" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">为了了解它的工作原理，让我们做一个简单的模型。例如，我们有一个包含 3 个训练示例的数据集，其中每个示例都是一个二维向量。</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="2a0e" class="la lb iq ki b gy my mz l na nb">import numpy as np</span><span id="ce52" class="la lb iq ki b gy nc mz l na nb">np.random.seed(0)<br/>data = np.random.randn(3, 2)</span></pre><p id="4cb0" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">首先，我们必须初始化变量。TF 2 中没有变量范围。因此，保持所有变量在一个计数中的最佳方式是使用 Keras 层。</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="5eb9" class="la lb iq ki b gy my mz l na nb">inputer = tf.keras.layers.InputLayer(input_shape=(2))</span><span id="64db" class="la lb iq ki b gy nc mz l na nb">denser1 = tf.keras.layers.Dense(4, activation='relu')</span><span id="3057" class="la lb iq ki b gy nc mz l na nb">denser2 = tf.keras.layers.Dense(1, activation='sigmoid')</span></pre><p id="12d3" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">然后我们可以定义一个简单的模型。我们可以仅仅通过调用函数来运行这个模型。在这里，数据进入具有 4 个隐藏单元的密集层，然后进入具有一个单元的最终层。</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="2554" class="la lb iq ki b gy my mz l na nb">def model_1(data):</span><span id="aa4e" class="la lb iq ki b gy nc mz l na nb">x = inputer(data)<br/>  x = denser1(x)<br/>  print('After the first layer:', x)<br/>  out = denser2(x)<br/>  print('After the second layer:', out)</span><span id="6be2" class="la lb iq ki b gy nc mz l na nb">return out</span><span id="cdd2" class="la lb iq ki b gy nc mz l na nb">print(‘Model\’s output:’, model(data))</span><span id="b063" class="la lb iq ki b gy nc mz l na nb">...<br/>After the first layer: tf.Tensor( <br/>[[0.9548421  0.         0.         1.4861959 ]  <br/> [1.3276602  0.18780036 0.50857764 0.        ]  <br/> [0.45720425 0.         0.         2.5268495 ]], shape=(3, 4), dtype=float32) <br/>After the second layer: tf.Tensor( <br/>[[0.27915245]  <br/> [0.31461754]  <br/> [0.39550844]], shape=(3, 1), dtype=float32) <br/>Model's output: tf.Tensor( <br/>[[0.27915245]  <br/> [0.31461754] <br/> [0.39550844]], shape=(3, 1), dtype=float32)</span></pre><p id="f7cf" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">要从张量中获取 numpy 数组:</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="dd25" class="la lb iq ki b gy my mz l na nb">print('Model\'s output:', model_1(data).numpy())</span><span id="544a" class="la lb iq ki b gy nc mz l na nb">...<br/>Model's output: [[0.27915245]  [0.31461754]  [0.39550844]]</span></pre><p id="f40c" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">然而，急切的执行可能会很慢。该图是动态计算的。也就是说，让我们看看它是如何在我们的模型中构建的。输入数据进入第一层，这是第一个节点。当添加第二个节点时，第一个节点的输出进入第二个节点，然后计算第二个节点的输出，以此类推。它允许我们打印模型的中间状态(就像我们在上面的例子中所做的那样)，但是会使计算变慢。</p><h2 id="9672" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">静态图</h2><p id="f599" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me lj mf mg mh ln mi mj mk lr ml mm mn mo ij bi translated">幸运的是，我们仍然可以通过用<code class="fe kf kg kh ki b">@tf.function</code>修饰模型来构建一个静态图。与动态图相反，静态图首先连接所有节点进行一个大的计算操作，然后执行它。因此，我们不能看到模型的中间状态，也不能动态地添加任何节点。</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="2afd" class="la lb iq ki b gy my mz l na nb">@tf.function<br/>def model_2(data):<br/>  x = inputer(data)<br/>  x = denser1(x)<br/>  print('After the first layer:', x)<br/>  out = denser2(x)<br/>  print('After the second layer:', out)<br/>  <br/>  return out</span><span id="8851" class="la lb iq ki b gy nc mz l na nb">print('Model\'s output:', model_2(data))</span><span id="bd94" class="la lb iq ki b gy nc mz l na nb">...<br/>After the first layer: Tensor("dense_12/Relu:0", shape=(3, 4), dtype=float32)<br/>After the second layer: Tensor("dense_13/Sigmoid:0", shape=(3, 1), dtype=float32)<br/>Model's output: tf.Tensor(<br/>[[0.27915245] <br/> [0.31461754] <br/> [0.39550844]], shape=(3, 1), dtype=float32)</span></pre><p id="b43a" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">第二个优点是静态图只构建一次，而动态图在每次模型调用后都要重新构建。当您重复使用同一个图形时，它会降低计算速度。例如，当您在培训期间重新计算批次损失时。</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="d6e8" class="la lb iq ki b gy my mz l na nb">for i, d in enumerate(data):<br/>  print('batch:', i)<br/>  model_1(d[np.newaxis, :])  # eager model</span><span id="70ee" class="la lb iq ki b gy nc mz l na nb">for i, d in enumerate(data):<br/>  print('batch:', i)<br/>  model_2(d[np.newaxis, :])  # static model</span><span id="282d" class="la lb iq ki b gy nc mz l na nb">...<br/>batch: 0<br/>After the first layer: tf.Tensor(<br/>[[0.9548421 0.        0.        1.486196 ]], shape=(1, 4), dtype=float32)<br/>After the second layer: tf.Tensor(<br/>[[0.27915245]], shape=(1, 1), dtype=float32)</span><span id="fd03" class="la lb iq ki b gy nc mz l na nb">batch: 1<br/>After the first layer: tf.Tensor(<br/>[[1.3276603  0.18780035 0.50857764 0.        ]], shape=(1, 4), dtype=float32)<br/>After the second layer: tf.Tensor(<br/>[[0.3146175]], shape=(1, 1), dtype=float32)</span><span id="d7ea" class="la lb iq ki b gy nc mz l na nb">batch: 2<br/>After the first layer: tf.Tensor(<br/>[[0.45720425 0.         0.         2.5268495 ]], shape=(1, 4), dtype=float32)<br/>After the second layer: tf.Tensor(<br/>[[0.39550844]], shape=(1, 1), dtype=float32)</span><span id="e31c" class="la lb iq ki b gy nc mz l na nb">batch: 0<br/>After the first layer: Tensor("dense_12/Relu:0", shape=(1, 4), dtype=float32)<br/>After the second layer: Tensor("dense_13/Sigmoid:0", shape=(1, 1), dtype=float32)</span><span id="943e" class="la lb iq ki b gy nc mz l na nb">batch: 1</span><span id="0f01" class="la lb iq ki b gy nc mz l na nb">batch: 2</span></pre><p id="12bb" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">内部打印只在图形构建期间调用，在第二种情况下，图形只构建一次，然后重用。对于大型数据集，时间上的差异可能是巨大的。</p><h2 id="956d" class="la lb iq bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">高级控制流程</h2><p id="3db2" class="pw-post-body-paragraph lw lx iq ly b lz ma jr mb mc md ju me lj mf mg mh ln mi mj mk lr ml mm mn mo ij bi translated">AutoGraph 简化了 if/else 语句和 for/while 循环的使用。与 TF 1 相反，现在它们可以用 python 语法来编写。例如:</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="091e" class="la lb iq ki b gy my mz l na nb">a = np.array([1, 2, 3], np.int32)</span><span id="5f5c" class="la lb iq ki b gy nc mz l na nb">@tf.function<br/>def foo(a):<br/>  b = tf.TensorArray(tf.string, 4)<br/>  b = b.write(0, "test")</span><span id="2d3c" class="la lb iq ki b gy nc mz l na nb">  for i in tf.range(3):<br/>    if a[i] == 2:<br/>      b = b.write(i, "fuzz")<br/>    elif a[i] == 3:<br/>      b = b.write(i, "buzz")</span><span id="3c1c" class="la lb iq ki b gy nc mz l na nb">return b.stack()</span><span id="c9e7" class="la lb iq ki b gy nc mz l na nb">...<br/>tf.Tensor([b'test' b'fuzz' b'buzz' b''], shape=(4,), dtype=string)</span></pre><p id="5a0d" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">现在数组的使用类似于 Java 中的数组。首先，用所需的数据类型和长度声明一个数组:</p><pre class="kk kl km kn gt mu ki mv mw aw mx bi"><span id="b422" class="la lb iq ki b gy my mz l na nb">tf.TensorArray(data_type, length)</span></pre><p id="0932" class="pw-post-body-paragraph lw lx iq ly b lz mp jr mb mc mq ju me lj mr mg mh ln ms mj mk lr mt mm mn mo ij bi translated">常见的数据类型有<code class="fe kf kg kh ki b">tf.int32, tf.float32, tf.string</code>。要将数组<code class="fe kf kg kh ki b">b</code>转换回张量，使用<code class="fe kf kg kh ki b">b.stack()</code>。</p></div></div>    
</body>
</html>