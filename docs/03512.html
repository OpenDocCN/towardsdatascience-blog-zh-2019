<html>
<head>
<title>An Intuitive Explanation to AutoEncoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器的直观解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoencoders-in-keras-273389677c20?source=collection_archive---------16-----------------------#2019-06-04">https://towardsdatascience.com/autoencoders-in-keras-273389677c20?source=collection_archive---------16-----------------------#2019-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="99b5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习</h2><div class=""/><div class=""><h2 id="73d9" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">以及如何在 Keras 中实现它们</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/92d30acecc50c508befe262ed43d6964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WBW_TCGwzVTURL_p"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@adityachinchure?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Aditya Chinchure</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="9d1b" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">动机</strong></h1><p id="c2da" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">最近的许多深度学习模型依赖于从数据中提取复杂的特征。目标是将输入从其原始格式转换成由神经网络计算的另一种表示。该表示包含描述输入的隐藏唯一特征的要素。</p><p id="2d15" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">考虑一个人脸数据集，其中每个输入都是一个人的图像。原始格式的图像表示太复杂，机器无法使用。相反，为什么不让神经网络自动计算每张脸的重要特征，比如:眼睛类型、鼻子类型、两眼之间的距离、鼻子位置等。嗯，这听起来很有趣…使用这些功能，我们可以很容易地<strong class="mc jd">比较两张脸</strong>，<strong class="mc jd">找到相似的脸</strong>，<strong class="mc jd">生成新的脸</strong>，以及许多其他有趣的应用。</p><p id="a695" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这个概念被称为<strong class="mc jd">编码</strong>，因为我们正在生成数据的编码版本。在本文中，我们将了解更多关于编码的内容，如何使用<strong class="mc jd">自动编码器</strong>计算它们，以及最终如何在<strong class="mc jd"> Keras </strong>中实现它们。</p><h1 id="4871" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">自动编码器</strong></h1><p id="2e3a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">一个<strong class="mc jd">自动编码器</strong>是一个奇怪的神经网络，因为它的输入和输出是相同的。所以，它是一个试图自我学习的网络！我知道这很疯狂，但是你会明白为什么这很有用。</p><p id="a8e5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">假设我们有以下神经网络:</p><ul class=""><li id="4a32" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">有 100 个神经元的输入层</li><li id="2733" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">具有 3 个神经元的隐藏层</li><li id="284d" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">具有 100 个神经元的输出层(与输入层相同)</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3692a59936d8d354bdd5fb69ac12edc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*KS9kYN-tC4vX_nDRGbsASA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Image by Author</figcaption></figure><p id="29d1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在，如果我们让神经网络接受输入，并试图预测输出中的相同值，会发生什么？这不就意味着网络学会了如何只用 3 维<em class="nq">(隐藏层中的神经元数量)</em>来表示 100 维输入，然后再次重建相同的输入吗？此外，这些三维或特征似乎足以表示输入值所描述的内容。这很有趣。这就像压缩文件一样。我们减小了文件大小，但我们可以再次解压缩它，并获得相同的数据。事实上，在自动编码器中数据并不完全相同，因为它们是有损耗的，但是你明白了。</p><h1 id="6652" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">目标</strong></h1><p id="c822" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们将使用著名的<strong class="mc jd"> MNIST </strong>数字数据集来演示这个想法。目标是从给定的 28*28 图像生成 2D 编码。因此，我们正在使用自动编码器实现一个降维算法！酷吧？让我们开始…</p><h1 id="2d20" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">编码时间</strong></h1><p id="1ff8" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">首先，我们导入数据集:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="3599" class="nw lj it ns b gy nx ny l nz oa">from keras.datasets import mnist<br/>(data, labels), (_, _) = mnist.load_data()</span></pre><p id="6ac3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">需要重塑和调整:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="0f0c" class="nw lj it ns b gy nx ny l nz oa">data = data.reshape(-1, 28*28) / 255.</span></pre><p id="ffdf" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">是时候定义网络了。我们需要三层:</p><ul class=""><li id="e170" class="nb nc it mc b md mw mg mx mj nd mn ne mr nf mv ng nh ni nj bi translated">大小为 28*28 的输入层</li><li id="174b" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">大小为 2 的隐藏层</li><li id="8927" class="nb nc it mc b md nk mg nl mj nm mn nn mr no mv ng nh ni nj bi translated">大小为 28*28 的输出图层</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/008017e328878316d3d72c97b45e2e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*4DbvFZkPbABtA5sGWUTpcg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Image by Author</figcaption></figure><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="7f0a" class="nw lj it ns b gy nx ny l nz oa">from keras import models, layers<br/>input_layer = layers.Input(shape=(28*28,))<br/>encoding_layer = layers.Dense(2)(input_layer)<br/>decoding_layer = layers.Dense(28*28) (encoding_layer)<br/>autoencoder = models.Model(input_layer, decoding_layer)</span></pre><p id="8bb0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们编译和训练…我们将使用像素值之间的二元交叉熵损失来拟合模型:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="344e" class="nw lj it ns b gy nx ny l nz oa">autoencoder.compile('adam', loss='binary_crossentropy')<br/>autoencoder.fit(x = data, y = data, epochs=5)</span></pre><p id="82d5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">你注意到这个窍门了吗？<strong class="mc jd"> X =数据</strong>和<strong class="mc jd"> y =数据</strong>也是如此。</p><p id="c6ab" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在拟合模型之后，网络应该学习如何计算隐藏编码。但我们还是要提取出对此负责的那一层。在下文中，我们定义了一个新模型，其中我们删除了最后一层，因为我们不再需要它:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="4d40" class="nw lj it ns b gy nx ny l nz oa">encoder = models.Model(input_layer, encoding_layer)</span></pre><p id="bf62" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在，我们不再预测最终输出，而是只预测隐藏的表示。看看我们如何使用它:</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="ba81" class="nw lj it ns b gy nx ny l nz oa">encodings = encoder.predict(data)</span></pre><p id="6ebc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">就是这样！现在你的<em class="nq">编码</em>变量是一个(n，m)数组，其中 n 是例子的数量，m 是维数。第一列是第一个特征，第二列是第二个特征。但是那些特征是什么呢？事实上，我们不知道。我们只知道它们是每个输入值的良好代表。</p><p id="9cde" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们把它们画出来，看看我们会得到什么。</p><pre class="ks kt ku kv gt nr ns nt nu aw nv bi"><span id="4ef4" class="nw lj it ns b gy nx ny l nz oa">import matplotlib.pyplot as plt<br/>plt.scatter(encodings[:, 0], encodings[:, 1], c=labels)<br/>plt.colorbar()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7f2487d2365d3b506ff3c0153977d41d.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*YnNFWgUBVQMkeMW9YQ00UA.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Image by Author</figcaption></figure><p id="52d6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">漂亮！看看神经网络是如何学习隐藏特征的。显然，它学会了每个数字的不同特征，以及它们在 2D 空间中的分布。现在，我们可以将这些功能用于可视化、聚类或任何其他目的…</p><h1 id="7da1" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">最终想法</strong></h1><p id="3683" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在这篇文章中，我们学习了自动编码器以及如何应用它们进行降维。自动编码器非常强大，并在许多现代神经网络架构中使用。在以后的文章中，您将了解更复杂的编码器/解码器网络。</p><p id="c8a8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你喜欢这篇文章，请点击“鼓掌”按钮，我将不胜感激👏所以可能会传染给他人。也可以在 <a class="ae lh" href="https://twitter.com/alimasri1991" rel="noopener ugc nofollow" target="_blank"> <em class="nq">推特</em> </a> <em class="nq">，</em> <a class="ae lh" href="https://www.facebook.com/alimasri91" rel="noopener ugc nofollow" target="_blank"> <em class="nq">脸书</em> </a> <em class="nq">，</em> <a class="ae lh" href="mailto:alimasri1991@gmail.com" rel="noopener ugc nofollow" target="_blank"> <em class="nq">上关注我直接发邮件给我</em> </a> <em class="nq">或者在</em><a class="ae lh" href="https://www.linkedin.com/in/alimasri/" rel="noopener ugc nofollow" target="_blank"><em class="nq">LinkedIn</em></a><em class="nq">上找我。</em></p></div></div>    
</body>
</html>