<html>
<head>
<title>Why Don’t We Trust Machines when We Obviously Should?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么我们明明应该信任机器，却不信任它？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-dont-we-trust-machines-when-we-obviously-should-dede847dde73?source=collection_archive---------21-----------------------#2019-12-08">https://towardsdatascience.com/why-dont-we-trust-machines-when-we-obviously-should-dede847dde73?source=collection_archive---------21-----------------------#2019-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3198" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">为什么</strong>人类<strong class="ak">必须留在自动化循环中？我们如何才能创造一个更好的人机关系的未来？</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5f0d3762c367a1ae7dd856b7d5a25e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*twbBhfSRFEk-upI0"/></div></div></figure><blockquote class="ku"><p id="b13f" class="kv kw it bd kx ky kz la lb lc ld le dk translated">人类不可思议。无限不可预测。这就是它们危险的原因。—丹尼尔·H·威尔逊</p></blockquote><p id="a2c1" class="pw-post-body-paragraph lf lg it lh b li lj ju lk ll lm jx ln lo lp lq lr ls lt lu lv lw lx ly lz le im bi translated"><a class="ae ma" rel="noopener" target="_blank" href="/not-what-you-think-the-future-of-human-machine-relationship-b890d7f2072b">在之前的</a>中，我谈到了人工智能如何改变人机关系。我贴了以下问题:</p><p id="5989" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">无人驾驶汽车应该由谁来做决定？人类应该总是能够否决机器人的决定吗？如果你只有一瞬间的反应时间怎么办？而如果你的亲人在车上，答案会改变吗？</p><p id="5a2b" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">这个场景呢？如果你明天要上法庭，你会选择缺乏同理心的算法还是倾向于偏见和错误的人类法官来决定你的判决？</p><blockquote class="mg mh mi"><p id="6b57" class="lf lg mj lh b li mb ju lk ll mc jx ln mk md lq lr ml me lu lv mm mf ly lz le im bi translated">即使知道人类法官可能会犯更多的错误，罪犯仍然更喜欢人类而不是算法。决策心理学教授曼德普·达米(Mandeep Dhami)说，“他们想要那种人情味。”。</p></blockquote><p id="c755" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">看来我们并不信任机器。还有其他例子:虽然研究表明自动驾驶汽车更安全，但近一半的美国人不喜欢使用自动驾驶汽车。</p><p id="aa01" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">我们正在积极寻找解决人工智能透明度和公平性问题的方法。但我们似乎不太关心这样一个事实，即人类大脑也像一个我们知之甚少的黑匣子一样运作。为什么会这样呢？</p><h1 id="dcc3" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">为什么我们明明应该信任机器，却不信任它们？</h1><p id="3fdc" class="pw-post-body-paragraph lf lg it lh b li nf ju lk ll ng jx ln lo nh lq lr ls ni lu lv lw nj ly lz le im bi translated">沃顿商学院教授 Kartik Hosanagar 认为，好于平均水平的效果是原因之一。换句话说，我们知道自动驾驶汽车总体上比人类司机更安全，但我们认为我们比其他人更好。</p><p id="4154" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">在一项研究中，参与者被要求在自己、他人和算法的预测中做出选择，研究人员发现，人类确实信任算法——尤其是信任其他人。我们似乎也比算法更能容忍自己的错误。当风险很高时，我们不太可能让机器替我们做决定。</p><p id="d652" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">机器确实会犯错。他们可能永远不会完美无缺。无论自动驾驶车辆行驶了多少英里，总会有边缘情况。但是我们在这里问了正确的问题吗？</p><p id="c70c" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">为什么我们必须在人类和机器之间做出选择？机器会犯错，我们也一样。为什么我们不能共同努力，让集体决策更准确，更少偏差？我们能帮助发现算法中的弱点吗，反之亦然？人类和机器的混合团队能否产生更多样化的解决方案，让世界变得更美好？</p><h1 id="7c41" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">更好的机器教学:以人为中心的人工智能</strong></h1><p id="dd4e" class="pw-post-body-paragraph lf lg it lh b li nf ju lk ll ng jx ln lo nh lq lr ls ni lu lv lw nj ly lz le im bi translated">就像以用户为中心的设计可以帮助增加产品的可用性一样，如果我们想确保 AI 让我们的生活变得更好，而不是更糟，我们就应该真正把人放在第一位！</p><p id="80c6" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">将人们融入到开发 ML 产品的过程中。这种方法被称为“以人为中心的人工智能”。不要开发科学家能想到的最酷的技术，而是专注于人类的可用性，并确保有效和令人满意的人机交互。</p><p id="7591" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">一个经验法则是评估人工智能是否可以用来自动化无聊或危险的任务，或者增加人类喜欢做的任务。</p><p id="8343" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">麻省理工学院的研究科学家 Lex Fridman 建议将人类深度融入 ML 模型训练和现实世界的操作中。通过引入人类监督，我们可以更好地确保人工智能是安全、公平和可解释的。</p><p id="8431" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">加州大学伯克利分校的计算机科学教授斯图尔特·拉塞尔认为，我们应该“给机器赋予目的。”他没有假设机器对目标有完全的了解，而是提出假设“模型对目标有不完全的了解。”</p><p id="f310" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">这就引出了我的下一个观点:我们需要更好地定义机器的目标函数，更重要的是，我们的社会！</p><h1 id="9174" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated"><strong class="ak">为机器和我们的社会提供更好的目标函数</strong></h1><p id="53cc" class="pw-post-body-paragraph lf lg it lh b li nf ju lk ll ng jx ln lo nh lq lr ls ni lu lv lw nj ly lz le im bi translated">ML 模型由回报函数(也称为目标函数或损失函数)指导。)目标函数定义问题。它们是决定 ML 模型预测是否正确的数学公式。</p><p id="cda0" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">换句话说，他们定义了成功或失败的激励机制。我们希望使用目标函数来优化或加强我们系统的行为，例如最大化保留或最小化错误。</p><p id="0b86" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">然而，定义目标函数并不是一项简单直接的任务。正如我在另一篇文章中提到的，模型准确性本身通常不是一个好的度量。我们将需要考虑精确度和召回权衡。此外，我们需要确保奖励功能为用户带来积极的体验，不仅仅是主要客户，而是我们所有的用户。</p><p id="4c8c" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">以拼车公司为例，虽然乘客满意度很重要，但司机留存对双方平台的增长也至关重要。定义一个为所有用户创造良好体验的奖励函数可能很有挑战性，尤其是当有利益冲突时，但这绝对是重要的。</p><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/designing-the-user-experience-of-ml-products-8aef5afb510b"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">设计 ML 产品的用户体验</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">三个原则:期望、错误和信任！</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ks nn"/></div></div></a></div><p id="e62b" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">奖励功能可能会给其他人带来意想不到的后果。我们如何考虑 ML 模型的潜在负面影响并设法减轻它？随着时间的推移，我们如何可能预见和跟踪我们的奖励函数的影响？</p><p id="4aec" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">一个更基本的问题是:我们如何确保公司在设计他们的 ML 产品时，总是将他们的用户、利益相关者和整个社会的利益考虑在内？在当前资本市场的奖励功能下，公司会被激励去设计正确的 ML 系统吗？</p><p id="4d48" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">如果留住用户可以增加公司的利润，他们会在乎用户花太多时间在线的负面影响吗？随着更多任务实现自动化，他们会分配足够的资源来帮助员工过渡到下一个角色吗？如果公司的唯一目标是最大化他们的股东价值，他们还会试图减轻他们的 ML 模式对员工和社会的任何潜在负面影响吗？</p><h1 id="fd22" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">摘要</h1><p id="ea9a" class="pw-post-body-paragraph lf lg it lh b li nf ju lk ll ng jx ln lo nh lq lr ls ni lu lv lw nj ly lz le im bi translated">输给深蓝并没有让加里·卡斯帕罗夫绝望。相反，他开始接受“半人马象棋”的想法通过人机合作，下棋的水平比以前更高了。</p><p id="02bc" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">伯克利教授肯·戈德堡也用顶级围棋选手与 Deepmind 的 Alphago 程序比赛的例子来倡导机器人与人类的联盟。</p><p id="b7be" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">我们不应该将人机关系视为零和游戏，而是应该转变思维模式，思考如何与机器合作，一起把蛋糕做大！这是一个很好的机会，让我们深入思考我们擅长什么，我们真正想做什么，我们如何更好地利用机器！</p><h2 id="e96b" class="oc mo it bd mp od oe dn mt of og dp mx lo oh oi mz ls oj ok nb lw ol om nd on bi translated">如果你想看更多这样的文章，请点击这里！</h2><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/not-what-you-think-the-future-of-human-machine-relationship-b890d7f2072b"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">不是你想的那样:人机关系的未来</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">机器人会继续为我们工作吗？还是反过来？未来会是什么样子？更重要的是，什么…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="oo l ny nz oa nw ob ks nn"/></div></div></a></div><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/its-here-how-ai-robot-will-revolutionize-manufacturing-44ce784438d4"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">它在这里！人工智能机器人将如何革新制造业</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">虽然制造业一直被认为是自动化程度最高的行业，但全自动化…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="op l ny nz oa nw ob ks nn"/></div></div></a></div><div class="nk nl gp gr nm nn"><a rel="noopener follow" target="_blank" href="/how-to-manage-machine-learning-products-part-1-386e7011258a"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">如何管理机器学习产品—第 1 部分</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">为什么管理机器学习产品这么难？为什么你应该关心？</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">towardsdatascience.com</p></div></div><div class="nw l"><div class="oq l ny nz oa nw ob ks nn"/></div></div></a></div><p id="4986" class="pw-post-body-paragraph lf lg it lh b li mb ju lk ll mc jx ln lo md lq lr ls me lu lv lw mf ly lz le im bi translated">Bastiane Huang 是 OSARO 的产品经理，OSARO 是一家总部位于旧金山的初创公司，致力于开发软件定义的机器人技术。她曾在亚马逊的 Alexa 小组和哈佛商业评论以及该大学的未来工作倡议中工作。 <a class="ae ma" href="https://bastiane.substack.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mj">她写的是关于 ML、机器人和产品管理的文章。跟随她来到这里</em>T5。</a></p></div></div>    
</body>
</html>