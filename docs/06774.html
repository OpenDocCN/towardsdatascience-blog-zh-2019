<html>
<head>
<title>Learning Parameters, Part 5: AdaGrad, RMSProp, and Adam</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习参数，第 5 部分:AdaGrad、RMSProp 和 Adam</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d?source=collection_archive---------4-----------------------#2019-09-27">https://towardsdatascience.com/learning-parameters-part-5-65a2f3583f7d?source=collection_archive---------4-----------------------#2019-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2caa" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/learning-parameters/latest" rel="noopener">学习参数</a></h2><div class=""/><div class=""><h2 id="9cc4" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">让我们看看具有自适应学习率的梯度下降。</h2></div><p id="1a2d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-4-6a18d1d3000b">第四部分</a>中，我们看了一些启发法，可以帮助我们更好地调整学习速度和动力。在<a class="ae lk" href="https://medium.com/tag/learning-parameters/latest" rel="noopener">系列</a>的最后一篇文章中，让我们来看看一种更有原则的调整学习速度的方法，并给学习速度一个适应的机会。</p><blockquote class="lm ln lo"><p id="a341" class="ko kp ll kq b kr ks ka kt ku kv kd kw lp ky kz la lq lc ld le lr lg lh li lj ij bi translated">引用说明:本博客中的大部分内容和图表直接取自 IIT 马德拉斯大学教授 Mitesh Khapra 提供的深度学习课程第 5 讲。</p></blockquote><h2 id="165a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">适应性学习率的动机</h2><p id="c930" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">考虑以下具有 sigmoid 激活的简单感知器网络。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/b3e77107b518fc5facb8aa8df6dc2320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*CRj9U6_LBVMFEaceunB0CA.png"/></div></figure><p id="d579" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">很容易看出，给定一个点(<strong class="kq ja"> x </strong>，<em class="ll"> y </em>)，梯度<strong class="kq ja"> w </strong>如下:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/dc736a735335ae2f4c68d83ec5078b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*8t9xxheID3mR741SMGBxwA.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">See <a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-1-eb3e8bb9ffbb">Part-1</a> if this is unclear.</figcaption></figure><p id="f7bc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"><em class="ll">【f(x)</em></strong><em class="ll"/>w . r . t 的梯度到一个特定的权重显然依赖于其相应的输入。如果有<em class="ll"> n </em>个点，我们可以将所有<em class="ll"> n </em>个点的梯度相加得到总梯度。这条新闻既不新鲜也不特别。但是，如果特征<strong class="kq ja"> <em class="ll"> x2 </em> </strong>非常稀疏(即，如果它的值对于大多数输入都是 0)，会发生什么呢？假设∇ <strong class="kq ja"> <em class="ll"> w2 </em> </strong>对于大多数输入(见公式)将为 0 是合理的，因此<strong class="kq ja"> <em class="ll"> w2 </em> </strong>将不会获得足够的更新。</p><p id="29d2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">不过，这有什么好困扰我们的呢？需要注意的是，如果<strong class="kq ja"><em class="ll">【x2】</em></strong>既稀疏又重要，我们会认真对待<strong class="kq ja"> <em class="ll"> w2 </em> </strong>的更新。为了确保即使在特定输入稀疏的情况下也能进行更新，我们是否可以为每个参数设定不同的学习速率，以考虑特征的频率？我们当然可以。我的意思是，这是这篇文章的全部要点。</p><h1 id="7880" class="nc lt iq bd lu nd ne nf lx ng nh ni ma kf nj kg md ki nk kj mg kl nl km mj nm bi translated"><strong class="ak"> AdaGrad —自适应梯度算法</strong></h1><h2 id="2fb7" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">直觉</h2><p id="248d" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">衰减参数的学习率与其更新历史成比例(更新越多，衰减越多)。</p><h2 id="1b1a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">AdaGrad 的更新规则</h2><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2e32acecc9dfef78d72dcb5fc9046289.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*KN7IkA5J5ZpQ6LjLrtmZwA.png"/></div></figure><p id="95e3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从更新规则可以清楚地看出，梯度的历史累积在<strong class="kq ja"> <em class="ll"> v </em> </strong>中。累积的梯度越小，<strong class="kq ja">T5【vT7】值就会越小，导致学习率越大(因为<strong class="kq ja">T9】vT11】除以<em class="ll"> η </em>)。</strong></strong></p><h2 id="eaf8" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">阿达格拉德的 Python 代码</h2><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="no np l"/></div></figure><h2 id="acdf" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">阿达格拉德在行动</h2><p id="4458" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">要查看 AdaGrad 的运行情况，我们需要首先创建一些其中一个要素稀疏的数据。对于我们在学习参数系列的所有部分中使用的玩具网络，我们将如何做呢？嗯，我们的网络正好有两个参数(<strong class="kq ja"> <em class="ll"> w </em> </strong>和<strong class="kq ja"> <em class="ll"> b </em> </strong>，参见<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-1-eb3e8bb9ffbb"> part-1 </a>中的<em class="ll">动机</em>)。这其中，b 对应的输入/特征是一直开着的，所以我们不能真的让它稀疏。所以唯一的选择就是让 x 稀疏。这就是为什么我们创建了 100 个随机的<em class="ll"> (x，y) </em>对，然后大约 80%的这些对我们将<em class="ll"> x </em>设置为 0，使得<strong class="kq ja"> <em class="ll"> w </em> </strong>的特征稀疏。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2b7a8c1f36a550b0ecfc54f9c5b3de26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*g8b6r_qAQC4NiJ2h_UFsuw.png"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Vanilla GD(black), momentum (red), NAG (blue)</figcaption></figure><p id="467b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们实际查看运行中的 AdaGrad 之前，请查看上面的其他 3 个优化器——香草 GD(黑色)、momentum(红色)、NAG(蓝色)。这 3 个优化器为这个数据集做了一些有趣的事情。你能发现它吗？请随意停下来思考。<strong class="kq ja">答:</strong>最初，三种算法都是主要沿垂直(<strong class="kq ja"> <em class="ll"> b </em> </strong>)轴运动，沿水平(<strong class="kq ja"> <em class="ll"> w </em> </strong>)轴运动很少。为什么？因为在我们的数据中，对应于<strong class="kq ja"> <em class="ll"> w </em> </strong>的特征是稀疏的，因此<strong class="kq ja"> <em class="ll"> w </em> </strong>经历很少的更新。另一方面，<strong class="kq ja"> <em class="ll"> b </em> </strong>非常密集，经历很多更新。这种稀疏性在包含 1000 个输入特征的大型神经网络中非常普遍，因此我们需要解决它。现在让我们来看看阿达格拉德的行动。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/6552cdcbacfd5810986a889c01cb51bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/1*aflJY_KSjorG1YHNiz7Rnw.gif"/></div></figure><p id="360f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">瞧啊。通过使用参数特定的学习率，AdaGrad 确保尽管稀疏度<strong class="kq ja"><em class="ll"/></strong>w 获得更高的学习率，并因此获得更大的更新。而且，这也保证了如果<strong class="kq ja">T5】bT7】经历大量更新，其有效学习率会因为分母的增长而下降。实际上，如果我们从分母中去掉平方根，效果就不那么好了(这是值得思考的)。另一面是什么？随着时间的推移，<strong class="kq ja"> <em class="ll"> b </em> </strong>的有效学习率将衰减到不再对<strong class="kq ja"> <em class="ll"> b </em> </strong>进行更新的程度。我们能避免这种情况吗？RMSProp 可以！</strong></p><h1 id="3a8d" class="nc lt iq bd lu nd ne nf lx ng nh ni ma kf nj kg md ki nk kj mg kl nl km mj nm bi translated">RMSProp — <strong class="ak">均方根传播</strong></h1><h2 id="7bec" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">直觉</h2><p id="034c" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">AdaGrad 非常积极地降低学习率(随着分母的增长)。结果，一段时间后，由于学习率的衰减，频繁参数将开始接收非常小的更新。为了避免这种情况，为什么不衰减分母，防止其快速增长。</p><h2 id="09e9" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">RMSProp 的更新规则</h2><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c683c13895af3e2d1b7b03b16b4a1316.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*oTk6bl5ccXp9540bFv8WXg.png"/></div></figure><p id="fbf8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一切都非常类似于阿达格拉德，除了现在我们也衰减分母。</p><h2 id="90d2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">RMSProp 的 Python 代码</h2><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="no np l"/></div></figure><h2 id="3280" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">RMSProp 正在运行</h2><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a425e7f0f2f79723423c80d9e9e3388a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/1*rn3JEQkOB3Znob1Y_EHakg.gif"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Vanilla GD(black), momentum (red), NAG (blue), AdaGrad (magenta)</figcaption></figure><p id="ba4e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你看到了什么？RMSProp 和 AdaGrad 有什么不同？请随意停下来思考。<strong class="kq ja">回答:</strong>阿达格拉德在接近收敛的时候卡住了，因为学习率的衰减，已经无法在垂直(<strong class="kq ja"> <em class="ll"> b </em> </strong>)方向<br/>移动。RMSProp 通过减少衰减来克服这个问题。</p><h1 id="67f3" class="nc lt iq bd lu nd ne nf lx ng nh ni ma kf nj kg md ki nk kj mg kl nl km mj nm bi translated">Adam — <strong class="ak">自适应矩估计</strong></h1><h2 id="b670" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated"><strong class="ak">直觉</strong></h2><p id="2189" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">尽 RMSProp 所能解决 AdaGrad 的分母衰减问题。除此之外，使用梯度的累积历史。</p><h2 id="5cfe" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">更新 Adam 的规则</h2><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b53268d38e4a10c45ee0a48b5ed300ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*Qzpf8aKwdBYTgMuL69C5qw.png"/></div></figure><p id="e52b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以及类似于<strong class="kq ja"><em class="ll">b</em></strong><em class="ll">_ t</em>的一组方程组。注意，Adam 的更新规则与 RMSProp 非常相似，除了我们也查看梯度的累积历史(<strong class="kq ja"><em class="ll">m</em></strong><em class="ll">_ t</em>)。注意，上面更新规则的第三步是偏差修正。Mitesh M Khapra 教授解释了为什么偏差校正是必要的，可在<a class="ae lk" href="https://www.youtube.com/watch?v=-0ZMU-gnm2g" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="d8ff" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">亚当的 Python 代码</h2><figure class="mq mr ms mt gt mu"><div class="bz fp l di"><div class="no np l"/></div></figure><h2 id="8669" class="ls lt iq bd lu lv lw dn lx ly lz dp ma kx mb mc md lb me mf mg lf mh mi mj iw bi translated">亚当在行动</h2><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/53fa589dcd24ec832460fb9fe30d5b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/1*m9JMNb9z2bzFlmPfK0WGUQ.gif"/></div></figure><p id="dcd1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">很明显，获取梯度的累积历史可以加快速度。对于这个玩具数据集，它似乎是超调(有点)，但即使这样，它收敛的方式比其他优化。</p><h1 id="43d9" class="nc lt iq bd lu nd ne nf lx ng nh ni ma kf nj kg md ki nk kj mg kl nl km mj nm bi translated">百万美元问题:使用哪种算法？</h1><ul class=""><li id="41e2" class="nv nw iq kq b kr mk ku ml kx nx lb ny lf nz lj oa ob oc od bi translated">亚当现在似乎或多或少是默认的选择(<em class="ll"> β1 </em> = 0.9，<em class="ll"> β2 </em> = 0.999，<em class="ll">ϵ</em>= 1e 8)。</li><li id="b8d1" class="nv nw iq kq b kr oe ku of kx og lb oh lf oi lj oa ob oc od bi translated">虽然它被认为对初始学习速率是鲁棒的，但是我们已经观察到对于序列生成问题<em class="ll"> η </em> = 0.001，0.0001 工作得最好。</li><li id="51da" class="nv nw iq kq b kr oe ku of kx og lb oh lf oi lj oa ob oc od bi translated">话虽如此，但许多论文报道，具有简单退火学习速率调度的动量(内斯特罗夫或经典)SGD 在实践中也运行良好(通常，序列生成问题从<em class="ll"> η </em> = 0.001 开始，0.0001)。</li><li id="fa6b" class="nv nw iq kq b kr oe ku of kx og lb oh lf oi lj oa ob oc od bi translated">亚当可能是最好的选择。</li><li id="452c" class="nv nw iq kq b kr oe ku of kx og lb oh lf oi lj oa ob oc od bi translated">最近的一些工作表明 Adam 存在问题，在某些情况下它不会收敛。</li></ul><h1 id="a7b0" class="nc lt iq bd lu nd ne nf lx ng nh ni ma kf nj kg md ki nk kj mg kl nl km mj nm bi translated">结论</h1><p id="4fdc" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">在本系列的最后一篇文章中，我们研究了具有自适应学习速率的梯度下降如何帮助加快神经网络的收敛。本文涵盖了直觉、python 代码和三个广泛使用的优化器的可视化说明——AdaGrad、RMSProp 和 Adam。Adam 结合了 RMSProp 和 AdaGrad 的最佳特性，即使在有噪声或稀疏的数据集上也能很好地工作。</p><h1 id="67fc" class="nc lt iq bd lu nd ne nf lx ng nh ni ma kf nj kg md ki nk kj mg kl nl km mj nm bi translated">承认</h1><p id="bc8e" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">IIT·马德拉斯教授的<a class="ae lk" href="https://www.cse.iitm.ac.in/~miteshk/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">和<a class="ae lk" href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> CS7015:深度学习</strong> </a> <strong class="kq ja"> </strong>课程如此丰富的内容和创造性的可视化，这要归功于 Mitesh M Khapra </strong> </a>教授和助教。我只是简单地整理了提供的课堂讲稿和视频。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="ok ol di om bf on"><div class="gh gi oj"><img src="../Images/000199b8e4919d7f7011bd2f6a551f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0_9l2EDmWXxkqCNxKoWfA.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk"><strong class="bd oo">Source:</strong> Paperspace article on RMSProp by <a class="ae lk" href="https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/" rel="noopener ugc nofollow" target="_blank">Ayoosh Kathuria</a>.</figcaption></figure></div></div>    
</body>
</html>