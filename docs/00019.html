<html>
<head>
<title>Using Object Detection for Complex Image Classification Scenarios Part 3:</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将对象检测用于复杂的图像分类场景第 3 部分:</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-object-detection-for-complex-image-classification-scenarios-part-3-770d3fc5e3f7?source=collection_archive---------8-----------------------#2019-01-02">https://towardsdatascience.com/using-object-detection-for-complex-image-classification-scenarios-part-3-770d3fc5e3f7?source=collection_archive---------8-----------------------#2019-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2d5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用 MobileNet 和迁移学习进行策略识别</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/ae317347ccf3a1b22cc44765c3cbcbf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/0*_LmsQxkNDLnAjG47.jpg"/></div></figure><p id="ae3c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">TLDR；本系列基于在下面的<a class="ae lj" href="https://www.microsoft.com/developerblog/2017/07/31/using-object-detection-complex-image-classification-scenarios/" rel="noopener ugc nofollow" target="_blank">现实生活代码故事</a>中检测复杂策略的工作。该系列的代码可以在<a class="ae lj" href="https://github.com/aribornstein/cvworkshop" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="9c16" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">第 3 部分:使用 MobileNet 和迁移学习进行策略识别</h1><p id="0bcd" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">在之前的教程中，我们概述了我们的<a class="ae lj" rel="noopener" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-1-779c87d1eecb">策略分类挑战</a>，并展示了我们如何使用<a class="ae lj" rel="noopener" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-2-54a3a7c60a63">定制视觉认知服务</a>来应对它。本教程介绍深度迁移学习作为一种手段，利用多个数据源来克服数据稀缺问题。</p><h1 id="1c2e" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">处理图像</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/4dc0c809b8e423378fd108f00ba196f6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*D4Lt6duo5-xVRwICKvdEjA.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Our goal is to extract the differential signal from our training images</figcaption></figure><p id="0359" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在我们尝试为我们的复杂策略构建分类器之前，让我们先来看看<a class="ae lj" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST 数据集</a>，以便更好地理解关键的图像分类概念，如一个热编码、线性建模、多层感知、掩蔽和卷积，然后我们将把这些概念放在一起，并将其应用到我们自己的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/890b373d3e140e3f57eb6b0f03c1ea0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UWCpKRWuPgimzY2R.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">A sample of the images in the MNIST Dataset</figcaption></figure><p id="7f92" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了在 MNIST 上训练一个分类模型，我们首先需要一种方法来<strong class="kp ir">表示我们的图像和标签</strong>。有许多方法可以将图像表示为张量(T12)、矩阵(T14)、向量(T15)或向量(T17)。</p><p id="fbad" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于我们的第一个模型，我们将使用矢量表示。为了做到这一点，我们首先将图像展平成一个长矢量，就像解开一根布的线一样。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/a5813985fb2e8df06e85d433ec878f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/0*4fN8c5buR2trfTVF.gif"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Unraveling a cloth to a thread <a class="ae lj" href="https://makeit-loveit.com/sew-shirring-smocking-elastic-thread" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="7f59" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当我们将此过程应用于下面的尺寸为 28 x 28 图像像素的“3”的图像时，它将产生长度为 784 像素的展平数组。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/70269edcc8752609bc8518662df86ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/0*teIkFyoG1mj8Ul-r"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The MNIST image 28 x 28 image pixels, it will result in a flattened array of length 784.</figcaption></figure><p id="f0c4" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，尽管我们很容易看到这个图像并知道它是一个“3”，但计算机天生不知道这一点，我们需要训练一个模型来学习如何识别图像中有一个“3”。要做到这一点，我们首先需要一种方法来表示上面的图片包含一个“3”的图像这一事实。</p><h1 id="fe39" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">编码图像标签</h1><p id="3be8" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">为此，我们将我们的每个图像与一个<a class="ae lj" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank"> 1-hot </a>编码标签相关联，其中第一个索引对应于数字<code class="fe mt mu mv mw b">0</code>，最后一个索引对应于数字<code class="fe mt mu mv mw b">9</code>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/8b022bfeb2cd423f1d7c25f515853b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/0*wo9sHCi74OGaRsPY"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">A one hot encoded vector representation of an MNIST 3</figcaption></figure><p id="4b0d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当我们训练一个模型时，我们使用这个值作为我们的目标。下面的<a class="ae lj" href="http://test" rel="noopener ugc nofollow" target="_blank"> Keras </a>代码加载 MNIST 数据</p><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="2fda" class="nb ll iq mw b gy nc nd l ne nf"><strong class="mw ir">from</strong> <strong class="mw ir">keras.datasets</strong> <strong class="mw ir">import</strong> mnist <br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.utils</strong> <strong class="mw ir">import</strong> np_utils </span><span id="bf90" class="nb ll iq mw b gy ng nd l ne nf">output_dim = nb_classes = 10 <br/>batch_size = 128 <br/>nb_epoch = 5</span><span id="7797" class="nb ll iq mw b gy ng nd l ne nf"><em class="nh"># the data, shuffled and split between train and test sets </em><br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="7ba7" class="nb ll iq mw b gy ng nd l ne nf">input_dim = 784 <em class="nh">#28*28 </em><br/>X_train = x_train.reshape(60000, input_dim) <br/>X_test = x_test.reshape(10000, input_dim) <br/>X_train = X_train.astype('float32') <br/>X_test = X_test.astype('float32') <br/>X_train /= 255 <br/>X_test /= 255</span><span id="375a" class="nb ll iq mw b gy ng nd l ne nf">Y_train = np_utils.to_categorical(y_train, nb_classes) <br/>Y_test = np_utils.to_categorical(y_test, nb_classes)</span></pre><p id="f15e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在，我们已经处理了 MNIST 图像及其标签，让我们使用 Keras 训练我们的第一个图像分类模型。</p><h1 id="853d" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">线性模型</h1><p id="27ca" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated"><a class="ae lj" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">逻辑回归</a> (LR)是一种基本的机器学习技术，它使用特征的线性加权组合，并生成不同类别的基于概率的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ni"><img src="../Images/5c0ff332279a2c4ee11dcef21dbc162e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FFE4iYM7dV7zVl3-"/></div></div></figure><p id="9f40" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了在 MNIST 上训练上面的 LR 模型，我们应用以下步骤:</p><ol class=""><li id="cb18" class="nj nk iq kp b kq kr kt ku kw nl la nm le nn li no np nq nr bi translated">初始化 784 个值的随机权重向量</li><li id="b00d" class="nj nk iq kp b kq ns kt nt kw nu la nv le nw li no np nq nr bi translated">取第一个 784 位 MNIST 训练图像向量，如上面的“3 ”,并将其乘以我们的权重向量。</li><li id="b0e9" class="nj nk iq kp b kq ns kt nt kw nu la nv le nw li no np nq nr bi translated">取我们相乘的结果，对 784 个值求和，直到得到一个数</li><li id="292c" class="nj nk iq kp b kq ns kt nt kw nu la nv le nw li no np nq nr bi translated">将数字传递到一个函数中，该函数取我们的总和，并将其拟合为 0-9 之间的分布，然后对输出进行热编码。对于第一个例子，这个数字很可能是不正确的，因为我们乘以了随机值</li><li id="943a" class="nj nk iq kp b kq ns kt nt kw nu la nv le nw li no np nq nr bi translated">将输出向量与图像标签向量进行比较，并使用<em class="nh">损失函数计算我们的预测有多接近。</em>损失函数的输出被称为<em class="nh">损失</em>。</li><li id="d96f" class="nj nk iq kp b kq ns kt nt kw nu la nv le nw li no np nq nr bi translated">针对损失值应用诸如 SGD 的优化，以更新权重向量中的每个值。</li></ol><div class="nx ny gp gr nz oa"><a href="https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">损失函数和优化算法。去神秘化。</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">深度学习模型的优化算法和损失函数的选择可以在生产过程中发挥重要作用</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="ok l ol om on oj oo kl oa"/></div></div></a></div><p id="34aa" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然后，我们对 MNIST 训练集中的每个图像重复这个过程。对于每个图像，权重值被更新，以便它们可以更好地将我们的输入 MNIST 向量转换为与其标签相匹配的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/39db241ae63f41d32cb93f2ea73ab14d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*pnGnKwqRZ4GS4Osn.jpg"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The machine learning process <a class="ae lj" href="https://www.improgrammer.net/wp-content/uploads/2018/10/Machine-learning-min.jpg" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="356d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当我们在称为<a class="ae lj" href="https://www.quora.com/What-is-epochs-in-machine-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir">时期</strong> </a>的训练集上完成上述步骤时。在第一个时期之后，值仍然可能是差的，但是在改组数据集并重复该过程几个更多的<strong class="kp ir">时期</strong>之后，线性模型学习线性权重，它们<strong class="kp ir">收敛</strong>在我们的数据的体面表示上。</p><p id="214d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">下面的 Keras 代码显示了这个过程的结果。</p><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="3648" class="nb ll iq mw b gy nc nd l ne nf"><strong class="mw ir">from</strong> <strong class="mw ir">keras.models</strong> <strong class="mw ir">import</strong> Sequential <br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.layers</strong> <strong class="mw ir">import</strong> Dense, Activation </span><span id="5099" class="nb ll iq mw b gy ng nd l ne nf">model = Sequential() <br/>model.add(Dense(output_dim, input_dim=input_dim, activation='softmax')) <br/>model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) <br/>history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test)) <br/>score = model.evaluate(X_test, Y_test, verbose=0) <br/>print('Test Loss:', score[0]) <br/>print('Test accuracy:', score[1])<br/></span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="c02c" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 1s 16us/step - loss: 1.2899 - acc: 0.6898 - val_loss: 0.8185 - val_acc: 0.8255<br/>Epoch 2/5<br/>60000/60000 [==============================] - 1s 17us/step - loss: 0.7228 - acc: 0.8374 - val_loss: 0.6113 - val_acc: 0.8588<br/>Epoch 3/5<br/>60000/60000 [==============================] - 1s 11us/step - loss: 0.5912 - acc: 0.8575 - val_loss: 0.5281 - val_acc: 0.8724<br/>Epoch 4/5<br/>60000/60000 [==============================] - 1s 11us/step - loss: 0.5280 - acc: 0.8681 - val_loss: 0.4821 - val_acc: 0.8800<br/>Epoch 5/5<br/>60000/60000 [==============================] - 1s 13us/step - loss: 0.4897 - acc: 0.8749 - val_loss: 0.4514 - val_acc: 0.8858<br/>Test Loss: 0.4514175675392151<br/>Test accuracy: 0.8858</span></pre><h1 id="17b9" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">非线性模型(MLP)</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi pc"><img src="../Images/60b2e02eb197488ddd1f2ac92b7aa190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w6ewozGA6cMw8ITM.png"/></div></div></figure><p id="767a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">可以想象，仅仅基于一个输出增加和求和权重向量值是次优的，并且在某些情况下是无效的。</p><p id="6786" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">毕竟不是所有的数据都是线性的。</p><p id="7028" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">以垃圾邮件和非垃圾邮件这两个图像类别为例。无论我们如何更新我们的权重，没有线性权重，我们可以学习区分这些类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/fe93b7212c38553e9379faac040e6656.png" data-original-src="https://miro.medium.com/v2/0*R4cLnK1LdTUsxBuR.gif"/></div></figure><p id="f361" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">但是，如果我们有一种方法来组合多个线性模型以获得更好的表示能力呢？然后我们可以训练一个模型来区分这两种图像类别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/dd5abf6a36b1a8564174105171e7b238.png" data-original-src="https://miro.medium.com/v2/format:webp/1*Q7JqxCpm-V8Pi69xtE12mA.png"/></div></figure><p id="0473" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们可以通过前馈神经网络来实现这一点，例如<a class="ae lj" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">多层感知</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/28b34f0c336183bbe63b413880abd99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*4BkGpTkfb9cL5kF_"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Multi Layer Perception</figcaption></figure><p id="e56f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于 MLPs 的工作，我们需要一个非线性的激活函数，如 RELU。为了简洁起见，我们将把它作为一个黑盒。更多关于这个主题的内容，请见下面的帖子。</p><div class="nx ny gp gr nz oa"><a href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">理解神经网络中的激活函数</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">最近，我的一个同事问了我几个类似“为什么我们有这么多激活功能？”，“为什么是…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="pe l ol om on oj oo kl oa"/></div></div></a></div><p id="7665" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">下面的 keras 代码显示了如何在 MNIST 上训练一个多层感知器，以获得比我们的线性模型更好的结果。</p><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="d48a" class="nb ll iq mw b gy nc nd l ne nf"><strong class="mw ir">from</strong> <strong class="mw ir">keras.models</strong> <strong class="mw ir">import</strong> Sequential <br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.layers</strong> <strong class="mw ir">import</strong> Dense, Activation <br/>output_dim = nb_classes = 10 <br/>batch_size = 128 <br/>nb_epoch = 5</span><span id="2d3a" class="nb ll iq mw b gy ng nd l ne nf">model = Sequential() <br/>model.add(Dense(input_dim, input_dim=input_dim, activation='relu')) <br/>model.add(Dense(input_dim, input_dim=input_dim, activation='relu'))<br/>model.add(Dense(output_dim, input_dim=input_dim, activation='softmax')) </span><span id="0854" class="nb ll iq mw b gy ng nd l ne nf">model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy']) <br/>history = model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,verbose=1, validation_data=(X_test, Y_test)) <br/>score = model.evaluate(X_test, Y_test, verbose=0) <br/>print('Test Loss:', score[0]) <br/>print('Test accuracy:', score[1])</span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="59e6" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 9s 150us/step - loss: 1.0790 - acc: 0.7676 - val_loss: 0.5100 - val_acc: 0.8773<br/>Epoch 2/5<br/>60000/60000 [==============================] - 9s 143us/step - loss: 0.4401 - acc: 0.8866 - val_loss: 0.3650 - val_acc: 0.9011<br/>Epoch 3/5<br/>60000/60000 [==============================] - 12s 194us/step - loss: 0.3530 - acc: 0.9032 - val_loss: 0.3136 - val_acc: 0.9127<br/>Epoch 4/5<br/>60000/60000 [==============================] - 16s 272us/step - loss: 0.3129 - acc: 0.9124 - val_loss: 0.2868 - val_acc: 0.9188<br/>Epoch 5/5<br/>60000/60000 [==============================] - 12s 203us/step - loss: 0.2875 - acc: 0.9194 - val_loss: 0.2659 - val_acc: 0.9246<br/>Test Loss: 0.2659078140795231<br/>Test accuracy: 0.9246</span></pre><h1 id="4796" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">当我们有很多非常大的图像时会发生什么？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f64535aabee1fe85bdab5c9bfb7a4ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*tyx-e31J4ZrtJO3I"/></div></figure><p id="22fb" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">请注意，MLP 模型比我们简单的线性模型要精确得多，但也慢得多。当我们有大于 500Kb 到 1Mb 的图像时，将我们的图像作为一个序列进行处理的计算成本越来越高。此外，在我们的序列数据中检测复杂的自参照和层次模式也变得更具挑战性。</p><p id="524d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这种维数灾难是计算机视觉领域停滞不前的关键原因之一，直到 2012 年 AlexNet 的出现。</p><p id="9c10" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果不是将我们的完整图像作为向量表示来传递，而是将我们的图像表示为矩阵(28×28 ),而是提取代表性特征来做出分类决策，会怎么样？这就是计算机视觉直到最近的工作方式。让我们通过尝试使用边缘作为模型的特征来更深入地了解传统的图像特征提取。</p><p id="40c0" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为了做到这一点，我们首先采取如下图所示的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/c763c2d5e41b779b39c89542be46284d.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*awNMlwra0rrEmQ5-.PNG"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Src <a class="ae lj" href="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f0/Valve_original_%281%29.PNG/300px-Valve_original_%281%29.PNG" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a></figcaption></figure><p id="617c" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然后我们使用一个预定义的<a class="ae lj" href="https://en.wikipedia.org/wiki/Mask_(computing)#Image_masks" rel="noopener ugc nofollow" target="_blank">图像遮罩</a>，在这种情况下是一个用于提取边缘的<a class="ae lj" href="https://en.wikipedia.org/wiki/Sobel_operator" rel="noopener ugc nofollow" target="_blank">索贝尔矩阵</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/0e09e1efcb01ec5f9e5af9db191bed18.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/0*E_MaVV1r8OTVygQz"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">src <a class="ae lj" href="https://www.researchgate.net/publication/275073476/figure/fig6/AS:340842360393754@1458274560103/Sobel-masks-a-horizontal-mask-and-b-vertical-mask.png" rel="noopener ugc nofollow" target="_blank">researchgate publication #275073476</a></figcaption></figure><p id="f22f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们应用索贝尔矩阵掩模像过滤器一样大步前进到我们的图像</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b5473e1d06eb6b14ef692ecfa5b8903d.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*aACP-Q-9BagKSHLE"/></div></figure><p id="7b37" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">当我们可视化生成的图像时，我们得到下面的边缘，这些边缘可以用作特征</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/3eab350560d479332d250a3d77169fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*kA861d4qfmpeilDs.PNG"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Src <a class="ae lj" href="https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Valve_sobel_%283%29.PNG/300px-Valve_sobel_%283%29.PNG" rel="noopener ugc nofollow" target="_blank">Wikimedia Commons</a></figcaption></figure><p id="6263" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">手工制作像 Sobel 面具这样的面具是一项艰苦而脆弱的工作，如果我们能学会面具会怎么样呢？这是卷积神经网络或 CNN 背后的关键见解。</p><h1 id="e366" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">卷积神经网络介绍(CNN 101)</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/bf6595d67a8537c7c26fb135ee05b349.png" data-original-src="https://miro.medium.com/v2/format:webp/1*bUHwHbeoemYsA-g_lzP9KA.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The result of applied different learned conventional filters on a source imageSource: <a class="ae lj" href="https://bit.ly/2PlxOFC" rel="noopener ugc nofollow" target="_blank">video</a></figcaption></figure><p id="ceb8" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">CNN 是一个深度神经网络，由一系列层组成，其中一层的输出被馈送到下一层(有更复杂的架构，可以跳过具有 dropout 的层，我们现在将把这视为给定的)。通常，CNN 从卷积层和汇集层(向下采样)之间的交替开始，然后以分类部分的全连接层结束。</p><h1 id="37e1" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">盘旋</h1><p id="f4d8" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">卷积层是一组过滤器。每个滤波器由权重(<strong class="kp ir"> W </strong>)矩阵和偏差(<strong class="kp ir"> b </strong>)定义。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/3a664b9b71fea8c37a0d59e08a12bed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*4gF8milWo1QETDMt"/></div></figure><h1 id="268a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">联营</h1><p id="bc25" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">一旦我们应用我们的面具，我们使用池来减少前一层的维度，这加快了网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/ba9ad88d53d11eb9123319c95314fea2.png" data-original-src="https://miro.medium.com/v2/1*Feiexqhmvh9xMGVVJweXhg.gif"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Source: stanford.io/2Td4J2d</figcaption></figure><p id="5c28" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">有许多不同的汇集方法<strong class="kp ir">最大</strong>和<strong class="kp ir">平均</strong>汇集是最常见的。</p><p id="8956" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是一个步长为 2 的最大和平均池的示例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl mh"><img src="../Images/dc41617e7aa1dfd0dad587614550bc3d.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AaQqX4542KI_nTSGAepXMw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Various Pooling operations | Source: bit.ly/2K5zlP2</figcaption></figure><h2 id="8c05" class="nb ll iq bd lm pj pk dn lq pl pm dp lu kw pn po lw la pp pq ly le pr ps ma pt bi translated">将所有这些放在一起:</h2><p id="5a4f" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">在大多数 CNN 中，我们堆叠一组卷积层和池层，直到我们有一组代表性的特征，我们可以将其展平并用于类预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi pu"><img src="../Images/42a4385a4d5a85b290cd4a113238bba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cTbXkwcJhGW6thCr"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">src <a class="ae lj" href="https://www.mathworks.com/content/mathworks/www/en/discovery/convolutional-neural-network/jcr:content/mainParsys/image_copy.adapt.full.high.jpg/1508999490138.jpg" rel="noopener ugc nofollow" target="_blank">mathworks.com</a></figcaption></figure><p id="ab36" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">下面的代码显示了如何从上面的 MNIST 图像训练 CNN。</p><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="f1be" class="nb ll iq mw b gy nc nd l ne nf"><strong class="mw ir">from</strong> <strong class="mw ir">keras.layers</strong> <strong class="mw ir">import</strong> Dropout, Flatten<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.layers</strong> <strong class="mw ir">import</strong> Conv2D, MaxPooling2D</span><span id="af8b" class="nb ll iq mw b gy ng nd l ne nf">model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(3, 3),<br/>                 activation='relu',<br/>                 input_shape=input_shape))<br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.25))<br/>model.add(Flatten())<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(nb_classes, activation='softmax'))</span><span id="382b" class="nb ll iq mw b gy ng nd l ne nf">model.compile(loss=keras.losses.categorical_crossentropy,<br/>              optimizer=keras.optimizers.Adadelta(),<br/>              metrics=['accuracy'])</span><span id="97f6" class="nb ll iq mw b gy ng nd l ne nf">model.fit(x_train, y_train,<br/>          batch_size=batch_size,<br/>          epochs=nb_epoch,<br/>          verbose=1,<br/>          validation_data=(x_test, y_test))<br/>score = model.evaluate(x_test, y_test, verbose=0)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])<br/></span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="679b" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">Train on 60000 samples, validate on 10000 samples<br/>Epoch 1/5<br/>60000/60000 [==============================] - 177s 3ms/step - loss: 0.2638 - acc: 0.9204 - val_loss: 0.0662 - val_acc: 0.9790<br/>Epoch 2/5<br/>60000/60000 [==============================] - 173s 3ms/step - loss: 0.0882 - acc: 0.9732 - val_loss: 0.0404 - val_acc: 0.9865<br/>Epoch 3/5<br/>60000/60000 [==============================] - 166s 3ms/step - loss: 0.0651 - acc: 0.9806 - val_loss: 0.0350 - val_acc: 0.9883<br/>Epoch 4/5<br/>60000/60000 [==============================] - 163s 3ms/step - loss: 0.0549 - acc: 0.9836 - val_loss: 0.0334 - val_acc: 0.9887<br/>Epoch 5/5<br/>60000/60000 [==============================] - 159s 3ms/step - loss: 0.0472 - acc: 0.9859 - val_loss: 0.0322 - val_acc: 0.9899<br/>Test loss: 0.03221080291894468<br/>Test accuracy: 0.9899</span></pre><p id="5af3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在 MNIST 数据集中，我们有成千上万的训练样本，如果我们的策略任务中的数据更少会怎样？这就是我们可以使用<strong class="kp ir">迁移学习</strong>的地方。从头开始训练一个深度神经网络需要成千上万的图像，但训练一个已经学习了你正在适应的领域的特征的网络需要的图像要少得多。</p><h1 id="f5a1" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">第四部分迁移学习简介</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f64535aabee1fe85bdab5c9bfb7a4ed4.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*tyx-e31J4ZrtJO3I"/></div></figure><h1 id="8821" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">什么是迁移学习？</h1><p id="ece7" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">迁移学习，使用预先训练好的模型，并使其适应我们自己的问题。在迁移学习中，我们利用在基本模型的培训过程中学习到的功能和概念。旧的和新的预测层的输入与基础模型相同，我们只是重复使用训练过的特征。然后，我们训练这个修改后的网络，或者只训练新预测层的新权重，或者训练整个网络的所有权重。</p><p id="79ce" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">例如，当我们有一小组与现有训练模型处于相似领域的图像时，可以使用这种方法。在我们的例子中，这意味着使在 ImageNet 图像上训练的网络适应策略分类的任务。</p><h1 id="4c7a" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">预训练模型(MobileNet)</h1><p id="4651" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">Aditya Ananthram 的<a class="ae lj" href="https://github.com/aditya9898/transfer-learning" rel="noopener ugc nofollow" target="_blank"> repo </a>和<a class="ae lj" rel="noopener" target="_blank" href="/keras-transfer-learning-for-beginners-6c9b8b7143e"> post </a>是本节的灵感来源，我强烈建议您查看一下。</p><p id="ff77" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于这个任务，我们选择使用预训练的 MobileNet 模型作为我们的基础模型。虽然有许多分类体系结构，但我们将使用 MobileNet，因为它在 CPU 上运行速度很快，并提供强大的结果。</p><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="c87d" class="nb ll iq mw b gy nc nd l ne nf"><strong class="mw ir">from</strong> <strong class="mw ir">keras.layers</strong> <strong class="mw ir">import</strong> Dense,GlobalAveragePooling2D<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.applications</strong> <strong class="mw ir">import</strong> MobileNet<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.preprocessing</strong> <strong class="mw ir">import</strong> image<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.applications.mobilenet</strong> <strong class="mw ir">import</strong> preprocess_input<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.preprocessing.image</strong> <strong class="mw ir">import</strong> ImageDataGenerator<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.models</strong> <strong class="mw ir">import</strong> Model<br/><strong class="mw ir">from</strong> <strong class="mw ir">keras.optimizers</strong> <strong class="mw ir">import</strong> Adam</span><span id="7df3" class="nb ll iq mw b gy ng nd l ne nf">base_model=MobileNet(weights='imagenet',include_top=<strong class="mw ir">False</strong>) <em class="nh">#imports the mobilenet model and discards the last 1000 neuron layer.</em></span><span id="7df7" class="nb ll iq mw b gy ng nd l ne nf">x=base_model.output<br/>x=GlobalAveragePooling2D()(x)<br/>x=Dense(1024,activation='relu')(x) <em class="nh">#we add dense layers so that the model can learn more complex functions and classify for better results.</em><br/>x=Dense(1024,activation='relu')(x) <em class="nh">#dense layer 2</em><br/>x=Dense(512,activation='relu')(x) <em class="nh">#dense layer 3</em><br/>preds=Dense(2,activation='softmax')(x) <em class="nh">#final layer with softmax activation</em></span><span id="2d98" class="nb ll iq mw b gy ng nd l ne nf">model=Model(inputs=base_model.input,outputs=preds)</span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="ebf0" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf"><strong class="mw ir">for</strong> layer <strong class="mw ir">in</strong> model.layers[:20]:<br/>    layer.trainable=<strong class="mw ir">False</strong><br/><strong class="mw ir">for</strong> layer <strong class="mw ir">in</strong> model.layers[20:]:<br/>    layer.trainable=<strong class="mw ir">True<br/></strong></span><span id="0e57" class="nb ll iq mw b gy ng nd l ne nf">Downloading data from <a class="ae lj" href="https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5" rel="noopener ugc nofollow" target="_blank">https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5</a><br/>17227776/17225924 [==============================] - 13s 1us/step</span></pre><h2 id="515d" class="nb ll iq bd lm pj pk dn lq pl pm dp lu kw pn po lw la pp pq ly le pr ps ma pt bi translated">让我们来看看这个 MobileNet 模型</h2><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="2552" class="nb ll iq mw b gy nc nd l ne nf">model.summary()</span><span id="9368" class="nb ll iq mw b gy ng nd l ne nf">_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         (None, None, None, 3)     0         <br/>_________________________________________________________________<br/>conv1_pad (ZeroPadding2D)    (None, None, None, 3)     0         <br/>_________________________________________________________________<br/>conv1 (Conv2D)               (None, None, None, 32)    864       <br/>_________________________________________________________________<br/>conv1_bn (BatchNormalization (None, None, None, 32)    128       <br/>_________________________________________________________________<br/>conv1_relu (ReLU)            (None, None, None, 32)    0         <br/>_________________________________________________________________<br/>conv_dw_1 (DepthwiseConv2D)  (None, None, None, 32)    288       <br/>_________________________________________________________________<br/>conv_dw_1_bn (BatchNormaliza (None, None, None, 32)    128       <br/>_________________________________________________________________<br/>conv_dw_1_relu (ReLU)        (None, None, None, 32)    0         <br/>_________________________________________________________________<br/>conv_pw_1 (Conv2D)           (None, None, None, 64)    2048      <br/>_________________________________________________________________<br/>conv_pw_1_bn (BatchNormaliza (None, None, None, 64)    256       <br/>_________________________________________________________________<br/>conv_pw_1_relu (ReLU)        (None, None, None, 64)    0         <br/>_________________________________________________________________<br/>conv_pad_2 (ZeroPadding2D)   (None, None, None, 64)    0         <br/>_________________________________________________________________<br/>conv_dw_2 (DepthwiseConv2D)  (None, None, None, 64)    576       <br/>_________________________________________________________________<br/>conv_dw_2_bn (BatchNormaliza (None, None, None, 64)    256       <br/>_________________________________________________________________<br/>conv_dw_2_relu (ReLU)        (None, None, None, 64)    0         <br/>_________________________________________________________________<br/>conv_pw_2 (Conv2D)           (None, None, None, 128)   8192      <br/>_________________________________________________________________<br/>conv_pw_2_bn (BatchNormaliza (None, None, None, 128)   512       <br/>_________________________________________________________________<br/>conv_pw_2_relu (ReLU)        (None, None, None, 128)   0         <br/>_________________________________________________________________<br/>conv_dw_3 (DepthwiseConv2D)  (None, None, None, 128)   1152      <br/>_________________________________________________________________<br/>conv_dw_3_bn (BatchNormaliza (None, None, None, 128)   512       <br/>_________________________________________________________________<br/>conv_dw_3_relu (ReLU)        (None, None, None, 128)   0         <br/>_________________________________________________________________<br/>conv_pw_3 (Conv2D)           (None, None, None, 128)   16384     <br/>_________________________________________________________________<br/>conv_pw_3_bn (BatchNormaliza (None, None, None, 128)   512       <br/>_________________________________________________________________<br/>conv_pw_3_relu (ReLU)        (None, None, None, 128)   0         <br/>_________________________________________________________________<br/>conv_pad_4 (ZeroPadding2D)   (None, None, None, 128)   0         <br/>_________________________________________________________________<br/>conv_dw_4 (DepthwiseConv2D)  (None, None, None, 128)   1152      <br/>_________________________________________________________________<br/>conv_dw_4_bn (BatchNormaliza (None, None, None, 128)   512       <br/>_________________________________________________________________<br/>conv_dw_4_relu (ReLU)        (None, None, None, 128)   0         <br/>_________________________________________________________________<br/>conv_pw_4 (Conv2D)           (None, None, None, 256)   32768     <br/>_________________________________________________________________<br/>conv_pw_4_bn (BatchNormaliza (None, None, None, 256)   1024      <br/>_________________________________________________________________<br/>conv_pw_4_relu (ReLU)        (None, None, None, 256)   0         <br/>_________________________________________________________________<br/>conv_dw_5 (DepthwiseConv2D)  (None, None, None, 256)   2304      <br/>_________________________________________________________________<br/>conv_dw_5_bn (BatchNormaliza (None, None, None, 256)   1024      <br/>_________________________________________________________________<br/>conv_dw_5_relu (ReLU)        (None, None, None, 256)   0         <br/>_________________________________________________________________<br/>conv_pw_5 (Conv2D)           (None, None, None, 256)   65536     <br/>_________________________________________________________________<br/>conv_pw_5_bn (BatchNormaliza (None, None, None, 256)   1024      <br/>_________________________________________________________________<br/>conv_pw_5_relu (ReLU)        (None, None, None, 256)   0         <br/>_________________________________________________________________<br/>conv_pad_6 (ZeroPadding2D)   (None, None, None, 256)   0         <br/>_________________________________________________________________<br/>conv_dw_6 (DepthwiseConv2D)  (None, None, None, 256)   2304      <br/>_________________________________________________________________<br/>conv_dw_6_bn (BatchNormaliza (None, None, None, 256)   1024      <br/>_________________________________________________________________<br/>conv_dw_6_relu (ReLU)        (None, None, None, 256)   0         <br/>_________________________________________________________________<br/>conv_pw_6 (Conv2D)           (None, None, None, 512)   131072    <br/>_________________________________________________________________<br/>conv_pw_6_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_pw_6_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_dw_7 (DepthwiseConv2D)  (None, None, None, 512)   4608      <br/>_________________________________________________________________<br/>conv_dw_7_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_dw_7_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pw_7 (Conv2D)           (None, None, None, 512)   262144    <br/>_________________________________________________________________<br/>conv_pw_7_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_pw_7_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_dw_8 (DepthwiseConv2D)  (None, None, None, 512)   4608      <br/>_________________________________________________________________<br/>conv_dw_8_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_dw_8_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pw_8 (Conv2D)           (None, None, None, 512)   262144    <br/>_________________________________________________________________<br/>conv_pw_8_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_pw_8_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_dw_9 (DepthwiseConv2D)  (None, None, None, 512)   4608      <br/>_________________________________________________________________<br/>conv_dw_9_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_dw_9_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pw_9 (Conv2D)           (None, None, None, 512)   262144    <br/>_________________________________________________________________<br/>conv_pw_9_bn (BatchNormaliza (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_pw_9_relu (ReLU)        (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_dw_10 (DepthwiseConv2D) (None, None, None, 512)   4608      <br/>_________________________________________________________________<br/>conv_dw_10_bn (BatchNormaliz (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_dw_10_relu (ReLU)       (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pw_10 (Conv2D)          (None, None, None, 512)   262144    <br/>_________________________________________________________________<br/>conv_pw_10_bn (BatchNormaliz (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_pw_10_relu (ReLU)       (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_dw_11 (DepthwiseConv2D) (None, None, None, 512)   4608      <br/>_________________________________________________________________<br/>conv_dw_11_bn (BatchNormaliz (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_dw_11_relu (ReLU)       (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pw_11 (Conv2D)          (None, None, None, 512)   262144    <br/>_________________________________________________________________<br/>conv_pw_11_bn (BatchNormaliz (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_pw_11_relu (ReLU)       (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pad_12 (ZeroPadding2D)  (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_dw_12 (DepthwiseConv2D) (None, None, None, 512)   4608      <br/>_________________________________________________________________<br/>conv_dw_12_bn (BatchNormaliz (None, None, None, 512)   2048      <br/>_________________________________________________________________<br/>conv_dw_12_relu (ReLU)       (None, None, None, 512)   0         <br/>_________________________________________________________________<br/>conv_pw_12 (Conv2D)          (None, None, None, 1024)  524288    <br/>_________________________________________________________________<br/>conv_pw_12_bn (BatchNormaliz (None, None, None, 1024)  4096      <br/>_________________________________________________________________<br/>conv_pw_12_relu (ReLU)       (None, None, None, 1024)  0         <br/>_________________________________________________________________<br/>conv_dw_13 (DepthwiseConv2D) (None, None, None, 1024)  9216      <br/>_________________________________________________________________<br/>conv_dw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      <br/>_________________________________________________________________<br/>conv_dw_13_relu (ReLU)       (None, None, None, 1024)  0         <br/>_________________________________________________________________<br/>conv_pw_13 (Conv2D)          (None, None, None, 1024)  1048576   <br/>_________________________________________________________________<br/>conv_pw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      <br/>_________________________________________________________________<br/>conv_pw_13_relu (ReLU)       (None, None, None, 1024)  0         <br/>_________________________________________________________________<br/>global_average_pooling2d_1 ( (None, 1024)              0         <br/>_________________________________________________________________<br/>dense_7 (Dense)              (None, 1024)              1049600   <br/>_________________________________________________________________<br/>dense_8 (Dense)              (None, 1024)              1049600   <br/>_________________________________________________________________<br/>dense_9 (Dense)              (None, 512)               524800    <br/>_________________________________________________________________<br/>dense_10 (Dense)             (None, 2)                 1026      <br/>=================================================================<br/>Total params: 5,853,890<br/>Trainable params: 5,817,986<br/>Non-trainable params: 35,904<br/>_________________________________________________________________</span></pre><h1 id="e5ab" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">让我们处理我们的数据</h1><p id="afd2" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">下面的代码展示了如何使用 Keras 在我们的定制策略上训练一个定制的 MobileNet 模型。</p><h2 id="45cf" class="nb ll iq bd lm pj pk dn lq pl pm dp lu kw pn po lw la pp pq ly le pr ps ma pt bi translated">训练数据/测试数据</h2><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="3965" class="nb ll iq mw b gy nc nd l ne nf">train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) <em class="nh">#included in our dependencies</em></span><span id="0a8c" class="nb ll iq mw b gy ng nd l ne nf">train_generator=train_datagen.flow_from_directory(<br/>'/data/dataset/Beverages/Train/',<br/>target_size=(224,224),<br/>color_mode='rgb',<br/>batch_size=32,<br/>class_mode='categorical',<br/>shuffle=<strong class="mw ir">True</strong>)</span><span id="7483" class="nb ll iq mw b gy ng nd l ne nf">test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)<br/>test_generator = test_datagen.flow_from_directory(<br/>    directory=r"/data/dataset/Beverages/Test/",<br/>    target_size=(224, 224),<br/>    color_mode="rgb",<br/>    batch_size=1,<br/>    class_mode='categorical',<br/>    shuffle=<strong class="mw ir">False</strong>,<br/>    seed=42<br/>)</span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="3d5f" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">Found 180 images belonging to 2 classes.</span><span id="1e58" class="nb ll iq mw b gy ng nd l ne nf">Found 60 images belonging to 2 classes.</span></pre><h2 id="9e47" class="nb ll iq bd lm pj pk dn lq pl pm dp lu kw pn po lw la pp pq ly le pr ps ma pt bi translated">查看图像数据样本</h2><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="a620" class="nb ll iq mw b gy nc nd l ne nf">i = 0<br/><strong class="mw ir">for</strong> data <strong class="mw ir">in</strong> test_generator:<br/>    <strong class="mw ir">if</strong> i &gt; 3: <strong class="mw ir">break</strong> <br/>    <strong class="mw ir">else</strong>: i+=1<br/>    img, cls = data<br/>    print(np.argmax(cls))<br/>    plt.imshow(img[0])<br/>    plt.show()</span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="2c84" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><span id="4822" class="nb ll iq mw b gy ng nd l ne nf">0</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/7434b56221ed01935968ef64b4299e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*Nnbk8xPbsqpch5FfRsqt4A.png"/></div></figure><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="582a" class="nb ll iq mw b gy nc nd l ne nf">Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><span id="6238" class="nb ll iq mw b gy ng nd l ne nf">0</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/854242fbe7d4fc0c666b8adbd44f59c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*jQ77_uMpOSUz1bxSx5f26g.png"/></div></figure><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="407e" class="nb ll iq mw b gy nc nd l ne nf">Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><span id="ac12" class="nb ll iq mw b gy ng nd l ne nf">0</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi px"><img src="../Images/e00eb6a26775642a5088f7a2817556f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*4s2THOAH6dixvap2M_S6aQ.png"/></div></figure><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="8493" class="nb ll iq mw b gy nc nd l ne nf">Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</span><span id="9d1f" class="nb ll iq mw b gy ng nd l ne nf">0</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi py"><img src="../Images/f1da551a575d269eaf51a385db6b1e80.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*h2sPoolsdGp6EuCyK3GoTw.png"/></div></figure><h1 id="7334" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">火车模型</h1><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="b70b" class="nb ll iq mw b gy nc nd l ne nf">model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])<br/><em class="nh"># Adam optimizer</em><br/><em class="nh"># loss function will be categorical cross entropy</em><br/><em class="nh"># evaluation metric will be accuracy</em></span><span id="b291" class="nb ll iq mw b gy ng nd l ne nf">step_size_train=train_generator.n//train_generator.batch_size<br/>model.fit_generator(generator=train_generator,<br/>                   steps_per_epoch=step_size_train,<br/>                   epochs=5)</span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="5a62" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">Epoch 1/5<br/>5/5 [==============================] - 96s 19s/step - loss: 0.8017 - acc: 0.7313<br/>Epoch 2/5<br/>5/5 [==============================] - 77s 15s/step - loss: 0.0101 - acc: 1.0000<br/>Epoch 3/5<br/>5/5 [==============================] - 79s 16s/step - loss: 0.0289 - acc: 0.9937<br/>Epoch 4/5<br/>5/5 [==============================] - 111s 22s/step - loss: 0.0023 - acc: 1.0000<br/>Epoch 5/5<br/>5/5 [==============================] - 87s 17s/step - loss: 0.0025 - acc: 1.0000</span></pre><h1 id="2d33" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">基准模型</h1><p id="89af" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">正如我们在下面看到的，MobileNet 是一个学习和展示我们玩具政策的强大模型。</p><pre class="kg kh ki kj gt mx mw my mz aw na bi"><span id="f124" class="nb ll iq mw b gy nc nd l ne nf"><strong class="mw ir">from</strong> <strong class="mw ir">utils</strong> <strong class="mw ir">import</strong> classification_report<br/>y_true = np.concatenate([np.argmax(test_generator[i][1], axis=1) <strong class="mw ir">for</strong> i <strong class="mw ir">in</strong> range(test_generator.n)])<br/>y_pred =  np.argmax(model.predict_generator(test_generator, steps=test_generator.n), axis=1)<br/>classification_report(y_true, y_pred)</span></pre></div><div class="ab cl oq or hu os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="ij ik il im in"><pre class="mx mw my mz aw na bi"><span id="18f7" class="nb ll iq mw b gy ox oy oz pa pb nd l ne nf">precision    recall  f1-score   support</span><span id="2899" class="nb ll iq mw b gy ng nd l ne nf">           0       1.00      1.00      1.00        30<br/>           1       1.00      1.00      1.00        30</span><span id="9fae" class="nb ll iq mw b gy ng nd l ne nf">   micro avg       1.00      1.00      1.00        60<br/>   macro avg       1.00      1.00      1.00        60<br/>weighted avg       1.00      1.00      1.00        60</span><span id="2706" class="nb ll iq mw b gy ng nd l ne nf">Confusion matrix, without normalization<br/>[[30  0]<br/> [ 0 30]]<br/>Normalized confusion matrix<br/>[[1. 0.]<br/> [0. 1.]]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/c05279c421866b59910b1e8406d30b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*euEGreD1ovdAshFYfUpCWA.png"/></div></figure><p id="48e5" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">然而，如果我们的政策更复杂，我们可能很难以这种方式建模。在下一篇文章中，我们将深入探讨如何在复杂的图像分类场景中使用对象检测。</p><h1 id="a1e6" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">资源</h1><div class="nx ny gp gr nz oa"><a href="https://medium.com/microsoftazure/the-pythic-coders-recommended-content-for-getting-started-with-machine-learning-on-azure-fcd1c5a8dbb4" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">Pythic Coder 推荐的 Azure 机器学习入门内容</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">Tldr 由于 DevOps 资源上的帖子很受欢迎，而且很难找到文档，所以我…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">medium.com</p></div></div><div class="oj l"><div class="qa l ol om on oj oo kl oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://github.com/aribornstein" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">aribornstein —概述</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">@ pythiccoder。aribornstein 有 68 个存储库。在 GitHub 上关注他们的代码。</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">github.com</p></div></div><div class="oj l"><div class="qb l ol om on oj oo kl oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a href="https://azure.microsoft.com/services/cognitive-services/?v=18.44a&amp;v=18.44a&amp;v=18.44a&amp;WT.mc_id=medium-blog-abornst" rel="noopener  ugc nofollow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">认知服务|微软 Azure</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">微软 Azure Stack 是 Azure 的扩展——将云计算的灵活性和创新性带到您的…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">azure.microsoft.com</p></div></div><div class="oj l"><div class="qc l ol om on oj oo kl oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/a-big-of-tricks-for-image-classification-fec41eb28e01"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">图像分类的一大妙招</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">来拿你的深度学习好东西吧</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="qd l ol om on oj oo kl oa"/></div></div></a></div><div class="nx ny gp gr nz oa"><a rel="noopener follow" target="_blank" href="/https-medium-com-rishabh-grg-the-ultimate-nanobook-to-understand-deep-learning-based-image-classifier-33f43fea8327"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd ir gy z fp of fr fs og fu fw ip bi translated">理解基于深度学习的图像分类器的终极纳米书</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">在当今世界，我们广泛使用图像。不是吗？你有没有想过脸书是如何自动…</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">towardsdatascience.com</p></div></div><div class="oj l"><div class="qe l ol om on oj oo kl oa"/></div></div></a></div><h1 id="3039" class="lk ll iq bd lm ln lo lp lq lr ls lt lu jw lv jx lw jz lx ka ly kc lz kd ma mb bi translated">下一篇文章</h1><p id="1189" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated">在下一篇文章<a class="ae lj" rel="noopener" target="_blank" href="/using-object-detection-for-complex-image-classification-scenarios-part-4-3e5da160d272">中，我们将深入探讨如何将物体检测用于复杂的图像分类场景</a>。未来的职位将涵盖。</p><ul class=""><li id="01a2" class="nj nk iq kp b kq kr kt ku kw nl la nm le nn li qf np nq nr bi translated">使用 Azure ML 服务在云上训练和计算机视觉模型</li><li id="bf0b" class="nj nk iq kp b kq ns kt nt kw nu la nv le nw li qf np nq nr bi translated">使用 Azure 机器学习在远程集群上训练计算机视觉模型</li></ul><p id="322e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果您有任何问题、评论或希望我讨论的话题，请随时在<a class="ae lj" href="https://twitter.com/pythiccoder" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注我。如果您认为我错过了某个里程碑，请告诉我。</p><h2 id="3378" class="nb ll iq bd lm pj pk dn lq pl pm dp lu kw pn po lw la pp pq ly le pr ps ma pt bi translated">关于作者</h2><p id="4e4b" class="pw-post-body-paragraph kn ko iq kp b kq mc jr ks kt md ju kv kw me ky kz la mf lc ld le mg lg lh li ij bi translated"><a class="ae lj" href="https://www.linkedin.com/in/aaron-ari-bornstein-22aa7a77/" rel="noopener ugc nofollow" target="_blank"> <strong class="kp ir">亚伦(阿里)</strong> </a>是一个狂热的人工智能爱好者，对历史充满热情，致力于新技术和计算医学。作为微软云开发倡导团队的开源工程师，他与以色列高科技社区合作，用改变游戏规则的技术解决现实世界的问题，然后将这些技术记录在案、开源并与世界其他地方共享。</p></div></div>    
</body>
</html>