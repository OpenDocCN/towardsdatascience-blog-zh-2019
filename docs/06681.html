<html>
<head>
<title>Embedding for spelling correction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于拼写纠正的嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/embedding-for-spelling-correction-92c93f835d79?source=collection_archive---------15-----------------------#2019-09-24">https://towardsdatascience.com/embedding-for-spelling-correction-92c93f835d79?source=collection_archive---------15-----------------------#2019-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div class="gh gi jx"><img src="../Images/4a0ebe68d1a8e456b9e78a82fd635abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*jExSUnuvGXJvkUF3wakLig.jpeg"/></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">Photo by <a class="ae kj" href="https://unsplash.com/@daninasplash?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Da Nina</a> on <a class="ae kj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7f6b" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">尽管自 70 年代以来一直在进行自动拼写纠正，但在缺乏大量用户数据的情况下，仍然很难解决。</p><p id="9be0" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">噪声文本对于许多 NLP 任务来说是有问题的，因为它导致基于机器学习的技术的准确性降低，并且增加了诸如 Word2Vec 或 GloVe 之类的流行技术不能处理的词汇表外(OOV)单词的数量。</p><p id="1a52" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">因此，作为管道预处理的一部分，我们探索了几种方法来主动纠正输入数据的拼写，以提高下游任务的准确性。在这里，我们将重点讨论如何通过字符级嵌入来解决这个问题。我们的工作可以从头开始训练，使用和可视化，可以通过点击<a class="ae kj" href="https://github.com/Lettria/Char2Vec" rel="noopener ugc nofollow" target="_blank"><strong class="km iu"/></a><strong class="km iu"/>【1】链接下载和实验。</p><h1 id="62aa" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">介绍</h1><p id="6b75" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">拼写错误可以分为三类，需要不同的处理方法:</p><ol class=""><li id="6dd5" class="ml mm it km b kn ko kr ks kv mn kz mo ld mp lh mq mr ms mt bi translated"><strong class="km iu">实词错误</strong> : ' <strong class="km iu">三个'</strong>拼成'<strong class="km iu">有'</strong></li></ol><p id="00c7" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">真实的单词错误是最难纠正的，因为它们依赖于上下文，为了纠正它们，我们必须分析所有的数据来检查语义一致性和语法。根据所使用的算法，我们引入了错误地纠正准确句子的可能性，并增加了大量的处理时间。处理这个问题最常见的方法是 seq2seq 模型或使用统计数据，如 ngram 令牌。</p><p id="fb50" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="km iu"> 2。简短形式</strong> : ' <strong class="km iu"> you' </strong>拼写为'<strong class="km iu"> u' </strong></p><p id="92b4" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">简写形式的特点是与想要表达的单词共用几个字母。就<a class="ae kj" href="https://en.wikipedia.org/wiki/Edit_distance#targetText=In%20computational%20linguistics%20and%20computer,one%20string%20into%20the%20other." rel="noopener ugc nofollow" target="_blank"> <strong class="km iu">编辑距离</strong> </a> <strong class="km iu"> </strong>而言,“u”比“你”更接近“我”,因此基于字典的方法不太有效。</p><p id="03cd" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="km iu"> 3 </strong>。<strong class="km iu">非单词错误</strong> : ' <strong class="km iu"> fast' </strong>拼写<em class="mu"> </em>为'<strong class="km iu"> fsat' </strong></p><p id="3c3a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">非单词错误是最常见的，大多是由错别字引起的，这将是我们这里的重点。</p><p id="f73d" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">拼写纠正可以分为三个主要任务:</p><ol class=""><li id="35dc" class="ml mm it km b kn ko kr ks kv mn kz mo ld mp lh mq mr ms mt bi translated">以一个句子作为输入，确定哪些单词拼写错误。</li><li id="9c94" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh mq mr ms mt bi translated">从拼写错误的单词中，查找替换的候选列表。</li><li id="8694" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh mq mr ms mt bi translated">从列表中选择合适的候选人。(在自动校正的情况下)</li></ol><p id="0b89" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">因为我们只对非单词错误感兴趣，所以确定哪些单词拼写错误只需对照字典进行检查，尽管必须特别注意专有名词。</p><h1 id="8e96" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">候选人选择</h1><p id="0513" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">有几种不同的方法来选择候选人:</p><ul class=""><li id="54e8" class="ml mm it km b kn ko kr ks kv mn kz mo ld mp lh na mr ms mt bi translated">最简单的方法是计算你的单词和整个词典之间的编辑距离。虽然这种方法很准确，但是非常昂贵。</li><li id="a953" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated"><a class="ae kj" href="https://en.wikipedia.org/wiki/Phonetic_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="km iu">语音算法</strong></a>【2】如 Soundex、Phonex 或 Metaphone。这些算法将任何字符串编码成一个短序列，允许字符串索引的发音。“hurry”和“hirry”都将返回“H600”。通过预处理您的整个词典并按语音代码索引，您可以很容易地找到发音相似的候选词。运行时很快，但它只纠正语音错误。</li><li id="960e" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated"><strong class="km iu">计算你的单词的可能拼写错误列表</strong>(插入、删除、换位或替换)，并将其与你的字典匹配。虽然这比简单的方法要好，但是由于拼写错误的集合以 54 * length+25 的速率增加，所以速度非常慢。更多解释见<a class="ae kj" href="https://norvig.com/spell-correct.html" rel="noopener ugc nofollow" target="_blank"><strong class="km iu">Peter nor vig</strong></a>【3】的优秀文章。</li><li id="e5d1" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated"><strong class="km iu">对称拼写校正</strong>采用前面的想法，并通过计算字典和拼写错误的单词的拼写错误来扩展它。详见<a class="ae kj" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank"> <strong class="km iu">符号拼写</strong></a>【4】。这种技术既准确又非常快，但代价是大量的预计算和磁盘空间，并且需要字典的频率列表。</li></ul><h2 id="9a1f" class="nb lj it bd lk nc nd dn lo ne nf dp ls kv ng nh lw kz ni nj ma ld nk nl me nm bi translated">字符级嵌入</h2><p id="57d2" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">我们已经探索了解决这个问题的另一种方法，即使用单词嵌入在字符级别表示单词。流行的单词嵌入方法，如 Word2Vec，试图通过语义来表示单词，因为这对各种任务非常有用。然而，通过使用基于字符的模型，有可能基于单词拼写来构建单词嵌入。</p><p id="53f3" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这个想法类似于语音算法，其目的是在相同的单一度量(字符串或浮点数)下表示相似的单词，但是我们通过使用 n 维嵌入来扩展这个想法。</p><p id="13d0" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我们的方法如下:</p><ul class=""><li id="df7c" class="ml mm it km b kn ko kr ks kv mn kz mo ld mp lh na mr ms mt bi translated">训练模型以产生字符级单词嵌入</li><li id="6093" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated">向量化我们的整个字典，并建立一个有效的搜索索引</li><li id="685f" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated">矢量化拼写错误的单词并查找最近的邻居</li></ul><p id="45ac" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">该模型使用两层 LSTM 来构建选定大小的嵌入。更高的维度以更长的计算时间为代价提供更精确的结果，对于我们的使用，我们已经满足于维度 150。该模型通过向其传递一组单词来训练，这些单词要么是<em class="mu">两个完全不同的单词</em>，要么是<em class="mu">单词及其拼写错误</em>。训练目标是使相似元组的两个嵌入之间的差异的范数最小化<em class="mu">，并使不同单词的差异最大化。</em></p><p id="50c7" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">已经使用 600k 单词的字典生成了训练数据，并且为每个单词生成了编辑距离 1 或 2 的几个拼写错误。我们对 AZERTY 键盘上原始字符附近的拼写错误赋予了更高的概率(因为我们是用法语工作的),以便模型支持这些错误。由于缺乏明确的训练指标，很难准确知道何时停止训练，但在经历了几百万次训练后，我们对结果感到满意。</p><p id="b292" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">一旦经过训练，该模型就可以用于向量化单词。通过使用 PCA 将它们投影到 2D 平面上，我们可以获得以下可视化:</p><pre class="jy jz ka kb gt nn no np nq aw nr bi"><span id="139d" class="nb lj it no b gy ns nt l nu nv">python usage_visualisation.py</span></pre><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/494e766aaf1771b4b72e46e1770b7f07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*HLmLzQ8UG7asDjHme_z9GQ.png"/></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">The original model we used was developed by <a class="ae kj" href="https://github.com/IntuitionEngineeringTeam/chars2vec" rel="noopener ugc nofollow" target="_blank">IntuitionEngineering</a>.</figcaption></figure><p id="61a6" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">成功！相似的单词似乎在向量空间中被分组在一起。</p><p id="6b32" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这本身不是很有用，所以现在我们要对整个 60 万单词字典进行矢量化。在现代 CPU 上，这个过程大约需要 10 分钟。</p><p id="b579" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">最后一步是构建一个索引，它允许有效地搜索最近的向量。由于我们不需要 100%的准确性，更关心计算速度，我们将使用<a class="ae kj" href="https://github.com/nmslib/nmslib" rel="noopener ugc nofollow" target="_blank"><strong class="km iu">NMS lib</strong></a>【5】，一个专门用于 ANN(近似最近邻)搜索的库。</p><p id="9d11" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这个库允许我们在索引中搜索给定向量的 k 个最近邻。我们可以调整 k 个最近邻的数量，以恢复计算时间与精确度之间的平衡。一旦我们得到了最近邻的列表，我们可以使用编辑距离进一步过滤，只保留相关的建议。</p><p id="4fdb" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这为我们提供了以下输出(法语):</p><pre class="jy jz ka kb gt nn no np nq aw nr bi"><span id="ca8a" class="nb lj it no b gy ns nt l nu nv">python usage_correction.py 'langqge'</span></pre><blockquote class="nx ny nz"><p id="13d2" class="kk kl mu km b kn ko kp kq kr ks kt ku oa kw kx ky ob la lb lc oc le lf lg lh im bi translated"><strong class="km iu">编辑距离 1:</strong><br/>langqge:[' langage ']</p><p id="fcb0" class="kk kl mu km b kn ko kp kq kr ks kt ku oa kw kx ky ob la lb lc oc le lf lg lh im bi translated"><strong class="km iu">编辑距离 2: </strong> <br/> langqge : ['langages '，' lange '，' langé'，' langage']</p></blockquote><h2 id="b806" class="nb lj it bd lk nc nd dn lo ne nf dp ls kv ng nh lw kz ni nj ma ld nk nl me nm bi translated">结果</h2><p id="a191" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">我们已经在通过我们的拼写错误生成器生成的语料库(大约 4k 个句子)上测试了我们的模型。我们的衡量标准是准确性，我们将准确性定义为在候选列表中找到正确单词的百分比。</p><p id="7829" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">在整个语料库中，chars2vec 模型的准确率为 85%，而标准语音算法的准确率约为 40%。结合这两种方法，我们有 90%的准确率。主要的瓶颈是 4 个字符或更少的单词，它们的性能不好。</p><p id="06ad" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">这是意料之中的，因为:</p><ul class=""><li id="7b75" class="ml mm it km b kn ko kr ks kv mn kz mo ld mp lh na mr ms mt bi translated">那么大的单词在短编辑距离内有很多邻居。</li><li id="cb3f" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated">3 个字母的单词中的打字错误比 10 个字母的单词更有影响力，这使得模型更难在向量空间中正确地映射单词。</li></ul><h1 id="2ce2" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">校正选择</h1><p id="9021" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">虽然我们前面的例子直接给出了我们想要的答案，但在编辑距离内经常有几个候选人，必须做出选择。</p><p id="3ac9" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">我们尝试了两种不同的方法:</p><ul class=""><li id="bf1d" class="ml mm it km b kn ko kr ks kv mn kz mo ld mp lh na mr ms mt bi translated">概率性:使用建立在大型语料库上的频率列表，选择最常用的词。明显的缺点是一些不常用的单词可能永远不会被选择，虽然这种方法非常简单，但却提供了很好的准确性。</li><li id="aa6b" class="ml mm it km b kn mv kr mw kv mx kz my ld mz lh na mr ms mt bi translated">语义:根据候选词与句子中周围单词的语义相似性对候选词进行排序。这可以通过取一个预先训练的跳格词 2Vec 并计算嵌入的候选词和周围词(或整个句子)之间的平均距离来完成。虽然计算量很大，但这对于具有良好语义的单词给出了非常好的结果，尽管对于可以在任何上下文中使用的单词如“there”、“I”、“are”不是非常有效。</li></ul><p id="e169" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">根据经验，我们发现语义方法对于较长的、不常用的单词非常有效，而概率方法对于较短的单词更好。将这两种方法与手工制作的规则相结合，我们在随机生成的语料库上获得了大约 70 %的正确率。</p><h2 id="ec64" class="nb lj it bd lk nc nd dn lo ne nf dp ls kv ng nh lw kz ni nj ma ld nk nl me nm bi translated">未来工作:N-grams 令牌</h2><p id="6026" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">理想的方法是将两者结合起来，而不是在语义和频率之间进行选择。这可以通过使用单词 N 元语法的频率表来实现。</p><p id="ae72" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">为了建立这些表，在非常大的语料库上对 N 个单词的连续序列进行计数和编译。对此可用的最佳资源是<a class="ae kj" href="https://books.google.com/ngrams/" rel="noopener ugc nofollow" target="_blank"> <strong class="km iu">谷歌图书 N-gram</strong></a>【6】<strong class="km iu"/>，它是通过分析各种语言的大约<em class="mu">500 万本图书</em>而构建的。</p><figure class="jy jz ka kb gt kc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi od"><img src="../Images/f54a95b021bba166a657b03f7e4fed18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7b8rw4uRv4GfL3w3Zi2cw.png"/></div></div><figcaption class="kf kg gj gh gi kh ki bd b be z dk">‘Spelling Correction’ bi-gram occurrences</figcaption></figure><p id="3a2a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">原始数据可通过此<a class="ae kj" href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" rel="noopener ugc nofollow" target="_blank"> <strong class="km iu">链接</strong></a>【7】获得，尽管由于文本数量庞大，下载并将其编译成可用状态需要大量时间。</p><h1 id="26b5" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结论</h1><p id="0926" class="pw-post-body-paragraph kk kl it km b kn mg kp kq kr mh kt ku kv mi kx ky kz mj lb lc ld mk lf lg lh im bi translated">使用字符级嵌入可以获得良好的整体性能，尤其是对于较长的单词。通过手工制作的预处理规则和适当的候选项选择，这为现有的拼写校正解决方案提供了一种体面的替代方案。</p><p id="b53a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">进一步的工作将包括与预先存在的流行解决方案和 n-gram 令牌的使用适当的基准。在此期间，这个包可以免费试用:<a class="ae kj" href="https://github.com/Lettria/Char2Vec" rel="noopener ugc nofollow" target="_blank"><strong class="km iu">char 2 vec</strong></a>【1】。</p></div><div class="ab cl jq jr hx js" role="separator"><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv jw"/><span class="jt bw bk ju jv"/></div><div class="im in io ip iq"><p id="e50a" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">[1]<a class="ae kj" href="https://github.com/Lettria/Char2Vec" rel="noopener ugc nofollow" target="_blank">https://github.com/Lettria/Char2Vec</a></p><p id="f1e4" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">[2]https://en.wikipedia.org/wiki/Phonetic_algorithm<a class="ae kj" href="https://en.wikipedia.org/wiki/Phonetic_algorithm" rel="noopener ugc nofollow" target="_blank"/></p><p id="2e51" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">[3]https://norvig.com/spell-correct.html<a class="ae kj" href="https://norvig.com/spell-correct.html" rel="noopener ugc nofollow" target="_blank"/></p><p id="c353" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><a class="ae kj" href="https://github.com/wolfgarbe/SymSpell" rel="noopener ugc nofollow" target="_blank">https://github.com/wolfgarbe/SymSpell</a></p><p id="312b" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><a class="ae kj" href="https://github.com/nmslib/nmslib" rel="noopener ugc nofollow" target="_blank">https://github.com/nmslib/nmslib</a></p><p id="356e" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><a class="ae kj" href="https://books.google.com/ngrams/" rel="noopener ugc nofollow" target="_blank">https://books.google.com/ngrams/</a></p><p id="eeb1" class="pw-post-body-paragraph kk kl it km b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">[7]<a class="ae kj" href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" rel="noopener ugc nofollow" target="_blank">http://storage . Google APIs . com/books/n grams/books/datasetsv 2 . html</a></p></div></div>    
</body>
</html>