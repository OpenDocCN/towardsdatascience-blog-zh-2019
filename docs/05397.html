<html>
<head>
<title>Learn to Pay Attention! Trainable Visual Attention in CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学会关注！细胞神经网络中可训练的视觉注意</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-to-pay-attention-trainable-visual-attention-in-cnns-87e2869f89f1?source=collection_archive---------5-----------------------#2019-08-10">https://towardsdatascience.com/learn-to-pay-attention-trainable-visual-attention-in-cnns-87e2869f89f1?source=collection_archive---------5-----------------------#2019-08-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/8a3dad513b0f81ab7228e713daf8a733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8syelw0dvzvVTZr6Hn1jXg.jpeg"/></div></div></figure><p id="4d66" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在训练图像模型时，我们希望模型能够关注图像的重要部分。实现这一点的方法之一是通过可训练的注意力机制。在这篇文章中，我们将首先讨论事后注意力和可训练注意力以及软注意力和硬注意力之间的区别。然后，我们将深入研究 2018 年 ICLR 的一篇论文“学会关注”的细节，该论文描述了一种用于图像分类的可训练软注意力的方法。</p><h2 id="2d70" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">关键论文参考</strong></h2><p id="616d" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">这里是我们在这篇文章中要研究的关键论文:<a class="ae lx" href="https://arxiv.org/abs/1804.02391" rel="noopener ugc nofollow" target="_blank">杰特莱 S，洛德纳，李 N，托尔 PH. <em class="ly">学会注意。</em>2018 年 ICLR 奥运会。</a></p><p id="bda3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里还有一个很好的 Pytorch 实现:<a class="ae lx" href="https://github.com/SaoYan/LearnToPayAttention" rel="noopener ugc nofollow" target="_blank">SaoYan/LearnToPayAttention</a></p><p id="f860" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意:注意力也广泛用于自然语言处理。这篇文章的重点是计算机视觉任务中的注意力。</p><h1 id="3bb8" class="lz la it bd lb ma mb mc le md me mf lh mg mh mi lk mj mk ml ln mm mn mo lq mp bi translated">什么是注意力？</h1><h2 id="7ca5" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">可训练与事后注意机制</strong></h2><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/c68f8cc7b2c02e453eb0d5c1997b7c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/0*JWQKSXz2IREQ1EFj"/></div></figure><p id="e0ba" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们先从<a class="ae lx" href="https://www.google.com/search?ei=2dNNXfX0DKu3ggfPqqDgAg&amp;q=definitoin+of+attention&amp;oq=definitoin+of+attention&amp;gs_l=psy-ab.3..35i304i39l2j0i13l8.1862.2606..2812...0.0..0.100.348.4j1......0....1..gws-wiz.......0i71j0i22i30j0i13i30.2xI6Auq8_X0&amp;ved=0ahUKEwi1xbTuzPbjAhWrm-AKHU8VCCwQ4dUDCAo&amp;uact=5" rel="noopener ugc nofollow" target="_blank">这个词的英文定义</a>:</p><blockquote class="mv mw mx"><p id="f4ab" class="kb kc ly kd b ke kf kg kh ki kj kk kl my kn ko kp mz kr ks kt na kv kw kx ky im bi translated">注意<em class="it">:对某人或某事的注意；认为某人或某事有趣或重要</em></p></blockquote><p id="9cd6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">同样，在机器学习中，“注意力”指的是:</p><blockquote class="mv mw mx"><p id="31a0" class="kb kc ly kd b ke kf kg kh ki kj kk kl my kn ko kp mz kr ks kt na kv kw kx ky im bi translated"><em class="it">定义(1): </em>可训练的注意力<em class="it">:帮助“训练中的模型”更有效地注意重要事物的一组技术</em></p></blockquote><p id="002a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">和</p><blockquote class="mv mw mx"><p id="dc7e" class="kb kc ly kd b ke kf kg kh ki kj kk kl my kn ko kp mz kr ks kt na kv kw kx ky im bi translated"><em class="it">定义(2): </em>事后注意力<em class="it">:一组帮助人类可视化已经训练好的模型认为重要的东西的技术</em></p></blockquote><p id="1708" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当人们想到注意力时，他们通常会想到定义(1)，即可训练的注意力。一种可训练的注意力机制在网络被训练的同时被训练，并且应该帮助网络聚焦于图像的关键元素。</p><p id="fc8f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">令人困惑的是，事后<a class="ae lx" href="https://glassboxmedicine.com/2019/06/11/cnn-heat-maps-class-activation-mapping-cam/" rel="noopener ugc nofollow" target="_blank">热图可视化技术</a>有时也被称为“注意力”,这就是我包含定义(2)的原因。这些事后注意机制根据已经训练好的网络创建热图，包括:</p><ul class=""><li id="c9bb" class="nb nc it kd b ke kf ki kj km nd kq ne ku nf ky ng nh ni nj bi translated"><a class="ae lx" href="https://arxiv.org/abs/1311.2901" rel="noopener ugc nofollow" target="_blank">通过遮挡的热图</a>(泽勒，2013 年)</li><li id="f588" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="http://glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/" rel="noopener ugc nofollow" target="_blank">显著图</a>(西蒙扬 2013)</li><li id="3a46" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated">【凸轮】(周 2016)</li><li id="a42c" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://arxiv.org/abs/1610.02391" rel="noopener ugc nofollow" target="_blank"> Grad-CAM </a>(塞尔瓦拉茹 2017)</li><li id="71fb" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated">修复了 Grad-CAM 的一个问题</li></ul><p id="4dd6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我强调，这些事后技术不是为了改变模型学习的方式，或者改变模型学习的内容。它们被应用于已经训练好的具有固定权重的模型，并且仅用于提供对模型决策的洞察。</p><p id="78a1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，让我们看看 Jetley 等人对“注意力地图”的定义:</p><blockquote class="mv mw mx"><p id="e104" class="kb kc ly kd b ke kf kg kh ki kj kk kl my kn ko kp mz kr ks kt na kv kw kx ky im bi translated">注意图<em class="it">:标量矩阵，表示在不同 2D 空间位置的层激活相对于目标任务</em>的相对重要性</p></blockquote><p id="cdb5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">即，注意力地图是指示什么 2D 位置对于任务是重要的数字网格。重要位置对应更大的数字，通常在热图中用红色表示。在以下示例中，显示了“边境牧羊犬”的注意力地图，强调了边境牧羊犬在原始蒙太奇中的位置:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/031ff6e6a0f65ece27bafab7688f09c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*P3n2-VQmgJgSF-kI"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk"><strong class="bd nu">Image Source: </strong><a class="ae lx" href="https://github.com/ramprs/grad-cam" rel="noopener ugc nofollow" target="_blank"><strong class="bd nu">ramprs/grad-cam</strong></a></figcaption></figure><p id="9500" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">软关注与硬关注</strong></p><p id="d807" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可能听说过“软注意”和“硬注意”这两个术语。它们之间的区别如下:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/86b6e6bbebfb3534e9783b488ee77043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*gIh9xiXM3FCCj9bY"/></div><figcaption class="nq nr gj gh gi ns nt bd b be z dk">Modified from <a class="ae lx" href="https://en.wikipedia.org/wiki/Samoyed_dog#/media/File:Samoyed-sweetjedysamoyeds.jpg" rel="noopener ugc nofollow" target="_blank">Image Source</a></figcaption></figure><ul class=""><li id="cbc9" class="nb nc it kd b ke kf ki kj km nd kq ne ku nf ky ng nh ni nj bi translated"><em class="ly">柔和关注</em>使用“柔和阴影”来关注区域。可以使用<a class="ae lx" href="https://glassboxmedicine.com/2019/01/17/introduction-to-neural-networks/" rel="noopener ugc nofollow" target="_blank">传统的反向传播/梯度下降</a>来学习软注意力(用于学习神经网络模型的权重的相同方法)。)软注意力地图通常包含 0 到 1 之间的小数。</li><li id="7cfa" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><em class="ly">硬关注</em>使用图像裁剪来关注区域。它不能使用梯度下降来训练，因为没有“在这里裁剪图像”过程的导数像<a class="ae lx" rel="noopener" target="_blank" href="/an-intuitive-explanation-of-policy-gradient-part-1-reinforce-aa4392cbfd3c">强化</a>这样的技术可以用来训练严格的注意力机制。完全符合 0 或 1 的硬注意图，没有介于两者之间的东西；1 对应于保留的像素，0 对应于裁剪掉的像素。</li></ul><p id="5e82" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">关于软注意力和硬注意力的更深入的讨论，请看<a class="ae lx" href="http://akosiorek.github.io/ml/2017/10/14/visual-attention.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>(小节“什么是注意力？”、“硬注意”和“软注意”)以及<a class="ae lx" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>(子部分“软与硬注意”)。</p><h1 id="55e0" class="lz la it bd lb ma mb mc le md me mf lh mg mh mi lk mj mk ml ln mm mn mo lq mp bi translated">学会集中注意力</h1><p id="3602" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">论文<a class="ae lx" href="https://arxiv.org/pdf/1804.02391.pdf" rel="noopener ugc nofollow" target="_blank">“学会集中注意力”</a>在 CNN 模型中展示了一种软可训练视觉注意力的方法。他们考虑的主要任务是<a class="ae lx" href="https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/" rel="noopener ugc nofollow" target="_blank">多类分类</a>，其中的目标是将输入图像分配给单个输出类，例如将一张熊的照片分配给“bear”类作者证明了软可训练注意力在<a class="ae lx" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR-100 </a>上提高了 7%的多类分类性能，他们展示了示例热图，强调了注意力如何帮助模型专注于与正确的类标签最相关的图像部分。</p><h2 id="9d59" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">型号概述</strong></h2><p id="63e0" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">这是他们的模型图，根据论文中的图 2 修改而来:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/caa3df47fc5e4203e21f67b3972b4fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*8r63L3yR66SVBgJR"/></div></figure><p id="a82f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该模型基于<a class="ae lx" href="https://neurohive.io/en/popular-networks/vgg16/" rel="noopener ugc nofollow" target="_blank"> VGG 卷积神经网络</a>。VGG 网络有不同的配置，如<a class="ae lx" href="https://neurohive.io/en/popular-networks/vgg16/" rel="noopener ugc nofollow" target="_blank">图 2 所示</a>。(如果你好奇的话,“学会集中注意力”这篇论文似乎使用了介于配置 D 和 d E 之间的 VGG 配置；具体来说，有三个类似配置 D 的 256 通道层，但有八个类似配置 e 的 512 通道层。)</p><p id="2eb6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该图的要点是作者对基本的 VGG 设置做了两个关键的改变:</p><ol class=""><li id="f7d9" class="nb nc it kd b ke kf ki kj km nd kq ne ku nf ky nv nh ni nj bi translated">他们在第 7 层、第 10 层和第 13 层(我用黄色突出显示的层数)后插入了注意力评估器。)第 7 层之后的注意力估计器获取第 7 层的输出，并计算 0 和 1 之间的数字的“注意力屏蔽”，然后将其与第 7 层的原始输出相乘，以产生“g_a”(如上图所示。)对于层 10 和 13 之后的注意力估计器发生相同的过程，以分别产生 g_a 和 g_a。</li><li id="3b1a" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky nv nh ni nj bi translated">下一个大的变化是，作者去掉了通常位于 VGG 末尾的全连接层来产生预测(通常在编号为“16”的 FC 层之后还有另一个 FC 层，但他们已经去掉了它。)相反，分类现在通过一个新的全连接层进行，该层从三个注意力估计器接收输入。</li></ol><h2 id="5d96" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">符号</strong></h2><p id="c971" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">在我们深入研究他们的注意力机制是如何工作的之前，这里有一个在这篇文章中使用的符号的总结，我们将在这篇文章的剩余部分使用它:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/847f0383cb8566f46d1e788b17a250fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*klpR9IlKCj4f2njS"/></div></figure><h1 id="66b2" class="lz la it bd lb ma mb mc le md me mf lh mg mh mi lk mj mk ml ln mm mn mo lq mp bi translated">注意力是如何工作的</h1><h2 id="40ca" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">第一步:计算兼容性得分。</strong></h2><p id="16dd" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">使用局部特征<em class="ly"> l </em>和全局特征向量<em class="ly">g</em>来计算“兼容性得分”</p><p id="ea2a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作者解释说，当由局部特征描述的图像块“包含主导图像类别的部分”时，兼容性得分旨在具有高值</p><p id="e3ba" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">例如，如果图像中有一只猫，我们假设整只猫是由全局特征向量<em class="ly"> g </em>描述的，此外，我们期望一个特别“像猫”的补丁(例如，猫脸上的一个补丁)将产生局部特征<em class="ly"> l </em>，当与<em class="ly">g</em>组合时，这些局部特征产生高的兼容性分数</p><p id="715d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作者提出了从局部特征<em class="ly"> l </em>和全局特征向量<em class="ly"> g </em>计算兼容性得分<em class="ly"> c </em>的两种不同方法:</p><p id="dbc1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="ly">方法 1 </em>:“参数化兼容性”或“pc”:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/874011c0c6000b69a06f2ca1b56daebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/0*Xq8ZCvjzStn64iiX"/></div></figure><p id="160c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="ly">方法 2 </em>:“点积”或“dp”:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/622a5e8ee63b9f19a56b1998fc5a6a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/0*gIJITm5HeZ1yY2Wu"/></div></figure><p id="2d7a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">(剧透警告:“参数化兼容性”在他们的结果中表现更好。)</p><p id="7b07" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在“参数化兼容性”中，我们首先将局部特征添加到全局特征中，<em class="ly"> l+g </em>，然后我们用学习向量<em class="ly"> u </em>进行点积。直觉上，<em class="ly"> l </em>和<em class="ly"> g </em>的连接可能比加法更有意义，但作者指出，“给定局部和全局图像描述符之间现有的自由参数[……]我们可以将连接[……]简化为加法运算”，以便“限制关注单元的参数。”(其中一位审稿人还问为什么做加法而不做串联；参见<a class="ae lx" href="https://openreview.net/forum?id=HyzbhfWRW" rel="noopener ugc nofollow" target="_blank">打开查看</a>。)</p><p id="452c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在“点积”方法中，我们简单地取局部特征<em class="ly"> l </em>和全局特征向量<em class="ly">g</em>的点积。注意，对于我们应用关注的每个 conv 层(层 7、10 和 13)，局部特征<em class="ly"> l </em>对于该层将是唯一的，但是全局特征向量<em class="ly"> g </em>是相同的。</p><h2 id="1b87" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">如果<em class="ny"> l </em>和<em class="ny"> g </em>大小不一样怎么办？</strong></h2><p id="a2a1" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">事实证明，conv 第 7 层的<em class="ly"> l </em>有 256 个通道，但是<em class="ly"> g </em>有 512 个通道。为了将两个向量相加(pc 方法)或进行点积(dp 方法)，向量必须大小相同。</p><p id="2f52" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作者说如果<em class="ly"> l </em>和<em class="ly"> g </em>大小不一样，那么他们先把<em class="ly"> g </em>投影到<em class="ly"> l </em>的低维空间。他们不将 l 投影到 g 的高维空间的原因是为了限制参数的数量。(他们所说的“项目”是指应用神经网络层，使<em class="ly"> l </em>与<em class="ly"> g </em>大小相同。)</p><p id="ec8e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，将<em class="ly"> l </em>投影到<em class="ly"> g </em>仍然是一个可接受的解决方案；你只是让<em class="ly"> l </em>变大，而不是让<em class="ly"> g </em>变小。<a class="ae lx" href="https://github.com/SaoYan/LearnToPayAttention/blob/master/model1.py" rel="noopener ugc nofollow" target="_blank">“学会集中注意力”的 Pytorch 实现</a>使用“c1，G1 = self . Attn 1(self.projector(L1)，g)”行投射<em class="ly"> l </em>到<em class="ly"> g </em>，其中 self . projector 是一个单卷积层，它采用<em class="ly"> l </em>作为 256 个通道的输入，并创建 512 个通道的输出，以匹配<em class="ly"> g </em>的 512 个通道。</p><h2 id="c8ed" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">步骤 2:根据相容性分数 c 计算注意力权重</strong> a</h2><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/11b153236118e04c9a959ccc6260f369.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/0*j1qqGjtcbbSHRMAX"/></div></figure><p id="cb40" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们在这里所做的就是使用一个 softmax 将兼容性得分<em class="ly"> c </em>压缩到范围(0，1)内，我们将输出<em class="ly">称为</em>。有关 softmax 操作的回顾，请参见<a class="ae lx" href="https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><h2 id="d18d" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">第三步:计算每层<em class="ny"> s </em>的注意机制的最终输出。</strong></h2><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/25564284c29d9625e640f36d883f37e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/0*aVT26xGvwe3KTn1G"/></div></figure><p id="da3a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里，我们通过对该层的<em class="ly"> l </em>进行加权组合来计算特定层<em class="ly"> s </em>的注意机制<em class="ly"> g_a </em>的最终输出(回想一下，l 只是该层的输出)。)我们使用的权重是我们刚刚计算的注意力权重<em class="ly"> a </em>。</p><h2 id="c8ff" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">步骤 4:基于关注度最终输出进行分类预测</strong></h2><p id="0479" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">现在我们想要使用我们刚刚为第 7、10 和 13 层计算的注意力输出<em class="ly"> g_a </em>来做出分类决定。作者研究了两种选择:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/d4af9f94910e3039c24499354f877202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*85M_w5MiZXyFvsQM"/></div></figure><p id="7540" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="ly">方法 1: </em>“串联”:首先串联注意力输出，然后将它们一起馈入单个全连接层，以获得最终的预测。</p><p id="6d2e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="ly">方法二:</em>“indep”:将每个注意力输出馈入一个独立的全连接层，得到中间预测，然后将那些中间预测平均，得到最终预测。</p><p id="089d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">(剧透警告:“concat”在他们的结果中表现更好。)</p><h1 id="d212" class="lz la it bd lb ma mb mc le md me mf lh mg mh mi lk mj mk ml ln mm mn mo lq mp bi translated">结果</h1><h2 id="2d64" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">可训练的注意力提高表现</strong></h2><p id="d35f" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">作者评估了他们在各种任务中的注意机制，包括用 CIFAR-10、CIFAR-100 和 SVHN 进行多类分类。他们发现，使用他们的可训练注意力机制提高了“无注意力”基线的性能，相比之下，事后注意力机制(<a class="ae lx" href="https://glassboxmedicine.com/2019/06/11/cnn-heat-maps-class-activation-mapping-cam/" rel="noopener ugc nofollow" target="_blank"> CAM </a>)会导致性能下降(由于 CAM 方法施加的架构限制)。)</p><p id="894b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">他们的方法的以下变体执行得最好:“参数化兼容性”(通过将<em class="ly"> l </em>和<em class="ly"> g </em>相加，然后与学习向量<em class="ly"> u </em>和“concat”(在将注意力输出<em class="ly"> g_a </em>送入 fc 层进行预测之前，将它们连接起来)来计算兼容性得分。)</p><p id="3a24" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是他们论文中图 3 的一部分，展示了一些关注相关物体的注意力地图示例:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2990ed30b7908f3d94baedcb36b65c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/0*Elt987Rm6YceVAsY"/></div></figure><h2 id="4c84" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated"><strong class="ak">全局特征向量<em class="ny"> g </em>有多大用处？</strong></h2><p id="e825" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">他们论文的附录包含了一个有趣的讨论，关于全局特征向量<em class="ly"> g </em>在注意力计算中的效用。回想一下，获得兼容性得分的“pc”和“dp”方法都利用了全局特征向量<em class="ly"> g </em>。作者进行了实验，他们交换了使用的 g，发现:</p><ul class=""><li id="1e36" class="nb nc it kd b ke kf ki kj km nd kq ne ku nf ky ng nh ni nj bi translated">对于基于点积的注意机制[dp，表现较差]:“全局向量在引导注意方面起着突出的作用”</li><li id="ccd1" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated">对于参数化的兼容性函数[pc，性能更好]:“全局特征向量似乎是多余的。全局特征向量的任何变化都不会转移到结果注意力图。事实上，数值观察表明，全局特征的数量级通常比相应的局部特征的数量级小几个数量级。因此，全局特征向量的变化对预测的注意力得分几乎没有影响。然而，注意力地图本身能够始终如一地突出物体相关的图像区域。因此，似乎在基于参数化兼容性的注意力的情况下，以对象为中心的高阶特征被学习为权重向量<em class="ly"> u </em>的一部分。</li></ul><p id="acae" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">换句话说，我们也许可以像这样计算兼容性得分:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/5859c96ccc6036b564f34053901f6aeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/0*kyKuRlDLeKD8GMco"/></div></figure><p id="be50" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">根本不用<em class="ly"> g </em>！</p><p id="e404" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不管<em class="ly"> g </em>扮演的确切角色是什么，这种“学会集中注意力”的方法确实改善了结果，也确实产生了不错的注意力地图——因此，可训练的注意力显然在做一些有用的事情，即使它没有按照作者最初打算的方式使用<em class="ly"> g </em>。</p><h1 id="25e9" class="lz la it bd lb ma mb mc le md me mf lh mg mh mi lk mj mk ml ln mm mn mo lq mp bi translated">摘要</h1><ul class=""><li id="6e49" class="nb nc it kd b ke ls ki lt km od kq oe ku of ky ng nh ni nj bi translated">可训练的注意力机制具有在训练期间学习的注意力权重，这有助于模型聚焦于对任务重要的图像的关键部分；</li><li id="7672" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated">事后注意机制是在模型完成训练后应用的技术，旨在提供模型在进行预测时观察哪里的洞察力；</li><li id="a142" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated">注意力集中是图像裁剪，可以通过强化来训练。软注意产生“模糊”的焦点区域，可以使用常规的反向传播来训练。</li><li id="c779" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated">“学会关注”是一篇有趣的论文，展示了软可训练的注意力如何提高图像分类性能并突出图像的关键部分。</li></ul><h2 id="6c89" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">额外资源</h2><ul class=""><li id="66ac" class="nb nc it kd b ke ls ki lt km od kq oe ku of ky ng nh ni nj bi translated"><a class="ae lx" href="http://akosiorek.github.io/ml/2017/10/14/visual-attention.html" rel="noopener ugc nofollow" target="_blank">神经网络中的注意力以及如何使用它</a>亚当·科西奥雷克著。这是一篇关于计算机视觉中注意力的伟大文章。它讨论了为什么注意是有用的，软与硬注意，高斯注意(代码)，空间转换器(代码)。</li><li id="0e9f" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">注意？立正！</a>作者莉莲·翁。这是另一篇很棒的文章，关注自然语言处理中的注意力。它讨论了 seq2seq，自我注意，软与硬注意，全局与局部注意，神经图灵机，指针网络，变形金刚，蜗牛和自我注意甘。</li><li id="80c9" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://glassboxmedicine.com/2019/08/15/the-transformer-attention-is-all-you-need/" rel="noopener ugc nofollow" target="_blank">变形金刚:注意力是你所需要的全部</a>:解释变形金刚，包括它的多头可训练自我注意力机制。</li><li id="c335" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://glassboxmedicine.com/2019/06/21/cnn-heat-maps-saliency-backpropagation/" rel="noopener ugc nofollow" target="_blank"> CNN 热图:显著性/反向传播</a>:解释事后注意力技术显著性映射。</li><li id="b584" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://glassboxmedicine.com/2019/10/06/cnn-heat-maps-gradients-vs-deconvnets-vs-guided-backpropagation/" rel="noopener ugc nofollow" target="_blank"> CNN 热图:梯度 vs .去卷积 vs .引导反向传播</a>:解释这三种事后注意力方法实际上是相同的，除了对非线性的处理。</li><li id="c1fb" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://glassboxmedicine.com/2019/06/11/cnn-heat-maps-class-activation-mapping-cam/" rel="noopener ugc nofollow" target="_blank"> CNN 热点图:类激活映射(CAM) </a>:解释了类激活映射架构和注意机制，<a class="ae lx" href="https://arxiv.org/abs/2011.08891" rel="noopener ugc nofollow" target="_blank">和 HiResCAM 一起被证明可以保证显示模型在看哪里。</a></li><li id="387d" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://glassboxmedicine.com/2019/10/12/guided-grad-cam-is-broken-sanity-checks-for-saliency-maps/" rel="noopener ugc nofollow" target="_blank">制导摄像机坏了！显著图的健全性检查</a>:解释 NeurIPS 的一篇论文，该论文揭示了几种流行的注意力机制的问题。</li><li id="951e" class="nb nc it kd b ke nk ki nl km nm kq nn ku no ky ng nh ni nj bi translated"><a class="ae lx" href="https://glassboxmedicine.com/2020/05/29/grad-cam-visual-explanations-from-deep-networks/" rel="noopener ugc nofollow" target="_blank"> Grad-CAM:来自深度网络的视觉解释</a>:解释事后注意机制 Grad-CAM。注意:尽管 Grad-CAM 的论文被引用了几千次，<a class="ae lx" href="https://arxiv.org/abs/2011.08891" rel="noopener ugc nofollow" target="_blank">最近的工作证明了 Grad-CAM 的一个严重问题</a>，即它有时会突出显示模型实际上没有使用的无关区域。</li></ul><h2 id="6e8c" class="kz la it bd lb lc ld dn le lf lg dp lh km li lj lk kq ll lm ln ku lo lp lq lr bi translated">关于特色图片</h2><p id="34e2" class="pw-post-body-paragraph kb kc it kd b ke ls kg kh ki lt kk kl km lu ko kp kq lv ks kt ku lw kw kx ky im bi translated">图片来源:<a class="ae lx" href="https://en.wikipedia.org/wiki/Poi_(performance_art)#/media/File:Flammenjongleur.jpg" rel="noopener ugc nofollow" target="_blank">表演者带火 poi </a>。<a class="ae lx" href="https://en.wikipedia.org/wiki/Poi_(performance_art)" rel="noopener ugc nofollow" target="_blank"> Poi </a>是一种专注于摆动系留重物的表演艺术。它起源于新西兰。</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><p id="8d5a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="ly">原载于 2019 年 8 月 10 日</em><a class="ae lx" href="https://glassboxmedicine.com/2019/08/10/learn-to-pay-attention-trainable-visual-attention-in-cnns/" rel="noopener ugc nofollow" target="_blank"><em class="ly">http://glassboxmedicine.com</em></a><em class="ly">。</em></p></div></div>    
</body>
</html>