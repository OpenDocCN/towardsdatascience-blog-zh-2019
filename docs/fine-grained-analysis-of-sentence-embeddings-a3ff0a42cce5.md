# 句子嵌入的细粒度分析

> 原文：<https://towardsdatascience.com/fine-grained-analysis-of-sentence-embeddings-a3ff0a42cce5?source=collection_archive---------16----------------------->

将单词或句子表示为高维空间中的实值向量，使我们能够将深度学习方法融入自然语言处理任务中。这些嵌入作为从序列标签到信息检索的各种机器学习任务的特征。单词 2Vec(及其变体)是通过使用分布假设*(出现在相同语境中的单词往往具有相似的含义)*来生成单词嵌入的 go 模型。

然而，当一个人想要对可变长度的句子进行编码时，事情就变得更加棘手了。在自然语言处理领域，为可变长度的句子生成固定长度的嵌入已经有了很多的研究。这些固定长度的句子嵌入对于涉及句子级语义的任务(如文本摘要)至关重要。

![](img/03e7679d8fe31a48c5cd93d106312e6e.png)

Similar sentences have similar embeddings. Image from [TechViz](https://prakhartechviz.blogspot.com/2019/05/baseline-sentence-embeddings.html)

生成句子嵌入的最简单方法是简单地连接组成单词的单词嵌入。然而，这为不同长度的句子产生了可变长度的嵌入。对于期望固定长度的特征向量的下游模型，这是有问题的。因此，我们需要想出一种方法，将单词嵌入简化为一个固定长度的向量，该向量可以捕捉句子的所有重要方面。

有两种流行的生成固定长度句子嵌入的技术:

*   连续单词袋(CBOW)
*   序列对序列模型，如 rnn 或 LSTMs

# 连续的单词袋

CBOW 方法不考虑句子中单词的顺序。这是一种非常天真的方法，只需对所有组成单词的单词嵌入进行求和(或求平均值)来生成句子嵌入。

*CBOW 是一个重载术语，也是用于训练 word2vec 算法的一种算法的名称。基本上，单词包用于描述任何忽略单词顺序的算法或嵌入技术。连续来自于这样一个事实，即我们在实值向量的域中操作。因此，CBOW 是一种建模方法，在这种方法中，人们在组合单词嵌入时忽略了单词排序。*

人们可能会觉得这是一种非常糟糕的获取句子信息的方式，因为它完全忽略了词序。

> 例如:*“努力学习，不要玩！”*、*“玩命，不学习！”*将具有相同的【CBOW 嵌入，尽管它们的**含义与**相反。

然而，CBOW 在实践中被证明是一种有效的嵌入技术，并且在更复杂的模型出现之前是标准的。

# 序列对序列模型

像 RNN、LSTM 这样的序列到序列模型逐个处理单词嵌入，同时保持存储上下文信息的隐藏状态。在处理一个句子结束时的隐藏状态实质上编码了来自整个句子的信息，因此代表了该句子的嵌入。

![](img/cc5ccc7187d61a2aac44558b65ef4690.png)

Input is the word embedding for (t+1)th word, and the (t+1)th output is the context aware embedding for that word. The state at this stage represents the context embedding till the (t+1)th word.

该架构以编码器-解码器的方式训练，其中 RNN 或 LSTM 充当编码器。由编码器产生的嵌入被馈送到执行一些其他任务的解码器。在这个任务上的损失训练了编码器-解码器架构，此后可以丢弃解码器，而仅仅使用编码器作为嵌入生成器。

这种产生句子嵌入的方法能够基于句子中的单词排序进行区分，因此可能提供比 CBOW 模型更丰富的嵌入。

# 分析嵌入

由上述两种方法生成的句子嵌入对于解释来说是高度不透明的，并且不能直接评估它们的强度。评估这些嵌入的唯一可能的方法是使用利用这些嵌入的下游任务，然后比较这个复合模型在任务上的性能。很明显，这不是最理想的，而且还有很多需要改进的地方。要打开这个黑箱， [*使用辅助预测任务对句子嵌入进行细粒度分析(阿迪等人)*](https://groups.csail.mit.edu/sls/publications/2017/ICLR17_Belinkov.pdf) 步骤在。他们提出了一种方法，在细粒度级别上比较句子嵌入的基本句子特征，如句子长度、句子中的项目及其顺序。

## 方法学

对于句子的每个低级特征，制定一个预测任务，并为该任务训练一个分类器网络。分类器的性能显示了句子嵌入能够多好地捕捉该特征。由于这些特征是可直接解释的句子的低级属性，因此该实验提供了对句子嵌入质量的洞察。

正在考虑的特征是:

*   **句子长度:**给定一个句子嵌入，分类器网络必须预测句子的长度(句子被分入不同的类别)。
*   **单词出现:**给定一个句子嵌入 **s** 和一个单词嵌入 **w** ，分类器要预测这个单词是否出现在句子中。
*   **单词排序:**给定一个句子嵌入 **s** 和单词嵌入 **w1** 和 **w2，**分类器需要预测哪个单词先出现。

考虑中的嵌入是使用 word2vec 嵌入上的平均的 CBOW，以及 LSTM 自动编码器-解码器。自动编码器-解码器意味着网络被训练产生与输出相同的输入。解码器和编码器都是 LSTM，编码器 LSTM 用于生成句子嵌入。

注:图例中的“Perm”指的是句子中的某些单词随机排列的实验。

## 结果

*   **长度实验**

![](img/162cc356aadd259fc7ddb8b084e3c321.png)

From the paper

显然，LSTM 架构能够对句子长度进行编码，准确率超过 85%。然而，更令人惊讶的是，与 20%的大多数预测准确性相比，CBOW 表现得非常好(65%)。

当人们看到句子嵌入的规范与句子长度的关系图时，可以解释 CBOW 的这种令人惊讶的表现。

![](img/a4c1a3d9d1670e85210b84b16234324b.png)

随着越来越多的单词嵌入被平均在一起，总和接近于零。单词嵌入在原点周围相当均匀地分布，因此根据中心极限定理，可以预期随着越来越多的嵌入被添加，总和接近零。因此，嵌入的规范作为句子长度的指标。

*   **词出现**

![](img/7021d735c6fdb884641921848b8fd586.png)

CBOW 的真正实力在这里可见一斑。对于低维嵌入，CBOW 能够比更复杂的顺序模型更好地捕捉单词身份。令人惊讶的是，性能随着维数的增加而降低。

*   **词序**

![](img/84e98d837b782ded55afdb219da38797.png)

正如预期的那样，LSTM 嵌入能够很好地捕捉单词的排序，并且随着嵌入维度的增加，性能也增加。然而，当考虑到 CBOW 模型没有试图保留任何单词顺序时，CBOW 的性能也是值得注意的，正如我们在上面看到的那样。CBOW 在平衡类设置中给出 70%的准确度，即随机预测准确度为 50%。

作者假设大部分词序信息是在词序统计中获得的，即统计上某些词出现在其他词之前。这一假设得到了一项实验的进一步支持，在该实验中，作者完全放弃了句子嵌入，而只使用单词嵌入来预测单词排序。

![](img/5f588545bf8315c733b9aa3d6537b209.png)

编码器-解码器架构的性能也下降到 CBOW 的水平。这表明一些关于词序的信息是由词序统计本身捕获的，而额外的排序信息是由句子排序提供的。

即使去除句子嵌入，CBOW 模型的性能也几乎不受影响。

# 最后的想法

我们看到，尽管 CBOW 是一个如此简单的模型，但它在某些任务上却惊人地有效。对于低维嵌入，它能够很好地保持单词的同一性，并且在一定程度上能够对句子长度和单词排序进行编码。

LSTM 嵌入在编码低级句子属性方面非常有效。然而，增加嵌入的维度超过某个点提供了边际收益，事实上在某些情况下是有害的。

最后，实验只考虑了低级的句子属性。这显示了嵌入在捕捉句子的表面方面的有效性，但是没有显示语义概括或更深的句法方面的许多细节。对于这种概括，需要考虑单独的辅助任务，作者将其作为未来的工作。

**免责声明**:本文提供了一个构建来介绍在 [*中提到的使用辅助预测任务对句子嵌入进行细粒度分析的结果(Adi 等人；17)*](https://groups.csail.mit.edu/sls/publications/2017/ICLR17_Belinkov.pdf) *。*所有显示结果的图片都来自论文，许多讨论和假设都直接取自论文，只是稍加转述。

干杯！