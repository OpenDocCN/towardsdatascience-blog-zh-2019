<html>
<head>
<title>The 5 Feature Selection Algorithms every Data Scientist should know</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个数据科学家都应该知道的 5 种特征选择算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2?source=collection_archive---------0-----------------------#2019-07-27">https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2?source=collection_archive---------0-----------------------#2019-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9c4cebf5892aa677f0d4b55d55f350a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Feid5O1I9KethU8WX45CTg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Source: <a class="ae jg" href="https://pixabay.com/photos/children-splash-asia-sunset-1822688/" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><h2 id="92f6" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/ds-algorithms" rel="noopener" target="_blank"> DS 算法</a></h2><div class=""/><div class=""><h2 id="095e" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">额外收获:是什么让一个优秀的足球运动员变得伟大？</h2></div><p id="10d5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据科学是对算法的研究。</p><p id="8a40" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我每天都在努力学习许多算法，所以我想列出一些最常见和最常用的算法，这些算法将在这个新的<a class="ae jg" href="https://towardsdatascience.com/tagged/ds-algorithms" rel="noopener" target="_blank"> DS 算法系列</a>中使用。</p><p id="0ee3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当你创建了大量的特性，然后你需要想办法减少特性的数量，这种情况已经发生了多少次了。</p><p id="16e4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们有时最终会使用相关性或基于树的方法来找出重要的特征。</p><p id="7efc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们能给它增加一些结构吗？</p><p id="b439" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> <em class="md">这篇文章是关于在处理数据时可以使用的一些最常见的特征选择技术。</em> </strong></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="be0a" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">为什么选择功能？</h1><p id="d5a3" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">在我们继续之前，我们需要回答这个问题。为什么不把所有的特征都交给 ML 算法，让它来决定哪个特征重要？</p><p id="1dbf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以我们有三个理由不这么做:</p><h2 id="a070" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">1.维数灾难——过度拟合</h2><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7c21b48cb7e8aa00842fa83340f58d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*tqjmErkEdzl2DrtjMg7GzQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://chrisalbon.com/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="97e2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果数据中的列数多于行数，我们将能够完美地拟合我们的训练数据，但这不会推广到新的样本。因此我们什么也没学到。</p><h2 id="3198" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">2.奥卡姆剃刀:</h2><p id="b3b7" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们希望我们的<strong class="lj jt"> <em class="md">模型是简单的</em> </strong>和可解释的。当我们有很多特征时，我们就失去了可解释性。</p><h2 id="c892" class="ni mm jj bd mn nj nk dn mr nl nm dp mv lq nn no mx lu np nq mz ly nr ns nb jp bi translated">3.垃圾输入垃圾输出:</h2><p id="84e4" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">大多数时候，我们会有许多非信息性的特征。例如，名称或 ID 变量。<strong class="lj jt"> <em class="md">低质量的输入会产生低质量的输出。</em>T15】</strong></p><p id="a745" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，大量的特征使得模型庞大、耗时，并且在生产中更难实现。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="65fa" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">那我们该怎么办？</h1><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/a1a5b1503ebe72d0403ea90d4ee6456f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fFuXBJ5HZHIEV5n7"/></div></div></figure><p id="5487" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们只选择有用的特征。</p><p id="d149" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">幸运的是，Scikit-learn 让我们选择特性变得非常容易。我们可以有很多方式来考虑特征选择，但是大多数特征选择方法可以分为三大类</p><ul class=""><li id="7d44" class="nz oa jj lj b lk ll ln lo lq ob lu oc ly od mc oe of og oh bi translated"><strong class="lj jt"> <em class="md">基于过滤:</em> </strong>我们指定一些度量并基于这些过滤特征。这种度量的一个例子可以是相关/卡方。</li><li id="b1e5" class="nz oa jj lj b lk oi ln oj lq ok lu ol ly om mc oe of og oh bi translated"><strong class="lj jt"> <em class="md">基于包装器:</em> </strong>包装器方法将一组特征的选择视为一个搜索问题。示例:递归特征消除</li><li id="39bb" class="nz oa jj lj b lk oi ln oj lq ok lu ol ly om mc oe of og oh bi translated"><strong class="lj jt"> <em class="md">嵌入式:</em> </strong>嵌入式方法使用具有内置特征选择方法的算法。例如，套索和射频有自己的特征选择方法。</li></ul></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="0e99" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">理论讲得够多了，让我们从五种特征选择方法开始。</p><p id="5aa1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将尝试使用数据集来更好地理解它。</p><p id="2856" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我将使用一个足球运动员数据集来找出<strong class="lj jt"> <em class="md">是什么让一个好球员变得伟大？</em> </strong></p><p id="021a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> <em class="md">不懂足球术语也不用担心。我会尽量把它保持在最低限度。</em> </strong></p><p id="f27e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是 Kaggle <a class="ae jg" href="https://www.kaggle.com/mlwhiz/feature-selection-using-football-data" rel="noopener ugc nofollow" target="_blank">内核</a>的代码，您可以自己尝试一下。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="134f" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">一些简单的数据预处理</h1><p id="232f" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">我们已经做了一些基本的预处理，如删除空值和一个热编码。并且使用以下方式将该问题转换成分类问题:</p><pre class="nu nv nw nx gt on oo op oq aw or bi"><span id="a83a" class="ni mm jj oo b gy os ot l ou ov">y = traindf['Overall']&gt;=87</span></pre><p id="2228" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这里，我们用高总体来代表一个伟大的球员。</p><p id="b0bf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们的数据集(X)如下所示，有 223 列。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/520d6578d41156bdcde7dac5899eddaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UidWexvUgDVFAGpMyKv2iQ.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">train Data X</figcaption></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="ee75" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">1.皮尔逊相关</h1><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/ea355a827d043b5eea7647d4a7ef448a.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/0*EOpX0Ofh_zqAkLRk.png"/></div></figure><p id="a389" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一种基于过滤器的方法。</p><p id="dfff" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们检查数据集中目标和数字特征之间的皮尔逊相关性的绝对值。我们根据这个标准保留前 n 个特征。</p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="5a76" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">2.卡方检验</h1><p id="a4c4" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">这是另一种基于过滤器的方法。</p><p id="15f4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在该方法中，我们计算目标和数值变量之间的卡方度量，并且仅选择具有最大卡方值的变量。</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/3677de59db386cccd2d082e1ca760133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l-qVhxRq6218__Qp.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://chrisalbon.com/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="ba97" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们创建一个小例子来说明如何计算样本的卡方统计量。</p><p id="4927" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设我们的数据集中有 75 个右前锋和 25 个非右前锋。我们观察到 40 个右前锋是好的，35 个不好。这是否意味着球员在右前卫会影响整体表现？</p><div class="nu nv nw nx gt ab cb"><figure class="pb iv pc pd pe pf pg paragraph-image"><img src="../Images/fb21696aeb761abcbbc20f154e3bb11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*qtDIt-7rlWqMVdwKAGJ4ww.png"/></figure><figure class="pb iv pc pd pe pf pg paragraph-image"><img src="../Images/5ae48a7ec5ac3613d182471d36a8de38.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*uQYw4RvEuyTpYU6F1H2rPw.png"/><figcaption class="jc jd gj gh gi je jf bd b be z dk ph di pi pj">Observed and Expected Counts</figcaption></figure></div><p id="d3dd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们计算卡方值:</p><p id="7050" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了做到这一点，我们首先找出如果两个分类变量之间确实存在独立性，我们期望落在每个桶中的值。</p><p id="da02" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这很简单。我们将每个单元格的行和与列和相乘，然后除以总观察值。</p><p id="2de3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">so Good and NotRightforward 时段期望值= 25(行总和)*60(列总和)/100(总观察值)</p><p id="dbcc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为什么会这样？由于数据中有 25%的非右前锋，我们预计我们在该单元观察到的 60 名优秀球员中有 25%是右前锋。因此 15 个玩家。</p><p id="a927" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后我们可以使用下面的公式对所有 4 个单元格求和:</p><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/599b9e9a46ec0e00bef311c44ff599dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_whJSrJbRUGnimXr.jpg"/></div></div></figure><p id="db8d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我不会在这里展示它，但卡方统计也以手动方式处理非负数字和分类特征。</p><p id="2e16" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以从数据集获得卡方特征，如下所示:</p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="0cc5" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">3.递归特征消除</h1><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/58786db9ab144cf20b2467ad9ddc450d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lkkVBAfy18P-ktY8.jpg"/></div></div></figure><p id="9465" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一个基于包装的方法。如前所述，包装器方法将一组特性的选择视为一个搜索问题。</p><p id="32e3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">来自<code class="fe pm pn po oo b">sklearn</code>文档:</p><blockquote class="pp pq pr"><p id="8e31" class="lh li md lj b lk ll kt lm ln lo kw lp ps lr ls lt pt lv lw lx pu lz ma mb mc im bi translated">递归特征消除(RFE)的目标是通过递归地考虑越来越小的特征集来选择特征。首先，在初始特征集上训练估计器，并且通过<code class="fe pm pn po oo b"><em class="jj">coef_</em></code>属性或通过<code class="fe pm pn po oo b"><em class="jj">feature_importances_</em></code>属性获得每个特征的重要性。然后，从当前特征集中删除最不重要的特征。该过程在删减集上递归重复，直到最终达到要选择的特征的期望数量。</p></blockquote><p id="db95" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如你已经猜到的，我们可以使用任何估计方法。在这种情况下，我们使用<code class="fe pm pn po oo b">LogisticRegression</code>，RFE 观察<code class="fe pm pn po oo b">LogisticRegression</code>对象的<code class="fe pm pn po oo b"><em class="md">coef_</em></code> <em class="md"> </em>属性<em class="md"> </em></p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="0910" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">4.套索:从模型中选择</h1><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ff52d50ca6a35cfc4e785802cd18674d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*c515Qbsy4xujVcOsLWza8Q.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://chrisalbon.com/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="ad5f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一种嵌入式方法。如前所述，嵌入式方法使用具有内置特征选择方法的算法。</p><p id="d1db" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">比如 Lasso 和 RF 都有自己的特征选择方法。套索正则化强制许多要素权重为零。</p><p id="6522" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们用套索来选择变量。</p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="cc80" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">5.基于树:SelectFromModel</h1><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pv"><img src="../Images/7a7394ff302647d4312730661f2ba984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rRkjb0kikflReKQH"/></div></div></figure><p id="b822" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一种嵌入式方法。如前所述，嵌入式方法使用具有内置特征选择方法的算法。</p><p id="a8b4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们还可以使用 RandomForest 根据特性的重要性来选择特性。</p><p id="7e9a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们使用每个决策树中的节点杂质来计算特征重要性。在随机森林中，最终特征重要性是所有决策树特征重要性的平均值。</p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure><p id="9645" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们也可以使用 LightGBM。或者 XGBoost 对象，只要它有一个<code class="fe pm pn po oo b">feature_importances_</code>属性。</p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="b86c" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">奖金</h1><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pw"><img src="../Images/05c463b1c9007e27a9c9bd0949c3079a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Wn1mNHThCdAKL8q4"/></div></div></figure><p id="67e3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> <em class="md">当我们可以拥有一切的时候，为什么要用一个？</em>T19】</strong></p><p id="7949" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">答案是，在大量数据和时间紧迫的情况下，有时这是不可能的。</p><p id="524b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是只要有可能，为什么不这样做呢？</p><figure class="nu nv nw nx gt iv"><div class="bz fp l di"><div class="oy oz l"/></div></figure><figure class="nu nv nw nx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi px"><img src="../Images/5f37d6948a6047cd92b828fed3cf9169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmTyZgeMNUx1fQHD_f5Hqg.png"/></div></div></figure><p id="08be" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们检查是否得到了基于所有方法的特征。在这种情况下，正如我们所看到的,<code class="fe pm pn po oo b">Reactions</code>和<code class="fe pm pn po oo b">LongPassing</code>是一个高评价玩家的优秀属性。不出所料<code class="fe pm pn po oo b">Ballcontrol</code>和<code class="fe pm pn po oo b">Finishing</code>也占据了榜首。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="ec16" class="ml mm jj bd mn mo mp mq mr ms mt mu mv ky mw kz mx lb my lc mz le na lf nb nc bi translated">结论</h1><p id="6122" class="pw-post-body-paragraph lh li jj lj b lk nd kt lm ln ne kw lp lq nf ls lt lu ng lw lx ly nh ma mb mc im bi translated">特征工程和特征选择是任何机器学习管道的关键部分。</p><p id="2f1e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们力求模型的准确性，如果不一次又一次地重温这些作品，就不可能达到良好的准确性。</p><p id="1837" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本文中，我试图解释一些最常用的特性选择技术，以及我在特性选择方面的工作流程。</p><p id="c09b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我也试图为这些方法提供一些直觉，但是你可能应该尝试更多地了解它，并尝试将这些方法融入到你的工作中。</p><p id="613f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你感兴趣的话，也请阅读我在特征工程上的<a class="ae jg" rel="noopener" target="_blank" href="/the-hitchhikers-guide-to-feature-extraction-b4c157e96631">帖子。</a></p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="3ddf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你想学习更多的数据科学知识，我想调出吴恩达的这个<a class="ae jg" href="https://coursera.pxf.io/NKERRq" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt"> <em class="md">精品课程</em> </strong> </a>。这是我开始的原因。一定要去看看。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="da7b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="lj jt">媒体</strong> </a>关注我，或者订阅我的<a class="ae jg" href="https://mlwhiz.ck.page/a9b8bda70c" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p></div></div>    
</body>
</html>