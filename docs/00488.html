<html>
<head>
<title>Scrape Reddit data using Python and Google BigQuery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 和 Google BigQuery 抓取 Reddit 数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892?source=collection_archive---------8-----------------------#2019-01-22">https://towardsdatascience.com/scrape-reddit-data-using-python-and-google-bigquery-44180b579892?source=collection_archive---------8-----------------------#2019-01-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0734" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">访问 Reddit API 和 Google Bigquery 的用户友好方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/3e1bd8a134413b9748deea434d7ce422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sSWGntJpZwQnIrZ53enngQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><a class="ae kr" href="http://webdata-scraping.com/web-scraping-trending-technique-in-data-science/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="d320" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">Reddit 是最古老的社交媒体平台之一，就其用户和每年产生的内容而言，它仍然保持着强劲的势头。在古老的用户界面背后，是数百万用户每天以问题和评论的形式创造的信息宝库。</p><p id="72ed" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在这篇文章中，我们将一步一步地看到如何使用 python 和 Google Bigquery 从 Reddit 网站获取数据。为了说明这一过程，我决定提取关于插队者的数据，这些人切断了他们的电缆连接并购买了流媒体网站订阅，因为这一现象令我感兴趣。</p><p id="eff0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">作为第一步，让我们了解 Reddit 网站的结构。</p><h2 id="4428" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">Reddit 中的信息是如何组织的？</h2><p id="cff5" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">这个网站被分成不同的<strong class="ku ir">子栏目，</strong>每个用户根据自己的兴趣选择他们想要订阅的子栏目。这些包括音乐子编辑区，在那里可以分享关于音乐的链接，体育子编辑区，人们可以详细谈论体育，或者在我们的例子中，人们可以讨论有线连接或他们新的流媒体订阅。</p><p id="f9c1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">向上投票和向下投票</strong>系统是 Reddit 的精髓，因为它显示了社区成员对特定主题的一致意见。一个帖子获得的支持票越多，它在网站上的显示就越显著。值得注意的是，评论和帖子一样重要，因为它们经常成为扩展的嵌套讨论。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h2 id="facd" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated"><strong class="ak">让我们从 Reddit 的数据收集开始</strong></h2><h2 id="af66" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated"><strong class="ak"> Reddit API: </strong></h2><p id="ff17" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">而网络抓取是著名的(或臭名昭著的！)从网站收集数据的方式，很多网站都提供 API 来访问它们在网站上托管的公共数据。这是为了避免抓取机器人产生的不必要的流量，经常导致网站崩溃，给用户带来不便。甚至 Reddit 也提供这种易于访问的 API。</p><p id="47b7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">以下是你在这个练习中需要的东西:</p><ol class=""><li id="ccb1" class="mt mu iq ku b kv kw ky kz lb mv lf mw lj mx ln my mz na nb bi translated"><strong class="ku ir"> Python 3.x </strong>:你可以在这里下载<a class="ae kr" href="https://www.anaconda.com/download/" rel="noopener ugc nofollow" target="_blank"/></li><li id="3929" class="mt mu iq ku b kv nc ky nd lb ne lf nf lj ng ln my mz na nb bi translated">Jupyter 笔记本电脑:我们将把它作为我们的交互控制台</li><li id="c837" class="mt mu iq ku b kv nc ky nd lb ne lf nf lj ng ln my mz na nb bi translated"><strong class="ku ir"> Reddit 账户</strong>:你必须创建一个<a class="ae kr" href="https://www.reddit.com/" rel="noopener ugc nofollow" target="_blank"> Reddit </a>账户</li></ol><p id="0187" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在获得所需主题的数据之前，您需要遵循几个步骤。</p><ol class=""><li id="ad50" class="mt mu iq ku b kv kw ky kz lb mv lf mw lj mx ln my mz na nb bi translated"><strong class="ku ir">创建一个应用:</strong></li></ol><p id="b128" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">注册后的第一步是创建一个应用程序来获取 Oauth 密钥以访问数据。点击<a class="ae kr" href="https://www.reddit.com/prefs/apps" rel="noopener ugc nofollow" target="_blank">这里</a>开始。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/bbe632bb30c0fadcd71beceaece9fb23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AFj3V5DjBl4nByG8RGoNrw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">creating an app</figcaption></figure><p id="412e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">单击创建一个应用程序，如快照所示。然后出现如下所示的对话框。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nm"><img src="../Images/44d9bc705dbad00051ffb1619a2609c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSpSZ_rxPoR7B0lT49mlKw.png"/></div></div></figure><p id="ebfd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在对话框中输入应用程序的名称，然后单击脚本，因为我们将把它用于个人用途。确保在重定向 URL 框中输入<a class="ae kr" href="http://localhost:8080" rel="noopener ugc nofollow" target="_blank"> http://localhost:8080 </a>。如果您需要任何澄清，您可以参考<a class="ae kr" href="https://praw.readthedocs.io/en/latest/getting_started/authentication.html#script-application" rel="noopener ugc nofollow" target="_blank"> praw 文档</a>。现在点击底部的创建应用程序按钮。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nn"><img src="../Images/0076717909d5500c1ee10ee72868e6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gatePR6hB1_Q-Lo0bjxYRQ.png"/></div></div></figure><p id="3f31" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在您的应用程序已经创建好了。将您的 14 字符个人使用脚本和 27 字符密钥存储在安全的地方。现在，您已经拥有了 OAuth2 身份验证连接到 Reddit API 所需的所有凭证。</p><p id="7700" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在是时候打开 Jupyter 笔记本了！</p><p id="75eb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> 2。建立连接</strong></p><p id="1777" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们在这个练习中需要的包装是 praw 和熊猫。PRAW 是说唱歌手的缩写，我们将使用它向 Reddit API 发出请求。确保您已经安装了这两个软件。第一步是导入这些包</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="07c7" class="lo lp iq np b gy nt nu l nv nw">import praw<br/>import pandas</span></pre><p id="fa22" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">导入包后的下一步是使用我们之前创建的凭证建立与 Reddit API 的连接。Client_id 将是您的 14 个字符的个人使用脚本密钥，client_secret 是您的 27 个字符的秘密密钥。用户名和密码是您的 Reddit 帐户凭据。其余的代码将保持不变。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="6108" class="lo lp iq np b gy nt nu l nv nw">reddit = praw.Reddit(user_agent='Comment Extraction (by /u/USERNAME)',client_id='**********',client_secret="***********",username='********', password='*******')</span></pre><p id="215e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">通过运行上面的代码片段，我们将建立连接并将该信息存储在一个名为 reddit 的变量中。</p><p id="6ce2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> 3。获取数据</strong></p><p id="01b4" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">正如我们之前所讨论的，我们将专注于为“cordcutter”子编辑获取数据。</p><p id="717b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">标题、分数、url、id、评论数量、创建日期、正文</strong>是从 Reddit API 获取数据时可用的字段。但是在我们的分析中，我不会考虑任何时间方面，我们主要关注的是从子编辑中获取正文(评论)。参考<a class="ae kr" href="https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html" rel="noopener ugc nofollow" target="_blank"> praw 文档</a>了解不同种类的实现。这里，我将代码限制为所需的输出，它只是所有注释的正文文本。</p><p id="5e7b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了让所有的评论都包含嵌套回复，我必须想出一个包含 3 个部分的嵌套代码。</p><p id="4e02" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">获取评论 id 列表</strong></p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="c2b1" class="lo lp iq np b gy nt nu l nv nw">comm_list = []<br/>header_list = []<br/>i = 0<br/>for submission in reddit.subreddit('cordcutters').hot(limit=2):<br/>    submission.comments.replace_more(limit=None)<br/>    comment_queue = submission.comments[:]</span></pre><p id="ba26" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在这个循环中，首先我们在循环开始时获取每个提交信息，然后提取所有的评论 id 并将它们存储在 list 中。</p><p id="f0e8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在这里。hot(limit)可以是任何数字，具体取决于您的要求。我在这里将它设置为 2 来说明输出，但是将其设置为 None 将获取 cordcutter subreddit 中所有的顶级提交。replace_more(limit=None)将帮助我们考虑包含嵌套回复的评论。</p><p id="788a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">该循环的输出如下所示:</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="1eff" class="lo lp iq np b gy nt nu l nv nw">[Comment(id='ed5ssfg'),<br/>  Comment(id='ed64a72'),<br/>  Comment(id='edth3nc'),<br/>  Comment(id='ed680cg'),<br/>  Comment(id='ed699q2'),<br/>  Comment(id='ed80ce8'),<br/>  Comment(id='edau9st'),<br/>  Comment(id='edcx477'),<br/>  Comment(id='ee0fp3g'),<br/>  Comment(id='ed5qrvh')]</span></pre><p id="3d1f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">获取所有嵌套回复</strong></p><p id="26f7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们就要得到我们想要的数据了。在这部分代码中，我们将获得之前获得的每个注释 id 的主体。如果评论有嵌套回复，它将进入下一个循环，并以类似的方式提取信息。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="f7a4" class="lo lp iq np b gy nt nu l nv nw">while comment_queue:<br/>    header_list.append(submission.title)<br/>    comment = comment_queue.pop(0)<br/>    comm_list.append(comment.body)<br/>    t = []<br/>    t.extend(comment.replies)<br/>    while t:<br/>        header_list.append(submission.title)<br/>        reply = t.pop(0)<br/>        comm_list.append(reply.body)</span></pre><p id="8e3d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">到目前为止，我们从 Reddit 下载了评论，并且需要一些预处理来将其下载为 csv 格式。</p><p id="80ff" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">下面是经过整理的代码</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="c8b0" class="lo lp iq np b gy nt nu l nv nw">comm_list = []<br/>header_list = []<br/>i = 0<br/>for submission in reddit.subreddit('cordcutters').hot(limit=2):<br/>    submission.comments.replace_more(limit=None)<br/>    comment_queue = submission.comments[:]  # Seed with top-level<br/>    while comment_queue:<br/>        header_list.append(submission.title)<br/>        comment = comment_queue.pop(0)<br/>        comm_list.append(comment.body)<br/>        t = []<br/>        t.extend(comment.replies)<br/>        while t:<br/>            header_list.append(submission.title)<br/>            reply = t.pop(0)<br/>            comm_list.append(reply.body)</span><span id="9a1e" class="lo lp iq np b gy nx nu l nv nw">df = pd.DataFrame(header_list)<br/>df['comm_list'] = comm_list<br/>df.columns = ['header','comments']<br/>df['comments'] = df['comments'].apply(lambda x : x.replace('\n',''))<br/>df.to_csv('cordcutter_comments.csv',index = False)</span></pre><p id="f7a7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">您可以在我的 github 资源库中找到代码的最终版本。</p><div class="ny nz gp gr oa ob"><a href="https://github.com/akhilesh-reddy/Cable-cord-cutter-lift-and-sentiment-analysis-using-Reddit-data" rel="noopener  ugc nofollow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">akhilesh-Reddy/使用 Reddit 数据的电缆切割升降机和情感分析</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">从 Reddit 抓取数据并执行命名实体识别，对评论进行主题建模以了解公众…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">github.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op kl ob"/></div></div></a></div><p id="1a7a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们的最终输出如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi oq"><img src="../Images/f788b3e73f4e5126a56d2d82f7534fe6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TWtaP1nhB6mQZSpydChEjQ.png"/></div></div></figure><p id="a59d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们有自己的数据，但这里有一个挑战。一般来说，使用 Reddit API 获取几个月的历史数据需要更多的时间。感谢<a class="ae kr" href="https://pushshift.io/" rel="noopener ugc nofollow" target="_blank">pushshift . io</a>(Reddit 上的又名<a class="ae kr" href="https://www.reddit.com/user/Stuck_In_the_Matrix" rel="noopener ugc nofollow" target="_blank"> /u/Stuck_In_The_Matrix </a>)的杰森·迈克尔·鲍姆加特纳，我们清理了多年的 Reddit 历史数据，并将其存储在 Bigquery 中，这是本文的第二部分。</p></div><div class="ab cl mm mn hu mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="ij ik il im in"><h2 id="5294" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated"><strong class="ak">big query 中的 Reddit 数据:</strong></h2><p id="4435" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">对于那些不知道 Bigquery 是什么的人来说，</p><blockquote class="or"><p id="a08b" class="os ot iq bd ou ov ow ox oy oz pa ln dk translated">Google BigQuery 是一个<a class="ae kr" href="https://cloud.google.com/solutions/bigquery-data-warehouse" rel="noopener ugc nofollow" target="_blank">企业数据仓库</a>，它通过使用 Google 基础设施的处理能力实现超快速 SQL 查询来解决这个问题。</p></blockquote><p id="14e2" class="pw-post-body-paragraph ks kt iq ku b kv pb jr kx ky pc ju la lb pd ld le lf pe lh li lj pf ll lm ln ij bi translated">最好的部分是查询这些数据将是免费的。Google 免费提供第一个 10GB 的存储和第一个 1 TB 的查询内存，作为免费层的一部分，我们的任务需要不到 1 TB 的内存。</p><p id="c6c5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">让我们看看如何查询这些信息。</strong></p><p id="de99" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">首先点击这个<a class="ae kr" href="https://bigquery.cloud.google.com/" rel="noopener ugc nofollow" target="_blank"> Google BigQuery </a>链接开始。Google 会自动使用您浏览器中存储的 Google 凭据让您登录。如果这是您第一次使用 BigQuery，将会出现一个对话框，要求您创建一个项目。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/f85912ca369b6feab9539b39cf2deef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*1dOfFVCiBZTyDyHT5anxDQ.png"/></div></figure><p id="3ab9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">点击创建一个项目按钮。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi ph"><img src="../Images/2c38d5ef8734da28a80c0a2359684cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzKHRDdn3DZxxfoHK396oA.png"/></div></div></figure><p id="0f4d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为该组织命名，然后单击顶部的 create project。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/4a52f8ee1179dfe11c2a5d2190f8177b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*T_x-nqlg9v1PdE2eCRN1Sg.png"/></div></figure><p id="e72f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">给出项目的名称，您可以暂时保留位置框。然后点击创建。现在您已经创建了您的项目，一个仪表板出现在屏幕上。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi pj"><img src="../Images/7f6a303c6a747743a59b498caf70c423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZVV-ANkq8YgTdMxLwGlyHg.png"/></div></div></figure><p id="a32f" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在在这之后，点击<a class="ae kr" href="https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_05" rel="noopener ugc nofollow" target="_blank"> <em class="pk">链接</em> </a>。这将在您创建的项目下打开 reddit 数据集。在左侧，您将看到在模式名 fh-bigquery 下每个月更新的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nm"><img src="../Images/6edbd984e1dc6713611604fdd4b626e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdRIQohPJxBKtiDEo5Ea7Q.png"/></div></div></figure><p id="62fd" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">让我们运行查询，从表中获取一个月的数据。</p><pre class="kg kh ki kj gt no np nq nr aw ns bi"><span id="80af" class="lo lp iq np b gy nt nu l nv nw">select subreddit,body,created_utc<br/>from `fh-bigquery.reddit_comments.2018_08` <br/>where subreddit = 'cordcutters'</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi pl"><img src="../Images/35c0c1da9be2df9135cb5e915d872e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Plb8xECi6Rp95HiMCjr9g.png"/></div></div></figure><p id="0eef" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这将得到所有关于“割线者”subreddit 的评论。但是请确保您没有选中选项中的“使用遗留 sql”复选框，因为上面的代码片段是在标准 SQL 中。但是，您可以选择您所选择的 sql，并相应地对代码进行更改。</p><p id="749e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这是结果的样子，您可以通过单击“下载为 csv”按钮下载 CSV 格式的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi pm"><img src="../Images/be9064d2bf0a8d6f82d230f4c4120ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSSWTkMfWDy0XLoarNAsBg.png"/></div></div></figure><p id="e996" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在这里，我只是专注于获取我们需要的数据。如果你想在 bigquery 上更多地使用 reddit 数据，你可以参考 Max Woolf 的这篇<a class="ae kr" href="https://minimaxir.com/2015/10/reddit-bigquery/" rel="noopener ugc nofollow" target="_blank">文章</a>，这篇文章更详细地介绍了 Bigquery 中的 Reddit 数据。</p><h2 id="6af2" class="lo lp iq bd lq lr ls dn lt lu lv dp lw lb lx ly lz lf ma mb mc lj md me mf mg bi translated">总结:</h2><p id="50b8" class="pw-post-body-paragraph ks kt iq ku b kv mh jr kx ky mi ju la lb mj ld le lf mk lh li lj ml ll lm ln ij bi translated">在这篇文章中，我们看到了如何创建 OAuth2 凭据以连接到 Reddit，向 Reddit API 发出数据请求以获取最新数据，并通过 Google Bigquery 快速查询历史数据。</p><p id="1d6d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">除了通过 API 和 Bigquery 获取数据，您可能会发现使用 Selenium 和 python 进行 web 抓取很有趣。下面是一篇由 UT Austin 的同学(<a class="ae kr" href="https://towardsdatascience.com/@bandiatindra" rel="noopener" target="_blank"> Atindra Bandi </a>)写的文章。</p><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/web-scraping-using-selenium-python-8a60f4cf40ab"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd ir gy z fp og fr fs oh fu fw ip bi translated">使用 Selenium-Python 进行 Web 抓取</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">在这篇文章中，你将学习如何浏览一个网站的多个页面并收集大量数据…</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="pn l om on oo ok op kl ob"/></div></div></a></div><p id="df48" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">那都是乡亲们！请继续关注我在未来几周发布的关于推荐系统、数据科学统计和数据可视化的一系列文章的更新。</p></div></div>    
</body>
</html>