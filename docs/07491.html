<html>
<head>
<title>Group Sparse Regularization for Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度神经网络的组稀疏正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/group-sparse-regularization-for-deep-neural-networks-6a70ecb1561c?source=collection_archive---------21-----------------------#2019-10-19">https://towardsdatascience.com/group-sparse-regularization-for-deep-neural-networks-6a70ecb1561c?source=collection_archive---------21-----------------------#2019-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="04ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何自动修剪神经网络中的节点？</h2></div><p id="e405" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://arxiv.org/pdf/1607.00485.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络的分组稀疏正则化论文摘要</a></p><h1 id="94f8" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">动机</h1><p id="94d3" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">随着硬件技术的进步和成本的降低，训练大型神经网络变得更加容易。正因为如此，即使是简单的任务也需要大型网络。网络中有太多的层会导致许多问题，</p><ul class=""><li id="b910" class="mc md it kk b kl km ko kp kr me kv mf kz mg ld mh mi mj mk bi translated">过度拟合</li><li id="839d" class="mc md it kk b kl ml ko mm kr mn kv mo kz mp ld mh mi mj mk bi translated">在部署到生产/移动/ IOT 设备之前，需要进行网络修剪</li></ul><p id="fea2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后就是特征选择的问题。特征选择是搜索对建模很重要的所有特征的子集的过程。这通常在建模开始前作为<strong class="kk iu">独立管道</strong>完成。</p><p id="c6a4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">稀疏群正则化(SGR)解决了这三个问题</p><ol class=""><li id="a8a5" class="mc md it kk b kl km ko kp kr me kv mf kz mg ld mq mi mj mk bi translated">优化神经网络的权重</li><li id="d797" class="mc md it kk b kl ml ko mm kr mn kv mo kz mp ld mq mi mj mk bi translated">优化每个隐藏层的神经元数量</li><li id="5c1f" class="mc md it kk b kl ml ko mm kr mn kv mo kz mp ld mq mi mj mk bi translated">特征选择</li></ol><p id="14f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并创建了一个<strong class="kk iu">单一流程/流水线</strong>来解决所有这些问题。在神经网络中，特征选择可以被认为是对输入层节点的修剪，这就是在稀疏群正则化中如何处理它。</p><h1 id="dfae" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">神经网络中的稀疏性</h1><p id="9d51" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">如果一个神经元的所有输入和输出权重都是 0，那么该节点可以被移除。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/99ba01b0047b53258de4ae65c2e46b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*1GdUI_A1M3kAKTHRTlQZxg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Figure 1</figcaption></figure><p id="f536" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面的人工神经网络图像中，红色连接的权重为零。由于来自输入功能<strong class="kk iu"> X2 </strong>的所有连接为零，因此该功能可以移除。类似地，对于节点<strong class="kk iu"> H3，</strong>由于所有输出权重都是零，因此该节点也可以被移除。这是在节点和特征的修剪中使用的稀疏性的思想。</p><h1 id="5af7" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">如何实现稀疏性？</h1><p id="587a" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">L1 正则化(Lasso)是实现权重稀疏性的一种方式。在这种技术中，权重的绝对值之和在训练期间被罚。</p><pre class="ms mt mu mv gt nd ne nf ng aw nh bi"><span id="d38e" class="ni lg it ne b gy nj nk l nl nm">New Cost = Cost + λ ||w||</span></pre><p id="39ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于权重因其绝对大小而受到惩罚，因此可以使用 L1 正则化来实现稀疏性。</p><p id="398d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/" rel="noopener ugc nofollow" target="_blank">点击此处了解更多关于 L1 和 L2 正规化的信息</a>。</p><p id="1c66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，单独使用 L1(套索)不能保证系统的稀疏性，使用它可以删除节点。为了删除一个节点，该节点的所有传出权重都应该为零。在非结构化的情况下，L1 可以导致不同类型的稀疏性。见下图</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/367abfa8027a148cd936c6794fcb578d.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*ZaLE5VA9R9vLHx05xCaMjg.png"/></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Figure 2 : Sparse connection with L1</figcaption></figure><p id="4c0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里 2 个权重为零，但是没有一个节点可以被移除。因此，L1 独自不能帮助修剪。这就是稀疏群正则化发挥作用的地方。</p><h1 id="0a09" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">稀疏群正则化</h1><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi no"><img src="../Images/7efa365abfb9056b3ad883e5574af53e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59bie6SULVcPTc2TEVirJg.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Figure 3. Schematic representation of a group LASSO regularization with two inputs (top), two hidden neurons with biases (middle), and one output (bottom). We have three groups of connections. Green: input groups; blue: hidden groups; red: bias groups. Image source [1]</figcaption></figure><p id="e6af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了获得用于修剪的结构化稀疏度，我们需要对来自每个神经元的传出连接进行分组，并强制它们为零。为此，考虑 3 种不同类型的重量组，</p><ol class=""><li id="2c12" class="mc md it kk b kl km ko kp kr me kv mf kz mg ld mq mi mj mk bi translated">输入组<strong class="kk iu"> Gin </strong>:一个元素 gi ∈ <strong class="kk iu"> Gin，</strong> i = 1，.。。d 是从第 I 个输入神经元到网络的所有输出连接的向量。在上图(图 3)中，有两个输入组(绿色)。x1 的输入组将包含从 x1 到隐藏层的所有权重。输入组用于特征选择。</li><li id="6257" class="mc md it kk b kl ml ko mm kr mn kv mo kz mp ld mq mi mj mk bi translated">隐藏组:<strong class="kk iu"> Gh </strong>:在这种情况下，单个元素 g ∈ <strong class="kk iu"> Gh </strong>对应于网络隐藏层中某个神经元的所有传出连接的向量。在图 3 中，有两个输出组(与隐藏节点的数量相同)。</li><li id="0fa5" class="mc md it kk b kl ml ko mm kr mn kv mo kz mp ld mq mi mj mk bi translated">偏差组:<strong class="kk iu"> Gb </strong>:这些是与网络上的偏差相对应的一维组(标量)。也可以使用包含所有偏差变量的单个组，而不是将每个偏差视为一个单独的组。</li></ol><p id="7485" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总组数<strong class="kk iu"> G </strong>为</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/51af907818d4d94c35d4ec108c1d4fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*cAdZeEQqoUzkufn1LxtwHw.png"/></div></figure><p id="012d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">稀疏组正则化</strong>可以写成，</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/716cb86b1fb585ee9d54e884688d13bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*UjN6Kib1AIC5atcu0CczZQ.png"/></div></figure><p id="9c2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> |g| </strong>表示向量 g 的维数，它确保每个组得到均匀的加权。这里使用每组的 L2 标准。因为 L2 范数不能像 L1 那样产生稀疏性，所以进行阈值化步骤以将低权重转换为零。</p><p id="5784" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个公式对于获得高水平的稀疏性来说仍然是次优的(即使在阈值化之后)。为了加强这一点，使用了套索和稀疏组惩罚的组合。<strong class="kk iu">稀疏群套索</strong>惩罚的定义是，</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nv"><img src="../Images/7f47d13b0d65e13e143681c64f82e632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1SPxT2bFv2SjiTMgYS7MA.png"/></div></div></figure><p id="46be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中第二项是权重的 L1 范数。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nw"><img src="../Images/213ffca91a6761247f031e4717186728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oEsFGRFuliqwDjU49mQMjQ.png"/></div></div><figcaption class="mz na gj gh gi nb nc bd b be z dk">Figure 4 : Comparison between Lasso, group Lasso, and sparse group Lasso applied to a single weight matrix. The removed connections are represented in gray. Image source [1]</figcaption></figure><blockquote class="nx ny nz"><p id="ecb6" class="ki kj oa kk b kl km ju kn ko kp jx kq ob ks kt ku oc kw kx ky od la lb lc ld im bi translated">图 4 显示了本博客中讨论的三种正则化惩罚之间的比较。2×5 矩阵表示 2 维输入层和 5 维输出层之间的<strong class="kk iu">权重矩阵</strong>。</p></blockquote><p id="3514" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">套索惩罚(l1)在不考虑节点级别的情况下移除连接。在图像中，它导致了 40%的连接稀疏度。输出层中的第二个神经元(5 dim)可以被移除，因为来自两个输入节点的所有传入连接都是灰色的(参见图 4 中 Lasso 中的列 2)。</p><p id="e2c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在组 Lasso 中(在稀疏组正则化中的阈值化之后)，来自第二神经元输入层的所有连接为零，因此可以被移除。</p><p id="e7db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">稀疏群套索惩罚结合了前两个公式的优点。来自输出层的两个节点(第 2 和第 5)可以被移除，并且第二输入神经元也可以被移除。连接数减少到 70%,从而形成一个非常紧凑的网络。</p><h1 id="096d" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">参考</h1><p id="374d" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">[1] Simone Scardapane，Amir Hussain 和 Aurelio Uncini，<a class="ae le" href="https://arxiv.org/pdf/1607.00485.pdf" rel="noopener ugc nofollow" target="_blank">深度神经网络的组稀疏正则化</a> s 2017</p></div></div>    
</body>
</html>