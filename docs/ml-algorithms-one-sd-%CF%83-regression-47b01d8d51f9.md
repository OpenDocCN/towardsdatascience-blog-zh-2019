# 最大似然算法:一个标准差(σ)-回归

> 原文：<https://towardsdatascience.com/ml-algorithms-one-sd-%CF%83-regression-47b01d8d51f9?source=collection_archive---------26----------------------->

## 机器学习回归算法简介

![](img/fbb6d8826d7175dad4544a2e710cc7e8.png)

当面对各种各样的机器学习算法时，要问的明显问题是“哪种算法更适合特定的任务，我应该使用哪种算法？”

> 回答这些问题取决于几个因素，包括:(1)数据的大小、质量和性质；(2)可用的计算时间；(3)任务的紧迫性；以及(4)你想用这些数据做什么。

这是我在以前的[文章](/ml-algorithms-one-sd-σ-74bcb28fafb6)中写的许多算法中的一部分。
在这一部分，我试图尽可能简单地展示和简要解释可用于回归任务的主要算法(尽管不是全部)。

# **回归算法:**

> *回归分析是一种预测建模技术，调查因变量(目标)和自变量(预测值)之间的关系。它可用于时间序列建模、预测和寻找变量之间的因果关系。例如，你可以用它来找出司机草率驾驶和交通事故数量之间的关系。*

*回归分析有几个好处:*

它告诉我们因变量和自变量之间的重要关系。

它告诉我们多个自变量对因变量的影响强度。当我说多个独立变量时，我指的是几个 x，例如在“时间延迟和方向对触觉对象识别的影响”中，我们将时间延迟和方向作为 x，将对象识别作为 y。

**普通最小二乘回归(OLSR)** 线性回归中的一种方法，通过创建一个模型来估计未知参数，该模型将最小化观察数据和预测数据(观察值和估计值)之间的误差平方和。
基本上是计算每个 Xi 的系数( **β** )的一种方法:

![](img/b2d6ae2da1446cddcf3f82478b858a6a.png)

找到β系数的方法是最小化误差，因此称为“最小二乘回归”。当相加时，偏差首先被平方，所以正值和负值之间没有抵消。

![](img/9bf45a06a26f23bb200d35886f10adfb.png)

OLSR 有一些局限性:冗余信息/两个解释变量之间的线性关联(即共线性)会导致对系数的误解，因此我们需要比 x 变量更多的观测值。为了克服这些，你可以使用 [PCR](/ml-algorithms-one-sd-σ-74bcb28fafb6) (主成分回归)。

**线性回归** 用于估计实际价值(房屋成本、通话次数、总销售额等)。)基于连续变量。

![](img/b0d79d4e68b7d94fef60932623745ccf.png)

*需要考虑的一些事情:*

自变量和因变量之间必须有线性关系。

多重回归存在多重共线性(多重回归模型中的多重共线性是两个或多个解释变量之间高度线性相关的关联)。

线性回归对异常值非常敏感。

线性回归是一种参数回归。这意味着，它假设因变量和自变量之间的关系的性质是已知的(例如，是线性的)。

您可以使用度量 R-square(R2)-由线性模型解释的响应变量变化的百分比来评估模型性能。

**逻辑回归** 用于根据给定的独立变量集估计离散值(如 0/1、是/否、真/假等二进制值)。

![](img/d911f73a41cef709ca20ebb692e50cb5.png)![](img/a709d912db222677616585101fcabd96.png)

*需要考虑的一些事情:*

它用于分类问题。

它不需要因变量和自变量之间的线性关系。

**逐步回归** 在我们处理多个自变量时使用。它会将特征一个接一个地添加到您的模型中，直到为您的特征集找到最佳分数。逐步选择在向前和向后之间交替，引入和移除满足进入或移除标准的变量，直到获得一组稳定的变量。

*需要考虑的一些事情:*

它使用 R 平方、t-stats 和 AIC 度量等统计值来识别重要变量。

**多元自适应回归样条(MARS**

高维数据的灵活回归建模，搜索有助于最大化预测准确性的交互和非线性关系。

![](img/facbbbc8798270489c12c51cb12f0ef8.png)

这种算法本质上是非线性的(这意味着您不需要通过手动添加模型项(平方项、交互效应)来使您的模型适应数据中的非线性模式)。MARS 是一种非参数回归——它没有对因变量与预测变量之间的关系做出任何假设。相反，它允许回归函数直接由数据“驱动”。MARS 通过一组系数和完全由回归数据确定的所谓基函数(预测值)来构建因变量和自变量之间的关系。

![](img/c77491241c711a09d3a5bbedf6be01ed.png)

*需要考虑的一些事情:*

MARS 在数据挖掘领域非常流行，因为它不假设任何特定类型或类别的关系(例如，线性、逻辑等)。)在预测变量和感兴趣的因变量(结果变量)之间。

如果你面对预测者和目标之间复杂的非线性关系，火星可能是有用的，尤其是在高维空间。

连续预测和分类预测都可以用于 MARS。然而，基本的 MARS 算法假设预测变量本质上是连续的。

因为 MARS 可以处理多个因变量，所以也很容易将该算法应用于分类问题。

火星倾向于过度拟合数据。为了克服这个问题，MARS 使用了一种修剪技术(类似于分类树中的修剪),通过减少其基函数的数量来限制模型的复杂性。基函数的选择和修剪使得该方法成为预测器选择的非常有力的工具。基本上，该算法将只选取那些对预测做出“相当大”贡献的基函数(和那些预测变量)。

在回归树模型也适用的情况下，MARS 特别有用，例如，在预测变量上分层组织的连续分割产生准确的预测。

您应该将 MARS 视为回归树的一般化，其中“硬”二元分割被“平滑”基函数所取代，而不是将其视为多元回归的一般化。

**局部估计散点图平滑(黄土)** 一种用于拟合两个变量之间的平滑曲线，或拟合结果与多达四个预测变量之间的平滑曲面的方法。基本上，它是一种用于回归分析的工具，通过散点图创建一条平滑的线，帮助您看到变量之间的关系并预测趋势。这个想法是，如果你的数据不是线性分布，你仍然可以应用回归的想法。您可以应用回归，这被称为局部加权回归。当自变量和因变量之间的关系是非线性时，可以应用黄土。目前，大多数算法(如经典的前馈神经网络、支持向量机、最近邻算法等。)是全局学习系统，其中它们用于最小化全局损失函数(例如误差平方和)。相比之下，局部学习系统会将全局学习问题分成多个更小/更简单的学习问题。这通常通过将成本函数分成多个独立的局部成本函数来实现。全局方法的缺点之一是有时没有参数值可以提供足够好的近似。但接下来是黄土——全局函数逼近的替代方法。

![](img/f6eafb22c926261ec6edc863cde658ec.png)

*需要考虑的一些事情:*

黄土图通常用于拟合散点图的直线，在散点图中，噪声数据值、稀疏数据点或弱相互关系会干扰您查看最佳拟合直线的能力。

![](img/e3bf7277d5ea70cc14c6cc61faceb2e9.png)

> *那么我该如何选择使用哪一个呢？*

在选择正确的模型之前，数据探索应该是您的第一步(确定变量的关系和影响)。

要比较模型的优劣，您可以使用不同的指标，如参数的统计显著性、R 平方、调整后的 R 平方、AIC、BIC 和误差项。

交叉验证是评估用于预测的模型的最佳方式-将您的数据分为训练和验证。观察值和预测值之间的简单均方差为您提供了预测准确性的度量

还要记住，像套索、岭和弹性网(保持阅读)这样的正则化方法在数据中变量之间的高维度和多重共线性的情况下工作得很好。

> 如果你对我的更多作品感兴趣，你可以看看我的 [Github](https://github.com/shaier) ，我的[学者页面](https://scholar.google.com/citations?user=paO-O00AAAAJ&hl=en&oi=sra)，或者我的[网站](https://shaier.github.io/)