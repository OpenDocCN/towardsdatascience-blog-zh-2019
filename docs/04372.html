<html>
<head>
<title>Bias and Variance in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的偏差和方差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bias-and-variance-in-machine-learning-fbf17ac6f500?source=collection_archive---------22-----------------------#2019-07-07">https://towardsdatascience.com/bias-and-variance-in-machine-learning-fbf17ac6f500?source=collection_archive---------22-----------------------#2019-07-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="135a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">用简单的例子</em></h2></div><p id="1552" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这些概念对数据科学的理论和实践都很重要。他们也会出现在工作面试和学术考试中。</p><p id="1164" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一个<em class="lf">偏置</em>预测器是偏心的，即它的预测始终是关闭的。不管它被训练得多好，它就是不明白。一般来说，这样的预测器对于手头的问题来说太简单了。不管数据有多丰富，它都不符合数据。</p><p id="83b2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一个<em class="lf">高方差</em>预测器在某种意义上是相反的。当试图修正偏置和过度补偿时，通常会出现这种情况。人们已经从一个过于简单(即有偏见)的模型转向一个过于复杂(即方差很大)的模型。它过度拟合了数据。</p><p id="1c2c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">介于这两者之间的是“最佳点”——最佳预测点。通常这不容易找到。数据科学家可以提供帮助。那是另一个故事了…</p><p id="8f0e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">示例</strong></p><p id="d0d9" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">输入是(比如说)华氏温度。我们希望它归类为<em class="lf">舒适</em>或<em class="lf">而非</em>。训练集可以捕捉单个人或一组人的判断。</p><p id="c6ba" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">正如在别处提到的，这个问题不是线性可分的。简单地说，正确的解决方案应该表现为</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="286a" class="lp lq it ll b gy lr ls l lt lu">Too low     -&gt; uncomfortable    (U)<br/>Just right  -&gt; comfortable      (C)<br/>Too high    -&gt; uncomfortable    (U)</span></pre><p id="fe45" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">很明显，我们需要两个阈值，一个用于区分<code class="fe lv lw lx ll b">too low</code>和<code class="fe lv lw lx ll b">just right</code>，一个用于区分<code class="fe lv lw lx ll b">just right</code>和<code class="fe lv lw lx ll b">too high</code>。</p><p id="00f7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">偏置</strong></p><p id="1848" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果我们被限制只能使用一个怎么办？例如当我们的分类器只能够学习<em class="lf">线性</em>决策边界时。无论多么丰富的训练集都无济于事。我们就是无法得到中间预测的<em class="lf">舒服</em>和两个极端预测的<em class="lf">不舒服</em>。</p><p id="2dd6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">迈向更好的解决方案</strong></p><p id="fa45" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果我们有领域知识。也就是说，我们知道问题的特征。这里需要两个门槛。我们可以想出一个好的解决办法。</p><p id="44d7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们假设我们没有这样的领域知识。换句话说，我们需要一个通用的解决方案。</p><p id="7386" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">最近邻法</strong></p><p id="2eb0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们将考虑单个最近邻方法，因为它可以学习非线性决策边界，并且易于描述。我们将针对我们的问题来描述它。这有两个原因:(I)更清晰，以及(ii)便于讨论其变化。</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="c78f" class="lp lq it ll b gy lr ls l lt lu">Predict the temperature to be comfortable if at least half the people labeled it comfortable; uncomfortable if not.</span></pre><p id="93f2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在假设算法在以下数据集上训练。每个温度都由 10 个人标注——C 或 U。</p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="1729" class="lp lq it ll b gy lr ls l lt lu">Temperature            10 … 45 50 55 60 … 80 … 100 ..<br/>% who Labeled C         0 … 50 30 40 70 … 70 … 0</span></pre><p id="ba4a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这个算法会把 45 标为 C，50 和 55 标为 U，60 标为 C，这好像不对。50 和 55 被认为不舒服。我们预计 45 也应该是。</p><p id="15fa" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这是怎么回事？我们只有 10 台贴标机。“他们有一半把 45 标成 C”是噪音，这是说得通的。随着更多的标记，这个分数可能会下降到 0.5 以下。</p><p id="a162" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">另一种看待这个问题的方式是，模型的预测具有很高的方差。这样想。想象一组不同的 10 个人来标记数据。在这套新设备上训练。45 岁时的预测不太可能是错误的。这太巧合了。也就是说，很可能在某个温度上预测是错误的。</p><p id="a826" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">因此，正如我们在示例中看到的，高方差预测器容易受到训练集中的噪声的影响。噪音不会重复出现。下次我们训练它时，它会在不同的输入上学习噪音。从一个用户的角度来看，他只是把它作为一个黑盒预测器，这些预测在训练运行中并不一致。</p><p id="b514" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><strong class="kl iu">减少差异</strong></p><p id="5b9f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">那么我们如何从算法上解决这个问题？也就是说，不需要额外的标签(这会产生成本)。我们平滑数据，或者在预处理期间，或者作为算法的一部分。下面我们选择后者。我们用三个代替单个最近邻<em class="lf">。为了简单起见，忽略计算效率的考虑，我们以下面的形式描述它:</em></p><pre class="lg lh li lj gt lk ll lm ln aw lo bi"><span id="8e20" class="lp lq it ll b gy lr ls l lt lu">For the input temperature T, get all the labelings of T, T-1, and T+1. Predict T’s label to be the majority label among all these.</span></pre><p id="5f7a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们有一个更强大的分类器。每个预测将基于大小为 30 的标记样本——比我们从单个最近邻分类器获得的样本多三倍。与单阈值分类器相比，这个分类器的偏差更小。它本质上仍然能够学习非线性决策边界。</p><p id="026d" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">当然，与前两个分类器相比，这个新分类器的准确程度是一个经验问题。这涉及到对每个分类器的偏差和方差的综合影响的精确比较。简而言之，它需要一个合适的列车测试实证评估。那是另一个故事。</p></div></div>    
</body>
</html>