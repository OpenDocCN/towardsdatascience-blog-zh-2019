<html>
<head>
<title>All I Want for Christmas Is AI: Write the Next Christmas Hit Using LSTMs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我想要的圣诞礼物就是人工智能:用 LSTMs 写出下一部圣诞大片</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-i-want-for-christmas-is-ai-write-the-next-christmas-hit-using-lstms-2e95ea3da25e?source=collection_archive---------35-----------------------#2019-12-30">https://towardsdatascience.com/all-i-want-for-christmas-is-ai-write-the-next-christmas-hit-using-lstms-2e95ea3da25e?source=collection_archive---------35-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/f4de47fae6f711a10bfd1b62fc6d6e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYcZ8mTSOGM3ZVyBLI2oTg.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="f7a5" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">如何用一些奇怪的人工智能生成的圣诞歌词来给你的假期增添趣味</h2></div><p id="ba65" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">厌倦了失去<a class="ae lp" href="https://www.whamageddon.com/" rel="noopener ugc nofollow" target="_blank">威猛的一击</a>？你是否暗自希望克里斯蒂娜·阿奎莱拉从未出生？你害怕麦可·布雷每年的回归胜过一切吗？<br/>嗯，事实证明在不久的将来我们可能会有<em class="lq">几十亿多</em>。</p><p id="450d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">可怕吧。</p><p id="cc51" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">用 AI </strong>生成文本是数据科学家能够完成的最<strong class="kv jf">完整和令人满意的任务</strong>之一:我们处理<strong class="kv jf">网页抓取</strong>、<strong class="kv jf">非结构化数据</strong>、<strong class="kv jf">预处理</strong>、<strong class="kv jf">数据探索</strong>、<strong class="kv jf">模型设计</strong>、<strong class="kv jf">超参数调优</strong>、数小时的<strong class="kv jf">培训</strong>以及——最重要的是——我们能够获得最终的、可触知的结果</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lr"><img src="../Images/276e9cae72b1ff4efdd193763486ff8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4Ldhq0yXK7KjM5ga.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">The most common words in the lyrics of the 20 most popular Christmas songs (from <a class="ae lp" href="https://www.vividseats.com/blog/most-common-words-in-popular-christmas-songs" rel="noopener ugc nofollow" target="_blank">Vivid Seats</a>)</figcaption></figure><p id="3fa9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">鉴于一年中的时间和圣诞颂歌主题的可预测性(这里是圣诞老人，那里是耶稣，红色，白色，四周是雪，圣诞树，快乐，幸福，我们完成了)，这里我们将尝试生成圣诞主题的歌词。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="0530" class="mh mi je bd mj mk ml mm mn mo mp mq mr kk ms kl mt kn mu ko mv kq mw kr mx my bi translated">语言模型</h1><p id="0a79" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">自动生成歌词是一项琐碎的任务。生成文本序列的一般方法是训练一个模型，在给定所有先前单词/字符的情况下预测下一个单词/字符。为文本生成脚本提供动力的引擎被称为统计语言模型，或者简称为语言模型。</p><h2 id="b8b4" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">统计语言模型</h2><p id="2e90" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated"><strong class="kv jf">语言模型是一种概率模型，它可以预测序列中的下一个单词，给定之前单词本身的序列</strong>、<strong class="kv jf">、</strong>试图捕捉它所训练的文本的统计结构(即潜在空间)。从技术上来说，它只是一系列单词 <em class="lq"> P </em> ( <em class="lq"> w1，w2，…，wₘ </em>)的<strong class="kv jf">概率分布，从中我们迭代得出最有可能的下一个单词评估<em class="lq"> P </em> ( <em class="lq"> wₙₑₓₜ | w1，w2，…，wₘ </em>)。这也是当谷歌自动完成我们奇怪的查询(提供更奇怪的建议)和我们无聊的圣诞祝福邮件时幕后发生的一部分。</strong></p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/b6892540f79541271fba966d87fba110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mN53phkOIrz6Duqy9ZPgOQ.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">Apparently, Google Search doesn’t have a good opinion on Christmas songs either</figcaption></figure><h2 id="01d9" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">基于字符的神经语言模型</h2><p id="963a" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">语言模型也可以在<strong class="kv jf">字符级别</strong>开发。基于字符的语言模型的主要好处是它们的<strong class="kv jf">小词汇量</strong>和处理任何单词、标点符号和特定文本结构的<strong class="kv jf">灵活性</strong>。这是以拥有更大的模型和<strong class="kv jf">更长的训练</strong> <strong class="kv jf">倍</strong>为代价的。<br/>如今用于构建语言模型的最常见的 ML 技术家族是<strong class="kv jf">递归神经网络</strong> (RNNs)，这是一种功能强大的神经网络，能够<strong class="kv jf">记住</strong>并通过其隐藏状态神经元处理过去的信息。</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/94b65ed33794fbe06732ff8b1918d29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*gANUvBfEoS3WoPey5tIGMQ.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">A simple example of RNN (with a single 3-units hidden layer) forward pass using the training sequence “hello”. For each (one-hot encoded) character in the sequence, the RNN predicts the next character assigning a confidence score to every character in the vocabulary ([“h”, “e”, “l”, “o”]). The objective of the network is to learn the set of weights that maximizes the green numbers in the output layer and minimize the red ones.</figcaption></figure><p id="afdb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">当足够的数据可用时，具有<a class="ae lp" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">长短期记忆</strong> </a> <strong class="kv jf"> </strong> (LSTM)味道的 rnn 更受欢迎，因为它们可以捕捉更复杂的文本依赖，并更好地处理<a class="ae lp" href="https://medium.com/learn-love-ai/the-curious-case-of-the-vanishing-exploding-gradient-bf58ec6822eb" rel="noopener">爆炸/消失梯度问题</a>。</p><h1 id="4631" class="mh mi je bd mj mk ns mm mn mo nt mq mr kk nu kl mt kn nv ko mv kq nw kr mx my bi translated">圣诞歌词生成器</h1><p id="6977" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">为了生成我们的圣诞歌词，我们需要一个合适的数据源。对我们来说幸运的是，互联网上有大量的歌词网站，可以很容易地搜索到。</p><h2 id="65b0" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">数据准备</h2><p id="91e4" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">一旦我们获得了数据源，我们就需要构建<strong class="kv jf">语料库</strong>，导入原始文本并应用一些普通的<strong class="kv jf">文本预处理</strong>，如(不需要的)标点删除和小写。<br/>由于我们正在研究基于字符的语言模型，<strong class="kv jf">文本必须映射到字符级</strong>。因此，必须建立一个独特的<strong class="kv jf">字符词汇表</strong>。<br/>我们神经网络的输入将是<strong class="kv jf">字符序列</strong>。因此，我们将语料库分成<em class="lq"> maxlen </em>大小的序列，对每个<em class="lq">步骤</em>字符进行采样。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h2 id="548a" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">模型设计和培训</h2><p id="c71f" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">在本文中，我们将尝试构建最简单的基于字符的神经语言模型:一个<strong class="kv jf"> 128 大小的单层 LSTM，带有 softmax 激活</strong>。<br/>记住<strong class="kv jf">输入</strong>序列和输出必须是<strong class="kv jf">一键编码</strong>。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="9ac4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">该网络被训练 1000 个时期，尽管在 500-600 个时期后损失似乎停止显著下降。</p><p id="10ec" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">(根据您的硬件，这可能需要几个小时到几天的时间。为了在合理的时间框架内训练您的模型，您可以随意减少历元的数量)</p><figure class="ls lt lu lv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/89ee829d6c7021e76326a3b587918793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GY6VnvxaEVAsYIRd0m93_Q.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">The loss stops decreasing significantly after 500 epochs approximately</figcaption></figure><h2 id="e4da" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">文本生成</h2><p id="5fe3" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">一旦模型训练完毕，我们就可以开始预测了。给定一个字符序列，该模型使用它的权重来输出一个字符分布，从这里我们可以对下一个字符进行采样，只要我们愿意，就可以重复这个过程。<strong class="kv jf">我们从输出分布中抽取下一个字符的方式至关重要</strong>。</p><p id="dcd7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">如果我们总是选择最有可能的单词，语言模型训练目标会导致我们陷入循环</strong>中，如<em class="lq">圣诞快乐。圣诞快乐。圣诞快乐</em>”。公平地说，这可以被认为是一首合法的圣诞颂歌，但可能不是我们想要在这里实现的。</p><p id="a2ba" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv jf">即使我们从分布中取样，我们仍然需要小心，因为最不可能的记号可能代表大部分的概率质量</strong>。例如，让我们考虑一下，字符记号的底部 50%具有 25%的总概率。这意味着我们有 1/4 的机会去越野，导致不可阻挡的<strong class="kv jf">误差传播</strong>贯穿整个生成过程。</p><p id="a8df" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">受统计热力学启发，<strong class="kv jf">温度</strong> <strong class="kv jf">采样</strong>是文本生成最常用的采样方法之一。这里的高温意味着更可能遇到低能态。因此，温度越低，模型对其首选越有信心，而温度高于 1 会降低信心。</p><figure class="ls lt lu lv gt iv"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h2 id="be29" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">结果</h2><p id="162c" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">让我们看看使用 0.2 的温度和种子会得到什么:</p><blockquote class="oa ob oc"><p id="bfcb" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">“感谢上帝今天是圣诞节<br/>一天。感谢上帝，今天是圣诞节</p></blockquote><p id="a200" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在<strong class="kv jf">第一个纪元</strong>之后，我们实现了这样的东西:</p><blockquote class="oa ob oc"><p id="d719" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">“感谢上帝，今天是圣诞节。感谢上帝，今天是圣诞节</p><p id="8d7e" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">e</p><p id="a6d7" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">一个男孩爱上了这个女孩，但是他却爱上了这个女孩</p><p id="9a4d" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">他把所有的人都带到了 tt</p><p id="2b89" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">以色列人在那里居住，所以他们在 etr 居住</p><p id="036a" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">awrmsesur ru uete errea […]" "</p></blockquote><p id="711d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">显然，该模型没有时间学习任何关于它所训练的语料库的语言。以下是《T21》10 个时代之后的歌词:</p><blockquote class="oa ob oc"><p id="2269" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">“感谢上帝，今天是圣诞节。感谢上帝，今天是圣诞节，我不想要你这个小槲寄生和星星中的星星</p><p id="52b7" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">孩子们和星星们。我想要星星中的星星，星星和圣诞老人我想看圣诞树。[…]"</p></blockquote><p id="b1f2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">看起来 10 个时代足以理解如何将字符粘在一起以获得单词和句子，但仍然不足以学习语言本身的更高结构。请注意，生成的模型让<strong class="kv jf">陷入了“星星”这个词的循环</strong>，通常<strong class="kv jf">无法生成有意义的东西</strong>。<br/>经过<strong class="kv jf"> 100 个时期</strong>，我们得到:</p><blockquote class="oa ob oc"><p id="509f" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">“感谢上帝，今天是圣诞节<br/>一天。感谢上帝，今天是圣诞节，新年快乐，我会喜欢这个世界</p><p id="c545" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">在槲寄生上的每一个角落，我都希望我能知道，我希望你能感受到你的血液，但我不能在圣诞节把我带回家</p><p id="7610" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">当圣诞夜来临的时候，我想唱圣诞节早晨天空的歌，因为雪正在飘落……”</p></blockquote><p id="3cda" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这已经是一个不识字的克里斯蒂娜·阿奎莱拉人能唱的了。圣诞颂歌的结构和反复出现的主题开始出现，同时还有令人毛骨悚然的废话，比如“<em class="lq">我希望你的窗户变成你的血</em>”这听起来更像黑色金属而不是圣诞节。<br/>快进到<strong class="kv jf"> 1000 个纪元</strong>:</p><blockquote class="oa ob oc"><p id="5c62" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">“感谢上帝，今天是圣诞节。感谢上帝，今天是圣诞节，你会成为你想要的人，祝你快乐。</p><p id="10b3" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">真是个又亮又丑的<br/>和妈妈早知道了<br/>我不要你的道听途说<br/>我已经容不下你一个小屁孩了，命运</p><p id="0ba4" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">我希望你身边有个男人。<br/>她为雪搬运一件东西。圣诞节到了，圣诞节到了，圣诞节没有给我一首圣诞歌，圣诞钟声响起。[…]"</p></blockquote><p id="ab87" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">现在我们肯定有收获了！废话仍然存在，但歌词的整体质量是体面的，整个事情可能听起来有意义。<br/>最后，我们可能会考虑用我们模型的超参数稍微调整一下<strong class="kv jf">。例如，我们可以尝试对更长和/或更远的输入序列进行采样，在数据预处理阶段增加<em class="lq"> maxlen </em>和<em class="lq"> step </em>参数。直观地说，<strong class="kv jf">增加输入序列的最大长度意味着给模型提供更多的上下文来学习</strong>，所以我们期待更好的结果。<br/>这是当<em class="lq"> maxlen </em>从 70 增加到 120 时我们可以得到的结果:</strong></p><blockquote class="oa ob oc"><p id="1263" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">“[……]打开我们天堂般的家<br/>让通往天堂的路变得安全<br/>关闭通往痛苦的道路。</p><p id="b50d" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">哦，来吧，摇一摇<br/>一首圣诞歌的花<br/>我整晚都有点醉<br/>在一个正确的酒吧局，哈利路亚？</p><p id="a2ed" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">我相信有圣诞老人，我相信我会在街上看到他把所有的玩具都搬出来</p><p id="39ad" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">格洛丽亚。透过天空的雪花看故事圣诞树，哦，圣诞节。<br/>上帝之子出生之子而降<br/>上帝之子出生之子而降</p><p id="84c0" class="kt ku lq kv b kw kx kf ky kz la ki lb od ld le lf oe lh li lj of ll lm ln lo im bi translated">我要送星星和雪<br/>，我想要一只河马。[…]"</p></blockquote><p id="aa10" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">不确定它是否能被认为在质量上比前一个更好，但明年圣诞节我也会要一只河马。</p><h1 id="c601" class="mh mi je bd mj mk ns mm mn mo nt mq mr kk nu kl mt kn nv ko mv kq nw kr mx my bi translated">结论</h1><h2 id="d04c" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">未来的发展</h2><p id="f37b" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">单层 LSTMs 只是神经语言建模的起点，因为<a class="ae lp" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">可以设计出更多的<strong class="kv jf">复杂网络</strong> </a>来更好地解决文本生成的问题。<br/>我们已经看到了<strong class="kv jf">超参数如何在开发一个像样的文本生成器</strong>中发挥重要作用。因此，我们可能会考虑在我们的网络中<strong class="kv jf">堆叠更多(更大)的层</strong>，以调整批量大小，并对<strong class="kv jf">不同的序列长度和温度</strong>进行更多实验。</p><h2 id="72be" class="ne mi je bd mj nf ng dn mn nh ni dp mr lc nj nk mt lg nl nm mv lk nn no mx np bi translated">进一步阅读</h2><p id="50d9" class="pw-post-body-paragraph kt ku je kv b kw mz kf ky kz na ki lb lc nb le lf lg nc li lj lk nd lm ln lo im bi translated">为了让每个人都能阅读，本文有意避免深入研究神经网络和 NLP 的数学和概率基础。这里有一个资源列表，您可以在其中探索我们留下的一些主题:</p><ul class=""><li id="758b" class="og oh je kv b kw kx kz la lc oi lg oj lk ok lo ol om on oo bi translated">关于自然语言处理技术的现状:<a class="ae lp" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"> BERT 解释说:自然语言处理技术语言模型的现状</a></li><li id="86ef" class="og oh je kv b kw op kz oq lc or lg os lk ot lo ol om on oo bi translated">字符感知神经语言模型:<a class="ae lp" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017" rel="noopener ugc nofollow" target="_blank"> Kim 等，2016 </a></li><li id="82ff" class="og oh je kv b kw op kz oq lc or lg os lk ot lo ol om on oo bi translated">非常好的 NLM 基础教程:<a class="ae lp" rel="noopener" target="_blank" href="/character-level-language-model-1439f5dd87fe">字符级语言模型</a></li><li id="65cb" class="og oh je kv b kw op kz oq lc or lg os lk ot lo ol om on oo bi translated">RNNs 和语言模型与有趣的可视化和例子:<a class="ae lp" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理的有效性</a></li><li id="f103" class="og oh je kv b kw op kz oq lc or lg os lk ot lo ol om on oo bi translated">语言建模概述:<a class="ae lp" href="https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/" rel="noopener ugc nofollow" target="_blank">统计语言建模和神经语言模型简介</a></li><li id="54f5" class="og oh je kv b kw op kz oq lc or lg os lk ot lo ol om on oo bi translated">我的灵感来源，在这里你可以找到歌词生成过程的更详细的描述:<a class="ae lp" rel="noopener" target="_blank" href="/generating-drake-rap-lyrics-using-language-models-and-lstms-8725d71b1b12">使用语言模型和 LSTMs 生成德雷克说唱歌词</a></li></ul><p id="3500" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请在评论区留下你的想法和建议，如果你觉得这有帮助，请分享！</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><div class="ls lt lu lv gt ou"><a href="https://www.linkedin.com/in/tbuonocore/" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jf gy z fp oz fr fs pa fu fw jd bi translated">Tommaso Buonocore -软件工程师- AMC 医学研究 BV | LinkedIn</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">查看世界上最大的职业社区 LinkedIn 上 Tommaso Buonocore 的个人资料。托马索列出了 5 项工作…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">www.linkedin.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ja ou"/></div></div></a></div></div></div>    
</body>
</html>