<html>
<head>
<title>Connecting the Dots (Python, Spark, and Kafka)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将点连接起来(Python、Spark 和 Kafka)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/connecting-the-dots-python-spark-and-kafka-19e6beba6404?source=collection_archive---------7-----------------------#2019-07-08">https://towardsdatascience.com/connecting-the-dots-python-spark-and-kafka-19e6beba6404?source=collection_archive---------7-----------------------#2019-07-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="80fa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python、Spark 和 Kafka 是数据科学家日常活动中的重要框架。让他们能够整合这些框架是至关重要的。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/524f9cdf55c90cd4c06f2b6d5b0add95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fhgsMTply_TkrwrWeDIxA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo By <a class="ae kv" href="https://www.pexels.com/@cesar-gaviria-232160" rel="noopener ugc nofollow" target="_blank">César Gaviria</a> from <a class="ae kv" href="https://www.pexels.com/@cesar-gaviria-232160" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h2 id="a43a" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">介绍</h2><p id="2d31" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">通常，数据科学家更喜欢使用 Python(在某些情况下是 R)来开发机器学习模型。在这里，他们有一个有效的理由，因为数据驱动的解决方案伴随着许多实验而来。进行实验需要与我们用来开发模型的语言进行大量的交互，python 中可用于开发机器学习模型的库和平台非常多。这是一个有效的论点；然而，当这些模型应用于生产时，我们会遇到问题。</p><p id="975f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">我们仍然有 Python 微服务库如 Flask 来部署机器学习模型并将其作为 API 发布。然而，问题是，“这能满足实时分析的需要吗？在实时分析中，你需要在一毫秒的时间内处理数百万个事件？”答案是否定的。这种情况是我写这篇文章的动机。</p><p id="1020" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">为了克服上述所有问题，我已经确定了一组可以适当连接的点。在本文中，我试图将这些点联系起来，它们是 Python、Apache Spark 和 Apache Kafka。</p><p id="ed46" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这篇文章是按以下顺序组织的:</p><ul class=""><li id="f453" class="mq mr iq lu b lv ml ly mm lf ms lj mt ln mu mk mv mw mx my bi translated">讨论在 Linux 环境中设置 Apache Spark 的步骤。</li><li id="9486" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">开始卡夫卡(更多详情请参考这篇<a class="ae kv" href="https://www.linkedin.com/pulse/quickstart-apache-kafka-kafka-python-kiruparan-balachandran/" rel="noopener ugc nofollow" target="_blank">文章</a>)。</li><li id="3022" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">创建一个 PySpark 应用程序，用于消费和处理事件并写回 Kafka。</li><li id="3eaf" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">使用 Kafka-Python 生成和消费事件的步骤。</li></ul><h1 id="fd8f" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">安装 Spark</h1><p id="5bbd" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">最新版本的 Apache Spark 可以在 http://spark.apache.org/downloads.html 的<a class="ae kv" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">买到</a></p><p id="c0e0" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在我撰写本文时，Spark-2.3.2 是最新版本。</p><p id="8198" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">步骤 1 </strong>:使用以下命令将 spark-2.3.2 下载到本地机器</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="9089" class="kw kx iq nq b gy nu nv l nw nx">wget http://www-us.apache.org/dist/spark/spark-2.3.2/spark-2.3.2-bin-hadoop2.7.tgz</span></pre><p id="ac6f" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第二步</strong>:拆包。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="b365" class="kw kx iq nq b gy nu nv l nw nx">tar -xvf spark-2.1.1-bin-hadoop2.7.tgz</span></pre><p id="db04" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第三步</strong>:创建软链接(可选)。</p><p id="9d80" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这一步是可选的，但却是优选的；方便以后升级 spark 版本。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="17ee" class="kw kx iq nq b gy nu nv l nw nx">ln -s /home/xxx/spark-2.3.2-bin-hadoop2.7/ /home/xxx/spark</span></pre><p id="5d5a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">步骤 4 </strong>:向 bashrc 添加 SPARK_HOME 条目</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="443f" class="kw kx iq nq b gy nu nv l nw nx"><em class="ny">#set spark related environment varibales</em><br/>SPARK_HOME="/home/xxx/spark"<br/>export PATH=$SPARK_HOME/bin:$PATH<br/>export PATH=$SPARK_HOME/sbin:$PATH</span></pre><p id="d110" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">步骤 5 </strong>:验证安装</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="c996" class="kw kx iq nq b gy nu nv l nw nx">pyspark</span></pre><p id="fb6a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果一切都准确无误，控制台上会显示以下输出:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="52ce" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">步骤 6 </strong>:启动本机主机</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="0143" class="kw kx iq nq b gy nu nv l nw nx">start-master.sh</span></pre><p id="8ee7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">Spark Master Web GUI(流动屏幕)可从以下网址访问:<a class="ae kv" href="http://iqx.iqmedialabs.com:8080/" rel="noopener ugc nofollow" target="_blank">http://abc.def.com:8080/</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/6c41819436731edcebb4a79789d6d29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CE9Ymfzu96YBWsORw_pf9g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Spark Master Web GUI — Image by Author</figcaption></figure><p id="4bfa" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第七步</strong>:启动工人</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="1279" class="kw kx iq nq b gy nu nv l nw nx">start-slave.sh spark:<em class="ny">//abc.def.ghi.jkl:7077</em></span></pre><p id="944e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果一切都准确，工人的条目将出现在同一个屏幕上。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/2784db8b78677be863addf30b0b0686a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FV5pWFwjRrvxFr608siw9w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Spark Master Web GUI with workers — Image by Author</figcaption></figure><h1 id="ea55" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">开始卡夫卡</h1><p id="eda0" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">在这里，Kafka 是一个流媒体平台，有助于生产和消费 spark 平台的事件。</p><p id="8860" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">更详细的说明请参考我已经写过的关于<a class="ae kv" href="https://medium.com/@kiruparan/quickstart-apache-kafka-kafka-python-e8356bec94" rel="noopener">卡夫卡</a>的文章。</p><p id="6cfa" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第一步</strong>:转到卡夫卡根文件夹</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="ac67" class="kw kx iq nq b gy nu nv l nw nx">cd /home/xxx/IQ_STREAM_PROCESSOR/kafka_2.12-2.0.0/</span></pre><p id="fa42" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第二步</strong>:启动卡夫卡动物园管理员</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="8cc7" class="kw kx iq nq b gy nu nv l nw nx">bin/zookeeper-server-start.sh config/zookeeper.properties</span></pre><p id="5256" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第三步</strong>:启动卡夫卡经纪人</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="d834" class="kw kx iq nq b gy nu nv l nw nx">bin/kafka-server-start.sh config/server.properties</span></pre><p id="d7ac" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第四步</strong>:创建两个 Kafka 主题(<em class="ny"> input_event 和</em> output_event)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h1 id="810e" class="ne kx iq bd ky nf ng nh lb ni nj nk le jw nl jx li jz nm ka lm kc nn kd lq no bi translated">Apache Spark (PySpark)上的事件处理</h1><h2 id="0875" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">设置火花</h2><p id="b7f3" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><strong class="lu ir">第一步</strong></p><p id="ed3e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">首先在集群的每个节点中设置 python 包，并指定每个 worker 节点的路径。这里最好安装 Anaconda，它包含了大部分必要的 python 包。</p><p id="c7f7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在<em class="ny"> spark-env.sh </em>中添加以下条目，以指定每个工作节点的路径。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="051d" class="kw kx iq nq b gy nu nv l nw nx">export PYSPARK_PYTHON='/home/xxx/anaconda3/bin/python'</span></pre><p id="34a4" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第二步</strong></p><p id="33f3" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">需要安装 spark 应用程序中使用的其他 python 依赖项。比如我们用 Kafka-python 把处理过的事件写回 Kafka。</p><p id="e456" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这是安装 Kafka python 的过程:</p><p id="86cd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在控制台中，转到 anaconda bin 目录</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="ea00" class="kw kx iq nq b gy nu nv l nw nx">cd /home/xxx/anaconda3/bin/</span></pre><p id="517a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">执行以下命令</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="ab04" class="kw kx iq nq b gy nu nv l nw nx">pip install kafka-python</span></pre><p id="0a8d" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">第三步</strong></p><p id="8386" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">从以下网址下载 Spark Streaming 的 Kafka 库:<a class="ae kv" href="https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-8-assembly" rel="noopener ugc nofollow" target="_blank">https://mvnrepository . com/artifact/org . Apache . Spark/Spark-Streaming-Kafka-0-8-assembly</a>【稍后提交 Spark 作业需要用到这个】。</p><p id="d694" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在我们已经安排好了运行 Spark 应用程序所需的整个氛围。</p><h2 id="cb64" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">创建并提交公园申请</h2><p id="c866" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated"><strong class="lu ir">创建 SparkContext </strong></p><p id="4c6e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">spark 上下文是访问 Spark 功能的入口点，并提供到 Spark 集群的连接。要创建 SparkContext，首先，我们应该创建 SparkConf，它包含传递给 SparkContext 所需的参数。下面的代码片段展示了如何创建 SparkContext。</p><p id="f045" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">这里，仅安排了主 URL 和应用程序名称，但不限于此。SparkConf 允许你控制更多的参数。例如，您可以指定驱动程序进程使用的内核数量、每个执行器进程使用的内存量等。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="446a" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">创建【流上下文 Kafka 经纪人输入流】</strong></p><p id="45ec" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">流上下文是访问 spark 流功能的入口点。流式上下文的关键功能是从不同的流式源创建离散流。以下代码片段显示了如何创建 StreamingContext。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="c745" class="kw kx iq nq b gy nu nv l nw nx"><em class="ny">#batch duration, here i process for each second</em><br/>ssc = StreamingContext(sc,1)</span></pre><p id="96db" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">接下来，我们为来自 Kafka 代理的 pulls 消息创建输入流。应该指定创建输入流的以下参数:</p><ul class=""><li id="b865" class="mq mr iq lu b lv ml ly mm lf ms lj mt ln mu mk mv mw mx my bi translated">从该流连接的 Zookeeper 的主机名和端口。</li><li id="f3e3" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">此消费者的组 id。</li><li id="2fc6" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">“每个主题要使用的 Kafka 分区数”:指定该流并行读取的分区数。</li></ul><p id="5a35" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">下面的代码片段表达了如何为 Kafka 代理创建输入流。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="d006" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">处理事件并写回卡夫卡</strong></p><p id="8a8e" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">在为 Kafka Brokers 创建流之后，我们从流中提取每个事件并处理这些事件。在这里，我演示了一个在大多数 spark 教程中引用的典型示例(字数统计),并进行了一些小的修改，以在整个处理期间保持键值并写回 Kafka。</p><p id="26f2" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">以下代码片段描述了接收入站流并使用已处理的事件创建另一个流:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="d808" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在，剩下的就是给卡夫卡回信了。我们获取处理后的流，并通过对 stream 应用输出操作(这里我们使用 foreachRDD)写回外部系统。这将每个 RDD 中的数据推送到外部系统(在我们的用例中，推送到 Kafka)。下面的代码 snipe 解释了如何将每个 RDD 中的数据写回卡夫卡:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="7715" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><strong class="lu ir">启动星火应用</strong></p><p id="711c" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">脚本 spark-submit 用于启动 spark 应用程序。在启动应用程序期间，应指定以下参数:</p><ul class=""><li id="f172" class="mq mr iq lu b lv ml ly mm lf ms lj mt ln mu mk mv mw mx my bi translated">master:连接 master 的 URL 在我们的例子中，它是 spark://abc.def.ghi.jkl:7077</li><li id="0f37" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">deploy-mode:部署驱动程序的选项(在 worker 节点或本地作为外部客户机)</li><li id="9820" class="mq mr iq lu b lv mz ly na lf nb lj nc ln nd mk mv mw mx my bi translated">jars:回忆一下我们关于 Spark Streaming 的卡夫卡图书馆的讨论；这里我们需要提交这个 jar 来提供 Kafka 依赖项。</li></ul><p id="fdfd" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">最后，我们必须提交我们在本节中编写的 PySpark 脚本，即 spark_processor.py</p><p id="be67" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">启动所有命令后，我们的 spark 应用程序将如下所示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="df73" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果一切正常，控制台中将出现以下输出:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="b818" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">现在我们有了必要的设置，是时候进行测试了。在这里，我使用 Kafka-python 来创建事件，并使用在我以前的<a class="ae kv" href="https://www.linkedin.com/pulse/quickstart-apache-kafka-kafka-python-kiruparan-balachandran/" rel="noopener ugc nofollow" target="_blank">文章</a>中已经讨论过的事件</p><p id="d7a1" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">以下是代码片段供您参考:</p><p id="f1d7" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><em class="ny">产生事件的代码</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="c144" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated"><em class="ny">消费事件的代码</em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><p id="abc5" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">如果一切都准确无误，流程事件将按如下方式消耗并显示在控制台中:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nz oa l"/></div></figure><h2 id="4579" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">最后的想法</h2><p id="0ab2" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">本文的要点是，<br/> 1) Python、Spark 和 Kafka 是数据科学家日常活动中的重要框架。<br/> 2)本文帮助数据科学家用 Python 进行实验，同时在可扩展的生产环境中部署最终模型。</p><p id="7741" class="pw-post-body-paragraph ls lt iq lu b lv ml jr lx ly mm ju ma lf mn mc md lj mo mf mg ln mp mi mj mk ij bi translated">感谢您阅读本文。希望你们也能把这些点联系起来！</p></div></div>    
</body>
</html>