<html>
<head>
<title>Predicting Taxi fares in NYC using Google Cloud AI Platform (Billion + rows) Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用谷歌云人工智能平台预测纽约市的出租车价格(十亿多行)第 3 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-taxi-fares-in-nyc-using-google-cloud-ai-platform-billion-rows-part-3-3c6a6b5cd1e2?source=collection_archive---------37-----------------------#2019-11-22">https://towardsdatascience.com/predicting-taxi-fares-in-nyc-using-google-cloud-ai-platform-billion-rows-part-3-3c6a6b5cd1e2?source=collection_archive---------37-----------------------#2019-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/70069e7b2945a67110c7ea416c197dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcEJ_C4qFOHgpEXUMU-TNA.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Taxis | Photo by Nout Gons on <a class="ae kc" href="https://www.pexels.com/photo/action-america-architecture-avenue-378570/" rel="noopener ugc nofollow" target="_blank">pexels.com</a></figcaption></figure><p id="6754" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一系列文章的目标是创建一个机器学习模型，能够在乘坐开始之前估计纽约市的出租车费用。这种模式的一个版本已经应用于拼车行业，用户喜欢在选择目的地后立即知道乘车的费用。</p><p id="49b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在前两篇帖子中，我们在谷歌云平台上创建了一个项目，清理了数据，创建了功能，并最终在本地这个庞大数据集的一个小样本上训练了一个模型。我们能够达到 4.13 的 RMSE。</p><div class="lb lc gp gr ld le"><a rel="noopener follow" target="_blank" href="/predicting-taxi-fares-in-nyc-using-google-cloud-ai-platform-billion-rows-part-1-ac121832babf"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">使用谷歌云人工智能平台预测纽约市的出租车价格(十亿多行)第 1 部分</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">这个项目旨在创建一个机器学习模型，使用数据集来估计纽约市的出租车费用…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls jw le"/></div></div></a></div><div class="lb lc gp gr ld le"><a rel="noopener follow" target="_blank" href="/predicting-taxi-fares-in-nyc-using-google-cloud-ai-platform-billion-rows-part-2-f0191a70dea8"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">使用谷歌云人工智能平台预测纽约市的出租车价格(十亿多行)第 2 部分</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">在这一系列文章中，我们正在处理一个真实世界的纽约出租车乘坐数据集，该数据集由 BigQuery 托管，以…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="lt l lp lq lr ln ls jw le"/></div></div></a></div><p id="1cc2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们对整个数据集进行训练，并执行超参数调整。我们开始吧。</p><p id="392a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我已经将来自 Bigquery 的整个 130 GB 数据集作为分片 CSV 文件存储到我在 GCP 的存储桶中。TensorFlow 可以直接从 BigQuery 中读取，但这个过程非常慢，而且最好是使用 CSV 文件，就像我在之前的<a class="ae kc" rel="noopener" target="_blank" href="/predicting-taxi-fares-in-nyc-using-google-cloud-ai-platform-billion-rows-part-2-f0191a70dea8">帖子</a>中使用示例所做的那样。</p><h2 id="0d0a" class="lu lv iq bd lw lx ly dn lz ma mb dp mc ko md me mf ks mg mh mi kw mj mk ml mm bi translated">云人工智能平台</h2><p id="df66" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">在云上训练有两个主要步骤:</p><ul class=""><li id="cefd" class="ms mt iq kf b kg kh kk kl ko mu ks mv kw mw la mx my mz na bi translated">把代码做成 Python 包(创建<strong class="kf ir"> __init__)。py、task.py、</strong>和<strong class="kf ir"> model.py </strong>文件)</li><li id="93a2" class="ms mt iq kf b kg nb kk nc ko nd ks ne kw nf la mx my mz na bi translated">将使用 gcloud 的 Python 包提交到云 AI 平台</li></ul><p id="cdae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">创建 task.py </strong></p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="74fe" class="lu lv iq nl b gy np nq l nr ns">%%writefile your_path/task.py<br/>import argparse<br/>import json<br/>import os</span><span id="52a3" class="lu lv iq nl b gy nt nq l nr ns">from . import model</span><span id="8587" class="lu lv iq nl b gy nt nq l nr ns">if __name__ == "__main__":<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument(<br/>        "--hidden_units",<br/>        help = "Hidden layer sizes,<br/>        type = str,<br/>        default = "16,16"<br/>    )<br/>    parser.add_argument(<br/>        "--train_data_path",<br/>        help = "Path to training data",<br/>        required = True<br/>    )<br/>    parser.add_argument(<br/>        "--train_steps",<br/>        help = "Training steps",<br/>        type = int,<br/>        default = 1000<br/>    )<br/>    parser.add_argument(<br/>        "--eval_data_path",<br/>        help = "Path to evaluation data",<br/>        required = True<br/>    )<br/>    parser.add_argument(<br/>        "--output_dir",<br/>        help = "Output Directory",<br/>        required = True<br/>    )<br/>    parser.add_argument(<br/>        "--job-dir",<br/>        help="Required by gcloud",<br/>    )<br/>    args = parser.parse_args().__dict__<br/>    <br/>    # Append trial_id r<br/>    args["output_dir"] = os.path.join(<br/>        args["output_dir"],<br/>        json.loads(<br/>            os.environ.get("TF_CONFIG", "{}")<br/>        ).get("task", {}).get("trial", "")<br/>    ) <br/>    # Run<br/>    model.train_and_evaluate(args)</span></pre><p id="c1ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">创建 model.py </strong></p><p id="3bff" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要优化的超参数需要作为命令行参数传入</p><pre class="ng nh ni nj gt nk nl nm nn aw no bi"><span id="4135" class="lu lv iq nl b gy np nq l nr ns">%%writefile your_path/model.py<br/>import tensorflow as tf<br/>import numpy as np<br/>import shutil<br/>print(tf.__version__)</span><span id="f15b" class="lu lv iq nl b gy nt nq l nr ns">CSV_COLUMN_NAMES = ["fare_amount","dayofweek","hourofday","pickuplon","pickuplat","dropofflon","dropofflat"]<br/>CSV_DEFAULTS = [[0.0],[1],[0],[-74.0],[40.0],[-74.0],[40.7]]</span><span id="773a" class="lu lv iq nl b gy nt nq l nr ns">def read_dataset(csv_path):<br/>    def _parse_row(row):<br/>        # Every row into a list of tensors<br/>        fields = tf.decode_csv(records = row, record_defaults = CSV_DEFAULTS)</span><span id="6167" class="lu lv iq nl b gy nt nq l nr ns"># All the features into a dictionary<br/>        features = dict(zip(CSV_COLUMN_NAMES, fields))<br/>        <br/>        # Feature Engineering<br/>        features = add_engineered_features(features)<br/>        <br/>        # Pop the fare_amount <br/>        label = features.pop("fare_amount")</span><span id="2660" class="lu lv iq nl b gy nt nq l nr ns">return features, label<br/>    <br/>    # Create a dataset<br/>    dataset = tf.data.Dataset.list_files(file_pattern = csv_path) # (i.e. data_file_*.csv)<br/>    dataset = dataset.flat_map(map_func = lambda filename:tf.data.TextLineDataset(filenames = filename).skip(count = 1))<br/>    dataset = dataset.map(map_func = _parse_row)<br/>    <br/>    return dataset</span><span id="284c" class="lu lv iq nl b gy nt nq l nr ns">def train_input_fn(csv_path, batch_size = 128):<br/>    # Create the training dataset<br/>    dataset = read_dataset(csv_path)<br/>      <br/>    # Shuffle<br/>    dataset = dataset.shuffle(buffer_size = 1000).repeat(count = None).batch(batch_size = batch_size)<br/>   <br/>    return dataset</span><span id="affd" class="lu lv iq nl b gy nt nq l nr ns">def eval_input_fn(csv_path, batch_size = 128):<br/>    # Create the Validation dataset<br/>    dataset = read_dataset(csv_path)</span><span id="afc3" class="lu lv iq nl b gy nt nq l nr ns"># Shuffle and batch <br/>    dataset = dataset.batch(batch_size = batch_size)<br/>   <br/>    return dataset<br/>  <br/># One Hot Encoding<br/>fc_dayofweek = tf.feature_column.categorical_column_with_identity(key = "dayofweek", num_buckets = 7)<br/>fc_hourofday = tf.feature_column.categorical_column_with_identity(key = "hourofday", num_buckets = 24)</span><span id="8c9e" class="lu lv iq nl b gy nt nq l nr ns"># Bucketize latitudes and longitudes<br/>NBUCKETS = 16<br/>latbuckets = np.linspace(start = 38.0, stop = 42.0, num = NBUCKETS).tolist()<br/>lonbuckets = np.linspace(start = -76.0, stop = -72.0, num = NBUCKETS).tolist()<br/>fc_bucketized_plat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = "pickuplon"), boundaries = lonbuckets)<br/>fc_bucketized_plon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = "pickuplat"), boundaries = latbuckets)<br/>fc_bucketized_dlat = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = "dropofflon"), boundaries = lonbuckets)<br/>fc_bucketized_dlon = tf.feature_column.bucketized_column(source_column = tf.feature_column.numeric_column(key = "dropofflat"), boundaries = latbuckets)</span><span id="c915" class="lu lv iq nl b gy nt nq l nr ns"># Crossed Column<br/>fc_crossed_day_hr = tf.feature_column.crossed_column(keys = [fc_dayofweek, fc_hourofday], hash_bucket_size = 24 * 7)</span><span id="0eff" class="lu lv iq nl b gy nt nq l nr ns">def add_engineered_features(features):<br/>    <br/>    <br/>    features["latdiff"] = features["pickuplat"] - features["dropofflat"] <br/>    features["londiff"] = features["pickuplon"] - features["dropofflon"] <br/>    features["euclidean_dist"] = tf.sqrt(x = features["latdiff"]**2 + features["londiff"]**2)<br/>    features["dayofweek"] = features["dayofweek"] - 1</span><span id="5f19" class="lu lv iq nl b gy nt nq l nr ns">return features</span><span id="22c1" class="lu lv iq nl b gy nt nq l nr ns">#1. Created using tf.feature_column module<br/>    tf.feature_column.indicator_column(categorical_column = fc_crossed_day_hr),<br/>    fc_bucketized_plat,<br/>    fc_bucketized_plon,<br/>    fc_bucketized_dlat,<br/>    fc_bucketized_dlon,<br/>    #2. Created in the input functions<br/>    tf.feature_column.numeric_column(key = "latdiff"),<br/>    tf.feature_column.numeric_column(key = "londiff"),<br/>    tf.feature_column.numeric_column(key = "euclidean_dist") <br/>]</span><span id="ac98" class="lu lv iq nl b gy nt nq l nr ns">def serving_input_receiver_fn():<br/>    receiver_tensors = {<br/>        'dayofweek' : tf.placeholder(dtype = tf.int32, shape = [None]), # shape is vector to allow batch of requests<br/>        'hourofday' : tf.placeholder(dtype = tf.int32, shape = [None]),<br/>        'pickuplon' : tf.placeholder(dtype = tf.float32, shape = [None]), <br/>        'pickuplat' : tf.placeholder(dtype = tf.float32, shape = [None]),<br/>        'dropofflat' : tf.placeholder(dtype = tf.float32, shape = [None]),<br/>        'dropofflon' : tf.placeholder(dtype = tf.float32, shape = [None]),<br/>    }<br/>    <br/>    features = add_engineered_features(receiver_tensors) # 'features' is what is passed on to the model<br/>    <br/>    return tf.estimator.export.ServingInputReceiver(features = features, receiver_tensors = receiver_tensors)<br/>  <br/>def train_and_evaluate(params):<br/>    OUTDIR = params["output_dir"]</span><span id="9e5d" class="lu lv iq nl b gy nt nq l nr ns">model = tf.estimator.DNNRegressor(<br/>        hidden_units = params["hidden_units"].split(","), <br/>        feature_columns = feature_cols, <br/>        model_dir = OUTDIR,<br/>        config = tf.estimator.RunConfig(<br/>            tf_random_seed = 1, # for reproducibility<br/>            save_checkpoints_steps = max(100, params["train_steps"] // 10)<br/>        ) <br/>    )</span><span id="9b97" class="lu lv iq nl b gy nt nq l nr ns">def my_rmse(labels, predictions):<br/>        pred_values = tf.squeeze(input = predictions["predictions"], axis = -1)<br/>        return {"rmse": tf.metrics.root_mean_squared_error(labels = labels, predictions = pred_values)}<br/>    <br/>    model = tf.contrib.estimator.add_metrics(model, my_rmse)</span><span id="7b48" class="lu lv iq nl b gy nt nq l nr ns">train_spec = tf.estimator.TrainSpec(<br/>        input_fn = lambda: train_input_fn(params["train_data_path"]),<br/>        max_steps = params["train_steps"])</span><span id="7a28" class="lu lv iq nl b gy nt nq l nr ns">exporter = tf.estimator.FinalExporter(name = "exporter", serving_input_receiver_fn = serving_input_receiver_fn)</span><span id="9252" class="lu lv iq nl b gy nt nq l nr ns">eval_spec = tf.estimator.EvalSpec(<br/>        input_fn = lambda: eval_input_fn(params["eval_data_path"]),<br/>        steps = None,<br/>        start_delay_secs = 1, <br/>        throttle_secs = 1, <br/>        exporters = exporter)</span><span id="00d7" class="lu lv iq nl b gy nt nq l nr ns">tf.logging.set_verbosity(v = tf.logging.INFO) # so loss is printed during training<br/>    shutil.rmtree(path = OUTDIR, ignore_errors = True) # start fresh each time</span><span id="1bcb" class="lu lv iq nl b gy nt nq l nr ns">tf.estimator.train_and_evaluate(model, train_spec, eval_spec)</span></pre><p id="f7ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">创建超参数调整配置</strong></p><p id="13d1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">云 AI 平台默认使用<strong class="kf ir">贝叶斯优化</strong>。当只调整几个参数时，我们也可以使用<strong class="kf ir">网格搜索</strong>或随机搜索。</p><p id="5036" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们只调整一个参数，即隐藏单元的数量。所有试验都是平行进行的。</p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d6fb4e67cd2d8f8533b3fec61957e476.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*vSSnNURx_57P4UsgGz7b-A.png"/></div></figure><p id="de7d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">启用 AI 平台 API </strong></p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nv"><img src="../Images/4c2eec8b3b281ab75651611228ea2a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6FBeRfE5jKqY4N_V886G5A.png"/></div></div></figure><p id="864f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">运行培训作业</strong></p><figure class="ng nh ni nj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nw"><img src="../Images/4a32fb498e987767b4f79c569a59d67b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNjnCRMW6eSHE-5NIDIOVg.png"/></div></div></figure><p id="e861" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在数据集的一部分上运行这个来获得更好的想法，然后在整个数据上尝试最佳组合。</p><p id="5ec8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我发现隐藏单元的最佳配置是“<strong class="kf ir"> 64，64，64，8 </strong>”，RMSE 为<strong class="kf ir"> 3.98。</strong></p><p id="da49" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的三部分系列到此结束。通过采用不同的方法，如设计新功能、选择不同的模型、调整更多的超参数等，可以实现许多改进。创造最好的模型需要耐心和大量的尝试和错误。</p><p id="13b6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">资源:<a class="ae kc" href="https://www.coursera.org/specializations/machine-learning-tensorflow-gcp" rel="noopener ugc nofollow" target="_blank">Google 云平台上 TensorFlow 的机器学习</a></p><p id="e009" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" href="http://www.linkedin.com/in/tejan-irla" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。你可以在这里找到完整的代码<a class="ae kc" href="https://github.com/tejanirla/taxi_fare_prediction/blob/master/taxi_fare_model_creation.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="5ef5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">干杯！！</p></div></div>    
</body>
</html>