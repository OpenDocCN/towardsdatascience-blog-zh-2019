# 用机器学习的合成数据扩充分类数据集。

> 原文：<https://towardsdatascience.com/augmenting-categorical-datasets-with-synthetic-data-for-machine-learning-a25095d6d7c8?source=collection_archive---------14----------------------->

## 在扩充的合成分类数据中进行标签验证的高斯混合模型。

考虑一个假设但常见的场景。您需要构建一个分类器来将样本分配给一个群体组。你有一个相当大的一百万样本的训练数据集。已经清洗、准备好并贴好标签。少数连续变量已经标准化，而代表大多数特征的分类变量使用一次性编码方案推出。现在构建一个简单的神经网络分类器，或者一个随机的 forrest，或者你的箭筒中的另一个首选方法应该是轻而易举的。但是由于您是一名彻底的数据科学家，您对数据集进行了快速的描述性分析，并发现了一个问题。不平衡的基本利率。

人口映射到 10 个类别，但其中两个代表 98%的数据。剩下的八个类只覆盖了可怜的 2 %,几乎不足以让你的统计模型舒适地收敛。你能做什么？

![](img/b1c93c728e4221315b78f363fbd3af83.png)

一种方法是增加数据并为代表性不足的类别合成新的样本，目的是平衡基本速率。该方法非常简单，常用于图像分类。每幅图像都被裁剪、旋转、移动多次，以增加分类器学习的可用样本数量。有一个软保证，图像的微小扰动不会改变分类标签。所以这项技术非常有效。为什么不对我们的连续和分类变量数据集使用类似的方法呢？难道我们不能随机地，但分钟地扰动数据集，并扩大我们的代表性不足的类吗？

我们不能。事实上，我们可能会遇到大麻烦。虽然扩充图像通常保留了像素之间的关系，从而保留了标签分配，但是用大部分二进制分类特征扰乱数据集会将被扰乱的样本转移到完全不同的类别中。例如，考虑跑步者和游泳者的数据集，然后考虑在 0 或 1 之间随机翻转性别特征。这种干扰可能会在增强期间将女性游泳运动员移动到男性游泳运动员类别中，但不会改变标签。最后，一双没用的泳裤可能会被推荐给那些寻找连体泳衣的人。

我将描述一种解决标签转换的方法，这种方法在过去为我们的项目发挥了作用。这个想法的主旨是

1.  从代表性不足的类别中选择所有样本
2.  将子集中的每个类复制 N 次，以获得合理的表示
3.  使用分布的平均值和标准偏差作为扰动界限，随机扰动每个子集中的要素，并对分类值归一化为 0 和 1
4.  验证每个新样本属于正确的类别，或者使用高斯混合模型和 Mahalanobis 距离重新分配到新的类别。

实现的实际机制与上面的元算法略有不同，但保留了理论结构。我们接下来会讨论。

# 样本分析的高斯混合模型。

出于直觉，让我们将高斯混合模型(GMM)视为 K-均值聚类的推广，因为大多数人在某个时候都使用 K-均值。一个区别是 GMM 考虑了多维分布的椭球形状，并允许每个样本的多分量分配，而 K-Means 基于球形分布假设和单分量分配。我将不讨论 GMM 背后的理论，而是将它们的好处用于我们的事业。

Mahalanobis 距离(MD)是一个多维度的概括，它表示样本与分布平均值之间的标准偏差。该度量是无单位的和尺度不变的，并且它尊重数据的椭圆形状，随着样本沿着每个主分量轴远离平均值而增加。这里我想插入一些 LaTex 代码来说明这个函数，但是 Medium 不支持它:-(。

提醒一下，我们的目标是通过扰动现有样本来合成新的类样本，以便平衡成功分类器训练的基本速率。我们的问题是大量的分类特征使得扰动不稳定——分类特征的小变化可能导致标签的大变化——新样本可能成为不同类别的成员。因此，我们必须验证每个新样本，如果扰动将样本转移到新的类别，则重新分配新的标签。

下面是我们在 N = 10 的 N 类例子中是如何做的。我假设你对转基因大豆和熊猫有一些经验。Python 警察注意:代码是为了清晰而写的，而不是为了效率——是的，有冗余，而且确实有更好的方法来做事。

1)在任何扩充之前，让我们训练 10 个单独的单分量高斯混合模型。我们从每个类别中提取所有样本的子集，然后为每个类别连续训练单个 GMM，并将μ和σ值存储在字典中。我们将使用`scikit`实现 GMMs。

2)让我们计算每个分布的平均 Mahalanobis 距离(MD ),稍后我们将使用它进行成员资格检查。

3)让我们使用 Mahalanobis 距离(MD)作为健全性检查，以验证我们已知样本的子集映射到由新训练的 GMM 表示的其对应的类。这里的想法是计算从每个样本到所有 GMM 的 Mahalanobis 距离。最小距离给出了我们将样本分配到的类别。我们使用正确和不正确的赋值来计算准确度。*注意:如果已知样本映射的精确度很低，那么你的类就不容易分离，这种方法就不起作用。我们的目标是 90%的准确率作为下限，但是准确率目标应该取决于您的问题的具体情况。*

4)将所有代表性不足的类别提取到单独的子集中，并捕获每个类别的分布统计。我们将使用平均值和标准偏差值作为参数，使用高斯先验为每个类别随机生成新样本。这相当于干扰现有的样本集。这实际上偏离了我之前介绍的元算法，但实现了相同的目标。

5)迭代新样本，并测量每个 GMM 的 MD。最小 MD 将表示课程分配。如果新样本最接近正确的 GMM，则保留类别标签。如果新样品最接近不同的 GMM，则首先检查样品的 MD 是否在我们在步骤 2 中为该 GMM 计算的平均值范围内。如果是，则重新分配样本类；否则，丢弃样品。

这一过程通过可靠的标签保存增加了连续和分类数据。从概念上讲，该过程类似于机器视觉中广泛使用的图像增强。然而，许多数据集不适合这种方法。你要了解你的数据，合理判断一次成功隆胸的概率。

这种方法帮助我们用不平衡的数据集训练分类器，这些数据集可以很好地推广到看不见的数据。我希望它对你的工作有用。