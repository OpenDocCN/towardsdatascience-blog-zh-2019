<html>
<head>
<title>Implementing the New State of the Art Mish Activation With 2 Lines of Code In Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Pytorch 中的 2 行代码实现最新的 Mish 激活</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-the-new-state-of-the-art-mish-activation-with-2-lines-of-code-in-pytorch-e7ef438a5ee7?source=collection_archive---------19-----------------------#2019-10-17">https://towardsdatascience.com/implementing-the-new-state-of-the-art-mish-activation-with-2-lines-of-code-in-pytorch-e7ef438a5ee7?source=collection_archive---------19-----------------------#2019-10-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7dea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最先进的深度学习从未如此简单</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6d015f042df7d3a9acd8451ed89f440c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wWOnTQO5XcEtAFc-fRoqw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">From <a class="ae ky" href="https://www.pexels.com/photo/action-blur-bulb-dark-355904/" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="cea9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1908.08681.pdf" rel="noopener ugc nofollow" target="_blank">本文</a>作者<a class="lv lw ep" href="https://medium.com/u/b7a37456ed33?source=post_page-----e7ef438a5ee7--------------------------------" rel="noopener" target="_blank">迪甘塔·米斯拉</a>最近发表了一个关于深度学习的新激活函数，叫做 mish activation。当在 CIFAR-100 上使用 Squeeze Excite Net-18 进行测试时，这种新的激活功能击败了 ReLU 和 swish 激活功能。如果你想知道关于研究和激活功能的细节，我强烈推荐你去阅读我上面链接的论文。我不打算深入研究论文的数学和研究，但函数看起来是这样的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lx"><img src="../Images/86404ca7a35ea2c800476b5603d17f01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RuNM4YV8ZuitdLgkq_MPUw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://arxiv.org/ftp/arxiv/papers/1908/1908.08681.pdf" rel="noopener ugc nofollow" target="_blank">Mish Activation Function from Paper</a></figcaption></figure><p id="76ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你熟悉激活功能，你可能会认为它看起来很像 swish 激活。这是因为 mish 受到了 swish 的启发。从论文的初步阅读来看，似乎 mish 可能比 swish 和非常受欢迎的 ReLU 激活都要好。这个全新的激活功能最棒的地方是你可以用 2 行代码实现它。</p><h1 id="f543" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">履行</h1><h2 id="2599" class="mq lz it bd ma mr ms dn me mt mu dp mi li mv mw mk lm mx my mm lq mz na mo nb bi translated">定制网络</h2><p id="8e03" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">首先，我将向您展示如何在您自己构建的神经网络中实现 mish。在构建我们的网络之前，我们需要使用 PyTorch 编写 mish 函数。正如承诺的那样，它只需要 2 行代码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/1eb03032d384080e3e9e0816fb089183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hc9s2u9Cs7iUmbmdhhTVgg.png"/></div></div></figure><p id="b524" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用这两行代码，我们写了一个最先进的激活函数。所以现在让我们编写一个基本的 CNN，并在其中实现我们的激活功能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/dd71b31f1650ea7ce311f74783f4f938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T20xOfUaCtYbruQl6ejPCg.png"/></div></div></figure><p id="5e2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在前面的部分，我将所有线性层的激活函数设置为我们上面写的 mish 激活。现在模型可以训练了。这是非常直接和容易实现的！我在 Kaggle 的空中仙人掌识别数据上运行了这个模型，并在 10 个训练时期后看到了比 ReLU 激活 1.5%的准确性增加。我不会抱怨那件事。</p><h2 id="a894" class="mq lz it bd ma mr ms dn me mt mu dp mi li mv mw mk lm mx my mm lq mz na mo nb bi translated">迁移学习</h2><p id="cc41" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">建立自己的神经网络很酷，但几乎不实用。当涉及到在深度学习中获得顶级结果时，迁移学习往往更有效。因此，让我们看看如何用 VGG-16 实现 mish 激活。我们需要为我们的 mish 激活写一个从 torch.nn.Module 类继承的类，而不是一个函数。它应该是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/35baa51ed293b429105d40b5933fd4f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1AtmBpHL_R34o0apFvySA.png"/></div></div></figure><p id="f889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我很抱歉，我答应 2 行代码，现在我把它改为 5。我希望在你用了这个并看到它有多酷之后，你会原谅我。把我们的激活函数写成一个类，我们现在可以准备把它添加到我们的 VGG-16 模型中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/6f82dabede21664c3fcf0246bdfa298e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2hEKCvQ7kFhzGlcdBAnFkA.png"/></div></div></figure><p id="c925" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将 VGG-16 的分类部分中的 ReLU 激活改为 mish 激活，并用一个用于我们的分类问题的层替换最后一层。除去最后一层的渐变后，我们就可以开始训练了！有了这些小代码，你就实现了一个最先进的激活功能。</p><h1 id="98ec" class="ly lz it bd ma mb mc md me mf mg mh mi jz mj ka mk kc ml kd mm kf mn kg mo mp bi translated">一些提示</h1><p id="8d7c" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">如果你想实现它，我认为在 mish 的文章中提到的一些事情是值得注意的:</p><ul class=""><li id="4db8" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">与其他激活函数相比，mish 函数在较低的学习速率下表现更好。所以一定不要去 high。</li><li id="25c1" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">Mish 激活似乎对超参数的变化相当稳健。查看报纸，了解更多这方面的信息。</li><li id="6b82" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">没有最佳激活函数。这不会总是比其他任何事情都好。不过这绝对值得一试。</li></ul><p id="922a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣看我的完整代码，我把它放在 Kaggle <a class="ae ky" href="https://www.kaggle.com/nelsongriffiths/mish-activation-and-transfer-learning-pytorch" rel="noopener ugc nofollow" target="_blank">这里</a>。随意克隆它，尝试不同的架构，或者将 mish 激活应用到卷积层。</p></div></div>    
</body>
</html>