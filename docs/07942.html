<html>
<head>
<title>Language Model Concept behind Word Suggestion Feature</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词暗示特征背后的语言模型概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentence-generation-with-n-gram-21a5eef36a1b?source=collection_archive---------12-----------------------#2019-11-02">https://towardsdatascience.com/sentence-generation-with-n-gram-21a5eef36a1b?source=collection_archive---------12-----------------------#2019-11-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8fbc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">N 元语法的简单介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2c07c9cfb009d9a20b4bf654ccd5575a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IwWFEPCzJZykRWIg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@dearseymour?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ksenia Makagonova</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="7201" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="765d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">你可能在智能手机上见过单词建议功能。当您键入一个单词时，键盘会理解您键入的内容，并建议下一个相关的单词。如果你选择了建议的单词，它会建议另一个，一个又一个，直到你得到这些有趣的句子。</p><p id="c075" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是一个简单的概念，称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">语言建模</strong> </a>。所以语言建模所做的是，它阅读书面文本，并试图为下一个单词分配概率。让我们看一个例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/4795c6aad2ea8a6154868bb3657cffff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*mv2ULBGFfcK09ss1rOwfVQ.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 1: Language model tries to guess what the next word is. (number on the right is probability)</figcaption></figure><p id="a919" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">直观地，假设我们有 100 个单词要考虑下一个单词，语言模型会考虑上一个单词，并给出这 100 个单词的概率。然后，作为单词建议功能的一部分，它可以向用户提供前 3 个选项。现在，让我们给 LM 一个正式的定义。</p><h1 id="b9f3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">语言模型</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/f04ea0fedc54bdff7bc094b9b9afc492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGq9IBSaXXSvG6fbKldWEg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 2: An example sentence with its notation</figcaption></figure><p id="ce74" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">通常，语言模型被称为<strong class="lq ir">，一种赋予句子概率的方式</strong>。</p><p id="d109" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是等等！我们之前不是提到过 LM 给下一个单词分配概率吗？让我们分解上面的等式来进一步研究这个问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/be0b98832f434afee5f1655b6e1df80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*MuxO4e24k7FPAV9jz_UZsg.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 1: Probability of a sentence based on probability of predicting next word</figcaption></figure><p id="eefb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">正如你在等式 1 中看到的，LM 不仅可以给下一个给定的单词分配概率，还可以给整个句子分配概率。然后，LM 可以理解哪个句子听起来好，哪个不好。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/b4711be7d82eb5a713b9396947949efc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSAhmHyf4c1qJPQaP2gt-Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 3: Language model determines which sentence is more suitable than the others</figcaption></figure><p id="88cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">太棒了，对吧？但是在如此强大的功能背后有什么呢？有许多方法可以创建这种语言模型。在这篇文章中，我们将探索一个简单的方法，叫做<strong class="lq ir"> n-gram </strong>。</p><h1 id="2999" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">n 元语法</h1><p id="dca6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">一个关键的想法是 ngram 一次只看 n 个单词。让我们以“猫坐在垫子上”为例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/011f9ddbb97b2d5e304cbf644ab64fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*jdjzV9Ynilw1cJAwBIX2MA.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 4: Behavior of bigram</figcaption></figure><p id="5820" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">假设 n=2(也称为 bigram)，它会尝试根据第一个词预测第二个词。例如:</p><ul class=""><li id="1405" class="mu mv iq lq b lr mk lu ml lx mw mb mx mf my mj mz na nb nc bi translated"><strong class="lq ir">猫</strong> : Bigram 试图记住“the”后面应该是“cat”。</li><li id="bf06" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated"><strong class="lq ir">cat sat</strong>:“cat”后面应该是“sat”。</li><li id="ee49" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated"><strong class="lq ir">坐在</strong>上:“坐”之后是“开”</li></ul><p id="d89f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">n=3 的情况也是如此。对于单词“猫坐”，三元模型知道“坐”在“猫”之后。我想你明白了。</p><p id="cbdd" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个概念可扩展到 n 个单词(n-gram)。那么，ngram 如何知道所有这些模式呢？是<strong class="lq ir">概率论</strong>。</p><h1 id="8426" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">二元模型</h1><p id="19f0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了简单起见，我们将看看二元模型的情况。从上面的例子中，我们知道“cat”在“the”之后，可以表示为<em class="ni"> P(cat | the) </em>。</p><blockquote class="nj nk nl"><p id="bbea" class="lo lp ni lq b lr mk jr lt lu ml ju lw nm mm lz ma nn mn md me no mo mh mi mj ij bi translated">P <!-- --> (cat | the):给定单词“the”，下一个单词是“cat”的概率。</p></blockquote><p id="ee56" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">要计算这个概率，只需使用下面的公式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/78b9a9d157b7cbb04586d1eb510eda7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUpTTJjBMPSoFmH0H4vlXQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 2: Probability for Bigram</figcaption></figure><p id="227e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其中<em class="ni"> c(猫)</em>是“猫”在整个语料库中的出现次数。同样的道理也适用于<em class="ni"> c(the) </em>。直观地说，这个等式表示:<strong class="lq ir">在所有的单词“the”中，有多少个单词后面是“cat”？瞧，现在我们有了。</strong></p><h1 id="ee75" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">二元模型与三元模型</h1><p id="8fcc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Bigram 很简单，功能也很强大，但是在很多情况下它无法执行。例如:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/7253caf51cd31d51f94f8b90764eb255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGsEWYET6GujRu55-Mp7Dw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 3: Bigram fails when it has the same previous word</figcaption></figure><p id="ecfe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在句子的开头，模型看到“the”后面跟着“cat”。如果我们使用 bigram，并且仅依靠<strong class="lq ir">之前的一个单词</strong>，则很难猜出下划线单词是“mat ”,因为它可能再次生成“cat”。幸运的是，三元模型或四元模型可以解决这个问题。也就是说，如果我们看到“坐在”,下一个词很可能是“垫子”。</p><p id="b496" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，这并不意味着我们考虑的序列越长，预测就越好。例如，对于 6-gram 来说，可能很难看到确切的“猫坐在”。因此，它不是很有效。</p><blockquote class="nj nk nl"><p id="141d" class="lo lp ni lq b lr mk jr lt lu ml ju lw nm mm lz ma nn mn md me no mo mh mi mj ij bi translated">大多数时候，二元模型和三元模型是常见的选择。</p></blockquote><h1 id="4518" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">后退和插值</h1><p id="0a35" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">也有这样的时候，你的数据集很小，3-gram 不能识别一些精确的单词；因此，导致概率为 0。这将使您的整个 3-gram 实现不再有用。</p><p id="649d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你可以很容易地做一个简单的 if-else，并在 3-gram 失败的情况下使用 2-gram。或者在 2 克失败的情况下使用 1 克。<strong class="lq ir">有种叫做，退避。</strong></p><p id="4e44" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">另一种技术，<strong class="lq ir">插值</strong>，是同时考虑不同的 ngrams。这个想法是给你的 n-gram 赋予权重(λ),这样你的 3-gram 模型也可以查看 2-gram 和 1-gram 的值。即</p><ul class=""><li id="3b6b" class="mu mv iq lq b lr mk lu ml lx mw mb mx mf my mj mz na nb nc bi translated">3 克装 60%</li><li id="8866" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated">2 克装 35%</li><li id="338c" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj mz na nb nc bi translated">1 克含 05%</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/242d42100376de2a09496a7e9eabf899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SImbH9dOleDpUB40x-1UGA.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 3: Interpolation of 3-gram</figcaption></figure><p id="8d8b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">手动配置重量，直到你觉得它是正确的。你也可以使用一些自动的方法，但是我们不会在这篇文章中讨论。有关如何正确设置这些重量的更多信息，您可以参考<a class="ae kv" href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" rel="noopener ugc nofollow" target="_blank">和</a>。</p><h1 id="3c28" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">履行</h1><p id="009b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在让我们试着用这个 LM 来生成一个基于随机首字的句子。对于这个任务，我们将使用<a class="ae kv" href="https://www.kaggle.com/gulsahdemiryurek/harry-potter-dataset" rel="noopener ugc nofollow" target="_blank">哈利波特与魔法石电影剧本</a>来训练我们的 ngram。对于下面的代码，你也可以在<a class="ae kv" href="https://www.kaggle.com/ruka96/sentence-generation-with-n-gram?scriptVersionId=22881807" rel="noopener ugc nofollow" target="_blank">上找到这个内核</a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="1d0f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，我们就可以统计出一元词、二元词和三元词的出现频率。这将在以后用于计算概率。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="f5e6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">既然我们可以计算 n-gram 的数目，我们就可以计算它的概率。对于 1 克、2 克和 3 克，我们将分别使用 0.01、0.4 和 0.5 的线性插值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="79a6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们试着造 20 个单词。首字母将被随机化，并选择最好的下一个单词。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="676f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">那么，这将如何表现呢？下面是每一代的结果。</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="8f33" class="nz kx iq nv b gy oa ob l oc od">Generation #1:<br/>protecting the stone , but i think i 'll be a wizard . . . . . . . .</span><span id="9e55" class="nz kx iq nv b gy oe ob l oc od">Generation #2:<br/>platform 9 3/4 ? think you 're going to be the same . . . . . . . </span><span id="2f76" class="nz kx iq nv b gy oe ob l oc od">Generation #3:<br/>sacrifice himself , he 's got a little bit . . . . . . . . . . .</span></pre><h1 id="611a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="387d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">N-gram 是一种简单而强大的语言建模技术，只需查看 2 到 3 个单词。由于计算简单快速，它在许多应用中非常实用，例如移动设备上的单词建议。然而，N-gram 在生成长句时并不健壮，因为长句中的当前单词依赖于句子中的第一个单词。这种长期依赖关系更适合神经网络之类的模型，但这是以后文章的主题。</p></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><h1 id="88cf" class="kw kx iq bd ky kz om lb lc ld on lf lg jw oo jx li jz op ka lk kc oq kd lm ln bi translated">参考</h1><ol class=""><li id="5681" class="mu mv iq lq b lr ls lu lv lx or mb os mf ot mj ou na nb nc bi translated"><a class="ae kv" href="https://web.stanford.edu/~jurafsky/slp3/3.pdf" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~jurafsky/slp3/3.pdf</a></li><li id="9472" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj ou na nb nc bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/introduction-to-language-models-n-gram-e323081503d9">https://towards data science . com/introduction-to-language-models-n-gram-e 323081503d 9</a></li><li id="b6fd" class="mu mv iq lq b lr nd lu ne lx nf mb ng mf nh mj ou na nb nc bi translated"><a class="ae kv" href="http://www.cs.umd.edu/class/fall2018/cmsc470/slides/slides_10.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . UMD . edu/class/fall 2018/cmsc 470/slides/slides _ 10 . pdf</a></li></ol></div></div>    
</body>
</html>