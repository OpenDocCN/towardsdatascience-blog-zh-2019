<html>
<head>
<title>Text preprocessing steps and universal reusable pipeline</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本预处理步骤和通用可重用流水线</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a?source=collection_archive---------2-----------------------#2019-05-20">https://towardsdatascience.com/text-preprocessing-steps-and-universal-pipeline-94233cb6725a?source=collection_archive---------2-----------------------#2019-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/adb99914a17e7a62c4673ebf1a3b77b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QGWyxDaFhavZa495eJBO9Q.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="8b72" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">所有文本预处理步骤的描述和可重用文本预处理管道的创建</h2></div><p id="0cd7" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在向任何 ML 模型提供某种数据之前，必须对其进行适当预处理。你一定听过这个谚语:<code class="fe lm ln lo lp b">Garbage in, garbage out</code> (GIGO)。文本是一种特殊的数据，不能直接输入到大多数 ML 模型中，所以在输入到模型中之前，你必须以某种方式从中提取数字特征，换句话说就是<code class="fe lm ln lo lp b">vectorize</code>。矢量化不是本教程的主题，但您必须了解的主要内容是，GIGO 也适用于矢量化，您只能从定性预处理的文本中提取定性特征。</p><p id="4bb0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们将要讨论的事情:</p><ol class=""><li id="03f2" class="lq lr jb ks b kt ku kw kx kz ls ld lt lh lu ll lv lw lx ly bi translated">标记化</li><li id="ee6a" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">清洁</li><li id="e472" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">正常化</li><li id="6b87" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">词汇化</li><li id="1287" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">汽蒸</li></ol><p id="f5b6" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">最后，我们将创建一个可重用的管道，您可以在您的应用程序中使用它。</p><p id="74b0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Kaggle 内核:<a class="ae me" href="https://www.kaggle.com/balatmak/text-preprocessing-steps-and-universal-pipeline" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/balat mak/text-预处理-步骤-通用-管道</a></p><p id="11d5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们假设这个示例文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="734f" class="mn mo jb lp b gy mp mq l mr ms">An explosion targeting a tourist bus has injured at least 16 people near the Grand Egyptian Museum, <br/>next to the pyramids in Giza, security sources say E.U.<br/><br/>South African tourists are among the injured. Most of those hurt suffered minor injuries, <br/>while three were treated in hospital, N.A.T.O. say.<br/><br/>http://localhost:8888/notebooks/Text%20preprocessing.ipynb<br/><br/>@nickname of twitter user and his email is email@gmail.com . <br/><br/>A device went off close to the museum fence as the bus was passing on 16/02/2012.</span></pre><h1 id="fb24" class="mt mo jb bd mu mv mw mx my mz na nb nc kh nd ki ne kk nf kl ng kn nh ko ni nj bi translated">标记化</h1><p id="49e6" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated"><code class="fe lm ln lo lp b">Tokenization</code>——文本预处理步骤，假设将文本分割成<code class="fe lm ln lo lp b">tokens</code>(单词、句子等)。)</p><p id="9780" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">看起来你可以使用某种简单的分隔符来实现它，但是你不要忘记，在许多不同的情况下，分隔符是不起作用的。例如，如果您使用带点的缩写，那么<code class="fe lm ln lo lp b">.</code>用于将标记化成句子的分隔符将会失败。所以你必须有一个更复杂的模型来达到足够好的结果。通常这个问题可以通过使用<code class="fe lm ln lo lp b">nltk</code>或<code class="fe lm ln lo lp b">spacy</code> nlp 库来解决。</p><p id="d98d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLTK:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="9a54" class="mn mo jb lp b gy mp mq l mr ms">from nltk.tokenize import sent_tokenize, word_tokenize<br/><br/>nltk_words = word_tokenize(example_text)<br/>display(f"Tokenized words: <strong class="lp jc">{nltk_words}</strong>")</span></pre><p id="cc2e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">输出:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="c50d" class="mn mo jb lp b gy mp mq l mr ms">Tokenized words: ['An', 'explosion', 'targeting', 'a', 'tourist', 'bus', 'has', 'injured', 'at', 'least', '16', 'people', 'near', 'the', 'Grand', 'Egyptian', 'Museum', ',', 'next', 'to', 'the', 'pyramids', 'in', 'Giza', ',', 'security', 'sources', 'say', 'E.U', '.', 'South', 'African', 'tourists', 'are', 'among', 'the', 'injured', '.', 'Most', 'of', 'those', 'hurt', 'suffered', 'minor', 'injuries', ',', 'while', 'three', 'were', 'treated', 'in', 'hospital', ',', 'N.A.T.O', '.', 'say', '.', 'http', ':', '//localhost:8888/notebooks/Text', '%', '20preprocessing.ipynb', '@', 'nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email', '@', 'gmail.com', '.', 'A', 'device', 'went', 'off', 'close', 'to', 'the', 'museum', 'fence', 'as', 'the', 'bus', 'was', 'passing', 'on', '16/02/2012', '.']</span></pre><p id="eda2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">空间:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="90db" class="mn mo jb lp b gy mp mq l mr ms">import spacy<br/>import en_core_web_sm<br/><br/>nlp = en_core_web_sm.load()<br/><br/>doc = nlp(example_text)<br/>spacy_words = [token.text for token <strong class="lp jc">in</strong> doc]<br/>display(f"Tokenized words: <strong class="lp jc">{spacy_words}</strong>")</span></pre><p id="8c96" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">输出:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="8593" class="mn mo jb lp b gy mp mq l mr ms">Tokenized words: ['\\n', 'An', 'explosion', 'targeting', 'a', 'tourist', 'bus', 'has', 'injured', 'at', 'least', '16', 'people', 'near', 'the', 'Grand', 'Egyptian', 'Museum', ',', '\\n', 'next', 'to', 'the', 'pyramids', 'in', 'Giza', ',', 'security', 'sources', 'say', 'E.U.', '\\n\\n', 'South', 'African', 'tourists', 'are', 'among', 'the', 'injured', '.', 'Most', 'of', 'those', 'hurt', 'suffered', 'minor', 'injuries', ',', '\\n', 'while', 'three', 'were', 'treated', 'in', 'hospital', ',', 'N.A.T.O.', 'say', '.', '\\n\\n', 'http://localhost:8888/notebooks', '/', 'Text%20preprocessing.ipynb', '\\n\\n', '@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com', '.', '\\n\\n', 'A', 'device', 'went', 'off', 'close', 'to', 'the', 'museum', 'fence', 'as', 'the', 'bus', 'was', 'passing', 'on', '16/02/2012', '.', '\\n']</span></pre><p id="1c70" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在 spacy 输出标记化中，而不是在 nltk 中:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="2a9b" class="mn mo jb lp b gy mp mq l mr ms">{'E.U.', '\\n', 'Text%20preprocessing.ipynb', 'email@gmail.com', '\\n\\n', 'N.A.T.O.', 'http://localhost:8888/notebooks', '@nickname', '/'}</span></pre><p id="0c4e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在 nltk 中但不在 spacy 中:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="4046" class="mn mo jb lp b gy mp mq l mr ms">{'nickname', '//localhost:8888/notebooks/Text', 'N.A.T.O', ':', '@', 'gmail.com', 'E.U', 'http', '20preprocessing.ipynb', '%'}</span></pre><p id="162e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们看到<code class="fe lm ln lo lp b">spacy</code>标记了一些奇怪的东西，比如<code class="fe lm ln lo lp b">\n</code>、<code class="fe lm ln lo lp b">\n\n</code>，但是能够处理 URL、电子邮件和类似 Twitter 的提及。此外，我们看到<code class="fe lm ln lo lp b">nltk</code>标记化的缩写没有最后的<code class="fe lm ln lo lp b">.</code></p><h1 id="87ed" class="mt mo jb bd mu mv mw mx my mz na nb nc kh nd ki ne kk nf kl ng kn nh ko ni nj bi translated">清洁</h1><p id="c4f9" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated"><code class="fe lm ln lo lp b">Cleaning</code> is 步骤假设删除所有不需要的内容。</p><h2 id="4caa" class="mn mo jb bd mu np nq dn my nr ns dp nc kz nt nu ne ld nv nw ng lh nx ny ni nz bi translated">删除标点符号</h2><p id="079d" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated">当标点符号不能为文本矢量化带来附加值时，这可能是一个好的步骤。标点符号删除最好在标记化步骤之后进行，在此之前进行可能会导致不良影响。<code class="fe lm ln lo lp b">TF-IDF</code>、<code class="fe lm ln lo lp b">Count</code>、<code class="fe lm ln lo lp b">Binary</code>矢量化的好选择。</p><p id="a5a2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们假设这一步的文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="1ca4" class="mn mo jb lp b gy mp mq l mr ms">@nickname of twitter user, and his email is email@gmail.com .</span></pre><p id="4a1b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在标记化之前:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="ef43" class="mn mo jb lp b gy mp mq l mr ms">text_without_punct = text_with_punct.translate(str.maketrans('', '', string.punctuation))<br/>display(f"Text without punctuation: <strong class="lp jc">{text_without_punct}</strong>")</span></pre><p id="753e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">输出:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="e926" class="mn mo jb lp b gy mp mq l mr ms">Text without punctuation: nickname of twitter user and his email is emailgmailcom</span></pre><p id="8a5e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在这里，您可以看到用于正确标记化的重要符号已被删除。现在电子邮件无法正常检测。正如您在<code class="fe lm ln lo lp b">Tokenization</code>步骤中提到的，标点符号被解析为单个符号，所以更好的方法是先进行符号化，然后删除标点符号。</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="2db2" class="mn mo jb lp b gy mp mq l mr ms">import spacy<br/>import en_core_web_sm<br/><br/>nlp = en_core_web_sm.load()</span><span id="4d22" class="mn mo jb lp b gy oa mq l mr ms">doc = nlp(text_with_punct)<br/>tokens = [t.text for t <strong class="lp jc">in</strong> doc]</span><span id="2224" class="mn mo jb lp b gy oa mq l mr ms"><em class="ob"># python based removal</em><br/>tokens_without_punct_python = [t for t <strong class="lp jc">in</strong> tokens if t <strong class="lp jc">not</strong> <strong class="lp jc">in</strong> string.punctuation]<br/>display(f"Python based removal: <strong class="lp jc">{tokens_without_punct_python}</strong>")</span><span id="3012" class="mn mo jb lp b gy oa mq l mr ms"># spacy based removal<br/>tokens_without_punct_spacy = [t.text for t <strong class="lp jc">in</strong> doc if t.pos_ != 'PUNCT']<br/>display(f"Spacy based removal: <strong class="lp jc">{tokens_without_punct_spacy}</strong>")</span></pre><p id="5723" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">基于 Python 的移除结果:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="339c" class="mn mo jb lp b gy mp mq l mr ms">['@nickname', 'of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com']</span></pre><p id="ca30" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">基于空间的移除:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="bd11" class="mn mo jb lp b gy mp mq l mr ms">['of', 'twitter', 'user', 'and', 'his', 'email', 'is', 'email@gmail.com']</span></pre><p id="91a7" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里你可以看到<code class="fe lm ln lo lp b">python-based</code>移除比 spacy 更有效，因为 spacy 将<code class="fe lm ln lo lp b">@nicname</code>标记为<code class="fe lm ln lo lp b">PUNCT</code>词性。</p><h2 id="f668" class="mn mo jb bd mu np nq dn my nr ns dp nc kz nt nu ne ld nv nw ng lh nx ny ni nz bi translated">停止单词删除</h2><p id="9dbd" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated"><code class="fe lm ln lo lp b">Stop words</code>通常指一种语言中最常见的词，通常不会带来额外的意义。没有一个所有 nlp 工具都使用的通用停用词列表，因为这个术语的定义非常模糊。尽管实践已经表明，当准备用于索引的文本时，这一步骤是必须的，但是对于文本分类目的来说可能是棘手的。</p><p id="d43c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">空间停止字数:<code class="fe lm ln lo lp b">312</code></p><p id="09a1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLTK 停止字数:<code class="fe lm ln lo lp b">179</code></p><p id="845b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们假设这一步的文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="7bf4" class="mn mo jb lp b gy mp mq l mr ms">This movie is just not good enough</span></pre><p id="e3e4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">空间:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="fb53" class="mn mo jb lp b gy mp mq l mr ms">import spacy<br/>import en_core_web_sm<br/><br/>nlp = en_core_web_sm.load()</span><span id="2945" class="mn mo jb lp b gy oa mq l mr ms">text_without_stop_words = [t.text for t <strong class="lp jc">in</strong> nlp(text) if <strong class="lp jc">not</strong> t.is_stop]<br/>display(f"Spacy text without stop words: <strong class="lp jc">{text_without_stop_words}</strong>")</span></pre><p id="b23f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">没有停用词的空白文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="69ae" class="mn mo jb lp b gy mp mq l mr ms">['movie', 'good']</span></pre><p id="fece" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLTK:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="5a98" class="mn mo jb lp b gy mp mq l mr ms">import nltk<br/><br/>nltk_stop_words = nltk.corpus.stopwords.words('english')<br/>text_without_stop_words = [t for t <strong class="lp jc">in</strong> word_tokenize(text) if t <strong class="lp jc">not</strong> <strong class="lp jc">in</strong> nltk_stop_words]<br/>display(f"nltk text without stop words: <strong class="lp jc">{text_without_stop_words}</strong>")</span></pre><p id="1424" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">无停用词的 NLTK 文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="22f7" class="mn mo jb lp b gy mp mq l mr ms">['This', 'movie', 'good', 'enough']</span></pre><p id="22d1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里你看到 nltk 和 spacy 的词汇量不一样，所以过滤的结果也不一样。但我想强调的主要一点是，单词<code class="fe lm ln lo lp b">not</code>被过滤了，这在大多数情况下是没问题的，但在你想确定这个句子的极性的情况下<code class="fe lm ln lo lp b">not</code>会带来额外的含义。</p><p id="78d1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于这种情况，您可以在空间库中设置可以忽略的停用词。在 nltk 的情况下，您可以删除或添加自定义单词到<code class="fe lm ln lo lp b">nltk_stop_words</code>，它只是一个列表。</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="736f" class="mn mo jb lp b gy mp mq l mr ms">import en_core_web_sm<br/><br/>nlp = en_core_web_sm.load()<br/><br/>customize_stop_words = [<br/>    'not'<br/>]<br/><br/>for w <strong class="lp jc">in</strong> customize_stop_words:<br/>    nlp.vocab[w].is_stop = False<br/><br/>text_without_stop_words = [t.text for t <strong class="lp jc">in</strong> nlp(text) if <strong class="lp jc">not</strong> t.is_stop]<br/>display(f"Spacy text without updated stop words: <strong class="lp jc">{text_without_stop_words}</strong>")</span></pre><p id="1168" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">没有更新停用词的空白文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="a60a" class="mn mo jb lp b gy mp mq l mr ms">['movie', 'not', 'good']</span></pre><h1 id="1cfb" class="mt mo jb bd mu mv mw mx my mz na nb nc kh nd ki ne kk nf kl ng kn nh ko ni nj bi translated">正常化</h1><p id="5a54" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated">像任何数据一样，文本也需要规范化。如果是文本，则为:</p><ol class=""><li id="3ee4" class="lq lr jb ks b kt ku kw kx kz ls ld lt lh lu ll lv lw lx ly bi translated">将日期转换为文本</li><li id="7343" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">数字到文本</li><li id="03e3" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">货币/百分比符号到文本</li><li id="3827" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">缩写扩展(内容相关)NLP —自然语言处理、神经语言编程、非线性编程</li><li id="09d1" class="lq lr jb ks b kt lz kw ma kz mb ld mc lh md ll lv lw lx ly bi translated">拼写错误纠正</li></ol><p id="e12d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">总而言之，规范化是将任何非文本信息转换成文本等效信息。</p><p id="bec3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为此，有一个很棒的库——<a class="ae me" href="https://github.com/EFord36/normalise" rel="noopener ugc nofollow" target="_blank">normalize</a>。我将从这个库的自述文件中向您展示这个库的用法。这个库基于<code class="fe lm ln lo lp b">nltk</code>包，所以它需要<code class="fe lm ln lo lp b">nltk</code>单词标记。</p><p id="9096" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们假设这一步的文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="ebdc" class="mn mo jb lp b gy mp mq l mr ms">On the 13 Feb. 2007, Theresa May announced on MTV news that the rate of childhod obesity had risen from 7.3-9.6<strong class="lp jc">% i</strong>n just 3 years , costing the N.A.T.O £20m</span></pre><p id="10a2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">代码:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="6e44" class="mn mo jb lp b gy mp mq l mr ms">from normalise import normalise<br/><br/>user_abbr = {<br/>    "N.A.T.O": "North Atlantic Treaty Organization"<br/>}<br/><br/>normalized_tokens = normalise(word_tokenize(text), user_abbrevs=user_abbr, verbose=False)<br/>display(f"Normalized text: {' '.join(normalized_tokens)}")</span></pre><p id="0c4d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">输出:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="09af" class="mn mo jb lp b gy mp mq l mr ms">On the thirteenth of February two thousand and seven , Theresa May announced on M T V news that the rate of childhood obesity had risen from seven point three to nine point six % in just three years , costing the North Atlantic Treaty Organization twenty million pounds</span></pre><p id="40f9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这个库中最糟糕的事情是，目前你不能禁用一些模块，如缩写扩展，它会导致像<code class="fe lm ln lo lp b">MTV</code> - &gt; <code class="fe lm ln lo lp b">M T V</code>这样的事情。但是我已经在这个库上添加了一个适当的问题，也许过一会儿就可以修复了。</p><h1 id="38e7" class="mt mo jb bd mu mv mw mx my mz na nb nc kh nd ki ne kk nf kl ng kn nh ko ni nj bi translated">脱镁和汽蒸</h1><p id="1e23" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated"><code class="fe lm ln lo lp b">Stemming</code>是将单词的词形变化减少到其词根形式的过程，例如将一组单词映射到同一个词干，即使该词干本身在语言中不是有效单词。</p><p id="b1f1" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><code class="fe lm ln lo lp b">Lemmatization</code>与词干不同，适当减少词根变化，确保词根属于该语言。在引理化中，词根称为引理。一个词条(复数词条或词条)是一组单词的规范形式、词典形式或引用形式。</p><p id="8df3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">让我们假设这一步的文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="a600" class="mn mo jb lp b gy mp mq l mr ms">On the thirteenth of February two thousand and seven , Theresa May announced on M T V news that the rate of childhood obesity had risen from seven point three to nine point six % in just three years , costing the North Atlantic Treaty Organization twenty million pounds</span></pre><p id="e94e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLTK 词干分析器:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="4dc2" class="mn mo jb lp b gy mp mq l mr ms">import numpy as np<br/>from nltk.stem import PorterStemmer<br/>from nltk.tokenize import word_tokenize</span><span id="e9ba" class="mn mo jb lp b gy oa mq l mr ms">tokens = word_tokenize(text)<br/>porter=PorterStemmer()</span><span id="d9d1" class="mn mo jb lp b gy oa mq l mr ms"># vectorizing function to able to call on list of tokens<br/>stem_words = np.vectorize(porter.stem)</span><span id="4757" class="mn mo jb lp b gy oa mq l mr ms">stemed_text = ' '.join(stem_words(tokens))<br/>display(f"Stemed text: <strong class="lp jc">{stemed_text}</strong>")</span></pre><p id="0044" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">带词干的文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="a173" class="mn mo jb lp b gy mp mq l mr ms">On the thirteenth of februari two thousand and seven , theresa may announc on M T V news that the rate of childhood obes had risen from seven point three to nine point six % in just three year , cost the north atlant treati organ twenti million pound</span></pre><p id="796f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLTK 术语化:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="04c7" class="mn mo jb lp b gy mp mq l mr ms">import numpy as np<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.tokenize import word_tokenize</span><span id="fb79" class="mn mo jb lp b gy oa mq l mr ms">tokens = word_tokenize(text)</span><span id="137f" class="mn mo jb lp b gy oa mq l mr ms">wordnet_lemmatizer = WordNetLemmatizer()</span><span id="cecb" class="mn mo jb lp b gy oa mq l mr ms"># vectorizing function to able to call on list of tokens<br/>lemmatize_words = np.vectorize(wordnet_lemmatizer.lemmatize)</span><span id="c166" class="mn mo jb lp b gy oa mq l mr ms">lemmatized_text = ' '.join(lemmatize_words(tokens))<br/>display(f"nltk lemmatized text: <strong class="lp jc">{lemmatized_text}</strong>")</span></pre><p id="4c31" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLTK 词条化文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="5b84" class="mn mo jb lp b gy mp mq l mr ms">On the thirteenth of February two thousand and seven , Theresa May announced on M T V news that the rate of childhood obesity had risen from seven point three to nine point six % in just three year , costing the North Atlantic Treaty Organization twenty million pound</span></pre><p id="027c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">空间引理化；</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="a566" class="mn mo jb lp b gy mp mq l mr ms">import en_core_web_sm<br/><br/>nlp = en_core_web_sm.load()</span><span id="5f4a" class="mn mo jb lp b gy oa mq l mr ms">lemmas = [t.lemma_ for t <strong class="lp jc">in</strong> nlp(text)]<br/>display(f"Spacy lemmatized text: {' '.join(lemmas)}")</span></pre><p id="636d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Spacy 词条化文本:</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="5e2b" class="mn mo jb lp b gy mp mq l mr ms">On the thirteenth of February two thousand and seven , Theresa May announce on M T v news that the rate of childhood obesity have rise from seven point three to nine point six % in just three year , cost the North Atlantic Treaty Organization twenty million pound</span></pre><p id="ba87" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们看到<code class="fe lm ln lo lp b">spacy</code>比 nltk 好得多，其中一个例子<code class="fe lm ln lo lp b">risen</code> - &gt; <code class="fe lm ln lo lp b">rise</code>，只有<code class="fe lm ln lo lp b">spacy</code>处理了它。</p><h1 id="ca90" class="mt mo jb bd mu mv mw mx my mz na nb nc kh nd ki ne kk nf kl ng kn nh ko ni nj bi translated">可重复使用管道</h1><p id="1f80" class="pw-post-body-paragraph kq kr jb ks b kt nk kc kv kw nl kf ky kz nm lb lc ld nn lf lg lh no lj lk ll ij bi translated">现在是我最喜欢的部分！我们将创建一个可重用的管道，您可以在您的任何项目中使用它。</p><pre class="mf mg mh mi gt mj lp mk ml aw mm bi"><span id="cdf1" class="mn mo jb lp b gy mp mq l mr ms">import numpy as np<br/>import multiprocessing as mp<br/><br/>import string<br/>import spacy <br/>import en_core_web_sm<br/>from nltk.tokenize import word_tokenize<br/>from sklearn.base import TransformerMixin, BaseEstimator<br/>from normalise import normalise<br/><br/>nlp = en_core_web_sm.load()<br/><br/><br/>class <strong class="lp jc">TextPreprocessor</strong>(BaseEstimator, TransformerMixin):<br/>    def __init__(self,<br/>                 variety="BrE",<br/>                 user_abbrevs={},<br/>                 n_jobs=1):<br/>        <em class="ob">"""</em><br/><em class="ob">        Text preprocessing transformer includes steps:</em><br/><em class="ob">            1. Text normalization</em><br/><em class="ob">            2. Punctuation removal</em><br/><em class="ob">            3. Stop words removal</em><br/><em class="ob">            4. Lemmatization</em><br/><em class="ob">        </em><br/><em class="ob">        variety - format of date (AmE - american type, BrE - british format) </em><br/><em class="ob">        user_abbrevs - dict of user abbreviations mappings (from normalise package)</em><br/><em class="ob">        n_jobs - parallel jobs to run</em><br/><em class="ob">        """</em><br/>        self.variety = variety<br/>        self.user_abbrevs = user_abbrevs<br/>        self.n_jobs = n_jobs<br/><br/>    def fit(self, X, y=None):<br/>        return self<br/><br/>    def transform(self, X, *_):<br/>        X_copy = X.copy()<br/><br/>        partitions = 1<br/>        cores = mp.cpu_count()<br/>        if self.n_jobs &lt;= -1:<br/>            partitions = cores<br/>        elif self.n_jobs &lt;= 0:<br/>            return X_copy.apply(self._preprocess_text)<br/>        else:<br/>            partitions = min(self.n_jobs, cores)<br/><br/>        data_split = np.array_split(X_copy, partitions)<br/>        pool = mp.Pool(cores)<br/>        data = pd.concat(pool.map(self._preprocess_part, data_split))<br/>        pool.close()<br/>        pool.join()<br/><br/>        return data<br/><br/>    def _preprocess_part(self, part):<br/>        return part.apply(self._preprocess_text)<br/><br/>    def _preprocess_text(self, text):<br/>        normalized_text = self._normalize(text)<br/>        doc = nlp(normalized_text)<br/>        removed_punct = self._remove_punct(doc)<br/>        removed_stop_words = self._remove_stop_words(removed_punct)<br/>        return self._lemmatize(removed_stop_words)<br/><br/>    def _normalize(self, text):<br/>        <em class="ob"># some issues in normalise package</em><br/>        try:<br/>            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))<br/>        except:<br/>            return text<br/><br/>    def _remove_punct(self, doc):<br/>        return [t for t <strong class="lp jc">in</strong> doc if t.text <strong class="lp jc">not</strong> <strong class="lp jc">in</strong> string.punctuation]<br/><br/>    def _remove_stop_words(self, doc):<br/>        return [t for t <strong class="lp jc">in</strong> doc if <strong class="lp jc">not</strong> t.is_stop]<br/><br/>    def _lemmatize(self, doc):<br/>        return ' '.join([t.lemma_ for t <strong class="lp jc">in</strong> doc])</span></pre><p id="b556" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">此代码可用于 sklearn 管道。</p><p id="f0f3" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">测量的性能:在 22 分钟内在 4 个进程上处理了 2225 个文本。甚至没有接近快！这导致了规范化部分，库没有充分优化，但产生了相当有趣的结果，并可以为进一步的矢量化带来额外的价值，所以是否使用它取决于您。</p><p id="1f8a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我希望你喜欢这篇文章，我期待你的反馈！</p><p id="cbe9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Kaggle 内核:<a class="ae me" href="https://www.kaggle.com/balatmak/text-preprocessing-steps-and-universal-pipeline" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/balat mak/text-预处理-步骤-通用-管道</a></p></div></div>    
</body>
</html>