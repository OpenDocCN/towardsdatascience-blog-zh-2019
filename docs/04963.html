<html>
<head>
<title>Review: FC-DenseNet — One Hundred Layers Tiramisu, Fully Convolutional DenseNet (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:FC-DenseNet——一百层提拉米苏，全卷积 dense net(语义分段)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-fc-densenet-one-hundred-layer-tiramisu-semantic-segmentation-22ee3be434d5?source=collection_archive---------9-----------------------#2019-07-26">https://towardsdatascience.com/review-fc-densenet-one-hundred-layer-tiramisu-semantic-segmentation-22ee3be434d5?source=collection_archive---------9-----------------------#2019-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9be0" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胜过<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------"> SegNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------"> DeconvNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv1 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------"> DilatedNet </a></h2></div><p id="1b3c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi lc translated"><span class="l ld le lf bm lg lh li lj lk di">在</span>中，简要介绍了由蒙特利尔学习算法研究所、蒙特利尔理工大学、Imagia 公司和计算机视觉中心开发的<strong class="ki ir">全卷积 DenseNet (FC-DenseNet) </strong>。<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> DenseNet </a>最初用于图像分类。使用<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> DenseNet </a>比使用<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"> ResNet </a>有几个优点:</p><ol class=""><li id="1297" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb lq lr ls lt bi translated"><strong class="ki ir">参数效率</strong> : <a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> DenseNets </a>在参数使用上更有效率。</li><li id="9d22" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb lq lr ls lt bi translated"><strong class="ki ir">隐式深度监督</strong> : <a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> DenseNets </a>由于到架构中所有特征映射的路径较短，因此可以执行深度监督</li><li id="ec05" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb lq lr ls lt bi translated"><strong class="ki ir">特征重用</strong>:所有的层都可以很容易地访问它们之前的层，使得重用之前计算的特征地图的信息变得很容易。</li></ol><p id="bc2e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，在本文中，<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> DenseNet </a>被修改用于语义分割。并发表在<strong class="ki ir"> 2017 CVPRW </strong>上，被<strong class="ki ir"> 300 多次引用</strong>。(<a class="lz ma ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----22ee3be434d5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @ Medium)</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="3a60" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">概述</h1><ol class=""><li id="d180" class="ll lm iq ki b kj na km nb kp nc kt nd kx ne lb lq lr ls lt bi translated"><strong class="ki ir">审查</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"><strong class="ki ir">dense net</strong></a><strong class="ki ir">连接</strong></li><li id="2fe0" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb lq lr ls lt bi translated"><strong class="ki ir"> FC-DenseNet 架构</strong></li><li id="b6bc" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb lq lr ls lt bi translated"><strong class="ki ir">实验</strong></li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="7a3a" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak"> 1。</strong>审查<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------">DenseNet</a>T24】连接</h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/10ec83edfc51f851df4cfdb11d4e60a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*ZyXRdegQ5JLR_nW00broFg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Standard Convolution</strong></figcaption></figure><ul class=""><li id="749d" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated"><strong class="ki ir">标准卷积</strong> : <em class="nt"> xl </em>通过对前一层<em class="nt"> xl </em> -1 的输出应用非线性变换<em class="nt"> Hl </em>来计算。</li></ul><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b42e1d67b31d6992fadb8b2b2bcf8c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*TSVobJ2EYhT56fwjCIPpSQ.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Residual Learning</strong></figcaption></figure><ul class=""><li id="0f6e" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated"><strong class="ki ir">残差学习</strong> : <a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8?source=post_page---------------------------"> ResNet </a>引入了一个残差块，它将一个层的输入到输出的同一性映射相加。</li></ul><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/24aecad81b20921854300942b2ff969a.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*CZMNqy54ZMuRM-u7GabSQA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"><strong class="bd nr">DenseNet</strong></a><strong class="bd nr"> Connection</strong></figcaption></figure><ul class=""><li id="13fb" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> <strong class="ki ir"> DenseNet </strong> </a>:输入以前馈方式连接所有先前的特征输出进行卷积。</li><li id="f663" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">(更多细节，请随意阅读我的<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------"> DenseNet </a>评论。)</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="fa14" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak"> 2。FC-DenseNet 架构</strong></h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/60e293ff98782eef7d4da9d0760f439a.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*5Bqcgzl6JDXrScL1RXd6Ag.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">FC-DenseNet</strong></figcaption></figure><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi nx"><img src="../Images/0286b88dab98a6b15d1fe99a02b22281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Pj56mTHPNha8Pg58fWJEQ.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">One Layer (Lefft), Transition Down (TD) (Middle), Transition Up (TU) (Right)</strong></figcaption></figure><ul class=""><li id="68d6" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated">在 FC-DenseNet 中，<strong class="ki ir">仅对前一密集块创建的特征图进行上采样</strong>。否则，将导致非常大的计算量和参数数量。</li><li id="af14" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">这意味着，密集块的输入不会与其输出连接在一起。因此，转置卷积仅应用于由最后一个密集块获得的特征图，而不是到目前为止连接的所有特征图。</li><li id="6df9" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">这导致合理的前 softmax 特征地图数量为 256。</li><li id="f90b" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated"><strong class="ki ir">跳过连接</strong>用于从下采样路径到上采样路径，就像<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------"> U-Net </a>或<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a>类网络。</li><li id="059f" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">这个模型可以从零开始<strong class="ki ir">训练，不需要任何预训练</strong>。</li></ul><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/06d5ba8ccaf811c1e2d6fed8af633695.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*F7CrNY8IPDtaXd-psASpvg.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Example of FC-DenseNet103</strong></figcaption></figure><ul class=""><li id="6c93" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated">以上是 FC-DenseNet103 的详细情况。</li><li id="3eb7" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated"><em class="nt"> m </em>对应于一个块末端的特征图总数。</li><li id="6652" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated"><em class="nt"> c </em>代表班级人数。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="1c03" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">3.实验</h1><h2 id="3153" class="od mj iq bd mk oe of dn mo og oh dp ms kp oi oj mu kt ok ol mw kx om on my oo bi translated">3.1.坎维德</h2><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi op"><img src="../Images/fc0e0fcc1d142f94c5042fb0a27bd645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sBO8nrNrH4IbcXxsz_8vpQ.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Results on CamVid Dataset</strong></figcaption></figure><ul class=""><li id="af1d" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated"><strong class="ki ir"> CamVid </strong>:用于城市场景理解的全分割视频数据集。有 367 帧用于训练，101 帧用于验证，233 帧用于测试。每个帧的大小为 360×480，其像素被标记为 11 个语义类别。</li><li id="c48a" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">FC-DenseNets 用 224×224 的作物和批量 3 进行训练。最后，用全尺寸图像对模型进行微调。没有时间平滑或任何后处理时间正则化。</li><li id="2b2d" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated"><strong class="ki ir"> FC-DenseNet56 </strong> : 56 层，每密块 4 层，增长率 12。</li><li id="8fe7" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated"><strong class="ki ir"> FC-DenseNet67 </strong> : 67 层，每密块 5 层，增长率 16。</li><li id="03d9" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated"><strong class="ki ir"> FC-DenseNet103 </strong> : 103 层，增长率 16。</li><li id="dbd5" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">经典采样:一种在上采样路径中使用标准卷积而不是密集块的架构。</li><li id="f9db" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">结果显示了所提出的上采样路径相对于经典上采样路径的明显优势。特别是，据观察，未被代表的类别明显受益于 FC-DenseNet 架构，即标志、行人、栅栏、骑自行车的人在性能方面经历了重要的提升(从 15%到 25%)。</li><li id="fba0" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">FC-DenseNet 模型受益于更深的深度和更多的参数。</li><li id="35dc" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">FC-DenseNet 最终胜过了最先进的方法，如<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------"> SegNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------"> DeconvNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------"> FCN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------"> DeepLabv1 </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------"> DilatedNet </a>。</li></ul><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="ny nz di oa bf ob"><div class="gh gi oq"><img src="../Images/dc03d376c707ed072f1375b1d7b604a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*78fged7Dre65XShbaBNN7w.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Visualization: Original (Left), Ground Truth (Middle), FC-DenseNet (Right)</strong></figcaption></figure><h2 id="d7d9" class="od mj iq bd mk oe of dn mo og oh dp ms kp oi oj mu kt ok ol mw kx om on my oo bi translated">3.2.盖特奇</h2><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/58f435a53d4567bc2cefc084a5b0f7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*_oBFDnlfdMa4OaGHc-AgTA.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk"><strong class="bd nr">Results on Gatech Dataset</strong></figcaption></figure><ul class=""><li id="c8e7" class="ll lm iq ki b kj kk km kn kp ln kt lo kx lp lb ns lr ls lt bi translated"><strong class="ki ir"> Gatech </strong>:几何场景理解数据集，由 63 个用于训练/验证的视频和 38 个用于测试的视频组成。</li><li id="cd80" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">数据集中有 8 个类:天空、地面、建筑物、多孔(主要是树)、人类、汽车、垂直混合和主混合。</li><li id="dcaa" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">FC-DenseNet103 模型在 CamVid 上进行预训练，删除了 softmax 层，并使用 224×224 的作物和批量 5 对其进行了 10 个时期的微调。</li><li id="9f17" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">FC-DenseNet103 从零开始，相对于之前发表的 2D 卷积、2D-V2V 的最新水平，在全局精度方面给出了 23.7%的令人印象深刻的改进。</li><li id="fa8f" class="ll lm iq ki b kj lu km lv kp lw kt lx kx ly lb ns lr ls lt bi translated">此外，FC-DenseNet(仅用 2D 卷积训练)也实现了基于时空 3D 卷积(3D-V2V 预训练)的最先进模型的显著改善(3.4%的改善)。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="542d" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">参考</h1><p id="1a24" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp os kr ks kt ot kv kw kx ou kz la lb ij bi translated">【2017 CVP rw】【FC-dense net】<br/><a class="ae kf" href="https://arxiv.org/abs/1611.09326" rel="noopener ugc nofollow" target="_blank">百层提拉米苏:用于语义分割的全卷积 dense net</a></p><h1 id="1828" class="mi mj iq bd mk ml ov mn mo mp ow mr ms jw ox jx mu jz oy ka mw kc oz kd my mz bi translated">我以前的评论</h1><p id="b9d5" class="pw-post-body-paragraph kg kh iq ki b kj na jr kl km nb ju ko kp os kr ks kt ot kv kw kx ou kz la lb ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(我)(们)(还)(没)(有)(什)(么)(好)(的)(情)(感)(,)(但)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(我)(们)(都)(不)(想)(要)(让)(这)(些)(人)(都)(有)(这)(些)(情)(况)(,)(我)(们)(还)(不)(想)(要)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(就)(是)(这)(些)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(都)(是)(很)(强)(的)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(起)(来)(,)(我)(们)(都)(是)(很)(强)(的)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(对)(</p><p id="431e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">物体检测</strong> [ <a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754?source=post_page---------------------------" rel="noopener">过食</a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1?source=post_page---------------------------" rel="noopener"> R-CNN </a> ] [ <a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba?source=post_page---------------------------" rel="noopener">快 R-CNN </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202?source=post_page---------------------------">快 R-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=post_page---------------------------">MR-CNN&amp;S-CNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6?source=post_page---------------------------">DeepID-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=post_page---------------------------">CRAFT</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c?source=post_page---------------------------">R-FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------">离子</a><a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766?source=post_page---------------------------"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=post_page---------------------------">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151?source=post_page---------------------------" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11?source=post_page---------------------------">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5?source=post_page---------------------------">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89?source=post_page---------------------------">yolo v1</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65?source=post_page---------------------------">yolo v2/yolo 9000</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=post_page---------------------------">yolo v3</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610?source=post_page---------------------------">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4?source=post_page---------------------------">retina net</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=post_page---------------------------">DCN</a></p><p id="1bd2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">语义切分</strong>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1?source=post_page---------------------------">FCN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e?source=post_page---------------------------">de convnet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d?source=post_page---------------------------">deeplabv 1&amp;deeplabv 2</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c?source=post_page---------------------------">CRF-RNN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96?source=post_page---------------------------">SegNet</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990?source=post_page---------------------------" rel="noopener">parse net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5?source=post_page---------------------------">dilated net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5?source=post_page---------------------------">DRN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1?source=post_page---------------------------">RefineNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=post_page---------------------------"/></p><p id="7cfc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">生物医学图像分割</strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6?source=post_page---------------------------" rel="noopener">cumevision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560?source=post_page---------------------------" rel="noopener">cumevision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760?source=post_page---------------------------">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6?source=post_page---------------------------" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43?source=post_page---------------------------" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc?source=post_page---------------------------">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974?source=post_page---------------------------">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1?source=post_page---------------------------">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1?source=post_page---------------------------">M FCN</a>]</p><p id="7d0b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">实例分割</strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b?source=post_page---------------------------" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979?source=post_page---------------------------">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339?source=post_page---------------------------">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61?source=post_page---------------------------">清晰度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413?source=post_page---------------------------">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34?source=post_page---------------------------"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92?source=post_page---------------------------">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2?source=post_page---------------------------"> FCIS </a></p><p id="3f59" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">)( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )(</p><p id="ba15" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------"> DeepPose </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------"> Tompson NIPS'14 </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------"> Tompson CVPR'15 </a> <a class="ae kf" href="https://medium.com/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------" rel="noopener"> CPM </a></strong></p></div></div>    
</body>
</html>