<html>
<head>
<title>Entity embedding using t-SNE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 t-SNE 的实体嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/entity-embedding-using-t-sne-973cb5c730d7?source=collection_archive---------17-----------------------#2019-07-28">https://towardsdatascience.com/entity-embedding-using-t-sne-973cb5c730d7?source=collection_archive---------17-----------------------#2019-07-28</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="f84e" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">众所周知的降维工具可以用于分类特征嵌入。</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/3d99b60da35fc9b8e1a81c4d3c413e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Fn0OJ-GM0EoXLqJNctuCw.jpeg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@leonkoye?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Leon Koye</a> on <a class="ae kz" href="https://unsplash.com/search/photos/dimension?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="950a" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">我们已经讨论了一些嵌入分类特征的可能方法:<a class="ae kz" rel="noopener" target="_blank" href="/entity-embedding-using-pca-and-kernel-pca-798b8b2e8c2f">核 PCA </a>和<a class="ae kz" rel="noopener" target="_blank" href="/spectral-encoding-of-categorical-features-b4faebdf4a">光谱编码</a>。这种嵌入的目标是将分类特征映射到低维空间中的向量。与 1-hot 编码相比，这种映射的优点是大大减少了过拟合。然而，如果嵌入选择不正确，我们可能会丢失信息并使学习更加困难。为了增加嵌入的质量，我们使用类别相似度信息(我们可以设定<em class="lw">先验</em>或通过计算条件概率分布的相似度)。核 PCA 方法也使用分类变量的概率分布，而谱编码不使用。为了完整性，我们也将使用 t-SNE 方法，我们可以讨论它的优点和缺点。</p><p id="daff" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">t-SNE 代表<a class="ae kz" href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" rel="noopener ugc nofollow" target="_blank">t-分布式随机邻居嵌入</a>，最初是为了高维数据的可视化而提出的。与 PCA 相反，它是一种非线性方法，其目标是保持数据点之间的相似性。它引入了从原始数据<strong class="lc iv"> x </strong>到低维向量<strong class="lc iv"> y </strong>的非线性转换，并使用学生 t 分布对数据点之间的相似性进行建模:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj lx"><img src="../Images/aa2a73be3a02eb03acc9dfc79e5c3c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*gyUKQFVFAaEWzph5L9hJTQ.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Credit: Wikipedia</figcaption></figure><p id="0e6b" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">点集<strong class="lc iv"> y </strong>通过最小化分布 Q 与原始分布 P 的 KL 散度来找到:</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ly"><img src="../Images/348522612550b01d2795b4c2c3609ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*gIdKS0i14IQfZkrBxhnd_Q.png"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Credit: Wikipedia</figcaption></figure><p id="ce8d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">最小化可以使用梯度下降来完成。</p><p id="f331" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">为了演示该方法如何工作，我们将使用只有一个分类变量星期几的合成数据集，0 表示星期一，6 表示星期日:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lz ma l"/></div></figure><pre class="kk kl km kn gu mb mc md me aw mf bi"><span id="e100" class="mg mh iu mc b gz mi mj l mk ml">array([ 23,  19,  15,  19,  22, 429, 473])</span></pre><p id="cb0f" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在这里，我们特意添加了更多周六和周日的数据，以了解 t-SNE 将如何考虑原始数据的概率分布。提醒你一下，拉普拉斯方法不受源数据概率分布的影响，而主成分分析和核主成分分析则受影响。</p><p id="9eb9" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">内核函数与前面示例中的相同</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lz ma l"/></div></figure><pre class="kk kl km kn gu mb mc md me aw mf bi"><span id="9ad8" class="mg mh iu mc b gz mi mj l mk ml">array([[1. , 0.9, 0.8, 0.7, 0.5, 0.1, 0.3],<br/>       [0.9, 1. , 0.9, 0.8, 0.5, 0.1, 0.2],<br/>       [0.8, 0.9, 1. , 0.9, 0.5, 0.1, 0.2],<br/>       [0.7, 0.8, 0.9, 1. , 0.6, 0.1, 0.2],<br/>       [0.5, 0.5, 0.5, 0.6, 1. , 0.7, 0.5],<br/>       [0.1, 0.1, 0.1, 0.1, 0.7, 1. , 0.8],<br/>       [0.3, 0.2, 0.2, 0.2, 0.5, 0.8, 1. ]])</span></pre><p id="31f0" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">t-SNE 的 Scikit-learn 实现实际上希望我提供度量函数，所以我们将内核矩阵转换回距离矩阵</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lz ma l"/></div></figure><pre class="kk kl km kn gu mb mc md me aw mf bi"><span id="0580" class="mg mh iu mc b gz mi mj l mk ml">array([[-0.        ,  0.21072103,  0.4462871 ,  0.71334989,  1.38629436,  4.60517019,  2.40794561],<br/>       [ 0.21072103, -0.        ,  0.21072103,  0.4462871 ,  1.38629436,  4.60517019,  3.21887582],<br/>       [ 0.4462871 ,  0.21072103, -0.        ,  0.21072103,  1.38629436,  4.60517019,  3.21887582],<br/>       [ 0.71334989,  0.4462871 ,  0.21072103, -0.        ,  1.02165125,  4.60517019,  3.21887582],<br/>       [ 1.38629436,  1.38629436,  1.38629436,  1.02165125, -0.        ,  0.71334989,  1.38629436],<br/>       [ 4.60517019,  4.60517019,  4.60517019,  4.60517019,  0.71334989, -0.        ,  0.4462871 ],<br/>       [ 2.40794561,  3.21887582,  3.21887582,  3.21887582,  1.38629436,  0.4462871 , -0.        ]])</span></pre><p id="09da" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里的适马是一个需要调整的重要超参数。t-SNE 方法的另一个重要的超参数是困惑分数，但在我们的情况下，它可以保留为默认值或设置为 sigma。</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lz ma l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mm"><img src="../Images/3c680c694c909538bbd833e0e6ea04a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*Lba3NZNZFmRy7ytJqNzfiw.png"/></div></figure><p id="b75d" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">在第一个例子中，我们使用了没有核的 t-SNE，这意味着一周中所有日子之间的距离是 1。这就是你仅仅使用 1-hot 编码所得到的结果。请注意，嵌入似乎是随机分布的，这与 PCA 不同，PCA 即使在没有核的情况下也使用初始数据概率分布来嵌入数据，这样可以最大化它们的线性可分性。让我们看看如果使用核函数会得到什么结果:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="lz ma l"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj mn"><img src="../Images/99ec90659e4c98d1717058684a10e6ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*k6nrqLOZOWDbwf43TDybFw.png"/></div></figure><p id="6055" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">这里的结果更有意义。星期一到星期四被分组在一起以保持它们的相似性。星期五、星期六和星期天彼此相距甚远，与其他日子也相距甚远。有趣的是，星期五比星期六更接近星期天。</p><h1 id="7307" class="mo mh iu bd mp mq mr ms mt mu mv mw mx ka my kb mz kd na ke nb kg nc kh nd ne bi translated">结论和讨论</h1><p id="f634" class="pw-post-body-paragraph la lb iu lc b ld nf jv lf lg ng jy li lj nh ll lm ln ni lp lq lr nj lt lu lv in bi translated">t-SNE 是类别嵌入的选项之一。通常，t-SNE 不会进行概化(因为它将无法绘制未知数据)，但对于分类特征来说，这并不重要，因为新数据将具有现有类别之一。一个更大的问题是，如果没有内核，它将无法工作，这与 PCA 相反，即使没有给定内核，PCA 也会给出合理的结果。就像核 PCA 一样，它使用概率分布来映射数据，即使是以不同的方式。如果核 PCA 试图在结果向量空间中实现平滑，t-SNE 更关心保持相似性。这可能会指导您决定哪种方法更适合您的模型。</p><p id="0bf6" class="pw-post-body-paragraph la lb iu lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv in bi translated">你可以在我的 github 库中找到这个故事的代码。</p></div></div>    
</body>
</html>