<html>
<head>
<title>Introduction to PyTorch BigGraph — with Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch BigGraph 简介—带示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-pytorch-biggraph-with-examples-b50ddad922b8?source=collection_archive---------6-----------------------#2019-06-21">https://towardsdatascience.com/introduction-to-pytorch-biggraph-with-examples-b50ddad922b8?source=collection_archive---------6-----------------------#2019-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/5c832fbef978a82bf9d9bba783b26b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69d8TcUidbReBRh_i5kheQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Network Photo by <a class="ae kf" href="https://unsplash.com/@alinnnaaaa?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Alina Grubnyak</a> on <a class="ae kf" href="https://unsplash.com/search/photos/network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b02f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PyTorch BigGraph 是一个为机器学习创建和处理大型图形嵌入的工具。目前在基于图的神经网络中有两种方法:</p><ul class=""><li id="5281" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">直接使用图形结构，并将其输入神经网络。然后在每一层都保留图形结构。graphCNNs 使用这种方法，例如参见<a class="ae kf" href="https://medium.com/@svenbalnojan/using-graph-cnns-in-keras-8b9f685c4ed0" rel="noopener">我的帖子</a>或<a class="ae kf" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</li><li id="4554" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">但是大多数图表都太大了。所以创建图的大嵌入也是合理的。然后把它作为传统神经网络的特征。</li></ul><p id="2dd2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PyTorch BigGraph 处理第二种方法，下面我们也将这样做。仅供参考，让我们先讨论一下尺寸。图通常由它们的<a class="ae kf" href="https://en.wikipedia.org/wiki/Adjacency_matrix" rel="noopener ugc nofollow" target="_blank">邻接矩阵</a>编码。如果你有一个有 3000 个节点的图，每个节点之间有一条边，那么你的矩阵中就有大约 10000000 个条目。即使这很少，但根据上面链接的论文中的<a class="ae kf" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">,这显然会使大多数 GPU 崩溃。</a></p><p id="2600" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想一想推荐系统中常用的图表，你会发现它们通常要大得多。现在已经有一些关于 BigGraph 的方式和原因的优秀帖子，所以我不会在这方面花更多时间。我对将 BigGraph 应用于我的机器学习问题很感兴趣，为此我喜欢举最简单的例子并让事情运转起来。我构建了两个例子，我们将一步一步来看。</p><p id="d03c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">整个代码都经过了重构，可以在<a class="ae kf" href="https://github.com/sbalnojan/biggraph-examples" rel="noopener ugc nofollow" target="_blank"> GitHub </a>获得。它改编自 BigGraph 存储库中的示例。</p><p id="0170" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一个示例是 LiveJournal 图表的一部分，数据如下所示:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ccbe" class="mb mc it lx b gy md me l mf mg"># FromNodeId ToNodeId</span><span id="615d" class="mb mc it lx b gy mh me l mf mg">0 1<br/>0 2<br/>0 3<br/>...<br/>0 10<br/>0 11<br/>0 12<br/>...<br/>0 46<br/>1 0<br/>...</span></pre><p id="f729" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二个例子是简单的带边的 8 个节点:</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="76a7" class="mb mc it lx b gy md me l mf mg"># FromNodeId ToNodeId<br/>0   1<br/>0   2<br/>0   3<br/>0   4<br/>1   0<br/>1   2<br/>1   3<br/>1   4<br/>2   1<br/>2   3<br/>2   4<br/>3   1<br/>3   2<br/>3   4<br/>3   7<br/>4   1<br/>5   1<br/>6   2<br/>7   3</span></pre><h1 id="5d45" class="mi mc it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated"><strong class="ak">嵌入 LiveJournals 图形的一部分</strong></h1><p id="7766" class="pw-post-body-paragraph kg kh it ki b kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">BigGraph 是为机器的内存限制而设计的，所以它是完全基于文件的。您必须触发进程来创建适当的文件结构。如果你想再次运行一个例子，你必须删除检查点。我们还必须预先分成训练和测试，再次以文件为基础。文件格式为 TSV，用制表符分隔值。</p><p id="b90b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们开始吧。第一段代码声明了两个助手函数，取自 BigGraph 源代码，设置了一些常量并运行文件分割。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">helper functions and random_split_file call.</figcaption></figure><p id="935f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这通过创建两个文件<em class="nm"> data/example_1/test.txt </em>和<em class="nm"> train.txt </em>将边分成测试和训练集。接下来，我们使用 BigGraphs 转换器为数据集创建基于文件的结构。我们将“分区”成 1 个分区。为此，我们已经需要部分配置文件。这是配置文件的相关部分，I/O 数据部分和图形结构。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="fb9a" class="mb mc it lx b gy md me l mf mg">entities_base = 'data/example_1' </span><span id="623a" class="mb mc it lx b gy mh me l mf mg">def get_torchbiggraph_config():     </span><span id="af53" class="mb mc it lx b gy mh me l mf mg">config = dict(       <br/>         # I/O data<br/>        entity_path=entities_base,<br/>        edge_paths=[],<br/>        checkpoint_path='model/example_1',</span><span id="cceb" class="mb mc it lx b gy mh me l mf mg">         # Graph structure<br/>        entities={<br/>            'user_id': {'num_partitions': 1},<br/>        },<br/>        relations=[{<br/>            'name': 'follow',<br/>            'lhs': 'user_id',<br/>            'rhs': 'user_id',<br/>            'operator': 'none',<br/>        }],<br/>...</span></pre><p id="5ca4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这告诉 BigGraph 在哪里可以找到我们的数据，以及如何解释我们的制表符分隔的值。有了这个配置，我们可以运行下一个 Python 代码片段。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">convert data to _partitioned data.</figcaption></figure><p id="68ee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果应该是数据目录中的一堆新文件，即:</p><ul class=""><li id="c095" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">两个文件夹<em class="nm"> test_partitioned，train_partitioned </em></li><li id="fb24" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">h5 格式的边缘每个文件夹一个文件，用于快速部分读取</li><li id="b7fd" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><em class="nm"> dictionary.json </em>文件包含“user_ids”和新分配的 id 之间的映射。</li><li id="106c" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">entity_count_user_id_0.txt 包含实体计数，在本例中为 47。</li></ul><p id="2971" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">dictionary.json 对于稍后将 BigGraph 模型的结果映射到我们想要的实际嵌入非常重要。准备够了，我们来训练嵌入。看一下<em class="nm"> config_1.py </em>，它包含三个相关的部分。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="ce1a" class="mb mc it lx b gy md me l mf mg">        # Scoring model - the embedding size<br/>        dimension=1024,<br/>        global_emb=False,</span><span id="447c" class="mb mc it lx b gy mh me l mf mg">         # Training - the epochs to train and the learning rate<br/>        num_epochs=10,<br/>        lr=0.001,</span><span id="c87c" class="mb mc it lx b gy mh me l mf mg">         # Misc - not important<br/>        hogwild_delay=2,<br/>    )</span><span id="4b48" class="mb mc it lx b gy mh me l mf mg">     return config</span></pre><p id="da16" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了进行训练，我们运行以下 Python 代码。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">train the embedding.</figcaption></figure><p id="8fd6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过这段代码，我们可以根据测试集上预先安装的一些指标来评估模型。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">evaluate the embedding.</figcaption></figure><p id="1e6c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们尝试检索实际的嵌入。同样，因为一切都是基于文件的，所以它现在应该位于<em class="nm"> models/ </em>文件夹中的 h5 位置。我们可以通过在字典中查找用户 0 的映射来加载用户 0 的嵌入，如下所示:</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">output the embedding.</figcaption></figure></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="0610" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们转到第二个例子，一个构造好的例子，我们希望可以在这个例子上做一些有用的事情。liveJournal 数据实在太大了，无法在合理的时间内浏览一遍。</p><h1 id="7189" class="mi mc it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated"><strong class="ak">对构建示例的链接预测和排序</strong></h1><p id="e5dc" class="pw-post-body-paragraph kg kh it ki b kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">好的，我们将重复第二个例子的步骤，除了我们将产生一个 10 维的嵌入，所以我们可以查看和使用它。此外，对我来说，10 维对 8 个顶点来说已经足够了。我们在<em class="nm"> config_2.py </em>中设置这些东西。</p><pre class="ls lt lu lv gt lw lx ly lz aw ma bi"><span id="29cb" class="mb mc it lx b gy md me l mf mg">entities_base = 'data/example_2'<br/> def get_torchbiggraph_config():<br/>     config = dict(<br/>        # I/O data<br/>        entity_path=entities_base,<br/>        edge_paths=[],<br/>        checkpoint_path='model/example_2',</span><span id="af04" class="mb mc it lx b gy mh me l mf mg">        # Graph structure<br/>        entities={<br/>            'user_id': {'num_partitions': 1},<br/>        },<br/>        relations=[{<br/>            'name': 'follow',<br/>            'lhs': 'user_id',<br/>            'rhs': 'user_id',<br/>            'operator': 'none',<br/>        }],</span><span id="7ed9" class="mb mc it lx b gy mh me l mf mg">         # Scoring model<br/>        dimension=10,<br/>        global_emb=False,</span><span id="9f4a" class="mb mc it lx b gy mh me l mf mg">         # Training<br/>        num_epochs=10,<br/>        lr=0.001,</span><span id="6698" class="mb mc it lx b gy mh me l mf mg">         # Misc<br/>        hogwild_delay=2,<br/>    )<br/>     return config</span></pre><p id="626f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们像以前一样运行相同的代码，但是一次完成，处理不同的文件路径和格式。在这种情况下，我们在数据文件顶部只有 3 行注释:</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="5dc9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为最终输出，你应该得到一堆东西，特别是所有的嵌入。让我们做一些嵌入的基本任务。当然，我们现在可以使用它，并将其加载到我们喜欢的任何框架中，<em class="nm"> keras </em>，<em class="nm"> tensorflow </em>，但是 BigGraph 已经为常见任务带来了一些实现，如<strong class="ki iu">链接预测</strong>和<strong class="ki iu">排名</strong>。所以让我们试一试。第一个任务是<strong class="ki iu">链路预测</strong>。我们预测实体<strong class="ki iu"> 0-7 </strong>和<strong class="ki iu">0-1</strong>的得分，因为我们从数据中知道<strong class="ki iu">0-1</strong>的可能性更大。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="b8f2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为比较器，我们加载了“DotComparator ”,它计算两个 10 维向量的点积或标量积。结果显示输出的数字很小，但至少 score_2 比 score_1 高得多，正如我们所预期的那样。</p><p id="dead" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，作为最后一段代码，我们可以生成一个相似项目的排名，它使用与前面相同的机制。我们使用标量积来计算嵌入到所有其他实体的距离，然后对它们进行排序。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="cdac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，顶级实体的顺序是 0、1、3、7……如果你观察数据，就会发现这似乎非常正确。</p><h1 id="4e41" class="mi mc it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">更有趣</h1><p id="0bde" class="pw-post-body-paragraph kg kh it ki b kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">这是我能想到的最基本的例子。我没有在 freebase 数据或 LiveJournal 数据上运行原始示例，只是因为它们需要相当长的训练时间。您可以在这里找到代码和参考资料:</p><ul class=""><li id="c361" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">PyTorch BigGraph 的<a class="ae kf" href="https://github.com/facebookresearch/PyTorch-BigGraph" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a></li><li id="3376" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://github.com/sbalnojan/biggraph-examples" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>带示例代码</li><li id="2660" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://arxiv.org/pdf/1903.12287.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1903.12287.pdf</a>，a .勒尔等人。艾尔。(2019)，PyTorch-BigGraph:大规模图嵌入系统。</li><li id="d715" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1609.02907</a>，T. N .基普夫，m .韦林(2016)，利用图卷积网络的半监督分类。</li></ul><h1 id="8faf" class="mi mc it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">你可能遇到的问题</h1><p id="2419" class="pw-post-body-paragraph kg kh it ki b kj nf kl km kn ng kp kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我在 mac 上运行代码，遇到了三个问题:</p><ul class=""><li id="8a2b" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">说明<em class="nm">“lib *…”的错误..原因:未找到映像:“</em>解决方案是安装缺少的部分，例如使用<em class="nm">“brew install libomp”</em></li><li id="619d" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">然后我遇到了一个错误<em class="nm">“attribute error:模块‘torch’没有属性' _ six '”</em>，这可能只是因为不兼容的 python &amp; torch 版本。反正我是从<em class="nm"> python 3.6 &amp;火炬 1.1</em>=&gt;<em class="nm">python 3.7&amp;火炬 1。X </em>解决了我的问题。</li><li id="95a0" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">在您继续之前，检查 train.txt 和 test.txt，我在测试时看到那里有一些丢失的新行。</li></ul><p id="98ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望这有所帮助，并且玩起来很有趣！</p></div></div>    
</body>
</html>