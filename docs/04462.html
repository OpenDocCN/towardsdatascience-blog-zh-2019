<html>
<head>
<title>Gini Index vs Information Entropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基尼指数与信息熵</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb?source=collection_archive---------1-----------------------#2019-07-10">https://towardsdatascience.com/gini-index-vs-information-entropy-7a7e4fed3fcb?source=collection_archive---------1-----------------------#2019-07-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="df09" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于杂质测量和信息增益，您需要了解的一切</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0a47ef580865fdca7486ac996b8d8221.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cq038u_GgnYwRkgrVD1KgA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://unsplash.com/photos/WahfNoqbYnM" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="5b40" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">简介:</h1><p id="d7e0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">对数据科学家来说，杂质/信息增益的度量，尤其是基尼指数和熵，是有趣且实用的概念。下面我们将通过简单易懂的例子来深入探讨这些概念。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="6784" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">目录</h1><ul class=""><li id="b824" class="mz na it lt b lu lv lx ly ma nb me nc mi nd mm ne nf ng nh bi translated">背景</li><li id="d7d7" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">基尼直觉</li><li id="d411" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">熵直觉</li><li id="7503" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">使用 python 进行可视化</li><li id="cf1e" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">信息增益比较</li><li id="67ba" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm ne nf ng nh bi translated">实用的外卖</li></ul></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="444a" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">背景:</h1><p id="256d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">决策树根据目标变量的纯度递归分割特征。该算法旨在找到最具预测性特征的最佳点，以便将 1 个数据集分成 2 个数据集。这两个新数据集的目标变量将比原始数据集的更纯。</p><p id="a683" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">然而，“纯粹”是这里的关键词。这个词到底是什么意思？在一般意义上,“纯洁”可以被认为是一个群体的同质化程度。但是同质性可能意味着不同的东西，这取决于你的决策树运行在哪个数学主干上。决策树的<em class="ns">决策</em>的两个最受欢迎的支柱是基尼指数和信息熵。</p><p id="0c4a" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><strong class="lt iu">下面这三个例子应该能让大家明白这一点:</strong></p><blockquote class="nt nu nv"><p id="284a" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">如果我们有 4 个红色口香糖球和 0 个蓝色口香糖球，那么这 4 个口香糖球就是 100%纯的。</p><p id="2d74" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">如果我们有 2 个红色和 2 个蓝色，那么这个组是 100%不纯的。</p><p id="2764" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">如果我们有 3 个红色和 1 个蓝色，如果我们分别使用基尼系数或熵，则该组的纯度为 75%或 81%。</p></blockquote><p id="957d" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">为什么这很重要？根据使用的杂质测量，树分类结果可能会有所不同。这可能对您的模型产生很小(有时很大)的影响。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="8d2c" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">基尼指数直觉:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/273b527aa59ec33081736ac20abcf4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*otdoiyIwxJI-UV0ukkyutw.png"/></div></figure><p id="0efb" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">让我们从基尼指数开始，因为它更容易理解。根据<a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">维基百科</a>，目标是<em class="ns">“测量从集合中随机选择的元素被错误标记的频率”[1]。</em></p><p id="e9fd" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">为了形象化，让我们回到口香糖的例子。如果我们决定任意将 4 个口香糖球都标记为红色，其中一个口香糖球被错误标记的频率是多少？</p><h2 id="8590" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">4 红色和 0 蓝色:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/886422aa4211e5715861dd04c6813ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*erKcPX4hLgOadCyoyNVlTQ.png"/></div></div></figure><p id="e84d" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">杂质测量值为 0，因为我们永远不会错误地标记这里的 4 个红色口香糖球中的任何一个。如果我们任意选择将所有的球标为“蓝色”，那么我们的指数仍然是 0，因为我们总是错误地给口香糖球标上标签。</p><blockquote class="nt nu nv"><p id="6ea8" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">无论你选择哪个阶层的概率，基尼系数总是一样的，因为在上面的公式中，基尼系数总是等于 0。</p></blockquote><p id="bb1d" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">基尼系数为 0 是最纯粹的分数。</p><h2 id="75be" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated"><strong class="ak"> 2 红 2 蓝:</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/32d8104af29cc70bfb2447675b34fd8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpi08Qustg9vSKYcL-UUWQ.png"/></div></div></figure><p id="772e" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">杂质测量值为 0.5，因为我们有一半时间会错误地给口香糖球贴上错误的标签。因为该指数用于二元目标变量(0，1)，0.5 的基尼指数是最不纯粹的分数。一半是一种类型，一半是另一种类型。<strong class="lt iu">将基尼系数除以 0.5，有助于直观理解该系数代表什么。0.5/0.5 = 1，意味着分组尽可能不纯(在只有 2 个结果的组中)。</strong></p><h2 id="4874" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">3 红色和 1 蓝色:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/5381b9d0bb40e43f4dffc5a8af11a41f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2TokVtgv-dbBYy3cg8EigA.png"/></div></div></figure><p id="d886" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">这里的杂质测量值是 0.375。为了更直观的理解，如果我们将此除以 0.5，我们将得到 0.75，这是错误/正确标记的概率。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="d15d" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">熵直觉:</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3d06720c50697898782c39bc83d14fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*-ZZAH1q_DHbVxobuXKY2ng.png"/></div></figure><p id="d149" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">由于等式中的对数，熵的计算量更大。像基尼一样，基本思想是通过目标变量来衡量一个群体的无序程度。这种方法不是利用简单的概率，而是采用概率的对数基数 2(然而，只要你是一致的，你可以使用任何对数基数)。熵方程使用对数，因为它有许多有利的性质。主要优点是它提供的附加性能。这些麻省理工学院的讲座<a class="ae ky" href="http://web.mit.edu/6.02/www/f2011/handouts/2.pdf" rel="noopener ugc nofollow" target="_blank">笔记</a>将有助于更清楚地掌握这个概念(pg8) [2]。</p><p id="950f" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">让我们用同样的口香糖场景来想象熵是如何工作的:</p><h2 id="215d" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">4 红色和 0 蓝色:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/4409e2204729faa1daec69e5eeca5a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVWAeM4YuboxpgTnbXr8MA.png"/></div></div></figure><p id="fac3" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">不出所料，熵的杂质测量值也是 0。这是使用信息熵的最大纯度分数。</p><h2 id="1550" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">2 个红色和 2 个蓝色:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/462f78f013724bfc93476199e3199bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*slKimbAlcP3F5_fOl0cb4A.png"/></div></div></figure><p id="4574" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">此处的杂质测量值为 1，因为这是可获得的最大杂质。</p><h2 id="df73" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">3 红色和 1 蓝色:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/6bc4cbc54bef29cb6e031bf09d1c101c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VuvJX7gw44VDWF-MyuBo6A.png"/></div></div></figure><p id="53c0" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">这里的纯度/杂质测量值是 0.811，比基尼系数差一点。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="87ab" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">形象化</h1><p id="e7b2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们用 python 中的一些代码来可视化基尼和熵曲线:</p><h2 id="c906" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">基尼系数:</h2><p id="78c5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下面我们制作一个函数来自动计算基尼系数。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="b340" class="oa la it ou b gy oy oz l pa pb">#Gini Function<br/>#a and b are the quantities of each class<br/>def gini(a,b):<br/>    a1 = (a/(a+b))**2<br/>    b1 = (b/(a+b))**2<br/>    return 1 - (a1 + b1)</span></pre><p id="2765" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">与我们的口香糖主题保持一致，让我们做一个循环，计算任何可以想象的红色和蓝色口香糖彩车组合的基尼系数，加到 4。我们将对上述基尼函数进行 10，000 次迭代，以便稍后绘制基尼曲线。</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="8ac4" class="oa la it ou b gy oy oz l pa pb">#Blank lists<br/>gini_list = []<br/>blue_list = []<br/>red_list = []<br/>blue_prob_list = []</span><span id="dff9" class="oa la it ou b gy pc oz l pa pb">#Looping Gini function on random blue and red float amounts<br/>for x in range (10000):<br/> blue = random.uniform(0, 4)<br/> red = abs(4-blue)<br/> a = gini(red,blue)<br/> b = blue/(blue+red)<br/> gini_list.append(a)<br/> blue_list.append(blue)<br/> red_list.append(red)<br/> blue_prob_list.append(b)</span><span id="3b98" class="oa la it ou b gy pc oz l pa pb">#Dataframe of amount of blue, red, Probability of blue, and gini score<br/>df = pd.DataFrame({“Blue”: blue_list, “Red”: red_list,”Gini Score”: gini_list, “Probability of Blue”: blue_prob_list})<br/>df = df[[‘Red’, ‘Blue’, ‘Probability of Blue’, ‘Gini Score’]]<br/>df</span></pre><p id="74cd" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">数据帧的开头在下面。其他 9994 行装不下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/027a9b7dc97db3356627ba14abef6a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jc6y4v2tkFvlhAo2TPBXPg.png"/></div></div></figure><p id="90fc" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">现在我们将绘制我们的曲线:</p><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="d53b" class="oa la it ou b gy oy oz l pa pb">plt.scatter(blue_prob_list,gini_list)<br/>plt.xlabel(‘Probability of Blue Gumball %’)<br/>plt.ylabel(‘Gini’)<br/>plt.title(‘Gini Curve’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/6cfb33e09fc47dbe83b566cd6085e184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kYvbxOQkeVE_QxbW502DuQ.png"/></div></div></figure><h2 id="6777" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated"><strong class="ak">熵:</strong></h2><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="d725" class="oa la it ou b gy oy oz l pa pb">#Gini Function<br/>#a and b are the quantities of each class. Base is the log base input.<br/>def entropy(base,a,b):<br/>    try:<br/>        var =  abs(((a)/(a+b)) * log(((a)/(a+b)),base)) - (((b)/(a+b)) * log(((b)/(a+b)),base))<br/>        return var<br/>    except (ValueError):<br/>        return 0</span><span id="1f18" class="oa la it ou b gy pc oz l pa pb">#Blank lists<br/>ent_list = []<br/>blue_list = []<br/>red_list = []<br/>blue_prob_list = []</span><span id="df5b" class="oa la it ou b gy pc oz l pa pb">#Loop with log base 2<br/>for x in range (10000):<br/>    blue = random.uniform(0, 4)<br/>    red = abs(4-blue)<br/>    a = entropy(2,red,blue)<br/>    b = blue/(blue+red)<br/>    ent_list.append(a)<br/>    blue_list.append(blue)<br/>    red_list.append(red)<br/>    blue_prob_list.append(b)</span><span id="e2af" class="oa la it ou b gy pc oz l pa pb">df = pd.DataFrame({"Blue": blue_list, "Red": red_list,"Entropy": ent_list, "Probability of Blue": blue_prob_list})<br/>df = df[['Red', 'Blue', 'Probability of Blue', 'Entropy']]<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/17283e52b72f09d8daa692e6fc8d3293.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-7D5HMIeOmwqS9w-3rs2A.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/055e1b8af3b0b37a86bac8258084b4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xiOMhF5yOYx1fJFf-1I7mQ.png"/></div></div></figure><h2 id="9c87" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated"><strong class="ak">对比:</strong></h2><p id="8201" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基尼是蓝色的，熵是橙色的。在下一节中，您将看到这些差异是如何在信息增益中体现出来的！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/236cd418b4d836f0151f8c8e61350016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*so3KThWQ-vUBNKdFjB51wQ.png"/></div></div></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="96a8" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">信息增益</h1><p id="66f9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">信息增益是杂质如此重要的原因。一旦我们获得了数据集的杂质，我们就可以看到当我们沿着树向下并测量节点的杂质时获得了多少信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/8e6b08ec3db40ca7c6b91668ceffcd91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o6UXK6cJgWPPKfwU16_H-Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source [3]</figcaption></figure><p id="e264" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">在下面的例子中，我们通过一个特定的属性(比如口香糖的<strong class="lt iu">大小</strong>)来分割口香糖偏好。它给出了父/子节点关系:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/47cbe6f547c7b869f8260f3c938383eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*ZyCSRmorlEMcG_2tsL26Ug.png"/></div></figure><h2 id="aa0b" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">基尼系数信息增益(根据上面的等式)</h2><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="3faa" class="oa la it ou b gy oy oz l pa pb">#Defining Gini info gain function:</span><span id="19d0" class="oa la it ou b gy pc oz l pa pb">def gini_info_gain(pa,pb,c1a,c1b,c2a,c2b):<br/>    return (gini(pa,pb))-((((c1a+c1b)/(pa+pb))*gini(c1a,c1b)) + (((c2a+c2b)/(pa+pb))*gini(c2a,c2b)))</span><span id="1784" class="oa la it ou b gy pc oz l pa pb">#Running Function</span><span id="fecf" class="oa la it ou b gy pc oz l pa pb">gini_info_gain(22,13,18,2,4,11)</span></pre><blockquote class="nt nu nv"><p id="5c9b" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">= 0.196 基尼信息增益</p><p id="bbc8" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">0.196/0.467 = 41.97%的增益</p></blockquote><h2 id="24c6" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">熵信息增益(来自上面的等式)</h2><pre class="kj kk kl km gt ot ou ov ow aw ox bi"><span id="f8ab" class="oa la it ou b gy oy oz l pa pb">#Defining Entropy info gain function:</span><span id="216d" class="oa la it ou b gy pc oz l pa pb">def entropy_info_gain(base,pa,pb,c1a,c1b,c2a,c2b):<br/>    return (entropy(base,pa,pb))-((((c1a+c1b)/(pa+pb))*entropy(base,c1a,c1b)) + (((c2a+c2b)/(pa+pb))*entropy(base,c2a,c2b)))</span><span id="2838" class="oa la it ou b gy pc oz l pa pb">#Running Function</span><span id="a856" class="oa la it ou b gy pc oz l pa pb">entropy_info_gain(2,22,13,18,2,4,11)</span></pre><blockquote class="nt nu nv"><p id="06a4" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">= 0.325 熵信息增益</p><p id="926e" class="lr ls ns lt b lu nn ju lw lx no jx lz nw np mc md nx nq mg mh ny nr mk ml mm im bi translated">0.325/0.952 = 34.14%的增益</p></blockquote><p id="a93b" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">对于这个例子，基尼具有更高的信息增益测量。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="d588" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">最终要点:</h1><ol class=""><li id="c217" class="mz na it lt b lu lv lx ly ma nb me nc mi nd mm pk nf ng nh bi translated">基尼的最大杂质是 0.5，最大纯度是 0</li><li id="a521" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm pk nf ng nh bi translated">熵的最大杂质是 1，最大纯度是 0</li><li id="8412" class="mz na it lt b lu ni lx nj ma nk me nl mi nm mm pk nf ng nh bi translated">不同的决策树算法利用不同的杂质度量:CART 使用 GiniID3 和 C4.5 使用熵。在您的模型中使用决策树/随机森林之前，这是值得研究的。</li></ol><p id="9446" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">*本出版物中的所有代码都可以在我的 github 上找到<a class="ae ky" href="https://github.com/ahershy/Gini-Index-and-Entropy-Exercise/blob/master/Entropy%2Bvs%2BGini%2BAnalysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a></p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="cca3" class="kz la it bd lb lc mu le lf lg mv li lj jz mw ka ll kc mx kd ln kf my kg lp lq bi translated">来源:</h1><p id="0308" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">[1]<a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Decision _ tree _ learning # Gini _ infinity</a></p><p id="0ccb" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">[2]http://web.mit.edu/6.02/www/f2011/handouts/2.pdf<a class="ae ky" href="http://web.mit.edu/6.02/www/f2011/handouts/2.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="e116" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated">[3]f . Provost 和 t . Fawcett(2013 年)。<em class="ns">商业数据科学:你需要了解的数据挖掘和数据分析思维</em>。加利福尼亚州科隆:奥赖利。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="0224" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><strong class="lt iu">考虑通过我的推荐链接加入 Medium:</strong><a class="ae ky" href="https://andrewhershy.medium.com/membership" rel="noopener">https://andrewhershy.medium.com/membership</a></p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="69a7" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><strong class="lt iu">如果您觉得这很有帮助，请订阅。如果你喜欢我的内容，请查看其他几个项目:</strong></p><p id="15ce" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4"> <em class="ns">随机森林是否优于 Logistic 回归？</em>(一比较)</a></p><p id="4f47" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/excel-vs-sql-a-conceptual-comparison-dcfbee640c83"> <em class="ns"> Excel vs SQL:概念上的比较</em> </a></p><p id="6efa" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc"> <em class="ns">用 Python 中的逻辑回归预测癌症</em> </a></p><p id="c17a" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/portfolio-linear-optimization-breakdown-f519546ed1ff"> <em class="ns">利用数学和 Python 优化你的投资</em> </a></p><p id="a1de" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/r-squared-recipe-5814995fa39a"> <em class="ns">从头开始计算 R 平方(使用 python) </em> </a></p><p id="369b" class="pw-post-body-paragraph lr ls it lt b lu nn ju lw lx no jx lz ma np mc md me nq mg mh mi nr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/word-clouds-in-python-comprehensive-example-8aee4343c0bf"><em class="ns">Python 中的字云:综合示例</em> </a></p></div></div>    
</body>
</html>