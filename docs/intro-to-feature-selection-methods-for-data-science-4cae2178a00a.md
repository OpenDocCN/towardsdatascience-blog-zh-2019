# 数据科学的特征选择方法介绍

> 原文：<https://towardsdatascience.com/intro-to-feature-selection-methods-for-data-science-4cae2178a00a?source=collection_archive---------6----------------------->

## *让数据更易于管理的指南*

作者:瑞安·法玛尔，韩宁，玛德琳·麦康贝

![](img/84b67d84fe019f92af2c0038f02f93a8.png)

Photo by [Eugenio Mazzone](https://unsplash.com/@eugi1492?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

# *什么是特征选择？*

好吧，让我们从定义什么是特性开始。要素是数据集中的 X 变量，通常由列定义。如今，许多数据集可能有 100 多个要素供数据分析师进行排序！这对于正常处理来说是一个荒谬的数量，这就是特征选择方法派上用场的地方。它们允许您在不牺牲预测能力的情况下减少模型中包含的特征数量。多余或不相关的特征实际上会对模型性能产生负面影响，因此删除它们是必要的(也是有益的)。想象一下，通过制作纸飞机来学习骑自行车。我怀疑你第一次骑马能走多远。

# *功能选择的好处*

特征选择的主要好处是它减少了过度拟合。通过删除无关的数据，它允许模型只关注数据的重要特性，而不会被无关紧要的特性所困扰。删除不相关信息的另一个好处是，它提高了模型预测的准确性。它还减少了获取模型所需的计算时间。最后，要素数量越少，模型的可解释性越强，也越容易理解。总的来说，特征选择是能够以任意精度预测值的关键。

# 概观

有三种类型的特征选择:包装方法(向前、向后和逐步选择)、过滤方法(ANOVA、Pearson 相关、方差阈值)和嵌入方法(Lasso、Ridge、决策树)。我们将在下面用 Python 的例子来解释每一个。

# *包装方法*

包装方法计算具有特定特征子集的模型，并评估每个特征的重要性。然后，他们迭代并尝试不同的功能子集，直到达到最佳子集。这种方法的两个缺点是，对于具有许多特征的数据，计算时间较长，并且当没有大量数据点时，它往往会使模型过拟合。最著名的特征选择包装方法是**向前选择、向后选择和逐步选择**。

**正向选择**从零个特征开始，然后针对每个单独的特征，运行一个模型，并确定与所执行的 t 检验或 F 检验相关的 p 值。然后，它会选择 p 值最低的要素，并将其添加到工作模型中。接下来，它采用所选的第一个要素，运行添加了第二个要素的模型，并选择 p 值最低的第二个要素。然后，它采用之前选择的两个要素，并使用第三个要素运行模型，依此类推，直到所有具有显著 p 值的要素都被添加到模型中。任何在迭代中尝试时从未具有显著 p 值的要素都将从最终模型中排除。

**反向选择**从数据集中包含的所有特征开始。然后运行模型，并为每个要素计算与模型的 t 检验或 F 检验相关联的 p 值。然后，将从模型中移除具有最大无关紧要 p 值的要素，并重新开始该过程。这种情况会持续下去，直到从模型中移除所有 p 值不显著的要素。

**逐步选择**是向前和向后选择的混合体。如上所述，它从零个要素开始，添加一个具有最低有效 p 值的要素。然后，遍历并找到具有最低有效 p 值的第二个要素。在第三次迭代中，它将寻找具有最低有效 p 值的下一个特征，并且它还将*移除* *先前添加的现在具有不重要 p 值*的任何特征。这使得最终的模型包含了所有重要的特性。

上述不同选择方法的好处是，如果您对数据以及哪些特征可能是重要的没有直觉，它们将为您提供一个良好的起点。此外，它有效地从大量数据中选择具有重要特征的模型。然而，一些缺点是，这些方法不能贯穿每一个单一的特征组合，因此它们可能不会以绝对最佳的模型结束。此外，它还会产生具有高度多重共线性的模型(由于要素之间的关系而导致 beta 系数膨胀)，这对于精确预测来说并不太好。

# *过滤方法*

过滤方法使用除差错率之外的度量来确定该特征是否有用。不像在包装器方法中那样调整模型，而是通过有用的描述性度量对特性进行排序来选择特性的子集。过滤方法的好处是计算时间非常短，不会过度拟合数据。然而，一个缺点是它们看不到特征之间的任何交互或相关性。这需要单独考虑，这将在下面解释。三种不同的过滤方法是 **ANOVA、Pearson correlation 和方差阈值法**。

**ANOVA** (方差分析)测试查看特征处理内以及处理间的变化。这些差异是这种特定过滤方法的重要指标，因为我们可以确定某个特征是否很好地解释了因变量的变化。如果每个特定处理的方差大于处理之间的方差，那么该特征没有很好地解释因变量的变化。为了进行 ANOVA 检验，使用分子(SST，通常与 SSTotal 混淆)中的处理之间的变化和分母中的处理内的变化来计算每个单独特征的 F 统计量。然后，针对零假设( *H0:所有处理的平均值相等*)和替代假设( *Hα:至少两个处理不同*)对该检验统计量进行检验。

**皮尔逊相关**系数是两个特征相似性的度量，范围在-1 和 1 之间。接近 1 或-1 的值表示这两个特征具有高相关性，并且可能是相关的。要使用此相关系数创建要素减少的模型，您可以查看所有相关性的热图(如下所示),并选择与响应变量(Y 变量或预测变量)相关性最高的要素。高相关性与低相关性的临界值取决于每个数据集中相关系数的范围。高相关性的一般度量是 0.7<|相关性| < 1.0。这将允许使用所选特征的模型包含数据集中包含的大部分有价值的信息。

![](img/0e47c9685ca4086a0ee147f33e98d2bd.png)

The response variable for this dataset SalePrice (top row) shows the correlation with the other variables. The light orange and dark purple show high correlations.

特征减少的另一种滤波方法是**方差阈值化**。一个特征的方差决定了它包含多少预测能力。方差越低，特征中包含的信息就越少，它在预测响应变量中的价值就越小。鉴于这一事实，方差阈值是通过找到每个特征的方差，然后将低于某个方差阈值的所有特征丢弃来完成的。如果您只想移除响应变量的每个实例中具有相同值的要素，则该阈值可以为 0。但是，要从数据集中移除更多要素，可以将阈值设置为 0.5、0.3、0.1 或其他对方差分布有意义的值。

如前所述，有时将**交互**添加到您的模型中会很有用，尤其是当您怀疑两个特征之间存在关系，可以为您的模型提供有用的信息时。交互可以作为交互项添加到回归模型中，显示为 B3X1X2。贝塔系数(B3)修改 X1 和 X2 的乘积，并测量两个特征(x)组合的模型的效果。要查看某个交互项是否显著，您可以执行 t 检验或 f 检验，并查看该项的 p 值是否显著。一个重要的注意事项是，如果相互作用项是重要的，两个低阶 X 项都必须保留在模型中，*，即使它们是不重要的*。这是为了保持 X1 和 X2 作为两个独立的变量，而不是一个新的变量。

# *嵌入方法*

嵌入式方法将特征选择作为模型创建过程的一部分。这通常导致前面解释的两种特征选择方法之间的折衷，因为选择是结合模型调整过程完成的。 **Lasso 和 Ridge regression** 是这种类型的两种最常见的特征选择方法，并且**决策树**也使用不同类型的特征选择来创建模型。

偶尔，您可能希望在最终模型中保留所有特征，但不希望模型过于关注任何一个系数。**岭回归**可以通过惩罚模型的 beta 系数过大来做到这一点。基本上，它降低了与可能没有其他变量重要的变量的相关性强度。这将处理数据中可能存在的任何多重共线性(要素之间的关系会增大它们的β值)。通过将惩罚项(也称为岭估计量或收缩估计量)添加到回归的成本函数中，可以实现骑行回归。惩罚项采用所有的 betas，并通过必须调整的项 lambda (λ)对它们进行缩放(通常使用交叉验证:比较相同的模型，但使用不同的 lambda 值)。Lambda 是一个介于 0 和无穷大之间的值，尽管从 0 和 1 之间的值开始比较好。λ值越高，系数收缩得越多。当 lambda 等于 0 时，结果将是没有惩罚的常规普通最小二乘模型。

![](img/c52bc89a8eb169508a3a377a2b108b49.png)

Function from: [https://codingstartups.com/practical-machine-learning-ridge-regression-vs-lasso/](https://codingstartups.com/practical-machine-learning-ridge-regression-vs-lasso/)

![](img/a8db124adf58e1cf7d3eebb0b113e912.png)

This shows how Ridge regression can adjust some of the large coefficients found in linear regression by making them closer to zero.

![](img/71270f3e2eff91ba6f98293be92af2d2.png)

As the value of lambda (alpha) increases, the coefficients are pushed toward zero with at the cost of MSE.

**套索回归**是另一种惩罚模型中贝塔系数的方法，与岭回归非常相似。它还向模型的成本函数添加了一个惩罚项，其中 lambda 值必须进行调整。与岭回归最重要的区别是，套索回归可以强制贝塔系数为零，这将从模型中移除该特征。这就是为什么套索有时是首选，尤其是当你想降低模型的复杂性。模型的特征数量越少，复杂性就越低。为了迫使系数为零，添加到成本函数中的罚项取β项的绝对值，而不是求其平方，当试图最小化成本时，这会否定函数的其余部分，导致β等于零。

![](img/fdd2ff26320d74243d49fda06dbc29e5.png)

Function from: [https://codingstartups.com/practical-machine-learning-ridge-regression-vs-lasso/](https://codingstartups.com/practical-machine-learning-ridge-regression-vs-lasso/)

脊套回归的一个重要注意事项是*你所有的特征都必须标准化*。Python 和 R 中的许多函数会自动完成这项工作，因为 lambda 必须平等地应用于每个特性。一个要素的值以千为单位，而另一个要素的值以十进制为单位，这种情况不会发生，因此需要标准化。

另一种使用特征选择对数据建模的常见方法称为**决策树**，它可以是回归树或分类树，分别取决于响应变量是连续的还是离散的。该方法基于某些特征在树中创建分割，以创建算法来找到正确的响应变量。构建树的方式使用嵌入方法中的包装方法。我们的意思是，在制作树模型时，函数内置了几种特征选择方法。在每次分割时，用于创建树的函数会尝试所有要素的所有可能分割，并选择将数据分割为最相似组的分割。简单地说，它选择最能预测树中每个点的响应变量的特性。这是一个包装器方法，因为它尝试了所有可能的特性组合，然后选择了最好的一个。

预测响应变量时最重要的特征用于在树的根(开始)附近进行分割，而更不相关的特征直到树的节点(结束)附近才用于进行分割。这样，决策树惩罚了对预测响应变量没有帮助的特征(嵌入式方法)。生成树后，可以选择返回并“修剪”一些没有为模型提供任何附加信息的节点。这可以防止过度拟合，通常通过维持测试集的交叉验证来实现。

# *总结*

那么，现在你已经经历了这一切，你要带走的最重要的想法是什么？尽管数据集可能有成百上千个要素，但这并不意味着所有要素都很重要或有用。尤其是现在，我们生活在一个拥有难以想象的海量数据的世界里，重要的是要努力关注那些重要的信息。还有很多(复杂的)方法来执行特征选择，我们在这里没有提到，但是所描述的方法是一个很好的起点！祝你好运，继续努力！

# 关键词汇:

**特征:**x 变量，通常是数据集中的一列
**特征选择:**通过选择要使用的特征子集来优化模型

**包装器方法:**用不同的特征子集尝试模型并挑选最佳组合
**正向选择:**逐个添加特征以达到最优模型
**反向选择:**逐个移除特征以达到最优模型
**逐步选择:**混合正向和反向选择，逐个添加和移除特征以达到最优模型

**过滤方法:**通过除误差之外的度量选择特征子集(该度量是特征固有的，不依赖于模型)
**皮尔逊相关:**两个变量之间线性相关的度量
**方差阈值:**选择方差截止点以上的特征以保留数据中的大部分信息
**ANOVA:** (方差分析)用于观察处理(样本)均值差异的一组统计估计过程和模型；可用于判断某个特征何时对模型具有统计显著性
**交互术语:**当两个特征依赖于另一个的值时，对它们之间的关系进行量化；减轻多重共线性，并可提供对数据的进一步了解
**多重共线性:**当两个或多个独立变量彼此高度相关时发生

**嵌入式方法:**在模型创建过程中选择和调整特征子集
**岭回归:**一种改进的最小二乘回归，通过将λ项应用于成本函数来惩罚具有膨胀的β系数的特征
**拉索回归:**类似于岭回归，但不同之处在于添加到成本函数的λ项可以迫使β系数为零
**决策树:**一种非参数模型，使用特征作为节点来分割样本以在随机森林模型中，可以使用平均下降基尼系数来计算特征重要性。
**交叉验证:**一种迭代生成训练和测试数据集的方法，用于估计未来未知数据集上的模型性能