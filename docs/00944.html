<html>
<head>
<title>Towards Fast Neural Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朝向快速神经类型转移</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-fast-neural-style-transfer-191012b86284?source=collection_archive---------15-----------------------#2019-02-12">https://towardsdatascience.com/towards-fast-neural-style-transfer-191012b86284?source=collection_archive---------15-----------------------#2019-02-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2859" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Gatys 等人[1]提出的神经类型转移的开创性论文展示了深度卷积神经网络的显著特征。从参数卷积层学习到的顺序表示可以分为“内容”和“样式”。风格转移背后的基本思想是，在图像网络分类等任务上预先训练的 DCNNs 可以用作描述符网络。图像通过预先训练的 DCNN，如 VGG [2]，中间特征激活可用于融合一个图像的“风格”与另一个图像的“内容”。从预训练网络的特征激活中导出损失函数是神经类型转移背后的基本思想。</p><p id="905b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">尽管有惊人的结果，根据[1]实现神经类型转移需要一个缓慢的迭代优化过程。首先，使用生成器网络生成图像。这个输出的图像通过预先训练的 VGG。通过计算 Gram 矩阵，来自层 1、2、3、4 和 5 的 ReLU 激活是非局部化的，这形成了样式输出。层 4 中 ReLU 激活的内积形成内容输出。然后通过反向传播优化图像，使得样式和内容输出与目标样式和目标内容图像相匹配。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/49f656fc11d14e37cb06129119f53318.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xkFdOa8je7cRjMGEzK3cUQ.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Results from Ulyanov et al. [3] ‘s Faster Style Transfer with Feed-Forward Networks</figcaption></figure><p id="5005" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文将介绍 Ulyanov 等人[3]的一篇论文，该论文通过训练前馈网络来加速这种神经风格转移的过程，使得只需要一次正向传递来风格化图像。以下是本文的链接:</p><div class="lb lc gp gr ld le"><a href="https://arxiv.org/abs/1603.03417" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">纹理网络:纹理和风格化图像的前馈合成</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">Gatys 等人最近证明了深度网络可以从一个单一的…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">arxiv.org</p></div></div></div></a></div><h1 id="da8a" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">报纸上有趣的快速统计</h1><ul class=""><li id="7f42" class="ml mm iq jp b jq mn ju mo jy mp kc mq kg mr kk ms mt mu mv bi translated">该网络在 NVIDIA Tesla K40 GPU 上训练了 2 个小时</li><li id="7eed" class="ml mm iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">风格转换 20 毫秒</li><li id="d497" class="ml mm iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">170 MB 以生成 256 x 256 的样本</li><li id="0f1b" class="ml mm iq jp b jq mw ju mx jy my kc mz kg na kk ms mt mu mv bi translated">作者发现用 16 张内容图片训练时效果最好</li></ul><h1 id="8e09" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">网络体系结构</h1><p id="63ac" class="pw-post-body-paragraph jn jo iq jp b jq mn js jt ju mo jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">我认为理解一篇新的深度学习论文的最有用的方法之一是查看所使用的架构，因此这就是本文将如何开始探索这项技术。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ne"><img src="../Images/a9c90c153f5b4ebb95152df2dff2048d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UDVu3DM0WsMV7-XPgmul0g.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Multi-Scale Generator Architecture for Feed-Forward Style Transfer [3]</figcaption></figure><p id="ba5e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个架构有很多部分。首先，它是一种多尺度架构，类似于 LAP-GAN 或渐进增长 GAN 中使用的架构。每个 z 代表不同空间分辨率的随机噪声输入。每个训练历元对包含 K 个张量的噪声向量 z 进行采样，在上图中，K = 5。每个噪声向量被卷积 3 次，然后通过放大和连接操作与下面的层结合。上图仅用于合成纹理。不幸的是，作者没有提供风格是如何合成的额外图片，但它是一个可以阅读和理解的快速增强。与上面的图像相反，当他们进行风格转换时，他们对内容图像 y 进行下采样，以匹配 z 中的每个噪声张量，并将它们连接在一起。例如，4x4xc 噪声张量 0 与下采样的 4×4×3 内容图像 8x8xc + 8x8x3 连接，..，等等。</p><p id="3eca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个有趣的细节是在网络末端使用 1x1 卷积。这样做是为了保持空间分辨率，但是减少特征图的深度，使得输出从 HxWxC 张量收敛到 HxWx3 RGB 图像，(H =高度，W =宽度，C =通道)。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/45a76a4b45faab54fba4617068fdf1fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*uDl9x-VKds1UmgSAYUvHpg.png"/></div></figure><p id="1202" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，也许比建筑更有趣的是这项任务中使用的损失函数的推导。这个损失函数由两部分组成，一个是风格损失，另一个是内容损失，这两部分都是从预先训练好的深度卷积神经网络(在这种情况下是 VGG-19)的中间激活中得到的。</p><p id="3f71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过 Gram 矩阵的计算，风格损失是非局部的:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/f07c870286e4b361f29455bc95353471.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*q1x15uZb_ntbAx6QdQiFiw.png"/></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Gram Matrix Equation</figcaption></figure><p id="6130" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这取每个深度的特征图的内积。例如，如果卷积的输出是具有 64 个特征映射的 50×50×64，则特征映射 1 的点积与特征映射 2 相乘，依此类推，直到特征映射 64。从而形成损失函数，</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/9e6a3a5ae39a35be40ce17fa8a3e8d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*LBNUKg5EeDDQykihLCMFxQ.png"/></div></figure><p id="8595" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相反，内容损失计算如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/3d7d55156bea96acc5d6896f9725f70f.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*D_pohr6gU1MciMxu1qn24A.png"/></div></figure><p id="d41f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该等式计算每个要素地图上空间位置之间的差异。因此，损失函数为每个任务捕捉了非常不同种类的信息。这两个损失被组合用于风格转移，并用参数α加权。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e47ea9f9771049c778b79bb84e1f6cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*0j2xi5t-eXW0FsQP6HB9kg.png"/></div></figure><p id="b8e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个损失函数与 Gatys 等人[1]提出的损失函数没有太大的不同，但是，想到它被用来训练一个前馈网络是很有趣的。</p><p id="1249" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">非常有趣的是，可以看到如何通过深度神经网络的单次正向传递来实现神经类型的转移。未来的工作强调，这种方法仅限于预先训练的风格，结果的质量并不总是像 Gatys 等人[1]那样高。看看神经风格转移算法如何进一步发展会很有趣，感谢阅读！</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nk"><img src="../Images/7e57a70edda506d04761692a61309adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8B2PVBVGZoOP42Xe9yR5Ew.png"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">More Results from This Paper [3]</figcaption></figure><h1 id="a37a" class="ln lo iq bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">参考</h1><p id="98ca" class="pw-post-body-paragraph jn jo iq jp b jq mn js jt ju mo jw jx jy nb ka kb kc nc ke kf kg nd ki kj kk ij bi translated">[1]莱昂·a·加蒂斯，亚历山大·s·埃克，马蒂亚斯·贝赫。艺术风格的神经算法。2015.</p><p id="fe02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]卡伦·西蒙扬，安德鲁·齐泽曼。用于大规模图像识别的非常深的卷积网络。2014.</p><p id="d485" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[3] Dmitry Ulyanov、Vadim Lebedev、Andrea Vedaldi、Victor Lempitsky。纹理网络:纹理和风格化图像的前馈合成。2016.</p></div></div>    
</body>
</html>