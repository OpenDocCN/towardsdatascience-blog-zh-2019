<html>
<head>
<title>Explained: Deep Learning in Tensorflow — Chapter 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释:Tensorflow 中的深度学习—第 1 章</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explained-deep-learning-in-tensorflow-chapter-1-9ab389fe90a1?source=collection_archive---------21-----------------------#2019-12-02">https://towardsdatascience.com/explained-deep-learning-in-tensorflow-chapter-1-9ab389fe90a1?source=collection_archive---------21-----------------------#2019-12-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6d09" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/all-about-deep-learning" rel="noopener" target="_blank">关于深度学习的一切</a></h2><div class=""/><div class=""><h2 id="a8a5" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">训练神经网络和数据预处理</h2></div><p id="b627" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di">我们</span>在上一章已经谈了很多关于前馈神经网络的内容。现在，我们将讨论一点张量流，然后，在张量流中实现 FFNN。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/de80750ddfc8facc82d502d65a52d8a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9aXc-89j8Ang3rSV.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">TF for DL</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/11c813b382871359de8a320bdaf3a5d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/0*4yvH8k8HfkbHAT4u.png"/></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Tensorflow 2.0 Architecture</figcaption></figure><p id="e748" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Tensorflow 提供高级 API:Keras 和 Estimator，用于创建深度学习模型。然后，tf.data 等 API 进行数据预处理。在最底层，每个 Tensorflow 操作都是使用高效的 C++代码实现的。大多数时候，我们只使用高级 API，但当我们需要更多的灵活性，如直接处理张量时，我们可以使用较低级别的 API，如 tf.nn，tf。GradientTape()等。Tensorflow 提供了广泛的库，如用于可视化的<a class="ae mk" href="http://tensorboard.dev/" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>，用于下载和重用预训练模型的<a class="ae mk" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>。它还提供了<a class="ae mk" href="https://www.tensorflow.org/tfx/serving/serving_basic" rel="noopener ugc nofollow" target="_blank"> Tensorflow 服务</a>，在这里我们可以轻松地将我们的模型公开为 REST API 用于生产。<a class="ae mk" href="https://medium.com/analytics-vidhya/understanding-the-search-query-part-ii-44d18892283f" rel="noopener">关于 TF 上菜的更多细节</a>。</p><h1 id="a744" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">tf.keras (Keras)</h1><p id="d810" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di">我们</span>先说说在<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/" rel="noopener ugc nofollow" target="_blank"> tf.keras </a>中实现 FFNN。在 Keras 中，我们可以通过三种方式创建模型:</p><h2 id="36c9" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated"><a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential" rel="noopener ugc nofollow" target="_blank"> 1。顺序 API </a></h2><p id="f964" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">它很容易使用，也很常用于创建线性层堆栈。它提供了<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer" rel="noopener ugc nofollow" target="_blank"> <em class="nt">输入层</em> </a>用于取输入，<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense" rel="noopener ugc nofollow" target="_blank"> <em class="nt">密集层</em> </a> <em class="nt"> </em>用于创建单层神经网络，内置<em class="nt"> tf.losses </em>用于选择一系列损失函数使用，内置<em class="nt">TF . optimizer</em>，内置<em class="nt"> tf.activation、</em>等。我们可以创建自定义层、损失函数等。我们很快就会看到。让我们以下面的 FFNN 为例进行说明:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nu"><img src="../Images/d61e46b0cabfb2756176181c25b4f732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Co-ySKu27fEb5nyh.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Sample FFNN</figcaption></figure><p id="34b9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上述使用顺序 API 的代码:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="6b18" class="ni mm iq nw b gy oa ob l oc od">#Assuming you can import the module needed from tf.keras<br/>#Creating model object<br/>model = Sequential()</span><span id="dbe1" class="ni mm iq nw b gy oe ob l oc od">#Input layer of shape (3,)<br/>model.add(Input(shape=(3,)))<br/>#Adding dense layers<br/>model.add(Dense(units=4))<br/>model.add(Dense(units=4)) #default activation=linear-&gt; x=max(0,x)<br/>model.add(Dense(units=1, activation = 'softmax'))</span><span id="4a93" class="ni mm iq nw b gy oe ob l oc od">#defining loss and optimizer<br/>model.compile(loss="mean_square_error", optimizer="sgd")</span><span id="354b" class="ni mm iq nw b gy oe ob l oc od">#start training<br/>model.fit(X_train, y_train, epochs=10)</span></pre><h2 id="e296" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated"><a class="ae mk" href="https://keras.io/models/model/\" rel="noopener ugc nofollow" target="_blank"> 2。函数 API </a></h2><p id="6b1b" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">有时，我们需要建立一个具有更复杂拓扑结构或多个输入或输出的神经网络，为此，Keras 提供了函数 API。上面的网络代码在函数 API 中可以看到没有太大的区别:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="0ffd" class="ni mm iq nw b gy oa ob l oc od">#Again Assuming you can import the module needed from tf.keras<br/>#Input layer of shape (3,)<br/>input = Input(shape=(3,))<br/>#Adding dense layers<br/>x = Dense(units=4) (input)<br/>x = Dense(units=4)(x) #default activation=ReLU<br/>output = Dense(units=1, activation = 'softmax')(x)<br/>#Keras model specifying input and output<br/>model = Keras.model(inputs=[input], outputs=[output])<br/>#Again follows the same compile, fit, evaluate, predict etc.</span></pre><h2 id="ac32" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated"><a class="ae mk" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" rel="noopener ugc nofollow" target="_blank"> 3。子类化 API /定制模型</a></h2><p id="c8b8" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">除非您需要涉及循环、条件分支和其他动态行为的模型，否则它不会被广泛使用。简单地子类化<em class="nt">模型类，</em>在构造函数中创建你需要的层，并使用它们在被覆盖的<em class="nt"> call() </em>方法中执行你想要的计算。子类化 API 中相同网络的代码:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="f9c5" class="ni mm iq nw b gy oa ob l oc od">class FFNN(keras.Model):<br/>   def __init__(self, activation="softmax", **kwargs):<br/>       super().__init__(**kwargs)<br/>       self.input = Input(shape=(3,))<br/>       self.hidden1 = Dense(units=4)<br/>       self.hidden2 = Dense(units=4)<br/>       self.output = Dense(units=1, activation = activation)</span><span id="8a43" class="ni mm iq nw b gy oe ob l oc od">   #override call method<br/>   def call(self, inputs):<br/>       input = self.input(inputs)<br/>       x = self.hidden1(input)<br/>       x = self.hidden2(x)<br/>       output = self.ouput(x)<br/>       return output<br/>#create model instance<br/>model = FFNN()</span></pre><h1 id="2e0d" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">使模型完美的成分！</h1><h2 id="2f44" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated"><strong class="ak"> 1。使用回调</strong></h2><p id="ba3d" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">它用于获得模型的最佳可能权重或避免过度拟合。tf.keras 中的<em class="nt"> fit 方法</em>将<em class="nt">回调</em>作为一个参数，它接受各种范围的回调，甚至我们也可以通过编写一个扩展<em class="nt">TF . keras . callbacks . callback .</em><a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint" rel="noopener ugc nofollow" target="_blank"><em class="nt">model check point</em></a>的方法来传递自定义回调，在训练期间以固定的间隔保存模型的检查点。它可以如下使用:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="2578" class="ni mm iq nw b gy oa ob l oc od">checkpoint_cb = <strong class="nw ja">tf.keras.callbacks.ModelCheckpoint</strong>("model.h5", save_best_only=True)</span><span id="6dc9" class="ni mm iq nw b gy oe ob l oc od">#fit the model with callback<br/>model.fit(X_train,y_train,..., callbacks=[checkpoint_cb])<br/>#load the best model after training<br/>tf.keras.models.load_model("model.h5")</span></pre><p id="093f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一个回调称为<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping" rel="noopener ugc nofollow" target="_blank"> <em class="nt">提前停止</em> </a> <em class="nt"> </em>当一个定义的指标在一定数量的时期(<em class="nt">耐心</em>)后停止改进时使用，这将可选地回滚到最佳模型。样本如下:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="c74b" class="ni mm iq nw b gy oa ob l oc od">early_stop_cb = <strong class="nw ja">tf.keras.callbacks.EarlyStopping</strong>(patience=5, restore_best_weight=True)<br/>#train the model with these callbacks<br/>model.fit(..., callbacks=[checkpoint_cb, early_stop_cb])</span></pre><p id="e17f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">两个检查点一起使用将保存检查点，并在没有更多进展时提前中断训练。</p><h2 id="9fac" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated"><strong class="ak"> 2。使用张量板进行可视化</strong></h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi of"><img src="../Images/0d21633ee24095442890b315a1b7a267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SjjbD_jIyjnI6NLZgfqxrg.png"/></div></div><figcaption class="mf mg gj gh gi mh mi bd b be z dk">Tensorboard</figcaption></figure><p id="3f24" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Tensorboard 是一个很棒的可视化工具，在这里我们可以看到训练过程中每一次跑步的标量、图表等。它读取名为<em class="nt">事件文件</em>的二进制日志文件来投射数据。事件文件可以通过写回调来生成，即<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard" rel="noopener ugc nofollow" target="_blank"><em class="nt">tensor board</em></a><em class="nt">。</em>它就像编写回调一样简单:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="accc" class="ni mm iq nw b gy oa ob l oc od">tensorboard_cb = <strong class="nw ja">keras.callbacks.TensorBoard(</strong>logdir, ) #provide a logdir where the event files will be stored<br/>#train the model<br/>model.fit(..., callbacks=[tensorboard_cb])</span></pre><p id="43c2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了可视化它，您需要通过以下命令启动 TensorBoard 服务器:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="2f8f" class="ni mm iq nw b gy oa ob l oc od">tensorboard --logdir = logdir<br/># this will create a server at 6006 port</span></pre><p id="f405" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或者，您可以将您的活动文件上传到<a class="ae mk" href="http://tensorboard.dev" rel="noopener ugc nofollow" target="_blank"> tensorboard.dev </a>并从<a class="ae mk" href="https://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌合作实验室</a>在线观看:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="2412" class="ni mm iq nw b gy oa ob l oc od">!tensorboard dev upload --logdir ./logs</span></pre><h2 id="f8ba" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated"><strong class="ak"> 3。使用</strong> <a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">批量规格化</strong> </a> <strong class="ak">和渐变裁剪</strong></h2><p id="7205" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">在训练期间，始终存在消失/爆炸梯度问题的风险，<strong class="kq ja">批量标准化</strong>解决了这些问题。这种技术包括在模型中每个隐藏层的激活函数之前或之后添加操作。它只是通过估计输入的平均值和标准偏差来对输入进行零中心化和标准化。在实现中，Keras 在训练期间通过使用层的输入均值和标准偏差的移动平均值来学习每层的每个输入的四个参数。在 ImageNet 分类任务中使用它时，已经观察到了巨大的改进。在 tf.keras 中，我们需要在每个隐藏层的激活函数之前或之后添加一个 BatchNormalization 层。以上面的例子为例:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="565e" class="ni mm iq nw b gy oa ob l oc od">model.add(Input(shape=(3,)))<br/>#Adding batch normalization layer<br/>model.add(<strong class="nw ja">tf.keras.layers.BatchNormalization</strong>())<br/>model.add(Dense(units=4)) </span><span id="68b0" class="ni mm iq nw b gy oe ob l oc od">model.add(<strong class="nw ja">tf.keras.layers.BatchNormalization</strong>())<br/>x = Dense(units=4)(x) #default activation=ReLU</span></pre><p id="5ffc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">渐变裁剪</strong>另一种缓解爆炸式渐变问题的流行技术。这种方法主要在 RNN 使用。它有助于将梯度向量的每个分量剪切到所提供的范围之间的值。例如</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="b01e" class="ni mm iq nw b gy oa ob l oc od">model.compile(loss="mean_squared_error", optimizer = keras.optimizers.SGD(<strong class="nw ja">clipvalue=1.0</strong>))</span></pre><p id="e227" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上面的代码将在-1 和 1 之间裁剪损失的偏导数。</p><h2 id="7d06" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated">4.使用正则化来避免过度拟合</h2><p id="d7a2" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">我们已经在上面讨论了一些避免过度拟合的技术，让我们看看其他流行的正则化技术。<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> L1 和 L2 正则化</strong> </a>涉及向成本函数添加一些数量级的权重。下面是我们如何实现:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="59a0" class="ni mm iq nw b gy oa ob l oc od">Dense(units=4, kernel_regularizer=<strong class="nw ja">keras.regularizers.l2</strong>(0.01))</span></pre><p id="2c89" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">退出</strong>是另一种技术，尤其适用于深度神经网络，其中每个神经元都有可能在特定步骤的训练过程中被暂时退出或完全忽略，但在下一步骤中可能是活跃的，这由称为退出率的参数<em class="nt"> p 决定。这种脱落在测试过程中是无效的。它可以在致密层之后立即使用。实现如下:</em></p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="44b4" class="ni mm iq nw b gy oa ob l oc od">model.add(Dense(units=4))<br/><strong class="nw ja">tf.keras.layers.Dropout</strong>(rate=0.3)</span></pre><h2 id="6846" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated">5.寻找超参数的最佳拟合</h2><p id="7e34" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">由于神经网络有太多的超参数需要调整，因此为网络找到超参数的最佳组合总是一个挑战。我们可以通过尝试尽可能多的组合来手动执行这个选择任务，看看哪一个效果最好。另一种方法是使用<a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> <em class="nt"> GridSearchCV </em> </a>或<a class="ae mk" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank"><em class="nt">RandomizedSearchCV</em></a><em class="nt">。</em>由于这些是 sklearn 方法，我们需要通过分别使用<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasRegressor" rel="noopener ugc nofollow" target="_blank"><em class="nt">keras regressor</em></a>或<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras/wrappers/scikit_learn/KerasClassifier" rel="noopener ugc nofollow" target="_blank"><em class="nt">keras classifier</em></a><em class="nt"/>进行回归和分类任务，将 tf.keras 模型包装在模拟常规 sklearn 回归器的对象中。它采用使用 tf.keras API 描述模型的方法，然后我们可以使用它的<em class="nt"> fit() </em>方法来训练模型。但是，我们的目标是找到 tf.keras 模型的最佳超参数，因此我们提到超参数的名称及其范围，并将其传递给 RandomizedSearchCV 方法，如下所示:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="361e" class="ni mm iq nw b gy oa ob l oc od">#tf.keras model<br/>def build_fn():<br/>     # the model which we built above using sequential API by making<br/>     the no. of hidden units and layers as variable</span><span id="75a3" class="ni mm iq nw b gy oe ob l oc od">     model = Sequential()<br/>     ..............<br/>     return model</span><span id="3c02" class="ni mm iq nw b gy oe ob l oc od">#KerasRegressor<br/>keras_reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_fn)<br/>#defining the parameters distribution<br/>param_distrib = {<br/>     "n_hidden": [3,4,5],<br/>     "n_neurons": [40,50,60,70]<br/>     }<br/>search_cv = RandomizedSearchCV(keras_reg, param_distrib, n_iter=4)<br/>search_cv.fit(...)</span></pre><p id="ba59" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">根据模型和资源的复杂程度，上述探索任务可能需要几个小时。</p><h1 id="477b" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">tf.keras 中的自定义图层</h1><p id="396b" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">有时，我们需要创建一个 tf.keras 没有提供默认实现的层。该层可以有两种类型:无状态和有状态。</p><p id="d90e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">无状态自定义层与展平层一样，没有要学习的权重。为此，我们使用 tf.keras.layers.Lambda 层如下:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="4b75" class="ni mm iq nw b gy oa ob l oc od">log_layer = tf.keras.layers.Lambda(lambda x: tf.log(x))</span></pre><p id="1038" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我已经在这里定义的生产用例中使用了这个自定义 lambda 层<a class="ae mk" href="https://medium.com/analytics-vidhya/elmo-embedding-the-entire-intent-of-a-query-530b268c4cd" rel="noopener">来生成特定句子的 ELMo 嵌入。</a></p><p id="f63b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有状态的自定义层具有要训练的权重，并且可以通过扩展/创建<em class="nt"> tf.keras.layers.Layer 类</em>的子类并覆盖<em class="nt"> call()方法</em>来实现。例子可以在<a class="ae mk" href="https://www.tensorflow.org/guide/keras/custom_layers_and_models" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="806e" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">tf。估计量</h1><p id="3611" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated"><a class="ae mk" href="http://tensorflow.org/api_docs/python/tf/estimator/Estimator" rel="noopener ugc nofollow" target="_blank"> tf。Estimator </a>是另一个高级 API，它在创建深度学习模型的同时提供了灵活性。它与 tf.data API 非常兼容，我们可以在不改变模型的情况下运行分布式多服务器环境。</p><blockquote class="og oh oi"><p id="7ccc" class="ko kp nt kq b kr ks ka kt ku kv kd kw oj ky kz la ok lc ld le ol lg lh li lj ij bi translated"><code class="fe om on oo nw b"><a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/estimator" rel="noopener ugc nofollow" target="_blank">tf.estimator</a></code>为<code class="fe om on oo nw b"><a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/keras" rel="noopener ugc nofollow" target="_blank">tf.keras</a>.</code>提供了一些目前仍在开发中的功能</p></blockquote><p id="a72a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">可以将 tf.keras 模型转换为 tf.Estimator。有几个预先制作的估计器，我们不需要创建计算图或会话，估计器已经处理了它。前馈神经网络的预制估计器为<em class="nt"/><a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier" rel="noopener ugc nofollow" target="_blank"><em class="nt">TF . estimator . dnn classifier</em></a>。</p><h2 id="e277" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated">使用预制评估工具</h2><p id="d30a" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">首先，我们需要根据特征字典对输入数据集进行预处理，该字典将键作为特征名，将值作为张量，将相应的张量列表作为标签。<a class="ae mk" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja"><em class="nt">TF . data</em></strong></a>用于加载并预处理输入数据。然后，一个<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/feature_column" rel="noopener ugc nofollow" target="_blank"><em class="nt">TF . feature _ column</em></a><em class="nt"/>标识一个特征名、其类型和任何输入预处理。实例化模型，将特征列作为参数传递，并调用传递输入特征的 train 方法。下面是创建 FFNN 的代码:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="c49f" class="ni mm iq nw b gy oa ob l oc od">estimator = tf.estimator.DNNClassifier(feature_columns=[feature1, feature2, feature3], hidden_units=[40, 40, 10]) #instantiate<br/>estimator.train(..)  #train</span></pre><h2 id="93b4" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated">使用自定义估算器</h2><p id="951a" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">这是我们根据自己的需求从头开始创建模型的地方，而不是由其他人创建的。这里，应该用相同的<em class="nt"> tf.data </em> API 对输入进行预处理，但是我们需要实例化<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator" rel="noopener ugc nofollow" target="_blank"><em class="nt">TF . estimator . estimator</em></a>，其中我们需要传递一个由<code class="fe om on oo nw b">model_fn</code>指定的模型，该模型返回执行训练、评估或预测所需的操作。该 ops 需要传递给<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/estimator/train_and_evaluate" rel="noopener ugc nofollow" target="_blank">TF . estimator . train _ and _ evaluate</a>以及<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/estimator/TrainSpec" rel="noopener ugc nofollow" target="_blank"> tf.estimator.TrainSpec </a>和<a class="ae mk" href="https://www.tensorflow.org/api_docs/python/tf/estimator/EvalSpec" rel="noopener ugc nofollow" target="_blank"> tf.estimator.EvalSpec </a>。下面是一个示例代码:</p><pre class="lu lv lw lx gt nv nw nx ny aw nz bi"><span id="08a9" class="ni mm iq nw b gy oa ob l oc od">#model_fn method<br/><strong class="nw ja">def</strong> model_fn(features, labels, mode, params):<br/>   .......<br/>   <strong class="nw ja">if</strong> mode == tf.estimator.ModeKeys.PREDICT:<br/>       <strong class="nw ja">return  </strong>tf.estimator.EstimatorSpec(mode,predictions=pred)<br/>   <strong class="nw ja">else</strong>:<br/>       #define loss, optimizer and another other metric, then<br/>       <strong class="nw ja">return</strong> tf.estimator.EstimatorSpec(<br/>               mode, loss=loss, train_op=train_op)</span><span id="04b0" class="ni mm iq nw b gy oe ob l oc od">estimator = tf.estimator.Estimator(model_fn, feature, labels, mode,      params) #instantiate estimator <br/>train_spec = tf.estimator.TrainSpec(input_fn=train_inp)<br/>eval_spec = tf.estimator.EvalSpec(input_fn=eval_inp)</span><span id="1b51" class="ni mm iq nw b gy oe ob l oc od">#finally train and evaluate<br/>tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)</span></pre><h1 id="0ffe" class="ml mm iq bd mn mo mp mq mr ms mt mu mv kf mw kg mx ki my kj mz kl na km nb nc bi translated">数据预处理</h1><p id="0649" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi lk translated"><span class="l ll lm ln bm lo lp lq lr ls di"> As </span> <em class="nt"> tf.data </em>对于评估者来说更受欢迎，因为它是分布式模型训练的一个很好的 API，在非常大的训练数据集的情况下，整个数据无法加载到 RAM 中。它可以从 tensor_slices、多个 CSV 文件中读取数据，如 tf.data.dataset，我们可以在其中使用映射、预取、批处理、混洗、填充批处理、重复等执行许多操作。对于相对较小的数据集，<a class="ae mk" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <em class="nt">熊猫</em> </a> <em class="nt">、numpy 和</em><a class="ae mk" href="https://www.google.com/search?q=sklearn.preprocessing&amp;oq=sklearn.p&amp;aqs=chrome.1.69i57j0l7.3087j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank"><em class="nt">sk learn . preprocessing</em></a>在使用 tf.keras 的同时<em class="nt"> </em>效果很好。tf.keras 还提供了预处理数据的实用程序，为训练和评估做好准备，就像对图像数据集一样。</p><p id="e04d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就是前向神经网络的两章内容。请关注下一章 RNN(循环神经网络和注意力)。</p><h2 id="5a95" class="ni mm iq bd mn nj nk dn mr nl nm dp mv kx nn no mx lb np nq mz lf nr ns nb iw bi translated">参考资料:</h2><p id="373b" class="pw-post-body-paragraph ko kp iq kq b kr nd ka kt ku ne kd kw kx nf kz la lb ng ld le lf nh lh li lj ij bi translated">书籍:奥雷连·杰龙的《机器学习实践》</p><div class="op oq gp gr or os"><a href="https://tensorflow.org" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd ja gy z fp ox fr fs oy fu fw iz bi translated">张量流</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">一个端到端的开源机器学习平台</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">tensorflow.org</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg md os"/></div></div></a></div></div></div>    
</body>
</html>