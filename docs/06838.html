<html>
<head>
<title>The Simple Math behind 3 Decision Tree Splitting criterions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">3 个决策树分裂标准背后的简单数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-simple-math-behind-3-decision-tree-splitting-criterions-85d4de2a75fe?source=collection_archive---------3-----------------------#2019-09-29">https://towardsdatascience.com/the-simple-math-behind-3-decision-tree-splitting-criterions-85d4de2a75fe?source=collection_archive---------3-----------------------#2019-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/702da0865b402131bb0b95a971550931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KnQLPuIbKIGwCTkJnSBLaA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image by <a class="ae jg" href="https://pixabay.com/users/photoshopper24-81349/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=683437" rel="noopener ugc nofollow" target="_blank">Bela Geletneky</a> from <a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=683437" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><div class=""/><div class=""><h2 id="5833" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">🌀理解分割标准</h2></div><p id="477b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">决策树很棒，对各种任务都很有用。它们构成了业界大多数性能最佳的模型的主干，如 XGboost 和 Lightgbm。</p><p id="9a5b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是它们到底是如何工作的呢？其实这是 ML/DS 面试中被问得最多的问题之一。</p><p id="cbf2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们通常知道它们以逐步的方式工作，并且有一个树形结构，在这个结构中，我们根据某种标准使用某种特征来分割一个节点。</p><p id="cfbe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是这些特征是如何被选择的，一个特定的阈值是如何被选择的呢？T3】</p><p id="b3e4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="lu">在这篇文章中，我将谈论决策树中使用的三个主要分裂标准以及它们为什么有效。这是一个已经被反复提及但从未真正做得足够好的东西。</em></strong></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="fe29" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">1.基尼杂质</h1><p id="7add" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">根据维基百科，</p><blockquote class="mz na nb"><p id="f1cd" class="ky kz lu la b lb lc kk ld le lf kn lg nc li lj lk nd lm ln lo ne lq lr ls lt im bi translated">Gini 杂质是一种度量，用于衡量从集合中随机选择的元素被错误标记的频率，前提是根据子集中的标签分布对其进行随机标记。</p></blockquote><p id="53b1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单来说，基尼杂质就是一个节点 中杂质的<strong class="la jk"> <em class="lu">度量。它的公式是:</em></strong></p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/fc8252320031f192bed1a6265d886010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WSCjiuZjNiWenj2c.png"/></div></div></figure><p id="3eeb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中 J 是节点中存在的类的数量，p 是节点中类的分布。</p><p id="c726" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了更好地理解这个公式，让我们具体讨论一下二进制的情况，其中我们的节点只有两个类。</p><p id="d7d4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在下面五个标有 A-E 的候选节点示例中，显示了正类和负类的分布，哪一个是理想状态？</p><p id="f6d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我想你会说 A 或 E，你是对的。最糟糕的情况是什么？我认为在这个节点上数据正好是 50:50。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/295d99f2498281a6d696d3889b971a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V4kvDGHzLuZDiK0jGU_W0g.png"/></div></div></figure><p id="9927" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这一切看起来很好，直觉上。基尼系数给了我们一个量化的方法。</p><p id="2f0f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们分别计算所有五个节点的基尼系数，并检查这些值。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/613d18ffe92cecd9750499b3daf024e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aut50u5j7ajG8KQCXY4d3Q.png"/></div></div></figure><p id="b7da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">✅基尼不纯的作品如预期的那样。节点 C 的最大值以及 A 和 e 的最小值。我们需要选择具有最小 Gini 杂质的节点。</p><p id="10ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以看到二元情况下的基尼系数图来验证上述情况。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/ce2ffb4b2f39c66aa2fa7339a13ed755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTbg4pSRhpMp9yvi6z1TJg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Gini Impurity</figcaption></figure><p id="024b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">❓So:我们如何在决策树中准确地使用它？</p><p id="cafc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设，我们有 UCI 心脏病的数据。“目标”字段是指患者是否存在心脏病。它是 0(不存在)或 1。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/ec0229c25878df191fdc21b84dc76060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3a5fc8ibj63xxiyU7xlDA.png"/></div></div></figure><p id="00c3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在已经有了一个衡量标准(基尼系数)，我们可以用它来评估一个特定变量在某个阈值(连续的)或值(分类的)上的分裂。</p><h2 id="8957" class="nn md jj bd me no np dn mi nq nr dp mm lh ns nt mo ll nu nv mq lp nw nx ms ny bi translated">分类变量拆分</h2><p id="5f19" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了简单起见，让我们从一个分类变量开始——性别。</p><p id="86af" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们按性别划分，我们的树将如下所示:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b6469142d241d543691bf2810f102f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*N4OdF6Pq6Fu3hThmByJk_w.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">If we split on Gender</figcaption></figure><p id="330e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意我们用 Sex =0 和 Sex！=0，因此这可以很好地推广到具有多个级别的类别。我们的根节点有 165 +ve 的例子和 138 -ve 的例子。当我们按性别分开时，我们得到两个子节点。</p><p id="2271" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经知道如何计算一个节点的杂质。所以我们计算左孩子和右孩子的杂质。</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="ff53" class="nn md jj ob b gy of og l oh oi">I_Left = 1 - (72/96)**2 - (24/96)**2<br/>I_Right = 1 - (93/207)**2 - (114/207)**2</span><span id="a859" class="nn md jj ob b gy oj og l oh oi">print("Left Node Impurity:",I_Left)<br/>print("Right Node Impurity:",I_Right)<br/>---------------------------------------------------------------<br/>Left Node Impurity: 0.375<br/>Right Node Impurity: 0.4948540222642302</span></pre><p id="52ac" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们这里有两个数字。我们需要得到一个单一的数字来提供单一分裂的杂质。那我们该怎么办？我们应该取一个平均值吗？我们可以取一个平均值，但是如果一个节点只有一个例子，而另一个节点有所有其他的例子，会发生什么呢？</p><p id="a11f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了减轻上述影响，我们采用了两种杂质的加权平均值，该平均值由单个节点中的样本数进行加权。在代码中:</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="0a3f" class="nn md jj ob b gy of og l oh oi">gender_split_impurity = 96/(96+207)*I_Left + 207/(96+207)*I_Right<br/>print(gender_split_impurity)<br/>----------------------------------------------------------------<br/>0.45688047065576126</span></pre><h2 id="13f4" class="nn md jj bd me no np dn mi nq nr dp mm lh ns nt mo ll nu nv mq lp nw nx ms ny bi translated">连续变量分割</h2><p id="82cf" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">我们也可以用连续变量来分割。让我们尝试使用数据集中的胆固醇特征进行分割。我们选择了 250 的阈值并创建了一棵树。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4460c708d70c7378a2b93a509d34ecf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*63QADTjiWajIdcvqLv_Ugg.png"/></div></figure><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="7a77" class="nn md jj ob b gy of og l oh oi">I_Left = 1 - (58/126)**2 - (68/126)**2<br/>I_Right = 1 - (107/177)**2 - (70/177)**2</span><span id="1c35" class="nn md jj ob b gy oj og l oh oi">print("Left Node Impurity:",I_Left)<br/>print("Right Node Impurity:",I_Right)<br/>---------------------------------------------------------------<br/>Left Node Impurity: 0.49685059208868737<br/>Right Node Impurity: 0.47815123368125373</span></pre><p id="b2c8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只看两者接近 0.5 的杂质，就可以推断不是好的拆分。尽管如此，我们还是像以前一样计算加权基尼系数:</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="71be" class="nn md jj ob b gy of og l oh oi">chol_split_impurity = 126/(126+177)*I_Left + 177/(126+177)*I_Right<br/>print(chol_split_impurity)<br/>---------------------------------------------------------------<br/>0.48592720450414695</span></pre><p id="7038" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从<code class="fe ok ol om ob b">chol_split_impurity</code> &gt; <code class="fe ok ol om ob b">gender_split_impurity</code>开始，我们根据性别进行划分。</p><p id="da71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，我们评估了很多不同的拆分。对于连续变量有不同的阈值。以及分类变量的所有级别。然后选择在子节点中为我们提供最低加权杂质的分裂。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="17eb" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">2.熵</h1><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/c969de95db60841edfde1aa5771d2be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EZIqOUL_nfHGRvdH"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Entropy == Randomness</figcaption></figure><p id="5a45" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一种非常流行的在决策树中分割节点的方法是熵。熵是系统中随机性的度量。熵的公式是:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3dd60367761b505412f07058abf8b7f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*VuU_zvVxVpbAXAUJ.png"/></div></figure><p id="ccbd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中 C 是节点中存在的类的数量，p 是节点中类的分布。</p><p id="ab48" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再次讨论我们之前讨论过的二元情况。从 A 到 E 的所有 5 个案例的熵值是多少？</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/2266a2a219a8d7d92bc4d93267c92d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzrDoeqzChQ3mIg2KiN0CQ.png"/></div></div></figure><p id="525d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">熵值按预期工作。节点 C 最大，A 和 e 最小。我们需要选择熵最小的节点。</p><p id="33da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以看到二元情况下的熵图来验证上述情况。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/fb87b5e5a66e23fecfc11d470530bce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XIingKchPaVbPUvQHi1cnw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Entropy</figcaption></figure><p id="ca7d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么我们如何在决策树中使用熵呢？</p><p id="3591" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们像以前一样使用心率示例。我们现在已经有了一个合适的度量(熵),使用它我们可以评估一个个体变量在某个阈值(连续的)或值(分类的)上的分裂。</p><h2 id="fc12" class="nn md jj bd me no np dn mi nq nr dp mm lh ns nt mo ll nu nv mq lp nw nx ms ny bi translated">分类变量拆分</h2><p id="7b1c" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">为了简单起见，让我们从一个分类变量开始——性别。</p><p id="45ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们按性别划分，我们的树将如下所示:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b6469142d241d543691bf2810f102f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*N4OdF6Pq6Fu3hThmByJk_w.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">If we split on Gender</figcaption></figure><p id="4f1f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们已经知道如何计算一个节点的随机性。所以我们计算左孩子和右孩子的随机性。</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="14ad" class="nn md jj ob b gy of og l oh oi">E_Left = -(72/96)*np.log2(72/96) - (24/96)*np.log2(24/96)<br/>E_Right = -(93/207)*np.log2(93/207) - (114/207)*np.log2(114/207)</span><span id="0ad8" class="nn md jj ob b gy oj og l oh oi">print("Left Node Randomness:",E_Left)<br/>print("Right Node Randomness:",E_Right)<br/>---------------------------------------------------------------<br/>Left Node Randomness: 0.8112781244591328<br/>Right Node Randomness: 0.992563136012236</span></pre><p id="ac82" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们这里有两个数字。我们需要得到一个单一的数字来提供单次分裂的随机性。那我们该怎么办？我们再次采用加权平均，其中我们根据单个节点中的示例数量进行加权。在代码中:</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="0345" class="nn md jj ob b gy of og l oh oi">gender_split_randomness = 96/(96+207)*E_Left + 207/(96+207)*E_Right<br/>print(gender_split_randomness)<br/>----------------------------------------------------------------<br/>0.9351263006686785</span></pre><h2 id="adae" class="nn md jj bd me no np dn mi nq nr dp mm lh ns nt mo ll nu nv mq lp nw nx ms ny bi translated">连续变量分割</h2><p id="7185" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">和之前一样，我们也可以用连续变量来分割。让我们尝试使用数据集中的胆固醇特征进行分割。我们选择阈值 250 并创建一棵树。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/4460c708d70c7378a2b93a509d34ecf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*63QADTjiWajIdcvqLv_Ugg.png"/></div></figure><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="0a68" class="nn md jj ob b gy of og l oh oi">E_Left = -(58/126)*np.log2(58/126) - (68/126)*np.log2(68/126)<br/>E_Right = -(107/177)*np.log2(107/177) - (70/177)*np.log2(70/177)</span><span id="b4af" class="nn md jj ob b gy oj og l oh oi">print("Left Node Randomness:",E_Left)<br/>print("Right Node Randomness:",E_Right)<br/>---------------------------------------------------------------<br/>Left Node Randomness: 0.9954515828457715<br/>Right Node Randomness: 0.9682452182690404</span></pre><p id="c974" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只看两者都接近 1 的随机性，就可以推断不是好的分裂。尽管如此，我们还是像以前一样计算我们的加权熵:</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="3248" class="nn md jj ob b gy of og l oh oi">chol_split_randomness = 126/(126+177)*E_Left + 177/(126+177)*E_Right<br/>print(chol_split_randomness)<br/>---------------------------------------------------------------<br/>0.9795587560138196</span></pre><p id="640f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从<code class="fe ok ol om ob b">chol_split_randomness</code> &gt; <code class="fe ok ol om ob b">gender_split_randomness</code>开始，我们根据性别进行拆分。和我们从基尼得到的结果完全一样。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="110d" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">3.差异</h1><p id="36b0" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">基尼系数和熵在分类场景中表现得相当好。</p><p id="c2fe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是回归呢？</p><p id="c8ae" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在回归的情况下，最常用的分割度量就是节点的加权方差。这也是有意义的:我们希望分割后节点的变化最小。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/e4aac4a0da58366890ab5833d7adb1c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*Kb4kpQGbNmgR5kHs.png"/></div></figure><p id="9c04" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要一个回归任务。所以，我们有 50 家创业公司的数据，我们想预测利润。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/8be721720d64a4625c5174823cb63985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0zJXqecJuCbADoiyfK-wZA.png"/></div></div></figure><h2 id="c224" class="nn md jj bd me no np dn mi nq nr dp mm lh ns nt mo ll nu nv mq lp nw nx ms ny bi translated">分类变量拆分</h2><p id="b178" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">让我们尝试按分类变量进行拆分州=佛罗里达。</p><p id="c17e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们按 State=FL 进行分割，我们的树将如下所示:</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/1adc283603785c42c8d1dde13343ca68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*Ikew_6ID7st-TRvPjDL-dQ.png"/></div></figure><p id="ac81" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总方差就是单个方差的加权和:</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="3be9" class="nn md jj ob b gy of og l oh oi">overall_variance = 16/(16+34)*Var_Left + 34/(16+34)*Var_Right<br/>print(overall_variance)<br/>----------------------------------------------------------------<br/>1570582843</span></pre><h2 id="7993" class="nn md jj bd me no np dn mi nq nr dp mm lh ns nt mo ll nu nv mq lp nw nx ms ny bi translated">连续变量分割</h2><p id="fe5a" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">和之前一样，我们也可以用连续变量来分割。让我们尝试使用数据集中的 R&amp;D 支出功能进行拆分。我们选择了 100000 的阈值并创建了一棵树。</p><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/9bf27cf994098b593a04448b7ab3c8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*7y8_yxfkDVktJ0GrlUhwNA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Splitting on R&amp;D</figcaption></figure><p id="8107" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只要看看这个，我们就能看到它比我们之前的分裂要好。因此，我们找到了这种情况下的总方差:</p><pre class="ng nh ni nj gt oa ob oc od aw oe bi"><span id="b6c9" class="nn md jj ob b gy of og l oh oi">overall_variance = 14/(14+36)*419828105 + 36/(14+36)*774641406<br/>print(overall_variance)<br/>----------------------------------------------------------<br/>675293681.7199999</span></pre><p id="f378" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自从有了<code class="fe ok ol om ob b">overall_variance(R&amp;D&gt;=100000)&lt; overall_variance(State==FL)</code>，我们更喜欢基于 R &amp; D 的拆分</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3ee8" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">继续学习</h1><figure class="ng nh ni nj gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/857de6c79fe08469fabd96494b1c1e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*komXZyJH_KrPqhfu"/></div></div></figure><p id="99a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想了解更多关于数据科学的知识，我想调出吴恩达的这个<a class="ae jg" href="https://coursera.pxf.io/NKERRq" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> <em class="lu">精品课程</em> </strong> </a>。这是我开始的原因。一定要去看看。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="ef7e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系。</p><p id="0e86" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——在这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>