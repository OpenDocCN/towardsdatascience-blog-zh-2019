<html>
<head>
<title>Feature selection techniques for classification and Python tips for their application</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类的要素选择技术及其应用的 Python 技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-techniques-for-classification-and-python-tips-for-their-application-10c0ddd7918b?source=collection_archive---------1-----------------------#2019-08-02">https://towardsdatascience.com/feature-selection-techniques-for-classification-and-python-tips-for-their-application-10c0ddd7918b?source=collection_archive---------1-----------------------#2019-08-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="870e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何使用最常见的特征选择技术解决分类问题的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b3d1e7fa4cdd17fed9cf5b7c00ea9278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k-o_cOg6ABwHfq33-xSrxg.jpeg"/></div></div></figure><p id="a0e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">选择使用哪些功能是任何机器学习项目中的关键一步，也是数据科学家日常工作中的一项经常性任务。在本文中，我回顾了分类问题中最常见的特征选择技术，将它们分为 6 大类。我提供了如何在机器学习项目中使用它们的技巧，并尽可能用 Python 代码给出例子。你准备好了吗？</p><h1 id="5601" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">TL；灾难恢复-汇总表</h1><p id="d5c4" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">下表总结了主要方法，并在以下部分进行了讨论。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/d99f89869ed4ffd35b7fc4ec8417d390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJ0slb7apPb1fQWVTAX2CA.jpeg"/></div></div></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="dc7b" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">什么是特征选择，为什么有用？</h1><p id="9c14" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">机器学习中最大的两个问题是<strong class="kw iu">过拟合</strong>(拟合在数据集之外不可概括的数据方面)和<strong class="kw iu">维数灾难</strong>(高维数据的非直观和稀疏特性)。</p><p id="2b80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过减少模型中的特征数量，尝试优化模型性能，特征选择有助于避免这两个问题。这样做，特性选择还提供了一个额外的好处:<strong class="kw iu">模型</strong>解释。随着特征的减少，输出模型变得更简单和更容易解释，并且人类更有可能相信模型做出的未来预测。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="fabb" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">无监督方法</h1><p id="2141" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">减少特征数量的一个简单方法是对数据应用<strong class="kw iu">降维技术</strong>。这通常以无人监督的方式完成，即不使用标签本身。</p><p id="73d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">降维实际上并不选择特征的子集，而是在低维空间中产生一组新的特征。这个新的集合可以用于分类过程本身。</p><p id="93c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下示例使用降维后的特征进行分类。更准确地说，它使用主成分分析<strong class="kw iu"/>(PCA)的前 2 个成分作为新的特征集。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="e306" class="nf lr it nb b gy ng nh l ni nj">from sklearn.datasets import load_iris<br/>from sklearn.decomposition import PCA<br/>from sklearn.svm import SVC<br/>import matplotlib.pyplot as plt<br/>from matplotlib.colors import ListedColormap</span><span id="7134" class="nf lr it nb b gy nk nh l ni nj">import numpy as np<br/>h = .01<br/>x_min, x_max = -4,4<br/>y_min, y_max = -1.5,1.5</span><span id="0f37" class="nf lr it nb b gy nk nh l ni nj"># loading dataset<br/>data = load_iris()<br/>X, y = data.data, data.target</span><span id="caca" class="nf lr it nb b gy nk nh l ni nj"># selecting first 2 components of PCA<br/>X_pca = PCA().fit_transform(X)<br/>X_selected = X_pca[:,:2]</span><span id="e283" class="nf lr it nb b gy nk nh l ni nj"># training classifier and evaluating on the whole plane<br/>clf = SVC(kernel='linear')<br/>clf.fit(X_selected,y)<br/>xx, yy = np.meshgrid(np.arange(x_min, x_max, h),<br/>                     np.arange(y_min, y_max, h))</span><span id="c10f" class="nf lr it nb b gy nk nh l ni nj">Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])<br/>Z = Z.reshape(xx.shape)</span><span id="5428" class="nf lr it nb b gy nk nh l ni nj"># Plotting<br/>cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])<br/>cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])<br/>plt.figure(figsize=(10,5))<br/>plt.pcolormesh(xx, yy, Z, alpha=.6,cmap=cmap_light)<br/>plt.title('PCA - Iris dataset')<br/>plt.xlabel('Dimension 1')<br/>plt.ylabel('Dimension 2')<br/>plt.scatter(X_pca[:,0],X_pca[:,1],c=data.target,cmap=cmap_bold)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/ce95fa2c69d5b8f99108db91828c6903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpKflAvXk7nzVyv7e938_A.png"/></div></div></figure><p id="562e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在评估特征的上下文中，降维的另一个用途是用于可视化:在较低维度的空间中，更容易从视觉上验证数据是否是潜在可分的，这有助于设置对分类准确性的期望。在实践中，我们对特征的子集执行维度缩减(例如 PCA ),并检查标签如何分布在缩减的空间中。如果它们看起来是分开的，这是一个明显的迹象，表明使用这组特征时，预期会有高的分类性能。</p><p id="1a76" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在下面的例子中，在一个 2 维的缩减空间中，不同的标签被显示为是相当可分的。这表明，在训练和测试分类器时，人们可以期待高性能。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="d646" class="nf lr it nb b gy ng nh l ni nj">from sklearn.datasets import load_iris<br/>from sklearn.decomposition import PCA<br/>import matplotlib.pyplot as plt</span><span id="a2aa" class="nf lr it nb b gy nk nh l ni nj">from mlxtend.plotting import plot_pca_correlation_graph</span><span id="124e" class="nf lr it nb b gy nk nh l ni nj">data = load_iris()<br/>X, y = data.data, data.target</span><span id="32f4" class="nf lr it nb b gy nk nh l ni nj">plt.figure(figsize=(10,5))<br/>X_pca = PCA().fit_transform(X)<br/>plt.title('PCA - Iris dataset')<br/>plt.xlabel('Dimension 1')<br/>plt.ylabel('Dimension 2')<br/>plt.scatter(X_pca[:,0],X_pca[:,1],c=data.target)<br/>_ = plot_pca_correlation_graph(X,data.feature_names)</span></pre><div class="kj kk kl km gt ab cb"><figure class="nm kn nn no np nq nr paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/ac5ae7ac9ea5304b194196e4e45c95e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*VKxn9Ptu9U8MuxVZnWjhjQ.png"/></div></figure><figure class="nm kn ns no np nq nr paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/afbc355101d608c32e6c4618cd66e03e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Vy1-UQWdKUIDvGlspg8KdA.png"/></div></figure></div><p id="d3ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">除此之外，我还绘制了<strong class="kw iu">相关圆</strong>，它显示了每个原始维度和新 PCA 维度之间的相关性。直观地说，该图显示了每个原始特征对新创建的 PCA 成分的贡献。在上面的例子中，花瓣长度和宽度与第一个主成分分析维度高度相关，萼片宽度对第二个维度贡献很大。</p></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="38b6" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">单变量滤波方法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/7e5e41f7fa200eb5108f5f35d08af28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jIDE_b7WsVO8JXaoHBiw6Q.jpeg"/></div></div></figure><p id="6099" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">过滤方法旨在对特征的重要性进行排序，而不使用任何类型的分类算法。</p><p id="14d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">单变量过滤方法单独评估每个特征，并且不考虑特征的相互作用。这些方法包括为每个特征提供一个分数，通常基于统计测试。</p><p id="023b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分数通常或者测量因变量和特征之间的相关性(例如 Chi2 和用于回归的 Pearls 相关系数)，或者测量给定类别标签的特征分布之间的差异(f 检验和 T 检验)。</p><p id="88e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">分数通常对基础数据的统计属性做出假设。理解这些假设对于决定使用哪种测试是很重要的，即使其中一些假设对于违反假设是稳健的。</p><p id="91d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基于统计测试的分数提供了一个<strong class="kw iu"> p 值</strong>，可以用来排除一些特征。如果 p 值高于某个阈值(通常为 0.01 或 0.05)，则会出现这种情况。</p><p id="5b5d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">常见测试包括:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/7a2a3f018499cb9e3584353072ca51e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9yzAkKWbl4fcHfira31RPg.png"/></div></div></figure><p id="768c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">包<code class="fe nv nw nx nb b">sklearn</code>实现了一些过滤方法。然而，由于大多数都是基于统计测试的，所以也可以使用统计包(比如<code class="fe nv nw nx nb b">statsmodels</code>)。</p><p id="83bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是一个例子:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="defd" class="nf lr it nb b gy ng nh l ni nj">from sklearn.feature_selection import f_classif, chi2, mutual_info_classif<br/>from statsmodels.stats.multicomp import pairwise_tukeyhsd</span><span id="2b09" class="nf lr it nb b gy nk nh l ni nj">from sklearn.datasets import load_iris</span><span id="3940" class="nf lr it nb b gy nk nh l ni nj">data = load_iris()<br/>X,y = data.data, data.target</span><span id="a5a9" class="nf lr it nb b gy nk nh l ni nj">chi2_score, chi_2_p_value = chi2(X,y)<br/>f_score, f_p_value = f_classif(X,y)<br/>mut_info_score = mutual_info_classif(X,y)</span><span id="06f2" class="nf lr it nb b gy nk nh l ni nj">pairwise_tukeyhsd = [list(pairwise_tukeyhsd(X[:,i],y).reject) for i in range(4)]</span><span id="1409" class="nf lr it nb b gy nk nh l ni nj">print('chi2 score        ', chi2_score)<br/>print('chi2 p-value      ', chi_2_p_value)<br/>print('F - score score   ', f_score)<br/>print('F - score p-value ', f_p_value)<br/>print('mutual info       ', mut_info_score)<br/>print('pairwise_tukeyhsd',pairwise_tukeyhsd)</span><span id="03b3" class="nf lr it nb b gy nk nh l ni nj">Out:</span><span id="4ebe" class="nf lr it nb b gy nk nh l ni nj">chi2 score         [ 10.82   3.71 116.31  67.05]<br/>chi2 p-value       [0.   0.16 0.   0.  ]<br/>F - score score    [ 119.26   49.16 1180.16  960.01]<br/>F - score p-value  [0. 0. 0. 0.]<br/>mutual info        [0.51 0.27 0.98 0.98]<br/>pairwise_tukeyhsd [[True, True, True], [True, True, True], [True, True, True], [True, True, True]]</span></pre><h2 id="0177" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">对特征进行分级的可视化方法</h2><h2 id="409a" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">箱线图和小提琴图</h2><p id="aa5f" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">箱线图/小提琴图可能有助于可视化给定类别的特征分布。对于 Iris 数据集，下面显示了一个示例。</p><p id="b158" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是有用的，因为统计测试通常只评估这种分布的平均值之间的差异。因此，这些图提供了关于特征质量的更多信息</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="5232" class="nf lr it nb b gy ng nh l ni nj">import pandas as pd<br/>import seaborn as sns<br/>sns.set()<br/>df = pd.DataFrame(data.data,columns=data.feature_names)<br/>df['target'] = data.target</span><span id="ef2c" class="nf lr it nb b gy nk nh l ni nj">df_temp = pd.melt(df,id_vars='target',value_vars=list(df.columns)[:-1], <br/>                  var_name="Feature", value_name="Value")<br/>g = sns.FacetGrid(data = df_temp, col="Feature", col_wrap=4, size=4.5,sharey = False)<br/>g.map(sns.boxplot,"target", "Value");<br/>g = sns.FacetGrid(data = df_temp, col="Feature", col_wrap=4, size=4.5,sharey = False)<br/>g.map(sns.violinplot,"target", "Value");</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/139b23735e4b1d14003bbc69ccbbe16e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUEZlf2dWElWkaGEdYwpKQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/f728131dcfcc10716c9e271d6970e0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9vKO5ytVs1UdZDwPsWk6jg.png"/></div></div></figure><h2 id="9ce2" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">用 ROC 曲线进行特征排序</h2><p id="9070" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">ROC 曲线可用于按重要性顺序排列特征，这给出了排列特征性能的直观方式。</p><p id="a771" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这种技术最适合二进制分类任务。为了应用于多类问题，可以使用<strong class="kw iu">微观或宏观</strong>平均值或基于多重比较的标准(类似于成对 Tukey 的范围测试)。</p><p id="6e84" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下示例绘制了各种特征的 ROC 曲线。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="1d7a" class="nf lr it nb b gy ng nh l ni nj">from sklearn.datasets import load_iris<br/>import matplotlib.pyplot as plt<br/>from sklearn.metrics import auc<br/>import numpy as np</span><span id="7551" class="nf lr it nb b gy nk nh l ni nj"># loading dataset<br/>data = load_iris()<br/>X, y = data.data, data.target</span><span id="881b" class="nf lr it nb b gy nk nh l ni nj">y_ = y == 2</span><span id="cbfb" class="nf lr it nb b gy nk nh l ni nj">plt.figure(figsize=(13,7))<br/>for col in range(X.shape[1]):<br/>    tpr,fpr = [],[]<br/>    for threshold in np.linspace(min(X[:,col]),max(X[:,col]),100):<br/>        detP = X[:,col] &lt; threshold<br/>        tpr.append(sum(detP &amp; y_)/sum(y_))# TP/P, aka recall<br/>        fpr.append(sum(detP &amp; (~y_))/sum((~y_)))# FP/N<br/>        <br/>    if auc(fpr,tpr) &lt; .5:<br/>        aux = tpr<br/>        tpr = fpr<br/>        fpr = aux<br/>    plt.plot(fpr,tpr,label=data.feature_names[col] + ', auc = '\<br/>                           + str(np.round(auc(fpr,tpr),decimals=3)))</span><span id="4534" class="nf lr it nb b gy nk nh l ni nj">plt.title('ROC curve - Iris features')<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/50b9e1db56b6eca835a735205ca0f145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBynkzQShYDrenQ6SafB1A.png"/></div></div></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="76f4" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">多元滤波方法</h1><p id="4b50" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">这些方法考虑了变量之间的相关性，而没有考虑任何类型的分类算法。</p><h2 id="fb24" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">mRMR</h2><p id="7a9e" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated"><strong class="kw iu"> mRMR(最小冗余最大相关性)</strong>是一种启发式算法，通过考虑特征的重要性和它们之间的相关性来寻找接近最优的特征子集。</p><p id="54cb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其思想是，即使两个特征高度相关，如果它们高度相关，将它们都添加到特征集中可能不是一个好主意。在这种情况下，添加两个特征会增加模型的复杂性(增加过度拟合的可能性)，但由于特征之间的相关性，不会添加重要的信息。</p><p id="a9a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在一组<em class="ol"> N </em>特征的<em class="ol"> S </em>中，特征的相关性(<em class="ol"> D </em>)计算如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/c72ee438e7f2c782f06ea33ee612cc01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yen7gH_IF_rexfVd3oed1g.png"/></div></div></figure><p id="57b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="ol"> I </em>为互信息算子。</p><p id="f232" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征的冗余表示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/6f6bb753f5b3f35a1df3bd428489e603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyHFTZB4YU8khRqa3xCN-A.png"/></div></div></figure><p id="9ff4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">集合<em class="ol"> S </em>的 mRMR 分数定义为(<em class="ol"> D - R) </em>。目标是找到具有最大值(<em class="ol"> D-R) </em>的特征子集。然而，在实践中，我们执行增量搜索(也称为前向选择)，在每一步，我们添加产生最大 mRMR 的特征。</p><p id="b399" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该算法由算法作者自己用 C 实现。你可以在这里找到这个包的源代码，以及原始论文<a class="ae oo" href="http://home.penglab.com/proj/mRMR/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="e564" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在名称<code class="fe nv nw nx nb b">pymrmr</code>上创建了一个(未维护的)python 包装器。如果<code class="fe nv nw nx nb b">pymrmr</code>有问题，我建议直接调用 C 级函数。</p><p id="4345" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的代码举例说明了<code class="fe nv nw nx nb b">pymrmr</code>的用法。注意，<code class="fe nv nw nx nb b">pandas</code>数据帧的列应按照 C 级包中的描述进行格式化(此处<a class="ae oo" href="http://home.penglab.com/proj/mRMR/" rel="noopener ugc nofollow" target="_blank">为</a>)。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="05d7" class="nf lr it nb b gy ng nh l ni nj">import pandas as pd<br/>import pymrmr</span><span id="6788" class="nf lr it nb b gy nk nh l ni nj">df = pd.read_csv('some_df.csv')<br/># Pass a dataframe with a predetermined configuration. <br/># Check http://home.penglab.com/proj/mRMR/ for the dataset requirements<br/>pymrmr.mRMR(df, 'MIQ', 10)</span></pre><p id="48ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">输出:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="cb1c" class="nf lr it nb b gy ng nh l ni nj">*** This program and the respective minimum Redundancy Maximum Relevance (mRMR)<br/>     algorithm were developed by Hanchuan Peng &lt;hanchuan.peng@gmail.com&gt;for<br/>     the paper<br/>     "Feature selection based on mutual information: criteria of<br/>      max-dependency, max-relevance, and min-redundancy,"<br/>      Hanchuan Peng, Fuhui Long, and Chris Ding,<br/>      IEEE Transactions on Pattern Analysis and Machine Intelligence,<br/>      Vol. 27, No. 8, pp.1226-1238, 2005.</span><span id="6bba" class="nf lr it nb b gy nk nh l ni nj">*** MaxRel features ***<br/> Order    Fea     Name    Score<br/> 1        765     v765    0.375<br/> 2        1423    v1423   0.337<br/> 3        513     v513    0.321<br/> 4        249     v249    0.309<br/> 5        267     v267    0.304<br/> 6        245     v245    0.304<br/> 7        1582    v1582   0.280<br/> 8        897     v897    0.269<br/> 9        1771    v1771   0.269<br/> 10       1772    v1772   0.269</span><span id="5974" class="nf lr it nb b gy nk nh l ni nj">*** mRMR features ***<br/> Order    Fea     Name    Score<br/> 1        765     v765    0.375<br/> 2        1123    v1123   24.913<br/> 3        1772    v1772   3.984<br/> 4        286     v286    2.280<br/> 5        467     v467    1.979<br/> 6        377     v377    1.768<br/> 7        513     v513    1.803<br/> 8        1325    v1325   1.634<br/> 9        1972    v1972   1.741<br/> 10       1412    v1412   1.689<br/>Out[1]:<br/> ['v765',<br/>  'v1123',<br/>  'v1772',<br/>  'v286',<br/>  'v467',<br/>  'v377',<br/>  'v513',<br/>  'v1325',<br/>  'v1972',<br/>  'v1412']</span></pre></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="1c0b" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">包装方法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/3c32fccd789e2dc4e40fd41775af2147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WnQSTzwrPVkCWxb3MRV4SA.jpeg"/></div></div></figure><p id="c595" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">包装器方法背后的主要思想是搜索哪组特性最适合特定的分类器。这些方法可以总结如下，并且在所使用的搜索算法方面有所不同。</p><ol class=""><li id="75f7" class="op oq it kw b kx ky la lb ld or lh os ll ot lp ou ov ow ox bi translated">选择一个性能指标(可能性、AIC、BIC、F1 分数、准确度、MSE、MAE…)，记为<strong class="kw iu"> M. </strong></li><li id="fc29" class="op oq it kw b kx oy la oz ld pa lh pb ll pc lp ou ov ow ox bi translated">选择一个分类器/回归器/ …，在这里记为<strong class="kw iu"> C </strong>。</li><li id="7412" class="op oq it kw b kx oy la oz ld pa lh pb ll pc lp ou ov ow ox bi translated"><strong class="kw iu">用给定的搜索方法搜索</strong>不同的特征子集。对于每个子集<strong class="kw iu"> S，</strong>执行以下操作:</li></ol><ul class=""><li id="416a" class="op oq it kw b kx ky la lb ld or lh os ll ot lp pd ov ow ox bi translated">使用<strong class="kw iu"> S </strong>作为分类器的特征，以交叉验证的方式训练和测试<strong class="kw iu">C</strong>；</li><li id="34b9" class="op oq it kw b kx oy la oz ld pa lh pb ll pc lp pd ov ow ox bi translated">从交叉验证程序中获得平均分数(对于指标<strong class="kw iu"> M </strong>)，并将该分数分配给子集<strong class="kw iu">S</strong>；</li><li id="4dc5" class="op oq it kw b kx oy la oz ld pa lh pb ll pc lp pd ov ow ox bi translated">选择一个新的子集并重做步骤<em class="ol">一个</em>。</li></ul><h2 id="ba15" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">详述步骤 3</h2><p id="4a3c" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">第三步未指定将使用哪种类型的搜索方法。几乎在任何情况下，测试所有可能的特征子集都是禁止的(<strong class="kw iu">强力选择</strong>)，因为这将需要执行步骤 3 指数次(特征数量的 2 次方)。除了时间复杂性之外，由于有如此大量的可能性，很可能某个特征组合仅仅是随机地表现得最好，这使得强力解决方案更容易过度拟合。</p><p id="e3c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">搜索算法在实践中往往能很好地解决这个问题。它们倾向于实现接近蛮力解决方案的性能，具有更少的时间复杂度和更少的过拟合机会。</p><p id="592f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">正向选择</strong>和<strong class="kw iu">反向选择</strong>(又名<strong class="kw iu">修剪</strong>)在实践中被大量使用，以及它们的搜索过程的一些小变化。</p><p id="b2d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">反向选择包括从具有全部特征的模型开始，并且在每一步中，移除没有特征的模型具有最高分数。正向选择以相反的方式进行:它从一组空的特征开始，并添加最能提高当前分数的特征。</p><p id="8f7c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">向前/向后选择仍然倾向于过度拟合，因为通常，分数倾向于通过添加更多特征来提高。避免这种情况的一种方法是使用惩罚模型复杂性的分数，如 AIC 或 BIC。</p><p id="e1db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">包装方法结构的图示如下。值得注意的是，特征集是(1)通过<strong class="kw iu">搜索方法</strong>找到的，以及(2)在打算使用的同一分类器上交叉验证的<strong class="kw iu">。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/802383a82247dda57e760559b05928e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AD2W_y3UP7pqOW197uiVXQ.png"/></div></div></figure><p id="9432" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第三步还开放了交叉验证参数。通常，使用 k-fold 程序。然而，使用大 k 会给整个包装器方法带来额外的复杂性。</p><h2 id="8dfb" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">包装方法的 Python 包</h2><p id="8b65" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">(<a class="ae oo" href="http://rasbt.github.io/mlxtend/" rel="noopener ugc nofollow" target="_blank">http://rasbt.github.io/mlxtend/</a>)是一个用于各种数据科学相关任务的有用包。这个包的包装方法可以在 SequentialFeatureSelector 上找到。它提供向前和向后的功能选择，有些变化。</p><p id="912e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该包还提供了一种通过函数 plot _ sequential _ feature _ selection 将分数可视化为要素数量的函数的方法。</p><p id="e451" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的例子摘自包的主页。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="2f2e" class="nf lr it nb b gy ng nh l ni nj">from mlxtend.feature_selection import SequentialFeatureSelector as SFS<br/>from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs</span><span id="9080" class="nf lr it nb b gy nk nh l ni nj">from sklearn.linear_model import LinearRegression<br/>from sklearn.datasets import load_boston</span><span id="82e9" class="nf lr it nb b gy nk nh l ni nj">boston = load_boston()<br/>X, y = boston.data, boston.target</span><span id="388c" class="nf lr it nb b gy nk nh l ni nj">lr = LinearRegression()</span><span id="8ae3" class="nf lr it nb b gy nk nh l ni nj">sfs = SFS(lr, <br/>          k_features=13, <br/>          forward=True, <br/>          floating=False, <br/>          scoring='neg_mean_squared_error',<br/>          cv=10)</span><span id="4452" class="nf lr it nb b gy nk nh l ni nj">sfs = sfs.fit(X, y)<br/>fig = plot_sfs(sfs.get_metric_dict(), kind='std_err')</span><span id="11a2" class="nf lr it nb b gy nk nh l ni nj">plt.title('Sequential Forward Selection (w. StdErr)')<br/>plt.grid()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/e3c001c9151ac4fa0b510c5a02a8f137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPL7lJUoc5bulhek4eSqHg.png"/></div></div></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="31e0" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">嵌入式方法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/1036bc5f088c2e39c2d3dd3ed07587af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fcDzWPA_fj5TLJcKvfvFNA.jpeg"/></div></div></figure><p id="8c60" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">训练一个分类器归结为一个优化问题，其中我们试图最小化其参数的函数(这里记为𝜃).这个函数被称为<strong class="kw iu">损失函数</strong>(记为𝐿(𝜃)).</p><p id="6212" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在一个更一般的框架中，我们通常希望最小化一个<strong class="kw iu">目标</strong> <strong class="kw iu">函数</strong>，它考虑了损失函数和对模型复杂性的<strong class="kw iu">惩罚</strong>(或<strong class="kw iu">正则化</strong>)(ω(𝜃):</p><p id="e6b2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">obj(𝜃)=𝐿(𝜃)+ω(𝜃)</strong></p><h2 id="aac7" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">线性分类器的嵌入式方法</h2><p id="de36" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">对于线性分类器(例如线性 SVM、逻辑回归)，损失函数表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/4ed5c415735e80b4ff00a69af9bba0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WMNgbzXySRWOdeF1UpXMgA.png"/></div></div></figure><p id="8c94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中每个<strong class="kw iu"> xʲ </strong>对应一个数据样本，而<strong class="kw iu"> Wᵀxʲ </strong>表示系数向量<strong class="kw iu"> (w₁,w₂,…w_n) </strong>与每个样本中的特征的内积。</p><p id="af00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于线性 SVM 和逻辑回归，铰链和逻辑损失分别为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/374ed9d883368ffef81534d5ec71c1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U5ondXRt899RK0Es_leR1Q.png"/></div></div></figure><p id="2de0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">线性分类器的两个最常见的惩罚是 L-1 和 L-2 惩罚:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/6345941828d0741f0287178b18657b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eRoW4R-M9jEY4E0pXy3IYA.png"/></div></div></figure><p id="bf8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> λ </strong>的值越高，惩罚越强，最优目标函数将趋向于以<strong class="kw iu">收缩</strong>越来越多的系数<strong class="kw iu"> w_i </strong>而结束。</p><p id="3d35" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">众所周知，“L1”惩罚会创建稀疏模型，这简单地意味着，在优化过程中，通过使一些系数等于零，它倾向于<strong class="kw iu">从模型中选择一些特征。</strong></p><p id="98c1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一个常见的处罚是 L-2。虽然 L-2 缩小了系数，因此有助于避免过拟合，但它不会创建稀疏模型，因此它不适合作为特征选择技术。</p><p id="a05a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于一些线性分类器(线性 SVM，逻辑回归)，可以有效地使用 L-1 罚分，这意味着有有效的数值方法来优化最终的目标函数。对于其他几个分类器(各种核 SVM 方法、决策树等等)，情况就不一样了。因此，<strong class="kw iu">不同的分类器应该使用不同的正则化方法</strong>。</p><p id="874e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">带有正则化的逻辑回归示例如下所示，我们可以看到，随着 C 的减少，算法排除了一些特征(想想如果<strong class="kw iu"> C </strong>为<strong class="kw iu"> 1/λ </strong>)。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="b8b6" class="nf lr it nb b gy ng nh l ni nj">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="f23d" class="nf lr it nb b gy nk nh l ni nj">from sklearn.svm import LinearSVC<br/>from sklearn.model_selection import ShuffleSplit<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.utils import check_random_state<br/>from sklearn import datasets<br/>from sklearn.linear_model import LogisticRegression</span><span id="bdd1" class="nf lr it nb b gy nk nh l ni nj">rnd = check_random_state(1)</span><span id="8a07" class="nf lr it nb b gy nk nh l ni nj"># set up dataset<br/>n_samples = 3000<br/>n_features = 15</span><span id="ec98" class="nf lr it nb b gy nk nh l ni nj"># l1 data (only 5 informative features)<br/>X, y = datasets.make_classification(n_samples=n_samples,<br/>                                        n_features=n_features, n_informative=5,<br/>                                        random_state=1)</span><span id="a1af" class="nf lr it nb b gy nk nh l ni nj">cs = np.logspace(-2.3, 0, 50)</span><span id="1b9f" class="nf lr it nb b gy nk nh l ni nj">coefs = []<br/>for c in cs:<br/>    clf = LogisticRegression(solver='liblinear',C=c,penalty='l1')<br/>    # clf = LinearSVC(C=c,penalty='l1', loss='squared_hinge', dual=False, tol=1e-3)<br/>    <br/>    clf.fit(X,y)<br/>    coefs.append(list(clf.coef_[0]))<br/>    <br/>coefs = np.array(coefs)<br/>plt.figure(figsize=(10,5))<br/>for i,col in enumerate(range(n_features)):<br/>    plt.plot(cs,coefs[:,col])<br/>plt.xscale('log')<br/>plt.title('L1 penalty - Logistic regression')<br/>plt.xlabel('C')<br/>plt.ylabel('Coefficient value')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/a9a2db699c66748d1be9c91648960cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YvvI1_ytooJX0ZszEYPIQ.png"/></div></div></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="62f8" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">基于树的模型的特征重要性</h1><p id="8efc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">另一种常见的特征选择技术包括从基于树的模型中提取特征重要性等级。</p><p id="f31c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">特征重要性本质上是由每个变量产生的分裂标准中的单个树的改进的平均值。换句话说，它是在使用特定变量分割树时分数(决策树符号上所谓的“杂质”)提高了多少。</p><p id="34f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它们可用于对要素进行分级，然后选择要素的子集。然而，<strong class="kw iu">应小心使用特性重要性，因为它们会受到偏差和的影响，并呈现出与高度相关特性</strong>相关的意外行为，不管它们有多强。</p><p id="7dd3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如本文中的<a class="ae oo" href="https://link.springer.com/content/pdf/10.1186%2F1471-2105-8-25.pdf" rel="noopener ugc nofollow" target="_blank">所示，随机森林特征重要性偏向于具有更多类别的特征。此外，如果两个特征高度相关，无论特征的质量如何，它们的分数都会大大降低。</a></p><p id="a9f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是如何从随机森林中提取要素重要性的示例。虽然是回归变量，但对于分类器来说，过程是相同的。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="bfa3" class="nf lr it nb b gy ng nh l ni nj">from sklearn.datasets import load_boston<br/>from sklearn.ensemble import RandomForestRegressor</span><span id="84ae" class="nf lr it nb b gy nk nh l ni nj">import numpy as np</span><span id="7004" class="nf lr it nb b gy nk nh l ni nj">boston = load_boston()<br/>X = boston.data<br/>Y = boston.target<br/>feat_names = boston.feature_names <br/>rf = RandomForestRegressor()<br/>rf.fit(X, Y)<br/>print("Features sorted by their score:")<br/>print(sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), feat_names), <br/>             reverse=True))<br/>Out:<br/>Features sorted by their score:<br/>[(0.4334, 'LSTAT'), (0.3709, 'RM'), (0.0805, 'DIS'), (0.0314, 'CRIM'), (0.0225, 'NOX'), (0.0154, 'TAX'), (0.0133, 'PTRATIO'), (0.0115, 'AGE'), (0.011, 'B'), (0.0043, 'INDUS'), (0.0032, 'RAD'), (0.0016, 'CHAS'), (0.0009, 'ZN')]</span></pre><h2 id="83aa" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">额外:树模型的主要杂质分数</h2><p id="6cd3" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如上所述，“杂质”是决策树算法在决定分割节点时使用的分数。有许多决策树算法(IDR3、C4.5、CART 等)，但一般规则是，我们用来分割树中节点的变量是对杂质产生最高改善的变量。</p><p id="ae57" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最常见的杂质是基尼杂质和熵。基尼系数杂质的改进被称为“<strong class="kw iu">基尼系数重要性</strong>，而熵的改进是<strong class="kw iu">信息增益。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/bbf6e0c55f708886ac0c2cd66d5ce8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jsCo6SlYTUI2AIVovfc3cg.png"/></div></div></figure><h2 id="3cc0" class="nf lr it bd ls ny nz dn lw oa ob dp ma ld oc od mc lh oe of me ll og oh mg oi bi translated">SHAP:来自树模型的可靠特征重要性</h2><p id="ac64" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">(感谢<a class="pk pl ep" href="https://medium.com/u/7ba65e2cae1e?source=post_page-----10c0ddd7918b--------------------------------" rel="noopener" target="_blank">恩里克·加斯帕里尼·菲乌萨·多纳西门托</a>的建议！)</p><p id="b969" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">SHAP 实际上远不止于此。它是一种算法，提供任何预测模型之外的模型解释。然而，对于基于树的模型，它特别有用:作者为这种模型开发了高速和精确(不仅仅是局部)的解释，与 X <em class="ol"> GBoost </em>、<em class="ol"> LightGBM </em>、<em class="ol"> CatBoost </em>和<em class="ol"> scikit-learn </em>树模型兼容。</p><p id="e776" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我鼓励检查一下 SHAP 提供的解释能力(比如<strong class="kw iu">特征依赖、交互效果、模型监控……</strong>)。下面，我(仅)绘制了 SHAP 输出的特征重要性，当对它们进行排序以进行特征选择时，这些特征重要性比原始树模型输出的特征重要性更可靠。这个例子摘自他们的<a class="ae oo" href="https://medium.com/p/10c0ddd7918b/edit" rel="noopener"> <em class="ol"> github </em> </a>页面。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="e9a7" class="nf lr it nb b gy ng nh l ni nj">import xgboost<br/>import shap</span><span id="20f3" class="nf lr it nb b gy nk nh l ni nj"># load JS visualization code to notebook<br/>shap.initjs()</span><span id="0956" class="nf lr it nb b gy nk nh l ni nj"># train XGBoost model<br/>X,y = shap.datasets.boston()<br/>model = xgboost.train({"learning_rate": 0.01}, xgboost.DMatrix(X, label=y), 100)</span><span id="b29c" class="nf lr it nb b gy nk nh l ni nj"># explain the model's predictions using SHAP values<br/># (same syntax works for LightGBM, CatBoost, and scikit-learn models)<br/>explainer = shap.TreeExplainer(model)<br/>shap_values = explainer.shap_values(X)</span><span id="f495" class="nf lr it nb b gy nk nh l ni nj">shap.summary_plot(shap_values, X, plot_type="bar")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/aee5d31bc5910f0c69129eb4f767aaf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7733HHmJQY1JpIKzDX6z6A.png"/></div></div></figure></div><div class="ab cl mo mp hx mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="im in io ip iq"><h1 id="c4af" class="lq lr it bd ls lt mv lv lw lx mw lz ma jz mx ka mc kc my kd me kf mz kg mg mh bi translated">结论——何时使用每种方法？</h1><p id="31fc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">嵌入式方法对于避免过度拟合和选择有用的变量通常非常有效。它们也是时间有效的，因为它们嵌入在目标函数中。它们的主要缺点是它们可能无法用于所需的分类器。</p><p id="3e51" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">包装方法在实践中往往工作得很好。然而，它们在计算上是昂贵的，特别是当处理数百个特征时。但是如果你有计算资源，这是一个很好的方法。</p><p id="2a5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果特征集非常大(大约数百或数千)，因为过滤方法很快，它们可以很好地作为选择的第一阶段，以排除一些变量。随后，可以将另一种方法应用于已经缩减的特征集。例如，如果您想要创建要素的组合，将它们相乘或相除，这将非常有用。</p><h1 id="b213" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">参考</h1><p id="9330" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated"><a class="ae oo" href="http://www.jmlr.org/papers/volume3/guyon03a/guyon03a.pdf" rel="noopener ugc nofollow" target="_blank">变量和特征选择介绍</a></p><p id="b9cd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae oo" href="https://link.springer.com/content/pdf/10.1186%2F1471-2105-8-25.pdf" rel="noopener ugc nofollow" target="_blank">随机森林变量重要性测量中的偏差:例证、来源和解决方案</a></p><p id="5be6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae oo" href="https://pdfs.semanticscholar.org/310e/a531640728702fce6c743c1dd680a23d2ef4.pdf" rel="noopener ugc nofollow" target="_blank">用于分类的特征选择:综述</a></p></div></div>    
</body>
</html>