<html>
<head>
<title>Codifying adversarial examples as Features</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将对立的例子编纂为特征</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/codifying-adversarial-examples-as-features-a886f6b1f7b6?source=collection_archive---------32-----------------------#2019-05-24">https://towardsdatascience.com/codifying-adversarial-examples-as-features-a886f6b1f7b6?source=collection_archive---------32-----------------------#2019-05-24</a></blockquote><div><div class="fc if ig ih ii ij"/><div class="ik il im in io"><div class=""/><div class=""><h2 id="e02e" class="pw-subtitle-paragraph jo iq ir bd b jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf dk translated">分离健壮和非健壮特征</h2></div><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj kg"><img src="../Images/6fdd471c9f68213d48229844112a6a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AyvZChrhJJ4JOCly5NMnQg.jpeg"/></div></div><figcaption class="ks kt gk gi gj ku kv bd b be z dk">Photo by <a class="ae kw" href="https://unsplash.com/photos/flha0KwRrRc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Nahel Abdul Hadi</a> on <a class="ae kw" href="https://unsplash.com/search/photos/hacker?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="09fb" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">对立的例子对人工智能从业者来说是一个巨大的麻烦，但在人工智能理论中却是一个巨大的红利，有助于我们理解机器学习模型和算法的内部，并为像甘这样的新兴技术注入活力。</p><p id="556e" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">因此，毫不奇怪，我将在这里回顾的这篇新论文在业界引起了轰动。</p><figure class="kh ki kj kk gu kl"><div class="bz fq l di"><div class="lt lu l"/></div></figure><p id="5210" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">或者，如果您喜欢使用另一个 PDF 阅读器应用程序阅读，请点击<a class="ae kw" href="https://arxiv.org/pdf/1905.02175.pdf" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><p id="1eea" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">计算机视觉算法的问题之一(详见 Hinton 的 Coursera 课程)是间接推理。这里的数据流可以用下图表示:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj lv"><img src="../Images/1311dce3f6cf764262a9143744617295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*WLa5RA6gfZmRYlClwTZrKA.png"/></div></figure><p id="69f9" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">计算机视觉算法的困难在于从物体到标签没有直接的联系。取而代之的是，拍摄物体的照片，用像素表示，算法学习从像素而不是物体本身获得标签。算法不确定你说的标签是什么意思。如果你给它一张绵羊在山坡上吃草的图像，算法并不知道绵羊是白色物体，还是下面的绿色物体，或者上面的蓝色物体。因此，你需要向算法提供更多的训练数据，以说服它一只羊与天空或草地不同。然而，它可能会学到错误的东西。例如，它可以学习基于 fir 或其他次要特征来识别猫和狗。这就是漏洞所在，通过操纵像素，你可以让算法错误地识别对象。</p><p id="a306" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">为了对抗对抗性学习，建议更新优化算法，以在对抗性失真下最小化成本函数而不是成本函数的最大值:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj lw"><img src="../Images/18c91bc512baf02bf8cd3c69ae273c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*hvpj_VOpAg82fSPup8uWVQ.png"/></div></figure><p id="9981" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在本文中，除了使用对抗性损失函数之外，还提出了识别鲁棒性特征，即在对抗性失真下不改变相关性符号的特征。</p><p id="f5bf" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">关于这篇论文，有一点要提一下，就是所谓的特性实际上是指倒数第二层的激活。在这种情况下，我们可以使用简单的皮尔逊相关，因为所有的非线性都是在前面的层中处理的。</p><p id="46a8" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">本文试图做的第一件事是理清稳健和非稳健特征。首先，我们使用对抗性损失函数来训练模型，然后用仅生成稳健特征的修改数据来替换训练数据:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj lx"><img src="../Images/c7e270282f244c974b4fc9bb7df5974b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*CYiqraOXuIVm_nkjQqB_kw.png"/></div></figure><p id="308b" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">这里的条件是只有鲁棒特征与标签相关，而非鲁棒特征与标签的相关性为零。这个数据集是通过使用反向传播来发现的，但是更新输入，而不是权重和偏差(很像在白盒对抗学习中)，最小化倒数第二层的激活的平方差:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj ly"><img src="../Images/2cbc67e944b1850ddb2955ff89842830.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*qnH-U-h95qRFM8GfG4GxTw.png"/></div></figure><p id="17ed" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在获得修改的数据集之后，对其执行另一轮训练，这一次使用标准(非对抗性)训练。最终的模型被发现在标准和敌对的环境下都能很好地工作。这个结果真的是个好消息，因为使用对抗性损失函数训练非常慢，你也不想太频繁。这里我们只做一次，然后生成新的测试数据，然后使用高效的标准训练程序来做我们的训练实验。在我看来，这是论文最好的实用结果。</p><p id="c728" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">以类似的方式，您可以生成非稳健数据集，而无需使用对抗性训练。结果如下图所示:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj lz"><img src="../Images/6e8f7191683576fee476d7b3dc1859b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uzc8-VBA0G8S1SL7UmZyWw.png"/></div></div></figure><p id="4eaa" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在我看来，作者没有实现健壮和非健壮特征的分离，因为使用了不同的训练程序，所以两者之间不可避免地存在重叠。</p><p id="0d9d" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">在接下来的实验中，作者故意给这些例子贴上随机标签。他们发现，使用鲁棒特征训练的模型会记住原始数据集，并纠正错误标记的数据本身，而标准分类器会学习新的和不正确的分类。这既是好消息，也是坏消息。虽然它表明该模型对错误标记是鲁棒的，但这也意味着降低了迁移学习的泛化能力和有用性。</p><p id="99da" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">本文的理论框架在于将数据点表示为两个高斯分布的混合，对应于二分类问题中的两个类别。使用最大似然法找到分布的参数，并且这些参数对应于逻辑回归中的参数。这是统计学习中的一种已知方法，可以追溯到几十年前。对抗性鲁棒学习使用相同的方法，但是扩展了样本以包括所有可能的对抗性例子:</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div class="gi gj ma"><img src="../Images/a761f802e057d0ddd22067a9df6a8df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*OX7WUcAsYNcrd1g9uUCG6Q.png"/></div></figure><p id="2e80" class="pw-post-body-paragraph kx ky ir kz b la lb js lc ld le jv lf lg lh li lj lk ll lm ln lo lp lq lr ls ik bi translated">因此，增加对抗性噪声<strong class="kz is"> ε </strong>有效地增加了习得的<strong class="kz is">σ</strong>，混合了组合分布。</p><figure class="kh ki kj kk gu kl gi gj paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gi gj mb"><img src="../Images/c53aefccbf00f0fc2eb1c097cb8a75bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6e868wVmGcNlMHwGd5LYA.png"/></div></div></figure><h1 id="05e3" class="mc md ir bd me mf mg mh mi mj mk ml mm jx mn jy mo ka mp kb mq kd mr ke ms mt bi translated">结论</h1><p id="d3b5" class="pw-post-body-paragraph kx ky ir kz b la mu js lc ld mv jv lf lg mw li lj lk mx lm ln lo my lq lr ls ik bi translated">本文是对抗鲁棒学习的一般框架的发展，在过去几年中一直在积极工作。它不再关注模型，而是将注意力转移到数据上，根据原始数据生成数据集，经过训练后，这些数据集会生成一个健壮的模型。我认为，这是本文的最大贡献，因为你可以节省时间，避免进行缓慢而昂贵的对抗性训练，而不是第一次为了产生稳健的数据集。</p><h1 id="becb" class="mc md ir bd me mf mg mh mi mj mk ml mm jx mn jy mo ka mp kb mq kd mr ke ms mt bi translated">参考</h1><ol class=""><li id="2cb1" class="mz na ir kz b la mu ld mv lg nb lk nc lo nd ls ne nf ng nh bi translated">安德鲁·易勒雅斯、什巴尼·桑图尔卡、迪米特里斯·齐普拉斯、洛根·恩斯特罗姆、布兰登·特兰、亚历山大·马德瑞。<em class="ni">对立的例子不是 bug，它们是特性</em>。<a class="ae kw" href="https://arxiv.org/abs/1905.02175" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.02175</a></li></ol></div></div>    
</body>
</html>