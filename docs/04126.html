<html>
<head>
<title>Suicide in the 21st Century (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">21 世纪的自杀(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/suicide-in-the-21st-century-part-2-d8f9c4b9e588?source=collection_archive---------17-----------------------#2019-06-28">https://towardsdatascience.com/suicide-in-the-21st-century-part-2-d8f9c4b9e588?source=collection_archive---------17-----------------------#2019-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/e4e26e61696968346ef91b6407125472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X1aCp2F4qf_-a52u"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@paolitta?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Paola Chaaya</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a65d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">欢迎回来！如果您没有了解第 1 部分，可以在下面找到:</p><div class="lb lc gp gr ld le"><a rel="noopener follow" target="_blank" href="/suicide-in-the-21st-century-part-1-904abe8e1f5c"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">21 世纪的自杀(上)</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">自杀不会传染，我们需要谈论它。</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls jw le"/></div></div></a></div><p id="487a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，第 2 部分将包含机器学习，或者更具体地说，使用 python 中的 K-Means 进行机器学习。</p><p id="fbf9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始之前，如果您错过了第 1 部分，这里有一个快速回顾。</p><h1 id="ff35" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">概述</h1><p id="eec8" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在第一部分中，我们主要做了数据预处理和一些 EDA(探索性数据分析)。使用的数据是来自<a class="ae kc" href="https://www.kaggle.com/russellyates88/suicide-rates-overview-1985-to-2016" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的全球自杀率数据集。我们清理了数据，然后增加了一个额外的栏目，快乐得分，摘自联合国每年发布的《世界快乐报告》。然后，我们以图表的形式进行了一些基本的视觉数据分析，其中一些可以在下面看到:</p><div class="mw mx my mz gt ab cb"><figure class="na jr nb nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/311709b2229e98033e1f99c6f073d7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Qu7Rgmv1Cp4YNIz01ChttA.png"/></div></figure><figure class="na jr ng nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/ba53a5269d3868b6901ab1f19a896e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*AWnlY5ruXca2VNxyntvZWA.png"/></div></figure></div><div class="ab cb"><figure class="na jr nh nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/9a1f18b4fff3b022948717c4afb7becc.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*lV_GDAeZdQc8GGsTarfcqw.png"/></div></figure><figure class="na jr ni nc nd ne nf paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/815ac3bf808911d90bbffe8035a135de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*WOk9wPVE8A7WBD5To3XB7w.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk nj di nk nl">Plots taken from Part 1</figcaption></figure></div><p id="bf35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经概括了，我们可以继续进行一些机器学习！</p><h2 id="1e2e" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mh ks nt nu ml kw nv nw mp nx bi translated">k 均值</h2><p id="7b6b" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">K-Means 是一种广泛使用且相对简单的无监督聚类机器学习技术。该算法迭代地工作，并试图将数据集划分为<em class="ny"> K </em>个预定义的子组(或聚类),由此每个数据点属于一个组。它旨在使聚类间的数据点尽可能相似，同时也试图使聚类尽可能远离。该算法的工作原理如下:</p><ol class=""><li id="d62f" class="nz oa iq kf b kg kh kk kl ko ob ks oc kw od la oe of og oh bi translated">初始化——首先，K-Means 从数据集中随机选择<em class="ny"> K </em>个数据点作为初始质心(质心是一个聚类的中心)。注意，该算法还不知道聚类的正确位置。</li><li id="1040" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la oe of og oh bi translated">聚类分配-计算每个数据点和聚类中心之间的欧几里德距离，然后将数据点分配给与聚类中心的距离最小的聚类。</li><li id="dc39" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la oe of og oh bi translated">移动质心——使用以下公式重新计算新的聚类中心，其中 Ci 代表第<em class="ny">和第</em>个聚类中的数据点数量。</li></ol><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi on"><img src="../Images/9ff6ef2f464829bae9f3f3efdb9e784d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5pI1Jy39CJUbmq3mOKwHZQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Recalculating Cluster Centres</figcaption></figure><h2 id="b82d" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mh ks nt nu ml kw nv nw mp nx bi translated">准备</h2><p id="f7d3" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在实现任何类型的机器学习之前，确保数据准备正确是至关重要的。否则，算法极有可能会出错。</p><p id="d98b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为准备工作的第一步，在先前使用的数据框架中增加了一个额外的列:基尼系数。基尼指数是由科拉多·基尼在 1912 年发展起来的一种衡量收入分配的方法；该指数(或系数)被用作衡量经济不平等的标准。系数用 0 到 1 之间的值来度量，其中 0 表示完全相等，1 表示完全不相等。基尼指数的数据是从中央情报局公共图书馆中提取的，并以与之前用于幸福指数相同的方式添加；创建一个所有基尼指数值的列表，然后读入新列，再次转换为浮点值。</p><p id="ab56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如前所述，K-Means 使用欧几里德距离，当面对比例/单位非常不同的要素时，这可能会成为问题。例如，GDP 通常是几万，而幸福指数通常是一个小于 10 的浮动值，这意味着 K 均值将是非常次优的。为了避免这一障碍，将最小-最大缩放器应用于数据帧。在此缩放中，从所有值中减去最小值。然后，将这些值除以最小值和最大值之间的差值，得到值在 0 和 1 之间的完整数据集。两个字符串列(国家和大陆)被删除，因为 MinMaxScaling 不能应用于字符串。然后应用定标器，产生一个新的数据帧— dfscaled —包含 sizes/100k pop、GdpPerCapita($)、HappinessScore 和 GiniIndex。</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="5042" class="nm lu iq op b gy ot ou l ov ow">from sklearn import preprocessing<br/>dfcontpre =  dfcont.drop('Country', axis=1)<br/>dfcontpre = dfcontpre.drop('Continent', axis=1)<br/>minmax_processed = preprocessing.MinMaxScaler().fit_transform(dfcontpre)<br/>dfscaled = pd.DataFrame(minmax_processed, index=dfcontpre.index, columns=dfcontpre.columns)</span></pre><p id="fc54" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实现该算法之前，选择最佳数量的<em class="ny"> K </em>个簇是很重要的。这在理论上可以通过反复试验来实现；然而，绘制肘形曲线更有效。这种方法的思想是对一系列值<em class="ny"> k </em>(在这种情况下，1–20)的数据集运行 K 均值，并对每 K 次迭代计算和绘制误差平方和(SSE)。理想情况下，这个线图看起来像一只手臂，肘部显示了数据集中聚类的最佳数量<em class="ny"> k </em>。</p><p id="8657" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们定义一个好的调色板</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="d66c" class="nm lu iq op b gy ot ou l ov ow">flatui = ["#6cdae7", "#fd3a4a", "#ffaa1d", "#ff23e5", "#34495e", "#2ecc71"]<br/>sns.set_palette(flatui)<br/>sns.palplot(sns.color_palette()</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5861d9dac1f53d1074f42bf55ae4650e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*JyTj8J6OUD8ul-ARyhiRvQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Our Palette for future plots</figcaption></figure><p id="6ee5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以绘制肘部曲线:</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="482c" class="nm lu iq op b gy ot ou l ov ow">from sklearn.cluster import KMeans<br/>from sklearn.decomposition import PCA</span><span id="5eee" class="nm lu iq op b gy oy ou l ov ow">#plotting elbow curve for ideal number of clusters<br/>Nc = range(1, 20)<br/>kmeans = [KMeans(n_clusters=i) for i in Nc]<br/>score = [kmeans[i].fit(Y).score(Y) for i in range(len(kmeans))]<br/>pl.plot(Nc,score)<br/>plt.xlabel('Number of Clusters')<br/>plt.ylabel('Score')<br/>plt.title('Elbow Curve')<br/>plt.show()</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oz"><img src="../Images/9cedd4700dd2749c0e5f6d1b5a6a0ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyRRPsw1VdRtJ55hG2Da-w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Plotted elbow curve</figcaption></figure><p id="4997" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从肘图中可以看出，3 将是最优的聚类数，虽然肘图并没有过分的与众不同，但足以决定最优的<em class="ny"> k </em>个聚类。</p><p id="e850" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经决定了要使用的最佳聚类数，我们可以开始实现 K-Means 了。</p><h2 id="389b" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mh ks nt nu ml kw nv nw mp nx bi translated">履行</h2><p id="1e44" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated"><strong class="kf ir"> <em class="ny">自杀率/100k pop vs GdpPerCapita($)</em></strong></p><p id="fb69" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，将两列的数据放入 1D NumPy 数组中。然后将这些数组压缩在一起，形成一个 2D NumPy 数组，其示例如下所示:</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="c140" class="nm lu iq op b gy ot ou l ov ow">print X[0:5]</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/8ad6e25a26c280c3b95f3fe841bb03fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*xGbL5orkciGGJkduaQmJOQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">A sample of the scaled 2D NumPy array (Note that all values are between 0 and 1)</figcaption></figure><p id="d9c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，使用前面提到的三个集群，在 2D 阵列 X 上运行 K-Means。然后创建一个散点图，X 轴和 Y 轴分别为硅化物/100kPop 和 GdpPerCapita($)。然后使用 kmeans.labels_，通过颜色将这些分散的数据分开，这意味着它们是根据 kmeans 分配的聚类进行分离的。然后使用 cmap 选择“viridus”配色方案绘制颜色(颜色对比比我们自定义的调色板更好)。最后，使用 kmeans.cluster_centres_ 绘制聚类中心，生成以下图:</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="eb1c" class="nm lu iq op b gy ot ou l ov ow">#k-means plot suicide rate vs gdp, 3 clusters<br/>kmeans = KMeans(n_clusters = 3, random_state = 0)                   <br/>kmeans.fit(X) <br/>plt.figure(figsize=(20,16))<br/>plt.scatter(X[:, 0], X[:, 1],c = kmeans.labels_,cmap='viridis', s = 300)<br/>plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'red')</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pb"><img src="../Images/d66c38abfb4d2a34415eaaa859a33d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yILld3po5S9VmY-HA4NtEQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">GDP vs Suicide Rates using K-Means</figcaption></figure><p id="1368" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个图显示了一个有趣的结果。可以看出，不存在 x 和 y 都为高的数据，当回头参考图 14 时，这是有意义的。因此，我们可以将这些集群分类如下:</p><ul class=""><li id="1acb" class="nz oa iq kf b kg kh kk kl ko ob ks oc kw od la pc of og oh bi translated">深紫色:高 GDP 低自杀风险国家</li><li id="d4bd" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">黄色:低 GDP 低自杀风险国家</li><li id="65ef" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">提尔:低 GDP 高自杀风险国家</li><li id="8f5a" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">相当令人惊讶的是，K-Means 成功地将这些群体有效地聚类，主要是因为当看起来人均 GDP 和每 10 万人口自杀率之间的相关性相当低。</li></ul><p id="2ece" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们希望看到哪个国家被分配到哪个集群，我们也可以运行下面的代码:</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="c6a7" class="nm lu iq op b gy ot ou l ov ow">cluster_map = pd.DataFrame()<br/>cluster_map['data_index'] = dfscaled.index.values<br/>cluster_map['cluster'] = kmeans.labels_<br/>cluster_map.head(50)</span></pre><p id="5e2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="ny">自杀/100kPop vs .快乐得分</em> </strong></p><p id="35da" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将 K-Means 应用于自杀/100kPop 和快乐核心的管理方式大致相同；两个一维 NumPy 数组被压缩成一个二维数组，然后 K-Means 对其进行处理以创建必要的聚类。然后绘制颜色并添加聚类质心。</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="66aa" class="nm lu iq op b gy ot ou l ov ow">#1d numpy arrays zipped to 2d<br/>f1 = dfscaled['Suicides/100kPop'].values<br/>f2 = dfscaled['HappinessScore'].values<br/>X = np.array(list(zip(f1, f2)))<br/>#k-means suicide rate vs happiness score<br/>kmeans = KMeans(n_clusters = 3, random_state = 0)                   <br/>kmeans.fit(X) <br/>plt.figure(figsize=(20,16))<br/>plt.scatter(X[:, 0], X[:, 1],c = kmeans.labels_,cmap='viridis', s = 300)<br/>plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'red')</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pd"><img src="../Images/7745bde0d244bda6cfba295a47b9d629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUzUAJxB0EBbGWUxZXYHQw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">HappinessScore vs Suicide Rates using K-Means</figcaption></figure><p id="bdef" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，从算法中可以看到三个相当不同的集群，它们可以分类如下:</p><ul class=""><li id="58a7" class="nz oa iq kf b kg kh kk kl ko ob ks oc kw od la pc of og oh bi translated">黄色:低风险的“快乐”国家</li><li id="646b" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">深紫色:低风险的“幸福”国家</li><li id="d968" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">缇尔:高风险“不快乐”国家</li></ul><p id="6d64" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，当看到低相关性(-0.24)(来自第 1 部分)时，数据被有效地聚类是令人惊讶的</p><p id="7ffc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir"> <em class="ny">自杀率/100kPop vs .基尼指数</em> </strong></p><p id="39a6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将 K-Means 应用于自杀率/100kPop 和基尼系数的方法与之前相同。</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="a1ef" class="nm lu iq op b gy ot ou l ov ow">#1d numpy arrays zipped to 2d<br/>f1 = dfscaled['Suicides/100kPop'].values<br/>f2 = dfscaled['GiniIndex'].values<br/>X = np.array(list(zip(f1, f2)))<br/>#plot k-means  suicide rate vs gini index<br/>kmeans = KMeans(n_clusters = 3, random_state = 0)                   <br/>kmeans.fit(X) <br/>plt.figure(figsize=(20,16))<br/>plt.scatter(X[:, 0], X[:, 1],c = kmeans.labels_,cmap='viridis', s = 300)<br/>plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'red')</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi pe"><img src="../Images/85727ee11c7a7fa26ed056a773b7b085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZY7CbZaALJCWQimgoApGgw.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">GiniIndex vs Suicide Rates using K-Means</figcaption></figure><p id="1078" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，该算法显示了三个不同的聚类，与之前的结果非常相似，右上角没有数据点(高 X 值和高 Y 值)。这些数据可以分类如下:</p><ul class=""><li id="a4be" class="nz oa iq kf b kg kh kk kl ko ob ks oc kw od la pc of og oh bi translated">深紫色:低风险的“不平等”国家</li><li id="f749" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">缇尔:低风险的“平等”国家</li><li id="57cc" class="nz oa iq kf b kg oi kk oj ko ok ks ol kw om la pc of og oh bi translated">黄色:高风险“平等”国家</li></ul><p id="1efa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个非常令人惊讶的结果，因为高基尼指数意味着更高的财富不平等，而且没有一个国家既有非常高的基尼指数，又有非常高的自杀率。这可以用多种因素来解释，例如财富不平等的国家的自杀率可能比报道的数字高得多。虽然这个数据的含义的表述本身就令人惊讶，但可以通过数据相关性来解释。</p><p id="3852" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在纳入新栏的同时再次关联数据显示，自杀率/100 千人口与基尼系数之间的相关性为-0.17，这意味着存在轻微的反比关系，即随着基尼系数的增加，自杀率/100 千人口下降。为了显示这些特征对自杀/100kPop 结果的重要性，可以应用通过随机森林的排列重要性。</p><h2 id="02c9" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mh ks nt nu ml kw nv nw mp nx bi translated">排列重要性</h2><p id="64b8" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">要素重要性是一种计算哪些要素对预测影响最大的方法。有许多方法可以实现这一点；然而，排列的重要性是最快的和广泛使用的。<br/>排列重要性的工作原理是，当一个特性变得不可用时，可以通过观察分数或准确度降低多少来测量特性的重要性。理论上，这可以通过删除一个特性，重新训练估计器，然后检查结果得分，并对每个特性重复进行测试。这将花费大量时间，并且计算量相当大。相反，从数据集的测试部分“移除”了一个特征。估计者会期望该特征存在，因此完全移除该特征会导致误差；因此，特征值被混洗所有特征值产生的噪声所取代。</p><p id="ca88" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还应该注意的是，排列重要性与大量的列进行斗争，因为它会变得非常耗费资源。然而，对于本文中使用的数据集来说，这不是问题，因为列很少。</p><p id="72c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，项目中引入了必要的模块——排列重要性和随机森林回归量。移除索引列时，从 dfscaled 创建了一个新的数据框 dfrfr。目标变量(自杀/100kPop)存储在 y 中，而其余的列存储在 x 中。然后实施使用随机森林的排列重要性，并显示结果权重和特征。</p><pre class="mw mx my mz gt oo op oq or aw os bi"><span id="8c61" class="nm lu iq op b gy ot ou l ov ow">#import modules for permutation importance, and show output<br/>import eli5<br/>from eli5.sklearn import PermutationImportance<br/>from sklearn.ensemble import RandomForestRegressor</span><span id="def2" class="nm lu iq op b gy oy ou l ov ow">dfrfr = dfscaled.drop("index", axis = 1)<br/>rfr = RandomForestRegressor(random_state=42)</span><span id="c1ac" class="nm lu iq op b gy oy ou l ov ow">y = dfrfr[['Suicides/100kPop']].values<br/>X = dfrfr.drop('Suicides/100kPop',axis=1).values</span><span id="1c9e" class="nm lu iq op b gy oy ou l ov ow">perm = PermutationImportance(rfr.fit(X,y), random_state=42).fit(X, y)<br/>eli5.show_weights(perm, feature_names = dfrfr.drop('Suicides/100kPop',axis=1).columns.tolist())</span></pre><figure class="mw mx my mz gt jr gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/e0c5c268801bb7401ddf4ba4200efd7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*Z_gacpcG7lQDT0pkVqHsrQ.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Permutation Importance using Random Forest</figcaption></figure><p id="8945" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当回头参考特征相关性并看到 HappinessScore 与自杀/100kPop 的相关性最高时，这个结果是有意义的，其次是 GdpPerCapita($)，最后是 GiniIndex。</p><h2 id="c8cb" class="nm lu iq bd lv nn no dn lz np nq dp md ko nr ns mh ks nt nu ml kw nv nw mp nx bi translated">结论</h2><p id="4792" class="pw-post-body-paragraph kd ke iq kf b kg mr ki kj kk ms km kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">总体而言，尽管数据没有显示预期的某些特征的影响，但它仍然显示了这些影响的一个非常有趣的结论，例如收入不平等对各国自杀率的影响似乎无关紧要——这是以前被认为有显著影响的。</p><p id="8f8a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于缺乏预期紧密相关的特征之间的相关性，该数据受到限制，例如，幸福感得分和自杀率/100k，因为普遍不幸福被认为会增加自杀率。它还受到缺乏某些国家和某些年份的数据的限制，这意味着必须对 2015 年而不是 2018/19 年的数据进行主要分析。</p><p id="f209" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在未来的分析中，最好有一个特征被证实对各国的自杀率有很大影响。这将允许更精确的图，也允许更有意义的 K-均值聚类。</p></div><div class="ab cl pg ph hu pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="ij ik il im in"><p id="636d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章的完整代码可以在下面的我的 github 上找到:</p><div class="lb lc gp gr ld le"><a href="https://github.com/HarryBitten/Suicide-Rates-Analysis" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">哈里比特/自杀率分析</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">通过在 GitHub 上创建一个帐户，为 Harry bitten/自杀率分析开发做出贡献。</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">github.com</p></div></div><div class="ln l"><div class="pn l lp lq lr ln ls jw le"/></div></div></a></div></div><div class="ab cl pg ph hu pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="ij ik il im in"><p id="27a1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢阅读！我对这一切都很陌生，但我打算继续发布许多有趣的项目，主要是在数据科学/机器学习方面</p></div></div>    
</body>
</html>