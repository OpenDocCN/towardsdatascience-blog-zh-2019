<html>
<head>
<title>Reinforcement Learning Using a Single Demonstration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单一演示的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-using-a-single-demonstration-7889fe5e9f41?source=collection_archive---------9-----------------------#2019-02-16">https://towardsdatascience.com/reinforcement-learning-using-a-single-demonstration-7889fe5e9f41?source=collection_archive---------9-----------------------#2019-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/3dde327c193437418c1de3fcd06559dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*9vlKL9TkewHFqfpCzh_VPg.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk"><a class="ae kb" href="https://pdfs.semanticscholar.org/5e01/c03f410e397d291f4e4472cd7876e9db873f.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="4a4e" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated"><strong class="ke iu">从演示中学习</strong></p><p id="b751" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">强化学习对未来有很大的希望；最近的成就显示了它在超级人类水平上解决问题的能力，像玩棋盘游戏，控制机械臂和在专业水平上玩实时策略游戏。这些成就展示了发现新策略的能力，这些新策略优于我们人类能够设计的策略，这是一个令人兴奋的前景。</p><p id="3488" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">RL 的另一个可能用途是在人的表现足够好的情况下，使人的决策自动化。在这个设置中，我们希望我们的代理<strong class="ke iu">模仿人类专家</strong>使用的策略，这为我们的代理提供了完成任务的“正确”方法的演示。这在几种情况下非常方便，例如:</p><p id="372c" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">a)当我们不能制定奖励函数，但是我们可以询问专家来指导学习过程，或者已经记录了专家的行为及其结果的数据时。这方面的一个实际例子可能是尝试学习“帮助台”风格的政策，其中我们的代理需要与客户互动并帮助解决他们的问题。由于我们不能很容易地对顾客的回报函数进行建模，我们不能采用标准的 RL 技术，但是我们可以利用人类专家的日志来学习策略。</p><p id="88c9" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">b)当我们有一个“标准”类型的学习问题，但从头开始学习太难了，我们的算法完全失败了。专家的示范可以帮助我们的学习算法朝着正确的方向前进</p><p id="b8a3" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">需要注意的重要一点是，专家不一定必须是人类。也许我们有一个优化或搜索算法可以完成这项工作，但对于我们的实时应用程序来说，它太慢了，我们需要使用学习的神经网络策略来近似它。在这种设置下，搜索算法可以在整个过程中为学习算法提供专家演示。</p><p id="3969" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">在某种意义上，从人类演示中学习更具挑战性，因为让人类在整个学习过程中坐下来并在必要时提供建议通常是不实际的，不像我们的搜索算法那样。人类专家的时间很可能是昂贵的，因此我们通常只有少量的这种数据。</p><p id="d78f" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated"><strong class="ke iu">模仿学习</strong></p><p id="261c" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">那么，我们如何利用专家的演示来学习政策呢？一个显而易见的和天真的方法是使用演示作为标记数据，并使用监督学习来尝试和预测专家在不同状态下采取的行动。监督学习比 RL 更容易理解，如果我们能够很好地预测行动，那么我们的代理就会表现得相对较好，或者至少在某种程度上接近专家。</p><p id="c9c9" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">然而，可悲的是，这种方法在许多情况下单独使用时非常失败。在大多数现实问题中，状态空间是非常大的，我们可能演示的状态的数量只是其中的一小部分。由于政策学习本质上处理多步骤过程，并且我们的天真的监督学习方法学习对单个状态的响应，所以一个事件开始时的小分歧可能会产生复合效应，将我们的代理带到它从未观察到并且不知道如何行动的状态。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/8da555e6e1f46f4e10052238b76179a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*-l9DZAlelpC9MngGHHSN1g.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk"><a class="ae kb" href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_2_behavior_cloning.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="132e" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">这种<strong class="ke iu">行为克隆</strong>的另一个问题是，我们的学习过程没有优化我们想要的指标，即从剧集中累积的回报，而是最小化模型预测和专家行动之间的距离。显然，如果我们的模型完美地预测了走势，它也会产生相同的回报，但是预测误差如何转化为回报的差异呢？小错误可能会对代理在我们任务中的表现产生很大影响。</p><p id="5ffa" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">为了有希望成功地使用这种方法，我们必须有大量的数据，涵盖非常广泛的国家，这可能是我们所没有的。即便如此，行为克隆也经常被用作另一种强化学习算法的初始化，以加快学习基础知识的过程。例如，最初的 AlphaGo 使用从在线游戏中收集的 3000 万人类专家棋步数据集来训练其策略网络，最近的 AlphaStar 代理使用职业玩家游戏进行初始化，然后两者都使用 RL 算法进行进一步训练。</p><p id="77d7" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated"><strong class="ke iu">从一次演示中学习</strong></p><p id="af72" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">近年来的几篇论文研究了使用人类演示来帮助代理学习困难问题的选项。经常被用作基准的一个众所周知的难题是 Montezuma 的复仇，这是一个 Atari 游戏，奖励极其稀疏和延迟，大多数标准 RL 算法都无法在其中取得哪怕是很小的进展。</p><p id="5b7a" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">谷歌 DeepMind 的研究人员发表了一篇关于这个主题的非常<a class="ae kb" href="https://arxiv.org/pdf/1805.11592.pdf" rel="noopener ugc nofollow" target="_blank">有趣的论文</a>，名为“通过观看 YouTube 玩艰难的探索游戏”。在这篇论文中，研究人员收集了许多人类玩家玩游戏的 YouTube 视频，训练了一个神经网络来预测同一集不同帧之间的时间差。这产生了来自不同来源的游戏状态的有意义的嵌入，具有视觉变化，如略微不同的颜色和分辨率，这使得沿着嵌入轨迹“种植”虚拟奖励成为可能，代理人可以从嵌入轨迹中得知它在正确的轨道上。</p><p id="e31f" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">虽然这种方法很好地利用了丰富的资源——YouTube 视频，但它可能不适用于许多数据很少的问题。OpenAI 的研究人员在最近一篇名为“从一次示威中学习蒙特祖马的复仇”的论文中解决了这个问题。解决方案非常简单:给定一个由专家提供的状态-动作轨迹，在轨迹的末端重新启动代理，并让它使用一些 RL 算法自行学习，然后在轨迹的早期逐步重新启动它，直到最终从头重新启动它，并让它从那里学习。</p><p id="7aa4" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">其背后的想法是，通过在轨迹的末端重新启动它，我们将它放置在奖励附近，事实上如此接近，以至于标准的 RL 算法将毫不费力地找到它。当代理已经学会满意地找到奖励时，我们在专家给出的状态-行动轨迹的更早部分重新启动它，并让它从那里学习。</p><p id="c7d7" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">为了直观地理解为什么这是一个好主意，让我们看看作者在论文中提供的一个简单的玩具问题；盲崖行走问题。在这个问题中，代理人必须使用两个动作中的任何一个，穿越一维悬崖回到安全的地方。第一个动作使它沿着悬崖前进，第二个动作使它坠落并死去。我们假设一个表格设置，其中代理不能在状态之间进行归纳，并且必须学习一个为每个状态指定一个动作的表格。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/d2d168fa72d72b4e928dbb11d2be09be.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*WWsbhTW2GEvlWb-Fo-wbjg.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk"><a class="ae kb" href="https://arxiv.org/pdf/1812.03381.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="0e1a" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">代理人只有在达到目标状态时才会收到奖励，因此必须首先使用随机动作来探索其环境。然而，期望达到回报所需的行动数量是悬崖长度的指数，这使得它在很短的长度之外不切实际。作者表明，通过一次成功的演示并使用所提出的方法，解决这一任务的时间与悬崖的长度成二次关系，这是一个巨大的进步。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/162ce4c059532a99364f3339638366f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*vyqR5Mh2hCClGs1w6SmekQ.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk"><a class="ae kb" href="https://arxiv.org/pdf/1812.03381.pdf" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="c4bb" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">作者指出，这非常像动态编程，在动态编程中，我们通常从最后开始向后解决问题，并引导我们的后期解决方案来帮助快速解决早期阶段。在动态编程中，我们实际上观察到对于诸如图中最短路径的问题，计算复杂性的非常相似的减少。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi la"><img src="../Images/ce7366c36542452b4a2fc603a7ebd03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*nUodo_ptYKI_COhuInXF9Q.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk"><a class="ae kb" href="https://www.cs.rit.edu/~ark/351/dp/fig04.png" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="bfae" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">研究人员将这种方法用于臭名昭著的蒙特祖马的复仇，并获得了当时最先进的结果，以相当大的优势击败了 DeepMind 论文的分数，并使用了少得多的数据。</p><p id="f748" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">这种方法的一个明显的缺点是，它需要有能力在沿着规定轨迹的不同状态下重新启动我们的代理，这意味着我们必须自由控制环境，或者它是确定性的，以便采取相同的动作序列将总是导致相同的状态。但是，如果该方法适用，它有两个主要优点:</p><p id="16e1" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">1)它需要很少的数据，正如论文中所证明的，甚至一个单独的轨迹就足以解决困难的问题。</p><p id="3499" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">2)它直接针对返回进行优化，因为轨迹仅用于初始化代理，并从那里开始使用标准 RL 进行学习。这使得原则上代理实际上比专家演示者表现得更好成为可能。</p><p id="06b1" class="pw-post-body-paragraph kc kd it ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz im bi translated">我总是喜欢简单的想法非常有效，这是一个很好的例子。检查<a class="ae kb" href="https://arxiv.org/pdf/1812.03381.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。</p></div></div>    
</body>
</html>