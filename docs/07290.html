<html>
<head>
<title>Web Scraping Board Game Descriptions with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 编写的网页刮痧板游戏描述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/web-scraping-board-game-descriptions-with-python-7b8f6a5be1f3?source=collection_archive---------22-----------------------#2019-10-13">https://towardsdatascience.com/web-scraping-board-game-descriptions-with-python-7b8f6a5be1f3?source=collection_archive---------22-----------------------#2019-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div class="gh gi ir"><img src="../Images/ad2d0a5c757cc8b0067e23ff1738746c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*L8mIhZDSZyPs8ot5dojE9w.png"/></div></figure><div class=""/><h1 id="3d58" class="jx jy ja bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">我喜欢收集数据！</h1><p id="5456" class="pw-post-body-paragraph kv kw ja kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我再说一遍，我爱收集数据！虽然通读 HTML 以找到您想要的标签有时会有点乏味，但是从网页中抓取数据提供了一个信息宝库。在<a class="ae lt" href="https://medium.com/@erickleppen01/embracing-manual-data-collection-9ebccbac578d" rel="noopener">之前的一篇文章</a>中，我讨论了一些我用来手动收集网络数据的技术。本文将介绍如何使用 Python 库 BeautifulSoup，并请求从网站<a class="ae lt" href="https://boardgamegeek.com" rel="noopener ugc nofollow" target="_blank">棋盘游戏极客</a>那里收集棋盘游戏描述。如果你想看看我是如何把它们放在一起的，请滚动到文章的底部，复制并粘贴我的代码！</p><h1 id="d22d" class="jx jy ja bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">属国</h1><p id="0b34" class="pw-post-body-paragraph kv kw ja kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了让这个过程简单一点，我从<a class="ae lt" href="https://github.com/beefsack" rel="noopener ugc nofollow" target="_blank">的“猛男”</a>的 Github repo 中找到了一份超过 15000 款桌游的列表，用于<a class="ae lt" href="https://github.com/beefsack/bgg-ranking-historicals" rel="noopener ugc nofollow" target="_blank">历史桌游极客排名</a>。该文件是一个 CSV 文件，使得使用熊猫很容易。以下是数据在文件中如何格式化的示例:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="00fe" class="md jy ja lz b gy me mf l mg mh">ID,Name,Year,Rank,Average,Bayes average,Users rated,URL,Thumbnail<br/>174430,Gloomhaven,2017,1,8.89,8.604,26069,/boardgame/174430/gloomhaven,<a class="ae lt" href="https://cf.geekdo-images.com/micro/img/8JYMPXdcBg_UHddwzq64H4NBduY=/fit-in/64x64/pic2437871.jpg" rel="noopener ugc nofollow" target="_blank">https://cf.geekdo-images.com/micro/img/8JYMPXdcBg_UHddwzq64H4NBduY=/fit-in/64x64/pic2437871.jpg</a></span></pre><p id="ba05" class="pw-post-body-paragraph kv kw ja kx b ky mi la lb lc mj le lf lg mk li lj lk ml lm ln lo mm lq lr ls im bi translated">使用这个文件，我构建了完整的 URL，可以从中抓取描述。我通过导入我的依赖项并加载 CSV 文件来开始这个过程:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="c65d" class="md jy ja lz b gy me mf l mg mh">import pandas as pd<br/>import requests<br/>from bs4 import BeautifulSoup<br/>import sqlite3 as sql<br/>from sqlite3 import Error</span><span id="884a" class="md jy ja lz b gy mn mf l mg mh">#load the csv file<br/>boardgames = pd.read_csv(r'board games\2019-07-08.csv')<br/>boardgames.head()</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/fcb7362ae127b644ecb087026ea64d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xqRSQ0iIsvYI-j6_Z7eYMw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">Top 5 rows from boardgames</figcaption></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/996b78dbe03765042b84ab041c67e5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*NN0YoU-PvuK5PFGRlATl0A.png"/></div></figure><h1 id="1eba" class="jx jy ja bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">探索 HTML</h1><p id="5e46" class="pw-post-body-paragraph kv kw ja kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在创建遍历所有 URL 并提取描述的循环之前，我研究了使用 Requests 和 BeautifulSoup 请求页面内容时返回的数据。这使得我更容易识别我需要找到的标签。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="110a" class="md jy ja lz b gy me mf l mg mh">#explore the first board game page<br/>url = r"<a class="ae lt" href="https://boardgamegeek.com/boardgame/174430/gloomhaven" rel="noopener ugc nofollow" target="_blank">https://boardgamegeek.com/boardgame/174430/gloomhaven</a>"<br/>page_data = requests.get(url)<br/>soup = BeautifulSoup(page_data.content, 'html.parser')<br/>soup</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nf"><img src="../Images/9e8e4b2da14db9f893e0dbb30efaed4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8ApbrknG78OfDkputnzKw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The parsed html data</figcaption></figure><p id="7100" class="pw-post-body-paragraph kv kw ja kx b ky mi la lb lc mj le lf lg mk li lj lk ml lm ln lo mm lq lr ls im bi translated">浏览数据，我看到描述被标记为带有属性<em class="ng"> og:description 的&lt;<em class="ng">元内容</em> &gt;。</em>我用 BeautifulSoup 的 find 方法提取数据。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="8200" class="md jy ja lz b gy me mf l mg mh">description = soup.find("meta",  property="og:description")<br/>print(description["content"])</span></pre></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="8d83" class="jx jy ja bd jz ka nh kc kd ke ni kg kh ki nj kk kl km nk ko kp kq nl ks kt ku bi translated">在页面间循环</h1><p id="0342" class="pw-post-body-paragraph kv kw ja kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一旦我找到了我要找的东西，我就从 Pandas 数据框中取出 URL，并将它们分配给一个变量。然后我为 Loop 创建了一个简单的<strong class="kx jb">来遍历每个 URL 并将棋盘游戏描述加载到一个列表中。在棋盘游戏没有描述的情况下，我使用一个<strong class="kx jb"> IF ELSE </strong>语句来防止循环中断。</strong></p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="0572" class="md jy ja lz b gy me mf l mg mh">#url part 1<br/>up1 = r"<a class="ae lt" href="https://boardgamegeek.com" rel="noopener ugc nofollow" target="_blank">https://boardgamegeek.com</a>"</span><span id="55a4" class="md jy ja lz b gy mn mf l mg mh">#URLs from the data frame<br/>urls = boardgames['URL']</span><span id="3a99" class="md jy ja lz b gy mn mf l mg mh">#index variable<br/>n = 0</span><span id="e63a" class="md jy ja lz b gy mn mf l mg mh">#list of descriptions<br/>desc = []</span><span id="7182" class="md jy ja lz b gy mn mf l mg mh">#for loop<br/>for u in urls:<br/>    url = up1 + urls[n]<br/>    print(url)<br/>    d = requests.get(url)<br/>    soup = BeautifulSoup(d.content, 'html.parser')<br/>    description = soup.find("meta",  property="og:description")<br/>    (desc.append(description["content"]) if description else desc.append("No Description"))<br/>    n+=1</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1bf5a239ddf1fdfad7b98f6ff109eb8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*wNF2ORbxWryKI9gZEz9Qgw.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The loop prints the URL to indicate it is working</figcaption></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/721c6a7d5c7961729161098bdec1f5a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*fxbBJ9YhFls_6gcLy35UwA.png"/></div></figure><h1 id="2490" class="jx jy ja bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">把所有的放在一起</h1><p id="a029" class="pw-post-body-paragraph kv kw ja kx b ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">循环完成后，我将描述添加到<em class="ng"> boardgames </em>数据框中，并将数据框保存到 sqlite 数据库中。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="4e49" class="md jy ja lz b gy me mf l mg mh">#add the descriptions to the dataframe<br/>boardgames['description'] = desc</span><span id="9052" class="md jy ja lz b gy mn mf l mg mh">#create an sqlite database and connection<br/>conn = sql.connect(r'Desktop\board games\bgdb')<br/>c = conn.cursor()</span><span id="f9cc" class="md jy ja lz b gy mn mf l mg mh">#load the data into the database<br/>boardgames.to_sql('boardGames', conn)</span><span id="6f56" class="md jy ja lz b gy mn mf l mg mh">#query the data to verify<br/>q = pd.read_sql('select * from boardGames', conn)<br/>q.head()</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi no"><img src="../Images/39705c83b0d746c424f975a7832a5b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eXvANuvsPuUbYtu3GEcvRQ.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The data frame with descriptions added</figcaption></figure></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="22d3" class="jx jy ja bd jz ka nh kc kd ke ni kg kh ki nj kk kl km nk ko kp kq nl ks kt ku bi translated">完整的代码</h1><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="228f" class="md jy ja lz b gy me mf l mg mh">import pandas as pd<br/>import requests<br/>from bs4 import BeautifulSoup<br/>import sqlite3 as sql<br/>from sqlite3 import Error</span><span id="9445" class="md jy ja lz b gy mn mf l mg mh">#load the csv file<br/>boardgames = pd.read_csv(r'board games\2019-07-08.csv')<br/>boardgames.head()</span><span id="c28a" class="md jy ja lz b gy mn mf l mg mh">#explore the first board game page<br/>url = r"<a class="ae lt" href="https://boardgamegeek.com/boardgame/174430/gloomhaven" rel="noopener ugc nofollow" target="_blank">https://boardgamegeek.com/boardgame/174430/gloomhaven</a>"<br/>page_data = requests.get(url)<br/>soup = BeautifulSoup(page_data.content, 'html.parser')<br/>soup</span><span id="45f4" class="md jy ja lz b gy mn mf l mg mh">description = soup.find("meta",  property="og:description")<br/>print(description["content"])</span><span id="d788" class="md jy ja lz b gy mn mf l mg mh">#url part 1<br/>up1 = r"<a class="ae lt" href="https://boardgamegeek.com" rel="noopener ugc nofollow" target="_blank">https://boardgamegeek.com</a>"</span><span id="8ea8" class="md jy ja lz b gy mn mf l mg mh">#URLs from the data frame<br/>urls = boardgames['URL']</span><span id="c75d" class="md jy ja lz b gy mn mf l mg mh">#index variable<br/>n = 0</span><span id="2828" class="md jy ja lz b gy mn mf l mg mh">#list of descriptions<br/>desc = []</span><span id="db46" class="md jy ja lz b gy mn mf l mg mh">#for loop<br/>for u in urls:<br/>    url = up1 + urls[n]<br/>    print(url)<br/>    d = requests.get(url)<br/>    soup = BeautifulSoup(d.content, 'html.parser')<br/>    description = soup.find("meta",  property="og:description")<br/>    (desc.append(description["content"]) if description else desc.append("No Description"))<br/>    n+=1</span><span id="3b4d" class="md jy ja lz b gy mn mf l mg mh">#add the descriptions to the dataframe<br/>boardgames['description'] = desc</span><span id="1905" class="md jy ja lz b gy mn mf l mg mh">#create an sqlite database and connection<br/>conn = sql.connect(r'Desktop\board games\bgdb')<br/>c = conn.cursor()</span><span id="dce1" class="md jy ja lz b gy mn mf l mg mh">#load the data into the database<br/>boardgames.to_sql('boardGames', conn)</span><span id="a986" class="md jy ja lz b gy mn mf l mg mh">#query the data to verify<br/>q = pd.read_sql('select * from boardGames', conn)<br/>q.head()</span></pre><p id="8a14" class="pw-post-body-paragraph kv kw ja kx b ky mi la lb lc mj le lf lg mk li lj lk ml lm ln lo mm lq lr ls im bi translated">抓取网络数据是一项很棒的技能！如果 API 不可用，它为您提供了一种从 web 页面收集数据的简化方法。如果您想了解更多信息，请查看以下资源:</p><div class="is it gp gr iu np"><a href="https://www.dataquest.io/blog/web-scraping-tutorial-python/" rel="noopener  ugc nofollow" target="_blank"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd jb gy z fp nu fr fs nv fu fw iz bi translated">教程:使用 BeautifulSoup 的 Python Web 抓取</h2><div class="nw l"><h3 class="bd b gy z fp nu fr fs nv fu fw dk translated">在执行数据科学任务时，通常希望使用在互联网上找到的数据。你通常可以…</h3></div><div class="nx l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">www.dataquest.io</p></div></div><div class="ny l"><div class="nz l oa ob oc ny od iw np"/></div></div></a></div><p id="9afa" class="pw-post-body-paragraph kv kw ja kx b ky mi la lb lc mj le lf lg mk li lj lk ml lm ln lo mm lq lr ls im bi translated"><a class="ae lt" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#" rel="noopener ugc nofollow" target="_blank"> B </a></p><h1 id="ceff" class="jx jy ja bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">谢谢大家！</h1><ul class=""><li id="2768" class="oe of ja kx b ky kz lc ld lg og lk oh lo oi ls oj ok ol om bi translated"><em class="ng">如果你喜欢这个，</em> <a class="ae lt" href="https://medium.com/@erickleppen" rel="noopener"> <em class="ng">跟我上 Medium </em> </a> <em class="ng">了解更多</em></li><li id="4352" class="oe of ja kx b ky on lc oo lg op lk oq lo or ls oj ok ol om bi translated"><a class="ae lt" href="https://erickleppen.medium.com/membership" rel="noopener"> <em class="ng">通过订阅</em> </a>获得完全访问权限并帮助支持我的内容</li><li id="fa78" class="oe of ja kx b ky on lc oo lg op lk oq lo or ls oj ok ol om bi translated"><em class="ng">我们连线上</em><a class="ae lt" href="https://www.linkedin.com/in/erickleppen01/" rel="noopener ugc nofollow" target="_blank"><em class="ng">LinkedIn</em></a></li><li id="7668" class="oe of ja kx b ky on lc oo lg op lk oq lo or ls oj ok ol om bi translated"><em class="ng">用 Python 分析数据？查看我的</em> <a class="ae lt" href="https://pythondashboards.com/" rel="noopener ugc nofollow" target="_blank"> <em class="ng">网站</em> </a></li></ul><p id="0d14" class="pw-post-body-paragraph kv kw ja kx b ky mi la lb lc mj le lf lg mk li lj lk ml lm ln lo mm lq lr ls im bi translated"><a class="ae lt" href="http://pythondashboards.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jb"> —埃里克·克莱本</strong> </a></p></div></div>    
</body>
</html>