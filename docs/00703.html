<html>
<head>
<title>Review: DCN / DCNv1 — Deformable Convolutional Networks, 2nd Runner Up in 2017 COCO Detection (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:DCN/DCN v1——可变形卷积网络，2017 年 COCO 检测(物体检测)亚军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=collection_archive---------3-----------------------#2019-02-02">https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44?source=collection_archive---------3-----------------------#2019-02-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fc18" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用可变形卷积，改进了更快的 R-CNN 和 R-FCN，在可可检测中获得亚军，在可可分割中获得季军。</h2></div><p id="9c8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> A </span>继<a class="ae lk" rel="noopener" target="_blank" href="/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa"> STN </a>之后，这次<strong class="kh ir"> DCN(可变形卷积网络)</strong>，由<strong class="kh ir">微软亚洲研究院(MSRA) </strong>进行评审。它也被称为<strong class="kh ir"> DCNv1 </strong>，因为后来作者也提出了 DCNv2。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/f5adf8cd1b65b745bebd1fb609ed4faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*iNz1rJzA5RzdjZrT97VOhw.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">(a) Conventional Convolution, (b) Deformable Convolution, (c) Special Case of Deformable Convolution with Scaling, (d) Special Case of Deformable Convolution with Rotation</strong></figcaption></figure><p id="0adb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">常规/常规卷积基于定义的滤波器尺寸，在来自输入图像或一组输入特征图</strong>的预定义矩形网格上操作。该网格的大小可以是 3×3 和 5×5 等。然而，我们想要检测和分类的对象可能会在图像中变形或被遮挡。</p><p id="492f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">在 DCN </strong>中，网格是可变形的，因为<strong class="kh ir">每个网格点都移动了一个可学习的偏移量。</strong>和<strong class="kh ir">卷积在这些移动的网格点</strong>上操作，因此被称为可变形卷积，类似于可变形 RoI 合并的情况。通过使用这两个新模块，DCN 提高了<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">fast R-CNN</a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>等的精确度。</p><p id="3821" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终，通过使用<strong class="kh ir">DCN+</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"><strong class="kh ir">FPN</strong></a><strong class="kh ir">+对齐</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568"> <strong class="kh ir">例外</strong> </a>，MSRA 获得了 COCO 检测挑战赛亚军和细分挑战赛<strong class="kh ir">季军</strong>。发表于<strong class="kh ir"> 2017 ICCV </strong>，引用<strong class="kh ir"> 200 余次</strong>。(<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----14e488efce44--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="d202" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">概述</h1><ol class=""><li id="876e" class="mz na iq kh b ki nb kl nc ko nd ks ne kw nf la ng nh ni nj bi translated"><strong class="kh ir">可变形回旋</strong></li><li id="b7e5" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><strong class="kh ir">可变形 RoI 汇集</strong></li><li id="5028" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><strong class="kh ir">可变形正敏感(PS) RoI 汇集</strong></li><li id="d95c" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><strong class="kh ir">使用 ResNet-101 &amp;对齐-初始-ResNet </strong>的可变形 ConvNets</li><li id="2d69" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><strong class="kh ir">消融研究&amp;结果</strong></li><li id="4cff" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><strong class="kh ir">使用比对异常的 COCO 检测挑战的更多结果</strong></li></ol></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="3d83" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">1.可变形卷积</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/6f4cc97c151b3c59094a8c9f8e62d808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06YpE_aFVQivZ5ACgv1qHA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Deformable Convolution</strong></figcaption></figure><ul class=""><li id="a9c4" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">规则卷积在规则网格<em class="ny"> R </em>上运算。</li><li id="65d1" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">可变形卷积运算在<em class="ny"> R </em>上进行，但是每个点都增加了一个可学习的偏移量∈<em class="ny">pn</em>。</li><li id="6822" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">卷积用于生成对应于<em class="ny"> N </em> 2D 偏移∈<em class="ny">pn</em>(<em class="ny">x</em>-方向和<em class="ny">y</em>-方向)的 2 <em class="ny"> N </em>个特征图。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0338fe4cee76407b61a0d35750f8b63a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*Mi6LqBIa8a4Ewo9DywHuzw.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Standard Convolution (Left), Deformable Convolution (Right)</strong></figcaption></figure><ul class=""><li id="1461" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">如上所示，可变形卷积将根据输入图像或特征图为卷积选取不同位置的值。</li><li id="991e" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">相比</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> <strong class="kh ir">阿特鲁卷积</strong> </a> : <a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">阿特鲁卷积</a>在卷积时具有较大但固定的膨胀值，而可变形卷积在卷积时对网格中的每个点应用不同的膨胀值。(<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">阿特鲁卷积</a>也叫<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩张卷积</a>或空洞算法。)</li><li id="1a69" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">相对于</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa"> <strong class="kh ir">空间变换网络</strong> </a> : <a class="ae lk" rel="noopener" target="_blank" href="/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa">空间变换网络</a>对输入图像或特征地图进行变换，而可变形卷积可以被视为一个极轻量级的<a class="ae lk" rel="noopener" target="_blank" href="/review-stn-spatial-transformer-network-image-classification-d3cbd98a70aa">空间变换网络</a>。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="e976" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">2.<strong class="ak">可变形 RoI 合并</strong></h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oa"><img src="../Images/50b91066c9e6689c00708967f73235b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0da-LD1sFMaDaV1g41qqQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Deformable RoI Pooling</strong></figcaption></figure><ul class=""><li id="3f1e" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">常规 RoI 池将任意大小的输入矩形区域转换为固定大小的特征。</li><li id="0a13" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">在可变形 RoI 合并中，<strong class="kh ir">首先，在顶部路径</strong>，我们仍然需要<strong class="kh ir">常规 RoI 合并</strong>来生成合并的特征图。</li><li id="da93" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">然后，<strong class="kh ir">一个全连接(fc)层生成归一化偏移</strong><strong class="kh ir">∈<em class="ny">p</em>̂<em class="ny">ij</em></strong>和<strong class="kh ir">，然后转换为偏移∈<em class="ny">pij</em></strong>(右下方的等式)，其中γ=0.1。</li><li id="dbdd" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">偏移归一化对于使偏移学习不随 RoI 尺寸变化是必要的。</li><li id="7c37" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">最后，<strong class="kh ir">在底部路径，</strong>我们执行<strong class="kh ir">可变形 RoI 合并。输出要素地图基于具有增大偏移的区域进行合并。</strong></li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="28cd" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak"> 3。可变形正敏感(PS) RoI 汇集</strong></h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bef52209eaa349e53d6e8375ca81f48f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*_6jC6kqON_yHa8jMcuW_3g.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Deformable Positive-Sensitive (PS) RoI Pooling (Colors are important here)</strong></figcaption></figure><ul class=""><li id="3a33" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">对于<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>中的原始正敏感(PS) RoI pooling，首先将所有输入特征图转换为每个对象类的<em class="ny"> k </em>得分图(总共<em class="ny"> C </em>对于<em class="ny"> C </em>对象类+ 1 个背景)(最好先阅读<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>来了解原始 PS RoI pooling)。如果感兴趣，请阅读关于它的评论。)</li><li id="af85" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">在可变形 PS RoI 合并中，<strong class="kh ir">首先，在顶部路径</strong>，与原始路径类似，<strong class="kh ir"> conv 用于生成 2 个<em class="ny"> k </em> ( <em class="ny"> C </em> +1)分数图。</strong></li><li id="dbae" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">这意味着对于每个类，将有 k 个特征地图。这些<strong class="kh ir"> k 特征图代表了{左上(TL)，中上(TC)、..，右下角(BR)} </strong>我们要学习的对象的偏移量。</li><li id="050b" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">偏移(顶部路径)的原始 PS RoI 汇集在</strong>中完成，即<strong class="kh ir">它们汇集在图中的相同区域和相同颜色。</strong>我们在这里得到补偿。</li><li id="0716" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">最后，<strong class="kh ir">在底部路径</strong>，我们执行<strong class="kh ir">可变形 PS RoI 合并</strong>来合并由偏移增加的特征图。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="4407" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak"> 4。使用 ResNet-101 &amp;对准初始 ResNet </strong>的可变形 ConvNets</h1><h2 id="08ee" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">4.1.对齐- <a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">盗梦空间-ResNet </a></h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oo"><img src="../Images/2cf8e0af7127d5f41ce63b97e9a6110c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tK7NHV33EsFtPiClxLW9UQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Aligned-</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="bd lx">Inception-ResNet</strong></a><strong class="bd lx"> Architecture (Left), Inception Residual Block (IRB) (Right)</strong></figcaption></figure><ul class=""><li id="1f43" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">在<strong class="kh ir">原始</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kh ir">Inception-ResNet</strong></a>中，在<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-v4 </a>中提出了<strong class="kh ir">对准问题</strong>，对于靠近输出的特征图上的细胞，其在图像上的投影空间位置与其感受野中心位置不对准。</li><li id="ba59" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">在<strong class="kh ir">Aligned-</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kh ir">Inception-ResNet</strong></a>中，我们可以看到在 Inception 残差块(IRB)内，<strong class="kh ir">所有用于因子分解的非对称卷积(例如:1×7，7×1，1×3，3×1 conv)都被去除了。如上所示，仅使用一种 IRB。此外，IRB 的编号不同于<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v1 </a>或<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>。</strong></li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi op"><img src="../Images/219307d78aad19378bc2e7ad2f00d88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*oTB2RB2sV8n-nzKHXl8a9A.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Error Rates on ImageNet-1K validation.</strong></figcaption></figure><ul class=""><li id="a45e" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">aligned-<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet</a>比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>的错误率低。</li><li id="6653" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">虽然 Aligned-<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet</a>比<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>有更高的错误率，Aligned-<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet</a>解决了对齐问题。</li></ul><h2 id="baee" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">4.2.修改后的 ResNet-101 &amp; Aligned-<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet</a></h2><ul class=""><li id="8e27" class="mz na iq kh b ki nb kl nc ko nd ks ne kw nf la nx nh ni nj bi translated">现在我们得到了两个主干:<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">ResNet-101</a>&amp;Aligned-<a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet</a>用于特征提取，原本用于图像分类任务。</li><li id="480f" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">然而，输出特征图太小，这对于对象检测和分割任务来说是不好的。</li><li id="b4fb" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">阿特鲁卷积</a>(或<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩张卷积</a>)用于减少最后一个块(conv5)的开始，步幅从 2 变为 1。</li><li id="43eb" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">因此，最后一个卷积块中的有效步幅从 32 个像素减少到 16 个像素，以增加特征图的分辨率。</li></ul><h2 id="bc6a" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">4.3.不同的物体探测器</h2><ul class=""><li id="0879" class="mz na iq kh b ki nb kl nc ko nd ks ne kw nf la nx nh ni nj bi translated">在特征提取之后，使用不同的对象检测器或分割方案，例如<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>、类别感知 RPN(或被视为简化的<a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>)、<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="88c0" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">5.消融研究和结果</h1><h2 id="54db" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated"><strong class="ak">语义分割</strong></h2><ul class=""><li id="7a9c" class="mz na iq kh b ki nb kl nc ko nd ks ne kw nf la nx nh ni nj bi translated"><strong class="kh ir"> PASCAL VOC </strong>，20 个类别，带有附加遮罩注释的 VOC 2012 数据集，用于训练的 10，582 幅图像，用于验证的 1，449 幅图像。<strong class="kh ir"> mIoU@V </strong>用于评价。</li><li id="2a44" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">城市景观</strong>，19 个类别+ 1 个背景类别，2975 张用于训练的图像，500 张用于验证的图像。<strong class="kh ir"> mIoU@C </strong>用于评估。</li></ul><h2 id="1841" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated"><strong class="ak">物体检测</strong></h2><ul class=""><li id="2298" class="mz na iq kh b ki nb kl nc ko nd ks ne kw nf la nx nh ni nj bi translated">PASCAL VOC ，VOC 2007 trainval 和 VOC 2012 trainval 联合培训，VOC 2007 测试评估。使用<strong class="kh ir">贴图@0.5 </strong>和<strong class="kh ir">贴图@0.7 </strong>。</li><li id="7e48" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir"> COCO </strong>，trainval 中 120k 图像，test-dev 中 20k 图像。<strong class="kh ir">mAP @ 0.5:0.95</strong>和<strong class="kh ir"> mAP@0.5 </strong>用于评估。</li></ul><h2 id="cc78" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">5.1.对不同数量的最后几层应用可变形卷积</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oq"><img src="../Images/b38bb341983d46142bbbb32810f97064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HY-pTJWRj_DVFG4sDxT8jw.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Results of using deformable convolution in the last 1, 2, 3, and 6 convolutional layers (of 3×3 filter) in </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd lx">ResNet-101</strong></a></figcaption></figure><ul class=""><li id="dc92" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">3 和 6 可变形回旋也是好的。最后，作者选择了<strong class="kh ir"> 3，因为对于不同的任务</strong>有一个<strong class="kh ir">的良好权衡。</strong></li><li id="4aba" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">而且我们还可以看到<strong class="kh ir"> DCN 提高了</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> <strong class="kh ir"> DeepLab </strong> </a> <strong class="kh ir">、类感知 RPN(或者视为简化的</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"><strong class="kh ir">SSD</strong></a><strong class="kh ir">)、</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="kh ir">更快的 R-CNN </strong> </a> <strong class="kh ir">和</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"><strong class="kh ir">R-FCN</strong></a><strong class="kh ir">。</strong></li></ul><h2 id="da05" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">5.2.<strong class="ak">分析可变形卷积偏移距离</strong></h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi or"><img src="../Images/a9deed79703450f43d39c533cec80dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*WejTHGdEPxexohsYiaRhJw.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Analysis of deformable convolution in the last 3 convolutional layers</strong></figcaption></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi os"><img src="../Images/95dafd24df0c7a6ebb56df80390e46e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTb_cAn0UMvZHb1_6Ht4LQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Examples: three levels of 3×3 deformable filters for three activation units (green points) on the background (left), a small object (middle), and a large object (right)</strong></figcaption></figure><ul class=""><li id="cf55" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">为了说明 DCN 的有效性，也进行了如上的分析。首先，根据地面真实边界框注释和滤波器中心的位置，将可变形卷积滤波器分为四类:小、中、大和背景。</li><li id="bd01" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">然后，测量扩张值(偏移距离)的平均值和标准偏差。</li><li id="49d8" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">发现<strong class="kh ir">可变形滤光器的感受野大小与对象大小</strong>相关，表明变形是从图像内容中有效学习的。</li><li id="b852" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">并且<strong class="kh ir">背景区域上的滤光器尺寸介于中大型物体上的滤光器尺寸之间，这表明相对较大的感受野对于识别背景区域是必要的</strong>。</li></ul><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ot"><img src="../Images/3f921a40ffb277031bd4bb146dfc73d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nw4NwNzM8vjs7LjVs2q7A.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Offset parts in deformable (positive sensitive) RoI pooling in </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"><strong class="bd lx">R-FCN</strong></a><strong class="bd lx"> and 3×3 bins (red) for an input RoI (yellow)</strong></figcaption></figure><ul class=""><li id="6428" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">类似地，对于可变形的 RoI 合并，现在部件被偏移以覆盖非刚性对象。</li></ul><h2 id="8c58" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">5.3.帕斯卡 VOC 与<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">阿特鲁卷积</a>的比较</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ou"><img src="../Images/9ed4f4061f365624fc0dbb494cccd593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xNfgXjCH_D4QYqjDd0ALQ.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Comparison of </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"><strong class="bd lx">Atrous Convolution</strong></a><strong class="bd lx"> &amp; Deformable Convolution</strong></figcaption></figure><ul class=""><li id="4873" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated"><strong class="kh ir">只有可变形卷积</strong> : <a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>、类感知 RPN、<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>有了改进，已经胜过<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLab </a>、类感知 RPN 和<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>有<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> atrous 卷积</a>。并且具有可变形卷积的<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>获得了与具有<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">不规则卷积</a> (4，4，4)的<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>的竞争结果。</li><li id="38a1" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">仅可变形 RoI 合并</strong>:仅在<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>中有 RoI 合并。<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">具有可变形 RoI 池的更快 R-CNN </a>获得与具有<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> atrous 卷积</a> (4，4，4)的<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快 R-CNN </a>竞争的结果。<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">具有可变形 RoI 池的 R-FCN </a>优于具有<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> atrous 卷积</a> (4，4，4)的 R-FCN 。</li><li id="912b" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">变形卷积&amp; RoI 合并</strong> : <a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>带变形卷积&amp; RoI 合并是所有设置中最好的。</li></ul><h2 id="5064" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">5.4.PASCAL VOC 上的模型复杂性和运行时间</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/282136e110fb0b446afe74cc239f102d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*gnw0iTyJ4S2thfwbQ8MpNA.png"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Model Complexity and Runtime</strong></figcaption></figure><ul class=""><li id="a237" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated"><strong class="kh ir">可变形的凸网只增加了模型参数和计算的少量开销。</strong></li><li id="761e" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">除了增加模型参数之外，显著的性能改进来自于对几何变换进行建模的能力。</li></ul><h2 id="53dd" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">5.5.COCO 上的对象检测</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ow"><img src="../Images/1e499e7767e32615abcbf129ff23d593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AcJLqgUbE5tEm2yhESSCRg.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Object Detection on COCO test-dev </strong>(<strong class="bd lx">M</strong>: Multi-Scale Testing with Shorter Side {480, 576, 688, 864, 1200, 1400}<strong class="bd lx">, B</strong>: Iterative Bounding Box Average)</figcaption></figure><ul class=""><li id="15d4" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">使用可变形的 ConvNet 始终优于普通的。</li><li id="6d7e" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">用<strong class="kh ir">对齐-</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kh ir">Inception-ResNet</strong></a>，用<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> <strong class="kh ir"> R-FCN </strong> </a>与<strong class="kh ir">可变形 ConvNet </strong>，加上<strong class="kh ir">多尺度测试</strong>和<strong class="kh ir">迭代包围盒平均</strong>，<strong class="kh ir">37.5% mAP @【0.5:0.95】</strong>得到。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="1dc4" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">6。使用比对异常检测 COCO 挑战的更多结果</h1><ul class=""><li id="faea" class="mz na iq kh b ki nb kl nc ko nd ks ne kw nf la nx nh ni nj bi translated">以上结果来自论文。他们还在 ICCV 2017 大会上展示了一项新成果。</li></ul><h2 id="cef8" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">6.1.对齐<a class="ae lk" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568">异常</a></h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ox"><img src="../Images/2ff14a7991232ba42cc3a96850140c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RfVeYw0FKwA8zwUVQwtsZw.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Aligned </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568"><strong class="bd lx">Xception</strong></a></figcaption></figure><ul class=""><li id="e8f4" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated">原始<a class="ae lk" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568">异常</a>的校准<a class="ae lk" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568">异常</a>的更新为蓝色。</li><li id="c915" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">简而言之，一些最大池操作被入口流中的可分离 conv 所取代。在中间流程中，重复次数从 8 次增加到 16 次。在出口流中增加了一个 conv。</li></ul><h2 id="ae82" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">6.2.可可检测挑战</h2><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oy"><img src="../Images/aa2d185898053c3e29f6893343aecfec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gdetpizce3reSHqYsMQMmA.png"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk"><strong class="bd lx">Object Detection on COCO test-dev</strong></figcaption></figure><ul class=""><li id="2f73" class="mz na iq kh b ki kj kl km ko nu ks nv kw nw la nx nh ni nj bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir"> ResNet-101 </strong> </a> <strong class="kh ir">作为特征提取器，</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"><strong class="kh ir"/></a><strong class="kh ir">+OHEM 作为物体检测器</strong>:获得了 40.5%的 mAP，已经高于上一节提到的结果。</li><li id="79d1" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">用对齐的</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568"> <strong class="kh ir">替换</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet-101</strong></a><strong class="kh ir">例外</strong> </a> : 43.3% mAP。</li><li id="56b7" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated"><strong class="kh ir">6 款车型组合+其他小改进</strong> : 50.7%地图。</li><li id="c661" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">在<strong class="kh ir"> COCO 2017 探测挑战排行榜</strong>中，50.4%的地图使其成为挑战亚军。</li><li id="2aa9" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">在<strong class="kh ir"> COCO 2017 细分挑战赛排行榜</strong>中，42.6%的地图使其成为挑战赛季军。</li><li id="86de" class="mz na iq kh b ki nk kl nl ko nm ks nn kw no la nx nh ni nj bi translated">排行榜:<a class="ae lk" href="http://cocodataset.org/#detection-leaderboard" rel="noopener ugc nofollow" target="_blank">http://cocodataset.org/#detection-leaderboard</a></li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="6226" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果有时间，希望也能复习一下 DCNv2。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="e635" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">参考</h2><p id="ac73" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko oz kq kr ks pa ku kv kw pb ky kz la ij bi translated">【2017 ICCV】【DCN】<br/><a class="ae lk" href="https://arxiv.org/abs/1703.06211" rel="noopener ugc nofollow" target="_blank">可变形卷积网络</a></p><h2 id="a15b" class="oc mi iq bd mj od oe dn mn of og dp mr ko oh oi mt ks oj ok mv kw ol om mx on bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki nb jr kk kl nc ju kn ko oz kq kr ks pa ku kv kw pb ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(们)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a></p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/>T32】[<a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超分辨率<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>】</p></div></div>    
</body>
</html>