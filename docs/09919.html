<html>
<head>
<title>Forward and backward propagations for 2D Convolutional layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2D 卷积层的向前和向后传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forward-and-backward-propagations-for-2d-convolutional-layers-ed970f8bf602?source=collection_archive---------10-----------------------#2019-12-28">https://towardsdatascience.com/forward-and-backward-propagations-for-2d-convolutional-layers-ed970f8bf602?source=collection_archive---------10-----------------------#2019-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="0735" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">深度学习:理解理论以便更好地实践</h2><div class=""/><div class=""><h2 id="3adb" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">推广到多通道输入和多滤波器</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3a98d9d7703ceb188310e1ddea0318b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INfSxUUsuJUlN82ukqw5JA.png"/></div></div></figure><h1 id="c6d4" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">动机</h1><p id="db36" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">已经有许多关于<em class="mr">到数据科学</em>的文章讨论了卷积神经网络的反向传播。</p><p id="ad3f" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">它们很好地解释了简单的情况(例如，只有一个通道的输入，当时只有一个卷积滤波器)，但我发现很难将反向传递实现推广到任意选择的层。</p><p id="92b7" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">为了克服这个困难，我决定回到反向传播的理论方面。找到这些方程后，实现变得容易多了。</p><p id="40fb" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">本文旨在给出多通道输入和多滤波器卷积层的前向传播和后向传播的关键方程，以及如何得到它们。</p><p id="674b" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">如果你只对结果感兴趣，请随意下结论！</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="a564" class="ld le it bd lf lg ne li lj lk nf lm ln ki ng kj lp kl nh km lr ko ni kp lt lu bi translated">几个定义</h1><p id="6e4d" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">卷积层执行…卷积！然后，我们需要定义有用的数学运算符:</p><p id="8f42" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">一方面，任意大小的图像和具有大小为(k1，k2)的核 K 的 C 个通道之间的<em class="mr">卷积</em>被定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/977b7d11f3dc0203369245542c45d0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*8_6U9VdjOn5zxnjp7icLBA@2x.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Convolution of image I with kernel K</figcaption></figure><p id="86c5" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">另一方面，任意大小的图像和具有大小为(k1，k2)的核 K 的 C 个通道之间的<em class="mr">互相关</em>由下式定义:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f7c0507b3ded390114a6d0fcd779ba54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*ts5u_HHgM6o0vVjbCvlRig@2x.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Cross-correlation of image I with kernel K</figcaption></figure><p id="07f4" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">明眼人会注意到，图像与核的卷积相当于图像与该核的互相关，但翻转了 180 °:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/225f21a263fd6f467ff4d97e1dcc877f.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*Mc_euREJrzodgnscT_kz_w@2x.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Kernel flip</figcaption></figure><p id="d406" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">因此，<em class="mr"> </em>我们可以直接将翻转的内核<em class="mr"> </em>视为我们想要用于卷积的内核，并且<strong class="lx jd"> <em class="mr">让我们的层仅计算互相关</em> </strong>。</p><p id="67a8" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">在本帖中，我们将把元素 X 视为 3 维或 4 维数组:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/8580bfbf55c625a1e5b0726e7f27aad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*l4LQsWvuzsm-QY8tInpcuA@2x.png"/></div></figure><p id="314e" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">在 X 有三维的情况下，f 不会出现在符号上。就是这样！我们现在可以讨论卷积层本身。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="8308" class="ld le it bd lf lg ne li lj lk nf lm ln ki ng kj lp kl nh km lr ko ni kp lt lu bi translated">卷积层</h1><p id="85d6" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">该层将高度 n_H_prev、宽度 n_W_prev 和 C 通道的前一层 A_prev 的输出转换成高度 n_H、宽度 n_W 和 F 通道的变量 Z。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/7e2740a3b757b5e5439fcb5cf3d1a5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KlIuSPuOu_GOk-X8EmfRQ@2x.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Convolutional layer: input and output shapes</figcaption></figure><p id="c148" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">该层的参数是:</p><ul class=""><li id="c11f" class="ns nt it lx b ly ms mb mt me nu mi nv mm nw mq nx ny nz oa bi translated"><em class="mr"> F 核</em>(或过滤器)由它们的<em class="mr">权重</em> w_{i,j,c}^f 和<em class="mr">偏差</em> b^f 定义</li><li id="5558" class="ns nt it lx b ly ob mb oc me od mi oe mm of mq nx ny nz oa bi translated">上面解释的<em class="mr">内核大小</em> (k1，k2)</li><li id="9fb3" class="ns nt it lx b ly ob mb oc me od mi oe mm of mq nx ny nz oa bi translated">一个<em class="mr">激活</em>功能</li><li id="7bb5" class="ns nt it lx b ly ob mb oc me od mi oe mm of mq nx ny nz oa bi translated"><em class="mr">步长</em> (s1，s2)，其定义了在输入图像上应用内核的步骤</li><li id="4171" class="ns nt it lx b ly ob mb oc me od mi oe mm of mq nx ny nz oa bi translated">p1，p2 定义了我们在 A_prev 的边界上添加的零的数量</li></ul><h1 id="10a9" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">正向传播</h1><p id="e3ea" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">卷积层转发<strong class="lx jd">填充的</strong>输入；因此，我们考虑将<em class="mr"> A_prev_pad </em>用于卷积。</p><p id="5dc8" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">正向传播的方程式是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/b5c2cc9d9a5b1d04c3f1d684d78c9363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uXLoSxUFlP7UfuqjAgH6GQ@2x.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Forward propagation equations</figcaption></figure><h1 id="0091" class="ld le it bd lf lg lh li lj lk ll lm ln ki lo kj lp kl lq km lr ko ls kp lt lu bi translated">反向传播</h1><p id="b067" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">反向传播有三个目标:</p><ul class=""><li id="0992" class="ns nt it lx b ly ms mb mt me nu mi nv mm nw mq nx ny nz oa bi translated">将错误从一层传播到上一层</li><li id="b52d" class="ns nt it lx b ly ob mb oc me od mi oe mm of mq nx ny nz oa bi translated">计算误差相对于权重的导数</li><li id="c495" class="ns nt it lx b ly ob mb oc me od mi oe mm of mq nx ny nz oa bi translated">计算误差相对于偏差的导数</li></ul><h2 id="4ba4" class="oh le it bd lf oi oj dn lj ok ol dp ln me om on lp mi oo op lr mm oq or lt iz bi translated">注释</h2><p id="f1bd" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">为了便于表示，我们定义如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/97c52aecf5a1f010bccaf14f6ffb25a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5E13fH1CgAqhiFg6Y3nOyg@2x.png"/></div></div></figure><h2 id="7f5c" class="oh le it bd lf oi oj dn lj ok ol dp ln me om on lp mi oo op lr mm oq or lt iz bi translated">数学！</h2><p id="08f3" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">在实践中，我们通过<strong class="lx jd">总是</strong>知道 da_{i，j，f}或 dz_{i，j，f}来执行层的向后传递，并且我们假设对于这种情况我们知道 da_{i，j，f}。</p><p id="2cdf" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">dz_{i，j，f}的表达式由等式给出。[2]:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/019120fcc58fcdd1d1a08cf968e6e104.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*DrgbwmgIt70ZWViYY9foJg@2x.png"/></div></figure><p id="8582" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">其中 g '是 g 的导数。</p><p id="4f8b" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">利用链式法则，我们可以计算 dw_{i,j,c}^f:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/84847d29eb672cf1523b23053c03052a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57LBDeiUs7IC8Gnlu_S8IA@2x.png"/></div></div></figure><p id="f337" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">回想一下，dz_{m，n，k}仅与第 k 个滤波器相关联(由等式给出。[1])，fth 内核的权重只和 dz 的 fth 通道挂钩；</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/655986ecd0d751fb20e5cd6f8777b9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*zJPhDg9lR2qlppMaqZTWpQ@2x.png"/></div></figure><p id="9d7f" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">然后，我们可以用方程式得到 dz_{m,n,f}/dw_{i,j,c}^f。[1]:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/3b8c91506dd9c7c9cfed7a9a14cb2347.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*uHQhZqwfX4yoqE3A7RZ9PQ@2x.png"/></div></div></figure><p id="7dcb" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">情商的表达。[5]然后是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/4fd8e76882cf2eea5cf837f15daa00f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFIVdIevuRWW-OcJiSL2YQ@2x.png"/></div></div></figure><p id="f2d3" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">可以注意到，这是 A_prev_pad 与内核 dZ 的互相关</p><p id="2bdd" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">对于偏置，遵循相同的程序:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/ee7bc7f9b4f97a41aa1e0172ee94e120.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*bH1Aqzi9ePSUL5XV8ygZuA@2x.png"/></div></figure><p id="3fee" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">因此:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/c0c5429aec745123d01015f36cc4cd09.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*FIBN8YhC72MtkC2C-qCbLg@2x.png"/></div></figure><p id="5092" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">最后要做的是误差的反向传播:找出 dA_prev 和 dZ 之间的关系。</p><p id="7934" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">记住我们在 dZ 和 dA_prev 的填充版本之间有关系(在等式中给出)。[1])，我们将考虑计算 da_prev_pad。</p><p id="a698" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">使用链式法则(再次！)，我们有:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/f02d176d357a0f41176e99a9109b3617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FI9ASlL6P0J829orRZxmlQ@2x.png"/></div></div></figure><p id="3a6f" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">我们认为 dz_{m，n，f}是和的第一项，这很好。让我们关注第二个术语:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/576e090bfb60a10d6ef67727c8921db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fwk0wR1jZ5-ccRpSjTd2jg@2x.png"/></div></div></figure><p id="d9a9" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">当且仅当 m'+m=i，n'+n=j，c'=c 时，它不等于零。</p><p id="28c0" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">因此:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/71dfbfd5db63ae8b5975f5120e429e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*UYn4stckUBnxgDrDj4K-2g@2x.png"/></div></figure><p id="b19d" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">所以，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/2b9aa4580caca6821030c15ea578daf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uiN9A4fAOdTo6XY-zuHAHQ@2x.png"/></div></div></figure><p id="e179" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">我们注意到等式。[9]描述了一种卷积，其中层的滤波器被认为是图像，dZ 是内核。</p><p id="ed18" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">我们最终通过选择 da_prev_pad_{i+p1，j+p2，c}获得 da_prev_{i，j，c}，p1 和 p2 是该层的第一和第二维周围的填充值。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="3461" class="ld le it bd lf lg ne li lj lk nf lm ln ki ng kj lp kl nh km lr ko ni kp lt lu bi translated">结论</h1><p id="3aab" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">在本文中，您了解了如何计算具有任意数量滤波器和输入通道的卷积层的前向和后向传播。</p><p id="d1c1" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">正向传递由两个等式给出:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/45a742de1a72827585853d4a1478a402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jvv2rBj6FBPY2j2L0fckgQ@2x.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Forward pass equations</figcaption></figure><p id="c1d0" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">要计算卷积层的反向传播，只需实现以下四个等式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/c7908ef55e7cc67bf58b6d2d4defa731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2K0tfxmAlyRlqqbj4z0Rg@2x.png"/></div></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Backward pass equations</figcaption></figure><p id="51e7" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">其他细节可以在本文中讨论(例如，如何计算 n_H 和 n_W 的值),但许多文章已经讨论了这些要点，因此我鼓励您在开始实现自己的卷积层之前阅读它们。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><h1 id="8ae6" class="ld le it bd lf lg ne li lj lk nf lm ln ki ng kj lp kl nh km lr ko ni kp lt lu bi translated">来源</h1><p id="21fa" class="pw-post-body-paragraph lv lw it lx b ly lz kd ma mb mc kg md me mf mg mh mi mj mk ml mm mn mo mp mq im bi translated">[1] <a class="ae pf" href="https://arxiv.org/pdf/1603.07285.pdf" rel="noopener ugc nofollow" target="_blank">杜默林诉&amp;维辛，F. (2016)。深度学习卷积算法指南(cite arxiv:1603.07285) </a></p><p id="9486" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">[2]<a class="ae pf" href="http://www.cs.umd.edu/~djacobs/CMSC426/Convolution.pdf" rel="noopener ugc nofollow" target="_blank">CMSC 426 相关与卷积课堂笔记，2005 秋季戴维·雅各布</a></p><p id="7c18" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">[3] <a class="ae pf" href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/" rel="noopener ugc nofollow" target="_blank">卷积神经网络 UFLDL 教程</a></p><p id="2f41" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">[4] <a class="ae pf" href="https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">卷积神经网络中的反向传播，Jefkine，2016 年 9 月 5 日</a></p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="2c01" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">祝你实现这些算法好运！</p><p id="90ae" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated">干杯，</p><p id="9c35" class="pw-post-body-paragraph lv lw it lx b ly ms kd ma mb mt kg md me mu mg mh mi mv mk ml mm mw mo mp mq im bi translated"><em class="mr">特里斯坦</em></p></div></div>    
</body>
</html>