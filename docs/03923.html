<html>
<head>
<title>Choose the Suitable Transformer Framework for Your Needs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">根据您的需求选择合适的转换器框架</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/choose-the-right-transformer-framework-for-you-b7c51737d45?source=collection_archive---------10-----------------------#2019-06-20">https://towardsdatascience.com/choose-the-right-transformer-framework-for-you-b7c51737d45?source=collection_archive---------10-----------------------#2019-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ad95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">比较不同的 Transformer 实现框架，选择最适合您自己需求的框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f30ebefe4cfcb0986e3b50b1b393d01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0NVOBXeZVYbhQvAV.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image credit: © Flynt — Bigstockphoto.com</figcaption></figure><h1 id="3483" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="e7e4" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">基于你对 PyTroch 或者 TensorFlow 的偏好，我推荐使用 Fairseq 或者 Tensor2Tensor。</p><p id="6bb6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你是一名研究人员，Fairseq 在定制方面足够灵活。但如果是做一些真实的应用，考虑部署的话，选择 Tensor2Tensor 会更好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/52c1e2a4add994854aa733ec0ec84c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-ASQ5eYiUJQWJqbhwWBCw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">comparison of different frameworks</figcaption></figure><h1 id="46dd" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">不同框架中的转换器</h1><h2 id="5f7f" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><a class="ae ne" href="https://github.com/pytorch/fairseq" rel="noopener ugc nofollow" target="_blank"><strong class="ak">fair seq</strong>T3】</a></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0ef66d60e7148fc1b3342542c50e1a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*0d3pgZSW6m5VndeE6HZ-hg.png"/></div></figure><p id="ffd4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一个序列建模工具包，允许研究人员和开发人员为翻译、摘要、语言建模和其他文本生成任务训练自定义模型。</p><p id="5efe" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">变压器</strong>(自我关注)网络:</p><ul class=""><li id="2900" class="ng nh it ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><a class="ae ne" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等人(2017):你需要的只是关注</a></li><li id="4d63" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><a class="ae ne" href="https://github.com/pytorch/fairseq/blob/master/examples/scaling_nmt/README.md" rel="noopener ugc nofollow" target="_blank">奥特等人(2018):缩放神经机器翻译</a></li><li id="e8a9" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><a class="ae ne" href="https://github.com/pytorch/fairseq/blob/master/examples/backtranslation/README.md" rel="noopener ugc nofollow" target="_blank"> Edunov 等人(2018):理解大规模回译</a></li><li id="df55" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls iu">新</strong> <a class="ae ne" href="https://github.com/pytorch/fairseq/blob/master/examples/language_model/transformer_lm/README.md" rel="noopener ugc nofollow" target="_blank"> Baevski 和 Auli (2018):神经语言建模的自适应输入表示</a></li><li id="d24b" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls iu">新</strong> <a class="ae ne" href="https://github.com/pytorch/fairseq/blob/master/examples/translation_moe/README.md" rel="noopener ugc nofollow" target="_blank">沈等(2019):多元机器翻译的混合模型:商业诀窍</a></li></ul><p id="afe2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们可以很容易地使用编码器和解码器。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="202c" class="ms kz it nv b gy nz oa l ob oc">class fairseq.models.transformer.TransformerModel(encoder, decoder)</span></pre><p id="8703" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">来自<a class="ae ne" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">《注意力是你所需要的》(瓦斯瓦尼等人，2017) </a>的变形金刚模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/ec9afe9d089f86ab27b2b13bd839895b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*QfiGZ5kjqEkeg5hI.gif"/></div></figure><p id="9966" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">参数:</p><ul class=""><li id="f400" class="ng nh it ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><strong class="ls iu">编码器</strong> ( <a class="ae ne" href="https://fairseq.readthedocs.io/en/latest/models.html#fairseq.models.transformer.TransformerEncoder" rel="noopener ugc nofollow" target="_blank"> <em class="oe">变压器编码器</em> </a> ) —编码器</li><li id="6ac3" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><strong class="ls iu">解码器</strong> ( <a class="ae ne" href="https://fairseq.readthedocs.io/en/latest/models.html#fairseq.models.transformer.TransformerDecoder" rel="noopener ugc nofollow" target="_blank"> <em class="oe">变压器解码器</em> </a> ) —解码器</li></ul><h2 id="fc38" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><a class="ae ne" href="https://github.com/tensorflow/tensor2tensor" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">张量 2 传感器</strong> </a></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/748913b01da0bc23646410dece3f19e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/0*dkwgn6bVPcP8KC-G"/></div></figure><p id="7999" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">深度学习模型和数据集的库，旨在使深度学习更容易访问并加速 ML 研究。</p><p id="56df" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">您可以尝试使用不同的<strong class="ls iu">变压器型号</strong>和超参数来解决问题，如<a class="ae ne" href="https://arxiv.org/abs/1812.02825" rel="noopener ugc nofollow" target="_blank">论文</a>中所述:</p><ul class=""><li id="cf87" class="ng nh it ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated">标准变压器:<code class="fe og oh oi nv b">--model=transformer</code> <code class="fe og oh oi nv b">--hparams_set=transformer_tiny</code></li><li id="ca31" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated">万能变压器:<code class="fe og oh oi nv b">--model=universal_transformer</code> <code class="fe og oh oi nv b">--hparams_set=universal_transformer_tiny</code></li><li id="1800" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><code class="fe og oh oi nv b">--model=universal_transformer</code>自适应万能变压器:<code class="fe og oh oi nv b">--hparams_set=adaptive_universal_transformer_tiny</code></li></ul><p id="71be" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这里有一个<a class="ae ne" href="https://github.com/tensorflow/tensor2tensor#walkthrough" rel="noopener ugc nofollow" target="_blank">预排</a>来实现来自<a class="ae ne" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <em class="oe">的变压器模型注意的是你所需要的</em> </a>关于 WMT 的数据。</p><h2 id="3f97" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><a class="ae ne" href="http://opennmt.net/" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> OpenNMT </strong> </a></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/ac0004aef2cb575bd1cb8f41d6b41b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*ZxD-GVOa_1XpV8SD"/></div></figure><p id="0728" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一个开源(MIT)神经机器翻译系统。它旨在方便研究人员在翻译、摘要、图像转文本、形态学和许多其他领域尝试新的想法。</p><p id="b554" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">OpenNMT 在两种流行的深度学习框架中提供实现:</p><p id="9803" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu"> OpenNMT-py </strong></p><p id="68e3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">得益于 PyTorch 易用性的可扩展快速实现。</p><ul class=""><li id="5483" class="ng nh it ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><a class="ae ne" href="http://opennmt.net/OpenNMT-py" rel="noopener ugc nofollow" target="_blank">文档</a></li><li id="d4de" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><a class="ae ne" href="http://opennmt.net/Models-py" rel="noopener ugc nofollow" target="_blank">预训练模型</a></li></ul><p id="7452" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><a class="ae ne" href="https://github.com/OpenNMT/OpenNMT-py/blob/master/onmt/decoders/transformer.py" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">变压器</strong>实现代码</a></p><p id="bd5c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu"> OpenNMT-tf </strong></p><p id="dad9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">依托 TensorFlow 生态系统的模块化稳定实施。</p><ul class=""><li id="5273" class="ng nh it ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated"><a class="ae ne" href="http://opennmt.net/OpenNMT-tf" rel="noopener ugc nofollow" target="_blank">文档</a></li><li id="98db" class="ng nh it ls b lt np lw nq lz nr md ns mh nt ml nl nm nn no bi translated"><a class="ae ne" href="http://opennmt.net/Models-tf" rel="noopener ugc nofollow" target="_blank">预训练模型</a></li></ul><p id="6ca4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><a class="ae ne" href="https://github.com/allenai/allennlp" rel="noopener ugc nofollow" target="_blank"> AllenNLP </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/893d1c276283c6da8f68dc429e1a7715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*94WGaM4mIW4hbWuI.png"/></div></div></figure><p id="a395" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">基于 PyTorch 构建的 Apache 2.0 NLP 研究库，用于开发各种语言任务的最新深度学习模型。</p><p id="7a87" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">AllenNLP 支持转换编码器，实现为<code class="fe og oh oi nv b">StackedSelfAttentionEncoder</code></p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="c5ac" class="ms kz it nv b gy nz oa l ob oc">encoder = StackedSelfAttentionEncoder(<br/>     input_dim=EN_EMBEDDING_DIM,<br/>     hidden_dim=HIDDEN_DIM,<br/>     projection_dim=128,<br/>     feedforward_hidden_dim=128,<br/>     num_layers=1,<br/>     num_attention_heads=8)</span></pre><p id="b976" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">推荐阅读:<a class="ae ne" href="http://www.realworldnlpbook.com/blog/building-seq2seq-machine-translation-models-using-allennlp.html" rel="noopener ugc nofollow" target="_blank">使用 AllenNLP 构建 Seq2Seq 机器翻译模型</a></p><h2 id="e452" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><a class="ae ne" href="https://github.com/PaddlePaddle/Paddle" rel="noopener ugc nofollow" target="_blank">T3】paddle paddleT5】</a></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7931a3ef64e34dd14a721ee66510d58a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*smpV-LMnOEoaeh_UtUVpNQ.png"/></div></figure><p id="d196" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">PaddlePaddle(并行分布式深度学习)是一个易用、高效、灵活、可扩展的深度学习平台，最初由百度科学家和工程师开发，目的是将深度学习应用于百度的许多产品。</p><p id="fa53" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">《注意力就是你需要的全部》中变形金刚模型的实现:<a class="ae ne" href="https://github.com/PaddlePaddle/models/tree/develop/PaddleNLP/neural_machine_translation/transformer" rel="noopener ugc nofollow" target="_blank">英文</a>，<a class="ae ne" href="http://paddlepaddle.org/documentation/models/en/1.3/fluid/PaddleNLP/neural_machine_translation/transformer/README_cn.html" rel="noopener ugc nofollow" target="_blank">中文</a></p><h2 id="4196" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><a class="ae ne" href="https://github.com/awslabs/sockeye" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> Sockeye </strong> </a></h2><p id="ad22" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">基于 Apache MXNet 的侧重于神经机器翻译的序列到序列框架。</p><ul class=""><li id="4232" class="ng nh it ls b lt mm lw mn lz ni md nj mh nk ml nl nm nn no bi translated">自关注变压器模型<a class="ae ne" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> Vaswani 等人</a>17、<a class="ae ne" href="https://github.com/awslabs/sockeye/blob/2f44099cd4f488bd8d348d74e9ae85095f72501e/sockeye/transformer.py" rel="noopener ugc nofollow" target="_blank">实现</a></li></ul><h2 id="547f" class="ms kz it bd la mt mu dn le mv mw dp li lz mx my lk md mz na lm mh nb nc lo nd bi translated"><a class="ae ne" href="https://github.com/tensorflow/lingvo" rel="noopener ugc nofollow" target="_blank">T19】LingvoT21】</a></h2><p id="072d" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Lingvo 是一个在 Tensorflow 中构建神经网络的框架，尤其是序列模型。</p><h1 id="2996" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><p id="cd3f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">一个广泛使用的后端框架可以确保你的模型可以被很多人使用。如果框架背后有某种组织，那么这个框架很有可能会长期存在。所以我收集相关信息。</p><p id="5bed" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">可能看到这篇文章的读者主要是研究人员和工程师。所以我把重点放在调试和部署的利弊上。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/52c1e2a4add994854aa733ec0ec84c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m-ASQ5eYiUJQWJqbhwWBCw.png"/></div></div></figure><p id="adae" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">基于你对 PyTroch 或者 TensorFlow 的偏好，我推荐使用 Fairseq 或者 Tensor2Tensor。如果你是一名研究人员，Fairseq 在定制方面足够灵活。但如果是做一些真实的应用，考虑部署的话，选择 Tensor2Tensor 会更好。</p><blockquote class="om on oo"><p id="b361" class="lq lr oe ls b lt mm ju lv lw mn jx ly op mo mb mc oq mp mf mg or mq mj mk ml im bi translated"><strong class="ls iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae ne" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="ls iu"> <em class="it">中</em> </strong> </a> <strong class="ls iu"> <em class="it">同</em> </strong> <a class="ae ne" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> <em class="it">一个分类查看</em> </strong> </a> <strong class="ls iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae ne" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu"><em class="it">bramble Xu</em></strong></a><strong class="ls iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae ne" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu"><em class="it">徐亮</em> </strong> </a> <strong class="ls iu"> <em class="it"> <br/>博客:</em></strong><a class="ae ne" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu"><em class="it">bramble Xu</em></strong></a></p></blockquote></div></div>    
</body>
</html>