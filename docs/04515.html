<html>
<head>
<title>Using TED talks for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用 TED 演讲进行机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-ted-talks-for-machine-learning-1cbbf22b4d72?source=collection_archive---------25-----------------------#2019-07-11">https://towardsdatascience.com/using-ted-talks-for-machine-learning-1cbbf22b4d72?source=collection_archive---------25-----------------------#2019-07-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5a94" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在我第一次尝试建立一个机器学习项目时，我对网上提供的大量资源和教程印象深刻。看看我经历了什么。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/62fb02b8e5976ddd84e2908c69a97850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M0MP3u791S2vy6yi"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@miguel_photo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Miguel Henriques</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="db9f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">这一切是如何开始的</strong></h1><p id="73a3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">今年夏天，我在学习一门关于应用机器学习的课程，并被要求寻找一个项目创意。在我研究的时候，我遇到了一个朋友。这个朋友话很多。我说的很多是指从一个句子开始另一个句子，直到不清楚他在说什么。当他继续说的时候，我试着去想他的句子结构会有多复杂。用积极的话来说:它们一定是在我周围摇曳的美丽而复杂的树。因此我有了一个宽泛的想法:分析文本的结构。我的另一个重要目标是掌握机器学习的四个阶段，即:</p><ol class=""><li id="d6d6" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">框定问题，</li><li id="4cd4" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">准备数据，</li><li id="8e02" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">训练 ML 模型并</li><li id="59d9" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">使用 ML 模型进行预测。</li></ol><p id="4685" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在这个领域做了一些研究(实际上是一整袋新的墨西哥卷饼，我真的很惊讶和不知所措)之后，一个特定的算法引起了我的注意:Word2vec 建模(<a class="ae ky" href="https://medium.com/artists-and-machine-intelligence/ami-residency-part-1-exploring-word-space-andprojecting-meaning-onto-noise-98af7252f749" rel="noopener">这里</a>是 Memo Akten 关于单词嵌入及其应用的一篇非常有趣的文章)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/78a56104cf283275769e9af0e47ddabe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4xeA0NmYiq46PrlJIFy-8Q.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">This is a web application where you can look for similar words according to TED talks, the final result of my work</figcaption></figure><h2 id="638b" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated"><strong class="ak">什么是 word2Vec 算法？</strong></h2><p id="216b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">作为输入，该算法接收大量文本数据(演讲、书籍内容、字典、从网站抓取的文本等)，并将每个单词分配给空间中相应的向量(向量的维度通常在 50–1000 左右，取决于数据集)。这些“单词向量”的位置靠近在类似上下文中使用的其他单词，例如在同一个句子中。例如，单词“sing”可以位于“song”附近。但是作业也取决于数据集(不管你用的是新闻文章还是哲学论文，但我会在最后一个阶段讲到)。</p><h1 id="8eb2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">机器学习的四个阶段</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/7437ade69cb8107f0c448642dabc933d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Wr5vvXNuYHjFNnMNA75lA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The four stages of a machine learning project</figcaption></figure><p id="9774" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">下面我将解释我是如何经历机器学习的四个阶段的。</p><h2 id="7774" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">1.框定问题</h2><p id="292d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">通常，首先弄清楚你想要解决的问题是什么以及一些成功指标，以及机器学习是否是唯一合适的技术是有帮助的(参见<a class="ae ky" href="https://developers.google.com/machine-learning/problem-framing/" rel="noopener ugc nofollow" target="_blank"> this) </a>。但是我没有什么特别的问题，我只是好奇去探索可能性并从中获得灵感。此外，我还想知道如何将机器学习输出用于艺术目的。因此，我的方法是探索各种可能性，并学习建立这样一个项目的基本步骤。</p><h2 id="9009" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">2.准备数据</h2><p id="998d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">准备数据是最有见地的部分。首先，问题是我想使用什么样的数据。因为 word2Vec 模型使用大量的文本是有帮助的，所以我决定使用英语的 TED 演讲记录。TED 演讲提供了大量关于各种事物的演讲。从<a class="ae ky" href="https://www.kaggle.com/rounakbanik/ted-talks" rel="noopener ugc nofollow" target="_blank"> kaggle </a>导入数据集后，该准备数据了。从多个教程(像<a class="ae ky" href="https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial" rel="noopener ugc nofollow" target="_blank">这个</a>或者<a class="ae ky" rel="noopener" target="_blank" href="/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72">这个</a>一个)我用了以下方法:</p><p id="57b3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">英语中有这种缩写，比如“has not”或“is not”，尤其是在口语中，比如 TED 演讲。为了确保这些短语不会影响训练，我扩展了文本语料库中的每个缩写。</p><p id="25a2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">删除特殊字符和数字<br/> </strong>这是一个很容易解释的步骤，用来清理模型并专注于单词。</p><p id="d067" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">删除停用词<br/> </strong>停用词是常用词(如‘a’，‘the’，‘of’，‘in’)，对文本的意义没有贡献，可能会严重影响模型的性能。</p><p id="5ca7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">小写全字<br/>T17】这是我经过几次训练后实施的一个步骤。预先降低所有单词的大小写可以极大地改进模型，因为如果没有它，模型将对“雄心”和“抱负”进行不同的处理。</strong></p><p id="9b5b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">对文本进行词汇化<br/> </strong>词汇化或词干化是自然语言处理中的常用术语。在我们所说的语言中，我们经常使用从另一个词派生出来的词。<br/>因此，有必要将这些特定的单词转换回词根，以避免冗余。</p><p id="db88" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">特别是对于 TED talk 抄本:移除抄本注释<br/> </strong>前面所有的步骤实际上都可以应用于所有的 nlp 项目。但是看一看个人的语料库也是很重要的，所以我注意到了在我的文本中频繁出现的抄本评论。因为它们总是以括号开始和结束，所以很容易通过正则表达式删除它们。</p><p id="5d74" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我对这些方法的顺序掌握得越精确，越能确保它们真正清理了语料库，训练就越好。</p><p id="c932" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我的最后一步是将语料库分成句子，并将每个句子分成模型的单词标记。</p><h2 id="42ed" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated"><strong class="ak"> 3。训练 ML 模型</strong></h2><p id="08dc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">就在培训之前，我检查了最常用的单词，以确保准备足够精确。如您所见，最常见的单词不包含任何特殊字符，并且几乎没有停用词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/becbdd824852e86c255cb7b3915465c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxMOBmRlaMIUwyJjz5vphQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Checking out the most frequent words before training the model</figcaption></figure><p id="c0e6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">然后我初始化了模型。我在这个项目中使用了 Gensim Word2Vec 模型。经过多次测试，这个数据集的理想维数是 50。超过 100 导致拟合不足，从而导致不显著的结果。</p><p id="74bc" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我将最小计数调整为 50，忽略了所有出现频率低于这个最小计数的单词。这导致了总共 4703 个词汇。后来我用 30 个历元训练了这个模型。培训没有像我预期的那样持续很长时间。总共是 3 分钟。</p><h2 id="b2bf" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated"><strong class="ak"> 4。使用 ML 模型预测</strong></h2><p id="3ab5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">由于 word2vec 模型是无监督的，所以我所能测试的就是单词根据一个人的上下文直觉映射的有多好。</p><p id="93ee" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">最有见地的是绘制模型。这意味着将高维向量投射到 2D 画布上(<a class="ae ky" rel="noopener" target="_blank" href="/google-news-and-leo-tolstoy-visualizing-word2vec-word-embeddings-with-t-sne-11558d8bd4d">这里是</a>一个很棒的教程，我就是为了这个而遵循的)。由于模型应该理想地将相似的词放置在彼此更近的位置，这些点应该显示某种聚类，因此这个情节非常令人宽慰(你必须想象:在长时间准备数据和训练之后，最终看到这样的结果是值得的)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/d9d1f0ef5a5ebac46feb129cdd6165bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2K-5vzpRfUr6PqwgSTIvBg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Plotting the word2vec model via TSNE</figcaption></figure><p id="62f9" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">当绘制类似的单词云，如“食物”、“经济”或“爱情”时，你会注意到模型训练得有多好。尤其是看到这些单词云的位置与我将要使用它们的上下文如此接近，令人印象深刻。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/c02a79c3ebc93d9a14fb2f79400fdb3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVYgkHY2twAvK5ReoEIYog.png"/></div></div></figure><h2 id="e976" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">我的申请</h2><p id="7c5b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了使用这个经过训练的模型，我构建了一个 web 应用程序，它不用深入代码就能提供相似的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/78a56104cf283275769e9af0e47ddabe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*4xeA0NmYiq46PrlJIFy-8Q.gif"/></div></div></figure><p id="67e2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这真的很有趣。我用了<a class="ae ky" href="https://nodejs.org/en/" rel="noopener ugc nofollow" target="_blank"> node.js </a>和<a class="ae ky" href="https://p5js.org/" rel="noopener ugc nofollow" target="_blank"> p5 </a>。在这里你可以查看我的<a class="ae ky" href="https://github.com/mekiii/word2vecApp" rel="noopener ugc nofollow" target="_blank"> github </a>。</p><h2 id="8604" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">替代文本语料库</h2><p id="890e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我也试过尼采的书和维基百科的电影情节。由于复杂的文本风格，尼采没有像其他模型那样成功。但是它提供了大量不同的词汇。</p><p id="f2ca" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">维基百科的动机图比谈话语料库大 10 倍，所以准备数据需要更长的时间。结果还没有出来，所以等我的下一篇文章吧😉。</p><h2 id="49db" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">结论</h2><p id="c160" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">总而言之，训练一个模型，而不仅仅是使用一个从模型中派生出来的应用程序，是一次很好的学习经历。现在有如此多的资源，用一些合理的编程知识开始一个项目并不困难。到目前为止，我还不能将这个模型用于艺术目的，但是让我们看看几个月后我会发布什么😉。</p></div></div>    
</body>
</html>