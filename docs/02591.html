<html>
<head>
<title>Need for Feature Engineering in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习需要特征工程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/need-for-feature-engineering-in-machine-learning-897df2ed00e6?source=collection_archive---------8-----------------------#2019-04-28">https://towardsdatascience.com/need-for-feature-engineering-in-machine-learning-897df2ed00e6?source=collection_archive---------8-----------------------#2019-04-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/cdf29a401c6b713b1f8555925dcc8e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NTskwP-xsnVVK9x_"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Franki Chamaki</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="cdd2" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">特性选择真的很重要吗？</h1><p id="9b16" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">特征选择/提取是机器学习中最重要的概念之一，机器学习是选择与问题的建模和商业目标最相关的相关特征/属性(例如表格数据中的列)的子集并忽略数据集中不相关的特征的过程。</p><p id="04b7" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">是的，特性选择真的很重要。不相关或部分相关的特征会对模型性能产生负面影响。</p><p id="b383" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">当特征的数量非常大时，我们不需要使用我们所能支配的每一个特征，这也变得很重要。</p><p id="a13b" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">对数据集进行要素工程的好处</p><h2 id="f59c" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">1.减少过度拟合</h2><h2 id="2bc2" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">2.提高准确性</h2><h2 id="e97a" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">3.减少培训时间</h2><p id="80fe" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们开始实践，当特征很大并且我们不知道如何从数据集中选择相关信息时，我们如何将各种特征工程技术应用于我们的数据集。</p><h2 id="8dfe" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 1:计算标准差为零的特征数。这些是不变的特征。由于这些特征没有变化，因此对模型性能没有影响。</h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="d1e4" class="mh kh it my b gy nc nd l ne nf">import pandas as pd<br/>import numpy as np<br/>data = pd.read_csv('./train.csv')<br/>print("Original data shape- ",data.shape)<br/># Remove Constant Features<br/>constant_features = [feat for feat in data.columns if data[feat].std() == 0]<br/>data.drop(labels=constant_features, axis=1, inplace=True)<br/>print("Reduced feature dataset shape-",data.shape)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/6cb2c4cda23619d731ed88b0c740102d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_SCjljWHnp-pnuw3MRIYQ.png"/></div></div></figure><h2 id="178c" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 2:计算方差较小的特征数。这可以通过使用 sklearn 库中的 VarianceThreshold 的阈值来应用。</h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="c663" class="mh kh it my b gy nc nd l ne nf">from sklearn.feature_selection import VarianceThreshold<br/>sel= VarianceThreshold(threshold=0.18)<br/>sel.fit(df)<br/>mask = sel.get_support()<br/>reduced_df = df.loc[:, mask]<br/>print("Original data shape- ",df.shape)<br/>print("Reduced feature dataset shape-",reduced_df.shape)<br/>print("Dimensionality reduced from {} to {}.".format(df.shape[1], reduced_df.shape[1]))</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/490bd8cdf7427d2e40d99af24c82ce32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MX6U3P1SNLuy-36b3cxsQQ.png"/></div></div></figure><h2 id="4eb9" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 3:去除相关性高的特征。相关性可以是正的(增加一个特征值会增加目标变量的值)，也可以是负的(增加一个特征值会减少目标变量的值)</h2><p id="57bc" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">皮尔逊相关系数为:</p><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/14a485c46ac6b9b7da1276188db4ecd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/0*gxTVcwthB51o1VXO.png"/></div></figure><p id="23d1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">可以使用阈值去除特征，即去除相关系数&gt; 0.8 的那些特征</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="2808" class="mh kh it my b gy nc nd l ne nf">import seaborn as sns<br/>import numpy as np<br/>corr=df_iter.corr()<br/>mask = np.triu(np.ones_like(corr, dtype=bool))<br/># Add the mask to the heatmap<br/>sns.heatmap(corr, mask=mask,  center=0, linewidths=1, annot=True, fmt=".2f")<br/>plt.show()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/133b189680ed540597d2d0395e09437b.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*g-fc3C1jsMTlQ_AnKPHGag.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">heat map with correlation coefficient</figcaption></figure><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="e8e2" class="mh kh it my b gy nc nd l ne nf">corr_matrix = df_iter.corr().abs()<br/># Create a True/False mask and apply it<br/>mask = np.triu(np.ones_like(corr_matrix, dtype=bool))<br/>tri_df = corr_matrix.mask(mask)</span><span id="646b" class="mh kh it my b gy nk nd l ne nf"># List column names of highly correlated features (r &gt;0.5 )<br/>to_drop = [c for c in tri_df.columns if any(tri_df[c] &gt; 0.5)]</span><span id="cd67" class="mh kh it my b gy nk nd l ne nf"># Drop the features in the to_drop list<br/>reduced_df = df_iter.drop(to_drop, axis=1)</span><span id="a213" class="mh kh it my b gy nk nd l ne nf">print("The reduced_df dataframe has {} columns".format(reduced_df.shape[1]</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/16bbaf7d062600bb0f19c10963253e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhJzI7vc-FR7xrY6I20upg.png"/></div></div></figure><h2 id="3b4b" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 4:使用逻辑回归找出关于特征的系数。移除那些具有低 lr_coef 的特征。</h2><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="cab6" class="mh kh it my b gy nc nd l ne nf">from sklearn.preprocessing import StandardScaler <br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression</span><span id="ece4" class="mh kh it my b gy nk nd l ne nf">#calculating the coeff with respect to columns <br/>scaler = StandardScaler()<br/>X_std = scaler.fit_transform(X)</span><span id="c073" class="mh kh it my b gy nk nd l ne nf"># Perform a 25-75% train test split<br/>X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.25, random_state=0)</span><span id="dc13" class="mh kh it my b gy nk nd l ne nf"># Create the logistic regression model and fit it to the data<br/>lr = LogisticRegression()<br/>lr.fit(X_train, y_train)</span><span id="9a75" class="mh kh it my b gy nk nd l ne nf"># Calculate the accuracy on the test set<br/>acc = accuracy_score(y_test, lr.predict(X_test))<br/>print("{0:.1%} accuracy on test set.".format(acc)) <br/>print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/30b509b4c34b68bb9e9d240f7e4f0114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ZZz6ZpaFmcOqjGxgDjkAA.png"/></div></div></figure><h2 id="74a5" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 5:使用 XGBoost 计算特性重要性。</h2><p id="f833" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">要素重要性为数据的每个要素提供一个分数，分数越高，要素对输出变量越重要或越相关。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="2f26" class="mh kh it my b gy nc nd l ne nf">import xgboost as xgb<br/>housing_dmatrix = xgb.DMatrix(X,y)</span><span id="4db0" class="mh kh it my b gy nk nd l ne nf"># Create the parameter dictionary: params<br/>params = {"objective":"reg:linear","max_depth":"4"}</span><span id="9b91" class="mh kh it my b gy nk nd l ne nf"># Train the model: xg_reg<br/>xg_reg = xgb.train(dtrain=housing_dmatrix,params=params,num_boost_round=10)</span><span id="b7d2" class="mh kh it my b gy nk nd l ne nf"># Plot the feature importances<br/>xgb.plot_importance(xg_reg)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/1b590b9772be41c40057bf0b5ce93cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXbSfsD5kFauvArVndpMAQ.png"/></div></div></figure><h2 id="d83e" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 6:使用额外树分类器的特征重要性。</h2><p id="c624" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">基于树的估计器(参见<code class="fe nn no np my b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">sklearn.tree</strong></a></code>模块和<code class="fe nn no np my b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">sklearn.ensemble</strong></a></code>模块中的森林)可用于计算特征重要性，进而可用于丢弃不相关的特征</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="1c4b" class="mh kh it my b gy nc nd l ne nf">X = df.iloc[:,0:370]  #independent columns<br/>y = df.iloc[:,-1]    #target column <br/>from sklearn.ensemble import ExtraTreesClassifier<br/>import matplotlib.pyplot as plt<br/>model = ExtraTreesClassifier()<br/>model.fit(X,y)<br/>print(model.feature_importances_) <br/>#use inbuilt class feature_importances of tree based classifiers<br/>#plot graph of feature importances for better visualization<br/>feat_importances = pd.Series(model.feature_importances_, index=X.columns)<br/>feat_importances.nlargest(20).plot(kind='barh')<br/>plt.show()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nq"><img src="../Images/904ff41e8a1d0393b5cf93415abdb5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3WTxulpc8OBHHG6g1tmkQ.png"/></div></div></figure><h2 id="541e" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 7:递归特征消除(RFE)</h2><p id="8a43" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">给定将权重分配给特征(例如，线性模型的系数)的外部估计器，递归特征消除(<code class="fe nn no np my b"><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE" rel="noopener ugc nofollow" target="_blank"><strong class="lg iu">RFE</strong></a></code>)是通过递归地考虑越来越小的特征集来选择特征。首先，在初始特征集上训练估计器，并且通过<code class="fe nn no np my b">coef_</code>属性或<code class="fe nn no np my b">feature_importances_</code>属性获得每个特征的重要性。然后，从当前特征集中删除最不重要的特征。该过程在删减集上递归重复，直到最终达到要选择的特征的期望数量。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="1b8b" class="mh kh it my b gy nc nd l ne nf">from sklearn.feature_selection import RFE</span><span id="6e17" class="mh kh it my b gy nk nd l ne nf">rfe = RFE(estimator=RandomForestClassifier(random_state=0),n_features_to_select=3,step=2,verbose=1)<br/>rfe.fit(X_train,y_train)<br/>mask=rfe.support_<br/>X_new=X.loc[:,mask]<br/>print(X_new.columns)</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nr"><img src="../Images/efbcffe332cbf29c8b5dc2caeb66ed62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*prEvL-PWKrzmmGMhQt7rzA.png"/></div></div></figure><h2 id="3fa9" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">方法 8:单变量特征选择(ANOVA)</h2><p id="a3cf" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是通过基于单变量统计测试(ANOVA)选择最佳特征来实现的。基于 f 检验的方法估计两个随机变量之间的线性相关程度。它们假设特征和目标之间是线性关系。这些方法还假设变量遵循高斯分布。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="4c43" class="mh kh it my b gy nc nd l ne nf">from sklearn.model_selection import train_test_split<br/>from sklearn.feature_selection import f_classif, f_regression<br/>from sklearn.feature_selection import SelectKBest, SelectPercentile</span><span id="0a85" class="mh kh it my b gy nk nd l ne nf">df= pd.read_csv('./train.csv')<br/>X = df.drop(['ID','TARGET'], axis=1)<br/>y = df['TARGET']<br/>df.head()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ns"><img src="../Images/82ed8f70f61cd0a71d5d4f5028a8e537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ElP3xGI9Q616R1wiTnN1A.png"/></div></div></figure><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="356b" class="mh kh it my b gy nc nd l ne nf"># Calculate Univariate Statistical measure between each variable and target<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)<br/>print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)<br/>univariate = f_classif(X_train.fillna(0), y_train)</span><span id="b376" class="mh kh it my b gy nk nd l ne nf"># Capture P values in a series<br/>univariate = pd.Series(univariate[1])<br/>univariate.index = X_train.columns<br/>univariate.sort_values(ascending=False, inplace=True)</span><span id="dc25" class="mh kh it my b gy nk nd l ne nf"># Plot the P values<br/>univariate.sort_values(ascending=False).plot.bar(figsize=(20,8))</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/cd70f8d5c60d9e9d694f5c6b12f2328f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iJ2bqnNi2qnBmKJS2jKODA.png"/></div></div></figure><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="1d87" class="mh kh it my b gy nc nd l ne nf"># Select K best Features<br/>k_best_features = SelectKBest(f_classif, k=10).fit(X_train.fillna(0), y_train)<br/>X_train.columns[k_best_features.get_support()]</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/cd7fc71ad53ca45c137cef2a9dd0d78a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VK-viZ-fDOzrFMfb-FjMEw.png"/></div></div></figure><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="693f" class="mh kh it my b gy nc nd l ne nf"># Apply the transformed features to dataset <br/>X_train = k_best_features.transform(X_train.fillna(0))<br/>X_train.shape</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nv"><img src="../Images/f0271316f1d6f85fa8a738c98a7f0caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYXXHOwE51oUUmD4fScm_g.png"/></div></div></figure><h1 id="623d" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">降维技术</h1><h2 id="f080" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">主成分分析</h2><p id="1427" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">原始数据有 9 列。在这一部分中，代码将 9 维的原始数据投影到 2 维。我应该注意的是，降维后，通常每个主成分都没有特定的含义。新组件只是变化的两个主要方面。</p><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="cac6" class="mh kh it my b gy nc nd l ne nf">from sklearn.decomposition import PCA<br/>dt=pd.read_csv('./dataset.csv')<br/>X=dt.iloc[0:,0:-1]<br/>y=dt.iloc[:,-1]<br/>pca = PCA(n_components=2)<br/>principalComponents = pca.fit_transform(X)<br/>principalDf = pd.DataFrame(data = principalComponents<br/>             , columns = ['principal component 1', 'principal component 2'])<br/>print("Dimension of dataframe before PCA",dt.shape)<br/>print("Dimension of dataframe after PCA",principalDf.shape)<br/>print(principalDf.head())<br/>finalDf = pd.concat([principalDf, y], axis = 1)<br/>print("finalDf")<br/>print(finalDf.head())</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/860ba63ab034246088e2e4f31351a8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYZUDWnHcSzdYvAIqGtnUg.png"/></div></div></figure><pre class="mt mu mv mw gt mx my mz na aw nb bi"><span id="78f7" class="mh kh it my b gy nc nd l ne nf">#Visualize 2D Projection<br/>fig = plt.figure(figsize = (8,8))<br/>ax = fig.add_subplot(1,1,1) <br/>ax.set_xlabel('Principal Component 1', fontsize = 15)<br/>ax.set_ylabel('Principal Component 2', fontsize = 15)<br/>ax.set_title('2 component PCA', fontsize = 20)<br/>targets = [0, 1]<br/>colors = ['r', 'g']<br/>for target, color in zip(targets,colors):<br/>    indicesToKeep = finalDf['Class'] == target<br/>    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']<br/>               , finalDf.loc[indicesToKeep, 'principal component 2']<br/>               , c = color<br/>               , s = 50)<br/>ax.legend(targets)<br/>ax.grid()</span></pre><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/0a9e8508582df21053aa6d60b9638ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DCu-VVmunopVOZsGOJyd6A.png"/></div></div></figure><h2 id="bfec" class="mh kh it bd ki mi mj dn km mk ml dp kq lp mm mn ku lt mo mp ky lx mq mr lc ms bi translated">解释方差</h2><figure class="mt mu mv mw gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/cd156a6cd978fd9696d0ac2582abd4cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaGOw9YG-8QTJFlhWHh7Ug.png"/></div></div></figure><p id="ae33" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">解释方差告诉你有多少信息(方差)可以归因于每个主成分。这一点很重要，因为当你可以将 371 维空间转换成 2 维空间时，你会丢失一些方差(信息)。通过使用属性<strong class="lg iu">explained _ variance _ ratio _</strong>，可以看到第一主成分包含 88.85%的方差，第二主成分包含 0.06%的方差。这两个部分总共包含 88.91%的信息。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="8958" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">谢谢你！</p><p id="e9b1" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在我的 Youtube 频道上关注我</p><p id="2833" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated"><a class="ae kf" href="https://www.youtube.com/channel/UCSp0BoeXI_EK2W0GzG7TxEw" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/channel/UCSp0BoeXI_EK2W0GzG7TxEw</a></p><p id="08cc" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">在此与我联系:</p><p id="5c67" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">领英:<a class="ae kf" href="https://www.linkedin.com/in/ashishban..." rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/ashishban...</a></p><p id="43a6" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">github:<a class="ae kf" href="https://github.com/Ashishb21" rel="noopener ugc nofollow" target="_blank">https://github.com/Ashishb21</a></p><p id="3896" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">中:<a class="ae kf" href="https://medium.com/@ashishb21" rel="noopener">https://medium.com/@ashishb21</a></p><p id="10f2" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">网站: <a class="ae kf" href="http://techplanetai.com/" rel="noopener ugc nofollow" target="_blank"> http://techplanetai.com/ </a></p><p id="b394" class="pw-post-body-paragraph le lf it lg b lh mc lj lk ll md ln lo lp me lr ls lt mf lv lw lx mg lz ma mb im bi translated">电子邮件 : ashishb21@gmail.com , techplanetai@gmail.com</p></div></div>    
</body>
</html>