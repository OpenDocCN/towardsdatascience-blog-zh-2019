<html>
<head>
<title>How Transformers Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器如何工作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-141e32e69591?source=collection_archive---------0-----------------------#2019-03-11">https://towardsdatascience.com/transformers-141e32e69591?source=collection_archive---------0-----------------------#2019-03-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="78da" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Open AI 和 DeepMind 使用的神经网络</h2></div><p id="61d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢这篇文章，并想了解机器学习算法是如何工作的，它们是如何产生的，它们将走向何方，我推荐以下内容:</p><div class="lb lc gp gr ld le"><a href="https://www.holloway.com/g/making-things-think" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">让事物思考:人工智能和深度学习如何为我们使用的产品提供动力</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">这是显而易见的，但大多数时候却很难看到。人们说‘这和你脸上的鼻子一样明显。’…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">www.holloway.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls lt le"/></div></div></a></div><p id="f891" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变压器是一种越来越受欢迎的神经网络架构。变形金刚最近被 OpenAI 用于他们的语言<a class="ae lu" href="https://blog.openai.com/better-language-models/" rel="noopener ugc nofollow" target="_blank">模型</a>，最近也被 DeepMind 用于<a class="ae lu" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" rel="noopener ugc nofollow" target="_blank">阿尔法星</a>——他们击败顶级职业星际玩家的程序。</p><p id="90ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变形金刚是为了解决<a class="ae lu" href="https://arxiv.org/abs/1211.3711" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">序列转导</strong></a><strong class="kh ir"/>或<strong class="kh ir">神经机器翻译的问题而开发的。</strong>这意味着将输入序列转换成输出序列的任何任务。这包括语音识别、文本到语音转换等..</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/58cf7d8be4d25984d2efbb89e675a4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*mn3V4GHG9OABem9i26NVfg.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Sequence transduction. The input is represented in green, the model is represented in blue, and the output is represented in purple. GIF from <a class="ae lu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">3</a></figcaption></figure><p id="5aac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于执行<strong class="kh ir">序列转导的模型来说，</strong>有某种记忆是必要的。例如，假设我们将下面的句子翻译成另一种语言(法语):</p><blockquote class="mg mh mi"><p id="5285" class="kf kg mj kh b ki kj jr kk kl km ju kn mk kp kq kr ml kt ku kv mm kx ky kz la ij bi translated">“变形金刚”是一支日本[[硬核朋克]]乐队。该乐队成立于 1968 年，正值日本音乐史上的鼎盛时期</p></blockquote><p id="3c93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本例中，第二句中的单词“the band”指的是第一句中介绍的乐队“The Transformers”。当你读到第二句中的乐队时，你知道它指的是“变形金刚”乐队。这对翻译可能很重要。有很多例子，有些句子中的词引用了前面句子中的词。</p><p id="10e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了翻译这样的句子，模型需要找出这种依赖和联系。由于递归神经网络(RNNs)和卷积神经网络(CNN)的特性，它们已经被用来处理这个问题。让我们回顾一下这两种架构及其缺点。</p><h1 id="10b9" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated"><strong class="ak">递归神经网络</strong></h1><p id="7bfa" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">递归神经网络中有环路，允许信息持续存在。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e63a8655740d0e056164d56210b728c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*L38xfe59H5tAgvuIjKoWPg.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">The input is represented as x_t</figcaption></figure><p id="0e89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上图中，我们看到神经网络的一部分，<strong class="kh ir"> A，</strong>处理一些输入 x_t 和输出 h_t，一个循环允许信息从一个步骤传递到下一个步骤。</p><p id="78a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些循环可以用不同的方式来思考。一个递归神经网络可以被认为是同一个网络的多个副本，<strong class="kh ir">一个</strong>，每个网络传递一个信息给下一个。考虑一下如果我们展开循环会发生什么:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nl"><img src="../Images/ce9ed6a2d5d360bb512d44d26069f785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">An unrolled recurrent neural network</figcaption></figure><p id="6abf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种链状性质表明，递归神经网络与序列和列表明显相关。这样，如果我们想要翻译一些文本，我们可以将每个输入设置为该文本中的单词。递归神经网络将前面单词的信息传递给下一个可以使用和处理该信息的网络。</p><p id="a6d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了序列到序列模型通常如何使用递归神经网络工作。每个单词被单独处理，通过将隐藏状态传递给解码阶段来生成结果句子，然后解码阶段生成输出。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/f1417d1c6bcec328139a7af2d259e6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*8GcdjBU5TAP36itWBcZ6iA.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">GIF from <a class="ae lu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">3</a></figcaption></figure><h2 id="3c47" class="nq mo iq bd mp nr ns dn mt nt nu dp mx ko nv nw mz ks nx ny nb kw nz oa nd ob bi translated">长期依赖的问题</h2><p id="e0b6" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">考虑一个语言模型，它试图根据前面的单词预测下一个单词。如果我们试图预测句子<strong class="kh ir">“天空中的云”</strong>的下一个单词，我们不需要进一步的上下文。很明显，下一个词将会是<strong class="kh ir">天空。</strong></p><p id="93fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，相关信息和需要的地方之间的差异很小，RNNs 可以学习使用过去的信息，并找出这个句子的下一个单词是什么。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4e38b44d295548364f1336d5268018e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uQNyY58RRgRNQSnjuYKvkQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">6</a></figcaption></figure><p id="8f1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是有些情况下我们需要更多的上下文。比如说你在试着预测课文的最后一个单词:<strong class="kh ir">“我在法国长大…我说得很流利…”。</strong>最近的信息表明，下一个单词可能是一种语言，但如果我们想缩小哪种语言的范围，我们需要法国的上下文，那是在文本的更后面。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi od"><img src="../Images/ed74111377f75c53e1e9cdc29aab973b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5EbLhyxbPR78PhiV5Esjg.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">6</a></figcaption></figure><p id="aef6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当相关信息和需要信息的点之间的差距变得非常大时，rnn 变得非常无效。这是因为信息是在每一步传递的，链越长，信息就越有可能沿着链丢失。</p><p id="1389" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理论上，RNNs 可以学习这种长期依赖性。在实践中，他们似乎不学<a class="ae lu" href="http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf" rel="noopener ugc nofollow" target="_blank">他们</a>。LSTM，一种特殊类型的 RNN，试图解决这种问题。</p><h1 id="9087" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">长短期记忆(LSTM)</h1><p id="c054" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">当安排一天的日程时，我们会优先安排我们的约会。如果有什么重要的事情，我们可以取消一些会议，把重要的事情放在一边。</p><p id="f14d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RNNs 不会这么做的。每当它添加新信息时，它通过应用一个函数来完全转换现有信息。整个信息都被修改，没有考虑什么重要什么不重要。</p><p id="1045" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LSTMs 通过乘法和加法对信息进行小的修改。在 LSTMs 中，信息通过一种称为单元状态的机制流动。通过这种方式，LSTMs 可以选择性地记住或忘记重要的和不那么重要的事情。</p><p id="a19d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在内部，LSTM 看起来如下:</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oe"><img src="../Images/cace770db820b756504598221c2c97d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">6</a></figcaption></figure><p id="09fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个单元将输入<strong class="kh ir"> x_t </strong>(在句子到句子翻译的情况下是一个单词)、<strong class="kh ir">前一个单元状态</strong>和前一个单元的<strong class="kh ir">输出</strong> <strong class="kh ir">作为输入。它处理这些输入，并基于它们生成新的细胞状态和输出。我不会详细讨论每个细胞的结构。如果你想了解每个细胞是如何工作的，我推荐克里斯托弗的博文:</strong></p><div class="lb lc gp gr ld le"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">了解 LSTM 网络——colah 的博客</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">这些循环使得循环神经网络看起来有点神秘。然而，如果你想得更多一点，事实证明…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">colah.github.io</p></div></div></div></a></div><p id="5aee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用单元状态，在翻译时，句子中对于翻译单词来说重要的信息可以从一个单词传递到另一个单词。</p><h2 id="4d45" class="nq mo iq bd mp nr ns dn mt nt nu dp mx ko nv nw mz ks nx ny nb kw nz oa nd ob bi translated">LSTMs 的问题是</h2><p id="c005" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">通常发生在 rnn 上的同样的问题也会发生在 lstm 上，也就是说，当句子太长时，lstm 仍然做得不太好。其原因是，保持远离当前正在处理的单词的单词的上下文的概率随着离它的距离而指数下降。</p><p id="4019" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着当句子很长时，模型经常会忘记序列中远处位置的内容。RNNs 和 LSTMs 的另一个问题是很难并行处理句子，因为你必须一个字一个字地处理。不仅如此，也没有长期和短期依赖关系的模型。总之，LSTMs 和 RNNs 存在 3 个问题:</p><ul class=""><li id="85e7" class="of og iq kh b ki kj kl km ko oh ks oi kw oj la ok ol om on bi translated">顺序计算抑制了并行化</li><li id="9204" class="of og iq kh b ki oo kl op ko oq ks or kw os la ok ol om on bi translated">没有长短期依赖关系的显式建模</li><li id="9ec3" class="of og iq kh b ki oo kl op ko oq ks or kw os la ok ol om on bi translated">位置之间的“距离”是线性的</li></ul><h1 id="423e" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">注意力</h1><p id="92c8" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">为了解决其中的一些问题，研究人员创造了一种关注特定单词的技术。</p><p id="189a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">翻译句子时，我会特别注意我正在翻译的单词。当我转录一段录音时，我会仔细听我主动写下的片段。如果你让我描述我正坐的房间，我会一边描述一边扫视周围的物体。</p><p id="c037" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络可以使用<strong class="kh ir"> <em class="mj">注意力</em> </strong>实现同样的行为，专注于它们被给予的信息子集的一部分。例如，一个 RNN 可以参与另一个 RNN 的输出。在每一个时间步，它聚焦于另一个 RNN 的不同位置。</p><p id="8e1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了解决这些问题，<strong class="kh ir">注意力</strong>是一种在神经网络中使用的技术。对于 RNNs 来说，每个单词都有一个相应的隐藏状态，并一直传递到解码阶段，而不是仅以隐藏状态对整个句子进行编码。然后，在 RNN 的每一步使用隐藏状态进行解码。下面的 gif 展示了这是如何发生的。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/f0a0d6321a4da351282ce27a7e55214b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*JrxKsw2LYU9emkM-jR13uQ.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">The <strong class="bd ot">green </strong>step is called the <strong class="bd ot">encoding stage </strong>and the purple step is the <strong class="bd ot">decoding stage. </strong>GIF from<strong class="bd ot"> </strong><a class="ae lu" href="http://3" rel="noopener ugc nofollow" target="_blank">3</a></figcaption></figure><p id="6531" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其背后的想法是，一个句子中的每个单词都可能有相关的信息。所以为了解码精确，它需要考虑输入的每个单词，使用<strong class="kh ir">注意力。</strong></p><p id="b7cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了在序列转导中引起对 RNNs 的注意，我们将编码和解码分成两个主要步骤。一步用绿色的<strong class="kh ir">表示，另一步用紫色的<strong class="kh ir">表示。</strong>绿色的<strong class="kh ir">步骤被称为<strong class="kh ir">编码阶段</strong>，紫色的步骤是<strong class="kh ir">解码阶段。</strong></strong></strong></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/3a86399d06200dbfb7c9a2782584ed1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*KD1xANybFo4EC2V2unn3RQ.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">GIF from <a class="ae lu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">3</a></figcaption></figure><p id="e31e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">绿色部分负责从输入中创建隐藏状态。我们不是像使用<strong class="kh ir"> attention </strong>之前那样只将一个隐藏状态传递给解码器，而是将句子的每个“单词”生成的所有隐藏状态传递给解码阶段。每个隐藏状态在解码阶段使用，以计算出网络应该注意的地方。</p><p id="55ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，当将句子“<strong class="kh ir">Je suisétudiant”</strong>翻译成英语时，要求解码步骤在翻译时查看不同的单词。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/85d7af14ded7123ef3a0c376c6b59281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*X5xkbiH-6N-VGeucGK6R5Q.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">This gif shows how the weight that is given to each hidden state when translating the sentence “Je suis étudiant” to English. The darker the color is, the more weight is associated to each word. GIF from <a class="ae lu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">3</a></figcaption></figure><p id="3a93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，当你翻译句子“1992 年 8 月欧洲经济区协议”从法语到英语，以及对每个输入的重视程度。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ou"><img src="../Images/b50f1d7850e9a41bf711801ea2e291fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*Bq8Dll0nAlzIEWwiS627Ew.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Translating the sentence “L’accord sur la zone économique européenne a été signé en août 1992.” to English. Image from <a class="ae lu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">3</a></figcaption></figure><p id="91a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是我们讨论过的一些问题，仍然没有通过使用<strong class="kh ir">注意力的 RNNs 得到解决。例如，并行处理输入(单词)是不可能的。对于大型文本语料库，这增加了翻译文本所花费的时间。</strong></p><h1 id="8d5d" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">卷积神经网络</h1><p id="8bf9" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">卷积神经网络有助于解决这些问题。有了他们，我们可以</p><ul class=""><li id="2897" class="of og iq kh b ki kj kl km ko oh ks oi kw oj la ok ol om on bi translated">并行化很简单(每层)</li><li id="8387" class="of og iq kh b ki oo kl op ko oq ks or kw os la ok ol om on bi translated">利用本地依赖性</li><li id="3b11" class="of og iq kh b ki oo kl op ko oq ks or kw os la ok ol om on bi translated">位置之间的距离是对数的</li></ul><p id="6df9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一些最流行的用于序列转导的神经网络 Wavenet 和 Bytenet 是卷积神经网络。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/55ffc25e1823735038319e2ec481c0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/1*www46FWqJCc3OZQKP_QRoQ.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Wavenet, model is a Convolutional Neural Network (CNN). Image from <a class="ae lu" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="noopener ugc nofollow" target="_blank">10</a></figcaption></figure><p id="8880" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">卷积神经网络可以并行工作的原因是输入的每个单词可以同时处理，而不一定依赖于要翻译的前面的单词。不仅如此，对于一个 CNN 来说，输出单词与任何输入之间的“距离”依次为<a class="ae lu" href="https://www.youtube.com/watch?v=rBCqOTEfxvg&amp;t=8m21s" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">【log(N)</strong></a>——也就是从输出到输入生成的树的高度大小(你可以在上面的 GIF 上看到。这比一个 RNN 的输出和一个输入的距离要好得多，后者的数量级为<a class="ae lu" href="https://www.youtube.com/watch?v=rBCqOTEfxvg&amp;t=8m21s" rel="noopener ugc nofollow" target="_blank">T5【N</a><strong class="kh ir">。</strong></p><p id="fdc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">问题是，卷积神经网络不一定有助于解决翻译句子时的依赖问题。这就是为什么<strong class="kh ir">变形金刚</strong>被创造出来，它们是两个 CNN 与关注的结合。</p><h1 id="1d7e" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">变形金刚(电影名)</h1><p id="3df5" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">为了解决并行化问题，Transformers 试图通过使用卷积神经网络和<strong class="kh ir">注意力模型来解决问题。</strong>注意力提高了模型从一个序列转换到另一个序列的速度。</p><p id="c3cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们来看看<strong class="kh ir">变压器</strong>是怎么工作的。Transformer 是一个利用<strong class="kh ir">注意力</strong>来提升速度的模型。更具体地说，它使用了<strong class="kh ir">自我关注。</strong></p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ow"><img src="../Images/a29b201e572c0d9d16e8fde047acc4c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aqcm4iX3AQNWx9Zb-z7o1Q.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">The Transformer. Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="90a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从内部来看，Transformer 的体系结构与前面的模型类似。但是转换器由六个编码器和六个解码器组成。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ox"><img src="../Images/023913b13b17281a03287ac365d89266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V2435M1u0tiSOz4nRBfl4g.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="796c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个编码器都非常相似。所有编码器都具有相同的架构。解码器共享相同的属性，即它们彼此也非常相似。每个编码器包括两层:<strong class="kh ir">自关注</strong>和一个前馈神经网络。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oy"><img src="../Images/85e700a5443ca56a5afeb9dd1acd9bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HaGTuYfNHWg45GZbTBnVSA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="b86f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">编码器的输入首先流经<strong class="kh ir">自关注</strong>层。它帮助编码器在对特定单词进行编码时查看输入句子中的其他单词。解码器有这两层，但在它们之间有一个注意力层，帮助解码器关注输入句子的相关部分。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/aa58792d97de6dcea6fe1bf90b82288e.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*QcTbVCVPj4WFnqvvWU5-hQ.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><h1 id="44a9" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">自我关注</h1><p id="3210" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated"><strong class="kh ir">注:</strong>此部分来自 Jay Allamar <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">博客文章</a></p><p id="b597" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始看看各种向量/张量，以及它们如何在这些组件之间流动，以将训练模型的输入转化为输出。正如 NLP 应用程序中的一般情况一样，我们首先使用<a class="ae lu" href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca" rel="noopener">嵌入算法</a>将每个输入单词转换成一个向量。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pa"><img src="../Images/a723e46c0831b1cde9ff5fa4008f0933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0oTRj6MKAYEs_cT1.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image taken from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="5082" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个单词被嵌入到大小为 512 的向量中。我们将用这些简单的盒子来表示这些向量。</p><p id="3223" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嵌入只发生在最底层的编码器中。所有编码器共有的抽象是它们接收每个大小为 512 的向量列表。</p><p id="196b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在底部的编码器中，这将是单词嵌入，但在其他编码器中，这将是直接在下面的编码器的输出。在我们的输入序列中嵌入单词后，它们中的每一个都流经编码器的两层中的每一层。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pb"><img src="../Images/49d415253a157f0ecc9f421541f62b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FVCP6TqLPQeWPZqt.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="a758" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我们开始看到转换器的一个关键属性，即每个位置的字在编码器中通过自己的路径流动。在自我关注层，这些路径之间存在依赖关系。然而，前馈层没有这些依赖性，因此各种路径可以在流经前馈层时并行执行。</p><p id="5315" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将把这个例子换成一个更短的句子，看看编码器的每个子层发生了什么。</p><h2 id="b511" class="nq mo iq bd mp nr ns dn mt nt nu dp mx ko nv nw mz ks nx ny nb kw nz oa nd ob bi translated">自我关注</h2><p id="560d" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">让我们首先看看如何使用向量来计算自我注意力，然后继续看看它实际上是如何实现的——使用矩阵。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pc"><img src="../Images/9dce5433c2adf1fbebae93ad04f94b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQzYZuAMWr3lN_IACBfvAA.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figuring out relation of words within a sentence and giving the right <strong class="bd ot">attention</strong> to it. Image from <a class="ae lu" href="http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf" rel="noopener ugc nofollow" target="_blank">8</a></figcaption></figure><p id="9c0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算自我注意力的第一步<strong class="kh ir">是从编码器的每个输入向量中创建三个向量(在这种情况下，是每个单词的嵌入)。因此，对于每个单词，我们创建一个查询向量、一个键向量和一个值向量。这些向量是通过将嵌入乘以我们在训练过程中训练的三个矩阵而创建的。</strong></p><p id="6236" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，这些新向量的维数小于嵌入向量。它们的维数是 64，而嵌入和编码器输入/输出向量的维数是 512。它们不一定要更小，这是一种架构选择，以使多头注意力的计算(大部分)保持不变。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pd"><img src="../Images/de58457ae195b54a9a82222adde0862a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-P9BdUe2FCSAIpxC.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image taken from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="3ac0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将 x1 乘以 WQ 权重矩阵产生 q1，即与该单词相关联的“查询”向量。我们最终创建了输入句子中每个单词的“查询”、“键”和“值”投影。</p><p id="59e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">什么是“查询”、“键”和“值”向量？</p><p id="0883" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它们是对计算和思考注意力有用的抽象概念。一旦你开始阅读下面的注意力是如何计算的，你就会知道你所需要知道的关于这些向量所扮演的角色。</p><p id="7c78" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算自我关注度的<strong class="kh ir">第二步</strong>是计算一个分数。假设我们在计算这个例子中第一个词“思考”的自我关注度。我们需要将输入句子中的每个单词与这个单词进行比较。分数决定了当我们在某个位置对一个单词进行编码时，对输入句子的其他部分的关注程度。</p><p id="4ab5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分数是通过查询向量与我们正在评分的相应单词的关键向量的点积来计算的。因此，如果我们正在处理位置#1 的单词的自我注意，第一个分数将是 q1 和 k1 的点积。第二个分数是 q1 和 k2 的点积。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pe"><img src="../Images/835c99096e298a2b0dd9010280b0b6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/0*KlFsyIDK3O54l14X.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="7562" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第三和第四步</strong>是将分数除以 8(文中使用的关键向量的维数的平方根— 64。这导致具有更稳定的梯度。这里可能有其他可能的值，但这是默认值)，然后通过 softmax 操作传递结果。Softmax 将分数标准化，因此它们都是正数，加起来等于 1。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pf"><img src="../Images/d1d00e6015027bdae36d09c14505e100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rqWSBLDcJcbMmGs2.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="a103" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个 softmax 分数决定了每个单词在这个位置将被表达多少。显然，在这个位置的单词将具有最高的 softmax 分数，但是有时关注与当前单词相关的另一个单词是有用的。</p><p id="5fd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第五步</strong>是将每个值向量乘以 softmax 分数(准备将它们相加)。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词(例如，通过将它们乘以像 0.001 这样的小数字)。</p><p id="5ec2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">第六步</strong>是对加权值向量求和。这就在这个位置产生了自我关注层的输出(针对第一个单词)。</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi pg"><img src="../Images/31a02e7873abbbb338ec375ca3be8686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ih2c_llIiOD1-aJN.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Image from <a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">4</a></figcaption></figure><p id="bfc4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自我关注的计算到此结束。得到的向量是我们可以发送给前馈神经网络的向量。然而，在实际实现中，这种计算是以矩阵形式进行的，以便更快地处理。现在让我们来看看，我们已经看到了单词级别的计算的直觉。</p><h2 id="929d" class="nq mo iq bd mp nr ns dn mt nt nu dp mx ko nv nw mz ks nx ny nb kw nz oa nd ob bi translated">多头注意力</h2><p id="626a" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">变形金刚基本就是这样工作的。还有一些其他细节使它们工作得更好。例如，变形金刚不是只在一个维度上关注彼此，而是使用了多头关注的概念。</p><p id="972f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其背后的想法是，每当你翻译一个单词时，你可能会根据你所提问题的类型对每个单词给予不同的关注。下图显示了这意味着什么。例如，每当你在翻译“我踢了球”这句话中的“kicked”时，你可能会问“谁踢了”。根据不同的答案，这个词的另一种语言的翻译会有所不同。或者问其他问题，比如“做了什么？”等等…</p><figure class="lw lx ly lz gt ma gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ph"><img src="../Images/fe686bf75c4d2a82a5d98d32589ae49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8H6TqcfHrtNCc9_Qva7xog.png"/></div></div></figure><div class="lw lx ly lz gt ab cb"><figure class="pi ma pj pk pl pm pn paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><img src="../Images/69cd0ce988c1302ab950924e9fec0d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*SO7kYPQuS0xi43VgrOBEGQ.png"/></div></figure><figure class="pi ma po pk pl pm pn paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><img src="../Images/5b8635614419d6433a763d569f5d165c.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*ned9q2Svk_LYNT6YqMxk-A.png"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk pp di pq pr">Images from <a class="ae lu" href="http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf" rel="noopener ugc nofollow" target="_blank">8</a></figcaption></figure></div><h2 id="38ca" class="nq mo iq bd mp nr ns dn mt nt nu dp mx ko nv nw mz ks nx ny nb kw nz oa nd ob bi translated">位置编码</h2><p id="3f97" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">转换器的另一个重要步骤是在对每个单词进行编码时添加位置编码。编码每个单词的位置是相关的，因为每个单词的位置与翻译相关。</p><h1 id="99f0" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">概观</h1><p id="b520" class="pw-post-body-paragraph kf kg iq kh b ki nf jr kk kl ng ju kn ko nh kq kr ks ni ku kv kw nj ky kz la ij bi translated">我概述了变压器是如何工作的，以及为什么这是用于序列转导的技术。如果你想深入了解这个模型的工作原理及其所有的细微差别，我推荐下面的帖子、文章和视频，我把它们作为总结这个技术的基础</p><ol class=""><li id="9785" class="of og iq kh b ki kj kl km ko oh ks oi kw oj la ps ol om on bi translated"><a class="ae lu" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a></li><li id="8ce6" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解 LSTM 网络</a></li><li id="b26e" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank">可视化神经机器翻译模型</a></li><li id="0d93" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图示变压器</a></li><li id="5dfe" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XIWlzBNKjOR" rel="noopener ugc nofollow" target="_blank">变压器——你只需要关注</a></li><li id="47b5" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" rel="noopener ugc nofollow" target="_blank">带注释的变压器</a></li><li id="2152" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="https://www.youtube.com/watch?v=rBCqOTEfxvg" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部注意力神经网络模型</a></li><li id="de76" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture14-transformers.pdf" rel="noopener ugc nofollow" target="_blank">生成模型的自我关注</a></li><li id="c45b" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8"> OpenAI GPT-2:通过可视化理解语言生成</a></li><li id="0988" class="of og iq kh b ki oo kl op ko oq ks or kw os la ps ol om on bi translated"><a class="ae lu" href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" rel="noopener ugc nofollow" target="_blank"> WaveNet:原始音频的生成模型</a></li></ol></div></div>    
</body>
</html>