# ç”¨äºæ¨ªç«¿çš„æ·±åº¦ Q å­¦ä¹ 

> åŸæ–‡ï¼š<https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f?source=collection_archive---------5----------------------->

è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯ä»‹ç»æ·±åº¦ Q å­¦ä¹ çš„æ¦‚å¿µï¼Œå¹¶ç”¨å®ƒæ¥è§£å†³ OpenAI å¥èº«æˆ¿çš„ CartPole ç¯å¢ƒã€‚

è¯¥å‘˜é¢å°†ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆ:

1.  å¼€æ”¾å¼äººå·¥æ™ºèƒ½å¥èº«æˆ¿ç¯å¢ƒä»‹ç»
2.  éšæœºåŸºçº¿ç­–ç•¥
3.  æ·±åº¦ Q å­¦ä¹ 
4.  å…·æœ‰é‡æ”¾è®°å¿†çš„æ·±åº¦ Q å­¦ä¹ 
5.  åŒé‡æ·±åº¦ Q å­¦ä¹ 
6.  è½¯æ›´æ–°

## ç¯å¢ƒ

[æ¨ªæ‹‰æ†ç¯å¢ƒ](https://gym.openai.com/envs/CartPole-v0/)ç”±ä¸€æ ¹æ²¿æ— æ‘©æ“¦è½¨é“ç§»åŠ¨çš„æ†å­ç»„æˆã€‚é€šè¿‡å¯¹æ¨è½¦æ–½åŠ +1 æˆ–-1 çš„åŠ›æ¥æ§åˆ¶è¯¥ç³»ç»Ÿã€‚é’Ÿæ‘†å¼€å§‹ç›´ç«‹ï¼Œç›®æ ‡æ˜¯é˜²æ­¢å®ƒç¿»å€’ã€‚çŠ¶æ€ç©ºé—´ç”±å››ä¸ªå€¼è¡¨ç¤º:å°è½¦ä½ç½®ã€å°è½¦é€Ÿåº¦ã€ç£æè§’åº¦å’Œç£æå°–ç«¯çš„é€Ÿåº¦ã€‚åŠ¨ä½œç©ºé—´ç”±ä¸¤ä¸ªåŠ¨ä½œç»„æˆ:å‘å·¦ç§»åŠ¨æˆ–å‘å³ç§»åŠ¨ã€‚æ†ä¿æŒç›´ç«‹çš„æ¯ä¸ªæ—¶é—´æ­¥é•¿æä¾›+1 çš„å¥–åŠ±ã€‚å½“æŸ±å­åç¦»å‚ç›´æ–¹å‘è¶…è¿‡ 15 åº¦ï¼Œæˆ–è€…æ‰‹æ¨è½¦åç¦»ä¸­å¿ƒè¶…è¿‡ 2.4 ä¸ªå•ä½æ—¶ï¼Œè¯¥é›†ç»“æŸã€‚

ä¸‹é¢çš„å•å…ƒæ ¼ç»˜åˆ¶äº†ç¯å¢ƒä¸­çš„ä¸€ç»„ç¤ºä¾‹å¸§:

```
# Demonstration
env = gym.envs.make("CartPole-v1")def get_screen():
    ''' Extract one step of the simulation.'''
    screen = env.render(mode='rgb_array').transpose((2, 0, 1))
    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255.
    return torch.from_numpy(screen)# Speify the number of simulation steps
num_steps = 2# Show several steps
for i in range(num_steps):
    clear_output(wait=True)
    env.reset()
    plt.figure()
    plt.imshow(get_screen().cpu().permute(1, 2, 0).numpy(),
               interpolation='none')
    plt.title('CartPole-v0 Environment')
    plt.xticks([])
    plt.yticks([])
    plt.show()
```

æ ¹æ®å‰§é›†çš„æ•°é‡ï¼Œè¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:

Untrained Agent

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œä»£ç†è¿˜æ²¡æœ‰ç»è¿‡è®­ç»ƒï¼Œæ‰€ä»¥å®ƒåªèƒ½èµ°å‡ æ­¥ã€‚æˆ‘ä»¬å°†å¾ˆå¿«æ¢è®¨ä¸€äº›èƒ½æ˜¾è‘—æé«˜æ€§èƒ½çš„ç­–ç•¥ã€‚ä½†æ˜¯é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰ç»˜å›¾å‡½æ•°ï¼Œå®ƒå°†å¸®åŠ©æˆ‘ä»¬åˆ†æç»“æœ:

```
def plot_res(values, title=''):   
    ''' Plot the reward curve and histogram of results over time.'''
    # Update the window after each episode
    clear_output(wait=True)

    # Define the figure
    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))
    f.suptitle(title)
    ax[0].plot(values, label='score per run')
    ax[0].axhline(195, c='red',ls='--', label='goal')
    ax[0].set_xlabel('Episodes')
    ax[0].set_ylabel('Reward')
    x = range(len(values))
    ax[0].legend()
    # Calculate the trend
    try:
        z = np.polyfit(x, values, 1)
        p = np.poly1d(z)
        ax[0].plot(x,p(x),"--", label='trend')
    except:
        print('')

    # Plot the histogram of results
    ax[1].hist(values[-50:])
    ax[1].axvline(195, c='red', label='goal')
    ax[1].set_xlabel('Scores per Last 50 Episodes')
    ax[1].set_ylabel('Frequency')
    ax[1].legend()
    plt.show()
```

æœ€ç»ˆçš„å‰§æƒ…ç”±ä¸¤ä¸ªæ”¯çº¿å‰§æƒ…ç»„æˆã€‚ç¬¬ä¸€ä¸ªå›¾æ˜¾ç¤ºäº†ä»£ç†åœ¨ä¸€æ®µæ—¶é—´å†…ç´¯ç§¯çš„æ€»å¥–åŠ±ï¼Œè€Œå¦ä¸€ä¸ªå›¾æ˜¾ç¤ºäº†ä»£ç†åœ¨è¿‡å» 50 é›†çš„æ€»å¥–åŠ±çš„ç›´æ–¹å›¾ã€‚å½“æˆ‘ä»¬åˆ†ææˆ‘ä»¬çš„ç­–ç•¥æ—¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ä¸€äº›å›¾è¡¨ã€‚

## åŸºçº¿éšæœºæ¨¡å‹

åœ¨å®ç°ä»»ä½•æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹‹å‰ï¼Œæˆ‘å†™äº†ä¸€ä¸ªç®€å•çš„ç­–ç•¥ï¼Œå…¶ä¸­åŠ¨ä½œæ˜¯ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé‡‡æ ·çš„ã€‚è¿™ç§æ–¹æ³•å°†ä½œä¸ºå…¶ä»–ç­–ç•¥çš„åŸºçº¿ï¼Œå¹¶ä½¿å…¶æ›´å®¹æ˜“ç†è§£å¦‚ä½•ä½¿ç”¨å¼€æ”¾çš„äººå·¥æ™ºèƒ½å¥èº«æˆ¿ç¯å¢ƒä¸ä»£ç†åˆä½œã€‚

```
def random_search(env, episodes, 
                  title='Random Strategy'):
    """ Random search strategy implementation."""
    final = []
    for episode in range(episodes):
        state = env.reset()
        done = False
        total = 0
        while not done:
            # Sample random actions
            action = env.action_space.sample()
            # Take action and extract results
            next_state, reward, done, _ = env.step(action)
            # Update reward
            total += reward
            if done:
                break
        # Add to the final reward
        final.append(total)
        plot_res(final,title)
    return final
```

ä¸€ä¸ªç¯å¢ƒæ­¥éª¤è¿”å›å‡ ä¸ªå€¼ï¼Œæ¯”å¦‚`next_state`ã€`reward`ï¼Œä»¥åŠæ¨¡æ‹Ÿæ˜¯å¦ä¸º`done`ã€‚ä¸‹å›¾æ˜¾ç¤ºäº† 150 é›†(æ¨¡æ‹Ÿè¿è¡Œ)çš„æ€»ç´¯ç§¯å¥–åŠ±:

![](img/373d5740044f10952a9d1e22d303931f.png)

Random Strategy

ä¸Šé¢çš„å›¾å±•ç¤ºäº†éšæœºç­–ç•¥ã€‚ä¸å‡ºæ‰€æ–™ï¼Œç”¨è¿™ç§æ–¹æ³•è§£å†³ç¯å¢ƒé—®é¢˜æ˜¯ä¸å¯èƒ½çš„ã€‚ä»£ç†æ²¡æœ‰ä»ä»–ä»¬çš„ç»éªŒä¸­å­¦ä¹ ã€‚å°½ç®¡æœ‰æ—¶å¾ˆå¹¸è¿(è·å¾—å·®ä¸å¤š 75 çš„å¥–åŠ±)ï¼Œä½†ä»–ä»¬çš„å¹³å‡è¡¨ç°ä½è‡³ 10 æ­¥ã€‚

## æ·±åº¦ Q å­¦ä¹ 

Q-learning èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡½æ•°ğ‘„:ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’Ã—ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†’â„ï¼Œå®ƒå¯ä»¥å‘Šè¯‰ä»£ç†ä»€ä¹ˆè¡Œä¸ºä¼šå¯¼è‡´ä»€ä¹ˆå›æŠ¥ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“ğ‘„çš„ä»·å€¼ï¼Œæˆ‘ä»¬å°±æœ‰å¯èƒ½åˆ¶å®šä¸€ä¸ªæœ€å¤§åŒ–å›æŠ¥çš„æ”¿ç­–:

ğœ‹(ğ‘ )=argmaxğ‘ ğ‘„(ğ‘ ,ğ‘)

ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬æ— æ³•è·å¾—å…¨éƒ¨ä¿¡æ¯ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æƒ³å‡ºè¿‘ä¼¼ğ‘„.çš„æ–¹æ³•ä¸€ç§ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªæŸ¥æ‰¾è¡¨ï¼Œå…¶ä¸­ğ‘„çš„å€¼åœ¨ä»£ç†çš„æ¯ä¸ªåŠ¨ä½œä¹‹åè¢«æ›´æ–°ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¾ˆæ…¢ï¼Œå¹¶ä¸”ä¸èƒ½æ‰©å±•åˆ°å¤§çš„åŠ¨ä½œå’ŒçŠ¶æ€ç©ºé—´ã€‚ç”±äºç¥ç»ç½‘ç»œæ˜¯é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼Œæˆ‘å°†è®­ç»ƒä¸€ä¸ªå¯ä»¥é€¼è¿‘ğ‘„.çš„ç½‘ç»œ

DQL ç±»çš„å®ç°åŒ…æ‹¬ä¸€ä¸ªåœ¨ PyTorch ä¸­å®ç°çš„ç®€å•ç¥ç»ç½‘ç»œï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦çš„æ–¹æ³•â€”â€”é¢„æµ‹å’Œæ›´æ–°ã€‚ç½‘ç»œå°†ä»£ç†çš„çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ¯ä¸ªåŠ¨ä½œçš„ğ‘„å€¼ã€‚ä»£ç†é€‰æ‹©æœ€å¤§ğ‘„å€¼æ¥æ‰§è¡Œä¸‹ä¸€ä¸ªæ“ä½œ:

```
class DQL():
    ''' Deep Q Neural Network class. '''
    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):
            self.criterion = torch.nn.MSELoss()
            self.model = torch.nn.Sequential(
                            torch.nn.Linear(state_dim, hidden_dim),
                            torch.nn.LeakyReLU(),
                            torch.nn.Linear(hidden_dim, hidden_dim*2),
                            torch.nn.LeakyReLU(),
                            torch.nn.Linear(hidden_dim*2, action_dim)
                    )
            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)def update(self, state, y):
        """Update the weights of the network given a training sample. """
        y_pred = self.model(torch.Tensor(state))
        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()def predict(self, state):
        """ Compute Q values for all actions using the DQL. """
        with torch.no_grad():
            return self.model(torch.Tensor(state))
```

q_learning å‡½æ•°æ˜¯åé¢æ‰€æœ‰ç®—æ³•çš„ä¸»å¾ªç¯ã€‚
å®ƒæœ‰è®¸å¤šå‚æ•°ï¼Œå³:

- `env`ä»£è¡¨æˆ‘ä»¬è¦è§£å†³çš„å¼€æ”¾ Ai å¥èº«æˆ¿ç¯å¢ƒ(CartPoleã€‚)
- `episodes`ä»£è¡¨æˆ‘ä»¬æƒ³ç©çš„æ¸¸æˆæ•°é‡ã€‚
- `gamma`æ˜¯ä¸€ä¸ªè´´ç°å› å­ï¼Œä¹˜ä»¥æœªæ¥å¥–åŠ±ï¼Œä»¥æŠ‘åˆ¶è¿™äº›å¥–åŠ±å¯¹ä»£ç†äººçš„å½±å“ã€‚å®ƒçš„ç›®çš„æ˜¯è®©æœªæ¥çš„å¥–åŠ±ä¸å¦‚çœ¼å‰çš„å¥–åŠ±æœ‰ä»·å€¼ã€‚
- `epsilon`è¡¨ç¤ºéšæœºè¡ŒåŠ¨ç›¸å¯¹äºè¡ŒåŠ¨è€…åœ¨äº‹ä»¶ä¸­ç§¯ç´¯çš„ç°æœ‰â€œçŸ¥è¯†â€æ‰€å‘ŠçŸ¥çš„è¡ŒåŠ¨çš„æ¯”ä¾‹ã€‚è¿™ç§ç­–ç•¥è¢«ç§°ä¸ºâ€œè´ªå©ªæœç´¢ç­–ç•¥â€åœ¨ç©æ¸¸æˆä¹‹å‰ï¼Œä»£ç†æ²¡æœ‰ä»»ä½•ç»éªŒï¼Œå› æ­¤é€šå¸¸ä¼šå°† epsilon è®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼Œç„¶åé€æ¸é™ä½å…¶å€¼ã€‚
- `eps_decay`è¡¨ç¤ºä»£ç†å­¦ä¹ æ—¶Îµå‡å°çš„é€Ÿåº¦ã€‚0.99 æ¥è‡ªæœ€åˆçš„ DQN è®ºæ–‡ã€‚

ç¨åå½“æˆ‘ä»¬åˆ°è¾¾ç›¸åº”çš„ä»£ç†æ—¶ï¼Œæˆ‘å°†è§£é‡Šå…¶ä»–å‚æ•°ã€‚

```
def q_learning(env, model, episodes, gamma=0.9, 
               epsilon=0.3, eps_decay=0.99,
               replay=False, replay_size=20, 
               title = 'DQL', double=False, 
               n_update=10, soft=False):
    """Deep Q Learning algorithm using the DQN. """
    final = []
    memory = []
    for episode in range(episodes):
        if double and not soft:
            # Update target network every n_update steps
            if episode % n_update == 0:
                model.target_update()
        if double and soft:
            model.target_update()

        # Reset state
        state = env.reset()
        done = False
        total = 0

        while not done:
            # Implement greedy search policy
            if random.random() < epsilon:
                action = env.action_space.sample()
            else:
                q_values = model.predict(state)
                action = torch.argmax(q_values).item()

            # Take action and add reward to total
            next_state, reward, done, _ = env.step(action)

            # Update total and memory
            total += reward
            memory.append((state, action, next_state, reward, done))
            q_values = model.predict(state).tolist()

            if done:
                if not replay:
                    q_values[action] = reward
                    # Update network weights
                    model.update(state, q_values)
                breakif replay:
                # Update network weights using replay memory
                model.replay(memory, replay_size, gamma)
            else: 
                # Update network weights using the last step only
                q_values_next = model.predict(next_state)
                q_values[action] = reward + gamma *       torch.max(q_values_next).item()
                model.update(state, q_values)state = next_state

        # Update epsilon
        epsilon = max(epsilon * eps_decay, 0.01)
        final.append(total)
        plot_res(final, title)
    return final
```

æœ€ç›´æ¥çš„ä»£ç†åŸºäºå…¶æœ€è¿‘çš„è§‚å¯Ÿæ›´æ–°å…¶ Q å€¼ã€‚å®ƒæ²¡æœ‰ä»»ä½•è®°å¿†ï¼Œä½†å®ƒé€šè¿‡é¦–å…ˆæ¢ç´¢ç¯å¢ƒï¼Œç„¶åé€æ¸é™ä½å…¶Îµå€¼æ¥åšå‡ºæ˜æ™ºçš„å†³å®šã€‚è®©æˆ‘ä»¬æ¥è¯„ä¼°è¿™æ ·ä¸€ä¸ªä»£ç†çš„æ€§èƒ½:

![](img/c18589ea9d4b2db9f0abe0207e979afb.png)

Deep Q Learning

ä¸Šå›¾æ˜¾ç¤ºä»£ç†çš„æ€§èƒ½æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚å®ƒè¾¾åˆ°äº† 175 æ­¥ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œè¿™å¯¹äºä¸€ä¸ªéšæœºçš„ä»£ç†äººæ¥è¯´æ˜¯ä¸å¯èƒ½çš„ã€‚è¶‹åŠ¿çº¿ä¹Ÿæ˜¯æ­£çš„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ€§èƒ½éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ã€‚åŒæ—¶ï¼Œä»£ç†åœ¨ 150 ä¸ªçºªå…ƒåæ²¡æœ‰æˆåŠŸè¾¾åˆ°ç›®æ ‡çº¿ä»¥ä¸Šï¼Œå…¶å¹³å‡æ€§èƒ½ä»åœ¨ 15 æ­¥å·¦å³ï¼Œå› æ­¤æœ‰è¶³å¤Ÿçš„æ”¹è¿›ç©ºé—´ã€‚

## é‡æ”¾è®°å¿†

ä¸€æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬çš„ğ‘„è¿‘ä¼¼ä¸æ˜¯å¾ˆæœ‰æ•ˆã€‚ä¸Šé¢çš„å›¾è¡¨å¾ˆå¥½åœ°è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸éšæœºä»£ç†ç›¸æ¯”ï¼Œç½‘ç»œè®¾æ³•å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒæ— æ³•åˆ°è¾¾ 195 çº§å°é˜¶çš„é—¨æ§›çº¿ã€‚æˆ‘å®ç°äº†ç»éªŒé‡æ”¾ï¼Œä»¥æé«˜ç½‘ç»œç¨³å®šæ€§ï¼Œå¹¶ç¡®ä¿ä»¥å‰çš„ç»éªŒä¸ä¼šè¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯ç”¨äºåŸ¹è®­ã€‚

ä½“éªŒå›æ”¾å°†ä»£ç†çš„ä½“éªŒå­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚æˆæ‰¹çš„ç»éªŒæ˜¯ä»è®°å¿†ä¸­éšæœºæŠ½å–çš„ï¼Œå¹¶ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œã€‚è¿™ç§å­¦ä¹ åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µâ€”â€”è·å¾—ç»éªŒå’Œæ›´æ–°æ¨¡å‹ã€‚é‡æ”¾çš„å¤§å°æ§åˆ¶äº†ç”¨äºç½‘ç»œæ›´æ–°çš„ä½“éªŒçš„æ•°é‡ã€‚å†…å­˜æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå­˜å‚¨ä»£ç†çš„çŠ¶æ€ã€å¥–åŠ±å’ŒåŠ¨ä½œï¼Œä»¥åŠåŠ¨ä½œæ˜¯å¦å®Œæˆæ¸¸æˆå’Œä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚

```
# Expand DQL class with a replay function.
class DQN_replay(DQN):
    def replay(self, memory, size, gamma=0.9):
        """ Add experience replay to the DQN network class. """
        # Make sure the memory is big enough
        if len(memory) >= size:
            states = []
            targets = []
            # Sample a batch of experiences from the agent's memory
            batch = random.sample(memory, size)

            # Extract information from the data
            for state, action, next_state, reward, done in batch:
                states.append(state)
                # Predict q_values
                q_values = self.predict(state).tolist()
                if done:
                    q_values[action] = reward
                else:
                    q_values_next = self.predict(next_state)
                    q_values[action] = reward + gamma * torch.max(q_values_next).item()targets.append(q_values)self.update(states, targets)
```

![](img/645a64dece4038994ca8a062449a26c1.png)

DQL with Replay

æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œä¸åªè®°ä½æœ€åä¸€ä¸ªåŠ¨ä½œçš„ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œå…·æœ‰é‡æ”¾åŠŸèƒ½çš„ç¥ç»ç½‘ç»œä¼¼ä¹æ›´åŠ å¥å£®å’Œæ™ºèƒ½ã€‚å¤§çº¦ 60 é›†ä¹‹åï¼Œä»£ç†äººè®¾æ³•è¾¾åˆ°äº†è·å¥–é—¨æ§›ï¼Œå¹¶ä¿æŒåœ¨è¿™ä¸€æ°´å¹³ã€‚å®ƒè¿˜è®¾æ³•è·å¾—äº†å¯èƒ½çš„æœ€é«˜å¥–åŠ±â€”â€”500 è‹±é•‘ã€‚

## åŒé‡æ·±åº¦ Q å­¦ä¹ 

ä¼ ç»Ÿçš„æ·±åº¦ Q å­¦ä¹ å¾€å¾€ä¼šé«˜ä¼°å›æŠ¥ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œè´¨é‡ç­–ç•¥è¾ƒä½ã€‚è®©æˆ‘ä»¬è€ƒè™‘ Q å€¼çš„ç­‰å¼:

![](img/62eb6cb24053e1ca322d92e6f7408ccf.png)

The Bellman Equation. Source: [Link](https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/)

ç­‰å¼çš„æœ€åä¸€éƒ¨åˆ†æ˜¯å¯¹æœ€å¤§å€¼çš„ä¼°è®¡ã€‚è¿™ä¸€ç¨‹åºå¯¼è‡´ç³»ç»Ÿæ€§é«˜ä¼°ï¼Œä»è€Œå¼•å…¥æœ€å¤§åŒ–åå·®ã€‚ç”±äº Q-learning æ¶‰åŠä»ä¼°è®¡ä¸­å­¦ä¹ ä¼°è®¡ï¼Œè¿™æ ·çš„é«˜ä¼°å°¤å…¶ä»¤äººæ‹…å¿§ã€‚

ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘å°†å®šä¹‰ä¸€ä¸ªæ–°çš„ç›®æ ‡ç½‘ç»œã€‚Q å€¼å°†å–è‡ªè¿™ä¸ªæ–°ç½‘ç»œï¼Œè¿™æ„å‘³ç€åæ˜ ä¸» DQN çš„çŠ¶æ€ã€‚ç„¶è€Œï¼Œå®ƒæ²¡æœ‰ç›¸åŒçš„æƒé‡ï¼Œå› ä¸ºå®ƒåªåœ¨ä¸€å®šæ•°é‡çš„é›†åæ›´æ–°ã€‚è¿™ä¸ªæƒ³æ³•åœ¨ [Hasselt et al .ï¼Œ2015](https://dl.acm.org/citation.cfm?id=3016191) ä¸­é¦–æ¬¡æå‡ºã€‚
æ·»åŠ ç›®æ ‡ç½‘ç»œå¯èƒ½ä¼šé™ä½è®­ç»ƒé€Ÿåº¦ï¼Œå› ä¸ºç›®æ ‡ç½‘ç»œä¸ä¼šæŒç»­æ›´æ–°ã€‚ç„¶è€Œï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒåº”è¯¥å…·æœ‰æ›´ç¨³å¥çš„æ€§èƒ½ã€‚

`q_learning`å¾ªç¯ä¸­çš„`n_update`æŒ‡å®šæ›´æ–°ç›®æ ‡ç½‘ç»œçš„æ—¶é—´é—´éš”ã€‚

```
class DQN_double(DQN):
    def __init__(self, state_dim, action_dim, hidden_dim, lr):
        super().__init__(state_dim, action_dim, hidden_dim, lr)
        self.target = copy.deepcopy(self.model)

    def target_predict(self, s):
        ''' Use target network to make predicitons.'''
        with torch.no_grad():
            return self.target(torch.Tensor(s))

    def target_update(self):
        ''' Update target network with the model weights.'''
        self.target.load_state_dict(self.model.state_dict())

    def replay(self, memory, size, gamma=1.0):
        ''' Add experience replay to the DQL network class.'''
        if len(memory) >= size:
            # Sample experiences from the agent's memory
            data = random.sample(memory, size)
            states = []
            targets = []
            # Extract datapoints from the data
            for state, action, next_state, reward, done in data:
                states.append(state)
                q_values = self.predict(state).tolist()
                if done:
                    q_values[action] = reward
                else:
                    # The only difference between the simple replay is in this line
                    # It ensures that next q values are predicted with the target network.
                    q_values_next = self.target_predict(next_state)
                    q_values[action] = reward + gamma * torch.max(q_values_next).item()targets.append(q_values)self.update(states, targets)
```

![](img/5ff9a17d67fb876766e91f1d03cc4cba.png)

Double DQL with Replay

åŒ DQL é‡æ”¾å·²ç»è¶…è¿‡äº†ä»¥å‰çš„ç‰ˆæœ¬ï¼Œå¹¶ä¸€ç›´æ‰§è¡Œ 300 æ­¥ä»¥ä¸Šã€‚ç”±äºåŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼°çš„åˆ†ç¦»ï¼Œæ€§èƒ½ä¼¼ä¹ä¹Ÿæ›´åŠ ç¨³å®šã€‚æœ€åï¼Œè®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹å¯¹ DQL ä»£ç†çš„æœ€åä¿®æ”¹ã€‚

## è½¯ç›®æ ‡æ›´æ–°

ä¸Šé¢å®ç°çš„ç”¨äºæ›´æ–°ç›®æ ‡ç½‘ç»œçš„æ–¹æ³•åœ¨æœ€åˆçš„ DQN è®ºæ–‡ä¸­ä»‹ç»è¿‡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦ä¸€ç§å®Œå–„çš„æ›´æ–°ç›®æ ‡ç½‘ç»œæƒé‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†åœ¨æ¯æ¬¡è¿è¡Œåä½¿ç”¨ä»¥ä¸‹å…¬å¼é€’å¢åœ°æ›´æ–°ç›®æ ‡ç½‘ç»œï¼Œè€Œä¸æ˜¯åœ¨ä¸€å®šæ•°é‡çš„æ­¥éª¤åæ›´æ–°æƒé‡:

ç›®æ ‡æƒé‡=ç›®æ ‡æƒé‡*(1-Ï„)+æ¨¡å‹æƒé‡*Ï„

å…¶ä¸­ 0 < TAU < 1

This method of updating the target network is known as â€œsoft target network updatesâ€ and was introduced in [Lillicrap ç­‰äººï¼Œ2016](https://arxiv.org/pdf/1509.02971.pdf) ã€‚è¯¥æ–¹æ³•çš„å®ç°å¦‚ä¸‹æ‰€ç¤º:

```
class DQN_double_soft(DQN_double):
    def target_update(self, TAU=0.1):
        ''' Update the targer gradually. '''
        # Extract parameters  
        model_params = self.model.named_parameters()
        target_params = self.target.named_parameters()

        updated_params = dict(target_params)for model_name, model_param in model_params:
            if model_name in target_params:
                # Update parameter
                updated_params[model_name].data.copy_((TAU)*model_param.data + (1-TAU)*target_params[model_param].data)self.target.load_state_dict(updated_params)
```

![](img/17eab5d1f99ab0c2bea99f3cc86eaf0e.png)

DDQL with Soft Update

å…·æœ‰è½¯ç›®æ ‡æ›´æ–°çš„ç½‘ç»œè¡¨ç°ç›¸å½“å¥½ã€‚ä½†æ˜¯ï¼Œå¥½åƒå¹¶ä¸æ¯”ç¡¬æƒæ›´æ–°å¥½ã€‚

è¿™å¼  gif å›¾å±•ç¤ºäº†ä¸€åè®­ç»ƒæœ‰ç´ çš„ç‰¹å·¥çš„è¡¨ç°:

Trained Agent

## ç»“è®º

ç»éªŒé‡æ”¾å’Œç›®æ ‡ç½‘ç»œçš„å®ç°æ˜¾è‘—æé«˜äº†å¼€æ”¾äººå·¥æ™ºèƒ½å¹³å°ç¯å¢ƒä¸‹æ·±åº¦ Q å­¦ä¹ ä»£ç†çš„æ€§èƒ½ã€‚å¯¹ä»£ç†çš„ä¸€äº›å…¶ä»–ä¿®æ”¹ï¼Œå¦‚å†³æ–—ç½‘ç»œæ¶æ„([ç‹ç­‰ï¼Œ2015](https://arxiv.org/pdf/1511.06581.pdf) )ï¼Œå¯ä»¥æ·»åŠ åˆ°è¯¥å®ç°ä¸­ï¼Œä»¥æé«˜ä»£ç†çš„æ€§èƒ½ã€‚è¯¥ç®—æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–ç¯å¢ƒã€‚è¯·éšæ„æµ‹è¯•å®ƒè§£å†³å…¶ä»–ä»»åŠ¡çš„èƒ½åŠ›ï¼

ç¬”è®°æœ¬é“¾æ¥:[https://github . com/ritakurban/Practical-Data-Science/blob/master/DQL _ å¡ç‰¹æ³¢å°”. ipynb](https://github.com/ritakurban/Practical-Data-Science/blob/master/DQL_CartPole.ipynb)

## å‚è€ƒ

(1)å¼ºåŒ– Qâ€”â€”ç”¨ OpenAI Gym åœ¨ Python ä¸­ä»å¤´å­¦ä¹ ã€‚(2019).Learndatasci.comã€‚æ£€ç´¢äº 2019 å¹´ 12 æœˆ 9 æ—¥ï¼Œæ¥è‡ª[https://www . learn data sci . com/tutorials/reinforcement-q-learning-scratch-python-open ai-gym/](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)

(2)å¸•å…¹å…‹ï¼Œa .(2019)ã€‚å¼ºåŒ–å­¦ä¹ (DQN)æ•™ç¨‹ã€‚æ£€ç´¢è‡ª:[https://py torch . org/tutorials/intermediate/reinforcement _ q _ learning . html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)

(3) Lillicrapï¼ŒT. P .ï¼ŒHuntï¼ŒJ. J .ï¼ŒPritzelï¼Œa .ï¼ŒHeessï¼Œn .ï¼ŒErezï¼Œt .ï¼ŒTassaï¼Œy .ï¼Œâ€¦ & Wierstraï¼ŒD. (2015)ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¿ç»­æ§åˆ¶ã€‚arXiv é¢„å°æœ¬ arXiv:1509.02971ã€‚

(4)èŒƒÂ·å“ˆç‘Ÿå°”ç‰¹(2016 å¹´ 3 æœˆå‡ºç‰ˆ)ã€‚åŒ q å­¦ä¹ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚åœ¨ç¬¬ä¸‰åå±Š AAAI äººå·¥æ™ºèƒ½ä¼šè®®ä¸Šã€‚

(5)ç‹ï¼Œz .ï¼Œç»å°”ï¼Œt .ï¼Œèµ«å¡å°”ï¼Œm .ï¼ŒèŒƒå“ˆç‘Ÿå°”ç‰¹ï¼Œh .ï¼Œå…°æ‰˜ç‰¹ï¼Œm .ï¼Œ&å¾·å¼—è±å¡”æ–¯ï¼ŒN. (2015)ã€‚ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å†³æ–—ç½‘ç»œæ¶æ„ã€‚arXiv é¢„å°æœ¬ arXiv:1511.06581ã€‚

(6)åŒ DQN å®ç°è§£å†³ OpenAI å¥èº«æˆ¿çš„æ¨ªæ’‘ v-0ã€‚(2019).ä¸­ç­‰ã€‚æ£€ç´¢äº 2019 å¹´ 12 æœˆ 20 æ—¥ï¼Œæ¥è‡ª[https://medium . com/@ Leo Simmons/double-dqn-implementation-to-solve-open ai-gyms-cart pole-v-0-df 554 CD 0614d](https://medium.com/@leosimmons/double-dqn-implementation-to-solve-openai-gyms-cartpole-v-0-df554cd0614d)