# A*搜索简介

> 原文：<https://towardsdatascience.com/intro-to-a-search-a3dfa444ad20?source=collection_archive---------36----------------------->

## 解释和实施

我们已经讨论过广度优先搜索和深度优先搜索[这里](/search-algorithm-introduction-1-a71e4a1911b3)，它们都有点残酷。考虑到搜索树太宽或太深的情况，这些算法可能会花费大量时间在无意义的搜索中徘徊。那么，我们有没有可能在搜索算法中加入一点智能呢？能否对环境有一个大概的估计，并整合到模型中？答案将我们引向 [A*搜索](https://en.wikipedia.org/wiki/A*_search_algorithm)。

# *的想法

A*的大图与 first-search 没有什么不同，而 BFS 以广度为优先，DFS 以深度为优先，A*也有自己的优先——路径的成本加上我们的估计(或启发式)的组合。

![](img/a2fa9caf11f1a408b58692064aeea69b.png)

假设我们现在在 A 点，准备移动到 B 点，目标是 c 点。从 A 点移动到 B 点有一个自然成本`g(B)`，在 BFS 可能是 1。与 BFS 不同，这里我们加上我们对从 B 到 C 的距离的另一个成本的**估计**，A*的成本就是`g(B) + h(B)`。

很明显，A*的优势在于我们添加了额外的环境信息，如果启发式算法选择正确，它将为我们提供从当前节点到目标的真实成本的更好估计。问题是如何选择一个好的启发式，我们来举个例子。

# 履行

我们将使用上述相同的示例:

在网格世界中，我们从左上角开始，找到到达右下角的路径。我们为每个网格生成的启发是直接的——简单地说就是从每个点到目标的最小距离。*注意启发式必须是* [*可接受的*](https://en.wikipedia.org/wiki/Admissible_heuristic) *，也就是说你的启发式必须小于最优路径*:

![](img/409d2ca21b92b7557e33af936a1c6f00.png)

其中`h(n)`是我们的启发式算法，`h^*(n)`是最优的。实际上，我们肯定不知道最优路径，但是我们可以设置一些宽松的条件来帮助我们生成一个绝对小于最优路径的启发式算法(在这里，我们简单地忽略这些块)。

实现与我们上面介绍的第一个搜索基本相同，唯一的区别在于我们如何引入启发式成本。

在扩展树之前，我们设置初始成本`f`:

```
h = heuristic[x][y]
f = g + h
```

在每个循环中，我们更新了每个子节点的启发式成本:

```
h2 = heuristic[x2][y2]
g2 = g + cost
f2 = g2 + h2
open.append([f2, g2, x2, y2])
```

添加了`f`作为第一个元素，以便开放列表总是能够探索具有最低成本的节点。

我们得到的结果是:

```
[[0, -1, -1, -1, -1, -1],
 [1, -1, -1, -1, -1, -1],
 [2, -1, -1, -1, -1, -1],
 [3, -1, 8, 9, 10, 11],
 [4, 5, 6, 7, -1, 12]]
```

其中`-1`是算法没有搜索到的区域，看看算法是如何避开右上角的大量节点的。由于启发式算法增加了算法探索它们的额外成本，它们最终没有被优先考虑。

# 最佳性

> 如果试探法是可接受的，那么 A*搜索是最优的。

容许使得无论您扩展哪个节点，它都确保当前估计值总是小于最优值，因此将要扩展的路径有机会找到最优路径。考虑上述示例中最简单的启发式算法，使启发式算法中的所有值都为 0，算法变成提供最优解的广度优先搜索。但是试探法越接近最优真值，算法就越有效，认为如果我们有一个试探法精确地给出每个节点的最优估计，那么算法将直接沿着正确的路径以最小的扩展到达目标。

**参考**:

1.  [https://classroom . uda city . com/courses/cs 373/lessons/48646841/concepts/112 e9f 79-63cd-44ee-883 f-652677 e64d 31](https://classroom.udacity.com/courses/cs373/lessons/48646841/concepts/112e9f79-63cd-44ee-883f-652677e64d31)