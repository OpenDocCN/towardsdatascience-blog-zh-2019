<html>
<head>
<title>Guiding Forgetful Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">引导健忘的机器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/guiding-forgetful-machines-72d1b8949138?source=collection_archive---------17-----------------------#2019-10-05">https://towardsdatascience.com/guiding-forgetful-machines-72d1b8949138?source=collection_archive---------17-----------------------#2019-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/916d3947380bf1deb09e98fd413a1a0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/0*fKU1M7uQmCyXOAtw"/></div></figure><p id="bf97" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在说机器之前，先说人类。更像是孩子。</p><p id="6b80" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">还记得学校里的乘法表吗？老实说，我很害怕。每天数学老师都会出现，开始背诵数字和它们的倍数。每天一张新桌子！几个星期后，我开始忘记基础知识。即使我能说出 12 * 5 是多少，我也没记住 5 * 2！然后，老师介绍了“躲避桌子”的概念——更可怕，但它帮助我比以前更好地记住了桌子。</p><p id="349a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为什么会这样？我现在意识到她确保了旧桌子的记忆不会离开我的小脑袋，即使我正在学习新的更复杂的东西。最终目标是让一个人能够“不断学习”。</p><p id="119a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">持续学习是从一系列数据中连续学习的能力，建立在以前所学的基础上，并且能够记住那些所学的任务。这是人类能够做到的，也是人工智能机器的最终目标。在我们的大脑中，新皮层依赖于突触巩固&amp;突触中编码的先前任务的知识，这些知识是不可改变的，因此在很长一段时间内是稳定的。</p><p id="6171" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">所以我想让我们来谈谈如何引导智能机器不要忘记！</p><p id="5c22" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们可以从数据入手，假设有一个我们搭建的猫狗图像识别器(分类器)。现在，如果我们想给它添加一个‘dear’类(我们已经有了这个类的支持数据集图像)，我们该如何着手呢？</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/9a0903b77f2be79df67a74a03abef33a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*Su4aq8vIpDu1gxIwlDYE3g.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">We let the Task A (cat&amp;dog) train, and weights are updated as inferred by the machine.</figcaption></figure><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi le"><img src="../Images/85e757a7743adc4b8d9027af660762e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*pnfz4575KRACJxwQXIjo_Q.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Now we put in the set 2 — Dear images to train &amp; as the machine learns, it updates the weights</figcaption></figure><p id="1b0c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">太好了，现在我们的机器可以识别三种东西了！一只猫/狗/亲爱的</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/0cc08a3d57611f3fa78f86bf0960d3fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*wcN5T0cq02sYUaz7K1ndRw.png"/></div></figure><p id="0d66" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">希望一切正常，我们测试我们的模型…</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/c673430c184056f4b0492935f22a22ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*L1EOzQrP76uRr_lOv1pLrA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">And as expected the model predicts a fine result</figcaption></figure><p id="35e1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">正如预期的那样，模型预测了一个很好的结果！</p><p id="b2b8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在让我们测试一些猫的图像🤞</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/dec61801d14b18e1b016d14f392e9ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*r_C6eQpcYEYxC57op_MMBg.png"/></div></figure><p id="9b09" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">但是这个失败了！怎么会？为什么？</p><p id="af14" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">原因:当添加新任务时，典型的深度神经网络容易出现<strong class="jz iu">灾难性遗忘。</strong></p><p id="a404" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们需要的是能够随着时间的推移吸收新记忆的网络，我们需要努力克服这种限制，训练网络能够<em class="li">在他们很长时间没有经历过的任务上保持</em>专业知识。我们基本上是在努力实现<em class="li">一般的智能</em> &amp;它需要特工们<em class="li">记住</em>许多不同的任务。持续学习是一项挑战，因为随着新任务/当前任务相关信息的临近，先前学习任务的<em class="li">知识</em>有突然丢失的趋势。当一个网络在多项任务上按顺序<em class="li">训练</em>时，会发生这种情况，导致网络中对任务 A 重要的权重发生变化，以满足最近任务 b 的目标。简单的方法是，我们可以-</p><ul class=""><li id="574b" class="lj lk it jz b ka kb ke kf ki ll km lm kq ln ku lo lp lq lr bi translated">让所有数据在培训期间同时可用。</li><li id="e460" class="lj lk it jz b ka ls ke lt ki lu km lv kq lw ku lo lp lq lr bi translated">交错数据又名多任务学习</li></ul><p id="62f1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为什么我们不能继续培训和再培训模特？—因为一个合适的算法将比每次需要学习新任务时从头重新训练模型更有效，而这是非常昂贵的＄＄＄!！！也不是可扩展的解决方案。</p><p id="e3c8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">哺乳动物的新大脑皮层依赖于<em class="li">特定任务</em> <strong class="jz iu">突触整合</strong>和先前任务的知识。它被编码在可塑性较小的突触中，因此长期稳定。科学家和研究人员做了大量的实验来了解我们大脑的内部运作。</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/df47fea493b1528bbd6c8899efa037d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/0*fRbQ3cJwgGdrS3Xi"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">oooh Brainnyy stuffz!</figcaption></figure><p id="3ebd" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">所以从聪明人那里得到了帮助，我指的是神经学家&amp;把他们的知识与酷孩子、ML 的人结合起来</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ly"><img src="../Images/eb270b3c9d248246af238268028e73dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*uSGd1Se8JXewWRRjOj7xkA.png"/></div></div></figure><p id="8cba" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">-生-&gt;神经网络突触巩固！</p><p id="8da9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">突触巩固(在人工神经网络中)意味着，随着机器学习更多的任务，对先前任务至关重要的突触的可塑性(或修改能力)会降低。这就引出了我们将要讨论的第一个解决方案！</p><blockquote class="md"><p id="9d11" class="me mf it bd mg mh mi mj mk ml mm ku dk translated">弹性重量合并[EWC]</p></blockquote><p id="b26f" class="pw-post-body-paragraph jx jy it jz b ka mn kc kd ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku im bi translated">这种算法(用外行的话来说)<em class="li">减缓了</em>对某些权重的学习，这些权重是基于它们对之前看到的任务的重要性。它使用贝叶斯网络和寻找局部最小值的概念。像 EWC 这样的许多方法使用类似的方法，如无遗忘学习、增量矩匹配等。</p><p id="8962" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">对 EWC 来说,<em class="li">的诀窍</em>就是在为下一个任务训练时<strong class="jz iu"> <em class="li">锁定</em> </strong> <em class="li"> </em>用于解决第一个任务的权重。通过锁定，神经网络能够学习新的任务，而不会忘记以前的任务。</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ms"><img src="../Images/b971758c838b4afef18dd9c34d607c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rq2TP4HqDqK29upDHKsjCw.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><a class="ae mt" href="https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/</a></figcaption></figure><p id="9dbe" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">嗯，不是字面上锁定权重，而是收敛到一个点，在这个点上，两个任务的 A &amp; B 都有一个低误差，使用下面的损失函数—</p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/de395bcc0981a34b46e2b3ede364a662.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*Ez0sucC2rJsISULhvIk72g.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">cool looking loss function</figcaption></figure><p id="7004" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">EWC 确实有一个很大的缺点。如前所述，它是基于贝叶斯学习的，因此这些方法<em class="li">将任务 B 的</em>参数限制在任务 A 的最优值周围的局部区域，以便最小程度地干扰任务 A 已经学习的内容。这可能会阻止神经网络在参数空间的偏远区域找到其他区域， 这可能包含任务 A &amp; B 的联合问题分配的损失函数的更好的最小值。我们可以采取一种变通办法，例如存储大部分工作存储器，并在训练新任务时在训练期间重放它，但是这样会占用大量存储器以及大量训练时间。 考虑到图像处理大部分是在 GPU 上完成的，这是不会省钱的！</p><p id="91aa" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在在神经生物学中，我们知道我们的大脑制造记忆——短期的(就像你脑中的声音为你朗读的),储存在一个地方。现在，当我们睡觉时，对你影响最大的事件的重要记忆将存储在大脑的其他部分<em class="li">，这些神经元以这种方式保持无弹性(不可修改)，因此帮助我们记住学到的知识，并在必要时应用它们。</em></p><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/98720a57f22895ae5537d0e425416aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*nA6PLocKv7MdXU2auTCz1A.jpeg"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">brain stuff for cool kids</figcaption></figure><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2b14ca943f200e21f2f709cc3ebcc008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*JmWHKRKkQv65evtt3F9OZQ.png"/></div></figure><p id="7a02" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">让我们混合 EWC 和记忆细胞的牵连吧！(耶？！)</p><blockquote class="md"><p id="ec0e" class="me mf it bd mg mh mi mj mk ml mm ku dk translated">对抗性记忆网络</p></blockquote><p id="62cd" class="pw-post-body-paragraph jx jy it jz b ka mn kc kd ke mo kg kh ki mp kk kl km mq ko kp kq mr ks kt ku im bi translated">这是代表长期记忆的对立子空间的交集，网络中的长期记忆是为每个任务独立存储在记忆单元中的<em class="li">。</em></p><blockquote class="mw mx my"><p id="b98c" class="jx jy li jz b ka kb kc kd ke kf kg kh mz kj kk kl na kn ko kp nb kr ks kt ku im bi translated">任务相关记忆单元(浮点张量)。</p></blockquote><figure class="kw kx ky kz gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1c83672e9e3e83bcb9e5db7adec89222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*BMlbtKXhtYQwaocSeOCchg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><a class="ae mt" href="https://www.ijcai.org/proceedings/2017/0311.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ijcai.org/proceedings/2017/0311.pdf</a></figcaption></figure><p id="6c16" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">记忆单元</p><ul class=""><li id="b3b9" class="lj lk it jz b ka kb ke kf ki ll km lm kq ln ku lo lp lq lr bi translated">扮演类似于对抗性输入图像的角色</li><li id="3e91" class="lj lk it jz b ka ls ke lt ki lu km lv kq lw ku lo lp lq lr bi translated">抓住每堂课的精髓</li><li id="5f00" class="lj lk it jz b ka ls ke lt ki lu km lv kq lw ku lo lp lq lr bi translated">跨越网络参数空间中高级空间的交集</li></ul><p id="35bd" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">使用这些记忆单元，我们就有了一个针对每个任务的<em class="li">的<em class="li">不同</em>类的联合抽象长期记忆。</em></p></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="6b22" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">结论将是看不到最容易做到的解决方案，因为它可能有主要围绕 GPU 使用和内存和成本的缺点！而且，这对环境也不好。看<a class="ae mt" href="https://interestingengineering.com/training-ai-is-shockingly-costly-to-the-environment" rel="noopener ugc nofollow" target="_blank">这个</a>！</p><p id="fc81" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">原来如此！你很好地理解了避免灾难性遗忘的重要性和方法。</strong></p><p id="6460" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">如果您有任何问题，请随时向我发送</strong> <a class="ae mt" href="https://twitter.com/aananya_27" rel="noopener ugc nofollow" target="_blank"> <strong class="jz iu">推文。</strong> </a></p><p id="5b10" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><a class="ae mt" href="https://medium.com/@aananya_27" rel="noopener"> <strong class="jz iu">关注我</strong> </a> <strong class="jz iu">随时更新我的帖子。祝您愉快！🎉</strong></p></div></div>    
</body>
</html>