<html>
<head>
<title>Selecting Optimal Parameters for XGBoost Model Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为 XGBoost 模型训练选择最佳参数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/selecting-optimal-parameters-for-xgboost-model-training-c7cd9ed5e45e?source=collection_archive---------3-----------------------#2019-03-12">https://towardsdatascience.com/selecting-optimal-parameters-for-xgboost-model-training-c7cd9ed5e45e?source=collection_archive---------3-----------------------#2019-03-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ce78" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">所描述的方法有助于找到用 XGBoost 为机器学习模型训练选择参数的方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/79342052fd5ba9adaf197f7a0b81995d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N3f1dzDCYpP2zjbWkucfyQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">XGBoost training results</figcaption></figure><p id="f259" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在为机器学习模型训练选择参数时，总会涉及一点运气。最近，我在研究渐变增强树，特别是 XGBoost。我们在企业中使用 XGBoost 来自动化重复性的人工任务。在用 XGBoost 训练 ML 模型时，我创建了一个选择参数的模式，这有助于我更快地构建新模型。我会在这个帖子里分享它，希望你也会觉得有用。</p><p id="eff9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我使用<a class="ae lr" href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names" rel="noopener ugc nofollow" target="_blank">皮马印第安人糖尿病数据库</a>进行训练，CSV 数据可以从<a class="ae lr" href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="443c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是运行 XGBoost 训练步骤并构建模型的 Python 代码。通过传递成对的训练/测试数据来执行训练，这有助于在模型构建期间特别评估训练质量:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="8567" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">XGBoost 中的关键参数(将极大影响模型质量的参数)，假设您已经选择了 max_depth(更复杂的分类任务，树更深)、子样本(等于评估数据百分比)、目标(分类算法):</p><ul class=""><li id="3e54" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated"><em class="md">n _ estimators</em>—XGBoost 将尝试学习的运行次数</li><li id="0efb" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">学习速率</em> —学习速度</li><li id="ad13" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">early _ stop _ rounds</em>—防止过拟合，如果学习没有改善，则提前停止</li></ul><p id="1a8d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当在 verbose=True 的情况下执行<em class="md"> model.fit </em>时，您将看到每个训练运行评估质量被打印出来。在日志的末尾，您应该看到哪个迭代被选为最佳迭代。可能训练轮数不足以检测最佳迭代，那么 XGBoost 将选择最后一次迭代来构建模型。</p><p id="72bf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用<em class="md"> matpotlib </em>库，我们可以绘制每次运行的训练结果(来自 XGBoost 输出)。这有助于理解被选择来构建模型的迭代是否是最好的。这里我们使用<em class="md"> sklearn </em>库来评估模型精度，然后用<em class="md"> matpotlib </em>绘制训练结果:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ls lt l"/></div></figure><p id="6847" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们描述一下我为 XGBoost 训练选择参数的方法(<em class="md"> n_estimators </em>，<em class="md"> learning_rate </em>，<em class="md"> early_stopping_rounds </em>)。</p><p id="75e8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第一步</strong>。根据你的经验，从你认为最有效的或者有意义的事情开始</p><ul class=""><li id="c576" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated"><em class="md">n _ 估计量</em> = 300</li><li id="af61" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">学习率</em> = 0.01</li><li id="1cc0" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">提前 _ 停止 _ 回合</em> = 10</li></ul><p id="6db8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果:</p><ul class=""><li id="aa4d" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated">停止迭代= 237</li><li id="de4f" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated">准确率= 78.35%</li></ul><p id="2178" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mj"><img src="../Images/9996dd7f51f2a5807c634742c84a4eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MsDX97KNIR21uULTbEHP8w.png"/></div></div></figure><p id="0c4e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过第一次尝试，我们已经在 Pima Indians 糖尿病数据集上获得了良好的结果。训练在迭代 237 时停止。分类误差图显示了在迭代 237 附近较低的误差率。这意味着学习率 0.01 适用于该数据集，并且提前停止 10 次迭代(如果结果在接下来的 10 次迭代中没有改善)是可行的。</p><p id="fa49" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第二步</strong>。尝试学习率，尝试设置一个较小的学习率参数，并增加学习迭代次数</p><ul class=""><li id="9f7d" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated"><em class="md">n _ 估算者</em> = 500</li><li id="b1bc" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">学习率</em> = 0.001</li><li id="1549" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">提前 _ 停止 _ 回合</em> = 10</li></ul><p id="cce1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果:</p><ul class=""><li id="e99c" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated">停止迭代=没有停止，花费了所有 500 次迭代</li><li id="29d2" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated">准确率= 77.56%</li></ul><p id="548c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/cd0a4488439db76eef7fbdb49d7a70ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ixsnSWf--HyLP5yMfhLIYA.png"/></div></div></figure><p id="50c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">较小的学习率对这个数据集不起作用。即使迭代 500 次，分类误差几乎不变，XGBoost 对数损失也不稳定。</p><p id="36e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第三步</strong>。尽量提高学习率。</p><ul class=""><li id="1a57" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated"><em class="md">n _ estimates</em>= 300</li><li id="7cee" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">学习率</em> = 0.1</li><li id="8280" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">提前 _ 停止 _ 回合</em> = 10</li></ul><p id="66ed" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果:</p><ul class=""><li id="3510" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated">停止迭代= 27</li><li id="140e" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated">准确率= 76.77%</li></ul><p id="4500" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/de3ee8d678f7b446fc8d29943884541c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHcyh-mG4iO0J4E1oqfdeA.png"/></div></div></figure><p id="8a15" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随着学习速率的增加，算法学习得更快，它已经在迭代 Nr 处停止。27.XGBoost log loss 误差趋于稳定，但总体分类精度并不理想。</p><p id="1191" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">第四步</strong>。从第一步开始选择最优学习率，增加提前停止(给算法更多机会找到更好的结果)。</p><ul class=""><li id="d3a6" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated"><em class="md">n _ 估算者</em> = 300</li><li id="7e75" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">学习率</em> = 0.01</li><li id="f3c6" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated"><em class="md">提前 _ 停止 _ 回合</em> = 15</li></ul><p id="80dd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果:</p><ul class=""><li id="7d33" class="lu lv iq kx b ky kz lb lc le lw li lx lm ly lq lz ma mb mc bi translated">停止迭代= 265</li><li id="a937" class="lu lv iq kx b ky me lb mf le mg li mh lm mi lq lz ma mb mc bi translated">准确率= 78.74%</li></ul><p id="287a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">结果图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/e96ecdfd517f92c5ea4bde9523db0df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WxpfnYRt_oVIeePW9N7wwQ.png"/></div></div></figure><p id="15f8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">稍微好一点的结果是 78.74%的准确度，这在分类误差图中是可见的。</p><p id="7e5e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">资源:</p><p id="c059" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://github.com/abaranovskis-redsamurai/automation-repo/blob/master/diabetes_redsamurai_db.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub 上的 Jupyter 笔记本</a></p><p id="8d5d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">博文— <a class="ae lr" href="https://bit.ly/2I7VKhr" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本—忘记 CSV，用 Python 从 DB 获取数据</a></p><p id="e88f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">博客文章— <a class="ae lr" href="https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/" rel="noopener ugc nofollow" target="_blank">通过在 Python 中使用 XGBoost 来避免过度拟合</a></p></div></div>    
</body>
</html>