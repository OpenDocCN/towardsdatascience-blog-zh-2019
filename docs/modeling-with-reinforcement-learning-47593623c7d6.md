# 强化学习建模

> 原文：<https://towardsdatascience.com/modeling-with-reinforcement-learning-47593623c7d6?source=collection_archive---------22----------------------->

## 概念和用例

强化学习包括弄清楚*在*哪种*情况下*做什么。这可能很棘手。所有可能的情况中只有极小一部分可能被经历过。如果那样的话。即使在熟悉的情况下，在特定的情况下，一个可靠的行动可能会产生意想不到的结果。环境可能会抛出一个曲线球。

行动有直接的和延迟的后果，可能是相互冲突的。一些延迟的后果可能是未知的。有些人即使在同样的情况下也不会重复。有些可能依赖于初始动作之后的动作。代理试图对所有这些进行分类，以发现并执行适当平衡这两者的行动策略。

在这篇文章中，我们将介绍强化学习的主要概念。我们的视角是一种建模视角。我们考虑各种用例。我们模拟了试图将它们建模为强化学习问题的过程。有趣的问题出现了。获得洞察力。

故事是这样的。我们从框架和基本概念开始。接下来，我们看一些例子来巩固我们的理解。接下来，我们将介绍更多的概念。最后，我们更详细地看一个例子。

**框架**

强化学习包括一个代理人在一个环境中行动，这个环境从某些状态传递奖励。(有时奖励是通过行动来实现的。我们稍后将对此进行深入研究。)在任何给定时间，代理都处于特定状态。她可以选择她能执行的动作。她选择了一个，这通常受到她当前所处状态的影响。她的长期目标是访问提供奖励的州，尽可能多地积累奖励。

然而，在许多用例中，会出现各种各样的挑战。第一个是“环境”并不总是合作。它通过移动到另一个状态来对她的动作做出反应。这一步甚至可能不是决定性的。在同样的情况下——也就是说，在同一个州采取同样的行动——环境可能会做出不同的反应。更糟糕的是，这种反应可能是对抗性的。它可能会故意阻止代理人访问奖励国。

第二个挑战是状态空间可能很大。代理访问一小部分甚至是不可行的。

第三个挑战是状态可能不是完全可观察的。也就是说，代理并不知道当前状态的所有信息。这可能会妨碍她选择最佳行动的能力。

第四种可能性是，同一个州可能会对不同的访问给予不同的回报。也就是说，任何特定状态下的奖励本身就是一个随机变量。

我们需要更多的概念和改进。在进一步讨论之前，我们先停下来看一些具体的例子。

**象棋**

你是代理人。板配置就是状态。有些代表获胜的州，有些代表失败的州。行动是你可以采取的行动。你想达到一个胜利的状态。也就是*赢*。

这些状态是完全可观测的。当前的棋盘配置包含了你选择下一步棋所需的所有信息。(你如何利用这些信息是另一回事。)

州的数量是巨大的。大约

10000000000000000000000000000000000000000000000

环境是对抗性的。你的对手想让你输。

**更多例子**

扑克。你是代理人。这些状态只是部分可观测的。如果你能看到其他玩家的牌，你可能会做得更好。但是你不能。

你正在努力减肥。状态就是你现在的体重。这是完全可以观察到的。你可能的行动是各种锻炼，各种饮食，药物治疗，什么都不做。它们的效果会延迟。你不会马上减肥。甚至不确定你会不会。有些方法可能根本不起作用。

你是一个渴望成为职业高尔夫球手的机器人。让我们集中精力打好一杆。状态的一些关键要素是球到球瓶的距离，以及你是在球道上还是在草地上还是在沙坑里。有些动作是用哪个球杆。包括挥杆在内的动作显然也很重要。想象一个机器人在荡秋千。

你是一个试图学习如何驾驶汽车的机器人。(我指的是传统型，有方向盘、刹车、油门踏板……)状态的某些方面是你当前的速度和方向，因为它们与你所处的道路有关。如果你走得太快，你可能要慢下来。如果你在高速公路上曲折行驶，你可能想“直起腰来”。你动作的一些要素是踩哪个踏板(刹车？，加速器？)，按多少，方向盘的朝向等等。很明显，行动既有(激烈的)直接后果，也有延迟的后果(开得太快有被开罚单或出事故的风险，开得太慢意味着你会迟到，再加上很多车对你鸣喇叭)。

你正试图从购物中心的 A 店走到 B 店。便利的是，购物中心的地图就在商店 a 的旁边。如果购物中心不拥挤，你可能会选择最短的路线。即使弄清楚这一点也可能会涉及到，这取决于 B 离 A 有多远，以及商场的走道是如何安排的。如果商场很拥挤，你还有一个复杂的问题要处理。如果一条最短的路线上的某些路段很拥挤，或者有人朝与你相反的方向移动，那么这条路线可能不是一条好路线。

接下来，前面提到的附加概念。

**行走和奖励**

遍历是一个特定的状态-动作对交替序列，从特定的状态开始，到特定的状态结束。这在下面描述。

```
W: s0 -> a0 -> s1 -> a1 -> ... -> sk
```

代理从状态 s0 开始并执行动作 a0。这导致移动到状态 s1。在 s1 中，代理执行 a1。诸如此类。最终代理结束于状态 sk。如前所述，虽然代理可以完全控制在任何给定的状态下要做的动作，但她通常不能完全控制下一个要访问的状态。环境会做出它选择的任何反应。

步行的(累积)奖励是被访问的州的折扣奖励的总和。

```
R(W) = r0 + a*r1 + a^2 * r2 + ... + a^k * rk
```

这里 0 和 1 之间的 *a* 是折扣因子，ri 是奖励状态 si 提供的。为什么是贴现因子？时光飞逝。有一个短期。有一个长期的。为了寻求即时的满足，将 *a* 设为零。寻求长期回报——即使短期回报不会到来——将 *a* 设为正数。

**代理人的政策**

代理寻求体验产生高累积回报的行走。她怎样才能达到这个目标呢？通过她选择的行动。

让我们把这正式定为代理的政策。策略指定代理从任何给定状态采取的操作。该策略可以是确定性的，即代理总是在每次访问某个状态时执行相同的动作。还是概率性的。在不同的访问中，代理人有不同的反应，尽管有些反应比其他反应更受欢迎。

一般来说，当代理人知道在某个特定的状态下该做什么及其后果时，她会想要利用她的知识。选择她的下一步行动。在不熟悉的状态下，她更倾向于探索。尝试不同的事情，看看会发生什么。这就是强化学习中所谓的探索 vs 开发权衡。

**状态值(在策略下)**

某个状态有多好？这通常取决于代理遵循的策略。对熟练的代理人有利的状态可能对不熟练的代理人不利。不熟练的代理人可能只是不知道如何利用其固有的优点。

让我们把这个正式化。在特定策略下，状态的值是代理可以从执行该策略的行走中期望得到的回报。单词“expect”提醒我们，即使在确定性策略下，行走也可能不完全在代理的控制之下。

在国际象棋中，棋盘配置的价值是代理人从它开始时获胜的可能性。这项政策是含蓄的。代理人想赢。

接下来，我们的最后一个例子。这一个更充实。部分是为了加强刚刚描述的概念。与此同时，也带出新鲜的微妙之处。

**通勤**

假设你从家通勤到公司(或学校)。想象一下你的通勤很“复杂”。不难想象。你在城市街道，高速公路，甚至泥路上行驶。你有很多选择。你刚刚读到过持续使用全球定位系统如何把你的大脑变成一个“沙发土豆”。所以你要练习用你的大脑来路由。嗯，加上你的边缘系统…

*状态和动作*

国家应该是什么样的？*时空*听起来很合理。即特定时间的特定位置。你什么时候出发可能会影响你从 A 到 B 的路线。

你的行动应该是什么？在时间 T 你能从你的当前位置选择的路线段？所谓路段，我们指的是从你当前的状态 A 出发，带你到某个特定位置 b 的道路。类似于地图应用程序将其建议的路线分成路段(转弯方向)。

*奖励和优化标准*

你想优化什么？找到最快的路线？还是最短的，风景最好的，最不丑的，坑洞最少的，红绿灯最少的，卡车流量最少的，海风最好的，树木最多的，…

嗯……又一个用脑的理由。不需要预先指定目标，甚至不需要明确地指定。你的大脑会“感知”你喜欢和不喜欢的路线。即使当你很难准确表达你喜欢什么，更不用说将你的偏好整合成一个多标准的目标函数时，这种方法仍然有效。

你应该选择什么作为你所在州的奖励？嗯……上一段列出的标准适用于*路线*而非*位置*。对你来说路线就是行动。所以看起来你想要的是行动上的奖励而不是状态上的奖励。实际上，想得更多一点，你可能也有在通勤期间你想要参观(或避免)的地点的偏好。所以你既想要行动上的回报，也想要状态上的回报。

嗯，一个路段上的奖励可以同时获取该路段的行驶奖励和该路段的目的地奖励。这是因为操作明确指定了目的地。也就是说，我们假设状态转换是确定的。如果我们在时间 T 从 A 取路段`A->R->B`,我们将在稍后的某个时间到达 B。

这种奖励机制描述如下。

```
reward(route-segment, T) = reward(route-segment.drive, T) + reward(route-segment.destination, destination.arrival-time)
```

到达时间是你到达目的地的时间。这个时间既取决于你什么时候开始这个路段，也取决于沿途发生的其他事情。

如果你的主要兴趣是最快的路线，`reward(route-segment.drive,T)`是一个随机变量。任何特定情况下的驾驶时间取决于交通状况和其他因素。开始时间 T 抓住了这种依赖性的一部分，但不是全部。

*状态值*

你在 t 时刻位于 A 点，你想要从这里开始工作的最快路线。你当前状态的价值是你期望从这里到工作地点需要的时间。如果没有流量，这只是最短路径的长度，长度是以时间而不是距离为单位定义的。引入流量(和其他因素)会注入一些可变性。这就是为什么我们将状态值建模为*预期*时间。

*你的政策*

再说一遍，假设你在优化通勤时间。假设没有交通堵塞，您的最佳策略是从当前位置到目的地走最短的路径。(和以前一样，长度是以时间而不是距离来衡量的。)找到这个策略需要解决一个最短路径问题。如果有交通或其他因素注入可变性，你的任务就更复杂了。如果这些因素取决于你的开始时间，那就更是如此。您可能需要在不同的时间尝试不同的起始状态，并从中采取不同的行动，即下一个路段。这将有助于您估计各种条件下不同路段的行驶时间。当然，你仍然需要从 A 到 B 在一个特定的时间从所有这些开始缝合一个好的多段路线。

上述过程可以用先验知识来播种。也就是说，你可能对不同路段的行程时间有合理的预估。这可以指导你早期的探索。

**总结**

我们已经介绍了强化学习的主题和其中的关键概念。我们的观点是一个模型。我们已经研究了稳定环境、非稳定环境、确定性行为和反应(由环境决定)，以及确定性和随机策略。

我们已经在许多环境中看到了这样的例子:游戏(国际象棋、扑克)、控制(机器人尝试打高尔夫球、开车)、健康(减肥)和路由(逛商场、通勤)。

我们希望读者能够更好地理解强化学习是什么，以及它能解决什么类型的问题。