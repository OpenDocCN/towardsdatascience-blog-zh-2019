<html>
<head>
<title>LSTM to Predict Stock Prices — Time-Series Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM 预测股票价格—时间序列数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recurrent-neural-network-to-predict-multivariate-commodity-prices-8a8202afd853?source=collection_archive---------4-----------------------#2019-10-13">https://towardsdatascience.com/recurrent-neural-network-to-predict-multivariate-commodity-prices-8a8202afd853?source=collection_archive---------4-----------------------#2019-10-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6e11" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">金融时间序列预测</h2><div class=""/><div class=""><h2 id="ad84" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基于神经网络的多变量建模方法</h2></div><p id="862b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi ln translated">金融序列的预测一直是一项复杂的任务，吸引了学术界、投资公司、银行等的极大关注。价格预测通常基于统计标准进行评估，例如<strong class="kt jd"> <em class="lw">平均误差、平均绝对误差(MAE)或均方根误差(RMSE) </em> </strong>。我们已经<a class="ae lx" rel="noopener" target="_blank" href="/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6"> <strong class="kt jd"> <em class="lw">看到了</em> </strong> </a>多个时间序列之间的关系<em class="lw"> ( </em> <strong class="kt jd"> <em class="lw">黄金价格、白银价格、原油价格、股票指数、利率和美元汇率)</em> </strong>以及它们如何相互影响<strong class="kt jd"> <em class="lw">。</em> </strong></p><p id="c82d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们使用相同的数据集，并尝试使用<strong class="kt jd"><em class="lw">【RNN】</em></strong>算法来解决我们的预测问题。众所周知，神经网络在多输入变量的情况下效率很高。</p><p id="1b73" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">先把已经处理好的数据<a class="ae lx" rel="noopener" target="_blank" href="/granger-causality-and-vector-auto-regressive-model-for-time-series-forecasting-3226a64889a6"><strong class="kt jd"><em class="lw"/></strong></a>载入；如果有任何数据错误，希望我们的模型将学会忽略。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ly"><img src="../Images/8e75731f6323b729631e67cc97de3ce1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UdQCK2-CWhzIMd9JgZsT-g.png"/></div></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi mk"><img src="../Images/1c3e202e8e30b3f2705ebbcab1cfe44f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*sVgZJXhQGxqRVsuQ322qlA.png"/></div></div></figure><h2 id="7620" class="ml mm it bd mn mo mp dn mq mr ms dp mt la mu mv mw le mx my mz li na nb nc iz bi translated">单输出前馈网络:</h2><p id="ae67" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">先来了解一下前馈网络的数学直觉。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ni"><img src="../Images/dfaa2aa80fd0f7489a52ee0d0a9cd974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rVu8S9wxIJVSe8m50tQ8tA.png"/></div></div></figure><p id="b252" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中，<em class="lw"> x1，x2…xk </em>是输入序列，<em class="lw"> ŷ </em>是输出序列，中间是一串隐藏层。然而，这里的问题是-</p><ul class=""><li id="8681" class="nj nk it kt b ku kv kx ky la nl le nm li nn lm no np nq nr bi translated">信息不考虑时间顺序</li><li id="4343" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated">输入是独立处理的</li><li id="8be1" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated">没有保存过去信息的机制</li></ul><blockquote class="nx"><p id="703d" class="ny nz it bd oa ob oc od oe of og lm dk translated">前馈网络结构没有内置存储器。此外，出现了一种情况，其中深层多层前馈网络不能将有用的梯度信息从模型的输出端传播回模型输入端附近的层”</p></blockquote><p id="a2a7" class="pw-post-body-paragraph kr ks it kt b ku oh kd kw kx oi kg kz la oj lc ld le ok lg lh li ol lk ll lm im bi translated">为了克服这些困难，<strong class="kt jd"> <em class="lw"> RNN </em> </strong>被引入，它带有一个网络架构，可以-</p><ul class=""><li id="348a" class="nj nk it kt b ku kv kx ky la nl le nm li nn lm no np nq nr bi translated">保留过去的信息</li><li id="5938" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated">跟踪世界的状态</li><li id="253d" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated">随着网络速度的加快，更新世界的状态</li></ul><blockquote class="nx"><p id="ee99" class="ny nz it bd oa ob oc od oe of og lm dk translated">“RNN 通过具有递归隐藏状态来处理可变长度序列，该隐藏状态的每次激活都依赖于前一次的激活”</p></blockquote><p id="ab76" class="pw-post-body-paragraph kr ks it kt b ku oh kd kw kx oi kg kz la oj lc ld le ok lg lh li ol lk ll lm im bi translated">下面的图表和等式清楚地说明了<em class="lw"> RNN </em>是如何工作的</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi om"><img src="../Images/500ec82c5e32756354f8818d4ab8c7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*GVWXH4otMLlt8Ti5671o-w.png"/></div></div></figure><p id="c8d3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">其中，𝑥𝑡表示输入，ℎ𝑡表示从输入到输出的隐含层，<em class="lw"> yt </em>为输出。最重要的是，它有一个循环回路，循环回到过去，以表示输出不仅是新输入的函数，而且是过去隐含层的函数，这样网络就能保持增长。然而，它有一个问题<strong class="kt jd">爆炸和消失梯度</strong>。<em class="lw">递归神经网络(LSTM) </em>解决了这个问题。它们是带有环路的网络，允许信息持续存在。对于<em class="lw"> LSTM </em>来说，最重要的概念是单元的状态，这些信息通过嵌入在单元顶部的<em class="lw"> yt </em>数据在单元之间相互传输。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi on"><img src="../Images/3bc71d0f456573727834d033653d330b.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*kYwSUayAe_uxD6CPgItRow.png"/></div></figure><p id="ac6f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在上图中，神经网络 A 的一个块查看某个输入 xt 并输出一个值 ht。环路允许信息从网络的一个步骤传递到下一个步骤。这些循环使得递归神经网络成为一种神秘的物体。然而，它们与普通的神经网络并没有什么不同。一个<em class="lw">递归神经网络</em>是同一个网络的多个副本，每个副本向后继者传递一个信息。细胞状态就像一条传送带，通过整个链条传送。细胞可以对这些信息进行微小的改变。牢房里有门。这些门包含一个<em class="lw"> sigmoid </em>功能，并根据输出值打开和关闭 sigmoid 功能。然后门输出值和<em class="lw"> yt </em>值相乘。sigmoid 函数输出可以取 0 到 1 之间的值，而零值将关闭信息转换，并在整个信息中移动一个值。如果循环展开，如下图所示-</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi oo"><img src="../Images/38cd0a04233d6eea1e5de3348724ca13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u25kNvJ8k7Es-IjyZM7tog.png"/></div></div></figure><p id="7efb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这种链状性质揭示了递归神经网络与序列和列表密切相关。它们是用于此类数据的神经网络的自然架构。让我们考虑在时间步<em class="lw"> t </em>的隐藏状态<em class="lw"> ht </em>。LSTM 单元需要决定的第一件事是报告单元状态。这个决定是由<em class="lw">忘栅层</em>做出的。<em class="lw">遗忘门层</em>通过查看<em class="lw">ht1</em>和𝑥𝑡.，为每个<em class="lw">yt1</em>生成一个在<em class="lw"> 0 </em>和<em class="lw"> 1 </em>之间的值 1 表示数据被存储，0 表示数据将被遗忘。由忘记栅极层执行的过程的数学表达式:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi op"><img src="../Images/1d9d84598af1e6cb1d20d9f8bad651ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*dGqR_KwygVwYtP7_mzzTkw.png"/></div></figure><p id="d5d8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">下一步是关于是否在单元状态中记录新信息。输入门层决定更新哪个值。输入栅极层和输入栅极层的输出可以表示为:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2aa6f9cdd4b1c55699c45f441ae551ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*_s5_id5MjRphxiad-TV30w.png"/></div></figure><p id="1feb" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">非线性函数层<em class="lw"> (tanh) </em>，其生成新的候选值的向量，<em class="lw"> ŷt </em>，其可以被添加到状态。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi or"><img src="../Images/05b3ef3730d32872b1e97745642ae176.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*vleteogP9Cz3xVj44ewLHw.png"/></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f221811b80c872f8472cab13666d7eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*rrsaOYpxutDYdllSsuOvMg.png"/></div></figure><p id="9432" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后更新旧的单元状态，并确定代表新的单元状态的 y𝑡值。可以写出下面的等式来获得新的 y𝑡值。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f221811b80c872f8472cab13666d7eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*rrsaOYpxutDYdllSsuOvMg.png"/></div></figure><p id="8703" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，为确保输出值介于-1 和 1 之间并对输出进行滤波，输出函数可指定为:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6633aeda4b6858a282aa9991e7b7041e.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*ewGR7L6ZOx4_5WZyCGrBIg.png"/></div></figure><p id="f447" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">LSTM 也有渐变消失的问题，但没有基本的 RNN 那么严重。区别在于对于基本 RNN，梯度随 wσ′(⋅衰减，而对于 LSTM，梯度随σ(⋅).衰减对于 LSTM，有一组可以学习的权重，例如σ(⋅)≈1.假设对于某个权重 w 和输入 x，vt + k = wx，那么<em class="lw">神经网络</em>可以学习一个大的<em class="lw"> w </em>来防止梯度消失。对于基本的<em class="lw"> RNN </em>，没有一套可以学习的砝码:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/622ee4ee684383496130b68b975575cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*XdQkml63WMpG0VkFjSRwcQ.png"/></div></figure><blockquote class="nx"><p id="792b" class="ny nz it bd oa ob oc od oe of og lm dk translated">在<strong class="ak"> Keras </strong>中的<strong class="ak"> LSTM </strong>模型假设数据分为输入(x)和输出(y)分量。这可以通过在时间序列中使用来自上一时间步(t-1)的观测值作为输入和当前时间步(t)的观测值作为输出来实现。</p></blockquote><p id="222f" class="pw-post-body-paragraph kr ks it kt b ku oh kd kw kx oi kg kz la oj lc ld le ok lg lh li ol lk ll lm im bi translated">让我们添加一个<em class="lw">“pred”</em>列作为我们的输出，并移动它。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ov"><img src="../Images/7d29fe48a22c0b839c947592c8c613dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvhpCbKhQynn9Jd32dS2hg.png"/></div></div></figure><h2 id="b42b" class="ml mm it bd mn mo mp dn mq mr ms dp mt la mu mv mw le mx my mz li na nb nc iz bi translated">标准化数据:</h2><p id="83c2" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">缩放和标准化是形成<em class="lw">神经网络架构</em>的重要活动。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/859f88114b09cc00516c1c80c378cb49.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*oAJFviuWPbiChgyWqg-0qw.png"/></div></figure><p id="acd1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们检查数据，它有一个很宽的值范围<em class="lw"> (1.336，13592.79) </em>而神经网络在这种范围内表现不好。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi ox"><img src="../Images/d99d25355b4cf6f70eed02617911a786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3d2Evwyq-NuXONgksTGDPA.png"/></div></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/9dc131dfae69af8fbf73b97d776d0045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*vnnXDqqx6n5Qn59d3d0Twg.png"/></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/db23cc5e8601c4933018588159c6895a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*envVeXbL6SXeyFYClCfpCw.png"/></div></figure><p id="a8c9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们定义了一个函数来指定回看间隔(60 个时间步长)和预测列，如下所示。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pa"><img src="../Images/f4943ef50ab9bc8966f05f8d69b250fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dojZAjdsWifzvfoMwu6T2A.png"/></div></div></figure><h2 id="7ae4" class="ml mm it bd mn mo mp dn mq mr ms dp mt la mu mv mw le mx my mz li na nb nc iz bi translated">拆分数据并将其转换为正确的形状:</h2><p id="aa93" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">在这里，我将数据按 80/20 的比例进行了拆分。此外，我们需要将数据整形为 3D 数组，以供<strong class="kt jd"><em class="lw"/></strong>读取和训练模型。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi pb"><img src="../Images/d5589320eeda7824f5511566b14881c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tz9_hE70wLkc7L6FmEtQ_A.png"/></div></div></figure><p id="b6d7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以看到，训练集包含 4103 个数据点，测试集包含 966 个数据点。</p><h1 id="a821" class="pc mm it bd mn pd pe pf mq pg ph pi mt ki pj kj mw kl pk km mz ko pl kp nc pm bi translated">LSTM 网络架构:</h1><p id="2e23" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">现在，我们添加<em class="lw"> LSTM </em>层和几个脱落层，以防止过度拟合。我在网络中增加了 3 层。对于网络架构，需要考虑以下因素:</p><ul class=""><li id="4173" class="nj nk it kt b ku kv kx ky la nl le nm li nn lm no np nq nr bi translated">50 个单位，这是输出空间的维度，</li><li id="51b2" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated"><em class="lw"> return_sequences = True，</em>决定是返回输出序列中的最后一个输出，还是整个序列，</li><li id="8969" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated"><em class="lw"> input_shape </em>作为训练集的形状，</li><li id="4a97" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated">指定为 0.2 的丢弃层，<em class="lw">即</em>将丢弃 20%的层，</li><li id="b34d" class="nj nk it kt b ku ns kx nt la nu le nv li nw lm no np nq nr bi translated">此后，我添加了指定 1 个单位输出的<em class="lw">密集</em>层，</li></ul><p id="9727" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，该模型适合在<em class="lw"> 100 个时期</em>上运行，并且<em class="lw">的批量大小为 32 </em>。代替在完整的观察序列上训练<strong class="kt jd"><em class="lw">【RNN】</em></strong>，我们使用了从训练数据中随机选取的一批较短的子序列(32)。</p><pre class="lz ma mb mc gt pn po pp pq aw pr bi"><span id="e3c3" class="ml mm it po b gy ps pt l pu pv">model_lstm = tf.keras.Sequential()<br/>model_lstm.add(tf.keras.layers.LSTM(75, return_sequence = True, input_shape = (xtrain.shape[1], xtrain.shape[2])))<br/>model_lstm.add(tf.keras.layers.LSTM(units=30, return_sequence=True))<br/>model_lstm.add(tf.keras.layers.LSTM(units=30))<br/>model_lstm.add(tf.keras.layers.Dense(units=1))<br/>model_lstm.Compile(loss = 'mae', optimizer = 'adam')<br/>model_lstm.summary()</span><span id="1ea3" class="ml mm it po b gy pw pt l pu pv">history_lstm = model_lstm.fit(xtrain, ytrain, epochs = 15,        batch_size=32, validation_data = (xtest, ytest), shuffle=False)</span></pre><p id="0456" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">创建了训练集和测试集的模型准确度的线图，显示了所有 15 个训练时期的性能变化。下面的测试数据(蓝色)与预测值(橙色)的线图提供了模型技能的背景。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi px"><img src="../Images/296328fb63d7951167946e6d869751b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sx3pDrHEbviE_q5RRRfsPw.png"/></div></div></figure><h2 id="fb71" class="ml mm it bd mn mo mp dn mq mr ms dp mt la mu mv mw le mx my mz li na nb nc iz bi translated">用于预测的拟合 LSTM 模型:</h2><p id="4bf0" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">模型符合训练数据；因此，它可以用来做预测。现在，使用训练好的模型来预测测试数据集中的<strong class="kt jd"><em class="lw">【pred】</em></strong>值。我在这里使用固定的方法进行预测。</p><blockquote class="py pz qa"><p id="af4a" class="kr ks lw kt b ku kv kd kw kx ky kg kz qb lb lc ld qc lf lg lh qd lj lk ll lm im bi translated">固定方法:我们可以决定对所有训练数据拟合一次模型，然后从测试数据中一次一个地预测每个新的时间步长</p><p id="20a8" class="kr ks lw kt b ku kv kd kw kx ky kg kz qb lb lc ld qc lf lg lh qd lj lk ll lm im bi translated">动态方法:当从测试数据中获得新的观察结果时，我们可以在测试数据的每个时间步重新拟合模型或更新模型。</p></blockquote><p id="7e6f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们必须反转预测的标度，将数值返回到原始标度，以便可以解释结果并计算出可比的误差分数。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi qe"><img src="../Images/6cb3179b7da88aaad55a793c3c405d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EK4-l-i3wU5IQtnBVL2BBQ.png"/></div></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi qf"><img src="../Images/b5846ea8ddf2347509d52516f38a986d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1Ajx9D1-Olkld-QIrdvjg.png"/></div></div></figure><p id="a637" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们可以看到，我们还没有完全实现。结果并不令人印象深刻。我们可以检查这两者之间的差异，并以各种方式比较结果&amp;在建立交易策略之前优化模型。</p><p id="e295" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让我们看看是否可以通过移动 10 个时间步长和做一些超参数调整来提高模型性能。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi qg"><img src="../Images/7d88c145304eb8121dc6e565055b7267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LmrgNpCWf_WfN5vwXM_EMw.png"/></div></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi qh"><img src="../Images/6409589c71d807ed7bee5c6ce8ac0953.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4_935I_-MWzA7gLFhevHw.png"/></div></div></figure><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi qf"><img src="../Images/ffa0d3e30e46c8b05f050fc67b4d2312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U70wOH-DwBcIxWyGcHzw6Q.png"/></div></div></figure><p id="56a1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在训练集和测试集上，这里的线图看起来比前两个更好。该模型似乎能够快速学习问题，在大约 20 个时期内收敛到一个解决方案。损耗指标，如 RMSE、平均损耗，可以通过前面所示的类似方法获得。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi qi"><img src="../Images/2b7b3b3c37dcfbb48338dc8fff1520c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*azixCDGvCh6h9N7THMiMgg.png"/></div></div></figure><p id="9fec" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">虽然有一些改进，但在预测方面，我们仍有改进的余地。这里的一个重要因素是，我们在这个模型中使用了每日价格，因此数据点实际上很少，对于<em class="lw">神经网络架构</em>，只有 5129 个数据点。我建议使用超过<em class="lw"> 100，000 个</em>数据点，使用分钟或分笔成交点数据来训练模型，同时建立<em class="lw"> ANN </em>或任何其他<em class="lw">深度学习</em>模型，以便最有效。</p><h1 id="6d47" class="pc mm it bd mn pd pe pf mq pg ph pi mt ki pj kj mw kl pk km mz ko pl kp nc pm bi translated">总结:</h1><p id="c012" class="pw-post-body-paragraph kr ks it kt b ku nd kd kw kx ne kg kz la nf lc ld le ng lg lh li nh lk ll lm im bi translated">为了公平地测量网络的性能，使用了六个不同的时间序列。利用格兰杰因果关系，我们发现这些序列互为因果。得到的估算结果与图表进行了比较。<em class="lw"> RMSE </em>和<em class="lw"> R </em>值作为预测成功的标准进行检查。然而，通过更多的数据点和改变<em class="lw"> LSTM </em>网络的超参数，有可能获得更成功的结果。</p><p id="ea2a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">我可以到达</strong> <a class="ae lx" href="https://www.linkedin.com/in/saritmaitra/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> <em class="lw">这里</em> </strong> </a> <strong class="kt jd">。</strong></p></div></div>    
</body>
</html>