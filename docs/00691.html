<html>
<head>
<title>ML Algorithms: One SD (σ)- Instance-based Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML 算法:一种基于 SD (σ)实例的算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ml-algorithms-one-sd-%CF%83-instance-based-algorithms-4349224ed4f3?source=collection_archive---------6-----------------------#2019-02-01">https://towardsdatascience.com/ml-algorithms-one-sd-%CF%83-instance-based-algorithms-4349224ed4f3?source=collection_archive---------6-----------------------#2019-02-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b029" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">基于实例的机器学习算法简介</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/cb32e2c9ae6dc2f184d68526604bd0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*SLkDA89qCjjyGiAwgNqQug.png"/></div></figure><p id="2b93" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi lj translated">当面对各种各样的机器学习算法时，一个显而易见的问题是“哪种算法更适合特定的任务，我应该使用哪种算法？”</p><blockquote class="ls lt lu"><p id="a954" class="kn ko lv kp b kq kr jr ks kt ku ju kv lw kx ky kz lx lb lc ld ly lf lg lh li ij bi translated">回答这些问题取决于几个因素，包括:(1)数据的大小、质量和性质；(2)可用的计算时间；(3)任务的紧迫性；以及(4)你想用这些数据做什么。</p></blockquote><p id="7898" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是我在<a class="ae lz" rel="noopener" target="_blank" href="/ml-algorithms-one-sd-σ-74bcb28fafb6">以前的文章</a>中写的许多算法中的一部分。<br/>在这一部分中，我试图尽可能简单地展示和简要解释可用于基于实例的任务的主要算法(尽管不是全部)。</p><h1 id="8abe" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated"><strong class="ak">基于实例的算法:</strong></h1><blockquote class="ls lt lu"><p id="3a24" class="kn ko lv kp b kq kr jr ks kt ku ju kv lw kx ky kz lx lb lc ld ly lf lg lh li ij bi translated"><em class="iq">这些算法不执行显式归纳，而是将新的问题实例与训练中看到的实例进行比较，这些实例已经存储在内存中。</em></p></blockquote><p id="a99a" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">K-最近邻(KNN) </strong></p><p id="0c94" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">可用于分类和回归问题。KNN 存储所有可用病例，并通过其 K 个邻居的多数投票对新病例进行分类。通过在整个训练集中搜索 K 个最相似的实例(邻居)并总结这 K 个实例的输出变量，对新的数据点进行预测。例如，如果我们取 K=3，并且我们想要决定一个新的例子属于哪一类，我们考虑离新的例子最近的 3 个点(通常是欧几里德距离)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f9b3cc10eacf4a4bf77679357c2b7f0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*2JSOmhGwRaqn3UPvHDSjyA.png"/></div></figure><p id="922f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">对于回归问题，这可能是平均输出变量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e8e74999083f9984f7d9ae1218884d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*euBte6gscmor1qg-YKiDnA.png"/></div></figure><p id="da27" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lv">需要考虑的一些事情:</em></p><p id="87a2" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">选择 K 的最佳值最好通过首先检查数据来完成(您可以使用弯头方法)。</p><p id="9423" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是一种监督学习算法。</p><p id="d8b1" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">学习矢量量化(LVQ) </strong></p><p id="6f1e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">作为分类算法开发。它能够支持二元(两类)和多类分类问题。K 近邻的一个缺点是，您需要保留整个训练数据集。LVQ 是一种人工神经网络算法，允许您选择要保留多少训练实例，并准确学习这些实例应该是什么样子。实例数量的值在学习过程中被优化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2faa81106c42ba2039829918136bff80.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*RGB20eoG1JjM5o03-4qHcQ.png"/></div></figure><p id="713d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lv">需要考虑的一些事情:</em></p><p id="af00" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是一种监督学习方法</p><p id="2087" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果您发现 KNN 在数据集上给出了很好的结果，请尝试使用 LVQ 来减少存储整个训练数据集的内存需求。</p><p id="274f" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">自组织映射(SOM) </strong></p><p id="54df" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">一种无监督的深度学习模型，主要用于特征检测或降维。SOM 不同于其他人工神经网络，因为它应用竞争学习，而不是纠错学习(如梯度下降的反向传播)，并且在某种意义上，它们使用邻域函数来保持输入空间的拓扑属性。SOM 执行从高维空间到二维空间的拓扑有序映射。换句话说，它产生训练样本集的输入空间的二维表示。</p><p id="d1e4" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">例如，让我们看看手写数字数据集。SOM 的输入是高维的，因为每个输入维度表示 28×28 图像上一个像素的灰度值，这使得输入是 784 维的(每个维度是 0 到 255 之间的值)。</p><p id="4ed7" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果我们将它们映射到 20x20 SOM，并根据它们的真实类别(从 0 到 9 的数字)对它们进行着色，我们将得到以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/62f37c05ea41b99b14049c13d40995d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*vJZRVEzEhLliKjQxhAZpFw.png"/></div></figure><p id="8d96" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">真实类别根据左下方的颜色进行标注。</p><p id="725d" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">看一下黄色区域。这就是 6 被映射到的地方，请注意与其他类别有一点重叠。相比之下，看看左下方，绿色和棕色的点重叠的地方。这就是高级官员在 4 和 9 之间感到“困惑”的地方。</p><p id="4cd3" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">SOM 的另一个例子是 NLP。我们可以用它对 200 万份医学论文进行分类。SOM 将创建一组意思相似的单词:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/ea765e868dec37bf5bcdf2b0f94aa71b.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*PeNKyjLl4q_GR2heHACfmA.png"/></div></figure><p id="cea2" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">右下方的单词与大脑相关，右上方的单词与医学影像相关。</p><p id="7476" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lv">需要考虑的一些事情:</em></p><p id="b007" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">SOM 输出任意数量指标的 2D 图。</p><p id="a70e" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们可以使用 SOM 对数据进行聚类，而不需要知道输入数据的类成员。</p><p id="41a5" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><strong class="kp ir">局部加权学习(LWL) </strong></p><p id="0e00" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">LWL 背后的基本思想是，不是为整个函数空间建立全局模型，而是基于查询点的相邻数据为每个感兴趣的点创建局部模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f9a960d92846b910dc57a0efc1c98960.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*usFtUwzlUDieICshY2EYpg.png"/></div></figure><p id="6971" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">为此，每个数据点成为一个加权因子，它表示该数据点对预测的影响。通常，与当前查询点邻近的数据点比远离的数据点接收更高的权重。基本上，假设你想预测未来会发生什么。你可以简单地进入你以前所有经历的数据库，然后抓住一些相似的经历，将它们组合起来(也许通过加权平均，更强地加权更多相似的经历)，并使用组合来做出预测。</p><p id="e200" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lv">需要考虑的一些事情:</em></p><p id="fcf8" class="pw-post-body-paragraph kn ko iq kp b kq kr jr ks kt ku ju kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">LWL 方法是非参数的。</p></div><div class="ab cl my mz hu na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="ij ik il im in"><blockquote class="ls lt lu"><p id="6eba" class="kn ko lv kp b kq kr jr ks kt ku ju kv lw kx ky kz lx lb lc ld ly lf lg lh li ij bi translated">如果你对我的更多作品感兴趣，你可以看看我的<a class="ae lz" href="https://github.com/shaier" rel="noopener ugc nofollow" target="_blank"> Github </a>，我的<a class="ae lz" href="https://scholar.google.com/citations?user=paO-O00AAAAJ&amp;hl=en&amp;oi=sra" rel="noopener ugc nofollow" target="_blank">学者页面</a>，或者我的<a class="ae lz" href="https://shaier.github.io/" rel="noopener ugc nofollow" target="_blank">网站</a></p></blockquote></div></div>    
</body>
</html>