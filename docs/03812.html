<html>
<head>
<title>An “Equation-to-Code” Machine Learning Project Walk-Through — Part 3 SGD</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“等式到代码”机器学习项目演练—第 3 部分 SGD</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-part-3-sgd-e4167225504b?source=collection_archive---------15-----------------------#2019-06-16">https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-part-3-sgd-e4167225504b?source=collection_archive---------15-----------------------#2019-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9372" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 Python 实现随机梯度下降(SGD)和小批量梯度下降的详细说明</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5410c1f5b4bdee5b89186442a9cf3f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jl9Q41DiXnUQwONB.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">from Shutterstock</figcaption></figure><p id="3b08" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">大家好！这是“等式到代码”演练的第 3 部分。</p><p id="4e9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在前面的文章中，我们在<strong class="la iu">中谈到了线性可分问题</strong>在<a class="ae lu" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7">第一部分</a>，在<a class="ae lu" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac">第二部分</a>中谈到了<strong class="la iu">非线性可分问题</strong>。这次我们将根据等式实现<strong class="la iu">随机梯度下降(SGD) </strong>。</p><p id="5dc4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第 3 部分是独立的。但对于<a class="ae lu" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac?source=your_stories_page---------------------------"> part 2 </a>中重复的内容我就不做过多解释了。如果你觉得有些东西很难理解，我推荐你先阅读<a class="ae lu" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac"> part 2 </a>。</p><p id="6b03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是<a class="ae lu" href="https://gist.github.com/BrambleXu/52b0aaf10987015a078d36c97729dace" rel="noopener ugc nofollow" target="_blank">数据</a>和<a class="ae lu" href="https://gist.github.com/BrambleXu/0e00bbd2f11ad7b3fa264c4ea27ea03b" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><p id="ae6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">内容结构如下。<code class="fe lv lw lx ly b">*</code>表示如果您已经完成第 2 部分，可以跳过这一步。</p><ol class=""><li id="5549" class="lz ma it la b lb lc le lf lh mb ll mc lp md lt me mf mg mh bi translated">预览*</li><li id="7803" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">随机梯度下降</li><li id="000d" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">小批量梯度下降</li><li id="d52a" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt me mf mg mh bi translated">摘要</li></ol><h1 id="6dd1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">1 预览</h1><blockquote class="nf ng nh"><p id="8842" class="ky kz ni la b lb lc ju ld le lf jx lg nj li lj lk nk lm ln lo nl lq lr ls lt im bi translated">如果您已经阅读了<a class="ae lu" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac">第 2 部分</a>，您可以跳过这一步</p></blockquote><p id="03eb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们看看我们在第 2 部分做了什么。</p><p id="e791" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里是数据，<a class="ae lu" href="https://gist.github.com/BrambleXu/a64df128d6c0c26143f82f7b6e889983" rel="noopener ugc nofollow" target="_blank"> non_linear_data.csv </a></p><pre class="kj kk kl km gt nm ly nn no aw np bi"><span id="5f7b" class="nq mo it ly b gy nr ns l nt nu">x1,x2,y<br/>0.54508775,2.34541183,0<br/>0.32769134,13.43066561,0<br/>4.42748117,14.74150395,0<br/>2.98189041,-1.81818172,1<br/>4.02286274,8.90695686,1<br/>2.26722613,-6.61287392,1<br/>-2.66447221,5.05453871,1<br/>-1.03482441,-1.95643469,1<br/>4.06331548,1.70892541,1<br/>2.89053966,6.07174283,0<br/>2.26929206,10.59789814,0<br/>4.68096051,13.01153161,1<br/>1.27884366,-9.83826738,1<br/>-0.1485496,12.99605136,0<br/>-0.65113893,10.59417745,0<br/>3.69145079,3.25209182,1<br/>-0.63429623,11.6135625,0<br/>0.17589959,5.84139826,0<br/>0.98204409,-9.41271559,1<br/>-0.11094911,6.27900499,0</span></pre><p id="df2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/7ade453276139aebde2a779b35800e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*pGNzrEkJtDTPhCkHDiZecQ.png"/></div></figure><p id="df58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在对数据作图后，我们发现一条直线无法将 X 和 o 分开，这类问题被称为<strong class="la iu">非线性可分问题</strong>，数据不是线性可分的。</p><p id="6799" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们引入多项式 logistic 回归，在线性函数中增加一个多项式项。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/1fe7abc3aac2d5f70ec337406cae4fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*j2qhn6jx1xoLRa0DaHK-dg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">polynomial function</figcaption></figure><p id="3881" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们用θ来表示参数。左边的θ标记表示函数 f(x)有参数θ。右边的θ表示有两个参数。最后一项是多项式项，它使模型推广到非线性可分数据。</p><p id="5607" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，我们在<a class="ae lu" href="https://gist.github.com/BrambleXu/a64df128d6c0c26143f82f7b6e889983" rel="noopener ugc nofollow" target="_blank"> non_linear_data.csv </a>中有 x1 和 x2 两个特征。我们选择 x1 作为多项式项。所以功能应该变成低于形式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3f19fd852c6c8878549a3d6a524e359b.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*SEYB_TDpXr5UFEc8ubfyVQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">a specific form fit to our data</figcaption></figure><p id="a771" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们引入标准化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5c3f709c7b442c54fde9bf1fc63508c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*kFMKX970oeEgG6cM7pRkLg.png"/></div></figure><ul class=""><li id="596e" class="lz ma it la b lb lc le lf lh mb ll mc lp md lt nz mf mg mh bi translated">𝜇在每一栏都很刻薄</li><li id="d12e" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt nz mf mg mh bi translated">𝜎是每列的标准偏差</li></ul><p id="fe9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于预测模型，我们使用 sigmoid 函数。下面是矢量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/6b8e85b53ae5c43c8ad4dff2f1299236.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*zjFf9ZzshxTJlC7edROoAw.png"/></div></figure><p id="f380" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们用 z 来表示线性函数，并将其传递给 sigmoid 函数。sigmoid 函数将给出每个数据样本的概率。我们数据中有两个类，一个是<code class="fe lv lw lx ly b">1</code>，另一个是<code class="fe lv lw lx ly b">0</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/598ca2a33e63ef98b71a31237199f730.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*4h5rdzqS4bctbo7ixZpHfA.png"/></div></figure><p id="1358" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，我们准备了数据、模型(sigmoid ),还需要什么？是的，一个目标函数。<strong class="la iu">目标函数可以指导我们如何以正确的方式更新参数。</strong>对于 sigmoid(逻辑回归)，我们通常使用<a class="ae lu" href="https://www.wikiwand.com/en/Likelihood_function#/Log-likelihood" rel="noopener ugc nofollow" target="_blank">对数似然</a>作为目标函数。更具体地说，我们需要计算对数似然函数的<strong class="la iu">导数</strong>。这里我直接给出最后的更新方程式。(如果你对如何得到这个方程感兴趣，这个<a class="ae lu" href="https://www.youtube.com/watch?v=SB2vz57eKgc" rel="noopener ugc nofollow" target="_blank">视频</a>应该会有帮助)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/92da534e72a3ead43346ba4017d5b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4SKBVfcX3OQ2uLqgnTgzTw.png"/></div></figure><p id="cf39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">θj 是第 j 个参数。</p><ul class=""><li id="85a4" class="lz ma it la b lb lc le lf lh mb ll mc lp md lt nz mf mg mh bi translated">η是学习率，我们设为 0.001 (1e-3)。</li><li id="9822" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt nz mf mg mh bi translated">n 是数据样本的数量，在我们的例子中，我们有 20 个。</li><li id="9b50" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt nz mf mg mh bi translated">I 是第 I 个数据样本</li></ul><p id="ac73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">类似 Numpy 数组的版本可能容易理解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/8548f80491498880aa47207a696be49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXq9DyKsVVusNtH_cd3TvQ.png"/></div></div></figure><p id="a4f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们绘制模型线和精度线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/6c1f0882db8ada4bf159cb93c511fce8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*79l7lRTc3HE59otGsds9sA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">model line</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/04f6e80589c0dad58f12db5b1e18d592.png" data-original-src="https://miro.medium.com/v2/format:webp/1*hOeQ3N-MoSvMYwWJ24VPyw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">accuracy line</figcaption></figure><p id="b989" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是我们在第 2 部分之后留下的全部代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><p id="203b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你觉得有些东西难以理解，你可以阅读<a class="ae lu" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac">第 2 部分</a>获得详细解释。</p><h1 id="425b" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">2 随机梯度下降法</h1><p id="31a5" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">我们使用 SGD 的主要原因是为了避免局部最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/81a8d373399c098c58d534d481c206be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*auf4ZsGId0UwIkN5lirkHw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">the parameter is trapped in a local minimum</figcaption></figure><p id="6a04" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本思想是通过在每次更新中随机选择一个数据来更新参数。所以参数更容易走出局部极小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/92da534e72a3ead43346ba4017d5b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4SKBVfcX3OQ2uLqgnTgzTw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">gradient descent</figcaption></figure><p id="c413" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是梯度下降形式。我们可以看到，为了更新θj，我们使用了整个训练数据(σ部分)。代码如下。</p><pre class="kj kk kl km gt nm ly nn no aw np bi"><span id="b20b" class="nq mo it ly b gy nr ns l nt nu"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="2bf5" class="nq mo it ly b gy on ns l nt nu"># update parameter<br/><strong class="ly iu">for _ in range(epoch):<br/>    theta = theta - ETA * np.dot(f(mat_x) - train_y, mat_x)</strong></span></pre><p id="094e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是在 SGD 中，我们一次只用一个数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ecb25001c56be3994cffd3454bb8e5bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*cQW0jNGwSufZ21yi0QyZMQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">stochastic gradient descent</figcaption></figure><p id="f846" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里的<code class="fe lv lw lx ly b">k</code>是指我们随机选取的数据。</p><pre class="kj kk kl km gt nm ly nn no aw np bi"><span id="52cc" class="nq mo it ly b gy nr ns l nt nu"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="4fa3" class="nq mo it ly b gy on ns l nt nu"># update parameter<br/>for _ in range(epoch):<strong class="ly iu"><br/></strong>    # sgd<br/>    <strong class="ly iu">p = np.random.permutation(len(mat_x))<br/>    for x, y in zip(mat_x[p, :], train_y[p]):<br/>        theta = theta - ETA * (f(x) - y) * x</strong></span></pre><ul class=""><li id="f15d" class="lz ma it la b lb lc le lf lh mb ll mc lp md lt nz mf mg mh bi translated">p 包含整个数据集的随机索引列表，例如，[ 5，12，17，14，8，9，10，2，13，18，15，16，1，0，6，11，7，4，3，19]</li><li id="1b38" class="lz ma it la b lb mi le mj lh mk ll ml lp mm lt nz mf mg mh bi translated">for 循环每次取一个数据来更新参数θ</li></ul><p id="f2b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以这样想 SGD。在每个历元中，梯度下降和 SGD 都使用整个数据集来更新参数。用<strong class="la iu">完整有序数据</strong>梯度下降更新参数。但是 SGD 用<strong class="la iu">一个随机选择的数据</strong>来更新参数，这样更容易<strong class="la iu">走出局部最小值</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3e19217eca4d30b6c488d90a6c4d2dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*w-XFJi6wEop2dBFJN75iaw.png"/></div></figure><p id="71b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">精确线。我们可以看到收敛比梯度下降快。</p><h1 id="322a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">3 小批量梯度下降</h1><p id="f93b" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">SGD 是好的，但是由于每次用一个数据更新参数，计算效率不高。</p><p id="5694" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">使用整体数据会造成局部极小问题(梯度下降)，每次使用一个数据效率低。这就是为什么我们使用小批量梯度下降。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/86d042522dc102ba89dff5cd2ed17cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*C3_OdGr4XwJ3Utoh2TEM4Q.png"/></div></figure><p id="8c84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与 SGD 不同，我们可以用几个数据样本更新参数。这里的<code class="fe lv lw lx ly b">K</code>是包含<code class="fe lv lw lx ly b">m</code>个随机选择的数据样本的索引集。</p><p id="657b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们有 20 个数据样本，我们将批量大小<code class="fe lv lw lx ly b">m</code>设为 5。</p><pre class="kj kk kl km gt nm ly nn no aw np bi"><span id="3170" class="nq mo it ly b gy nr ns l nt nu"><strong class="ly iu">import math</strong></span><span id="f600" class="nq mo it ly b gy on ns l nt nu"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="3afd" class="nq mo it ly b gy on ns l nt nu"><strong class="ly iu"># batch size<br/>batch = 5</strong></span><span id="c396" class="nq mo it ly b gy on ns l nt nu"><strong class="ly iu"># calculate steps based on batch size<br/>steps = int(math.ceil(len(train_x)/batch))</strong></span><span id="5f9a" class="nq mo it ly b gy on ns l nt nu"># update parameter<br/>for _ in range(epoch):<br/>    <strong class="ly iu">p = np.random.permutation(len(mat_x)) <br/>    shuffle_x = mat_x[p]<br/>    shuffle_y = train_y[p]</strong></span><span id="a302" class="nq mo it ly b gy on ns l nt nu"><strong class="ly iu">for step in range(steps):<br/>        x = shuffle_x[step:step + batch, :]<br/>        y = shuffle_y[step:step + batch]<br/>        theta = theta - ETA * np.dot(f(x) - y, x)</strong></span></pre><p id="6066" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们必须在每个时期混洗数据。该计算与通过矩阵乘法的梯度下降相同。如果你感兴趣，你可以在第一部分或第二部分找到详细的解释。</p><p id="f444" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">精确度线</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/544f5d71af1b45b4ce2927fc4e929994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*8OGlyGZlffSM6PiImMEoug.png"/></div></figure><p id="c527" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">收敛速度与梯度下降相同。但是计算效率更高。</p><h1 id="dfb6" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">4 摘要</h1><p id="8a28" class="pw-post-body-paragraph ky kz it la b lb oh ju ld le oi jx lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在第 3 部分中，我们讨论了如何实现 SGD 和小批量梯度下降。你可以在下面找到完整的代码。留下评论让我知道我的文章是否易懂。请继续关注这个关于正则化的“公式到代码”系列的最后一篇文章。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of og l"/></div></figure><blockquote class="nf ng nh"><p id="337c" class="ky kz ni la b lb lc ju ld le lf jx lg nj li lj lk nk lm ln lo nl lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae lu" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="la iu"> <em class="it">中等</em> </strong> </a> <strong class="la iu"> <em class="it">同</em> </strong> <a class="ae lu" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="it">一个分类查看</em> </strong> </a> <strong class="la iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae lu" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"><em class="it">bramble Xu</em></strong></a><strong class="la iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae lu" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"><em class="it">徐亮</em> </strong> </a> <strong class="la iu"> <em class="it"> <br/>博客:</em></strong><a class="ae lu" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="la iu"><em class="it">bramble Xu</em></strong></a></p></blockquote></div></div>    
</body>
</html>