<html>
<head>
<title>OpenPose, PNASNet 5 for Pose Classification Competition (Fastai)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenPose，PNASNet 5 进行姿势分类比赛(Fastai)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/openpose-pnasnet-5-for-pose-classification-competition-fastai-dc35709158d0?source=collection_archive---------9-----------------------#2019-08-25">https://towardsdatascience.com/openpose-pnasnet-5-for-pose-classification-competition-fastai-dc35709158d0?source=collection_archive---------9-----------------------#2019-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8f7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我最近参加了一个当地的人工智能比赛，挑战包括 15 个不同类别的人体姿势分类。这是我第一次参加人工智能比赛，这是一次令人羞愧的经历。基于我们的模型准确性、创造力和团队合作，我的团队获得了亚军。我肯定会推荐学习者参加竞赛或黑客马拉松，因为它们是一个很好的网络平台和训练场地，可以磨练你的技术技能。在这里，我将向您介绍我们的模型构建过程，该过程最终帮助我们在最终测试集上实现了 83%的高准确率。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><p id="21f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们面临的挑战是开发一种图像分类算法，可以区分 15 种不同的人类姿势。</p><p id="b44a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">姿势是:</p><p id="1eb4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">椅子式、儿童式、轻拍式、手枪式、握手式、HulkSmash 式、KoreanHeart 式、KungfuCrane 式、KungfuSalute 式、敬礼式、战士式、鹰式、胸部撞击式、高跪式和蜘蛛侠式</p><p id="2067" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据竞赛条款和条件，我们无权发布数据集。因此，以下所有图片都是我们在比赛期间拍摄的。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/60efc293d66467647b9bd660488840cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TM8vxST4IR1E94yPCSmrfg.jpeg"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Left to Right— KoreanHeart, KungfuCrane, HandShake</figcaption></figure><p id="66a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练数据集中的每个类包含大约 100 幅图像，而验证集中的每个类有 25 幅图像。对于 15 个类别的分类任务来说，这是相对少量的数据，说明了我们需要通过拍摄自己的图像来增加数据集。</p><p id="0cec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ll translated">penPose 是一种人体姿态估计和特征提取步骤，用于检测图像中的人体。使用该模型来标识各个身体部位的关键点，并且可以绘制连接这些关键点的人体骨架。从每幅图像中提取人体姿态作为预处理步骤，以减少数据中的噪声。</p><p id="0a00" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过大量参考这个<a class="ae lu" href="https://www.learnopencv.com/deep-learning-based-human-pose-estimation-using-opencv-cpp-python/" rel="noopener ugc nofollow" target="_blank">站点</a>，我们使用预先训练好的 MPII 模型生成了每张图像的关键点。</p><pre class="kw kx ky kz gt lv lw lx ly aw lz bi"><span id="03b5" class="ma mb it lw b gy mc md l me mf">cwd = os.getcwd()</span><span id="9db6" class="ma mb it lw b gy mg md l me mf"># Specify the paths for the 2 files<br/> protoFile = "{}/pose_deploy_linevec_faster_4_stages.prototxt".format(cwd)<br/> weightsFile = "{}/pose_iter_160000.caffemodel".format(cwd)<br/> nPoints = 15<br/> POSE_PAIRS = [[0,1], [1,2], [2,3], [3,4], [1,5], [5,6], [6,7], [1,14], [14,8], [8,9], [9,10], [14,11], [11,12], [12,13] ]<br/>  <br/> # Read the network into Memory<br/> net = cv2.dnn.readNetFromCaffe(protoFile, weightsFile)</span><span id="4075" class="ma mb it lw b gy mg md l me mf">frameWidth = 640<br/> frameHeight = 480<br/> threshold = 0.1</span><span id="4e61" class="ma mb it lw b gy mg md l me mf"># Forward training set into OpenPose model to generate output</span><span id="51fd" class="ma mb it lw b gy mg md l me mf">inWidth = 299<br/> inHeight = 299</span><span id="d744" class="ma mb it lw b gy mg md l me mf">m = train_images.shape[0]<br/> train_outputs = np.zeros((1,44,38,38))</span><span id="c2e1" class="ma mb it lw b gy mg md l me mf">for i in range(m):<br/>   inpBlob = cv2.dnn.blobFromImage(train_images[i], 1.0/255,    (inWidth, inHeight),(0, 0, 0), swapRB=True, crop=False)</span><span id="b2d8" class="ma mb it lw b gy mg md l me mf">net.setInput(inpBlob)<br/>   output = net.forward()<br/>   train_outputs = np.vstack((train_outputs,output))<br/>   <br/>   <br/> outputs = np.delete(train_outputs,(0),axis=0)<br/> H = train_outputs.shape[2]<br/> W = train_outputs.shape[3]<br/> print(train_outputs.shape)</span><span id="d61e" class="ma mb it lw b gy mg md l me mf"># Generate keypoints for training set<br/> m = 973<br/> H = 38<br/> W = 38</span><span id="c353" class="ma mb it lw b gy mg md l me mf">train_points = np.zeros((m,15,2))</span><span id="0a23" class="ma mb it lw b gy mg md l me mf">for sample in range(m):<br/>   for i in range(nPoints):<br/>       # confidence map of corresponding body's part.<br/>       probMap = train_outputs[sample, i, :, :]</span><span id="2049" class="ma mb it lw b gy mg md l me mf"># Find global maxima of the probMap.<br/>       minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)</span><span id="695c" class="ma mb it lw b gy mg md l me mf"># Scale the point to fit on the original image<br/>       x = (frameWidth * point[0]) / W<br/>       y = (frameHeight * point[1]) / H<br/>       if prob &gt; threshold :<br/>           train_points[sample,i,0] = int(x)<br/>           train_points[sample,i,1] = int(y)</span></pre><p id="7b54" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，使用存储在变量 train_points 中的关键点，在每幅图像上绘制一个人体骨架。</p><pre class="kw kx ky kz gt lv lw lx ly aw lz bi"><span id="1a2c" class="ma mb it lw b gy mc md l me mf"># Processed images with sticks on original image<br/> train_processed = np.copy(train_images).astype(np.uint8)</span><span id="46d5" class="ma mb it lw b gy mg md l me mf">for sample in range(m):<br/>   for point in range(nPoints):<br/>     if train_points[sample,point,0] !=0 and train_points[sample,point,1] !=0 :<br/>       cv2.circle(train_processed[sample], (int(train_points[sample,point,0]), int(train_points[sample,point,1])), 10, (255,255,0), thickness=-1, lineType=cv2.FILLED)<br/>     <br/>     # draw lines <br/>     for pair in POSE_PAIRS:<br/>       partA = pair[0]<br/>       partB = pair[1]</span><span id="2cf5" class="ma mb it lw b gy mg md l me mf">if train_points[sample,partA,0] != 0 and train_points[sample,partA,1] != 0 and train_points[sample,partB,0] != 0 and train_points[sample,partB,1] != 0:<br/>          cv2.line(train_processed[sample], (int(train_points[sample,partA,0]),int(train_points[sample,partA,1]))<br/>                   , (int(train_points[sample,partB,0]),int(train_points[sample,partB,1])), (255,255,0), 3)</span><span id="7035" class="ma mb it lw b gy mg md l me mf"># Processed images with sticks on a black background</span><span id="98e1" class="ma mb it lw b gy mg md l me mf">train_processed_grey = np.zeros((m,train_images.shape[1],train_images.shape[2],1)).astype(np.uint8)</span><span id="cda4" class="ma mb it lw b gy mg md l me mf">for sample in range(m):<br/>   for point in range(nPoints):<br/>     if train_points[sample,point,0] !=0 and train_points[sample,point,1] !=0 :<br/>       cv2.circle(train_processed_grey[sample], (int(train_points[sample,point,0]), int(train_points[sample,point,1])), 10, (1), thickness=50, lineType=cv2.FILLED)<br/>     <br/>     # draw lines <br/>     for pair in POSE_PAIRS:<br/>       partA = pair[0]<br/>       partB = pair[1]</span><span id="28b2" class="ma mb it lw b gy mg md l me mf">if train_points[sample,partA,0] != 0 and train_points[sample,partA,1] != 0 and train_points[sample,partB,0] != 0 and train_points[sample,partB,1] != 0:<br/>          cv2.line(train_processed_grey[sample], (int(train_points[sample,partA,0]),int(train_points[sample,partA,1]))<br/>                   , (int(train_points[sample,partB,0]),int(train_points[sample,partB,1])), (1), 3)</span></pre><p id="2d0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过这一步，我们获得了 3 个不同的数据集；</p><ol class=""><li id="4332" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">原象</li><li id="30e7" class="mh mi it js b jt mq jx mr kb ms kf mt kj mu kn mm mn mo mp bi translated">原始图像+骨架叠加</li><li id="51e0" class="mh mi it js b jt mq jx mr kb ms kf mt kj mu kn mm mn mo mp bi translated">背景空白的骨架</li></ol><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi mv"><img src="../Images/2fb789c5b7297f298eca23839663d7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWJf7xtVQ-kc5Kz-Vm7beA.png"/></div></div></figure></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><p id="8914" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ll translated"><span class="l lm ln lo bm lp lq lr ls lt di"> T </span>转移学习允许我们使用比从头开始训练算法所需的数据少得多的数据来训练深度神经网络。它还经常导致更高精度的模型，因为来自大数据集的信息被转移和训练以适合我们的数据。对于这次比赛，我的团队决定测试 3 个预训练模型的性能，即 ResNet-50、ResNeXt-101 和 PNASnet-5。所有模型都是使用 fastai 库构建的。</p><pre class="kw kx ky kz gt lv lw lx ly aw lz bi"><span id="e31b" class="ma mb it lw b gy mc md l me mf">import fastai<br/> from fastai.metrics import error_ratefrom torchvision.models import *<br/> import pretrainedmodels<br/> from fastai.callbacks.tracker import SaveModelCallback<br/> from fastai.vision import *<br/> from fastai.vision.models import *<br/> from fastai.vision.learner import model_meta</span><span id="6da1" class="ma mb it lw b gy mg md l me mf">bs = 8</span><span id="03eb" class="ma mb it lw b gy mg md l me mf"># Importing of dataset</span><span id="795e" class="ma mb it lw b gy mg md l me mf">data = ImageDataBunch.from_folder(base_dir,train='train',valid='val', ds_tfms = get_transforms(), size =299, bs=bs).normalize(imagenet_stats)</span><span id="9864" class="ma mb it lw b gy mg md l me mf">#ResNet-50<br/> models.resnet50</span><span id="803b" class="ma mb it lw b gy mg md l me mf">#ResNeXt-101</span><span id="a40b" class="ma mb it lw b gy mg md l me mf">def resnext101_64x4d(pretrained=True):<br/>     pretrained = 'imagenet' if pretrained else None<br/>     model = pretrainedmodels.resnext101_64x4d(pretrained=pretrained)<br/>     all_layers = list(model.children())<br/>     return nn.Sequential(*all_layers[0], *all_layers[1:])</span><span id="7523" class="ma mb it lw b gy mg md l me mf">#PASNet-5</span><span id="4edf" class="ma mb it lw b gy mg md l me mf">def identity(x): return x</span><span id="f870" class="ma mb it lw b gy mg md l me mf">def pnasnet5large(pretrained=True):    <br/>     pretrained = 'imagenet' if pretrained else None<br/>     model = pretrainedmodels.pnasnet5large(pretrained=pretrained, num_classes=1000) <br/>     model.logits = identity<br/>     return nn.Sequential(model)</span><span id="03c1" class="ma mb it lw b gy mg md l me mf"># Training of model</span><span id="0e2c" class="ma mb it lw b gy mg md l me mf">learn = cnn_learner(data, model, metrics = accuracy)</span><span id="994b" class="ma mb it lw b gy mg md l me mf">learn.fit(20,callbacks=[SaveModelCallback(learn,monitor='accuracy',every="improvement",name ="top_acc")])</span><span id="28ac" class="ma mb it lw b gy mg md l me mf">learn.lr_find()<br/> learn.recorder.plot()</span><span id="8a9b" class="ma mb it lw b gy mg md l me mf">learn.unfreeze()<br/> learn.fit_one_cycle(5,callbacks=[SaveModelCallback(learn,monitor='accuracy',every="improvement",name="top_acc_1")])</span></pre><p id="54af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们总共构建了 9 个模型，每个模型都用我们生成的数据集进行了预训练。以下是结果；</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/b438747024b0ccededa887026dd28ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*nZdkBtYUpICCI6jfeQe8lA.png"/></div></figure><p id="7576" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于最终的测试集，我们使用在原始模型上训练的 PNASnet-5 模型预测了未知数据集的分类，并获得了 83%的准确率，使我们在比赛中获得了第三名。</p></div><div class="ab cl ko kp hx kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="im in io ip iq"><p id="a8ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是使用 PNASnet-5 预训练模型的人体姿势分类。令人失望的是，OpenPose 特征提取没有提高模型的准确性，但我相信在比赛的时间限制下，我们做得很好。对于那些想知道 PNASnet-5 如何比其他预训练模型表现得更好的人，下面是作者对算法的总结。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/fce057b0b3ab3f3cfc12916a30cb22df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*BjWd3AKHSJabghkJplRdww.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk"><a class="ae lu" href="https://cs.jhu.edu/~cxliu/posters/pnas_poster.pdf" rel="noopener ugc nofollow" target="_blank">https://cs.jhu.edu/~cxliu/posters/pnas_poster.pdf</a></figcaption></figure></div></div>    
</body>
</html>