<html>
<head>
<title>Label Smoothing: An ingredient of higher model accuracy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">标签平滑:更高模型精度的一个要素</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0?source=collection_archive---------6-----------------------#2019-06-24">https://towardsdatascience.com/label-smoothing-making-model-robust-to-incorrect-labels-2fae037ffbd0?source=collection_archive---------6-----------------------#2019-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><h1 id="5ba5" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">1.介绍</h1><p id="2e22" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="kq iu">图像分类</strong>是从一组固定的类别中给输入图像分配一个标签的任务。这是计算机视觉中的核心问题之一，尽管它很简单，却有大量的实际应用。</p><p id="b2bc" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated"><strong class="kq iu">示例:</strong>例如，在下图中，一个图像分类模型获取一张图像，并将概率分配给两个标签:<em class="lr">{猫，狗} </em>。如图所示，请记住，对于计算机来说，图像是一个大型的三维数字数组。在这个例子中，猫图像是 248 像素宽，400 像素高，并且具有三个颜色通道红色、绿色、蓝色(或简称为 RGB)。因此，图像由 248 x 400 x 3 个数字组成，即总共 297，600 个数字。每个数字都是从 0(黑色)到 255(白色)的整数。我们的任务是将这 25 万个数字转换成一个标签，比如<em class="lr">“cat”</em>。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="ab gu cl lx"><img src="../Images/6a5a7eba5be81f34d60012503660020a.png" data-original-src="https://miro.medium.com/v2/0*_zoc_5LvmIN4wb-6"/></div></figure><p id="d54e" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">同样地，你可以训练一个模型，让它通过看卡戴珊的照片告诉你你在看哪个——金，凯莉，还是其他</p><h1 id="1085" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">2.训练分类模型</h1><p id="8aea" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">训练将图像分类为猫图像或狗图像的模型是二元分类的一个例子。</p><p id="96eb" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated"><strong class="kq iu">图像分类流水线</strong>。我们已经看到，图像分类的任务是获取代表单个图像的像素阵列，并为其分配标签。我们的完整渠道可以概括如下:</p><ul class=""><li id="6fb7" class="ma mb it kq b kr lm kv ln kz mc ld md lh me ll mf mg mh mi bi translated"><strong class="kq iu">输入:</strong>我们的输入由一组<em class="lr"> N </em>图像组成，每个图像都标有一个<em class="lr"> K </em>不同的类。我们将该数据称为<em class="lr">训练集</em>。</li></ul><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="mk ml di mm bf mn"><div class="gh gi mj"><img src="../Images/808a482b1e2f2694cff22910200b8859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKygw991Az4ItAIrstZLfw.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">training dataset</figcaption></figure><p id="e37e" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated"><strong class="kq iu">学习:</strong>我们的任务是使用训练集来学习每个类的样子。我们称这个步骤为<em class="lr">训练分类器</em>或<em class="lr">学习模型</em>。</p><h1 id="b78e" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> 3。数据标注错误问题</strong></h1><p id="649e" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">但是如果你的训练数据包含不正确的标注呢？如果一只狗被贴上猫的标签会怎样？如果凯莉被贴上肯德尔的标签，或者金被贴上坎耶的标签怎么办？如果你从互联网上获取数据，这种错误的数据标注可能会发生。T29】</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="ab gu cl lx"><img src="../Images/4408c4e69093484fc3ec518b8d64ca33.png" data-original-src="https://miro.medium.com/v2/format:webp/1*BQosKd3FHZWsfKRF6pdVtQ.png"/></div></figure><p id="8052" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这最终会导致您的问题要么学习数据集中的噪声，要么学习不正确的要素。但是，这在一定程度上是可以避免的。如果你在一个小数据集上训练，你可以检查所有的标签并手动检查，或者使用你的爪牙来做脏活。下一节将分享另一种数学方法。</p><p id="282c" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">所以，问题是你的模型将会学习不正确的特征(从一只狗身上),并将这些特征与标签“猫”联系起来。我们如何解决这个问题？为了深入了解这一点，让我们看看在图像分类问题中使用的损失函数。</p><p id="fc98" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">在我们得到损失函数之前，我们应该确定分段模型给出了每类的概率<strong class="kq iu">:</strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/813565c41d8347a6e04ede5d415e2b2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*gSIxMzQl4XMC8LDz3zbnXQ.jpeg"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">…maybe this is not how <em class="mt">you</em> see her…</figcaption></figure><p id="4532" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">在图像分类问题中，我们使用<a class="ae mu" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html" rel="noopener ugc nofollow" target="_blank"> softmax loss </a>，其定义如下两类:</p><blockquote class="mv mw mx"><p id="66c0" class="ko kp lr kq b kr lm kt ku kv ln kx ky my lo lb lc mz lp lf lg na lq lj lk ll im bi translated">l =(<em class="it">y</em>log(<em class="it">p</em>)+(1<em class="it">y</em>)log(1<em class="it">p</em>))</p></blockquote><p id="168a" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这里，<em class="lr"> L </em>为损失，<em class="lr"> y </em>为真实标签(0 —猫，1 —狗)，<em class="lr"> p </em>为图像属于 1 类即狗的概率。模型的目标是减少损失。</p><p id="5235" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">损失本质上驱动了你的“梯度”，简单来说就是决定了模型的“学习”。因此，我们需要密切关注损失。</p><p id="53ea" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">比方说，你得到一张狗的图片，概率为 0.99。你的损失将是:</p><p id="b17b" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">l =-(1 *(log(0.99)+(1–0.99)* log(0.01))≈0</p><p id="a2fe" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">哪个好！预测准确的情况下损失应该不大！</p><p id="b05e" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated"><em class="lr">如果你的数据有不正确的标签，这种损失会特别大，从而导致</em> <strong class="kq iu"> <em class="lr">在学习</em> </strong> <em class="lr">时出现问题。</em></p><p id="2639" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">那么我们该如何处理呢？在下一节中，我们将看到一种在标签不正确的情况下将损失最小化的方法。</p><h1 id="effa" class="jq jr it bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated"><strong class="ak"> 3。标签平滑</strong> —一种可能的解决方案</h1><p id="f961" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">向标签平滑问好！</p><p id="618c" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">当我们将交叉熵损失应用于分类任务时，我们期望真正的标签为 1，而其他标签为 0。换句话说，我们毫不怀疑真正的标签是真的，其他的不是。总是这样吗？也许不是。许多手动注释是多个参与者的结果。他们可能有不同的标准。他们可能会犯一些错误。他们毕竟是人。因此，我们一直坚信的地面真相标签可能是错误的。</p><p id="cb25" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">一个可能的解决办法是放松我们对标签的信心。例如，我们可以将损失目标值从 1 稍微降低到 0.9。自然地，我们稍微增加了其他人的目标值 0。这种想法被称为标签平滑。</p><p id="9bc8" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">下面是张量流中定义的交叉熵损失的论点:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/981128040666042c66e8f1128f479c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*9uT6N3_QuEMm7Z7UoY11JA.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">arguments in softmax cross entropy loss</figcaption></figure><p id="0545" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这是 Tensorflow <a class="ae mu" href="https://www.tensorflow.org/api_docs/python/tf/losses/softmax_cross_entropy" rel="noopener ugc nofollow" target="_blank">文档</a>对<code class="fe nc nd ne nf b">label_smoothing</code>参数的描述:</p><blockquote class="mv mw mx"><p id="a89b" class="ko kp lr kq b kr lm kt ku kv ln kx ky my lo lb lc mz lp lf lg na lq lj lk ll im bi translated">如果<code class="fe nc nd ne nf b">label_smoothing</code>不为零，则将标签向 1/num_classes 方向平滑:new _ onehot _ labels = onehot _ labels *(1-label _ smoothing)+label _ smoothing/num _ classes</p></blockquote><p id="66eb" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这是什么意思？</p><p id="76c8" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">假设你正在训练一个二元分类模型。你的标签应该是 0-猫，1-不是猫。</p><p id="bb01" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">现在，假设你<code class="fe nc nd ne nf b">label_smoothing</code> = 0.2</p><p id="da6f" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">使用上面的等式，我们得到:</p><p id="3f3d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">new _ onehot _ labels =[0 1]*(1—0.2)+0.2/2 =[0 1]*(0.8)+0.1</p><p id="4c7d" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">new_onehot_labels =[0.9 0.1]</p><p id="7e32" class="pw-post-body-paragraph ko kp it kq b kr lm kt ku kv ln kx ky kz lo lb lc ld lp lf lg lh lq lj lk ll im bi translated">这些是软标签，而不是硬标签，即 0 和 1。当有一个不正确的预测时，这将最终给你较低的损失，随后，你的模型将惩罚和学习不正确的程度稍低。</p><blockquote class="ng"><p id="1310" class="nh ni it bd nj nk nl nm nn no np ll dk translated">本质上，标签平滑将帮助您的模型围绕错误标签的数据训练<em class="mt">，从而提高其健壮性和性能。</em></p></blockquote><h1 id="024e" class="jq jr it bd js jt ju jv jw jx jy jz ka kb nq kd ke kf nr kh ki kj ns kl km kn bi translated">4.进一步阅读</h1><p id="184f" class="pw-post-body-paragraph ko kp it kq b kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><a class="ae mu" href="https://arxiv.org/pdf/1906.02629.pdf" rel="noopener ugc nofollow" target="_blank">标签平滑什么时候有帮助？</a></p></div></div>    
</body>
</html>