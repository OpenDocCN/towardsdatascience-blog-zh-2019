<html>
<head>
<title>Detecting pedestrians and bikers on a drone with Jetson Xavier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Jetson Xavier 在无人机上探测行人和骑车人</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-pedestrians-and-bikers-on-a-drone-with-jetson-xavier-93ce92e2c597?source=collection_archive---------18-----------------------#2019-11-13">https://towardsdatascience.com/detecting-pedestrians-and-bikers-on-a-drone-with-jetson-xavier-93ce92e2c597?source=collection_archive---------18-----------------------#2019-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="cadf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi ko translated">rones 是每个制造商和发烧友都想拥有的最酷的技术之一。与此同时，随着无人机变得越来越普遍，人工智能正在快速发展，我们现在正处于一个可以在无人机上进行物体检测和语义分割的状态。在这篇博文中，我将分享如何对无人机拍摄的图像进行物体检测。</p><p id="c129" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">手头问题的一些基础知识</strong></p><ul class=""><li id="cfd8" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn lc ld le lf bi translated">首先，重要的是要认识到，我们无法在通用嵌入式硬件(如 raspberry pi)上实际运行对象检测，因此需要为人工智能推理构建专用硬件。</li><li id="bb99" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">第二，如果你运行常用的对象检测模型，如在 COCO 和 Pascal VOC 数据集上训练的 YOLO 或 SSD，它们根本不会做得很好。这是因为从高处看一个物体和在地面上看是完全不同的。因此，推理数据的分布将与模型在训练期间遇到的分布非常不同，这将导致它失败。</li></ul><p id="f27a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">一些解决方案</strong></p><ul class=""><li id="f4ab" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn lc ld le lf bi translated">正如这篇文章的标题中提到的，我将使用目前自主机器人可用的最高端嵌入式处理器，来自 Nvidia 的 Jetson AGX Xavier。如果你想使用 Jetson TX2 或 nano，我会在文章末尾提供一些改进它们性能的建议。</li><li id="8253" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">为了解决不同数据分布的问题，斯坦福大学的研究人员发布了一个名为<a class="ae ll" href="http://cvgl.stanford.edu/projects/uav_data/" rel="noopener ugc nofollow" target="_blank">斯坦福无人机数据集</a>的数据集，其中包含几个从无人机上拍摄的视频，以及每个视频每一帧的标签。被检测的有六个等级:<code class="fe lm ln lo lp b">Biker</code>、<code class="fe lm ln lo lp b">Car</code>、<code class="fe lm ln lo lp b">Bus</code>、<code class="fe lm ln lo lp b">Cart</code>、<code class="fe lm ln lo lp b">Skater</code>和<code class="fe lm ln lo lp b">Pedestrian</code>。</li></ul><p id="c553" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">任何经过斯坦福数据集训练的像样的对象检测模型都应该能够很好地检测这六个对象。在这篇文章中，我们将使用<a class="ae ll" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank"> RetinaNet </a>，这是一个由脸书(FAIR)发布的非常好的对象检测模型，它以这样一种方式塑造损失函数，即模型在训练期间学习专注于困难的例子，从而学习得更好。更多关于 RetinaNet 的细节可以在<a class="ae ll" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="9cf9" class="lq lr it bd ls lt lu dn lv lw lx dp ly kb lz ma mb kf mc md me kj mf mg mh mi bi translated"><strong class="ak">获取模型并转换为 fp16 </strong></h2><p id="d35b" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi ko translated"><span class="l kp kq kr bm ks kt ku kv kw di">答</span>虽然 FAIR 的 Detectron 2 <a class="ae ll" href="https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md" rel="noopener ugc nofollow" target="_blank"> model zoo </a>有几个模型可以下载和使用，但它们都是在 COCO 和 Pascal 数据集上训练的，正如我们所讨论的，这对我们的任务没有用。在斯坦福数据集上从头训练一个模型将是一项艰巨的任务。幸运的是，我发现 Nvidia 提供了一个在这个数据集上训练的模型，作为他们在 DeepStream SDK 上的一个网络研讨会的一部分。我并不想在这篇博文中支持 Nvidia，但是如果你想在没有训练自己的模型的情况下开始，最快的方法是注册<a class="ae ll" href="http://go.nvidianews.com/dc/Ty58ol2Ube3I8pgmu_WArvStj6XbWRGXWkAZY1LzhWTzLXNHPKS0BV7BGSc_UfLnrsN9qjvyMiH8wtAcnwkDDFhf65REN7qhIZDTPZHoMdmGxzwGGPr7l-zysQq6xmLAbcs84_0Q4Bf6aa9jge1FX0qOVqI71plWvsc13G6SusnN_kbGc0qq7rJXpy2d9tWjLXELID3wYEHRZoNdEA8P42FfKySP092XdQni3ZHH-b8=/s0bif6s0UFMC0O31wc00E0N" rel="noopener ugc nofollow" target="_blank">网上研讨会</a>并下载模型作为网上研讨会资源的一部分。你要找的文件是大约 150 MB 的<code class="fe lm ln lo lp b">stanford_resnext50.onnx</code>。</p><p id="70ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦你有了这个文件，你就可以使用 GitHub 的<code class="fe lm ln lo lp b">retinanet-examples</code>库的 C++ API 将<code class="fe lm ln lo lp b">onnx</code>文件转换成一个引擎<code class="fe lm ln lo lp b">plan</code>文件，这个文件是专门为你正在使用的 Jetson 设备编译的。以下是这些步骤的演练:</p><pre class="mo mp mq mr gt ms lp mt mu aw mv bi"><span id="006b" class="lq lr it lp b gy mw mx l my mz">git clone <a class="ae ll" href="https://github.com/NVIDIA/retinanet-examples.git" rel="noopener ugc nofollow" target="_blank">https://github.com/NVIDIA/retinanet-examples.git</a><br/>cd retinanet-examples/extras/cppapi<br/>mkdir build &amp;&amp; cd build<br/>cmake -DCMAKE_CUDA_FLAGS="--expt-extended-lambda -std=c++11" ..<br/>make<br/>cp /path/to/onnx/file . #copy onnx file to this directory</span><span id="102d" class="lq lr it lp b gy na mx l my mz">./export <!-- -->stanford_resnext50.onnx engine.plan</span></pre><p id="2395" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">成功创建<code class="fe lm ln lo lp b">engine.plan</code>后，您可以使用<code class="fe lm ln lo lp b">infer</code>实用程序在图像上测试模型的性能:</p><pre class="mo mp mq mr gt ms lp mt mu aw mv bi"><span id="f5ae" class="lq lr it lp b gy mw mx l my mz">./infer engine.plan image.jpg</span></pre><p id="8e65" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将编写一个名为<code class="fe lm ln lo lp b">detections.png</code>的文件，该文件将包含图像中检测到的对象的边界框。我给了斯坦福数据集中的一个视频图像作为模型的输入，下面是它输出的结果:</p><figure class="mo mp mq mr gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nb"><img src="../Images/510491f09361a501315e65c676fa3288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9sdnOvm_IFwP0KfvFezJHg.png"/></div></div></figure><p id="9267" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到，该模型已经检测到几个行人在人行道上行走。这对于 Pascal 或 COCO 训练的模型是不可能的。这个图像的纵横比看起来很奇怪，因为 infer 实用程序将图像的大小调整为 1280x1280，这是模型的输入大小。</p><h2 id="e4c2" class="lq lr it bd ls lt lu dn lv lw lx dp ly kb lz ma mb kf mc md me kj mf mg mh mi bi translated"><strong class="ak">对视频的推断</strong></h2><p id="7877" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">既然我们可以在单个图像中检测行人，那么通过编辑<code class="fe lm ln lo lp b">cppapi</code>目录中的<code class="fe lm ln lo lp b">infer.cpp</code>文件，就可以直接将其扩展到视频中。我想到了这个:</p><figure class="mo mp mq mr gt nc"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="5248" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要使用这个脚本，将其保存为<code class="fe lm ln lo lp b">cppapi</code>目录中的<code class="fe lm ln lo lp b">infervideo.cpp</code>，并编辑<code class="fe lm ln lo lp b">CMakeLists.txt</code>以添加添加<code class="fe lm ln lo lp b">infervideo</code>可执行文件的行，并将其链接到<code class="fe lm ln lo lp b">retinanet</code>和其他库。</p><p id="66e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">完成后，您可以切换到<code class="fe lm ln lo lp b">build</code>目录，并像以前一样调用<code class="fe lm ln lo lp b">cmake</code>和<code class="fe lm ln lo lp b">make</code>:</p><pre class="mo mp mq mr gt ms lp mt mu aw mv bi"><span id="e6c5" class="lq lr it lp b gy mw mx l my mz">cmake -DCMAKE_CUDA_FLAGS="--expt-extended-lambda -std=c++11" ..<br/>make</span></pre><p id="b8b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦构建了目标，您将在构建目录中看到一个名为<code class="fe lm ln lo lp b">infervideo</code>的新可执行文件，它可以用作:</p><pre class="mo mp mq mr gt ms lp mt mu aw mv bi"><span id="163e" class="lq lr it lp b gy mw mx l my mz">./infervideo engine.plan input.mov output.mp4</span></pre><p id="202e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这将创建一个名为<code class="fe lm ln lo lp b">output.mp4</code>的新视频，显示每个对象的边界框。如果您想要对来自无人机的实时视频流执行对象检测，您可以简单地提供摄像机的 gstreamer 管道作为脚本的第二个参数，它也会处理摄像机。</p><p id="0d6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我用上面的脚本对我的母校新德里 IIT 德里校区拍摄的一段视频进行了推理。我使用了 0.2 的低阈值来绘制边界框，这就是为什么下面的视频中有一些误报。</p><figure class="mo mp mq mr gt nc"><div class="bz fp l di"><div class="nl nk l"/></div></figure><h2 id="10f7" class="lq lr it bd ls lt lu dn lv lw lx dp ly kb lz ma mb kf mc md me kj mf mg mh mi bi translated"><strong class="ak">提高性能</strong></h2><p id="0c00" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">如果您在 Xavier 上运行上面提供的脚本，您会发现视频的每一帧都需要 150 ms 来进行推断。这在 Xavier 上非常慢，在 TX2 或 nano 这样的小型 Jetsons 上就更慢了。以下是我们可以提高性能的一些方法:</p><ul class=""><li id="d614" class="kx ky it js b jt ju jx jy kb kz kf la kj lb kn lc ld le lf bi translated">我们在这篇文章中创建的引擎是用于 fp16 precision 的。您可以在整数精度上运行该模型，这将显著提高其性能。为此，您可以使用斯坦福数据集的一小部分数据和 tensorRT 的<code class="fe lm ln lo lp b">trtexec</code>实用程序来创建 INT8 校准文件，并将该文件提供给我们构建目录中的<code class="fe lm ln lo lp b">export</code>实用程序。</li><li id="1fe7" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">实际上，任何实时对象检测管道都不会对每一帧执行完整的推断，而是通常将其与计算成本低廉的跟踪器(如卡尔曼滤波器或光流)混合在一起。可以使用 opencv 的 KalmanFilter 类跨帧跟踪对象，每 4 或 5 帧只进行一次推断。如果无人机在推理过程中没有突然的抖动，这在实践中会很好。</li><li id="7319" class="kx ky it js b jt lg jx lh kb li kf lj kj lk kn lc ld le lf bi translated">我们正在使用的模型非常大，因为它需要 1280x1280 的图像。您可以在较低分辨率的图像甚至自定义数据集上训练模型，以显著改善模型的延迟和吞吐量。训练模型的说明在<code class="fe lm ln lo lp b">retinanet-examples</code>存储库中，但是最好在一个配备支持 CUDA 的 GPU 的 x86 工作站上完成。</li></ul><h2 id="7ac9" class="lq lr it bd ls lt lu dn lv lw lx dp ly kb lz ma mb kf mc md me kj mf mg mh mi bi translated"><strong class="ak">结论</strong></h2><p id="7a83" class="pw-post-body-paragraph jq jr it js b jt mj jv jw jx mk jz ka kb ml kd ke kf mm kh ki kj mn kl km kn im bi translated">这篇博客旨在为那些在 Jetson Xavier 上部署 retinanet 模型有困难的人提供帮助，并记录我为在无人机上运行良好的对象检测管道所做的努力。虽然我们得到了比 Coco/Pascal 模型更好的结果，但要使模型在 Jetson 设备上实时运行，还需要做许多改进。请将此作为您自己项目的起点，如果您有任何其他改进性能的建议，请在下面发表评论。</p></div></div>    
</body>
</html>