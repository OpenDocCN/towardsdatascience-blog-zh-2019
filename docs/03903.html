<html>
<head>
<title>Improving Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">改进深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/improving-deep-neural-networks-b5984e29e336?source=collection_archive---------22-----------------------#2019-06-19">https://towardsdatascience.com/improving-deep-neural-networks-b5984e29e336?source=collection_archive---------22-----------------------#2019-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b15e89225858b4419cfefa87943b4b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TGWyvYU6Vx8J-nN6d7brLw.png"/></div></div></figure><div class=""/><p id="91c7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">深度神经网络是自然语言处理、计算机视觉、语音合成等复杂任务的解决方案。提高他们的表现和理解他们如何工作一样重要。要了解它们是如何工作的，你可以参考我以前的帖子。在这篇文章中，我将解释与改善神经网络相关的各种术语和方法。</p><h1 id="a26f" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">偏差和方差</h1><p id="c8e9" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">偏差和方差是解释网络在训练集和测试集上表现如何的两个基本术语。让我们用一个 2 类问题来简单直观地理解偏差和方差。蓝线表示由神经网络计算的决策边界。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mc"><img src="../Images/fa8f332c59f019c9965bf5ddea30efb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f_nYb2n_bEuucCkATwSUxg.png"/></div></div></figure><ol class=""><li id="7712" class="mh mi je kd b ke kf ki kj km mj kq mk ku ml ky mm mn mo mp bi translated">最左图显示神经网络存在<strong class="kd jf">偏高的问题。</strong>在这种情况下，网络已经学习了一个简单的假设，因此不能根据训练数据进行适当的训练。因此，它不能区分不同类别的示例，并且<strong class="kd jf"> <em class="mq">在</em> </strong>的训练集和测试集上都表现不佳。我们也可以说这个网络不适合。</li><li id="f47d" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mm mn mo mp bi translated">最右边的图显示神经网络存在<strong class="kd jf">方差高的问题。在这种情况下，网络已经学习了一个非常复杂的假设，因此不能概括。因此，<strong class="kd jf"> <em class="mq">在训练数据上表现出色，而在测试数据</em> </strong>上表现不佳。我们也可以说网络<strong class="kd jf">过度拟合。</strong></strong></li><li id="8834" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mm mn mo mp bi translated">中心图显示了一个<strong class="kd jf"> <em class="mq">【恰到好处】</em> </strong>的神经网络。它已经学习了理想假设，这有助于网络过滤掉异常，并对数据进行归纳。我们的目标应该是实现这样的网络类型。</li></ol><h1 id="b011" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">训练食谱</h1><p id="00c1" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">既然我们知道什么样的神经网络是可取的；让我们看看如何实现我们的目标。这些步骤首先解决偏差问题，然后解决方差问题。</p><p id="791c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们应该问的第一个问题是“是否存在高偏差？”如果答案是<strong class="kd jf">是</strong>，那么我们应该尝试以下步骤:</p><ul class=""><li id="1396" class="mh mi je kd b ke kf ki kj km mj kq mk ku ml ky mw mn mo mp bi translated">培养一个更大的网络。它包括增加隐含层的数目和隐含层中神经元的数目。</li><li id="49e7" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mw mn mo mp bi translated">长时间训练网络。可能的情况是，完整的训练尚未完成，将需要更多的迭代。</li><li id="ad89" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mw mn mo mp bi translated">尝试不同的优化算法。这些算法包括 Adam、Momentum、AdaDelta 等。</li><li id="e475" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mw mn mo mp bi translated">反复执行上述步骤，直到偏差问题得到解决，然后进入第二个问题。</li></ul><p id="1622" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果答案是<strong class="kd jf">否，</strong>说明我们已经克服了偏倚问题，是时候关注方差问题了。我们现在应该问的第二个问题是“方差高吗？”如果答案是<strong class="kd jf">是的，</strong>那么我们应该尝试以下步骤:</p><ul class=""><li id="42fc" class="mh mi je kd b ke kf ki kj km mj kq mk ku ml ky mw mn mo mp bi translated">收集更多的训练数据。随着我们收集更多的数据，我们将获得更多的数据变化，从较少变化的数据中习得的假设的复杂性将被打破。</li><li id="2746" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mw mn mo mp bi translated">试试正规化。我将在下一节谈到它。</li><li id="b38f" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mw mn mo mp bi translated">反复执行上述步骤，直到方差问题得到解决。</li></ul><p id="a580" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果答案是<strong class="kd jf">否，</strong>说明我们已经克服了方差问题，现在我们的神经网络是<strong class="kd jf"> <em class="mq">【刚刚好】</em> </strong>。</p><h1 id="d378" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">正规化</h1><p id="c89f" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">正则化是一种逻辑技术，有助于减少神经网络中的过拟合。当我们将正则化加入到我们的网络中时，我们添加了一个新的正则化项，并且损失函数被修改。修改后的成本函数<strong class="kd jf"> J </strong>的数学公式为:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/4f751502079d6a1021bd4b8239ed3658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FhdVBNN_Wpgc7Lce"/></div></div></figure><p id="b0f0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">带有<strong class="kd jf"><em class="mq">λ</em></strong>的第二项称为正则化项。术语<strong class="kd jf"> <em class="mq"> ||W|| </em> </strong>通称为<strong class="kd jf"> <em class="mq">弗罗贝纽斯范数</em> </strong>(矩阵中元素的平方和)。随着正则化的引入，<strong class="kd jf"><em class="mq">λ</em></strong>成为新的超参数，可以对其进行修改以提高神经网络的性能。上述正则化也被称为<strong class="kd jf"> <em class="mq"> L-2 正则化。</em> </strong></p><p id="553c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">之前，我们使用以下更新规则来更新权重:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/323316e96a2f9cea6f9bdeb81f4f7578.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*khFsVcMfqrSbpATroSqKBA.png"/></div></figure><p id="bce1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">由于在包括正则化的修改的成本函数<strong class="kd jf"> <em class="mq"> J，</em> </strong>中有新的正则化项，我们将以如下方式更新权重:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/06e0ef9ff34bc93e2806c5747aa4915c.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*8s6h1rCxO4w3TfV-0vsSBw.png"/></div></figure><p id="2674" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里我们可以看到权重值减少了一个小于 1 的小数值。因此，我们也将这类正则化称为<strong class="kd jf"> <em class="mq">权重衰减。</em> </strong>衰减值取决于学习率<strong class="kd jf"><em class="mq">α</em></strong>和正则项<strong class="kd jf"><em class="mq">λ。</em>T47】</strong></p><h2 id="bbee" class="na la je bd lb nb nc dn lf nd ne dp lj km nf ng ln kq nh ni lr ku nj nk lv nl bi translated">为什么正规化行得通？</h2><p id="f034" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">训练神经网络的最终目标是最小化成本函数<strong class="kd jf"> <em class="mq"> J </em> </strong>以及正则化项。现在我们知道了什么是正则化，让我们试着理解它为什么有效。</p><p id="edc1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">第一个直觉是，如果我们增加<strong class="kd jf"><em class="mq">λ、</em> </strong>的值，那么<strong class="kd jf"> <em class="mq"> Frobenius 范数</em> </strong>就变小了，权重值就变得接近 0。这种方法主要清除某些神经元，使网络变得很浅。可以认为是将学习复杂假设的深层网络转换成学习简单假设的浅层网络。我们知道，简单假设导致复杂特征减少，过拟合就会减少，我们就会得到一个<strong class="kd jf"> <em class="mq">【恰到好处】</em> </strong>的神经网络。</p><p id="ba79" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一种直觉可以从应用正则化时神经元的激活方式中获得。为此，让我们考虑<strong class="kd jf"> <em class="mq"> tanh(x) </em> </strong>激活。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f5fa08f81255ef5034ea4aac1c96d7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*nrq3oFbWLOApi9cocdqcsg.png"/></div></figure><p id="00a8" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们增加<strong class="kd jf"><em class="mq">λ</em></strong>的值，那么<strong class="kd jf"> <em class="mq"> Frobenius 范数</em> </strong>变小，即权重<strong class="kd jf"> <em class="mq"> W </em> </strong>变小。因此，该层的输出将变小，并将位于激活函数的蓝色区域。我们可以看到，蓝色区域的激活几乎是线性的，网络将表现得类似于浅层网络，即网络将不学习复杂的假设(将避免尖锐的曲线)，并且过拟合将最终减少，并且我们将获得一个<strong class="kd jf"> <em class="mq">【恰到好处】</em> </strong>神经网络。</p><p id="f1db" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，<strong class="kd jf"><em class="mq">λ</em></strong>的值过小将导致过拟合，因为<strong class="kd jf"> <em class="mq"> Frobenius 范数</em> </strong>将会很大，并且神经元将不会被清除，并且层的输出将不会在线性区域中。类似地，过大的<strong class="kd jf"><em class="mq">λ</em></strong>值将导致欠拟合。因此，找到<strong class="kd jf"><em class="mq">λ</em></strong>的理想值是提高神经网络性能的一项至关重要的任务。</p><h1 id="741f" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">辍学正规化</h1><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/0facf481ce1fe4a7b85275f47802aa2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXf1Src1R1ttZZR6EsW3wg.png"/></div></div></figure><p id="e896" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">丢弃正则化是另一种正则化技术，其中我们丢弃某些神经元以及它们在神经网络中的连接。概率<strong class="kd jf"> <em class="mq"> keep_prob </em> </strong>决定了将要丢弃的神经元。在神经元被移除之后，网络在剩余的神经元上被训练。重要的是要注意，在测试时间/推断时间期间，所有的神经元都被考虑用于确定输出。让我们借助一个例子来理解这个概念:</p><pre class="md me mf mg gt no np nq nr aw ns bi"><span id="7484" class="na la je np b gy nt nu l nv nw"># Define the probablity that a neuron stays.<br/>keep_prob = 0.5</span><span id="1e0f" class="na la je np b gy nx nu l nv nw"># Create a probability mask for a layer eg. layer 2. The mask should # have same dimensions as the weight matrix so that the connections # can be removed.<br/>d2 = np.random.rand(a2.shape[0],a2.shape[1]) &lt; keep_prob</span><span id="5ec7" class="na la je np b gy nx nu l nv nw"># Obtain the new output matrix.<br/>a2 = np.multiply(a2,d2)</span><span id="3644" class="na la je np b gy nx nu l nv nw"># Since few neurons are removed, we need to boost the weights of    # remaining neurons to avoid weight imbalance during test time.<br/>a2 = a2/keep_prob</span></pre><p id="7928" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">由于我们首先丢弃概率为<strong class="kd jf"> <em class="mq"> keep_prob </em> </strong>的神经元，然后用<strong class="kd jf"> <em class="mq"> keep_prob </em> </strong>提升剩余的神经元，这种类型的丢弃称为<strong class="kd jf"> <em class="mq">反向丢弃。</em>T11】</strong></p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5f52526c48bfb3d651b0d5fd7a3e9a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*mrQfchnOoeJMacZKJvYsJA.png"/></div></figure><p id="bc8b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">dropout 之间的直觉是，它禁止神经元只依赖于某些特征，因此，权重是分散的。可能的情况是，神经元变得依赖于某些输入特征来确定输出。在 dropout 正则化的帮助下，对于训练过程中的不同训练样本，特定神经元每次只获得少量特征作为输入。最终，权重分布在所有输入中，网络使用所有输入特征来确定输出，而不依赖于任何一个特征，从而使网络更加健壮。它也被称为<strong class="kd jf"><em class="mq">L2 正则化的自适应形式。</em>T15】</strong></p><p id="9d1c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们也可以为每一层单独设置<strong class="kd jf"><em class="mq">【keep _ prob】</em></strong><em class="mq"/>。由于被丢弃的神经元数量与<strong class="kd jf"> <em class="mq"> keep_prob </em> </strong>成反比；建立<strong class="kd jf"><em class="mq">keep _ prob</em></strong><em class="mq"/>的一般标准是，密集连接应该具有相对较少的<strong class="kd jf"><em class="mq">keep _ prob</em></strong><em class="mq"/>以便丢弃更多的神经元，反之亦然。</p><p id="a61f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一个直觉是，随着退出正规化，深层网络在训练阶段模仿浅层网络的工作。这进而导致减少过拟合，我们获得一个<strong class="kd jf"> <em class="mq">【恰到好处】</em> </strong>神经网络。</p><h1 id="a586" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">提前停止</h1><p id="03a2" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">早期停止是一种训练方法，在这种方法中，我们在较早的阶段停止训练神经网络，以防止它过度拟合。我们跟踪<strong class="kd jf"> <em class="mq"> train_loss </em> </strong>和<strong class="kd jf"> <em class="mq"> dev_loss </em> </strong>来确定何时停止训练。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/ee80a7b9f18e1afc14440a15787de38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*lNeWhf-MmcbQDVLXVZiRbQ.png"/></div></figure><p id="2c40" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">刚好<strong class="kd jf"> <em class="mq"> dev_loss </em> </strong>开始超调；我们停止训练过程。这种方法论被称为<strong class="kd jf"><em class="mq"/></strong>。但是，由于以下两个原因，早期停止不是训练网络的推荐方法:</p><ol class=""><li id="494c" class="mh mi je kd b ke kf ki kj km mj kq mk ku ml ky mm mn mo mp bi translated">当我们停止训练过程时，损失不是最小的。</li><li id="9d6d" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mm mn mo mp bi translated">我们正试图减少训练不当的网络上的过度拟合。</li></ol><p id="d70f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">过早停止会使事情变得复杂，我们无法获得<strong class="kd jf"> <em class="mq">【恰到好处】</em> </strong>神经网络。</p></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><h1 id="fcff" class="kz la je bd lb lc oh le lf lg oi li lj lk oj lm ln lo ok lq lr ls ol lu lv lw bi translated">参考</h1><ol class=""><li id="16bb" class="mh mi je kd b ke lx ki ly km om kq on ku oo ky mm mn mo mp bi translated"><a class="ae op" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">维基百科—激活功能</a></li><li id="ec97" class="mh mi je kd b ke mr ki ms km mt kq mu ku mv ky mm mn mo mp bi translated"><a class="ae op" href="https://www.coursera.org/learn/deep-neural-network/home/welcome" rel="noopener ugc nofollow" target="_blank">Coursera——深度学习课程 2 </a></li></ol></div><div class="ab cl oa ob hx oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="im in io ip iq"><p id="08e8" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mq">我想感谢读者阅读这个故事。如果你有任何问题或疑问，请在下面的评论区提问。我将非常乐意回答这些问题并帮助你。如果你喜欢这个故事，请关注我，以便在我发布新故事时获得定期更新。我欢迎任何能改进我的故事的建议。</em></p></div></div>    
</body>
</html>