<html>
<head>
<title>Review: G-RMI — Winner in 2016 COCO Detection (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:G-RMI——2016 年 COCO 检测(物体检测)冠军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=collection_archive---------9-----------------------#2019-01-11">https://towardsdatascience.com/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4?source=collection_archive---------9-----------------------#2019-01-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f422" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">检测架构选择指南:更快的 R-CNN、R-FCN 和 SSD</h2></div><p id="55b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di"> T </span>他的时间，<strong class="kh ir"> G-RMI </strong>，<strong class="kh ir">谷歌研究与机器智能</strong>，2016 年 MS COCO 检测挑战赛获得<strong class="kh ir">第一名的人回顾。G-RMI 是参加挑战赛的队伍名称。这不是一个提议方法的名称，因为他们没有任何创新的想法，例如修改深度学习架构以赢得挑战。名为“现代卷积对象检测器的速度/精度权衡”的论文也给了我们一些提示，<strong class="kh ir">他们</strong> <strong class="kh ir">系统地研究了不同种类的对象检测器和特征提取器。</strong>具体来说:</strong></p><ul class=""><li id="bee8" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir"> 3 个对象检测器(元架构)</strong> : <a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>、<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a></li><li id="9560" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated"><strong class="kh ir"> 6 个特征提取器</strong> : <a class="ae lt" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG-16 </a>，<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>，<a class="ae lt" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> Inception-v2 </a>，<a class="ae lt" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>，<a class="ae lt" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNet </a></li></ul><p id="e02f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">他们还分析了其他参数的影响，如输入图像大小和区域提议的数量。</strong>最后，几个模特的合奏达到了最先进的效果，赢得了挑战。并发表在<strong class="kh ir"> 2017 CVPR </strong>上，引用<strong class="kh ir"> 400 余次</strong>。(<a class="lz ma ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----af3f2eaf87e4--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="9262" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">概述</h1><ol class=""><li id="22a9" class="lk ll iq kh b ki na kl nb ko nc ks nd kw ne la nf lq lr ls bi translated"><strong class="kh ir">元架构</strong></li><li id="8734" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">特征提取器</strong></li><li id="28ba" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">精度对时间</strong></li><li id="763f" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">特征提取器的作用</strong></li><li id="7c26" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">对象大小的影响</strong></li><li id="a40b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">图像尺寸的影响</strong></li><li id="fdeb" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">提案数量的影响</strong></li><li id="492c" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir"> FLOPs 分析</strong></li><li id="d116" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">内存分析</strong></li><li id="bb76" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated"><strong class="kh ir">0.75 IOU 下的良好定位意味着所有 IOU 阈值下的良好定位</strong></li><li id="f33b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la nf lq lr ls bi translated">可可的最新检测结果</li></ol></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="8058" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">1.元架构</h1><p id="28ea" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">对象检测器在这里被称为元体系结构。<strong class="kh ir">调查了三种元架构</strong>:<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>、<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi nj"><img src="../Images/14b2f5eaac3a012b06b90eed9704ab7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mqUHIntTDLUGPKvbtMiCpw.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Abstract Architecture</strong></figcaption></figure><p id="42fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> <strong class="kh ir"> SSD </strong> </a></p><ul class=""><li id="bc8f" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">它使用<strong class="kh ir">单个前馈卷积网络来直接预测类别和锚偏移，而不需要第二阶段的每建议分类操作</strong>。</li></ul><p id="a75d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="kh ir">更快 R-CNN </strong> </a></p><ul class=""><li id="dbb7" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">在被称为<strong class="kh ir">区域提议网络(RPN) </strong>的<strong class="kh ir">第一阶段</strong>中，图像由特征提取器(例如<a class="ae lt" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG-16 </a>处理，在一些选定的中间级别(例如“conv5”)的特征被用于<strong class="kh ir">预测类别不可知的盒子提议</strong>。</li><li id="02ee" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">在<strong class="kh ir">第二阶段</strong>中，这些(通常为 300) <strong class="kh ir">框提议被用于从相同的中间特征图(ROI 汇集)</strong>中裁剪特征，这些特征随后被馈送到特征提取器的剩余部分(例如，“fc6”后面跟着“fc7”)，以便<strong class="kh ir">为每个提议</strong>预测类别和类别特定的框细化。</li></ul><p id="2057" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"><strong class="kh ir">FCN</strong></a></p><ul class=""><li id="c658" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">与<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>类似的还有<strong class="kh ir">第一级</strong>中的<strong class="kh ir"> RPN </strong>。</li><li id="e349" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">在第二阶段，使用<strong class="kh ir">正敏感得分图</strong>，以便在预测之前从最后一层特征中提取<strong class="kh ir">裁剪(ROI 合并)。</strong>这使得<strong class="kh ir">每个投资回报的运营成本变得非常低</strong>，因为几乎所有运营都是在投资回报池之前共享的。</li><li id="3155" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">因此，它通常以更快的运行时间实现了与更快的 R-CNN 相当的准确性。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="7da0" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak"> 2。特征提取器</strong></h1><p id="e9ac" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated"><strong class="kh ir">尝试了六个特征提取器</strong>:<a class="ae lt" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener">VGG-16</a>，<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">雷斯网-101 </a>，<a class="ae lt" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">盗梦空间-v2 </a>，<a class="ae lt" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener">盗梦空间-v3 </a>，<a class="ae lt" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">盗梦空间-雷斯网-v2 </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>。</p><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oa"><img src="../Images/f4814b7505edcddde308ac68db7c67ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_hN3ifK_jgmmfCMaCyDoiw.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Top-1 classification accuracy on ImageNet</strong></figcaption></figure><ul class=""><li id="2f23" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">不同的特征提取器，不同的层用于提取用于对象检测的特征。</li><li id="599b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">对一些特征提取器进行了一些修改，例如，使用了扩展的卷积，或者使最大池步幅变小，以便在特征提取之后步幅大小不会太小。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="a093" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">3.精度与时间</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ob"><img src="../Images/1407ca892d6a6f07b97ba0ad31fa0f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xgBs8CZdf1AvaFz92ERB6A.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Accuracy vs Time, The dotted Line is Optimality Frontier</strong></figcaption></figure><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oc"><img src="../Images/4ab81b59ab75d180f1e389ba1681167e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L4JKsuGNDNtGF7WuqcpuXw.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Test-dev performance of the “critical” points along our optimality frontier</strong></figcaption></figure><ul class=""><li id="0575" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">颜色:特征提取器</li><li id="d42b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">标记形状:元架构</li></ul><h2 id="2000" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">3.1.一般观察</h2><ul class=""><li id="4a01" class="lk ll iq kh b ki na kl nb ko nc ks nd kw ne la lp lq lr ls bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>平均速度更快。</li><li id="e1e2" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">更快的 R-CNN 速度较慢，但更准确，每张图像至少需要 100 毫秒。</li></ul><h2 id="5837" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">3.2.最优边界上的临界点</h2><p id="5582" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated"><strong class="kh ir">最快:</strong><a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"><strong class="kh ir">SSD</strong></a><strong class="kh ir">w/</strong><a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"><strong class="kh ir">MobileNet</strong></a></p><ul class=""><li id="0d51" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">带<a class="ae lt" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> Inception-v2 </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNet </a>的固态硬盘</a>是速度最快的型号中最精确的。</li><li id="d4c7" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">忽略后处理成本，<a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNet </a>似乎比<a class="ae lt" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> Inception-v2 </a>快大约一倍，但准确性稍差。</li></ul><p id="e46f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">甜蜜点:</strong><a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"><strong class="kh ir">R-FCN</strong></a><strong class="kh ir">w/</strong><a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">或</strong> <a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="kh ir">更快 R-CNN</strong></a><strong class="kh ir">w/</strong><a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">并且只有 50 个提案</strong></p><ul class=""><li id="b275" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">使用<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>特征提取器的<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>模型占据的最优性边界中间有一个“肘”。</li><li id="6c19" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">这是各种型号配置中速度和精度之间的最佳平衡。</li></ul><p id="7e59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">最准确:</strong> <a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="kh ir">更快 R-CNN</strong></a><strong class="kh ir">w/</strong><a class="ae lt" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kh ir">Inception-ResNet</strong></a><strong class="kh ir">at stride 8</strong></p><ul class=""><li id="6a7c" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">具有密集输出的更快 R-CNN</a><a class="ae lt" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">Inception-ResNet-v2</a>模型在我们的最优边界上获得了最佳的可能精度。</li><li id="1c09" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">然而，这些模型很慢，需要将近一秒的处理时间。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="ff3b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">4.特征提取器的作用</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi op"><img src="../Images/1d1c0d6b46fb15e978641fa85f82bc4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ig1ehmgquEOopyXyY1_ApQ.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Accuracy of detector (mAP on COCO) vs accuracy of feature extractor</strong></figcaption></figure><ul class=""><li id="31f3" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">直觉上，更强的分类性能应该与更强的 COCO 检测性能正相关。</li><li id="7c2b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">这种相关性似乎只对<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>有意义，而<a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>的性能似乎不太依赖其特征提取器的分类精度。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="596e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">5.物体大小的影响</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oq"><img src="../Images/b86dee7e48f8805c948c69cea88dfac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XicNKMdQSlZFEmS_tVsv9g.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Accuracy stratified by object size, meta-architecture and feature extractor, image resolution is fixed to 300</strong></figcaption></figure><ul class=""><li id="ba73" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">所有的方法在大对象上都做得更好。</li><li id="76b1" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">固态硬盘</a>通常在小对象上的性能(非常)差，但<a class="ae lt" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">固态硬盘</a>在大对象上仍然与<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>有竞争力。</li><li id="3098" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">后来，有<a class="ae lt" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a>来解决小物体检测问题。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="fa1b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">6.图像大小的影响</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi or"><img src="../Images/2d33d76e62ffb477d987a7e11c3b4995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZ7wF9v94zqpxU27skoarA.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Effect of image resolution</strong></figcaption></figure><ul class=""><li id="0c16" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">将分辨率在两个维度上降低两倍会持续降低准确度(平均降低 15.88%)，但也会将推断时间平均降低 27.4%。</li><li id="a659" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">高分辨率输入允许分辨小物体。</li><li id="5715" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">高分辨率模型在小对象上产生明显更好的贴图结果(在许多情况下是 2 倍),在大对象上也产生稍微更好的贴图结果。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="16b0" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">7.提案数量的影响</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi os"><img src="../Images/23eeca06587c0eed3af13af4c1a345f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85LjTxVxCsa_7V-QdcaMgA.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Faster R-CNN (Left), R-FCN (Right)</strong></figcaption></figure><p id="8388" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以在 RPN(第一阶段)输出不同数量的建议。提案越少，运行时间越快，反之亦然。</p><h2 id="223c" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a></h2><ul class=""><li id="6ecd" class="lk ll iq kh b ki na kl nb ko nc ks nd kw ne la lp lq lr ls bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet </a>，有 300 个提案有 35.4%的 mAP，在只有 10 个提案的情况下依然可以有惊人的高准确率(29%的 mAP)。</li><li id="e4d4" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">最佳点可能是 50 个建议，在这里，我们能够获得使用 300 个建议的 96%的准确性，同时将运行时间减少 3 倍。</li></ul><h2 id="52e9" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a></h2><ul class=""><li id="6125" class="lk ll iq kh b ki na kl nb ko nc ks nd kw ne la lp lq lr ls bi translated">在<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>设置中使用更少的建议所节省的计算量很小。</li><li id="8b8b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">这并不奇怪，因为如上所述，由于正敏感得分图的共享计算，对于<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>，每个 ROI 的计算成本较低。</li></ul><h2 id="ba47" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">比较<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>和<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a></h2><ul class=""><li id="eb5a" class="lk ll iq kh b ki na kl nb ko nc ks nd kw ne la lp lq lr ls bi translated">在 100 个建议时，具有<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>的<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>模型的速度和准确性变得与同等的<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c"> R-FCN </a>模型大致相当，后者在地图和 GPU 速度上都使用了 300 个建议。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="056c" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">8.FLOPs 分析</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ot"><img src="../Images/c117d10a12e8a7f8def6b10b5564ee32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-SJOFryU26fUzTvxj_OhnA.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">FLOPs vs Time</strong></figcaption></figure><ul class=""><li id="83ab" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">对于更密集的块模型，如<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>，FLOPs/GPU 时间通常大于 1。</li><li id="b492" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">对于 Inception 和<a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNet </a>型号，这个比率通常小于 1。</li><li id="d55b" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">也许，因式分解减少了 FLOPs，但增加了更多的内存 I/O 开销，或者可能是当前的 GPU 指令(cuDNN)更适合密集卷积。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="2d0e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">9.记忆分析</h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ou"><img src="../Images/5fe0da032500806dc0f2e91489c6fac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrNj8q1_AOV11tqZ-JGyCg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Memory (Mb) vs Time</strong></figcaption></figure><ul class=""><li id="96f3" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">与运行时间高度相关，更大更强大的特征提取器需要更多的内存。</li><li id="d0ca" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">与速度一样，<a class="ae lt" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNet </a>是最便宜的，在几乎所有设置中需要不到 1Gb(总)的内存。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="9aae" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated"><strong class="ak"> 10。75 IOU 的良好定位意味着所有 IOU 阈值的良好定位</strong></h1><figure class="nk nl nm nn gt no gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/76f0a988c971ffc0bcb02bcec0990ff5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*DWhLCfo4AzYSWtRBudRRPQ.png"/></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Overall COCO mAP (@[.5:.95]) for all experiments plotted against corresponding mAP@.50IOU and mAP@.75IOU</strong></figcaption></figure><ul class=""><li id="9072" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">mAP@.5 和 mAP@.75 性能几乎与 mAP@[.5:.95]完全线性相关。</li><li id="015d" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">mAP@.75 与 mAP@[.5:.95]的相关性稍强(R &gt; 0.99)，因此，如果我们要在单个 IOU 阈值下用 mAP 替换标准 COCO 指标，IOU = .75 可能会被选中。</li></ul></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="73f5" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">11.可可上最先进的检测结果</h1><h2 id="8f1e" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">11.1.集合和多作物</h2><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ow"><img src="../Images/d2f0a76e1d0d50b87740012b28b66ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e8rFX0PnGEERmzj_AnxZAg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Summary of 5 Faster R-CNN single models</strong></figcaption></figure><ul class=""><li id="901e" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated">由于 mAP 是 COCO 检测挑战的主要目标，因此最准确但耗时的<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>被考虑。</li><li id="3203" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">不同的结果鼓励<strong class="kh ir">集合</strong>。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi ox"><img src="../Images/a11c23f19fa1fd6aa086d7a738c20754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HA89PucHtoHl7dapJZITTw.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Performance on the 2016 COCO test-challenge dataset.</strong></figcaption></figure><ul class=""><li id="9317" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir"> G-RMI </strong>:将上述 5 个模型集合，多作物生成最终模型。它超过了 2015 年的冠军和 2016 年的第二名。</li><li id="ae92" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">2015 年的胜者使用<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a> + <a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a> + <a class="ae lt" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener"> NoCs </a>。(请看我对<a class="ae lt" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoCs</a>COCO 挑战赛结果的点评。)</li><li id="f5d7" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">Trimps-Soushen，2016 年第 2 名，使用<a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a> + ensemble 多个模型+其他论文的改进。(COCO challenge 上没有关于 Trimps-Soushen 的细节。)</li><li id="069e" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">注意:这里没有多尺度训练、水平翻转、框细化、框投票或全局上下文。</li></ul><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oy"><img src="../Images/3b3924ad7959bfafb9a2df57fc5eacb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OvdWjWC-LubQMRicme2Twg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Effects of ensembling and multicrop inference.</strong></figcaption></figure><ul class=""><li id="5a66" class="lk ll iq kh b ki kj kl km ko lm ks ln kw lo la lp lq lr ls bi translated"><strong class="kh ir">第二排</strong> : 6 个更快的 RCNN 型号，带 3 个<a class="ae lt" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>和 3 个<a class="ae lt" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>。</li><li id="1c5c" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated"><strong class="kh ir">第三行</strong>:本节第一个表中的不同集合结果。</li><li id="eea0" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">因此，多样性是令人鼓舞的，与使用人工选择的集合相比，这确实有很大帮助。</li><li id="7b36" class="lk ll iq kh b ki lu kl lv ko lw ks lx kw ly la lp lq lr ls bi translated">集成和多作物比单一模式提高了近 7 个百分点。</li></ul><h2 id="9c2f" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">11.2.来自 5 种不同型号的检测</h2><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi oz"><img src="../Images/fc0e377bf60bbae5177619806b6aba9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EoFUcN6bCifFoKXMTqfDcA.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Beach</strong></figcaption></figure><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pa"><img src="../Images/9ef779f147a7c7241807d0e0a7bf38a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B_T6418ao-6wrfXxeY5wsg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Baseball</strong></figcaption></figure><figure class="nk nl nm nn gt no gh gi paragraph-image"><div role="button" tabindex="0" class="np nq di nr bf ns"><div class="gh gi pb"><img src="../Images/a739c92e32e5fb0e0a1254efa7f77803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4cvjQBWa3riRvd2R1Zj0Q.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk"><strong class="bd nz">Elephants</strong></figcaption></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h2 id="2527" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">参考</h2><p id="5759" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">【2017 CVPR】【G-RMI】<br/><a class="ae lt" href="https://arxiv.org/abs/1611.10012" rel="noopener ugc nofollow" target="_blank">现代卷积物体探测器的速度/精度权衡</a></p><h2 id="346a" class="od mj iq bd mk oe of dn mo og oh dp ms ko oi oj mu ks ok ol mw kw om on my oo bi translated">我的相关评论</h2><p id="5972" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko ng kq kr ks nh ku kv kw ni ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lt" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lt" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lt" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lt" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lt" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lt" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lt" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/>T31】[<a class="ae lt" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a>][<a class="ae lt" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a>][<a class="ae lt" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>][<a class="ae lt" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>][<a class="ae lt" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a>][<a class="ae lt" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a>]</strong></p><p id="98cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/> </strong> [ <a class="ae lt" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lt" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lt" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a> ] [ <a class="ae lt" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lt" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ]</p></div></div>    
</body>
</html>