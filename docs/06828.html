<html>
<head>
<title>The crux of word embedding layers -Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">word 嵌入层的关键-第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-crux-of-word-embedding-layers-part-1-97e4a612277d?source=collection_archive---------20-----------------------#2019-09-28">https://towardsdatascience.com/the-crux-of-word-embedding-layers-part-1-97e4a612277d?source=collection_archive---------20-----------------------#2019-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="25c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae ko" href="https://medium.com/analytics-vidhya/essence-of-rasa-nlu-899629a83a89" rel="noopener">最后一个故事</a>中，我们讨论了 RASA NLU，这是一个开源的对话式人工智能工具。我们使用 Tensorflow 管道进行意图分类。流水线有不同的组件，如记号赋予器、特征赋予器、实体提取器和<strong class="js iu">意图分类器</strong>。我们的意图分类器本身有子组件，如 TensorFlow 嵌入。现在我们将讨论嵌入层及其在管道中的重要性。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/cec070e49184658472ebaca17ca24c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TqS-SqVY3VWualcZ.jpg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk"><a class="ae ko" href="https://blog.paralleldots.com/wp-content/uploads/2018/12/abstract-ai-art-373543-1024x683.jpg" rel="noopener ugc nofollow" target="_blank">Parallel dots</a></figcaption></figure><h1 id="fc9e" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">目标</h1><p id="7051" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">人类可以非常直观地处理文本格式，但是如果我们在一天内生成数百万甚至数十亿个文档，我们不可能让人类执行所有的任务。它既不可扩展也不有效。因此，我们有计算机来执行所有使用机器学习技术的任务。在本文中，我们将讨论各种可用的嵌入层。我们这篇文章的目标是实现这些单词嵌入层。我们将深入了解这些层。</p><p id="2ed5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">什么是文字嵌入？</strong></p><p id="3348" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嵌入不过是把东西从一个维度放到另一个维度。它从高维空间中选取点，放入低维空间。单词嵌入是将文本转换成数字，并且这些数字可能具有相同文本的不同数字表示。简单地说，它是文档(句子)词汇的表示。</p><p id="8829" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">为什么要嵌入单词？</strong></p><p id="0c39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">大多数机器学习算法甚至深度学习架构都能够处理句子或文本。我们可以对每个字符采用基于字符的编码。但是它不会帮助我们理解一个词的意思。</p><p id="b0a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例子:我们来举个例子。一种常见的简单字符编码是 ASCII(美国信息交换标准代码)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/0f77d54f5ed6dea59a3a98f5a27b89f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/0*y63_cARebktxLRfR.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">ASCII of sweat</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/b8193b5f6877a8a5e9297a1822bcd124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*_FYoMibjFOcY2qc0.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">ASCII of waste</figcaption></figure><p id="a58d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以，你可以看到汗水和浪费的意义是不同的。但是它们仍然共享 ASCII 值。对于计算机来说，这可能是一项令人生畏的任务。</p><p id="f3c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个词嵌入有两类。</p><ol class=""><li id="4716" class="mk ml it js b jt ju jx jy kb mm kf mn kj mo kn mp mq mr ms bi translated"><strong class="js iu">简单的基于频率的嵌入</strong></li><li id="fd41" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated"><strong class="js iu">基于预测的嵌入方法</strong></li></ol><p id="f114" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">a) <strong class="js iu">计数矢量器&amp;一键编码</strong></p><p id="a5b4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">I . count vectorizer 提供了一种简单的方法，既可以标记一组文本文档，构建已知单词的词汇表，还可以使用该词汇表对新文档进行编码。</p><p id="d6b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">二。一键编码是将分类变量表示为二进制向量。每个整数值都表示为一个二进制向量，除了用 1 标记的整数索引之外，其他都是零值。</p><p id="7857" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">三。余弦相似度<strong class="js iu"> </strong>是一种度量，用于确定文档的相似程度，而不考虑它们的大小，并且它测量在多维空间中投影的两个向量之间的角度的余弦。</p><p id="926a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以以(I，ii，iii)为例</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi my"><img src="../Images/bfcdd8d609ab6d4eea47a92477aaee33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JothSk2KA0kUKfee.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Example</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/89b005a987f3f2fbc41510ce60392292.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/0*K2mzSL4_PV5dCY8S.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Cosine distance</figcaption></figure><p id="948c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">x 和 y 的余弦相似度是 0.75</p><p id="f688" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">b) <strong class="js iu"> TF-IDF 矢量器</strong></p><p id="7bdb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> TF </strong>表示词频，idf 表示逆文档频率。TF 计算所有项，并对它们一视同仁。</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="2e35" class="nf lg it nb b gy ng nh l ni nj">TF= term count/document word count</span></pre><p id="5fa6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> IDF </strong>代表逆文档频率，表示日志。对数是指数运算的反函数。1614 年，天文学家和数学家约翰·耐普尔首先发现了对数，用来解决复杂的数学运算。</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="612d" class="nf lg it nb b gy ng nh l ni nj">IDF = log(number of documents/ number of documents with term)</span></pre><p id="6568" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">为什么要日志？</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/0f0f71da36f4d0783ebf0b6c3c4fba6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*qjfKtCA33gdaT2dK.gif"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk"><a class="ae ko" href="https://tenor.com/view/jennifer-aniston-thinking-contemplate-agree-yes-gif-3372425" rel="noopener ugc nofollow" target="_blank">Tenor</a></figcaption></figure><p id="ee56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以将 log 与<strong class="js iu"> Zipf 定律</strong>联系起来，Zipf 定律是一条经验定律<strong class="js iu"> </strong>使用数理统计方法制定，并以语言学家 George Kingsley Zipf 的名字命名。它指出，给定一个使用的单词的大样本，任何单词的频率都与其在频率表中的排名成反比。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/49c5fd5a58e05f9fa2bfb5327b949c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/0*zH49BOzls98O7lCO.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Zipf’s law</figcaption></figure><p id="84ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">TF-IDF 的主要目标是缩小语料库中信息量较少的术语的影响。在语料库中频繁出现但意义较小的术语，如(“this”、“will”)。我们需要降低常用词的权重，同时降低生僻词的权重。</p><p id="afbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">示例:</strong>假设我们有一个包含 1800 个单词的文档，该文档有 50 个出现术语“apple”的实例。</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="c122" class="nf lg it nb b gy ng nh l ni nj"><strong class="nb iu">TF = 50/1,800 = 0.027</strong></span></pre><p id="82fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，假设我们有另一个包含 1000 万个单词的文档，单词“apple”出现了 1250 次，那么 IDF 和 TF-IDF 将是:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="7947" class="nf lg it nb b gy ng nh l ni nj"><strong class="nb iu">IDF = log(10,000,000/1,250) = 3.9</strong></span><span id="fb5c" class="nf lg it nb b gy nm nh l ni nj"><strong class="nb iu">TF-IDF = 0.027 * 3.9 = 0.10</strong></span></pre><p id="a279" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以现在我们已经减轻了我们任期的分量。</p><p id="80ad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">c) <strong class="js iu"> word2vec </strong></p><p id="f748" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以说是自 2013 年以来最重要的单词嵌入模型。如果有两个单词 w1 和 w2，并且如果它们相似，那么向量 v1 和 v2 会更接近。Word2Vec 是使用浅层神经网络学习单词嵌入的最流行的技术之一。它是由谷歌的托马斯·米科洛夫于 2013 年开发的。它用于学习单词的向量表示，称为“单词嵌入”，并作为预处理步骤完成，之后学习的向量被输入到模型中，以生成预测并执行各种有趣的事情。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9ec5a5e64703f06bbcf4f77452761f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/0*34eLg1DnJo61oerz.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">word2vector</figcaption></figure><p id="9669" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Word2vec 构建于:</p><ol class=""><li id="06de" class="mk ml it js b jt ju jx jy kb mm kf mn kj mo kn mp mq mr ms bi translated">滔滔不绝的话</li><li id="08be" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">跳过克</li></ol><p id="0a2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">一.连续词汇袋(CBOW) </strong></p><p id="f290" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CBOW 的核心思想是给定的上下文词预测焦点词。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi no"><img src="../Images/6723ab29ee0286598bb55b1b38b60fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*tdE__stNOp5gxn96VjxsYw.jpeg"/></div></figure><p id="6009" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简单来说就是使用目标单词 ft 前后的 n 个单词进行预测。你也可以称之为多类分类器。我们的目标是在给定当前(聚焦)单词的情况下，找到对预测周围单词有用的单词表示。</p><p id="3a94" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们从神经网络的角度来理解 CBOW(最后😍)</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi np"><img src="../Images/1e08a761db700bb612f82108cffa004d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjuogMGuKD6JV5JLPkMAJg.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">CBOW (NN Intuition)</figcaption></figure><p id="64f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">二。跳过程序</strong></p><p id="b567" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Skip-gram 的工作方式与 CBOW 略有不同。我们可以说 skip-gram 是 CBOW 的反义词。skip-gram 背后的核心思想是给出一个聚焦的单词来查找上下文单词。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nq"><img src="../Images/2d75200a1dd8a3ad3747ec8c7c711b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wLV7u0av2DbNgBLq5s87Q.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Skip-gram (NN intuition)</figcaption></figure><p id="7cd7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Word2vec 提供了在(CBOW 和 skip-gram)之间进行选择的选项。这些参数是在模型训练期间提供的。可以选择使用 softmax 或分层 softmax 层。</p><p id="bb37" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们来计算时间复杂度</p><ul class=""><li id="1b6f" class="mk ml it js b jt ju jx jy kb mm kf mn kj mo kn nr mq mr ms bi translated">CBOW → 1 软最大值</li><li id="9a85" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn nr mq mr ms bi translated">Skip-gram → k softmax</li></ul><p id="15fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为 Skipgram 有 k 个 softmax，所以计算量更大。</p><p id="0090" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">利弊</strong></p><p id="c779" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CBOW:</p><blockquote class="ns nt nu"><p id="6fab" class="jq jr nv js b jt ju jv jw jx jy jz ka nw kc kd ke nx kg kh ki ny kk kl km kn im bi translated">训练速度更快</p><p id="7c3e" class="jq jr nv js b jt ju jv jw jx jy jz ka nw kc kd ke nx kg kh ki ny kk kl km kn im bi translated">对常用词更有效</p></blockquote><p id="2cb6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">跳过程序:</p><blockquote class="ns nt nu"><p id="9faa" class="jq jr nv js b jt ju jv jw jx jy jz ka nw kc kd ke nx kg kh ki ny kk kl km kn im bi translated"><em class="it">较慢/困难/复杂</em></p><p id="b26c" class="jq jr nv js b jt ju jv jw jx jy jz ka nw kc kd ke nx kg kh ki ny kk kl km kn im bi translated"><em class="it">可以很好地处理较小的数据</em></p><p id="4dec" class="jq jr nv js b jt ju jv jw jx jy jz ka nw kc kd ke nx kg kh ki ny kk kl km kn im bi translated"><em class="it">对不常用的单词更有效</em></p></blockquote><p id="ba01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">注意:</strong>如果上下文单词增加，维数 N 增加，如果 N 增加，word2vec 工作得更好。</p><p id="80dc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们计算重量</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nz"><img src="../Images/c4dc9171699744a26e4855372fa6e682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TiQ1vaTBbfqM0IZidLKrZg.jpeg"/></div></div></figure><p id="695a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2250 万的重量😧</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/165ca586fee666d61d265fcc5174ac3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/0*SyUUl45NMrPzRrGZ.gif"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk"><a class="ae ko" href="https://media.giphy.com/media/t9ctG5MZhyyU8/giphy.gif" rel="noopener ugc nofollow" target="_blank">Giphy</a></figcaption></figure><blockquote class="ns nt nu"><p id="d6d0" class="jq jr nv js b jt ju jv jw jx jy jz ka nw kc kd ke nx kg kh ki ny kk kl km kn im bi translated"><em class="it">训练 2250 万个重量需要叶… </em></p></blockquote><p id="aa19" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">等等！优化</strong>前来救援☺️</p><p id="0f34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">算法优化简介:</strong></p><p id="4e48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">分级 softmax </strong></p><p id="2fbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">传统的 softmax 方法在大型语料库上可能是昂贵的。因此，我们将使用分层的 softmax →修改的 softmax 使其达到最优。</p><p id="3ee2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">分层 softmax 是受(霍夫曼)二叉树(数据结构😋)是由 Morin 和 Bengio (2005)提出的。它本质上是用一个以单词为叶子的分层层来替换平面的 softmax 层。通过使用数据结构，我们最终能够轻松有效地解决问题。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/30effa46bd4809837b51777b82e28dc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*i4bFSf4NfWlMQ1wt8cpkhw.jpeg"/></div></figure><p id="36cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我有 8 个包含概率的激活单元(每个单词 wn)。所以 wn 的和将会是 1。我们的任务是找到概率最高的单词。</p><p id="db15" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我只需要 3 个激活单位就能得到我的单词 w3，而不是应用 8 个激活单位。因此，通过使用二叉树方法，我们的时间和计算复杂性大大降低。</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="5152" class="nf lg it nb b gy ng nh l ni nj"><strong class="nb iu">log base 2 (8) = 3</strong></span></pre><p id="55f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">阴性采样</strong></p><p id="1636" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">负采样采用训练样本，并且只修改一小部分权重，而不是所有权重。使用负采样，我们随机选择少量的词来更新权重。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ob"><img src="../Images/82f85168f701f882d074b3f840baec40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_D1QWLcSDAzc8DYGDO7K4g.jpeg"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Negative sampling</figcaption></figure><ol class=""><li id="26d2" class="mk ml it js b jt ju jx jy kb mm kf mn kj mo kn mp mq mr ms bi translated">始终保持目标词(在我们的例子中是 3)</li><li id="f526" class="mk ml it js b jt mt jx mu kb mv kf mw kj mx kn mp mq mr ms bi translated">在所有非目标词中，不要更新所有词，只需使用样本。对于采样，我们有一个如上所示的公式。</li></ol><p id="e998" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在下一篇文章中，我们将深入了解算法优化，如 softmax 函数、分层 softmax、负采样、噪声对比估计、快速文本、glove 和脸书的拼写错误遗忘(单词)嵌入(MOE)。</p><p id="0c7f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如有疑问，请通过<a class="ae ko" href="http://shubhamdeshmukh40@yahoo.in/" rel="noopener ugc nofollow" target="_blank">shubhamdeshmukh @<strong class="js iu">Yahoo . in</strong>T3】联系作者。如果你对数据科学/机器学习有热情，请随时在</a><a class="ae ko" href="https://www.linkedin.com/in/shubhamdeshmukh619/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上加我。</p></div></div>    
</body>
</html>