# ä½¿ç”¨ fastText çš„ Twitter æƒ…æ„Ÿåˆ†æ

> åŸæ–‡ï¼š<https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597?source=collection_archive---------6----------------------->

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¿«é€Ÿæ–‡æœ¬åº“æ¥åˆ†æå„ç§æ¨æ–‡çš„æƒ…ç»ªï¼Œè¯¥åº“æ˜“äºä½¿ç”¨å’Œå¿«é€Ÿè®­ç»ƒã€‚

![](img/0d22e011563ae0e3681bb4310df17f29.png)

Twitter sentiment analysis

# ä»€ä¹ˆæ˜¯ fastTextï¼Ÿ

FastText æ˜¯ç”±è„¸ä¹¦äººå·¥æ™ºèƒ½å¼€å‘çš„è‡ªç„¶è¯­è¨€å¤„ç†åº“ã€‚è¿™æ˜¯ä¸€ä¸ªå¼€æºã€å…è´¹ã€è½»é‡çº§çš„åº“ï¼Œå…è®¸ç”¨æˆ·å­¦ä¹ æ–‡æœ¬è¡¨ç¤ºå’Œæ–‡æœ¬åˆ†ç±»å™¨ã€‚å®ƒåœ¨æ ‡å‡†çš„é€šç”¨ç¡¬ä»¶ä¸Šå·¥ä½œã€‚æ¨¡å‹å¯ä»¥ç¼©å°å°ºå¯¸ï¼Œç”šè‡³é€‚åˆç§»åŠ¨è®¾å¤‡ã€‚

# ä¸ºä»€ä¹ˆé€‰æ‹© fastTextï¼Ÿ

æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸»è¦ç¼ºç‚¹æ˜¯å®ƒä»¬éœ€è¦å¤§é‡çš„æ—¶é—´æ¥è®­ç»ƒå’Œæµ‹è¯•ã€‚åœ¨è¿™é‡Œï¼ŒfastText æœ‰ä¸€ä¸ªä¼˜åŠ¿ï¼Œå› ä¸ºå®ƒåªéœ€è¦å¾ˆå°‘çš„æ—¶é—´æ¥è®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥åœ¨æˆ‘ä»¬çš„å®¶ç”¨ç”µè„‘ä¸Šé«˜é€Ÿè®­ç»ƒã€‚

æ ¹æ® fastText ä¸Šçš„[è„¸ä¹¦äººå·¥æ™ºèƒ½åšå®¢](https://research.fb.com/fasttext/)çš„è¯´æ³•ï¼Œè¿™ä¸ªåº“çš„å‡†ç¡®æ€§ä¸æ·±åº¦ç¥ç»ç½‘ç»œç›¸å½“ï¼Œå¹¶ä¸”åªéœ€è¦å¾ˆå°‘çš„æ—¶é—´æ¥è®­ç»ƒã€‚

![](img/e850ea29faffd15f2253108f5e19b65a.png)

comparison between fastText and other deep learning based models

ç°åœ¨ï¼Œæˆ‘ä»¬çŸ¥é“äº† fastText ä»¥åŠæˆ‘ä»¬ä¸ºä»€ä¹ˆä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨è¿™ä¸ªåº“è¿›è¡Œæƒ…æ„Ÿåˆ†æã€‚

# è·å–æ•°æ®é›†

æˆ‘ä»¬å°†ä½¿ç”¨ betsentiment.com çš„[ä¸Šå¯ç”¨çš„æ•°æ®é›†ã€‚æ¨æ–‡æœ‰å››ä¸ªæ ‡ç­¾ï¼Œåˆ†åˆ«æ˜¯æ­£å€¼ã€è´Ÿå€¼ã€ä¸­æ€§å’Œæ··åˆå‹ã€‚æˆ‘ä»¬ä¼šå¿½ç•¥æ‰€æœ‰å¸¦æœ‰æ··åˆæ ‡ç­¾çš„æ¨æ–‡ã€‚](https://betsentiment.com/resources/dataset/english-tweets)

æˆ‘ä»¬å°†ä½¿ç”¨å›¢é˜Ÿ tweet æ•°æ®é›†ä½œä¸ºè®­ç»ƒé›†ï¼Œè€Œçƒå‘˜æ•°æ®é›†ä½œä¸ºéªŒè¯é›†ã€‚

# æ¸…æ´—æ•°æ®é›†

æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œåœ¨è®­ç»ƒä»»ä½•æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ¸…ç†æ•°æ®ï¼Œåœ¨è¿™é‡Œä¹Ÿæ˜¯å¦‚æ­¤ã€‚

## æˆ‘ä»¬å°†æ ¹æ®è¿™äº›è§„åˆ™æ¸…ç†æ¨æ–‡:

1.  ç§»é™¤æ‰€æœ‰æ ‡ç­¾ï¼Œå› ä¸ºæ ‡ç­¾ä¸ä¼šå½±å“æƒ…ç»ªã€‚
2.  åˆ é™¤æåŠï¼Œå› ä¸ºå®ƒä»¬åœ¨æƒ…æ„Ÿåˆ†æä¸­ä¹Ÿä¸é‡è¦ã€‚
3.  å°†ä»»ä½•è¡¨æƒ…ç¬¦å·æ›¿æ¢ä¸ºå®ƒä»¬æ‰€ä»£è¡¨çš„æ–‡æœ¬ï¼Œä½œä¸ºè¡¨æƒ…ç¬¦å·æˆ–è¡¨æƒ…ç¬¦å·åœ¨ä»£è¡¨ä¸€ç§æƒ…ç»ªæ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚
4.  ç”¨å®Œæ•´çš„å½¢å¼ä»£æ›¿æ”¶ç¼©ã€‚
5.  åˆ é™¤æ¨æ–‡ä¸­å‡ºç°çš„ä»»ä½• URLï¼Œå› ä¸ºå®ƒä»¬åœ¨æƒ…æ„Ÿåˆ†æä¸­å¹¶ä¸é‡è¦ã€‚
6.  åˆ é™¤æ ‡ç‚¹ç¬¦å·ã€‚
7.  ä¿®å¤æ‹¼å†™é”™è¯¯çš„å•è¯(éå¸¸åŸºç¡€ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸è€—æ—¶çš„æ­¥éª¤)ã€‚
8.  å°†æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºå°å†™ã€‚
9.  åˆ é™¤ HTML æ ‡ç­¾(å¦‚æœæœ‰)ã€‚

## æ¸…ç†æ¨æ–‡çš„è§„åˆ™:

æˆ‘ä»¬å°†æ¸…ç†è¿™æ¡æ¨æ–‡

```
tweet = '<html> bayer leverkusen goalkeeeeper bernd leno will not be #going to napoli. his agent uli ferber to bild: "I can confirm that there were negotiations with napoli, which we have broken off. napoli is not an option." Atletico madrid and Arsenal are the other strong rumours. #b04 #afc </html>'
```

## åˆ é™¤ HTML æ ‡ç­¾

æœ‰æ—¶ twitter å“åº”åŒ…å« HTML æ ‡ç­¾ï¼Œæˆ‘ä»¬éœ€è¦åˆ é™¤å®ƒã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`[Beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)` [åŒ…](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)ã€‚

å¦‚æœæ²¡æœ‰ HTML æ ‡ç­¾ï¼Œé‚£ä¹ˆå®ƒå°†è¿”å›ç›¸åŒçš„æ–‡æœ¬ã€‚

```
tweet = BeautifulSoup(tweet).get_text()#output
'bayer leverkusen goalkeeeeper bernd leno will not be #going to napoli. his agent uli ferber to bild: "I can confirm that there were negotiations with napoli, which we have broken off. napoli is not an option." Atletico madrid and Arsenal are the other strong rumours. #b04 #afc'
```

æˆ‘ä»¬å°†ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥åŒ¹é…è¦åˆ é™¤æˆ–è¦æ›¿æ¢çš„è¡¨è¾¾å¼ã€‚ä¸ºæ­¤ï¼Œå°†ä½¿ç”¨`[re](https://docs.python.org/3/library/re.html)` [åŒ…](https://docs.python.org/3/library/re.html)ã€‚

## ç§»é™¤æ ‡ç­¾

Regex `@[A-Za-z0-9]+`ä»£è¡¨æåŠæ¬¡æ•°ï¼Œ`#[A-Za-z0-9]+`ä»£è¡¨æ ‡ç­¾ã€‚æˆ‘ä»¬å°†ç”¨ç©ºæ ¼æ›¿æ¢åŒ¹é…è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼çš„æ¯ä¸ªå•è¯ã€‚

```
tweet = ' '.join(re.sub("(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)", " ", tweet).split())#output
'bayer leverkusen goalkeeeeper bernd leno will not be to napoli. his agent uli ferber to bild: "I can confirm that there were negotiations with napoli, which we have broken off. napoli is not an option." Atletico madrid and Arsenal are the other strong rumours.'
```

## åˆ é™¤ URL

Regex `\w+:\/\/\S+`åŒ¹é…æ‰€æœ‰ä»¥ http://æˆ– https://å¼€å¤´å¹¶ç”¨ç©ºæ ¼æ›¿æ¢çš„ URLã€‚

```
tweet = ' '.join(re.sub("(\w+:\/\/\S+)", " ", tweet).split())#output
'bayer leverkusen goalkeeeeper bernd leno will not be to napoli. his agent uli ferber to bild: "I can confirm that there were negotiations with napoli, which we have broken off. napoli is not an option." Atletico madrid and Arsenal are the other strong rumours.'
```

## åˆ é™¤æ ‡ç‚¹ç¬¦å·

ç”¨ç©ºæ ¼æ›¿æ¢æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œå¦‚`.,!?:;-=`ã€‚

```
tweet = ' '.join(re.sub("[\.\,\!\?\:\;\-\=]", " ", tweet).split())#output 
'bayer leverkusen goalkeeeeper bernd leno will not be napoli his agent uli ferber to bild "I can confirm that there were negotiations with napoli which we have broken off napoli is not an option " Atletico madrid and Arsenal are the other strong rumours'
```

## å°å†™å­—æ¯ç›˜

ä¸ºäº†é¿å…å¤§å°å†™æ•æ„Ÿé—®é¢˜

```
tweet = tweet.lower()#output
'bayer leverkusen goalkeeeeper bernd leno will not be napoli his agent uli ferber to bild "i can confirm that there were negotiations with napoli which we have broken off napoli is not an option " atletico madrid and arsenal are the other strong rumours'
```

## æ›¿æ¢æ”¶ç¼©

å»æ‰ç¼©å†™ï¼Œç¿»è¯‘æˆåˆé€‚çš„ä¿šè¯­ã€‚æ²¡æœ‰é€šç”¨çš„åˆ—è¡¨æ¥ä»£æ›¿ç¼©å†™ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸ºäº†è‡ªå·±çš„ç›®çš„åˆ¶ä½œäº†è¿™ä¸ªåˆ—è¡¨ã€‚

```
CONTRACTIONS = {"mayn't":"may not", "may've":"may have",......}tweet = tweet.replace("â€™","'")
words = tweet.split()
reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]
tweet = " ".join(reformed)#input
'I maynâ€™t like you.'#output
'I may not like you.'
```

## ä¿®å¤æ‹¼å†™é”™è¯¯çš„å•è¯

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰æ„å»ºä»»ä½•å¤æ‚çš„å‡½æ•°æ¥çº æ­£æ‹¼å†™é”™è¯¯çš„å•è¯ï¼Œè€Œåªæ˜¯æ£€æŸ¥æ¯ä¸ªå­—ç¬¦åœ¨æ¯ä¸ªå•è¯ä¸­å‡ºç°çš„æ¬¡æ•°æ˜¯å¦ä¸è¶…è¿‡ 2 æ¬¡ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸åŸºæœ¬çš„æ‹¼å†™é”™è¯¯æ£€æŸ¥ã€‚

```
tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))#output
'bayer leverkusen goalkeeper bernd leno will not be napoli his agent uli ferber to bild "i can confirm that there were negotiations with napoli which we have broken off napoli is not an option " atletico madrid and arsenal are the other strong rumours'
```

## æ›¿æ¢è¡¨æƒ…ç¬¦å·æˆ–è¡¨æƒ…ç¬¦å·

ç”±äºè¡¨æƒ…ç¬¦å·å’Œè¡¨æƒ…ç¬¦å·åœ¨è¡¨è¾¾æƒ…æ„Ÿæ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ï¼Œæˆ‘ä»¬éœ€è¦ç”¨å®ƒä»¬åœ¨ç®€å•è‹±è¯­ä¸­æ‰€ä»£è¡¨çš„è¡¨è¾¾æ–¹å¼æ¥å–ä»£å®ƒä»¬ã€‚

å¯¹äºè¡¨æƒ…ç¬¦å·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`emoji`åŒ…ï¼Œå¯¹äºè¡¨æƒ…ç¬¦å·ï¼Œæˆ‘ä»¬å°†å»ºç«‹è‡ªå·±çš„å­—å…¸ã€‚

```
SMILEYS = {":â€‘(":"sad", ":â€‘)":"smiley", ....}words = tweet.split()
reformed = [SMILEY[word] if word in SMILEY else word for word in words]
tweet = " ".join(reformed)#input 
'I am :-('#output
'I am sad' 
```

## è¡¨æƒ…ç¬¦å·

è¡¨æƒ…åŒ…è¿”å›ç»™å®šè¡¨æƒ…çš„å€¼ä¸º`:flushed_face:`ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä»ç»™å®šçš„è¾“å‡ºä¸­åˆ é™¤`:`ã€‚

```
tweet = emoji.demojize(tweet)
tweet = tweet.replace(":"," ")
tweet = ' '.join(tweet.split())#input
'He is ğŸ˜³'#output
'He is flushed_face'
```

æ‰€ä»¥ï¼Œæˆ‘ä»¬å·²ç»æ¸…ç†äº†æˆ‘ä»¬çš„æ•°æ®ã€‚

# ä¸ºä»€ä¹ˆä¸ç”¨ NLTK åœç”¨è¯ï¼Ÿ

æ¸…é™¤æ•°æ®æ—¶ï¼Œåˆ é™¤åœç”¨è¯æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ–¹æ³•ã€‚å®ƒå»æ‰äº†æ‰€æœ‰æ— å…³ç´§è¦çš„è¯ï¼Œé€šå¸¸æ˜¯æ¯ä¸ªå¥å­ä¸­æœ€å¸¸ç”¨çš„è¯ã€‚è·å– NLTK åº“ä¸­å­˜åœ¨çš„æ‰€æœ‰åœç”¨è¯

```
from nltk.corpus import stopwords
stop_words = stopwords.words('english')
print(stop_words)
```

![](img/22565d0a334fc36444425c2635c16a40.png)

NLTK stop words

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœä½¿ç”¨ NLTK åœç”¨è¯ï¼Œé‚£ä¹ˆæ‰€æœ‰çš„è´Ÿé¢ç¼©å†™éƒ½å°†è¢«ç§»é™¤ï¼Œè¿™åœ¨æƒ…æ„Ÿåˆ†æä¸­èµ·ç€é‡è¦çš„ä½œç”¨ã€‚

# æ ¼å¼åŒ–æ•°æ®é›†

éœ€è¦æ ¼å¼åŒ– fastText ç›‘ç£å­¦ä¹ æ‰€éœ€çš„æ•°æ®ã€‚

[FastText](https://github.com/facebookresearch/fastText/blob/master/README.md#text-classification) å‡è®¾æ ‡ç­¾æ˜¯ä»¥å­—ç¬¦ä¸²`__label__`ä¸ºå‰ç¼€çš„å•è¯ã€‚

fastText æ¨¡å‹çš„è¾“å…¥åº”è¯¥å¦‚ä¸‹æ‰€ç¤º

```
__label__NEUTRAL _d i 'm just fine i have your fanbase angry over
__label__POSITIVE what a weekend of football results & hearts
```

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ–¹å¼æ ¼å¼åŒ–æ•°æ®

```
def transform_instance(row):
    cur_row = []
    #Prefix the index-ed label with __label__
    label = "__label__" + row[4]  
    cur_row.append(label)
    cur_row.extend(nltk.word_tokenize(tweet_cleaning_for_sentiment_analysis(row[2].lower())))
    return cur_rowdef preprocess(input_file, output_file):
    i=0
    with open(output_file, 'w') as csvoutfile:
        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\n')
        with open(input_file, 'r', newline='', encoding='latin1') as csvinfile: # encoding='latin1'
            csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='"')
            for row in csv_reader:
                if row[4]!="MIXED" and row[4].upper() in ['POSITIVE','NEGATIVE','NEUTRAL'] and row[2]!='':
                    row_output = transform_instance(row)
                    csv_writer.writerow(row_output )
                    # print(row_output)
                i=i+1
                if i%10000 ==0:
                    print(i)
```

è¿™é‡Œï¼Œæˆ‘ä»¬å¿½ç•¥æ ‡ç­¾ä¸æ˜¯`Positive, Negative and neutral`çš„æ¨æ–‡ã€‚

`nltk.[word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize)()`å°†å­—ç¬¦ä¸²è½¬æ¢æˆç‹¬ç«‹çš„å•è¯ã€‚

```
nltk.word_tokenize('hello world!')#output
['hello', 'world', '!']
```

# å¯¹æ•°æ®é›†è¿›è¡Œä¸Šé‡‡æ ·

åœ¨æˆ‘ä»¬çš„æ•°æ®é›†ä¸­ï¼Œæ•°æ®å¹¶æ²¡æœ‰è¢«å¹³å‡åˆ’åˆ†åˆ°ä¸åŒçš„æ ‡ç­¾ä¸­ã€‚å®ƒåŒ…å«ä¸­æ€§æ ‡ç­¾ä¸­å¤§çº¦ 72%çš„æ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¾€å¾€ä¼šè¢«å¤§ç­æ·¹æ²¡ï¼Œè€Œå¿½ç•¥å°ç­ã€‚

```
import pandas as pd
import seaborn as snsdf = pd.read_csv('betsentiment-EN-tweets-sentiment-teams.csv',encoding='latin1')df['sentiment'].value_counts(normalize=True)*100
```

![](img/c600787c67546ce30b613e1da942c53a.png)

percentage of tweets for each labels

```
sns.countplot(x="sentiment", data=df)
```

![](img/3ca937d09c41a326a315a3b5b8d28e96.png)

countplot for sentiment labels

ç”±äºä¸­æ€§ç±»ç”±æ•°æ®é›†çš„å¤§éƒ¨åˆ†ç»„æˆï¼Œè¯¥æ¨¡å‹å°†å§‹ç»ˆå°è¯•é¢„æµ‹ä¸­æ€§æ ‡ç­¾ï¼Œå› ä¸ºå®ƒå°†ä¿è¯ 72%çš„å‡†ç¡®æ€§ã€‚ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦æ¯ä¸ªæ ‡ç­¾æœ‰ç›¸åŒæ•°é‡çš„æ¨æ–‡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡å‘ minor ç±»æ·»åŠ æ–°çš„ tweets æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å‘å°‘æ•°æ—è£”æ ‡ç­¾æ·»åŠ æ–°æ¨æ–‡çš„è¿‡ç¨‹è¢«ç§°ä¸ºä¸Šé‡‡æ ·ã€‚

æˆ‘ä»¬å°†é€šè¿‡ä¸€æ¬¡åˆä¸€æ¬¡åœ°é‡å¤ç»™å®šæ ‡ç­¾ä¸­çš„ tweet æ¥å®ç°ä¸Šé‡‡æ ·ï¼Œç›´åˆ°æ¯ä¸ªæ ‡ç­¾ä¸­ tweet çš„æ•°é‡ç›¸ç­‰ã€‚

```
def upsampling(input_file, output_file, ratio_upsampling=1):
    # Create a file with equal number of tweets for each label
    #    input_file: path to file
    #    output_file: path to the output file
    #    ratio_upsampling: ratio of each minority classes vs majority one. 1 mean there will be as much of each class than there is for the majority class 

    i=0
    counts = {}
    dict_data_by_label = {}# GET LABEL LIST AND GET DATA PER LABEL
    with open(input_file, 'r', newline='') as csvinfile: 
        csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='"')
        for row in csv_reader:
            counts[row[0].split()[0]] = counts.get(row[0].split()[0], 0) + 1
            if not row[0].split()[0] in dict_data_by_label:
                dict_data_by_label[row[0].split()[0]]=[row[0]]
            else:
                dict_data_by_label[row[0].split()[0]].append(row[0])
            i=i+1
            if i%10000 ==0:
                print("read" + str(i))# FIND MAJORITY CLASS
    majority_class=""
    count_majority_class=0
    for item in dict_data_by_label:
        if len(dict_data_by_label[item])>count_majority_class:
            majority_class= item
            count_majority_class=len(dict_data_by_label[item])  

    # UPSAMPLE MINORITY CLASS
    data_upsampled=[]
    for item in dict_data_by_label:
        data_upsampled.extend(dict_data_by_label[item])
        if item != majority_class:
            items_added=0
            items_to_add = count_majority_class - len(dict_data_by_label[item])
            while items_added<items_to_add:
                data_upsampled.extend(dict_data_by_label[item][:max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))])
                items_added = items_added + max(0,min(items_to_add-items_added,len(dict_data_by_label[item])))# WRITE ALL
    i=0with open(output_file, 'w') as txtoutfile:
        for row in data_upsampled:
            txtoutfile.write(row+ '\n' )
            i=i+1
            if i%10000 ==0:
                print("writer" + str(i))
```

è‡³äºé‡å¤æ¨æ–‡ï¼Œä¸€æ¬¡åˆä¸€æ¬¡ï¼Œå¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬çš„æ¨¡å‹è¿‡åº¦é€‚åº”æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œä½†ç”±äºæˆ‘ä»¬çš„æ•°æ®é›†å¾ˆå¤§ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚

# åŸ¹å…»

å°è¯•ç”¨ [git å…‹éš†](https://github.com/facebookresearch/fastText/tree/master/python#building-fasttext)å®‰è£… fastTextï¼Œè€Œä¸æ˜¯ä½¿ç”¨ pipã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ç›‘ç£è®­ç»ƒæ³•ã€‚

```
hyper_params = {"lr": 0.01,
                "epoch": 20,
                "wordNgrams": 2,
                "dim": 20}     

        print(str(datetime.datetime.now()) + ' START=>' + str(hyper_params) )# Train the model.
        model = fastText.train_supervised(input=training_data_path, **hyper_params)
        print("Model trained with the hyperparameter \n {}".format(hyper_params))
```

`lr`ä»£è¡¨`learning rate`ï¼Œ`epoch`ä»£è¡¨`number of epoch`ï¼Œ`wordNgrams`ä»£è¡¨`max length of word Ngram`ï¼Œ`dim`ä»£è¡¨`size of word vectors`ã€‚

`train_supervised`æ˜¯ç”¨äºä½¿ç”¨ç›‘ç£å­¦ä¹ æ¥è®­ç»ƒæ¨¡å‹çš„å‡½æ•°ã€‚

# è¯„ä»·

æˆ‘ä»¬éœ€è¦è¯„ä¼°è¿™ä¸ªæ¨¡å‹ä»¥ç¡®å®šå®ƒçš„å‡†ç¡®æ€§ã€‚

```
model_acc_training_set = model.test(training_data_path)
model_acc_validation_set = model.test(validation_data_path)

# DISPLAY ACCURACY OF TRAINED MODEL
text_line = str(hyper_params) + ",accuracy:" + str(model_acc_training_set[1])  + ",validation:" + str(model_acc_validation_set[1]) + '\n' print(text_line)
```

æˆ‘ä»¬å°†åœ¨è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ä¸Šè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ã€‚

`test`è¿”å›æ¨¡å‹çš„ç²¾åº¦å’Œå¬å›ç‡ï¼Œè€Œä¸æ˜¯ç²¾åº¦ã€‚ä½†æ˜¯åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œä¸¤ä¸ªå€¼å‡ ä¹ç›¸åŒï¼Œæ‰€ä»¥æˆ‘ä»¬åªä½¿ç”¨ç²¾åº¦ã€‚

æ€»çš„æ¥è¯´ï¼Œè¯¥æ¨¡å‹å¯¹è®­ç»ƒæ•°æ®ç»™å‡ºäº† 97.5%çš„å‡†ç¡®åº¦ï¼Œå¯¹éªŒè¯æ•°æ®ç»™å‡ºäº† 79.7%çš„å‡†ç¡®åº¦ã€‚

# é¢„æµ‹

æˆ‘ä»¬å°†é¢„æµ‹ä¼ é€’ç»™æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹çš„æ–‡æœ¬çš„æƒ…æ„Ÿã€‚

```
model.predict(['why not'],k=3)
model.predict(['this player is so bad'],k=1)
```

`predict`è®©æˆ‘ä»¬é¢„æµ‹ä¼ é€’çš„å­—ç¬¦ä¸²çš„æƒ…æ„Ÿï¼Œè€Œ`k`ä»£è¡¨è¿”å›çš„å¸¦æœ‰ç½®ä¿¡åº¦å¾—åˆ†çš„æ ‡ç­¾çš„æ•°é‡ã€‚

# é‡åŒ–æ¨¡å‹

é‡åŒ–æœ‰åŠ©äºæˆ‘ä»¬é™ä½æ¨¡å‹çš„è§„æ¨¡ã€‚

```
model.quantize(input=training_data_path, qnorm=True, retrain=True, cutoff=100000)
```

# ä¿å­˜æ¨¡å‹

æˆ‘ä»¬å¯ä»¥ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œç„¶åéšæ—¶ä½¿ç”¨ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½è®­ç»ƒå®ƒã€‚

```
model.save_model(os.path.join(model_path,model_name + ".ftz"))
```

# ç»“è®º

æˆ‘ä»¬å­¦ä¹ å¦‚ä½•æ¸…ç†æ•°æ®ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™è®­ç»ƒæ¨¡å‹æ¥é¢„æµ‹æ¨æ–‡çš„æƒ…ç»ªã€‚æˆ‘ä»¬è¿˜å­¦ä¹ ä½¿ç”¨ fastText å®ç°æƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‚