<html>
<head>
<title>Distributed Data Pre-processing using Dask, Amazon ECS and Python (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Dask、Amazon ECS 和 Python 的分布式数据预处理(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-2-af14a1ac1b25?source=collection_archive---------18-----------------------#2019-01-03">https://towardsdatascience.com/serverless-distributed-data-pre-processing-using-dask-amazon-ecs-and-python-part-2-af14a1ac1b25?source=collection_archive---------18-----------------------#2019-01-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/fff9df5c87e22d46bf217058d9681962.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SgtOoMxxRjpp4Ivb0pvIvw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Source: <a class="ae jg" href="https://pixabay.com/en/organ-pipes-church-music-cathedral-1550156/" rel="noopener ugc nofollow" target="_blank">pixabay.com</a></figcaption></figure><div class=""/><div class=""><h2 id="483c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用 Dask 进行 EDA 和超参数优化(HPO)</h2></div><p id="d5dd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本系列的第 1 部分中，我解释了如何在<a class="ae jg" href="https://aws.amazon.com/fargate/" rel="noopener ugc nofollow" target="_blank"> AWS Fargate </a>上构建 Dask 调度器和工作器的无服务器集群。上下调整工人数量相当简单。您可以通过运行以下 AWS CLI 命令来实现这一点:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="1313" class="md me jj lz b gy mf mg l mh mi">bash~# aws ecs update-service — service Dask-Workers — desired-count 10 — cluster Fargate-Dask-Cluster &gt; /dev/null</span><span id="5c01" class="md me jj lz b gy mj mg l mh mi">bash~# aws ecs update-service — service Dask-Scheduler — desired-count 1 — cluster Fargate-Dask-Cluster &gt; /dev/null</span></pre><p id="540d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">既然我已经扩展了无服务器 Fargate 集群，让我们尝试一些探索性数据分析(EDA)。我使用的是著名的<a class="ae jg" href="http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml" rel="noopener ugc nofollow" target="_blank"> NY Yellow Taxi 2017 数据集。</a></p><p id="be12" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们将数据帧装入集群内存。为此，只需从 dask 库中导入 dataframe 类，然后使用<em class="mk"> read_csv() </em>函数加载文件:</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ml"><img src="../Images/0ab2737ba45bc4e3d667a8946af3c36b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3UU8-RJoMp-vQwY-1Gneg.png"/></div></div></figure><p id="065c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"><em class="mk">client . persist()</em></strong>函数的工作原理是将数据异步保存到内存中，然后立即返回。它将任务图提交给集群，并返回一个<em class="mk"> Future </em>对象。然后，我们可以在输出数据帧上运行快速查询。让我们进行一些分析:</p><h2 id="84a5" class="md me jj bd mm mn mo dn mp mq mr dp ms lh mt mu mv ll mw mx my lp mz na nb nc bi translated">1.1 数据框架分析</h2><p id="92f8" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">让我们在数据框架上得到一些描述性的统计数据。代码看起来应该完全一样，只是对于 Dask dataframes，您需要添加一个<strong class="la jk"> <em class="mk"> compute() </em> </strong>函数，以便在笔记本上立即获得结果。如您所见，在数据帧上进行一些复杂的计算需要大约 7 秒钟，例如:(计算分位数、平均值、计数、最小值、最大值和标准偏差)。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/9020eb71c5cce890cbc684f355f5549f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69ZrPb37Kcpv2Q9N5MbEhA.png"/></div></div></figure><h2 id="0106" class="md me jj bd mm mn mo dn mp mq mr dp ms lh mt mu mv ll mw mx my lp mz na nb nc bi translated">1.2 计算总行程距离，并为每个供应商计算:</h2><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/044a0684efac4e6d7974bdf2b6ed3190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RHwKuUtFFl6a4MOGtrFMuw.png"/></div></div></figure><p id="3d58" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完成这条<strong class="la jk"> <em class="mk"> groupby() </em> </strong>语句大概花了 5.5 秒。如您所见，供应商 2 的行程比供应商 1 多得多，行驶的距离也长得多。</p><h2 id="11d6" class="md me jj bd mm mn mo dn mp mq mr dp ms lh mt mu mv ll mw mx my lp mz na nb nc bi translated">1.3 计算每个特征的缺失值:</h2><p id="06ba" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">数据集中没有缺失值。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/ffec83f9241b2ac72748a1ab93d8e510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SufIGsO0laaw7QsKlGjeEg.png"/></div></div></figure><h2 id="ed1e" class="md me jj bd mm mn mo dn mp mq mr dp ms lh mt mu mv ll mw mx my lp mz na nb nc bi translated">1.4 显示特征之间的相关性:</h2><p id="82ed" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">有一些明显的相关性，如票价金额和行程距离、小费金额和票价金额之间的高度正相关。我需要移除高度相关的要素(total_amount)以避免类似于<a class="ae jg" href="https://en.wikipedia.org/wiki/Multicollinearity" rel="noopener ugc nofollow" target="_blank">多重共线性</a>问题<strong class="la jk">的问题。</strong></p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ml"><img src="../Images/c481620b95efc4bf6c26ce11412b25a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iHnS5Dti1d82KHYCEhHAA.png"/></div></div></figure><p id="4e39" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们再试一次<strong class="la jk"> 1.2 </strong>但是这一次，我将把数据保存在内存中，并让它在后台工作，同时我做一些其他的任务。我还将打印出进度条，以查看任务何时完成。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/e9767a394d6e56c83c934c418c7af680.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1bw6f7YdARlZ-E6bgniucw.gif"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Persist function progress bar</figcaption></figure><p id="3547" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，当使用集群工作线程在后台加载/计算数据时，提示符会立即返回。</p><p id="030f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当进度条完成后(全部为绿色)，可以快速查询数据的输出。请注意，当使用集群工作线程在后台加载/计算数据时，提示符会立即返回。</p><h1 id="a613" class="nm me jj bd mm nn no np mp nq nr ns ms kp nt kq mv ks nu kt my kv nv kw nb nw bi translated">使用 Dask 进行机器学习:</h1><p id="c5e1" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">机器学习中计算量最大的任务之一是超参数优化(HPO)。HPO 是一种用于调整在训练过程中没有学习到的 ML 参数<em class="mk">的技术，例如学习速率、优化器、正则化或神经网络中的隐藏层数。它的工作原理是为您要调优的每个超参数探索预定义范围的搜索空间。有许多库和技术可以实现 HPO 过程，但是在本文中，我将重点关注使用网格搜索技术和 Python 中的 Scikit-Learn 库来调优超参数。</em></p><h2 id="e890" class="md me jj bd mm mn mo dn mp mq mr dp ms lh mt mu mv ll mw mx my lp mz na nb nc bi translated">HPO 使用网格搜索:</h2><p id="4eed" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">网格搜索技术是一种<a class="ae jg" href="https://en.wikipedia.org/wiki/Brute-force_search" rel="noopener ugc nofollow" target="_blank">穷举搜索</a>，通过手动指定学习算法的超参数空间的子集。然后，该算法将遍历每个超参数的组合，旨在(最大化/最小化)客观度量(准确性/损失)。它最终会给出最好的结果，但是优化的超参数越多，优化过程就越复杂。</p><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/083c8bf98b27d8f036e63e7ca043424c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*v79u8Tmn0Udc4d--kshlQA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">2-dimensional Grid Search for HPO</figcaption></figure><h2 id="6f59" class="md me jj bd mm mn mo dn mp mq mr dp ms lh mt mu mv ll mw mx my lp mz na nb nc bi translated">HPO 在 Dask 集群上使用网格搜索:</h2><p id="2188" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">由于 HPO 过程计算量大，我们将在 Dask 集群上运行它，以便利用其规模和弹性。Scikit-learn 使用一个名为<a class="ae jg" href="https://github.com/joblib/joblib" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> joblib </strong> </a> <strong class="la jk"> </strong>的非常强大的库来跨多个 CPU 内核并行化进程。</p><p id="c93f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> Joblib </strong>也为其他并行系统提供接口，成为执行引擎。我们可以通过使用<code class="fe ny nz oa lz b">parallel_backend</code>上下文管理器在集群中运行数千个内核来实现这一点:</p><p id="1e9e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们需要从<strong class="la jk"> sklearn externals </strong>导入<strong class="la jk"> joblib </strong>，然后注册 Dask Distributed 作为 joblib 的并行后端引擎。</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="6c8b" class="md me jj lz b gy mf mg l mh mi">from sklearn.externals.joblib import _dask, parallel_backend<br/>from sklearn.utils import register_parallel_backend<br/>from sklearn.externals.joblib import parallel_backend</span><span id="2515" class="md me jj lz b gy mj mg l mh mi">register_parallel_backend('distributed',_dask.DaskDistributedBackend)</span></pre><p id="7962" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们需要运行下面一行来开始使用集群作为执行引擎:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="680d" class="md me jj lz b gy mf mg l mh mi">with parallel_backend('distributed', scheduler_host='dask-Scheduler.local-dask:8786'):<br/>     &lt;Normal sklearn Code&gt;</span></pre><p id="91da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后你的 sklearn 代码逻辑保持完全一样，没有变化。</p><p id="8a03" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是使用 HPO 网格搜索为随机森林分类器查找最佳超参数的完整代码，该分类器将对 MNIST 数据集中的手写数字进行分类:</p><pre class="lu lv lw lx gt ly lz ma mb aw mc bi"><span id="e60b" class="md me jj lz b gy mf mg l mh mi">from sklearn import datasets<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="8b9b" class="md me jj lz b gy mj mg l mh mi">import numpy as np<br/>from time import time</span><span id="b15c" class="md me jj lz b gy mj mg l mh mi">from sklearn.externals.joblib import _dask, parallel_backend<br/>from sklearn.utils import register_parallel_backend<br/>register_parallel_backend('distributed', _dask.DaskDistributedBackend)</span><span id="b0b5" class="md me jj lz b gy mj mg l mh mi"># Loading the Digits dataset<br/>digits = datasets.load_digits()</span><span id="ffd1" class="md me jj lz b gy mj mg l mh mi"># To apply an classifier on this data, we need to flatten the image, to<br/># turn the data in a (samples, feature) matrix:<br/>n_samples = len(digits.images)<br/>X = digits.images.reshape((n_samples, -1))<br/>y = digits.target</span><span id="98c5" class="md me jj lz b gy mj mg l mh mi"># Split the dataset in two equal parts<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)</span><span id="2ea2" class="md me jj lz b gy mj mg l mh mi">clf = RandomForestClassifier(n_estimators=20)</span><span id="468f" class="md me jj lz b gy mj mg l mh mi"># use a full grid over all parameters<br/>param_grid = {"max_depth": [3,4,5,6, None],<br/>              "max_features": [1, 3, 10, None],<br/>              "min_samples_split": [2, 3, 10],<br/>              "bootstrap": [True, False],<br/>              "criterion": ["gini", "entropy"]}</span><span id="eda0" class="md me jj lz b gy mj mg l mh mi"># run grid search<br/>grid_search = GridSearchCV(clf, param_grid=param_grid, cv=8, iid=True)</span><span id="96ce" class="md me jj lz b gy mj mg l mh mi">start = time()<br/>with parallel_backend('distributed', scheduler_host='dask-Scheduler.local-dask:8786'):<br/>    grid_search.fit(X, y)<br/>    clf.fit(X, y)</span><span id="3546" class="md me jj lz b gy mj mg l mh mi">print("GridSearchCV took %.2f seconds for %d candidate parameter settings."<br/>      % (time() - start, len(grid_search.cv_results_['params'])))</span><span id="4476" class="md me jj lz b gy mj mg l mh mi">results = grid_search.cv_results_<br/>    <br/># Return the index of the best validation score<br/>idx = np.flatnonzero(results['rank_test_score'] == 1 )<br/>print("The best score is: " + str(results['mean_test_score'][idx[0]]))<br/>                     <br/>#print the parameters for the best job      <br/>print("Parameters: {0}".format(results['params'][idx[0]]))</span></pre><figure class="lu lv lw lx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/5effce69d3d93d10f57c0ad0c31f61b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xCVMQ43UbLt3EfhfURqbDg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">The output of the above code</figcaption></figure><p id="cc11" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在一个 10 节点集群上，找到分类器的最佳超参数组合需要大约 40 秒，而在一台机器上(即使有多个内核/多个 CPU)，如果没有并行化特性，将需要许多分钟。</p><h1 id="20c0" class="nm me jj bd mm nn no np mp nq nr ns ms kp nt kq mv ks nu kt my kv nv kw nb nw bi translated">总结:</h1><p id="31ed" class="pw-post-body-paragraph ky kz jj la b lb nd kk ld le ne kn lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">根据我使用 Dask 的经验，它是一个很好的以分布式方式预处理大型数据集的库。如果你是熊猫和 Numpy 的粉丝，并且很难将你的数据存储到内存中，那么 Dask 绝对是一个不错的选择。对于时间和成本敏感的机器学习任务，如 HPO、数据插补、数据预处理和探索性分析，这绝对是一个很好的解决方案。</p></div></div>    
</body>
</html>