<html>
<head>
<title>Algorithmic bias: can the computer be racist?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">算法偏见:计算机可能是种族主义者吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithmic-bias-can-the-computer-be-racist-eb62ef91b045?source=collection_archive---------23-----------------------#2019-08-22">https://towardsdatascience.com/algorithmic-bias-can-the-computer-be-racist-eb62ef91b045?source=collection_archive---------23-----------------------#2019-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a884" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">是的，这是数据科学中的一个普遍问题。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/b87fbc3a3766db94bced05aacfa5cd7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*4GQWldaTlNAS8Ul1.jpg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">The machine learning algorithms behind predictive policing programs seem dangerously prone to bias. <a class="ae ku" href="https://www.smithsonianmag.com/innovation/artificial-intelligence-is-now-used-predict-crime-is-it-biased-180968337/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="f9de" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">近年来，算法偏见受到了越来越多的审视，特别是在 2016 年凯茜·奥尼尔的高超的<em class="lr">数学毁灭武器</em>出版之后(任何考虑从事数据科学职业的人都必须阅读)。随着越来越多的组织寻求利用复杂且经常不透明的机器学习模型来利用不断增长的数据池(一个常见的数字是每天 25 万亿字节的数据，尽管我找不到这个数字的好来源)，算法偏见的破坏性潜力除了增长之外什么也没有做。</p><p id="f60a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">令人沮丧的是，尽管在一些圈子里意识越来越强，但我们似乎还没有找到一种方法来处理奥尼尔在她的书中指出的那种问题。事实上，自从<em class="lr">数学毁灭武器</em>问世以来，她所强调的行业和用例几乎没有增长。早在 2016 年，奥尼尔就讨论过申请人跟踪系统的问题，该系统读取简历和其他申请人信息，并自动对求职者进行分类或取消其资格，但 ATS 系统仍然普遍存在(被<a class="ae ku" href="https://www.jobscan.co/blog/fortune-500-use-applicant-tracking-systems/" rel="noopener ugc nofollow" target="_blank">大多数大公司</a>使用)，ATS 市场预计将在<a class="ae ku" href="https://www.globenewswire.com/news-release/2019/07/22/1885923/0/en/Applicant-Tracking-System-ATS-Market-To-Reach-USD-2-34-Billion-By-2026-Reports-And-Data.html" rel="noopener ugc nofollow" target="_blank">增长</a>。她强调了所谓的健康计划的兴起带来的一些问题，雇主利用这些计划来尝试和激励健康的行为以降低医疗费用，但是健康计划仍然很受欢迎，尽管越来越多的证据表明它们可能不起作用。奥尼尔展示了像<a class="ae ku" href="https://en.wikipedia.org/wiki/CompStat" rel="noopener ugc nofollow" target="_blank"> CompStat </a>或<a class="ae ku" href="https://en.wikipedia.org/wiki/PredPol" rel="noopener ugc nofollow" target="_blank"> PredPol </a>这样的“热点”警务计划，表面上通过将资源集中在最有可能发生犯罪的地区来寻求改善警务，但实际上很容易对穷人和少数族裔社区产生偏见，并容易造成预测和执行的恶性、自我实现的循环，但这些计划仍在使用。具有讽刺意味的是，许多警察部门似乎正在重新考虑这些计划，不是因为这些计划具有高度歧视性，而是因为他们认为这些计划实际上并不太奏效。</p><p id="f64d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为什么我们只是非常缓慢地处理这些问题？首先，在某些情况下，非算法的人类选择本身就是歧视性的，不清楚算法是否没有对现状做出改善。是的，许多法院现在用来设定保释金和决定量刑的算法模型是有缺陷的，并且<a class="ae ku" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" rel="noopener ugc nofollow" target="_blank">明显对非裔美国人有偏见</a>，但是人类的选择也是有偏见的，并且经常如此反复无常和不均衡，以至于<a class="ae ku" href="https://www.nytimes.com/2017/12/20/upshot/algorithms-bail-criminal-justice-system.html" rel="noopener ugc nofollow" target="_blank">算法可能更好</a>。</p><p id="a340" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">另一个原因是一个看似普遍的概念，即算法和机器学习模型在某种程度上是天生中立或“公平”的。统计模型周围有一种科学的气氛——难道它们不是从数据中衍生出来的吗？当新闻中提到算法偏差的问题时，仍然很容易找到这样的回答:算法不会真的有偏差，因为它们是由数学驱动的<a class="ae ku" href="https://twitter.com/RealSaavedra/status/1087627739861897216" rel="noopener ugc nofollow" target="_blank"/>。认为算法仅仅是一点应用数学，因此不会有偏见，这是从根本上误解了什么是算法，对于任何处理数据的人来说，理解这一点很重要。</p><p id="0afb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">什么是算法？</strong></p><p id="f634" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">虽然算法肯定是数学工具，但它们不一定像定理那样被可靠地证明和明确。算法更像是一种策略，一套关于如何找到你认为不管输入是什么都行得通的答案的指导方针。例如，假设我洗了一副牌，然后让你把它放回新的牌组顺序中，所有的花色放在一起，a 到 k 排列。你将如何着手做那件事？在某种程度上，对于你需要做什么具体的动作，你并没有一个“答案”可以提前说明。你不能说“我把第 15 个位置的牌拿过来，移到第一个”这样的话，因为这副牌已经被洗牌了，你不知道每张牌在哪里。</p><p id="7a8b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">相反，你能做的是用算法来解决这个问题，并想出一系列适用于任何卡片配置的操作。第一步，也许，仔细检查每张牌，根据花色把它们分成四堆。第二步，挑一堆找到 a，然后是 2，以此类推。第 3 步，重复每一堆，等等。这一系列的步骤就是算法，一组指令，以一种通用的方式引导你找到解决方案。至关重要的是，就其本质而言，算法不一定是绝对正确的。我的卡片分类算法应该总是有效的，但是我不知道这是最有效的算法；也许有更好的方法来完成任务，通常会更快或需要更少的移动。更糟糕的是，有些问题，比如著名的<a class="ae ku" href="https://en.wikipedia.org/wiki/Travelling_salesman_problem" rel="noopener ugc nofollow" target="_blank">旅行推销员问题</a>，是出了名的难，并且对算法简化有抵抗力。</p><p id="ea86" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">洗牌可能看起来与我在这篇文章开始时提出的现实世界的后果有点不同，但是算法的原理是一样的。例如，考虑一家银行决定哪些申请人将获得抵押贷款。银行只想把钱借给那些愿意还钱的人，并且只借给任何一个有能力还钱的人。就像你不知道一副洗牌牌中卡片的确切顺序一样，每份贷款申请都是不同的，事先也不知道，所以对于“是否应该给这个申请人贷款，如果应该给多少钱？”这个问题，没有一个简单的答案</p><p id="0426" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">银行最终会根据自己的算法做出贷款决定。也许，第一步是剔除所有信用分数足够低的申请人，第二步可能是查看申请人的收入，等等。与纸牌的例子不同，银行的算法不能保证得到“正确”的答案。该银行最终可能会贷款给一些将会违约的候选人，而拒绝其他不会违约的候选人。在这一点上，应该清楚的是，成为算法的一部分并不能保证有效性或公平性，并且算法绝对可能有偏差；“不要借给非裔美国人”将是一个算法。</p><p id="7a38" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">一家银行拒绝贷款给非裔美国人显然是种族歧视，但我想强调的是，算法可能会以更加微妙但同样阴险的方式产生偏见。即使是在没有人为偏见的情况下创建的模型，也会因为各种原因而产生有偏差的结果，理解这些原因很重要。对“数据”的训练并不能保证中立，因为数据本身的收集方式可能会给结果带来偏见。</p><p id="50f3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">风险 1:数据收集本身就有偏差</strong></p><p id="765f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">模型的好坏取决于它所依据的数据，在现实世界中，数据源通常是不可靠的。从历史上看，收集数据既昂贵又耗时，因此很难获得完整的数据。当收集数据时，希望研究某个特定主题的人会试图获得最大的回报，这通常意味着去最容易收集数据的地方。这就是为什么如此多的心理学研究是在大学生身上进行的，即使他们不一定能代表更大的人口。对于大学校园里的研究人员来说，找到年轻的中上阶层白人学生进行测试比派人到全国各地寻找一系列科目更容易。</p><p id="2ac7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如今，研究人员可以廉价地利用在线或互联设备生成的大量数据，但覆盖范围仍然不普遍或不公正。例如，根据定义，互联设备产生的数据将有利于当前联网的人。在一个有趣的案例研究中，波士顿市发布了一款智能手机应用,司机可以在开车时使用它被动识别路面坑洼。该应用程序使用手机的加速度计来识别旅途中的颠簸，这些颠簸使用手机的 GPS 来绘制，帮助城市决定在哪里进行道路施工。这很好，但是结果并不完美。首先，在发布之时，智能手机在低收入群体中的渗透率仍然很低，在老年人中甚至更低。因此，该应用程序将倾向于在贫困地区或老年司机较多的地方进行<a class="ae ku" href="https://hbr.org/2013/04/the-hidden-biases-in-big-data" rel="noopener ugc nofollow" target="_blank">资源不足的道路工程</a>。</p><p id="0667" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">当模型被操作化并用于指导政策时，这个问题可以自我强化:想想像 CompStat 这样的犯罪热点预测器，它们对犯罪数据进行训练，但也影响警察的派遣。如果没有警察举报，许多轻微的“妨害治安罪”可能不会被举报。警察被模型派遣到特定的社区，并报告这些地区的游荡或乱穿马路的情况，但他们不会报告他们没有被派遣到的社区的同样的轻微犯罪。因此，在警察已经被派往的地方，更多的犯罪被报道，使得他们看起来更加充满犯罪，而选择派遣更多的警察到那里。</p><p id="7983" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">风险 2:使用的数据是不完美或有偏见的代理</strong></p><p id="f42d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">持怀疑态度的读者可能会正确地指出，我之前举的银行拒绝向黑人贷款的例子有点滑稽；毕竟，银行在贷款决策中使用种族因素是违法的。这是真的，但如果银行使用与种族密切相关的其他因素，那么即使没有明确使用“种族”作为考虑因素，也可能会得出相同的种族主义结果。考虑居住隔离——不同的社区可能有不同的人口统计数据，因此如果你知道一个人住在哪里，你可能不需要明确询问他们的种族。在许多城市，简单地问一个居民他们的邮政编码可能足以告诉他们的种族。一个至关重要的方面是，特别是随着更多的算法决策是由计算机自动生成的，你可以通过这种方式获得有偏见的结果，而不会有人做出明确的种族主义决定:没有考虑过住宅隔离的人认为社区似乎是一个合理的事情，可以包括在贷款申请中，并毫不犹豫地将信息输入计算机。后来计算机似乎拒绝向黑人贷款，但结果没有受到质疑，因为计算机不可能是种族主义者，对吗？</p><p id="f894" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">当一个特定的因素不能或不能被直接测量，数据科学家或研究人员被迫试图找到它的替代品时，这也是一个问题。例如，人类健康很难量化，因为它与许多不同的因素有关，并以许多不同的方式表现出来。很难获得一个人健康或身体状况的完整图片。另一方面，一个人的体重指数相对容易测量，因为它只需要知道这个人的身高和体重。然而，身体质量指数是众所周知的健康状况的原始代表。数据科学家经常倾向于使用糟糕的、可能有偏见的代理，因为他们是所有可用的，但是这些决策应该被仔细审查。</p><p id="fa3c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">风险 3:外部强加的规则</strong></p><p id="f581" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">模型或实际的实现有创造者强加给它们的规则或限制，因此，从定义上来说，带有模型制造者的目标/意识形态。如果创造者决定邮政编码<em class="lr">应该是</em>借贷算法中的一个重要因素，那么种族的代理就已经通过了那扇门。没有模型是在真空中创造的；有人总是在决定包括哪些因素，将哪些数据输入机器，以及一旦算法开始做出预测，如何操作算法。认为机器学习算法不受人类影响是错误的，因为它们是由计算机衍生的。</p><p id="9bb0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">风险四:成功条件有偏差</strong></p><p id="371b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">任何给定的算法或模型都有一个目标，它试图预测或做出决策的一些值。在每一种情况下，计算机都需要被训练什么是成功的预测，或者什么是“正确的”决策。监督学习模型需要良好标记的数据集，包括模型将寻求预测输入数据的目标变量。先前如何确定目标变量可能对基于该数据训练的模型有重大影响。考虑一家银行利用其过去的贷款历史——被红线标记——来训练一个决定谁被批准贷款的模型。或者简历阅读器根据先前的雇用决定通过申请人，这可能会对女性申请人不利。如果之前的借贷或雇佣决定是种族主义或性别歧视的，那么根据这些决定训练一个模型会给你一个有偏见的模型。</p><p id="0a7f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">还有一个持续反馈的相关问题。计算机模型不会改变，除非通过合并新规则或输入数据来明确改变。如果算法返回有偏差或不准确的结果，则需要干预。</p><p id="96d4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">数据工作者面临的挑战</strong></p><p id="048a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我想强调的是，这些偏见的来源不仅限于种族主义或性别歧视等重大社会问题，这些问题可能会在任何数据科学的应用中出现。每个数据科学家都需要应对算法偏差。结果可能是良性的，比如模型的准确性有一点损失，也可能是严重的，比如某人被累犯算法拒绝保释，但问题无处不在，随着机器学习模型的扩展，它们的偏差也随之扩展。</p></div></div>    
</body>
</html>