<html>
<head>
<title>Predicting Hospital Readmission with Deep Learning from Scratch and with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始利用深度学习和 Keras 预测医院再入院</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-hospital-readmission-with-deep-learning-from-scratch-and-with-keras-309efc0f75fc?source=collection_archive---------27-----------------------#2019-11-24">https://towardsdatascience.com/predicting-hospital-readmission-with-deep-learning-from-scratch-and-with-keras-309efc0f75fc?source=collection_archive---------27-----------------------#2019-11-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="32c6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们使用深度学习来识别有再次入院风险的患者！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c38bf2e115017230187b4a20c3c83a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGQFXABB1VKYHeAj5HtjWg.png"/></div></div></figure><h1 id="39d1" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">介绍</h1><p id="30df" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">最近，我看了我的朋友<a class="ae mf" href="https://www.linkedin.com/in/ericmjl/" rel="noopener ugc nofollow" target="_blank"> Eric Ma </a>关于深度学习基础的视频<a class="ae mf" href="https://www.youtube.com/watch?v=JPBz7-UCqRo" rel="noopener ugc nofollow" target="_blank"/>。为了教授深度学习，他将其分为 3 个关键部分:模型、损失函数和优化例程。在整个教程中，他使用了一个自动微分工具箱。然而，我发现自己做导数是非常令人满意的(至少对于简单的情况)。今天，我想按照 Eric 的方法从头开始构建一个 2 层神经网络，但使用代数导数(来自吴恩达的 Coursera <a class="ae mf" href="https://www.coursera.org/learn/neural-networks-deep-learning" rel="noopener ugc nofollow" target="_blank"> class </a>)，然后使用 Keras(一个深度学习框架)再次实现它。</p><h1 id="2501" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">数据集</h1><p id="d4ec" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">对于这个项目，我们将使用与我上一篇关于预测医院入院的文章相同的数据集，该数据集来自 UCI 的糖尿病医院数据集(<a class="ae mf" href="https://archive.ics.uci.edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/diabetes+130-us+hospitals+for+years+1999-2008</a>)。关于这个项目和功能工程的回顾，请参见我之前的帖子<a class="ae mf" rel="noopener" target="_blank" href="/predicting-hospital-readmission-for-patients-with-diabetes-using-scikit-learn-a2e359b15f0">https://towards data science . com/predicting-hospital-re-admission-for-patients-with-diabetes-using-scikit-learn-a 2e 359 b 15 f 0</a></p><h1 id="b643" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">项目定义</h1><p id="5636" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">预测糖尿病患者是否会在 30 天内再次入院。</p><h1 id="556b" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">特征工程</h1><p id="67a5" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们将开始这篇文章，就好像我们已经完成了我上一篇文章的特性工程部分，其中包括创建数字、分类(一键编码)和顺序特性。这些功能方便地保存在以前的笔记本中，并包含在我的<a class="ae mf" href="https://github.com/andrewwlong/diabetes_readmission_deep" rel="noopener ugc nofollow" target="_blank"> github repo </a>中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/fbd9332c07ecffc070ca08a413b526df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*78PkeXg1yZSC3Wgty8j8-Q.png"/></div></div></figure><p id="6c04" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">对于深度学习来说，重要的是填充缺失值并对数据进行归一化。我们将从 scikit-learn 中使用 SimpleImputer 和 StandardScaler 来做这件事。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mm"><img src="../Images/a6aedf13ccfef36b1fd77b998597e278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a7Jdqu_pMg1gi9hJPEx7wA.png"/></div></div></figure><h1 id="ccac" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">从头做起</h1><p id="a3a5" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">这里，我们将使用 Eric Ma 介绍的结构从头开始构建一个简单的两层神经网络:</p><ul class=""><li id="7f5c" class="mn mo iq ll b lm mh lp mi ls mp lw mq ma mr me ms mt mu mv bi translated">模型</li><li id="f991" class="mn mo iq ll b lm mw lp mx ls my lw mz ma na me ms mt mu mv bi translated">损失函数</li><li id="11e7" class="mn mo iq ll b lm mw lp mx ls my lw mz ma na me ms mt mu mv bi translated">优化程序</li></ul><p id="c38f" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我会尽量坚持吴恩达在他的 Coursera specialization(<a class="ae mf" href="https://www.coursera.org/specializations/deep-learning" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/specializations/deep-learning</a>)中介绍的符号。</p><h1 id="feed" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">模型</h1><p id="76cc" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们将使用的模型是一个双层神经网络，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/843b3b4791f22422ec56b103e34a1406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*K303fS90r_27eWv0V0dd2A.png"/></div></figure><p id="0814" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这里我们将有 n_x 个输入变量、n_1 个隐藏节点和一个包括 m 个样本的输出节点。对于这个模型，我们将对隐藏层节点和输出层中的激活函数使用逻辑回归。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/da4799ef00429ccefafab4aa9135f2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*RLnzNu4_DiUgRuuQYlCXIw.png"/></div></figure><p id="ef87" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这里，我们的激活函数将具有以下形式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e65fe939fe1811c6ab376373b508fdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*r_SKXnzVh_5MnyYrT_M6Dg.png"/></div></figure><p id="3f9e" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">为了更有效地计算，我们将使用矢量化记号。在这种表示法中，X 的第一列将是第一个样本的所有特征(注意，这与 Python 中此时加载的内容相反，因此我们需要转置 X 矩阵)。</p><p id="5648" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我们将用于该模型的参数将具有以下维度</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/83737823d68f1a6ca27d8bb43a403d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*sXmqdJ_UO_vsmZLc-Tk9Jw.png"/></div></figure><p id="606f" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">为了简单起见，让我们选择 n1 = 64 个节点来近似地将输入变量的数量减半。按照 Eric 的符号，让我们将所有这些参数保存在一个字典中。我们将随机初始化这些，因为设置为 0 将不起作用，因为节点与节点之间的权重都是相同的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/915fc49fdaef6f4f5eccc8a0d21f455e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*cGasVmuWA-e4YLjJxfC35w.png"/></div></figure><p id="1781" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">为了计算我们的<code class="fe ng nh ni nj b">m</code>示例中 y_hat 的估计值，我们可以使用下面的等式通过模型向前馈送信息(注意维度在花括号中)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/fac1b93219cd2edfbe831b1f61f3c3c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*z-zMQcDOJlnUd06tvWpqeA.png"/></div></figure><p id="235f" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我们可以用下面的前馈函数来编写代码。这里我们将隐藏层的激活函数作为函数的变量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/aa70bdd8e102cc4f357dd91b74d9615b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*myDfMfrA9G0zCaytH9JGFg.png"/></div></figure><h1 id="d756" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">损失函数</h1><p id="8e0b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在我们有了一个给定一些参数计算 y_hat 的方法，我们需要找到“最佳”参数。为了定义“最佳”，我们需要一个成本函数来定义参数有多好。我们用于二元分类的损失函数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/52429e22a65c4a51b50dc124ef1da1c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YdvWDvCz026yub_DUo9WFQ.png"/></div></div></figure><p id="447a" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">很明显这是从哪里来的，对吗？</p><p id="a2c2" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我更愿意看看这是从哪里来的，所以让我们绕一小段路，推导出这个方程。如果我们把我们模型的输出看作给定 x 的 y 的概率，我们可以为一个例子写如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d2bd6d4f1d3d5f6caebcf83de181caed.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*0pdS6pzTMDPw74e6IhMtEA.png"/></div></figure><p id="c70f" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">可以更巧妙的写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/bd6feaa7fb7b2c8bf383c8f82c5f2700.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*41m6BW4M6qLesHBZejvR-g.png"/></div></figure><p id="8de8" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">如果我们假设所有样本都是独立的，那么看到所有数据的可能性就是个体概率的乘积:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/4f1b8f4a362511f3ddc89fcde0d84686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*mB5y0ElREcuFB4J0cFY-EQ.png"/></div></figure><p id="9cb0" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">现在我们要做的就是找到最大化这种可能性的参数。这听起来很复杂，因为有产品术语。幸运的是，最大化似然函数的对数也最大化了似然函数(因为对数是单调增加的)。在我们这样做之前，让我们提醒自己关于日志的属性:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b1e1a2d067bc7f06fdab231e70440786.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*EbfHCXe0S1cqDZkIJsflag.png"/></div></figure><p id="f762" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">应用于我们的似然函数，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/52f9febd6f8374efc0a6ad72f781742e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WC_Ntt2MO3j03GrOOFzCA.png"/></div></div></figure><p id="e347" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这已经接近我们的成本函数了！唯一的区别是我们乘以-1，然后除以 m(样本数)。负乘法将它从最大化问题转换到最小化问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/227e70dfc9ab13fe6f210b3be22cbba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzsYJKkLzPx-Nvk4gQbrog.png"/></div></div></figure><p id="74fb" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">对于我们的优化程序，我们将需要这个成本函数<code class="fe ng nh ni nj b">J</code>的导数。Eric 用 python 包 jax.grad 完成了这个任务</p><p id="dd9a" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated"><code class="fe ng nh ni nj b">dlogistic_loss = grad(logistic_loss)</code></p><p id="514d" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">但是我想把它明确地写出来，以便更好地理解数学。</p><p id="ed1d" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">为了做这个导数，我们实际上在我们的神经网络中从右向左工作，这个过程被称为反向传播。我们可以用导数的基本原理做到这一点:链式法则！</p><p id="c868" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">在我们深入研究这个之前，让我们先来看一些我们将要用到的函数导数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/9c1c4d92faa04da9ad7cb538555c616f.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*6OjfpYcp2CTBaNstqrsJDQ.png"/></div></div></figure><p id="3589" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">和线性</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7a6ac0eaa9d3ec78e972dfd9e557c7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*rGX9xTzq_i62V3qpyX1RhQ.png"/></div></figure><p id="a19a" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">由于我们的成本是对每个样本求和的，现在让我们从计算中去掉每个样本的符号(和 1/m 乘数)。这里我们将使用吴恩达的简写符号和定义</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/2b80a0134210cf61bed6bf3bca362383.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*Z_4hLkcE5L0Qqmh8yL8nfQ.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e777b7661d9b79537d520c2f9b306d97.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*moSAI0271Zb-2WFMkjbhfg.png"/></div></figure><p id="a111" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">现在，我们可以对输出层中的参数求导(并适当考虑矩阵数学):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/43e23955e3c98ff1cf0efbd8b51c2e8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*het2SO_eirLlUKuStNz0ew.png"/></div></figure><p id="2a75" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">在网络中向后移动并考虑任何激活功能<code class="fe ng nh ni nj b">g^[layer](Z[layer])</code>。这里<code class="fe ng nh ni nj b">*</code>代表元素级乘法，因为我们对单个样本使用链规则，所以它开始起作用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/baeca18b3b8c46a74074c8f3460d7aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*NVGmhE_j-NU8_JG_TXRWaA.png"/></div></figure><p id="0c93" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">正如你在这里看到的，有一个清晰的模式，允许我们将其扩展到任意数量的隐藏层。</p><p id="34dc" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">现在让我们编写反向传播函数，它将参数、激活函数的导数函数、前馈值和输出值作为输入。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/2973ea75926d5e2f823994b47f477718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HG0Ol-K3olJoiOPUomIVzg.png"/></div></div></figure><p id="1655" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这里<code class="fe ng nh ni nj b">d_logistic</code>是逻辑函数相对于<code class="fe ng nh ni nj b">z</code>的导数。</p><p id="9df7" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">此时，最好验证我们的参数和 d_params 对于每个参数集具有相同的形状。</p><h1 id="cca8" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">优化程序</h1><p id="e3b5" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们将使用梯度下降来更新我们的参数。梯度下降基本上通过将梯度的相反方向移动学习量<code class="fe ng nh ni nj b">alpha</code>来迭代更新参数。我们可以在 for 循环中运行这个函数，并记录损失。注意 tqdmn 允许我们用一个进度条来查看进度(有点整洁，谢谢 Eric！).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/3ee76c5c9a3351a7806b26db3ab4969c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*-25RaIgtP3qbesUtSAkvaA.png"/></div></figure><p id="c7ed" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我们可以验证损耗随着迭代而减少:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f6e56b5072a45b30b8a71a3ffad93098.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*SdiaVUKdTcLmlrZRILCqOQ.png"/></div></figure><p id="8aab" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">然后，我们可以计算训练集和验证集的预测值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/266fc3bcaefd84a2cea297988088e04d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*H68FHmz_5AXGcvYBnjzshg.png"/></div></figure><p id="b4f7" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">使用 scikit-learn 指标，我们可以绘制 ROC 曲线:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/33cb959b51a19be7eb58999a1faaeffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*6538m3YF2YEVOHA_MzrziA.png"/></div></figure><p id="f88c" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这种临时模型的训练非常缓慢。让我们使用 Keras，它有额外的更有效的优化例程，如 Adam。Keras 也非常适合用几行代码构建更复杂的网络。</p><h1 id="ead8" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">克拉斯</h1><p id="d0bd" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">首先让我们导入一些包</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/74d3cd013b36072f10c7742e5f914fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*vWhYaSAKVT5KZLRVM9eCYg.png"/></div></figure><p id="c3ef" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我们需要稍微调整一下 Keras 的输出标签，使每个标签都有一列:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/12113f96a8ef383283a5e95c4c863fe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*UYyHVXOnvY2w47tmcICidg.png"/></div></figure><p id="94db" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">现在我们可以使用序列来构建我们的模型。在这里，我将使用 ReLu 激活函数，而不是逻辑函数，因为 ReLu 往往工作得更好。我还会添加辍学，这是一种正规化的形式，有助于减少过度拟合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/dc31da84d8f9af51dd31f3158f5627d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*tRJbruqBEapS-iSdRB3kYg.png"/></div></figure><p id="f66a" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这里，最终的输出层有两个节点(每个标签一个)。使用 softmax 函数将这些输出标准化，以将分数转换为概率。</p><p id="4815" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">然后，我们用指定的损失函数和优化器来编译模型。这里‘分类交叉熵’是上面定义的损失函数的公式。这也可以扩展到包括任意数量的结果</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/68598382707d2ce9258d6eb9e82d85f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*iLBX7mJgxWcJpmzCunWaIw.png"/></div></figure><p id="d97e" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">现在我们用这个模型</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b925f1b749f0dba48ca6b4274512eb5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*adzN3LcqnKmUQRpLvfdt3A.png"/></div></figure><p id="63b2" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">这里有两个输入参数<code class="fe ng nh ni nj b">batch_size</code>和<code class="fe ng nh ni nj b">epochs</code>包含在拟合中。批量大小表示每次迭代中使用的样本数量。在我们的临时实现中，我们在每次迭代中包含了所有的样本，这需要更多的时间来计算。如果您用较小的批处理运行计算，您就能够更快地迭代。Epoch 被定义为在将整个数据集分成更小的批次后迭代整个数据集的次数。</p><p id="e39a" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">与 scikit-learn 类似，我们使用 predict proba 获得预测，并只获取第二列:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi gj"><img src="../Images/fe48856149693a2f7cb3853d5f4fc4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OAJwgZhDKsKeRSLE4tVWGw.png"/></div></div></figure><p id="85aa" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">我在隐藏层、丢弃率和附加层中试验了不同数量的节点。最终模型的验证 AUC = 0.66，如以下 ROC 所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1f13be79c7ad18c2c36e400a3444a10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*avZzGkPBx6d_Jhmcjzo-dA.png"/></div></figure><p id="16b1" class="pw-post-body-paragraph lj lk iq ll b lm mh jr lo lp mi ju lr ls mj lu lv lw mk ly lz ma ml mc md me ij bi translated">不幸的是，这与我们在前一篇文章中训练的所有其他模型具有相同的性能！</p><h1 id="3218" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">结论</h1><p id="29ce" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">在这篇文章中，我们用 Keras 从头开始训练了一个 2 层神经网络。如果你有任何问题，请在下面评论。代码在我的 github 回购上:【https://github.com/andrewwlong/diabetes_readmission_deep T2】</p></div></div>    
</body>
</html>