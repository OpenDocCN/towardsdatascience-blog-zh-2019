<html>
<head>
<title>Teaching A Computer To Land On The Moon</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教电脑登陆月球</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/teaching-a-computer-to-land-on-the-moon-c168d551fc68?source=collection_archive---------25-----------------------#2019-11-09">https://towardsdatascience.com/teaching-a-computer-to-land-on-the-moon-c168d551fc68?source=collection_archive---------25-----------------------#2019-11-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/d13631fb48b3df76aaa8bfcd06ffcf0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*TfgisC1rNmMMHJgc.PNG"/></div></figure><p id="5687" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">去年我花了相当多的时间来了解机器学习领域的进展。现在可用的工具真的令人印象深刻——现在，利用可用的库，只需几行代码就可以实现复杂的神经网络。</p><p id="69ab" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我一直对机器学习复杂任务的想法很感兴趣，比如通过一遍又一遍地做它们来学习飞行，并看看什么有效，所以我选择了 OpenAI Gym 月球着陆器环境进行第一次实验。进行学习和控制的程序被称为代理。</p><p id="7958" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">代理学习解决问题的正确方法，而没有被提供许多解决的例子，这是无监督学习。这样做的一个方法是建立一个培训环境，奖励做得好的代理人，这样代理人就可以强化这种行为。这叫做强化学习。</p><p id="703b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">OpenAI 健身房提供了一个模拟不同问题的一致的训练环境——游戏、物理模拟等。它处理给出所需的奖励，以便代理可以学习。</p><p id="563a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">有一个很好的库可以做到这一点，叫做 Keras-RL，它和 OpenAI Gym 配合得非常好。</p><p id="948e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">下面显示了训练过程的视频，每隔一段时间采样一次。它从火箭坠毁时的有效随机发射发展到犹豫盘旋，再到平稳着陆。学习代理的代码如下。</p><figure class="kv kw kx ky gt ju"><div class="bz fp l di"><div class="kz la l"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">The training process</figcaption></figure><p id="d257" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">该计划是第一次训练，这可能需要几天时间，如果你没有使用 GPU 加速。然后保存训练模型的参数，并由测试程序加载，该测试程序演示学习的着陆技术。在云系统或 GPU 加速桌面上进行培训将大大加快这一过程。</p><p id="8a8c" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这些程序由 Keras-RL 演示代码修改而来。</p><pre class="kv kw kx ky gt lf lg lh li aw lj bi"><span id="f6d0" class="lk ll it lg b gy lm ln l lo lp">import numpy as np import gym from keras.models<br/>import Sequential from keras.layers <br/>import Dense, Activation, Flatten from keras.optimizers <br/>import Adam from keras.callbacks <br/>import EarlyStopping from rl.agents.dqn <br/>import DQNAgent from rl.policy <br/>import BoltzmannQPolicy from rl.policy <br/>import EpsGreedyQPolicy from rl.memory <br/>import SequentialMemory ENV_NAME = 'LunarLander-v2' </span><span id="8852" class="lk ll it lg b gy lq ln l lo lp">env = gym.make(ENV_NAME) <br/>env = gym.wrappers.Monitor(env, 'recordings12') <br/>np.random.seed() env.seed() <br/>nb_actions = env.action_space.n </span><span id="af96" class="lk ll it lg b gy lq ln l lo lp"># Next, we build a very simple model. <br/>model = Sequential() model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) </span><span id="2530" class="lk ll it lg b gy lq ln l lo lp">model.add(Dense(128)) model.add(Activation('relu')) model.add(Dense(64)) model.add(Activation('relu')) model.add(Dense(32)) model.add(Activation('relu')) model.add(Dense(nb_actions)) model.add(Activation('linear')) </span><span id="ed1d" class="lk ll it lg b gy lq ln l lo lp">print(model.summary())</span><span id="85b5" class="lk ll it lg b gy lq ln l lo lp"># configure and compile our agent. </span><span id="5808" class="lk ll it lg b gy lq ln l lo lp">memory = SequentialMemory(limit=1000000, window_length=1) <br/>policy = EpsGreedyQPolicy() <br/>earlystop = EarlyStopping(monitor = 'episode_reward', min_delta=.1, patience=5, verbose=1, mode='auto') <br/>callbacks = [earlystop] <br/>nb_steps_warmup = 1000 <br/>target_model_update = .2 <br/>gamma = .99 <br/>lr = .0001 <br/>training_steps = 4000000 <br/>epochs = training_steps/1000 decay = float(lr/epochs) <br/>dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=nb_steps_warmup, target_model_update = target_model_update, policy=policy, gamma = gamma)</span><span id="9b03" class="lk ll it lg b gy lq ln l lo lp">dqn.compile(Adam(lr=lr), metrics=['mae']) </span><span id="4f88" class="lk ll it lg b gy lq ln l lo lp"># Train model <br/>dqn.fit(env, nb_steps=training_steps, visualize=False, verbose=1) </span><span id="139d" class="lk ll it lg b gy lq ln l lo lp"># After training is done, we save the final weights. dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)</span></pre><p id="358b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><strong class="jz iu">测试程序</strong></p><pre class="kv kw kx ky gt lf lg lh li aw lj bi"><span id="9866" class="lk ll it lg b gy lm ln l lo lp">import numpy as np <br/>import gym from keras.models <br/>import Sequential from keras.layers <br/>import Dense, Activation, Flatten from keras.optimizers <br/>import Adam from rl.agents.dqn <br/>import DQNAgent from rl.policy <br/>import EpsGreedyQPolicy from rl.memory <br/>import SequentialMemory </span><span id="4142" class="lk ll it lg b gy lq ln l lo lp">ENV_NAME = 'LunarLander-v2' <br/>env = gym.make(ENV_NAME) <br/>env = gym.wrappers.Monitor(env, 'recordings2') <br/>np.random.seed() <br/>env.seed() <br/>nb_actions = env.action_space.n <br/>model = Sequential() model.add(Flatten(input_shape=(1,) + env.observation_space.shape)) </span><span id="272b" class="lk ll it lg b gy lq ln l lo lp">model.add(Dense(128)) <br/>model.add(Activation('relu')) <br/>model.add(Dense(64)) <br/>model.add(Activation('relu')) <br/>model.add(Dense(32)) <br/>model.add(Activation('relu')) <br/>model.add(Dense(nb_actions)) <br/>model.add(Activation('linear')) <br/>print(model.summary()) <br/>memory = SequentialMemory(limit=75000, window_length=1) <br/>policy = EpsGreedyQPolicy() <br/>dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10000, target_model_update=.2, policy=policy) </span><span id="a9cd" class="lk ll it lg b gy lq ln l lo lp">dqn.compile(Adam(lr=.0001), metrics=['mae']) dqn.load_weights('dqn_{}_weights.h5f'.format(ENV_NAME)) dqn.test(env, nb_episodes=10, visualize=True)</span></pre></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="b25b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="ly">最初发表于</em><a class="ae lz" href="https://shortcircuitsandinfiniteloops.blogspot.com/2019/08/teaching-computer-to-land-on-moon.html" rel="noopener ugc nofollow" target="_blank">T5【http://shortcircuitsandinfiniteloops.blogspot.com】</a><em class="ly">。</em></p></div></div>    
</body>
</html>