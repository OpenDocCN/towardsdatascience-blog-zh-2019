# 种族主义数据？人类偏见正在影响人工智能的发展

> 原文：<https://towardsdatascience.com/racist-data-human-bias-is-infecting-ai-development-8110c1ec50c?source=collection_archive---------7----------------------->

![](img/fe425087c90df00e61cbf4e824b179db.png)

机器学习算法处理大量数据，发现相关性、趋势和异常，其水平甚至远远超过最聪明的人类思维。但是，正如人类的智能依赖于准确的信息一样，机器也是如此。算法需要训练数据来学习。这些训练数据是由人类创建、选择、整理和注释的。这就是问题所在。

偏见是生活的一部分，这个星球上没有一个人可以摆脱它。当然，存在不同程度的偏见——从被熟悉事物吸引的倾向，到最强有力的种族主义形式。

这种偏见可以，而且经常会，在人工智能平台中找到自己的路。这完全是在雷达下发生的，没有工程师的共同努力。BDJ 采访了英特尔公司总裁杰森·彭博，他是一位领先的行业分析师，也是《敏捷架构革命》一书的作者，他谈到了偏见悄悄进入人工智能领域所面临的危险。

## **偏见无处不在**

当确定偏见给机器学习算法带来了多大的问题时，重要的是要专注于问题源于人工智能开发的特定领域。不幸的是，这是一个非常人为的问题。

> **“关于人类的数据集特别容易受到偏见的影响，而关于物理世界的数据则不那么容易受到影响。”**

“由于人类行为构成了人工智能研究的一大部分，偏见是一个重大问题，”杰森说。“关于人类的数据集特别容易受到偏见的影响，而关于物理世界的数据则不那么容易受到影响。”

微软注定失败的社交人工智能聊天机器人 Tay。Tay 向公众展示了人工智能成长和向周围人学习的潜力。她被设计成通过 Twitter 与人交流，随着时间的推移，展示出由这些对话塑造的发展中的个性。

![](img/cc6ff39d124fa2f18e6669ab574c94b2.png)

不幸的是，Tay 无法选择忽略别人对她说的那些更消极的方面。当用户发现这一点时，他们蜂拥而至。这引发了一连串种族主义和性别歧视的评论，泰像海绵一样吸收了这些评论。不久，她也表达了类似的观点，在活跃了仅仅 16 个小时后，[微软被迫让她下线](https://techcrunch.com/2016/03/24/microsoft-silences-its-new-a-i-bot-tay-after-twitter-users-teach-it-racism/)。

Tay 的案例研究是人工智能接受人类偏见的一个极端例子，但它突显了机器学习算法受输入数据支配的本质。

## **不是恶意的问题**

在人工智能开发中，偏见更是一个微妙的问题。这是一个与性别和种族相关的现有社会偏见可以感受到的问题。去年，苹果发现自己陷入了困境,因为用户注意到写下“CEO”这样的词会导致 iOS 默认提供“男商人”表情符号。虽然苹果使用的算法是严格保密的，但人工智能平台中类似的性别假设问题已经出现。

![](img/d49f788cb641164026f9f7d071f6d653.png)

有理论认为，这些偏见是因为用于训练人工智能的学习数据而产生的。这是一个被称为单词嵌入的机器学习概念的例子——查看像“首席执行官”和“消防员”这样的单词。

如果这些机器学习算法在这些文本数据集中找到更多类似“men”这样的词的例子，它们就会以此为参考框架，将这些位置与未来的男性联系起来。

> “人工智能中出现的偏见并不是故意和恶意将程序员的偏见注入他们的项目的自动迹象……这些人工智能程序只是反映了已经存在的示例偏见。"

在这一点上要做的一个重要区别是，出现在人工智能中的这种偏见并不是故意和恶意地将程序员的偏见注入到他们的项目中的自动迹象。如果有的话，这些人工智能程序只是反映了已经存在的例子偏见。即使人工智能使用大量数据进行训练，它仍然可以很容易地找到导致性别假设等问题的模式，因为包含这些链接词的发布材料范围很广。

在语言翻译方面，这个问题更加突出。[一个广为人知的例子](https://www.princeton.edu/news/2017/04/18/biased-bots-artificial-intelligence-systems-echo-human-prejudices)是谷歌翻译及其对土耳其语中性短语的解释。“医生”和“护士”这两个词是中性的，但是谷歌把“o bir doktor”和“o bir hemş ire”分别翻译成了“他是医生”和“她是护士”。

## **依靠错误的训练数据**

这种机器学习的单词嵌入模型可以突出现有社会偏见和文化假设的问题，这些问题有被公布的历史，但数据工程师也可以通过使用限制性数据集引入其他偏见途径。

2015 年，谷歌的另一个人工智能平台面部识别程序[将两名非洲裔美国人标记为“大猩猩”](https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/)。虽然这个错误很快得到纠正，但许多人将其归因于过度依赖人工智能训练数据中使用的白人面孔。在缺乏全面的不同肤色的人脸范围的情况下，该算法做出了这一剧烈的飞跃，产生了明显的进攻结果。

> “人为生成的数据是偏见的最大来源，例如，在调查结果、招聘模式、犯罪记录或其他人类行为中。”

不过，种族引发了更多令人担忧的例子，说明人工智能中存在偏见的危险。杰森指出:“人类生成的数据是偏见的最大来源，例如，在调查结果、招聘模式、犯罪记录或其他人类行为中。”

这里面有很多东西需要解开。一个主要的领域是美国法院和惩教系统使用人工智能的问题，以及越来越多的公开指责这些人工智能程序犯下种族偏见的例子。

一个名为 COMPAS 的人工智能程序已经被威斯康星法院用来预测罪犯再次犯罪的可能性。[ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)去年的一篇调查文章发现，这种风险评估系统对黑人囚犯有偏见，错误地将他们标记为比白人囚犯更有可能再次犯罪(分别为 45%和 24%)。这些预测导致被告被判更长的刑期，如威斯康辛诉卢米斯案。

![](img/c63af0574f1aaa9cbb1368d7b2beaad3.png)

有人呼吁将 COMPAS 和其他类似系统背后的算法变得更加透明，从而创建一个制衡系统，以防止种族偏见被这些人工智能系统用作法庭的认可工具。

这种透明度被许多人视为人工智能发展过程中必不可少的一环。随着像 COMPAS 这样的风险评估程序继续被开发，它们迎来了神经网络的到来，这是人工智能扩散链中的下一个环节。

神经网络使用深度学习算法，在进化过程中有机地创建连接。在这个阶段，人工智能程序变得更加难以筛选偏差的痕迹，因为它们没有运行一组严格的初始数据参数。

## **艾不招人待见**

Jason 强调雇佣模式是另一个易受偏见影响的人为数据的例子。

这是人工智能发展的一个领域，因其增加工作场所多样性或保持同质性的潜力而受到关注。越来越多的公司正在使用人工智能程序来帮助他们的招聘过程，但像科技这样的行业长期以来一直没有足够多样化的劳动力。

美国平等就业机会委员会的一份报告发现，科技公司中有很大一部分是白种人、亚洲人和男性，但拉丁美洲人和女性的比例远远不足。

> "使用历史上的限制性数据只会重复这些算法的问题."

“重点应该是创建无偏见的数据集和无偏见的人工智能算法，”Jason 说。人们必须认识到有偏见的数据，并积极寻求抵消它。这种认知需要训练。“对于利用人工智能进行招聘的公司来说，这是一个关键问题。使用历史上的限制性数据只会重复这些算法的问题。”

AI 产生偏差的原因也是它的解决方案——人。正如 Jason 指出的，数据算法是由训练它们的数据集创建的，因此使用有偏见的来源存在因果关系是很自然的。不幸的是，因为偏见往往如此微妙，所以需要专门的培训来消除它。

“IBM 和微软已经公开讨论了他们在抵制偏见方面的投资，但现在谈论他们或其他人会有多成功还为时过早，”Jason 指出。事实上， [IBM](https://www.research.ibm.com/5-in-5/ai-and-bias/) 和[微软](https://www.technologyreview.com/s/611138/microsoft-is-creating-an-oracle-for-catching-biased-ai-algorithms/)都公开承诺研究和解决他们自己的程序以及第三方程序中的偏见问题。

至关重要的是，为了让人工智能发展抵消偏见的危险，需要认识到这种技术不是绝对可靠的。“有偏见的数据导致有偏见的结果，即使我们可能倾向于相信人工智能的结果，因为它是人工智能。因此，主要的危险是把我们的信仰放在它不属于的地方，”杰森说。

随着人工智能展示基于种族的不公正和进一步限制招聘过程的广泛报道，这些可以作为足够的爆发点，很容易引起公众对这件事的关注。希望这能转化为解决这个问题的进一步研究和资源。

## **泰的多灾多难第二次发布**

![](img/409169a7cd9bdbd33c2e8b797d4f209c.png)

在微软的人工智能聊天机器人 Tay 非常公开的 16 个小时的起伏之后，它的开发者回到了绘图板。不幸的是，在她准备发布之前，微软的某人意外地再次激活了她的 Twitter。提示可怜的老泰关于“在警察面前抽大麻！”

她很快再次离线，但这引发了许多人对“杀死”一个正在学习的人工智能程序的道德问题的辩论。对一些人来说，虽然泰的评论是冒犯性的，但她代表了一种所谓感觉的新概念。微软已经宣布，他们打算再次向公众发布 Tay，当他们解决了这些问题，包括如此迅速地将这种程度的偏见注入她的“个性”中。如果她从中得到暗示的人能不再那么糟糕，那也会有所帮助。

[](https://twitter.com/BD_JohnM)**是本文最初发表地* [*二元区*](https://journal.binarydistrict.com/) *专注于机器学习的科技记者。**