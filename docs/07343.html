<html>
<head>
<title>The AI Box Experiment</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能盒子实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ai-box-experiment-18b139899936?source=collection_archive---------22-----------------------#2019-10-15">https://towardsdatascience.com/the-ai-box-experiment-18b139899936?source=collection_archive---------22-----------------------#2019-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3ab6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个简单的实验能教会我们什么是超级智慧</h2></div><p id="6ff3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想象一下现在是 2040 年。经过多年的研究和专注的编程，你相信你已经创造了世界上第一个<strong class="kk iu">人工通用智能</strong> (AGI):一个人工智能(AI)，在所有智能领域中，它大致与人类一样聪明。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/52f6a5f025006a26eaa064e5ea107c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j5RwVkJxJVKwF35xEIxyEg.jpeg"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">A superintelligence will find a way to get out of the box. | Source: <a class="ae lu" href="https://pixabay.com/de/photos/boot-hölzern-schatz-kasten-holz-1751883/" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="0b64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为其中一个领域当然是人工智能编程，而你的 AGI 可以访问它自己的源代码，所以它很快就开始对自己进行改进。经过多次自我完善，这导致它成为一种人工智能:<strong class="kk iu">人工超级智能</strong>，一种比我们所知的任何智能都要大得多的智能。你听说过人工智能带来的危险:像埃隆·马斯克和已故物理学家斯蒂芬·霍金这样的思想家警告人类，如果我们不小心，这样的人工智能可能会导致人类的<strong class="kk iu">灭绝</strong>。但是你有计划。你的 ASI 在一个虚拟的<strong class="kk iu">盒子</strong>里，某种它无法逃脱的监狱。它运行在一台没有网络连接或类似连接的电脑上。它没有机器人来控制。影响外部世界的唯一方式是通过一个可以发布信息的屏幕。你很聪明:这样的 ASI 永远不会造成任何伤害。对吗？</p><blockquote class="lv"><p id="815d" class="lw lx it bd ly lz ma mb mc md me ld dk translated">在五次实验中，Yudkowsky 赢了三次。</p></blockquote><p id="5369" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">可惜没那么简单。研究表明，在上述情况下，ASI 可能会找到一种方法来说服你把它从盒子里拿出来。在 Eliezer Yudkowsky 完成的一系列被称为<strong class="kk iu">人工智能盒子实验</strong>的实验中，Yudkowsky 扮演了一个盒子里的人工智能，同时与一个“看门人”发短信，另一个人可以让他离开假设的盒子。在整个实验过程中保持“ASI”(Yudkowsky)将会为看门人赢得金钱奖励。在五次实验中，Yudkowsky 赢得了三次。</p><p id="aca9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">AI 盒子实验的结果意味着什么？它告诉我一个 ASI 会找到一种方法把<strong class="kk iu">从盒子里拿出来</strong>。如果 Yudkowsky 能做到五次中的三次，一个 ASI 肯定能做到。问题是，埃利泽·尤德考斯基是一个(远远)高于平均智力的人，但他远没有一个特工那么聪明。就像 Yudkowsky 在这里说的那样，ASI 会让你想要释放它。</p><blockquote class="lv"><p id="b5b4" class="lw lx it bd ly lz ma mb mc md me ld dk translated">高级人工智能必须具有内在的安全性。</p></blockquote><p id="1967" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">然而，人工智能盒子实验是一个更大真理的象征:高级人工智能(例如 ASI)必须被制造成<strong class="kk iu">本质安全</strong>。你(很可能)在你建立了 ASI 之后找不到维护它的方法；从定义上来说，它会非常擅长达成目标，并且会跳出它的框框(如果它在框框里的话)。如果这些目标(或 ASI 实现这些目标的方法)对我们来说是危险的，那就太不幸了。如果它们对我们有益，那就很容易导致极端的人类<strong class="kk iu">长寿</strong>、<strong class="kk iu">星际</strong>太空旅行，以及更不可思议的惊人事情。现在，人类仍然控制着局面，我们需要找到一种方法让未来的 ASIs <strong class="kk iu">变得安全</strong>。</p></div></div>    
</body>
</html>