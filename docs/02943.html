<html>
<head>
<title>An Introduction to Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-gradient-descent-c9cca5739307?source=collection_archive---------6-----------------------#2019-05-13">https://towardsdatascience.com/an-introduction-to-gradient-descent-c9cca5739307?source=collection_archive---------6-----------------------#2019-05-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b963" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习和深度学习之旅</h2><div class=""/><div class=""><h2 id="c3e9" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从理论到实践，在数据集上实现批量梯度下降、小批量梯度下降和随机梯度下降</h2></div><p id="2616" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本博客将涵盖以下问题和主题:</p><p id="b3e2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">1.什么是渐变？</p><p id="b118" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.什么是梯度下降？</p><p id="9be3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">3.三种常见的梯度下降</p><p id="3bc6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">4.用 Python 实现</p><p id="e9b0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 1。梯度</strong></p><p id="e4ac" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">梯度是一个向量，它与一个函数相切，并指向该函数最大增长的方向。梯度在局部最大值或最小值处为零，因为没有单一的增加方向。在数学中，梯度被定义为函数的每个输入变量的偏导数。例如，我们有一个函数:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/666f1bc6cf176bc8419f43dfb0b13be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/1*TFHYWUKSz8QmKv3ifuwz9w.gif"/></div></figure><p id="152a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">函数的图形如下所示，我们可以看到函数的最小值是(0，0)。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/95dea52ccd710f48a1f3603a56881102.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*KqFfkdaFjWP7GXD_OaFdLA.png"/></div></figure><p id="3c5b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这种情况下，梯度的 x 分量是相对于 x 的偏导数，梯度的 y 分量是相对于 y 的偏导数。上述函数的梯度为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/8cc7c6f28eb7f15b0888c02876a7e6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/1*9uWfEhE9yQ6IZDWky36DRQ.gif"/></div></figure><p id="59f0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果我们想找到在点(1，2)处增加函数最多的方向，我们可以将(1，2)代入上面的公式，得到:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/c1e14db1e68de8461354ae43b514b343.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/1*0QEqLufSCTN7Ryfn5E1pnA.gif"/></div></figure><p id="1491" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 2。梯度下降</strong></p><p id="cce9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">由于梯度是指向函数最大增量的向量，负梯度是指向函数最大减量的向量。因此，我们可以通过向负梯度方向迭代移动一点来最小化一个函数。这就是梯度下降的逻辑。</p><p id="a87e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">给定一个起点:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/a19f46c825aaf9361cfb72176eb441f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/1*DtAAzbn8p4YTb4GbrBfsSw.gif"/></div></figure><p id="35e7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们可以构建一个迭代过程:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/e8b90071766d8b5de2073f4439386f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/1*k_VEFf_b6Ml18EZ7xYNIoA.gif"/></div></figure><p id="c4d1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi">…</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/0b67e34d52338543178c0b678b4b1229.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/1*l9I18_wo09VET-civSsCow.gif"/></div></figure><p id="e289" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上式中的参数α被称为学习率，它在大多数情况下是一个小常数，范围从 0 到 1。迭代过程不会停止，直到它收敛。以前面的例子为例，我们已经知道梯度是:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/d7f2865b2b5bdf145817637ef061af81.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*ey_SJ7QzcPB-LfWAkwij3w.png"/></div></figure><p id="27a7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，梯度下降的迭代过程可以写成:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/7a5e5f6d1a498bec425829b98cd21b9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/1*8ZSGHnQVw0wdvxZ_jzv7tw.gif"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/bb819e04f8bc9d84aa2a9d0dddc88a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/1*kQaohqJYbx5gY2tL9ottVg.gif"/></div></figure><p id="0052" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">那么我们可以得到:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi md"><img src="../Images/04e222b43d2f36464edac7a7bb55a502.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/1*TUI6EO8gH_Le8eMKx6fcIA.gif"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi me"><img src="../Images/3a02a23371bab882b650eb632192279c.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/1*c2DFA8ONhJ__W7paRz0zjw.gif"/></div></figure><p id="c17c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，假设α小于 1，那么我们可以得到:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/ae9007d57fb791e77f6b3007c4357166.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/1*APDRZikSe1uy94A8fnjw1Q.gif"/></div></figure><p id="9229" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">结论与我们在上图中观察到的一样。</p><p id="24ba" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">解释梯度下降的另一种直观方式是在函数的上下文中考虑下面的三维图形。我们的目标是从右上角的山移动到左下角的深蓝色的海。箭头代表从任何给定点开始的最陡下降方向(负梯度)——尽可能快地降低函数的方向。”[3]</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/abd05e11d752d8366a9efc10329f43af.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*v7Yx8sBsj0ywYPjiZUk-Cw.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image Source: <a class="ae ml" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html</a></figcaption></figure><p id="e92e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在机器学习中，我们更关注代价函数和参数之间的关系，而不是因变量和自变量。机器学习中梯度下降的一般思想是迭代地调整参数，以便最小化成本函数。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/d13ee43f51e5adf1c9b6afefc2bfa6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image Source: <a class="ae ml" href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/</a></figcaption></figure><p id="2257" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在某些情况下，我们可以应用封闭形式的方程来直接计算最适合模型训练数据集的参数。例如，为了最小化线性回归的 MSE，参数可以写成:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f88a2bb767be178dd34070bfa7a41a81.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/1*tHcgXTv1m4wxauq27U5vgA.gif"/></div></figure><p id="cb2b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，在其他情况下，我们没有封闭形式的方程，如逻辑回归。因此，应用类似梯度下降的迭代优化方法。</p><p id="bf30" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">梯度下降中的一个重要参数是学习率，它决定了每一步的大小。当学习率太大时，梯度下降可能会跳过山谷，到达另一边。这将导致成本函数发散。另一方面，当学习率太小时，算法需要很长时间才能收敛。因此，在梯度下降开始之前，需要适当的学习速率。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/2f7c6f9221a7c4de4822d678874d5e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hGhRddOUV8h0pdQek8T35A.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image Source: <a class="ae ml" rel="noopener" target="_blank" href="/gradient-descent-in-a-nutshell-eaf8c18212f0">https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0</a></figcaption></figure><p id="e199" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">归一化对梯度下降起着重要作用。如果特征没有被归一化，则具有大规模的特征将在更新中占主导地位，因此该算法将生成之字形学习路径。要达到最小值需要很多不必要的步骤和更长的时间。在所有特征被归一化之后，成本函数是更接近球形的形状。梯度下降算法直接走向最小值。执行标准化的一种方法是减去平均值并除以标准偏差。也可以直接在 Scikit-Learn 中应用 StandardScaler 函数。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mt"><img src="../Images/abd6984cdabf454054be789e0fc2c472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1nCPybEDj7gcdi07y9ULfA.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image Source: <a class="ae ml" href="https://www.jeremyjordan.me/batch-normalization/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/batch-normalization/</a></figcaption></figure><p id="37ca" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 3。三种典型的梯度下降</strong></p><p id="0dd5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们将研究在机器学习中广泛使用的梯度下降的几种变体:批量梯度下降、小批量梯度下降和随机梯度下降。</p><p id="bc9f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">批量梯度下降</strong></p><p id="57f3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">批量梯度下降在每一步都使用整批训练数据。它计算每条记录的误差，并取平均值来确定梯度。分批梯度下降法的优点是计算效率更高，产生稳定的学习路径，更容易收敛。但是，当训练集很大时，批量梯度下降需要更长的时间。</p><p id="31fe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">随机梯度下降</strong></p><p id="958b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在另一种极端情况下，随机梯度下降法在每一步只从训练集中选取一个实例，并只根据该单个记录更新梯度。随机梯度下降的优点是算法在每次迭代时都快得多，这弥补了批量梯度下降的局限性。然而，与批量梯度下降相比，该算法产生的学习路径不太规则和稳定。成本函数不是平滑递减，而是上下跳动。经过多轮迭代后，算法可能会找到一个好的参数，但最终结果不一定是全局最优的。</p><p id="d23d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">小批量梯度下降</strong></p><p id="8da0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">小批量梯度下降结合了批量和随机梯度下降的概念。在每一步，该算法基于训练集的子集而不是完整数据集或仅一个记录来计算梯度。小批量梯度下降法的优点是算法在计算过程中可以利用矩阵运算，并且代价函数比随机梯度下降法下降得更平稳。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mu"><img src="../Images/d4e884b0b4c8fc9a29f1e1d28af669a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3L2t1Da4M3ztbB0I1Torhw.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image Source: <a class="ae ml" rel="noopener" target="_blank" href="/gradient-descent-algorithm-and-its-variants-10f652806a3">https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3</a></figcaption></figure><p id="e618" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 4。用 Python 实现</strong></p><p id="9192" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这一部分中，我将使用著名的数据集 iris 来展示梯度体面如何在逻辑回归中工作。</p><p id="d02a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，导入包。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="ad95" class="na nb it mw b gy nc nd l ne nf"><em class="ng">from sklearn import datasets<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import matplotlib.lines as mlines</em></span></pre><p id="8aec" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">接下来，加载数据。注意，为了简单起见，我只选择 2 种虹膜。</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="ac1c" class="na nb it mw b gy nc nd l ne nf"># Load data<br/>iris = datasets.load_iris()<br/>X=iris.data[0:99,:2]<br/>y=iris.target[0:99]</span><span id="9c2a" class="na nb it mw b gy nh nd l ne nf"># Plot the training points<br/>x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5<br/>y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5</span><span id="d9f0" class="na nb it mw b gy nh nd l ne nf">plt.figure(2, figsize=(8, 6))<br/>plt.clf()</span><span id="a2a5" class="na nb it mw b gy nh nd l ne nf">plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,edgecolor='k')<br/>plt.xlabel('Sepal length')<br/>plt.ylabel('Sepal width')<br/>plt.xlim(x_min, x_max)<br/>plt.ylim(y_min, y_max)</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ni"><img src="../Images/8e634e88cf1574fcefb1bdf98bdccdd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*havYaKrIaHrx960Lhx6B6w.png"/></div></div></figure><p id="c431" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">批量梯度下降</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="d01a" class="na nb it mw b gy nc nd l ne nf"># Function for batch gradient decent    <br/>def Batch_GD (Learning_Rate,num_iterations,X,y):<br/>    #Step 1: Initial Parameter<br/>    N=len(X)<br/>    w=np.zeros((X.shape[1],1))<br/>    b=0<br/>    costs=[]<br/>    # Starting Loop<br/>    for i in range(num_iterations):<br/>        #Step 2: Apply Sigmoid Function and get y prediction<br/>        Z=np.dot(w.T,X.T)+b<br/>        y_pred=1/(1+1/np.exp(Z))<br/>        <br/>        #Step 3: Calculate Loss Function<br/>        cost=-(1/N)*np.sum(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))<br/>        <br/>        #Step 4: Calculate Gradient<br/>        dw=1/N*np.dot(X.T,(y_pred-y).T)<br/>        db=1/N*np.sum(y_pred-y)<br/>        <br/>        #Step 5: Update w &amp; b<br/>        w = w - Learning_Rate * dw<br/>        b = b - Learning_Rate * db<br/>        <br/>        # Records cost<br/>        if i % 1000 == 0:<br/>            costs.append(cost)<br/>            #print(cost)<br/>    return(w,b,costs)</span><span id="b9ab" class="na nb it mw b gy nh nd l ne nf"># Run a function<br/>Result_BatchGD=Batch_GD(Learning_Rate=0.01,num_iterations=100000,X=X,y=y)</span></pre><p id="e118" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随机梯度下降</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="8caa" class="na nb it mw b gy nc nd l ne nf"># Function for Stochastic Gradient Descent       <br/>def Stochastic_GD (Learning_Rate,num_iterations,X,y):<br/>    # Step 1: Initial Parameter<br/>    N=len(X)<br/>    w=np.zeros((X.shape[1],1))<br/>    b=0<br/>    costs=[]<br/>    # Starting two layer of loops<br/>    for i in range(num_iterations):<br/>        for j in range(N):<br/>            # Choose 1 record<br/>            XX=X[j,:]<br/>            yy=y[j]<br/>            # Step 2: Apply Sigmoid Function and get y prediction<br/>            Z=np.dot(w.T,XX.T)+b<br/>            y_pred=1/(1+1/np.exp(Z))<br/>            #Step 3: Calculate Loss Function<br/>            cost=-(yy*np.log(y_pred)+(1-yy)*np.log(1-y_pred))<br/>            #Step 4: Calculate Gradient<br/>            dw=np.multiply(XX,(y_pred-yy)).reshape((2,1))<br/>            db=y_pred-yy<br/>            #Step 5: Update w &amp; b<br/>            w = w - Learning_Rate * dw<br/>            b = b - Learning_Rate * db<br/>        <br/>        #Step 6: Calculate Loss Function       <br/>        Z_full=np.dot(w.T,X.T)+b<br/>        y_pred_full=1/(1+1/np.exp(Z_full))<br/>        cost=-(1/N)*np.sum(y*np.log(y_pred_full)+(1-y)*np.log(1-y_pred_full))<br/>        #Records cost<br/>        if i % 100 == 0:<br/>            costs.append(cost)<br/>            #print(cost)<br/>    <br/>    return(w,b,costs)</span><span id="fef8" class="na nb it mw b gy nh nd l ne nf"># Run a function<br/>Result_Stoc_GD=Stochastic_GD(Learning_Rate=0.01,num_iterations=2000,X=X,y=y)</span></pre><p id="a98f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">小批量梯度下降</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="d35f" class="na nb it mw b gy nc nd l ne nf"># Function for mini batch Gradient Descent<br/>def Minibatch_GD (Learning_Rate,num_iterations,X,y,Minibatch):<br/>    # Part 1: Mini Batch <br/>    np.random.seed(1000)<br/>    N=len(X)<br/>    mini_batches=[]<br/>    <br/>    #Step 1: Shuffle (X,y)<br/>    permutation=list(np.random.permutation(N))<br/>    shuffled_X=X[permutation,:]<br/>    shuffled_y=y[permutation]<br/>    <br/>    #Step 2: Partition<br/>    num_complete_minibatches=int(np.floor(N/Minibatch))<br/>    <br/>    for i in range(num_complete_minibatches):<br/>        mini_batch_X=shuffled_X[i*Minibatch:(i+1)*Minibatch,:]<br/>        mini_batch_y=shuffled_y[i*Minibatch:(i+1)*Minibatch]<br/>        <br/>        mini_batch = (mini_batch_X, mini_batch_y)<br/>        mini_batches.append(mini_batch)<br/>    <br/>    if N % Minibatch !=0:<br/>        mini_batch_X=shuffled_X[N-Minibatch:N,:]<br/>        mini_batch_y=shuffled_y[N-Minibatch:N]<br/>        <br/>        mini_batch = (mini_batch_X, mini_batch_y)<br/>        mini_batches.append(mini_batch)<br/>    <br/>    # Part 2: Gradient Descent<br/>    w=np.zeros((X.shape[1],1))<br/>    b=0<br/>    costs=[]<br/>    <br/>    for i in range(num_iterations):<br/>        for j in range(num_complete_minibatches+1):<br/>            #Select Minibatch<br/>            XX=mini_batches[j][0]<br/>            yy=mini_batches[j][1]<br/>            #Step 2: Apply Sigmoid Function and get y prediction<br/>            Z=np.dot(w.T,XX.T)+b<br/>            y_pred=1/(1+1/np.exp(Z))<br/>            <br/>            #Step 3: Calculate Gradient<br/>            dw=1/Minibatch*np.dot(XX.T,(y_pred-yy).T)<br/>            db=1/Minibatch*np.sum(y_pred-yy)<br/>            #Step 4: Update w &amp; b<br/>            w = w - Learning_Rate * dw<br/>            b = b - Learning_Rate * db<br/>        <br/>        #Step 5: Calculate Loss Function       <br/>        Z_full=np.dot(w.T,X.T)+b<br/>        y_pred_full=1/(1+1/np.exp(Z_full))<br/>        cost=-(1/N)*np.sum(y*np.log(y_pred_full)+(1-y)*np.log(1-y_pred_full))<br/>        <br/>        if i % 1000 ==0:<br/>            costs.append(cost)<br/>            #print(cost)<br/>            <br/>    return(w,b,costs)</span><span id="0b4b" class="na nb it mw b gy nh nd l ne nf"># Run a function<br/>Result_MiniGD=Minibatch_GD(Learning_Rate=0.01,num_iterations=100000,X=X,y=y,Minibatch=50)</span></pre><p id="e200" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">可视化结果</p><pre class="lo lp lq lr gt mv mw mx my aw mz bi"><span id="d6b0" class="na nb it mw b gy nc nd l ne nf"># Plot linear classification<br/>fig, ax = plt.subplots()<br/>ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,edgecolor='k')<br/>line_B_GD=mlines.Line2D([0,7],[-0.5527,4.1577],color='red')<br/>line_Mini_GD=mlines.Line2D([0,7],[-0.56185,4.1674],color='blue')<br/>line_Sto_GD=mlines.Line2D([0,7],[-0.5488,4.1828],color='green')<br/>ax.add_line(line_B_GD)<br/>ax.add_line(line_Mini_GD)<br/>ax.add_line(line_Sto_GD)<br/>ax.set_xlabel('Sepal length')<br/>plt.show()</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nj"><img src="../Images/faf519c6fcd19dfb417daf9971404686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iaL0gQ6fmqtOMTMknhQP3A.png"/></div></div></figure><p id="63aa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从上图可以看出，三种梯度下降产生了相似的线性决策边界。</p><p id="15ad" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">总结</strong></p><p id="eec3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇文章中，你学到了很多关于梯度下降的知识。你现在知道了梯度背后的基本数学，并且理解了算法是如何在幕后工作的。第二，你了解了为什么学习速率和规范化对算法的成功如此重要。最后，您了解了最常见的梯度下降类型以及如何在 python 中实现这些算法。这些知识使你能够更好地理解机器学习和深度学习。你可以点击以下链接阅读更多博客:</p><div class="nk nl gp gr nm nn"><a href="https://medium.com/@songyangdetang_41589/table-of-contents-689c8af0c731" rel="noopener follow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd jd gy z fp ns fr fs nt fu fw jc bi translated">机器学习和深度学习之旅</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">这一系列博客将从理论和实现两个方面对深度学习进行介绍。</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">medium.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob lt nn"/></div></div></a></div><p id="ee5a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">参考</p><p id="67fa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[1] Aurélien Géron，使用 Scikit-Learn &amp; TensorFlow 进行机器学习，2018 年</p><p id="cc00" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[2]伊恩·古德费勒，约舒阿·本吉奥，亚伦·库维尔，(2017) <em class="ng">深度学习</em></p><p id="b4d9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[3]<a class="ae ml" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/gradient _ descent . html</a></p></div></div>    
</body>
</html>