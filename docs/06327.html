<html>
<head>
<title>Initializing neural networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初始化神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/initializing-neural-networks-3a774eb63745?source=collection_archive---------27-----------------------#2019-09-11">https://towardsdatascience.com/initializing-neural-networks-3a774eb63745?source=collection_archive---------27-----------------------#2019-09-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/951bf1236eb105495cbc84930644ea19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5iPe2STROJUXmjhVi3R8SQ.jpeg"/></div></div></figure><h2 id="505d" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">设置</h2><p id="9f57" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">让我们从获取 MNIST 数据集开始。因为我们经常这样做，所以我们将定义一个函数来这样做。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/c2f3794a223eccaac85d238b22a0ba3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9gnlQvj_oCR0pw9AF2FmAQ.png"/></div></div></figure><p id="9ced" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">现在让我们计算数据的平均值和标准差。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/608ad3429650cfe6c0cbf25176f8e008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*clwmKuIUgrDyIicmU083Og.png"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lz"><img src="../Images/47043212a7aeeb7b4d0d9aa9e35a040f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJ0E1AmIF6B5THNCY9fpcQ.png"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ma"><img src="../Images/ea6f2e38f3152fe9f51e5e7ad7279adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b5Donnum6LgXa79bPWTZdw.png"/></div></div></figure><p id="d0b0" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">注意，我们用<code class="fe mb mc md me b">train_mean</code>而不是<code class="fe mb mc md me b">valid_mean</code>来标准化验证集，以保持训练集和验证集在相同的规模上。</p><p id="abd6" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">由于平均值(或 sd)永远不会精确地为 0(或 1)，我们还定义了一个函数来测试它们是否接近 0(有一些阈值)。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/c9e91e07f33fc7c312ed969a23713763.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92WZvvZTab9iiauu37OBxg.png"/></div></div></figure><p id="7278" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">接下来，让我们初始化我们的神经网络。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mf"><img src="../Images/b43a47ff7f3fa9cd45a78c483dd2c696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1aeMNnWzRDwbXNLM-Yvvg.png"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mg"><img src="../Images/28f274a59d446832871fd2e76b02bdba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jZ7GV_0KgoUSpFFnTMu9iQ.png"/></div></div></figure><h2 id="d4cb" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">初始化问题</h2><p id="53d0" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated"><strong class="kw ir"> <em class="mh">初始化神经网络</em> </strong>是深度学习的重要组成部分。这就是为什么我们可以让我们的神经网络像今天这样深的核心。初始化决定了我们是否收敛得好，收敛得快。</p><p id="b673" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">我们希望以这样一种方式初始化我们的权重，即当我们通过不同的层时，均值和方差保持不变。我们当前的初始化不会发生这种情况。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mi"><img src="../Images/1709b7655c1a67397f1386ed3b322bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TGhYoeTj1RwW6IQ4g-oigw.png"/></div></div></figure><p id="a55f" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">我们可以看到，仅仅一层之后，我们的激活值(一层的输出)相差甚远。如果我们对许多层重复这个过程，它将导致<strong class="kw ir"> <em class="mh">渐变爆炸</em> </strong>，如下所示。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mj"><img src="../Images/9fd1a99a1722d95cb19335f4e42f372e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u2HvrlcTSOU_FnrxWR8oXw.png"/></div></div></figure><p id="f60d" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">我们模型的激活增长远远超过合理的值，以至于达到无穷大。这甚至不需要 100 次乘法运算就能实现。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/719ea1774b1cefe41a4caa577428e64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0U53PlzMz1B2JLGpPptaQ.png"/></div></div></figure><p id="139b" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">那么我们该如何应对呢？也许我们可以把它们缩小一个系数来防止它们爆炸。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/40563ee46b8c4ed949489c562638214b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*os8gw47U-u3fQMW9fJMm1w.png"/></div></div></figure><p id="c498" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">那也没用。虽然想法是正确的，但选择错误的因子会导致<strong class="kw ir"> <em class="mh">渐变</em> </strong>(值达到 0)。</p><h2 id="c758" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">选择正确的比例因子— Xavier init</h2><p id="2ccd" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">比例因子的值应该是多少？</p><p id="920d" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">答案是(1 /⎷input).这种初始化技术被称为<strong class="kw ir"> <em class="mh"> Xavier 初始化</em> </strong>。如果你想了解相同背后的数学，可以阅读<a class="ae mm" href="http://proceedings.mlr.press/v9/glorot10a.html" rel="noopener ugc nofollow" target="_blank">原文</a>或者本文末尾提到的参考文章之一。阅读研究论文的一个好建议是搜索总结论文的文章。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/123311210f536f763c8732b6d7529463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UzoLcE53mlONP39WKSr0LA.png"/></div></div></figure><p id="09a0" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">除以⎷input 确实有效。注意，如果我们想在反向传递中保持梯度，我们将除以⎷output.</p><p id="f23f" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">Xavier 初始化文档也提供了如下所示的一些很好的可视化效果。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/1894fd66b8eb16ad4b1884008c9368af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4A0vdamSwpbEXbHRqLUmcw.png"/></div></div></figure><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mp"><img src="../Images/a3f7f9def2ab28ccb6e8835be3fc1b92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYI8GT7yuXvaSn_DFVS0SA.png"/></div></div></figure><h2 id="5f48" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">Xavier 初始化有问题</h2><p id="7aa6" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">Xavier init 论文假设我们的激活函数将是线性的(事实并非如此)。因此，它忽略了激活函数对均值和方差的影响。让我们想想 ReLU。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/5690f17968729437392132b068f794c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MUfv_1_E7F7B9d8jnCqhBA.png"/></div></div></figure><p id="911c" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">一个 ReLU 把分布中所有的负值都变成 0。这当然不能保持我们数据的均值和方差。如果说有什么不同的话，那就是它们的价值只有原来的一半。每一层都会发生这种情况，所以 1/2 会累加起来。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/4705e26b23bdb846c40214c780e3b67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VzDesUnJR_RlVpfnymWDYw.png"/></div></div></figure><p id="64d4" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">在一篇名为<a class="ae mm" href="https://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank">深入研究整流器:在 ImageNet 分类上超越人类水平的性能的论文中提出了这个问题的解决方案。</a></p><p id="3755" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">简单的想法是，由于我们的值每次都减少一半，我们只需在分子中增加一个额外的 2 来抵消它。这种初始化技术被称为<strong class="kw ir"><em class="mh"/>初始化</strong>或<strong class="kw ir"> <em class="mh">何初始化</em> </strong>。</p><figure class="lq lr ls lt gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/debda640e2426c77230dbe6d916d5d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fW7d__eWqDvPAhQubmL7ig.png"/></div></div></figure><p id="eafc" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">尽管我们的均值不太好，但它确实有助于我们的标准差。好的初始化能做的事情是惊人的。</p><p id="01dc" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">有一篇名为<a class="ae mm" href="https://arxiv.org/abs/1901.09321" rel="noopener ugc nofollow" target="_blank"> Fixup initialization </a>的论文，作者仅通过仔细的初始化，就训练了一个 10000 层深度的神经网络，没有任何归一化。这应该足以让你相信很好地初始化神经网络是重要的。</p><p id="b890" class="pw-post-body-paragraph ku kv iq kw b kx lu kz la lb lv ld le kh lw lg lh kl lx lj lk kp ly lm ln lo ij bi translated">如果你想了解更多关于深度学习的知识，可以看看我在这方面的系列文章:</p><div class="mt mu gp gr mv mw"><a href="https://medium.com/@dipam44/deep-learning-series-30ad108fbe2b" rel="noopener follow" target="_blank"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd ir gy z fp nb fr fs nc fu fw ip bi translated">深度学习系列</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">我所有关于深度学习的文章的系统列表</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">medium.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk jw mw"/></div></div></a></div><h2 id="61c5" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考资料:</h2><ol class=""><li id="0423" class="nl nm iq kw b kx ky lb lc kh nn kl no kp np lo nq nr ns nt bi translated"><a class="ae mm" href="https://www.fast.ai" rel="noopener ugc nofollow" target="_blank">从基础开始深度学习，fast.ai </a></li><li id="df93" class="nl nm iq kw b kx nu lb nv kh nw kl nx kp ny lo nq nr ns nt bi translated"><a class="ae mm" href="https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">理解深度神经网络中的 Xavier 初始化。</a></li><li id="4102" class="nl nm iq kw b kx nu lb nv kh nw kl nx kp ny lo nq nr ns nt bi translated"><a class="ae mm" href="https://pouannes.github.io/blog/initialization/" rel="noopener ugc nofollow" target="_blank">深度神经网络如何初始化？</a></li><li id="a80f" class="nl nm iq kw b kx nu lb nv kh nw kl nx kp ny lo nq nr ns nt bi translated"><a class="ae mm" href="https://madaan.github.io/init/#tl-dr" rel="noopener ugc nofollow" target="_blank">关于深度神经网络权重初始化的说明</a></li><li id="c911" class="nl nm iq kw b kx nu lb nv kh nw kl nx kp ny lo nq nr ns nt bi translated"><a class="ae mm" href="https://stats.stackexchange.com/questions/52646/variance-of-product-of-multiple-random-variables" rel="noopener ugc nofollow" target="_blank">多个随机变量乘积的方差</a></li></ol></div></div>    
</body>
</html>