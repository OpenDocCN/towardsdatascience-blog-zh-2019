<html>
<head>
<title>Naive Bayes Document Classification in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的朴素贝叶斯文档分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e?source=collection_archive---------0-----------------------#2019-06-23">https://towardsdatascience.com/naive-bayes-document-classification-in-python-e33ff50f937e?source=collection_archive---------0-----------------------#2019-06-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5075" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我能在多大程度上根据摘要对一篇哲学论文进行分类？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1c7e4a6404c22517c3b62b1d5df75255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Apv-o_hmRHEbvfor5m2I5w.jpeg"/></div></div></figure><p id="614e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">朴素贝叶斯对于文档分类任务来说是一种相当有效的策略，尽管顾名思义，它是“幼稚的”</p><p id="4d13" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">朴素贝叶斯分类利用贝叶斯定理来确定一个项目属于某个类别的可能性有多大。如果我有一个包含单词“信任”、“美德”或“知识”的文档，它属于“伦理学”而不是“认识论”的概率是多少朴素贝叶斯根据哪个概率最高来对项目进行分类。</p><p id="1a6e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这是“幼稚的”,因为它把每个单词出现在文档中的概率视为独立于任何其他单词出现的概率。这种假设对于我们想要分类的任何文档来说几乎都是不成立的，它们往往遵循语法、句法和交流的规则。当我们遵循这些规则时，一些单词往往会与其他单词相关联。</p><p id="6121" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这里，我设计了一个我认为有点困难的分类任务:对哲学文章的摘要进行分类。我选择了截然不同但有大量重叠的子学科:认识论和伦理学。两者都使用了证明和理由的语言。它们也经常交叉(例如信仰的伦理、道德知识等等)。最终，朴素贝叶斯在对这些文档进行分类时表现得出奇的好。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lq lr l"/></div></figure><p id="7581" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">什么是朴素贝叶斯分类？</strong></p><p id="d47a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ls">贝叶斯定理</em></p><p id="3412" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">贝叶斯定理告诉我们，给定一些证据的假设的概率等于假设的概率乘以给定假设的证据的概率，然后除以证据的概率。</p><blockquote class="lt lu lv"><p id="52a4" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">Pr(H|E) = Pr(H) * Pr(E|H) / Pr(E)</p></blockquote><p id="3bc7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我们正在对文档进行分类，所以“假设”是:文档属于类别 c。“证据”是文档中出现的单词 W。</p><p id="887f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于分类任务涉及比较两个(或更多)假设，我们可以使用贝叶斯定理的比率形式，它比较每个假设的上述公式的分子(对于贝叶斯爱好者:先验乘以似然性):</p><blockquote class="lt lu lv"><p id="4b0b" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">pr(c₁|w)/pr(c₂|w)=pr(c₁)*pr(w|c₁)/pr(c₂)*pr(w|c₂)</p></blockquote><p id="23f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于文档中有许多单词，因此公式变为:</p><blockquote class="lt lu lv"><p id="b5b4" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">Pr(C₁|W₁，W₂ …Wn) / Pr(C₂|W₁，W₂ …Wn)=</p><p id="5e6b" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">pr(c₁)*(pr(w₁|c₁)*pr(w₂|c₁)*…pr(wn|c₁))/</p><p id="5f70" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">pr(c₂)*(pr(w₁|c₂)*pr(w₂|c₂)*…pr(wn|c₂))</p></blockquote><p id="29ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，如果我想知道包含单词“预热烤箱”的文档是否属于“烹饪书”类别而不是“小说”类别，我会比较以下内容:</p><blockquote class="lt lu lv"><p id="8229" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">Pr(食谱)* Pr("预热" |食谱)* Pr("烹饪" |食谱)* Pr("烤箱" |食谱)</p></blockquote><p id="4344" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对此:</p><blockquote class="lt lu lv"><p id="f890" class="ku kv ls kw b kx ky ju kz la lb jx lc lw le lf lg lx li lj lk ly lm ln lo lp im bi translated">Pr(小说)* Pr("预热" |小说)* Pr(" the " |小说)* Pr("烤箱" |小说)</p></blockquote><p id="a842" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果给定文档中出现的单词，它是食谱的概率大于它是小说的概率，则朴素贝叶斯返回“食谱”。如果反过来，朴素贝叶斯返回“小说”。</p><p id="fc92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">演示:根据摘要对哲学论文进行分类</strong></p><ol class=""><li id="7218" class="lz ma it kw b kx ky la lb ld mb lh mc ll md lp me mf mg mh bi translated"><em class="ls">准备数据</em></li></ol><p id="7990" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将尝试分类的文件是来自一个名为<a class="ae mi" href="https://philpapers.org" rel="noopener ugc nofollow" target="_blank"> PhilPapers </a>的数据库的文章摘要。Philpapers 是一个综合性的哲学研究数据库。由于这个数据库是由大量的主题编辑管理的，我们有理由相信网站上给出的文档分类是正确的。</p><p id="141c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我从网站上为二元朴素贝叶斯分类器选择了两个哲学分支学科:伦理学或认识论。从每个分支学科中，我选择了一个主题。对于伦理学，我选择了题目<a class="ae mi" href="https://philpapers.org/browse/varieties-of-virtue-ethics" rel="noopener ugc nofollow" target="_blank">“各种美德伦理学”</a>，对于认识论，我选择了“<a class="ae mi" href="https://philpapers.org/browse/trust" rel="noopener ugc nofollow" target="_blank">信任</a>”我收集了 80 篇伦理学和 80 篇认识论摘要。</p><p id="efde" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我最初的数据帧的头部和尾部是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/19e4b920239165b97287ee24f54f893d.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*EvjI9GHTNo3Y_R48-k6I-g.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/ea45246057547fc0fed953b2c43d1060.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*MAQyOtIl2shZu2IB26cKIQ.png"/></div></figure><p id="f383" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了在 Scikit Learn 中运行朴素贝叶斯分类器，类别必须是数字，所以我将标签 1 分配给所有伦理学摘要，将标签 0 分配给所有认识论摘要(即<em class="ls">而不是伦理学</em>):</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="cab4" class="mp mq it ml b gy mr ms l mt mu">df[‘label’] = df[‘category’].apply(lambda x: 0 if x==’Epistemology’ else 1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/e92752941802e33d01c2ddafddc4d3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*pSeIIOQjKcsI4ST_aCCGig.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/3fab195cce14eb0bbe7106fce35dfcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*4FOEdjva4_-wz2-XiCgk9A.png"/></div></figure><p id="61cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ls"> 2。将数据分成训练集和测试集</em></p><p id="0c63" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">保留一些数据很重要，这样我们可以验证我们的模型。为此，我们可以使用 Scikit Learn 的 train_test_split。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="2afe" class="mp mq it ml b gy mr ms l mt mu">from sklearn.model_selection import train_test_split</span><span id="1c0f" class="mp mq it ml b gy mw ms l mt mu">X_train, X_test, y_train, y_test = train_test_split(df[‘abstract’], df[‘label’], random_state=1)</span></pre><p id="66d8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ls"> 3。将摘要转换成字数矢量</em></p><p id="ea9f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">朴素贝叶斯分类器需要能够计算每个单词在每个文档中出现多少次，以及在每个类别中出现多少次。要实现这一点，数据需要看起来像这样:</p><p id="ab18" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi">[0, 1, 0, …]</p><p id="fa2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi">[1, 1, 1, …]</p><p id="e527" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi">[0, 2, 0, …]</p><p id="5666" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每行代表一个文档，每列代表一个单词。第一行可能是包含“预热”的零、“The”的一和“烤箱”的零的文档。这意味着文档包含单词“the”的一个实例，但没有“preheat”或“oven”</p><p id="8dbf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要获得这种格式的摘要，我们可以使用 Scikit Learn 的计数矢量器。CountVectorizer 为每个摘要创建一个字数矢量，形成一个矩阵。每个索引对应一个词，出现在摘要中的每个词都有代表。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="f8d0" class="mp mq it ml b gy mr ms l mt mu">from sklearn.feature_extraction.text import CountVectorizer</span><span id="0341" class="mp mq it ml b gy mw ms l mt mu">cv = CountVectorizer(strip_accents=’ascii’, token_pattern=u’(?ui)\\b\\w*[a-z]+\\w*\\b’, lowercase=True, stop_words=’english’)</span><span id="9f38" class="mp mq it ml b gy mw ms l mt mu">X_train_cv = cv.fit_transform(X_train)<br/>X_test_cv = cv.transform(X_test)</span></pre><p id="f562" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以使用 strip_accents、token_pattern、lowercase 和 stopwords 参数来排除非单词、数字、冠词和其他对从计数中预测类别没有用的东西。有关详细信息，请参见<a class="ae mi" href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="4938" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您想查看数据并调查字数，您可以使用以下代码制作一个字数的数据框架:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="a318" class="mp mq it ml b gy mr ms l mt mu">word_freq_df = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())</span><span id="8ed8" class="mp mq it ml b gy mw ms l mt mu">top_words_df = pd.DataFrame(word_freq.sum()).sort_values(0, ascending=False)```</span></pre><p id="6903" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ls"> 4。拟合模型并进行预测</em></p><p id="7e32" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们准备将<em class="ls">多项式朴素贝叶斯分类器</em>模型拟合到我们的训练数据中，并使用它来预测测试数据的标签:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="2ccb" class="mp mq it ml b gy mr ms l mt mu">from sklearn.naive_bayes import MultinomialNB<br/>naive_bayes = MultinomialNB()<br/>naive_bayes.fit(X_train_cv, y_train)<br/>predictions = naive_bayes.predict(X_test_cv)</span></pre><p id="47c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ls"> 5。检查结果</em></p><p id="b6d8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看看模型在测试数据上的表现:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="21fa" class="mp mq it ml b gy mr ms l mt mu">from sklearn.metrics import accuracy_score, precision_score, recall_score</span><span id="40b0" class="mp mq it ml b gy mw ms l mt mu">print(‘Accuracy score: ‘, accuracy_score(y_test, predictions))<br/>print(‘Precision score: ‘, precision_score(y_test, predictions))<br/>print(‘Recall score: ‘, recall_score(y_test, predictions))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/88be4a9d5ec0ed24874094314e89171c.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*S_K0uIywQeXqJw9Rjt7S9Q.png"/></div></figure><p id="3349" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了理解这些分数，看一下细目分类会有帮助:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="a494" class="mp mq it ml b gy mr ms l mt mu">from sklearn.metrics import confusion_matrix<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="3831" class="mp mq it ml b gy mw ms l mt mu">cm = confusion_matrix(y_test, predictions)<br/>sns.heatmap(cm, square=True, annot=True, cmap=’RdBu’, cbar=False,<br/>xticklabels=[‘epistemology’, ‘ethics’], yticklabels=[‘epistemology’, ‘ethics’])<br/>plt.xlabel(‘true label’)<br/>plt.ylabel(‘predicted label’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/2ba45c8670e471fc73e4aa9141c0d45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*t5qOIEIA4jSs1UwLSiZP7w.png"/></div></figure><p id="0d1e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">准确度分数</strong>告诉我们:在我们所做的所有识别中，有多少是正确的？</p><ul class=""><li id="9b8e" class="lz ma it kw b kx ky la lb ld mb lh mc ll md lp mz mf mg mh bi translated">真阳性+真阴性/总观察值:(18 + 19) / 40</li></ul><p id="6a18" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">精确分数</strong>告诉我们:在我们进行的所有<em class="ls">道德</em>鉴定中，有多少是正确的？</p><ul class=""><li id="1a99" class="lz ma it kw b kx ky la lb ld mb lh mc ll md lp mz mf mg mh bi translated">真阳性/(真阳性+假阳性):18 / (18+2)</li></ul><p id="205b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回忆分数告诉我们:在所有真实的道德案例中，我们正确识别了多少？</p><ul class=""><li id="f4d5" class="lz ma it kw b kx ky la lb ld mb lh mc ll md lp mz mf mg mh bi translated">真阳性/(真阳性+假阴性):18/(18+1)</li></ul><p id="f272" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="ls"> 6。调查模型的失误</em></p><p id="34f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了调查不正确的标签，我们可以将实际标签和预测标签并排放在一个数据帧中。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="5837" class="mp mq it ml b gy mr ms l mt mu">testing_predictions = []</span><span id="a6bc" class="mp mq it ml b gy mw ms l mt mu">for i in range(len(X_test)):<br/>    if predictions[i] == 1:<br/>        testing_predictions.append(‘Ethics’)<br/>    else:<br/>        testing_predictions.append(‘Epistemology’)</span><span id="f372" class="mp mq it ml b gy mw ms l mt mu">check_df = pd.DataFrame({‘actual_label’: list(y_test), ‘prediction’: testing_predictions, ‘abstract’:list(X_test)})<br/>check_df.replace(to_replace=0, value=’Epistemology’, inplace=True)<br/>check_df.replace(to_replace=1, value=’Ethics’, inplace=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d14b99567eb9dc1816c9cbf03c6fe769.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*spjUqJDbVqJkn2wfpFiuvA.png"/></div></figure><p id="8c68" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">总的来说，我的朴素贝叶斯分类器在测试集上表现良好。40 个标签中只有 3 个不匹配。</p><p id="cb02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">推荐阅读:</strong></p><div class="nb nc gp gr nd ne"><a href="https://plato.stanford.edu/entries/logic-inductive/" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">归纳逻辑(斯坦福哲学百科全书)</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">指定归纳支持函数的逻辑公理的一个好方法如下。这些公理显然是…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">plato.stanford.edu</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ks ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">加法平滑-维基百科</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">从贝叶斯的观点来看，这对应于后验分布的期望值，使用对称的…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/naive-bayes-intuition-and-implementation-ac328f9c9718"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">朴素贝叶斯:直觉和实现</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">了解朴素贝叶斯算法及其预测类的能力。</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns ks ne"/></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">深度:朴素贝叶斯分类</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">因为朴素贝叶斯分类器对数据做了如此严格的假设，它们通常不会表现得那么好…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">jakevdp.github.io</p></div></div></div></a></div><div class="nb nc gp gr nd ne"><a href="https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">将多项式朴素贝叶斯应用于 NLP 问题:一个实用的解释</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">1.简介朴素贝叶斯是一个基于应用贝叶斯定理的算法家族，它具有很强的(朴素)推理能力</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="nu l np nq nr nn ns ks ne"/></div></div></a></div></div></div>    
</body>
</html>