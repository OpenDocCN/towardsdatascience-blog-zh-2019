<html>
<head>
<title>Word Embedding (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单词嵌入(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-embeddings-intuition-and-some-maths-to-understand-end-to-end-skip-gram-model-cab57760c745?source=collection_archive---------11-----------------------#2019-02-27">https://towardsdatascience.com/word-embeddings-intuition-and-some-maths-to-understand-end-to-end-skip-gram-model-cab57760c745?source=collection_archive---------11-----------------------#2019-02-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5876" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直觉和(一些)数学来理解端到端跳格模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/00059eea5832ab64b7424fa887e9ef76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cbj_BcK5ePD4Mc4TK4sfvQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/photos/2JIvboGLeho?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Susan Yin</a> on <a class="ae ky" href="https://unsplash.com/search/photos/library?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dab3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然语言处理的最初问题是将一个单词/句子编码成计算机可理解的格式。向量空间中单词的表示允许 NLP 模型学习。如图 1 所示，将单词表示成向量的第一种简单表示法是一热编码法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/790cd475fc2c812848282151d8466ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPRUDu5YY83Ls766EmtMiQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig 1: One-hot-vectors of each words from the sentence “The quick brown fox runs away.”</figcaption></figure><p id="294e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种实现缺乏像单词的上下文(T2)这样的信息，我们所说的上下文是指给定单词与哪些其他单词相关。事实上，“布朗”和“福克斯”是正交的，因此给定我们的一次性编码是不相关的。最重要的是，one-hot-vector 的大小与词汇表一样，可以达到数千个单词，这使得它不切实际。</p><p id="0ab5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对这些限制的第一个解决方案将是:<br/> 1)考虑给定其<strong class="lb iu">上下文</strong> <br/>的单词的含义 2)将它的表示维度减少到<strong class="lb iu">更实际的大小</strong></p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="afce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Skip-gram </strong>模型是最流行的单词嵌入模型之一，它旨在根据给定的上下文对单词进行编码。</p><blockquote class="md me mf"><p id="e939" class="kz la mg lb b lc ld ju le lf lg jx lh mh lj lk ll mi ln lo lp mj lr ls lt lu im bi translated">“从一个人和什么样的人交往，你就可以知道他是什么样的人”(弗斯，J. R. 1957)</p></blockquote><p id="4119" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">引用 20 世纪语言学家弗斯的话，很好地说明了我们的担忧。通过“它保持的公司”或上下文，我们指的是在固定大小的窗口中出现在中心单词附近的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mk"><img src="../Images/aed49a3c4b4822a40d1a9d9d97571cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqg2BtsaN4spImPKEzE2cQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig 2: Context words of “brown” within a one window size (one the right and one on the left).</figcaption></figure><p id="726e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法是，出现在相似上下文中的单词将具有相同的单词表示。为了实现这个目标，<strong class="lb iu"> Skip-gram </strong>将作为给定中心单词的固定大小<em class="mg"> m </em>的窗口内的上下文单词的预测器。因此，对于大小为<em class="mg"> T </em>的词汇表，<strong class="lb iu">跳格模型</strong>将希望最大化以下预测准确性或可能性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/578b8b1574293a37f4dd9a99e69719a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*RHpP5F4E0UqiAUFd6IgBKg.png"/></div></figure><p id="9a45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了便于计算，并且因为在机器学习中我们更喜欢最小化函数而不是最大化它们，我们现在将考虑平均负对数似然性并将其最小化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/36fc97587503b172cd34faa5bdf8ca63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPtGxhwOi9agsLwWSTMM8w.png"/></div></div></figure><p id="d4b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算 P(w_{i+j} | w_{i})，我们将使用两个向量:<br/> -当 w 是上下文词时 u _ { w }<br/>-当 w 是中心词时 v_{w}</p><p id="e230" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将使用 softmax 函数计算给定中心词<strong class="lb iu"> c </strong>的上下文词<strong class="lb iu"> o </strong>(外部词)的概率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/62989a890c848299589568111ff8f177.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*eauJTuMByyaWG5ux0QzC2A.png"/></div></figure><p id="039f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来解释这个等式，以便更好地理解<strong class="lb iu"> Skip-gram </strong>模型背后的直觉。中心单词和上下文单词之间的相似度越高，分子上的点积越高，因此上下文单词被预测为邻近单词的概率越高。<br/>这里，我们强制<strong class="lb iu"> Skip-gram </strong>模型学习那些 u_{w}和 v_{w}向量，以便做出正确的预测。</p><p id="51c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Skip-gram </strong>模型可通过下图进行总结。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/ae625b552f2947c95ff11ec2af5bf1b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1yE9fEP6pB2Yn5vUUxOTw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig 3: End-to-end Skip-gram model training on the sentence “The quick brown fox”. We choose the window dimension m=1 for representation ease.</figcaption></figure><p id="8bec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦<strong class="lb iu">跳跃式语法</strong>模型已经训练了它的预测任务，单词表示在中心单词的单词嵌入矩阵表示中是可用的。</p><p id="2f77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊！</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="20eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们能够用<strong class="lb iu">跳格</strong>模型嵌入单词。然而，我们必须注意到<strong class="lb iu"> Skip-gram </strong>在给定单词的本地上下文的情况下捕捉单词的意思，而没有考虑更全面的学习。例如，假设我们有这样一个句子:“狐狸……”，“the”和“狐狸”可能经常一起出现，但是<strong class="lb iu"> Skip-gram </strong>不知道“The”是一个常用词还是一个与“狐狸”特别相关的词。</p><p id="e9eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了应对这个潜在的问题，已经创建了<strong class="lb iu"> Glove </strong>模型来查看单词的本地上下文和全局统计。如果你想更深入地研究单词嵌入方法，这里有一个<a class="ae ky" href="https://medium.com/@matyasamrouche19/word-embedding-part-ii-intuition-and-some-maths-to-understand-end-to-end-glove-model-9b08e6bf5c06" rel="noopener">简短演示</a>来快速理解这个<strong class="lb iu">手套</strong>模型。</p><p id="a2e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考资料和其他有用的资源:</strong><em class="mg"><br/>-</em>-<a class="ae ky" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">原文 Skip-gram 论文</a> <br/> - <a class="ae ky" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/syllabus.html" rel="noopener ugc nofollow" target="_blank"> Standford NLP 资源</a> <br/> - <a class="ae ky" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank"> Word2Vec 教程</a></p></div></div>    
</body>
</html>