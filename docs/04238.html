<html>
<head>
<title>Deep Dive into the Computer Vision World: Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入了解计算机视觉世界:第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-dive-into-the-computer-vision-world-f35cd7349e16?source=collection_archive---------15-----------------------#2019-07-02">https://towardsdatascience.com/deep-dive-into-the-computer-vision-world-f35cd7349e16?source=collection_archive---------15-----------------------#2019-07-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fca5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从 VGG、ResNet、GoogLeNet 和 MobileNet 开始</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e3a47a3d4861e8603e4d039fae122466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GhwhRLNyKvjdqSxE5SAzbA.png"/></div></div></figure><p id="db3b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在完成了神经网络的基础知识之后，下一步将是学习这个领域中的一些“著名摇滚明星模型”。ResNet，Inception Net，Faster R-CNN，YOLO 等等。研究这些模型可以分为三个部分:这些架构背后的应用程序、实现和直觉。关于如何使用预训练模型以及如何构建它们，已经有大量的资源。但抓住模型背后真正的直觉有时会被忽略。研究人员用这种结构建立模型的意图是什么？是什么促使他们采取这样的方法？我们能从结果中推断出什么？</p><p id="371a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文是下一个系列<a class="ae lq" rel="noopener" target="_blank" href="/the-most-intuitive-and-easiest-guide-for-artificial-neural-network-6a3f2bc0eecb?source=friends_link&amp;sk=a9021d009dcd1651c16b59c6e48d0533"> <strong class="kw iu">最直观最简单的指南</strong> </a>教程，该系列的完整设置如下:</p><ol class=""><li id="5be2" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">从 VGG 开始，ResNet，Inception Network 和 MobileNet</li><li id="c6ec" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/deep-dive-into-the-computer-vision-world-part-2-7a24efdb1a14?source=friends_link&amp;sk=4fec4dfc9499c930f263c6808b2f369d">CNN 地区，让我们开始物体探测吧！</a></li><li id="ee3b" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/deep-dive-into-the-computer-vision-world-part-3-abd7fd2c64ef?source=friends_link&amp;sk=876a90f05dcef8f9f0546b42adaec42d"> YOLO，SSD 和 RetinaNet，比较统一的</a></li><li id="7465" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">从对象检测到实例分割(TBU)</li></ol><p id="2b9d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文假设你已经熟悉了卷积神经网络的基本概念。除此之外，我们还将深入了解 VGG、ResNet、Inception Network 和 MobileNet 的各种卷积变换和修改。一个接一个地浏览它们，我们将回答上面提到的问题，这样我们才能真正理解这些架构的潜在含义。</p><h1 id="a70d" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">VGG</h1><p id="a18a" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">ImageNet 是一个用于计算机视觉领域研究的大型数据集。自 2010 年以来，每年都举办名为<a class="ae lq" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> ImageNet 大规模视觉识别挑战赛</a> (ILSVRC)的比赛。VGGNet 是 2014 年 ILSVRC 竞赛的获胜者。尽管与其他复杂的网络相比，它的体系结构简单，但它仍然是最受欢迎的网络之一。</p><p id="6220" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个网络的一个有趣的部分是，即使在第一层中，它也只使用了 3×3 滤波器。如果和之前的比较，<a class="ae lq" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> AlexNet </strong> </a> <strong class="kw iu">，</strong>2012 年的赢家，<strong class="kw iu"> </strong>在前两层使用了<strong class="kw iu"> 11x11 </strong>和<strong class="kw iu"> 5x5 </strong>滤镜。还有<a class="ae lq" href="https://arxiv.org/pdf/1311.2901.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> ZFnet </strong> </a> <strong class="kw iu">，</strong>2013 年的冠军，用的是<strong class="kw iu"> 7x7 </strong>滤镜。代替在第一卷积层中使用相对大尺寸的滤波器，所使用的 VGG 网络滤波器的尺寸仅为 3×3。</p><p id="d38e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么尝试这个的意图是什么呢？答案是减少参数的数量。让我们在这里做一些计算。通道尺寸为 C 的单个<strong class="kw iu"> 7x7 </strong>卷积层需要多少个参数？是<strong class="kw iu"> 49C </strong>。现在，通道尺寸相同的 3 层<strong class="kw iu"> 3x3 </strong>卷积层的数量是多少？(9C )x3 = <strong class="kw iu"> 27C </strong>。看看有多少参数减少了。减少权重的数量可以通过降低网络的复杂性来处理过拟合。此外，由于我们可以为每个卷积层设置一个激活层，这也有助于模型的学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/8e881e1590805db388cb42961fb7d05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m10KcUlwlcT7bhZUahVdvw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><a class="ae lq" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">The architecture of VGG-19</a></figcaption></figure><p id="7e13" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与 VGG 网络或 AlexNet 一样，大多数卷积网络都遵循基本结构:一系列卷积层、一个汇集层和一个带有一些规范化的激活层。但是越来越深入，人们遇到了一些严重的问题，并开始重新设计这种方法。</p><h1 id="cb1f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">雷斯内特</h1><p id="db27" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">直觉上，我们希望“越深越好”然而，研究人员发现事实并非如此。随着网络变得越来越深，性能越来越差。原因之一是消失/爆炸梯度问题。虽然我们已经有几种方法来解决这个问题，如梯度裁剪，有一个退化问题，不是由于过度拟合。ResNet 的诞生就是从这里开始的。<strong class="kw iu"> <em class="nh">为什么会这样？</em> </strong></p><p id="f0ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们假设我们有一个具有理想精确度的良好网络。现在，我们将复制这个网络，只添加 1 层，但没有额外的功能。这被称为<strong class="kw iu">身份映射</strong>，这意味着获得与输入完全相同的输出。与其他具有许多参数的复杂层相比，这对于我们的“智能”模型来说是小菜一碟。所以对网络的性能应该没有什么危害吧？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/69c387edcb8ab49164f9d38a1a2a718a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SltFjgDwRVXoGHQQN-BWcg.png"/></div></div></figure><p id="0d78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么如果我们附加越来越多的身份层会有什么结果呢？由于身份层对输出没有影响，因此性能不会下降。这似乎是一个合理的猜测，但这个实验的结果与我们的预期不同。该模型的性能比没有身份层的较浅模型差。</p><p id="f20f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个结果表明“越深越好”不适用于神经网络，即使有身份映射。仅仅将身份层直接附加到网络上就很难训练。<em class="nh">好吧，那么把身份层作为一个附加层怎么样？</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/f22fd41f77ba7e48132a39c62b797a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KsNEtDepdgy4HM_vn4Z75w.png"/></div></div></figure><p id="d6ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，这就是 ResNet 发挥作用的地方。假设<strong class="kw iu"> H(x) </strong>是网络的结果。而如果我们放一条额外的路径，原来的函数就变成了<strong class="kw iu"> F(x) + x </strong>其中<strong class="kw iu"> F(x) </strong>是<strong class="kw iu">主分支</strong>的函数如上图。身份层也被称为<strong class="kw iu">快捷连接</strong>或<strong class="kw iu">跳过连接</strong>。这就给网络带来了“剩余学习”的概念。现在主分支被训练为逼近<strong class="kw iu"> H(x) — x </strong>并使<strong class="kw iu"> F(x) </strong>更接近零。这意味着给予各层一定的学习方向，从而使培训更容易。此外，网络可以通过身份层跳过一些层。这允许我们有更深的层，但仍然具有高性能。</p><p id="4053" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">你可能会问，当两层的大小不同时，怎么可能把<strong class="kw iu"> F(x) </strong>和 x 相加。我们可以通过在快捷连接处进行零填充或线性投影来解决这个问题。所以 ResNet 中有两种快捷连接，一种是没有权重的，一种是有权重的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/d3039b11c96c5feac4fe36b29775a238.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPlZWKddl1AI9u0doMZN_Q.png"/></div></div></figure><p id="b9de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上图是 ResNet 50 层的卷积块。它有一个具有 3 个回旋层的主分支和一个<strong class="kw iu"> 1x1 </strong>回旋的快捷连接。这里我们可能需要问，<strong class="kw iu"> <em class="nh">为什么要 1x1–3x 3–1x1 卷积？</em> </strong></p><p id="c544" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">要回答这个问题，需要了解数据的维度是如何根据过滤器大小而变化的。请看看右边的图片。如果我们说输入的形状是(32，32，256)，那么与上一步相比，当它通过<strong class="kw iu"> 3x3 </strong>过滤器时，维数会减小。当它通过第二个 1x1 滤波器时，它再次增加。这是一个瓶颈。通过强制数据适应更小的维度，我们可以获得更有效的特征表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/ecbafabe63b1603d16d335429ddc82d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzHdLSCXGf6jyTTiCMkcOw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><a class="ae lq" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">The comparison of a plain network and ResNet</a></figcaption></figure><p id="f5da" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用更少的滤波器、更少的计算量和更高的精度来容易地优化深度神经网络。这种简单而新颖的重构带来了如此美丽的成就，这就是 ResNet 在 2015 年如何震撼人们的心灵。带着 ResNet 上的这种根本直觉，我想让你从<a class="ae lq" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">原论文</strong> </a>开始探索网络。而<a class="ae lq" href="https://arxiv.org/pdf/1603.05027.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">这里的</strong> </a>是实验残块变体研究的第二部分。或者你可以在 ResNet <a class="ae lq" rel="noopener" target="_blank" href="/an-overview-of-resnet-and-its-variants-5281e2f56035"> <strong class="kw iu">这里</strong> </a>找到一个简单的解释。</p><h1 id="2013" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">谷歌网</h1><p id="f29b" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">高性能越深入越好。我们需要深度神经网络来执行复杂而具有挑战性的任务。但是，当我们评估网络时，还有一个因素需要考虑。想想我们什么时候必须在移动应用程序中嵌入模型。肯定不希望有一个“太重”的模型。但除此之外，还有一个在实际应用中非常关键的计算成本问题。所以现在的问题就变成了，<strong class="kw iu"> <em class="nh">怎样才能以更高的效率深入？</em>T15】</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/28daf5b5b5c9114d8211e94ee4482fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*aifMXoQqlR-EGQUC.jpg"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><a class="ae lq" href="https://knowyourmeme.com/memes/we-need-to-go-deeper" rel="noopener ugc nofollow" target="_blank">Know your meme</a></figcaption></figure><p id="867e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">研究人员发现低效计算发生的一个地方是完全连接的架构。在卷积方法中，全连接架构表示层<strong class="kw iu"> <em class="nh"> l </em> </strong>从层<strong class="kw iu"><em class="nh">l-1</em></strong><em class="nh"/>馈入，并连接到下一层<strong class="kw iu"> <em class="nh"> l+1 </em> </strong>，如下左图所示。研究人员发现，过滤器数量的任何均匀增加都会使计算加倍。所以他们建议从密集连接的架构转移到稀疏连接的架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/a7028ccb1b7654cf71b5bf1e1ec6ea63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LLDMLUHAyzRZNCh3CtLFw.png"/></div></div></figure><p id="95cc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不是实现一个<strong class="kw iu"> <em class="nh">类型</em> </strong>的卷积，稀疏架构执行具有多个滤波器大小的多个卷积，如上所示。并且来自这些层的输出的维度可以与“相同的”填充相匹配。</p><p id="ea0f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这里有几个不同尺寸的过滤器有一个重要的意义。在训练过程中，网络检测到的特征的规模起初是小的和局部的，并且随着其深入而变大。例如，假设模型检测到一只猫。该模型看到小比例的特征，例如猫的皮毛图案，然后它移动到更大比例，例如它的耳朵形状，然后是腿的数量。</p><div class="kj kk kl km gt ab cb"><figure class="nn kn no np nq nr ns paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d0761227c21563a1b056843daf39dca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*DdoXtROUqQNFZROb"/></div></figure><figure class="nn kn no np nq nr ns paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/788078cc41897820cd67d485a448ee7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*gijmQgWP1YRP4AyP"/></div></figure></div><p id="7f27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，即使图片中的猫处于不同的比例，模型仍然处理相同比例的特征映射。这是低效的，并且会损害性能。因此，拥有多个过滤器表明它现在知道如何根据输入图像“放大和缩小”。它使模型能够一次检测不同比例的图像。通过这样做，我们还可以处理选择参数的工作以及减少无效的计算。这是《盗梦空间》的第一脚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/148ad4f8b8cd8f7bff05a17ebdba6f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZW2AbyxrHYim1FWl9udzjw.png"/></div></div></figure><p id="31bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，Inception 模块看起来就像上图左边的那个。但是有一个问题。<strong class="kw iu"> <em class="nh">但是计算成本呢？</em> </strong>无论如何，稀疏连接的架构仍然需要相同或更多的计算量！研究人员解决这个问题的方法是实现<strong class="kw iu"> 1x1 </strong>卷积，如上图右侧所示。这里的<strong class="kw iu"> 1x1 </strong>卷积的目的是降维，就像我们在 ResNet 中讨论的那样。考虑到输入图像的(N，N，C)形状，计算时间减少了。如果图像的大小是 32×32，有 129 个通道，我们可以将参数的数量减少 9 倍！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/b13b0c305cdef55540c65c74bc7913b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DvMmswp1n2vf5y4mx0Ubw.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><a class="ae lq" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank">The architecture of Inception Network</a></figcaption></figure><p id="d1ee" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">GoogleNet 的最终架构如上图所示。这些模块串联在一起，总共形成 22 层。这是一个相当深的网络，它也无法避免臭名昭著的消失梯度问题。所以研究人员处理这个问题的方法是制造侧枝。</p><p id="3a6c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“渐变消失”是什么意思它表明梯度的值随着深度的增加而变得越来越小，这意味着梯度没有多少信息可以学习。所以我们可以说，从早期阶段的梯度携带更多的信息。换句话说，我们可以通过在中间层添加辅助分类器来获得额外的信息。此外，正如我们上面讨论的，检测到的特征随着层的不同而不同。因此，“中间产品”也有利于标签的分类。我们将这些支路的损耗加到加权网络的总损耗中。</p><h1 id="87a9" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">MobileNet</h1><p id="31e7" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">受我们上面讨论的网络的启发，“更深入和更复杂”是 CNN 研究的高度趋势。虽然这些网络带来了更高的准确性，但对于现实世界的应用来说，它们不够高效或“轻便”。为了在计算有限的平台上及时执行，它们需要更小更快。</p><p id="8c4b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">研究人员提出的一个解决方案是形成分解卷积。分解卷积表示将标准卷积分解成深度卷积和点态卷积。这被称为深度方向可分卷积。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/ff9bc84c023b98a9392c3bf9a2518519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*idbfJJLR_w8ibvb2LTOtIA.png"/></div></div></figure><p id="d448" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在深度方向可分离卷积中，我们首先分离输入和滤波器的每个通道，然后用相应的滤波器或以<em class="nh">深度方向</em>的方式对输入进行卷积。然后将输出连接在一起。这是<strong class="kw iu"> <em class="nh">深度方向的卷积</em> </strong>。之后我们做<strong class="kw iu"> <em class="nh">逐点卷积</em> </strong>，和<em class="nh"> 1x1 卷积</em>一样。现在看最后的结局。和标准的 3x3 卷积不一样吗？如果是，那么这样做有什么意义？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/c4d7e3896f8945d5556207dc5836dbc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nt9pYOgWOt5RMqm7Hr6MDQ.png"/></div></div></figure><p id="dbe1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">答案是计算。我们来比较一下两者的计算成本。当输入的波形为(<strong class="kw iu"> N </strong>、<strong class="kw iu"> N </strong>、<strong class="kw iu"> C1 </strong>)，输出通道为<strong class="kw iu"> C2 </strong>，滤波器大小为<strong class="kw iu"> F </strong>时，两者的总运算量应该如左图所示。如你所见，深度方向可分离卷积降低了计算成本。根据原始论文中的<a class="ae lq" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">所述，与采用标准卷积的相同架构相比，MobileNet 所需的计算量减少了 8 到 9 倍，精确度仅略有下降。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/9a37e53020733557aeb5e09cdde0f1bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kJegKTV54PIbgK-Zgs4XaQ.png"/></div></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><a class="ae lq" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank">The applications of MobileNet</a></figcaption></figure><p id="48dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">深度方向可分卷积也用于 Xception，其论文发表早于 MobileNet。但是我发现 MobileNet 论文对深度方向可分离卷积的概念给出了更直接的解释。</p><h1 id="566c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">减去</h1><p id="9f5a" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">现在，架构对你来说变得更简单和有形了吗？如我所说，理解结构背后的论点是至关重要的。所以我想再回顾一下电视网的一些亮点。</p><p id="0399" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，卷积层的滤波器大小不仅仅具有特征映射的意义。它可以像 VGG 一样改变参数的数量。我们还可以创建瓶颈来提取有意义的数据表示。<strong class="kw iu"> 1x1 </strong>卷积不仅仅是线性变换。它可以作为一个有用的技巧，在不显著影响输入数据的情况下，给出更多的非线性，改变维度，并减少计算。</p><p id="9813" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了获得更高的性能，ResNet 通过快捷连接来利用残差。这一层通过跨层有效地传递权重来更好地促进优化。另一方面，初始网络的策略是通过构建稀疏架构来使用多个过滤器。这是为了应对训练中音阶的变化。最后，MobileNet 选择将卷积分解为两个步骤。利用深度方向可分离的卷积，MobileNet 可以显著减少计算量和模型大小。</p><h1 id="39c0" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">参考</h1><ul class=""><li id="1576" class="lr ls it kw b kx mx la my ld nx lh ny ll nz lp oa lx ly lz bi translated">卡伦·西蒙扬和安德鲁·齐泽曼，<a class="ae lq" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> <em class="nh">用于大规模图像识别的极深度卷积网络</em> </a>，2015</li><li id="9ae0" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp oa lx ly lz bi translated">、何等<a class="ae lq" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> <em class="nh">深度残差学习用于图像识别</em> </a>，2015</li><li id="1572" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp oa lx ly lz bi translated">Christian Szegedy 等人<a class="ae lq" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank"><em class="nh"/></a>2014 年</li><li id="6223" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp oa lx ly lz bi translated">Christian Szegedy 等人，<a class="ae lq" href="https://arxiv.org/pdf/1602.07261.pdf" rel="noopener ugc nofollow" target="_blank"> Inception-v4，Inception-ResNet 以及剩余连接对学习的影响</a>，2016</li><li id="5f2b" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp oa lx ly lz bi translated">Franc ois Chollet，<a class="ae lq" href="https://arxiv.org/pdf/1610.02357.pdf" rel="noopener ugc nofollow" target="_blank">异常:深度可分卷积深度学习</a>，2017</li><li id="d460" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp oa lx ly lz bi translated">Andrew G. Howard 等人，<a class="ae lq" href="https://arxiv.org/pdf/1704.04861.pdf" rel="noopener ugc nofollow" target="_blank"> MobileNets:面向移动视觉应用的高效卷积神经网络</a>，2017</li></ul></div><div class="ab cl ob oc hx od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="im in io ip iq"><p id="c007" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个故事引起你的共鸣了吗？请与我们分享您的见解。我总是乐于交谈，所以请在下面留下评论，分享你的想法。我还在<a class="ae lq" href="https://www.linkedin.com/in/jiwon-jeong/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> LinkedIn </strong> </a>上分享有趣和有用的资源，所以请随时关注并联系我。下次我会带来另一个有趣的故事。一如既往，敬请期待！</p></div></div>    
</body>
</html>