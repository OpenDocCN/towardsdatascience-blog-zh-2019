<html>
<head>
<title>Linear Regression In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-in-python-64679ab58fc7?source=collection_archive---------7-----------------------#2019-07-26">https://towardsdatascience.com/linear-regression-in-python-64679ab58fc7?source=collection_archive---------7-----------------------#2019-07-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a09d026d460c9799500403fcc08c782a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*z24E9VtJbKzf5z33"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/concentrated-black-kid-doing-sums-5905857/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/concentrated-black-kid-doing-sums-5905857/</a></figcaption></figure><div class=""/><p id="e037" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以你决定学习机器学习。无论你是出于职业原因还是纯粹出于好奇，你都来对地方了。在接下来的文章中，我们将了解机器学习的<em class="le">“Hello World”</em>、<strong class="ki jk">线性回归</strong>。</p><h1 id="998d" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">算法</h1><p id="27eb" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">在机器学习的背景下，当人们谈到模型时，他们指的是在给定一组输入(<em class="le"> x1，x2，x3… </em>)的情况下用于预测因变量(<em class="le"> y </em>)的函数。在线性回归的情况下，模型采用<strong class="ki jk"> <em class="le"> y = wx + b </em> </strong>的形式。</p><p id="f90a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们绘制了学习时间和期末考试成绩之间的关系。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/329005c71109d5c3405feaccf2d1ba7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*eKMDnpRk4ZHIWNpS7q_buQ.png"/></div></figure><p id="c4c4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，假设我们为我们的模型<strong class="ki jk">任意选择了<strong class="ki jk"> <em class="le"> y = 3x + 2 </em> </strong>。如果我们画一条相应的线，它可能看起来像这样。</strong></p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d32688aad1df2e133d7f92e1a676d2ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*7uR31oSLb98nosBGnHqfEQ.png"/></div></figure><p id="aefd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性回归的目标是找到最佳拟合线，其中最佳拟合线由具有最小可能损失的线定义。最常用的损失函数之一是均方误差(MSE)。它的方程式写如下。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mo"><img src="../Images/00587a6b01883eb04590857ad708220e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E3hOkCUcCkEzJESei1o29w.png"/></div></div></figure><p id="73cc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过从一个数据点减去另一个数据点来计算从直线到给定数据点的距离。我们取误差的平方，因为我们不希望低于实际值的预测值被高于实际值的预测值抵消。换句话说，我们想消除负面影响。你还会看到人们使用平均绝对误差(MAE ),只要你在比较模型时坚持使用一个，使用一个比另一个没有任何优势。我们对距离求和，以获得整个数据集的总误差。然后我们将它除以样本总数，因为当你比较模型时，样本越多的数据集误差越大，因为它的样本越多。</p><p id="846f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在前面的图像中，从直线到每个点的距离被绘制为红色箭头。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/a52344a9d5a82e93c52dc42774036e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*5JSBPCfNWW9OGnSAC2is3A.png"/></div></figure><h2 id="e97d" class="mq lg jj bd lh mr ms dn ll mt mu dp lp kr mv mw lt kv mx my lx kz mz na mb nb bi translated">梯度下降</h2><p id="28e3" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">如果我们画一个曲线图，描述斜率 w 和总损耗 T23 之间的关系，它看起来如下。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/fd980588388d02f47e6bb769f9cab8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*yJkcgGhUo-2R-_enOTGrmA.png"/></div></div></figure><p id="61da" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它呈现抛物线形状，因为当斜率 w 接近无穷大时，损耗趋于无穷大。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/c9169addb7fa2fe6ea59ad9af17f0a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*fLDtjbOL02fP4q7ghx265A.png"/></div></figure><p id="96f8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，随着斜率 w 接近负无穷大，损耗趋于无穷大。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/38e310da585946d17e69dd7e05674b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*gqUe4xyU-YTtvEHp6Fxm-A.png"/></div></figure><p id="18cd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当损失最小时，最佳拟合线的斜率将等于<em class="le"> w </em>的值。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/47207e33aa6f1ea896253c4970baf7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*SPdfIxVTgPnb06rEPdxLMQ.png"/></div></figure><p id="dfd5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以利用<strong class="ki jk">梯度下降算法</strong>向全局最小值收敛，而不是以伪随机方式选择斜率值(即查看图表并进行有根据的猜测)。</p><p id="8be2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">梯度</em>是微积分中一个众所周知的概念。<em class="le">梯度</em>是偏导数的向量，它总是指向<strong class="ki jk">最陡上升</strong>的方向(在<em class="le"> f(x) </em>中最大增加的方向)。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/8e70d546b2fac47bb1e02f0b5873747e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/0*qP2RUJMdFWnoLEzy.png"/></div></figure><p id="d74e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在我们的例子中，我们正在寻找使我们的损失函数最小化的方向，因此“<strong class="ki jk"> <em class="le">【下降】</em></strong><strong class="ki jk"><em class="le">【梯度下降】。</em> </strong></p><p id="0924" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了确定损失函数曲线上的下一个点，梯度下降算法将梯度大小的一部分添加到起始点，如下图所示。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/8bc63a0a328e17f3802848f018356fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*QLkPFHEHvnLcotdxv1zVTA.png"/></div></figure><p id="7648" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">梯度的大小乘以一个叫做<strong class="ki jk">学习率</strong>的东西。学习率决定了算法的步长。</p><p id="128a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果幅度梯度(偏导数)为 10，学习率为 0.01，那么梯度下降算法将选择距离前一个点 0.1 的下一个点。</p><p id="52b5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们有大量的数据，并且选择的学习率太小，那么训练我们的模型可能需要很长时间。从表面上看，这听起来没那么糟糕，但是一个需要花几天时间训练的模型很难进行调整和实验。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0b959f24a0d0ee5dbe049b722fe41b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*tGjjhtsrtaGJtirHbo4o6A.png"/></div></figure><p id="782a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果学习率太大，我们可能会错过全局最小值。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/b119f5db1520b4f19b3ce2653db2ca53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*IecAXCXGxjh437_Vo021zA.png"/></div></figure><p id="9f0c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个回归问题，都有一个既不太大也不太小的金发女孩学习率。数据科学家的部分工作是处理<strong class="ki jk">超参数</strong>(即学习率和迭代次数)，否则学习过程可能会花费太长时间和/或最终结果不佳。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/66255d4e56b8c04593280eb505739c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*svIuWxUlVYl7Bcaf2bblfQ.png"/></div></figure><p id="96e4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看梯度下降的数学基础。我保证没那么糟。首先，我们用直线方程代替回归线的 y 值。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/6c6fb151771fd9089ad54a81c46c1927.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQlOmLHkMVSD_s9VOz8-ag.png"/></div></div></figure><p id="a8c5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们计算斜率和 y 轴截距的偏导数。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/8fc410d7a5e8c5084f84d9b604811dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rsNO9ytzEVQQzwdFa0DdKg.png"/></div></div></figure><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/c8a1f06bf1a8d976932d3bc0f0a50200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faSooj6-CrQhVkuHTJ_j_g.png"/></div></div></figure><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/fe49c6f746cba6d788a185ef584a145d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUcjvD-CkPZQ5ujk_L2enA.png"/></div></div></figure><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/9029e3ddc12c2f017974158cea7c0f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TX2KoH1Qxo0bwsXXps89-Q.png"/></div></div></figure><p id="1d68" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了确定每个变量的新值，我们在每次迭代中重复这个过程。正如我们之前提到的，我们将梯度乘以一个叫做学习率的常数。值得一提的是，方程是单独写的，但更多的时候你会看到它被写成一个向量。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/6161161fdcc54076a0c20baa10bdda3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2lQvdvk6IrVIHYGXFCYaIg.png"/></div></div></figure><p id="56f9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练了我们的线性回归模型(<strong class="ki jk"> <em class="le"> y = wx + b </em> </strong>)之后，我们获得了我们的数据集的最佳拟合线。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2bf54ef9dc5bdaeee0408aac1057844f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*h3g363wHJOcJgtqlHwf9dQ.png"/></div></figure><p id="93b4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们可以使用我们的模型，根据学生学习的小时数来预测他们的成绩。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7e0caa2186e790c613419e7c53153583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*UTNGq7mpYUkg-94V-iiamw.png"/></div></figure><h1 id="7602" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">密码</h1><p id="6d1c" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">让我们看看如何使用 Python 从头开始实现线性回归。首先，导入以下库。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="5051" class="mq lg jj nu b gy ny nz l oa ob">from sklearn.datasets import make_regression<br/>from matplotlib import pyplot as plt<br/>from sklearn.linear_model import LinearRegression<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="6b4b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用<code class="fe oc od oe nu b">scikit-learn</code>库来生成非常适合回归的样本数据。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="14a0" class="mq lg jj nu b gy ny nz l oa ob">x, y = make_regression(n_samples=50, n_features=1, n_informative=1, n_targets=1, noise=5)</span></pre><p id="1403" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很多时候，你需要某种基准来衡量你的模型的性能，通常对于回归问题，我们使用平均值。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="9b25" class="mq lg jj nu b gy ny nz l oa ob">starting_slope = 0<br/>starting_intercept = float(sum(y)) / len(y)</span></pre><p id="0dbb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们利用<code class="fe oc od oe nu b">matplotlib</code>来绘制数据和平均值。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="6b56" class="mq lg jj nu b gy ny nz l oa ob">plt.scatter(x, y)<br/>plt.plot(x, starting_slope * x + starting_intercept, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/31700f379ed2b25dd902253a8be1322e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*6SUvN7hQUyxZa5PJCaZaQw.png"/></div></figure><p id="bc34" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们编写一个函数来计算均方差。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="2293" class="mq lg jj nu b gy ny nz l oa ob">def mse(y_actual, y_pred):<br/>    error = 0<br/>    <br/>    for y, y_prime in zip(y_actual, y_pred):<br/>        error += (y - y_prime) ** 2<br/>    <br/>    return error</span></pre><p id="6aaa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">均方差越高，模型越差。想象一下从直线到每个数据点的箭头。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="7667" class="mq lg jj nu b gy ny nz l oa ob">mse(y, starting_slope * x + starting_intercept)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/40203a09bffb195b7d1e83c057d8c6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*H2L4Su_KCks_ytU0Y4K7uw.png"/></div></figure><p id="e2ac" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们看看如何从头开始实现梯度下降。</p><p id="2ff0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们创建一个函数来计算斜率和截距的偏导数。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="2c18" class="mq lg jj nu b gy ny nz l oa ob">def calculate_partial_derivatives(x, y, intercept, slope):<br/>    partial_derivative_slope = 0<br/>    partial_derivative_intercept = 0<br/>    n = len(x)</span><span id="0ce9" class="mq lg jj nu b gy oh nz l oa ob">    for i in range(n):<br/>        <br/>        xi = x[i]<br/>        yi = y[i]</span><span id="c91e" class="mq lg jj nu b gy oh nz l oa ob">    partial_derivative_intercept += - (2/n) * (yi - ((slope * xi) +      intercept))<br/>    partial_derivative_slope += - (2/n) * xi * (yi - ((slope * xi) + intercept))<br/>        <br/>    return partial_derivative_intercept, partial_derivative_slope</span></pre><p id="eef8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们定义一个函数，通过向解决方案迈出一小步来迭代改进我们的模型。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="8c68" class="mq lg jj nu b gy ny nz l oa ob">def train(x, y, learning_rate, iterations, intercept, slope):</span><span id="ded8" class="mq lg jj nu b gy oh nz l oa ob">for i in range(iterations):<br/>        <br/>        partial_derivative_intercept, partial_derivative_slope = calculate_partial_derivatives(x, y, intercept, slope)<br/>            <br/>        intercept = intercept - (learning_rate * partial_derivative_intercept)<br/>        slope = slope - (learning_rate * partial_derivative_slope)<br/>        <br/>    return intercept, slope</span></pre><p id="25db" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们任意选择学习率和迭代次数。对于额外的练习，我建议您尝试替换一些其他值，并观察模型的行为。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="95d2" class="mq lg jj nu b gy ny nz l oa ob">learning_rate = 0.01<br/>iterations = 300</span></pre><p id="d7a1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们训练模型并获得最佳拟合线的截距和斜率值。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="c526" class="mq lg jj nu b gy ny nz l oa ob">intercept, slope = train(x, y, learning_rate, iterations, starting_intercept, starting_slope)</span></pre><p id="3f8e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用列表理解来获得沿着我们的线的每个 x 值对应的 y 值。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="c84f" class="mq lg jj nu b gy ny nz l oa ob">linear_regression_line = [slope * xi + intercept for xi in x]</span></pre><p id="8a23" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们绘制数据来看看我们做得如何。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="c0c9" class="mq lg jj nu b gy ny nz l oa ob">plt.scatter(x, y)<br/>plt.plot(x, linear_regression_line, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/07cfd0d115a77143f034fc3b9bafcbd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*PoobB9GjjWjiuc5i-xkSlQ.png"/></div></figure><p id="0f14" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看起来比平均水平好多了。然而，在比较模型时，有时使用更具体的东西会有所帮助。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="e25c" class="mq lg jj nu b gy ny nz l oa ob">mse(y, linear_regression_line)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e4bb837a0013af6e313e9e7f4f507f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*FY6pCP59JrcdFpC4rzir8g.png"/></div></figure><p id="d1d8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如我们所见，均方误差比平均值产生的误差低得多。</p><p id="6af5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用<code class="fe oc od oe nu b">scikit-learn</code>库提供的预定义类，而不是每次都从头开始实现梯度下降算法。首先，我们创建一个类<code class="fe oc od oe nu b">LinearRegression</code>的实例。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="7198" class="mq lg jj nu b gy ny nz l oa ob">lr = LinearRegression()</span></pre><p id="5bf8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们通过调用<code class="fe oc od oe nu b">fit</code>方法来训练模型。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="68fb" class="mq lg jj nu b gy ny nz l oa ob">lr.fit(x, y)</span></pre><p id="b485" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了获得沿线的值，我们调用<code class="fe oc od oe nu b">predict</code>函数。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="c9da" class="mq lg jj nu b gy ny nz l oa ob">y_pred = lr.predict(x)</span></pre><p id="5c27" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，我们得到或多或少相同的结果。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="ba8c" class="mq lg jj nu b gy ny nz l oa ob">plt.scatter(x, y)<br/>plt.plot(x, y_pred, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9a08cb57c5645cd6521f72b3d80f1f3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*JJC_voxs1cEr8XutUQmoLQ.png"/></div></figure><p id="f077" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">均方误差略好于我们的实现。我鼓励你检查源代码，找到我们的原因。</p><pre class="mj mk ml mm gt nt nu nv nw aw nx bi"><span id="6986" class="mq lg jj nu b gy ny nz l oa ob">mse(y, y_pred)</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/3b412a32695cf41f2e234094b9a659c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*9OUcDPSoTec8TCznQzyAXQ.png"/></div></figure></div></div>    
</body>
</html>