<html>
<head>
<title>Scale, Standardize, or Normalize with Scikit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Scikit-Learn 进行扩展、标准化或规范化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02?source=collection_archive---------1-----------------------#2019-03-04">https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02?source=collection_archive---------1-----------------------#2019-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e0bf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">何时使用<em class="ki">最小最大缩放器</em>、<em class="ki">鲁棒缩放器</em>、<em class="ki">标准缩放器</em>和<em class="ki">规格化器</em></h2></div><p id="86a3" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">当要素处于相对相似的规模并且接近正态分布时，许多机器学习算法会工作得更好。<em class="lf"> MinMaxScaler </em>、<em class="lf"> RobustScaler </em>、<em class="lf"> StandardScaler </em>和<em class="lf"> Normalizer </em>是<a class="ae lg" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>方法，用于为机器学习预处理数据。您需要哪种方法(如果有)取决于您的模型类型和特征值。</p><p id="e2a2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">本指南将重点介绍这些方法之间的差异和相似之处，并帮助您了解何时使用哪种工具。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lh"><img src="../Images/f9e2430125eb1f9ad81c30ece94d2401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZM1_f0wqIE-bsECnvd-yg.jpeg"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk">Scales</figcaption></figure><p id="af67" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这些方法经常出现在机器学习工作流中，我发现很难找到关于何时使用哪种方法的信息。评论员经常交替使用术语<em class="lf">标度</em>、<em class="lf">标准化</em>和<em class="lf">正常化</em>。然而，它们也有一些不同之处，我们将研究的四个 scikit-learn 函数做不同的事情。</p><p id="f621" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">首先，请注意以下几点:</p><ul class=""><li id="c5d9" class="lx ly it kl b km kn kp kq ks lz kw ma la mb le mc md me mf bi translated">这篇文章所基于的 Jupyter 笔记本可以在<a class="ae lg" href="https://www.kaggle.com/discdiver/guide-to-scaling-and-standardizing" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</li><li id="2646" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">在本文中，我们不考虑对数变换或其他旨在减少误差异方差的变换。</li><li id="1532" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">本指南自<a class="ae lg" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn v0.20.3 </a>起生效。</li></ul><h1 id="af31" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">这些术语是什么意思？</h1><p id="bfd3" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated"><a class="ae lg" href="https://en.wikipedia.org/wiki/Scaling_(geometry)" rel="noopener ugc nofollow" target="_blank">缩放</a>一般是指改变数值的<strong class="kl iu">范围</strong>。分布的形状不会改变。想想一个建筑的比例模型是如何与原始模型具有相同的比例，只是比例更小。这就是为什么我们说它是按比例绘制的。该范围通常设置为 0 到 1。</p><p id="4eea" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><a class="ae lg" href="https://en.wikipedia.org/wiki/Standard_score" rel="noopener ugc nofollow" target="_blank">标准化</a>通常是指改变数值，使分布的<strong class="kl iu"> <em class="lf">标准</em> </strong>偏差等于 1。缩放通常是隐含的。</p><p id="904b" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><a class="ae lg" href="https://en.wikipedia.org/wiki/Normalization_(statistics)" rel="noopener ugc nofollow" target="_blank"> Normalize </a>可以用来表示以上任何一种(以及更多！).我建议你避免使用术语<em class="lf"> normalize，</em>，因为它有很多定义，容易造成混淆。</p><p id="ef35" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">如果你在交流中使用这些术语，我强烈建议你定义它们。</p><h1 id="989a" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">为什么要扩展、标准化或规范化？</h1><p id="a361" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated">当特征处于相对相似的规模和/或接近正态分布时，许多机器学习算法执行得更好或收敛得更快。这种算法族的例子包括:</p><ul class=""><li id="cb45" class="lx ly it kl b km kn kp kq ks lz kw ma la mb le mc md me mf bi translated">线性和逻辑回归</li><li id="aa58" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">最近的邻居</li><li id="9917" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">神经网络</li><li id="ef14" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">具有径向偏向核函数的支持向量机</li><li id="86e6" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">主成分分析</li><li id="3d15" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">线性判别分析</li></ul><p id="4853" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">缩放和标准化可以帮助这些算法以更易理解的形式获得特性。</p><p id="5a2a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们正在检查的四个 scikit-learn 预处理方法遵循如下所示的 API。<em class="lf"> X_train </em>和<em class="lf"> X_test </em>是通常的数字数组或熊猫数据帧。</p><pre class="li lj lk ll gt ni nj nk nl aw nm bi"><span id="3ca1" class="nn mm it nj b gy no np l nq nr">from sklearn import preprocessing</span><span id="3528" class="nn mm it nj b gy ns np l nq nr">mm_scaler = preprocessing.MinMaxScaler()<br/>X_train_minmax = mm_scaler.fit_transform(X_train)</span><span id="a0e1" class="nn mm it nj b gy ns np l nq nr">mm_scaler.transform(X_test)</span></pre><p id="2993" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我们将查看一些发行版，并对它们应用四种 scikit-learn 方法中的每一种。</p><h1 id="25f9" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">原始资料</h1><p id="888e" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated">我创建了四个具有不同特征的发行版。这些分布是:</p><ul class=""><li id="eb67" class="lx ly it kl b km kn kp kq ks lz kw ma la mb le mc md me mf bi translated"><strong class="kl iu">贝塔</strong>——负偏斜</li><li id="7878" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated"><strong class="kl iu">指数</strong>——正偏斜</li><li id="c2a9" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated"><strong class="kl iu">轻微型</strong> —正常，轻微型</li><li id="2d0f" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated"><strong class="kl iu">板状软骨</strong> —正常，板状软骨</li><li id="97ca" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated"><strong class="kl iu">双峰</strong> —双峰</li></ul><p id="45ac" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这些值都具有相对相似的比例，如下面的内核密度估计图(kdeplot)的 x 轴所示。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nt"><img src="../Images/8c86b8c5dd5e644199e41e62361b57d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9A-rlaYWkOnWu-JiftKDQA.png"/></div></div></figure><p id="2e19" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">然后我添加了第六个分布，它有更大的值(正态分布)——<strong class="kl iu">正态</strong>。</p><p id="6296" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在我们的 kdeplot 看起来像这样:</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nt"><img src="../Images/346e2d380e6f5426e6a78a32c5ebe50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pKGpDDC1lACuwDKmmjsyow.png"/></div></div></figure><p id="9975" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">仔细看显示器，你可能会注意到右边有一个绿色的大数值小条。这是我们特征的描述性统计数据。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nu"><img src="../Images/333e6350b1704339ec2c0766de95c388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYYapWbtMgpEoiZ0zqfsTg.png"/></div></div></figure><p id="c013" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">好吧，让我们开始缩放！</p><h1 id="8891" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">最小最大缩放器</h1><p id="b705" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated">对于特性中的每个值，<a class="ae lg" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" rel="noopener ugc nofollow" target="_blank">最小最大值缩放器</a>减去特性中的最小值，然后除以范围。该范围是原始最大值和原始最小值之间的差值。</p><p id="bde9" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">MinMaxScaler 保留原始分布的形状。它不会有意义地改变嵌入在原始数据中的信息。</p><p id="b9fe" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">注意，MinMaxScaler 不会降低异常值的重要性。</p><p id="d341" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">MinMaxScaler 返回的特征的默认范围是 0 到 1。</p><p id="4a23" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">下面是应用 MinMaxScaler 后的 kdeplot。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nt"><img src="../Images/6d18973a57af0d8b5735860bef1365e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQ3jl2lecfgjIQT94mAFpw.png"/></div></div></figure><p id="94f6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">请注意这些特征是如何以相同的相对比例呈现的。每个要素值之间的相对间距保持不变。</p><p id="74cc" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">MinMaxScaler 是一个不错的起点，除非您知道您希望您的要素具有正态分布，或者您有异常值，并且您希望它们具有较小的影响。</p><div class="li lj lk ll gt ab cb"><figure class="nv lm nw nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><img src="../Images/70193a99e6213f0454ad57adaeeec065.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*J8hwbr6C_2Yq8QbpEsOv8Q.jpeg"/></div></figure><figure class="nv lm ob nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><img src="../Images/f05d025c0129b3fdbcefdf939d05ce3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*Vf-eJZwWThIODPbxCO9O0g.jpeg"/></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk oc di od oe">Different types of scales</figcaption></figure></div><h1 id="044c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">鲁棒定标器</h1><p id="adfd" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated"><a class="ae lg" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank"> RobustScaler </a>通过减去中值，然后除以四分位数范围(75%值-25%值)来转换特征向量。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nt"><img src="../Images/bd18f22df4729fb5a2471741fc44b915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1nBw0cKMR3K3zHsMisckfA.png"/></div></div></figure><p id="a68f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">像 MinMaxScaler 一样，我们的具有大值的特征—<em class="lf">normal—big</em>—现在与其他特征具有相似的比例。请注意，RobustScaler 不像 MinMaxScaler 那样将数据缩放到预定的间隔。它不符合我前面介绍的<em class="lf">规模</em>的严格定义。</p><p id="494a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">请注意，应用 RobustScaler 后，各功能的范围比应用 MinMaxScaler 时更大。</p><p id="e0b4" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">相对于最小最大缩放器，如果要减少异常值的影响，请使用鲁棒缩放器。</p><p id="9655" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在让我们转向 StandardScaler。</p><h1 id="11c8" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">标准缩放器</h1><p id="8468" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated">StandardScaler 是业界公认的算法。🙂</p><p id="7950" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated"><a class="ae lg" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">标准缩放器</a>通过减去平均值然后缩放至单位方差来标准化特征。单位方差是指所有值除以标准偏差。StandardScaler 不符合我前面介绍的<em class="lf">标度</em>的严格定义。</p><p id="92bd" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">StandardScaler 产生标准差等于 1 的分布。方差也等于 1，因为<em class="lf">方差=标准差的平方</em>。而 1 的平方= 1。</p><p id="d278" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">StandardScaler 使分布的平均值约为 0。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nt"><img src="../Images/af20c5d04ccc45d99c56e9eb753c759d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iYwthw50xJ1vtovHTkGtbg.png"/></div></div></figure><p id="06b5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在上图中，你可以看到所有四个分布都有一个接近于零的平均值和单位方差。这些值的比例相似，但范围比 MinMaxScaler 之后的更大。</p><p id="0ab7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">深度学习算法通常要求零均值和单位方差。回归型算法也受益于小样本的正态分布数据。</p><p id="944e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">现在让我们看看规格化器。</p><h1 id="7ed3" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">标准化者</h1><p id="937e" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated"><a class="ae lg" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html" rel="noopener ugc nofollow" target="_blank">规格化器</a>作用于行，而不是列！我觉得这很不直观。在文档中很容易遗漏这些信息。</p><p id="2aa6" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">默认情况下，L2 归一化应用于每个观测值，以便一行中的值具有单位范数。<em class="lf">具有 L2 的单位范数</em>意味着如果每个元素被平方并求和，则总和等于 1。或者，可以应用 L1(又名出租车或曼哈顿)归一化来代替 L2 归一化。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nt"><img src="../Images/f8cacdf04022dc8c223204c307a41a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euz0G-_ZV3IC3P7-fkimzw.png"/></div></div></figure><p id="ed1c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">Normalizer 确实将所有特征转换为-1 和 1 之间的值(<em class="lf">此文本更新于 2019 年 7 月</em>)。在我们的示例中，<em class="lf"> normal_big </em>的所有值都转换为. 9999999。</p><p id="72b0" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">你找到 Normalizer 的好用例了吗？如果有，请在 Twitter @discdiver 上告诉我。</p><p id="1fc9" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在大多数情况下，上面的其他预处理工具会更有帮助。</p><p id="99d8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">同样，scikit-learn 的规格化器作用于行，而不是列。</p><h1 id="a12c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">比较</h1><p id="6e17" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated">以下是应用最小最大缩放器、鲁棒缩放器和标准缩放器之前和之后的原始分布图。</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi of"><img src="../Images/9989cfbdd800e422a2d15f52828e8110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIWsHyk8SWRZAaMeh5HUYg.png"/></div></div></figure><p id="35ac" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">请注意，在这三种变换中的任何一种变换之后，这些值的比例都是相似的。🎉</p><h1 id="86be" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">包装</h1><p id="200b" class="pw-post-body-paragraph kj kk it kl b km nd ju ko kp ne jx kr ks nf ku kv kw ng ky kz la nh lc ld le im bi translated">扩展和标准化您的数据通常是一个好主意。我强烈建议在将数据输入深度学习算法之前，使用标准缩放器，或者使用依赖于观察相对距离的算法，或者使用 L2 正则化的算法。请注意，标准缩放会使回归系数的解释变得有点棘手。😉</p><h2 id="bbc7" class="nn mm it bd mn og oh dn mr oi oj dp mv ks ok ol mx kw om on mz la oo op nb oq bi translated">小贴士:</h2><ul class=""><li id="54c3" class="lx ly it kl b km nd kp ne ks or kw os la ot le mc md me mf bi translated">如果希望每个要素都具有零均值、单位标准差，请使用 StandardScaler。如果您想要更多正态分布的数据，并且愿意转换您的数据。查看 scikit-learn 的<code class="fe ou ov ow nj b"><a class="ae lg" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.QuantileTransformer.html" rel="noopener ugc nofollow" target="_blank">QuantileTransformer(output_distribution='normal')</a></code>。</li><li id="c1ae" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">如果你想轻触，使用 MinMaxScaler。它不失真。</li><li id="1749" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">如果您有异常值并希望减少它们的影响，您可以使用 RobustScaler。</li><li id="4660" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">谨慎使用规格化器——它规格化样本行，而不是特征列。它可以使用 l2 或 l1 归一化。</li></ul><h2 id="7d45" class="nn mm it bd mn og oh dn mr oi oj dp mv ks ok ol mx kw om on mz la oo op nb oq bi translated">这里有一个我在谷歌表单中做的<a class="ae lg" href="https://docs.google.com/spreadsheets/d/1woVi7wq13628HJ-tN6ApaRGVZ85OdmHsDBKLAf5ylaQ/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">备忘单，帮助你保持选项的直线性:</a></h2><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ox"><img src="../Images/356980520c9caac13456b63637821558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A9d4SEX0t_bAAPzZeVqwAQ.png"/></div></div></figure><p id="d824" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在本文中，您看到了 scikit-learn 如何帮助您对数据进行缩放、标准化和规范化。* * *我在 2021 年 8 月更新了这篇文章的图片和一些文字。感谢读者的反馈！***</p><p id="7be2" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">深入研究的资源:</p><ul class=""><li id="8776" class="lx ly it kl b km kn kp kq ks lz kw ma la mb le mc md me mf bi translated"><a class="ae lg" href="http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler" rel="noopener ugc nofollow" target="_blank">这里有一个关于预处理数据的 scikit-learn 文档</a>。</li><li id="0e4a" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated"><a class="ae lg" href="http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py." rel="noopener ugc nofollow" target="_blank">这是另一篇关于 sci kit-learn scaler 对异常值的影响的文档</a>。</li><li id="c613" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated">肖恩·欧文为概率分布提供了一个很好的指导。</li><li id="692b" class="lx ly it kl b km mg kp mh ks mi kw mj la mk le mc md me mf bi translated"><a class="ae lg" href="http://benalexkeen.com/feature-scaling-with-scikit-learn/" rel="noopener ugc nofollow" target="_blank">这是 Ben Alex Keen 的另一个比较这些功能的指南</a>。</li></ul><p id="3153" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我希望这个指南对你有所帮助。如果你有，请分享到你最喜欢的社交媒体频道。👏</p><p id="04a8" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">我写关于<a class="ae lg" href="https://memorablepython.com" rel="noopener ugc nofollow" target="_blank"> Python </a>、<a class="ae lg" href="https://memorabledocker.com" rel="noopener ugc nofollow" target="_blank"> Docker </a>、<a class="ae lg" href="https://memorablesql.com" rel="noopener ugc nofollow" target="_blank"> SQL </a>、<a class="ae lg" href="https://memorablepandas.com" rel="noopener ugc nofollow" target="_blank">熊猫</a>以及其他数据科学主题。如果你对这些话题感兴趣，阅读更多<a class="ae lg" href="https://medium.com/@jeffhale" rel="noopener">这里</a>并关注我的 Medium。😃</p><figure class="li lj lk ll gt lm gh gi paragraph-image"><a href="https://dataawesome.com"><div class="ab gu cl pa"><img src="../Images/ba32af1aa267917812a85c401d1f7d29.png" data-original-src="https://miro.medium.com/v2/format:webp/1*oPkqiu1rrt-hC_lDMK-jQg.png"/></div></a></figure><figure class="li lj lk ll gt lm gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi pb"><img src="../Images/501be93ecd109b6c2341e232caf4d9ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C-321Bvdsq6Oz4-rtFOMMw.jpeg"/></div></div><figcaption class="lt lu gj gh gi lv lw bd b be z dk">Scale on!</figcaption></figure></div></div>    
</body>
</html>