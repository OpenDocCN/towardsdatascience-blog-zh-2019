<html>
<head>
<title>Revisiting previous experiences can help artificial systems to learn faster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">重温以前的经历可以帮助人工系统更快地学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/revisiting-previous-experiences-can-help-artificial-systems-to-learn-faster-4975298697e8?source=collection_archive---------35-----------------------#2019-09-03">https://towardsdatascience.com/revisiting-previous-experiences-can-help-artificial-systems-to-learn-faster-4975298697e8?source=collection_archive---------35-----------------------#2019-09-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="934f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">优先体验重放建议混合新旧体验以加速学习</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d640120349a986d1fb90d200905a7312.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9fI-rWO37EjlYuk_"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@averey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Robina Weermeijer</a></figcaption></figure><p id="8177" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated">e enforcement Learning(RL)是一个框架，它有很大的潜力成为“下一级”人工智能的基础，即所谓的人类级人工智能。在[1]中，Nils J. Nilsson 将这个下一级描述为:</p><blockquote class="mb mc md"><p id="715f" class="kw kx me ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">“[……]通用的、可教育的系统，可以学习并被教会执行人类可以执行的数千种工作中的任何一种。”</p></blockquote><p id="2b9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">RL 成为实现这一目标的良好候选的原因很简单:它基于奖励的学习机制接近于人类大脑用于学习事物的机制。RL 代理通过试错来学习如何表现以最大化收到的奖励(可以想象一只狗从它的主人/训练员那里得到款待)。</p><p id="93b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管具有潜力，经典 RL 方法显示出许多局限性，即使能够解决许多复杂的问题，也远远不能实现这一通用的长期目标。</p><p id="8a0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在寻求改进的过程中，许多研究人员提出了扩展 RL 的生物启发方法。下面描述的方法包括以有效的方式使用存储的存储器来加速学习。</p><h1 id="bb32" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">记忆帮助我们学习</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/23b65de7d655ec686b4a2664c3b2ae5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkgGJyyX5769eaJAPjfwog.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://pixabay.com/users/Conmongt-1226108/" rel="noopener ugc nofollow" target="_blank">Conmongt</a></figcaption></figure><p id="59d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">神经科学领域的研究支持，包含先前经历的记忆在啮齿动物的海马体中重演，这在不同水平的大脑活动中观察到。</p><p id="b3e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果还显示，导致某种程度奖励的经历会更频繁地重演。更有趣的是，新鲜感还与重演某些经历的概率相关，如[2]所述:</p><blockquote class="mb mc md"><p id="c9be" class="kw kx me ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">“[……]我们已经确定，例如，在新的环境和奖励驱动的空间任务中，多巴胺能释放对于偏向随后重放轨迹的内容是重要的，有可能加强新的位置细胞组合和位置-奖励关联。”</p></blockquote><p id="f69d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着学习也是通过重温旧的经历来进行的，并且使用某种机制来选择更好的经历以优化学习过程，例如，选择呈现某种程度的新颖性或与收到的奖励相关联的经历。</p><h1 id="1ee3" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">体验回放</h1><p id="3af9" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">为了模仿生物系统中存在的这种经验机制，在[3]中提出了称为经验重放的方法，以及用于规划和教学的学习行为模型。这三种扩展方法被建议用来加速 AHC(自适应启发式批评家)和 Q-学习算法的学习。应用体验回放的动机是:</p><blockquote class="mb mc md"><p id="2438" class="kw kx me ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">“基本的 AHC 和 Q 学习算法[……]效率低下，因为通过反复试验获得的经验仅用于调整网络一次，然后就被丢弃。这是一种浪费，因为有些经历可能是罕见的，而有些经历(如涉及损害的经历)的获得成本很高。应该以有效的方式重复利用经验。”</p></blockquote><p id="7118" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设是存储和处理以前的经验比与环境互动“更便宜”。</p><p id="969f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经验定义为四重，<strong class="ky ir"> <em class="me"> (s，a，s’，r) </em> </strong>，意思是执行一个动作<strong class="ky ir"> <em class="me">一个</em> </strong>处于一个状态<strong class="ky ir"> <em class="me"> s </em> </strong>产生一个新的状态<strong class="ky ir"><em class="me">‘s’</em></strong>和奖励<strong class="ky ir"> <em class="me"> r </em> </strong>。当与环境交互时，代理记住(通过采样)它过去的经验，以便从中学习更多。</p><p id="4ea4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过应用体验重放缓解的两个主要问题是:</p><ol class=""><li id="e679" class="ng nh iq ky b kz la lc ld lf ni lj nj ln nk lr nl nm nn no bi translated"><strong class="ky ir">强相关更新:</strong>流行的基于随机梯度的算法假设变量是独立同分布的(独立同分布)。然而，独立同分布假设不太适合马尔可夫链(状态序列)。当把在线体验和存储的体验混合在一起时，就有可能“打破”连续状态之间的关联。</li><li id="7b24" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated"><strong class="ky ir">遗忘重要经验:</strong>如前所述，有些情况可能很少经历，经典 RL 算法只是在下一次迭代中丢弃旧的经验。</li></ol><h1 id="3e53" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">优先体验重放(PER)</h1><p id="86d3" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">经验回放的引入已经是 RL 的一大进步。然而，生物学证据支持一些经历比其他经历更频繁地重演。</p><p id="e718" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在[4]中，提出了经验重放方法的改进版本 PER。主要思想是使用 TD 误差作为一个度量来定义重放一些经验的概率。</p><p id="b8a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TD 误差代表给定状态的预测误差(模型输出和接收到的奖励之间的差异)，经常被用作优先化机制(例如，人工好奇心[5]和特征选择[6])。</p><p id="0c8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其动机是，类似于生物系统，学习系统可以从中学习更多的经验(即与大的 TD 误差相关的状态)应该更频繁地重放，从而加速学习过程。</p><p id="9d97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]中的结果表明，在应该学习如何玩 Atari 游戏的代理上应用 PER 允许将学习速度提高 2 倍。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/39a327d7371c29d793ca1d587432b778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vRlS0zpe8JHFriJ9"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@sharkovski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kirill Sharkovski</a></figcaption></figure><h1 id="5fab" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">不仅 RL 可以从中受益</h1><p id="fb88" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">[4]中给出的另一个有趣的结果是，同样的方法可以用于改进监督学习方法。该方法被重新命名为优先采样，包括更频繁地使用代表更高误差的例子，以集中优化(学习)过程:</p><blockquote class="mb mc md"><p id="b7be" class="kw kx me ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">“在监督学习的环境中，类似于优先重放的方法是从数据集中进行非均匀采样，每个样本使用基于其最后出现的错误的优先级。这可以帮助将学习集中在那些仍然可以学习的样本上，将额外的资源投入到(硬)边界情况中”</p></blockquote><p id="ad6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种方法在处理不平衡数据集时尤其有用，因为包含较少表示的类的示例将具有较高的采样优先级，前提是它们仍然会导致较大的预测误差。</p><p id="3527" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当应用于经典 MNIST 数字分类问题的类不平衡变体时，优先化采样被证明导致在泛化和学习速度方面的性能提高。</p><h1 id="11e0" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">参考</h1><p id="6d44" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">[1]Nils j . Nilsson《人类级人工智能？认真点！."艾杂志 26.4(2005):68–68。阿泽顿、劳拉·a、大卫·杜普雷和杰克·r·梅勒。"记忆痕迹重放:通过神经调节形成记忆巩固."神经科学趋势 38.9(2015):560–570。<br/> [3]林，隆基。"基于强化学习、规划和教学的自我改进反应代理."机器学习 8.3–4(1992):293–321。<br/> [4]绍尔，汤姆等，“优先化的经验重放”arXiv 预印本 arXiv:1511.05952 (2015)。<br/>【5】“人工智能如何变得好奇？”"【https://towardsdatascience . com/artificial-curiosity-e 1837 E4 ca 2c 9<br/>[6]孙，易，等《基于时差误差的增量基构造》。“ICML。2011.</p></div></div>    
</body>
</html>