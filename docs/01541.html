<html>
<head>
<title>How to Improve a Neural Network With Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用正则化方法改进神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3?source=collection_archive---------1-----------------------#2019-03-12">https://towardsdatascience.com/how-to-improve-a-neural-network-with-regularization-8a18ecda9fe3?source=collection_archive---------1-----------------------#2019-03-12</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><div class=""><h2 id="9b30" class="pw-subtitle-paragraph js iu iv bd b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj dk translated">了解如何应用 L2 正则化和辍学的神经网络</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/fb26023238f04cf9c7425bf6fb50737c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T8hl-RxL8p9YLhak"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Photo by <a class="ae la" href="https://unsplash.com/@jruscello?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jessica Ruscello</a> on <a class="ae la" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="7314" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">您刚刚构建了您的神经网络，并注意到它在训练集上表现得非常好，但在测试集上却没有那么好。</p><p id="88db" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这是过度拟合的标志。你的神经网络有很高的方差，它不能很好地概括未经训练的数据。</p><p id="2dc4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">解决过度拟合有两种常见方法:</p><ul class=""><li id="a9f0" class="lx ly iv ld b le lf lh li lk lz lo ma ls mb lw mc md me mf bi translated">获取更多数据</li><li id="5fa7" class="lx ly iv ld b le mg lh mh lk mi lo mj ls mk lw mc md me mf bi translated">使用正则化</li></ul><p id="943e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">获取更多的数据有时是不可能的，有时又非常昂贵。因此，正则化是减少过拟合从而提高模型性能的常用方法。</p><p id="04dd" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在这篇文章中，<strong class="ld iw"> L2 正则化</strong>和<strong class="ld iw">退出</strong>将被介绍为神经网络的正则化方法。然后，我们将对每种方法进行编码，看看它是如何影响网络性能的！</p><p id="89d6" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们走吧！</p><blockquote class="ml"><p id="8bc5" class="mm mn iv bd mo mp mq mr ms mt mu lw dk translated">对于机器学习、深度学习和人工智能的实践视频教程，请查看我的<a class="ae la" href="https://www.youtube.com/channel/UC-0lpiwlftqwC7znCcF83qg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> YouTube 频道</a>。</p></blockquote><figure class="mv mw mx my mz kp"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="ac01" class="nc nd iv bd ne nf ng nh ni nj nk nl nm kb nn kc no ke np kf nq kh nr ki ns nt bi translated">L2 正则化</h1><p id="e357" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">回想一下，在深度学习中，我们希望最小化以下成本函数:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi nz"><img src="../Images/09e4316b1c2df2d3faa79b2a59fd5b3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vfkbq9TKDkf594ff4cx8Ag.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Cost function</figcaption></figure><p id="9d4f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">其中<em class="oa"> L </em>可以是任意损失函数(如<a class="ae la" rel="noopener" target="_blank" href="/step-by-step-guide-to-building-your-own-neural-network-from-scratch-df64b1c5ab6e">交叉熵损失函数</a>)。现在，对于<strong class="ld iw"> L2 正则化</strong>我们增加了一个惩罚大权重的组件。</p><p id="0999" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，等式变为:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ob"><img src="../Images/9e1942bb3a2aa6a4ff7238ae054f57e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7kgUhfEzQ7qFqpWuDVDbw.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">L2 regularization</figcaption></figure><p id="5bcf" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">其中<em class="oa">λ</em>是<strong class="ld iw">正则化参数</strong>。注意增加了 Frobenius 范数，用下标<em class="oa"> F </em>表示。这实际上相当于矩阵的平方范数。</p><p id="62a4" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，<em class="oa">λ</em>是一个可以调整的参数。如果<em class="oa">λ</em>的值较大，则较大的权重值将受到更多的惩罚。类似地，对于较小的<em class="oa">λ</em>值，正则化效果较小。</p><p id="492e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这是有意义的，因为成本函数必须最小化。通过添加权重矩阵的平方范数并将其乘以正则化参数，大的权重将被降低，以便最小化成本函数。</p><h2 id="9265" class="oc nd iv bd ne od oe dn ni of og dp nm lk oh oi no lo oj ok nq ls ol om ns on bi translated">为什么正规化有效？</h2><p id="ceaf" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">如前所述，添加正则化分量将使权重矩阵的值下降。这将有效地去相关神经网络。</p><p id="fa69" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">回想一下，我们为激活函数提供了以下加权和:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oo"><img src="../Images/9f381c8ad2ec610801249fda8956dc5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MWFM3lNkwLabDhRw3OH_6g.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Weighted sum</figcaption></figure><p id="2409" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">通过减少权重矩阵中的值，<em class="oa"> z </em>也将减少，这反过来减少了激活函数的效果。因此，一个不太复杂的函数将适合数据，有效地减少过度拟合。</p><h1 id="67f8" class="nc nd iv bd ne nf ng nh ni nj nk nl nm kb nn kc no ke np kf nq kh nr ki ns nt bi translated">辍学正规化</h1><p id="107e" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">退出包括检查神经网络中的所有层，并设置保留或不保留某个节点的概率。</p><p id="f00a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">当然，输入层和输出层保持不变。</p><p id="2411" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">保持每个节点的概率设置为<strong class="ld iw">随机</strong>。您只需决定<strong class="ld iw">阈值</strong>:一个决定节点是否被保留的值。</p><p id="3b24" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">例如，如果您将阈值设置为 0.7，则有 30%的概率会从网络中删除一个节点。</p><p id="320f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，这将产生一个更小、更简单的神经网络，如下所示。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi op"><img src="../Images/8ff96d2850bc8cd1850326dbf938edef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skMXofkjeXtKzSr5lqIEmg.png"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Left: neural network before dropout. Right: neural network after dropout.</figcaption></figure><h2 id="b813" class="oc nd iv bd ne od oe dn ni of og dp nm lk oh oi no lo oj ok nq ls ol om ns on bi translated">为什么退学有效？</h2><p id="ed31" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">从一个神经网络中随机移除节点来调整它可能看起来很疯狂。然而，这是一种广泛使用的方法，并且被证明可以极大地提高神经网络的性能。那么，为什么它的效果这么好呢？</p><p id="9c6d" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">丢失意味着神经网络不能依赖任何输入节点，因为每个节点都有被删除的随机概率。因此，神经网络不愿意给予某些特征高的权重，因为它们可能会消失。</p><p id="474b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">因此，权重分布在所有要素上，使它们变得更小。这有效地缩小了模型并使其正规化。</p><h1 id="514b" class="nc nd iv bd ne nf ng nh ni nj nk nl nm kb nn kc no ke np kf nq kh nr ki ns nt bi translated">正规化的实施</h1><p id="f7dc" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">现在，让我们对一些样本数据实施丢失和 L2 正则化，以查看它如何影响神经网络的性能。</p><p id="09f2" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">参考完整笔记本<a class="ae la" href="https://github.com/marcopeix/Deep_Learning_AI/blob/master/2.Improving%20Deep%20Neural%20Networks/1.Practical%20Aspects%20of%20Deep%20Learning/Regularization.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="a2cb" class="oc nd iv bd ne od oe dn ni of og dp nm lk oh oi no lo oj ok nq ls ol om ns on bi translated">创建样本数据集</h2><p id="9973" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">我们首先创建一个样本数据集。在导入必要的<a class="ae la" href="https://github.com/marcopeix/Deep_Learning_AI/blob/master/2.Improving%20Deep%20Neural%20Networks/1.Practical%20Aspects%20of%20Deep%20Learning/Regularization.ipynb" rel="noopener ugc nofollow" target="_blank">库之后，</a>我们运行下面这段代码:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oq"><img src="../Images/3a24aacd880dee6469fdb38ea03d85a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnZyZCno-IFhKGnJmwddjA.png"/></div></div></figure><p id="c3f2" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们应该得到:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi or"><img src="../Images/96ccc9b335d980bcaf243bec5073ffc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*NOnWfsvKLJcriOM37tNJGw.png"/></div></figure><p id="5dcd" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太好了！这是一个简单的随机数据集，有两个类，我们现在将尝试编写一个神经网络，它将对每个数据进行分类，并生成一个决策边界。</p><h2 id="0c87" class="oc nd iv bd ne od oe dn ni of og dp nm lk oh oi no lo oj ok nq ls ol om ns on bi translated">非正则化模型</h2><p id="55cc" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">现在，我们定义一个模型模板来适应正则化:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="os nb l"/></div></figure><p id="6ac0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">花时间阅读代码并理解它的作用。请注意对 L2 正则化有用的<em class="oa"> lambd </em>变量。此外，<em class="oa"> keep_prob </em>变量将用于 dropout。</p><p id="2e26" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，让我们运行一个没有正则化的神经网络，作为基准性能。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ot"><img src="../Images/baf72a59f7dc2986e23a5798c05e3ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TWDji81TTaWnJjTDJRzpQ.png"/></div></div></figure><p id="336a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">我们得到了:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e13f56af59dc4a17257af2c52138867c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*SF_UALXyguSW83Jukqwwhw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Baseline performance of a non-regularized model</figcaption></figure><p id="5d1b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">还不错！让我们画出决策边界:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ov"><img src="../Images/1ff3ad2035313809567a3c3fed4be1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YnLVk77pqQIghitUa6QrDA.png"/></div></div></figure><p id="d917" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">您应该会看到:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/a63adf0a0cb2f90dff080c0287d3716c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*iuk82QCAH0-zmuQl4aaVxA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Decision boundary without regularization</figcaption></figure><p id="a0e9" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在上面的图中，您会注意到模型过度拟合了数据的某些部分。我们将以此为基准，看看正则化如何提高模型的性能。</p><h2 id="a5ed" class="oc nd iv bd ne od oe dn ni of og dp nm lk oh oi no lo oj ok nq ls ol om ns on bi translated">L2 正则化</h2><p id="cebc" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">在使用 L2 正则化之前，我们需要定义一个函数来计算适应正则化的成本:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ow"><img src="../Images/3d4538011221b4b089a874d1f976bec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTG_ZEyQq0LrT87r014mxw.png"/></div></div></figure><p id="b00a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">最后，我们用正则化定义反向传播:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ox"><img src="../Images/c3f3237be52040e219efda1564bf1004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V6wI25ZunUPHVOPn9zf3-A.png"/></div></div></figure><p id="46d5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太好了！现在，我们可以使用我们的模型模板与 L2 正则化！将<em class="oa">λ</em>的值设置为 0.7，我们得到:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/24124f669e4dc9a8d8410481d6991685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*Ny3mfQOhIY5kVXbm72UIcA.png"/></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/fb3f6d7b7dc619db51a9a5f1cebddf38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*k3vbvbXmf8YjGJWrJBRkxg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Decision boundary with L2 regularization</figcaption></figure><p id="7e63" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">厉害！我们提高了测试精度，您会注意到模型不再过度拟合数据了！</p><p id="9258" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，让我们看看辍学者是否能做得更好。</p><h2 id="6c7d" class="oc nd iv bd ne od oe dn ni of og dp nm lk oh oi no lo oj ok nq ls ol om ns on bi translated">拒绝传统社会的人</h2><p id="69a4" class="pw-post-body-paragraph lb lc iv ld b le nu jw lg lh nv jz lj lk nw lm ln lo nx lq lr ls ny lu lv lw io bi translated">首先，我们需要重新定义正向传播，因为我们需要随机消除某些节点的影响:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="os nb l"/></div></figure><p id="dc6a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">当然，我们现在必须定义辍学的反向传播:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="os nb l"/></div></figure><p id="73bf" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太好了！让我们看看阈值为 0.8 时，模型在辍学情况下的表现如何:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/4a62cb3141d4d00165b31fd0981fce3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*CSN0fkvs3A_yy2HWLyDdcQ.png"/></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/9b1cbc4286d916d8109348e7f0535f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*GX0EA9qZgpoktT-iFrGzOQ.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Decision boundary with dropout</figcaption></figure><p id="f0a0" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太神奇了！我们实现了更高的精度和下降率！</p></div><div class="ab cl pb pc hz pd" role="separator"><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg ph"/><span class="pe bw bk pf pg"/></div><div class="io ip iq ir is"><p id="5a93" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">干得好！您了解了正则化如何改进神经网络，并且实现了 L2 正则化和剔除来改进分类模型！</p><p id="1796" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在以后的文章中，我将展示如何通过选择正确的优化算法来进一步改进神经网络。</p><p id="549b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">敬请关注，继续学习！</p></div></div>    
</body>
</html>