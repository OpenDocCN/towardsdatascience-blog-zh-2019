<html>
<head>
<title>Get Started With Reinforcement Learning and Python: How to Automatize a Warehouse Robot</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习和 Python 入门:如何自动化仓库机器人</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-started-with-reinforcement-learning-and-python-how-to-automatize-a-warehouse-robot-4f996bede325?source=collection_archive---------12-----------------------#2019-12-19">https://towardsdatascience.com/get-started-with-reinforcement-learning-and-python-how-to-automatize-a-warehouse-robot-4f996bede325?source=collection_archive---------12-----------------------#2019-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ed4e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">在本教程中，我将向您展示如何使用强化学习来自动化自主仓库机器人，以找到不同位置之间的最佳路径。</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/b3c95a3ad1cb0433f61cd06ed47a52cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H8d_byqG9sSpfdzx52JOZw.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by Pixabay on Pexels.com</figcaption></figure><h1 id="43ba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="5fce" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">机器人技术的应用在每个商业领域都在不断扩大。自动化处理重复性任务，旨在消除人工输入，以优化流程和削减成本。</p><p id="4a7c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2012 年，亚马逊<a class="ae ms" href="https://pitchbook.com/news/articles/ma-flashback-amazon-announces-775m-kiva-systems-acquisition" rel="noopener ugc nofollow" target="_blank">收购了开发仓库机器人和相关技术的公司 Kiva Systems，Kiva 以 7.75 亿美元被收购。此外，许多其他公司，如阿里巴巴、大众汽车或</a><a class="ae ms" href="https://www.scmp.com/tech/start-ups/article/3031314/robotics-start-geekplus-push-expand-chinas-smart-logistics" rel="noopener ugc nofollow" target="_blank"> Geek+ </a>不断实现机器人和相关技术。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><blockquote class="mv"><p id="23cd" class="mw mx it bd my mz na nb nc nd ne mm dk translated">对于初学者来说，开始这样的话题可能是一场斗争，这就是为什么我认为把事情放到背景中，然后开始深入细节是重要的。</p></blockquote><h1 id="4278" class="kz la it bd lb lc ld le lf lg lh li lj jz nf ka ll kc ng kd ln kf nh kg lp lq bi translated">范围</h1><p id="4b16" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">本指南的范围是关于强化学习如何在仓库自动化中使用的 Python 实用实践教程，例如亚马逊等公司。我邀请你打开你最喜欢的编辑器(<strong class="lt iu"> Jupyter Notebook </strong>、<strong class="lt iu"> Spyder </strong>等等……)，跟着一起编码。以防你拿不到我的 Google Colab 笔记本。</p><h2 id="070c" class="ni la it bd lb nj nk dn lf nl nm dp lj ma nn no ll me np nq ln mi nr ns lp nt bi translated">让我们开始:</h2><p id="6a08" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本指南中，我将模拟自主仓库机器人需要采取的行动，以便以最佳方式收集交付的产品，同时考虑机器人的位置、中间位置和最终位置。</p><p id="f74d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此模拟中使用的仓库由不同的 12 个点组成，其形状如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/c1c01936170ffaa9740c5ad957a12545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*nRC1BqxAQBPHHT4umPvGZw.png"/></div></figure><p id="5091" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该系统需要实时排列在这 12 个位置收集产品的优先级。例如，在特定时间<code class="fe nv nw nx ny b">t</code>，系统将返回以下排名:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/d8017d43148bfbb1f4ba8bbbfa803f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*qS60wanHujlwjN3RXsXyBw.png"/></div></figure><p id="ee3a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这个例子中，位置 G 具有最高优先级，机器人必须以系统计算的最短路线移动到这个位置。</p><p id="b6c6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，位置<code class="fe nv nw nx ny b">K</code>和<code class="fe nv nw nx ny b">L</code>位于前 3 个优先级中，因此系统将通过在到达其最终最高优先级位置之前“步行”到一些中间位置来计算最短路线。</p><p id="999f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了在 Python 中实现这一逻辑，有必要通过定义以下 3 个元素来将这一任务置于上下文环境中:</p><ul class=""><li id="b011" class="oa ob it lt b lu mn lx mo ma oc me od mi oe mm of og oh oi bi translated"><strong class="lt iu">美国</strong></li><li id="f75a" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><strong class="lt iu">动作</strong></li><li id="31e7" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><strong class="lt iu">奖励</strong></li></ul><p id="0f44" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">状态</strong>是机器人在每个时间<code class="fe nv nw nx ny b">t</code>可以处于的位置:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="a5c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">动作</strong>是机器人从一个位置移动到另一个位置时可能做的动作:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="d10a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当然，当机器人在一个特定的位置时，有些动作它不能执行。这在模拟中通过<strong class="lt iu">奖励</strong>的矩阵以及通过对其不能执行的动作给予奖励 0 来指定。</p><p id="a6ea" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">奖励矩阵由状态和动作矩阵组成，0 代表机器人在该状态下不能执行的动作，1 代表机器人可以执行的动作。</p><p id="f407" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从<code class="fe nv nw nx ny b">A</code>位置开始，根据仓库地图，机器人只能去<code class="fe nv nw nx ny b">A</code>位置，而在<code class="fe nv nw nx ny b">B</code>位置则有可能移动到<code class="fe nv nw nx ny b">A</code>、<code class="fe nv nw nx ny b">C</code>或<code class="fe nv nw nx ny b">F</code>位置。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi op"><img src="../Images/2846f760f7715bf886379bb20ffcbe93.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*IKaKl8W8KuVTw3iyqQFpOw.png"/></div></div></figure><p id="4c0e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">该系统采用奖励矩阵，并向最高优先级位置分配高奖励，返回该位置的最佳路径，该系统基于<strong class="lt iu">马尔可夫决策过程</strong>，该过程可表示为以下元组:</p><p id="6807" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nv nw nx ny b">(S, A, T, R)</code></p><p id="41d1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中:</p><ul class=""><li id="13d2" class="oa ob it lt b lu mn lx mo ma oc me od mi oe mm of og oh oi bi translated"><code class="fe nv nw nx ny b">S</code>是状态的集合</li><li id="7f60" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><code class="fe nv nw nx ny b">A</code>可以进行的动作集合</li><li id="a08b" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><code class="fe nv nw nx ny b">T</code>定义在时间 t 处于状态 s 的动作 a 将导致在时间<code class="fe nv nw nx ny b">t</code> +1 处于状态 s’的概率的转移规则</li><li id="e539" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><code class="fe nv nw nx ny b">R</code>由于动作<code class="fe nv nw nx ny b">A</code>，从状态 s 转换到状态<code class="fe nv nw nx ny b">s`</code>后接收到的奖励函数</li></ul><p id="913a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">系统包含一个策略函数，它给定一个状态<code class="fe nv nw nx ny b">S(t)</code>返回动作<code class="fe nv nw nx ny b">A(t)</code>。</p><p id="dcf7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">用π表示所有可能的策略动作的集合产生了最优化问题，其中最优策略π∫最大化累积奖励。</p><p id="498f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">每一对动作<code class="fe nv nw nx ny b">(s, a)</code>都关联有一个数值，记为<code class="fe nv nw nx ny b">Q-value</code>；在<code class="fe nv nw nx ny b">t=0 </code>处，当在时间<code class="fe nv nw nx ny b">t</code>和状态<code class="fe nv nw nx ny b">s(t)</code>处正在进行随机动作时，所有 Q 值被初始化为 0，其带来状态<code class="fe nv nw nx ny b">s(t) + 1</code>和奖励<code class="fe nv nw nx ny b">R(s(t), a(t))</code>。</p><p id="2c4a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个被称为 Q-Learning 的整个算法可以总结如下:</p><p id="bd97" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于所有状态对<code class="fe nv nw nx ny b">s</code>和动作对<code class="fe nv nw nx ny b">a</code>，Q 值被初始化为 0。</p><p id="4ef5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">初始状态是<code class="fe nv nw nx ny b">s(0)</code>，然后，执行一个随机的可能动作，并到达第一状态<code class="fe nv nw nx ny b">s(1)</code>。</p><p id="0bf3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于每一个<code class="fe nv nw nx ny b">t</code> ≥ <code class="fe nv nw nx ny b">1</code>，直到某个数字(在本案例研究中为 1000 次)，重复以下步骤:</p><p id="64a2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">这可以应用到我们的例子中，让我们看看到底发生了什么:</strong></p><p id="9767" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从可能的 12 种状态中选择一种随机状态<code class="fe nv nw nx ny b">s(t)</code></p><ul class=""><li id="3a5d" class="oa ob it lt b lu mn lx mo ma oc me od mi oe mm of og oh oi bi translated">播放导致下一个可能状态的随机动作<code class="fe nv nw nx ny b">a(t)</code></li><li id="9480" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated">到达下一个状态<code class="fe nv nw nx ny b">s(t) + 1</code>，并且产生奖励<code class="fe nv nw nx ny b">R(s(t), a(t))</code></li><li id="cad1" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated">时间差<code class="fe nv nw nx ny b">TD(t)(s(t); a(t))</code>:计算如下:</li></ul><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0d2a26b85d5091792c24163d95fdd625.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*JnuzSk7e8DD6As4KP8M6JA.png"/></div></figure><ul class=""><li id="6849" class="oa ob it lt b lu mn lx mo ma oc me od mi oe mm of og oh oi bi translated">Q 值通过应用<a class="ae ms" href="https://en.wikipedia.org/wiki/Bellman_equation" rel="noopener ugc nofollow" target="_blank">贝尔曼</a>公式进行更新:</li></ul><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi or"><img src="../Images/970f23bf6f26f64711f31c41253accaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*J-sndveRpgL1pqNleyTmEQ.png"/></div></figure><h1 id="1ec7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">实施:</h1><p id="1f45" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这一部分中，正在实施 Q-learning 过程以创建位置<code class="fe nv nw nx ny b">G</code>的奖励矩阵。</p><p id="9868" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">虽然之前已经定义了<strong class="lt iu">动作</strong>和<strong class="lt iu">位置-状态</strong>，但是现在有必要定义奖励矩阵和参数γ和α:</p><p id="90ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nv nw nx ny b">gamma = 0.75</code></p><p id="5454" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nv nw nx ny b">alpha = 0.9</code></p><p id="bcf4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于位置<code class="fe nv nw nx ny b">G</code>具有最高优先级，因此可以如下定义奖励矩阵，给予位置<code class="fe nv nw nx ny b">G</code>较高的奖励:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="7a68" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">q 值由零矩阵初始化:</p><p id="18c8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nv nw nx ny b">Q = np.array(np.zeroes([12, 12]))</code></p><p id="ebc4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后实施 Q 学习过程，for 循环迭代 1000 次，重复 1000 次算法步骤:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="8140" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">位置 G 的 Q 值由算法计算，并且可以将它们可视化:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi os"><img src="../Images/3de97976d85586b4be724ad68fc7fde8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vD3nEv2_5U9tAEDIfr09wA.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">You can change the background color in a pandas dataframe by doing: df.style.background_gradient()</figcaption></figure><p id="1f2b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从图中可以看出，位置<code class="fe nv nw nx ny b">G</code>的 Q 值最高，而远离<code class="fe nv nw nx ny b">G</code>的位置 Q 值较低。</p><p id="7631" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下一步是计算一个函数，该函数能够返回任意位置的最佳路线，而不仅仅是“硬编码”的<code class="fe nv nw nx ny b">G</code>位置。</p><p id="9c94" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">首先，有必要像之前在<code class="fe nv nw nx ny b">location_to_state</code>中所做的那样，将每个状态映射到位置:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="5a0a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此时，要计算返回任意位置最佳路径的函数，需要重新定义<code class="fe nv nw nx ny b">R</code>矩阵，删除硬编码的奖励:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="0eeb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们将计算理想路线的逻辑封装到一个函数中:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="0cc0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以起始位置和结束位置作为参数调用此函数将返回所需的路径:</p><p id="36fc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nv nw nx ny b">route("E", "G")</code></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/31da2257e4a89a36fc6f555cac358e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*2Jb-bRKbPly0JB8gbKGTLg.png"/></div></figure><p id="9bad" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在可以创建一个额外的函数<code class="fe nv nw nx ny b">best_route()</code>，它将起始、中间和结束位置作为输入，它将调用<code class="fe nv nw nx ny b">route()</code>函数两次，第一次在起始和中间位置之间，第二次在中间到结束位置之间:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="oo mu l"/></div></figure><p id="42bc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><code class="fe nv nw nx ny b">best_route("E", "K", "G")</code></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/61536985f047a65cb176a01d8b4b1f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*rQYKcQbzI9ZkzBnp1QsGrA.png"/></div></figure><h1 id="9893" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">参考</h1><ul class=""><li id="3c43" class="oa ob it lt b lu lv lx ly ma ot me ou mi ov mm of og oh oi bi translated"><a class="ae ms" href="https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/" rel="noopener ugc nofollow" target="_blank">Q-Learning 简介</a></li><li id="fc03" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><a class="ae ms" href="https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265" rel="noopener">强化学习</a></li><li id="99e0" class="oa ob it lt b lu oj lx ok ma ol me om mi on mm of og oh oi bi translated"><a class="ae ms" href="https://medium.com/datadriveninvestor/math-of-q-learning-python-code-5dcbdc49b6f6" rel="noopener">Q-Learning 的数学</a></li></ul><h2 id="c205" class="ni la it bd lb nj nk dn lf nl nm dp lj ma nn no ll me np nq ln mi nr ns lp nt bi translated">进一步阅读</h2><p id="966e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">另外我想推荐一下这篇文章作者<a class="ae ms" href="https://neptune.ai/" rel="noopener ugc nofollow" target="_blank"> neptune.ai </a> : <a class="ae ms" href="https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses" rel="noopener ugc nofollow" target="_blank">最佳强化学习教程、范例、项目、课程</a>。</p><p id="5edd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在他们的博客中，他们收集了一系列不同的资源来进一步激发你的兴趣，并开始强化学习！</p></div><div class="ab cl ow ox hx oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="im in io ip iq"><pre class="kk kl km kn gt pd ny pe pf aw pg bi"><span id="b886" class="ni la it ny b gy ph pi l pj pk"><strong class="ny iu">I have a newsletter 📩.</strong> </span><span id="9ae7" class="ni la it ny b gy pl pi l pj pk">Every week I’ll send you a brief findings of articles, links, tutorials, and cool things that caught my attention. If tis sounds cool to you subscribe. </span><span id="16f2" class="ni la it ny b gy pl pi l pj pk"><em class="pm">That means </em><strong class="ny iu"><em class="pm">a lot</em></strong><em class="pm"> for me.</em></span></pre><div class="pn po gp gr pp pq"><a href="https://relentless-creator-2481.ck.page/68d9def351" rel="noopener  ugc nofollow" target="_blank"><div class="pr ab fo"><div class="ps ab pt cl cj pu"><h2 class="bd iu gy z fp pv fr fs pw fu fw is bi translated">米尔斯形式</h2><div class="px l"><h3 class="bd b gy z fp pv fr fs pw fu fw dk translated">编辑描述</h3></div><div class="py l"><p class="bd b dl z fp pv fr fs pw fu fw dk translated">无情-创造者-2481.ck.page</p></div></div></div></a></div></div></div>    
</body>
</html>