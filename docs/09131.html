<html>
<head>
<title>Understanding Word2vec Embedding in Practice</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在实践中理解 Word2vec 嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953?source=collection_archive---------4-----------------------#2019-12-04">https://towardsdatascience.com/understanding-word2vec-embedding-in-practice-3e9b8985953?source=collection_archive---------4-----------------------#2019-12-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/388dd424634512b350f1de0ae90c9b6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6c42Kmc1WIB0rH3VfNCTg.png"/></div></div></figure><div class=""/><div class=""><h2 id="f22a" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">单词嵌入，向量空间模型，Gensim</h2></div><p id="2428" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这篇文章旨在用 Python 中的<a class="ae lm" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"> Gensim </a>实现 Word2vec 嵌入的同时，以直观的方式解释<a class="ae lm" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2vec </a>的概念以及概念背后的数学原理。</p><p id="d7ed" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Word2vec 的基本思想是，不是在高维空间将单词表示为一键编码(<a class="ae lm" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">count vectorizer</a>/<a class="ae lm" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank">tfidfvectorizer</a>)，而是在稠密的低维空间中以相似单词得到相似单词向量的方式来表示单词，从而映射到附近的点上。</p><p id="c262" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Word2vec 不是深度神经网络，它把文本变成深度神经网络可以作为输入处理的数值形式。</p><h2 id="8e61" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">word2vec 模型是如何训练的</h2><ul class=""><li id="9db5" class="mg mh jb ks b kt mi kw mj kz mk ld ml lh mm ll mn mo mp mq bi translated">使用滑动窗口浏览训练语料库:每个单词都是一个预测问题。</li><li id="5f58" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">目标是使用相邻单词来预测当前单词(反之亦然)。</li><li id="8a99" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">预测的结果决定了我们是否调整当前的单词向量。渐渐地，向量收敛到(希望)最优值。</li></ul><p id="ae1f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">比如我们可以用“人工”来预测“智能”。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mw"><img src="../Images/9c280561dc03a0913937d7573d8c4f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WcIBmz0jR8KTtTkwGhQmmg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Source: <a class="ae lm" href="https://www.infoq.com/presentations/nlp-practitioners/?itm_source=presentations_about_Natural-Language-Processing&amp;itm_medium=link&amp;itm_campaign=Natural-Language-Processing" rel="noopener ugc nofollow" target="_blank">https://www.infoq.com/presentations/nlp-practitioners/?itm_source=presentations_about_Natural-Language-Processing&amp;itm_medium=link&amp;itm_campaign=Natural-Language-Processing</a></figcaption></figure><p id="1425" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">然而，预测本身并不是我们的目标。它是学习向量表示的代理，以便我们可以将它用于其他任务。</p><h2 id="4450" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">Word2vec 跳跃式网络架构</h2><p id="43bb" class="pw-post-body-paragraph kq kr jb ks b kt mi kc kv kw mj kf ky kz nf lb lc ld ng lf lg lh nh lj lk ll ij bi translated">这是 word2vec 模型架构之一。它只是一个简单的隐藏层和一个输出层。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/ba4bed358a219d8aef045ddef21cd553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XxSnprkyh2I7Jts4lpxAg.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Source: <a class="ae lm" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></figcaption></figure><h2 id="a32b" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">数学</h2><p id="95a0" class="pw-post-body-paragraph kq kr jb ks b kt mi kc kv kw mj kf ky kz nf lb lc ld ng lf lg lh nh lj lk ll ij bi translated">下面是 word2vec 嵌入背后的数学。输入层是独热编码向量，因此它在单词索引中得到“1”，在其他地方得到“0”。当我们将这个输入向量乘以权重矩阵时，我们实际上拉出了对应于该单词索引的一行。这里的目标是提取重要的行，然后，我们丢弃其余的行。</p><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/612d5e25c6b0915ba6915d5f72b25681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HxUIgAhn4VSHqr9gn6SsBA.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Source: <a class="ae lm" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a></figcaption></figure><p id="d4dd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这是 word2vec 工作的主要机制。</p><p id="4521" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">当我们使用<a class="ae lm" href="https://www.tensorflow.org/tutorials/text/word_embeddings" rel="noopener ugc nofollow" target="_blank"> Tensorflow / Keras </a>或者<a class="ae lm" href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>来做这件事的时候，他们有一个专门的层用于这个过程，叫做“嵌入层”。所以，我们不打算自己做数学，我们只需要传递一个热编码向量，“嵌入层”做所有的脏工作。</p><h2 id="0df1" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">预处理文本</h2><p id="0137" class="pw-post-body-paragraph kq kr jb ks b kt mi kc kv kw mj kf ky kz nf lb lc ld ng lf lg lh nh lj lk ll ij bi translated">现在我们将为一个<a class="ae lm" href="https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv" rel="noopener ugc nofollow" target="_blank"> BBC 新闻数据集</a>实现 word2vec 嵌入。</p><ul class=""><li id="3414" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">我们用 Gensim 来训练 word2vec 嵌入。</li><li id="081d" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">我们使用 NLTK 和 spaCy 对文本进行预处理。</li><li id="6c84" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">我们使用 t-SNE 来可视化高维数据。</li></ul><figure class="mx my mz na gt is"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">clean_text.py</figcaption></figure><ul class=""><li id="8dcb" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">我们使用空间来进行引理化。</li><li id="46fc" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">禁用命名实体识别以提高速度。</li><li id="b465" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">去掉代词。</li></ul><figure class="mx my mz na gt is"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">lemmatize.py</figcaption></figure><ul class=""><li id="c6a5" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">现在我们可以看看最常用的 10 个单词。</li></ul><figure class="mx my mz na gt is"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">word_freq.py</figcaption></figure><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/012fc2feef6962549e6875fc30b06253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BBebencfZ0HPcmJRuxLAzA.png"/></div></div></figure><h2 id="9282" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">在 Gensim 中实现 Word2vec 嵌入</h2><ul class=""><li id="665d" class="mg mh jb ks b kt mi kw mj kz mk ld ml lh mm ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">min_count</code>:语料库中要包含在模型中的单词的最小出现次数。数字越大，语料库中的单词就越少。</li><li id="fe82" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">window</code>:句子内当前词和预测词之间的最大距离。</li><li id="cb78" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">size</code>:特征向量的维数。</li><li id="2f87" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">workers</code>:我知道我的系统有 4 个内核。</li><li id="b3da" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">model.build_vocab</code>:准备模型词汇。</li><li id="315b" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">model.train</code>:训练词向量。</li><li id="3553" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated"><code class="fe nq nr ns nt b">model.init_sims()</code>:当我们不打算进一步训练模型时，我们使用这一行代码来提高模型的内存效率。</li></ul><figure class="mx my mz na gt is"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">word2vec_model.py</figcaption></figure><h2 id="a11e" class="ln lo jb bd lp lq lr dn ls lt lu dp lv kz lw lx ly ld lz ma mb lh mc md me mf bi translated">探索模型</h2><ul class=""><li id="3f36" class="mg mh jb ks b kt mi kw mj kz mk ld ml lh mm ll mn mo mp mq bi translated">找出与“经济”最相似的单词</li></ul><pre class="mx my mz na gt nu nt nv nw aw nx bi"><span id="5bc2" class="ln lo jb nt b gy ny nz l oa ob">w2v_model.wv.most_similar(positive=['economy'])</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ef2869c68f4910feda73c2e59a5361e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*S4oNUzf287HciT18y7JbQw.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Figure 1</figcaption></figure><ul class=""><li id="ec72" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">找出与“总统”最相似的单词</li></ul><pre class="mx my mz na gt nu nt nv nw aw nx bi"><span id="037e" class="ln lo jb nt b gy ny nz l oa ob">w2v_model.wv.most_similar(positive=['president'])</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2c44b4a273e5629519d45899da106fe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*obkCRxt4T9ocoC_V7IpNDg.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Figure 2</figcaption></figure><ul class=""><li id="3606" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">这两个词有多相似？</li></ul><pre class="mx my mz na gt nu nt nv nw aw nx bi"><span id="c0cb" class="ln lo jb nt b gy ny nz l oa ob">w2v_model.wv.similarity('company', 'business')</span></pre><figure class="mx my mz na gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bd3ce2272a658203c807ef27e6ef4711.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*OGZxuLkJ1uz4vr6Wo1oNEQ.png"/></div></figure><p id="3054" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">请注意，如果我们改变<code class="fe nq nr ns nt b">min_count</code>，上述结果可能会改变。例如，如果我们设置<code class="fe nq nr ns nt b">min_count=100</code>，我们将有更多的单词可以使用，其中一些可能比上面的结果更接近目标单词；如果我们设置<code class="fe nq nr ns nt b">min_count=300</code>，上面的一些结果可能会消失。</p><ul class=""><li id="4561" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">我们使用 t-SNE 来表示低维空间中的高维数据。</li></ul><figure class="mx my mz na gt is"><div class="bz fp l di"><div class="nn no l"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">tsne.py</figcaption></figure><figure class="mx my mz na gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/08cf66d0fc32593045112a6645eb6154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-zSgujq5atTAZ4IauTcAQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Figure 3</figcaption></figure><ul class=""><li id="fc77" class="mg mh jb ks b kt ku kw kx kz nk ld nl lh nm ll mn mo mp mq bi translated">很明显有些词彼此接近，比如“球队”、“进球”、“伤病”、“奥运”等等。这些词往往被用在与体育相关的新闻报道中。</li><li id="2e15" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">其他聚集在一起的词，如“电影”、“演员”、“奖项”、“奖品”等，它们很可能在谈论娱乐的新闻文章中使用。</li><li id="fe76" class="mg mh jb ks b kt mr kw ms kz mt ld mu lh mv ll mn mo mp mq bi translated">又来了。剧情看起来如何很大程度上取决于我们如何设定<code class="fe nq nr ns nt b">min_count</code>。</li></ul><p id="d020" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><a class="ae lm" href="https://github.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/blob/master/Word2vec%20BBC%20news.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae lm" href="https://github.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/blob/master/Word2vec%20BBC%20news.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受这周剩下的时光。</p><p id="48f7" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">参考:<a class="ae lm" href="https://learning.oreilly.com/videos/oreilly-strata-data/9781492050681/9781492050681-video327451?autoplay=false" rel="noopener ugc nofollow" target="_blank">https://learning . oreilly . com/videos/oreilly-strata-data/9781492050681/9781492050681-video 327451？自动播放=假</a></p></div></div>    
</body>
</html>