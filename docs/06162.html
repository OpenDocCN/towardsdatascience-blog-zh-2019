<html>
<head>
<title>Decision tree: Part 2/2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树:第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-part-2-34b31b1dc328?source=collection_archive---------9-----------------------#2019-09-06">https://towardsdatascience.com/decision-tree-part-2-34b31b1dc328?source=collection_archive---------9-----------------------#2019-09-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2665" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">手动计算熵和信息增益</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1c56f871b2b97fdd06dd47c3e5e391b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jVxEOG2Tce_vaY9k"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@subtlecinematics?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Subtle Cinematics</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b3d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> T </span>他的文章是“决策树”系列的第二篇，本系列的<a class="ae kv" rel="noopener" target="_blank" href="/decision-tree-overview-with-no-maths-66b256281e2b"> <strong class="ky ir"> <em class="mb">第一篇</em> </strong> </a>发展了关于决策树的直觉，并给你一个在哪里画决策边界的想法。在这篇文章中，我们将看到决策树是如何做到这一点的。</p><blockquote class="mc md me"><p id="9cc3" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><em class="iq">🙊</em> <strong class="ky ir"> <em class="iq">剧透:</em> </strong> <em class="iq">这涉及到一些数学。</em></p></blockquote><p id="5e98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用一个非常小的数据集，以便于可视化和跟踪。然而，在实践中，这样的数据集肯定会过度拟合。这个数据集决定你是否应该买一辆有 3 个特征的车:年龄，里程，以及这辆车是否是路试。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/1e21b3813534f9676c8deb204d9cd727.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*x9PUfIFPm_gg4m2f7shB-A.png"/></div></figure><p id="e981" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上一篇文章中，我们仅通过查看图表来绘制决策边界。但是树做不到这一点。作为人类的好处，对吧？决策树使用熵来寻找分割点和要分割的特征。</p><h1 id="4d8b" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">熵</h1><p id="0eb3" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">熵的定义是缺乏秩序或可预测性。在一堆例子中，它是杂质的量度。如果节点只有一个类的实例，那么它就是最纯的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a9ffcec58861fb909b7b4e6995c03a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*GZ6V7rsIvRUblBDofYMxGg.png"/></div></figure><blockquote class="mc md me"><p id="9f2e" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">其中<em class="iq"> n </em> =特征数量</p><p id="31f7" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated"><em class="iq"> i </em> =特征</p><p id="b543" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">P =概率<em class="iq"> i </em></p></blockquote><p id="aaba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为每个特征计算熵，并且选择产生最小值的一个用于分割。熵的数学范围是从 0 到 1。</p><h1 id="def2" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">我们开始吧！</h1><p id="26ae" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">现在让我们把它付诸实践。为每个节点计算熵。第一个节点，即根节点，总是具有数据集中的所有示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/9df18a8f853b206fd462e88fa3486a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*p9Fg15jv5Unzjn-kDyKbKA.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/6ba0fa036e7ad4fcafd5c2bacc3f9d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*LLDr3HTK1eyZhH-4VKHD5g.png"/></div></figure><p id="eba2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你所看到的，父节点的熵是 1 。请记住这个值，我们将在接下来的步骤中计算信息增益时使用它。</p><h1 id="620e" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">信息增益</h1><p id="7857" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">下一步是找到信息增益(IG)，它的值也在 0-1 的范围内。信息增益帮助树决定分割哪个特征:给出最大信息增益的特征。我们现在将逐个计算每个特征的信息增益。对于这个迭代，父节点的熵是我们上面计算的 1。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/03d1d9f55c61f6524157aa9a670cede3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*sum1ZSPefapYJbZxGzopCw.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/0d27849aac86ebe1b548648912baaefa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kz6ormXB5h1yAVSU92m1g.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/789f948563bb734bab254056e426b1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*cLZpDA-UsHlIhIBo7gGLiA.png"/></div></figure><p id="1a4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上图中，我们可以看到左边子节点的熵是 1，因为类实例的比率是 1:1。这是最不纯的一个节点。右边的子节点也是如此。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/b507b4300f6b8331c5d95cdc716fed5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*PC2X8x7aH5-Xad2r4t98EQ.png"/></div></figure><p id="9aaa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，子节点的熵值为 0，因为每个子节点只有一个类的实例:最纯粹的节点。</p><p id="241c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总而言之，下表显示了每个特征的<strong class="ky ir">信息增益</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/7125b445492ef4588bb9afe75c386d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*UNZ8jNWFaa5ox1BEtd5tsg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Information gain for every feature</figcaption></figure><p id="648f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最大信息增益是针对特征<strong class="ky ir">“路试”</strong>的，因此我们将选择它作为我们的第一个分割特征。这样做将生成下面给出的树结构:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/ea194cc151ee84785dd7cbb8707da457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*B3nddNQPsGxWdpMisG7jmA.png"/></div></figure><p id="2e4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于叶节点的熵为零，我们将停止切割。如果不是这种情况，那么我们将继续寻找前面的父节点和子节点的信息增益，直到满足任何一个停止标准。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><h1 id="fd22" class="mj mk iq bd ml mm nq mo mp mq nr ms mt jw ns jx mv jz nt ka mx kc nu kd mz na bi translated">基尼杂质</h1><p id="f486" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">和熵一样，基尼杂质也是计算节点杂质的一种度量，具有相同的数学范围[0–1]。其中 1 表示最大杂质，0 表示最小杂质。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/06ecb2582137b11d01a327b81e093d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*d58oqIyAEIcrU4ZvHOpUuQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image source: <em class="nv">Hands-On Machine Learning with Scikit-Learn &amp; TensorFlow</em></figcaption></figure><h1 id="eded" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">基尼还是熵？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/89e8d169a1f979d54262c627ae106df1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*952d-KPEr6HvZx0x"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@jcotten?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Joshua J. Cotten</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0661" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也可以用<em class="mb">基尼杂质</em>代替<em class="mb">熵</em>。尽管如此，无论您使用哪一种，结果都不会有太大的不同。但是计算 Gini 杂质比熵稍微快一点。熵产生更平衡的树，而基尼不纯倾向于将多数阶级分成自己的分支。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/23efc699a7ed351cebb2b9a3aa69f9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xI6i-bz1a_9RtUPp"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@hannynaibaho?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Hanny Naibaho</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="mc md me"><p id="980c" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">如果您有任何问题或建议，请随时在下面发表。你也可以在<a class="ae kv" href="https://www.linkedin.com/in/azika-amelia/" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> Linkedin </strong> </a>上和我联系。<em class="iq">💼</em></p><p id="328b" class="kw kx mb ky b kz la jr lb lc ld ju le mf lg lh li mg lk ll lm mh lo lp lq lr ij bi translated">直到那时和平结束。<em class="iq"> ✌ </em></p></blockquote></div></div>    
</body>
</html>