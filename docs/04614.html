<html>
<head>
<title>Building Neural Network Using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PyTorch 构建神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-neural-network-using-pytorch-84f6e75f9a?source=collection_archive---------2-----------------------#2019-07-15">https://towardsdatascience.com/building-neural-network-using-pytorch-84f6e75f9a?source=collection_archive---------2-----------------------#2019-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="4542" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“计算机能否思考的问题并不比潜艇能否游泳的问题更有趣。”<br/> ― <strong class="js iu">埃德格·w·迪杰斯特拉</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/566411b1e847aa9e6766e37b2f055cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rFwNcoyJtLbXoJGQ8kRm_A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">source: <a class="ae le" href="https://deeplizard.com/learn/video/k4jY9L8H89U" rel="noopener ugc nofollow" target="_blank">here</a></figcaption></figure><p id="0392" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本教程中，我们将使用 PyTorch 从头开始实现一个简单的神经网络。我正在分享我从最近的 facebook-udacity 奖学金挑战项目中学到的东西。本教程假设你事先了解神经网络如何工作。</p><p id="df48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然有很多库可以用于深度学习，但我最喜欢 PyTorch。作为一名 python 程序员，我喜欢 PyTorch 的 python 行为是背后的原因之一。它主要使用 python 的风格和功能，易于理解和使用。</p><p id="4109" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">py torch 的核心提供了两个主要特性:</strong></p><ul class=""><li id="d7c5" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated">n 维张量，类似于 numpy，但可以在 GPU 上运行</li><li id="cad1" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">用于建立和训练神经网络的自动微分</li></ul></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="f3bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">什么是神经网络？</strong></p><p id="5752" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络是一组算法，大致模仿人脑，用于识别模式。网络是由近似神经元的单个部分构成的，通常称为单元或简称为“<strong class="js iu">神经元</strong>”每个单元都有一些加权输入。这些加权输入相加在一起(线性组合)，然后通过一个激活函数得到单元的输出。</p><h2 id="eb39" class="ma mb it bd mc md me dn mf mg mh dp mi kb mj mk ml kf mm mn mo kj mp mq mr ms bi translated">神经网络中的节点类型:</h2><ol class=""><li id="033c" class="lf lg it js b jt mt jx mu kb mv kf mw kj mx kn my ll lm ln bi translated">输入单元—向网络提供来自外部世界的信息，统称为“输入层”。这些节点不执行任何计算，它们只是将信息传递给隐藏节点。</li><li id="90f9" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn my ll lm ln bi translated">隐藏单元—这些节点与外界没有任何直接的联系。它们执行计算并将信息从输入节点传输到输出节点。隐藏节点的集合形成了“隐藏层”。虽然前馈网络只有一个输入层和一个输出层，但它可以有零个或多个隐藏层。</li><li id="f574" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn my ll lm ln bi translated">输出单元-输出节点统称为“输出层”，负责计算和将信息从网络传输到外部世界。</li></ol><p id="2cf8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">每层包括一个或多个节点。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="4230" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">构建神经网络</strong></p><p id="5f72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PyTorch 提供了一个模块<code class="fe mz na nb nc b">nn</code>,使得构建网络更加简单。我们将看到如何用<code class="fe mz na nb nc b">784 inputs</code>、<code class="fe mz na nb nc b">256 hidden units</code>、<code class="fe mz na nb nc b">10 output units</code>和<code class="fe mz na nb nc b">softmax output</code>构建一个神经网络。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="802f" class="ma mb it nc b gy nh ni l nj nk">from torch import nn</span><span id="20ae" class="ma mb it nc b gy nl ni l nj nk">class Network(nn.Module):<br/>    def __init__(self):<br/>        super().__init__()<br/>        <br/>        # Inputs to hidden layer linear transformation<br/>        self.hidden = nn.Linear(784, 256)<br/>        # Output layer, 10 units - one for each digit<br/>        self.output = nn.Linear(256, 10)<br/>        <br/>        # Define sigmoid activation and softmax output <br/>        self.sigmoid = nn.Sigmoid()<br/>        self.softmax = nn.Softmax(dim=1)<br/>        <br/>    def forward(self, x):<br/>        # Pass the input tensor through each of our operations<br/>        x = self.hidden(x)<br/>        x = self.sigmoid(x)<br/>        x = self.output(x)<br/>        x = self.softmax(x)<br/>        <br/>        return x</span></pre><blockquote class="nm nn no"><p id="f4f0" class="jq jr np js b jt ju jv jw jx jy jz ka nq kc kd ke nr kg kh ki ns kk kl km kn im bi translated">注:<code class="fe mz na nb nc b"><strong class="js iu">softmax</strong></code> <strong class="js iu">函数，</strong>也称为<code class="fe mz na nb nc b"><strong class="js iu">softargmax</strong></code>或<code class="fe mz na nb nc b"><strong class="js iu">normalized</strong></code> <strong class="js iu"> </strong> <code class="fe mz na nb nc b"><strong class="js iu">exponential function</strong></code>是一个以<em class="it"> K </em>实数的向量为输入，并将其归一化为由<em class="it"> K </em>个概率组成的<a class="ae le" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank">概率分布</a>的函数。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/04cd3c8c6424a8cb8cf065a0c5a5812c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWhBextdDSkxYvz0kEMTVg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">image from google</figcaption></figure><p id="26e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们一行一行地过一遍。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="b781" class="ma mb it nc b gy nh ni l nj nk"><strong class="nc iu">class</strong> Network(nn.Module):</span></pre><p id="0c2c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这里，我们继承了<code class="fe mz na nb nc b">nn.Module</code>。与<code class="fe mz na nb nc b">super().__init__()</code>结合，这创建了一个跟踪架构的类，并提供了许多有用的方法和属性。当你为你的网络创建一个类时，从<code class="fe mz na nb nc b">nn.Module</code>继承是强制性的。类本身的名称可以是任何东西。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="7d75" class="ma mb it nc b gy nh ni l nj nk">self.hidden <strong class="nc iu">=</strong> nn.Linear(784, 256)</span></pre><p id="9311" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一行创建了一个用于线性变换的模块，𝑥𝐖+𝑏xW+b，有 784 个输入和 256 个输出，并将其分配给<code class="fe mz na nb nc b">self.hidden</code>。该模块自动创建我们将在<code class="fe mz na nb nc b">forward</code>方法中使用的权重和偏差张量。一旦使用<code class="fe mz na nb nc b">net.hidden.weight</code>和<code class="fe mz na nb nc b">net.hidden.bias</code>创建了网络(<code class="fe mz na nb nc b">net</code>，您就可以访问权重和偏差张量。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="699f" class="ma mb it nc b gy nh ni l nj nk">self.output <strong class="nc iu">=</strong> nn.Linear(256, 10)</span></pre><p id="654f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似地，这创建了另一个具有 256 个输入和 10 个输出的线性转换。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="e6cb" class="ma mb it nc b gy nh ni l nj nk">self.sigmoid <strong class="nc iu">=</strong> nn.Sigmoid()<br/>self.softmax <strong class="nc iu">=</strong> nn.Softmax(dim<strong class="nc iu">=</strong>1)</span></pre><p id="efaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里我定义了 sigmoid 激活和 softmax 输出的操作。在<code class="fe mz na nb nc b">nn.Softmax(dim=1)</code>中设置<code class="fe mz na nb nc b">dim=1</code>计算各列的 softmax。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="43c3" class="ma mb it nc b gy nh ni l nj nk"><strong class="nc iu">def</strong> forward(self, x):</span></pre><p id="f28f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">用<code class="fe mz na nb nc b">nn.Module</code>创建的 PyTorch 网络必须定义一个<code class="fe mz na nb nc b">forward</code>方法。它接受一个张量<code class="fe mz na nb nc b">x</code>并通过您在<code class="fe mz na nb nc b">__init__</code>方法中定义的操作传递它。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="0bcd" class="ma mb it nc b gy nh ni l nj nk">x <strong class="nc iu">=</strong> self.hidden(x)<br/>x <strong class="nc iu">=</strong> self.sigmoid(x)<br/>x <strong class="nc iu">=</strong> self.output(x)<br/>x <strong class="nc iu">=</strong> self.softmax(x)</span></pre><p id="dc0a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，输入张量<code class="fe mz na nb nc b">x</code>通过每个操作，并重新分配给<code class="fe mz na nb nc b">x</code>。我们可以看到，输入张量经过隐藏层，然后是 sigmoid 函数，然后是输出层，最后是 softmax 函数。只要操作的输入和输出与您想要构建的网络体系结构相匹配，您在这里给变量取什么名字并不重要。在<code class="fe mz na nb nc b">__init__</code>方法中定义事物的顺序并不重要，但是您需要在<code class="fe mz na nb nc b">forward</code>方法中对操作进行正确排序。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="09b8" class="ma mb it nc b gy nh ni l nj nk"># Create the network and look at it's text representation<br/>model = Network()<br/>model</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="8b3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">使用</strong>构建神经网络<code class="fe mz na nb nc b"><strong class="js iu">nn.Sequential</strong></code></p><p id="75cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PyTorch 提供了一种方便的方法来构建这样的网络，其中张量通过运算顺序传递，<code class="fe mz na nb nc b">nn.Sequential</code> ( <a class="ae le" href="https://pytorch.org/docs/master/nn.html#torch.nn.Sequential" rel="noopener ugc nofollow" target="_blank">文档</a>)。用它来构建等效网络:</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="bc88" class="ma mb it nc b gy nh ni l nj nk"># Hyperparameters for our network<br/>input_size = 784<br/>hidden_sizes = [128, 64]<br/>output_size = 10</span><span id="e110" class="ma mb it nc b gy nl ni l nj nk"># Build a feed-forward network<br/>model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),<br/>                      nn.ReLU(),<br/>                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),<br/>                      nn.ReLU(),<br/>                      nn.Linear(hidden_sizes[1], output_size),<br/>                      nn.Softmax(dim=1))<br/>print(model)</span></pre><blockquote class="nm nn no"><p id="b375" class="jq jr np js b jt ju jv jw jx jy jz ka nq kc kd ke nr kg kh ki ns kk kl km kn im bi translated">这里我们的型号和之前一样:<code class="fe mz na nb nc b"> 784 input units</code>、<code class="fe mz na nb nc b">a hidden layer with 128 units</code>、<code class="fe mz na nb nc b"> ReLU activation</code>、<code class="fe mz na nb nc b">64 unit hidden layer</code>，再来一个<code class="fe mz na nb nc b"> ReLU</code>，然后是<code class="fe mz na nb nc b">output layer with 10 units</code>，再来一个<code class="fe mz na nb nc b">softmax output</code>。</p></blockquote><p id="5df2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您还可以传入一个<code class="fe mz na nb nc b">OrderedDict</code>来命名各个层和操作，而不是使用增量整数。注意字典键必须是唯一的，所以<em class="np">每个操作必须有不同的名称</em>。</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="fa10" class="ma mb it nc b gy nh ni l nj nk">from collections import OrderedDict<br/>model = nn.Sequential(OrderedDict([<br/>                      ('fc1', nn.Linear(input_size, hidden_sizes[0])),<br/>                      ('relu1', nn.ReLU()),<br/>                      ('fc2', nn.Linear(hidden_sizes[0], hidden_sizes[1])),<br/>                      ('relu2', nn.ReLU()),<br/>                      ('output', nn.Linear(hidden_sizes[1], output_size)),<br/>                      ('softmax', nn.Softmax(dim=1))]))</span><span id="380d" class="ma mb it nc b gy nl ni l nj nk"><br/>model</span></pre><p id="dc8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，您可以通过整数或名称来访问图层</p><pre class="kp kq kr ks gt nd nc ne nf aw ng bi"><span id="0d82" class="ma mb it nc b gy nh ni l nj nk">print(model[0])<br/>print(model.fc1)</span></pre><p id="45ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">今天到此为止。接下来我们将训练一个神经网络。你会在这里找到它。</p><p id="527c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们随时欢迎您提出任何建设性的批评或反馈。</p></div></div>    
</body>
</html>