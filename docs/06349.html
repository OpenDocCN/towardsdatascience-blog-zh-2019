<html>
<head>
<title>A Minimalist End-to-End Scrapy Tutorial (Part IV)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个极简的端到端剪贴簿教程(第四部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?source=collection_archive---------11-----------------------#2019-09-12">https://towardsdatascience.com/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?source=collection_archive---------11-----------------------#2019-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eade" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">面向初学者的系统化网页抓取</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a3da604b0fa67929569c0519ebc2c4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vwcmxXN0G3zaGL1k"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@pawel_czerwinski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Paweł Czerwiński</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d64f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0?source=friends_link&amp;sk=c9f8e32f28a88c61987ec60f93b93e6d" rel="noopener">第一部分</a>，<a class="ae kv" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-ii-b917509b73f7?source=friends_link&amp;sk=ebd3a9cee8b2097b3857194fee3821a6">第二部分</a>，<a class="ae kv" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-iii-bcd94a2e8bf3?source=friends_link&amp;sk=a1fdde9c9dd5383d8de2e08395ee3f98">第三部分</a>，<a class="ae kv" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?sk=6f0902f9a15092575814ab533a56f8ef" rel="noopener">第四部分</a>，<a class="ae kv" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-v-e7743ee9a8ef?source=friends_link&amp;sk=c1c5110f63c7ccbe4eb8c6209ee2f57c" rel="noopener">第五部分</a></p><p id="0320" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在前三部分中，您已经开发了一个蜘蛛，它从<a class="ae kv" href="http://quotes.toscrape.com" rel="noopener ugc nofollow" target="_blank">http://quotes.toscrape.com</a>中提取报价信息，并将数据存储到本地 SQLite 数据库中。在这一部分，我将向您展示如何将蜘蛛部署到云中。</p><p id="0b23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们看看你如何部署到<a class="ae kv" href="https://scrapinghub.com" rel="noopener ugc nofollow" target="_blank">https://scrapinghub.com</a>——开源 Scrapy 框架背后的团队运行的商业服务。</p><p id="068f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创建一个免费帐户和一个新项目:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/1491e26c92c9bb959540b98ce9d962de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4d74lHPEnjUFRs-9p5b81Q.png"/></div></div></figure><p id="a4f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，点击 Code and deployments 菜单，按照屏幕上显示的说明安装<code class="fe lt lu lv lw b">shub</code>——记录您的 API 密钥和部署号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lx"><img src="../Images/bdaa66f5e0e249c646cc8afc8625b82f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPdPW5fZicdh0R5YfVlCXQ.png"/></div></div></figure><p id="471c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到 scrapy-tutorial 的根目录(scrapy 项目的根目录)，使用命令<code class="fe lt lu lv lw b">shub login</code>和<code class="fe lt lu lv lw b">shub deploy</code>将你的项目部署到 Scrapy hub:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="7f2f" class="mc md iq lw b gy me mf l mg mh">(venv) dami:scrapy-tutorial harrywang$ shub login<br/>Enter your API key from <a class="ae kv" href="https://app.scrapinghub.com/account/apikey" rel="noopener ugc nofollow" target="_blank">https://app.scrapinghub.com/account/apikey</a><br/>API key: xxxxx<br/>Validating API key...<br/>API key is OK, you are logged in now.</span><span id="25d7" class="mc md iq lw b gy mi mf l mg mh">(venv) dami:scrapy-tutorial harrywang$ shub deploy 404937<br/>Messagepack is not available, please ensure that msgpack-python library is properly installed.<br/>Saving project 404937 as default target. You can deploy to it via 'shub deploy' from now on<br/>Saved to /Users/harrywang/xxx/scrapy-tutorial/scrapinghub.yml.<br/>Packing version b6ac860-master<br/>Created setup.py at /Users/harrywang/xxx/scrapy-tutorial<br/>Deploying to Scrapy Cloud project "404937"<br/>{"status": "ok", "project": 4xxx, "version": "b6ac860-master", "spiders": 3}<br/>Run your spiders at: <a class="ae kv" href="https://app.scrapinghub.com/p/404937/" rel="noopener ugc nofollow" target="_blank">https://app.scrapinghub.com/p/404937/</a></span></pre><p id="262b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Scrapinghub 配置文件<code class="fe lt lu lv lw b">scrapinghub.yml</code>在根文件夹中创建，您需要编辑它来指定我们特定的软件包需求。否则，将使用默认设置:</p><ul class=""><li id="2fb3" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">运行 Python 3 的 scrapy 1.7</li><li id="e29e" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">其他包的需求文件</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/ed9840cfee7f9fe385eb0cbfba18bfb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*PLAmhkb_40AzqWUD8N2Rhw.png"/></div></figure><p id="fa1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行<code class="fe lt lu lv lw b">$ shub deploy</code>再次部署，新配置生效。</p><p id="499a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我在 repo 中有 3 个蜘蛛(<code class="fe lt lu lv lw b">quotes_spider_v1.py</code>和<code class="fe lt lu lv lw b">quotes_spider_v2.py</code>是用于演示目的的中间蜘蛛)，您应该在已部署的项目中看到 3 个蜘蛛(<code class="fe lt lu lv lw b">quotes_spider.py</code>是主蜘蛛):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/7ce8d9c2633fe7df7ef896c581f2c408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtatL3OYbJBIWa1tKgZeOQ.png"/></div></div></figure><p id="87a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，你可以运行你的蜘蛛:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/88461d648ed9ff19cf0cd8c13d79220a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEoNBOxRlzmgUeezsMbwAg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/5a4786bfce017d8e448ba8e3f8293b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*qc-1zrsxxkZwQOHzmaHE9A.png"/></div></figure><p id="817f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作业完成后，您可以检查结果并下载不同格式的项目:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/ddb32ce340056774687c986894566aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UpBp8iRXLIz7cbdYgu01nw.png"/></div></div></figure><p id="ec19" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，您必须付费才能运行定期作业，例如，每周二上午 8 点运行您的蜘蛛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/ff710093458e79f1cfb5182d437d06b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l69NIFJqYLmNW5RgxYXDaA.png"/></div></div></figure><p id="14fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在搜索运行定期爬行作业的免费选项时，我遇到了由<a class="ae kv" href="https://github.com/my8100" rel="noopener ugc nofollow" target="_blank"> my8100 </a>开发的伟大的开源项目 S<a class="ae kv" href="https://github.com/my8100/scrapydweb" rel="noopener ugc nofollow" target="_blank">crapydWeb</a>——非常感谢作者开发了这样一个具有伟大特性和文档的好项目。接下来，我将带你通过 Heroku 使用 ScrapydWeb 建立你自己的“ScrapingHub.com”的过程(你也可以按照<a class="ae kv" href="https://github.com/my8100/scrapyd-cluster-on-heroku" rel="noopener ugc nofollow" target="_blank">作者的文档</a>)。</p><p id="48a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图显示了 ScrapydWeb 的架构，该架构旨在支持分布式爬行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/9280bd6018da637914aae050852a5413.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*42NsBF4TbexFjZTwtqqQEw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">source: <a class="ae kv" href="https://github.com/my8100/scrapyd-cluster-on-heroku" rel="noopener ugc nofollow" target="_blank">https://github.com/my8100/scrapyd-cluster-on-heroku</a></figcaption></figure><p id="3100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我不介绍分布式爬行。相反，我将只设置两个服务器:一个<strong class="ky ir"> ScrapydWeb 服务器</strong>(该服务器提供 web UI 来管理不同的蜘蛛和作业)和一个<strong class="ky ir"> Scrapyd 服务器</strong>(该服务器托管您的蜘蛛代码并实际发送/接收请求/响应)。</p><p id="d769" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ScrapydWeb 的作者通过在他的 repo 中预先配置 Heroku 使这个部署过程变得非常简单，repo 位于<a class="ae kv" href="https://github.com/my8100/scrapyd-cluster-on-heroku" rel="noopener ugc nofollow" target="_blank">ttps://github . com/my 8100/scrapyd-cluster-on-Heroku</a>。</p><ul class=""><li id="67e3" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">scrapyd-cluster-on-heroku/ScrapydWeb:该文件夹包含<strong class="ky ir"> ScrapydWeb 服务器</strong>的 Heroku 配置</li><li id="01fc" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">Scrapyd-cluster-on-heroku/Scrapyd:该文件夹包含<strong class="ky ir"> Scrapyd 服务器</strong>的 Heroku 配置</li></ul><p id="b1c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要定制部署，因为我们的 scrapy 项目有特定的包需求，例如，SQLAlchemy、MySQL、Python 3.x 等。因此，你需要将<a class="ae kv" href="https://github.com/my8100/scrapyd-cluster-on-heroku" rel="noopener ugc nofollow" target="_blank">https://github.com/my8100/scrapyd-cluster-on-heroku</a>的副本放到你的 github 账户中，例如<a class="ae kv" href="https://github.com/harrywang/scrapyd-cluster-on-heroku" rel="noopener ugc nofollow" target="_blank">https://github.com/harrywang/scrapyd-cluster-on-heroku</a>并做一些修改来支持本教程。</p><p id="6fb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以在<a class="ae kv" href="https://github.com/harrywang/scrapyd-cluster-on-heroku/commit/e612dcb9a6c158da4b744d311e82c529497fba7c" rel="noopener ugc nofollow" target="_blank">https://github . com/Harry Wang/scrapyd-cluster-on-heroku/commit/e 612 dcb 9 a6 c 158 da 4b 744d 311 e 82 c 529497 fba7c</a>查看我对作者回购的修改，包括:</p><ul class=""><li id="50fe" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">在 scrapyd/requirements.txt 中添加 MySQL 和 SQLAlchem 包</li><li id="3162" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">在 scrapyd/runtime.txt 中将 python 版本更改为 python-3.6.8</li><li id="7b86" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">打开 Scrapy web 服务器认证，在 Scrapy web/Scrapy web _ settings _ v10 . py 中设置用户名和密码(不要这样公开用户名和密码)</li></ul><p id="f559" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，在 heroku.com 创建一个免费账户并安装 Heroku CLI: <code class="fe lt lu lv lw b">brew tap heroku/brew &amp;&amp; brew install heroku</code></p><p id="eaee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，克隆回购:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="0cd4" class="mc md iq lw b gy me mf l mg mh">git clone <a class="ae kv" href="https://github.com/harrywang/scrapyd-cluster-on-heroku" rel="noopener ugc nofollow" target="_blank">https://github.com/harrywang/scrapyd-cluster-on-heroku</a><br/>cd scrapyd-cluster-on-heroku/</span></pre><p id="f911" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">登录 Heroku:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="b335" class="mc md iq lw b gy me mf l mg mh">scrapyd-cluster-on-heroku harrywang$ heroku login<br/>heroku: Press any key to open up the browser to login or q to exit:<br/>Opening browser to <a class="ae kv" href="https://cli-auth.heroku.com/auth/browser/3ba7221b-9c2a-4355-ab3b-d2csda" rel="noopener ugc nofollow" target="_blank">https://cli-auth.heroku.com/auth/browser/3ba7221b-9c2a-4355-ab3b-d2csda</a><br/>Logging in… done<br/>Logged in as <a class="ae kv" href="mailto:xxx@gmail.com" rel="noopener ugc nofollow" target="_blank">xxx@gmail.com</a></span></pre><p id="8755" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">部署垃圾服务器/应用程序:</p><ul class=""><li id="c31a" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">首先转到/scrapyd 文件夹，并通过运行以下 git 命令使该文件夹成为 git repo:</li></ul><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="5b5b" class="mc md iq lw b gy me mf l mg mh">git init<br/>git status<br/>git add .<br/>git commit -a -m "first commit"<br/>git status</span></pre><ul class=""><li id="03aa" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">创建一个名为<code class="fe lt lu lv lw b">scrapy-server1</code>的新应用程序(如果这个应用程序被占用，请选择另一个)</li><li id="84ad" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">设置一个名为 heroku 的 git 遥控器</li><li id="89f3" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">检查 git 遥控器</li><li id="31f7" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">将/scrapyd 文件夹中的内容推送到遥控器以部署应用程序</li></ul><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="8c61" class="mc md iq lw b gy me mf l mg mh">$ pwd<br/>/Users/harrywang/xxx/scrapyd-cluster-on-heroku/scrapyd</span><span id="c58b" class="mc md iq lw b gy mi mf l mg mh">$ heroku apps:create scrapy-server1</span><span id="2728" class="mc md iq lw b gy mi mf l mg mh">$ heroku git:remote -a scrapy-server1</span><span id="18de" class="mc md iq lw b gy mi mf l mg mh">set git remote heroku to https://git.heroku.com/scrapy-server1.git</span><span id="430d" class="mc md iq lw b gy mi mf l mg mh">$ git remote -v</span><span id="39c6" class="mc md iq lw b gy mi mf l mg mh">heroku https://git.heroku.com/scrapy-server1.git (fetch)</span><span id="db77" class="mc md iq lw b gy mi mf l mg mh">heroku https://git.heroku.com/scrapy-server1.git (push)</span><span id="cf61" class="mc md iq lw b gy mi mf l mg mh">origin https://github.com/harrywang/scrapyd-cluster-on-heroku (fetch)</span><span id="2186" class="mc md iq lw b gy mi mf l mg mh">origin https://github.com/harrywang/scrapyd-cluster-on-heroku (push)</span><span id="5cae" class="mc md iq lw b gy mi mf l mg mh">$ git push heroku master</span></pre><p id="a0ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以为远程垃圾服务器设置环境变量，例如设置时区:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="026c" class="mc md iq lw b gy me mf l mg mh">$ heroku config:set TZ=US/Eastern</span><span id="fd81" class="mc md iq lw b gy mi mf l mg mh">Setting TZ and restarting ⬢ scrapy-server1... done, <strong class="lw ir">v4</strong></span><span id="fe98" class="mc md iq lw b gy mi mf l mg mh">TZ: US/Eastern</span></pre><p id="9b83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，你有了一个运行在 http://scrapy-server1.herokuapp.com 的 scrapyd 服务器</p><p id="fdd8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，让我们设置 web 应用程序，它为我们添加 scrapyd 服务器、上传 scrapy 项目和安排爬行作业提供了 UI。</p><p id="b609" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，让我们部署 ScrapydWeb 服务器/应用程序:</p><ul class=""><li id="a345" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">首先转到/scrapydweb 文件夹，通过运行以下 git 命令使该文件夹成为 git repo:</li></ul><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="6d9a" class="mc md iq lw b gy me mf l mg mh">git init<br/>git status<br/>git add .<br/>git commit -a -m "first commit"<br/>git status</span></pre><ul class=""><li id="0340" class="mj mk iq ky b kz la lc ld lf ml lj mm ln mn lr mo mp mq mr bi translated">创建一个名为<code class="fe lt lu lv lw b">scrapyd-web</code>的新应用</li><li id="1632" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">设置一个名为 heroku 的 git 遥控器</li><li id="c499" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">检查 git 遥控器</li><li id="9ca5" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">将/scrappy web 文件夹中的内容推送到远程设备以部署应用程序</li><li id="d169" class="mj mk iq ky b kz ms lc mt lf mu lj mv ln mw lr mo mp mq mr bi translated">设置时区</li></ul><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="19d1" class="mc md iq lw b gy me mf l mg mh">$ pwd</span><span id="f46f" class="mc md iq lw b gy mi mf l mg mh">/Users/harrywang/xxx/scrapyd-cluster-on-heroku/scrapydweb</span><span id="6b3f" class="mc md iq lw b gy mi mf l mg mh">$ heroku apps:create scrapyd-web</span><span id="3f9a" class="mc md iq lw b gy mi mf l mg mh">$ heroku git:remote -a scrapyd-web</span><span id="4f86" class="mc md iq lw b gy mi mf l mg mh">set git remote heroku to <a class="ae kv" href="https://git.heroku.com/scrapyd-web.git" rel="noopener ugc nofollow" target="_blank">https://git.heroku.com/scrapyd-web.git</a></span><span id="a478" class="mc md iq lw b gy mi mf l mg mh">$ git remote -v<br/>$ git push heroku master<br/>$ heroku config:set TZ=US/Eastern<br/>Setting TZ and restarting ⬢ scrapyd-web... done, <strong class="lw ir">v6</strong></span><span id="ecfc" class="mc md iq lw b gy mi mf l mg mh">TZ: US/Eastern</span></pre><p id="91b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您需要向 web 服务器添加至少一个 Scrapyd 服务器(让我们添加您刚刚在<a class="ae kv" href="http://scrapy-server1.herokuapp.com" rel="noopener ugc nofollow" target="_blank">scrapy-server1.herokuapp.com</a>上面设置的服务器)。您可以以类似的方式添加更多用于分布式爬网的 scrapyd 服务器:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="4a74" class="mc md iq lw b gy me mf l mg mh">$ heroku config:set SCRAPYD_SERVER_1=scrapy-server1.herokuapp.com:80</span><span id="cc9a" class="mc md iq lw b gy mi mf l mg mh">Setting SCRAPYD_SERVER_1 and restarting ⬢ scrapyd-web... done, <strong class="lw ir">v6</strong></span><span id="de81" class="mc md iq lw b gy mi mf l mg mh">SCRAPYD_SERVER_1: scrapy-server1.herokuapp.com:80</span></pre><p id="8eab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，你有了运行在 http://scrapyd-web.herokuapp.com 的 scrapyd 网络服务器。在浏览器中打开该地址，使用您在 scrapydweb/scrapydweb _ settings _ v10 . py 文件中指定的用户名和密码登录，您应该会看到 web 服务器的管理 UI:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/1bf733cb8fb14a7a4b1ebbd8ab5a46d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hn23jLK8RxQQv0clkj9LGA.png"/></div></div></figure><p id="b9c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一个任务是使用<code class="fe lt lu lv lw b"><a class="ae kv" href="https://github.com/scrapy/scrapyd-client" rel="noopener ugc nofollow" target="_blank">scrapyd-client</a></code>部署我们的 scrapy 项目。</p><p id="1498" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">去我们的 scrapy 项目回购:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="9b18" class="mc md iq lw b gy me mf l mg mh">$ pwd</span><span id="3da8" class="mc md iq lw b gy mi mf l mg mh">/Users/harrywang/xxx/scrapy-tutorial</span></pre><p id="da81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用<code class="fe lt lu lv lw b">pip install git+<a class="ae kv" href="https://github.com/scrapy/scrapyd-client" rel="noopener ugc nofollow" target="_blank">https://github.com/scrapy/scrapyd-client</a></code>安装 scrapyd-client。</p><p id="d93b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">打开<code class="fe lt lu lv lw b">scrapy.cfg</code>文件，修改其内容，添加如下部署配置:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="7213" class="mc md iq lw b gy me mf l mg mh">[settings]<br/>default = tutorial.settings</span><span id="c349" class="mc md iq lw b gy mi mf l mg mh">[deploy]<br/>url = <a class="ae kv" href="http://scrapy-server1.herokuapp.com" rel="noopener ugc nofollow" target="_blank">http://scrapy-server1.herokuapp.com</a><br/>username = admin<br/>password = scrapydweb<br/>project = scrapy-tutorial</span></pre><p id="81c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，使用<code class="fe lt lu lv lw b">scrapyd-deploy</code>将我们的项目打包并部署到 scrapyd 服务器:</p><pre class="kg kh ki kj gt ly lw lz ma aw mb bi"><span id="c359" class="mc md iq lw b gy me mf l mg mh">$ scrapyd-deploy</span><span id="bcf1" class="mc md iq lw b gy mi mf l mg mh">/Users/harrywang/xxx/scrapy-tutorial/venv/lib/python3.6/site-packages/scrapyd_client/deploy.py:23: ScrapyDeprecationWarning: Module `scrapy.utils.http` is deprecated, Please import from `w3lib.http` instead.</span><span id="ec0a" class="mc md iq lw b gy mi mf l mg mh">from scrapy.utils.http import basic_auth_header</span><span id="b184" class="mc md iq lw b gy mi mf l mg mh">Packing version 1568331034</span><span id="a7d5" class="mc md iq lw b gy mi mf l mg mh">Deploying to project "scrapy-tutorial" in http://scrapy-server1.herokuapp.com/addversion.json</span><span id="7068" class="mc md iq lw b gy mi mf l mg mh">Server response (200):</span><span id="9884" class="mc md iq lw b gy mi mf l mg mh">{"node_name": "5f9ee34d-f6c8-4d80-ac05-3657c4920124", "status": "ok", "project": "scrapy-tutorial", "version": "1568331034", "spiders": 3}</span></pre><p id="6326" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在浏览器中打开<a class="ae kv" href="http://scrapyd-web.herokuapp.com/1/projects/" rel="noopener ugc nofollow" target="_blank">http://scrapyd-web.herokuapp.com/1/projects/</a>，您应该看到项目成功部署:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/f48837f65fd6ade2fc0b00bbfcea257c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*su4OwUkuHi7r3E47pToplA.png"/></div></div></figure><p id="8f9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">点击菜单"运行蜘蛛"，可以运行"行情"蜘蛛:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/2b78992117b4004cb4bb28897fd2e60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KA3dhR_0xFBee8C-Ba55tA.png"/></div></div></figure><p id="0226" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，您可以在“作业”菜单中检查结果，并在“项目”菜单中下载项目——您可以尝试其他菜单来查看统计数据、错误等。：</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/3ed16817c4f65b4378cfbf9272cc09f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9rUZPiI9XJQB8tm-yzVbg.png"/></div></div></figure><p id="3e57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一个任务是指定一个定时任务，比如每 10 分钟自动运行一次报价蜘蛛:点击“定时器任务”，下面的截图显示了一个每 10 分钟运行一次的任务——定时器任务的特性是基于一个名为<a class="ae kv" href="https://apscheduler.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> APScheduler </a>的高级 Python 调度器库，参见<a class="ae kv" href="https://apscheduler.readthedocs.io/en/latest/modules/triggers/cron.html#expression-types" rel="noopener ugc nofollow" target="_blank">这部分文档</a>来弄清楚如何为定时器设置不同的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/82e591381f5a2fd78f79be52c6ecd80c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RPrEybyn_G5a1y5tL9p6Vw.png"/></div></div></figure><p id="0129" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，您可以检查任务结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/7185b7503cf15fe03e943f0514283919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnXZdwkZ_hbem9VGJ0nJow.png"/></div></div></figure><p id="0223" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面显示计时器任务已经触发了 4 次:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/3c25fccbd9ec9f138d6d4937f86a33ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nj3fM9YWbh50_iQusQw6rw.png"/></div></div></figure><p id="f35c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，在将我们的项目部署到服务器时，我们不应该使用本地 SQLite 数据库。相反，我们应该保存到一个远程数据库，比如 MySQL 服务器——您只需要像我们在第三部分中讨论的那样更改 CONNECTION_STRING 变量。</p><p id="9787" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">恭喜你！你已经完成了本教程，我希望你喜欢学习。</strong></p><p id="e936" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为奖励，我还创建了一个单独的 repo ( <a class="ae kv" href="https://github.com/harrywang/scrapy-selenium-demo" rel="noopener ugc nofollow" target="_blank"> Scrapy + Selenium </a>)来展示如何抓取动态网页(例如通过滚动加载附加内容的页面)以及如何使用代理网络(ProxyMesh)来避免被禁止，请阅读第五部分。</p><p id="9d98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-i-11e350bcdec0?source=friends_link&amp;sk=c9f8e32f28a88c61987ec60f93b93e6d" rel="noopener">第一部分</a>、<a class="ae kv" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-ii-b917509b73f7?source=friends_link&amp;sk=ebd3a9cee8b2097b3857194fee3821a6">第二部分</a>、<a class="ae kv" rel="noopener" target="_blank" href="/a-minimalist-end-to-end-scrapy-tutorial-part-iii-bcd94a2e8bf3?source=friends_link&amp;sk=a1fdde9c9dd5383d8de2e08395ee3f98">第三部分</a>、<a class="ae kv" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-iv-3290d76a2aef?sk=6f0902f9a15092575814ab533a56f8ef" rel="noopener">第四部分</a>、<a class="ae kv" href="https://medium.com/@HarryWang/a-minimalist-end-to-end-scrapy-tutorial-part-v-e7743ee9a8ef?source=friends_link&amp;sk=c1c5110f63c7ccbe4eb8c6209ee2f57c" rel="noopener">第五部分</a></p></div></div>    
</body>
</html>