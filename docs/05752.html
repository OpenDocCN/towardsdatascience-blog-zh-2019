<html>
<head>
<title>Top Down View at Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的俯视图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/top-down-view-at-reinforcement-learning-f4a8b35ebf9a?source=collection_archive---------16-----------------------#2019-08-22">https://towardsdatascience.com/top-down-view-at-reinforcement-learning-f4a8b35ebf9a?source=collection_archive---------16-----------------------#2019-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4faf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将强化学习的不同部分和分支缝合在一起</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b99cbf4562f67969f5da799cce3fe1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ivj1t3qWkGdhxiGG"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@joshwp?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Josh Power</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5b05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae ky" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="2401" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当你是强化学习的新手时，你无疑会被一些奇怪的术语轰炸，比如基于模型的、无模型的、策略上的、策略外的等等…</p><p id="ccbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很快你会发现跟踪这些似乎无处不在、术语之间没有明显联系的术语令人疲惫不堪。</p><p id="e829" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章将尝试把所有这些术语放在一个角度，这样初学者就不会感到不知所措。</p><p id="81cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">声明:本文假设你已经知道什么是强化学习和一些现有的算法。它没有介绍或解释任何特定的算法，但它会试图<strong class="lb iu">将不同的分支</strong>放在一起，这样你就可以得到一个全面的大画面，以及这些分支是如何适应的。</p><h2 id="939f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">强化学习</h2><p id="d5b2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">正如已经建立的强化学习是一个框架，让代理人从经验中学习决策。它由一个与环境交互的代理组成，在环境中它采取行动并收集奖励。代理人的目标是收集最大的回报。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/05c02e0702301db4b54f35697dc60024.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/0*gVe5EeFBS4lZEEtE.png"/></div></figure><p id="800e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们需要建立以下定义。</p><h2 id="c980" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">状态</h2><p id="7f90" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">状态是描述某一时刻或某一时间步的情况的元素或特征的集合。状态的例子可以是机器人的位置、方向、周围的风景、风速、温度等等</p><h2 id="5139" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">环境安全</h2><p id="2d67" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">环境状态是描述某一时刻或某一时间步的环境的状态。环境的状态可能包含太多的细节，这些细节可能是不可能的，也没有兴趣包含在任何计算中。例如，机器人运动中的原子状态问题就不值得考虑。</p><h2 id="8e4e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">代理状态</h2><p id="28fa" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">代理状态是代理感知到的状态。代理可能无法检测环境的完整状态，例如，带有固定摄像机的机器人无法看到 360°视图。在扑克游戏中，代理人只知道对手的公共牌。一般来说，代理状态不同于环境状态，但在最简单的情况下，它们是相同的，例如在一些棋盘游戏中。</p><p id="c459" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在完全可观察的环境中，代理看到完整的环境状态，因此以这种方式观察是环境状态。<br/>这个智能体被称为处于马尔可夫决策过程(MDP)。</p><p id="40d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">部分可观察的环境，代理获得部分信息。这被称为部分可观察 MDP (POMDP)，env 仍然可以是 MDP，但是代理不知道它。</p><h2 id="38bb" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">历史</h2><p id="b16a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">历史是一系列的观察、行动和回报。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/1a5aafb1d0670184bcf74f45812ca931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sd7lvgsxpHHGClgL05A6EQ.png"/></div></div></figure><h2 id="a3c3" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">马尔可夫决策过程</h2><p id="651d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">马尔可夫决策过程(Markov Decision Process)(MDP)是一种在各种情况下对决策进行建模的数学框架。<br/>一个过程是马尔可夫的如果下一个状态只依赖于当前状态，任何过去的状态都是无关紧要的。</p><h2 id="a682" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">状态/动作值函数</h2><p id="dd05" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@zsalloum/q-vs-v-in-reinforcement-learning-the-easy-way-9350e1523031" rel="noopener">状态和动作值函数</a>，是给某个状态<strong class="lb iu"> <em class="mv"> s </em> </strong>或对状态<strong class="lb iu"><em class="mv"/></strong>执行的动作<strong class="lb iu"><em class="mv"/></strong>赋值的函数。这个想法是评估处于某个状态和/或执行某个动作相对于其他状态或动作的重要性。简而言之，它们告诉我们处于那种状态有多有价值，采取那种行动有多好。想象一盘国际象棋，白方有机会将死。处于这样的位置是一种非常有价值的状态，在这个位置上所有可能的行动中，进行将死是最好的行动。<br/>在状态数量有限的问题中，很容易计算出状态和动作的确切值。然而，当状态的数量变得非常大时，为了节省时间和资源，近似的需要将更加迫切。对于这类问题，使用函数近似法。</p><h2 id="f7bc" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">政策</h2><p id="a8fb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">策略<strong class="lb iu"> <em class="mv"> 𝜋(s) </em> </strong>是将状态映射到动作的功能。这就像在某种情况下，你问自己“我现在应该做什么？”。政策告诉你该采取什么行动。策略可以是确定性的，即相同的状态导致相同的动作，也可以是随机性的，即相同的状态根据某种概率分布导致不同的动作。在文章“<a class="ae ky" rel="noopener" target="_blank" href="/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182">开发者强化学习策略</a>”中可以找到对策略的简单介绍</p><h2 id="3652" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型</h2><p id="6364" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">一个模型预测环境下一步会做什么。例如，转移概率预测下一个状态，奖励函数预测下一个奖励。一个模型不会自动给我们一个好的政策，我们仍然需要计划。</p><h2 id="34f1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">学习和规划</h2><p id="5c3e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">RL 中的两个基本问题:</p><ul class=""><li id="50ed" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">学习:环境最初是未知的，代理通过与环境的交互来学习。</li><li id="a396" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">规划:给定一个模型，模型中的 agent 规划(无外部交互)。我们所说的计划是指推理、思考和探索。</li></ul><h2 id="dadc" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">预测和控制</h2><p id="2177" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">预测是在给定政策的情况下对未来的评估，而控制是通过找到最佳政策来优化未来，使累积回报最大化。</p><h2 id="c6f6" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">探索与开发</h2><p id="94e8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">探索就是寻找更多关于环境的信息。利用是指利用获得的信息来获得最大的回报。重要的是要知道<a class="ae ky" rel="noopener" target="_blank" href="/exploration-in-reinforcement-learning-e59ec7eeaa75">有多少东西要探索</a>有多少东西要开发。</p><h2 id="a153" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">基于模型/无模型</h2><p id="e65a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/model-based-reinforcement-learning-cb9e41ff1f0d">基于模型的强化学习</a>是关于环境动态的知识，例如状态之间的转移概率，以及奖励。这在棋盘游戏中是众所周知的，但在现实生活中却很难做到。</p><p id="b3b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型可以被给定或被学习，然后它可以服从计划，或以不需要采取实际行动的方式被学习。规划阶段使用专门的算法，如<a class="ae ky" href="https://en.wikipedia.org/wiki/AlphaZero" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>中使用的<a class="ae ky" rel="noopener" target="_blank" href="/monte-carlo-tree-search-in-reinforcement-learning-b97d3e743d0f">蒙特卡罗树搜索</a>。<br/>不言而喻，模型必须足够精确才能代表真实的问题，否则基于不精确的模型来规划行动将会浪费时间和资源，导致在真实环境中表现不佳。</p><p id="acf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/@zsalloum/dynamic-programming-in-reinforcement-learning-the-easy-way-359c7791d0ac" rel="noopener">动态规划</a>是基于模型的算法之一。</p><p id="ae0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，无模型算法不依赖于模型来学习，它们通过直接经验来学习，这意味着它们在真实环境中采取行动。在这些算法中，我们可以找到<a class="ae ky" href="https://medium.com/@zsalloum/monte-carlo-in-reinforcement-learning-the-easy-way-564c53010511" rel="noopener">蒙特卡罗</a>和<a class="ae ky" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce">时间差异(TD) </a>。</p><h2 id="125b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">开策略/关策略</h2><p id="6355" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在<a class="ae ky" rel="noopener" target="_blank" href="/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce"> TD 学习</a>中，我们计算状态 s 下的动作值 Q(s，a)，同时考虑下一个状态 Q(s’，a’)下的动作值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/30c685b08fd0d3ebb76e0fda0bffaee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*JtGgRX1a1BAbwtECmGqAjA.png"/></div></figure><p id="865a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个状态的 q 受开/关策略方法的影响。</p><p id="8814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在策略上包括基于某个策略计算 Q(s，a)值<strong class="lb iu"><em class="mv"/></strong>，意味着 Q(s，a)需要值 q(s’，a’)。为了得到 Q(s '，a ')的值，我们需要动作<strong class="lb iu"> <em class="mv"> a' </em> </strong>，这是使用带来动作<strong class="lb iu"><em class="mv"/></strong>的相同策略<strong class="lb iu"><em class="mv">【𝜋】</em></strong>得到的。当移动到状态<strong class="lb iu"><em class="mv">' s '</em></strong>时，我们仍然遵循先前已经确定的动作<strong class="lb iu"> <em class="mv"> a' </em> </strong>。在策略上使用的算法称为 SARSA。</p><p id="1cd5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，关闭策略通过在<strong class="lb iu"><em class="mv"/></strong>使用所有可用动作的最大 Q(s’)来计算 Q(s，a)。这意味着我们不必根据策略<strong class="lb iu"><em class="mv"/></strong>选择特定动作<strong class="lb iu"><em class="mv">a’</em></strong>我们只需选择具有最高值的 q(s’，a’)，然后当移动到状态<strong class="lb iu"><em class="mv">s’</em></strong>时，我们不必遵循带来最大 q 值的动作<strong class="lb iu"> <em class="mv">。</em> </strong>使用这种技术的算法叫做 Q-Learning。</p><h2 id="b0d7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">分类代理</h2><p id="22a0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">根据智能体算法用于学习和决策的组件类型，智能体可以以不同的方式进行分类:</p><p id="3637" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/math-behind-reinforcement-learning-the-easy-way-1b7ed0c030f4">基于值的</a></p><ul class=""><li id="895c" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">没有策略(它是隐式的)</li><li id="37e2" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">价值函数</li></ul><p id="8883" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/policy-based-reinforcement-learning-the-easy-way-8de9a3356083">基于策略</a></p><ul class=""><li id="c760" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">政策</li><li id="a7a5" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">无价值函数</li></ul><p id="4a56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-actor-critic-7642bdb2b3d2">演员评论家</a></p><ul class=""><li id="3953" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">政策</li><li id="b90a" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">价值函数</li></ul><p id="b0a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与上述类别正交，上述每种类型的代理可以是:</p><p id="03f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无模型</p><ul class=""><li id="2295" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">政策/价值功能</li><li id="90bc" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">没有模型</li></ul><p id="e654" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于模型:</p><ul class=""><li id="7d66" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">可选策略/价值函数</li><li id="fde9" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">模型</li></ul><p id="14b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图总结了强化学习中常用的代理类别</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/9e135c13cec13287d916a1f556832bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RE87ovXug27R5xU24L76qg.png"/></div></div></figure><p id="3215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这张图片展示了一些 RL 算法及其类别:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/1477637e1f7ff34ef5c3f5dadb259d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hbQSg0w8u3hPF8ra2xRemA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms" rel="noopener ugc nofollow" target="_blank">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#a-taxonomy-of-rl-algorithms</a></figcaption></figure></div></div>    
</body>
</html>