<html>
<head>
<title>Understanding Neural Machine Translation: Encoder-Decoder Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经机器翻译:编码器-解码器架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-neural-machine-translation-encoder-decoder-architecture-80f205643ba4?source=collection_archive---------13-----------------------#2019-11-10">https://towardsdatascience.com/understanding-neural-machine-translation-encoder-decoder-architecture-80f205643ba4?source=collection_archive---------13-----------------------#2019-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7319" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">机器翻译的技术水平已经利用了使用编码器-注意力-解码器模型的递归神经网络(RNNs)。在这里，我将尝试从一个高层次的角度来介绍它是如何工作的。</p><h1 id="c7be" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">语言翻译:组件</h1><p id="5dda" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们可以把翻译分成两个部分:单个单位和语法:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/550c01849c61411f2658663046f52078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*zXIgGOzWGyiebEwnKVUwaA.png"/></div></figure><p id="36d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了在神经网络中进行计算，我们需要将单词序列编码到向量空间中。因为单词也具有有意义的序列，所以递归神经网络适合于这项任务:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/7128fd1ead5b897cd9d9d45a9b8fbdb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*7AHmFKyen8w3mZ3d5BXfig.png"/></div></figure><h1 id="6e0f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">问题是</h1><p id="e2dc" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">然而，这种编码器-解码器架构在大约 20 个以上单词的句子之后就崩溃了:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/8c6085962ff189a82e2ed6fa3026ea8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*3ihYPptjXdcK8MwY7gpJLQ.png"/></div></figure><p id="5372" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么？较长的句子说明了单向编码器-解码器结构的局限性。</p><p id="7d1e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为语言由记号和语法组成，这个模型的问题是它没有完全解决语法的复杂性。</p><p id="6318" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">具体来说，当翻译源语言中的第 n 个单词时，RNN 只考虑源句子中的第一个 n 个单词，但从语法上来说，一个单词的含义取决于句子中<strong class="jp ir">之前和</strong>之后的单词的顺序:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/d87c3e0b55cc91a22e15265f681f6e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Jz_eZ0z1nXob2JxeWd5CuA.png"/></div></figure><h1 id="bbba" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">一个解决方案:双向 LSTM 模型</h1><p id="57da" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">如果我们使用双向模型，它允许我们输入过去和未来单词的<strong class="jp ir">的上下文，以创建准确的编码器输出向量:</strong></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/f8b70c4f44391f69f5b419dc593e56e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghtokIlQiwIWve-R-y7RmA.png"/></div></div></figure><p id="5aa9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，接下来的挑战就变成了，<strong class="jp ir">在一个序列中，我们需要关注哪个单词？</strong></p><p id="3102" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2016 年，巴赫达瑙等人。艾尔。发表了一篇论文，表明我们可以通过存储 LSTM 细胞以前的输出，然后根据每个输出的相关性对它们进行排序，并选择得分最高的单词，来学习要关注源语言中的哪些单词:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mb"><img src="../Images/ccb961f80f220de4dda771f4f0b43cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFp1rPwDV7qKv6A367PXcw.png"/></div></div></figure><p id="4262" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面，您可以在图表中看到这种情况:最终的架构将这种注意力机制嵌入编码器和解码器之间:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/5e529a1831cedd388c010ea51befecb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*JnY4vWYc2ewre6WybhRKGg.png"/></div></figure><p id="302f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以看到，与之前的编码器-解码器架构相比，性能有所提升:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mc"><img src="../Images/835180a41bb33c48ca302e021436b2a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8w_Sc-aas5PCHdBQ8lSl_w.png"/></div></div></figure><p id="587d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简而言之就是这样，这也是谷歌翻译的 NMT 的工作方式，尽管编码器 LSTMs 的层数更多。</p><p id="eca1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单来说，就是这样:</p><ol class=""><li id="7ae0" class="md me iq jp b jq jr ju jv jy mf kc mg kg mh kk mi mj mk ml bi translated">编码器获取源语言中的每个单词，并将其编码到向量空间中</li><li id="d27d" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">然后，单词的这些矢量表示被传递到注意机制，该机制确定在为期望的语言生成一些输出的同时关注哪些源单词。</li><li id="b99d" class="md me iq jp b jq mm ju mn jy mo kc mp kg mq kk mi mj mk ml bi translated">这个输出通过一个解码器，将向量表示转换成目标语言</li></ol><p id="141b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">来自<a class="ae mr" href="https://www.youtube.com/watch?v=AIpXjFwVdIE&amp;t=405s" rel="noopener ugc nofollow" target="_blank"> CS 道场社区</a>的插图</p></div></div>    
</body>
</html>