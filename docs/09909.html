<html>
<head>
<title>Emotion Detection: a Machine Learning Project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">情绪检测:一个机器学习项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/emotion-detection-a-machine-learning-project-f7431f652b1f?source=collection_archive---------0-----------------------#2019-12-28">https://towardsdatascience.com/emotion-detection-a-machine-learning-project-f7431f652b1f?source=collection_archive---------0-----------------------#2019-12-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c5f6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个关于情感检测的计算机视觉项目</h2></div><h1 id="6ec5" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">情绪检测(<em class="la"> n.): </em></h1><p id="7471" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="lx">识别人类情感的过程</em></p><p id="e699" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">如果有人给你看一张某人的照片，并让你猜猜他们的感受，你很可能会有很好的想法。如果你的电脑也能做到这一点会怎么样？如果它能变得比你更好呢？这似乎是一个荒谬的想法，对不对？</p><p id="b70f" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在我的上一篇博客(<em class="lx">读作:</em> <a class="ae md" href="https://medium.com/@aarohigupta2211/demystify-artificial-intelligence-4d5310ea1294" rel="noopener"> <em class="lx">揭秘人工智能</em> </a>)中，我说过我会解释一个项目，展示所讨论的概念是如何应用的。</p><p id="8ab7" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">下面的博客包含了我们在 InspiritAI 项目期间所做的一系列事情的回忆，虽然它确实显示了进行情绪检测所需的大量代码，但这绝不是训练模型的最快方法。使用多个 ML 和 AI 模型来查看它们之间的差异。</p><p id="9f10" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">情绪检测的三个主要组成部分如下:</p><ol class=""><li id="2069" class="me mf it ld b le ly lh lz lk mg lo mh ls mi lw mj mk ml mm bi translated">图像预处理</li><li id="9e1a" class="me mf it ld b le mn lh mo lk mp lo mq ls mr lw mj mk ml mm bi translated">特征抽出</li><li id="6d0e" class="me mf it ld b le mn lh mo lk mp lo mq ls mr lw mj mk ml mm bi translated">特征分类</li></ol><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ms"><img src="../Images/24719f61847df08a7253d1c6038953f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsr8-BRHxXKwFQZjBP1qAQ.png"/></div></div></figure><h2 id="9e09" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">面部检测:</h2><p id="9297" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">人脸检测是情感检测中的一个重要步骤。它会删除图像中不相关的部分。这里有一种在图像中检测人脸的方法。</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="75c9" class="ne kj it nr b gy nv nw l nx ny">import dlib<br/>import numpy as np</span><span id="853b" class="ne kj it nr b gy nz nw l nx ny">frontalface_detector = dlib.get_frontal_face_detector()</span><span id="e92f" class="ne kj it nr b gy nz nw l nx ny">def rect_to_bb(rect):<br/>    x = rect.left()<br/>    y = rect.top()<br/>    w = rect.right() - x<br/>    h = rect.bottom() - y<br/>    return (x, y, w, h)</span><span id="78ff" class="ne kj it nr b gy nz nw l nx ny">def detect_face(image_url):<br/>    try:<br/>        url_response = urllib.request.urlopen(image_url)<br/>        img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)<br/>        image = cv2.imdecode(img_array, -1)</span><span id="bd87" class="ne kj it nr b gy nz nw l nx ny">rects = frontalface_detector(image, 1)</span><span id="9add" class="ne kj it nr b gy nz nw l nx ny">if len(rects) &lt; 1:<br/>    return "No Face Detected"</span><span id="60d5" class="ne kj it nr b gy nz nw l nx ny">for (i, rect) in enumerate(rects):<br/>    (x, y, w, h) = rect_to_bb(rect)<br/>    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)</span><span id="b0e2" class="ne kj it nr b gy nz nw l nx ny">plt.imshow(image, interpolation='nearest')<br/>plt.axis('off')<br/>plt.show()</span></pre><p id="338a" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">另一种方法是使用 dlib 的预训练人脸检测器模型，该模型也将在下一点中使用。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/61f10144354fe82794a5b0a3640e4f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*fhfhBoekfxDptuxcP2ROow.png"/></div></figure><h2 id="d16f" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">面部标志:</h2><p id="1b96" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">面部标志是人脸图像上的一组关键点。这些点由它们在图像上的(x，y)坐标定义。这些点用于定位和表示面部的显著区域，例如眼睛、眉毛、鼻子、嘴和下颌线。</p><p id="ced1" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们使用的面部标志模型是 Dlib 的预训练面部标志检测模型，它检测人脸上的 68 个二维点。</p><p id="dfa4" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">您可以像这样加载模型:</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="575f" class="ne kj it nr b gy nv nw l nx ny">import dlib<br/>import numpy as np</span><span id="cfa5" class="ne kj it nr b gy nz nw l nx ny">frontalface_detector = dlib.get_frontal_face_detector()</span><span id="8b0d" class="ne kj it nr b gy nz nw l nx ny">landmark_predictor=dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')</span><span id="b415" class="ne kj it nr b gy nz nw l nx ny">def get_landmarks(image_url):<br/>    try:<br/>        url_response = urllib.request.urlopen(image_url)<br/>        img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)<br/>        image = cv2.imdecode(img_array, -1)</span><span id="2921" class="ne kj it nr b gy nz nw l nx ny">    except Exception as e:<br/>        print ("Please check the URL and try again!")<br/>        return None,None<br/>    faces = frontalface_detector(image, 1)</span><span id="e64b" class="ne kj it nr b gy nz nw l nx ny">    if len(faces):<br/>        landmarks = [(p.x, p.y) for p in landmark_predictor(image, faces[0]).parts()]</span><span id="7253" class="ne kj it nr b gy nz nw l nx ny">    else:<br/>        return None,None<br/>    <br/>    return image,landmarks</span><span id="685a" class="ne kj it nr b gy nz nw l nx ny">def image_landmarks(image,face_landmarks):<br/>    radius = -1<br/>    circle_thickness = 4<br/>    image_copy = image.copy()<br/>    for (x, y) in face_landmarks:<br/>        cv2.circle(image_copy, (x, y), circle_thickness, (255,0,0), radius)<br/>        plt.imshow(image_copy, interpolation='nearest')<br/>        plt.axis('off')<br/>        plt.show()</span></pre><p id="fba2" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">使用该模型后，您的输出应该如下所示:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/3dedf3a0efbb9225927c93a0b96873fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*MXZPIYvbJ0FQrZMTka5vUA.png"/></div></figure><p id="cb14" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">在这个模型中，面部特征的具体标志是:</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="9b25" class="ne kj it nr b gy nv nw l nx ny">Jawline = 0–16<br/>Eyebrows = 17–26<br/>Nose = 27–35<br/>Left eye = 36–41<br/>Right eye = 42–47<br/>Mouth = 48–67</span></pre><p id="5c02" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">区分两种情绪的一种方法是看这个人的嘴和眼睛是否张开。我们可以找到嘴部各点之间的欧几里德距离，如果一个人感到惊讶，这个距离会比他们没有惊讶时更大。</p><h1 id="799f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据预处理</h1><p id="5543" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在使用数据之前，重要的是要经过一系列被称为预处理的步骤。这使得数据更容易处理。</p><p id="01bb" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们将使用由五个情感标签组成的 fer2013 数据集的修改版本。</p><p id="3d3c" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">数据集存储在 CSV 文件中。CSV 文件中的每一行都代表一个实例。每个实例都有两个列属性:</p><ul class=""><li id="573b" class="me mf it ld b le ly lh lz lk mg lo mh ls mi lw oc mk ml mm bi translated">以字符串格式存储的图像像素</li><li id="8222" class="me mf it ld b le mn lh mo lk mp lo mq ls mr lw oc mk ml mm bi translated">目标标签的整数编码</li></ul><p id="54d0" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">共有 20，000 个图像平均分布在五种情绪中。这些图像是 48*48 灰度的裁剪图像。CSV 文件由以字符串形式存储的图像的扁平数组组成</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="fa59" class="ne kj it nr b gy nv nw l nx ny">The target labels are integer encoded in the csvfile. They are mapped as follows:</span><span id="c82e" class="ne kj it nr b gy nz nw l nx ny">0 — -&gt; Angry<br/>1 — -&gt; Happy<br/>2 — -&gt; Sad<br/>3 — -&gt; Surprise<br/>4 — -&gt; Neutral</span></pre><h2 id="3e97" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">加载数据集</h2><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="a122" class="ne kj it nr b gy nv nw l nx ny">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="623c" class="ne kj it nr b gy nz nw l nx ny">label_map={"0":"ANGRY","1":"HAPPY","2":"SAD","3":"SURPRISE","4":"NEUTRAL"}</span><span id="f803" class="ne kj it nr b gy nz nw l nx ny">df = pd.read_csv("./ferdata.csv")</span><span id="8975" class="ne kj it nr b gy nz nw l nx ny">df.head()</span></pre><p id="3588" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">该数据集包含图像的原始像素值。</p><h2 id="6b4c" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">拆分数据</h2><p id="8539" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">正如上次讨论的那样，数据需要分成两个不同的集合:</p><ol class=""><li id="2bb8" class="me mf it ld b le ly lh lz lk mg lo mh ls mi lw mj mk ml mm bi translated">训练集:算法会一遍又一遍地读取或“训练”它，以尝试和学习它的任务。</li><li id="c5d7" class="me mf it ld b le mn lh mo lk mp lo mq ls mr lw mj mk ml mm bi translated">测试集:算法在这些数据上进行测试，看看它的效果如何。</li></ol><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="6086" class="ne kj it nr b gy nv nw l nx ny">from sklearn.model_selection import train_test_split</span><span id="a047" class="ne kj it nr b gy nz nw l nx ny">X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.1,random_state=42,stratify =dataY)</span></pre><h2 id="ced8" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">使数据标准化</h2><p id="dfb8" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">标准化是将不同的变量放在同一尺度上的过程。它重新调整数据，使平均值为 0，标准差为 1。这种转换以数据为中心。</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="e5be" class="ne kj it nr b gy nv nw l nx ny">from sklearn.preprocessing import StandardScaler</span><span id="6638" class="ne kj it nr b gy nz nw l nx ny">scaler = StandardScaler()<br/>scaler.fit(X_train)<br/>X_train = scaler.transform(X_train)<br/>X_test = scaler.transform(X_test)</span></pre><h1 id="683c" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">线性模型</h1><h2 id="efab" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">k-最近邻</h2><p id="4046" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">KNN 是一种非参数学习算法，这意味着它不对数据的分布做出任何假设。我们使用点之间的欧几里得距离作为数据。</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="867c" class="ne kj it nr b gy nv nw l nx ny">from sklearn.neighbors import KNeighborsClassifier</span><span id="0632" class="ne kj it nr b gy nz nw l nx ny">knn = KNeighborsClassifier(n_neighbors=3)<br/>knn.fit(X_train, Y_train)<br/>predictions = knn.predict(X_test)</span></pre><p id="51da" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">要确定模型的准确性:</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="aa97" class="ne kj it nr b gy nv nw l nx ny">from sklearn.metrics import accuracy_score</span><span id="437c" class="ne kj it nr b gy nz nw l nx ny">print(accuracy_score(Y_test, predictions)*100)</span></pre><p id="bbdc" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们的准确率约为 50%，所以我们尝试了一些非线性模型。</p><p id="c917" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">您可以尝试将原始像素值输入到模型中，并查看它如何影响模型的准确性。</p><h1 id="fdd9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">非线性模型</h1><h2 id="fc3f" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">多层感知器</h2><p id="9754" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">MLPs 是神经网络的一个子类。它们由一层或多层神经元组成。输入层是输入数据的地方，之后可能有一个或多个隐藏层。预测来自输出层。</p><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="b7a4" class="ne kj it nr b gy nv nw l nx ny">from keras.models import Sequential<br/>from keras.utils import to_categorical<br/>from keras.layers import Dense, Dropout, Flatten, Activation<br/>from keras.losses import categorical_crossentropy<br/>from keras.callbacks import EarlyStopping, ModelCheckpoint<br/>from keras.models import load_model<br/>from keras.optimizers import Adam, SGD</span><span id="d367" class="ne kj it nr b gy nz nw l nx ny">mlp_model = Sequential()</span><span id="b402" class="ne kj it nr b gy nz nw l nx ny">mlp_model.add(Dense(1024, input_shape = (2304,), activation = 'relu', kernel_initializer='glorot_normal'))</span><span id="4484" class="ne kj it nr b gy nz nw l nx ny">mlp_model.add(Dense(512, activation = 'relu', kernel_initializer='glorot_normal'))</span><span id="0c5d" class="ne kj it nr b gy nz nw l nx ny">mlp_model.add(Dense(5, activation = 'softmax'))</span><span id="4057" class="ne kj it nr b gy nz nw l nx ny">mlp_model.compile(loss=categorical_crossentropy, optimizer=SGD(lr=0.001), metrics=['accuracy']) </span><span id="b39d" class="ne kj it nr b gy nz nw l nx ny">checkpoint = ModelCheckpoint('best_mlp_model.h5',verbose=1,<br/>monitor='val_acc', save_best_only=True,mode='auto')</span><span id="32a4" class="ne kj it nr b gy nz nw l nx ny">mlp_history = mlp_model.fit(X_train, y_train, batch_size=batch_size,<br/>epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data(X_test, y_test),shuffle=True)</span></pre><p id="91ee" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">我们使用像素的准确率大约是 50%，当我们使用面部标志之间的距离而不是像素值时，准确率有所提高。然而，我们想要更精确的模型，所以我们决定使用 CNN。</p><h2 id="b3dd" class="ne kj it bd kk nf ng dn ko nh ni dp ks lk nj nk ku lo nl nm kw ls nn no ky np bi translated">卷积神经网络</h2><pre class="mt mu mv mw gt nq nr ns nt aw nu bi"><span id="265a" class="ne kj it nr b gy nv nw l nx ny">width, height = 48, 48</span><span id="dd41" class="ne kj it nr b gy nz nw l nx ny">X_train = X_train.reshape(len(X_train),height,width)</span><span id="d97c" class="ne kj it nr b gy nz nw l nx ny">X_test = X_test.reshape(len(X_test),height,width)</span><span id="5708" class="ne kj it nr b gy nz nw l nx ny">X_train = np.expand_dims(X_train,3)</span><span id="773b" class="ne kj it nr b gy nz nw l nx ny">X_test = np.expand_dims(X_test,3)</span><span id="1248" class="ne kj it nr b gy nz nw l nx ny">cnn_model = Sequential()</span><span id="acbc" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(Conv2D(5000, kernel_size=(4, 4), activation='relu', padding='same', input_shape = (width, height, 1)))</span><span id="b569" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(BatchNormalization())</span><span id="85ee" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(MaxPooling2D(pool_size=(3, 3), strides=(4, 4)))</span><span id="b2e1" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(Dropout(0.2))</span><span id="51ed" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(Flatten())</span><span id="a3d4" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(Dense(2000, activation='relu'))</span><span id="d9ac" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(Dropout(0.2))</span><span id="aab1" class="ne kj it nr b gy nz nw l nx ny">cnn_model.add(Dense(5, activation='softmax'))</span><span id="7ed2" class="ne kj it nr b gy nz nw l nx ny">checkpoint = ModelCheckpoint('best_cnn_model.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')</span><span id="92c7" class="ne kj it nr b gy nz nw l nx ny">cnn_model.compile(loss=categorical_crossentropy,<br/>optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999), <br/>metrics=['accuracy'])</span><span id="1d1a" class="ne kj it nr b gy nz nw l nx ny">cnn_history = cnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], <br/>validation_data=(X_test, y_test),shuffle=True)</span><span id="4605" class="ne kj it nr b gy nz nw l nx ny">cnn_model.save('cnn_model.h5')</span></pre><p id="30f5" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">为了提高性能，你可以改变下降，密集层的数量和激活功能。我们还使用名为 VGG 的 CNN 进行迁移学习，这是一种用于图像分类的预训练卷积神经网络。</p><h1 id="f867" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><span class="l od oe of bm og oh oi oj ok di"> E </span>估价</h1><p id="a67a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">使用 VGG 得到的结果最好，其正确率约为 68–70 %,但即使是线性模型也做得非常好。虽然 50%的准确率看起来不算多，但这仍然比你随机选取一张图片和一个标签要高。在这一点上，你大约有 20%的几率是正确的。</p><p id="c297" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">然而，在这个特定的数据集中，VGG 的表现甚至比人类更好。CNN 和 MLP 之间的区别在于，CNN 提取他们自己认为重要的特征，而我们将像素或界标作为特征提供给 MLP。</p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="693a" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated">更多关于 KNNs、CNN、MLPs 和其他基本机器学习主题的信息，请点击<a class="ae md" href="https://medium.com/@aarohigupta2211/demystify-artificial-intelligence-4d5310ea1294" rel="noopener">此链接</a>。</p><p id="bb55" class="pw-post-body-paragraph lb lc it ld b le ly ju lg lh lz jx lj lk ma lm ln lo mb lq lr ls mc lu lv lw im bi translated"><em class="lx">特别感谢 Adeesh Goel 组织了这次令人惊叹的研讨会，查看 facebook 页面</em><a class="ae md" href="https://www.facebook.com/inspiritAI/" rel="noopener ugc nofollow" target="_blank"><em class="lx"/></a><em class="lx">【InspiritAI】，感谢 Tyler Bonnen 担任如此出色的导师并指导我们完成该计划，并感谢我的团队成员 Khushi 和 Harsh。</em></p></div></div>    
</body>
</html>