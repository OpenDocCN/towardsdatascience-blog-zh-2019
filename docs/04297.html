<html>
<head>
<title>Truecasing in natural language processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的 Truecasing</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21?source=collection_archive---------6-----------------------#2019-07-04">https://towardsdatascience.com/truecasing-in-natural-language-processing-12c4df086c21?source=collection_archive---------6-----------------------#2019-07-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="66fc" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">全面回顾使用 NLTK、spaCy、StandfordNLP 纠正单词大写的 NLP 方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c0f45ac4c3823e030a576905e2f5d5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P2eCxnzlXqPBHvTH9Q950g.jpeg"/></div></div></figure><p id="9865" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">自然语言处理(NLP)是分析以自然语言之一表示记录的文本数据的学科。Ethnologue.com(第 21 版)有数据表明，在目前列出的 7，111 种现存语言中，有 3，995 种有发达的书写系统(如英语、法语、<a class="ae lq" href="http://yemba.net" rel="noopener ugc nofollow" target="_blank"> Yemba </a>、汉语、……)。自然语言处理的应用包括文本分类、拼写 T20 语法纠正、信息提取、语音识别、机器翻译、文本到语音合成、同义词生成，以及更高级的领域，如摘要、问答、对话系统和语音模仿。</p><p id="31f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我们将通过一个实际的例子来回顾现有的解决 Truecasing 的方法。<a class="ae lq" href="https://en.wikipedia.org/wiki/Truecasing" rel="noopener ugc nofollow" target="_blank">true case</a>是一个自然语言处理问题，即在无法获得相关信息的文本中找到单词的正确大写。Truecasing 有助于完成 NLP 任务，如<a class="ae lq" href="https://en.wikipedia.org/wiki/Named_entity_recognition" rel="noopener ugc nofollow" target="_blank">命名实体识别</a>、<a class="ae lq" href="https://en.wikipedia.org/wiki/Automatic_content_extraction" rel="noopener ugc nofollow" target="_blank">自动内容提取</a>、以及<a class="ae lq" href="https://en.wikipedia.org/wiki/Machine_translation" rel="noopener ugc nofollow" target="_blank">机器翻译</a>。专有名词大写可以更容易地发现专有名词，并有助于提高翻译的准确性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/4d4ef187b217cf1b5515cd0822d36071.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*nLl2AKSoIf9hWydcpNheWA.png"/></div></figure><p id="bf73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有几种实用的方法可以解决真实情况问题:</p><ul class=""><li id="ebe4" class="ls lt it kw b kx ky la lb ld lu lh lv ll lw lp lx ly lz ma bi translated"><strong class="kw iu">句子分割</strong>:将输入的文本拆分成句子，并将每个句子的第一个单词大写。</li><li id="91c0" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated"><strong class="kw iu">词性标注</strong>:查找句子中每个词的定义和上下文，并分配最合适的标注。最后，用特定的标签包装单词。</li><li id="d054" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated"><strong class="kw iu">名称-实体-识别(NER) </strong>:将句子中的单词分类到特定的类别，并决定大写例如人名等。</li><li id="c180" class="ls lt it kw b kx mb la mc ld md lh me ll mf lp lx ly lz ma bi translated"><strong class="kw iu">统计建模</strong>:通过对常用大写形式的词和词组进行统计模型训练。</li></ul><p id="72b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将探索每一种方法，寻找现有的开源库和代码片段。我们将实施和测试有希望的方法，并解释它们如何工作或不工作。最后，我们将提出一个建议，以及开放的关注点和未来的工作。</p><p id="bae4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们考虑下面的句子，大部分单词都是小写的。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="a2e5" class="ml mm it mh b gy mn mo l mp mq">text = "I think that john stone is a nice guy. there is a stone on the grass. i'm fat. are you welcome and smart in london? is this martin's dog?"</span></pre><p id="b30a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">分词</strong></p><p id="046d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">自然语言工具包(NLTK)为计算机语言学家和 NLP 专家所熟知。它最初创建于 2001 年，是宾夕法尼亚大学计算语言学课程的一部分。它提供了一个直观的框架以及大量的构建模块，给用户一个实用的 NLP 知识，而不会陷入通常与处理带注释语言<br/>数据相关的繁琐的内务处理中。通过在 Python Anaconda 提示符下运行以下命令来安装 NLTK。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="06d1" class="ml mm it mh b gy mn mo l mp mq">pip install –-upgrade nltk</span></pre><p id="9a5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">NLTK 附带了记号赋予器<strong class="kw iu">。它们的工作方式类似于正则表达式，用于将字符串分成子字符串列表。特别是，下面使用句子标记器来查找我们文本中的句子列表。</strong></p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="6f54" class="ml mm it mh b gy mn mo l mp mq">from nltk.tokenize import sent_tokenize<br/>import re</span><span id="c314" class="ml mm it mh b gy mr mo l mp mq">def truecasing_by_sentence_segmentation(input_text):<br/>    # split the text into sentences<br/>    sentences = sent_tokenize(input_text, language='english')<br/>    # capitalize the sentences<br/>    sentences_capitalized = [s.capitalize() for s in sentences]<br/>    # join the capitalized sentences<br/>    text_truecase = re.sub(" (?=[\.,'!?:;])", "", ' '.join(sentences_capitalized))<br/>    return text_truecase</span><span id="4089" class="ml mm it mh b gy mr mo l mp mq">truecasing_by_sentence_segmentation(text)<br/>"I think that john stone is a nice guy. There is a stone on the grass. I'm fat. Are you welcome and smart in london? Is this martin's dog?"</span></pre><p id="e849" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们看到的第一个单词，如果每个句子的大写字母正确。然而，像<code class="fe ms mt mu mh b">john</code>或<code class="fe ms mt mu mh b">london</code>这样的词仍然需要正确处理。</p><p id="f8b2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">用 NLTK 进行词性标注</strong></p><p id="c932" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了能够将单词<em class="mv"> john </em>和<em class="mv"> london </em>识别为人名和城市，我们使用了所谓的词性技术，这是根据单词的定义和上下文将文本中的单词标记为与特定词性相对应的过程。这是一个基于上下文的监督学习解决方案，在流行的 Penn Treebank 标签集上训练。NLTK 库中的 POS 标记器为某些单词输出特定的标记。比如 NN 代表名词，用单数，比如<em class="mv">斯通</em>、<em class="mv">约翰</em>和<em class="mv">盖伊</em>。NNS 代表复数名词。下面我们将文本中的所有名词都大写。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="3bef" class="ml mm it mh b gy mn mo l mp mq">from nltk.tokenize import word_tokenize<br/>from nltk.tag import pos_tag<br/>def truecasing_by_pos(input_text):<br/>    # tokenize the text into words<br/>    words = nltk.word_tokenize(text)<br/>    # apply POS-tagging on words<br/>    tagged_words = nltk.pos_tag([word.lower() for word in words])<br/>    # apply capitalization based on POS tags<br/>    capitalized_words = [w.capitalize() if t in ["NN","NNS"] else w for (w,t) in tagged_words]<br/>    # capitalize first word in sentence<br/>    capitalized_words[0] = capitalized_words[0].capitalize()<br/>    # join capitalized words<br/>    text_truecase = re.sub(" (?=[\.,'!?:;])", "", ' '.join(capitalized_words))<br/>    return text_truecase</span><span id="e4bb" class="ml mm it mh b gy mr mo l mp mq">truecasing_by_pos(text)</span><span id="3b64" class="ml mm it mh b gy mr mo l mp mq">"I think that John Stone is a nice Guy. there is a Stone on the Grass. i'm fat. are you welcome and smart in London? is this Martin's Dog?"</span></pre><p id="5204" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所看到的，词性标注方法比句子分割方法取得了更好的效果。然而，我们将像<code class="fe ms mt mu mh b">grass</code>和<code class="fe ms mt mu mh b">dog</code>这样在标准英语中不应该大写的单词大写。</p><p id="b744" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">带空间的命名实体识别</strong></p><p id="c0fe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">spaCy 是工业级 NLP 的快速库。它支持词向量、语义分布和许多统计模型，我们不仅可以使用这些模型进行更好的词性标注(POS)，还可以用于命名实体识别(NER)。</p><p id="9c24" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">而词性标注(POS)的目的是识别一个词属于哪个语法组，所以它是名词、形容词、代词等。；另一方面，命名实体识别(NER)试图找出一个词是否是一个命名实体，如人、地点、组织等。</p><p id="4453" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">spaCy 带有预先训练的语言模型，通常是用于各种语言处理任务的卷积神经网络，如 POS 和 NER。我们将使用 en_core_web_lg 模型，这是一个英语多任务 CNN (700 MB)。也可以使用较小的型号 en_core_web_sm (10 MB)，结果稍差。</p><p id="9653" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">spaCy 通过运行命令<code class="fe ms mt mu mh b">pip install -U spacy</code>来安装，然后在 Anaconda 提示符下运行<code class="fe ms mt mu mh b">python -m spacy download en_core_web_lg</code>来下载模型。下面我们显示文本中的单词及其相应的位置标签和 NER。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="7e2c" class="ml mm it mh b gy mn mo l mp mq">import spacy<br/>spacy_nlp = spacy.load('en_core_web_lg')<br/>words = spacy_nlp(text)<br/>[(w.text, w.tag_) for w in words]<br/>[('I', 'PRP'),<br/> ('think', 'VBP'),<br/> ('that', 'IN'),<br/> ('john', 'NNP'),<br/> ('stone', 'NN'),<br/> ('is', 'VBZ'),<br/> ('a', 'DT'),<br/> ('nice', 'JJ'),<br/> ('guy', 'NN'),<br/> ('.', '.'),<br/> ('there', 'EX'),<br/> ('is', 'VBZ'),<br/> ('a', 'DT'),<br/> ('stone', 'NN'),<br/> ('on', 'IN'),<br/> ('the', 'DT'),<br/> ('grass', 'NN'),<br/> ('.', '.'),<br/> ('i', 'PRP'),<br/> ("'m", 'VBP'),<br/> ('fat', 'JJ'),<br/> ('.', '.'),<br/> ('are', 'VBP'),<br/> ('you', 'PRP'),<br/> ('welcome', 'JJ'),<br/> ('and', 'CC'),<br/> ('smart', 'JJ'),<br/> ('in', 'IN'),<br/> ('london', 'NN'),<br/> ('?', '.'),<br/> ('is', 'VBZ'),<br/> ('this', 'DT'),<br/> ('martin', 'NNP'),<br/> ("'s", 'POS'),<br/> ('dog', 'NN'),<br/> ('?', '.')]<br/>print([(w.text, w.label_) for w in words.ents])<br/>[('john', 'PERSON'), ('martin', 'PERSON')]<br/>print([(w, w.ent_iob_, w.ent_type_) for w in words])<br/>[(I, 'O', ''), (think, 'O', ''), (that, 'O', ''), (john, 'B', 'PERSON'), (stone, 'O', ''), (is, 'O', ''), (a, 'O', ''), (nice, 'O', ''), (guy, 'O', ''), (., 'O', ''), (there, 'O', ''), (is, 'O', ''), (a, 'O', ''), (stone, 'O', ''), (on, 'O', ''), (the, 'O', ''), (grass, 'O', ''), (., 'O', ''), (i, 'O', ''), ('m, 'O', ''), (fat, 'O', ''), (., 'O', ''), (are, 'O', ''), (you, 'O', ''), (welcome, 'O', ''), (and, 'O', ''), (smart, 'O', ''), (in, 'O', ''), (london, 'O', ''), (?, 'O', ''), (is, 'O', ''), (this, 'O', ''), (martin, 'B', 'PERSON'), ('s, 'O', ''), (dog, 'O', ''), (?, 'O', '')]</span></pre><p id="49f9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们获得了比 NLTK 更详细的标记，但是我们仍然不能区分物体(<em class="mv">石头</em>和城市(<em class="mv">伦敦</em>)的 NN 标记。尽管约翰被 NER 处理器识别为一个人，但是斯帕西并不将单词<em class="mv">斯通</em>的第一个实例识别为一个人。</p><p id="c4bb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">基于统计的真实大小写</strong></p><p id="a392" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">推断正确大小写的任务经常会出现歧义:考虑句子<em class="mv">中的单词<em class="mv"> stone </em>他有一块石头</em>和<em class="mv">他与 stone </em>交谈。前者指一个物体，后者指一个人。在这篇<a class="ae lq" href="https://www.cs.cmu.edu/~llita/papers/lita.truecasing-acl2003.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中提出了一种统计方法来处理这种模糊性。下面我们改编了它的<a class="ae lq" href="https://github.com/nreimers/truecaser" rel="noopener ugc nofollow" target="_blank">实现</a>。</p><p id="603c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该模型需要从训练语料库中获得的关于单字、双字和三字的频率的信息。作者提供了这样一组预训练的频率，下面称为<em class="mv">distributions _ English . obj</em>。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="9663" class="ml mm it mh b gy mn mo l mp mq">import string<br/>import math<br/>import pickle</span><span id="c112" class="ml mm it mh b gy mr mo l mp mq">"""<br/>This file contains the functions to truecase a sentence.<br/>"""<br/><br/>def getScore(prevToken, possibleToken, nextToken, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist):<br/>    pseudoCount = 5.0<br/>    <br/>    #Get Unigram Score<br/>    nominator = uniDist[possibleToken]+pseudoCount    <br/>    denominator = 0    <br/>    for alternativeToken in wordCasingLookup[possibleToken.lower()]:<br/>        denominator += uniDist[alternativeToken]+pseudoCount<br/>        <br/>    unigramScore = nominator / denominator<br/>        <br/>        <br/>    #Get Backward Score  <br/>    bigramBackwardScore = 1<br/>    if prevToken != None:  <br/>        nominator = backwardBiDist[prevToken+'_'+possibleToken]+pseudoCount<br/>        denominator = 0    <br/>        for alternativeToken in wordCasingLookup[possibleToken.lower()]:<br/>            denominator += backwardBiDist[prevToken+'_'+alternativeToken]+pseudoCount<br/>            <br/>        bigramBackwardScore = nominator / denominator<br/>        <br/>    #Get Forward Score  <br/>    bigramForwardScore = 1<br/>    if nextToken != None:  <br/>        nextToken = nextToken.lower() #Ensure it is lower case<br/>        nominator = forwardBiDist[possibleToken+"_"+nextToken]+pseudoCount<br/>        denominator = 0    <br/>        for alternativeToken in wordCasingLookup[possibleToken.lower()]:<br/>            denominator += forwardBiDist[alternativeToken+"_"+nextToken]+pseudoCount<br/>            <br/>        bigramForwardScore = nominator / denominator<br/>        <br/>        <br/>    #Get Trigram Score  <br/>    trigramScore = 1<br/>    if prevToken != None and nextToken != None:  <br/>        nextToken = nextToken.lower() #Ensure it is lower case<br/>        nominator = trigramDist[prevToken+"_"+possibleToken+"_"+nextToken]+pseudoCount<br/>        denominator = 0    <br/>        for alternativeToken in wordCasingLookup[possibleToken.lower()]:<br/>            denominator += trigramDist[prevToken+"_"+alternativeToken+"_"+nextToken]+pseudoCount<br/>            <br/>        trigramScore = nominator / denominator<br/>        <br/>    result = math.log(unigramScore) + math.log(bigramBackwardScore) + math.log(bigramForwardScore) + math.log(trigramScore)<br/>  <br/>  <br/>    return result<br/><br/>def getTrueCase(tokens, outOfVocabularyTokenOption, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist):<br/>    """<br/>    Returns the true case for the passed tokens.<br/>    @param tokens: Tokens in a single sentence<br/>    @param outOfVocabulariyTokenOption:<br/>        title: Returns out of vocabulary (OOV) tokens in 'title' format<br/>        lower: Returns OOV tokens in lower case<br/>        as-is: Returns OOV tokens as is<br/>    """<br/>    tokensTrueCase = []<br/>    for tokenIdx in range(len(tokens)):<br/>        token = tokens[tokenIdx]<br/>        if token in string.punctuation or token.isdigit():<br/>            tokensTrueCase.append(token)<br/>        else:<br/>            if token in wordCasingLookup:<br/>                if len(wordCasingLookup[token]) == 1:<br/>                    tokensTrueCase.append(list(wordCasingLookup[token])[0])<br/>                else:<br/>                    prevToken = tokensTrueCase[tokenIdx-1] if tokenIdx &gt; 0  else None<br/>                    nextToken = tokens[tokenIdx+1] if tokenIdx &lt; len(tokens)-1 else None<br/>                    <br/>                    bestToken = None<br/>                    highestScore = float("-inf")<br/>                    <br/>                    for possibleToken in wordCasingLookup[token]:<br/>                        score = getScore(prevToken, possibleToken, nextToken, wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist)<br/>                           <br/>                        if score &gt; highestScore:<br/>                            bestToken = possibleToken<br/>                            highestScore = score<br/>                        <br/>                    tokensTrueCase.append(bestToken)<br/>                    <br/>                if tokenIdx == 0:<br/>                    tokensTrueCase[0] = tokensTrueCase[0].title();<br/>                    <br/>            else: #Token out of vocabulary<br/>                if outOfVocabularyTokenOption == 'title':<br/>                    tokensTrueCase.append(token.title())<br/>                elif outOfVocabularyTokenOption == 'lower':<br/>                    tokensTrueCase.append(token.lower())<br/>                else:<br/>                    tokensTrueCase.append(token) <br/>    <br/>    return tokensTrueCase</span><span id="b59e" class="ml mm it mh b gy mr mo l mp mq">f = open('english_distributions.obj', 'rb')<br/>uniDist = pickle.load(f)<br/>backwardBiDist = pickle.load(f)<br/>forwardBiDist = pickle.load(f)<br/>trigramDist = pickle.load(f)<br/>wordCasingLookup = pickle.load(f)<br/>f.close()</span><span id="2511" class="ml mm it mh b gy mr mo l mp mq">def truecasing_by_stats(input_text):<br/>    truecase_text = ''<br/>    sentences = sent_tokenize(input_text, language='english')<br/>    for s in sentences:<br/>        tokens = [token.lower() for token in nltk.word_tokenize(s)]<br/>        tokensTrueCase = getTrueCase(tokens, 'lower', wordCasingLookup, uniDist, backwardBiDist, forwardBiDist, trigramDist)<br/>        sTrueCase = re.sub(" (?=[\.,'!?:;])", "", ' '.join(tokensTrueCase))<br/>        truecase_text = truecase_text + sTrueCase + ' '<br/>    return truecase_text.strip()</span><span id="d42d" class="ml mm it mh b gy mr mo l mp mq">truecasing_by_stats(text)</span><span id="8149" class="ml mm it mh b gy mr mo l mp mq">"I think that John stone is a nice guy. There is a stone on the grass. I'm fat. Are you welcome and smart in London? Is this Martin's dog?"</span></pre><p id="6db0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这似乎是迄今为止取得的最好成绩。问题仍然是将石<em class="mv">一词的一审认定为人名，即应当大写。虽然像<em class="mv">雪</em>这样的著名名字被算法正确识别并大写，但是<em class="mv">石</em>并没有被推断为人名。单词<em class="mv"> stone </em>可能没有作为人名出现在该方法的作者所使用的训练语料库中。</em></p><p id="e8f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">带标准套管的真实套管</strong></p><p id="786c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://github.com/stanfordnlp/stanfordnlp" rel="noopener ugc nofollow" target="_blank"> StanfordNLP </a>是一个 Python 自然语言分析包，构建在 PyTorch 之上。由<a class="ae lq" href="https://stackoverflow.com/questions/49387699/extracting-the-person-names-in-the-named-entity-recognition-in-nlp-using-python" rel="noopener ugc nofollow" target="_blank">社区</a>推荐，特别用于识别与人名相关的实体。它是通向<a class="ae lq" href="https://stanfordnlp.github.io/CoreNLP/index.html" rel="noopener ugc nofollow" target="_blank"> Stanford CoreNLP </a>的桥梁，最初是用 Java 编写的，但是即使在 NLTK 中也有使用它的方法。</p><p id="6092" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果尚未安装，则需要以下软件包:</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="77e4" class="ml mm it mh b gy mn mo l mp mq">pip install stanfordnlp</span></pre><p id="09d2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，我们需要运行以下命令来下载神经管道的英文模型。这将带来 ca。1.96 克数据到我们的机器。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="3346" class="ml mm it mh b gy mn mo l mp mq">stanfordnlp.download('en')</span></pre><p id="2f8f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下代码使用 POS 处理器创建了一个管道，并在我们的文本上评估 StandfordNLP 模型。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="d0d3" class="ml mm it mh b gy mn mo l mp mq">&gt;&gt;&gt; stf_nlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos')<br/>&gt;&gt;&gt; doc = stf_nlp(text)<br/>&gt;&gt;&gt; print(*[f'word: {word.text+" "}\tupos: {word.upos}\txpos: {word.xpos}' for sent in doc.sentences for word in sent.words], sep='\n')</span><span id="8b8f" class="ml mm it mh b gy mr mo l mp mq">word: I         upos: PRON      xpos: PRP<br/>word: think     upos: VERB      xpos: VBP<br/>word: that      upos: SCONJ     xpos: IN<br/>word: john      upos: PROPN     xpos: NNP<br/>word: stone     upos: PROPN     xpos: NNP<br/>word: is        upos: AUX       xpos: VBZ<br/>word: a         upos: DET       xpos: DT<br/>word: nice      upos: ADJ       xpos: JJ<br/>word: guy       upos: NOUN      xpos: NN<br/>word: .         upos: PUNCT     xpos: .<br/>word: there     upos: PRON      xpos: EX<br/>word: is        upos: VERB      xpos: VBZ<br/>word: a         upos: DET       xpos: DT<br/>word: stone     upos: NOUN      xpos: NN<br/>word: on        upos: ADP       xpos: IN<br/>word: the       upos: DET       xpos: DT<br/>word: grass     upos: NOUN      xpos: NN<br/>word: .         upos: PUNCT     xpos: .<br/>word: i         upos: PRON      xpos: PRP<br/>word: 'm        upos: AUX       xpos: VBP<br/>word: fat       upos: ADJ       xpos: JJ<br/>word: .         upos: PUNCT     xpos: .<br/>word: are       upos: AUX       xpos: VBP<br/>word: you       upos: PRON      xpos: PRP<br/>word: welcome   upos: ADJ       xpos: JJ<br/>word: and       upos: CCONJ     xpos: CC<br/>word: smart     upos: ADJ       xpos: JJ<br/>word: in        upos: ADP       xpos: IN<br/>word: london    upos: PROPN     xpos: NNP<br/>word: ?         upos: PUNCT     xpos: .<br/>word: is        upos: AUX       xpos: VBZ<br/>word: this      upos: DET       xpos: DT<br/>word: martin    upos: PROPN     xpos: NNP<br/>word: 's        upos: PART      xpos: POS<br/>word: dog       upos: NOUN      xpos: NN<br/>word: ?         upos: PUNCT     xpos: .</span></pre><p id="e3d8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">用 StandfordNLP 得到的 POS 标签看起来棒极了！单词<em class="mv"> stone </em>的第一个实例现在被正确识别为人名，允许正确的大写，如下所示。</p><pre class="kj kk kl km gt mg mh mi mj aw mk bi"><span id="2bea" class="ml mm it mh b gy mn mo l mp mq">[w.text.capitalize() if w.upos in ["PROPN","NNS"] else w.text for sent in doc.sentences for w in sent.words]<br/>['I', 'think', 'that', 'John', 'Stone', 'is', 'a', 'nice', 'guy', '.', 'there', 'is', 'a', 'stone', 'on', 'the', 'grass', '.', 'i', "'m", 'fat', '.', 'are', 'you', 'welcome', 'and', 'smart', 'in', 'London', '?', 'is', 'this', 'Martin', "'s", 'dog', '?']</span></pre><p id="43e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">斯坦福 CoreNLP 也提供了一套强大的工具。它可以检测单词的基本形式(词条)、词性、公司名称、人名等。它还可以标准化日期、时间和数字量。它还被用来标记短语和句法依存关系，表示情感，以及获取人们所说的引语。StanfordNLP 只需几行代码就可以开始利用 CoreNLP 复杂的 API。想深入了解的，查看链接<a class="ae lq" href="https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/" rel="noopener ugc nofollow" target="_blank">的帖子这里</a>。</p><p id="228b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">结论</strong></p><p id="493a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这篇文章中，我们研究了没有案例信息的文本的案例恢复。使用的所有技术都是在单词级别使用 NLTK、spaCy 和 StandfordNLP 工具包进行操作。在链接<a class="ae lq" href="https://www.aclweb.org/anthology/D16-1225" rel="noopener ugc nofollow" target="_blank">这里</a>的文章中提出了一种使用字符级递归神经网络(RNN)的方法，供我们当中的一些英雄参考。</p></div></div>    
</body>
</html>