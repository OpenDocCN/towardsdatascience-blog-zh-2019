<html>
<head>
<title>Word Representation in Natural Language Processing Part III</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的词表示第三部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word-representation-in-natural-language-processing-part-iii-2e69346007f?source=collection_archive---------11-----------------------#2019-05-21">https://towardsdatascience.com/word-representation-in-natural-language-processing-part-iii-2e69346007f?source=collection_archive---------11-----------------------#2019-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/1acf69aaedc37a9d939c60ca57f8a769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfy18qhYh9cwxNctAyAiEg.jpeg"/></div></div></figure><div class=""/><p id="7b29" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我的博客系列关于单词表示的<a class="ae kw" rel="noopener" target="_blank" href="/word-representation-in-natural-language-processing-part-ii-1aee2094e08a"> <strong class="ka jc">第二部分</strong> </a> <strong class="ka jc"> </strong>中，我谈到了 Word2Vec 和 GloVe 等分布式单词表示。这些表示将单词的语义(意义)和相似性信息合并到嵌入中。然而，它们不能推广到不属于训练集的“词汇之外”的单词(OOV)。在这一部分，我将描述减轻这个问题的模型。具体说一下最近提出的两个模型:ELMo 和 FastText。ELMo 和 FastText 背后的想法是利用单词的字符和形态结构。与其他模型不同，ELMo 和 FastText 不将单词视为一个原子单位，而是其字符组合的联合，例如"<em class="kx">remaking-&gt;re+make+ing "</em></p><h2 id="e99f" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">以前模型的问题</h2><p id="8c34" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">诸如 Word2Vec 和 GloVe 的先前模型从字典中查找预训练的单词嵌入，因此没有考虑在特定上下文中使用的词义。换句话说，一词多义(单词的多重含义)没有被考虑在内。例如:</p><blockquote class="lw lx ly"><p id="36e1" class="jy jz kx ka b kb kc kd ke kf kg kh ki lz kk kl km ma ko kp kq mb ks kt ku kv ij bi translated">"我的朋友正在考虑向银行贷款."</p><p id="89ef" class="jy jz kx ka b kb kc kd ke kf kg kh ki lz kk kl km ma ko kp kq mb ks kt ku kv ij bi translated">"降雨导致莱茵河决堤. "</p></blockquote><p id="18e5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果使用 Word2Vec 或 GloVe，那么单词<em class="kx">“bank”</em>将只有一次嵌入。但在上面两句话里有不同的内涵。在第一句话中，它的意思是为客户提供金融服务的机构。在第二句中，它暗示了水体旁边的斜坡。</p><h2 id="1f07" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">ELMo:来自语言模型的嵌入</h2><p id="8c00" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">新模型 ELMo 能够通过将整个序列视为输入来解决以前模型的这个问题。它根据参照系动态地产生单词嵌入。该模型由三层组成:(1)卷积神经网络，(2)双向长短期记忆(LSTM)和嵌入。</p><figure class="md me mf mg gt is gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/cb0da1b75e8c6a28696e3f64a61cecdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*BLf5_2gRWECvkv7P5xNIWg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk"><strong class="bd ml">Figure 1</strong></figcaption></figure><p id="fca6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">ELMo 的模型输入(即 CNN 的输入)完全基于字符。所以最初，我们给 CNN 提供原始字符。然后 CNN 产生紧凑的嵌入，该嵌入被传递给双向 LSTMs。</p><p id="97f7" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">b)双向 LSTM 层指示模型以正序和逆序在输入序列上运行。例如，让我们在下面的输入中获得单词“<em class="kx">后跟</em>的嵌入:</p><p id="86c9" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">input = "多云的上午<strong class="ka jc">紧接着</strong>是一个阳光明媚的下午。"</p><p id="ef8d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">context_straight= ['a '，'多云'，'早晨']</p><p id="6fbc" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">context_reverse = ['by '，' sunny '，' a '，'大部分'，' sunny '，'午后']</p><p id="6564" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，对于每个目标单词，模型可以观察它周围的前面和后面的单词。从图 1 中可以看出，堆叠的 LSTM 构成了多层 LSTM。每个 LSTM 都将前一个的输出序列作为输入，除了第一个 LSTM 层从 CNN 获得字符嵌入。</p><p id="a22e" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">c)嵌入层连接 LSTM 方向的隐藏状态，并产生依赖于上下文的嵌入。在论文中，作者将其定义为隐藏状态乘以特定任务模型权重的线性组合。它为模型正在使用的每个任务学习单独的 ELMo 表示。因此，ELMo 提高了许多 NLP 任务的性能。为了简单起见，我省略了细节。这里可以找到<a class="ae kw" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"><strong class="ka jc"/></a>。</p><h2 id="b333" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">使用 Tensorflow-hub 的 EMLo</h2><p id="1a19" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">依赖关系:</p><ul class=""><li id="51e6" class="mm mn jb ka b kb kc kf kg kj mo kn mp kr mq kv mr ms mt mu bi translated">Python 3.6</li><li id="7661" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">张量流 1.13.1</li><li id="327e" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">张量流-集线器 0.4.0</li></ul><p id="8915" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以使用以下方式安装库:</p><blockquote class="lw lx ly"><p id="22ae" class="jy jz kx ka b kb kc kd ke kf kg kh ki lz kk kl km ma ko kp kq mb ks kt ku kv ij bi translated">pip 安装张量流==1.13</p></blockquote><p id="f023" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从 ELMo 模块加载预训练的单词嵌入:</p><figure class="md me mf mg gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="e5c2" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">参数<code class="fe nc nd ne nf b">trainable = False</code>因为我只想加载预先训练好的重量。但是图的参数不一定是固定的。通过将其设置为<code class="fe nc nd ne nf b">True</code>，可以重新训练模型并更新参数。</p><p id="2f18" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从预先训练的 ELMo 获得单词嵌入:</p><figure class="md me mf mg gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="a67d" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">默认输出大小为 1024，让我们在上面两句话中寻找单词<em class="kx">“bank”</em>的前 10 个维度:</p><figure class="md me mf mg gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="5c1b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">正如我们注意到的，在不同的上下文中嵌入同一个单词是不同的。如前所述，ELMo 通过输入一系列字符来动态构建单词嵌入。它依赖于句子中当前周围的单词。</p><h2 id="a1ee" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">FastText:子字模型</h2><p id="e704" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">以前的模型的问题，他们不能为 OOV 生成单词嵌入(见上文)。这个问题不仅由 ELMo 解决，而且在子字模型中也解决了。此外，与 ELMo 相反，子词模型能够利用形态学信息。在子词模型中，具有相同词根的词共享参数。它被集成为 FastText 库的一部分，这就是它被称为 FastText 的原因。</p><p id="fba8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">子词模型是跳格模型(Word2Vec)的扩展，它产生给定一个词的上下文的概率。模型损耗定义如下:</p><figure class="md me mf mg gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1c92853b1dbcb2041ab045b112a97795.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*F6PVYH1vx2f17_3PE3X4Xg.png"/></div></figure><p id="b173" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">损失函数的第一部分将所有上下文单词<em class="kx"> w_c </em>视为正例，第二部分随机采样<em class="kx"> N_t，c </em>作为位置<em class="kx"> t. </em>处的<em class="kx">T5】作为反例。一方面，目标是<em class="kx"> </em>将共现和相似的单词放置在彼此靠近的位置。另一方面，它旨在定位向量空间中彼此远离的不同单词。子词模型以类似的方式训练，除了它将计算的 n 元语法添加到特征。n 元语法定义为给定数量的项目序列。例如，n=2 的 n 元模型将给出单词“banking”的以下输出:{ <em class="kx"> ba，an，nk，ki，in，ng </em> }。</em></p><p id="6143" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面是另一个例子:</p><blockquote class="lw lx ly"><p id="ef5b" class="jy jz kx ka b kb kc kd ke kf kg kh ki lz kk kl km ma ko kp kq mb ks kt ku kv ij bi translated">“<strong class="ka jc">河</strong>岸”</p></blockquote><p id="ea2b" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在跳格模型中，单词"<em class="kx"> river </em>"有两个上下文标记:<em class="kx"> the </em> and <em class="kx"> bank。</em>而 n-gram(n = 2)的子词模型有 13 个上下文标记:<em class="kx"> th，eh，e_，_a，an，nd，d_，_b，ba，an，nk，the and bank。</em>在实际中，我们提取 3≤ n ≤ 6 的所有 n 元文法。这是一种非常简单的方法，可以考虑不同组的 n 元语法，例如，取一个单词的所有前缀和后缀。</p><p id="b9f6" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种 n 元语法信息用子词信息丰富了词向量，并使模型能够为看不见的词构建向量。这对于形态丰富的语言和包含大量生僻字的数据集非常有益。德语就是一个很好的例子，因为它有丰富的复合名词。“Tischtennis”这个词翻译过来就是乒乓球。这个单词的嵌入将通过简单的加法来构建，例如 Tisch + Tennis → Tischtennis。</p><h2 id="55fe" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">使用 Gensim 的快速文本</h2><p id="d5f8" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">依赖关系:</p><ul class=""><li id="9787" class="mm mn jb ka b kb kc kf kg kj mo kn mp kr mq kv mr ms mt mu bi translated">Python 3.6</li><li id="b3dc" class="mm mn jb ka b kb mv kf mw kj mx kn my kr mz kv mr ms mt mu bi translated">Gensim 3.7.2</li></ul><p id="27e5" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">可以使用以下方式安装库:</p><blockquote class="lw lx ly"><p id="8857" class="jy jz kx ka b kb kc kd ke kf kg kh ki lz kk kl km ma ko kp kq mb ks kt ku kv ij bi translated">pip 安装张量流==1.13</p></blockquote><p id="577f" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">使用自定义数据创建模型</p><figure class="md me mf mg gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="23c0" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们得到一个不属于训练的单词的嵌入:</p><figure class="md me mf mg gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="00b8" class="pw-post-body-paragraph jy jz jb ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">有 157 种语言使用 wiki 数据的预训练模型，这里 可以找到<a class="ae kw" href="https://fasttext.cc/docs/en/pretrained-vectors.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ka jc">。也可以使用 gensim 加载它们。</strong></a></p><figure class="md me mf mg gt is"><div class="bz fp l di"><div class="na nb l"/></div></figure><h2 id="a9cb" class="ky kz jb bd la lb lc dn ld le lf dp lg kj lh li lj kn lk ll lm kr ln lo lp lq bi translated">拿走</h2><p id="ef2c" class="pw-post-body-paragraph jy jz jb ka b kb lr kd ke kf ls kh ki kj lt kl km kn lu kp kq kr lv kt ku kv ij bi translated">ELMO 和子词是高级模型，能够为词汇表中存在和不存在的词产生高质量的嵌入。特别地，ELMo 能够在产生单词嵌入时考虑上下文信息。与其他现有模型相比，它具有更高的矢量质量。但是因为它在运行时进行预测，所以它有推理成本。另一方面，子字非常快，能够有效地合并 n 元语法。</p></div></div>    
</body>
</html>