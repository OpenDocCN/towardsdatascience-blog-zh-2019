<html>
<head>
<title>Understanding the Spark insertInto function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解 Spark 插入功能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-the-spark-insertinto-function-1870175c3ee9?source=collection_archive---------4-----------------------#2019-10-22">https://towardsdatascience.com/understanding-the-spark-insertinto-function-1870175c3ee9?source=collection_archive---------4-----------------------#2019-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d9c14b848ec95138aef557f598d5e857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ajdBQlPsL1fWtd6NQ88x1w.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@marcusloke" rel="noopener ugc nofollow" target="_blank">@marcusloke</a> on <a class="ae kf" href="https://unsplash.com/photos/oyXis2kALVg" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="87d4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 spark 将原始数据吸收到数据湖中是目前常用的 ETL 方法。在某些情况下，原始数据被清理、序列化并作为分析团队用来执行 SQL 类操作的配置单元表公开。因此，spark 为表的创建提供了两种选择:<strong class="ki iu"> <em class="le">托管</em> </strong>和<strong class="ki iu"> <em class="le">外部</em> </strong>表。这两者的区别在于，与 spark 控制存储和元数据的管理表不同，在外部表上，spark 不控制数据位置，只管理元数据。</p><p id="ebcc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，通常需要一种重试策略来覆盖一些失败的分区。例如，分区<strong class="ki iu"> 22/10/2019 </strong>的批处理作业(时间戳分区)失败，我们需要重新运行作业，写入正确的数据。因此有两种选择:<strong class="ki iu"> a) </strong>重新生成并覆盖所有数据，或者<strong class="ki iu"> b) </strong>处理并覆盖所需分区的数据。由于性能问题，第二种方法被放弃了，想象一下，您必须处理整整一个月的数据。</p><p id="ee91" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，使用了选项 first 选项，幸运的是 spark 有选项<strong class="ki iu">dynamic</strong><strong class="ki iu">partitionOverwriteMode</strong>，即<strong class="ki iu"> </strong>仅对当前批处理中存在的分区覆盖数据<strong class="ki iu"> </strong>。当将数据写入外部数据存储时，如<strong class="ki iu"> HDFS </strong>或<strong class="ki iu"> S3，这个选项非常有效；</strong>的情况下，可以通过一个简单的<strong class="ki iu">重载外部表元数据，创建外部表</strong>命令<strong class="ki iu">。</strong></p><p id="2e7e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，对于存储在具有动态分区的元存储中的配置单元表，为了保持数据质量和一致性，我们需要理解一些行为。首先，即使 spark 提供了两个函数来存储表中的数据<strong class="ki iu"> saveAsTable </strong>和<strong class="ki iu"> insertInto，</strong>它们之间也有一个重要的区别:</p><ul class=""><li id="a8b7" class="lf lg it ki b kj kk kn ko kr lh kv li kz lj ld lk ll lm ln bi translated"><strong class="ki iu"> SaveAsTable: </strong>创建表格结构并存储数据的第一个版本。然而，覆盖保存模式<strong class="ki iu">适用于所有</strong>分区，即使配置了动态。</li><li id="4ca7" class="lf lg it ki b kj lo kn lp kr lq kv lr kz ls ld lk ll lm ln bi translated"><strong class="ki iu"> insertInto: </strong>不创建表结构，但是，当配置了动态时，覆盖保存模式<strong class="ki iu">只对</strong>需要的分区起作用。</li></ul><p id="0f8b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，可以使用 SaveAsTable 从原始数据帧定义创建表，然后在创建表之后，使用 insertInto 函数以简单的方式进行覆盖。尽管如此，insertInto 在写入分区数据时表现出一些没有很好记录的行为，在处理包含模式更改的数据时也存在一些挑战。</p><h1 id="6b19" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">列的顺序问题</h1><p id="20e2" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">让我们编写一个简单的单元测试，从数据帧中创建一个表。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="9c73" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu"><em class="le">it </em>should</strong> "Store table and insert into new record on new partitions" in {<br/>  val spark = ss<br/>  import spark.implicits._<br/>  <strong class="nb iu">val targetTable</strong> = "companies_table"</span><span id="f1dd" class="nf lu it nb b gy nk nh l ni nj">  <strong class="nb iu">val companiesDF</strong> = <em class="le">Seq</em>(("A", "Company1"), ("B", "Company2")).toDF("id", "company")<br/>     companiesDF.write.mode(SaveMode.<em class="le">Overwrite</em>).<strong class="nb iu">partitionBy</strong>("id").<strong class="nb iu">saveAsTable</strong>(targetTable)<br/><br/>  <strong class="nb iu">val companiesHiveDF</strong> = ss.sql(s"SELECT * FROM <strong class="nb iu">$</strong>{targetTable}")</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/5ff1c1f6e8b570fa1bf49ecef0feac1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w7FMQjSeoIe82NfVly9j9w.png"/></div></div></figure><p id="d5e9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，该表创建正确。然后，让我们使用 insertInto 覆盖一些数据，并执行一些断言。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="6045" class="nf lu it nb b gy ng nh l ni nj"> <strong class="nb iu">val secondCompaniesDF </strong>= <em class="le">Seq</em>(("C", "Company3"), ("D", "Company4"))<br/>    .toDF("id", "company")<br/><br/>secondCompaniesDF.write.mode(SaveMode.<em class="le">Append</em>).<strong class="nb iu">insertInto</strong>(targetTable)</span><span id="ce81" class="nf lu it nb b gy nk nh l ni nj">  <strong class="nb iu">val companiesHiveAfterInsertDF</strong> = ss.sql(s"SELECT * FROM <strong class="nb iu">$</strong>{targetTable}")<br/><br/>  companiesDF.count() should equal(2)<br/>  companiesHiveAfterInsertDF.count() should equal(4)<br/>  companiesHiveDF.select("id").collect().map(_.get(0)) should <em class="le">contain </em><strong class="nb iu">allOf("A", "B")</strong><br/>  companiesHiveAfterInsertDF.select("id").collect() should <em class="le">contain </em><strong class="nb iu">allOf("A", "B", "C", "D")</strong><br/><br/>}</span></pre><p id="1b27" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这应该可以正常工作。但是，请看下面的数据打印:</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/6edfae4a5892345a2475a946c82c5b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PnYhSg5Mi2B3KClCK4zPsQ.png"/></div></figure><p id="72b1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，由于列的位置，断言失败了。原因有二:<strong class="ki iu"> a) </strong> <em class="le"> saveAsTable </em>使用分区列，并在末尾添加。<strong class="ki iu"> b) </strong> <em class="le"> insertInto </em>使用列的顺序(就像调用 SQL insertInto 一样)而不是列名。因此，在末尾添加分区列可以解决这个问题，如下所示:</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="a442" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu"> //partition column should be at the end to match table schema.</strong><br/>  <strong class="nb iu">val secondCompaniesDF</strong> = <em class="le">Seq</em>(("Company3", "C"), ("Company4", "D"))<br/>    .toDF("company", <strong class="nb iu">"id"</strong>)<br/><br/>  secondCompaniesDF.write.mode(SaveMode.<em class="le">Append</em>).<strong class="nb iu">insertInto</strong>(targetTable)</span><span id="332e" class="nf lu it nb b gy nk nh l ni nj">  <strong class="nb iu">val companiesHiveAfterInsertDF</strong> = ss.sql(s"SELECT * FROM <strong class="nb iu">$</strong>{targetTable}")</span><span id="71eb" class="nf lu it nb b gy nk nh l ni nj">  companiesHiveAfterInsertDF.printSchema()<br/>  companiesHiveAfterInsertDF.show(false)<br/><br/>  companiesDF.count() should equal(2)<br/>  companiesHiveAfterInsertDF.count() should equal(4)<br/>  companiesHiveDF.select("id").collect().map(_.get(0)) <strong class="nb iu">should</strong> <em class="le">contain </em><strong class="nb iu">allOf("A", "B")</strong><br/>  companiesHiveAfterInsertDF.select("id").collect().map(_.get(0)) <strong class="nb iu">should</strong> <em class="le">contain </em><strong class="nb iu">allOf("A", "B", "C", "D")</strong><br/><br/>}</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a4a48f6a9bf99c3fe3426b8b69835f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*lVttmEGDpGwgVhuaLhW8mA.png"/></div></figure><p id="18ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在测试通过了，数据被正确地覆盖了。</p><h1 id="44ce" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated"><strong class="ak">匹配表模式</strong></h1><p id="1375" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">如前所述，列的顺序对于<em class="le"> insertInto </em>函数很重要。此外，假设您正在接收模式不断变化的数据，并且您收到了一批具有不同列数的新数据。</p><h2 id="8e9d" class="nf lu it bd lv no np dn lz nq nr dp md kr ns nt mh kv nu nv ml kz nw nx mp ny bi translated">带有额外列的新批次</h2><p id="f568" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">让我们首先测试添加更多列的情况。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="6343" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu">//again adding the partition column at the end and trying to overwrite partition C.</strong><br/><strong class="nb iu">val thirdCompaniesDF</strong> = <em class="le">Seq</em>(("Company4", 10, "C"), ("Company5", 20,  "F"))<br/>  .toDF("company", "size", "id")<br/><br/>thirdCompaniesDF.write.mode(<strong class="nb iu">SaveMode.<em class="le">Overwrite</em></strong>).<strong class="nb iu">insertInto</strong>(targetTable)</span></pre><p id="671e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尝试调用<em class="le"> insertInto </em>时，显示以下错误:</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nz"><img src="../Images/5270c6b69d5b7b499d0c275ba4dc0da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EmrsmOFeN79qoeYNQ93x0g.png"/></div></div></figure><p id="ab49" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，需要一个返回表中缺失列的函数:</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="fdf6" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu">def</strong> <strong class="nb iu">getMissingTableColumnsAgainstDataFrameSchema</strong>(<strong class="nb iu">df</strong>: DataFrame, <strong class="nb iu">tableDF</strong>: DataFrame): Set[String] = {<br/>  <strong class="nb iu">val dfSchema</strong> = df.schema.fields.map(v =&gt; (v.name, v.dataType)).toMap<br/>  <strong class="nb iu">val tableSchema</strong> = tableDF.schema.fields.map(v =&gt; (v.name, v.dataType)).toMap<br/>  <strong class="nb iu">val columnsMissingInTable</strong> = dfSchema.keys.toSet.diff(tableSchema.keys.toSet).map(x =&gt; x.concat(s" <strong class="nb iu">$</strong>{dfSchema.get(x).get.sql}"))<br/><br/>  <strong class="nb iu">columnsMissingInTable</strong><br/>}</span></pre><p id="48d5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，执行<strong class="ki iu"> SQL ALTER TABLE </strong>命令。此后，<em class="le"> insertInto </em>函数正常工作，表模式被合并，如下所示:</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="7263" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu">val tableFlatDF</strong> = ss.sql(s"SELECT * FROM <strong class="nb iu">$</strong>targetTable limit 1")<br/><br/><strong class="nb iu">val columnsMissingInTable = </strong>DataFrameSchemaUtils.<em class="le">getMissingTableColumnsAgainstDataFrameSchema</em>(thirdCompaniesDF, tableFlatDF)<br/><br/>if (columnsMissingInTable.size &gt; 0) {<br/>  ss.sql((s"ALTER TABLE <strong class="nb iu">$</strong>targetTable " +<br/>    s"ADD COLUMNS (<strong class="nb iu">$</strong>{columnsMissingInTable.mkString(" , ")})"))<br/>}<br/><br/><strong class="nb iu">thirdCompaniesDF</strong>.write.mode(SaveMode.<em class="le">Overwrite</em>).insertInto(targetTable)<br/><br/><strong class="nb iu">val companiesHiveAfterInsertNewSchemaDF</strong> = ss.sql(s"SELECT * FROM <strong class="nb iu">$</strong>targetTable")<br/><br/><strong class="nb iu">companiesHiveAfterInsertNewSchemaDF</strong>.printSchema()<br/><strong class="nb iu">companiesHiveAfterInsertNewSchemaDF</strong>.show(false)</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/dacdc2efb085ed24923a1d6b123b8d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*vKSEAAp-ZKHba-ODzlkQpg.png"/></div></figure><h2 id="9387" class="nf lu it bd lv no np dn lz nq nr dp md kr ns nt mh kv nu nv ml kz nw nx mp ny bi translated">列数更少的新批次</h2><p id="5ada" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">现在让我们测试接收到较少列的情况。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="ccd3" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu">val fourthCompaniesDF</strong> = <em class="le">Seq</em>("G", "H")<br/>  .toDF("id")<br/><br/>fourthCompaniesDF.write.mode(SaveMode.<em class="le">Overwrite</em>).<strong class="nb iu">insertInto</strong>(targetTable)</span></pre><p id="663d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将显示以下错误:</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/3a548129853efcab1123a28b953cf72d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VvhuFtMOO4DSK9gSXOXIaQ.png"/></div></div></figure><p id="35fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，需要一个向数据框添加缺失列的函数:</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="f27d" class="nf lu it nb b gy ng nh l ni nj"><strong class="nb iu">def mergeDataFrameSchemaAgainstTable</strong>(<strong class="nb iu">tableDF</strong>: DataFrame)(<strong class="nb iu">df</strong>: DataFrame): DataFrame = {<br/>  <strong class="nb iu">val dfSchema</strong> = df.schema.fields.map(v =&gt; (v.name, v.dataType)).toMap<br/>  <strong class="nb iu">val tableSchema</strong> = tableDF.schema.fields.map(v =&gt; (v.name, v.dataType)).toMap<br/><br/>  <strong class="nb iu">val columnMissingInDF</strong> = tableSchema.keys.toSet.diff(dfSchema.keys.toSet).toList<br/><br/>  <strong class="nb iu">val mergedDFWithNewColumns</strong> = columnMissingInDF.foldLeft(df) { (currentDF, colName) =&gt;<br/>    currentDF.<strong class="nb iu">withColumn</strong>(<br/>      colName,<br/>      <em class="le">lit</em>(null).cast(tableSchema.get(colName).get.typeName)<br/>    )<br/>  }<br/><br/>  <strong class="nb iu">mergedDFWithNewColumns</strong><br/>}</span></pre><p id="2b2f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，合并的数据框被写入并正常工作，如下所示:</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="4167" class="nf lu it nb b gy ng nh l ni nj">val <strong class="nb iu">mergedFlatDF</strong> = <strong class="nb iu">fourthCompaniesDF</strong>.transform(DataFrameSchemaUtils.<strong class="nb iu"><em class="le">mergeDataFrameSchemaAgainstTable</em></strong>(companiesHiveDF))<br/>mergedFlatDF.write.mode(SaveMode.<em class="le">Overwrite</em>).<strong class="nb iu">insertInto</strong>(targetTable)</span><span id="a920" class="nf lu it nb b gy nk nh l ni nj">mergedFlatDF.printSchema()<br/>mergedFlatDF.show(false)</span></pre><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/27f9ad040111d8fec5515bfd972c9fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*Ud7WQhvvwM8LPDBExzfiSw.png"/></div></figure><h1 id="7e09" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated"><strong class="ak">结论</strong></h1><p id="97c2" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">Spark 提供了多种功能来集成我们的数据管道和 Hive。然而，需要很好地理解它们是如何工作的，以避免在写数据时出错。具体来说，在使用动态分区时，<em class="le"> insertInto </em>函数有两个应该考虑的重要特性:</p><ol class=""><li id="2409" class="lf lg it ki b kj kk kn ko kr lh kv li kz lj ld od ll lm ln bi translated">分区列应该总是在末尾，以匹配配置单元表模式定义。</li><li id="36bf" class="lf lg it ki b kj lo kn lp kr lq kv lr kz ls ld od ll lm ln bi translated"><em class="le"> InsertInto </em>使用列的顺序而不是名称。因此，您应该保证始终具有相同的列数，并保持它们相同的插入顺序。</li></ol></div></div>    
</body>
</html>