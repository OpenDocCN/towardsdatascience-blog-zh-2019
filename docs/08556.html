<html>
<head>
<title>Web Scraping of 10 Online Shops in 30 Minutes with Python and Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 和 Scrapy 在 30 分钟内抓取 10 家在线商店的网页</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/web-scraping-of-10-online-shops-in-30-minutes-with-python-and-scrapy-a7f66e42446d?source=collection_archive---------7-----------------------#2019-11-19">https://towardsdatascience.com/web-scraping-of-10-online-shops-in-30-minutes-with-python-and-scrapy-a7f66e42446d?source=collection_archive---------7-----------------------#2019-11-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="f37a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">获取启动应用程序项目所需的源数据</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/94d7d3c156814218e8de82bc43d6c9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rRDtMiQqXf72oTeI"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@new_data_services?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">NEW DATA SERVICES</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ul class=""><li id="4947" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated">你是一个全栈开发者</li><li id="49b2" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">你想开发一个奇妙的网络应用程序</li><li id="a9f6" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated">你充满动力，全身心地投入到你的项目中</li></ul><p id="719c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">即使您勾选了上面的框，在编写一行代码之前，您仍然需要一个领域相关的数据集。这是因为现代应用程序同时或成批处理大量数据，为用户提供价值。</p><p id="ecc3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我将解释我生成这样一个数据集的工作流程。你将会看到我是如何在没有任何人工干预的情况下处理许多网站的自动抓取的。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="6039" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我的目标是为一个<strong class="js iu">价格比较 WebApp </strong>生成一个数据集。我将使用的产品类别是手提袋。对于这样的应用程序，应该每天从不同的在线卖家那里收集手袋的产品和价格信息。虽然有些卖家提供了一个 API 供你访问所需的信息，但并不是所有的卖家都遵循同样的途径。所以，web 报废是不可避免的！</p><p id="548c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在整个例子中，我将使用<strong class="js iu"> Python </strong>和<strong class="js iu"> Scrapy </strong>为 10 个不同的卖家生成网络蜘蛛。然后，我将使用<strong class="js iu"> Apache Airflow </strong>自动执行这个过程，这样就不需要人工干预来定期执行整个过程。</p><h1 id="6fb3" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">源代码和现场演示 Web 应用程序</h1><p id="e91d" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">你可以在我的<a class="ae le" href="https://github.com/eisbilen/FashionSearch" rel="noopener ugc nofollow" target="_blank"> GitHub 资源库</a>中找到所有相关的源代码。你也可以<a class="ae le" href="https://fashionsearch-2cab5.web.app/" rel="noopener ugc nofollow" target="_blank">访问实时网络应用</a>，它使用了这个网络废弃项目提供的数据。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nd"><img src="../Images/9e73654e6369111de58439972afc1631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ssYG6TEftoc_NPi1"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@waldemarbrandt67w?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Waldemar Brandt</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="6096" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">我的网页抓取工作流程</h1><p id="384e" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">在开始任何网页抓取项目之前，我们必须定义哪些网站将被纳入该项目。我决定覆盖土耳其手提袋类的 10 个访问量最大的网上商店。你可以在我的 GitHub 库中看到它们<a class="ae le" href="https://github.com/eisbilen/FashionSearch" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="9006" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">步骤 1:安装 Scrapy 并设置项目文件夹</h1><p id="4440" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">你必须将 Scrapy 安装到你的电脑中，并在创建 Scrapy 蜘蛛之前生成一个 Scrapy 项目。请看看下面的帖子了解更多信息。</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/fuel-up-the-deep-learning-custom-dataset-creation-with-web-scraping-ba0f44414cf7"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">为深度学习加油:通过网络抓取创建自定义数据集</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">使用 Scrapy 和 Python 创建深度学习数据集。</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ky nh"/></div></div></a></div><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="cf42" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu">#install the scrapy</strong><br/>$ pip install scrapy</span><span id="d799" class="ob mb it nx b gy og od l oe of"><strong class="nx iu">#install the image for downloading the product images</strong><br/>$ pip install image</span><span id="0e47" class="ob mb it nx b gy og od l oe of"><strong class="nx iu">#start web scraping project with scraps</strong><br/>$ scrapy startproject <strong class="nx iu">fashionWebScraping</strong><br/>$ cd <strong class="nx iu">fashionWebScraping</strong><br/>$ ls</span><span id="adc2" class="ob mb it nx b gy og od l oe of"><strong class="nx iu">#create project folders which are explained below</strong><br/>$ mkdir csvFiles<br/>$ mkdir images_scraped<br/>$ mkdir jsonFiles<br/>$ mkdir utilityScripts</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="0d1b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">项目文件夹和文件</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/5c0e655f7c4018fa65aadbc58c8184ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*7hqZ6dHz_RBuBB0fL7W87w.jpeg"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">The folder structure of the project</figcaption></figure><p id="3820" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我在本地计算机上创建了一个文件夹结构，将项目文件整齐地放入不同的文件夹中。</p><p id="4f17" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">【CSV files】</strong>文件夹包含每个网站抓取的 CSV 文件。蜘蛛将从这些 CSV 文件中读取“起始 URL”来启动抓取，因为我不想在蜘蛛中硬编码它们。</p><p id="5731" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">‘fashion web scraping’</strong>文件夹包含了 Scrapy 蜘蛛和助手脚本，如<strong class="js iu">‘settings . py’</strong>，<strong class="js iu">‘item . py’</strong>，<strong class="js iu">‘pipelines . py’</strong>。我们必须修改一些零碎的助手脚本来成功地执行抓取过程。</p><p id="ab53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">刮下的产品图片将保存在<strong class="js iu">‘images _ scraped’</strong>文件夹中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/cb4a4eaea09db65e7ccfd60a63b22bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*4wui6Dyct6W7PI3Vh_OzWQ.jpeg"/></div></figure><p id="970d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在抓取网页的过程中，所有的产品信息，如价格、名称、产品链接和图片链接都将存储在<strong class="js iu">‘JSON files’</strong>文件夹中的 JSON 文件中。</p><p id="29c4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将有实用程序脚本来执行一些任务，如:</p><ul class=""><li id="720f" class="lf lg it js b jt ju jx jy kb lh kf li kj lj kn lk ll lm ln bi translated"><strong class="js iu"> 'deldub.py' </strong>检测并删除报废结束后 JSON 文件中重复的产品信息。</li><li id="d5f0" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu"> 'jsonPrep.py' </strong>是另一个实用程序脚本，用于在报废结束后检测并删除 JSON 文件中的空行项目。</li><li id="3b5c" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu"> 'deleteFiles.py' </strong>删除前一次报废时生成的所有 JSON 文件。</li><li id="9ae5" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu">' JSON tos . py '</strong>通过读取 JSON 文件，在远程位置填充 ElasticSearch 集群。这是提供实时全文搜索体验所必需的。</li><li id="ee06" class="lf lg it js b jt lo jx lp kb lq kf lr kj ls kn lk ll lm ln bi translated"><strong class="js iu"> 'sitemap_gen.py' </strong>用于生成涵盖所有产品链接的站点地图。</li></ul></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="1123" class="ma mb it bd mc md oj mf mg mh ok mj mk ml ol mn mo mp om mr ms mt on mv mw mx bi translated">步骤 2:了解特定网站的 URL 结构，并填充 CSV 文件以启动 URL</h1><p id="f811" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">创建项目文件夹后，下一步是用我们想要抓取的每个网站的起始 URL 填充 CSV 文件。</p><p id="710b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">几乎每个电子商务网站都提供分页功能，让用户浏览产品列表。每次导航到下一页时，URL 中的 page 参数都会增加。请参见下面的示例 URL，其中使用了“page”参数。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="862a" class="ob mb it nx b gy oc od l oe of"><a class="ae le" href="https://www.derimod.com.tr/kadin-canta-aksesuar/?p=1" rel="noopener ugc nofollow" target="_blank">https://www.derimod.com.tr/kadin-canta-aksesuar/?page=1</a></span></pre><p id="a8c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我将使用<code class="fe oo op oq nx b">{}</code>占位符，这样我们可以通过增加“page”的值来迭代 URL。我还将使用 CSV 文件中的“性别”列来定义特定 URL 的性别类别。</p><p id="3102" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，最终的 CSV 文件将如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/45e196d0822ed3d69268329f7595f65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*pTA8kKfOBgSzKtxyRm033g.jpeg"/></div></figure><p id="9d0b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样的原则也适用于项目中的其他网站。你可以在我的 GitHub 库中找到填充的 CSV 文件。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="a0fd" class="ma mb it bd mc md oj mf mg mh ok mj mk ml ol mn mo mp om mr ms mt on mv mw mx bi translated">步骤 3:修改“项目. py”和“设置. py”</h1><p id="8b77" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">要开始抓取，我们必须修改“items.py”来定义用于存储抓取数据的“item objects”。</p><blockquote class="os ot ou"><p id="15a9" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated">为了定义通用的输出数据格式，Scrapy 提供了<code class="fe oo op oq nx b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/items.html#scrapy.item.Item" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">Item</em></strong></a></code>类。<code class="fe oo op oq nx b"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/items.html#scrapy.item.Item" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">Item</em></strong></a></code>对象是用来收集抓取数据的简单容器。它们提供了一个类似于<a class="ae le" href="https://docs.python.org/2/library/stdtypes.html#dict" rel="noopener ugc nofollow" target="_blank">字典的</a> API，用一种方便的语法来声明它们的可用字段。</p><p id="7fa7" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated">【scrapy.org】<em class="it">出自</em><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/items.html" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it"/></strong></a></p></blockquote><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="6b11" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu">#items.py in fashionWebScraping folder</strong><br/>import scrapy<br/>from scrapy.item import Item, Field</span><span id="3fea" class="ob mb it nx b gy og od l oe of">class FashionwebscrapingItem(scrapy.Item):<br/> <br/> <strong class="nx iu">#product related items, such as id,name,price</strong><br/> gender=Field()<br/> productId=Field()<br/> productName=Field()<br/> priceOriginal=Field()<br/> priceSale=Field()</span><span id="68d8" class="ob mb it nx b gy og od l oe of"><strong class="nx iu">#items to store links</strong><br/> imageLink = Field()<br/> productLink=Field()</span><span id="39cc" class="ob mb it nx b gy og od l oe of"><strong class="nx iu">#item for company name</strong><br/> company = Field()</span><span id="f7a7" class="ob mb it nx b gy og od l oe of">pass</span><span id="f2a7" class="ob mb it nx b gy og od l oe of">class ImgData(Item):</span><span id="bd20" class="ob mb it nx b gy og od l oe of"><strong class="nx iu">#image pipline items to download product images</strong><br/> image_urls=scrapy.Field()<br/> images=scrapy.Field()</span></pre><p id="268e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后我们要修改‘settings . py’。这是定制图像管道和蜘蛛行为所必需的。</p><blockquote class="os ot ou"><p id="4e03" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated">Scrapy <a class="ae le" href="https://docs.scrapy.org/en/latest/topics/settings.html" rel="noopener ugc nofollow" target="_blank">设置</a>允许你定制所有 Scrapy 组件的行为，包括核心、扩展、管道和蜘蛛本身。</p><p id="6117" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><strong class="js iu"> <em class="it">出自</em></strong><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/settings.html" rel="noopener ugc nofollow" target="_blank">【scrapy.org】<em class="it"/></a></p></blockquote><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="8966" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu"># settings.py in fashionWebScraping folder<br/># Scrapy settings for fashionWebScraping project</strong></span><span id="1751" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># For simplicity, this file contains only settings considered important or commonly used. You can find more settings consulting the documentation:</strong></span><span id="b7a1" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># </strong><a class="ae le" href="https://doc.scrapy.org/en/latest/topics/settings.html" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">https://doc.scrapy.org/en/latest/topics/settings.html</strong></a><strong class="nx iu"><br/># </strong><a class="ae le" href="https://doc.scrapy.org/en/latest/topics/downloader-middleware.html" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">https://doc.scrapy.org/en/latest/topics/downloader-middleware.html</strong></a><strong class="nx iu"><br/># </strong><a class="ae le" href="https://doc.scrapy.org/en/latest/topics/spider-middleware.html" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">https://doc.scrapy.org/en/latest/topics/spider-middleware.html</strong></a></span><span id="2671" class="ob mb it nx b gy og od l oe of">BOT_NAME = 'fashionWebScraping'<br/>SPIDER_MODULES = ['fashionWebScraping.spiders']<br/>NEWSPIDER_MODULE = 'fashionWebScraping.spiders'</span><span id="dca1" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Crawl responsibly by identifying yourself (and your website) on the user-agent</strong><br/>USER_AGENT = 'fashionWebScraping'</span><span id="696b" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Obey robots.txt rules</strong><br/>ROBOTSTXT_OBEY = True</span><span id="1aa4" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># See </strong><a class="ae le" href="https://doc.scrapy.org/en/latest/topics/settings.html" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">https://doc.scrapy.org/en/latest/topics/settings.html</strong></a><strong class="nx iu"><br/># download-delay<br/># See also autothrottle settings and docs<br/># This to avoid hitting servers too hard<br/></strong>DOWNLOAD_DELAY = 1</span><span id="ee43" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Override the default request headers:<br/></strong>DEFAULT_REQUEST_HEADERS = {<br/>'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',<br/>'Accept-Language': 'tr',<br/>}</span><span id="1554" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Configure item pipelines<br/># See </strong><a class="ae le" href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">https://doc.scrapy.org/en/latest/topics/item-pipeline.html</strong></a><strong class="nx iu"><br/></strong>ITEM_PIPELINES = {'scrapy.pipelines.images.ImagesPipeline': 1}</span><span id="6eb8" class="ob mb it nx b gy og od l oe of">IMAGES_STORE = '/Users/erdemisbilen/Angular/fashionWebScraping/images_scraped'</span></pre><p id="7778" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“item.py”和“settings.py”对我们项目中的所有蜘蛛都有效。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/3ad8c2ec51afd6478bfc2325904eac46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5UpOVaBou9sll329"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@renichinguyen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nguyen Bui</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="5c5f" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">第四步:构建蜘蛛</h1><blockquote class="os ot ou"><p id="fd47" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/spiders.html" rel="noopener ugc nofollow" target="_blank">Scrapy spider</a>是定义如何抓取某个站点(或一组站点)的类，包括如何执行抓取(即跟随链接)以及如何从页面中提取结构化数据(即抓取项目)。换句话说，蜘蛛是为特定站点(或者，在某些情况下，一组站点)定义抓取和解析页面的自定义行为的地方。</p><p id="d3a8" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><strong class="js iu"> <em class="it">出自</em></strong><a class="ae le" href="https://docs.scrapy.org/en/latest/topics/spiders.html" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">scrapy.org</em></strong></a></p></blockquote><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="1de4" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu">fashionWebScraping $</strong> scrapy genspider fashionBOYNER boyner.com<br/><em class="ov">Created spider ‘fashionBOYNER’ using template ‘basic’ in module:<br/>fashionWebScraping.spiders.fashionBOYNER</em></span></pre><p id="1d9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的 shell 命令创建了一个空的蜘蛛文件。让我们将代码写入我们的<strong class="js iu"><em class="ov">fashion boyner . py</em></strong>文件:</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="4104" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu"># 'fashionBOYNER.py' in fashionWebScraping/Spiders folder</strong></span><span id="7fde" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># import scrapy and scrapy items</strong><br/>import scrapy<br/>from fashionWebScraping.items import FashionwebscrapingItem<br/>from fashionWebScraping.items import ImgData<br/>from scrapy.http import Request</span><span id="5050" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># To read from a csv file</strong><br/>import csv</span><span id="d747" class="ob mb it nx b gy og od l oe of">class FashionboynerSpider(scrapy.Spider):<br/> name = 'fashionBOYNER'<br/> allowed_domains = ['BOYNER.com']<br/> start_urls = ['http://BOYNER.com/']</span><span id="6280" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This function helps us to scrape the whole content of the website<br/> # by following the starting URLs in a csv file.</strong></span><span id="2c0f" class="ob mb it nx b gy og od l oe of">def start_requests(self):</span><span id="6ab9" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Read main category URLs from a csv file</strong></span><span id="0354" class="ob mb it nx b gy og od l oe of">with open ("/Users/erdemisbilen/Angular/fashionWebScraping/<br/>  csvFiles/SpiderMainCategoryLinksBOYNER.csv", "rU") as f:<br/>  <br/>    reader=csv.DictReader(f)</span><span id="78fa" class="ob mb it nx b gy og od l oe of">for row in reader:<br/>      url=row['url']</span><span id="bfa4" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Change the page value incrementally to navigate through<br/>      the product list<br/>      # You can play with the range value according to maximum  <br/>      product quantity, 30 pages to scrape as default</strong><br/>      link_urls = [url.format(i) for i in range(1,30)]</span><span id="7aef" class="ob mb it nx b gy og od l oe of">for link_url in link_urls:<br/>        print(link_url)</span><span id="ba04" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Pass the each link containing products to <br/>        parse_ product_pages function with the gender metadata</strong></span><span id="1f9a" class="ob mb it nx b gy og od l oe of">request=Request(link_url, callback=self.parse_product_pages,<br/>        meta={'gender': row['gender']})</span><span id="2726" class="ob mb it nx b gy og od l oe of">yield request</span><span id="254b" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This function scrapes the page with the help of xpath provided</strong><br/> def parse_product_pages(self,response):<br/> <br/>  item=FashionwebscrapingItem()<br/>  <br/><strong class="nx iu">  # Get the HTML block where all the products are listed<br/>  # &lt;div&gt; HTML element with the "product-list-item" class name</strong></span><span id="86ce" class="ob mb it nx b gy og od l oe of">content=response.xpath('//div[starts-with(@class,"product-list-<br/>  item")]')</span><span id="ac37" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># loop through the each &lt;div&gt; element in the content</strong><br/>  for product_content in content:</span><span id="aa9d" class="ob mb it nx b gy og od l oe of">image_urls = []</span><span id="5078" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># get the product details and populate the items</strong><br/>   item['productId']=product_content.xpath('.//a/@data<br/>   -id').extract_first()</span><span id="fb4b" class="ob mb it nx b gy og od l oe of">item['productName']=product_content.xpath('.//img/@title').<br/>   extract_first()</span><span id="29ce" class="ob mb it nx b gy og od l oe of">item['priceSale']=product_content.xpath('.//ins[@class=<br/>   "price-payable"]/text()').extract_first()</span><span id="c7e9" class="ob mb it nx b gy og od l oe of">item['priceOriginal']=product_content.xpath('.//del[@class=<br/>   "price-psfx"]/text()').extract_first()</span><span id="635a" class="ob mb it nx b gy og od l oe of">if item['priceOriginal']==None:<br/>    item['priceOriginal']=item['priceSale']</span><span id="95df" class="ob mb it nx b gy og od l oe of">item['imageLink']=product_content.xpath('.//img/<br/>   @data-original').extract_first()<br/>   <br/>   item['productLink']="https://www.boyner.com.tr"+<br/>   product_content.xpath('.//a/@href').extract_first()</span><span id="06a7" class="ob mb it nx b gy og od l oe of">image_urls.append(item['imageLink'])</span><span id="4203" class="ob mb it nx b gy og od l oe of">item['company']="BOYNER"<br/>   item['gender']=response.meta['gender']</span><span id="c147" class="ob mb it nx b gy og od l oe of">if item['productId']==None:<br/>    break</span><span id="4d94" class="ob mb it nx b gy og od l oe of">yield (item)</span><span id="cced" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># download the image contained in image_urls</strong><br/>   yield ImgData(image_urls=image_urls)</span><span id="44a6" class="ob mb it nx b gy og od l oe of">def parse(self, response):<br/>  pass</span></pre><p id="31b6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的蜘蛛类包含两个函数，分别是'<strong class="js iu"> start_requests </strong>和'<strong class="js iu"> parse_product_pages </strong>'。</p><p id="13c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在'<strong class="js iu"> start_requests </strong>'函数中，我们从已经生成的特定 CSV 文件中读取，以获得起始 URL 信息。然后我们迭代占位符<code class="fe oo op oq nx b">{}</code>来将产品页面的 URL 传递给‘parse _ product _ pages’函数。</p><p id="ef18" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们还可以使用“meta={'gender': row['gender']} '参数将“gender”元数据传递给“Request”方法中的“parse_product_pages”函数。</p><p id="056c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在'<strong class="js iu"> parse_product_pages </strong>'函数中，我们执行实际的 web 抓取，并用抓取的数据填充 Scrapy 条目。</p><p id="c759" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我使用 Xpath 来定位网页上包含产品信息的 HTML 部分。</p><p id="79e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的第一个 Xpath 表达式从被废弃的当前页面中提取整个产品列表。所有必需的产品信息都包含在“content”的“div”元素中。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="a72b" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu">#  // Selects nodes in the document from the current node that matches the selection no matter where they are</strong></span><span id="e48e" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># '//div[starts-with(@class,"product-list-item")]' selects the all div elements which has class value start</strong></span><span id="da35" class="ob mb it nx b gy og od l oe of">content = response.xpath('//div[starts-with(@class,"product-list-item")]')</span></pre><p id="df06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要在“内容”中循环访问各个产品，并将它们存储在 Scrapy 项目中。借助 XPath 表达式，我们可以很容易地在“内容”中找到所需的 HTML 元素。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="f007" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu"># loop through the each &lt;div&gt; element in the content</strong><br/>  for product_content in content:</span><span id="dafd" class="ob mb it nx b gy og od l oe of">image_urls = []</span><span id="780a" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># get the product details and populate the items<br/>   <br/>   # ('.//a/@data-id') extracts 'data-id' value of &lt;a&gt; element<br/>   inside the product_content<br/>   </strong>item['productId']=product_content.xpath('.//a/@data<br/>   -id').extract_first()</span><span id="8dc8" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># ('.//img/@title') extracts 'title' value of &lt;img&gt; element<br/>   inside the product_content</strong>   <br/>   item['productName']=product_content.xpath('.//img/@title').<br/>   extract_first()</span><span id="1a16" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># ('.//ins[@class= "price-payable"]/text()') extracts text value<br/>   of &lt;ins&gt; element which has 'price-payable' class attribute inside<br/>   the product_content </strong>  <br/>   item['priceSale']=product_content.xpath('.//ins[@class=<br/>   "price-payable"]/text()').extract_first()</span><span id="7d28" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># ('.//del[@class="price-psfx"]/text()') extracts text value<br/>   of &lt;del&gt; element which has 'price-psfx' class attribute inside<br/>   the product_content</strong><br/>   item['priceOriginal']=product_content.xpath('.//del[@class=<br/>   "price-psfx"]/text()').extract_first()</span><span id="73e8" class="ob mb it nx b gy og od l oe of">if item['priceOriginal']==None:<br/>     item['priceOriginal']=item['priceSale']</span><span id="4eef" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># ('.//img/@data-original') extracts 'data-original' value of<br/>   &lt;img&gt; element inside the product_content</strong><br/>   item['imageLink']=product_content.xpath('.//img/<br/>   @data-original').extract_first()</span><span id="0083" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># ('.//a/@href') extracts 'href' value of<br/>   &lt;a&gt; element inside the product_content<br/></strong>   item['productLink']="https://www.boyner.com.tr"+<br/>   product_content.xpath('.//a/@href').extract_first()</span><span id="2235" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># assigns the product image link into the 'image_urls' which is<br/>   defined in the image pipeline</strong><br/>   image_urls.append(item['imageLink'])</span><span id="ce0e" class="ob mb it nx b gy og od l oe of">item['company']="BOYNER"<br/>   item['gender']=response.meta['gender']</span><span id="1973" class="ob mb it nx b gy og od l oe of">if item['productId']==None:<br/>    break</span><span id="3e11" class="ob mb it nx b gy og od l oe of">yield (item)<br/> <br/>   <strong class="nx iu"># download the image contained in image_urls</strong><br/>   yield ImgData(image_urls=image_urls)</span></pre><p id="05da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样的原则也适用于其他网站。你可以在我的 GitHub 库里看到<a class="ae le" href="https://github.com/eisbilen/FashionSearch/tree/master/fashionWebScraping/spiders" rel="noopener ugc nofollow" target="_blank">所有 10 个蜘蛛的代码。</a></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/2c261f2b88cc8041a929c3d871389481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WKY0Anadt82-gt9G"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@timmossholder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tim Mossholder</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="f73a" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">第五步:运行蜘蛛并将抓取的数据存储在 JSON 文件中</h1><p id="fb70" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">在抓取过程中，每个产品项目都存储在一个 JSON 文件中。每个网站都有一个特定的 JSON 文件，其中填充了每次蜘蛛运行的数据。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="677c" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu">fashionWebScraping $ </strong>scrapy crawl -o rawdata_BOYNER.json -t jsonlines fashionBOYNER</span></pre><p id="aa81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<strong class="js iu"> jsonlines </strong>格式比<strong class="js iu"> JSON </strong>格式更节省内存，尤其是当你在一个会话中抓取大量网页时。</p><p id="9504" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，JSON 文件名以“rawdata”开头，这表示下一步是在我们的应用程序中使用废弃的原始数据之前检查和验证它们。</p><h1 id="896a" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">步骤 6:清理和验证 JSON 文件中的抓取数据</h1><p id="0819" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">在废弃过程结束后，在应用程序中使用它们之前，您可能需要从 JSON 文件中删除一些行项目。</p><p id="d9dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">您可能有包含空字段或重复值的行项目。这两种情况都需要一个我用<strong class="js iu"> 'jsonPrep.py' </strong>和<strong class="js iu"> 'deldub.py '处理的修正过程。</strong></p><p id="466c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 'jsonPrep.py' </strong>查找具有空值的行项目，并在检测到时删除它们。您可以在下面找到带有解释的代码:</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="dfbf" class="ob mb it nx b gy oc od l oe of"># <strong class="nx iu">‘jsonPrep.py’ in fashionWebScraping/utilityScripts folder</strong><br/>import json<br/>import sys<br/>from collections import OrderedDict<br/>import csv<br/>import os</span><span id="e993" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Reads from jsonFiles.csv file for the names and locations of all json files need to be validated</strong> <br/>with open("/Users/erdemisbilen/Angular/fashionWebScraping/csvFiles/ jsonFiles.csv", "rU") as f:</span><span id="a8f6" class="ob mb it nx b gy og od l oe of">reader=csv.DictReader(f)</span><span id="effe" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Iterates the json files listed in jsonFiles.csv</strong><br/> for row in reader:<br/> <br/> <strong class="nx iu"> # Reads from jsonFiles.csv file for jsonFile_raw column</strong><br/>  jsonFile=row['jsonFile_raw']</span><span id="c565" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Opens the jsonFile</strong><br/>   with open(jsonFile) as json_file:<br/>    data = []<br/>    i = 0</span><span id="b3c9" class="ob mb it nx b gy og od l oe of">seen = OrderedDict()<br/>    <br/>    <strong class="nx iu"># Iterates in the rows of json file</strong><br/>    for d in json_file:<br/>     seen = json.loads(d)</span><span id="49ca" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Do not include the line item if the product Id is null<br/></strong>     try:<br/>      if seen["productId"] != None:<br/>       for key, value in seen.items():<br/>        print("ok")<br/>        i = i + 1<br/>        data.append(json.loads(d))<br/>     <br/>     except KeyError:<br/>      print("nok")<br/>    <br/>    print (i)<br/>    <br/>    baseFileName=os.path.splitext(jsonFile)[0]</span><span id="00e1" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Write the result as a json file by reading filename from the<br/>    'file_name_prep' column<br/>    </strong>with open('/Users/erdemisbilen/Angular/fashionWebScraping/<br/>    jsonFiles/'+row['file_name_prep'], 'w') as out:</span><span id="2695" class="ob mb it nx b gy og od l oe of">json.dump(data, out)</span></pre><p id="f3bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">删除空行项目后，结果将保存到“jsonFiles”项目文件夹中，文件名以“prepdata”开头。</p><p id="d516" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 'deldub.py' </strong>查找重复的行项目，并在检测到时删除它们。您可以在下面找到带有解释的代码:</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="23d1" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu"># 'deldub.py' in fashionWebScraping/utilityScripts folder</strong><br/>import json<br/>import sys<br/>from collections import OrderedDict<br/>import csv<br/>import os</span><span id="0e46" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Reads from jsonFiles.csv file for the names and locations of all json files need to be validated for dublicate lines</strong></span><span id="9c28" class="ob mb it nx b gy og od l oe of">with open("/Users/erdemisbilen/Angular/fashionWebScraping/csvFiles/ jsonFiles.csv", newline=None) as f:<br/> <br/> reader=csv.DictReader(f)</span><span id="b62a" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Iterates the json files listed in jsonFiles.csv</strong><br/> for row in reader:</span><span id="c189" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Reads from jsonFiles.csv file for jsonFile_raw column</strong><br/>  jsonFile=row['jsonFile_prep']</span><span id="e459" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Opens the jsonFile</strong><br/>  with open(jsonFile) as json_file:<br/>   data = json.load(json_file)</span><span id="cc9a" class="ob mb it nx b gy og od l oe of">seen = OrderedDict()<br/>   dubs = OrderedDict()</span><span id="85ad" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Iterates in the rows of json file</strong><br/>   for d in data:<br/>    oid = d["productId"]</span><span id="7688" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Don't include the item if the product Id has dublicate value</strong><br/>    if oid not in seen:<br/>     seen[oid] = d</span><span id="0eb0" class="ob mb it nx b gy og od l oe of">else:<br/>     dubs[oid]=d</span><span id="8bc8" class="ob mb it nx b gy og od l oe of">baseFileName=os.path.splitext(jsonFile)[0]</span><span id="6ca7" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># Write the result as a json file by reading filename from the<br/>     'file_name_final' column</strong></span><span id="a8b1" class="ob mb it nx b gy og od l oe of">with open('/Users/erdemisbilen/Angular/fashionWebScraping/<br/>     jsonFiles/'+row['file_name_final'], 'w') as out:<br/>      json.dump(list(seen.values()), out)</span><span id="d091" class="ob mb it nx b gy og od l oe of">with open('/Users/erdemisbilen/Angular/fashionWebScraping/<br/>     jsonFiles/'+'DELETED'+row['file_name_final'], 'w') as out:<br/>      json.dump(list(dubs.values()), out)</span></pre><p id="6e20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">删除重复的行项目后，将结果保存到“jsonFiles”项目文件夹中，文件名以“finaldata”开头。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nd"><img src="../Images/9c40f750f31b246a542c43a017dce1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d5bvfeBhkiRW-qIy"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@jamesponddotco?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">James Pond</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="9141" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">使用 Apache Airflow 自动执行完整的刮擦工作流程</h1><p id="0cc5" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">一旦我们定义了刮擦过程，我们就可以进入工作流自动化。我将使用<strong class="js iu"> Apache Airflow，</strong>它是由<strong class="js iu"> Airbnb 开发的基于 Python 的工作流自动化工具。</strong></p><p id="5e03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我将提供安装和配置 Apache Airflow 的终端命令，你可以参考我下面的帖子了解更多细节。</p><div class="ne nf gp gr ng nh"><a href="https://medium.com/swlh/my-deep-learning-journey-from-experimentation-to-production-844cb271a476" rel="noopener follow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">我的深度学习之旅:从实验到生产</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">用 Apache Airflow 构建自动化机器学习管道</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">medium.com</p></div></div><div class="nq l"><div class="pb l ns nt nu nq nv ky nh"/></div></div></a></div><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="3449" class="ob mb it nx b gy oc od l oe of">$ python3 --version<br/>Python 3.7.3$ virtualenv --version<br/>15.2.0$ cd /path/to/my/airflow/workspace</span><span id="eafb" class="ob mb it nx b gy og od l oe of">$ virtualenv -p `which python3` venv<br/>$ source venv/bin/activate</span><span id="7e3f" class="ob mb it nx b gy og od l oe of">(venv) $ pip install apache-airflow<br/>(venv) $ mkdir airflow_home<br/>(venv) $ export AIRFLOW_HOME=`pwd`/airflow_home<br/>(venv) $ airflow initdb<br/>(venv) $ airflow webserver</span></pre><p id="96d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">创建 DAG 文件</strong></p><blockquote class="os ot ou"><p id="d394" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><em class="it">在 Airflow 中，一个</em><code class="fe oo op oq nx b"><em class="it">DAG</em></code><em class="it">——或者一个有向无环图——是你想要运行的所有任务的集合，以反映它们的关系和依赖性的方式组织。</em></p><p id="7f20" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><em class="it">例如，一个简单的 DAG 可以包含三个任务:A、B 和 C。它可以说 A 必须在 B 可以运行之前成功运行，但是 C 可以随时运行。它可以说任务 A 在 5 分钟后超时，B 在失败的情况下最多可以重启 5 次。它还可能会说工作流将在每晚 10 点运行，但不应该在某个特定日期开始。</em></p></blockquote><p id="0a32" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 Python 文件中定义的 DAG 用于组织任务流。我们不会在 DAG 文件中定义实际的任务。</p><p id="a3eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们创建一个 DAG 文件夹和一个空的 python 文件，开始用 Python 代码定义我们的工作流。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="3a6b" class="ob mb it nx b gy oc od l oe of">(venv) $ mkdir dags</span></pre><p id="6bb3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Airflow 提供了几个运算符来描述 DAG 文件中的任务。下面我列出了常用的。</p><blockquote class="os ot ou"><p id="0d44" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><code class="fe oo op oq nx b"><a class="ae le" href="http://airflow.apache.org/_api/airflow/operators/bash_operator/index.html#airflow.operators.bash_operator.BashOperator" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">BashOperator</em></strong></a></code> <em class="it"> -执行一个 bash 命令</em></p><p id="7774" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><code class="fe oo op oq nx b"><a class="ae le" href="http://airflow.apache.org/_api/airflow/operators/python_operator/index.html#airflow.operators.python_operator.PythonOperator" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">PythonOperator</em></strong></a></code> <em class="it"> -调用任意的 Python 函数</em></p><p id="6766" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><code class="fe oo op oq nx b"><a class="ae le" href="http://airflow.apache.org/_api/airflow/operators/email_operator/index.html#airflow.operators.email_operator.EmailOperator" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">EmailOperator</em></strong></a></code><em class="it">——发邮件</em></p><p id="31f1" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><code class="fe oo op oq nx b"><a class="ae le" href="http://airflow.apache.org/_api/airflow/operators/http_operator/index.html#airflow.operators.http_operator.SimpleHttpOperator" rel="noopener ugc nofollow" target="_blank"><strong class="js iu"><em class="it">SimpleHttpOperator</em></strong></a></code><em class="it">——发送一个 HTTP 请求</em></p><p id="8100" class="jq jr ov js b jt ju jv jw jx jy jz ka ow kc kd ke ox kg kh ki oy kk kl km kn im bi translated"><code class="fe oo op oq nx b"><strong class="js iu"><em class="it">Sensor</em></strong></code> <em class="it"> -等待一定时间，文件，数据库行，S3 键等… </em></p></blockquote><p id="75b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我打算现在只使用<strong class="js iu">‘bash operator’</strong>，因为我将使用 python 脚本完成我的所有任务。</p><p id="fd82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于我将使用<strong class="js iu">‘bash operator’，</strong>有一个包含特定任务的所有命令的 bash 脚本来简化 dag 文件会很好。</p><p id="b622" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" href="https://www.taniarascia.com/how-to-create-and-use-bash-scripts/" rel="noopener ugc nofollow" target="_blank">通过遵循本教程</a>，我为每个任务生成了 bash 脚本。你可以在<a class="ae le" href="https://github.com/eisbilen/FashionSearch/tree/master/bin" rel="noopener ugc nofollow" target="_blank">我的 Github 库</a>中找到它们。</p><p id="ba9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我可以使用我创建的 bash 命令编写如下所示的 DAG 文件。有了下面的配置，我的任务将被安排和执行每天的基础上气流。您可以根据需要在 DAG 文件中更改开始日期或计划间隔。您可以进一步使用本地执行器或 celery 执行器来并行运行任务实例。由于我使用的是顺序执行器，这是最原始的执行器，所以我的所有任务实例都将顺序工作。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="812f" class="ob mb it nx b gy oc od l oe of"><strong class="nx iu"># 'fashionsearch_dag.py' in Airflow dag folder</strong><br/>import datetime as dt<br/>from airflow import DAG<br/>from airflow.operators.bash_operator import BashOperator<br/>from datetime import datetime, timedelta</span><span id="d558" class="ob mb it nx b gy og od l oe of">default_args = {<br/>'owner': 'airflow',<br/>'depends_on_past': False,<br/>'start_date': datetime(2019, 11, 23),<br/>'retries': 1,<br/>'retry_delay': timedelta(minutes=5),</span><span id="820c" class="ob mb it nx b gy og od l oe of"># 'queue': 'bash_queue',<br/># 'pool': 'backfill',<br/># 'priority_weight': 10,<br/># 'end_date': datetime(2016, 1, 1),<br/>}</span><span id="324f" class="ob mb it nx b gy og od l oe of">dag = DAG(dag_id='fashionsearch_dag', default_args=default_args, schedule_interval=timedelta(days=1))</span><span id="6e87" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task deletes all json files which are generated in previous scraping sessions</strong><br/>t1 = BashOperator(<br/>task_id='delete_json_files',<br/>bash_command='run_delete_files',<br/>dag=dag)</span><span id="dc71" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.boyner.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.boyner.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t2 = BashOperator(<br/>task_id='boyner_spider',<br/>bash_command='run_boyner_spider',<br/>dag=dag)</span><span id="6634" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.derimod.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.derimod.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t3 = BashOperator(<br/>task_id='derimod_spider',<br/>bash_command='run_derimod_spider',<br/>dag=dag)</span><span id="257b" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.hepsiburada.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.hepsiburada.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t4 = BashOperator(<br/>task_id='hepsiburada_spider',<br/>bash_command='run_hepsiburada_spider',<br/>dag=dag)</span><span id="7a6e" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.hm.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.hm.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t5 = BashOperator(<br/>task_id='hm_spider',<br/>bash_command='run_hm_spider',<br/>dag=dag)</span><span id="9722" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.koton.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.koton.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t6 = BashOperator(<br/>task_id='koton_spider',<br/>bash_command='run_koton_spider',<br/>dag=dag)</span><span id="981d" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.lcwaikiki.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.lcwaikiki.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t7 = BashOperator(<br/>task_id='lcwaikiki_spider',<br/>bash_command='run_lcwaikiki_spider',<br/>dag=dag)</span><span id="8a59" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.matmazel.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.matmazel.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t8 = BashOperator(<br/>task_id='matmazel_spider',<br/>bash_command='run_matmazel_spider',<br/>dag=dag)</span><span id="aecc" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.modanisa.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.modanisa.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t9 = BashOperator(<br/>task_id='modanisa_spider',<br/>bash_command='run_modanisa_spider',<br/>dag=dag)</span><span id="b062" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.morhipo.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.morhipo.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t10 = BashOperator(<br/>task_id='morhipo_spider',<br/>bash_command='run_morhipo_spider',<br/>dag=dag)</span><span id="b740" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.mudo.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.mudo.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t11 = BashOperator(<br/>task_id='mudo_spider',<br/>bash_command='run_mudo_spider',<br/>dag=dag)</span><span id="b8e3" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.trendyol.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.trendyol.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t12 = BashOperator(<br/>task_id='trendyol_spider',<br/>bash_command='run_trendyol_spider',<br/>dag=dag)</span><span id="b59c" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task runs the spider for </strong><a class="ae le" href="http://www.yargici.com" rel="noopener ugc nofollow" target="_blank"><strong class="nx iu">www.yargici.com</strong></a><strong class="nx iu"><br/># And populates the related json file with data scraped</strong><br/>t13 = BashOperator(<br/>task_id='yargici_spider',<br/>bash_command='run_yargici_spider',<br/>dag=dag)</span><span id="2108" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task checks and removes null line items in json files</strong><br/>t14 = BashOperator(<br/>task_id='prep_jsons',<br/>bash_command='run_prep_jsons',<br/>dag=dag)</span><span id="8866" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task checks and removes dublicate line items in json files</strong><br/>t15 = BashOperator(<br/>task_id='delete_dublicate_lines',<br/>bash_command='run_del_dub_lines',<br/>dag=dag)</span><span id="404e" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># This task populates the remote ES clusters with the data inside the JSON files</strong><br/>t16 = BashOperator(<br/>task_id='json_to_elasticsearch',<br/>bash_command='run_json_to_es',<br/>dag=dag)</span><span id="fd6d" class="ob mb it nx b gy og od l oe of"><strong class="nx iu"># With sequential executer, all tasks depends on previous task<br/># No paralell task execution is possible<br/># Use local executer at least for paralell task execution</strong></span><span id="de40" class="ob mb it nx b gy og od l oe of">t1 &gt;&gt; t2 &gt;&gt; t3 &gt;&gt; t4 &gt;&gt; t5 &gt;&gt; t6 &gt;&gt; t7 &gt;&gt; t8 &gt;&gt; t9 &gt;&gt; t10 &gt;&gt; t11 &gt;&gt; t12 &gt;&gt; t13 &gt;&gt; t14 &gt;&gt; t15 &gt;&gt; t16</span></pre><p id="7818" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要启动 DAG 工作流，我们需要运行气流调度程序。这将使用<em class="ov">‘air flow . CFG’</em>文件中指定的配置执行调度程序。Scheduler 监控位于“dags”文件夹中的每个 DAG 中的每个任务，并在满足依赖关系时触发任务的执行。</p><pre class="kp kq kr ks gt nw nx ny nz aw oa bi"><span id="20de" class="ob mb it nx b gy oc od l oe of">(venv) $ airflow scheduler</span></pre><p id="3e3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们运行了气流调度程序，我们就可以通过在浏览器上访问<a class="ae le" href="http://0.0.0.0:8080" rel="noopener ugc nofollow" target="_blank">http://0 . 0 . 0:8080</a>来查看我们的任务状态。Airflow 提供了一个用户界面，我们可以在其中查看和跟踪我们计划的 Dag。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/4bc6deee1694b42000b59abef69f89f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TcedWeg20J8upR0EMYzLPQ.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">AirFlow Dag Graph View</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/c615d188638b3b6fee6e425761b9ecc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sq8zsQKG9iczVH8E7YiZbA.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">AirFlow Dag Tree View</figcaption></figure><h1 id="e69c" class="ma mb it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">结论</h1><p id="fc86" class="pw-post-body-paragraph jq jr it js b jt my jv jw jx mz jz ka kb na kd ke kf nb kh ki kj nc kl km kn im bi translated">我试着从头到尾向你展示我的网页抓取工作流程。</p><p id="70b0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">希望这能帮助你掌握 web 报废和工作流自动化的基础知识。</p><p id="7346" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">查看我的新文章，了解我如何使用 Web scrapping 和自然语言处理来自动创建电子学习内容:</p><div class="ne nf gp gr ng nh"><a rel="noopener follow" target="_blank" href="/how-to-automate-content-creation-process-for-e-learning-platform-852877fcd668"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">如何自动化电子学习平台的内容创建流程</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">在 5 分钟内生成 2000 个问题</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">towardsdatascience.com</p></div></div><div class="nq l"><div class="pe l ns nt nu nq nv ky nh"/></div></div></a></div></div></div>    
</body>
</html>