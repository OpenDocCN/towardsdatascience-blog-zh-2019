<html>
<head>
<title>Understanding Gradient Descent And Its Variants</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解梯度下降及其变体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-gradient-descent-and-its-variants-cf0df5c45478?source=collection_archive---------7-----------------------#2019-12-25">https://towardsdatascience.com/understanding-gradient-descent-and-its-variants-cf0df5c45478?source=collection_archive---------7-----------------------#2019-12-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1b21" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简要了解优化算法如何支持机器学习模型中的学习过程</h2></div><p id="6ffe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习模型很神奇；他们可以识别视频中的物体；它们可以自动为图像生成字幕，并对猫狗图片<em class="le">(有时)进行准确分类。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/038376474f05d35b400945bcec2d69c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*vZE-LKuvkWq3OBUDJRF-pg.jpeg"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Image from <a class="ae lr" href="https://www.kaggle.com/c/dogs-vs-cats" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/dogs-vs-cats</a></figcaption></figure><p id="ed80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文将提供对机器学习模型下发生的事情的表面理解。更具体地说，我们将探索使这些机器学习模型能够学习的“主干算法”。</p><p id="6558" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">“骨干算法”被称为优化算法。下面是你在这篇文章中会遇到的一些关键词的定义，优化算法也在提供的描述中。</p><ul class=""><li id="bcb4" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated"><strong class="kk iu">优化算法</strong>:执行预定次数的算法，用于寻找问题的最优解，在数学术语中，这些“问题”被称为函数。</li><li id="a2f5" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">梯度下降</strong>:该优化算法被用来寻找降低成本函数的值。这是通过梯度值的计算来完成的，该梯度值用于在找到成本函数的局部最小值的每一步选择值。梯度的负值用于寻找局部最小值。</li><li id="a242" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">成本函数</strong>:这是一种量化机器学习模型执行“有多好”的方法。量化是基于一组输入的输出(成本)，这些输入被称为参数值。参数值用于估计预测，而“成本”是预测值和实际值之间的差异。</li><li id="9446" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">全局最小值</strong>:这是位于成本函数整个域内的最小参数值。您可能会遇到局部最小值，它是指位于成本函数的设定范围内的最低参数值。</li><li id="0c84" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">收敛</strong>:这描述了在机器学习环境中使用时向最佳参数值或全局最小值移动的概念</li></ul><p id="9e32" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文底部是一些标准机器学习术语及其定义的链接。<em class="le">把它当成圣诞礼物</em>😊。</p><p id="8149" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在本文中探索的优化算法是<strong class="kk iu">梯度下降。</strong></p><h1 id="0a1c" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">梯度下降</h1><p id="346b" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">梯度下降是一种非常常见的优化算法，很可能是许多机器学习工程师和数据科学家引入的第一种优化算法。</p><p id="4396" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画一幅画。我们有一个成本函数，我们需要找到最优解来求解成本函数。接下来是梯度下降，这是一种算法，它通过改变模型中的参数值来工作，所有这些都是为了最小化成本函数。成本函数的一个例子是<a class="ae lr" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">均方差</a>。</p><p id="e3cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">梯度下降固有功能的工作原理是，根据从误差函数获得的关于特定数据点的参数的计算梯度，找到朝向局部最小值的方向。</p><p id="31d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能有助于理解一些图像和可视化梯度下降。</p><p id="0658" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用一个包含一条碗形曲线和一个球的图形，球放在曲线的左上方。球表示最初随机选择的参数空间中的某个点(值)，曲线表示相对于参数值范围绘制的成本值。目标是达到提供最低成本值的参数值。</p><p id="23a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在图的 X 轴上是代表成本的值，在 y 轴上是由‘X’表示的值，其代表我们用来求解成本函数的参数值的范围。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/2473f198a7412ca9ce00786710194087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0gcHzZbrs5TjQNFUbTZMSg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Image of a cost function curvature</figcaption></figure><p id="407f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最小值(单数)/极小值(复数)是斜率中存在最小化成本函数的最佳值的点，梯度下降是在几个步骤(迭代)中将我们的球导向最小值的算法。</p><p id="1888" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了求解成本函数，我们寻找曲线的最低点，这是梯度为零或接近零的点。</p><blockquote class="ni nj nk"><p id="606a" class="ki kj le kk b kl km ju kn ko kp jx kq nl ks kt ku nm kw kx ky nn la lb lc ld im bi translated"><strong class="kk iu">快速提示</strong>:成本函数曲线并不总是具有一个局部最小值的碗形。在上图中使用的示例中，成本函数只有一个输入参数(一维参数空间)，但实际上，参数空间往往有更多的维度。</p></blockquote><h1 id="0cb6" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">批量梯度下降</h1><p id="9a22" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">我们了解梯度下降是如何工作的，现在可以将其应用于我们的训练数据。梯度下降算法对训练数据的应用有多种形式。一种形式叫做<strong class="kk iu">批量梯度下降(BGD) </strong>。</p><p id="2778" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上图中，我们朝着局部最小值前进。在 BGD，我们实际上利用我们掌握的每一个训练数据来决定我们向哪个方向移动以及移动多少。我们在每一步都使用所有的训练数据。</p><p id="0a5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于大量的训练数据，训练过程可能会延长，但在计算上是高效的，因为我们不像梯度下降的其他变体那样经常对我们的模型参数进行任何改变。</p><p id="43bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管 BGD 的内存效率不高，但你可以想象，在训练模型时，我们需要内存中所有可用的数据集。</p><h1 id="4bf1" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">随机梯度下降</h1><p id="7f79" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">在 BGD 硬币的另一面，我们有<strong class="kk iu">随机梯度下降</strong> (SGD)。</p><p id="0847" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与遍历训练集中的每个数据，然后向局部最小值前进相反，SGD 的工作方式是从训练集中实际选取一个数据点，并基于这个数据点计算梯度。</p><p id="a8b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可能会发现，在 BGD 和 SGD 之间，SGD 是更快的算法，因为您是基于数据的单个实例而不是整个数据集来计算梯度的，但代价是什么。</p><p id="862e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 SGD 时，梯度下降期间在参数空间内进行的更新可能会有噪声。SGD 的噪声特性是其随机特性的结果，当从训练集中选择数据点来计算每一步的梯度时，会出现这种随机特性。</p><p id="fad0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了适应 SGD 的噪声性质，并确保我们达到最佳参数值，我们必须对训练数据迭代一定次数，并确保在梯度下降过程的开始，训练数据被打乱。</p><p id="8841" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">噪声导致求解成本函数的模糊参数值，尽管给定足够的时间，SGD 将接近局部最小值。SGD 的噪声和随机性也是有益的。当算法陷入不是全局最小值的局部最小值时，这很有用。</p><p id="69c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与 BGD 相比，SGD 在每一步分配参数值时，由于其随机和不稳定的性质，具有避开局部最小值和找到全局最小值的优点。</p><p id="15d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是与 SGD 相比，BGD 参数值更接近全局最小值和最优值。当面临在梯度下降算法的两个变型之间进行选择时，在速度和最优性之间有一个折衷。</p><h1 id="c649" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">小批量梯度下降</h1><p id="d4c5" class="pw-post-body-paragraph ki kj it kk b kl my ju kn ko mz jx kq kr na kt ku kv nb kx ky kz nc lb lc ld im bi translated">如何利用新加坡元和 BGD 的优点？</p><p id="88bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">小批量梯度下降基于训练集中随机选择的数据计算梯度，就像 SGD 一样，但在计算梯度时不包括整个数据集，所以它也不完全是 BGD。你可以说它是一个混合体。</p><p id="638c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Mini Batch GD 在计算梯度时使用少量数据；与 BGD 相比，它更快，但与新加坡元相比，它仍然较慢。</p><p id="dc87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相对于 SGD，小批量 GD 的优势在于降低了参数空间内的噪声。这意味着利用小批量 GD，意味着更容易达到最佳参数值。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="4972" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">唷，这还不算太糟，接下来，你可以在网上寻找关于每种算法的代码实现的资源。</p><p id="14ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我应该会发布一篇中型文章，概述如何实现本文中提到的一些梯度下降算法。</p><p id="7efb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">关注我，获取更多类似的文章。</strong></p><p id="b8b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如承诺的那样，下面是一些常见机器学习术语定义的链接。</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/30-pocket-sized-terms-for-machine-learning-beginners-5e381ed93055"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">机器学习初学者的 30 个袖珍术语</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">你在数据科学或机器学习职业生涯中会遇到的有用的机器学习术语列表。</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ll ny"/></div></div></a></div></div></div>    
</body>
</html>