# 如何正确地将正则化添加到 Keras 预训练模型中

> 原文：<https://towardsdatascience.com/how-to-add-regularization-to-keras-pre-trained-models-the-right-way-743776451091?source=collection_archive---------29----------------------->

## 使用 Tensorflow 2.0 正则化预训练的 Keras 模型

![](img/1706b223e9509d9d19121f6f2935fe8b.png)

Photo by Kelly Sikkema on Unsplash

# 介绍

如果你以训练深度学习模型为生，你可能会厌倦知道一件具体而重要的事情:

> 微调深度预训练模型需要大量的正则化工作。

作为对比，你可能已经注意到，如何向来自深度学习库(如 Keras)的预训练模型添加正则化并不总是显而易见的。此外，找到这个问题的正确答案也不容易。在撰写这篇文章的过程中，我遇到了许多关于堆栈溢出的代码片段和一些博客文章，它们根本没有得到正确的解释。然后，作为减少您的 Google 搜索的一种方式(并帮助我未来的自己)，我将向您展示如何以正确的方式将正则化添加到预训练的 Keras 模型中。

让我们从基础开始。

# 微调

微调是指采用预先训练好的模型，并将其作为优化不同(大多数情况下相关)任务的起点的过程。你可以想象使用一个在 ImageNet 数据库上训练的 ResNet50 模型，并用它来解决一个新问题，比如昆虫分类。

这个过程通常遵循简单的步骤。

1.  **我们首先加载模型架构和预训练权重。**对于迁移学习更为成熟的计算机视觉，这是我们加载这些著名架构之一的地方，如 DenseNets 或 MobileNets 及其各自的权重(在 ImageNet 上训练)。
2.  **然后，我们在预训练模型的顶部添加一个特定于任务的分类层。**这通常是一个具有 softmax 或 sigmoid 激活的致密层。请注意，分类层中单元的数量必须等于新问题的类的数量。所以，如果你的昆虫数据集包含 28 种虫子之类的东西，那么最后一层需要有 28 个单元。
3.  然后，我们完成了模型的准备。在 Keras 中，我们使用优化器和损失函数编译模型，设置超参数，并调用 fit。

*附注:这可能过于简单，但对我们的例子来说很好。*

# 对抗过度拟合

我们必须记住的一件事是:

> 当微调预训练模型时，过度拟合是一个更大的问题。

问题很容易看出来。如果你有一个小的训练数据，你会不断地向网络一遍又一遍地显示相同的实例。此外，正如我们所知，ImageNet 上预先训练的 ConvNets 通常非常复杂；即它们有许多训练参数。因此，模型会很快完全记住你的训练数据。

作为解决方案，微调通常需要两件事:

1.  大量的正规化
2.  非常小的学习率

对于正规化，任何事情都可能有所帮助。我通常使用 l1 或 l2 正则化，并提前停止。对于没有批量标准化的 ConvNets，[空间丢失](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SpatialDropout2D)也是有帮助的

作为补充说明，深度学习模型是众所周知的数据饥渴。这些模型需要大量数据来将非常复杂的高维空间分解成特征空间中的线性可分决策。许多人将微调视为使用较小数据集训练深度模型的一种方式。虽然在实践中，这种观点听起来可能是正确的，但这里有一个重要的陷阱。即使您可以使用小得多的数据集来拟合新模型，请记住，您的预训练模型训练了几天(使用多个 GPU)。换个说法，微调其实就是——***站在巨人的肩膀上。***

现在让我们直接进入代码。

# 黑客攻击

直观地说，添加正则化的过程很简单。在加载我们预先训练好的模型后，称为 ***基础模型*** ，我们将遍历它的所有层。对于每一层，我们检查它是否支持正则化，如果支持，我们添加它。代码如下所示。

看起来我们结束了。事实上，如果你在谷歌上搜索如何将正则化添加到 Keras 预训练模型中，你会发现相同的结果。

作为安全检查，让我们确保正则化设置正确。在 Keras 中，我们可以通过访问层或模型的 loss 属性来检索损耗。在我们的例子中，我们可以通过以下方式访问所有损失的列表(通过正则化来自所有层):

如果你对术语感到困惑，这个属性叫做损失，因为在优化过程中正则化惩罚被添加到损失函数中。

如你所见，有些奇怪的事情正在发生。**列表是空的，这意味着没有正则化惩罚应用于卷积核。**

但是刚刚发生了什么？

好吧，直接进入问题，看起来当我们改变一个层的属性时，正如我们所做的，**唯一真正改变的是模型配置。**因此，模型对象本身就和我们加载时一样。完全没有变化。

添加正则化后，看一下配置文件。***kernel _ regulator***属性和我们设置的一样。

这个问题的一个简单解决方案是重新加载模型配置。这很容易做到，也解决了问题。

现在，如果我们试图看到*，我们就有了。*

*然而，作为一种常见的黑客行为，这又引入了另一个问题。如果您更仔细地注意模型的权重，在从配置文件重新加载模型之后，权重被重置！ ***我们刚刚丢失了，所有的 ImageNet 预训练参数！****

*好吧，一个快速的解决方法是使用相同的策略。我们可以在重新加载模型配置之前保存权重，并在模型正确加载后重新加载权重。*

*下面的函数完成了全部工作。您可以传递来自 [Keras 应用](https://keras.io/applications/) ***(使用 Tensorflow 2.0)*** 的任何模型，以及您想要的正则化器，它会返回正确配置的模型。请注意，在从配置文件重新加载模型之前和之后，我们是如何保存和重新加载模型权重的。*

*同样，您可以使用相同的代码添加***bias _ regulator***和***activity _ regulator***。*

*就是这样。一个快速但有希望的有用的技巧来调整你的预训练模型。*

***感谢阅读！***