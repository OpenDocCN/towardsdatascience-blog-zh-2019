<html>
<head>
<title>TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python .</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">python 中从头开始的 TF(词频)-IDF(逆文档频)。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558?source=collection_archive---------1-----------------------#2019-12-10">https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558?source=collection_archive---------1-----------------------#2019-12-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1c86868de191b17b12bf7d5ed97109eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qQgnyPLDIkUmeZKN2_ZWbQ.png"/></div></div></figure><div class=""/><h1 id="3899" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">从头开始创建 TF-IDF 模型</h1><p id="a17b" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在这篇文章中，我将解释如何用 python 从头开始实现 tf-idf 技术，这种技术用于寻找由单词组成的句子的含义，并抵消单词袋技术的无能，这种技术有利于文本分类或帮助机器阅读数字中的单词。</p><h1 id="23b6" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">目录:</h1><ul class=""><li id="5dac" class="lx ly je lb b lc ld lg lh lk lz lo ma ls mb lw mc md me mf bi translated">术语。</li><li id="b96b" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">词频(TF)。</li><li id="6962" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">文档频率。</li><li id="95d7" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">逆文档频率。</li><li id="18c5" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">用 Python 实现。</li></ul><h1 id="d74b" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1 -术语:</h1><ul class=""><li id="bb51" class="lx ly je lb b lc ld lg lh lk lz lo ma ls mb lw mc md me mf bi translated">t —术语(单词)</li><li id="bb9c" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">d-文档(一组单词)</li><li id="88ca" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">N —语料库的计数</li><li id="b9f3" class="lx ly je lb b lc mg lg mh lk mi lo mj ls mk lw mc md me mf bi translated">语料库—整个文档集</li></ul><h1 id="e0c1" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak"> 2 项频率(TF): </strong></h1><p id="b5da" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">假设我们有一组英文文本文档，并希望对哪个文档与查询最相关进行排序，“数据科学太棒了！”一种简单的开始方式是通过消除不包含所有三个词“数据”、“是”、“科学”和“棒极了”的文档，但是这仍然留下许多文档。为了进一步区分它们，我们可以计算每个术语在每个文档中出现的次数；一个术语在文档中出现的次数称为其<em class="ml">词频</em>。</p><p id="e9f2" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi mr translated"><span class="l ms mt mu bm mv mw mx my mz di"> T </span> <strong class="lb jf"> <em class="ml">文档中出现的术语的权重简单地与术语频率成比例。</em>T9】</strong></p><h1 id="8dc8" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">公式:</h1><blockquote class="na"><p id="e3ea" class="nb nc je bd nd ne nf ng nh ni nj lw dk translated"><em class="nk"> tf(t，d)= d 中 t 的计数/d 中的字数</em></p></blockquote><h1 id="c7e0" class="kb kc je bd kd ke kf kg kh ki kj kk kl km nl ko kp kq nm ks kt ku nn kw kx ky bi translated">3-文档频率:</h1><p id="92dc" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">这个<strong class="lb jf"> </strong>衡量的是文档在整套语料库中的重要性，这和 TF 非常相似。唯一的区别是 TF 是文档 d 中术语 t 的频率计数器，其中 DF 是术语 t 在文档集 n 中出现<strong class="lb jf">次</strong>的计数。换句话说，DF 是该单词出现在其中的文档的数量。如果该术语在文档中至少出现一次，我们认为出现一次，我们不需要知道该术语出现的次数。</p><blockquote class="na"><p id="bc55" class="nb nc je bd nd ne nf ng nh ni nj lw dk translated"><em class="nk">df(t)= t 在文档中的出现次数</em></p></blockquote><h1 id="a532" class="kb kc je bd kd ke kf kg kh ki kj kk kl km nl ko kp kq nm ks kt ku nn kw kx ky bi translated">4-反向文档频率(IDF):</h1><p id="fe6e" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在计算 TF 时，所有项都被认为是同等重要的。然而，众所周知，某些术语，如“是”、“的”和“那个”，可能会出现很多次，但并不重要。因此，我们需要降低频繁项的权重，同时增加罕见项的权重，通过计算 IDF，结合了一个<em class="ml">逆文档频率</em>因子，该因子减少了文档集中出现频率非常高的项的权重，并增加了出现频率很低的项的权重。</p><p id="0ba1" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">IDF 是测量术语 t 的信息量的文档频率的倒数。当我们计算 IDF 时，对于最常出现的词(例如停用词)来说，IDF 将非常低(因为停用词(例如“is ”)出现在几乎所有的文档中，并且 N/df 将给予该词非常低的值)。这最终给出了我们想要的相对权重。</p><blockquote class="na"><p id="6bf8" class="nb nc je bd nd ne nf ng nh ni nj lw dk translated"><em class="nk"> idf(t) = N/df </em></p></blockquote><p id="073b" class="pw-post-body-paragraph kz la je lb b lc no le lf lg np li lj lk nq lm ln lo nr lq lr ls ns lu lv lw im bi translated">现在，idf 很少有其他问题，在大型语料库的情况下，比如说 100，000，000，IDF 值会爆炸，为了避免这种影响，我们采用 IDF 的日志。</p><p id="27f9" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">在查询期间，当出现不在 vocab 中的单词时，df 将为 0。因为我们不能被 0 整除，所以我们通过在分母上加 1 来平滑这个值。</p><p id="6bb7" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">这是最终的公式:</p><h1 id="e9fd" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">公式:</h1><blockquote class="na"><p id="8451" class="nb nc je bd nd ne nf ng nh ni nj lw dk translated"><em class="nk"> idf(t) = log(N/(df + 1)) </em></p></blockquote><p id="944f" class="pw-post-body-paragraph kz la je lb b lc no le lf lg np li lj lk nq lm ln lo nr lq lr ls ns lu lv lw im bi translated">tf-idf 现在是评估一个单词对一个集合或语料库中的文档有多重要的正确度量。</p><h1 id="0be4" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">公式:</h1><blockquote class="na"><p id="eab2" class="nb nc je bd nd ne nf ng nh ni nj lw dk translated"><em class="nk"> tf-idf(t，d) = tf(t，d) * log(N/(df + 1)) </em></p></blockquote><h1 id="3392" class="kb kc je bd kd ke kf kg kh ki kj kk kl km nl ko kp kq nm ks kt ku nn kw kx ky bi translated">5-从头开始用 Python 实现 TF-IDF:</h1><p id="5cd1" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为了用 python 从头开始制作 TF-IDF，让我们想象不同文档中的两句话:</p><p id="9591" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">第一句话:“数据科学是 21 世纪最性感的工作”。</p><p id="9163" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">第二句话:“机器学习是数据科学的关键”。</p><p id="81bd" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">第一步，我们必须创建 TF 函数来计算所有文档的总词频。以下是代码:</p><p id="5bbd" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">首先，像往常一样，我们应该导入必要的库:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="3994" class="oc kc je ny b gy od oe l of og">import pandas as pd<br/>import sklearn as sk<br/>import math </span></pre><p id="6eb6" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">因此，让我们加载我们的句子，并将它们组合在一个集合中:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="5509" class="oc kc je ny b gy od oe l of og">first_sentence = "Data Science is the sexiest job of the 21st century"<br/>second_sentence = "machine learning is the key for data science"</span><span id="387c" class="oc kc je ny b gy oh oe l of og">#split so each word have their own string</span><span id="ae20" class="oc kc je ny b gy oh oe l of og">first_sentence = first_sentence.split(" ")<br/>second_sentence = second_sentence.split(" ")#join them to remove common duplicate words<br/>total= set(first_sentence).union(set(second_sentence))</span><span id="a811" class="oc kc je ny b gy oh oe l of og">print(total)</span></pre><p id="887c" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">输出:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="7c2d" class="oc kc je ny b gy od oe l of og">{'data', 'Science', 'job', 'sexiest', 'the', 'for', 'science', 'machine', 'of', 'is', 'learning', '21st', 'key', 'Data', 'century'}</span></pre><p id="5209" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">现在让我们添加一种方法，对两个句子使用字典键值对来计算单词:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="7cf9" class="oc kc je ny b gy od oe l of og">wordDictA = dict.fromkeys(total, 0) <br/>wordDictB = dict.fromkeys(total, 0)</span><span id="a07c" class="oc kc je ny b gy oh oe l of og">for word in first_sentence:<br/>    wordDictA[word]+=1<br/>    <br/>for word in second_sentence:<br/>    wordDictB[word]+=1</span></pre><p id="f2c8" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">现在，我们将它们放入数据帧，然后查看结果:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="6cc2" class="oc kc je ny b gy od oe l of og">pd.DataFrame([wordDictA, wordDictB])</span></pre><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/d8ba4a003a61b5e72be22ad5bdb8a786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pugWKFSw9kWDDrf_LOx3jQ.png"/></div></div></figure><p id="93fe" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">不，让我们写 TF 函数:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="53bd" class="oc kc je ny b gy od oe l of og">def computeTF(wordDict, doc):<br/>    tfDict = {}<br/>    corpusCount = len(doc)<br/>    for word, count in wordDict.items():<br/>        tfDict[word] = count/float(corpusCount)<br/>    return(tfDict)</span><span id="75cf" class="oc kc je ny b gy oh oe l of og">#running our sentences through the tf function:</span><span id="bb3e" class="oc kc je ny b gy oh oe l of og">tfFirst = computeTF(wordDictA, first_sentence)<br/>tfSecond = computeTF(wordDictB, second_sentence)</span><span id="3b82" class="oc kc je ny b gy oh oe l of og">#Converting to dataframe for visualization</span><span id="de79" class="oc kc je ny b gy oh oe l of og">tf = pd.DataFrame([tfFirst, tfSecond])</span></pre><p id="74a2" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">这是预期的输出:</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/934d2d2cc1d378d36f878eee26597943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3WwlcabDKuIljpk2ec1r-w.png"/></div></div></figure><p id="ff81" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">这就是 TF 公式的全部内容，我想谈谈停用词，我们应该删除它们，因为它们是最常见的词，不会给文档向量带来任何额外的价值。事实上，删除这些将增加计算和空间效率。</p><p id="e3da" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">nltk 库有一个下载停用词的方法，所以我们可以直接使用 nltk 库，遍历所有的词并删除停用词，而不是自己显式地提到所有的停用词。有许多有效的方法可以做到这一点，但我只给出一个简单的方法。</p><p id="1a6f" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">这些是英语中停用词的例子:</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/437960a2e96ab092234fee4a45b6dc15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1NmayfziRv8QKKUw4dt_w.png"/></div></div></figure><p id="dd52" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">这是一个简单的代码，可以下载停用词并删除它们。</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="f0cd" class="oc kc je ny b gy od oe l of og">import nltk</span><span id="048e" class="oc kc je ny b gy oh oe l of og">nltk.download('stopwords')</span><span id="b451" class="oc kc je ny b gy oh oe l of og">from nltk.corpus import stopwords</span><span id="0006" class="oc kc je ny b gy oh oe l of og">stop_words = set(stopwords.words('english'))</span><span id="216a" class="oc kc je ny b gy oh oe l of og">filtered_sentence = [w for w in wordDictA if not w in stop_words]</span><span id="fdb6" class="oc kc je ny b gy oh oe l of og">print(filtered_sentence)</span></pre><p id="33e7" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">输出:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="5097" class="oc kc je ny b gy od oe l of og">['data', 'Science', 'job', 'sexiest', 'science', 'machine', 'learning', '21st', 'key', 'Data', 'century']</span></pre><p id="a7c6" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">现在我们已经完成了 TF 部分，接下来是 IDF 部分:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="5cec" class="oc kc je ny b gy od oe l of og">def computeIDF(docList):<br/>    idfDict = {}<br/>    N = len(docList)<br/>    <br/>    idfDict = dict.fromkeys(docList[0].keys(), 0)<br/>    for word, val in idfDict.items():<br/>        idfDict[word] = math.log10(N / (float(val) + 1))<br/>        <br/>    return(idfDict)</span><span id="8a7b" class="oc kc je ny b gy oh oe l of og">#inputing our sentences in the log file<br/>idfs = computeIDF([wordDictA, wordDictB])</span></pre><p id="fe34" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">现在我们实施 idf 公式，让我们以计算 TFIDF 结束</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="ca5f" class="oc kc je ny b gy od oe l of og">def computeTFIDF(tfBow, idfs):<br/>    tfidf = {}<br/>    for word, val in tfBow.items():<br/>        tfidf[word] = val*idfs[word]<br/>    return(tfidf)<br/>#running our two sentences through the IDF:</span><span id="5953" class="oc kc je ny b gy oh oe l of og">idfFirst = computeTFIDF(tfFirst, idfs)<br/>idfSecond = computeTFIDF(tfSecond, idfs)<br/>#putting it in a dataframe<br/>idf= pd.DataFrame([idfFirst, idfSecond])<br/>print(idf)</span></pre><p id="343b" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">输出:</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/26248864b0d80bff63b7e9aec3f8ad9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYSSzfKKCHYSkZXsNAITzA.png"/></div></div></figure><p id="af37" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">那是许多工作。但是，如果将来要求您从头开始编写 TF-IDF 代码，这是很方便的。然而，由于 sklearn 库，这可以简单得多。让我们看看下面的例子:</p><pre class="nt nu nv nw gt nx ny nz oa aw ob bi"><span id="a357" class="oc kc je ny b gy od oe l of og">#first step is to import the library</span><span id="30e2" class="oc kc je ny b gy oh oe l of og">from sklearn.feature_extraction.text import TfidfVectorizer<br/>#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase</span><span id="04e0" class="oc kc je ny b gy oh oe l of og">firstV= "Data Science is the sexiest job of the 21st century"<br/>secondV= "machine learning is the key for data science"</span><span id="93b6" class="oc kc je ny b gy oh oe l of og">#calling the TfidfVectorizer<br/>vectorize= TfidfVectorizer()<br/>#fitting the model and passing our sentences right away:</span><span id="84de" class="oc kc je ny b gy oh oe l of og">response= vectorize.fit_transform([firstV, secondV])</span></pre><p id="38e5" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">这是预期的输出:</p><figure class="nt nu nv nw gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3c8e24c5c02aeafacfd1d062f9bfc241.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*djorfP9hUMLZ2iOVmShdyw.png"/></div></figure><h1 id="4cee" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">总结:</h1><p id="7a3a" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在这篇文章中，我们将解释如何使用 python 和一种称为词频的自然语言处理(NLP)技术—逆文档频率(<strong class="lb jf"> tf-idf </strong>)来总结文档。</p><p id="3598" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">我们将使用<a class="ae on" href="http://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> sklearn </a>和<a class="ae on" href="http://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> nltk </a>来完成这项任务。</p><p id="f5f6" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated">记住你可以在我的 github 库<a class="ae on" href="https://github.com/Yassine-Hamdaoui/Tf-Idf" rel="noopener ugc nofollow" target="_blank">这里</a>找到完整的工作代码。</p><p id="1642" class="pw-post-body-paragraph kz la je lb b lc mm le lf lg mn li lj lk mo lm ln lo mp lq lr ls mq lu lv lw im bi translated"><strong class="lb jf">感谢</strong>阅读，我很高兴讨论您可能有的任何问题或更正:)如果您想讨论机器学习或其他任何问题，请在<a class="ae on" href="https://www.linkedin.com/in/yassine-hamdaoui/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我。</p></div></div>    
</body>
</html>