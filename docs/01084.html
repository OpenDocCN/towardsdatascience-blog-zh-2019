<html>
<head>
<title>AI — The End of the WoRLd?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AI——世界末日？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-the-end-of-the-world-9277ab8bd765?source=collection_archive---------24-----------------------#2019-02-19">https://towardsdatascience.com/ai-the-end-of-the-world-9277ab8bd765?source=collection_archive---------24-----------------------#2019-02-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f671" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">—使用强化学习(并让他们玩视频游戏)，教虚拟代理像人类一样学习</h2></div><p id="296a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">埃隆·马斯克对人工智能接管世界感到恐惧。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="0a25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但这种担心真的有道理吗？真的有什么可担心的吗？如果世界上最聪明的人之一被该领域最近的指数级发展吓坏了，你也应该害怕吗？</p><p id="1fab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果人工智能系统只是基于一些聪明的数学和编程原理，我们真的有什么好担心的吗？</p><p id="2f6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">是的。</p><p id="5bb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">没有。</p><p id="3273" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有点复杂。</p><p id="eee5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我解释一下。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="6209" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">为什么埃隆吓得发抖</h1><p id="f822" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">当马斯克先生谈到机器获胜时，他最有可能指的是已经学会使用强化学习(RL)模型进行操作的系统。他已经警告我们关于<a class="ae mm" href="https://www.youtube.com/watch?v=MuWWZ91-G6w" rel="noopener ugc nofollow" target="_blank"> Deepmind 的 AlphaGo </a>的潜在危险，并建立了<a class="ae mm" href="https://openai.com/about/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>来创造一条通往人工通用智能(AGI)的安全之路，后者也主要专注于强化学习研究。</p><p id="7a0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些发展是一件大事。目前大多数常见的人工智能系统只被编程为具有完成一项特定任务的能力，如检测物体或生成猫的图片。它们是使用监督学习技术的狭义系统。因为我们知道正确的答案是什么样的，所以我们可以预测他们理想情况下应该给出什么样的输出。我们知道它试图分类的对象是一只狗，或者如果它被这样训练，系统将总是生成一只猫的图片。</p><p id="fd44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是当我们不知道正确答案是什么的时候呢？</p><h1 id="55e6" class="lp lq iq bd lr ls mo lu lv lw mp ly lz jw mq jx mb jz mr ka md kc ms kd mf mg bi translated">输入:强化学习</h1><p id="e3f5" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在这里，<strong class="kh ir">我们只能通过试错</strong>来学习。就像，我们给代理一个任务，让它知道如何去做，但是我们甚至不知道正确的方法去做。</p><p id="3a82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数研究人员认为，这是我们创建通用人工智能(AGI)系统的方式，它可以知道如何完成任何任务。包括接管人类。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/422fa03495aa73975b2d0bb088c74402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jy-HtP18ZXiZpOiFqTEubw.jpeg"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">If RL systems get out of control, we could have SkyNet for real</figcaption></figure><p id="c59b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么 RL 真的能终结人类吗？不，至少还没有。普通智力是复杂的，人类经过数百万年的进化才得以发展。在天网成为现实之前，还需要更多的研究。</p><p id="a4de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前，它最可靠的功能是玩视频游戏。让我们稍微分解一下这个过程，并弄清楚构建一个 RL 代理实际需要什么。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="a866" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">直观理解强化学习</h1><p id="be7c" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">假设你拿起你的第一个街机游戏，却不知道怎么玩。没有人真的会阅读说明，所以你的学习过程基本上就是不停地按按钮，然后看看你的角色在屏幕上发生了什么。除了尽可能获得高分之外，你在没有任何先验知识或预期目标的情况下想出了该做什么。</p><p id="bb06" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了我们用更多的行话描述他们的过程(行话是斜体的，为了让你阅读愉快)，这几乎就是 RL 代理玩视频游戏所做的事情。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/35d87091afa5e36cab7d13287f677a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/1*xJmWwel3xP7kZs0nu52DYQ.gif"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">Human or AI? Who knows? (I do. It’s an RL agent I trained.)</figcaption></figure><p id="9e59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个 RL <em class="mn">特工</em> <strong class="kh ir"> </strong>基本上就是游戏中的玩家——射击敌人的绿色小飞船。</p><p id="e0d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它的<em class="mn">环境</em> <strong class="kh ir"> </strong>就是它周围的一切——游戏或地图包括敌人和障碍。</p><p id="f1ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它可以采取<em class="mn">动作</em> <strong class="kh ir"> </strong>以便与其环境互动。在像太空入侵者这样的视频游戏中，可能的动作是向左移动、向右移动、射击或什么都不做。</p><p id="836b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当它采取行动(不行动仍然是行动)时，<strong class="kh ir">环境给代理一个<em class="mn">奖励</em>，这个奖励可以是正的<em class="mn">也可以是负的</em>，这取决于行动的质量</strong>和将来能得到多少奖励。这就像对代理采取的任何行动的反馈——积极的行动最终会导致更高的分数，而消极的行动会导致失去生命。</p><p id="b9c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每当代理完成这个循环，它就转换到一个新的<em class="mn">状态。</em> <strong class="kh ir"> </strong>这就像代理的位置——根据它的状态它可以访问环境的不同部分。就像如果我们向左移动，所有的敌人都在左边，我们就可以射杀更多的敌人。理想情况下，我们希望代理人最大化长期回报(得分)。</p><p id="6db6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是代理实际上是如何知道该做什么的呢？它如何知道从不同的场景中采取什么行动来获得可能的最高分？</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="a696" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated"><strong class="ak">输入:Q-学习</strong></h1><p id="416c" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">RL 代理学习一个<em class="mn">策略</em> <strong class="kh ir"> </strong>来决定他们在不同场景下的行为。他们学习 Q 值来计算从某个状态采取某个行动有多好。行动的质量取决于我们采取行动后期望从我们所处的状态得到多少回报。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nf"><img src="../Images/411bbf5caa49d64366ea4ac15362db3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ldELhO93oGqq5YahYMtogQ.png"/></div></div></figure><p id="82cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，采取向右移动的动作将对应于高 Q 值，因为它使代理处于一个位置，可以射击更多的敌人，而这些敌人不会为了获得更高的分数而向它开火。但向左移动没有意义，也不会提高代理人获得更多长期奖励的能力，因为没有任何敌人可以射击。</p><p id="87ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，从这个博弈<em class="mn">状态</em> <strong class="kh ir"> </strong>中，代理人在中间，敌人在右边，代理人最有可能采取向右移动的<em class="mn">动作</em>。这个状态-动作对将被存储在一个 Q 表中，下次代理遇到同样的情况时，它将知道该做什么。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="ef1e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">现在我们开始'<em class="ng">深入'</em> —进入:用 DQNs 进行深度 Q 学习</h1><p id="e909" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">实际上，遍历数十亿个可能的状态-动作对组合对于任何真正严肃的任务(如玩视频游戏)来说几乎是不可能的，所以我们将一些深度学习与经典的 Q-学习模型结合起来，以获得迄今为止最酷的 RL 时髦词——深度 Q-网络。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nh"><img src="../Images/4cf28ca8b75b813215b90fc91148b642.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Y5a2LGmdX7Lo3chri5oCQ.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk">DQN’s basically 1) see a game screen, 2) think about what to do, and 3) do something</figcaption></figure><p id="3eb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用 CNN——一种监督学习模型，让代理看到游戏中发生的事情。它通过 3 个卷积层查看不同的空间和纹理特征，如形状和边缘，并在 2 个完全连接的层中进行一些处理，以确定应该做出什么决定。</p><p id="624b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个<strong class="kh ir">允许代理学习某些场景之间的相关性，因此它的行为可以在它们之间推广</strong>，而不局限于一个精确的状态-动作对。</p><p id="4a1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么模型实际上是如何学习和提高的呢？</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="e55b" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">大数学:损失函数和经验回放</h1><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi ni"><img src="../Images/b362df1578b1aba3a9f3742dc9f850d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EeRhsqTJ-JVN5Bdw9V7hpg.png"/></div></div></figure><p id="fcdf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们把它分解开来，这其实并不难理解。</p><p id="6718" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的<em class="mn">目标网络</em>和实际网络几乎相同——唯一真正的区别是目标网络更有信心。它知道基于某个策略该做什么(大脑在给定的状态下计算出该做什么动作)，并且总是 100%确定什么是正确的动作。我们的 Q-network 对自己不太确定。它将输出执行某个动作的概率——如果这是正确的动作，那么我们使用优化器让网络学习执行更多这些动作。错了就少拿点。</p><p id="9644" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着我们的 Q 网络训练和变得更好，它对于每个状态-动作对的 Q 值变得更高。因此，随着我们网络的改善，损耗变得更低，这是有道理的，因为损耗是衡量网络表现有多差的一个指标。<strong class="kh ir">少亏=少坏=我们的网络玩的更好！</strong></p><p id="085e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每当我们的网络经历 1)从一个状态开始，2)做一个动作，3)获得奖励，4)过渡到下一个状态的序列时，我们都会记住这个序列，并在未来的一个叫做<em class="mn">体验回放的过程中学习。</em>那些存储的经验被随机挑选出来，并作为参数(s，a，r，s `)被抛入损失函数以更新网络。</p><p id="a2d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们随机选择体验，以确保网络不会对学习特定场景感到太舒服。例如，如果我们训练我们的代理在所有敌人都在屏幕左侧的场景中扮演太空入侵者，它通常会学习如何向左移动并射击。但如果我们太频繁地这样做，当它看到右边的敌人时，它会不知道该怎么做，因为它以前从未见过。它只会做它知道的事情——向左移动并射击。但是，如果它从随机的过去经验中学习，我们可以 1)对相同的数据多次训练网络，2)它将学习如何在各种情况下表现。</p></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="e18a" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">酷！(理论上)——但是 AI 真的会接管世界吗？</h1><p id="601f" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">问题是，RL 模型可能可以做人类能做的任何事情——问题只是我们能以多快的速度推进这个领域，以便让它们做所有和我们一样的任务。一旦我们到了那一步，接下来的问题就是制造一个人工智能系统，它有可能学会同时完成所有这些任务。</p><p id="9583" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在在人工智能领域有一些非常聪明的人在争论人工智能的进步将被用于什么方向。伦理辩论不仅出现在人工通用智能(可以学习做任何任务的人工智能代理)上，许多人认为这将通过 RL 策略实现，还出现在<a class="ae mm" href="https://www.youtube.com/watch?v=gLoI9hAX9dw" rel="noopener ugc nofollow" target="_blank"> DeepFake 算法</a>和<a class="ae mm" href="https://blog.openai.com/better-language-models/" rel="noopener ugc nofollow" target="_blank"> OpenAI 的文本生成模型</a>上，如果不道德地使用，它们可能会真正扰乱我们世界的本质。</p><p id="ac2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种微妙的情况下，没有什么是确定的。只要记住墨菲定律:</p><blockquote class="nj"><p id="8ee2" class="nk nl iq bd nm nn no np nq nr ns la dk translated">“任何可能出错的事情都会出错”</p></blockquote></div><div class="ab cl li lj hu lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="ij ik il im in"><h1 id="d2b9" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated"><strong class="ak">关键要点</strong></h1><ul class=""><li id="12b9" class="nt nu iq kh b ki mh kl mi ko nv ks nw kw nx la ny nz oa ob bi translated">强化学习是一个主体与其环境互动的过程，以找出在不同状态下采取的最佳行动，从而最大化长期回报</li><li id="ae23" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated">Q- Learning 帮助我们确定从某种状态采取某种行动的质量</li><li id="32d0" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated">深度 Q 网络允许我们使用卷积层结构来计算游戏状态，并基于通过计算我们的理想目标网络和我们的实际 Q 网络之间的平方差发现的损失进行更新</li><li id="102a" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated">损失函数的参数是由状态、动作、奖励和下一状态组成的不同体验，这些体验是从过去体验的数据集中随机采样的，以便更好地训练</li><li id="cce2" class="nt nu iq kh b ki oc kl od ko oe ks of kw og la ny nz oa ob bi translated">RL 模型有潜力创建能够像人类一样学习如何完成各种任务的通用系统😮</li></ul><p id="01be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢你的阅读，我希望你能学到一些关于 RL 的有趣的东西！联系 <a class="ae mm" href="https://www.linkedin.com/in/aadillpickles/" rel="noopener ugc nofollow" target="_blank"> <em class="mn"> LinkedIn </em> </a> <em class="mn">并访问我的</em> <a class="ae mm" href="https://aadilali.com" rel="noopener ugc nofollow" target="_blank"> <em class="mn">网站</em> </a> <em class="mn">，在那里您可以注册我的</em> <a class="ae mm" href="https://aadilali.us19.list-manage.com/subscribe?u=b1a0190d700080e749848161b&amp;id=75ee2a93f6" rel="noopener ugc nofollow" target="_blank"> <em class="mn">简讯</em> </a> <em class="mn">以获得我的每月进展更新！</em></p></div></div>    
</body>
</html>