<html>
<head>
<title>Review: SegNet (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:SegNet(语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96?source=collection_archive---------4-----------------------#2019-02-10">https://towardsdatascience.com/review-segnet-semantic-segmentation-e66f2e30fb96?source=collection_archive---------4-----------------------#2019-02-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7054" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">编码器解码器架构使用最大池索引进行上采样，性能优于 FCN、DeepLabv1 和 DeconvNet</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/67b9bb82c5a0ec1928514675676b0b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*k8ejti9_6CHwxzFQ.gif"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">SegNet by Authors (</strong><a class="ae ks" href="https://www.youtube.com/watch?v=CxanE_W46ts" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=CxanE_W46ts</a>)</figcaption></figure><p id="fed9" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">在</span>这个故事中，<strong class="kv ir">剑桥大学</strong>的<strong class="kv ir"> SegNet </strong>做了简要回顾。最初，它被提交到 2015 年 CVPR，但最终它没有在 CVPR 出版(但它是<strong class="kv ir"> 2015 arXiv </strong> tech report 版本，仍然获得了<strong class="kv ir"> 100 多次引用</strong>)。而是发表在<strong class="kv ir"> 2017 TPAMI </strong>超过<strong class="kv ir"> 1800 次引用</strong>。而现在第一作者已经成为 Magic Leap Inc .(<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----e66f2e30fb96--------------------------------" rel="noopener" target="_blank">Sik-Ho Tsang</a>@ Medium)深度学习和 AI 的总监</p><p id="74bc" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">以下是作者的演示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ma mb l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="ak">SegNet by Authors (</strong><a class="ae ks" href="https://www.youtube.com/watch?v=CxanE_W46ts" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=CxanE_W46ts</a>)</figcaption></figure><p id="50b8" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">还有一个有趣的演示，我们可以选择一个随机的图像，甚至上传我们自己的图像来尝试 SegNet。我做了如下尝试:</p><ul class=""><li id="a508" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated"><a class="ae ks" href="http://mi.eng.cam.ac.uk/projects/segnet/demo.php" rel="noopener ugc nofollow" target="_blank">http://mi.eng.cam.ac.uk/projects/segnet/demo.php</a></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/7665c343ec619a6430d7d32d115e266e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TX4rkqNaqX_aA-CAId8WPw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">The segmentation result for a road scene image that I found from internet</strong></figcaption></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="fb6c" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">概述</h1><ol class=""><li id="819f" class="mc md iq kv b kw np kz nq lc nr lg ns lk nt lo nu mi mj mk bi translated"><strong class="kv ir">编码器解码器架构</strong></li><li id="62f2" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo nu mi mj mk bi translated"><strong class="kv ir">与 DeconvNet 和 U-Net 的区别</strong></li><li id="bca1" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo nu mi mj mk bi translated"><strong class="kv ir">结果</strong></li></ol></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="2e18" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated"><strong class="ak"> 1。编码器解码器架构</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oa"><img src="../Images/65fad409764acb124c81a7fe625817ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qIwQ7drLTf08gami25QDw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">SegNet: Encoder Decoder Architecture</strong></figcaption></figure><ul class=""><li id="8ea0" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">SegNet 有一个<strong class="kv ir">编码器</strong>网络和一个相应的<strong class="kv ir">解码器</strong>网络，后面是最终的逐像素分类层。</li></ul><h2 id="3999" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">1.1.编码器</h2><ul class=""><li id="79cd" class="mc md iq kv b kw np kz nq lc nr lg ns lk nt lo mh mi mj mk bi translated">在编码器处，执行卷积和最大池。</li><li id="e81f" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">VGG-16 有 13 个卷积层。(原始的完全连接的层被丢弃。)</li><li id="f2a5" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">进行 2×2 最大汇集时，会存储相应的最大汇集索引(位置)。</li></ul><h2 id="36a8" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">1.2.解码器</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/055e9d2fe6aef13215dfd4d369223171.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*wdNu8Wd2HOLsOfRUInwwaA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Upsampling Using Max-Pooling Indices</strong></figcaption></figure><ul class=""><li id="0795" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">在解码器处，执行上采样和卷积。最后，每个像素都有一个 softmax 分类器。</li><li id="2199" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">在上采样期间，如上所示，调用相应编码器层的最大池索引来进行上采样。</li><li id="89fb" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">最后，使用 K 类 softmax 分类器来预测每个像素的类别。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="8c52" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated"><strong class="ak"> 2。与 DeconvNet 和 U-Net 的区别</strong></h1><p id="8a4d" class="pw-post-body-paragraph kt ku iq kv b kw np jr ky kz nq ju lb lc oo le lf lg op li lj lk oq lm ln lo ij bi translated"><a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e"> DeconvNet </a>和<a class="ae ks" href="http://U-Net" rel="noopener ugc nofollow" target="_blank"> U-Net </a>的结构与 SegNet 类似。</p><h2 id="ac87" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">2.1.与<a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">解除配置</a>的区别</h2><ul class=""><li id="c597" class="mc md iq kv b kw np kz nq lc nr lg ns lk nt lo mh mi mj mk bi translated">使用类似的被称为解组的上采样方法。</li><li id="a98e" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">然而，存在使模型更大的完全连接的层。</li></ul><h2 id="a741" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">2.2.与<a class="ae ks" href="http://U-Net" rel="noopener ugc nofollow" target="_blank"> U-Net </a>的差异</h2><ul class=""><li id="9ffc" class="mc md iq kv b kw np kz nq lc nr lg ns lk nt lo mh mi mj mk bi translated">它用于生物医学图像分割。</li><li id="78db" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">代替使用汇集索引，整个特征映射从编码器传输到解码器，然后连接以执行卷积。</li><li id="1228" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">这使得模型更大，需要更多的内存。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="286c" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">3.结果</h1><ul class=""><li id="4391" class="mc md iq kv b kw np kz nq lc nr lg ns lk nt lo mh mi mj mk bi translated">尝试了两个数据集。一个是用于道路场景分割的 CamVid 数据集。一个是用于室内场景分割的 SUN RGB-D 数据集。</li></ul><h2 id="c8c7" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">3.1.用于道路场景分割的 CamVid 数据集</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi or"><img src="../Images/cb5c3f85eef637e59210a4d3f4920638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GHGbOKojBBvtxyBlG8XAlw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Compared With Conventional Approaches on CamVid dataset for Road Scene Segmentation</strong></figcaption></figure><ul class=""><li id="cc90" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">如上所示，SegNet 在许多类上都取得了非常好的结果。它还获得了最高的班级平均水平和全球平均水平。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi os"><img src="../Images/3237d4c3434cd0cc15c100e75fa784c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSanrhujo09-PYNneyPBvw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Compared With Deep Learning Approaches on CamVid dataset for Road Scene Segmentation</strong></figcaption></figure><ul class=""><li id="4cdd" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">SegNet 获得最高的全局平均精度(G)、类平均精度(C)、mIOU 和边界 F1-measure (BF)。它胜过<a class="ae ks" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>、<a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 </a>和<a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e"> DeconvNet </a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ot"><img src="../Images/3a8776f46eb31d3db3b97f90ca3ce2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YEl-0zr77hfUetR-HmnJw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Qualitative Results</strong></figcaption></figure><h2 id="5087" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">3.2.用于室内场景分割的太阳 RGB-D 数据集</h2><ul class=""><li id="7f65" class="mc md iq kv b kw np kz nq lc nr lg ns lk nt lo mh mi mj mk bi translated">仅使用 RGB，不使用深度(D)信息。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ou"><img src="../Images/c5fc6163308fd365dd03b94922d009c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tB_mufCuOLK8imyUHceSVQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Compared With Deep Learning Approaches on SUN RGB-D Dataset for Indoor Scene Segmentation</strong></figcaption></figure><ul class=""><li id="21ab" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">同样，SegNet 的表现优于<a class="ae ks" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>、<a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e"> DeconvNet </a>和<a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 </a>。</li><li id="d63b" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">SegNet 对于 mIOU 来说只比<a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 </a>差了一点点。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ov"><img src="../Images/b68578759ab276e1ef43c737fd74557f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_7h5yLmGj4gbh-aD7_-Ydw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Class Average Accuracy for Different Classes</strong></figcaption></figure><ul class=""><li id="cf47" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">大规模班级的精确度更高。</li><li id="19c9" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">小规模班级的精确度较低。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ow"><img src="../Images/bc778405e1593fa038923c3a5b7d626d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0WEadFwdQ1JWyHaH7UoPw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Qualitative Results</strong></figcaption></figure><h2 id="fb52" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">3.3.记忆和推理时间</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ox"><img src="../Images/0b75045cc6ffcac24ff2763acfbac3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q9-COyyJ2d8oK6XhuKzb-g.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Memory and Inference Time</strong></figcaption></figure><ul class=""><li id="25d8" class="mc md iq kv b kw kx kz la lc me lg mf lk mg lo mh mi mj mk bi translated">SegNet 比<a class="ae ks" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>和<a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 </a>要慢，因为 SegNet 包含解码器架构。而且它比<a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e"> DeconvNet </a>更快，因为它没有完全连接的层。</li><li id="cfd6" class="mc md iq kv b kw nv kz nw lc nx lg ny lk nz lo mh mi mj mk bi translated">并且 SegNet 在训练和测试期间都具有低的内存需求。而且型号尺寸远小于<a class="ae ks" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>和<a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convent</a>。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="de3b" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">参考</h2><p id="9eaa" class="pw-post-body-paragraph kt ku iq kv b kw np jr ky kz nq ju lb lc oo le lf lg op li lj lk oq lm ln lo ij bi translated">【2015 arXiv】【seg net】<br/><a class="ae ks" href="https://arxiv.org/abs/1505.07293" rel="noopener ugc nofollow" target="_blank">seg net:一种深度卷积编码器-解码器架构，用于鲁棒的语义像素式标记</a></p><p id="3317" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">【2017 TPAMI】【SegNet】<br/><a class="ae ks" href="https://arxiv.org/abs/1511.00561" rel="noopener ugc nofollow" target="_blank">SegNet:一种用于图像分割的深度卷积编解码架构</a></p><h2 id="93e9" class="ob my iq bd mz oc od dn nd oe of dp nh lc og oh nj lg oi oj nl lk ok ol nn om bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kt ku iq kv b kw np jr ky kz nq ju lb lc oo le lf lg op li lj lk oq lm ln lo ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(们)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">物体检测<br/></strong><a class="ae ks" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae ks" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae ks" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae ks" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae ks" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae ks" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae ks" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a> yolo 9000[<a class="ae ks" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolov 3</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">语义切分<br/></strong><a class="ae ks" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae ks" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae ks" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae ks" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae ks" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">生物医学图像分割<br/></strong>[<a class="ae ks" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae ks" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae ks" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a></p><p id="3134" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">实例分割<br/>T32】[<a class="ae ks" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae ks" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">)( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )(</p></div></div>    
</body>
</html>