<html>
<head>
<title>Risks and Caution on applying PCA for Supervised Learning Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对监督学习问题应用主成分分析的风险和注意事项</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/risks-and-caution-on-applying-pca-for-supervised-learning-problems-d7fac7820ec3?source=collection_archive---------14-----------------------#2019-09-10">https://towardsdatascience.com/risks-and-caution-on-applying-pca-for-supervised-learning-problems-d7fac7820ec3?source=collection_archive---------14-----------------------#2019-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="64d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">合著者:<a class="ko kp ep" href="https://medium.com/u/197eb9da7059?source=post_page-----d7fac7820ec3--------------------------------" rel="noopener" target="_blank">阿姆兰·乔蒂·达斯</a>，<a class="ko kp ep" href="https://medium.com/u/b1e0d684eddd?source=post_page-----d7fac7820ec3--------------------------------" rel="noopener" target="_blank">赛·亚斯旺思</a></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/f530e37e72d0e1fab0fe5169bae3cf6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-m5U4xSpny5W56yKsu8RyQ.jpeg"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk"><a class="ae lg" href="https://unsplash.com/s/photos/sparse-deep-learning" rel="noopener ugc nofollow" target="_blank">Reference</a></figcaption></figure><p id="c11a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">高维空间及其诅咒</strong></p><p id="0d58" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在处理通常为高维数据的真实数据集时，维数灾难是一个非常关键的问题。随着特征空间维度的增加，配置的数量可以指数增长，因此观察覆盖的配置的数量减少。</p><p id="0b04" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这种情况下，主成分分析在有效地减少数据的维度，同时尽可能多地保留数据集中存在的变化方面起着主要作用。</p><p id="2571" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在深入实际问题之前，我们先对主成分分析做一个非常简单的介绍。</p><p id="83c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">主成分分析-定义</strong></p><p id="4d52" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">P <em class="lh">主成分分析(PCA) </em>的中心思想是降低由大量相关变量组成的数据集的维度，同时保留数据集中存在的最大可能变化。</p><p id="14c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们定义一个对称矩阵 A，</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi li"><img src="../Images/842e9aee8611976e42871ee0d327ba0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*QbOwE8ofnVIKOl9hZO4pLQ.png"/></div></figure><p id="6da4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 X 是独立变量的 m×n 矩阵，m 是列数，n 是数据点数。矩阵 A 可以分解成以下形式</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/511a0426fb4bdb4cf181176a9e98c64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*K1fWMnhsebywt9jXm5ddWw.png"/></div></figure><p id="81ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 D 是对角矩阵，E 是按列排列的 A 的特征向量矩阵。</p><p id="87dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">x 的主成分(PCs)是 XX <strong class="js iu"> ᵀ </strong>的特征向量，这表明特征向量/主成分的方向取决于独立变量(x)的变化。</p><p id="4006" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">为什么盲目应用 PCA 是监督问题中的魔咒？？？？</strong></p><p id="40c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在回归中使用主成分分析在文献中受到了很多关注，并且作为处理多重共线性的方法被广泛使用。</p><p id="ed81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是随着主成分回归的使用，关于主成分和它们各自的重要性顺序对反应变量的解释能力有许多误解。</p><p id="5f6f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在各种论文和书籍中多次出现的常见谬误是，在受监督的主成分回归框架中，具有低特征值的独立变量的主成分在解释响应变量时不起任何作用，这将我们带到本博客的目的，即在解释响应变量时，证明具有低特征值的成分可能与具有较大特征值的主成分一样重要，甚至重要得多。</p><p id="fabd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面列出了中指出的一些例子</p><p id="4e72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1].<strong class="js iu"> Mansfield 等人(1977 年，第 38 页)</strong>提出，如果仅删除方差较小的成分，则回归中的预测性损失非常小。</p><p id="0685" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2].在<strong class="js iu"> Gunst 和 Mason (1980) </strong>的书中，12 页致力于主成分回归，并且大多数讨论假设删除主成分仅仅基于它们的方差。(第 327-328 页)。</p><p id="8929" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3].<strong class="js iu"> Mosteller 和 Tukey(1977 年，第 397-398 页)</strong>类似地认为，方差小的成分在回归中不太可能是重要的，显然是基于自然是“狡猾的”而不是“完全平均的”。</p><p id="aaa1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4].<strong class="js iu"> Hocking(1976 年，第 31 页)</strong>更坚定地定义了在基于方差的回归中保留主成分的规则。</p><p id="9c82" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">理论解释和理解</strong></p><p id="2ece" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们给你一个上述假设的适当的数学证明，然后我们可以用几何可视化和模拟来解释直觉。</p><p id="09d5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">比方说</p><p id="a801" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Y —响应变量</p><p id="4aee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">X —设计矩阵—特征空间矩阵</p><p id="650c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">z——X 的标准化版本</p><p id="54af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让𝜆₁≥𝜆₂&gt;….≥ 𝜆p 是 Z <strong class="js iu"> ᵀ </strong> Z(相关矩阵)的特征值，v 是相应的特征向量，那么在 W = ZV 中，w 中的列将代表 z 的主分量。在主分量回归中执行的标准方法是回归 y 上的前 m 个 PC，该问题可以通过下面的定理及其解释[2]看出。</p><p id="5b8b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lh">定理:</em> </strong></p><p id="261e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">设 W= (W₁,…,Wp)是 x 的 PCs。现在考虑回归模型</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/cf254c319b441e1c193c8489d56b31d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*aiFxze7-nmOsCbEKZSunFQ.png"/></div></figure><p id="6fcd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果回归系数的真实向量<strong class="js iu"> 𝛽 </strong>是在 z 的 j <strong class="js iu"> ᵗʰ </strong>特征向量<strong class="js iu"> ᵀ </strong> Z 的方向上，那么当 y 在 w 上回归时，j <strong class="js iu"> ᵗʰ </strong> PC Wⱼ将独自对拟合贡献一切，而其余 PC 将什么都不贡献。</p><p id="f7c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lh">证明:</em> </strong>设 V=(V₁,…,Vp)是包含 z 的特征向量的矩阵<strong class="js iu"> ᵀ </strong> Z .那么</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ll"><img src="../Images/8b8e77402165d1b84a158b316c8d2107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccgyoPueyfrNxL2MXe0jAA.png"/></div></div></figure><p id="a527" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果<strong class="js iu"> 𝛽 </strong>在 j <strong class="js iu"> ᵗʰ </strong>特征向量 Vⱼ的方向，那么<strong class="js iu"> Vⱼ = a𝛽 </strong>，其中 a 是非零标量。因此<strong class="js iu"> 𝜃j = Vⱼᵀ𝛽 = a𝛽ᵀ𝛽，𝜃ᴋ = Vᴋᵀ𝛽 = 0 </strong>，每当<strong class="js iu"> k≠j </strong>。因此，<strong class="js iu"> 𝜃ᴋ </strong>对应<strong class="js iu"> Wᴋ </strong>的回归系数等于零，对于<strong class="js iu"> k≠j，</strong>因此</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/2ae97ffce039bd884512e3ad337f96bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*qOlJTSu4ldBkfKNeovwt-A.png"/></div></figure><p id="5acf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为，变量<strong class="js iu"> Wᴋ </strong>不产生平方和的任何减少当且仅当它的回归系数为零时，那么<strong class="js iu"> Wj </strong>将独自贡献所有来拟合，而剩余的 PC 将不贡献任何东西。</p><p id="afeb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">几何意义和模拟</strong></p><p id="06a3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们现在做模拟，并有数学直觉的几何理解。已经使用二维特征空间(X)和单个响应变量的模拟说明了该解释，使得直观地理解假设变得容易。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ln"><img src="../Images/0d3b2d52f2774c823da8f3f9d9250184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGQrff_Blr9DRM1TeaiKdQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 1 : Univariate and Bivariate plots for simulated variable X1 and X2</figcaption></figure><p id="a6db" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在模拟的第一步中，设计特征空间已经从变量之间具有非常高相关性的多元正态分布中模拟出来，并且实现了 PCA。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi lo"><img src="../Images/d823cc38a74e7afb325cec73cb01c6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*bMJxxjEUCjsmcaHsrd0KWw.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 2 : Correlation heat-map for PC1 and PC2</figcaption></figure><p id="d873" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以清楚地看出，电脑之间绝对没有关联。第二步是模拟响应变量 Y 的值，使得 PCs 上 Y 系数的方向是第二主分量的方向。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/365b3184467ff5a75a50abdf81979e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*M1Hle4gcBiWcAEQE8RQUSQ.png"/></div></figure><p id="9b64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦响应变量被模拟，相关矩阵看起来就像这样。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi lq"><img src="../Images/6c1a772b634a6a2ff666f3ba1a8cf00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdQPLJPd1FNqGgCC0vgV_Q.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 3 : Correlation heat-map for simulated variable Y and PC1 and PC2</figcaption></figure><p id="621c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从图中可以清楚地看出，y 和 PC2 而不是 PC1 之间有很高的相关性，这证明了我们的假设。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/cf13acb857e7a3149d897e09c0d07fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zNqXro4p5Sl3zuA06TYRhg.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 4 : Variance in Feature Space explained by PC1 and PC2</figcaption></figure><p id="4994" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如图所示，PC1 解释了 X 中 95%的方差，因此，如果按照上面的逻辑，我们应该在进行回归时完全忽略 PC2。</p><p id="8996" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们跟随它，看看会发生什么！！！</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ls"><img src="../Images/8d7f38e4d0275e05d63836526d4c38a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tNtbL3w0MOyimADXf-DGEg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 5: Regression Summary with Y and PC1</figcaption></figure><p id="50fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，R 为 0 表明，即使 PC1 解释了 X 中 95%的变化，仍然无法解释响应变量。</p><p id="5808" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们用 PC2 做同样的事情，它只能解释 X 的 5%的变化，看看会发生什么！！！！</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi lt"><img src="../Images/2616ab00227258eaec6bacc02970a167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mp2hhNdhQzL_GDgRVmLKag.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Figure 6: Regression Summary with Y and PC2</figcaption></figure><p id="96c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">呼呼！！！！你一定在想刚刚发生了什么，主成分解释了 X 中大约 5%的变化，解释了 Y 中 72%的变化。</strong></p><p id="bec9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也有一些真实生活场景来验证中指出的假设</p><p id="048d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">【1】</strong>。<strong class="js iu"> Smith 和 Campbell (1980) </strong>举了一个化学工程的例子，其中有九个回归变量，当第八个主成分的可变性占总变差的 0.06%时，根据低变差标准，该总变差将被去除。</p><p id="5a56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">【2】</strong>。Kung 和 Sharif(1980 年)提供了第二个例子。在用十个气象变量预测季风爆发日期的研究中，显著主成分依次为第八、第二和第十。它表明，即使是具有最低特征值的主成分在解释响应变量的可变性方面也是第三重要的。</p><p id="8a06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">结论</strong>:上述例子表明，去除低特征值的主成分是不可取的，因为它们只表明特征空间中的可解释性，而不表明响应变量中的可解释性。因此，我们应该保留所有组件并进行监督学习，否则我们应该采用监督降维方法，如<strong class="js iu">偏最小二乘回归、最小角度回归</strong>，我们将在即将到来的博客中解释这些方法。</p><p id="16e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参考资料:</p><p id="590d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]伊恩·t·乔利弗，“关于主成分在回归分析中的应用的说明”皇家统计学会杂志。C 系列(应用统计学)，第 31 卷，第 3 号，1982 年，第 300-303 页。www.jstor.org/stable/2348005.·JSTOR</p><p id="666a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]哈迪、阿里·s 和罗伯特·f·林。"关于使用主成分回归的一些注意事项."《美国统计学家》,第 52 卷，第 1 期，1998 年，第 15-19 页。www.jstor.org/stable/2685559.·JSTOR</p><p id="ef5e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]霍金斯博士(1973 年)。主成分分析在替代回归研究中的应用。应用统计学家。, 22, 275–286</p><p id="e947" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4]曼斯菲尔德、韦伯斯特、J. T .和冈斯特，R. F. (1977 年)。主成分回归的分析变量选择技术。应用统计学家。, 26, 34–40.</p><p id="722f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5]f . MOSTELLER 和 j . w . TUKEY(1977 年)。数据分析和回归:统计学第二教程。读书，弥撒。:艾迪森-韦斯利</p><p id="90c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">6 GUNST，R. F .和 MASON，R. L. (1980)。回归分析及其应用:一种面向数据的方法。纽约:马塞尔·德克尔。</p><p id="b81b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">7 杰弗斯，J. N. R. (1967 年)。主成分分析应用中的两个案例。应用统计学家。, 16, 225- 236.(1981).替代回归调查:一些实例。统计学家，30，79-88 岁。</p><p id="c37b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[8]肯德尔博士(1957 年)。多元分析教程。伦敦:格里芬。</p><p id="aa34" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您有任何想法、意见或问题，请在下面留下评论或在 LinkedIn 上联系我们</p><div class="lu lv gp gr lw lx"><a href="https://www.linkedin.com/in/souradip-chakraborty/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">Souradip Chakraborty -数据科学家-沃尔玛印度实验室| LinkedIn</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">查看 Souradip Chakraborty 在全球最大的职业社区 LinkedIn 上的个人资料。</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">www.linkedin.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml la lx"/></div></div></a></div><div class="lu lv gp gr lw lx"><a href="https://www.linkedin.com/in/amlanjd/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">Amlan Jyoti Das -沃尔玛实验室高级数据科学家| LinkedIn</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">经验丰富的数据科学家，有在零售行业解决业务问题和…</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">www.linkedin.com</p></div></div></div></a></div><div class="lu lv gp gr lw lx"><a href="https://www.linkedin.com/in/sai-yaswanth-86893959/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">Sai Yaswanth -沃尔玛印度实验室高级数据科学家| LinkedIn</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">查看世界上最大的职业社区 LinkedIn 上 Sai Yaswanth 的个人资料。</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">www.linkedin.com</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml la lx"/></div></div></a></div><p id="c19a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">敬请关注。快乐阅读！！！:)</p></div></div>    
</body>
</html>