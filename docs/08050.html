<html>
<head>
<title>Predict Customer Churn using PySpark Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 PySpark 机器学习预测客户流失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predict-customer-churn-using-pyspark-machine-learning-519e866449b5?source=collection_archive---------16-----------------------#2019-11-05">https://towardsdatascience.com/predict-customer-churn-using-pyspark-machine-learning-519e866449b5?source=collection_archive---------16-----------------------#2019-11-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/548a0c4cb61f49ce3fe0626aba502ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g4niq_iXdSzD3ABXsC0-XA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: Globallinker</figcaption></figure><p id="e794" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">预测客户流失是数据科学家目前面临的一个具有挑战性的常见问题。预测特定客户面临高风险的能力，同时还有时间做些什么，这对于每个面向客户的企业来说都是一个巨大的额外潜在收入来源。</p><p id="9c7e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在这篇文章中，我将指导你创建一个能够预测客户流失的机器学习解决方案。这个解决方案将通过<a class="ae ld" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇火花</a>实现。Apache Spark 是一个流行的分布式数据处理引擎，可以以多种方式部署，为 Java、Scala、Python 和 r 提供原生绑定。它提供了一系列库，包括 Spark SQL、Spark Streaming、用于机器学习的 MLlib 和用于图形处理的 GraphX。对于这个项目，我们将重点关注机器学习库 MLlib。我们将使用 Python API for Spark，称为 PySpark。</p><p id="d8fe" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果您阅读这篇文章，您将学会如何:</p><ul class=""><li id="7bd4" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">将大型数据集加载到 Spark 中，并使用 Spark SQL 和 Spark Dataframes 操纵它们，以设计相关功能来预测客户流失，</li><li id="6ced" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">使用 Spark ML 中的机器学习 API 来构建和调整模型。</li></ul><h1 id="67d7" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">介绍</h1><p id="d654" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">想象一下，你正在一个类似于 Spotify 的流行数字音乐服务的数据团队工作。姑且称之为 Sparkify 吧。用户每天播放他们喜欢的歌曲，要么使用在歌曲之间放置广告的免费层，要么使用付费订阅模式，他们免费播放音乐，但按月支付固定费用。用户可以随时升级、降级和取消服务。每当用户与该服务交互时，如播放歌曲、注销或竖起大拇指喜欢一首歌，它都会生成数据。所有这些数据都包含了让用户满意和帮助企业发展的关键见解。我们数据团队的工作是预测哪些用户面临取消账户的风险。如果我们能够在这些用户离开之前准确地识别他们，我们的企业就可以为他们提供折扣和激励，从而潜在地为我们的企业节省数百万美元的收入。</p><h1 id="f369" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">导入库并设置 Spark</h1><p id="2326" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">我使用 IBM Watson Studio(默认 Spark Python 3.6 XS，一个驱动程序，两个执行程序，Spark 2.3 版)来完成这个项目。与 PySpark 数据帧的交互不如与 pandas 数据帧的交互方便。这就是为什么我建议安装并导入<code class="fe mv mw mx my b">pixiedust</code>:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="b553" class="nh lt it my b gy ni nj l nk nl">!pip install --upgrade pixiedust<br/>import pixiedust</span></pre><p id="89e9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mv mw mx my b">pixiedust</code>是一个开源的 Python 助手库，作为 Jupyter 笔记本的附加组件，极大地改善了我们与 PySpark 数据帧的交互方式。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="05f0" class="nh lt it my b gy ni nj l nk nl">import numpy as np<br/>import pandas as pd<br/>%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import datetime<br/><br/>from sklearn.metrics import f1_score, recall_score, precision_score<br/>from pyspark.sql import SparkSession<br/>import pyspark.sql.functions as F<br/>from pyspark.sql.types import IntegerType, DoubleType, DateType, FloatType<br/>from pyspark.ml.feature import VectorAssembler, MinMaxScaler<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator<br/>from pyspark.ml.tuning import ParamGridBuilder, CrossValidator<br/>from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, GBTClassifier, LinearSVC<br/></span></pre><p id="3be4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">创建 Spark 会话并读取 Sparkify 数据集:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="0c36" class="nh lt it my b gy ni nj l nk nl"># create a Spark session<br/>spark = SparkSession \<br/>    .builder \<br/>    .appName("Sparkify") \<br/>    .getOrCreate()</span><span id="54d3" class="nh lt it my b gy nm nj l nk nl"># read in dataset<br/>df = spark.read.json('medium-sparkify-event-data.json')</span></pre><p id="887b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe mv mw mx my b">pixiedust</code>现在派上了用场，我们可以显示数据帧的第一个条目。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="4e24" class="nh lt it my b gy ni nj l nk nl">display(df)</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/fb795430c1d9a898a1a8e7bb605fe755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t7K8YE4zx0cKQk0Cd-Pong.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 1</figcaption></figure><p id="9579" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">浏览一下模式:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="2781" class="nh lt it my b gy ni nj l nk nl">df.printSchema()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ae3c517010766f6c27437280e83fc642.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*vywjWd_jfFbIwEeNgQQn9Q.jpeg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 2</figcaption></figure><p id="a2e7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">该数据集包含有关用户如何与流媒体平台交互、他们听了哪些歌曲、他们访问了哪个页面、他们的帐户状态等信息。任何用户交互都存储有 UNIX 时间戳，这使得分析用户行为随时间的变化成为可能。我们将在以后的特征工程过程中利用这些信息。</p><h1 id="287b" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">探索性数据分析</h1><p id="4dd0" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">接下来，我们将通过在 PySpark 中进行基本操作来执行 EDA。</p><p id="eeca" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了了解用户如何与音乐服务交互，我们可能想看看他们查看最多的页面。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="7741" class="nh lt it my b gy ni nj l nk nl">df.groupBy('page').count().sort(F.desc('count')).show()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/eba0827748db49f8baad91b55eee2347.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*LhWSIJ2vWKeKNGqQwnTdDg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 3</figcaption></figure><p id="a5f7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以清楚地看到，“NextSong”是最受欢迎的页面视图，这对于音乐服务来说非常有意义。然而，还有许多其他页面视图对于从该原始数据集中设计相关要素也很重要。我们使用“取消确认”页面，计算 99 次访问，来创建机器学习模型的标签。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="dbf2" class="nh lt it my b gy ni nj l nk nl">flag_cancellation_event = F.udf(lambda x: 1 if x == 'Cancellation Confirmation' else 0, IntegerType())<br/>df = df.withColumn('label', flag_cancellation_event('page'))</span></pre><p id="1632" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">基于 UNIX 时间戳<em class="nq"> ts </em>，我们可以按小时计算统计数据。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="7dff" class="nh lt it my b gy ni nj l nk nl">get_hour = F.udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).hour, IntegerType())<br/>df = df.withColumn('hour', get_hour(df.ts))</span></pre><p id="4612" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">由于 matplotlib 不支持 PySpark 数据帧，我们将其转换回 pandas 数据帧，并按小时绘制用户活动。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="302d" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Count the events per hour</em><br/>songs_by_hour = df.groupBy('hour').count().orderBy(df.hour)<br/>songs_by_hour_pd = songs_by_hour.toPandas()<br/>songs_by_hour_pd.hour = pd.to_numeric(songs_by_hour_pd.hour)</span><span id="afc8" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Plot the events per hour aggregation</em><br/>plt.scatter(songs_by_hour_pd['hour'], songs_by_hour_pd['count'])<br/>plt.xlim(-1, 24)<br/>plt.ylim(0, 1.2 * max(songs_by_hour_pd['count']))<br/>plt.xlabel('Hour')<br/>plt.ylabel('Events');</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/225546250c75be53bf7dbd927f348e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*ZdlGMr09-0xCS6jKDaxsgQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 4</figcaption></figure><h1 id="984f" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">特征工程</h1><p id="f131" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">特征工程在大数据分析中发挥着关键作用。没有数据，机器学习和数据挖掘算法就无法工作。如果只有很少的特征来表示底层的数据对象，那么就很难实现什么，并且这些算法的结果的质量很大程度上取决于可用特征的质量。<br/>因此，我们开始构建我们发现有希望训练模型的特征。</p><p id="b046" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为此，我们从头开始创建一个新的 py spark data frame<em class="nq">feature _ df</em>，每行代表一个用户。我们将从数据帧<em class="nq"> df </em>中创建特征，并将它们依次连接到数据帧<em class="nq"> feature_df 中。</em></p><p id="28d3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">基于<em class="nq"> df </em>中的<em class="nq">标签</em>列，我们可以将被搅动的用户与其他人分开。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="791a" class="nh lt it my b gy ni nj l nk nl">churned_collect = df.where(df.label==1).select('userId').collect()<br/>churned_users = set([int(row.userId) for row in churned_collect])</span><span id="aa44" class="nh lt it my b gy nm nj l nk nl">all_collect = df.select('userId').collect()<br/>all_users = set([int(row.userId) for row in all_collect])</span><span id="13f2" class="nh lt it my b gy nm nj l nk nl">feature_df = spark.createDataFrame(all_users, IntegerType()).withColumnRenamed('value', 'userId')</span></pre><h2 id="576d" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">编码标签</h2><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="c27b" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Create label column</em><br/>create_churn = F.udf(lambda x: 1 if x in churned_users else 0, IntegerType())<br/>feature_df = feature_df.withColumn('label', create_churn('userId'))</span></pre><h2 id="87e1" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">将性别和帐户级别编码为特征</h2><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="321d" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Create binary gender column</em><br/>convert_gender = F.udf(lambda x: 1 if x == 'M' else 0, IntegerType())<br/>df = df.withColumn('GenderBinary', convert_gender('Gender'))</span><span id="527b" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Add gender as feature</em><br/>feature_df = feature_df.join(df.select(['userId', 'GenderBinary']), 'userId') \<br/>    .dropDuplicates(subset=['userId']) \<br/>    .sort('userId')</span><span id="433c" class="nh lt it my b gy nm nj l nk nl">convert_level = F.udf(lambda x: 1 if x == 'free' else 0, IntegerType())<br/>df = df.withColumn('LevelBinary', convert_level('Level'))</span><span id="ae9d" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Add customer level as feature</em><br/>feature_df = feature_df.join(df.select(['userId', 'ts', 'LevelBinary']), 'userId') \<br/>    .sort(F.desc('ts')) \<br/>    .dropDuplicates(subset=['userId']) \<br/>    .drop('ts')</span></pre><h2 id="5ebb" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">将页面视图编码为功能</h2><p id="8810" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">每次用户与平台交互时，它都会生成数据。这意味着我们确切地知道每个用户在数据提取期间经历了什么。我的方法是将页面分成几类:</p><ul class=""><li id="005e" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">中性页面:“取消”、“主页”、“注销”、“保存设置”、“关于”、“设置”</li><li id="b88b" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">负面页面:“拇指朝下”、“滚动广告”、“帮助”、“错误”</li><li id="27f0" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">正面页面:“添加到播放列表”、“添加朋友”、“下一首歌”、“竖起大拇指”</li><li id="ab94" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">降级页面:“提交降级”、“降级”</li><li id="f281" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">升级页面:“提交升级”、“升级”</li></ul><p id="2503" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这种方法背后的原因是，我们可以计算用户与正面页面进行交互的频率。我们可以为每个页面分别做这件事，但这会导致一个更高的特征空间。</p><p id="b19d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们把它写成代码:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="182e" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Create a dictonary which maps page views and PySpark dataframes </em><br/>pages = {}<br/>pages['neutralPages'] = df.filter((df.page == 'Cancel') | (df.page == 'Home') | (df.page == 'Logout') \<br/>    | (df.page == 'Save Settings') | (df.page == 'About') | (df.page == 'Settings'))<br/>pages['negativePages'] = df.filter((df.page == 'Thumbs Down') | (df.page == 'Roll Advert') | (df.page == 'Help') \<br/>    | (df.page == 'Error'))<br/>pages['positivePages'] = df.filter((df.page == 'Add to Playlist') | (df.page == 'Add Friend') | (df.page == 'NextSong') \<br/>    | (df.page == 'Thumbs Up'))<br/>pages['downgradePages'] = df.filter((df.page == 'Submit Downgrade') | (df.page == 'Downgrade'))<br/>pages['upgradePages'] = df.filter((df.page == 'Upgrade') | (df.page == 'Submit Upgrade'))</span><span id="71e7" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Loop through page views and aggregate the counts by user</em><br/>for key, value in pages.items():<br/>    value_df = value.select('userId') \<br/>        .groupBy('userId') \<br/>        .agg({'userId':'count'}) \<br/>        .withColumnRenamed('count(userId)', key)<br/>    <br/>    <em class="nq"># Add page view aggregations as features</em><br/>    feature_df = feature_df.join(value_df, 'userId', 'left').sort('userId') \<br/>        .fillna({key:'0'})</span></pre><p id="3d27" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们将计算用户与平台互动的天数:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="0c62" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Create dataframe with users and date counts</em><br/>dateCount_df = df.select('userId', 'date') \<br/>    .groupby('userId') \<br/>    .agg(F.countDistinct('date')) \<br/>    .withColumnRenamed('count(DISTINCT date)', 'dateCount')<br/><br/><em class="nq"># Add date count as feature</em><br/>feature_df = feature_df.join(dateCount_df, 'userId', 'left').sort('userId') \<br/>        .fillna({'dateCount':'1'})</span></pre><p id="2382" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这些页面视图特征是计算出现次数的绝对值。然而，如果一些用户在数据提取结束时注册，而另一些用户从一开始就使用该平台，这可能会导致误导性的结果。为此，我们将通过在特定于用户的时间窗口内划分汇总结果，获得计数/天，从而使汇总结果具有可比性。我在这里跳过代码，完整的代码可以在<a class="ae ld" href="https://github.com/scientist94/Udacity_DSND_Spark" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><h2 id="1de7" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">将一段时间内的用户活动编码为特征</h2><p id="f02f" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">另一个有希望的特性是用户交互如何随时间变化。首先，我们计算每天的用户交互次数。其次，我们使用 numpy.polyfit 为每个用户拟合一个线性回归模型。我们将获取这些线的斜率，移除异常值，并将缩放后的斜率作为特征插入分类算法中。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2e9710dd3f3a7c7e96c6f3274bf37147.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*pTToF1mCGd3bjkLJHXJQ6A.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 5</figcaption></figure><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="8a6e" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Create dataframe with users and their activity per day</em><br/>activity_df = df.select('userId', 'date') \<br/>    .groupby('userID', 'date') \<br/>    .count()</span><span id="39e7" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># initialize slopes</em><br/>slopes = []<br/>for user in all_users:<br/>    <em class="nq"># Create pandas dataframe for slope calculation</em><br/>    activity_pandas = activity_df.filter(activity_df['userID'] == user).sort(F.asc('date')).toPandas()<br/>    if activity_pandas.shape[0]==1:<br/>        slopes.append(0)<br/>        continue<br/>    <em class="nq"># Fit a line through the user activity counts and retrieve its slope</em><br/>    slope = np.polyfit(activity_pandas.index, activity_pandas['count'], 1)[0]<br/>    slopes.append(slope)</span></pre><h2 id="9eef" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">特征缩放，将列合并为一个特征向量</h2><p id="6c62" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">作为特征工程过程的最后一步，我们将迭代创建的特征，并使用 MinMaxScaler 缩放它们。然后，我们将这些特征放入一个向量中，这样我们就可以将它插入 pyspark.ml 算法中。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="7f01" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># UDF for converting column type from vector to double type</em><br/>unlist = F.udf(lambda x: round(float(list(x)[0]),3), DoubleType())<br/><br/><em class="nq"># Iterate over columns to be scaled</em><br/>for i in ['neutralPagesNormalized', 'negativePagesNormalized', 'positivePagesNormalized', 'downgradePagesNormalized', 'upgradePagesNormalized', 'dateCountNormalized', 'hourAvg', 'UserActiveTime', 'Slope']:<br/>    <em class="nq"># VectorAssembler Transformation - Convert column to vector type</em><br/>    assembler = VectorAssembler(inputCols=[i],outputCol=i+"_Vect")<br/><br/>    <em class="nq"># MinMaxScaler Transformation</em><br/>    scaler = MinMaxScaler(inputCol=i+"_Vect", outputCol=i+"_Scaled")<br/><br/>    <em class="nq"># Pipeline of VectorAssembler and MinMaxScaler</em><br/>    pipeline = Pipeline(stages=[assembler, scaler])<br/><br/>    <em class="nq"># Fitting pipeline on dataframe</em><br/>    feature_df = pipeline.fit(feature_df).transform(feature_df) \<br/>        .withColumn(i+"_Scaled", unlist(i+"_Scaled")).drop(i+"_Vect")<br/><br/><em class="nq"># Merge columns to one feature vector</em><br/>assembler = VectorAssembler(inputCols=['neutralPagesNormalized_Scaled', 'negativePagesNormalized_Scaled', 'positivePagesNormalized_Scaled',                                   'downgradePagesNormalized_Scaled', 'upgradePagesNormalized_Scaled', 'dateCountNormalized_Scaled',                                   'hourAvg_Scaled', 'UserActiveTime_Scaled', 'Slope_Scaled', 'LevelBinary', 'GenderBinary'], outputCol='features')<br/>feature_df = assembler.transform(feature_df)</span></pre><p id="006d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">遵循<em class="nq"> feature_df 的模式:</em></p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/7cfc397e37056545366fdd980b4ed9bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*fnQ1tJd9Q0TntN-YlDQYiA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 6</figcaption></figure><p id="b432" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="nq">特征</em>列保存每个用户的组合特征向量:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1c4361cc84c913520b9d77f442f04a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*mDeeMz_jUxmn59CMLeQvtQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 7</figcaption></figure><h1 id="b3f4" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">机器学习建模</h1><p id="c238" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">创建特征后，我们可以继续将整个数据集分为训练和测试两部分。我们将测试几种用于分类任务的常见机器学习方法。将评估模型的准确性，并相应地调整参数。根据 F1 分数、精确度和召回率，我们将确定获胜的型号。</p><p id="3811" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">将特性数据框架分为训练和测试，并检查类别不平衡。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="bfbf" class="nh lt it my b gy ni nj l nk nl">train, test = feature_df.randomSplit([0.7, 0.3], seed = 42)</span><span id="7bf5" class="nh lt it my b gy nm nj l nk nl">plt.hist(feature_df.toPandas()['label'])<br/>plt.show()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3eac61bd212c899923609bcefaaa5ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*3R_pIPfAHwZGpqzI82R9Bg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 8</figcaption></figure><p id="4eea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">检查数据中潜在的阶级不平衡是至关重要的。这在实践中是非常普遍的，并且许多分类学习算法对于不常见的类具有低的预测准确度。</p><h2 id="6025" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">机器学习超参数调整和评估</h2><p id="0c6b" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">Spark 的 MLlib 支持<code class="fe mv mw mx my b"><a class="ae ld" href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator" rel="noopener ugc nofollow" target="_blank">CrossValidator</a></code>等模型选择的工具。这需要满足以下条件:</p><ul class=""><li id="ca26" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">估计器:算法还是流水线</li><li id="f203" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">参数集:要搜索的参数网格</li><li id="fe21" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">Evaluator:度量模型在测试数据集上的表现的指标。</li></ul><p id="d1e6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">将为每个分类器专门设置估计器和参数。</p><p id="1795" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了评测，我们取支持“areaUnderROC”和“areaUnderPR”的<code class="fe mv mw mx my b"><a class="ae ld" href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.BinaryClassificationEvaluator" rel="noopener ugc nofollow" target="_blank">BinaryClassificationEvaluator</a></code> <a class="ae ld" href="https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.BinaryClassificationEvaluator" rel="noopener ugc nofollow" target="_blank"> </a>。由于我们在数据中有一个等级不平衡，我们采用“areaUnderPR”作为我们的评估指标，因为 PR 曲线在这种情况下提供了更多信息(参见<a class="ae ld" href="http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf" rel="noopener ugc nofollow" target="_blank">http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf</a>)。</p><p id="f5a9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">由于 pyspark.ml.evaluation 中的类<code class="fe mv mw mx my b">BinaryClassificationEvaluator</code>只提供了“areaUnderPR”和“areaUnderROC”这两个指标，我们将使用<code class="fe mv mw mx my b">sklearn</code>来计算 F1 分数、精确度和召回率。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="ed23" class="nh lt it my b gy ni nj l nk nl">evaluator = BinaryClassificationEvaluator(metricName = 'areaUnderPR')</span></pre><p id="3b68" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">逻辑回归</strong></p><p id="83ce" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">逻辑回归基本上是一种线性回归模型，它解释了因变量与一个或多个名义变量、序数变量、区间变量或比率级自变量之间的关系，唯一的区别是，线性回归的输出是一个具有实际意义(标签)的数字，而逻辑回归的输出是一个代表事件发生概率(即客户删除其账户的概率)的数字。</p><p id="e5cc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在实例化逻辑回归对象之前，我们计算一个平衡比率来说明类的不平衡。我们使用<code class="fe mv mw mx my b">weightCol</code>参数根据预先计算的比率对训练实例进行过采样/欠采样。我们想要“欠采样”负类和“过采样”正类。逻辑损失目标函数应该用较低的权重处理负类(标签== 0)。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="da48" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Calculate a balancing ratio to account for the class imbalance</em><br/>balancing_ratio = train.filter(train['label']==0).count()/train.count()<br/>train=train.withColumn("classWeights", F.when(train.label == 1,balancing_ratio).otherwise(1-balancing_ratio))<br/><br/><em class="nq"># Create a logistic regression object</em><br/>lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', weightCol="classWeights")</span></pre><p id="16fe" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于逻辑回归，pyspark.ml 支持在训练集上从模型的中提取一个<code class="fe mv mw mx my b"><a class="ae ld" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegressionSummary" rel="noopener ugc nofollow" target="_blank">trainingSummary</a></code> <a class="ae ld" href="https://spark.apache.org/docs/2.1.0/api/python/pyspark.ml.html#pyspark.ml.classification.LogisticRegressionSummary" rel="noopener ugc nofollow" target="_blank">。这不适用于拟合的<code class="fe mv mw mx my b">CrossValidator</code>对象，这就是为什么我们从没有参数调整的拟合模型中取出它。</a></p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="c34c" class="nh lt it my b gy ni nj l nk nl">lrModel = lr.fit(train)<br/>trainingSummary = lrModel.summary</span></pre><p id="9e5c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以使用它来绘制阈值-召回曲线，阈值-精度曲线，召回-精度曲线和阈值-F-测量曲线，以检查我们的模型如何执行。此阈值是一个预测阈值，它根据模型输出的概率确定预测的类别。模型优化包括调整该阈值。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="5b9e" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Plot the threshold-recall curve</em><br/>tr = trainingSummary.recallByThreshold.toPandas()<br/>plt.plot(tr['threshold'], tr['recall'])<br/>plt.xlabel('Threshold')<br/>plt.ylabel('Recall')<br/>plt.show()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/a4d38ae18506f0042a63b70a3beb30db.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*6ZMXv_qE8wj5WSIBSjK7KA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 9</figcaption></figure><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="b657" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Plot the threshold-precision curve</em><br/>tp = trainingSummary.precisionByThreshold.toPandas()<br/>plt.plot(tp['threshold'], tp['precision'])<br/>plt.xlabel('Threshold')<br/>plt.ylabel('Precision')<br/>plt.show()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/183a81299f9e3b870c3cfb928d0d372b.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*PnJudSTZ-PA7948o7FNT5A.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 10</figcaption></figure><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="7d00" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Plot the recall-precision curve</em><br/>pr = trainingSummary.pr.toPandas()<br/>plt.plot(pr['recall'], pr['precision'])<br/>plt.xlabel('Recall')<br/>plt.ylabel('Precision')<br/>plt.show()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7aa68cbc2929b7bb138c4b92ab50d670.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*m90c-eIK7m7l9q4LmnUgjA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 11</figcaption></figure><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="4854" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Plot the threshold-F-Measure curve</em><br/>fm = trainingSummary.fMeasureByThreshold.toPandas()<br/>plt.plot(fm['threshold'], fm['F-Measure'])<br/>plt.xlabel('Threshold')<br/>plt.ylabel('F-1 Score')<br/>plt.show()</span></pre><figure class="mz na nb nc gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/4e9d170c43b34d30408bcd0c33101bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*9gMwSCCpHgE39MROXlYJgw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 12</figcaption></figure><p id="311a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">随着我们提高预测阈值，召回率开始下降，而精确度分数提高。常见的做法是将相互竞争的指标可视化。</p><p id="1565" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们使用交叉验证来调整我们的逻辑回归模型:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="dc14" class="nh lt it my b gy ni nj l nk nl"><em class="nq"># Create ParamGrid for Cross Validation</em><br/>paramGrid = (ParamGridBuilder()<br/>             .addGrid(lr.regParam, [0.01, 0.5, 2.0])<br/>             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])<br/>             .addGrid(lr.maxIter, [1, 5, 10])<br/>             .build())<br/><br/>cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)<br/><br/><em class="nq"># Run cross validations</em><br/>cvModel = cv.fit(train)<br/>predictions = cvModel.transform(test)<br/>predictions_pandas = predictions.toPandas()<br/>print('Test Area Under PR: ', evaluator.evaluate(predictions))</span><span id="2fda" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Calculate and print f1, recall and precision scores</em><br/>f1 = f1_score(predictions_pandas.label, predictions_pandas.prediction)<br/>recall = recall_score(predictions_pandas.label, predictions_pandas.prediction)<br/>precision = precision_score(predictions_pandas.label, predictions_pandas.prediction)<br/><br/>print('F1-Score: {}, Recall: {}, Precision: {}'.format(f1, recall, precision))</span></pre><p id="8ca0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">参数调整后，逻辑回归显示以下性能:</p><ul class=""><li id="cea6" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">f1-得分:0.66</li><li id="11c7" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">回忆:0.84</li><li id="2700" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">精度:0.54</li></ul><h2 id="77b4" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">梯度增强树分类器</h2><p id="102c" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">梯度推进树是一种使用决策树集成的流行分类方法。对于类不平衡的数据，提升算法通常是很好的选择。PySpark 的 MLlib 支持它，所以让我们在我们的数据集上尝试一下:</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="28c1" class="nh lt it my b gy ni nj l nk nl">gbt = GBTClassifier()</span><span id="e1e9" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Create ParamGrid for Cross Validation</em><br/>paramGrid = (ParamGridBuilder()<br/>             .addGrid(gbt.maxDepth, [2, 4, 6])<br/>             .addGrid(gbt.maxBins, [20, 60])<br/>             .addGrid(gbt.maxIter, [10, 20])<br/>             .build())<br/>cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)<br/><br/><em class="nq"># Run cross validations</em><br/>cvModel = cv.fit(train)<br/>predictions = cvModel.transform(test)<br/>predictions_pandas = predictions.toPandas()<br/>print('Test Area Under PR: ', evaluator.evaluate(predictions))</span><span id="152b" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Calculate and print f1, recall and precision scores</em><br/>f1 = f1_score(predictions_pandas.label, predictions_pandas.prediction)<br/>recall = recall_score(predictions_pandas.label, predictions_pandas.prediction)<br/>precision = precision_score(predictions_pandas.label, predictions_pandas.prediction)<br/><br/>print('F1-Score: {}, Recall: {}, Precision: {}'.format(f1, recall, precision))</span></pre><p id="0f82" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">参数调整后，梯度增强树分类器显示出以下性能:</p><ul class=""><li id="072a" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">f1-得分:0.58</li><li id="8369" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">回忆:0.56</li><li id="8ab4" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">精度:0.61</li></ul><h2 id="2bdf" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">决策树分类器</h2><p id="2010" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">决策树是一种流行的分类和回归方法。</p><pre class="mz na nb nc gt nd my ne nf aw ng bi"><span id="2b0c" class="nh lt it my b gy ni nj l nk nl">dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label')</span><span id="76c6" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Create ParamGrid for Cross Validation</em><br/>paramGrid = (ParamGridBuilder()<br/>             .addGrid(dt.maxDepth, [2, 4, 6])<br/>             .addGrid(dt.maxBins, [20, 60])<br/>             .addGrid(dt.impurity, ['gini', 'entropy'])<br/>             .build())<br/>cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)<br/><br/><em class="nq"># Run cross validations</em><br/>cvModel = cv.fit(train)<br/>predictions = cvModel.transform(test)<br/>predictions_pandas = predictions.toPandas()<br/>print('Test Area Under PR: ', evaluator.evaluate(predictions))</span><span id="b9fc" class="nh lt it my b gy nm nj l nk nl"><em class="nq"># Calculate and print f1, recall and precision scores</em><br/>f1 = f1_score(predictions_pandas.label, predictions_pandas.prediction)<br/>recall = recall_score(predictions_pandas.label, predictions_pandas.prediction)<br/>precision = precision_score(predictions_pandas.label, predictions_pandas.prediction)<br/><br/>print('F1-Score: {}, Recall: {}, Precision: {}'.format(f1, recall, precision))</span></pre><p id="0194" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">参数调整后，决策树分类器表现出以下性能:</p><ul class=""><li id="4818" class="le lf it kh b ki kj km kn kq lg ku lh ky li lc lj lk ll lm bi translated">f1-得分:0.56</li><li id="26ed" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">回忆:0.60</li><li id="e6cb" class="le lf it kh b ki ln km lo kq lp ku lq ky lr lc lj lk ll lm bi translated">精度:0.52</li></ul><h2 id="25d6" class="nh lt it bd lu ns nt dn ly nu nv dp mc kq nw nx mg ku ny nz mk ky oa ob mo oc bi translated">结论</h2><p id="dcfe" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">该项目的目标是利用 Apache Spark 的分析引擎进行大规模数据处理的能力，来检测即将停止使用 Sparkify 音乐流媒体服务的客户。</p><p id="442c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们应用了数据科学流程的典型步骤，如了解数据、数据准备、建模和评估。</p><p id="df1d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">逻辑回归模型显示出最高的性能(F1-得分:0.66，回忆:0.84，精度:0.54)。我们能够召回 84%的流失客户，并为他们提供特别优惠，防止他们删除 Sparkify 账户。然而，我们需要考虑 54%的中等精度分数。这意味着，在所有将获得特别优惠的顾客中，46%的顾客实际上对服务感到满意，不需要任何特殊待遇。</p><p id="ceea" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这篇文章的源代码可以在<a class="ae ld" href="https://github.com/scientist94/Udacity_DSND_Spark" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。我期待听到任何反馈或问题。</p></div></div>    
</body>
</html>