# 在人工智能中避免副作用和奖励黑客

> 原文：<https://towardsdatascience.com/avoiding-side-effects-and-reward-hacking-in-artificial-intelligence-18c28161190f?source=collection_archive---------19----------------------->

![](img/7d41b555af661f6dcba4a4a3c919ad27.png)

Photo by [@pawel_czerwinski](https://unsplash.com/@pawel_czerwinski) Unsplash

## 《人工智能安全中的具体问题》节选小结

我决定再次后退一步。这一次到 2016 年 6 月发表在 OpenAI 页面上写的关于 AI 安全的论文叫做[***AI 安全中的具体问题***](https://arxiv.org/pdf/1606.06565.pdf) **。写这篇文章的时候，现在是 7 月 26 日。然而，我很怀疑自己现在是否比当时这群思想家懂得更多。但是我会尽我最大的努力来检查这篇论文。**

*“机器学习和人工智能(AI)的快速发展使人们越来越关注 AI 技术对社会的潜在影响。在本文中，我们讨论了一个这样的潜在影响:机器学习系统中的事故问题，定义为可能因现实世界人工智能系统的糟糕设计而出现的意外和有害行为。”*

据此，他们列出了与人工智能风险相关的五个实际问题，并按以下方式分类

1.  **A *排泄*副作用 **
2.  ***奖励黑客***
3.  *可扩展监督*
4.  *安全探索*
5.  *分配转移*

我将简要地看一下在**粗体**中列出的两个。

他们回顾了这些领域以前的工作，并提出了与“前沿”人工智能系统相关的研究方向。然而，他们确实把这些变成了具体的例子，比如机器人手臂撞倒了花瓶；淘气的清洁机器人；培训期间的评估；电源插座中的湿拖把；环境不同。随着我们的进展，我将自由地思考，如果不清楚文章的建议和我自己在这方面的想法，我道歉。

在撰写本文时，他们指出了**三个趋势**，这使得人工智能安全变得非常重要。在这个*中，他们声称:*

*   ***强化学习(RL)的前景越来越光明**，它允许代理与他们的环境进行高度交织的交互。*
*   ***更复杂的代理和环境**。“副作用”更有可能发生在复杂的环境中，一个代理可能需要非常复杂才能以一种危险的方式破解它的奖励功能。*
*   *增加人工智能系统的自主性。输出建议，如语音系统，通常具有相对有限潜在危害。相比之下，控制工业过程的机器会造成伤害。*

## *避免副作用和奖励黑客——目标函数*

*他们谈论的 ***【负面副作用】*** 。在文章中，他们质疑他们所谓的“形式目标函数”，以及是否有可能选择了错误的目标函数。**目标函数**是线性规划中希望最大化或最小化的函数。用简单的英语，也许太简单了，我们可能会问:*我们做得对吗？如果我们将矿产资源开采的价值最大化，却忘记了可能的环境外部性(理解为:损害)，这可能就是一个例子。在此，他们将**事故**定义为:**

**“从广义上讲，事故可以被描述为这样一种情况:一个人类设计者想到了某个(可能是非正式指定的)目标或任务，但是为该任务设计和部署的系统产生了有害的和意想不到的结果。”**

*如果忽略了潜在的非常大的环境问题，就会出现安全问题。即使在“完美的学习和无限的数据”的限制下，目标函数也会导致有害的结果。这些思想被发扬光大，但可以说不是 AI 领域特别独特的。尽管如此，它被考虑的事实是重要的，看起来显而易见的并不总是最明显的行动过程。*

*我最近[总结了欧盟基本权利机构(FRA)的一篇论文](/facebook-vs-eu-artificial-intelligence-and-data-politics-8ab5ba4abe40)，名为 [*“数据质量和人工智能——减少偏见和错误以保护基本权利*](https://fra.europa.eu/en/publication/2019/artificial-intelligence-data-quality?source=post_page---------------------------) *”。在这篇文章中，我看到了一个有趣的陈述，我想再次强调一下。数据是否“符合目的”，这与数据质量有关，但显然在人工智能安全中，它同样与形式目标函数的问题有关:**

> *"因此，数据的质量在很大程度上取决于它们的使用目的."*

*回到 OpenAI 的论文。他们进一步描述道:“在**《黑客行动奖励》**中，设计者写下的目标函数承认一些聪明的‘简单’的解决方案，这些解决方案在形式上最大化了目标函数，但是扭曲了设计者意图的精神(即目标函数可以被‘游戏化’)，这是引线问题的一种概括。”*【加粗】*那么什么是引线问题呢？*

****“电线头*** *是大脑体验快感的人工刺激，通常是通过用电流直接刺激个体大脑的奖赏或快感中枢。它还可以在更广泛的意义上使用，指任何一种通过直接最大化良好感觉来产生某种形式的假冒效用，但未能实现我们所珍视的东西的方法。”
-*[](https://wiki.lesswrong.com/wiki/Wireheading)*

**他们打算如何解决这个**负面副作用**？**

1.  ****定义影响规则**:如果我们不想要副作用，惩罚“改变环境”似乎是很自然的挑战在于我们需要将“对环境的改变”正式化**
2.  ****学习影响正则化**:另一种更灵活的方法是通过对许多任务的训练来学习(而不是定义)一个通用的影响正则化。这是迁移学习的一个例子。**
3.  ****惩罚影响**:除了不做有副作用的事情之外，我们可能也更希望代理不要进入一个很容易做有副作用的事情的位置，即使那可能很方便。**
4.  **多主体方法:避免副作用可以被看作是我们真正关心的事情的代理:避免负外部性。**
5.  ****回报的不确定性**:我们希望避免意料之外的副作用，因为根据我们的偏好，环境已经很好了——随机的变化更有可能是非常糟糕，而不是非常好。**

**此外，他们还提出了一些避免奖励黑客攻击的建议:**

1.  ****部分观察到的目标**:在大多数现代 RL 系统中，假设奖励是直接体验到的，即使环境的其他方面只是被部分观察到。然而，在现实世界中，任务常常涉及到将外部世界带入某种客观状态，而这种客观状态只有代理人通过不完美的感知才能确认。**
2.  ****复杂系统**:任何强大的智能体都是一个复杂系统，目标函数是其中的一部分。正如计算机代码中的错误概率随着程序的复杂性而大大增加一样，存在影响奖励函数的可行黑客的概率也随着代理及其可用策略的复杂性而大大增加。**
3.  ****抽象奖励**:复杂的奖励功能将需要引用抽象的概念(比如评估一个概念性的目标是否已经实现)。这些概念可能需要像神经网络这样的模型来学习，神经网络很容易受到对立的反例的影响。**
4.  ****古德哈特定律**:如果设计师选择了一个看似与完成任务高度相关的目标函数，但当目标函数被强烈优化时，这种相关性就消失了，那么另一个奖励黑客行为的来源就会出现。**
5.  ****反馈循环**:有时一个目标函数有一个可以自我强化的成分，最终被放大到淹没或严重扭曲设计者想要表达的目标函数的程度。**
6.  ****环境嵌入**:在强化学习的形式主义中，奖励被认为来自环境。这种想法通常不能从字面上理解，但它确实是真的，即使它是一个抽象的想法，如棋盘游戏中的分数，也必须在某个地方计算，如一个传感器或一组晶体管。**

**和往常一样，当然还有更多要说的，我只是从包含这些思想的大文本中摘录了一小段。**

**然而，我希望这是有帮助的，并引发你的兴趣。**

****这是第 500 天的第 54 天****

**我已经坚持每天写一篇文章 50 天了。这些文章在探索人工智能领域中社会科学和计算机科学的交叉方面表现得很一般。从第 50 天到第 100 天，我目前的重点是人工智能安全。**