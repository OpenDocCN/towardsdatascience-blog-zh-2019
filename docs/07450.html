<html>
<head>
<title>Estimating Uncertainty in Machine Learning Models — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">估计机器学习模型中的不确定性—第 3 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/estimating-uncertainty-in-machine-learning-models-part-3-22b8c58b07b?source=collection_archive---------18-----------------------#2019-10-18">https://towardsdatascience.com/estimating-uncertainty-in-machine-learning-models-part-3-22b8c58b07b?source=collection_archive---------18-----------------------#2019-10-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><blockquote class="jn jo jp"><p id="9b14" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated"><em class="iq">看看这个系列</em>的第一部(<a class="ae kp" href="https://medium.com/comet-ml/estimating-uncertainty-in-machine-learning-models-part-1-2bd1209c347c" rel="noopener"><em class="iq"/></a><em class="iq">)和第二部(</em><a class="ae kp" href="https://medium.com/comet-ml/estimating-uncertainty-in-machine-learning-models-part-2-8711c832cc15" rel="noopener"><em class="iq"/></a><em class="iq">)</em></p><p id="0b88" class="jq jr js jt b ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko ij bi translated"><strong class="jt ir">作者:Dhruv Nair，数据科学家，Comet.ml </strong></p></blockquote><p id="55a4" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">在关于不确定性估计的系列文章的最后一部分中，我们讨论了大型模型的自举等方法的局限性，并展示了如何使用<a class="ae kp" href="https://arxiv.org/pdf/1506.02142.pdf" rel="noopener ugc nofollow" target="_blank"> MC Dropout </a>来估计神经网络预测的不确定性。</p><p id="3e0e" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">到目前为止，我们研究的方法包括在数据集或模型参数中创建变量来估计不确定性。这里的主要缺点是，它要求我们要么训练多个模型，要么进行多次预测，以便计算出我们的模型预测的方差。</p><p id="ad26" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">在有延迟约束的情况下，诸如 MC 丢弃之类的技术可能不适合用于估计预测间隔。我们能做些什么来减少估计区间所需的预测次数？</p><h1 id="fd15" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">用极大似然法估计区间</h1><p id="4c22" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kq lt ke kf kr lu ki kj ks lv km kn ko ij bi translated">在本系列的第 1 部分中，我们假设因变量<strong class="jt ir"> μ(y|x) </strong>、<strong class="jt ir"> </strong>的平均响应呈正态分布。</p><p id="7ca7" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">MLE 方法包括建立两个模型，一个用于估计条件平均响应<strong class="jt ir"> μ(y|x) </strong>，另一个用于估计预测响应中的方差<strong class="jt ir"> σ </strong>。</p><p id="9e18" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">首先，我们将训练数据分成两半。前半部分模型<strong class="jt ir"> mμ </strong>使用前半部分数据作为常规回归模型进行训练。然后，该模型用于对数据的后半部分进行预测。</p><p id="86d4" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">第二个模型，<strong class="jt ir"> mσ </strong>使用数据的后半部分进行训练，并将<strong class="jt ir"> mμ </strong>的残差平方作为因变量。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/cfd940d8ecb91409e678a407016fd729.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/0*HU4RRTgtaFw_0Awk.png"/></div></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi me"><img src="../Images/58350f128524fdf3e0b92295c5aa4e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*xNWlOUomIKgznxCd.png"/></div></figure><p id="8fd5" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">最终预测区间可以用以下方式表示</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/a226afeac1c14b14e82addceb48a89ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/0*APJiwSmhOSAXmdeC.png"/></div></figure><p id="0a13" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">这里<strong class="jt ir"> α </strong>是根据高斯分布的期望置信水平。</p><h1 id="7661" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">让我们试一试</h1><p id="d9b4" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kq lt ke kf kr lu ki kj ks lv km kn ko ij bi translated">我们将再次使用自动 MPG 数据集。请注意，在最后一步中，训练数据是如何再次拆分的。</p><pre class="lx ly lz ma gt mg mh mi mj aw mk bi"><span id="2824" class="ml ku iq mh b gy mm mn l mo mp"><strong class="mh ir">Mean Variance Estimation Method</strong></span><span id="e282" class="ml ku iq mh b gy mq mn l mo mp">dataset_path = keras.utils.get_file("auto-mpg.data", "<a class="ae kp" href="http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data" rel="noopener ugc nofollow" target="_blank">http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data</a>")</span><span id="a52f" class="ml ku iq mh b gy mq mn l mo mp">column_names = <br/>['MPG','Cylinders','Displacement','Horsepower','Weight',<br/>                'Acceleration', 'Model Year', 'Origin']<br/>raw_dataset = pd.read_csv(dataset_path, names=column_names,<br/>                      na_values = "?", comment='\t',<br/>                      sep=" ", skipinitialspace=True)</span><span id="d844" class="ml ku iq mh b gy mq mn l mo mp">dataset = raw_dataset.copy()<br/>dataset = dataset.dropna()</span><span id="340e" class="ml ku iq mh b gy mq mn l mo mp">origin = dataset.pop('Origin')</span><span id="5ccc" class="ml ku iq mh b gy mq mn l mo mp">dataset['USA'] = (origin == 1)*1.0<br/>dataset['Europe'] = (origin == 2)*1.0<br/>dataset['Japan'] = (origin == 3)*1.0</span><span id="c9a1" class="ml ku iq mh b gy mq mn l mo mp">train_dataset = dataset.sample(frac=0.8,random_state=0)<br/>test_dataset = dataset.drop(train_dataset.index)</span><span id="f9e1" class="ml ku iq mh b gy mq mn l mo mp">mean_dataset = train_dataset.sample(frac=0.5 , random_state=0)<br/>var_dataset = train_dataset.drop(mean_dataset.index)</span></pre><p id="f497" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">接下来，我们将创建两个模型来估计数据的均值和方差</p><pre class="lx ly lz ma gt mg mh mi mj aw mk bi"><span id="5fd4" class="ml ku iq mh b gy mm mn l mo mp">import keras</span><span id="146d" class="ml ku iq mh b gy mq mn l mo mp">from keras.models import Model<br/>from keras.layers import Input, Dense, Dropout<br/>dropout_rate = 0.5</span><span id="1b48" class="ml ku iq mh b gy mq mn l mo mp">def model_fn():<br/>    inputs = Input(shape=(9,))<br/>    x = Dense(64, activation='relu')(inputs)<br/>    x = Dropout(dropout_rate)(x)<br/>    x = Dense(64, activation='relu')(x)<br/>    x = Dropout(dropout_rate)(x)<br/>    outputs = Dense(1)(x)<br/>    <br/>    model = Model(inputs, outputs)<br/>    <br/>    return model</span><span id="8311" class="ml ku iq mh b gy mq mn l mo mp">mean_model = model_fn()<br/>mean_model.compile(loss="mean_squared_error", optimizer='adam')</span><span id="48b8" class="ml ku iq mh b gy mq mn l mo mp">var_model = model_fn()<br/>var_model.compile(loss="mean_squared_error", optimizer='adam')</span></pre><p id="e138" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">最后，我们将标准化我们的数据，并开始训练</p><pre class="lx ly lz ma gt mg mh mi mj aw mk bi"><span id="efe8" class="ml ku iq mh b gy mm mn l mo mp">train_stats = train_dataset.describe()<br/>train_stats.pop("MPG")<br/>train_stats.transpose()</span><span id="a1b8" class="ml ku iq mh b gy mq mn l mo mp">def norm(x):<br/>    return (x - train_stats.loc['mean'])/ train_stats.loc['std']</span><span id="0917" class="ml ku iq mh b gy mq mn l mo mp">normed_train_data = norm(train_dataset)<br/>normed_mean_data = norm(mean_dataset)<br/>normed_var_data = norm(var_dataset)<br/>normed_test_data = norm(test_dataset)</span><span id="8701" class="ml ku iq mh b gy mq mn l mo mp">train_labels = train_dataset.pop('MPG')<br/>mean_labels = mean_dataset.pop('MPG')<br/>var_labels = var_dataset.pop('MPG')<br/>test_labels = test_dataset.pop('MPG')</span></pre><p id="a19c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">训练好均值模型后，我们可以使用它对数据集的后一半进行预测，并计算残差的平方。</p><pre class="lx ly lz ma gt mg mh mi mj aw mk bi"><span id="ee38" class="ml ku iq mh b gy mm mn l mo mp">EPOCHS = 100</span><span id="932b" class="ml ku iq mh b gy mq mn l mo mp">mean_model.fit(normed_mean_data, mean_labels, epochs=EPOCHS, validation_split=0.2, verbose=0)</span><span id="9bc2" class="ml ku iq mh b gy mq mn l mo mp">mean_predictions = mean_model.predict(normed_var_data)<br/>squared_residuals = (var_labels.values.reshape(-1,1) - mean_predictions) ** 2</span><span id="d65f" class="ml ku iq mh b gy mq mn l mo mp">var_model.fit(normed_var_data, squared_residuals, epochs=EPOCHS, validation_split=0.2, verbose=0)</span></pre><p id="65e3" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">让我们来看看这种方法产生的音程。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d3a87730b0d4590bddf3c6e1a0155ac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/0*qpD-mF1yf6_F3J6u.png"/></div></figure><p id="3079" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">你会注意到高度不准确的预测在平均值附近有更大的间隔。</p><h1 id="a78a" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">使用分位数回归估计区间</h1><p id="8d91" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kq lt ke kf kr lu ki kj ks lv km kn ko ij bi translated">如果我们不想对我们的响应变量的分布作出假设，而想直接估计我们的目标变量的上限和下限，该怎么办？</p><p id="d574" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">分位数损失可以帮助我们估计目标百分位数反应，而不是平均反应。也就是说，预测我们目标的 0.25 分位数值将告诉我们，给定我们当前的一组特征，我们期望 25%的目标值等于或小于我们的预测。</p><p id="2b9c" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">如果我们训练两个独立的回归模型，一个用于 0.025 百分位，另一个用于 0.9725 百分位，我们实际上是说我们预期 95%的目标值落在这个区间内，即<strong class="jt ir">95%的预测区间</strong></p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/661ac5a60fe4ba24391842d8058fe1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tRUJNy3r07puLP-r.png"/></div></div></figure><p id="4a4d" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated"><a class="ae kp" href="https://arxiv.org/pdf/1806.11222.pdf" rel="noopener ugc nofollow" target="_blank">分位数回归损失函数</a></p><h1 id="fbad" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">让我们试一试</h1><p id="6c98" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kq lt ke kf kr lu ki kj ks lv km kn ko ij bi translated">Keras 没有默认的分位数损失，所以我们将使用来自<a class="ae kp" href="https://towardsdatascience.com/@sachin.abeywardana" rel="noopener" target="_blank"> Sachin Abeywardana </a>的<a class="ae kp" rel="noopener" target="_blank" href="/deep-quantile-regression-c85481548b5a">跟随实现</a></p><pre class="lx ly lz ma gt mg mh mi mj aw mk bi"><span id="77e0" class="ml ku iq mh b gy mm mn l mo mp">import keras.backend as K</span><span id="9d66" class="ml ku iq mh b gy mq mn l mo mp">def tilted_loss(q,y,f):<br/>    e = (y-f)<br/>    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)</span><span id="57b5" class="ml ku iq mh b gy mq mn l mo mp">model = model_fn()<br/>model.compile(loss=lambda y,f: tilted_loss(0.5,y,f), optimizer='adam')</span><span id="970e" class="ml ku iq mh b gy mq mn l mo mp">lowerq_model = model_fn()<br/>lowerq_model.compile(loss=lambda y,f: tilted_loss(0.025,y,f), optimizer='adam')</span><span id="c03f" class="ml ku iq mh b gy mq mn l mo mp">upperq_model = model_fn()<br/>upperq_model.compile(loss=lambda y,f: tilted_loss(0.9725,y,f), optimizer='adam')</span></pre><p id="4315" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">由此产生的预测如下所示</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1e34fef8fc51f398eeefde66ab605e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/0*ESgfhWXO6McqiJSC.png"/></div></figure><p id="e8ad" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">这种方法的一个缺点是它倾向于产生非常宽的间隔。您还会注意到，间隔并不是关于中间估计值(蓝点)对称的。</p><h1 id="48e9" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">评估预测间隔</h1><p id="5e41" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kq lt ke kf kr lu ki kj ks lv km kn ko ij bi translated">在上一篇<a class="ae kp" href="https://medium.com/comet-ml/estimating-uncertainty-in-machine-learning-models-part-2-8711c832cc15" rel="noopener">文章</a>中，我们引入了两个指标来评估我们区间预测的质量，PICP 和 MPIW。下表比较了我们用来估计神经网络中不确定性的最后三种方法的这些度量。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi my"><img src="../Images/d29d146297ba778625132ae8e9b52094.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/0*S9JRjIHHrPm51aCK.png"/></div></figure><p id="8e70" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">神经网络中不确定性估计技术的比较</p><p id="e2db" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">我们看到，均值-方差估计方法产生最小宽度的区间，这导致其 PICP 分数的降低。MC 下降和分位数回归产生非常宽的区间，导致完美的 PICP 分数。</p><p id="3214" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">MPIW 和 PICP 之间的平衡是一个开放性的问题，完全取决于模型的应用方式。理想情况下，我们希望我们的区间尽可能紧凑，具有较低的平均宽度，并且在大多数情况下还包括我们的目标值。</p><h1 id="44b3" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结论</h1><p id="948d" class="pw-post-body-paragraph jq jr iq jt b ju lr jw jx jy ls ka kb kq lt ke kf kr lu ki kj ks lv km kn ko ij bi translated">这些技术可以很容易地在您现有的模型上实现，只需很少的更改，并且为您的预测提供不确定性估计，使它们更值得信赖。</p><p id="661a" class="pw-post-body-paragraph jq jr iq jt b ju jv jw jx jy jz ka kb kq kd ke kf kr kh ki kj ks kl km kn ko ij bi translated">我希望你喜欢我们关于不确定性的系列。请继续关注此空间，了解更多精彩内容！！</p></div></div>    
</body>
</html>