<html>
<head>
<title>Step-by-step understanding LSTM Autoencoder layers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逐步了解 LSTM 自动编码器图层</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352?source=collection_archive---------1-----------------------#2019-06-04">https://towardsdatascience.com/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352?source=collection_archive---------1-----------------------#2019-06-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bc9b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这里我们将分解一个 LSTM 自动编码器网络来逐层理解它们。我们将检查各层之间的输入和输出流，并将 LSTM 自动编码器与常规的 LSTM 网络进行比较。</h2></div><p id="476c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">&lt;<download the="" free="" book="" class="ae lb" href="https://www.understandingdeeplearning.com/" rel="noopener ugc nofollow" target="_blank">了解深度学习，了解更多&gt;T30】</download></p><p id="7d96" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我之前的文章<a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb">极端罕见事件分类的 LSTM 自动编码器</a> [ <a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"> 1 </a> ]中，我们学习了如何为多元时间序列数据构建 LSTM 自动编码器。</p><p id="730a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，深度学习中的 LSTMs 稍微复杂一些。理解 LSTM 中间层及其设置并不简单。例如，<code class="fe lc ld le lf b">return_sequences</code>参数、<code class="fe lc ld le lf b">RepeatVector</code>和<code class="fe lc ld le lf b">TimeDistributed</code>层的用法可能会令人混淆。</p><p id="2e2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">LSTM 教程已经很好的解释了 LSTM 单元的结构和输入输出，例如[ <a class="ae lb" href="https://machinelearningmastery.com/lstm-autoencoders/" rel="noopener ugc nofollow" target="_blank"> 2 </a>，<a class="ae lb" href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" rel="noopener"> 3 </a> ]。但是，尽管有其特殊性，很少发现解释 LSTM 层在网络中一起工作的机制。</p><p id="7b6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们将分解一个 LSTM 自动编码器网络来逐层理解它们。此外，普遍使用的<strong class="kh ir"> seq2seq </strong>网络类似于 LSTM 自动编码器。因此，这些解释中的大多数也适用于 seq2seq。</p><p id="7234" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将使用一个简单的玩具例子来学习，</p><ul class=""><li id="8e68" class="lg lh iq kh b ki kj kl km ko li ks lj kw lk la ll lm ln lo bi translated"><code class="fe lc ld le lf b">return_sequences=True</code>、<code class="fe lc ld le lf b">RepeatVector()</code>、<code class="fe lc ld le lf b">TimeDistributed()</code>的含义。</li><li id="8216" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">了解每个 LSTM 网络层的输入和输出。</li><li id="725c" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">常规 LSTM 网络和 LSTM 自动编码器之间的区别。</li></ul><h1 id="0446" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">理解模型架构</h1><p id="52dc" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">首先进口我们的必需品。</p><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="c9ac" class="mz lv iq lf b gy na nb l nc nd"><em class="ne"># lstm autoencoder to recreate a timeseries</em><br/><strong class="lf ir">import</strong> <strong class="lf ir">numpy</strong> <strong class="lf ir">as</strong> <strong class="lf ir">np</strong><br/><strong class="lf ir">from</strong> <strong class="lf ir">keras.models</strong> <strong class="lf ir">import</strong> Sequential<br/><strong class="lf ir">from</strong> <strong class="lf ir">keras.layers</strong> <strong class="lf ir">import</strong> LSTM<br/><strong class="lf ir">from</strong> <strong class="lf ir">keras.layers</strong> <strong class="lf ir">import</strong> Dense<br/><strong class="lf ir">from</strong> <strong class="lf ir">keras.layers</strong> <strong class="lf ir">import</strong> RepeatVector<br/><strong class="lf ir">from</strong> <strong class="lf ir">keras.layers</strong> <strong class="lf ir">import</strong> TimeDistributed</span><span id="3ae3" class="mz lv iq lf b gy nf nb l nc nd"><em class="ne">'''</em><br/><em class="ne">A UDF to convert input data into 3-D</em><br/><em class="ne">array as required for LSTM network.</em><br/><em class="ne">'''</em><br/><br/><strong class="lf ir">def</strong> temporalize(X, y, lookback):<br/>    output_X = []<br/>    output_y = []<br/>    <strong class="lf ir">for</strong> i <strong class="lf ir">in</strong> range(len(X)-lookback-1):<br/>        t = []<br/>        <strong class="lf ir">for</strong> j <strong class="lf ir">in</strong> range(1,lookback+1):<br/>            <em class="ne"># Gather past records upto the lookback period</em><br/>            t.append(X[[(i+j+1)], :])<br/>        output_X.append(t)<br/>        output_y.append(y[i+lookback+1])<br/>    <strong class="lf ir">return</strong> output_X, output_y</span></pre><h2 id="01f0" class="mz lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">创建示例数据</h2><p id="8bea" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">我们将创建一个多元时间序列数据的玩具示例。</p><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="58d3" class="mz lv iq lf b gy na nb l nc nd"><em class="ne"># define input timeseries</em><br/>timeseries = np.array([[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],<br/>                       [0.1**3, 0.2**3, 0.3**3, 0.4**3, 0.5**3, 0.6**3, 0.7**3, 0.8**3, 0.9**3]]).transpose()<br/><br/>timesteps = timeseries.shape[0]<br/>n_features = timeseries.shape[1]</span><span id="bc85" class="mz lv iq lf b gy nf nb l nc nd">timeseries</span></pre><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi nr"><img src="../Images/17811c07d6f62bd85809baf378d1cab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DChcLUlEAB3ZsCWJ-wTblg@2x.jpeg"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 1.1. Raw dataset.</figcaption></figure><p id="e634" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据 LSTM 网络的要求，我们需要将一个输入数据整形为<em class="ne"> n_samples </em> x <em class="ne">时间步长</em> x <em class="ne"> n_features </em>。在这个例子中，<code class="fe lc ld le lf b">n_features</code> <em class="ne"> </em>是 2。我们将制作<code class="fe lc ld le lf b">timesteps = 3</code>。这样，结果<code class="fe lc ld le lf b">n_samples</code>是 5(因为输入数据有 9 行)。</p><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="5590" class="mz lv iq lf b gy na nb l nc nd">timesteps = 3<br/>X, y = temporalize(X = timeseries, y = np.zeros(len(timeseries)), lookback = timesteps)<br/><br/>n_features = 2<br/>X = np.array(X)<br/>X = X.reshape(X.shape[0], timesteps, n_features)<br/><br/>X</span></pre><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi od"><img src="../Images/f5e025b4e986f4c64516b016052bc812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZs92u0uL2wsvJkX6L1LcQ.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 1.2. Data transformed to a 3D array for an LSTM network.</figcaption></figure><h2 id="b26c" class="mz lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">了解 LSTM 自动编码器结构</h2><p id="9c76" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在本节中，我们将构建一个 LSTM 自动编码器网络，并可视化其架构和数据流。我们还将考察一个常规的 LSTM 网络，比较它与自动编码器的不同之处。</p><p id="b81d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">定义 LSTM 自动编码器。</p><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="59c2" class="mz lv iq lf b gy na nb l nc nd"><em class="ne"># define model</em><br/>model = Sequential()<br/>model.add(LSTM(128, activation='relu', input_shape=(timesteps,n_features), return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(LSTM(64, activation='relu', return_sequences=<strong class="lf ir">False</strong>))<br/>model.add(RepeatVector(timesteps))<br/>model.add(LSTM(64, activation='relu', return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(LSTM(128, activation='relu', return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(TimeDistributed(Dense(n_features)))<br/>model.compile(optimizer='adam', loss='mse')<br/>model.summary()</span></pre><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi oe"><img src="../Images/aacdbb5a1aa692314125ae9a6e217850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*duhC2qlIlGX3mc4CNt_aFA.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 2.1. Model Summary of LSTM Autoencoder.</figcaption></figure><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="1c32" class="mz lv iq lf b gy na nb l nc nd"><em class="ne"># fit model</em><br/>model.fit(X, X, epochs=300, batch_size=5, verbose=0)<br/><em class="ne"># demonstrate reconstruction</em><br/>yhat = model.predict(X, verbose=0)<br/>print('---Predicted---')<br/>print(np.round(yhat,3))<br/>print('---Actual---')<br/>print(np.round(X, 3))</span></pre><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi of"><img src="../Images/d1c02366461df4bb8637d37038d12525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0yLySRMJDbaBWTdkxbJVzw.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 2.2. Input Reconstruction of LSTM Autoencoder.</figcaption></figure><p id="43a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lc ld le lf b">model.summary()</code>提供了模型架构的概要。为了更好的理解，让我们在下面的图 2.3 中可视化它。</p><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi og"><img src="../Images/fad01306bb9d956b091e1bcfb879ba31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWc8g2yiQrOzntbVeGzbEQ.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 2.3. LSTM Autoencoder Flow Diagram.</figcaption></figure><p id="63a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该图示出了一个数据样本通过 LSTM 自动编码器网络的各层的数据流。数据样本是数据集中的一个实例。在我们的例子中，一个样本是图 1.2 中大小为 3x2 的子数组。</p><p id="b60e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从这个图表中，我们了解到</p><ul class=""><li id="98cf" class="lg lh iq kh b ki kj kl km ko li ks lj kw lk la ll lm ln lo bi translated">LSTM 网络采用 2D 阵列作为输入。</li><li id="bfb7" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">一层 LSTM 的像元数量与时间步长一样多。</li><li id="1244" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">设置<code class="fe lc ld le lf b">return_sequences=True</code>使每个时间步长的每个单元发出一个信号。</li><li id="eab5" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">这在图 2.4 中变得更加清晰，该图显示了<code class="fe lc ld le lf b">return_sequences</code>为<code class="fe lc ld le lf b">True</code>(图 2.4a)与<code class="fe lc ld le lf b">False</code>(图 2.4b)之间的差异。</li></ul><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi oh"><img src="../Images/8478dfc38dbc4e46230d4d0c0874d93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K5FAZ2au0WEh8y9x78Z8xQ.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 2.4. Difference between return_sequences as True and False.</figcaption></figure><ul class=""><li id="917b" class="lg lh iq kh b ki kj kl km ko li ks lj kw lk la ll lm ln lo bi translated">在图 2.4a 中，来自一层中时间步长单元的信号被后续层中相同时间步长的单元接收。</li><li id="ab6b" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">在 LSTM 自动编码器的编码器和解码器模块中，重要的是在如图 2.4a 所示的连续 LSTM 层中的各个时间步长单元之间具有直接连接</li><li id="314b" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">在图 2.4b 中，只有最后一个时间步长单元发出信号。因此，<strong class="kh ir">的输出是一个矢量</strong>。</li><li id="b8a1" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">如图 2.4b 所示，如果后续层是 LSTM，我们使用<code class="fe lc ld le lf b">RepeatVector(timesteps)</code>复制这个向量，以获得下一层的 2D 数组。</li><li id="f789" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">如果后续层是<code class="fe lc ld le lf b">Dense</code>，则不需要变换(因为<code class="fe lc ld le lf b">Dense</code>层需要一个矢量作为输入)。</li></ul><p id="bf0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回到图 2.3 中的 LSTM 自动编码器。</p><ul class=""><li id="7818" class="lg lh iq kh b ki kj kl km ko li ks lj kw lk la ll lm ln lo bi translated">输入数据有 3 个时间步长和 2 个特征。</li><li id="1938" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">第 1 层，LSTM(128)，读取输入数据并输出 128 个特征，每个特征有 3 个时间步长，因为<code class="fe lc ld le lf b">return_sequences=True</code>。</li><li id="04e5" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">第二层，LSTM(64)，采用第一层的 3x128 输入，并将特征尺寸减小到 64。从<code class="fe lc ld le lf b">return_sequences=False</code>开始，它输出大小为 1x64 的特征向量。</li><li id="e8ac" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">该层的输出是输入数据的<strong class="kh ir">编码特征向量</strong>。</li><li id="e494" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">这个编码的特征向量可以被提取并用作数据压缩，或者任何其他监督或非监督学习的特征(在下一篇文章中我们将看到如何提取它)。</li><li id="d0b8" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">层 3，RepeatVector(3)，复制特征向量 3 次。</li><li id="d8cd" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">RepeatVector 层充当编码器和解码器模块之间的桥梁。</li><li id="ddf3" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">它为解码器中的第一个 LSTM 层准备 2D 阵列输入。</li><li id="5792" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">解码器层设计用于展开<em class="ne">编码。</em></li><li id="b706" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">因此，解码器层以与编码器相反的顺序堆叠。</li><li id="ffef" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">第 4 层 LSTM (64)和第 5 层 LSTM (128)分别是第 2 层和第 1 层的镜像。</li><li id="4910" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">最后添加第 6 层(时间分布式(密集(2))，以获得输出，其中“2”是输入数据中的要素数。</li><li id="cb22" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">时间分布图层创建一个长度等于前一图层输出的要素数量的矢量。在这个网络中，第 5 层输出 128 个特征。因此，时间分布图层创建了一个 128 长的矢量，并将其复制了 2 (= n_features)次。</li><li id="2be0" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">第 5 层的输出是一个 3x128 的数组，我们将其表示为 U，而第 6 层的时间分布的输出是一个 128x2 的数组，表示为 V，U 和 V 之间的矩阵乘法产生一个 3x2 的输出。</li><li id="8057" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">拟合网络的目的是使输出接近输入。请注意，这个网络本身确保了输入和输出维度匹配。</li></ul><h2 id="7873" class="mz lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated">LSTM 自动编码器与常规 LSTM 网络的比较</h2><p id="1453" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">当我们将其与为重构输入而构建的常规 LSTM 网络相比较时，上述理解变得更加清晰。</p><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="00dc" class="mz lv iq lf b gy na nb l nc nd"><em class="ne"># define model</em><br/>model = Sequential()<br/>model.add(LSTM(128, activation='relu', input_shape=(timesteps,n_features), return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(LSTM(64, activation='relu', return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(LSTM(64, activation='relu', return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(LSTM(128, activation='relu', return_sequences=<strong class="lf ir">True</strong>))<br/>model.add(TimeDistributed(Dense(n_features)))<br/>model.compile(optimizer='adam', loss='mse')<br/>model.summary()</span></pre><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi oi"><img src="../Images/3ad8d906770d05be54ffbfda6f477cd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZltEfO7HvvV8XoiEWdC8DA.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 3.1. Model Summary of LSTM Autoencoder.</figcaption></figure><pre class="mr ms mt mu gt mv lf mw mx aw my bi"><span id="3d9c" class="mz lv iq lf b gy na nb l nc nd"><em class="ne"># fit model</em><br/>model.fit(X, X, epochs=300, batch_size=5, verbose=0)<br/><em class="ne"># demonstrate reconstruction</em><br/>yhat = model.predict(X, verbose=0)<br/>print('---Predicted---')<br/>print(np.round(yhat,3))<br/>print('---Actual---')<br/>print(np.round(X, 3))</span></pre><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi oj"><img src="../Images/3e1e8cfd0da4bb33bba8f2058356e2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5O5sOwhNH7CzfnzF-xYEg.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 3.2. Input Reconstruction of regular LSTM network.</figcaption></figure><figure class="mr ms mt mu gt ns gh gi paragraph-image"><div role="button" tabindex="0" class="nt nu di nv bf nw"><div class="gh gi ok"><img src="../Images/a08b7597013b7bcb207ba6297cbb1bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAzPMNGegqV0DXZhYprIUw.png"/></div></div><figcaption class="nz oa gj gh gi ob oc bd b be z dk">Figure 3.3. Regular LSTM Network flow diagram.</figcaption></figure><p id="e88a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">常规 LSTM 网络和 LSTM 自动编码器的区别</strong></p><ul class=""><li id="9bd0" class="lg lh iq kh b ki kj kl km ko li ks lj kw lk la ll lm ln lo bi translated">我们在所有的 LSTM 图层中都使用了<code class="fe lc ld le lf b">return_sequences=True</code>。</li><li id="5731" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">这意味着，每一层都输出一个包含每个时间步长的 2D 数组。</li><li id="b9b1" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">因此，没有一维编码的特征向量作为任何中间层的输出。因此，将样本编码成特征向量是不可能的。</li><li id="c91e" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated"><strong class="kh ir">这个编码</strong>向量的缺失将用于重建的常规 LSTM 网络与 LSTM 自动编码器区分开来。</li><li id="ba19" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">但是，请注意，在自动编码器(图 2.1)和常规网络(图 3.1)中，参数的数量是相同的。</li><li id="3c4f" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">这是因为，自动编码器中额外的<code class="fe lc ld le lf b">RepeatVector</code>层没有任何附加参数。</li><li id="50b2" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">最重要的是，<strong class="kh ir">两个网络的重建精度相似</strong>。</li></ul><h1 id="d5d8" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">引人深思的事</h1><p id="a78a" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在<a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"> LSTM 罕见事件分类自动编码器</a> [ <a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"> 1 </a> ]中讨论的使用异常检测方法的罕见事件分类正在训练 LSTM 自动编码器来检测罕见事件。[ <a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"> 1 </a>中自动编码器网络的目标是重建输入，并将重建不佳的样本归类为罕见事件。</p><p id="118c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然，我们也可以建立一个常规的 LSTM 网络来重建一个时间序列数据，如图 3.3 所示，<strong class="kh ir">这样会改善结果吗？</strong></p><p id="2e66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这背后的假设是，</p><blockquote class="ol om on"><p id="404f" class="kf kg ne kh b ki kj jr kk kl km ju kn oo kp kq kr op kt ku kv oq kx ky kz la ij bi translated">由于没有编码层，在某些情况下重建的精度会更好(因为时间维度的维数没有减少)。除非任何其他分析需要编码向量，否则尝试常规 LSTM 网络值得尝试稀有事件分类。</p></blockquote><h2 id="5ee0" class="mz lv iq bd lw ng nh dn ma ni nj dp me ko nk nl mg ks nm nn mi kw no np mk nq bi translated"><a class="ae lb" href="https://github.com/cran2367/understanding-lstm-autoencoder/blob/master/understanding-lstm-autoencoder.ipynb" rel="noopener ugc nofollow" target="_blank"> Github 资源库</a></h2><p id="2a5b" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">完整的代码可以在<a class="ae lb" href="https://github.com/cran2367/understanding-lstm-autoencoder/blob/master/understanding-lstm-autoencoder.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><div class="or os gp gr ot ou"><a href="https://github.com/cran2367/understanding-lstm-autoencoder/blob/master/understanding-lstm-autoencoder.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd ir gy z fp oz fr fs pa fu fw ip bi translated">cran 2367/理解-lstm-自动编码器</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">了解 LSTM 自动编码器。通过创建一个项目，为 cran 2367/understanding-lstm-auto encoder 的开发做出贡献</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">github.com</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi nx ou"/></div></div></a></div><h1 id="1f3f" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论</h1><p id="7430" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在本文中，我们</p><ul class=""><li id="b7dc" class="lg lh iq kh b ki kj kl km ko li ks lj kw lk la ll lm ln lo bi translated">通过一个玩具示例来一层一层地理解 LSTM 网络。</li><li id="2112" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">了解每层之间的输入和输出流。</li><li id="decf" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">理解了<code class="fe lc ld le lf b">return_sequences</code>、<code class="fe lc ld le lf b">RepeatVector()</code>、<code class="fe lc ld le lf b">TimeDistributed()</code>的含义。</li><li id="8394" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la ll lm ln lo bi translated">比较和对比了 LSTM 自动编码器和常规 LSTM 网络。</li></ul></div><div class="ab cl pj pk hu pl" role="separator"><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po pp"/><span class="pm bw bk pn po"/></div><div class="ij ik il im in"><p id="8050" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一篇文章中，我们将学习优化一个网络:<strong class="kh ir"> <em class="ne">如何决定添加一个新层及其大小</em> </strong>？</p><h1 id="4033" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">参考</h1><ol class=""><li id="7f50" class="lg lh iq kh b ki mm kl mn ko pq ks pr kw ps la pt lm ln lo bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb">Keras 极端罕见事件分类的 LSTM 自动编码器</a></li><li id="f4ff" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la pt lm ln lo bi translated"><a class="ae lb" href="https://machinelearningmastery.com/lstm-autoencoders/" rel="noopener ugc nofollow" target="_blank">LSTM 自动编码器简介</a></li><li id="3311" class="lg lh iq kh b ki lp kl lq ko lr ks ls kw lt la pt lm ln lo bi translated"><a class="ae lb" href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714" rel="noopener">了解 LSTM 及其图表</a></li></ol></div></div>    
</body>
</html>