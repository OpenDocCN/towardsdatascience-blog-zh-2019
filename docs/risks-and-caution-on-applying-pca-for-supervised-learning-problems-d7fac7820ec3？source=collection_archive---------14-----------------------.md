# 对监督学习问题应用主成分分析的风险和注意事项

> 原文：<https://towardsdatascience.com/risks-and-caution-on-applying-pca-for-supervised-learning-problems-d7fac7820ec3?source=collection_archive---------14----------------------->

合著者:[阿姆兰·乔蒂·达斯](https://medium.com/u/197eb9da7059?source=post_page-----d7fac7820ec3--------------------------------)，[赛·亚斯旺思](https://medium.com/u/b1e0d684eddd?source=post_page-----d7fac7820ec3--------------------------------)

![](img/f530e37e72d0e1fab0fe5169bae3cf6d.png)

[Reference](https://unsplash.com/s/photos/sparse-deep-learning)

**高维空间及其诅咒**

在处理通常为高维数据的真实数据集时，维数灾难是一个非常关键的问题。随着特征空间维度的增加，配置的数量可以指数增长，因此观察覆盖的配置的数量减少。

在这种情况下，主成分分析在有效地减少数据的维度，同时尽可能多地保留数据集中存在的变化方面起着主要作用。

在深入实际问题之前，我们先对主成分分析做一个非常简单的介绍。

**主成分分析-定义**

P *主成分分析(PCA)* 的中心思想是降低由大量相关变量组成的数据集的维度，同时保留数据集中存在的最大可能变化。

让我们定义一个对称矩阵 A，

![](img/842e9aee8611976e42871ee0d327ba0b.png)

其中 X 是独立变量的 m×n 矩阵，m 是列数，n 是数据点数。矩阵 A 可以分解成以下形式

![](img/511a0426fb4bdb4cf181176a9e98c64d.png)

其中 D 是对角矩阵，E 是按列排列的 A 的特征向量矩阵。

x 的主成分(PCs)是 XX **ᵀ** 的特征向量，这表明特征向量/主成分的方向取决于独立变量(x)的变化。

**为什么盲目应用 PCA 是监督问题中的魔咒？？？？**

在回归中使用主成分分析在文献中受到了很多关注，并且作为处理多重共线性的方法被广泛使用。

但是随着主成分回归的使用，关于主成分和它们各自的重要性顺序对反应变量的解释能力有许多误解。

在各种论文和书籍中多次出现的常见谬误是，在受监督的主成分回归框架中，具有低特征值的独立变量的主成分在解释响应变量时不起任何作用，这将我们带到本博客的目的，即在解释响应变量时，证明具有低特征值的成分可能与具有较大特征值的主成分一样重要，甚至重要得多。

下面列出了中指出的一些例子

[1]. **Mansfield 等人(1977 年，第 38 页)**提出，如果仅删除方差较小的成分，则回归中的预测性损失非常小。

[2].在 **Gunst 和 Mason (1980)** 的书中，12 页致力于主成分回归，并且大多数讨论假设删除主成分仅仅基于它们的方差。(第 327-328 页)。

[3]. **Mosteller 和 Tukey(1977 年，第 397-398 页)**类似地认为，方差小的成分在回归中不太可能是重要的，显然是基于自然是“狡猾的”而不是“完全平均的”。

[4]. **Hocking(1976 年，第 31 页)**更坚定地定义了在基于方差的回归中保留主成分的规则。

**理论解释和理解**

首先，让我们给你一个上述假设的适当的数学证明，然后我们可以用几何可视化和模拟来解释直觉。

比方说

Y —响应变量

X —设计矩阵—特征空间矩阵

z——X 的标准化版本

让𝜆₁≥𝜆₂>….≥ 𝜆p 是 Z **ᵀ** Z(相关矩阵)的特征值，v 是相应的特征向量，那么在 W = ZV 中，w 中的列将代表 z 的主分量。在主分量回归中执行的标准方法是回归 y 上的前 m 个 PC，该问题可以通过下面的定理及其解释[2]看出。

***定理:***

设 W= (W₁,…,Wp)是 x 的 PCs。现在考虑回归模型

![](img/cf254c319b441e1c193c8489d56b31d4.png)

如果回归系数的真实向量 **𝛽** 是在 z 的 j **ᵗʰ** 特征向量 **ᵀ** Z 的方向上，那么当 y 在 w 上回归时，j **ᵗʰ** PC Wⱼ将独自对拟合贡献一切，而其余 PC 将什么都不贡献。

***证明:*** 设 V=(V₁,…,Vp)是包含 z 的特征向量的矩阵 **ᵀ** Z .那么

![](img/8b8e77402165d1b84a158b316c8d2107.png)

如果 **𝛽** 在 j **ᵗʰ** 特征向量 Vⱼ的方向，那么 **Vⱼ = a𝛽** ，其中 a 是非零标量。因此 **𝜃j = Vⱼᵀ𝛽 = a𝛽ᵀ𝛽，𝜃ᴋ = Vᴋᵀ𝛽 = 0** ，每当 **k≠j** 。因此， **𝜃ᴋ** 对应 **Wᴋ** 的回归系数等于零，对于 **k≠j，**因此

![](img/2ae97ffce039bd884512e3ad337f96bd.png)

因为，变量 **Wᴋ** 不产生平方和的任何减少当且仅当它的回归系数为零时，那么 **Wj** 将独自贡献所有来拟合，而剩余的 PC 将不贡献任何东西。

**几何意义和模拟**

让我们现在做模拟，并有数学直觉的几何理解。已经使用二维特征空间(X)和单个响应变量的模拟说明了该解释，使得直观地理解假设变得容易。

![](img/0d3b2d52f2774c823da8f3f9d9250184.png)

Figure 1 : Univariate and Bivariate plots for simulated variable X1 and X2

在模拟的第一步中，设计特征空间已经从变量之间具有非常高相关性的多元正态分布中模拟出来，并且实现了 PCA。

![](img/d823cc38a74e7afb325cec73cb01c6cc.png)

Figure 2 : Correlation heat-map for PC1 and PC2

从图中可以清楚地看出，电脑之间绝对没有关联。第二步是模拟响应变量 Y 的值，使得 PCs 上 Y 系数的方向是第二主分量的方向。

![](img/365b3184467ff5a75a50abdf81979e11.png)

一旦响应变量被模拟，相关矩阵看起来就像这样。

![](img/6c1a772b634a6a2ff666f3ba1a8cf00b.png)

Figure 3 : Correlation heat-map for simulated variable Y and PC1 and PC2

从图中可以清楚地看出，y 和 PC2 而不是 PC1 之间有很高的相关性，这证明了我们的假设。

![](img/cf13acb857e7a3149d897e09c0d07fa9.png)

Figure 4 : Variance in Feature Space explained by PC1 and PC2

如图所示，PC1 解释了 X 中 95%的方差，因此，如果按照上面的逻辑，我们应该在进行回归时完全忽略 PC2。

让我们跟随它，看看会发生什么！！！

![](img/8d7f38e4d0275e05d63836526d4c38a4.png)

Figure 5: Regression Summary with Y and PC1

因此，R 为 0 表明，即使 PC1 解释了 X 中 95%的变化，仍然无法解释响应变量。

现在让我们用 PC2 做同样的事情，它只能解释 X 的 5%的变化，看看会发生什么！！！！

![](img/2616ab00227258eaec6bacc02970a167.png)

Figure 6: Regression Summary with Y and PC2

**呼呼！！！！你一定在想刚刚发生了什么，主成分解释了 X 中大约 5%的变化，解释了 Y 中 72%的变化。**

也有一些真实生活场景来验证中指出的假设

**【1】**。 **Smith 和 Campbell (1980)** 举了一个化学工程的例子，其中有九个回归变量，当第八个主成分的可变性占总变差的 0.06%时，根据低变差标准，该总变差将被去除。

**【2】**。Kung 和 Sharif(1980 年)提供了第二个例子。在用十个气象变量预测季风爆发日期的研究中，显著主成分依次为第八、第二和第十。它表明，即使是具有最低特征值的主成分在解释响应变量的可变性方面也是第三重要的。

**结论**:上述例子表明，去除低特征值的主成分是不可取的，因为它们只表明特征空间中的可解释性，而不表明响应变量中的可解释性。因此，我们应该保留所有组件并进行监督学习，否则我们应该采用监督降维方法，如**偏最小二乘回归、最小角度回归**，我们将在即将到来的博客中解释这些方法。

参考资料:

[1]伊恩·t·乔利弗，“关于主成分在回归分析中的应用的说明”皇家统计学会杂志。C 系列(应用统计学)，第 31 卷，第 3 号，1982 年，第 300-303 页。www.jstor.org/stable/2348005.·JSTOR

[2]哈迪、阿里·s 和罗伯特·f·林。"关于使用主成分回归的一些注意事项."《美国统计学家》,第 52 卷，第 1 期，1998 年，第 15-19 页。www.jstor.org/stable/2685559.·JSTOR

[3]霍金斯博士(1973 年)。主成分分析在替代回归研究中的应用。应用统计学家。, 22, 275–286

[4]曼斯菲尔德、韦伯斯特、J. T .和冈斯特，R. F. (1977 年)。主成分回归的分析变量选择技术。应用统计学家。, 26, 34–40.

[5]f . MOSTELLER 和 j . w . TUKEY(1977 年)。数据分析和回归:统计学第二教程。读书，弥撒。:艾迪森-韦斯利

6 GUNST，R. F .和 MASON，R. L. (1980)。回归分析及其应用:一种面向数据的方法。纽约:马塞尔·德克尔。

7 杰弗斯，J. N. R. (1967 年)。主成分分析应用中的两个案例。应用统计学家。, 16, 225- 236.(1981).替代回归调查:一些实例。统计学家，30，79-88 岁。

[8]肯德尔博士(1957 年)。多元分析教程。伦敦:格里芬。

如果您有任何想法、意见或问题，请在下面留下评论或在 LinkedIn 上联系我们

[](https://www.linkedin.com/in/souradip-chakraborty/) [## Souradip Chakraborty -数据科学家-沃尔玛印度实验室| LinkedIn

### 查看 Souradip Chakraborty 在全球最大的职业社区 LinkedIn 上的个人资料。

www.linkedin.com](https://www.linkedin.com/in/souradip-chakraborty/)  [## Amlan Jyoti Das -沃尔玛实验室高级数据科学家| LinkedIn

### 经验丰富的数据科学家，有在零售行业解决业务问题和…

www.linkedin.com](https://www.linkedin.com/in/amlanjd/) [](https://www.linkedin.com/in/sai-yaswanth-86893959/) [## Sai Yaswanth -沃尔玛印度实验室高级数据科学家| LinkedIn

### 查看世界上最大的职业社区 LinkedIn 上 Sai Yaswanth 的个人资料。

www.linkedin.com](https://www.linkedin.com/in/sai-yaswanth-86893959/) 

敬请关注。快乐阅读！！！:)