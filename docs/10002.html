<html>
<head>
<title>The math behind GANs (Generative Adversarial Networks)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GANs(生成对抗网络)背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-math-behind-gans-generative-adversarial-networks-3828f3469d9c?source=collection_archive---------3-----------------------#2019-12-31">https://towardsdatascience.com/the-math-behind-gans-generative-adversarial-networks-3828f3469d9c?source=collection_archive---------3-----------------------#2019-12-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eca5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">详细了解原始 GANs 背后的数学原理，包括其局限性</h2></div><h1 id="389f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.介绍</h1><p id="10e2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">生成对抗网络(GAN)由两个模型组成:生成模型<strong class="lc iu"> G </strong>和鉴别模型<strong class="lc iu"> D </strong>。生成模型可以被认为是一个试图制造假币并使用它而不被抓住的伪造者，而鉴别模型类似于警察，试图抓住假币。这场竞争一直持续到伪造者变得足够聪明，能够成功骗过警察。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi lw"><img src="../Images/e39737336d30c69affe530a5a1567a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GVn3FkfSWT2gcEH3oYs6Uw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Figure 1: Representation of the generator and discriminator as a counterfeiter and police, respectively. Figure from [1].</figcaption></figure><p id="f9a7" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">换句话说，</p><p id="2140" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated"><strong class="lc iu">鉴别器:</strong>作用是区分真实的和生成的(假的)数据。</p><p id="5576" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated"><strong class="lc iu">生成器:</strong>作用是以一种可以欺骗鉴别器的方式创建数据。</p><h1 id="9250" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.一些参数和变量</h1><p id="a41e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们进入推导之前，让我们描述一些参数和变量。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mr"><img src="../Images/6537f00e2058523708bc26a35867e870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*No6f6hSCKaiXs2Mr9T9wpA.png"/></div></div></figure><h1 id="6801" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.损失函数的求导</h1><p id="8f2a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">Ian Goodfellow 等人在原论文中描述的损失函数可以从二元交叉熵损失公式中导出。二进制交叉熵损失可以写成:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ms"><img src="../Images/e01255279e851161e3cb32d052debf9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kACQPgX3Dk-X7u3J51JXxw.png"/></div></div></figure><h2 id="45e6" class="mt kj it bd kk mu mv dn ko mw mx dp ks lj my mz ku ln na nb kw lr nc nd ky ne bi translated">3.1 鉴频器损耗</h2><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nf"><img src="../Images/f95dc7588676635620ea6b029b35f682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brURhcYCI6WtbTuEw1XJ-Q.png"/></div></div></figure><p id="097e" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">现在，鉴别器的目标是正确地分类伪数据集和真实数据集。为此，等式(1)和(2)应最大化，鉴频器的最终损失函数可由下式给出:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ng"><img src="../Images/45096d85c0116e2f2d95122800fb7f7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fWHhgZg_KNS3h8qq82QD3Q.png"/></div></div></figure><h2 id="fefa" class="mt kj it bd kk mu mv dn ko mw mx dp ks lj my mz ku ln na nb kw lr nc nd ky ne bi translated">3.2 发电机损耗</h2><p id="3d5e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里，生成器与鉴别器竞争。因此，它将尝试最小化等式(3)并且损失函数被给定为:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nh"><img src="../Images/28c63425f9bc197db252091f4a34da97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hdgo1Slf4fDiSrUfO3IwIQ.png"/></div></div></figure><h2 id="f5f3" class="mt kj it bd kk mu mv dn ko mw mx dp ks lj my mz ku ln na nb kw lr nc nd ky ne bi translated">3.3 组合损失函数</h2><p id="0c8e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以组合等式(3)和(4)并写成:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ni"><img src="../Images/0d4a858dad6027398f24463d41675c56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vFj5-pJP866w-LzdaHdoA.png"/></div></div></figure><p id="4a95" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">请记住，上述损失函数仅对单个数据点有效，要考虑整个数据集，我们需要将上述等式的期望值取为</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nj"><img src="../Images/afe3b6a10bb4830a872f0fae7704684e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*79sBUnY8G3nlV9khUr1jkg.png"/></div></div></figure><p id="dc90" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">该等式与 Goodfellow 等人在原始论文中描述的等式相同。</p><h1 id="6b5b" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">4.算法</h1><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nk"><img src="../Images/b896d8fe506152bd1083215e7cbc29ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i9Zti7ooRBCDpl3XgvBGUA.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Figure 2: Algorithm described in the original paper by Goodfellow et al. Figure from [2].</figcaption></figure><p id="8cfd" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">从上面的算法可以注意到，生成器和鉴别器是分开训练的。在第一部分中，真实数据和虚假数据被插入到具有正确标签的鉴别器中，并进行训练。梯度传播保持发电机固定。此外，我们通过提升其随机梯度来更新鉴别器，因为对于鉴别器，我们想要最大化等式(6)中给出的损失函数。</p><p id="e2aa" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">另一方面，我们通过保持鉴别器固定和传递带有假标签的假数据来更新生成器，以便欺骗鉴别器。这里，我们通过降低其随机梯度来更新发生器，因为对于发生器，我们希望最小化等式(6)中给出的损失函数。</p><h1 id="5a44" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">5.Pg = Pdata 的全局最优</h1><p id="6bbc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于任何给定的发生器<strong class="lc iu"> G </strong>的最佳鉴别器<strong class="lc iu"> D </strong>可以通过对损失函数(等式(6))求导来找到，</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nl"><img src="../Images/4aa76ecf8a6a0e12b1b47972898d4e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKxEQFfvLEjcxj07X9YkTA.png"/></div></div></figure><p id="323f" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">上述等式在数学上非常重要，但在现实中，你无法计算出最优的<strong class="lc iu"> D </strong>，因为<strong class="lc iu"> Pdata(x) </strong>是未知的。现在，当我们有最佳的<strong class="lc iu"> D </strong>时，对于<strong class="lc iu"> G </strong>的损失可以通过将等式(7)代入损失函数中得到，</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nm"><img src="../Images/baedd26c82f796fc141733e53433ccc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F0dPdkOJHF7_wCyokjo0ag.png"/></div></div></figure><p id="a945" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">现在，Kullback-Leibler(KL)和 Jensen-Shannon(JS)的散度由下式给出:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nn"><img src="../Images/78df379a43e22afd57e4cda457a105cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MoH4sKuguPRMs0Mw-6J3Q.png"/></div></div></figure><p id="053e" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">因此，</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi no"><img src="../Images/766f2fea4eaaa8b371648e12c6c4fa14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cx-dkavC6yJ-nXDaLgkn7g.png"/></div></div></figure><p id="982f" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">由于散度变为零，当<strong class="lc iu"> Pg </strong>接近<strong class="lc iu"> Pdata </strong>时，上述等式简化为<strong class="lc iu"> -2log2 </strong>。</p><h1 id="d597" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">6.限制</h1><p id="2856" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">导出的损失函数(等式(9))有一些限制，这将在本节中描述。</p><h2 id="d032" class="mt kj it bd kk mu mv dn ko mw mx dp ks lj my mz ku ln na nb kw lr nc nd ky ne bi translated">6.1 消失渐变</h2><p id="6de9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">等式(9)的优化目的是将<strong class="lc iu"> Pg </strong>向<strong class="lc iu"> Pdata </strong>或<strong class="lc iu"> Pr </strong>移动，以获得最佳<strong class="lc iu"> D </strong>。如果<strong class="lc iu"> Pr </strong>和<strong class="lc iu"> Pg </strong>之间没有重叠，JS 偏差保持不变(图 3)。可以观察到，距离大于 5 时，JS 散度为常数，其梯度接近于 0，代表训练过程对<strong class="lc iu"> G </strong>没有任何影响(图 4)。只有当<strong class="lc iu"> Pg </strong>和<strong class="lc iu"> Pr </strong>有明显重叠时，梯度才不为零，这意味着当<strong class="lc iu"> D </strong>接近最优时，<strong class="lc iu"> G </strong>将面临梯度消失的问题。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi np"><img src="../Images/caa8118901b91d62b3f8ae600122c6df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_nNc7_9bSR0jTTtG1wKng.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Figure 3: Illustration of training progress for a GAN. Two normal distributions are used here for visualization. Given an optimal D, the objective of GANs is to update G in order to move the generated distribution Pg (red) towards the real distribution Pr (blue) (G is updated from left to right in this figure. Left: initial state, middle: during training, right: training converging). However, JS divergence for the left two figures are both 0.693 and the figure on the right is 0.336, indicating that JS divergence does not provide sufficient gradient at the initial state. Figure from [3].</figcaption></figure><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nq"><img src="../Images/879f71d8090b37006baebe98fb414486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T3ymSEy4IvFW3Uc_k_29Ag.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Figure 4: JS divergence and gradient change with the distance between Pr and Pg. The distance is the difference between the two distribution means. Figure from [3].</figcaption></figure><p id="4d1d" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">这个问题可以通过修改<strong class="lc iu"> G </strong>的原始损失函数来解决，</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi nr"><img src="../Images/023b980ccb8092cba46b1f8e69e83f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7pI_wd7H8HsDY-6wkA9-Q.png"/></div></div></figure><h2 id="5746" class="mt kj it bd kk mu mv dn ko mw mx dp ks lj my mz ku ln na nb kw lr nc nd ky ne bi translated">6.2 模式崩溃</h2><p id="92b1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在训练过程中，发电机可能会陷入一个设置，在那里它总是产生相同的输出。这被称为模式崩溃。发生这种情况是因为<strong class="lc iu"> G </strong>的主要目的是欺骗<strong class="lc iu"> D </strong>不产生不同的输出。这背后的数学有点复杂，将在以后的文章中讨论。</p><p id="cb76" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">顺便说一句，这是我的第一个故事，我希望你喜欢它。</p><h1 id="2f4f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">7.参考</h1><p id="9eff" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">1]阿蒂恩萨，罗威尔。<em class="ns">使用 Keras 的高级深度学习:应用深度学习技术、自动编码器、GANs、可变自动编码器、深度强化学习、策略梯度等</em>。帕克特出版有限公司，2018 年。</p><p id="79c3" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">2]古德费勒，伊恩，等《生成性对抗性网络》<em class="ns">神经信息处理系统的进展</em>。2014.</p><p id="d08f" class="pw-post-body-paragraph la lb it lc b ld mm ju lf lg mn jx li lj mo ll lm ln mp lp lq lr mq lt lu lv im bi translated">[3]王、、佘琦、沃德。"生成性对抗网络:综述与分类."<em class="ns"> arXiv 预印本 arXiv:1906.01529 </em> (2019)。</p></div></div>    
</body>
</html>