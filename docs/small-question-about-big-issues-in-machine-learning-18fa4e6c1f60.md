# 关于机器学习中“更大”问题的小问题

> 原文：<https://towardsdatascience.com/small-question-about-big-issues-in-machine-learning-18fa4e6c1f60?source=collection_archive---------23----------------------->

## 控制理论、线性回归和模型假设！

# ***ML 模型和数据分布***

**问题 1:机器学习模型有‘数据’分布假设，对吗？**

**回答 1:** 不是所有型号！“统计建模:两种文化”强调了两种截然不同的统计建模方法之间的重要区别:

1.  随机技术，从连续的抽取中估计分布；因此，模型假设了一些潜在的分布。
2.  算法技术:函数 y = f(x)适合观察到的数据。逻辑回归，以及线性回归和它的各种风味(套索，岭等。)是常见的(受监督的)随机技术，在讨论“机器学习”时通常不是首选。决策树、随机森林、Adaboost、梯度提升树、支持向量机等。是常见的算法技术，不假设任何类型的底层数据分布。这些技术是人们在谈论“机器学习”时想到的最常见的例子。毫不奇怪，后一种技术在 Kaggle 等网站上最受欢迎。部分原因是在它们能够被很好地应用之前，对它们(特别是分布假设)了解得很少。

# **别怪你线性回归！！**

你可能在机器学习和数据科学课程中没有听说过，但在控制理论和信号处理课程中你会遇到几次:“线性回归的扩展家族:广义、加权和普通。”

**问题 2:何时使用 GLS 和 WLS？**

**答案 2** :线性回归系数的广义最小二乘(GLS)估计量是普通最小二乘(OLS)估计量的推广。它用于处理 OLS 估计量不是 BLUE(最佳线性无偏估计量)的情况，因为违反了高斯-马尔可夫定理的一个主要假设，即同方差和没有序列相关性。在这种情况下，假设高斯-马尔可夫定理的其他假设得到满足，GLS 估计量是蓝色的。当误差项不相关时，GLS 估计量称为加权最小二乘估计量(WLS)。

***附言*** 他们在实践中(不仅仅是理论上)表现良好

# **机器学习与控制理论**

**问题 3:机器学习是否正在取代控制理论？**

**回答三:**没有！

一个“转换工程师”的自白:控制理论仍然比机器学习有一个主要优势:证明！控制工程是一门严格的学科，有很好的稳定性、鲁棒性和最优性理论。的确，一些计算机科学研究者将最大似然技术应用于一些控制问题，而这些问题已经被经典控制算法所解决。但是，当存在具有数百个参数的 ML 算法时，不能证明闭环系统的任何稳定性、鲁棒性。一般来说，当我们试图解决的问题没有模型时，例如感知问题，ML 会很方便。

对于控制应用，我们知道汽车如何驾驶，直升机如何飞行，机械臂如何移动；而且工业界更喜欢只有少数几个参数的 PID 控制算法，而不是有几十亿个参数的深度学习算法。一个重要的任务是控制算法的验证和确认，这个任务通常被计算机科学学者在他们的学术环境中忽略。当回路中有 PID 控制算法时，很容易检查闭环系统是否稳定。然而，对于任何已知 ML 算法，都不能检查这种基本属性。