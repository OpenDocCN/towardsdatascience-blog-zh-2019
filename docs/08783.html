<html>
<head>
<title>Logistic Regression: Bottoms-up Approach.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归:自下而上的方法。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=collection_archive---------32-----------------------#2019-11-24">https://towardsdatascience.com/logistic-regression-bottoms-up-approach-feature-engineering-ideology-a-bonus-81807fa881be?source=collection_archive---------32-----------------------#2019-11-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6f2f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">(特征工程思想——额外收获)</h2></div><p id="fb14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">快速提问？</strong></p><p id="f8df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用 RMSE 或 MSE 作为分类的评估指标吗？如果有，什么时候可以用？如果不是，为什么我们不能使用它？</p><p id="696d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我最后会透露答案。</p><p id="004a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">对你有什么好处？</strong></p><p id="9724" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章将带你了解逻辑回归的细微差别，并让你熟悉特性工程的思想。我想澄清的第一件事是，逻辑回归的核心是回归，但它是一种分类算法。</p><p id="1510" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">我的告白:</strong></p><p id="cefc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我仅次于决策树的第二喜欢的算法，仅仅是因为它提供的简单性和健壮性。尽管有大量的分类算法，但这种过时的算法经受住了时间的考验，你可以看到它与神经网络的无缝集成是它非常特殊的地方。</p><p id="bf7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">我们来谈谈数据</strong></p><p id="8dd8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了达到目标，我挑选了一个<a class="ae le" href="https://www.kaggle.com/bhavikbb/password-strength-classifier-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>，它将允许我讨论特征工程。这是一个密码强度分类数据集。它有 3 个等级，0-弱，1-中等和 2-强。它只有两列，分别是密码和力量等级。该数据集有大约 700，000 条记录，因此可以肯定它不是一个玩具数据集。</p><p id="98c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">免责声明！</strong></p><p id="b8a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章期望你能提供一些概率概念。请刷新<a class="ae le" href="https://www.youtube.com/watch?v=o4QmoNfW3bI" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">概率</strong> </a> (5 分钟)和<a class="ae le" href="https://www.youtube.com/watch?v=GxbXQjX7fC0" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">赔率</strong> </a> (8 分钟)。阅读 BetterExplained 的这些优秀文章:<a class="ae le" href="http://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">指数函数直观指南&amp; e </strong> </a>和<a class="ae le" href="http://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">揭秘自然对数(ln) </strong> </a>。然后，复习一下这个<a class="ae le" href="http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/12_e_log_examples.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">对指数函数和对数的</strong> </a>的简要总结。</p><p id="abbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> OMG！我没有正确的数据。</strong></p><p id="07a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里可以关注代码<a class="ae le" href="https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="2232" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">陷阱 1:在密码数据集中可能有逗号(，)，并且您正在读取 CSV 文件，这是一个常见的分隔值文件。您可能会读取过多的逗号，并且可能会出现异常。所以，你可以通过设置<em class="lf"> error_bad_lines = False </em>来避开这个问题。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/bc3b7b116b92f373401389f3c7eb5216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*fmWVvvLC0EY15SUw4nMxlQ.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Distribution of passwords based on their strength</figcaption></figure><p id="0024" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们拥有的数据是密码列表，熊猫将其识别为“对象类型”。但是，为了使用 ML 模型，我们需要数字特征。让我们从现有的文本中生成新的特征。这就是特征工程拯救我们的地方。特征工程通常基于直觉。做这件事没有固定的正确方法。通常，在行业中，人们倾向于依赖领域专业知识来识别新特性。根据我们的数据，你是领域专家。您知道理想密码的关键要素，我们将从这里开始。</p><p id="4dff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创建的特征:</p><ul class=""><li id="1979" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated">密码中的字符数</li><li id="9d45" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">密码中的数字个数</li><li id="8e0c" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">密码中的字母数</li><li id="45c6" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">密码中元音的数量</li><li id="a91d" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">密码中辅音的数量(越多意味着密码的意义越小)</li></ul><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mg"><img src="../Images/d92bf3c23ccf880b1abc6318424e4c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZN44wgUzVJZ5KHB8oaMjw.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">New features. We will have to see if they actually contribute something.</figcaption></figure><h1 id="de11" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">逻辑函数</h1><p id="e6c0" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">因此，我们希望返回一个介于 0 和 1 之间的值，以确保我们实际上表示的是一个概率。为此，我们将利用逻辑功能。逻辑函数在数学上看起来像这样:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/027c06fdb31bcb91236a35e9b1b10b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*fW_dXCylEo_6nvW7XPfnkg.png"/></div></figure><p id="440d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看剧情</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/d71dbc0994b074d24a73e7c047358ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*0SQ5RDJUgbY0n6zxCbagDA.png"/></div></figure><p id="cbc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以看到为什么这是一个伟大的概率测量函数。y 值表示概率，范围仅在 0 和 1 之间。同样，对于 0 的 x 值，你得到 0.5 的概率，当你得到更多的正 x 值时，你得到更高的概率，更多的负 x 值时，你得到更低的概率。</p><h1 id="7570" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">利用我们的数据</h1><p id="73ec" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">好吧——这很好，但是我们到底该如何使用它呢？我们知道我们有五个属性——char _ count，numerics，alpha，元音，辅音——我们需要在逻辑函数中使用它们。我们可以做的一件显而易见的事情是:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1b3afe523a53a148342866c515ae9ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/1*ClULnb6EZX6z-KKM5x4Z8A.gif"/></div></figure><p id="7f8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 CC 是 char_count 的值，N 是 numerics 的值，A 是 alpha 的值，V 是元音的值，CO 是辅音的值。对于那些熟悉<a class="ae le" href="https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323" rel="noopener">线性回归</a>的人来说，这看起来很熟悉。基本上，我们假设 x 是我们的数据加上截距的线性组合。例如，假设我们有一个密码，其 char_count 为 8，numerics 为 4，alpha 为 4，元音为 1，辅音为 3。一些神谕告诉我们，𝛽0=1、𝛽1=2、𝛽2=4、𝛽3=6、𝛽4=8 和𝛽5=10.这意味着:</p><p id="d0dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">x = 1+(2 * 8)+(4 * 4)+(6 * 4)+(8 * 1)+(10 * 3)= 95</p><p id="bde5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将此代入我们的物流功能，得出:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/ddec27cb8e342bc45d446ca1f05d153d.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/1*a_e49a60x_Tlf5rHgflbjw.gif"/></div></figure><p id="6b22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们认为具有这些特征的密码有 100%的可能性是中等(我取了第一行数据)</p><h1 id="a293" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">学问</h1><p id="6c9a" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">好吧——有道理。但是是谁给了我们𝛽价值观呢？好问题！这就是机器学习中学习的用武之地:)。我们将了解我们的𝛽价值观。</p><h1 id="5201" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">步骤 1-定义你的成本函数</h1><p id="a190" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">为了简单起见，我们将去掉一个类，使之成为一个二元分类问题。在本文的后面，我们将看到如何进行多重分类。现在，我把中等和弱混合作为一个类。如果你一直在研究机器学习，你可能会听到“成本函数”这个词。不过，在此之前，让我们思考一下。我们正在尝试选择𝛽值，以最大化正确分类密码的概率。这就是我们问题的定义。假设有人给了我们一些𝛽价值观，我们如何确定它们是否是好的价值观？我们在上面看到了如何获得一个例子的概率。现在想象一下，我们对所有的密码观察—所有的 700k 都这样做了。我们现在会有 70 万的概率分数。我们希望强密码的概率值接近 1，弱密码的概率值接近 0。</p><p id="33f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们并不关心只得到一个观察值的正确概率，我们想要正确地分类我们所有的观察值。如果我们假设我们的数据是<a class="ae le" href="http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">独立且同分布的</a>，我们可以取所有我们单独计算的概率的乘积，这就是我们想要最大化的值。所以在数学中，如果我们定义逻辑函数和 x 为:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f29356a3c8616561a0316bcc729b7b85.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*t55xPJaM1ojSZJHAabwOcA.png"/></div></figure><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1b3afe523a53a148342866c515ae9ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/1*ClULnb6EZX6z-KKM5x4Z8A.gif"/></div></figure><p id="0f5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可以简化为:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/55ba5cce923d49df8322861baae8991f.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/1*ua9Uv68HVNfKf1IzrZ1B6A.gif"/></div></figure><p id="f647" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">∏符号表示将产品归类为该密码进行观察。在这里，我们利用我们的数据被标记的事实，所以这被称为监督学习。另外，你会注意到，对于弱观测值，我们取 1 减去逻辑函数。这是因为我们试图找到一个最大化的值，由于弱观测值的概率应该接近于零，1 减去概率应该接近于 1。所以我们现在有了一个我们试图最大化的价值。通常，人们通过使其为负来将其转换为最小化:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi no"><img src="../Images/96b9e859897e02a8173d8e6a1f54fb0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/1*WC0xeAISd1W3ltw0FVRa4Q.gif"/></div></figure><p id="b701" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意:最小化负面和最大化正面是一样的。上面的公式将被称为我们的成本函数。</p><h1 id="266c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">步骤 2 —渐变</h1><p id="e0ac" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">现在我们有一个要最小化的值，但是我们实际上如何找到最小化我们的成本函数的𝛽β值呢？我们要不要试一堆？这似乎不是一个好主意…</p><p id="3a89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是<a class="ae le" href="http://en.wikipedia.org/wiki/Convex_optimization" rel="noopener ugc nofollow" target="_blank">凸优化</a>发挥作用的地方。我们知道物流成本函数是<a class="ae le" href="http://en.wikipedia.org/wiki/Convex_function" rel="noopener ugc nofollow" target="_blank">凸</a>——请相信我。因为它是凸的，它有一个全局最小值，我们可以使用<a class="ae le" href="http://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>收敛到这个最小值。</p><p id="5008" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一个凸函数的图像:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi np"><img src="../Images/fb3c43312e45e1f53c66445a2b822b54.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/1*KCrpYecidJbX2yKf3xTKYQ.gif"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">source: utexas.edu</figcaption></figure><p id="bd98" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在你可以想象，这条曲线是我们上面定义的成本函数，如果我们在曲线上选择一个点，然后沿着它下降到最小值，我们最终会达到最小值，这就是我们的目标。这里的是一个动画。这就是梯度下降背后的思想。</p><p id="ee09" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们跟踪曲线的方法是，计算梯度，或者成本函数对每个𝛽.的一阶导数所以让我们做些数学计算。首先认识到，我们也可以将成本函数定义为:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/341e895fdbf6fedc7e979dc97d958482.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/1*xV-NSovSXbLyI3akGCjWGQ.gif"/></div></figure><p id="a288" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是因为当我们取对数时，我们的乘积变成了一个和。参见<a class="ae le" href="http://www.mathwords.com/l/logarithm_rules.htm" rel="noopener ugc nofollow" target="_blank">日志规则</a>。如果我们定义𝑦𝑖在强观察时为 1，弱观察时为 0，那么我们只对强观察做 h(x ),对弱观察做 1 — h(x)。让我们对这个新版本的成本函数，对𝛽0.求导记住我们的𝛽0 在我们的 x 值里。所以记住 log(x)的导数是 1/𝑥，所以我们得到(对于每次观察):</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0959772ff9fb9858656b254051e0a0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*FIk8w9dGEEvsMKMORzxSVQ.png"/></div></figure><p id="7493" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<a class="ae le" href="https://www.math.hmc.edu/calculus/tutorials/quotient_rule/" rel="noopener ugc nofollow" target="_blank">商法则</a>我们看到 h(x)的导数是:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/fb0d4d2e054d2d75ab43975c51956a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*4Jk2Jje0AKPhbpfKOZCT2A.png"/></div></figure><p id="ba89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">x 对𝛽0 的导数是 1。综上所述，我们得到:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7fe9e02073d9132b4138a465f81d742d.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*0V5_XnOcqtJIqE7tSBuadw.png"/></div></figure><p id="b0b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简化为:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nu"><img src="../Images/bc16cdbab103ad7ae0025021ea022410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyOqe3F4Ol_O4VHv_8hApQ.png"/></div></div></figure><p id="c0f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">引入负数和，我们得到对𝛽0 的偏导数是:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a1673a0d9141744cb70276057ca5afdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/1*xm7aKxLjzb1G5vLeFnE0Hw.gif"/></div></figure><p id="3caf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在其他的偏导数就简单了。唯一的变化是𝑥𝑖的导数不再是 1。对𝛽1 来说是 CC𝑖，对𝛽2 来说是倪。所以𝛽1 的偏导数是:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8dd56503068d7a2038f1e2dabecffc35.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/1*fkDdufklgbYR9IhsglnxWw.gif"/></div></figure><p id="2db4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了𝛽2:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b54062c42fe8b04398a8c0204f8d7528.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/1*4AvCKdbROgXJESuXHhd_hw.gif"/></div></figure><h1 id="c162" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">步骤 3 —梯度下降</h1><p id="b56e" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">现在我们有了梯度，我们可以使用梯度下降算法来找到最小化成本函数的𝛽s 值。梯度下降算法非常简单:</p><ul class=""><li id="0f03" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated">最初猜测您的𝛽值的任何值</li><li id="d259" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">重复直到收敛:</li><li id="6834" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">相对于𝛽𝑖的𝛽𝑖=𝛽𝑖−(𝛼∗梯度)</li></ul><p id="17e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里𝛼是我们的学习率。基本上，我们的成本曲线需要多大的步骤。我们现在做的是取当前的𝛽值，然后减去梯度的一部分。我们做减法是因为梯度是最大增加的方向，但是我们想要最大减少的方向，所以我们做减法。换句话说，我们在成本曲线上选取一个随机点，通过使用梯度的负值来检查我们需要向哪个方向靠近最小值，然后更新我们的𝛽值以向最小值靠近。重复直到收敛意味着不断更新我们的𝛽值，直到我们的成本值收敛或停止下降，这意味着我们已经达到了最小值。此外，同时更新所有𝛽值也很重要。这意味着您使用相同的先前的𝛽值来更新所有接下来的𝛽值。</p><h1 id="3a5d" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">梯度下降技巧</h1><p id="2870" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">标准化变量:</p><ul class=""><li id="8f9a" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated">这意味着每个变量减去平均值并除以标准差。</li><li id="4a4f" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">学习率:如果不收敛，学习率需要更小——但是收敛需要更长的时间。值得尝试的好值…，. 001，. 003，. 01，. 03，. 1，. 3，1，3，…</li><li id="befe" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">如果成本下降小于 10^−3，则声明收敛(这只是一个不错的建议)</li><li id="1d35" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">绘制收敛图作为检查</li></ul><p id="8910" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遵循代码来实现。从零开始建立模型后，我们得到了 76%的准确率，我使用了 sklearn 软件包，得到了 99%的准确率，这相当不错。</p><h1 id="2a92" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">高级优化</h1><p id="4b1d" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">所以梯度下降是学习𝛽值的一种方法，但是还有其他的方法。基本上，这些是更高级的算法，我不会解释，但一旦你定义了你的成本函数和梯度，就可以很容易地在 Python 中运行。这些算法是:</p><ul class=""><li id="7f63" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated"><a class="ae le" href="http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_bfgs.html" rel="noopener ugc nofollow" target="_blank"> BFGS </a></li><li id="34c3" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><a class="ae le" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html" rel="noopener ugc nofollow" target="_blank"> L-BFGS </a>:类似 BFGS，但使用有限的内存</li><li id="ab71" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><a class="ae le" href="http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_cg.html" rel="noopener ugc nofollow" target="_blank">共轭梯度</a></li></ul><p id="2ea6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我会把它留在这里让你去探索。我建议没有多少人知道这些先进的优化技术，而是他们很好地工作与梯度下降及其变种，因为他们做了相当不错的工作。</p><h1 id="805e" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">评估分类</h1><p id="d282" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">现在，我们已经花了一些时间来理解逻辑回归背后的数学，让我们看看如何评估分类问题。我们现在可以拟合一个逻辑回归模型。逻辑回归也可以利用我们在线性回归文章中使用的正则化技术。它们的使用方式完全相同——我们通过向成本函数添加 L1 或 L2 范数来惩罚大系数。</p><p id="681a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们不会像线性回归一样在这里演示超参数优化的 CV，但这仍然适用。事实上，我们在上一篇文章中讨论的几乎所有技术都可以在这里使用。我们将讨论新的话题。我们已经知道了前一篇文章中的训练和测试数据。让我们看看这个模型在测试数据上表现如何。</p><h1 id="7d65" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">准确(性)</h1><p id="4d68" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">分类时最容易理解的度量之一是准确性:您正确预测的分数是多少。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ny"><img src="../Images/a3d9ea84687f434fada3024e5c03d0f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QI5gqo8KcgC96Ssf5PUr8g.png"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Excellent!</figcaption></figure><p id="808d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">准确性的一个缺点是，当你有<strong class="kk iu">不平衡的</strong>类时，这是一个相当<strong class="kk iu">可怕的</strong>度量。让我们来看看我们的阶级平衡:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a8f1b72090678974109e0f7334d39cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*Ypcqpf9-DQWus802iwX98A.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Class Imbalance</figcaption></figure><p id="ec45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们大约 88%的数据是负面的——因此，如果我们只是预测一切都是负面的，我们的准确率将是 88%左右！还不错。想象一下其他数据，其中负类只出现了 1%的时间——你可以通过预测所有事物都是正的来获得 99%的准确率。这种准确性可能感觉很好，但如果这是你的模型，它是非常没有价值的。</p><h1 id="7b34" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">精确度、召回率和 F1</h1><p id="27cf" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">为了处理其中的一些问题，您经常会看到使用精度、召回率和 F1 来评估分类任务。</p><p id="571e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">精确度</strong>是真阳性的数量除以所有的阳性预测(真阳性加上假阳性):基本上，当我们预测某事为阳性时，我们的预测有多精确。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/f6ea41c8fae6f57ee83142a14b4b3330.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/1*z1eYt-j4xt_GJ08Nig_upw.gif"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Precision</figcaption></figure><p id="851a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">回忆</strong>是真阳性的数量除以所有实际阳性的数量(真阳性加上假阴性):基本上，我们实际预测了所有阳性的多少。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/ee64b2dd69d29ec41c949f786798f581.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/1*sTZX_qWeMss9rqsc0FFgGA.gif"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Recall</figcaption></figure><p id="33b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两个指标相互之间有所取舍。对于同一个模型，你可以以召回为代价提高精度，反之亦然。通常，您必须确定对于手头的问题哪个更重要。我们希望我们的预测是精确的还是高召回率的？</p><p id="1b63" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">什么是真/假阳性/阴性？我们来看一个<a class="ae le" href="https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">混淆矩阵</strong> </a>。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/f879c9147e681af11adaf60cec2b74fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*TumQSQsQoXdBHPFOsAubsA.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Confusion matrix</figcaption></figure><p id="9bde" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们测试预测的混淆矩阵。这些行就是真相。因此，第 0 行是实际的 0 标签，第 1 行是实际的 1 标签。列是预测。因此，单元格 0，0 计算我们正确得到的 0 类的数量。这被称为真正的否定(假设我们认为标签 0 是否定标签)。单元格 1，1 计算我们得到正确的 1 类的数量—真阳性。单元格 0，1 是我们的假阳性，单元格 1，0 是假阴性。</p><p id="a7d2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你可能知道的，混淆矩阵对于错误分析是非常有用的工具。在这个例子中，我们有非常对称的错误——两个类中都有四个错误。情况并不总是这样，通常会有两个以上的类，因此了解哪些类会让其他人感到困惑，对于改进模型非常有用。</p><p id="705c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有时候你只想在精确度和召回率之间取得平衡。如果这是你的目标，<strong class="kk iu"> F1 </strong>是一个非常常见的指标。这是精确和回忆的调和平均值:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi od"><img src="../Images/89dc7c45ca67f846ffa98f9a31e36024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*sJMessLqVw0BFL-Daq0wfQ.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Harmonic mean of precision and recall</figcaption></figure><p id="f0ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用调和平均值是因为它强烈倾向于被平均的最小元素。如果 precision 或 recall 是 0，那么 F1 是 0。最好的 F1 成绩是 1。</p><h1 id="48a8" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">精度/召回曲线</h1><p id="83b9" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">对于之前的所有指标，我们一直使用二元预测。但是真正的逻辑回归返回概率，这是非常有用的。</p><p id="a3af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在每个预测不是一个 1/0 的数字，我们有两个数字:被分类为 0 的概率和被分类为 1 的概率。这两个数字的和必须是 1，所以 1 的概率= 0 的概率。一件有用的事情是确保这些概率实际上似乎与类别相关联:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2ef7e331084624d9b9c2f111fc51b5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*XmeOaCvbCSky0odmERt3fQ.png"/></div></figure><p id="9e73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这张图向我们展示的是，对于我们的训练数据，我们的积极类确实有很高的概率是积极的，而我们的消极类有很高的概率是消极的。这基本上是我们设计算法的目的，所以这是可以预料的。检查验证集是否如此也是很好的。</p><p id="95fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Sklearn 的 predict 函数只是把概率最高的类作为预测类，这个方法还不错。然而，人们可能想要一个非常精确的模型来进行积极的预测。我们可以这样做的一个方法是，只有当概率超过 90%时，才称之为肯定的。为了理解不同截止值下精确度和召回率之间的权衡，我们可以绘制它们:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ab5208d0cbadb6d01fe6f2d5071329a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*j1zi1rIyAiy8H58agFHnsw.png"/></div></figure><p id="20ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不错！当我们调整阈值时，我们可以清楚地看到 P 和 R 之间的权衡。事实上，我们可以找到最大化 F1 的截止点(<strong class="kk iu">注</strong>:我们在这里使用测试数据，这在实践中并不好。我们希望使用验证集来选择临界值)</p><h1 id="9ec5" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">受试者工作特征曲线</h1><p id="c972" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">ROC 曲线是评估分类任务的另一种流行方法。它是一个图，y 轴是召回值(或真阳性率)，x 轴是假阳性率(假阳性除以所有实际阴性)。因此，它显示了您的模型如何在这两个值之间进行权衡。您的值越靠近图表的左上角越好。让我们来看看我们的测试数据:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi og"><img src="../Images/15bef3e4a70ba5ba64b94041a5eead93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*0u_eeRE1CQc3j_AWnkpy8w.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">ROC curve USP: It would work even with highly imbalanced data</figcaption></figure><p id="80cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，我们的模型做得非常好。您在右下角看到的 AUC(曲线下面积)分数是 ROC 曲线下的面积，1 是最好的值。我们有一个。如果您知道想要在 TPR 和 FPR 之间做出的权衡，该曲线也可用于选择阈值。</p><h1 id="00fa" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">多类</h1><p id="a159" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">逻辑回归也可以处理 2 个以上的类。有两种方法可以做到这一点:</p><ul class=""><li id="b87f" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier" rel="noopener ugc nofollow" target="_blank"> One-vs-rest </a>方法:使用这种方法，我们可以为每个类训练一个分类器，当该类是活动的时，正值，而对于所有其他类，负值。</li><li id="1c24" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><a class="ae le" href="https://datascience.stackexchange.com/questions/20296/cross-entropy-loss-explanation" rel="noopener ugc nofollow" target="_blank">交叉熵</a>损失:我们也可以改变我们的损失函数来合并多个类。最常见的方法是使用交叉熵损失:</li></ul><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7d3151e675865e38216b3842e809d181.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*lnxsL8vl3br8OYGUMPKukQ.png"/></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Cross entropy loss</figcaption></figure><p id="3ca9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种损失采用正确类别的预测概率的负平均值。因此，优化它试图获得尽可能高的正确类别的预测概率。</p><h1 id="3819" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">解释逻辑回归</h1><p id="a054" class="pw-post-body-paragraph ki kj it kk b kl nd ju kn ko ne jx kq kr nf kt ku kv ng kx ky kz nh lb lc ld im bi translated">解释逻辑回归的系数比解释线性回归的系数要稍微复杂一些。让我们来看看我们的系数:</p><p id="ba6c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们最正的系数是 char_counts 的值 5(粗略估计)。由于我们使用对数损失，我们需要通过 e^5 来转换这个值，即 147.52。也就是说，char_counts 每增加 1 个单位，密码是强密码的几率就会增加 14752%，因为我们现在已经转换为一个几率比。关于这个的更多细节，请看这个<a class="ae le" href="https://stats.idre.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/" rel="noopener ugc nofollow" target="_blank">的帖子</a>。</p><p id="bfe3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">问题答案</strong></p><p id="e7c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我想用一个例子来说明。假设一个 2 类分类问题和 6 个实例。</p><p id="3f5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设，真实概率= [1，0，0，0，0]</p><p id="cc08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">案例 1: </strong>预测概率= [0.2，0.16，0.16，0.16，0.16，0.16，0.16]</p><p id="234d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">情况二:</strong>预测概率= [0.4，0.5，0.1，0，0，0]</p><p id="73a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">情况 1 和情况 2 的均方误差分别为<strong class="kk iu"> 0.128 </strong>和<strong class="kk iu"> 0.1033 </strong>。</p><p id="a494" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管如果 0.5 被认为是阈值，情况 1 在预测该实例的类时更正确，但是情况 1 中的损失高于情况 2 中的损失。这是为什么呢？MSE 是一种度量标准，用于测量预测值与实际值的标准偏差。而且它的应用可能仅限于二元分类。它仍然可能在特定的情况下使用。RMSE 是衡量分类器性能的一种方式。错误率(或错误分类的数量)是另一个因素。如果我们的目标是一个低错误率的分类器，RMSE 是不合适的，反之亦然。这解释了为什么我们不使用 MSE/RMSE 作为分类问题的评估指标。其他度量比 MSE 提供了更多的灵活性和对分类器性能的理解。</p><p id="efd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">代号:</strong> <a class="ae le" href="https://nbviewer.jupyter.org/github/hdev7/medium-article-logistic-regression-follow-up-notebook/blob/master/logistic%20regression%20bottoms%20up%20approach.%20%28Feature%20engineering%20ideology%20-%20a%20bonus%29.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a></p><p id="470c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我的文章关于<a class="ae le" href="https://medium.com/@hemanthsaid7/linear-regression-bottoms-up-approach-intro-to-spark-a-bonus-b923ae594323" rel="noopener">线性回归</a></p><p id="d97e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你需要任何帮助，你可以联系:<a class="ae le" href="https://www.linkedin.com/in/hemanthsaid/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><p id="379e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="e5fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Geron Aurelien 的动手机器学习</p></div></div>    
</body>
</html>