<html>
<head>
<title>Dynamic Meta Embeddings in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 中的动态元嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dynamic-meta-embeddings-in-keras-42393d246963?source=collection_archive---------22-----------------------#2019-11-28">https://towardsdatascience.com/dynamic-meta-embeddings-in-keras-42393d246963?source=collection_archive---------22-----------------------#2019-11-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="042a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解不同嵌入的有价值的组合</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/811f3f6d6362e0828b473b306864e8cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SxNSdbmj3IP1e9nw"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@khushi22?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Khushbu hirpara</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="00b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">许多 NLP 解决方案利用预先训练的单词嵌入。选择使用哪一种通常与最终性能有关，并且是在大量试验和手动调谐之后实现的。脸书的人工智能实验室一致认为，做出这种选择的最佳方式是让神经网络自己找出答案。</p><p id="9dc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们引入了<a class="ae ky" href="https://arxiv.org/abs/1804.07983" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="lv">动态元嵌入</em> </strong> </a> <em class="lv">，这是一种简单而有效的嵌入集成监督学习方法，可以在同一模型类中的各种任务上实现最先进的性能。这个简单但非常有效的方法允许学习一组选择的单词嵌入的线性组合，这优于各种嵌入的简单串联。</em></p><p id="6cfd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，作者在 NLP 领域的各种任务上证明了他们的解决方案的有效性。我们限制自己在一个文本分类问题中采用这些技术，其中我们有 2 个预训练的嵌入，并希望智能地组合它们以提高最终的性能。</p><h1 id="b88b" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据</h1><p id="7b2b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我在<a class="ae ky" href="https://www.kaggle.com/yufengdev/bbc-fulltext-and-category" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上发现了一个有价值的数据集，其中包含了来自 BBC 档案馆的文章全文(总共 2225 篇)。新闻属于 5 个主题领域:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/dd77b2db5bc50cbc41d6a8dddf3f5b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*dpHX0wH3V0ZDqdLa6fJkYA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Label distribution</figcaption></figure><p id="8b8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的目标是对它们进行正确分类，为此，我们希望训练不同类型的嵌入，智能地组合它们，并在此基础上构建我们的分类器。开始时，对原始语料库应用标准的清洗程序。作为嵌入模型，我选择了最常见的类型:Word2Vec 和 FastText。我们可以使用 Gensim 轻松地训练它们，将它们“拖放”到 Keras 架构中(记住为每个选定的嵌入框架保持相同的嵌入大小)。我手工仔细计算了这个过程，以便在需要时控制填充过程:Keras 的 Tokenizer 对象和 pad_sequence 函数使所有事情变得简单。</p><p id="7884" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们以多次训练的嵌入(也是像手套或类似的模型的预训练形式是完美的)和顺序语料库结束时，我们准备好组合我们的权重。</p><h1 id="17ce" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">模特们</h1><p id="fb0c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在原始论文中介绍了两种不同的技术:</p><ul class=""><li id="be49" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu"> <em class="lv">动态元嵌入(DME) </em> </strong> <em class="lv"> : </em>原始嵌入被投影到新的空间中，通过 LSTM 编码器添加额外的可学习权重，遵循注意机制。然后将它们与其原始格式线性组合。用喀拉斯语说:</li></ul><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="2122" class="ni lx it ne b gy nj nk l nl nm">def DME(maxlen):</span><span id="de97" class="ni lx it ne b gy nn nk l nl nm">    inp = Input(shape=(maxlen, 100, 2))<br/>    x = Reshape((maxlen, -1))(inp)<br/>    x = LSTM(2, return_sequences=True)(x)<br/>    x = Activation('sigmoid')(x)<br/>    x = Reshape((maxlen, 1, 2))(x)<br/>    x = multiply([inp, x])<br/>    out = Lambda(lambda t: K.sum(t, axis=-1))(x)</span><span id="9238" class="ni lx it ne b gy nn nk l nl nm">    return Model(inp, out)</span></pre><ul class=""><li id="1c2c" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated"><strong class="lb iu"> <em class="lv">【上下文动态元嵌入(CDME) </em> </strong> <em class="lv"> : </em>如上，<em class="lv"> </em>原始嵌入被投影到一个新的空间，增加额外的可学习权重；但是现在通过 BiLSTM-Max 编码器应用了上下文相关的系统。最终追求的是自我关注机制和与原格式的加权组合。在 Keras 语言中(没有提供 Keras 双向层的最大池合并，所以我们必须自己编码):</li></ul><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="2690" class="ni lx it ne b gy nj nk l nl nm">def CDME(maxlen, latent_dim=2):</span><span id="c8c8" class="ni lx it ne b gy nn nk l nl nm">    inp = Input(shape=(maxlen, 100, 2))<br/>    x = Reshape((maxlen, -1))(inp)<br/>    x = Bidirectional(LSTM(latent_dim, return_sequences=True))(x)<br/>    x = Lambda(lambda t: [t[:,:,:int(latent_dim/2+1)],  <br/>                          t[:,:,int(latent_dim/2+1):]])(x)<br/>    x = Maximum()(x)<br/>    x = Activation('sigmoid')(x)<br/>    x = Reshape((maxlen, 1, 2))(x)<br/>    x = multiply([inp, x])<br/>    out = Lambda(lambda t: K.sum(t, axis=-1))(x)</span><span id="e7df" class="ni lx it ne b gy nn nk l nl nm">    return Model(inp, out)</span></pre><p id="f6ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们重新创建了两个通用代码块，它们通过一个动态过程来执行嵌入组合。这两种解决方案都可以放在网络的起点，紧接在我们的嵌入的读取和连接之后。在它们上面，我们可以根据不同的目的叠加正常的图层。在我们的例子中，我们添加一些递归层来正确分类我们的新闻文章。我们以这两种架构结束:</p><p id="e1fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu"> DME </strong>:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="576c" class="ni lx it ne b gy nj nk l nl nm">concat_inp = Concat_Emb([embedding_matrix_w2v, embedding_matrix_ft], maxlen=max_len)<br/>dme = DME(max_len)<br/>x = dme(concat_inp.output)<br/>x = GRU(128, dropout=0.2, return_sequences=True)(x)<br/>x = GRU(32, dropout=0.2)(x)<br/>out = Dense(y.shape[1], activation='softmax')(x)</span><span id="ff93" class="ni lx it ne b gy nn nk l nl nm">dme_model = Model(concat_inp.input, out)<br/>dme_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="72f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu"> CDME: </strong></p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="5f3a" class="ni lx it ne b gy nj nk l nl nm">concat_inp = Concat_Emb([embedding_matrix_w2v, embedding_matrix_ft], maxlen=max_len)<br/>cdme = CDME(max_len)<br/>x = cdme(concat_inp.output)<br/>x = GRU(128, dropout=0.2, return_sequences=True)(x)<br/>x = GRU(32, dropout=0.2)(x)<br/>out = Dense(y.shape[1], activation='softmax')(x)</span><span id="4dfd" class="ni lx it ne b gy nn nk l nl nm">cdme_model = Model(concat_inp.input, out)<br/>cdme_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="d9c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经准备好执行培训，并看到一些结果。这两个模型都能够在测试数据上达到大约 93%的总体准确率，并且每个类别都有很高的召回分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/c3bbc85425bee6bd81fc3219e6161b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUHWkGkW02yUPOC6dQ2K1g.png"/></div></div></figure><h1 id="4d08" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="dcb1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">允许访问多种类型的嵌入(不管它们是预先训练的还是特别构建的)，我们允许神经网络通过预测每种嵌入类型的权重来学习它偏好的嵌入(<em class="lv"> DME </em>)，可选地取决于上下文(<em class="lv"> CDME </em>)。我们在文本分类的 NLP 任务中实现了这一点，只是简单地将这一过程与处理这类问题的常规方法结合起来。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="7a6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的 GITHUB 回购</strong> </a></p><p id="733b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="9602" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="ab8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">改进的句子表征的动态元嵌入:<em class="lv">杜维基拉，王和赵京贤；脸书人工智能研究；纽约大学；CIFAR 全球学者。</em></p></div></div>    
</body>
</html>