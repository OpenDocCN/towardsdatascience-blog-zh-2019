<html>
<head>
<title>Exercise Classification with Machine Learning (Part II)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的运动分类(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exercise-classification-with-machine-learning-part-ii-d60d1928f31d?source=collection_archive---------20-----------------------#2019-07-29">https://towardsdatascience.com/exercise-classification-with-machine-learning-part-ii-d60d1928f31d?source=collection_archive---------20-----------------------#2019-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3a07" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇分为两部分的文章中，我们将深入探讨一个具体问题:对人们进行各种锻炼的视频进行分类。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ce8c0411d77feaf2f18168ae3b26705b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBky-yyktU3SbbdQM0aTdQ.png"/></div></div></figure><p id="9872" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<a class="ae lq" href="https://medium.com/@trevor.j.phillips/exercise-classification-with-machine-learning-part-i-7cc336ef2e01" rel="noopener">的上一篇文章</a>中，我们重点关注了一种更为<strong class="kw iu">的算法</strong>方法，使用<em class="lr">k-最近邻</em>对未知视频进行分类。在这篇文章中，我们将关注一种专门的<strong class="kw iu">机器学习</strong> (ML)方法。</p><p id="8d9a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将要讨论的所有内容的代码都可以在这个  GitHub 库的<a class="ae lq" href="https://github.com/trevphil/TechniqueAnalysis" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">中找到。算法方法(<a class="ae lq" href="https://medium.com/@trevor.j.phillips/exercise-classification-with-machine-learning-part-i-7cc336ef2e01" rel="noopener">第一部分</a>)用 Swift 编写，可作为<a class="ae lq" href="https://cocoapods.org/pods/TechniqueAnalysis" rel="noopener ugc nofollow" target="_blank"> CocoaPod </a>获得。ML 方法(第二部分)是用 Python/TensorFlow 编写的，可以作为 GitHub 资源库的一部分找到。</strong></a></p><h1 id="664e" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">背景</h1><p id="5021" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">视频分类与图像分类的根本区别在于我们必须考虑<em class="lr">时间维度</em>。无论我们使用什么样的 ML 模型，它都需要学习具有时间成分的特征。为此开发了各种架构，现在我们将讨论其中一些。</p><h2 id="f777" class="mp lt it bd lu mq mr dn ly ms mt dp mc ld mu mv me lh mw mx mg ll my mz mi na bi translated">递归神经网络</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/79117c8ddc2dea35a65552bd34334f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzMsZQlaLOL9mR5-qOuxvg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Unrolled RNN with many-to-one architecture (<a class="ae lq" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="c4f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">rnn 非常适合输入数据序列，比如文本，或者在我们的例子中，是视频帧。RNN 的每一步都是特定时间点输入的函数，也是前一步的“隐藏状态”。以这种方式，RNN 能够学习时间特征。权重和偏差通常在步骤之间共享。</p><p id="f755" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">rnn 的缺点是它们经常遭受消失/爆炸梯度问题。另外，反向传播在计算上是昂贵的，因为我们必须通过网络中的所有步骤进行传播。</p><h2 id="afb2" class="mp lt it bd lu mq mr dn ly ms mt dp mc ld mu mv me lh mw mx mg ll my mz mi na bi translated">长短期记忆网络</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/3e9280d3a2aa01a705c4aaf81c6d91d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5J4kWIF4SeDf2LfXofwhSQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk"><a class="ae lq" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><p id="0c64" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">LSTMs 是普通 rnn 的一个更复杂的变体，旨在捕获长期依赖关系。LSTM 的一个“单元”包含 4 个可训练门:</p><ul class=""><li id="a81c" class="nh ni it kw b kx ky la lb ld nj lh nk ll nl lp nm nn no np bi translated"><em class="lr">遗忘门</em>:一个 sigmoid 函数，确定从先前单元的状态中“遗忘”什么</li><li id="e6c0" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><em class="lr">输入门</em>:一个 sigmoid 函数，决定我们将哪些输入值写入单元格</li><li id="758b" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><em class="lr">双曲正切门</em>:双曲正切函数，将输入门的结果映射为-1 和 1 之间的值</li><li id="2ef5" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><em class="lr">输出门</em>:一个 sigmoid 函数，过滤前面门的输出</li></ul><p id="c34a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一般来说，LSTM 架构有助于防止消失梯度，因为它的附加相互作用。</p><h2 id="2b6f" class="mp lt it bd lu mq mr dn ly ms mt dp mc ld mu mv me lh mw mx mg ll my mz mi na bi translated">三维卷积神经网络(CNN)</h2><p id="2e4b" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">神经网络的 2D 卷积层使用一组“过滤器”(内核)，这些过滤器滑过输入数据并将其映射到输出空间。下面，我们可以看到一个过滤器的动画示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/dce9fe74bd31341e922b2b52c68514d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/1*Ff6mG3aPdWFkjb6hZPy3Xw.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk"><a class="ae lq" href="https://mlnotebook.github.io/img/CNN/convSobel.gif" rel="noopener ugc nofollow" target="_blank">https://mlnotebook.github.io/img/CNN/convSobel.gif</a></figcaption></figure><p id="3f29" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将 2D 卷积层扩展到时间维度相对简单，我们有一个 3D 输入，其中第三维是时间，而不是 2D 滤波器，我们使用一个 3D 滤波器(想象一个小盒子)，它在 3D 输入(一个大盒子)中滑动，执行矩阵乘法。</p><p id="4aa9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过这个简单的 3D 膨胀，我们现在可以(希望)使用 CNN 来学习时间特征。然而，将内核扩展到 3D 意味着我们有更多的参数，因此模型变得更加难以训练。</p><h1 id="94c2" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">膨胀的 3D ConvNet (I3D)</h1><p id="3d43" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">让我们回到文章的目标:对人们进行锻炼的视频进行分类。我选择了最后一个架构的变体，3D CNN，基于论文“<a class="ae lq" href="https://arxiv.org/abs/1705.07750" rel="noopener ugc nofollow" target="_blank"> Quo Vadis，动作识别？新模型和动力学数据集</a>。</p><p id="75e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，作者介绍了一种称为“膨胀 3D conv nets”(I3D)的新架构，该架构将过滤器和池层扩展到 3D 中。I3D 模型基于<a class="ae lq" href="https://arxiv.org/abs/1409.4842" rel="noopener ugc nofollow" target="_blank"> Inception v1 </a>和<a class="ae lq" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">批量规格化</a>，因此非常深入。</p><h2 id="4cd2" class="mp lt it bd lu mq mr dn ly ms mt dp mc ld mu mv me lh mw mx mg ll my mz mi na bi translated">迁移学习</h2><p id="a002" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们训练 ML 模型变得善于检测数据中的特定特征，例如边缘、直线、曲线等。如果两个领域相似，则模型用于检测一个领域中的特征的权重和偏差对于检测另一个领域中的特征通常很有效。这叫做<strong class="kw iu">迁移学习</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b3eba3db515e169bdc5cc9878383dce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUFBrK-vFOJQd1SHqq3U4Q.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">General guidelines for transfer learning</figcaption></figure><p id="0e27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">I3D 的创造者依靠迁移学习，使用 Inception v1 模型的权重和偏差，该模型在 ImageNet 上预先训练。当将 2D 模型膨胀到 3D 时，他们只是简单地将每个 2D 层的权重和偏差“叠加”起来，形成第三维度。以这种方式初始化权重和偏差(与从头开始训练相比)提高了测试精度。</p><p id="13f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最终的 I3D 架构是在<a class="ae lq" href="https://deepmind.com/research/open-source/open-source-datasets/kinetics/" rel="noopener ugc nofollow" target="_blank"> Kinetics </a>数据集上训练出来的，该数据集是 YouTube 上 400 多个人类动作和每个动作 400 多个视频样本的大规模汇编。鉴于动力学数据集和手头任务(对人们做运动的视频进行分类)之间的相似性，我相信使用<a class="ae lq" href="https://github.com/deepmind/kinetics-i3d" rel="noopener ugc nofollow" target="_blank">公开可用的 I3D 模型</a>进行迁移学习有很大的机会。</p><h2 id="d231" class="mp lt it bd lu mq mr dn ly ms mt dp mc ld mu mv me lh mw mx mg ll my mz mi na bi translated">数据预处理</h2><p id="eb15" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">先说数据预处理。I3D 模型使用双流架构，其中视频被预处理成两个流:<em class="lr"> RGB </em>和<em class="lr">光流</em>。我用来创建这些流的代码在这里<a class="ae lq" href="https://github.com/trevphil/TechniqueAnalysis/blob/master/video-classifier/process_video.py" rel="noopener ugc nofollow" target="_blank">可用</a>并且基于<a class="ae lq" href="https://github.com/deepmind/kinetics-i3d" rel="noopener ugc nofollow" target="_blank"> I3D GitHub repo </a>的实现细节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/0132a02aa283b95076c1e40c5f1f804c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OBhSxq8p--Lt9S-sb-S5Aw.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">RBG and optical flow visualizations of a long jump</figcaption></figure><p id="c10e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我使用 Python 的<code class="fe ny nz oa ob b">cv2</code>库中的函数<code class="fe ny nz oa ob b">calcOpticalFlowFarneback()</code>生成了光流数据。注意，I3D 模型不像在 RNN 或 LSTM 中那样具有显式递归关系，然而光流数据是隐式递归的，因此我们获得了类似的优点。</p><p id="49b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在测试时，通过添加具有相等权重的逻辑并使用结果来形成类预测，来组合这两个流。</p><h1 id="4944" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">进一步的细节</h1><p id="c2ee" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我剥离了 I3D 模型的最后两层(逻辑层和预测层)并用类似的层替换它们，这样新的逻辑层就有了我的用例的正确数量的<code class="fe ny nz oa ob b">output_channels</code>(下面列出了 6 个类)。</p><pre class="kj kk kl km gt oc ob od oe aw of bi"><span id="5fcb" class="mp lt it ob b gy og oh l oi oj">bw-squat_correct<br/>bw-squat_not-low<br/>pull-up_chin-below-bar<br/>pull-up_correct<br/>push-up_correct<br/>push-up_upper-body-first</span></pre><p id="1a8a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每个流的逻辑值、<em class="lr"> RGB </em>和<em class="lr">光流</em>被组合并用于形成类似于原始实现中的预测。我使用了一个由 128 个视频组成的数据集(不幸的是这个数据集很小),这些视频是从不同的摄像机角度录制的，我只对新图层<em class="lr">和</em>进行了反向传播，依靠现有图层的权重和偏差将视频映射到相关的特征中。</p><p id="7680" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我用的是批量<code class="fe ny nz oa ob b">1</code>，正则化强度<code class="fe ny nz oa ob b">0.25</code> (L 范数)，学习率<code class="fe ny nz oa ob b">5e-4</code>的<code class="fe ny nz oa ob b">tf.train.AdamOptimizer</code>。训练-测试分割为 80% — 20%，在训练数据集中，另外 20%被保留用于验证和超参数调整。TensorBoard 的计算图如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/7eab2a7f53dcef6a2cbd1c4242c4ce35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DdHAJS-lNAsc0oh4WxCwnQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Modified I3D computational graph from TensorBoard</figcaption></figure><h1 id="37fa" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">结果</h1><p id="c093" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">训练通常需要大约 30 个历元，直到损失达到稳定，在最好的情况下，最后 100 次迭代的平均损失是<code class="fe ny nz oa ob b">0.03058</code>。测试精度达到<code class="fe ny nz oa ob b">69.23%</code>。</p><p id="18c5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个项目的结果受到小数据集大小的限制，但随着更多的视频，我相信一个微调版本的 I3D 模型可以实现更高的测试精度。</p><p id="af41" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将端到端的 ML 模型集成到移动客户端提出了一个不同的挑战。例如，在 iOS 领域，模型必须采用 CoreML 可接受的格式。存在诸如<a class="ae lq" href="https://github.com/tf-coreml/tf-coreml" rel="noopener ugc nofollow" target="_blank"> tfcoreml </a>的工具，用于将 TensorFlow 模型转换为 coreml 模型，但是支持转换的 op 数量有限。</p><p id="1b19" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在撰写本文时，tfcoreml 不支持 T2 或任何三维操作，这意味着还不能将 I3D 模型集成到 iOS 应用程序中。</p></div></div>    
</body>
</html>