<html>
<head>
<title>Clustering metrics better than the elbow-method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">比肘法更好的聚类度量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6?source=collection_archive---------3-----------------------#2019-09-06">https://towardsdatascience.com/clustering-metrics-better-than-the-elbow-method-6926e1f723a6?source=collection_archive---------3-----------------------#2019-09-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e8d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们展示了使用什么样的度量来可视化和确定最佳的集群数量，这比通常的做法好得多——肘方法。</h2></div><h1 id="1483" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="d5ef" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">聚类是利用数据科学的商业或科学企业的机器学习管道的重要部分。顾名思义，它有助于识别数据块中密切相关(通过某种距离度量)的数据点的集合，否则很难理解。</p><p id="8f4a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">然而，大多数情况下，聚类过程属于<a class="ae mb" href="https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294" rel="noopener">无监督机器学习</a>的领域。无人监管的 ML 是一个混乱的行业。</p><p id="2b29" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">没有已知的答案或标签来指导优化过程或衡量我们的成功。我们处于未知的领域。</p><div class="mc md gp gr me mf"><a href="https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">人类的机器学习，第 3 部分:无监督学习</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">聚类与降维:k-means 聚类，层次聚类，PCA，SVD。</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">medium.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt mu mf"/></div></div></a></div><p id="9a72" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">因此，毫不奇怪，当我们问这个基本问题时，像<a class="ae mb" rel="noopener" target="_blank" href="/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1"> <strong class="lc iu"> k 均值聚类</strong> </a>这样的流行方法似乎不能提供完全令人满意的答案:</p><blockquote class="mv mw mx"><p id="0de1" class="la lb my lc b ld lw ju lf lg lx jx li mz ly ll lm na lz lp lq nb ma lt lu lv im bi translated">"<strong class="lc iu">从</strong>开始，我们如何知道集群的实际数量？"</p></blockquote><p id="d02d" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这个问题非常重要，因为聚类过程通常是进一步处理单个聚类数据的先导，因此计算资源的量可能取决于这种测量。</p><p id="6736" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在业务分析问题的情况下，反响可能会更糟。这种分析通常会进行聚类，目的是细分市场。因此，很容易想象，根据集群的数量，适当的营销人员将被分配到该问题。因此，对集群数量的错误评估会导致宝贵资源的次优分配。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/0563e81a62583a315ea9e0c1a2c2e6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*2MKKujl2jG7DLoZ6O-mK9w.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Source: <a class="ae mb" href="https://www.singlegrain.com/digital-marketing/strategists-guide-marketing-segmentation/" rel="noopener ugc nofollow" target="_blank">https://www.singlegrain.com/digital-marketing/strategists-guide-marketing-segmentation/</a></figcaption></figure><h1 id="86a9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">肘法</h1><p id="11ba" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于 k-means 聚类方法，回答这个问题最常见的方法是所谓的<a class="ae mb" href="https://bl.ocks.org/rpgove/0060ff3b656618e9136b" rel="noopener ugc nofollow" target="_blank">肘方法</a>。它包括在一个循环中多次运行该算法，增加聚类选择的数量，然后绘制聚类分数作为聚类数量的函数。</p><p id="8c80" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">肘法的得分或衡量标准是什么？为什么叫'<em class="my">肘</em>法？</p><p id="a82a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">典型图如下所示:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/c813013e966e0a0467288d78ce1f1724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wV1j-klQA1xFvfaNXuVzg.png"/></div></div></figure><p id="04dd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">通常，分数是 k-means 目标函数上的输入数据的度量，即<strong class="lc iu">某种形式的类内距离相对于类内距离</strong>。</p><p id="2009" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">例如，在 Scikit-learn 的<a class="ae mb" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> k 均值估计器</a>中，有一种<code class="fe ns nt nu nv b">score</code>方法可用于此目的。</p><p id="a3b5" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">但是再看剧情。有时候会让人困惑。在这里，我们应该将 4、5 还是 6 作为集群的最佳数量？</p><blockquote class="mv mw mx"><p id="1057" class="la lb my lc b ld lw ju lf lg lx jx li mz ly ll lm na lz lp lq nb ma lt lu lv im bi translated">不总是那么明显，是吗？</p></blockquote><h1 id="9e0f" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">轮廓系数——一个更好的指标</h1><p id="df7b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">使用每个样本的平均组内距离(<code class="fe ns nt nu nv b">a</code>)和平均最近组距离(<code class="fe ns nt nu nv b">b</code>)计算<a class="ae mb" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" rel="noopener ugc nofollow" target="_blank">轮廓系数</a>。样本的轮廓系数为<code class="fe ns nt nu nv b">(b - a) / max(a, b)</code>。澄清一下，<code class="fe ns nt nu nv b">b</code>是样本和样本不属于的最近聚类之间的距离。我们可以计算所有样本的平均轮廓系数，并将其用作判断聚类数量的度量。</p><p id="3bbd" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这是 Orange 关于这个话题的一个视频，</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="b4f3" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">举例来说，我们使用 Scikit-learn 的<code class="fe ns nt nu nv b">make_blob</code>函数在 4 个特征维度和 5 个聚类中心上生成随机数据点。因此，问题的基本事实是<strong class="lc iu">数据是围绕 5 个聚类中心</strong>生成的。然而，k-means 算法没有办法知道这一点。</p><p id="c22b" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">可以如下绘制聚类(成对特征),</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ny"><img src="../Images/628d3d6956ed4c3da6a77b3cba0467b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*589QI2NPYakYJty5i8m9kw.png"/></div></div></figure><p id="2ae8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">接下来，我们运行 k 均值算法，选择<em class="my"> k </em> =2 至<em class="my"> k </em> =12，计算每次运行的默认 k 均值得分和平均轮廓系数，并将它们并排绘制。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nz"><img src="../Images/dae4352f1c2899d41ec6fb33cec724f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxFPxOL-XJUsZ83Bnu31KA.png"/></div></div></figure><p id="b7c2" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">这种差异再明显不过了。平均轮廓系数增加到当<em class="my"> k </em> =5 时的点，然后对于更高的<em class="my"> k </em>值急剧减少，即<strong class="lc iu">它在<em class="my"> k </em> =5、</strong>处呈现出清晰的峰值，这是生成原始数据集的聚类数。</p><blockquote class="oa"><p id="38ce" class="ob oc it bd od oe of og oh oi oj lv dk translated">与弯管法中的平缓弯曲相比，轮廓系数呈现出峰值特征。这更容易想象和推理。</p></blockquote><p id="b1a5" class="pw-post-body-paragraph la lb it lc b ld ok ju lf lg ol jx li lj om ll lm ln on lp lq lr oo lt lu lv im bi translated">如果我们在数据生成过程中增加高斯噪声，聚类看起来会更加重叠。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi op"><img src="../Images/6d178ba6d2044ec6f5bbf703cb51b5c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bLymfNY9rcLOUtMDRwmO8g.png"/></div></div></figure><p id="8022" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">在这种情况下，使用 elbow 方法的默认 k-means 得分会产生更不明确的结果。在下面的肘部图中，很难选择一个合适的点来显示真正的弯曲。是 4，5，6，还是 7？</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi oq"><img src="../Images/12c6e11c511c17a4ff52a7586e67115e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aI_dkLIlXW9EvpYjYcA8iQ.png"/></div></div></figure><p id="8240" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">但是轮廓系数图仍然设法在 4 或 5 个聚类中心附近保持峰值特征，并使我们的生活更容易。</p><p id="02c0" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">事实上，如果你回头看重叠的聚类，你会看到大多数情况下有 4 个聚类可见-虽然数据是使用 5 个聚类中心生成的，但由于高方差，只有 4 个聚类在结构上显示出来。剪影系数很容易发现这种行为，并显示出介于 4 和 5 之间的最佳聚类数。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi or"><img src="../Images/1057272af713828af38b624507b5b530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3TFszJ3qaPvpWTZBPtYQ7A.png"/></div></div></figure><h1 id="d56e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">高斯混合模型下的 BIC 分数</h1><p id="47d8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">还有其他优秀的度量来确定聚类的真实计数，例如<a class="ae mb" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">B</strong>a<strong class="lc iu">I</strong>information<strong class="lc iu">C</strong>riterion</a>(<strong class="lc iu">BIC</strong>)但是只有当我们愿意将聚类算法扩展到 k-means 以外的更一般化的版本时，才能应用这些度量</p><p id="1a1e" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">基本上，GMM 将一个数据块视为多个具有独立均值和方差的高斯数据集的叠加。然后应用<a class="ae mb" href="https://www.geeksforgeeks.org/ml-expectation-maximization-algorithm/" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">E</strong>expectation-<strong class="lc iu">M</strong>axi mization(<strong class="lc iu">EM</strong>)算法</a>来近似确定这些均值和方差。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi os"><img src="../Images/cc999f2dca96244ebf62b5a372d0d500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TWOnWj2RtDFVhqelA1pHOw.png"/></div></div></figure><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-models-explained-6986aaf5a95"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">高斯混合模型解释</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">在机器学习领域，我们可以区分两个主要领域:监督学习和非监督学习。主要的…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ot l mq mr ms mo mt mu mf"/></div></div></a></div><h2 id="17c8" class="ou kj it bd kk ov ow dn ko ox oy dp ks lj oz pa ku ln pb pc kw lr pd pe ky pf bi translated">BIC 作为正规化的理念</h2><p id="75ff" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">你可能从统计分析或你以前与线性回归的互动中认识到术语 BIC。BIC 和 AIC(赤池信息标准)被用作变量选择过程的线性回归中的正则化技术。</p><blockquote class="oa"><p id="1728" class="ob oc it bd od oe of og oh oi oj lv dk translated">BIC/AIC 用于线性回归模型的正则化。</p></blockquote><p id="bb91" class="pw-post-body-paragraph la lb it lc b ld ok ju lf lg ol jx li lj om ll lm ln on lp lq lr oo lt lu lv im bi translated">这个想法同样适用于 BIC。理论上，极其复杂的数据聚类也可以建模为大量高斯数据集的叠加。对于为此目的使用多少高斯函数没有限制。</p><p id="1d88" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">但这类似于线性回归中增加模型复杂性，其中大量特征可用于拟合任何任意复杂的数据，但却失去了泛化能力，<strong class="lc iu">因为过于复杂的模型拟合了噪声而不是真实模式。</strong></p><p id="a472" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated"><strong class="lc iu">BIC 方法惩罚了大量的高斯分布</strong>，并试图保持模型足够简单，以解释给定的数据模式。</p><blockquote class="oa"><p id="95e6" class="ob oc it bd od oe of og oh oi oj lv dk translated">BIC 方法不利于大量的高斯模型，即过于复杂的模型。</p></blockquote><p id="e2e1" class="pw-post-body-paragraph la lb it lc b ld ok ju lf lg ol jx li lj om ll lm ln on lp lq lr oo lt lu lv im bi translated">因此，我们可以对一系列聚类中心运行 GMM 算法，BIC 得分将增加到一个点，但之后将随着惩罚项的增加而开始下降。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/a047877843f2ec956979ed98a216dc04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*gzJnyN5V12QXh9Y0L8QbUw.png"/></div></figure><h1 id="73c9" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">摘要</h1><p id="c10d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><a class="ae mb" href="https://github.com/tirthajyoti/Machine-Learning-with-Python/blob/master/Clustering-Dimensionality-Reduction/Clustering_metrics.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu">这里是本文的 Jupyter 笔记本</strong> </a>。随意叉着玩。</p><p id="47c8" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们讨论了常用的 elbow 方法的几个替代选项，用于在使用 k-means 算法的无监督学习设置中挑选正确数量的聚类。</p><p id="fc1a" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi translated">我们表明，侧影系数和 BIC 评分(来自 k-means 的 GMM 扩展)是视觉识别最佳聚类数的肘方法的更好替代方法。</p></div><div class="ab cl ph pi hx pj" role="separator"><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm pn"/><span class="pk bw bk pl pm"/></div><div class="im in io ip iq"><p id="1c50" class="pw-post-body-paragraph la lb it lc b ld lw ju lf lg lx jx li lj ly ll lm ln lz lp lq lr ma lt lu lv im bi po translated"><span class="l pp pq pr bm ps pt pu pv pw di">如果</span>您有任何问题或想法要分享，请通过<a class="ae mb" href="mailto:tirthajyoti@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="lc iu">tirthajyoti【AT】Gmail . com</strong></a>联系作者。此外，您可以查看作者的<a class="ae mb" href="https://github.com/tirthajyoti?tab=repositories" rel="noopener ugc nofollow" target="_blank"> <strong class="lc iu"> GitHub </strong> </a> <strong class="lc iu">资源库</strong>中 Python、R 和机器学习资源中其他有趣的代码片段。如果你像我一样对机器学习/数据科学充满热情，请随时<a class="ae mb" href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener ugc nofollow" target="_blank">在 LinkedIn 上添加我</a>或<a class="ae mb" href="https://twitter.com/tirthajyotiS" rel="noopener ugc nofollow" target="_blank">在 Twitter 上关注我。</a></p><div class="mc md gp gr me mf"><a href="https://www.linkedin.com/in/tirthajyoti-sarkar-2127aa7/" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Tirthajyoti Sarkar - Sr .首席工程师-半导体、人工智能、机器学习- ON…</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">通过写作使数据科学/ML 概念易于理解:https://medium.com/@tirthajyoti 开源和有趣…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">www.linkedin.com</p></div></div><div class="mo l"><div class="px l mq mr ms mo mt mu mf"/></div></div></a></div></div></div>    
</body>
</html>