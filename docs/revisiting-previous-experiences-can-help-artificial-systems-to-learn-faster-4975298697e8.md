# 重温以前的经历可以帮助人工系统更快地学习

> 原文：<https://towardsdatascience.com/revisiting-previous-experiences-can-help-artificial-systems-to-learn-faster-4975298697e8?source=collection_archive---------35----------------------->

## 优先体验重放建议混合新旧体验以加速学习

![](img/d640120349a986d1fb90d200905a7312.png)

Photo by [Robina Weermeijer](https://unsplash.com/@averey?utm_source=medium&utm_medium=referral)

e enforcement Learning(RL)是一个框架，它有很大的潜力成为“下一级”人工智能的基础，即所谓的人类级人工智能。在[1]中，Nils J. Nilsson 将这个下一级描述为:

> “[……]通用的、可教育的系统，可以学习并被教会执行人类可以执行的数千种工作中的任何一种。”

RL 成为实现这一目标的良好候选的原因很简单:它基于奖励的学习机制接近于人类大脑用于学习事物的机制。RL 代理通过试错来学习如何表现以最大化收到的奖励(可以想象一只狗从它的主人/训练员那里得到款待)。

尽管具有潜力，经典 RL 方法显示出许多局限性，即使能够解决许多复杂的问题，也远远不能实现这一通用的长期目标。

在寻求改进的过程中，许多研究人员提出了扩展 RL 的生物启发方法。下面描述的方法包括以有效的方式使用存储的存储器来加速学习。

# 记忆帮助我们学习

![](img/23b65de7d655ec686b4a2664c3b2ae5f.png)

Photo by [Conmongt](https://pixabay.com/users/Conmongt-1226108/)

神经科学领域的研究支持，包含先前经历的记忆在啮齿动物的海马体中重演，这在不同水平的大脑活动中观察到。

结果还显示，导致某种程度奖励的经历会更频繁地重演。更有趣的是，新鲜感还与重演某些经历的概率相关，如[2]所述:

> “[……]我们已经确定，例如，在新的环境和奖励驱动的空间任务中，多巴胺能释放对于偏向随后重放轨迹的内容是重要的，有可能加强新的位置细胞组合和位置-奖励关联。”

这意味着学习也是通过重温旧的经历来进行的，并且使用某种机制来选择更好的经历以优化学习过程，例如，选择呈现某种程度的新颖性或与收到的奖励相关联的经历。

# 体验回放

为了模仿生物系统中存在的这种经验机制，在[3]中提出了称为经验重放的方法，以及用于规划和教学的学习行为模型。这三种扩展方法被建议用来加速 AHC(自适应启发式批评家)和 Q-学习算法的学习。应用体验回放的动机是:

> “基本的 AHC 和 Q 学习算法[……]效率低下，因为通过反复试验获得的经验仅用于调整网络一次，然后就被丢弃。这是一种浪费，因为有些经历可能是罕见的，而有些经历(如涉及损害的经历)的获得成本很高。应该以有效的方式重复利用经验。”

假设是存储和处理以前的经验比与环境互动“更便宜”。

经验定义为四重， ***(s，a，s’，r)*** ，意思是执行一个动作 ***一个*** 处于一个状态 ***s*** 产生一个新的状态***‘s’***和奖励 ***r*** 。当与环境交互时，代理记住(通过采样)它过去的经验，以便从中学习更多。

通过应用体验重放缓解的两个主要问题是:

1.  **强相关更新:**流行的基于随机梯度的算法假设变量是独立同分布的(独立同分布)。然而，独立同分布假设不太适合马尔可夫链(状态序列)。当把在线体验和存储的体验混合在一起时，就有可能“打破”连续状态之间的关联。
2.  **遗忘重要经验:**如前所述，有些情况可能很少经历，经典 RL 算法只是在下一次迭代中丢弃旧的经验。

# 优先体验重放(PER)

经验回放的引入已经是 RL 的一大进步。然而，生物学证据支持一些经历比其他经历更频繁地重演。

在[4]中，提出了经验重放方法的改进版本 PER。主要思想是使用 TD 误差作为一个度量来定义重放一些经验的概率。

TD 误差代表给定状态的预测误差(模型输出和接收到的奖励之间的差异)，经常被用作优先化机制(例如，人工好奇心[5]和特征选择[6])。

其动机是，类似于生物系统，学习系统可以从中学习更多的经验(即与大的 TD 误差相关的状态)应该更频繁地重放，从而加速学习过程。

[4]中的结果表明，在应该学习如何玩 Atari 游戏的代理上应用 PER 允许将学习速度提高 2 倍。

![](img/39a327d7371c29d793ca1d587432b778.png)

Photo by [Kirill Sharkovski](https://unsplash.com/@sharkovski?utm_source=medium&utm_medium=referral)

# 不仅 RL 可以从中受益

[4]中给出的另一个有趣的结果是，同样的方法可以用于改进监督学习方法。该方法被重新命名为优先采样，包括更频繁地使用代表更高误差的例子，以集中优化(学习)过程:

> “在监督学习的环境中，类似于优先重放的方法是从数据集中进行非均匀采样，每个样本使用基于其最后出现的错误的优先级。这可以帮助将学习集中在那些仍然可以学习的样本上，将额外的资源投入到(硬)边界情况中”

这种方法在处理不平衡数据集时尤其有用，因为包含较少表示的类的示例将具有较高的采样优先级，前提是它们仍然会导致较大的预测误差。

当应用于经典 MNIST 数字分类问题的类不平衡变体时，优先化采样被证明导致在泛化和学习速度方面的性能提高。

# 参考

[1]Nils j . Nilsson《人类级人工智能？认真点！."艾杂志 26.4(2005):68–68。阿泽顿、劳拉·a、大卫·杜普雷和杰克·r·梅勒。"记忆痕迹重放:通过神经调节形成记忆巩固."神经科学趋势 38.9(2015):560–570。
[3]林，隆基。"基于强化学习、规划和教学的自我改进反应代理."机器学习 8.3–4(1992):293–321。
[4]绍尔，汤姆等，“优先化的经验重放”arXiv 预印本 arXiv:1511.05952 (2015)。
【5】“人工智能如何变得好奇？”"【https://towardsdatascience . com/artificial-curiosity-e 1837 E4 ca 2c 9
[6]孙，易，等《基于时差误差的增量基构造》。“ICML。2011.