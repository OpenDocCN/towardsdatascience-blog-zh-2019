<html>
<head>
<title>Reinforcement Learning — Generalisation of On-policy Function Approximation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——策略函数逼近的推广</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-on-policy-function-approximation-2f47576f772d?source=collection_archive---------11-----------------------#2019-08-03">https://towardsdatascience.com/reinforcement-learning-on-policy-function-approximation-2f47576f772d?source=collection_archive---------11-----------------------#2019-08-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4f3f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将 Q 函数应用于连续状态空间</h2></div><p id="3394" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在以前的帖子中，我们已经将强化学习的思想从离散状态空间扩展到连续状态空间，并且实现了 1000 个状态的随机行走示例，在这种情况下，给定了一个策略，因为在所有状态下向左或向右的动作总是具有相等的概率，并且唯一的问题是基于给定的策略来测量每个状态的值函数(我们将这类问题称为预测问题)。</p><p id="319b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，让我们将这个想法扩展到更一般的情况，称为控制问题，其中没有给定策略，我们的目标是使用 Q 函数来学习每个状态下的最佳行动。在这篇文章中，我将:</p><ol class=""><li id="3634" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">介绍连续状态空间设置的策略算法</li><li id="35e6" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">实现并应用该算法解决经典的山地汽车问题</li></ol><p id="4e5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(注:我们将应用 tile 编码作为近似函数，如果您不熟悉，请参考我上一篇<a class="ae ls" rel="noopener" target="_blank" href="/reinforcement-learning-tile-coding-implementation-7974b600762b">帖子</a>)</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/b32446a305dcbfc10c4efd67d50034f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VP9KGpymWEfJTy4w3q9exQ.jpeg"/></div></div></figure><h1 id="b1bd" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">控制问题</h1><p id="4cbb" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">与行为分布已知的预测问题不同，在控制问题中，代理的任务是探索环境并学习每个状态下的最佳行为，因此，当公式化问题时，考虑的是<code class="fe nc nd ne nf b">Q(s, a)</code>函数而不是价值函数<code class="fe nc nd ne nf b">V(s)</code>。</p><h2 id="4f15" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">半梯度 Sarsa</h2><p id="df7a" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">另一方面，像预测问题一样，参数值函数的算法可以自然地扩展到参数 Q 函数，只需用 Q 函数替换值函数。让我们直接进入我们要应用的算法:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ns"><img src="../Images/5d2586076cfe2cdaeba476f4a7467bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ygkyZNY5uLIiYiOxE28rMw.png"/></div></div></figure><p id="ab0c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你一直关注我以前的帖子，你会再次看到这个算法非常熟悉。类似于离散状态空间中的 n 步 Sarsa，这种半梯度 n 步 Sarsa 与完全相同，只是在每个时间步，参数<code class="fe nc nd ne nf b">w</code>被更新，而不是直接更新 Q 函数。<strong class="kk iu">其思想是在每次更新时间</strong> <code class="fe nc nd ne nf b">τ</code> <strong class="kk iu">时，使用到</strong> <code class="fe nc nd ne nf b">τ+n</code> <strong class="kk iu">的累积值来校正当前估计，并根据梯度下降的思想，将参数权重</strong> <code class="fe nc nd ne nf b">w</code> <strong class="kk iu">与其导数和增量成比例地稍微向实际值更新。</strong></p><h1 id="488e" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">山地车问题</h1><p id="0f1e" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">如果你仍然有一些困惑，那完全没问题。我相信理解一个算法的每个细节的最好方法是实现它，并把它应用到一个实际问题上。我们来做一个经典的强化学习问题，山地车。</p><h2 id="e668" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">山地汽车场景</h2><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nt"><img src="../Images/e0badaa6f550c74002d00cd6395d5f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*ORMjTwWAKpUBRisb6TRfuA.png"/></div></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Mountain Car</figcaption></figure><p id="da6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ny">考虑驾驶一辆动力不足的汽车在陡峭的山路上行驶的任务，如图所示。困难在于重力比汽车的发动机更强，即使在全油门的情况下，汽车也无法加速上陡坡。唯一的解决办法是先离开目标，然后爬上左边的反坡。</em></p><p id="7e64" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ny">这个问题中的奖励在所有时间步长上都是-1，直到汽车在山顶移动经过它的目标位置，这就结束了这一集。有三种可能的操作:全油门向前(+1)、全油门向后(-1)和零油门(0)。汽车根据简化的物理学原理行驶。其位置、</em> <code class="fe nc nd ne nf b"><em class="ny">x_t</em></code> <em class="ny">和速度、</em> <code class="fe nc nd ne nf b"><em class="ny">x_ ̇t</em></code> <em class="ny">由</em>更新</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nz"><img src="../Images/c29c8719f41925517371967ff47443ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOrsb5fQK2mhaeLCeKMfGw.png"/></div></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Update Rule</figcaption></figure><p id="1a0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ny">此处绑定操作强制执行</em> <code class="fe nc nd ne nf b"><em class="ny">-1.2 &lt;= x_t+1 &lt;= 0.5</em></code> <em class="ny">和</em> <code class="fe nc nd ne nf b"><em class="ny">-0.07 &lt;= x_ ̇t+1 &lt;= 0.07</em></code> <em class="ny">。另外，当</em> <code class="fe nc nd ne nf b"><em class="ny">x_t+1</em></code> <em class="ny">到达左界时，</em> <code class="fe nc nd ne nf b"><em class="ny">x_ ̇t+1</em></code> <em class="ny">被复位为零。到了右边界，目标达到，插曲终止。每集从</em> <code class="fe nc nd ne nf b"><em class="ny">[-0.6, -0.4)</em></code> <em class="ny">中的任意位置</em> <code class="fe nc nd ne nf b"><em class="ny">x_t</em></code> <em class="ny">和零速度开始。</em>(抱歉无格式的语法编辑，我不知道如何在媒体上编辑数学语法)</p><p id="7574" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单来说，汽车以 0 速度从-0.6 到 0.4 之间的位置出发，有 3 个离散动作，停留(0)，前进(1)，后退(-1)，目标是到达最右边的山顶。所以<strong class="kk iu">这个问题中的状态包括</strong> <code class="fe nc nd ne nf b">(position, velocity)</code> <strong class="kk iu">，动作集合为</strong> <code class="fe nc nd ne nf b">(-1, 0, 1)</code> <strong class="kk iu">，奖励为</strong> <code class="fe nc nd ne nf b">-1</code> <strong class="kk iu">除了目标状态(注意状态是连续的，动作是离散的)。</strong></p><h2 id="f974" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">价值函数</h2><p id="97b2" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">了解了问题设置，我们现在需要一个 Q 值函数来跟踪所有状态的值，并在探索过程中更新值估计。记得在上一篇文章中，我们已经讲述了使用 tile 编码对连续状态空间进行编码并将其应用于更新过程的基础知识。在本帖中，我们将继续在我们的值函数中使用平铺编码，但代码略有不同:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="77d4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里使用的图块编码功能是指这里的<a class="ae ls" href="https://github.com/MJeremy2017/reinforcement-learning-an-introduction/blob/master/chapter10/TileCoding.py" rel="noopener ugc nofollow" target="_blank">和</a>。我没有使用上一篇文章中介绍的 tile 编码的原因是，我介绍的那个有点基本，具有均匀分布的偏移，我亲自测试的结果不如这个，而且这似乎是 Sutton 在他的书中介绍的 tile 编码函数。</p><p id="3f35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不管怎样，让我们回到正题。这里的想法是完全相同的，每个连续的状态都被编码到一些图块的索引中，在每集的每一步，其对应图块的权重都被更新。</p><p id="a3c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们仔细看看这个函数。<code class="fe nc nd ne nf b">numberOfTilings = 8</code>和<code class="fe nc nd ne nf b">maxSize = 2048</code>，意味着我们设置了 8 个 tilings，每个 tilings 有<code class="fe nc nd ne nf b">16*16</code>个网格数，所以<code class="fe nc nd ne nf b">16*16*8 = 2048</code>。更具体地说，<code class="fe nc nd ne nf b">(position, velocity)</code>的状态由一个维度为<code class="fe nc nd ne nf b">16*16*8</code>的 3D 立方体表示，而<code class="fe nc nd ne nf b">tiles</code>函数，从我提到的文件中，能够获得给定状态和动作的活动瓦片。您可能已经注意到，在<code class="fe nc nd ne nf b">init</code>函数中，状态被重新调整为:</p><pre class="lu lv lw lx gt oc nf od oe aw of bi"><span id="1809" class="ng mg it nf b gy og oh l oi oj"># position and velocity needs scaling to satisfy the tile software<br/>self.positionScale = self.numOfTilings / (POSITION_BOUND[1] - POSITION_BOUND[0])</span><span id="5966" class="ng mg it nf b gy ok oh l oi oj">self.velocityScale = self.numOfTilings / (VELOCITY_BOUND[1] - VELOCITY_BOUND[0])</span></pre><p id="1dac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">老实说，我不知道它为什么以这种方式缩放，但看起来像是使用这种平铺编码功能，状态的连续值总是要被重新缩放。</p><p id="375b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像所有 Q 值函数一样，它至少有两个函数:</p><ol class=""><li id="ff35" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated"><code class="fe nc nd ne nf b">value</code>功能:返回当前状态的值，以及动作。当代理达到目标状态时，返回 0。</li><li id="8376" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><code class="fe nc nd ne nf b">update</code>功能:根据新的估计值更新权重(记住在 tile 编码中，权重参数的导数总是 1，因此更新是直接加上<code class="fe nc nd ne nf b">delta</code>值)。</li></ol><p id="b85f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne nf b">costToGo</code>函数用于后面步骤的可视化，它简单地返回该状态和动作的最大负值(因为奖励总是-1，所以结果被赋予一个<code class="fe nc nd ne nf b">-</code>符号)。</p><h1 id="4255" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">山地汽车实施</h1><p id="94aa" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">到目前为止，我们实际上已经完成了最棘手的部分，剩下的是我们一直在重复做的事情。(<a class="ae ls" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/MountainCar/MountainCar.py" rel="noopener ugc nofollow" target="_blank">全面实施</a>)</p><h2 id="97ba" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">初始化</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="bbd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像往常一样，在<code class="fe nc nd ne nf b">init</code>函数中，我们指定了动作和状态范围，以及所有强化学习问题中的一些通用元素。</p><h2 id="8ef9" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">动作功能</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="f3c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne nf b">takeAction</code>采取行动并决定汽车的下一个状态。更新规则遵循我在上面介绍的公式。<code class="fe nc nd ne nf b">chooseAction</code>函数仍然使用ϵ-greedy 方法，在贪婪部分，它选择由我们上面描述的<code class="fe nc nd ne nf b">valueFunc</code>给出的最大估计值的动作结果。</p><h2 id="1706" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">报酬</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="cbdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了目标状态获得奖励<code class="fe nc nd ne nf b">0</code>，其他所有状态都获得奖励<code class="fe nc nd ne nf b">-1</code>。</p><h2 id="750e" class="ng mg it bd mh nh ni dn ml nj nk dp mp kr nl nm mr kv nn no mt kz np nq mv nr bi translated">发动汽车！</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="181f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是所有函数中最长的一个，但不用担心，事实上，一切都和它在离散设置中一样(我实际上是从我以前的代码中复制粘贴的)，除了值函数被改为我们刚才介绍的函数。目标<code class="fe nc nd ne nf b">G</code>是累积的 n 步值，包括第<code class="fe nc nd ne nf b">tau+n</code>步的值，在更新过程中<code class="fe nc nd ne nf b">state, action and G</code>一起发送给<code class="fe nc nd ne nf b">valueFunction.update()</code>函数。</p><h1 id="8efe" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">(英)可视化(= visualization)</h1><p id="2492" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在本节中，我们将使用在<code class="fe nc nd ne nf b">ValueFunction</code>中定义的<code class="fe nc nd ne nf b">costToGo</code>函数来衡量在给定状态和行动的情况下达到目标的成本。图的代码如下所示:</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="2f62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看步骤 100 和步骤 9000 的结果:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f329ba325cabb4a8890f08ab5d17e21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*_qz_N0dSkDn-sPJdqIDp5w.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Step 100</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4de3a257d73a0354dbe735a1ed4b930d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*0MZdkeOWYgcAlQkN6KbCZA.png"/></div><figcaption class="nu nv gj gh gi nw nx bd b be z dk">Step 9000</figcaption></figure><p id="9314" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">成本越低，越接近目标状态。事实上，较低的成本要么出现在最左边的位置，要么出现在最右边的位置，这验证了只有到达目标的相反方向，智能体才能到达最终的目标状态。</p><p id="37fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考</strong>:</p><ul class=""><li id="cb35" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld om lk ll lm bi translated"><a class="ae ls" href="http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="0d71" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld om lk ll lm bi translated"><a class="ae ls" href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/Shang tong Zhang/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>