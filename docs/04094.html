<html>
<head>
<title>Reverse Image Search using Auto-Encoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自动编码器的反向图像搜索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reverse-image-search-using-auto-encoders-afbf906970f5?source=collection_archive---------17-----------------------#2019-06-27">https://towardsdatascience.com/reverse-image-search-using-auto-encoders-afbf906970f5?source=collection_archive---------17-----------------------#2019-06-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e483" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你有没有想过谷歌反向图片搜索是如何工作的？他们如何快速扫描所有图像并返回合适的结果？在这篇博客中，我们将制作自己的轻量级反向搜索引擎。为此，我们将使用自动编码器。</p><h2 id="ae34" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">什么是自动编码器？</h2><p id="d6f9" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">自动编码器是一种特殊类型的前馈神经网络，其输入与输出相同。它们以无人监督的方式接受训练，以学习输入的低级特征。这些低级特征通常被称为<strong class="jp ir">潜在</strong>特征。然后，这些潜在特征被用于重构原始输入。</p><p id="8814" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自动编码器由 3 个主要组件组成</p><ul class=""><li id="a1d2" class="lj lk iq jp b jq jr ju jv jy ll kc lm kg ln kk lo lp lq lr bi translated">编码器:用于压缩输入图像</li><li id="5a7a" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">潜在表征:保留大部分信息的输入的低层次特征</li><li id="c3cc" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">解码器:用于从潜在特征中重建原始输入。</li></ul><p id="2904" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这种情况下，输入是一个图像。下图清楚地解释了自动编码器的工作原理。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi lx"><img src="../Images/961fd84ab4aa93ce483527f91a5f3177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z5baTwQbFNC-YBPZLZubzA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk"><a class="ae mn" href="https://github.com/udacity/deep-learning/blob/master/autoencoder/Simple_Autoencoder_Solution.ipynb" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="703c" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">我们将在这个博客中讨论什么？</h2><ul class=""><li id="5c0b" class="lj lk iq jp b jq le ju lf jy mo kc mp kg mq kk lo lp lq lr bi translated">我们将使用时尚 MNIST 数据集并对其进行预处理</li><li id="c237" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">训练自动编码器学习所有图像的潜在表示</li><li id="95d6" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">使用骚扰索引潜在特征</li><li id="764c" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">查找给定图像的相似图像</li></ul><p id="2353" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们开始吧。</p><h2 id="c02e" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">时尚 MNIST 数据集</h2><p id="61f2" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">在加载和处理数据之前，了解数据集、类的数量和数据集中的样本数量是一个很好的做法。</p><p id="2e6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">时尚 MNIST 数据库由时尚产品的 28*28 灰度图像的 60000 个训练样本和 10000 个测试样本组成。每个类别有 6000 个训练样本和 1000 个测试样本。你可以在这里阅读更多关于数据集<a class="ae mn" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">的信息</a>。</p><h2 id="13f2" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">导入库</h2><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="4ef0" class="kl km iq ms b gy mw mx l my mz">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf<br/>from tensorflow import keras<br/>from annoy import AnnoyIndex<br/>import os</span></pre><h2 id="e2e3" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">下载数据</h2><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="f40d" class="kl km iq ms b gy mw mx l my mz">fashion_mnist = keras.datasets.fashion_mnist<br/>(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()</span></pre><h2 id="8068" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">预处理和探索性数据分析</h2><p id="d0d6" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">像素值介于 0 和 255 之间。我们将在 0 和 1 之间缩放像素值。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="7bb5" class="kl km iq ms b gy mw mx l my mz"><em class="na"># resize the pixel values between 0 and 255</em><br/>train_images = train_images/255<br/>test_images = test_images/255</span></pre><p id="29da" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将分类标签映射到适当的产品类别。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="b818" class="kl km iq ms b gy mw mx l my mz"><em class="na"># different product categories in the dataset </em><br/>labeldict = {<br/>    0: 'T-shirt/top',<br/>    1: 'Trouser',<br/>    2: 'Pullover',<br/>    3: 'Dress',<br/>    4: 'Coat',<br/>    5: 'Sandal',<br/>    6: 'Shirt',<br/>    7: 'Sneaker',<br/>    8: 'Bag',<br/>    9: 'Ankle boot'<br/>}</span><span id="33a9" class="kl km iq ms b gy nb mx l my mz"><em class="na"># no of times each product category is present in the dataset</em><br/>category_counts = dict(zip(*np.unique(train_labels, return_counts=True)))</span></pre><p id="b608" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们展示一些样本图像。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="9b0d" class="kl km iq ms b gy mw mx l my mz">plt.figure(figsize=(12,8))<br/>for index in range(16):<br/>  rand_idx = np.random.randint(0,train_labels.shape[0])<br/>  plt.subplot(4,4,index+1)<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  plt.grid('off')<br/>  plt.imshow(train_images[rand_idx], cmap='Greys_r')<br/>  plt.xlabel(labeldict[train_labels[rand_idx]])</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7022fc3987de8c5138bb34867a3e7e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*warOcUcx7fVGMfGMRYjd8w.png"/></div></figure><h2 id="24b3" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">助手功能</h2><p id="c46c" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">让我们定义一个效用函数来绘制原始图像和重建图像。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="de4b" class="kl km iq ms b gy mw mx l my mz">def plot_images(original, reconstructed):<br/>  <br/>  fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(12,4))<br/>  <br/>  for images, axes in zip([original, reconstructed], axes):<br/>    for image, ax in zip(images, axes):<br/>      ax.imshow(image.reshape(28,28), cmap="Greys_r")<br/>      ax.get_xaxis().set_visible(False)<br/>      ax.get_yaxis().set_visible(False)<br/>      <br/>  fig.tight_layout(pad=0.1)</span></pre><h2 id="930b" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">占位符</h2><p id="89cc" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">让我们为输入和目标定义占位符。在这种情况下，输入和目标是相等的。我们还将为批量大小定义一个占位符。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="752c" class="kl km iq ms b gy mw mx l my mz">def placeholders(image_size, n_channels ):<br/>  inputs = tf.placeholder(dtype=tf.float32, shape=[None, image_size, image_size,n_channels], name='inputs')</span><span id="c111" class="kl km iq ms b gy nb mx l my mz">  targets = tf.placeholder(dtype=tf.float32, shape=[None, image_size, image_size, n_channels], name='targets')</span><span id="bf88" class="kl km iq ms b gy nb mx l my mz">  batch_size = tf.placeholder(dtype=tf.int32, name='batch_size')</span><span id="070e" class="kl km iq ms b gy nb mx l my mz">  return inputs, targets, batch_size</span></pre><h2 id="3094" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">编码器和解码器网络</h2><p id="d6cc" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">我们将使用卷积神经网络来训练我们的模型。编码器网络将 28*28 图像转换成 4*4*8 的低级表示。解码器使用这种低级表示来重建 28*28 的图像。网络参数取自<a class="ae mn" href="https://github.com/udacity/deep-learning/blob/master/autoencoder/Convolutional_Autoencoder_Solution.ipynb" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="5d6f" class="kl km iq ms b gy mw mx l my mz">def encoder_decoder_network(X):<br/>  <br/>  <em class="na">#ENCODER NETOWRK</em><br/>  <br/>  <em class="na"># X's shape - 28*28*1</em><br/>  W1 = tf.get_variable("W1", shape=[3,3,1,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  <em class="na"># 28*28*16 </em><br/>  conv1 = tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME')<br/>  relu1 = tf.nn.relu(conv1)<br/>  <em class="na">#14*14*16</em><br/>  pool1 = tf.nn.max_pool(relu1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')<br/>  <br/>  <em class="na">#14*14*8</em><br/>  W2 = tf.get_variable("W2", shape=[3,3,16,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  conv2 = tf.nn.conv2d(pool1, W2, strides=[1,1,1,1], padding='SAME')<br/>  relu2 = tf.nn.relu(conv2)<br/>  <em class="na">#7*7*8</em><br/>  pool2 = tf.nn.max_pool(relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')<br/>  <em class="na">#7*7*8</em><br/>  W3 = tf.get_variable("W3", shape=[3,3,8,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  conv3 = tf.nn.conv2d(pool2, W3, strides=[1,1,1,1], padding='SAME')<br/>  relu3 = tf.nn.relu(conv3)<br/>  <em class="na">#4*4*8</em><br/>  pool3 = tf.nn.max_pool(relu3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME', name='encodings')<br/>  <br/>  encoded = pool3<br/>  <br/>  <em class="na"># the image is now 4*4*8 </em><br/>  <br/>  <em class="na"># DECODER NETWORK </em><br/>  <br/>  <br/>  upsample1 = tf.image.resize_nearest_neighbor(encoded, (7,7))<br/>  <em class="na">#7*7*8</em><br/>  W4 = tf.get_variable("W4", shape=[3,3,8,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  conv4 = tf.nn.conv2d(upsample1, W4, strides=[1,1,1,1], padding='SAME')<br/>  relu4 = tf.nn.relu(conv4)<br/>  <br/>  upsample2 = tf.image.resize_nearest_neighbor(relu4, (14,14))<br/>  <em class="na"># 14*14*8</em><br/>  W5 = tf.get_variable("W5", shape=[3,3,8,8], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  conv5 = tf.nn.conv2d(upsample2, W5, strides=[1,1,1,1], padding='SAME')<br/>  relu5 = tf.nn.relu(conv5)<br/>  <br/>  <em class="na"># 28*28*8</em><br/>  upsample3 = tf.image.resize_nearest_neighbor(relu5, (28,28))<br/>  <br/>  W6 = tf.get_variable("W6", shape=[3,3,8,16], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  conv6 = tf.nn.conv2d(upsample3, W6, strides=[1,1,1,1], padding='SAME')<br/>  relu6 = tf.nn.relu(conv6)<br/>  <br/>  W7 = tf.get_variable("W7", shape=[3,3,16,1], initializer=tf.contrib.layers.xavier_initializer(seed=0))<br/>  conv7 = tf.nn.conv2d(relu6, W7, strides=[1,1,1,1], padding='SAME')<br/>  <br/>  logits = conv7<br/>  <br/>  decoded = tf.nn.sigmoid(logits, name='decoded')<br/>  <br/>  return encoded, decoded, logits</span></pre><h2 id="24dc" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">定义培训操作</h2><p id="a705" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">然后，我们计算原始图像和重建图像之间的误差。由于目标是最小化误差，我们将使用 Adam 优化算法来学习误差最小的网络参数。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="32ac" class="kl km iq ms b gy mw mx l my mz">def train_operations(logits, targets, learning_rate):</span><span id="8009" class="kl km iq ms b gy nb mx l my mz">  <em class="na"># define the loss</em><br/>  loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=targets))</span><span id="697c" class="kl km iq ms b gy nb mx l my mz">  <em class="na"># use adam optimizer for faster convergence</em><br/>  training_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)<br/>  return loss, training_op</span></pre><h2 id="b28a" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">训练模型</h2><p id="b677" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">接下来，我们将定义一个用于训练模型的函数。代替使用<em class="na"> feed_dict，</em>我们将<em class="na"> </em>使用推荐的方式，使用<em class="na">数据集和迭代器</em>向模型提供数据。如果你不知道数据集和迭代器，你可以参考这个博客。</p><p id="d7e3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦训练结束，我们还会保存模型，以便以后恢复。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="766f" class="kl km iq ms b gy mw mx l my mz">def train_model(epochs, image_size, n_channels, batch_size, learning_rate, model_save_path):<br/>  <br/>  <em class="na"># reset the graphs</em><br/>  tf.reset_default_graph()<br/>  <br/>  <em class="na"># get the placeholders</em><br/>  inputs, targets, batch_op = placeholders(image_size, n_channels)<br/>  <br/>  <em class="na"># create a Dataset from the input data</em><br/>  dataset = tf.data.Dataset.from_tensor_slices((inputs,targets))<br/>  <br/>  <em class="na"># create batches of data </em><br/>  dataset = dataset.batch(batch_size)<br/>  <br/>  <em class="na"># define an iterator to consume the data</em><br/>  iterator = tf.data.Iterator.from_structure(dataset.output_types,dataset.output_shapes)<br/>  <br/>  train_initializer = iterator.make_initializer(dataset, name='init_iterator')<br/>  <br/>  <em class="na"># get the batch of data using get_next</em><br/>  input_batch, target_batch = iterator.get_next()<br/>  <br/>  encoded, decoded, logits = encoder_decoder_network(input_batch)<br/>  loss, training_op = train_operations(logits, target_batch, learning_rate)<br/>  <br/>  saver = tf.train.Saver()<br/>  with tf.Session() as sess:<br/>    sess.run(tf.global_variables_initializer())<br/>    for epoch in range(epochs):<br/>      epoch_loss = 0<br/>      <br/>      <em class="na"># run the initializer </em><br/>      sess.run(train_initializer, feed_dict = {<br/>          inputs: train_images.reshape(-1, image_size, image_size, n_channels),<br/>          targets : train_images.reshape(-1, image_size, image_size, n_channels),<br/>          batch_op: batch_size<br/>      })<br/>      <br/>      try:<br/>        while True:<br/>          batch_loss, _ = sess.run([loss, training_op])<br/>          epoch_loss += batch_loss<br/>      except tf.errors.OutOfRangeError:<br/>        pass<br/>        <br/>      print("Epoch {}/{}: Loss is {:.3f}".format(epoch+1, epochs, epoch_loss))<br/>      <br/>      <br/>    print("Training over\n")<br/>    <em class="na"># save the model </em><br/>    saver.save(sess,model_save_path)<br/>    <br/>    print("Model saved at {}".format(model_save_path))</span></pre><h2 id="dc46" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">模型参数</h2><p id="2b44" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">我们将以 400 个为一批，总共 20 个时期来训练模型。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="d1fe" class="kl km iq ms b gy mw mx l my mz">epochs = 20<br/>batch_size = 400<br/>image_size = 28 <br/>n_channels = 1<br/>learning_rate = 0.001</span></pre><h2 id="5cb5" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">运行模型</h2><p id="696a" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">在运行模型之前，我们将创建一个<strong class="jp ir">检查点</strong>目录来保存我们的模型。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="515e" class="kl km iq ms b gy mw mx l my mz">if not os.path.exists('checkpoints'):<br/>  os.mkdir('checkpoints')<br/><br/><em class="na"># checpoint directory </em><br/>chkpt_dir = os.path.join(os.getcwd(), 'checkpoints')<br/><em class="na"># path to save the model</em><br/>model_save_path = os.path.join(chkpt_dir, 'fashion-mnist.chkpt')<br/><br/>train_model(epochs=epochs, <br/>            batch_size=batch_size, <br/>            image_size=image_size, <br/>            n_channels= n_channels, <br/>            learning_rate = learning_rate,<br/>            model_save_path = model_save_path)</span></pre><h2 id="51c0" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">失败</h2><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="26eb" class="kl km iq ms b gy mw mx l my mz">Epoch 1/20: Loss is 64.375<br/>Epoch 2/20: Loss is 48.220<br/>Epoch 3/20: Loss is 46.779<br/>Epoch 4/20: Loss is 46.140<br/>Epoch 5/20: Loss is 45.726<br/>Epoch 6/20: Loss is 45.435<br/>Epoch 7/20: Loss is 45.215<br/>Epoch 8/20: Loss is 45.031<br/>Epoch 9/20: Loss is 44.868<br/>Epoch 10/20: Loss is 44.724<br/>Epoch 11/20: Loss is 44.593<br/>Epoch 12/20: Loss is 44.470<br/>Epoch 13/20: Loss is 44.357<br/>Epoch 14/20: Loss is 44.251<br/>Epoch 15/20: Loss is 44.152<br/>Epoch 16/20: Loss is 44.060<br/>Epoch 17/20: Loss is 43.975<br/>Epoch 18/20: Loss is 43.894<br/>Epoch 19/20: Loss is 43.820<br/>Epoch 20/20: Loss is 43.750</span></pre><h2 id="86d7" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">可视化重建的图像</h2><p id="b030" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">让我们定义一个函数，它接受保存模型的文件的路径和需要重建的图像列表。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="0403" class="kl km iq ms b gy mw mx l my mz">def test_network(model_path, images):<br/>  with tf.Session() as sess:<br/>    saver = tf.train.Saver()<br/>    saver.restore(sess, model_path)<br/>    default_graph = tf.get_default_graph()<br/>    inputs = default_graph.get_tensor_by_name('inputs:0')<br/>    targets = default_graph.get_tensor_by_name('targets:0')<br/>    <br/>    test_iterator_init = default_graph.get_operation_by_name('init_iterator')<br/>    decoded = default_graph.get_tensor_by_name('decoded:0')<br/>    reconstructed =[]<br/>    sess.run(test_iterator_init, feed_dict={<br/>        inputs:images,<br/>        targets:images<br/>    })<br/>    try:<br/>      while True:<br/>        reconstructed.append(sess.run(decoded))<br/>    except tf.errors.OutOfRangeError:<br/>      pass<br/>    return reconstructed</span></pre><p id="19aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们恢复了训练时定义的各种张量和运算。<strong class="jp ir">输入</strong>和<strong class="jp ir">目标</strong>张量用于将图像输入网络。运行<strong class="jp ir">解码后的</strong>张量以恢复重建图像。</p><p id="b3c9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将测试数据集中的一些图像传递给模型，并可视化它们的重建图像。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="7393" class="kl km iq ms b gy mw mx l my mz">test_sample_images = test_images[:10]<br/>test_sample_images = test_sample_images.reshape(-1, image_size, image_size, n_channels)<br/>reconstructed_images = test_network(model_save_path, test_sample_images)<br/>reconstructed_images = np.array(reconstructed_images).reshape(10,28,28,1)<br/>plot_images(test_sample_images, reconstructed_images)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi nd"><img src="../Images/97bddbf8ca58a906b2526c6a92190d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pkwc670zUvqXcxt51xM95w.png"/></div></div></figure><p id="fe61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个模型在重建图像方面做得相当不错。</p><h2 id="2caf" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">潜在特征</h2><p id="11e0" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">既然我们对训练模型感到满意，让我们计算所有训练样本的潜在表示。为此，我们将定义一个函数，它接受保存模型的文件的路径，一个我们想要获得潜在表示的图像列表和编码图像的大小。</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="8edb" class="kl km iq ms b gy mw mx l my mz">def get_encodings(model_path, images, encoding_vector_length):<br/>  with tf.Session() as sess:<br/>    saver = tf.train.Saver()<br/>    saver.restore(sess, model_path)<br/>    default_graph = tf.get_default_graph()<br/>    inputs = default_graph.get_tensor_by_name('inputs:0')<br/>    targets = default_graph.get_tensor_by_name('targets:0')<br/>    <br/>    iterator_init = default_graph.get_operation_by_name('init_iterator')<br/>    encoded = default_graph.get_tensor_by_name('encodings:0')<br/>    encoding_vectors =[]<br/>    sess.run(iterator_init, feed_dict={<br/>        inputs:images,<br/>        targets:images<br/>    })<br/>    try:<br/>      while True:<br/>        encoding_vectors.append(sess.run(encoded))<br/>    except tf.errors.OutOfRangeError:<br/>      pass<br/>    return np.array(encoding_vectors).reshape(-1, encoding_vector_length)</span></pre><h2 id="6a9c" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">烦恼</h2><p id="6c29" class="pw-post-body-paragraph jn jo iq jp b jq le js jt ju lf jw jx jy lg ka kb kc lh ke kf kg li ki kj kk ij bi translated">近似最近邻(<a class="ae mn" href="https://pypi.org/project/annoy/" rel="noopener ugc nofollow" target="_blank">aroy</a>)是一个库，用于搜索空间中更靠近给定查询点的点。我们将使用 Annoy 来保存所有训练样本的所有潜在表示。</p><p id="8a40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，给定一个查询图像，我们将计算给定图像的潜在表示，并将其与所有编码表示进行比较，以找到相似的图像。如果你想了解更多关于 Annoy 是如何工作的，请参考<a class="ae mn" href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html" rel="noopener ugc nofollow" target="_blank">这篇</a>精彩的博文。</p><p id="b305" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来定义构建烦恼指数所需的参数。</p><ul class=""><li id="d0aa" class="lj lk iq jp b jq jr ju jv jy ll kc lm kg ln kk lo lp lq lr bi translated">编码向量长度:图像的编码表示的大小</li><li id="fcbb" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">骚扰文件名称:骚扰索引将被保存的文件的名称</li><li id="a75d" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk lo lp lq lr bi translated">num_trees:构建 n 个树的森林。树的数量越多，精度越高。</li></ul><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="db90" class="kl km iq ms b gy mw mx l my mz"># our encoded image is of the shape 4*4*8, hence it's represted by a # vector of length 128<br/>encoding_vector_length = 128<br/>annoy_file_name = 'fashion-mnist.annoy.index'<br/>num_trees = 10</span></pre><p id="2ee4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">创建骚扰指数</strong></p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="2463" class="kl km iq ms b gy mw mx l my mz">def build_annoy_index(encoding_dim, num_trees, annoy_index_file, encodings):   <br/>  ann = AnnoyIndex(encoding_dim)   <br/>  for index, encoding in enumerate(encodings):<br/>     ann.add_item(index, encoding)</span><span id="e7d4" class="kl km iq ms b gy nb mx l my mz">     # builds a forest of num_trees, higher the number of trees,     higher the precision<br/>     ann.build(num_trees)<br/> <br/>     #save the index to a file<br/>     ann.save(annoy_index_file)<br/>     print("Created Annoy Index Successfully")</span><span id="a6f9" class="kl km iq ms b gy nb mx l my mz"><br/># resize the training images<br/>train_images = train_images.reshape(train_images.shape[0], image_size, image_size, n_channels)</span><span id="41fb" class="kl km iq ms b gy nb mx l my mz"># get the encoded representations of all the training samples<br/>encodings = get_encodings(model_save_path, train_images, encoding_vector_length)</span><span id="c3c0" class="kl km iq ms b gy nb mx l my mz"># create the annoy index<br/>build_annoy_index(encoding_vector_length, num_trees, annoy_file_name, encodings)</span></pre><p id="0aa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们定义一个函数来获取给定查询图像的相似图像。要搜索的相似图像的数量由参数<em class="na">n _ 相似</em>表示</p><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="2212" class="kl km iq ms b gy mw mx l my mz">def get_similar_images(image, n_similar=10):</span><span id="9fa4" class="kl km iq ms b gy nb mx l my mz">  # get the encoded representation of the image<br/>  encoding = get_encodings(model_save_path,image.reshape(-1, image_size, image_size,n_channels), encoding_vector_length)</span><span id="8634" class="kl km iq ms b gy nb mx l my mz">  # Load the annoy index<br/>  saved_ann = AnnoyIndex(encoding_vector_length)<br/>  saved_ann.load(annoy_file_name)</span><span id="d626" class="kl km iq ms b gy nb mx l my mz">  # get the nearest images <br/>  #get_nns_by_vector returns the indices of the most similar images <br/>  nn_indices = saved_ann.get_nns_by_vector(encoding[0], n_similar)</span><span id="d835" class="kl km iq ms b gy nb mx l my mz">  print("Similar images are")<br/>  for i, index in enumerate(nn_indices,1):<br/>    image = train_images[index].reshape(28,28)<br/>    plt.subplot(2,5,i)<br/>    plt.xticks([])<br/>    plt.yticks([])<br/>    plt.imshow(image, cmap='Greys_r')</span></pre><h2 id="6dc3" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">查询图像</h2><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="ce98" class="kl km iq ms b gy mw mx l my mz">sample_image = test_images[0]<br/>print("Sample Image")<br/>plt.imshow(sample_image, cmap='Greys_r')</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/412324251bd1ea4d69b4d50416f2bc92.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*eHT346g1tl2dj-ARclsu0w.png"/></div></figure><h2 id="4914" class="kl km iq bd kn ko kp dn kq kr ks dp kt jy ku kv kw kc kx ky kz kg la lb lc ld bi translated">相似的图像</h2><pre class="ly lz ma mb gt mr ms mt mu aw mv bi"><span id="0441" class="kl km iq ms b gy mw mx l my mz">get_similar_images(sample_image)</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9cc370207f34f9e0ea284375ffd793d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*22UMSa734knutBZXgCkFUg.png"/></div></figure><p id="3d75" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有返回的图像都与查询图像属于同一类。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><p id="0f13" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读博客。完整的代码可以在<a class="ae mn" href="https://github.com/animesh-agarwal/Auto-Encoders/blob/master/Reverse_Image_Search.ipynb" rel="noopener ugc nofollow" target="_blank">这个</a> Jupyter 笔记本里找到。</p><p id="5930" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你有任何问题或者你有任何改进这篇文章的建议，请在下面留下你的评论。</p><p id="426c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参考资料:</p><ol class=""><li id="a8a9" class="lj lk iq jp b jq jr ju jv jy ll kc lm kg ln kk nn lp lq lr bi translated"><a class="ae mn" href="https://github.com/udacity/deep-learning/tree/master/autoencoder" rel="noopener ugc nofollow" target="_blank">https://github . com/uda city/deep-learning/tree/master/auto encoder</a></li><li id="47af" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk nn lp lq lr bi translated"><a class="ae mn" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">https://github.com/zalandoresearch/fashion-mnist</a></li><li id="b3d5" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk nn lp lq lr bi translated"><a class="ae mn" href="https://pypi.org/project/annoy/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/annoy/</a></li><li id="8bad" class="lj lk iq jp b jq ls ju lt jy lu kc lv kg lw kk nn lp lq lr bi translated"><a class="ae mn" href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html" rel="noopener ugc nofollow" target="_blank">https://Erik Bern . com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces . html</a></li></ol></div></div>    
</body>
</html>