<html>
<head>
<title>Scalable Log Analytics with Apache Spark — A Comprehensive Case-Study</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Apache Spark 的可扩展日志分析—综合案例研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/scalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977?source=collection_archive---------7-----------------------#2019-04-10">https://towardsdatascience.com/scalable-log-analytics-with-apache-spark-a-comprehensive-case-study-2be3eb3be977?source=collection_archive---------7-----------------------#2019-04-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a96f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">半结构化数据的大规模数据分析和可视化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c5c9e9b6b6b86faf27f82e0fc0c9a31b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nki3OBH7921l077S3cnD-Q.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/photos/dPgPoiUIiXk?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Robin Pierre</a> on <a class="ae ky" href="https://unsplash.com/collections/291422/night-lights?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="ea38" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="e765" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当今利用分析的最流行和最有效的企业案例研究之一是日志分析。如今，几乎每个小型和大型组织都有多个系统和基础架构日复一日地运行。为了有效地保持业务运营，组织需要知道其基础架构是否发挥了最大潜力。这包括分析系统和应用程序日志，甚至可能对日志数据应用预测分析。日志数据量通常很大，这取决于组织基础架构的类型以及在其上运行的应用程序。由于计算限制，我们只能尝试在单台机器上分析数据样本的日子已经一去不复返了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mn"><img src="../Images/79d700c1587c61e9ef7fba3d5d1dbaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bO6ExMd0kK9fLBJD.jpg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://doughenschen.com/2015/06/17/spark-on-fire-why-all-the-hype/" rel="noopener ugc nofollow" target="_blank">Doug Henschen</a></figcaption></figure><p id="9f4f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在大数据、更好的分布式计算、大数据处理和 Spark 等开源分析框架的支持下，我们可以每天对潜在的数百万甚至数十亿条日志消息执行可扩展的日志分析。本案例研究导向教程的目的是采用实践方法展示我们如何利用 Spark 对半结构化日志数据执行大规模日志分析。如果您对 Spark 的可伸缩 SQL 感兴趣，可以随意查看 Spark 的<a class="ae ky" rel="noopener" target="_blank" href="/sql-at-scale-with-apache-spark-sql-and-dataframes-concepts-architecture-and-examples-c567853a702f"> <em class="mt"> SQL 和 Spark 的</em> </a>。</p><p id="e099" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">今天，我们将在本文中讨论以下主要话题。</p><ul class=""><li id="7b2c" class="mu mv it lt b lu mo lx mp ma mw me mx mi my mm mz na nb nc bi translated"><a class="ae ky" href="#2226" rel="noopener ugc nofollow"> <strong class="lt iu">主要目标——NASA 日志分析</strong> </a></li><li id="6b2e" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><a class="ae ky" href="#2b55" rel="noopener ugc nofollow"> <strong class="lt iu">设置依赖关系</strong> </a></li><li id="8ef2" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><a class="ae ky" href="#9c29" rel="noopener ugc nofollow"> <strong class="lt iu">加载并查看 NASA 日志数据集</strong> </a></li><li id="6977" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><a class="ae ky" href="#c893" rel="noopener ugc nofollow"> <strong class="lt iu">数据角力</strong> </a></li><li id="b472" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><a class="ae ky" href="#511f" rel="noopener ugc nofollow"> <strong class="lt iu">我们的网络日志数据分析</strong> </a></li></ul><p id="21cb" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">虽然有很多优秀的开源日志分析框架和工具，包括 elasticsearch，但本教程的目的是展示如何利用 Spark 来大规模分析日志。在现实世界中，当分析日志数据时，您可以自由选择工具箱。我们开始吧！</p><h1 id="2226" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">主要目标——NASA 日志分析</h1><p id="a706" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">正如我们之前提到的，Apache Spark 是一个优秀的、理想的开源框架，用于大规模地对结构化和非结构化数据进行辩论、分析和建模！在本教程中，我们的主要目标是关注业界最流行的案例研究之一——日志分析。通常，服务器日志是企业中非常常见的数据源，通常包含可操作的见解和信息的金矿。企业中的日志数据有许多来源，如 web、客户端和计算服务器、应用程序、用户生成的内容、平面文件。它们可以用于监控服务器、提高业务和客户智能、构建推荐系统、欺诈检测等等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f3f991a359a706b1bf703779cdb99179.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*vjWzYQbEGmETn6QNGjAeZw.png"/></div></figure><p id="5966" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">Spark 允许您廉价地将日志转储和存储在磁盘上的文件中，同时仍然提供丰富的 API 来执行大规模的数据分析。这个动手案例研究将向您展示如何在 NASA 的真实生产日志上使用 Apache Spark，并学习数据争论和探索性数据分析中的基本而强大的技术。在本案例研究中，我们将分析来自佛罗里达州 NASA 肯尼迪航天中心 web 服务器的日志数据集。完整的数据集可以免费下载<a class="ae ky" href="http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">这里</strong> </a>。</p><p id="9c5a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这两个数据集包含了两个月来对佛罗里达州 NASA 肯尼迪航天中心 WWW 服务器的所有 HTTP 请求。您可以前往<a class="ae ky" href="http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">网站</strong> </a>并根据需要下载以下文件(或直接点击以下链接)。</p><ul class=""><li id="98d3" class="mu mv it lt b lu mo lx mp ma mw me mx mi my mm mz na nb nc bi translated"><strong class="lt iu">07 月 01 日至 07 月 31 日，ASCII 格式，</strong>T0:<strong class="lt iu">ftp://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz</strong></li><li id="09bd" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><strong class="lt iu">04 年 8 月至 08 月 31 日，ASCII 格式，</strong>T1:<strong class="lt iu">ftp://ita.ee.lbl.gov/traces/NASA_access_log_Aug95.gz</strong></li></ul><p id="b0d0" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">确保这两个文件与包含教程的笔记本在同一个目录下，该教程可以在<a class="ae ky" href="https://github.com/dipanjanS/data_science_for_all/tree/master/tds_scalable_log_analytics" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="mt">my GitHub</em></strong></a>上找到。</p><h1 id="2b55" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">设置相关性</h1><p id="4a0b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">第一步是确保您能够访问 Spark 会话和集群。为此，您可以使用自己的本地设置或基于云的设置。一般来说，目前大多数云平台都会提供 Spark 集群，你也可以免费选择，包括<a class="ae ky" href="https://community.cloud.databricks.com" rel="noopener ugc nofollow" target="_blank"> Databricks 社区版</a>。本教程假设您已经有了 Spark 设置，因此我们不会花费额外的时间从头开始配置或设置 Spark。</p><p id="5b1f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">通常，当您启动 jupyter 笔记本服务器时，预先配置的 Spark 设置已经预先加载了必要的环境变量或依赖项。在我的情况下，我可以使用笔记本中的以下命令来检查它们。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="5e52" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">spark</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/94385a996a963d8bb524da916549e82a.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*dMkndoIz5k3bIRhZRDWrlw.png"/></div></figure><p id="3289" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这表明我的集群目前运行的是 Spark 2.4.0。我们还可以使用下面的代码检查<code class="fe nj nk nl nm b"><strong class="lt iu">sqlContext</strong></code> <strong class="lt iu"> </strong>是否存在。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="2f34" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">sqlContext</strong></span><span id="3a1c" class="nr la it nm b gy nx nt l nu nv"><strong class="nm iu">#Output:<br/>&lt;pyspark.sql.context.SQLContext at 0x7fb1577b6400&gt;</strong></span></pre><p id="42bc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在，如果您没有预先配置这些变量并得到一个错误，您可以加载它们并使用下面的代码配置它们。除此之外，我们还加载了一些其他库来处理数据帧和正则表达式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="3d9e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">使用正则表达式将是解析日志文件的主要方面之一。正则表达式是一种非常强大的模式匹配技术，可用于提取和发现半结构化和非结构化数据中的模式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a2f2b4ae099220a899c815d94973c208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/0*zCkgaGHgFqf5htkU.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://www.xkcd.com/1171/" rel="noopener ugc nofollow" target="_blank">xkcd</a></figcaption></figure><p id="62af" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">正则表达式可能非常有效和强大，但它们有时可能令人不知所措或困惑。不过不要担心，通过更多的练习，你可以真正发挥它的最大潜力。以下示例展示了在 Python 中使用正则表达式的方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="3a65" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">&lt;_sre.SRE_Match object; span=(0, 25), match="I'm searching for a spark"&gt; 0 25<br/>&lt;_sre.SRE_Match object; span=(25, 36), match=' in PySpark'&gt; 25 36</strong></span></pre><p id="dd0c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们进入分析的下一部分。</p><h1 id="9c29" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">加载和查看 NASA 日志数据集</h1><p id="c0af" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">假设我们的数据存储在下面提到的路径中(以平面文件的形式)，让我们将它加载到 DataFrame 中。我们会一步一步来。下面的代码获取我们磁盘中的日志数据文件名。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="9aaa" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">['NASA_access_log_Jul95.gz', 'NASA_access_log_Aug95.gz']</strong></span></pre><p id="5b21" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在，我们将使用<code class="fe nj nk nl nm b"><strong class="lt iu">sqlContext.read.text()</strong></code>或<code class="fe nj nk nl nm b"><strong class="lt iu">spark.read.text()</strong></code>来读取文本文件。这将产生一个 DataFrame，其中有一个名为<code class="fe nj nk nl nm b"><strong class="lt iu">value</strong></code>的字符串列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="11b7" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">root<br/> |-- value: string (nullable = true)</strong></span></pre><p id="412e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这使我们可以看到日志数据的模式，它看起来很像文本数据，我们将很快对其进行检查。您可以使用下面的代码查看保存日志数据的数据结构的类型。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="1928" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">type(base_df)</strong></span><span id="c12e" class="nr la it nm b gy nx nt l nu nv"><strong class="nm iu">#Output:</strong><br/><strong class="nm iu">pyspark.sql.dataframe.DataFrame</strong></span></pre><p id="349c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们将在整个教程中使用 Spark 数据帧。但是，如果您愿意，也可以将数据帧转换为 RDD，即 Spark 的原始数据结构(弹性分布式数据集)。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="e25c" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">base_df_rdd = base_df.rdd<br/>type(base_df_rdd)</strong></span><span id="2d26" class="nr la it nm b gy nx nt l nu nv"><strong class="nm iu">#Output<br/>pyspark.rdd.RDD</strong></span></pre><p id="1544" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在让我们看一下数据帧中的实际日志数据。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="8d96" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">base_df.show(10, truncate=False)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/d2a6052531f2dff80df49d62ea10fb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LFtR5oQFEO5YCBR5oC7xjw.png"/></div></div></figure><p id="8850" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这肯定看起来像半结构化的标准服务器日志数据，我们肯定需要做一些数据处理和争论，然后才能有用。请记住，从 rdd 访问数据与下面看到的略有不同。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="d8bc" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">base_df_rdd.take(10)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/16d9f433a5586957e653c861ee4dea14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*In-R33wmLnSa0QOqH1RgVQ.png"/></div></div></figure><p id="f55c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在我们已经加载并查看了日志数据，让我们来处理和讨论它。</p><h1 id="c893" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据争论</h1><p id="17b6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这一节中，我们将尝试清理和解析我们的日志数据集，以真正从每个日志消息中提取具有有意义信息的结构化属性。</p><h2 id="465a" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">日志数据理解</h2><p id="73d9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果您熟悉 web 服务器日志，您会发现上面显示的数据是以<a class="ae ky" href="https://www.w3.org/Daemon/User/Config/Logging.html#common-logfile-format" rel="noopener ugc nofollow" target="_blank">通用日志格式</a>显示的。</p><p id="f887" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这些字段是:<code class="fe nj nk nl nm b"><strong class="lt iu">remotehost rfc931 authuser [date] "request" status bytes</strong></code></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="a5b1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们将需要使用一些特定的技术来从日志数据中解析、匹配和提取这些属性。</p><h2 id="a58d" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">用正则表达式解析和提取数据</h2><p id="2677" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">接下来，我们必须将半结构化日志数据解析成单独的列。我们将使用特殊的内置<code class="fe nj nk nl nm b"><a class="ae ky" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.regexp_extract" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">regexp_extract()</strong></a></code>函数来进行解析。该函数将一列与一个或多个<a class="ae ky" href="http://regexone.com/lesson/capturing_groups" rel="noopener ugc nofollow" target="_blank">捕获组</a>的正则表达式进行匹配，并允许您提取一个匹配的组。我们将为希望提取的每个字段使用一个正则表达式。</p><p id="667b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">到目前为止，您一定听说过或使用过相当多的正则表达式。如果您发现正则表达式令人困惑(它们当然<em class="mt">也可能</em>困惑)，并且您想了解更多关于它们的知识，我们建议查看一下<a class="ae ky" href="http://regexone.com/" rel="noopener ugc nofollow" target="_blank"> RegexOne 网站</a>。你可能还会发现 Goyvaerts 和 Levithan 的<a class="ae ky" href="http://shop.oreilly.com/product/0636920023630.do" rel="noopener ugc nofollow" target="_blank"> <em class="mt">正则表达式手册</em> </a>作为参考很有用。</p><p id="744c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">让我们看看我们在数据集中处理的日志总数。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="a4d8" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">print((base_df.count(), len(base_df.columns)))</strong></span><span id="5c0c" class="nr la it nm b gy nx nt l nu nv"><strong class="nm iu">#Output<br/>(3461613, 1)</strong></span></pre><p id="7f96" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来我们总共有大约 346 万条日志消息。不是一个小数目！让我们摘录并查看一些示例日志消息。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/65ed43bea631ca6f95aaeca5df630abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTj18zv_wJcYV4SmOLi0RQ.png"/></div></div></figure><h2 id="6be8" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">提取主机名</h2><p id="bd3a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们尝试编写一些正则表达式来从日志中提取主机名。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="f142" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">['199.72.81.55',<br/> 'unicomp6.unicomp.net',<br/> '199.120.110.21',<br/> 'burger.letters.com',<br/> ...,<br/> ..., <br/> 'unicomp6.unicomp.net',<br/> 'd104.aa.net',<br/> 'd104.aa.net']</strong></span></pre><h2 id="0f7f" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">提取时间戳</h2><p id="ef67" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们尝试使用正则表达式从日志中提取时间戳字段</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="d7fd" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">['01/Jul/1995:00:00:01 -0400',<br/> '01/Jul/1995:00:00:06 -0400',<br/> '01/Jul/1995:00:00:09 -0400',<br/>  ...,<br/>  ...,<br/> '01/Jul/1995:00:00:14 -0400',<br/> '01/Jul/1995:00:00:15 -0400',<br/> '01/Jul/1995:00:00:15 -0400']</strong></span></pre><h2 id="81ea" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">提取 HTTP 请求方法、URIs 和协议</h2><p id="ec20" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们尝试使用正则表达式从日志中提取 HTTP 请求方法、URIs 和协议模式字段。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="d794" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">[('GET', '/history/apollo/', 'HTTP/1.0'),<br/> ('GET', '/shuttle/countdown/', 'HTTP/1.0'),<br/>  ...,<br/>  ...,<br/> ('GET', '/shuttle/countdown/count.gif', 'HTTP/1.0'),<br/> ('GET', '/images/NASA-logosmall.gif', 'HTTP/1.0')]</strong></span></pre><h2 id="675e" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">提取 HTTP 状态代码</h2><p id="3c33" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们尝试使用正则表达式从日志中提取 HTTP 状态代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="af4c" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">['200', '200', '200', '304', ..., '200', '200']</strong></span></pre><h2 id="6524" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">正在提取 HTTP 响应内容大小</h2><p id="528e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们尝试使用正则表达式从日志中提取 HTTP 响应内容大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="891d" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">['6245', '3985', '4085', '0', ..., '1204', '40310', '786']</strong></span></pre><h2 id="4ca0" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">把所有的放在一起</h2><p id="59f1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们尝试利用我们之前构建的所有正则表达式模式，并使用<code class="fe nj nk nl nm b"><strong class="lt iu">regexp_extract(...)</strong></code>方法来构建我们的数据帧，将所有日志属性整齐地提取到它们自己的单独列中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/116ce42d09f702ba5dc2c9952772ca4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CM4Yb_nNxbHSagQWKNVNUA.png"/></div></div></figure><h2 id="dd38" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">查找缺失值</h2><p id="dc6e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">缺失值和空值是数据分析和机器学习的祸根。让我们看看我们的数据解析和提取逻辑工作得有多好。首先，让我们验证原始数据帧中没有空行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="75a2" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">0</strong></span></pre><p id="8148" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">一切都好！现在，如果我们的数据解析和提取工作正常，我们不应该有任何可能为空值的行。让我们来测试一下吧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="6cd2" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">33905</strong></span></pre><p id="6fdc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">哎哟！看起来我们的数据中有超过 33K 个丢失的值！我们能处理这件事吗？</p><p id="d259" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">请记住，这不是一个常规的 pandas 数据框架，您可以直接查询并获得哪些列为空。我们所谓的<em class="mt">大数据集</em>驻留在磁盘上，它可能存在于 spark 集群的多个节点中。那么我们如何找出哪些列有潜在的空值呢？</p><h2 id="b90e" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">查找空计数</h2><p id="f737" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们通常可以使用下面的技术来找出哪些列有空值。</p><p id="eb38" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">(<strong class="lt iu">注:</strong>此方法改编自 StackOverflow 上的一个<a class="ae ky" href="http://stackoverflow.com/a/33901312" rel="noopener ugc nofollow" target="_blank">精彩回答</a>。)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/b123a364a1edbb43aa4cc4163c955417.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*8vmh5UTDY2qUb9B4f7SXQA.png"/></div></figure><p id="3edd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">嗯，看起来我们在<code class="fe nj nk nl nm b"><strong class="lt iu">status</strong></code> <strong class="lt iu"> </strong>列中少了一个值，其他的都在<code class="fe nj nk nl nm b"><strong class="lt iu">content_size</strong></code>列中。让我们看看是否能找出问题所在！</p><h2 id="3768" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">在 HTTP 状态下处理空值</h2><p id="bcd2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们最初解析<code class="fe nj nk nl nm b">status</code>列的正则表达式是:</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="1cbd" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">regexp_extract('value', r'\s(\d{3})\s', 1).cast('integer')<br/>                                          .alias( 'status')</strong></span></pre><p id="785a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">会不会是数字多了让我们的正则表达式错了？还是数据点本身不好？让我们试着找出答案吧！</p><p id="fac3" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><strong class="lt iu">注</strong>:在下面的表达式中，<code class="fe nj nk nl nm b"><strong class="lt iu">~</strong></code>的意思是“不是”。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="e631" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">1</strong></span></pre><p id="9c47" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们来看看这个不良记录是什么样的！</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="72ed" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">null_status_df.show(truncate=False)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e28533f95abdef04e4bb118883f43b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*CB0gowzb7SezU9To4ko1GQ.png"/></div></figure><p id="9326" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来像是有很多缺失信息的记录！让我们通过我们的日志数据解析管道来传递它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/de4c701514b844cb031de827c77e3eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*mpHtnQ_ziHTloqWqMWLO7g.png"/></div></figure><p id="811f" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来记录本身是一条不完整的记录，没有任何有用的信息，最好的选择是删除这条记录，如下所示！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/574cff2d6461ddad99f169900646b268.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*ruAUzrIoQbyZfSz-UUmnxA.png"/></div></figure><h2 id="363f" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">处理 HTTP 内容大小中的空值</h2><p id="269c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">基于我们之前的正则表达式，我们对<code class="fe nj nk nl nm b"><strong class="lt iu">content_size</strong></code> <strong class="lt iu"> </strong>列的原始解析正则表达式为:</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="b607" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">regexp_extract('value', r'\s(\d+)$', 1).cast('integer')<br/>                                       .alias('content_size')</strong></span></pre><p id="a8f1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们的原始数据集本身会有缺失的数据吗？让我们试着找出答案吧！我们首先尝试找出基本数据帧中可能缺少内容大小的记录。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="9ff6" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">33905</strong></span></pre><p id="8f4b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">该数量似乎与我们处理的数据帧中缺失的内容大小值的数量相匹配。让我们来看看数据框中缺失内容大小的前十条记录。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="5858" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">null_content_size_df.take(10)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/73f7aa7f0692e785223211e19489be98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O29BHfLX_VgkiStTofrBqw.png"/></div></div></figure><p id="c851" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">很明显，坏的原始数据记录对应于错误响应，没有内容被发回，服务器为<code class="fe nj nk nl nm b"><strong class="lt iu">content_size</strong></code>字段发出一个“<code class="fe nj nk nl nm b"><strong class="lt iu">-</strong></code>”。</p><p id="ae86" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">因为我们不想从分析中丢弃这些行，所以让我们将它们估算或填充为 0。</p><h2 id="58d4" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">修复 content_size 为空的行</h2><p id="4f1a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最简单的解决方案是像我们之前讨论的那样用 0 替换<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>中的空值。Spark DataFrame API 提供了一组专门用于处理空值的函数和字段，其中包括:</p><ul class=""><li id="1379" class="mu mv it lt b lu mo lx mp ma mw me mx mi my mm mz na nb nc bi translated"><code class="fe nj nk nl nm b"><a class="ae ky" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.fillna" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">fillna()</strong></a></code>，用指定的非空值填充空值。</li><li id="fc63" class="mu mv it lt b lu nd lx ne ma nf me ng mi nh mm mz na nb nc bi translated"><code class="fe nj nk nl nm b"><a class="ae ky" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.na" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">na</strong></a></code>，它返回一个<code class="fe nj nk nl nm b"><a class="ae ky" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameNaFunctions" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">DataFrameNaFunctions</strong></a></code>对象，该对象具有许多用于操作空列的函数。</li></ul><p id="5a4d" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">有几种方法可以调用这个函数。最简单的方法是用已知值替换所有的空列。但是，为了安全起见，最好传递一个包含<code class="fe nj nk nl nm b"><strong class="lt iu">(column_name, value)</strong></code>映射的 Python 字典。这就是我们要做的。文档中的一个示例如下所示</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="7d8b" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">&gt;&gt;&gt; df4.na.fill({'age': 50, 'name': 'unknown'}).show()<br/>+---+------+-------+<br/>|age|height|   name|<br/>+---+------+-------+<br/>| 10|    80|  Alice|<br/>|  5|  null|    Bob|<br/>| 50|  null|    Tom|<br/>| 50|  null|unknown|<br/>+---+------+-------+</strong></span></pre><p id="1709" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在我们使用这个函数，用 0 填充<code class="fe nj nk nl nm b"><strong class="lt iu">content_size</strong></code>字段中所有缺失的值！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/f5f31674cfc92bfa49bf85af9454e636.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*MKkKG8rgjDTbdbrUq_4S2Q.png"/></div></figure><p id="e340" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看看，没有丢失值！</p><h2 id="c83c" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">处理时态字段(时间戳)</h2><p id="84a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们有了一个干净的、解析过的数据帧，我们必须将时间戳字段解析成一个实际的时间戳。通用日志格式时间有些不标准。用户定义的函数(UDF)是解析它最直接的方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="bcad" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在让我们使用这个函数来解析 dataframe 中的<code class="fe nj nk nl nm b"><strong class="lt iu">time</strong></code>列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/4dee6f8d45991d1aedf4467239881972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5WtvZBCJIuVCJaYD3waJQ.png"/></div></div></figure><p id="64ba" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">事情看起来不错！让我们通过检查数据帧的模式来验证这一点。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="c028" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">logs_df.printSchema()</strong></span><span id="c7be" class="nr la it nm b gy nx nt l nu nv"><strong class="nm iu">root<br/> |-- host: string (nullable = true)<br/> |-- method: string (nullable = true)<br/> |-- endpoint: string (nullable = true)<br/> |-- protocol: string (nullable = true)<br/> |-- status: integer (nullable = true)<br/> |-- content_size: integer (nullable = false)<br/> |-- time: timestamp (nullable = true)</strong></span></pre><p id="c929" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在让我们缓存<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>,因为我们将在下一部分的数据分析部分广泛使用它！</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="9c17" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">logs_df.cache()</strong></span></pre><h1 id="511f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">对我们的网络日志进行数据分析</h1><p id="cdf4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在我们有了一个数据帧，其中包含了作为数据帧的经过解析和清理的日志文件，我们可以执行一些有趣的探索性数据分析(EDA)来尝试获得一些有趣的见解！</p><h2 id="308f" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">内容大小统计</h2><p id="77b7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们计算一些关于 web 服务器返回的内容大小的统计数据。特别是，我们想知道平均、最小和最大内容大小是多少。</p><p id="fe1c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们可以通过调用<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>的<code class="fe nj nk nl nm b"><strong class="lt iu">content_size</strong></code>列上的<code class="fe nj nk nl nm b"><strong class="lt iu">.describe()</strong></code>来计算统计数据。<code class="fe nj nk nl nm b"><strong class="lt iu">.describe()</strong></code>函数返回给定列的计数、平均值、标准偏差、最小值和最大值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/489776bc1e2f9b01be6c765cc47226c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*nYQN8e73mT-AiJzX4CNVVA.png"/></div></figure><p id="ea87" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">或者，我们可以使用 SQL 直接计算这些统计数据。您可以在<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions" rel="noopener ugc nofollow" target="_blank">文档</a>的<code class="fe nj nk nl nm b"><strong class="lt iu">pyspark.sql.functions</strong></code>模块中探索许多有用的功能。</p><p id="416b" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">在我们应用了<code class="fe nj nk nl nm b"><strong class="lt iu">.agg()</strong></code>函数之后，我们调用<code class="fe nj nk nl nm b"><strong class="lt iu">toPandas()</strong></code>来提取结果并将其转换成一个<code class="fe nj nk nl nm b"><strong class="lt iu">pandas</strong></code>数据帧，这个数据帧在 Jupyter 笔记本上有更好的格式。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4c9b7b44dfa7f641daf15fdf1af58fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*XRBgH7cGjQvR6fnpKk9xdw.png"/></div></figure><p id="c777" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们可以验证结果，看它们是否和预期的一样。</p><h2 id="e7c8" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">HTTP 状态代码分析</h2><p id="e0c1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">接下来，让我们看看日志中出现的状态代码值。我们想知道哪些状态代码值出现在数据中，出现了多少次。我们再次从<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>开始，然后按<code class="fe nj nk nl nm b"><strong class="lt iu">status</strong></code> <strong class="lt iu"> </strong>列分组，应用<code class="fe nj nk nl nm b"><strong class="lt iu">.count()</strong></code>聚合函数，并按<code class="fe nj nk nl nm b"><strong class="lt iu">status</strong></code> <strong class="lt iu"> </strong>列排序。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="36f0" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">Total distinct HTTP Status Codes: 8</strong></span></pre><p id="23bc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来我们总共有 8 个不同的 HTTP 状态代码。让我们以频率表的形式来看看它们的出现情况。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e0e19e5d3098ce5a99be5d6f7291b200.png" data-original-src="https://miro.medium.com/v2/resize:fit:266/format:webp/1*4lYoPa4r56b-Da1Qj3hvZA.png"/></div></figure><p id="b05c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来状态代码 200 OK 是最常见的代码，这是一个很好的迹象，表明大部分时间工作正常。让我们想象一下。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/c6c857ccee2a646682efe9e2f9acb81c.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*Srw9cdBw7oUelcshiHp_Ig.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">HTTP Status Code occurrences</figcaption></figure><p id="e3bd" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">不算太差！但是由于数据中的巨大偏差，几个状态代码几乎不可见。让我们进行一次对数变换，看看情况是否有所改善。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e8bb62d86a872ced0c2789a91bddb15b.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*gAz8NzyG2ryTkKwGMMJ1NA.png"/></div></figure><p id="fd5a" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">结果肯定看起来不错，似乎已经处理了偏斜，让我们通过可视化这些数据来验证这一点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/3780cf521fae0420eb32bf2d2b3fb19c.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*gVDBwU0Jb6282kAo2AzwjA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">HTTP Status Code occurrences — Log Transformed</figcaption></figure><p id="426c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这绝对看起来更好，更少歪斜！</p><h2 id="2684" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">分析频繁主机</h2><p id="5bab" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们看看频繁访问服务器的主机。我们将尝试获得每个<code class="fe nj nk nl nm b"><strong class="lt iu">host</strong></code> <strong class="lt iu"> </strong>的总访问次数，然后按次数排序，只显示前十个最频繁的主机。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/2e9caf33945fe3f45715a47418cc0773.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*7nsH2pSiV2afEd2m0FtG4w.png"/></div></figure><p id="0f89" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这看起来不错，但是让我们更仔细地检查第 9 行中的空白记录。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="4f25" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">host_sum_pd_df = host_sum_df.toPandas()<br/>host_sum_pd_df.iloc[8]['host']</strong></span><span id="1259" class="nr la it nm b gy nx nt l nu nv"><strong class="nm iu">''</strong></span></pre><p id="4397" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来我们有一些空字符串作为顶级主机名之一！这给我们上了宝贵的一课，不仅要检查空值，还要在数据冲突时检查潜在的空字符串。</p><h2 id="8de0" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">显示前 20 个常用端点</h2><p id="03ae" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，让我们来看看日志中对端点(URIs)的点击数。为了执行这个任务，我们从我们的<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code> <strong class="lt iu"> </strong>开始，按<code class="fe nj nk nl nm b"><strong class="lt iu">endpoint</strong></code>列分组，按计数聚合，并像上一个问题一样按降序排序。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/0aa1a7229c5fe69b31b01ef1e65515be.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*cyKWMb_sC4r4oLUHtxPW_Q.png"/></div></figure><p id="3846" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">毫不奇怪，gif、主页和一些 CGI 脚本似乎是最常被访问的资产。</p><h2 id="5999" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">十大错误终点</h2><p id="cae3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">请求的没有返回代码 200 (HTTP 状态正常)的前十个端点是什么？我们创建一个排序列表，其中包含端点和它们被访问的次数，返回代码不为 200，并显示前十名。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/df6152b5a6a00b44a3f9d72be02c8875.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*YV_LZYN5-WTrGP2yPeTpzQ.png"/></div></div></figure><p id="d632" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来 gif(动画\静态图像)加载失败最多。你知道为什么吗？考虑到这些日志是 1995 年的，考虑到当时的网速，我一点也不惊讶！</p><h2 id="0bb2" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">唯一主机的总数</h2><p id="cdc4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这两个月中，访问 NASA 网站的独特主机的总数是多少？我们可以通过几个变换找到这个答案。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="ea9b" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">137933</strong></span></pre><h2 id="03b3" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">每日唯一主机的数量</h2><p id="a0ed" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">作为一个高级示例，我们来看一种方法，它可以每天确定整个日志中唯一主机的数量。该计算将为我们提供每日唯一主机的数量。</p><p id="5317" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们希望有一个数据帧，按月中增加的一天排序，其中包括一个月中的一天，以及这一天的唯一主机的相关数量。</p><p id="6a42" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">考虑一下您需要执行哪些步骤来计算每天发出请求的不同主机的数量。<em class="mt">由于日志仅涵盖一个月，您可以忽略该月。</em>您可能想要使用<code class="fe nj nk nl nm b"><strong class="lt iu">pyspark.sql.functions</strong></code>模块中的<code class="fe nj nk nl nm b"><a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">dayofmonth</strong></a></code> <a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.dayofmonth" rel="noopener ugc nofollow" target="_blank">功能</a>(我们已经将其作为<code class="fe nj nk nl nm b"><strong class="lt iu">F</strong></code>导入)。</p><p id="8f3c" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe nj nk nl nm b"><strong class="lt iu">host_day_df</strong></code> <strong class="lt iu"> : </strong>有两列的数据帧</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f91f9e492b0d9001e0b6f836561f96b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*x9P4SjsghAoiXWtGXxwiuQ.png"/></div></figure><p id="afa4" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">对于<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>中的每一行，该数据帧中将有一行。本质上，我们只是在变换<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>的每一行。例如，对于<code class="fe nj nk nl nm b"><strong class="lt iu">logs_df</strong></code>中的这一行:</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="b050" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">unicomp6.unicomp.net - - [01/Aug/1995:00:35:41 -0400] "GET /shuttle/missions/sts-73/news HTTP/1.0" 302 -</strong></span></pre><p id="4cd1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">你的<code class="fe nj nk nl nm b"><strong class="lt iu">host_day_df</strong></code> <strong class="lt iu"> </strong>应该有:<code class="fe nj nk nl nm b"><strong class="lt iu">unicomp6.unicomp.net 1</strong></code></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/33f98eb4e105bfe75fc55b19a5ff2ec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*hQ8AFuFhgiAuRyo7QSzMsQ.png"/></div></figure><p id="b5f5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe nj nk nl nm b"><strong class="lt iu">host_day_distinct_df</strong></code> <strong class="lt iu"> : </strong>该数据帧具有与<code class="fe nj nk nl nm b"><strong class="lt iu">host_day_df</strong></code>相同的列，但删除了重复的<strong class="lt iu"> ( </strong> <code class="fe nj nk nl nm b"><strong class="lt iu">day</strong></code> <strong class="lt iu">，</strong> <code class="fe nj nk nl nm b"><strong class="lt iu">host</strong></code> <strong class="lt iu"> ) </strong>行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/bbe7430d1ea792103498fa41dbb7fb50.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*PUrAEe8k83oC-56oCOvAow.png"/></div></figure><p id="5f2e" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated"><code class="fe nj nk nl nm b"><strong class="lt iu">daily_unique_hosts_df</strong></code> <strong class="lt iu"> : </strong>有两列的数据帧:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/2a1952aa4682be696a0dcfd32d7af176.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*YacCyXze02Ax6ma23aXxsQ.png"/></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/5064f6dafb45cbbc6baaf4a74e06ebd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/1*2o7t4o2zfiBaixT7CsyG1g.png"/></div></figure><p id="95d1" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">这为我们提供了一个很好的数据框架，显示了每天唯一主机的总数。让我们想象一下！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/48036810da0dc072818c12fc44243603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*PkFhaMMEATbKMxjQZlyUHQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Unique Hosts per Day</figcaption></figure><h2 id="0972" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">每台主机的平均每日请求数</h2><p id="5bba" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在前面的示例中，我们研究了一种方法来确定每天整个日志中唯一主机的数量。现在让我们试着根据我们的日志找出每台主机每天向 NASA 网站发出的平均请求数。我们希望有一个数据帧，按一个月中增加的日期排序，包括一个月中的某一天，以及每台主机在这一天发出的平均请求数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0036c7ef695424c253ad7a55a7e40dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*Gz2rm6Ql71sShAvPNXg1fA.png"/></div></figure><p id="5234" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我们现在可以直观地看到每台主机的平均每日请求。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/77a5a48f5690b45d006094f66cb6e2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*M3zqKalXdr6miW27yfrP2w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Average Daily Requests per Host</figcaption></figure><p id="09f7" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来第 13 天每个主机的请求数达到了最大值。</p><h2 id="1780" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">统计 404 个响应代码</h2><p id="5954" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">创建一个只包含带有 404 状态代码(未找到)的日志记录的数据帧。我们确保<code class="fe nj nk nl nm b"><strong class="lt iu">cache()</strong></code><code class="fe nj nk nl nm b"><strong class="lt iu">not_found_df</strong></code><strong class="lt iu"/>数据帧，因为我们将在这里的其余示例中使用它。你觉得日志里有多少 404 记录？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="22ef" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">Total 404 responses: 20899</strong></span></pre><h2 id="a129" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">列出前二十个 404 响应代码端点</h2><p id="1e5a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用我们之前缓存的仅包含带有 404 响应代码的日志记录的数据帧，我们现在将打印出生成最多 404 错误的前 20 个端点的列表。<em class="mt">记住，顶部端点应该是有序的。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/ae7f0765a0c9685a24d7929beb6f67e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*rHgfHKVC55H7uHBWmRcUIA.png"/></div></figure><h2 id="3e60" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">列出前二十个 404 响应代码主机</h2><p id="49d2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用我们之前缓存的仅包含带有 404 响应代码的日志记录的数据帧，我们现在将打印出生成最多 404 错误的前 20 台主机的列表。请记住，顶级主机应该按顺序排列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/36b97581939808e050e48d30263aef11.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*UU1Zk-c4SvFqXpS3_QinGg.png"/></div></figure><p id="f200" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">给了我们一个好主意，哪个主机最终为 NASA 网页产生了最多的 404 错误。</p><h2 id="4a8a" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">每天可视化 404 个错误</h2><p id="4106" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们暂时(按时间)探索我们的 404 记录。类似于显示每日唯一主机数量的示例，我们将按天分解 404 个请求，并在<code class="fe nj nk nl nm b"><strong class="lt iu">errors_by_date_sorted_df</strong></code>中按天排序每日计数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/febb8e1c293e59aeaf4efea957292a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:228/format:webp/1*Y-Vvey25bHJ5FPrKzNA62w.png"/></div></figure><p id="d309" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">现在让我们想象一下每天总共有 404 个错误。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/df0380b15f791e485eee2414ea4a9b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*ePam3QMB2w1jgG70jPCNjw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Total 404 Error per Day</figcaption></figure><h2 id="8ae2" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">404 错误的前三天</h2><p id="15d9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">根据前面的情节，一个月中出现 404 错误最多的前三天是哪三天？为此，我们可以利用我们之前创建的<code class="fe nj nk nl nm b"><strong class="lt iu">errors_by_date_sorted_df</strong></code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/e2ea911811288627c129e5e19526fb64.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*v2czgs078TA3VpV-E2zkdQ.png"/></div></figure><h2 id="ed2b" class="nr la it bd lb od oe dn lf of og dp lj ma oh oi ll me oj ok ln mi ol om lp on bi translated">可视化每小时 404 次错误</h2><p id="3ea0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用我们之前缓存的 DataFrame <code class="fe nj nk nl nm b"><strong class="lt iu">not_found_df</strong></code> <strong class="lt iu"> </strong>，我们现在将按照一天中的小时以升序进行分组和排序，以创建一个 DataFrame，其中包含一天中每个小时(午夜从 0 开始)对 HTTP 请求的 404 个响应的总数。然后，我们将从数据帧构建一个可视化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/a03314d35eeb8d859d76790ed69edd02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*YPqaELmvPtovdGRqNXyo0Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Total 404 Error per Hour</figcaption></figure><p id="4ecc" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">看起来 404 错误在下午最多，在凌晨最少。我们现在可以将 pandas 显示的最大行数重置为默认值，因为我们之前已经将其更改为显示有限的行数。</p><pre class="kj kk kl km gt nn nm no np aw nq bi"><span id="fc20" class="nr la it nm b gy ns nt l nu nv"><strong class="nm iu">pd.set_option('max_rows', def_mr)</strong></span></pre><h1 id="3344" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="6169" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们在一个关于日志分析的非常常见但重要的案例研究中，采取了动手操作的方法来进行大规模的数据争论、解析、分析和可视化。虽然从规模或数量的角度来看，我们在这里处理的数据可能不是传统意义上的“大数据”，但这些技术和方法足够通用，可以扩展到更大数量的数据。我希望这个案例研究能够让您很好地了解如何轻松地利用 Apache Spark 这样的开源框架来处理大规模的结构化和半结构化数据！</p></div><div class="ab cl ps pt hx pu" role="separator"><span class="pv bw bk pw px py"/><span class="pv bw bk pw px py"/><span class="pv bw bk pw px"/></div><div class="im in io ip iq"><p id="39e5" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">本文附带的所有代码和分析都可以在<a class="ae ky" href="https://github.com/dipanjanS/data_science_for_all/tree/master/tds_scalable_log_analytics" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="mt">我的 GitHub 资源库</em> </strong> </a>中找到。</p><p id="8236" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">你可以在这个<a class="ae ky" href="https://nbviewer.jupyter.org/github/dipanjanS/data_science_for_all/blob/master/tds_scalable_log_analytics/Scalable_Log_Analytics_Spark.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="mt"> Jupyter 笔记本</em> </strong> </a>里找到一步一步的方法。</p></div><div class="ab cl ps pt hx pu" role="separator"><span class="pv bw bk pw px py"/><span class="pv bw bk pw px py"/><span class="pv bw bk pw px"/></div><div class="im in io ip iq"><p id="bd39" class="pw-post-body-paragraph lr ls it lt b lu mo ju lw lx mp jx lz ma mq mc md me mr mg mh mi ms mk ml mm im bi translated">我利用数据科学、人工智能、机器学习和深度学习来解决现实世界的问题。我也在业余时间做一些咨询、研究和指导。如果您需要集中咨询、培训课程，希望我在活动中发言，或者如果您想发表一篇关于<a class="ae ky" href="https://towardsdatascience.com/" rel="noopener" target="_blank"><strong class="lt iu"><em class="mt">TDS</em></strong></a><strong class="lt iu"><em class="mt"/></strong>的文章，请随时通过<a class="ae ky" href="https://www.linkedin.com/in/dipanzan/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">LinkedIn</strong></a><strong class="lt iu">联系我。</strong></p><div class="pz qa gp gr qb qc"><a href="https://www.linkedin.com/in/dipanzan/" rel="noopener  ugc nofollow" target="_blank"><div class="qd ab fo"><div class="qe ab qf cl cj qg"><h2 class="bd iu gy z fp qh fr fs qi fu fw is bi translated">Dipanjan Sarkar -数据科学家-红帽| LinkedIn</h2><div class="qj l"><h3 class="bd b gy z fp qh fr fs qi fu fw dk translated">查看 Dipanjan Sarkar 在世界最大的职业社区 LinkedIn 上的个人资料。Dipanjan 有 9 份工作列在…</h3></div><div class="qk l"><p class="bd b dl z fp qh fr fs qi fu fw dk translated">www.linkedin.com</p></div></div><div class="ql l"><div class="qm l qn qo qp ql qq ks qc"/></div></div></a></div></div></div>    
</body>
</html>