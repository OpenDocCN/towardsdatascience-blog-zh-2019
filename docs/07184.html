<html>
<head>
<title>Deduplication Deduplication</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">重复数据删除</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deduplication-deduplication-1d1414ffb4d2?source=collection_archive---------13-----------------------#2019-10-10">https://towardsdatascience.com/deduplication-deduplication-1d1414ffb4d2?source=collection_archive---------13-----------------------#2019-10-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="26ea" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">是的，这就是我想帮你解决的问题。删除那些造成伤害、阻碍某些任务的效率甚至污染我们系统的肮脏的小副本。</h2></div><blockquote class="ki kj kk"><p id="5fb5" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="ko iu">重复数据删除</strong> <br/> /diːˌdjuːplɪˈkeɪʃ(ə)n/</p><p id="3e5a" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated"><strong class="ko iu"> <em class="it">名词</em> </strong> <em class="it"> <br/> </em>消除重复或多余的信息，尤其是计算机数据。</p><p id="8a81" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">“重复数据删除在存储之前删除重复信息”</p></blockquote><p id="3da9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">正如定义所说，我们要做的任务是删除重复的文本/句子等等。这只不过是检查文本彼此有多相似的行为。他们可以一模一样的像:<br/> <strong class="ko iu">深度学习牛逼！</strong>和<strong class="ko iu">深度学习牛逼！</strong>。或者，就句子试图传达的内容而言，它们可能非常相似，比如:<br/> <strong class="ko iu">深度学习太棒了！</strong>和<strong class="ko iu">深度学习太酷了。<br/> </strong>我们知道这两句话传达的是同一个东西，这也是我们希望我们的机器捕捉到的东西。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/4b88068bb7fbf26da1e8acbe696b5cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/0*rkekSYCilegy33Rq.png"/></div></figure><p id="cc0f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这样的任务在文献中被称为<strong class="ko iu">语义文本相似性(STS)。</strong>它处理确定两段<strong class="ko iu">文本</strong>有多相似。这将不仅包括<strong class="ko iu">句法相似度</strong>，即两个句子中使用的单词有多相似或相同，还包括<strong class="ko iu">语义相似度</strong>，它捕捉了使用两个句子所传达的内容的相似度，即文本的含义在确定相似和不相似方面起着重要作用。</p><h1 id="80f0" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated"><strong class="ak">问题</strong></h1><p id="50f8" class="pw-post-body-paragraph kl km it ko b kp ml ju kr ks mm jx ku li mn kx ky lj mo lb lc lk mp lf lg lh im bi translated">问题。是的，这是我们的主要目标。来解决问题。我给你举个例子。比方说，你必须通过电子邮件向一群人发送非常有趣的笑话(你的笑话可以是一句话或一串话)。你的老板要求你确保人们不会收到同样的笑话。所以你必须确保你所有的笑话都是独一无二的，人们不会对其内容感到厌烦。<br/> <em class="kn">什么工作，认真？</em></p><p id="6302" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">作为一名出色的程序员，你决定自动完成这项任务。你有这个神奇的 API，可以免费给你很多笑话，你写一个脚本，把这些笑话发给你老板喜欢的那群人。但是，我们不能真的相信这个神奇的 API，不是吗？简直<em class="kn">神奇。</em>API 给你开类似的玩笑怎么办？你不能冒险惹恼你的老板。<br/>这是你可以使用<strong class="ko iu"> <em class="kn">重复数据删除引擎</em> </strong>的地方，确保发送的笑话不会与过去发送的笑话相似。</p><p id="94c1" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我在这里的主要目的不是谈论这些模型。而是为了帮助您在实际工作中使用它们，就像上面提到的那样。我承认，为了给你的老板留下深刻印象而向别人发送笑话并不实际。</p><h1 id="63db" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">在 STS 的空间里…</h1><p id="6785" class="pw-post-body-paragraph kl km it ko b kp ml ju kr ks mm jx ku li mn kx ky lj mo lb lc lk mp lf lg lh im bi translated">让我们试着把它分解成这种相似性度量是如何定义的，以及我们试图找出哪两个实体之间的相似性(字面上是文本本身，还是别的什么？).</p><p id="32ee" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">首先是</strong>，说到相似性度量，可以使用的有不少。只是为了完整起见，列举几个:<br/> 1。<a class="ae mq" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> Jaccard 相似度</strong> </a> <strong class="ko iu"> <br/> </strong> 2。<strong class="ko iu"> </strong> <a class="ae mq" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu">余弦相似度</strong> </a> <strong class="ko iu"> <br/> </strong> 3。<strong class="ko iu"> </strong> <a class="ae mq" href="https://en.wikipedia.org/wiki/Earth_mover%27s_distance" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu">推土机距离</strong> </a> <strong class="ko iu"> <br/> </strong> 4。<strong class="ko iu"> </strong> <a class="ae mq" href="https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu">詹森-香农距离</strong> </a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/f5e5147322b575a2bfd8796a12600525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F4pdrdeqm2Hwxk7R.png"/></div></div></figure><p id="0bd9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">但是为了切入正题，我们将使用<strong class="ko iu">余弦相似度。</strong>数学上，<strong class="ko iu">余弦相似度</strong>是<a class="ae mq" href="https://en.wikipedia.org/wiki/Inner_product_space" rel="noopener ugc nofollow" target="_blank">内积空间</a>的两个向量(非零)之间相似度<a class="ae mq" href="https://en.wikipedia.org/wiki/Measure_of_similarity" rel="noopener ugc nofollow" target="_blank">的度量，度量它们之间角度的</a><a class="ae mq" href="https://en.wikipedia.org/wiki/Cosine" rel="noopener ugc nofollow" target="_blank">余弦</a>。</p><p id="c1f3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">如果两个文档相似，并且在欧几里得空间中相距很远，它们仍然可以彼此非常接近。这是由余弦距离捕获的，因此是有利的。</p><p id="679c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">其次，</strong>这个余弦距离我们用在哪里？成对的句子串之间？没有。这就是我们利用<em class="kn">自然语言处理</em>和<em class="kn">深度学习</em>的力量。我们使用<em class="kn">向量</em>。</p><p id="cbce" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu"> <em class="kn">一个单词/句子向量是一行实数值</em> </strong>(与虚拟数字相反)，其中<strong class="ko iu"> <em class="kn">每个点捕捉单词/句子的一个维度的含义</em> </strong>和<strong class="ko iu"> <em class="kn">，其中语义相似的单词/句子具有相似的向量。</em>T11】</strong></p><p id="4a11" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">同样，有很多方法可以得到这些向量。举几个:<br/> <strong class="ko iu">单词嵌入</strong> : word2vec、GloVe、BERT 单词嵌入、ELMo 等等。<br/> <strong class="ko iu">语句嵌入</strong> : BERT 语句嵌入，通用语句编码器等。</p><p id="0818" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我将直接进入我亲自试验过的方法，这些方法对我非常有效。</p><pre class="lm ln lo lp gt mw mx my mz aw na bi"><span id="159d" class="nb lu it mx b gy nc nd l ne nf">               <a class="ae mq" href="https://github.com/mmihaltz/word2vec-GoogleNews-vectors" rel="noopener ugc nofollow" target="_blank">word2vec</a> + <a class="ae mq" href="https://tfhub.dev/google/universal-sentence-encoder-large/3" rel="noopener ugc nofollow" target="_blank">Universal Sentence Encoder</a></span></pre><p id="788d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">为了避免这篇文章成为一篇纯粹的面向实现的文章(这正是我们想要的)，我将试着简单地解释一下这些模型是什么。</p><h1 id="3d1e" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">word2vec</h1><p id="a496" class="pw-post-body-paragraph kl km it ko b kp ml ju kr ks mm jx ku li mn kx ky lj mo lb lc lk mp lf lg lh im bi translated">word2vec 有两种变体:<strong class="ko iu"> Skip-Gram </strong>和<strong class="ko iu">连续单词包模型(CBOW)。</strong>如果你想寻找详细的解释，关于这两种变体有大量的材料。我会很干脆的。skip-gram 模型速度稍慢，但通常在处理不常用的单词时效果更好。因此，这是经常使用的。这个我们简单说一下。</p><p id="d857" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">你会在几乎所有的 word2vec (Skip-Gram model)博客和教程中找到这个图表。在这种架构中，模型使用当前单词来预测上下文单词的周围窗口。它对附近的上下文单词的加权比对远处的上下文单词的加权更重。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ng"><img src="../Images/17b0b81fd5decc91a968f3aa3cace4b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3OdsDJ6HYUXzqTfx.png"/></div></div></figure><p id="4f31" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这里，我们看一个上下文单词的窗口(在这种情况下，每边 2 个单词),并尝试预测中心单词。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/893ff2c92c107117790124b98074f439.png" data-original-src="https://miro.medium.com/v2/format:webp/1*SR6l59udY05_bUICAjb6-w.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">The Skip-gram model architecture (<a class="ae mq" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf</a>)</figcaption></figure><p id="f296" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">考虑<em class="kn"> w(t) </em>是输入字，通常权重矩阵和输入向量<em class="kn"> w(t) </em>之间的点积是由单隐层完成的。我们将<em class="kn"> softmax </em>函数应用于隐藏层的输出向量和权重矩阵之间的点积。这给出了单词在当前单词位置出现在<em class="kn"> w(t) </em>的上下文中的概率。</p><p id="afbd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">隐藏层中的矢量成为了这个单词的矢量表示。但是这些都是'<em class="kn">字'嵌入，</em>和我们要找的相似的'句子'。那么，<em class="kn">我们如何得到句子的向量表示而不仅仅是单词嵌入呢？</em></p><p id="3ee5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">一种简单而琐碎的方法(我今天将展示的方法)是简单地对该句子的所有单词的单词嵌入进行平均。很简单，不是吗？</p><p id="971d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在对于主要部分，让我们把它编码进去。</p><pre class="lm ln lo lp gt mw mx my mz aw na bi"><span id="594a" class="nb lu it mx b gy nc nd l ne nf">w2vmodel = <strong class="mx iu">gensim.models.KeyedVectors.load_word2vec_format</strong>(<br/>'models/GoogleNews-vectors-negative300.bin.gz'), binary=True)<br/></span><span id="f1cb" class="nb lu it mx b gy nm nd l ne nf"><strong class="mx iu">def</strong> sent2vec<strong class="mx iu">(s)</strong>:                               <br/>'''<br/>Finding word2vec vector representation of sentences                               @param s  : sentence                                <br/>'''                               <br/>    words = <strong class="mx iu">str</strong>(s).<strong class="mx iu">lower</strong>()                               <br/>    words = <strong class="mx iu">word_tokenize</strong>(words)                               <br/>    words = [w <strong class="mx iu">for</strong> w <strong class="mx iu">in</strong> words <strong class="mx iu">if</strong> <strong class="mx iu">not</strong> w <strong class="mx iu">in</strong> stop_words]<br/>    words = [w <strong class="mx iu">for</strong> w <strong class="mx iu">in</strong> words <strong class="mx iu">if</strong> w.<strong class="mx iu">isalpha</strong>()]<br/>    <br/>    featureVec = <strong class="mx iu">np.zeros</strong>((300,), dtype="float32")<br/>    nwords = 0<br/>                               <br/>    <strong class="mx iu">for</strong> w <strong class="mx iu">in</strong> words:                                   <br/>        <strong class="mx iu">try</strong>:                                       <br/>            nwords = nwords + 1                                       <br/>            featureVec = <strong class="mx iu">np.add</strong>(featureVec, w2vmodel[w])<br/>        <strong class="mx iu">except</strong>:                                       <br/>            <strong class="mx iu">continue</strong>                               <br/>        # averaging                               <br/>        <strong class="mx iu">if</strong> nwords &gt; 0:                                   <br/>            featureVec = <strong class="mx iu">np.divide</strong>(featureVec, nwords)<br/>    <strong class="mx iu">return</strong> featureVec<br/></span><span id="28dc" class="nb lu it mx b gy nm nd l ne nf"><strong class="mx iu">def</strong> get_w2v_vectors<strong class="mx iu">(list_text1, list_text2)</strong>: <br/>‘’’<br/>Computing the word2vec vector representation of list of sentences<br/>@param list_text1 : first list of sentences<br/>@param list_text2 : second list of sentences <br/>‘’’ <br/>    <strong class="mx iu">print</strong>(“Computing first vectors…”)</span><span id="d3c3" class="nb lu it mx b gy nm nd l ne nf">    text1_vectors = <strong class="mx iu">np.zeros</strong>((len(list_text1), 300))<br/>    <strong class="mx iu">for</strong> i, q <strong class="mx iu">in</strong> tqdm(enumerate(list_text1)):<br/>        text1_vectors[i, :] = sent2vec(q)</span><span id="5fe2" class="nb lu it mx b gy nm nd l ne nf">    text2_vectors = <strong class="mx iu">np.zeros</strong>((len(list_text2), 300))<br/>    <strong class="mx iu">for</strong> i, q <strong class="mx iu">in</strong> tqdm(enumerate(list_text2)):<br/>        text2_vectors[i, :] = sent2vec(q)</span><span id="0cb2" class="nb lu it mx b gy nm nd l ne nf"><strong class="mx iu">    return</strong> text1_vectors, text2_vectors</span></pre><p id="c459" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">就是这样！🤷‍♂你有你的句子嵌入使用<strong class="ko iu"> word2vec </strong>。</p><h1 id="d4af" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">通用句子编码器</h1><p id="9ac1" class="pw-post-body-paragraph kl km it ko b kp ml ju kr ks mm jx ku li mn kx ky lj mo lb lc lk mp lf lg lh im bi translated">谷歌展示了一系列用于将句子编码成向量的模型。作者特别针对下游任务，即迁移学习任务。STS 就是这样一个任务。</p><p id="e7b0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">它有两种变体:<br/> 1。一个带有<a class="ae mq" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu">变压器</strong> </a>编码器<br/> 2。一用一<a class="ae mq" href="http://mlexplained.com/2018/05/11/paper-dissected-deep-unordered-composition-rivals-syntactic-methods-for-text-classification-explained/" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu">深度平均网络</strong> </a></p><p id="d374" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">他们每个人都有不同的设计目标:<br/> 1。以更大的模型复杂性和资源消耗为代价来实现高精度。<br/> 2。目标是以稍微降低的准确度进行有效的推理。</p><p id="70af" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我已经用我最喜欢的博客将这两种架构都做了超链接，这些博客对它们都做了很好的解释。让我们更关注如何实现它们。</p><pre class="lm ln lo lp gt mw mx my mz aw na bi"><span id="31a8" class="nb lu it mx b gy nc nd l ne nf">usemodel = <strong class="mx iu">hub.Module</strong>('models/sentence_encoder')</span><span id="8745" class="nb lu it mx b gy nm nd l ne nf"><strong class="mx iu">def</strong> get_use_vectors<strong class="mx iu">(list_text1, list_text2)</strong>:<br/>'''<br/>Computing the USE vector representation of list of sentences<br/>@param  list_text1  :   first list of sentences<br/>@param  list_text2  :   second list of sentences <br/>'''<br/>    <strong class="mx iu">print</strong>("Computing second vectors...")<br/>    messages1 = list_text1<br/>    messages2 = list_text2</span><span id="8cd1" class="nb lu it mx b gy nm nd l ne nf">    num_batches = <strong class="mx iu">math.ceil</strong>(len(messages1) / BATCH_SIZE)</span><span id="4357" class="nb lu it mx b gy nm nd l ne nf">    # Reduce logging output.<br/>    <strong class="mx iu">tf.logging.set_verbosity</strong>(tf.logging.ERROR)</span><span id="72e6" class="nb lu it mx b gy nm nd l ne nf">    message_embeddings1 = []<br/>    message_embeddings2 = []</span><span id="5dbd" class="nb lu it mx b gy nm nd l ne nf">    <strong class="mx iu">with</strong> tf.Session() <strong class="mx iu">as</strong> session:<br/>        session.<strong class="mx iu">run</strong>([tf.global_variables_initializer(),<br/>             tf.tables_initializer()])</span><span id="8892" class="nb lu it mx b gy nm nd l ne nf">    <strong class="mx iu">for</strong> batch <strong class="mx iu">in</strong> range(num_batches):<br/>        <strong class="mx iu">print</strong>(batch * BATCH_SIZE, batch *<br/>              BATCH_SIZE + BATCH_SIZE)<br/>        batch_msgs1 = messages1[batch * BATCH_SIZE: batch *<br/>                    BATCH_SIZE + BATCH_SIZE]<br/>        batch_msgs2 = messages2[batch * BATCH_SIZE: batch *<br/>                    BATCH_SIZE + BATCH_SIZE]</span><span id="dacb" class="nb lu it mx b gy nm nd l ne nf">        message_embeddings1_temp, message_embeddings2_temp =  session.<strong class="mx iu">run</strong>([usemodel(batch_msgs1), usemodel(batch_msgs2)])<br/>        <br/>        message_embeddings1.<strong class="mx iu">append</strong>(message_embeddings1_temp)<br/>        message_embeddings2.<strong class="mx iu">append</strong>(message_embeddings2_temp)</span><span id="e7ad" class="nb lu it mx b gy nm nd l ne nf">    all_embedding1 = <strong class="mx iu">np</strong>.<strong class="mx iu">concatenate</strong>(<strong class="mx iu">tuple</strong>(message_embeddings1))<br/>    all_embedding2 = <strong class="mx iu">np</strong>.<strong class="mx iu">concatenate</strong>(<strong class="mx iu">tuple</strong>(message_embeddings2))</span><span id="4163" class="nb lu it mx b gy nm nd l ne nf">    <strong class="mx iu">return</strong> all_embedding1, all_embedding2</span></pre><p id="2dad" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">再说一遍，就是这样！🤷‍♂</p><p id="ad17" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在我们有了来自两个不同模型的笑话的句子嵌入。</p><p id="f43d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在我们需要余弦相似度！</p><pre class="lm ln lo lp gt mw mx my mz aw na bi"><span id="5301" class="nb lu it mx b gy nc nd l ne nf"><strong class="mx iu">def</strong> cosine_similarity<strong class="mx iu">(list_vec1, list_vec2)</strong>:<br/>'''<br/>Computing the cosine similarity between two vector representation<br/><a class="ae mq" href="http://twitter.com/param" rel="noopener ugc nofollow" target="_blank">@param</a>  list_text1  :   first list of sentences<br/><a class="ae mq" href="http://twitter.com/param" rel="noopener ugc nofollow" target="_blank">@param</a>  list_text2  :   second list of sentences <br/>'''<br/>    cosine_dist = [cosine(x, y) <strong class="mx iu">for</strong> (x, y) <strong class="mx iu">in</strong> <strong class="mx iu">zip</strong>(np.nan_to_num(list_vec1), np.nan_to_num(list_vec2))]</span><span id="27ba" class="nb lu it mx b gy nm nd l ne nf">    cosine_sim = [(1 - dist) <strong class="mx iu">for</strong> dist <strong class="mx iu">in</strong> cosine_dist]</span><span id="346c" class="nb lu it mx b gy nm nd l ne nf"><strong class="mx iu">    return</strong> cosine_sim</span></pre></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="771a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">当我在你之前做这份工作时，我有一堆不重复的笑话，很少有重复的。具体来说，我有<strong class="ko iu"> 385K </strong>非重复对和<strong class="ko iu"> 10K </strong>重复对。我仅使用<strong class="ko iu"> word2vec </strong>模型绘制了这项任务的<em class="kn"> AUC-ROC </em>。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nu"><img src="../Images/6f4e844dbeb7e5050fbaaf34f3dbd3d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/0*zJT9AAlU6Rkh6A6B"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">AUC-ROC</figcaption></figure><p id="9e2d" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">不错！曲线看起来很漂亮。(我故意省略了混淆矩阵)。</p><p id="cfc5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">TPR/召回率/敏感度</strong>:77%<br/><strong class="ko iu">FPR</strong>:2.2%</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="7dd5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们看看<strong class="ko iu">通用句子编码器</strong>的表现如何。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nu"><img src="../Images/f01115f381a15fac428ebba808e4edad.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/0*AUG_3GavZZzs5gYi"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">AUC-ROC</figcaption></figure><p id="4d2e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">曲线下的区域稍微好一点，不是吗？</p><p id="24ed" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">TPR/召回/敏感度</strong>:77%<br/><strong class="ko iu">FPR</strong>:2.2%</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="ad7b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">让我们看看当我们把它们结合在一起时会发生什么。通过组合，我的意思是平均来自两种方法的余弦相似性，并检查度量。“一个平均的<strong class="ko iu">集合</strong>的模型”。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nu"><img src="../Images/eee18d01b14476abd6d47f811d76be0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/0*xTwzzwf5LTh1Qa1F"/></div></div></figure><p id="e62b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">迄今为止最好的一个！</p><p id="c714" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu">TPR/召回/敏感度</strong>:78.2%<br/><strong class="ko iu">FPR</strong>:1.5%</p><p id="6482" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">喔喔喔！🎉🎉我们有一个明确的赢家！</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="716b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这是一种快速查找重复项的方法。无需培训，只需下载现有的优秀模型并将其用于您的 STS 任务！</p><p id="359c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">有了这一块，您可以轻松构建您的<strong class="ko iu">重复数据删除引擎。只需存储你第一天发送到老板小组的所有笑话，然后对于每个新来的笑话，将它们与之前看到的所有笑话配对，并使用这个集合模型来确定它们是否重复。如果是，就扔掉。</strong></p><p id="6dc0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">通过这种方式，让你老板的朋友开心，老板开心，得到一份不错的薪水，让你自己开心:)</p></div></div>    
</body>
</html>