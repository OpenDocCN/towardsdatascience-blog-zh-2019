<html>
<head>
<title>Intuitive Hyperparameter Optimization : Grid Search, Random Search and Bayesian Search!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">直观的超参数优化:网格搜索，随机搜索和贝叶斯搜索！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/intuitive-hyperparameter-optimization-grid-search-random-search-and-bayesian-search-2102dbfaf5b?source=collection_archive---------16-----------------------#2019-10-30">https://towardsdatascience.com/intuitive-hyperparameter-optimization-grid-search-random-search-and-bayesian-search-2102dbfaf5b?source=collection_archive---------16-----------------------#2019-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="525a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">机器学习算法中的超参数就像煤气炉中的旋钮。就像我们调整煤气炉上的旋钮，直到我们达到正确的设置，让食物按照我们喜欢的方式烹饪。同样，我们调整机器学习算法的超参数，使其工作在最佳水平，并获得我们想要的性能水平。</p><p id="6268" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我开始讨论超参数优化的搜索算法之前，让我打破一些人们对超参数的常见误解！</p><blockquote class="ko kp kq"><p id="fc5b" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu"> <em class="it">误区一:参数和超参数相同</em> </strong></p></blockquote><p id="5d24" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种混乱背后的原因是，在机器学习算法中存在两种类型的可学习实体:参数和超参数。参数由模型在学习期间学习，而超参数由用户在学习之前设置。</p><blockquote class="ko kp kq"><p id="4ac0" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">流言终结者警报</strong></p><p id="39ff" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu"> <em class="it">参数:</em> </strong>训练时学习到的机器学习模型的实体。</p><p id="bb0d" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu"> <em class="it">超参数</em> </strong>:训练开始前设置的机器学习模型的实体。</p></blockquote><p id="2b96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，让我们考虑逻辑回归。逻辑回归是一种分类算法。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi kv"><img src="../Images/22a70cf664ac8de92438f63525c7a222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QiU0ykmUwFUZFp6NDBhMpw.png"/></div></div></figure><p id="bd20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图展示了逻辑回归模型。让我简单解释一下逻辑回归背后的思想。对于任何给定的输入数据 X，我们首先学习 Z=WX+B 形式的线性模型，其中 W 和 B 充当参数。然后我们计算 Z 的 sigmoid，结果是 0 和 1 之间的值。因此，为了决定给定数据点属于哪一类(类 0 或类 1)，我们通常设置阈值(例如阈值=0.5)，这有助于决定类。如果 sigmoid 输出的值大于阈值，则分类任务的输出为 1，即数据点属于类 1，否则属于类 0。诸如 W 和 B 的实体被认为是在给定数据 x 的训练期间学习的参数。而诸如优化算法的类型、正则化技术、α或学习速率、C(用于保持正则化强度的正则化参数的逆，即 C = 1/λ)的实体是超参数的例子。</p><p id="b855" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看另一个关于超参数优化的常见误区。</p><blockquote class="ko kp kq"><p id="b213" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">误区 2 </strong> : <strong class="js iu">使用超参数的默认值就足够了。没有必要试着去调它们。</strong></p></blockquote><p id="1ed3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">超参数的默认值可以被各种机器学习库如 sklearn、Hyperopt 等选为默认值，因为它们对于学术文献中的某些测试问题是足够好的组合。但是，这样的值对于您的问题陈述可能并不理想。永远记住，运用这些价值观，看看什么最适合你。不相信我？开始在数据集上测试这些超参数的不同组合。你会自动看到不同之处！</p><blockquote class="ko kp kq"><p id="1f20" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">流言终结者警报</strong></p><p id="a6fd" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">默认值</strong>仅针对选定的少数问题陈述进行测试，可能不是您的问题<strong class="js iu">的最佳超参数选择。所以，总是调整你的超参数！</strong></p></blockquote><p id="6780" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们已经打破了一些关于超参数优化的常见神话。让我来讨论一些最常见的超参数优化技术。即<strong class="js iu"> <em class="kr">网格搜索</em> </strong>，<strong class="js iu"> <em class="kr">随机搜索</em> </strong>和<strong class="js iu"> <em class="kr">贝叶斯搜索</em> </strong>。</p><p id="6c81" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kr">网格搜索</em> </strong>是一种尝试超参数集所有可能组合的方法。超参数的每个组合代表一个机器学习模型。因此，N 个组合代表 N 个机器学习模型。通过网格搜索，我们确定了表现出最佳性能的模型。这使得网格搜索不仅在时间复杂度上非常昂贵，在空间复杂度上也是如此。</p><p id="4615" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看下图。正则化类型(惩罚)和 C 是逻辑回归的超参数，如下所示。</p><blockquote class="ko kp kq"><p id="8a49" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">逻辑回归的超参数</strong></p><p id="736f" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">正则化类型(或惩罚)= </strong> { l1，l2，None}</p><p id="8baa" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu"><em class="it"/></strong><em class="it">= { 0.1，1} </em></p></blockquote><p id="0414" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">网格搜索在给定数据上测试这些值的所有组合(所有可能的模型),并通过使用用户指定的评估度量(如准确度、精确度、召回率等)来找到最佳可能的模型。</p><blockquote class="ko kp kq"><p id="3270" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu"> <em class="it">组合</em> </strong> <em class="it"> : </em></p><p id="f157" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><em class="it"> {penalty: l1，C=0.1} == &gt; </em>模型 1</p><p id="b4d2" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><em class="it">{惩罚:l1，C=1}。== &gt; </em>型号 2</p><p id="1769" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><em class="it">{惩罚:l2，C=0.1} == &gt; </em>模型 3</p><p id="e642" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><em class="it"> {penalty: l2，C=1} == &gt; </em>模型 4</p><p id="64ec" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><em class="it">{罚:无，C=0.1} == &gt; </em>模型 5</p><p id="e3aa" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><em class="it">{惩罚:无，C=1} == &gt; </em>模型 6</p></blockquote><p id="80a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，网格搜索将一次获取一个惩罚和 C 的组合(一次一个模型),并将其拟合到训练数据(或交叉验证折叠)上，并计算其表现如何。它将对所有可能的(如上所示)组合执行此操作，最终选择表现最佳的模型。请注意，存在更多的逻辑回归超参数，但为了简洁起见，我只选择了其中的两个来演示网格搜索是如何工作的。</p><blockquote class="ko kp kq"><p id="f0a7" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">网格搜索:</strong>测试给定机器学习算法的超参数的所有可能排列组合。</p></blockquote><p id="875b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kr">随机搜索</em> </strong>另一方面<em class="kr">、</em> <strong class="js iu"> <em class="kr"> </em> </strong>是一种我们尝试随机选择超参数组合的方法。这通常在计算上非常便宜，并设法给我们提供足够好的超参数组合，从而实现所需的性能水平。</p><p id="6e28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">换句话说，随机搜索可能很快从所有可能性中选择惩罚和 C 的随机组合(如上所示),并且只测试那些选择的组合。</p><blockquote class="ko kp kq"><p id="b07c" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">随机搜索</strong>:从可搜索空间中随机选择超参数的组合(所有可能的组合)。</p></blockquote><p id="52c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随机搜索的唯一问题是，它没有告诉我们如何选择超参数组合。这个过程完全是随机的，没有办法知道是否存在更好的组合。没有办法缩小搜索空间，因为我们实际上不知道在哪里可以找到更好的价值。</p><p id="405b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="kr">贝叶斯搜索</em> </strong>从另一方面解决了上述问题。这是另一种众所周知的超参数优化方法。顾名思义，该技术基于贝叶斯原理。贝叶斯原理基本上说，后验概率分布与赋予它的先验(先验概率分布)和似然函数成正比。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="gh gi lh"><img src="../Images/1300fdc71703e0a5eda612caf2b18210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vp4WjLi5-3lTCyXkRChCzg.jpeg"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Source of Image: <a class="ae lm" href="https://luminousmen.com/post/data-science-bayes-theorem" rel="noopener ugc nofollow" target="_blank">https://luminousmen.com/post/data-science-bayes-theorem</a></figcaption></figure><p id="04b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之，先验概率分布可以被视为提供给模型的专家知识，以帮助增加机器学习模型通过查看训练数据掌握的知识。假设事件 A 已经发生，似然函数可以被视为对事件 B 有多可能发生(或为真)的理解。最后但并非最不重要的是，后验概率分布可以被认为是最终的学习模型，它包含专家领域知识并考虑给定输入数据的可能性。</p><p id="76b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，贝叶斯优化或贝叶斯搜索考虑了先前已知的知识(先验知识),并且仅搜索那些它认为将提高模型性能的超参数组合。</p><blockquote class="ko kp kq"><p id="fe98" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu"> <em class="it">贝叶斯搜索</em> </strong>:基于贝叶斯规则并考虑先前已知的知识，帮助缩小良好超参数组合的搜索空间。</p></blockquote><p id="e425" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">贝叶斯搜索通常比随机搜索花费更多时间，但比网格搜索花费更少时间。</p><blockquote class="ko kp kq"><p id="caa4" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated"><strong class="js iu">按时间复杂度排序</strong></p><p id="64d5" class="jq jr kr js b jt ju jv jw jx jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kn im bi translated">网格搜索&gt;贝叶斯搜索&gt;随机搜索</p></blockquote><p id="41f8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望这篇文章有助于澄清一些关于超参数优化的最常见的疑问。</p><p id="9f71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢您的阅读！</p><p id="dbe3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">继续学习！</p><p id="23f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">资源:</strong></p><p id="2dcc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">sk learn/Scikit learn</strong>:<a class="ae lm" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/</a></p><p id="19e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">逻辑回归</strong>:<a class="ae lm" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。LogisticRegression.html</a></p><p id="5491" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">远视</strong>:<a class="ae lm" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">https://github.com/hyperopt/hyperopt</a></p><p id="20a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">理解贝叶斯原理的一个很好的来源是:【https://luminousmen.com/post/data-science-bayes-theorem】<a class="ae lm" href="https://luminousmen.com/post/data-science-bayes-theorem" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>