<html>
<head>
<title>Support Vector Machine Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机 Python 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8?source=collection_archive---------0-----------------------#2019-08-12">https://towardsdatascience.com/support-vector-machine-python-example-d67d9b63f1c8?source=collection_archive---------0-----------------------#2019-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/801cecd292d2614c848800329d00f49c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TQTntTnSwpNbGHpy"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@craftedbygc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Green Chameleon</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="1e89" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">支持向量机(SVM)是一种有监督的机器学习算法，能够执行分类、回归甚至离群点检测。线性 SVM 分类器的工作原理是在两个类之间画一条直线。落在线一侧的所有数据点将被标记为一类，落在线另一侧的所有数据点将被标记为第二类。听起来很简单，但是有无限多的行可供选择。我们如何知道哪一行在分类数据方面做得最好？这就是 LSVM 算法发挥作用的地方。LSVM 算法将选择一条线，该线不仅将两个类分开，而且尽可能远离最近的样本。事实上，“支持向量机”中的“支持向量”指的是从原点到指定决策边界的点所绘制的两个位置向量。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi le"><img src="../Images/4b86c1243433698373d3227cd7f10b36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oPJfynmi4gRfBws1"/></div></div></figure><h1 id="e63e" class="lj lk jj bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">算法</h1><p id="8f96" class="pw-post-body-paragraph kg kh jj ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">假设，我们有一个向量<em class="mm"> w </em>，它总是垂直于超平面(垂直于 2 维中的直线)。通过将样本的位置向量投影到向量<em class="mm"> w </em>上，我们可以确定样本距离我们的决策边界有多远。快速复习一下，两个向量的点积与第一个向量到第二个向量的投影成正比。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/93b81a488a1a4561bd7b5056509ff89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9ILfqWj6xO6sUhzS"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://commons.wikimedia.org/wiki/File:Scalar_Projection.png" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Scalar_Projection.png</a></figcaption></figure><p id="91a3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果它是一个正样本，我们将坚持认为前面的决策函数(给定样本的位置向量和<em class="mm"> w </em>的点积加上某个常数)返回一个大于或等于 1 的值。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e06850466011062755e5335a7c7efdcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*RjIAQPDlBO4it_2DXrZ4nw.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/4f7c89fb3d05d4937e6461bcc92a34ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-KQ26FPzcrYxt4qxeo8Ww.png"/></div></div></figure><p id="abf9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，如果它是一个负样本，我们将坚持继续决策函数返回一个小于或等于-1 的值。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/781a438a5b3332477e8298ac54365189.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*9Hf1NC_0rA9cxWhTcbDScg.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/a711bc43bc3a7cad4f568a6f46e1051e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*z03dHPa-3am8zZ6TISHYXA.png"/></div></figure><p id="b854" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，我们不会考虑位于决策边界和支持向量之间的任何样本。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/64d0075aa70d62e2942814fd8c605bc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxY6SfjyBp1TLkf-8zOXsw.png"/></div></div></figure><p id="4d8d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如在<a class="ae jg" href="https://www.youtube.com/watch?v=_PwhiWxHK8o" rel="noopener ugc nofollow" target="_blank">麻省理工学院讲座</a>中所述，为了方便起见，我们引入了一个附加变量。变量<em class="mm"> y </em>对于所有正样本等于正 1，对于所有负样本等于负 1。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/7282736b16833724d2fd0d2d3727d083.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*HktqZDeqLGGGn8UHZ7cQLA.png"/></div></div></figure><p id="2276" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在乘以<em class="mm"> y </em>之后，正样本和负样本的等式彼此相等。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/f8eaf831202e475e057301e578de8cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*0H5xajon7RsfuQw2URq8Fg.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/76e50db669e6ea4b9f195d4eea30e6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*Q27tIVPebxek0-VFzskKsA.png"/></div></figure><p id="1f62" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也就是说，我们可以将约束简化为一个等式。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/6ee027b1eaff3e7d0bdf6e74175abbfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*C0rRhX8lu3un6bQ8LAg4vQ.png"/></div></figure><p id="68a4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们需要解决我们着手最大化利润的过程。为了得到边缘宽度的等式，我们从下面的一个中减去第一个支持向量，然后将结果乘以单位向量<em class="mm"> w </em>，其中<em class="mm"> </em>总是垂直于决策边界。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/038a9386c3cf66ff4be2e28d9c68e47b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*SFlmkQ23OfOyspDeJCXAqw.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/415b41a2ec6ae29a46b54d3e71e5d9b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*ZLjASX9ZWyNCmFYoFTUDPw.png"/></div></figure><p id="394c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用上面的约束和一点代数，我们得到下面的。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/43be7e75b1d3f22045d261d3206d12b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*mNNj3yTWI-OJvgOcWccAOw.png"/></div></div></figure><p id="c1c8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了选择最佳决策边界，我们必须最大化我们刚刚计算的方程。在继续之前，我们还应用了一些技巧(参考<a class="ae jg" href="https://www.youtube.com/watch?v=_PwhiWxHK8o" rel="noopener ugc nofollow" target="_blank">麻省理工学院讲座</a>)。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi na"><img src="../Images/d2c1ecb5fbfeb3669f929f1242fd4e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ja3MzP82nKReMdi9C8EvUw.png"/></div></div></figure><p id="3200" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，在大多数机器学习算法中，我们会使用类似梯度下降的东西来最小化所述函数，然而，对于支持向量机，我们使用拉格朗日函数。拉格朗日超出了本文的范围，但是如果你需要一个快速速成班，我推荐你去看看<a class="ae jg" href="https://www.youtube.com/watch?v=hQ4UNu1P2kw" rel="noopener ugc nofollow" target="_blank"> <em class="mm">汗学院</em> </a>。本质上，使用拉格朗日，我们可以像在高中水平的微积分中一样求解全局最小值(即，取函数的导数并使其等于零)。拉格朗日告诉我们，通过对所有约束求和来减去成本函数，其中每个约束将乘以某个常数α(拉格朗日通常记为λ)。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/3efdf5dc1d4800491222816d559b443a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*vJeT0ECHOz8cigv65DYwbQ.png"/></div></figure><p id="7574" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们执行更多的代数运算，将上一步中找到的方程插回到原始方程中。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nc"><img src="../Images/71dc6de4e910f821f76f9aa82de49d4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*an7vrB2vocOXwU5AEnz26A.png"/></div></div></figure><p id="b15b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们进一步讨论之前，我们需要用矩阵来表示这个方程，而不是求和。原因是，来自<a class="ae jg" href="https://cvxopt.org/userguide/coneprog.html#quadratic-programming" rel="noopener ugc nofollow" target="_blank"> CVXOPT </a>库的<code class="fe nd ne nf ng b">qp</code>函数，我们将使用它来求解拉格朗日函数，接受非常具体的参数。因此，我们需要从:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f584eabbaddb00cc41b05887c8e70447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*LH08DrU10nDb4aRpPdSdKQ.png"/></div></figure><p id="4f7a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/033f8a8f249dcc00fe872256acfa57cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*XuB9OeXcDTiQRj0LIyLi5w.png"/></div></figure><p id="8b66" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并且:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/496194ccbbe7bc38632ae0a9b90acd6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*ZPlD40xGsRa5ML5urT9EIw.png"/></div></figure><p id="9db6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">收件人:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1e827f192f7d055e5cf32b908b9a2cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*RL0haMnOBXmrT2bkArSo7Q.png"/></div></figure><p id="cb3a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用以下标识来实现这一点:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f021243c07b76b909754008195d8aff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*5kgx5xulhwL7JiStznOkGw.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/add96086c6960aa8bf4c622900dc5080.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*1bIawXsqg7rs3KIJu8Z3xA.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/f92eddfbb42fa8dc8baa4921878c076b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gklwi_dktCs520vo3q0xFw.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/babf08a0535e623a27e0cf865695c1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*AbTKtk6hvxaQw4gTFJQm3w.png"/></div></figure><p id="1a6e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在应用它们时，我们得到:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/935a6e1e3edfaa3f68b8d0b6b4ff8cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B7q7uhFVeD0AhlTmQa7M5g.png"/></div></div></figure><p id="a19b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mm">注:X 是 X 和 y 的乘积(不要和 X 混淆)</em></p><p id="e12d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们将变量映射到<a class="ae jg" href="https://cvxopt.org/userguide/coneprog.html#quadratic-programming" rel="noopener ugc nofollow" target="_blank"> CVXOPT </a>库所期望的变量。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/221548d5bd95c9562e762f469989c1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*quqDW0Y46m3LObWcKEOAAw.png"/></div></div></figure><h1 id="601e" class="lj lk jj bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">Python 代码</h1><p id="d0fc" class="pw-post-body-paragraph kg kh jj ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">现在，我们准备写一些代码。我们将从导入必要的库开始。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="2012" class="nv lk jj ng b gy nw nx l ny nz">import numpy as np<br/>import cvxopt<br/>from sklearn.datasets.samples_generator import make_blobs<br/>from sklearn.model_selection import train_test_split<br/>from matplotlib import pyplot as plt<br/>from sklearn.svm import LinearSVC<br/>from sklearn.metrics import confusion_matrix</span></pre><p id="af6b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们定义我们的 SVM 类。正如我们之前提到的，我们可以使用<em class="mm"> </em>拉格朗日函数直接求解<em class="mm"> w </em>和<em class="mm"> b </em>，而不是像线性回归那样使用梯度下降来寻找最佳拟合线。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="e15b" class="nv lk jj ng b gy nw nx l ny nz">class SVM:</span><span id="83d9" class="nv lk jj ng b gy oa nx l ny nz">def fit(self, X, y):<br/>        n_samples, n_features = X.shape</span><span id="a04d" class="nv lk jj ng b gy oa nx l ny nz"># P = X^T X<br/>        K = np.zeros((n_samples, n_samples))<br/>        for i in range(n_samples):<br/>            for j in range(n_samples):<br/>                K[i,j] = np.dot(X[i], X[j])</span><span id="3cd1" class="nv lk jj ng b gy oa nx l ny nz">P = cvxopt.matrix(np.outer(y, y) * K)</span><span id="e114" class="nv lk jj ng b gy oa nx l ny nz"># q = -1 (1xN)<br/>        q = cvxopt.matrix(np.ones(n_samples) * -1)</span><span id="c02f" class="nv lk jj ng b gy oa nx l ny nz"># A = y^T <br/>        A = cvxopt.matrix(y, (1, n_samples))</span><span id="6304" class="nv lk jj ng b gy oa nx l ny nz"># b = 0 <br/>        b = cvxopt.matrix(0.0)</span><span id="e8a2" class="nv lk jj ng b gy oa nx l ny nz"># -1 (NxN)<br/>        G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))</span><span id="bc8b" class="nv lk jj ng b gy oa nx l ny nz"># 0 (1xN)<br/>        h = cvxopt.matrix(np.zeros(n_samples))</span><span id="c156" class="nv lk jj ng b gy oa nx l ny nz">solution = cvxopt.solvers.qp(P, q, G, h, A, b)</span><span id="b259" class="nv lk jj ng b gy oa nx l ny nz"># Lagrange multipliers<br/>        a = np.ravel(solution['x'])</span><span id="41f9" class="nv lk jj ng b gy oa nx l ny nz"># Lagrange have non zero lagrange multipliers<br/>        sv = a &gt; 1e-5<br/>        ind = np.arange(len(a))[sv]<br/>        self.a = a[sv]<br/>        self.sv = X[sv]<br/>        self.sv_y = y[sv]</span><span id="9788" class="nv lk jj ng b gy oa nx l ny nz"># Intercept<br/>        self.b = 0<br/>        for n in range(len(self.a)):<br/>            self.b += self.sv_y[n]<br/>            self.b -= np.sum(self.a * self.sv_y * K[ind[n], sv])<br/>        self.b /= len(self.a)</span><span id="54b9" class="nv lk jj ng b gy oa nx l ny nz"># Weights<br/>        self.w = np.zeros(n_features)<br/>        for n in range(len(self.a)):<br/>            self.w += self.a[n] * self.sv_y[n] * self.sv[n]<br/>        <br/>    def project(self, X):<br/>        return np.dot(X, self.w) + self.b<br/>    <br/>    <br/>    def predict(self, X):<br/>        return np.sign(self.project(X))</span></pre><p id="6159" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，我们将使用<code class="fe nd ne nf ng b">scikit-learn</code>库来生成线性可分数据。我们将阴性样品标记为<code class="fe nd ne nf ng b">-1</code>而不是<code class="fe nd ne nf ng b">0</code>。<code class="fe nd ne nf ng b">cvxopt</code>希望数据采用特定的格式，这就是我们采取中间步骤的原因。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="21c5" class="nv lk jj ng b gy nw nx l ny nz">X, y = make_blobs(n_samples=250, centers=2,<br/>                  random_state=0, cluster_std=0.60)</span><span id="64c5" class="nv lk jj ng b gy oa nx l ny nz">y[y == 0] = -1</span><span id="08de" class="nv lk jj ng b gy oa nx l ny nz">tmp = np.ones(len(X))</span><span id="5dd7" class="nv lk jj ng b gy oa nx l ny nz">y = tmp * y</span></pre><p id="9f22" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们通过绘制图表来感受一下这些数据。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="5c20" class="nv lk jj ng b gy nw nx l ny nz">plt.scatter(X[:, 0], X[:, 1], c=y, cmap='winter')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/7cdfad93e08757cd68bddc98540a1022.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*Ry2SsPq0XdGaZGABxIY5zA.png"/></div></figure><p id="d293" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将数据分成训练集和测试集。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="8c0e" class="nv lk jj ng b gy nw nx l ny nz">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span></pre><p id="916d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们创建并训练我们的支持向量机类的一个实例。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="d6f2" class="nv lk jj ng b gy nw nx l ny nz">svm = SVM()</span><span id="0650" class="nv lk jj ng b gy oa nx l ny nz">svm.fit(X_train, y_train)</span></pre><p id="4d6c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们绘制决策边界和支持向量。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="0271" class="nv lk jj ng b gy nw nx l ny nz">def f(x, w, b, c=0):<br/>    return (-w[0] * x - b + c) / w[1]</span><span id="7c76" class="nv lk jj ng b gy oa nx l ny nz">plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='winter')</span><span id="7e11" class="nv lk jj ng b gy oa nx l ny nz"># w.x + b = 0<br/>a0 = -4; a1 = f(a0, svm.w, svm.b)<br/>b0 = 4; b1 = f(b0, svm.w, svm.b)<br/>plt.plot([a0,b0], [a1,b1], 'k')</span><span id="a54d" class="nv lk jj ng b gy oa nx l ny nz"># w.x + b = 1<br/>a0 = -4; a1 = f(a0, svm.w, svm.b, 1)<br/>b0 = 4; b1 = f(b0, svm.w, svm.b, 1)<br/>plt.plot([a0,b0], [a1,b1], 'k--')</span><span id="2041" class="nv lk jj ng b gy oa nx l ny nz"># w.x + b = -1<br/>a0 = -4; a1 = f(a0, svm.w, svm.b, -1)<br/>b0 = 4; b1 = f(b0, svm.w, svm.b, -1)<br/>plt.plot([a0,b0], [a1,b1], 'k--')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/119966afef9c8f2f4d29dbc516f45c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*L_7VY1YVkIgeR0q2klQMEA.png"/></div></figure><p id="7d8e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用我们的模型来预测测试集中样本的类别。假设我们使用我们的模型来分类数据，我们使用混淆矩阵来评估其准确性。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="094c" class="nv lk jj ng b gy nw nx l ny nz">y_pred = svm.predict(X_test)</span><span id="b60f" class="nv lk jj ng b gy oa nx l ny nz">confusion_matrix(y_test, y_pred)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/c029d67bd295c1b1092e45056e3a0269.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*hplrb1uNJajyhiUjCaQNmg.png"/></div></figure><p id="6edb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用支持向量分类器的<code class="fe nd ne nf ng b">scikit-learn</code>实现来尝试同样的事情。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="edf8" class="nv lk jj ng b gy nw nx l ny nz">svc = LinearSVC()</span><span id="6e80" class="nv lk jj ng b gy oa nx l ny nz">svc.fit(X_train, y_train)</span></pre><p id="843d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练我们的模型之后，我们绘制决策边界和支持向量。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="f2f7" class="nv lk jj ng b gy nw nx l ny nz">plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='winter');<br/>ax = plt.gca()<br/>xlim = ax.get_xlim()<br/>w = svc.coef_[0]<br/>a = -w[0] / w[1]<br/>xx = np.linspace(xlim[0], xlim[1])</span><span id="d76d" class="nv lk jj ng b gy oa nx l ny nz">yy = a * xx - svc.intercept_[0] / w[1]<br/>plt.plot(xx, yy)</span><span id="d067" class="nv lk jj ng b gy oa nx l ny nz">yy = a * xx - (svc.intercept_[0] - 1) / w[1]<br/>plt.plot(xx, yy, 'k--')</span><span id="bd4d" class="nv lk jj ng b gy oa nx l ny nz">yy = a * xx - (svc.intercept_[0] + 1) / w[1]<br/>plt.plot(xx, yy, 'k--')</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/9b369c21844e51a8feeb8f4e59ec9dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*N1QxCRi5XpSVsRVBYk-4cg.png"/></div></figure><p id="964c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们根据样本落在线的哪一侧来预测哪个样本属于哪个类别。</p><pre class="lf lg lh li gt nr ng ns nt aw nu bi"><span id="a2d7" class="nv lk jj ng b gy nw nx l ny nz">y_pred = svc.predict(X_test)</span><span id="8843" class="nv lk jj ng b gy oa nx l ny nz">confusion_matrix(y_test, y_pred)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/febda6bb132abc97914757c58e8c01c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:310/format:webp/1*Un6OJ6u1C2l2aVfKo5RHOg.png"/></div></div></figure><p id="a1a7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，分类器正确地对每个样本进行了分类。</p><h1 id="6e82" class="lj lk jj bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">结论</h1><p id="22b8" class="pw-post-body-paragraph kg kh jj ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">我们看到了如何使用拉格朗日函数来确定最佳的数据分割线。在现实世界中，大多数问题都不是线性可分的。因此，我们利用一种叫做内核技巧的东西，用直线以外的东西来分离数据。请继续关注我们即将发表的关于这个主题的文章。</p></div></div>    
</body>
</html>