# 理解神经网络

> 原文：<https://towardsdatascience.com/understanding-neural-networks-677a1b01e371?source=collection_archive---------21----------------------->

![](img/ace52a39dc2a90b14db34b984e00ed37.png)

Image of neurons inside our brains

神经网络我们这里要考虑的是人工神经网络( **ANN** )。顾名思义，人工神经网络是受人脑功能和结构启发的计算系统。

> 在本文中，我们将讨论**什么是神经网络以及神经网络是如何工作的。**

# 什么是神经网络？

在了解什么是人工神经网络之前，我们必须首先了解神经元如何在大脑中相互通信。所以让我们开始吧…

![](img/1e6a1e65694427f273eb578092268075.png)

Image of communication between neurons

人脑由数十亿个被称为神经元的细胞组成。信息在神经元内部以电信号的形式传递。任何需要传达给大脑任何其他部分的信息都由神经元中的树突收集，然后在神经元细胞体中进行处理，并通过轴突传递给其他神经元。下一个神经元可以根据其强度接受或拒绝传播的电信号。

现在让我们试着理解什么是人工神经网络:

![](img/97e3bba5b5b7bcf6cfe3d041143773f4.png)

Image of ANN-1

人工神经网络是试图模仿大脑内部生物神经元功能的人工神经元的集合。每个连接都可以将信号从一个节点传输到另一个节点，在那里可以进一步处理并传输到下一个连接的人工神经元。人工神经网络可以学习和区分非常复杂的模式，这些模式很难人工提取并输入到机器中。人工神经网络由三种类型的层组成，每一层都是一堆人工神经元。每层人工神经元的数量可以根据你的个人选择而有所不同。

为了更好地理解 ANN，让我们来理解每一层的功能。

*   **输入层**由人工神经元组成，将输入数据带入系统，供后续人工神经元进一步处理。输入层出现在人工神经网络的起点。
*   **隐藏层**在输入层和输出层之间。它接收一组**加权输入**，并通过**激活功能**产生输出。该层被命名为 hidden，因为它不构成输入层或输出层。这是所有处理发生的层。根据问题的复杂程度，可以有一个或多个隐藏层。

> 我们将在本文的后半部分更详细地讨论**加权输入**和**激活函数**。现在请记住，修改后的输入被输入到隐藏层，在该隐藏层的人工神经元内部发生一些处理，产生一个输出，用作下一个隐藏层或输出层的输入。

*   **输出层**是人工神经网络架构的最后一层，为特定问题产生输出。这是在预测问题的结果之前进行任何处理的最后一层。

## 神经元内部发生了什么？

为了更好地理解 ANN 的工作，我们需要了解神经元上到底发生了什么。所以，让我们开始吧…

![](img/5553e37cbc11cc2ab5972bb5787a9fd0.png)

Image of ANN-2

这个 ANN 架构由 n 个输入和单个人工神经元以及单个输出**y _ j组成。这里 ***w1，w2…wn*** 是对应于 ***x1，x2…的输入信号的强度。xn 分别为*和**。**

![](img/3ea9fa16e6694629793fe42d8ab5f539.png)

Image of operations inside an artificial neuron

上图显示的是一个人工神经元。在每个神经元处， ***∑ xiwi*** 通过将权重乘以它们各自的输入并求和来计算。其上应用了激活功能***【f】***。激活只是一个非线性函数，应用于 ***∑ xiwi*** 以在架构中添加非线性。所谓非线性，我的意思是我们在日常生活中面临的大多数问题都很复杂，无法通过线性函数来解决，激活函数帮助我们在我们的架构中添加非线性，以获得解决此类问题的更好结果。一个没有激活函数的神经网络只是线性回归。激活功能帮助它学习更复杂的任务并预测结果。

![](img/8252227592342255e389f8d05ddd232f.png)

With the help of activation functions, neural networks are able to classify linearly non-separable data on the right

## 几个最常用的非线性激活函数是:

![](img/50db08c727b0e60a27419b8f1f509c88.png)

Image of the non-linear activation functions

*   ***s 形函数→f(x) =*** 1/(1+ exp(-x))

它用于二进制分类问题的输出层，会产生输出 0 或 1。由于 sigmoid 函数的值介于 0 和 1 之间，如果该值大于 0.5，我们可以很容易地预测为 1，否则为 0。对于多类分类问题，Softmax 函数是最常见的选择。softmax 函数类似于 sigmoid 函数。你可以在这里阅读更多关于 Softmax 函数[的内容。](https://en.wikipedia.org/wiki/Softmax_function)

*   ***Tanh 函数→f(x)=***2/(1+exp(-2x))-1

它用于神经网络的隐藏层，因为它的值介于-1 到 1 之间，因此隐藏层的平均值为 0 或接近 0。这有助于更容易地学习下一层。

*   ***RELU 函数→f(x) =*** max(0，x)

RELU 比 sigmoid 和 Tanh 函数快得多。它涉及简单的数学。如果你不知道选择哪一个激活函数，那么简单地使用 RELU ，因为它是最通用的激活函数，使用最多。

你可以在这里阅读更多关于其他激活功能[的信息。](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/)

# 神经网络是如何工作的？

既然我们对人工神经网络体系结构的基本结构以及它的几个组成部分的功能有了概念，我们就可以理解神经网络实际上是如何工作的了。

![](img/59674e94ee9804c52dc7fb467040fe61.png)

Image of ANN-3

我们将以*(安-3 的图像)*为例。

*   写有 1 的神经元是偏见。偏置单元是添加到每个预输出层的“额外”神经元，存储值 1。偏置单元没有连接到任何先前的层，从这个意义上说，不代表一个真正的“活动”。为了清楚地理解我们为什么需要偏见，你可以阅读[这篇文章](https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)。
*   ***x_1*** 和 ***x_2*** 为输入。
*   ***(W _11)*** 表示从***x1***分配给隐层第一个神经元的权重，***【W _ 12】***表示从 x1 分配给隐层第二个神经元的权重，以此类推。上标表示权重分配给哪一层，如 ***W*** ，表示权重从输入层到第一个隐藏层， ***W*** 表示权重从第一个隐藏层到输出层。
*   右边的箭头是我们的 ANN 架构的最终输出*(将使用 y^来表示)*。

神经网络的工作过程可以分解为三个步骤:

## 1) →初始化

设计神经网络后的第一步是初始化:

*   用正态分布的随机数初始化所有权重 *(W _11，W _12 …)* ，即~N(0，1)。保持权重接近 0 有助于神经网络更快地学习。
*   将所有偏差设置为 1。

这些是一些最常见的初始化实践。还有各种其他的权重初始化技术，你可以阅读这篇[帖子](http://cs231n.github.io/neural-networks-2/)了解更多信息。

## 2) →前馈

前馈是神经网络用来将输入转化为输出的过程。我们将参考下图来计算 y^.

![](img/70d115899b67c87549cdd7d292d96737.png)

Feedforward calculation for ANN-3 architecture

从右到左阅读图像。

*   第一个操作是将包含偏差的输入向量乘以 W 矩阵。请记住，您可能需要做一些矩阵转换，以使矩阵乘法可行*(将输入矩阵转换为 1X3，然后乘以 W，以创建 1X2 形状的输出)*。
*   矩阵相乘后，应用 sigmoid 激活函数。
*   到目前为止，我们已经计算了第一个隐藏层的值，现在我们将添加一个新的偏差到隐藏层矩阵*(这里从 1X2 变成 1X3)。*下一步是将隐藏层矩阵乘以 W。
*   在矩阵乘法之后，我们将剩下单个值，该值将被馈送到 sigmoid 激活函数，从而产生 y^.。这种类型的 ANN 架构可以用于二进制分类*(如果输出大于 0.5，我们可以预测 1 否则为 0)* 。

按照这些步骤背后的思想，我们可以将一个人工神经网络的输入转化为输出，即使它有 100 个隐藏层。

> 在开始反向传播步骤之前，我们必须了解一些解释反向传播所需的概念。概念包括:
> 
> **→成本函数**
> 
> **→梯度下降**

## 什么是成本函数？

神经网络在机器学习领域获得了如此大的流行，因为它们在每次预测其输出时都具有学习和改进的能力。为了能够改进，我们需要一个误差函数或成本函数，让我们知道我们的预测与实际输出有多远*(成本函数是一种衡量神经网络相对于其给定训练样本和预期输出“有多好”的方法)。*在神经网络中，有许多成本函数。其中一个常用的是伯努利负对数似然损失。

![](img/559ba9114a200d95b37a07e893dcd2e8.png)

Image of Bernoulli negative loglikelihood loss

> 忽略 **L** 前面的“-”。

*   **N** 这里表示训练样本数据点的数量。
*   **p_i** 表示我们模型的预测。
*   **y_i** 为实际输出。
*   M 表示多分类问题中输出类的数量。

成本函数导致总体训练点数的平均损失。通过查看二元和多分类对数似然损失方程，我们还可以得出结论，对于 M = 2，它们导致相同的成本(*误差*)。如果你有兴趣，你可以在这里阅读更多关于其他成本函数[的信息。](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications)

## 什么是梯度下降？

梯度下降是一种优化算法，用于通过沿梯度的负值*(斜率或曲线的陡度，使用偏导数计算)*定义的最陡下降方向迭代移动来最小化某个函数。在机器学习中，我们使用梯度下降来更新模型的参数。参数包括[线性回归](https://en.wikipedia.org/wiki/Linear_regression)的系数和神经网络的权重和偏差。

那么让我们来理解梯度下降是如何帮助神经网络学习的。

![](img/ee9111798be5960033417bf7eb940f99.png)

Graph of cost function(J(w)) vs weight(w)

我们将使用上面的图像*(成本函数(J(w))对权重(w))* 来解释梯度下降。

*   假设对于特定的权重和偏差值，我们得到下面的平均成本*(误差)*，用图中的球表示我们的训练点。我们还可以观察到，通过微调参数，平均成本的值可以低至 J_min(w)。
*   为了更新权重和偏差以最小化成本，梯度下降将在箭头指示的方向上迈出小步。它通过计算成本函数相对于权重和偏差的偏导数，并从相应的权重和偏差中减去它来实现。由于球所在位置的导数为正，如果我们从 w 中减去成本函数相对于 w 的导数，它将减小并更接近最小值。
*   我们迭代地继续这个过程，直到我们到达图表的底部，或者到达我们不能再向下移动的点——局部最小值。对于每个训练样本，神经网络中的所有权重和偏差都会发生这种情况。

> 这些步骤的大小被称为学习率。有了高学习率，我们可以采取更大的步骤，但我们有超过最小值的风险，因为曲线的斜率是不断变化的。由于学习率很低，我们可以自信地向负梯度的方向前进，因为我们经常重新计算它。低学习率更精确，但是计算梯度很耗时，所以我们需要非常长的时间才能收敛。

## 3) →反向传播

反向传播算法只是它的一个扩展，使用链式法则来找出连接输入层和隐藏层的权重的误差(对于两层网络)。如果您对链式法则概念不太熟悉，请务必查看下面的[链接](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction)。

反向传播的主要思想是更新可以帮助我们降低网络成本的权重，从而生成更好的预测。要使用梯度下降更新隐藏层的权重，您需要知道每个隐藏单元对最终输出的影响有多大。由于层的输出由层之间的权重决定，因此由单元产生的误差由通过网络前进的权重来缩放。由于我们知道输出端的误差，我们可以使用权重反向工作到隐藏层。

## 使用梯度下降更新权重

我们可以借助以下等式来更新权重:

![](img/283d7a3b67618d3ac0e6f26631d150e6.png)

Equation-1

> 其中 **η** 是学习率， **δ** 是误差项，表示为

![](img/564c50d063dbc2f19e5a3c26310ece87.png)

Equation-2

> 记住，上式中 **(y−y^ )** 是输出误差，**f′(h)**是指激活函数的导数， **f(h)** 。我们称这个导数为输出梯度。这个**δwi**被添加到相应的权重，这有助于减少网络的误差，如(成本函数(J(w))对权重(w)的图表图像)中所解释的。

为了更好地理解，我们将考虑一个网络，其中输出层具有归因于每个输出单元 *k* 的误差 *δ⁰_k* 。然后，归因于隐藏单元 *j* 的误差是输出误差，由输出和隐藏层(以及梯度)之间的权重来缩放:

![](img/01741c78a239fdebe6229ff298053ec8.png)

Equation-3

然后，梯度下降步骤与之前相同，只是有新的误差:

![](img/f629ffc94aa36fb6e8267a0b43de90fb.png)

Equation-4

其中 ***wij*** 为输入和隐藏层之间的权重，**为输入单元值。无论有多少层，这种形式都适用。权重步长等于步长乘以图层的输出误差乘以该图层的输入值。**

**![](img/e72f00458e13b87339a85f953f877426.png)**

**Equation-5**

**这里，通过从更高层向后传播误差，得到输出误差 ***、δ输出*** 。并且输入值 ***Vin*** 是层的输入，例如隐藏层激活到输出单元。**

# **通过一个例子**

**![](img/7f1a13d72e0a2cd31d02e988d831683e.png)**

**Image of a two-layer network**

> **下图描述了一个双层网络。(**注:**输入值显示为图像底部的节点，而网络的输出值显示为顶部的 **y^** 。输入本身不能算作一个层，这就是为什么这被认为是一个两层网络。)**

**假设我们试图拟合一些二进制数据，目标是 y = 1。我们将从正向传递开始，首先计算隐藏单元的输入。出于理解的目的，在这个网络中没有偏见。**

**![](img/4b4875dc76ea6303fddc353898e1cad6.png)**

**Equation-6**

**和隐藏单元的输出。**

**![](img/49ec2668e62859303acf70388e7eae1a.png)**

**Equation-7**

**将此作为输出单元的输入，网络的输出为**

**![](img/a5b256a626af23ea7449f1f9e9a00669.png)**

**Equation-8**

**有了网络输出，我们可以开始反向传递来计算两个层的权重更新。利用以下事实，对于 sigmoid 函数*f*'(*w*⋅*a*)=*f*(*w*⋅*a*)(1*f*(*w*⋅*a*)，输出单元的误差项为**

**![](img/d8ae7c76eaef121fae1b2ad4ee65c000.png)**

**Equation-9**

**现在我们需要计算带有反向传播的隐藏单元的误差项。这里我们将通过权重 ***W*** 将输出单元的误差项与隐藏单元连接起来。对于隐藏单元误差项，*(方程式-1 的图像)*，但是由于我们有一个隐藏单元和一个输出单元，这就简单多了。**

**![](img/216999fa0ae1990e8ec58c2f6eb4e331.png)**

**Equation-10**

**现在我们有了误差，我们可以计算梯度下降的步骤。隐藏到输出权重步骤是学习率乘以输出单元误差，再乘以隐藏单元激活值。**

**![](img/44cd939bd14f4110c32ce7841b697224.png)**

**Equation-11**

**然后，对于输入到隐藏权重 ***wi*** ，它是学习率乘以隐藏单元误差，再乘以输入值。**

**![](img/eebc0f40ec76e7544b4e2d84ec6e134f.png)**

**Equation-12**

**计算***δwi***后，添加到各自的 wi 中。这就是如何更新权重，以使网络在每次迭代中变得越来越好。**

**从这个例子中，您可以看到使用 sigmoid 函数进行激活的效果之一。sigmoid 函数的最大导数为 0.25，因此输出层中的误差至少减少了 75%，隐藏层中的误差至少减少了 93.75%！您可以看到，如果您有很多图层，使用 sigmoid 激活函数会将输入附近图层中的权重步长快速降低到很小的值。这就是所谓的**消失梯度**问题。你可以在这里阅读更多关于消失渐变问题[。](/the-vanishing-gradient-problem-69bf08b15484)**