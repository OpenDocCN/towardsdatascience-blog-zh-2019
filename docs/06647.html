<html>
<head>
<title>Adding Interpretability to Multiclass Text Classification models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为多类文本分类模型增加可解释性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adding-interpretability-to-multiclass-text-classification-models-c44864e8a13b?source=collection_archive---------9-----------------------#2019-09-23">https://towardsdatascience.com/adding-interpretability-to-multiclass-text-classification-models-c44864e8a13b?source=collection_archive---------9-----------------------#2019-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3315f8b68bb66b6465defeaa132b837d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MJ_rWdGSiskNUXHsWlB_Nw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">ELI5: Image by <a class="ae jg" href="https://pixabay.com/users/sasint-3639875/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1782427" rel="noopener ugc nofollow" target="_blank">Sasin Tipchai</a> from <a class="ae jg" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1782427" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><div class=""/><div class=""><h2 id="6614" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">ELI5:增加可解释性，但不损失准确性</h2></div><p id="3de6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">像我 5 岁一样解释。</p><p id="7906" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对我来说，这是学习的基本原则之一，我试图以一种更容易接受的形式提炼任何概念。正如费曼所说:</p><blockquote class="lu"><p id="f1fa" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">我做不到。我不能把它降低到大一的水平。这意味着我们并没有真正理解它。</p></blockquote><p id="211e" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">所以，当我看到旨在解释机器学习模型的 ELI5 库时，我只是必须尝试一下。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="070d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在向企业解释我们复杂的机器学习分类器时，我们面临的一个基本问题是<strong class="la jk"><em class="mq"/></strong>。</p><p id="1927" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有时候利益相关者想要理解——是什么导致了特定的结果？<strong class="la jk"> <em class="mq">可能是因为手头的任务非常关键，我们不能做出错误的决定。</em> </strong>想象一个基于用户评论采取自动货币行为的分类器。</p><p id="42d5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="mq">也可能是对业务/问题空间了解多一点。</em>T11】</strong></p><p id="ada4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也可能是为了增加你的模型的<strong class="la jk"> <em class="mq">社会接受度</em> </strong>。</p><p id="06c0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="mq">这个帖子是关于解读复杂文本分类模型的。</em>T19】</strong></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="5a1c" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">数据集:</h1><p id="1e96" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">为了解释 ELI5 如何工作，我将使用 Kaggle 上的堆栈溢出数据集。这个数据集包含大约 40000 个帖子和相应的帖子标签。</p><p id="6e9c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是数据集的外观:</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/7405bafe595ce4f9e0ce7ca4378afffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QmqpaMZTYIt6M938V4Si1Q.png"/></div></div></figure><p id="ef3c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是不同类别的分布情况。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nt"><img src="../Images/6909120e940716cab7ea5c07a5329797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lW1_7fBIS23601xJGOW9g.png"/></div></div></figure><p id="374c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个平衡的数据集，因此非常适合我们理解的目的。</p><p id="2881" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们开始吧。你可以跟随这个<a class="ae jg" href="https://www.kaggle.com/mlwhiz/interpreting-text-classification-models-with-eli5" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>中的代码</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="c6b9" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">凝视简单:</h1><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/1b74fba9c72aa181a173074553f026ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IKs3GxWQPCE1AVz6.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://christophm.github.io/interpretable-ml-book/terminology.html" rel="noopener ugc nofollow" target="_blank">Interpretable ML Book</a></figcaption></figure><p id="22e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们首先尝试使用一个简单的 scikit-learn 管道来构建我们的文本分类器，稍后我们将尝试解释它。<strong class="la jk"> <em class="mq">在这个管道中，我将使用一个非常简单的计数矢量器和逻辑回归。</em>T25】</strong></p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="c39e" class="oa ms jj nw b gy ob oc l od oe">from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.linear_model import LogisticRegressionCV<br/>from sklearn.pipeline import make_pipeline</span><span id="c91e" class="oa ms jj nw b gy of oc l od oe"># Creating train-test Split<br/>X = sodata[['post']]<br/>y = sodata[['tags']]</span><span id="9226" class="oa ms jj nw b gy of oc l od oe">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)</span><span id="a4fb" class="oa ms jj nw b gy of oc l od oe"># fitting the classifier<br/>vec = CountVectorizer()<br/>clf = LogisticRegressionCV()<br/>pipe = make_pipeline(vec, clf)<br/>pipe.fit(X_train.post, y_train.tags)</span></pre><p id="b255" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看我们得到的结果:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="d50e" class="oa ms jj nw b gy ob oc l od oe">from sklearn import metrics</span><span id="7fd1" class="oa ms jj nw b gy of oc l od oe">def print_report(pipe):<br/>    y_actuals = y_test['tags']<br/>    y_preds = pipe.predict(X_test['post'])<br/>    report = metrics.classification_report(y_actuals, y_preds)<br/>    print(report)<br/>    print("accuracy: {:0.3f}".format(metrics.accuracy_score(y_actuals, y_preds)))</span><span id="258f" class="oa ms jj nw b gy of oc l od oe">print_report(pipe)</span></pre><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi og"><img src="../Images/b891dca6d2ad957fcfa6b288f6db2ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuVYrbUwVhgJ0SloYTCuMg.png"/></div></div></figure><p id="88e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面是一个非常简单的逻辑回归模型，它表现很好。<strong class="la jk"> <em class="mq">我们可以使用下面的函数来检查它的权重:</em> </strong></p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="e983" class="oa ms jj nw b gy ob oc l od oe">for i, tag in enumerate(clf.classes_):<br/>    coefficients = clf.coef_[i]<br/>    weights = list(zip(vec.get_feature_names(),coefficients))<br/>    print('Tag:',tag)<br/>    print('Most Positive Coefficients:')<br/>    print(sorted(weights,key=lambda x: -x[1])[:10])<br/>    print('Most Negative Coefficients:')<br/>    print(sorted(weights,key=lambda x: x[1])[:10])<br/>    print("--------------------------------------")</span><span id="6801" class="oa ms jj nw b gy of oc l od oe">------------------------------------------------------------<br/>OUTPUT:<br/>------------------------------------------------------------</span><span id="1698" class="oa ms jj nw b gy of oc l od oe">Tag: python<br/>Most Positive Coefficients:<br/>[('python', 6.314761719932758), ('def', 2.288467823831321), ('import', 1.4032539284357077), ('dict', 1.1915110448370732), ('ordered', 1.1558015932799253), ('print', 1.1219958415166653), ('tuples', 1.053837204818975), ('elif', 0.9642251085198578), ('typeerror', 0.9595246314353266), ('tuple', 0.881802590839166)]<br/>Most Negative Coefficients:<br/>[('java', -1.8496383139251245), ('php', -1.4335540858871623), ('javascript', -1.3374796382615586), ('net', -1.2542682749949605), ('printf', -1.2014123042575882), ('objective', -1.1635960146614717), ('void', -1.1433460304246827), ('var', -1.059642972412936), ('end', -1.0498078813349798), ('public', -1.0134828865993966)]<br/>--------------------------------------<br/>Tag: ruby-on-rails<br/>Most Positive Coefficients:<br/>[('rails', 6.364037640161158), ('ror', 1.804826792986176), ('activerecord', 1.6892552000017307), ('ruby', 1.41428459023012), ('erb', 1.3927336940889532), ('end', 1.3650227017877463), ('rb', 1.2280121863441906), ('gem', 1.1988196865523322), ('render', 1.1035255831838242), ('model', 1.0813278895692746)]<br/>Most Negative Coefficients:<br/>[('net', -1.5818801311532575), ('php', -1.3483618692617583), ('python', -1.201167422237274), ('mysql', -1.187479885113293), ('objective', -1.1727511956332588), ('sql', -1.1418573958542007), ('messageform', -1.0551060751109618), ('asp', -1.0342831159678236), ('ios', -1.0319120624686084), ('iphone', -0.9400116321217807)]<br/>--------------------------------------<br/>.......</span></pre><p id="5918" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一切都很好。我们可以看到这些系数是有意义的，我们可以尝试使用这些信息来改进我们的模型。</p><p id="3057" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是上面有很多代码。<strong class="la jk"> <em class="mq"> ELI5 让这个练习对我们来说相当简单</em> </strong>。我们只需使用下面的命令:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="5dc4" class="oa ms jj nw b gy ob oc l od oe">import eli5<br/>eli5.show_weights(clf, vec=vec, top=20)</span></pre><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/20378f577248be07d94398f7ef251cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ctzs7qMP0UaRKJOn1Wa-NA.png"/></div></div></figure><p id="9c6a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在你可以看到 Python 的权重值与我们从手动编写的函数中得到的值相同。探索它会更加美丽和有益健康。</p><p id="9fa8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但这只是冰山一角。正如我们在下面看到的，ELI5 还可以帮助我们调试模型。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="1eff" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">理解我们的简单文本分类模型</h1><p id="1e42" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">现在让我们试着找出为什么一个特殊的例子被错误分类。我使用的例子最初来自 Python 类，但被错误地归类为 Java:</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="91da" class="oa ms jj nw b gy ob oc l od oe">y_preds = pipe.predict(sodata['post'])</span><span id="7cf9" class="oa ms jj nw b gy of oc l od oe">sodata['predicted_label'] = y_preds</span><span id="66cd" class="oa ms jj nw b gy of oc l od oe">misclassified_examples = sodata[(sodata['tags']!=sodata['predicted_label'])&amp;(sodata['tags']=='python')&amp;(sodata['predicted_label']=='java')]</span><span id="aff2" class="oa ms jj nw b gy of oc l od oe"><strong class="nw jk"><em class="mq">eli5.show_prediction(clf, misclassified_examples['post'].values[1], vec=vec)</em></strong></span></pre><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/6bf8ebc9d5d6310343fed930bc5fe5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTwhVGvk1N7yv--VTSKJGQ.png"/></div></div></figure><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/80903b4b39be3311005462d1fc350581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VIuZZgjJhVMzs5bIni1rg.png"/></div></div></figure><p id="449b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上面的例子中，分类器以低概率预测 Java。我们可以检查上面例子中发生的许多事情来改进我们的模型。例如:</p><ol class=""><li id="6662" class="ok ol jj la b lb lc le lf lh om ll on lp oo lt op oq or os bi translated">我们看到分类器考虑了很多数字(不好)，这让我们得出清理数字的结论。或者用日期时间标记替换日期时间对象。</li><li id="4a0f" class="ok ol jj la b lb ot le ou lh ov ll ow lp ox lt op oq or os bi translated">还可以看到，虽然 dictionary 对 Java 的权重为负，但单词<code class="fe oy oz pa nw b">dictionaries</code>的权重为正。所以也许词干也有帮助。</li><li id="7b1e" class="ok ol jj la b lb ot le ou lh ov ll ow lp ox lt op oq or os bi translated">我们还看到像<code class="fe oy oz pa nw b">&lt;pre&gt;&lt;code&gt;</code>这样的词正在影响我们的分类器。清洗的时候要把这些字去掉。</li><li id="155c" class="ok ol jj la b lb ot le ou lh ov ll ow lp ox lt op oq or os bi translated">为什么<code class="fe oy oz pa nw b">date</code>这个词会影响结果？一些值得思考的事情。</li></ol><p id="c757" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看看更多的例子来获得更多这样的想法。你知道要点了。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="3302" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">深入而复杂</h1><p id="9f3b" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">这一切都很好，但是如果我们使用的模型不能像 LSTM 那样提供个体特征的权重呢？正是有了这些模型，可解释性才能发挥非常重要的作用。</p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/5671400b91147d98f28e03043d16de21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5xNgK_ecrURET0sS.png"/></div></div></figure><p id="b5ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了理解如何做到这一点，我们首先在我们的数据上创建一个 TextCNN 模型。<em class="mq">为了节省空间，没有显示模型创建过程</em>，而是将其视为一系列预处理步骤，然后创建深度学习模型。如果有兴趣，你可以看看这个<a class="ae jg" href="https://www.kaggle.com/mlwhiz/interpreting-text-classification-models-with-eli5" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>中的建模步骤。</p><p id="b783" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们有一个经过训练的黑盒模型对象时，从我们的角度来看，事情变得有趣了。 </p><p id="1f75" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ELI5 为我们提供了<code class="fe oy oz pa nw b">eli5.lime.TextExplainer</code>来调试我们的预测——检查文档中什么是重要的，以做出预测决策。</p><p id="4857" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使用<code class="fe oy oz pa nw b"><a class="ae jg" href="https://eli5.readthedocs.io/en/latest/autodocs/lime.html#eli5.lime.lime.TextExplainer" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">TextExplainer</strong></a></code>实例，我们向<code class="fe oy oz pa nw b"><a class="ae jg" href="https://eli5.readthedocs.io/en/latest/autodocs/lime.html#eli5.lime.lime.TextExplainer.fit" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">fit()</strong></a></code>方法传递一个要解释的文档和一个黑盒分类器(一个返回概率的<code class="fe oy oz pa nw b">predict</code>函数)。从文档来看，我们的预测函数应该是这样的:</p><blockquote class="pc pd pe"><p id="fb78" class="ky kz mq la b lb lc kk ld le lf kn lg pf li lj lk pg lm ln lo ph lq lr ls lt im bi translated"><strong class="la jk">预测</strong> ( <em class="jj">可调用</em> ) —黑盒分类流水线。<code class="fe oy oz pa nw b"><strong class="la jk"><em class="jj">predict</em></strong></code>应该是一个函数，它接受一个字符串(文档)列表，并返回一个带有概率值的形状矩阵<code class="fe oy oz pa nw b"><strong class="la jk"><em class="jj">(n_samples, n_classes)</em></strong></code>-每个文档一行，每个输出标签一列。</p></blockquote><p id="f20b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，要使用 ELI5，我们需要定义自己的函数，该函数将一系列字符串(文档)作为输入，并返回一个形状为<code class="fe oy oz pa nw b"><strong class="la jk"><em class="mq">(n_samples, n_classes)</em></strong></code> <strong class="la jk"> <em class="mq">的矩阵。</em> </strong> <em class="mq">你可以看到我们是如何先预处理再预测的。</em></p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="0f8f" class="oa ms jj nw b gy ob oc l od oe">def predict_complex(docs):<br/>    # preprocess the docs as required by our model<br/>    val_X = tokenizer.texts_to_sequences(docs)<br/>    val_X = pad_sequences(val_X, maxlen=maxlen)<br/>    y_preds = model.predict([val_X], batch_size=1024, verbose=0)<br/>    return y_preds</span></pre><p id="6ef0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面给出了我们如何使用<code class="fe oy oz pa nw b">TextExplainer</code>。在我们的简单分类器中使用与之前相同的错误分类示例。</p><pre class="np nq nr ns gt nv nw nx ny aw nz bi"><span id="eb42" class="oa ms jj nw b gy ob oc l od oe">import eli5<br/><strong class="nw jk">from eli5.lime import TextExplainer</strong></span><span id="193a" class="oa ms jj nw b gy of oc l od oe"><strong class="nw jk">te = TextExplainer(random_state=2019)</strong><br/>te.fit(sodata['post'].values[0], predict_complex)<br/>te.show_prediction(target_names=list(encoder.classes_))</span></pre><figure class="np nq nr ns gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pi"><img src="../Images/8283d258d9026e14f6f759268336b444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DUnbGPdtd-ae00Sx3KWR0A.png"/></div></div></figure><p id="f640" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这次它不会被错误分类。你可以看到关键字<code class="fe oy oz pa nw b">dict</code>和<code class="fe oy oz pa nw b">list</code>的出现影响了我们的分类器的决定。一个人可以尝试看到更多的例子，以找到更多的见解。</p><p id="acc7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="mq">那么这到底是怎么运作的呢？</em> </strong></p><p id="7ad5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe oy oz pa nw b"><a class="ae jg" href="https://eli5.readthedocs.io/en/latest/autodocs/lime.html#eli5.lime.lime.TextExplainer" rel="noopener ugc nofollow" target="_blank"><strong class="la jk">TextExplainer</strong></a></code>通过删除一些单词生成大量与文档相似的文本，然后训练一个白盒分类器，预测黑盒分类器的输出，而不是真正的标签。我们看到的解释是针对这个白盒分类器的。</p><p id="ae47" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本质上，这有点类似于师生模型的提炼，我们使用一个简单的模型来预测一个复杂得多的教师模型的输出。</p><p id="1db6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简而言之，它试图创建一个简单的模型来模拟一个复杂的模型，然后向我们展示更简单的模型权重。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="3c04" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">结论</h1><blockquote class="lu"><p id="f940" class="lv lw jj bd lx ly lz ma mb mc md lt dk translated">理解至关重要。能够解释我们的模型可以帮助我们更好地理解我们的模型，从而更好地解释它们。</p></blockquote><p id="b8a1" class="pw-post-body-paragraph ky kz jj la b lb me kk ld le mf kn lg lh mg lj lk ll mh ln lo lp mi lr ls lt im bi translated">ELI5 为我们提供了一个很好的方法来做到这一点。它适用于各种模型，这个库的<a class="ae jg" href="https://eli5.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档是我见过的最好的文档之一。</a></p><p id="a204" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，我喜欢 ELI5 库提供的修饰输出，它以简单快捷的方式解释我的模型。并调试它们。</p><p id="c4e7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要在你的模型中使用 ELI5，你可以跟随这个<a class="ae jg" href="https://www.kaggle.com/mlwhiz/interpreting-text-classification-models-with-eli5" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>中的代码</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6752" class="mr ms jj bd mt mu mv mw mx my mz na nb kp nc kq nd ks ne kt nf kv ng kw nh ni bi translated">继续学习</h1><p id="25c1" class="pw-post-body-paragraph ky kz jj la b lb nj kk ld le nk kn lg lh nl lj lk ll nm ln lo lp nn lr ls lt im bi translated">如果你想<a class="ae jg" rel="noopener" target="_blank" href="/how-did-i-start-with-data-science-3f4de6b501b0?source=---------8------------------">学习</a>更多关于 NLP 和如何创建文本分类模型的知识，我想调出<a class="ae jg" href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">高级机器学习专业化</strong> </a>中的<a class="ae jg" href="https://www.coursera.org/learn/language-processing?ranMID=40328&amp;ranEAID=lVarvwc5BD0&amp;ranSiteID=lVarvwc5BD0-HcQgnbxBjnlE7bTEy2jJRw&amp;siteID=lVarvwc5BD0-HcQgnbxBjnlE7bTEy2jJRw&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk"> <em class="mq">自然语言处理</em> </strong> </a>课程。一定要去看看。它讲述了许多从初学者到 NLP 高级水平的主题。你可能也想看看我在 NLP 学习系列的 NLP 上的一些<a class="ae jg" href="https://towardsdatascience.com/tagged/nlp-learning-series" rel="noopener" target="_blank">帖子。</a></p><p id="fe50" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你的阅读。将来我也会写更多初学者友好的帖子。在<a class="ae jg" href="https://medium.com/@rahul_agarwal?source=post_page---------------------------" rel="noopener"> <strong class="la jk">媒体</strong> </a>关注我或者订阅我的<a class="ae jg" href="http://eepurl.com/dbQnuX?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="la jk">博客</strong> </a>了解他们。一如既往，我欢迎反馈和建设性的批评，可以通过 Twitter <a class="ae jg" href="https://twitter.com/MLWhiz?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> @mlwhiz </a>联系</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="38e3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，一个小小的免责声明——在这篇文章中可能会有一些相关资源的附属链接，因为分享知识从来都不是一个坏主意。</p></div></div>    
</body>
</html>