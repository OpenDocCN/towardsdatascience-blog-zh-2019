<html>
<head>
<title>Use-cases of Google’s Universal Sentence Encoder in Production</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Google 的通用句子编码器在生产中的用例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15?source=collection_archive---------3-----------------------#2019-01-24">https://towardsdatascience.com/use-cases-of-googles-universal-sentence-encoder-in-production-dd5aaab4fc15?source=collection_archive---------3-----------------------#2019-01-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div class="gh gi io"><img src="../Images/432aad0771fea15b0fd35dc6a1fa6b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*uvVyFZ08RbvLHReCnRxJ2Q.png"/></div></figure><div class=""/><p id="c416" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在自然语言处理(NLP)中建立任何深度学习模型之前，文本嵌入起着主要作用。文本嵌入将文本(单词或句子)转换成数字向量。</p><p id="edaf" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">为什么我们要把文本转换成向量？</p><p id="5a83" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">向量是特定维度的数字数组。一个 5×1 大小的向量包含 5 个数，我们可以把它看作 5D 空间中的一个点。如果有两个 5 维的向量，它们可以被认为是 5D 空间中的两点。因此，我们可以根据这两个向量之间的距离来计算它们的远近。</p><p id="f355" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此，在机器学习的研究中，大量的努力被投入到将数据转换成向量中，因为一旦数据被转换成向量，我们就可以通过计算它们的距离来判断两个数据点是否相似。像 Word2vec 和 Glove 这样的技术是通过将单词转换成矢量来实现的。因此“猫”的对应向量将比“鹰”更接近“狗”。但是在嵌入一个句子时，需要在这个向量中捕获整个句子的上下文以及单词。这就是“通用句子编码器”的用武之地。</p><p id="7983" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">通用语句编码器将文本编码成高维向量，这些向量可用于文本分类、语义相似性、聚类和其他自然语言任务。预训练的通用句子编码器在<a class="ae ks" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> Tensorflow-hub 公开提供。</a>它有两种变化，即一种用<strong class="jw iy">变压器编码器</strong>训练，另一种用<strong class="jw iy">深度平均网络(DAN) </strong>训练。这两者在准确性和计算资源需求之间有一个折衷。虽然具有变换器编码器的那个具有更高的精度，但是它在计算上更加密集。使用 DNA 编码的方法在计算上花费较少，并且准确性也稍低。</p><p id="f07f" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里，我们将使用<strong class="jw iy">变压器编码器</strong>版本。在 5 GB ram 实例中，它与其他 5 个深度学习模型一起运行时，对我们来说效果很好。此外，我们可以使用这个版本的通用语句编码器在嵌入级别训练一个具有 150 万数据的分类器。我遇到的通用句子编码器的几个用例是:</p><ol class=""><li id="27e8" class="kt ku ix jw b jx jy kb kc kf kv kj kw kn kx kr ky kz la lb bi translated">作为深度学习模型开始时的嵌入层。</li><li id="7758" class="kt ku ix jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">通过寻找语义相似的句子来执行分类。</li><li id="ad10" class="kt ku ix jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">在分析之前去掉重复的句子或短语。</li></ol><p id="4938" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们看看如何使用 Tensorflow-hub 上提供的预训练通用句子编码器，用于 python 中的上述用例。</p><p id="5126" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，让我们导入所需的库:</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="b0f8" class="lq lr ix lm b gy ls lt l lu lv">import tensorflow as tf<br/>import tensorflow_hub as hub</span></pre><p id="2b34" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在生产中使用时，我们需要将预先训练好的通用语句编码器下载到本地，这样我们每次调用它时都不会被下载。</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="dd7d" class="lq lr ix lm b gy ls lt l lu lv">#download the model to local so it can be used again and again<br/>!mkdir ../sentence_wise_email/module/module_useT<br/># Download the module, and uncompress it to the destination folder. <br/>!curl -L "<a class="ae ks" href="https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed</a>" | tar -zxvC ../sentence_wise_email/module/module_useT</span></pre><p id="7024" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这里，”../sentence _ wise _ email/module/module _ useT”是下载句子编码器文件的文件夹。该编码器优化了大于单词长度的文本，因此可以应用于句子，短语或短段落。</p><p id="38d2" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">例如(官方网站示例):</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="0b5a" class="lq lr ix lm b gy ls lt l lu lv">embed = hub.Module("../sentence_wise_email/module/module_useT")</span><span id="7390" class="lq lr ix lm b gy lw lt l lu lv"># Compute a representation for each message, showing various lengths supported.<br/>word = "Elephant"<br/>sentence = "I am a sentence for which I would like to get its embedding."<br/>paragraph = (<br/>    "Universal Sentence Encoder embeddings also support short paragraphs. "<br/>    "There is no hard limit on how long the paragraph is. Roughly, the longer "<br/>    "the more 'diluted' the embedding will be.")<br/>messages = [word, sentence, paragraph]</span><span id="1986" class="lq lr ix lm b gy lw lt l lu lv"># Reduce logging output.<br/>tf.logging.set_verbosity(tf.logging.ERROR)</span><span id="44af" class="lq lr ix lm b gy lw lt l lu lv">with tf.Session() as session:<br/>    session.run([tf.global_variables_initializer(), tf.tables_initializer()])<br/>    message_embeddings = session.run(embed(messages))</span><span id="e918" class="lq lr ix lm b gy lw lt l lu lv">for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):<br/>        print("Message: {}".format(messages[i]))<br/>        print("Embedding size: {}".format(len(message_embedding)))<br/>        message_embedding_snippet = ", ".join((str(x) for x in        message_embedding[:3]))<br/>        print("Embedding[{},...]\n".<br/>                   format(message_embedding_snippet))</span></pre><p id="ea31" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">它给出的输出:</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="6863" class="lq lr ix lm b gy ls lt l lu lv">Message: Elephant<br/>Embedding size: 512<br/>Embedding: [0.04498474299907684, -0.05743394419550896, 0.002211471786722541, ...]<br/><br/>Message: I am a sentence for which I would like to get its embedding.<br/>Embedding size: 512<br/>Embedding: [0.05568016692996025, -0.009607920423150063, 0.006246279925107956, ...]<br/><br/>Message: Universal Sentence Encoder embeddings also support short paragraphs. There is no hard limit on how long the paragraph is. Roughly, the longer the more 'diluted' the embedding will be.<br/>Embedding size: 512<br/>Embedding: [0.03874940797686577, 0.0765201598405838, -0.0007945669931359589, ...]</span></pre><p id="2da5" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">可以看出，无论是单词、句子还是短语，句子编码器都能够给出大小为 512 的嵌入向量。</p><p id="0ac8" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">如何在 Rest API 中使用</strong></p><p id="77b6" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在 Rest API 中使用它时，您必须多次调用它。一次又一次地调用模块和会话将会非常耗时。(在我们的测试中，每次通话大约需要 16 秒)。可以做的一件事是调用模块并在开始时创建会话，然后继续重用它。(第一次呼叫需要大约 16s，然后连续呼叫需要大约. 3s)。</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="ba63" class="lq lr ix lm b gy ls lt l lu lv">#Function so that one session can be called multiple times. <br/>#Useful while multiple calls need to be done for embedding. <br/>import tensorflow as tf<br/>import tensorflow_hub as hub<br/>def embed_useT(module):<br/>    with tf.Graph().as_default():<br/>        sentences = tf.placeholder(tf.string)<br/>        embed = hub.Module(module)<br/>        embeddings = embed(sentences)<br/>        session = tf.train.MonitoredSession()<br/>    return lambda x: session.run(embeddings, {sentences: x})</span><span id="0166" class="lq lr ix lm b gy lw lt l lu lv">embed_fn = embed_useT('../sentence_wise_email/module/module_useT')<br/>messages = [<br/>    "we are sorry for the inconvenience",<br/>    "we are sorry for the delay",<br/>    "we regret for your inconvenience",<br/>    "we don't deliver to baner region in pune",<br/>    "we will get you the best possible rate"<br/>]<br/>embed_fn(messages)</span></pre><p id="63c1" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">输出是一个 5*512 维的矩阵。(每个句子是一个大小为 512 的向量)。因为这些值是归一化的，所以编码的内积可以被视为相似性矩阵。</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="b3a5" class="lq lr ix lm b gy ls lt l lu lv">encoding_matrix = embed_fn(messages)<br/>import numpy as np<br/>np.inner(encoding_matrix, encoding_matrix)</span></pre><p id="c6d9" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">输出是:</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="15fb" class="lq lr ix lm b gy ls lt l lu lv">array([[1.        , 0.87426376, 0.8004891 , 0.23807861, 0.46469775],<br/>       [0.87426376, 1.0000001 , 0.60501504, 0.2508136 , 0.4493388 ],<br/>       [0.8004891 , 0.60501504, 0.9999998 , 0.1784874 , 0.4195464 ],<br/>       [0.23807861, 0.2508136 , 0.1784874 , 1.0000001 , 0.24955797],<br/>       [0.46469775, 0.4493388 , 0.4195464 , 0.24955797, 1.0000002 ]],<br/>      dtype=float32)</span></pre><p id="36ef" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从这里可以看出，“我们对给您带来的不便感到抱歉”和“我们对延迟感到抱歉”之间的相似度是 0.87(第 1 行，第 2 列)，而“我们对给您带来的不便感到抱歉”和“我们将为您提供尽可能好的价格”之间的相似度是 0.46(第 1 行，第 5 列)，这很惊人。还有其他方法可以从编码中找到相似性得分，如余弦相似性、曼哈顿距离等。(文末提到的我的 Github repo 里有代码)。</p><p id="8d2d" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">删除重复文本</strong></p><p id="c88a" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在开发问答验证系统时，一个主要问题是回答语句的重复。遍历我们的可用数据，我们发现语义相似性得分&gt; 0.8(通过上述编码的内积计算)的句子实际上是重复语句，因此我们删除了它们。</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="63f7" class="lq lr ix lm b gy ls lt l lu lv">#It takes similarity matrix (generated from sentence encoder) as input and gives index of redundant statements<br/>def redundant_sent_idx(sim_matrix):<br/>    dup_idx = [] <br/>    for i in range(sim_matrix.shape[0]):<br/>        if i not in dup_idx:<br/>            tmp = [t+i+1 for t in list(np.where( sim_matrix[i][i+1:] &gt; 0.8 )[0])]<br/>            dup_idx.extend(tmp)<br/>    return dup_idx<br/>#indexes of duplicate statements.<br/>dup_indexes  = redundant_sent_idx(np.inner(encoding_matrix,<br/>                                           encoding_matrix))</span><span id="69d7" class="lq lr ix lm b gy lw lt l lu lv">unique_messages = np.delete(np.array(messages), dup_indexes)</span></pre><p id="d76d" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">独特的信息是:</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="c41d" class="lq lr ix lm b gy ls lt l lu lv">array(['we are sorry for the inconvenience',<br/>       "we don't deliver to baner region in pune",<br/>       'we will get you the best possible rate'], dtype='&lt;U40')</span></pre><p id="51b5" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">基本上，它放弃了陈述“我们为延迟感到抱歉”和“我们为你的不便感到抱歉”,因为它们是第一句的重复。</p><p id="c872" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw iy">通过查找语义相似的句子进行分类</strong></p><p id="3b65" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在构建答案评估系统时，我们遇到了检测后续语句的问题。但是没有足够的数据来训练一个统计模型。多亏了通用句子编码器，我们可以解决这个问题。我们采用的方法是，创建一个所有可用数据的编码矩阵。然后获取用户输入的编码，看是否与任何可用数据的相似度超过 60%，以此作为后续。简单的问候标识可以是:</p><pre class="lh li lj lk gt ll lm ln lo aw lp bi"><span id="87f6" class="lq lr ix lm b gy ls lt l lu lv">greets = ["What's up?",<br/> 'It is a pleasure to meet you.',<br/> 'How do you do?',<br/> 'Top of the morning to you!',<br/> 'Hi',<br/> 'How are you doing?',<br/> 'Hello',<br/> 'Greetings!',<br/> 'Hi, How is it going?',<br/> 'Hi, nice to meet you.',<br/> 'Nice to meet you.']<br/>greet_matrix = embed_fn(greets)<br/>test_text = "Hey, how are you?"<br/>test_embed = embed_fn([test_text])<br/>np.inner(test_embed, greet_matrix)<br/>sim_matrix  = np.inner(test_embed, greet_matrix)<br/>if sim_matrix.max() &gt; 0.8:<br/>    print("it is a greetings")<br/>else:<br/>    print("it is not a greetings")</span></pre><div class="ip iq gp gr ir lx"><a href="https://github.com/sambit9238/Deep-Learning/blob/master/tensorflow_hub_useT.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iy gy z fp mc fr fs md fu fw iw bi translated">sambit 9238/深度学习</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">深度学习技术在自然语言处理、计算机视觉等领域的实现。-sambit 9238/深度学习</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">github.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml it lx"/></div></div></a></div><p id="94b9" class="pw-post-body-paragraph ju jv ix jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://tfhub.dev/google/universal-sentence-encoder-large/3" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder-large/3</a></p><div class="ip iq gp gr ir lx"><a href="https://arxiv.org/abs/1803.11175" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iy gy z fp mc fr fs md fu fw iw bi translated">通用句子编码器</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">我们提出了将句子编码成嵌入向量的模型，这些向量专门针对其他自然语言处理的迁移学习</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>