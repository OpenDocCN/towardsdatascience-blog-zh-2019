<html>
<head>
<title>PySpark debugging — 6 common issues</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark 调试— 6 个常见问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyspark-debugging-6-common-issues-8ab6e7b1bde8?source=collection_archive---------8-----------------------#2019-10-17">https://towardsdatascience.com/pyspark-debugging-6-common-issues-8ab6e7b1bde8?source=collection_archive---------8-----------------------#2019-10-17</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><figure class="gm go js jt ju jv gi gj paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="gi gj jr"><img src="../Images/81e7f2cf8b1cb577d6b8ec21547323eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgkjkj6BLVS1uD4mw_sTEg.png"/></div></div></figure><p id="8d69" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">调试 spark 应用程序可以是有趣的，也可以是非常(我是说<em class="la">非常</em>)令人沮丧的经历。</p><p id="ab8f" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">我已经开始收集我不时遇到的问题，列出最常见的问题及其解决方案。</p><p id="126a" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">这是这个列表的第一部分。我希望它对你有用，并能为你节省一些时间。大多数问题都很容易解决，但是它们的堆栈跟踪可能很神秘，没有什么帮助。</p><figure class="lc ld le lf gu jv gi gj paragraph-image"><div class="gi gj lb"><img src="../Images/37f8f1cfd773ed4dfd4a9554acadfd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*qA55zTTQZzuX8WZa5ZuJ-A.png"/></div></figure></div><div class="ab cl lg lh hy li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="in io ip iq ir"><h1 id="5a59" class="ln lo iu bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated">1.从 udf 返回的列为空</h1><p id="4f96" class="pw-post-body-paragraph kc kd iu ke b kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz in bi translated">当您使用 udf 向数据帧添加一列，但结果为<strong class="ke iv"> Null </strong>时:udf 返回的数据类型与定义的不同</p><p id="69d9" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">例如，如果您定义一个 udf 函数，将两个数字<code class="fe mq mr ms mt b">a</code>和<code class="fe mq mr ms mt b">b</code>作为输入，并返回<code class="fe mq mr ms mt b">a / b</code>，这个 udf 函数将返回一个 float(在 Python 3 中)。如果 udf 定义为:</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="cf8c" class="my lo iu mt b gz mz na l nb nc">udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.BooleanType())<br/># or <br/>udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.DoubleType())</span></pre><p id="ad7d" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">而不是:</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="fe96" class="my lo iu mt b gz mz na l nb nc">udf_ratio_calculation = F.udf(calculate_a_b_ratio, T.FloatType())</span></pre><p id="4a6e" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">那么使用 udf 的结果将是这样的:</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="8acb" class="my lo iu mt b gz mz na l nb nc">df = df.withColumn('a_b_ratio', udf_ratio_calculation('a', 'b'))<br/>df.show() <br/>+---+---+---------+<br/>|  a|  b|a_b_ratio|<br/>+---+---+---------+<br/>|  1|  0|     null|<br/>| 10|  3|     null|<br/>+---+---+---------+</span></pre><p id="d9fe" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">完整示例:</p><figure class="lc ld le lf gu jv"><div class="bz fq l di"><div class="nd ne l"/></div><figcaption class="nf ng gk gi gj nh ni bd b be z dk">Example of wrongly defined udf return datatype</figcaption></figure><h1 id="3f87" class="ln lo iu bd lp lq nj ls lt lu nk lw lx ly nl ma mb mc nm me mf mg nn mi mj mk bi translated">2.ClassNotFoundException</h1><p id="e09a" class="pw-post-body-paragraph kc kd iu ke b kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz in bi translated">当您试图将应用程序连接到外部系统(如数据库)时，通常会发生这种异常。下面的 stacktrace 来自于在 Postgres 中保存数据帧的尝试。</p><figure class="lc ld le lf gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="gi gj no"><img src="../Images/6ac7ba4826237561ec8b097e95d65267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZaIDezUc93UGgF-qUK3KQ.png"/></div></div></figure><p id="5186" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">这意味着 spark 无法找到连接数据库所需的 jar 驱动程序。在实例化会话时，我们需要在 spark 配置中为应用程序提供正确的 jar</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="e4e8" class="my lo iu mt b gz mz na l nb nc"><strong class="mt iv">from </strong>pyspark <strong class="mt iv">import </strong>SparkConf<br/><strong class="mt iv">from </strong>pyspark.sql <strong class="mt iv">import </strong>SparkSession<br/><br/><br/>conf = SparkConf()<br/>conf.set('<strong class="mt iv">spark.jars</strong>', '<strong class="mt iv">/full/path/to/postgres.jar,/full/path/to/other/jar</strong>')</span><span id="6277" class="my lo iu mt b gz np na l nb nc"><br/>spark_session = SparkSession.builder \<br/>    .config(conf=conf) \<br/>    .appName(<strong class="mt iv">'</strong>test<strong class="mt iv">'</strong>) \<br/>    .getOrCreate()</span></pre><p id="63dd" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">或者作为命令行参数——取决于我们如何运行我们的应用程序。</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="7114" class="my lo iu mt b gz mz na l nb nc">spark-submit --jars /full/path/to/postgres.jar,/full/path/to/other/jar ...</span></pre><p id="2a84" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated"><strong class="ke iv">注 1</strong>:jar 对于所有节点 都是<strong class="ke iv"> <em class="la">可访问的，而不是驱动程序本地的，这一点非常重要。</em></strong></p><p id="18f1" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated"><strong class="ke iv">注 2 </strong>:该错误也可能意味着集群组件之间的 spark 版本不匹配。还有其他更常见的指示器，如<code class="fe mq mr ms mt b">AttributeError</code>。更多关于这个<a class="ae nq" href="https://radanalytics.io/howdoi/recognize-version-clash" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="2385" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated"><strong class="ke iv">注 3 </strong>:确保罐子列表中的逗号之间没有<strong class="ke iv">空格。</strong></p><h2 id="5e8e" class="my lo iu bd lp nr ns dn lt nt nu dp lx kn nv nw mb kr nx ny mf kv nz oa mj ob bi translated">3.低内存消耗——为什么 spark 没有用完我的所有资源？</h2><p id="7d9c" class="pw-post-body-paragraph kc kd iu ke b kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz in bi translated">火花驱动器存储器和火花执行器存储器默认设置为<code class="fe mq mr ms mt b">1g</code>。一般来说，查看许多配置参数及其默认值是非常有用的，因为有许多因素会影响您的 spark 应用程序。</p><div class="oc od gq gs oe of"><a href="https://spark.apache.org/docs/latest/configuration.html" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fp"><div class="oh ab oi cl cj oj"><h2 class="bd iv gz z fq ok fs ft ol fv fx it bi translated">火花配置</h2><div class="om l"><h3 class="bd b gz z fq ok fs ft ol fv fx dk translated">火花提供了三个位置来配置系统:火花属性控制大多数应用参数，可以…</h3></div><div class="on l"><p class="bd b dl z fq ok fs ft ol fv fx dk translated">spark.apache.org</p></div></div><div class="oo l"><div class="op l oq or os oo ot ka of"/></div></div></a></div><p id="e2b3" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">当 spark 在本地运行时，您应该将<code class="fe mq mr ms mt b">spark.driver.memory</code>调整为对您的系统合理的值，例如<code class="fe mq mr ms mt b">8g</code>,当在集群上运行时，您可能还想调整<code class="fe mq mr ms mt b">spark.executor.memory</code>,尽管这取决于您的集群类型及其配置。</p><figure class="lc ld le lf gu jv"><div class="bz fq l di"><div class="nd ne l"/></div></figure><h1 id="19ce" class="ln lo iu bd lp lq nj ls lt lu nk lw lx ly nl ma mb mc nm me mf mg nn mi mj mk bi translated">4.文件不存在:Spark 在本地模式下运行正常，但在 YARN 中运行时找不到文件</h1><p id="bffc" class="pw-post-body-paragraph kc kd iu ke b kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz in bi translated">和第二步一样，所有必要的文件/jar 应该放在集群中所有组件都可以访问的地方，例如 FTP 服务器或一个普通的挂载驱动器。</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="3e4d" class="my lo iu mt b gz mz na l nb nc">spark-submit --master yarn --deploy-mode cluster http://somewhere/accessible/to/master/and/workers/test.py</span></pre><h1 id="321b" class="ln lo iu bd lp lq nj ls lt lu nk lw lx ly nl ma mb mc nm me mf mg nn mi mj mk bi translated">5.尝试连接到数据库:Java . SQL . SQL 异常:没有合适的驱动程序</h1><figure class="lc ld le lf gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="gi gj ou"><img src="../Images/0e88682841a0e625303b7e19c48a2d84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OvVc7OLdDo8JFrl7QQgpZg.png"/></div></div></figure><p id="4e87" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">或者，如果在试图保存到数据库时发生错误，您将得到一个<code class="fe mq mr ms mt b">java.lang.NullPointerException</code>:</p><figure class="lc ld le lf gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="gi gj ov"><img src="../Images/7fbc49eeb25972dbecf30b0f016d2049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dtRnUsYor70JUgjtyGa_xA.png"/></div></div></figure><p id="9995" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">这通常意味着我们忘记了设置<code class="fe mq mr ms mt b"><strong class="ke iv">driver</strong></code>，例如 Postgres 的<code class="fe mq mr ms mt b"><strong class="ke iv">org.postgresql.Driver</strong></code> <strong class="ke iv"> </strong>:</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="bccc" class="my lo iu mt b gz mz na l nb nc">df = spark.read.format(<strong class="mt iv">'jdbc'</strong>).options(<br/>    url= <strong class="mt iv">'db_url'</strong>,<br/>    driver='<strong class="mt iv">org.postgresql.Driver</strong>',  # &lt;-- here<br/>    dbtable=<strong class="mt iv">'table_name'</strong>,<br/>    user=<strong class="mt iv">'user'</strong>,<br/>    password=<strong class="mt iv">'password'</strong><br/>).load()</span></pre><p id="a43d" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">另外，请确保检查#2，以便正确设置驱动程序 jar。</p><h1 id="be95" class="ln lo iu bd lp lq nj ls lt lu nk lw lx ly nl ma mb mc nm me mf mg nn mi mj mk bi translated">6.NoneType '对象没有属性'<code class="fe mq mr ms mt b">_jvm'</code></h1><p id="3662" class="pw-post-body-paragraph kc kd iu ke b kf ml kh ki kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz in bi translated">出于各种原因，您可能会得到以下可怕的堆栈跟踪。</p><figure class="lc ld le lf gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="jw jx di jy bf jz"><div class="gi gj ow"><img src="../Images/e6d9664254cec63a909ae20cdee09f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-ISgPMdFJMUY9SjxmQ0uw.png"/></div></div></figure><p id="76f9" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">最常见的两种是:</p><ul class=""><li id="c439" class="ox oy iu ke b kf kg kj kk kn oz kr pa kv pb kz pc pd pe pf bi translated">您在没有激活 spark 会话的情况下使用 pyspark 函数</li></ul><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="78a8" class="my lo iu mt b gz mz na l nb nc"><strong class="mt iv">from </strong>pyspark.sql <strong class="mt iv">import </strong>SparkSession, functions <strong class="mt iv">as </strong>F</span><span id="f8c6" class="my lo iu mt b gz np na l nb nc">class A(object):<br/>    def __init__(self):<br/>        self.calculations = F.col('a') / F.col('b')</span><span id="835a" class="my lo iu mt b gz np na l nb nc">...</span><span id="f376" class="my lo iu mt b gz np na l nb nc">a = A() # instantiating A without an active spark session will give you this error </span></pre><ul class=""><li id="cab3" class="ox oy iu ke b kf kg kj kk kn oz kr pa kv pb kz pc pd pe pf bi translated">或者在 udf 中使用 pyspark 函数:</li></ul><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="35d3" class="my lo iu mt b gz mz na l nb nc"><strong class="mt iv">from </strong>pyspark <strong class="mt iv">import </strong>SparkConf<br/><strong class="mt iv">from </strong>pyspark.sql <strong class="mt iv">import </strong>SparkSession, functions <strong class="mt iv">as </strong>F, types <strong class="mt iv">as </strong>T<br/></span><span id="4362" class="my lo iu mt b gz np na l nb nc">conf = SparkConf()<br/>spark_session = SparkSession.builder \<br/>    .config(conf=conf) \<br/>    .appName(<strong class="mt iv">'test'</strong>) \<br/>    .getOrCreate()<br/><br/><em class="la"># create a dataframe<br/></em>data = [{<strong class="mt iv">'a'</strong>: 1, <strong class="mt iv">'b'</strong>: 0}, {<strong class="mt iv">'a'</strong>: 10, <strong class="mt iv">'b'</strong>: 3}]<br/>df = spark_session.createDataFrame(data)<br/>df.show()<br/><br/><em class="la"># +---+---+<br/># |  a|  b|<br/># +---+---+<br/># |  1|  0|<br/># | 10|  3|<br/># +---+---+<br/><br/><br/># define a simple function that returns a / b<br/># we *cannot* use pyspark functions inside a udf<br/># udfs operate on a row per row basis while pyspark functions on a column basis<br/></em><strong class="mt iv">def </strong>calculate_a_b_max(a, b):<br/>    <strong class="mt iv">return </strong>F.max([a, b])<br/><br/><em class="la"># and a udf for this function - notice the return datatype<br/></em>udf_max_calculation = F.udf(calculate_a_b_ratio, T.FloatType())<br/>df = df.withColumn(<strong class="mt iv">'a_b_max'</strong>, udf_max_calculation(<strong class="mt iv">'a'</strong>, <strong class="mt iv">'b'</strong>))<br/>df.show()</span></pre><p id="bba1" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">两者都会给你这个错误。</p><p id="a1f9" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">在最后一个例子中，<code class="fe mq mr ms mt b">F.max</code>需要一个<strong class="ke iv">列</strong>作为输入，而不是一个列表，所以正确的用法应该是:</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="f436" class="my lo iu mt b gz mz na l nb nc">df = df.withColumn(<strong class="mt iv">'a_max'</strong>, F.max(<strong class="mt iv">'a'</strong>))</span></pre><p id="3848" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">这将为我们提供列 <code class="fe mq mr ms mt b"><em class="la">a</em></code> <em class="la"> </em> — <strong class="ke iv">的最大值，而不是 udf 试图做的事情</strong>。</p><p id="b1d9" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">设置 udf 来计算每行两列之间的最大值的正确方法是:</p><pre class="lc ld le lf gu mu mt mv mw aw mx bi"><span id="0d9c" class="my lo iu mt b gz mz na l nb nc">def calculate_a_b_max(a, b):<br/>    return max([a, b]) </span></pre><p id="b019" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">假设<code class="fe mq mr ms mt b">a</code>和<code class="fe mq mr ms mt b">b</code>是数字。<br/>(当然没有 udf 也有其他的方法。)</p></div><div class="ab cl lg lh hy li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="in io ip iq ir"><p id="1d71" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">我希望这有所帮助。我计划继续这个列表，并及时处理更复杂的问题，比如调试 pyspark 应用程序中的内存泄漏。非常欢迎任何想法、问题、更正和建议:)</p><p id="767a" class="pw-post-body-paragraph kc kd iu ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz in bi translated">如果您想了解 Spark 的工作原理，请查看:</p><div class="oc od gq gs oe of"><a rel="noopener follow" target="_blank" href="/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9"><div class="og ab fp"><div class="oh ab oi cl cj oj"><h2 class="bd iv gz z fq ok fs ft ol fv fx it bi translated">用非技术性的方式解释技术性的东西——Apache Spark</h2><div class="om l"><h3 class="bd b gz z fq ok fs ft ol fv fx dk translated">什么是 Spark 和 PySpark，我可以用它做什么？</h3></div><div class="on l"><p class="bd b dl z fq ok fs ft ol fv fx dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="pg l oq or os oo ot ka of"/></div></div></a></div></div></div>    
</body>
</html>