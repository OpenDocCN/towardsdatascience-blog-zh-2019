<html>
<head>
<title>Build the right Autoencoder — Tune and Optimize using PCA principles. Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6?source=collection_archive---------3-----------------------#2019-07-12">https://towardsdatascience.com/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6?source=collection_archive---------3-----------------------#2019-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8b14" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b" rel="noopener">第一部分</a>的延续中，我们将在这里定义并实现自定义约束，以构建一个适定的自动编码器。一个适定的自动编码器是一个正则化的模型，它改善了测试重建误差。</h2></div><p id="bd4c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">&lt;<download the="" free="" book="" class="ae kf" href="https://www.understandingdeeplearning.com" rel="noopener ugc nofollow" target="_blank">了解深度学习，了解更多&gt; &gt;</download></p><p id="885f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b" rel="noopener"> <em class="lc">去看前传第一部</em> </a></p><p id="ad1a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b" rel="noopener">第一部分</a>中，我们了解到 PCA 和自动编码器在架构上有相似之处。但是尽管如此，自动编码器本身并不具有 PCA 属性，例如正交性。我们知道，结合 PCA 属性将为自动编码器带来显著的好处，例如解决消失和爆炸梯度，以及通过正则化进行过拟合。</p><p id="fa37" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">基于此，我们希望自动编码器继承的属性是:</p><ol class=""><li id="e786" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb li lj lk ll bi translated">捆绑重物，</li><li id="0ad4" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb li lj lk ll bi translated">正交权重，</li><li id="3d63" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb li lj lk ll bi translated">不相关的特征，以及</li><li id="f779" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb li lj lk ll bi translated">单位定额。</li></ol><p id="07f1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在本文中，我们将</p><ul class=""><li id="0728" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated">实现自定义层和约束来合并它们。</li><li id="fd6e" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">演示它们是如何工作的，以及它们带来的重构误差的改善。</li></ul><p id="d483" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这些实现将能够构造一个适定的自动编码器并对其进行优化。在我们的示例中，优化将重建误差提高了 50%以上。</p><blockquote class="ls lt lu"><p id="462e" class="kg kh lc ki b kj kk jr kl km kn ju ko lv kq kr ks lw ku kv kw lx ky kz la lb ij bi translated">注意:正则化技术，如<em class="iq">辍学</em>，是普遍使用的。但是没有一个适定的模型，这些方法需要更长的时间来优化。</p></blockquote><p id="d4ee" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">下一节详细展示了该实现。读者可以跳至<strong class="ki ir">关键要点</strong>部分进行简要总结。</p><h1 id="6a79" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">一个适配的自动编码器</h1><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/b75688d141da3d4b6255fee470612619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i0Bhdyf1ONlM03AlxZ3yA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 1. Apply constraints for a well-posed Autoencoder.</figcaption></figure><p id="a51a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们将为随机生成的数据集开发一个具有五个特征的自动编码器。我们将数据集分为训练集和测试集。当我们添加约束时，我们将使用测试数据重构误差来评估性能。</p><p id="1339" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">本文包含实现细节，以帮助从业者尝试各种选择。完整的代码显示在<a class="ae kf" href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h2 id="48ae" class="ng lz iq bd ma nh ni dn me nj nk dp mi kp nl nm mk kt nn no mm kx np nq mo nr bi translated">导入库</h2><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="5860" class="ng lz iq nt b gy nx ny l nz oa">from numpy.random import seed<br/>seed(123)<br/>from tensorflow import set_random_seed<br/>set_random_seed(234)</span><span id="8291" class="ng lz iq nt b gy ob ny l nz oa">import sklearn<br/>from sklearn import datasets<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler, MinMaxScaler<br/>from sklearn import decomposition<br/>import scipy</span><span id="8946" class="ng lz iq nt b gy ob ny l nz oa">import tensorflow as tf<br/>from keras.models import Model, load_model<br/>from keras.layers import Input, Dense, Layer, InputSpec<br/>from keras.callbacks import ModelCheckpoint, TensorBoard<br/>from keras import regularizers, activations, initializers, constraints, Sequential<br/>from keras import backend as K<br/>from keras.constraints import UnitNorm, Constraint</span></pre><h2 id="9a05" class="ng lz iq bd ma nh ni dn me nj nk dp mi kp nl nm mk kt nn no mm kx np nq mo nr bi translated">生成和准备数据</h2><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="d35e" class="ng lz iq nt b gy nx ny l nz oa">n_dim = 5<br/>cov = sklearn.datasets.make_spd_matrix(n_dim, random_state=None)mu = np.random.normal(0, 0.1, n_dim)</span><span id="0a8c" class="ng lz iq nt b gy ob ny l nz oa">n = 1000</span><span id="534e" class="ng lz iq nt b gy ob ny l nz oa">X = np.random.multivariate_normal(mu, cov, n)</span><span id="d84e" class="ng lz iq nt b gy ob ny l nz oa">X_train, X_test = train_test_split(X, test_size=0.5, random_state=123)</span><span id="9cbe" class="ng lz iq nt b gy ob ny l nz oa"># <em class="lc">Scale the data between 0 and 1.</em><br/>scaler = MinMaxScaler()<br/>scaler.fit(X_train)</span><span id="a044" class="ng lz iq nt b gy ob ny l nz oa">X_train_scaled = scaler.transform(X_train)</span><span id="ecce" class="ng lz iq nt b gy ob ny l nz oa">X_test_scaled = scaler.transform(X_test)</span><span id="4e4b" class="ng lz iq nt b gy ob ny l nz oa">X_train_scaled</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oc"><img src="../Images/498d970dbc0cc4819320850ec58b7564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xnrUwryRqAI3GdHEE_sP3A.png"/></div></div></figure><p id="8b9c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">估算参数</strong></p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="c333" class="ng lz iq nt b gy nx ny l nz oa">nb_epoch = 100<br/>batch_size = 16<br/>input_dim = X_train_scaled.shape[1] <em class="lc">#num of predictor variables, </em><br/>encoding_dim = 2<br/>learning_rate = 1e-3</span></pre><p id="7324" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">基线模型</strong></p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="3fa0" class="ng lz iq nt b gy nx ny l nz oa">encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias = <strong class="nt ir">True</strong>) <br/>decoder = Dense(input_dim, activation="linear", use_bias = <strong class="nt ir">True</strong>)<br/><br/>autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)<br/><br/>autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()<br/><br/>autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=<strong class="nt ir">True</strong>,<br/>                verbose=0)</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi od"><img src="../Images/476222587481ce45b91069c5f5adab2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WmMQlfQZZ5Ue39uQhQ-4uQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 2.1. Baseline Model Parameters.</figcaption></figure><p id="5732" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">基线重建误差</strong></p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="fdfd" class="ng lz iq nt b gy nx ny l nz oa">train_predictions = autoencoder.predict(X_train_scaled)<br/>print('Train reconstrunction error<strong class="nt ir">\n</strong>', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))<br/>test_predictions = autoencoder.predict(X_test_scaled)<br/>print('Test reconstrunction error<strong class="nt ir">\n</strong>', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oe"><img src="../Images/394605bf8b7667d7ad02ba111e56bbc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5av9_njxIFfVGXKerDU8A.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 2.2. Baseline Autoencoder Reconstruction Error.</figcaption></figure><h2 id="9077" class="ng lz iq bd ma nh ni dn me nj nk dp mi kp nl nm mk kt nn no mm kx np nq mo nr bi translated">自动编码器优化</h2><p id="5874" class="pw-post-body-paragraph kg kh iq ki b kj of jr kl km og ju ko kp oh kr ks kt oi kv kw kx oj kz la lb ij bi translated">Keras 提供了各种层和约束。对于<strong class="ki ir">单位定额，我们有一个可用的约束。</strong>对于其他人，我们将构建自定义层和约束。</p><ol class=""><li id="13ff" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb li lj lk ll bi translated"><strong class="ki ir">自定义图层:捆绑砝码。</strong></li></ol><p id="5695" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">有了这个自定义层，我们可以使编码器和解码器的权重相等。数学上，解码器权重的转置等于编码器权重(等式)。第一部分 7a)。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="4926" class="ng lz iq nt b gy nx ny l nz oa"><strong class="nt ir">class</strong> <strong class="nt ir">DenseTied</strong>(Layer):<br/>    <strong class="nt ir">def</strong> __init__(self, units,<br/>                 activation=<strong class="nt ir">None</strong>,<br/>                 use_bias=<strong class="nt ir">True</strong>,<br/>                 kernel_initializer='glorot_uniform',<br/>                 bias_initializer='zeros',<br/>                 kernel_regularizer=<strong class="nt ir">None</strong>,<br/>                 bias_regularizer=<strong class="nt ir">None</strong>,<br/>                 activity_regularizer=<strong class="nt ir">None</strong>,<br/>                 kernel_constraint=<strong class="nt ir">None</strong>,<br/>                 bias_constraint=<strong class="nt ir">None</strong>,<br/>                 tied_to=<strong class="nt ir">None</strong>,<br/>                 **kwargs):<br/>        self.tied_to = tied_to<br/>        <strong class="nt ir">if</strong> 'input_shape' <strong class="nt ir">not</strong> <strong class="nt ir">in</strong> kwargs <strong class="nt ir">and</strong> 'input_dim' <strong class="nt ir">in</strong> kwargs:<br/>            kwargs['input_shape'] = (kwargs.pop('input_dim'),)<br/>        super().__init__(**kwargs)<br/>        self.units = units<br/>        self.activation = activations.get(activation)<br/>        self.use_bias = use_bias<br/>        self.kernel_initializer = initializers.get(kernel_initializer)<br/>        self.bias_initializer = initializers.get(bias_initializer)<br/>        self.kernel_regularizer = regularizers.get(kernel_regularizer)<br/>        self.bias_regularizer = regularizers.get(bias_regularizer)<br/>        self.activity_regularizer = regularizers.get(activity_regularizer)<br/>        self.kernel_constraint = constraints.get(kernel_constraint)<br/>        self.bias_constraint = constraints.get(bias_constraint)<br/>        self.input_spec = InputSpec(min_ndim=2)<br/>        self.supports_masking = <strong class="nt ir">True</strong><br/>                <br/>    <strong class="nt ir">def</strong> build(self, input_shape):<br/>        <strong class="nt ir">assert</strong> len(input_shape) &gt;= 2<br/>        input_dim = input_shape[-1]<br/><br/>        <strong class="nt ir">if</strong> self.tied_to <strong class="nt ir">is</strong> <strong class="nt ir">not</strong> <strong class="nt ir">None</strong>:<br/>            self.kernel = K.transpose(self.tied_to.kernel)<br/>            self._non_trainable_weights.append(self.kernel)<br/>        <strong class="nt ir">else</strong>:<br/>            self.kernel = self.add_weight(shape=(input_dim, self.units),<br/>                                          initializer=self.kernel_initializer,<br/>                                          name='kernel',<br/>                                          regularizer=self.kernel_regularizer,<br/>                                          constraint=self.kernel_constraint)<br/>        <strong class="nt ir">if</strong> self.use_bias:<br/>            self.bias = self.add_weight(shape=(self.units,),<br/>                                        initializer=self.bias_initializer,<br/>                                        name='bias',<br/>                                        regularizer=self.bias_regularizer,<br/>                                        constraint=self.bias_constraint)<br/>        <strong class="nt ir">else</strong>:<br/>            self.bias = <strong class="nt ir">None</strong><br/>        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})<br/>        self.built = <strong class="nt ir">True</strong><br/><br/>    <strong class="nt ir">def</strong> compute_output_shape(self, input_shape):<br/>        <strong class="nt ir">assert</strong> input_shape <strong class="nt ir">and</strong> len(input_shape) &gt;= 2<br/>        output_shape = list(input_shape)<br/>        output_shape[-1] = self.units<br/>        <strong class="nt ir">return</strong> tuple(output_shape)<br/><br/>    <strong class="nt ir">def</strong> call(self, inputs):<br/>        output = K.dot(inputs, self.kernel)<br/>        <strong class="nt ir">if</strong> self.use_bias:<br/>            output = K.bias_add(output, self.bias, data_format='channels_last')<br/>        <strong class="nt ir">if</strong> self.activation <strong class="nt ir">is</strong> <strong class="nt ir">not</strong> <strong class="nt ir">None</strong>:<br/>            output = self.activation(output)<br/>        <strong class="nt ir">return</strong> output</span></pre><p id="3e5f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">带绑定解码器的自动编码器。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="a475" class="ng lz iq nt b gy nx ny l nz oa">encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias = True) <br/>decoder = DenseTied(input_dim, activation="linear", tied_to=encoder, use_bias = True)</span><span id="c6cf" class="ng lz iq nt b gy ob ny l nz oa">autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)</span><span id="46bd" class="ng lz iq nt b gy ob ny l nz oa">autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()</span><span id="9917" class="ng lz iq nt b gy ob ny l nz oa">autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=True,<br/>                verbose=0)</span></pre><p id="aa17" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">观察结果</strong></p><p id="f825" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">1a。同等重量。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="adea" class="ng lz iq nt b gy nx ny l nz oa">w_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[0]), 3)<br/>w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 3)<br/>print('Encoder weights<strong class="nt ir">\n</strong>', w_encoder)<br/>print('Decoder weights<strong class="nt ir">\n</strong>', w_decoder)</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ok"><img src="../Images/1f563a1c245737f1f3863d733c2aa067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BoxFUBvSALcycMNAcKDB7g.png"/></div></div></figure><p id="13ee" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">1b。偏见是不同的。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="dee5" class="ng lz iq nt b gy nx ny l nz oa">b_encoder = np.round(np.transpose(autoencoder.layers[0].get_weights()[1]), 3)<br/>b_decoder = np.round(np.transpose(autoencoder.layers[1].get_weights()[0]), 3)<br/>print('Encoder bias\n', b_encoder)<br/>print('Decoder bias\n', b_decoder)</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ol"><img src="../Images/7586733a5a850bd3297e418a272f36f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60kVSP4N49OWtfG3ebu8rw.png"/></div></div></figure><p id="1a43" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> 2。自定义约束:权重正交性。</strong></p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="9a04" class="ng lz iq nt b gy nx ny l nz oa"><strong class="nt ir">class</strong> <strong class="nt ir">WeightsOrthogonalityConstraint</strong> (Constraint):<br/>    <strong class="nt ir">def</strong> __init__(self, encoding_dim, weightage = 1.0, axis = 0):<br/>        self.encoding_dim = encoding_dim<br/>        self.weightage = weightage<br/>        self.axis = axis<br/>        <br/>    <strong class="nt ir">def</strong> weights_orthogonality(self, w):<br/>        <strong class="nt ir">if</strong>(self.axis==1):<br/>            w = K.transpose(w)<br/>        <strong class="nt ir">if</strong>(self.encoding_dim &gt; 1):<br/>            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)<br/>            <strong class="nt ir">return</strong> self.weightage * K.sqrt(K.sum(K.square(m)))<br/>        <strong class="nt ir">else</strong>:<br/>            m = K.sum(w ** 2) - 1.<br/>            <strong class="nt ir">return</strong> m<br/><br/>    <strong class="nt ir">def</strong> __call__(self, w):<br/>        <strong class="nt ir">return</strong> self.weights_orthogonality(w)</span></pre><p id="51bc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对编码器和解码器权重应用正交性。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="8082" class="ng lz iq nt b gy nx ny l nz oa">encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias=<strong class="nt ir">True</strong>, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)) <br/>decoder = Dense(input_dim, activation="linear", use_bias = <strong class="nt ir">True</strong>, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=1))<br/><br/>autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)<br/><br/>autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()<br/><br/>autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=<strong class="nt ir">True</strong>,<br/>                verbose=0)</span></pre><p id="d7b6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">观察。</strong></p><p id="74ea" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">2a。编码器和解码器的权重接近正交。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="42b5" class="ng lz iq nt b gy nx ny l nz oa">w_encoder = autoencoder.layers[0].get_weights()[0]<br/>print('Encoder weights dot product<strong class="nt ir">\n</strong>', np.round(np.dot(w_encoder.T, w_encoder), 2))<br/><br/>w_decoder = autoencoder.layers[1].get_weights()[0]<br/>print('Decoder weights dot product<strong class="nt ir">\n</strong>', np.round(np.dot(w_decoder, w_decoder.T), 2))</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ok"><img src="../Images/0c368c951ce2986ca9d3f668a9b233fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nHy6h9GixYoKY3enfEY4eA.png"/></div></div></figure><p id="08ab" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> 3。自定义约束:不相关的编码特征。</strong></p><p id="b102" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于不相关的特征，我们将对编码特征协方差的非对角元素的<em class="lc">和</em>施加惩罚。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="a5b9" class="ng lz iq nt b gy nx ny l nz oa"><strong class="nt ir">class</strong> <strong class="nt ir">UncorrelatedFeaturesConstraint</strong> (Constraint):<br/>    <br/>    <strong class="nt ir">def</strong> __init__(self, encoding_dim, weightage = 1.0):<br/>        self.encoding_dim = encoding_dim<br/>        self.weightage = weightage<br/>    <br/>    <strong class="nt ir">def</strong> get_covariance(self, x):<br/>        x_centered_list = []<br/><br/>        <strong class="nt ir">for</strong> i <strong class="nt ir">in</strong> range(self.encoding_dim):<br/>            x_centered_list.append(x[:, i] - K.mean(x[:, i]))<br/>        <br/>        x_centered = tf.stack(x_centered_list)<br/>        covariance = K.dot(x_centered, K.transpose(x_centered)) / tf.cast(x_centered.get_shape()[0], tf.float32)<br/>        <br/>        <strong class="nt ir">return</strong> covariance<br/>            <br/>    <em class="lc"># Constraint penalty</em><br/>    <strong class="nt ir">def</strong> uncorrelated_feature(self, x):<br/>        <strong class="nt ir">if</strong>(self.encoding_dim &lt;= 1):<br/>            <strong class="nt ir">return</strong> 0.0<br/>        <strong class="nt ir">else</strong>:<br/>            output = K.sum(K.square(<br/>                self.covariance - tf.math.multiply(self.covariance, K.eye(self.encoding_dim))))<br/>            <strong class="nt ir">return</strong> output<br/><br/>    <strong class="nt ir">def</strong> __call__(self, x):<br/>        self.covariance = self.get_covariance(x)<br/>        <strong class="nt ir">return</strong> self.weightage * self.uncorrelated_feature(x)</span></pre><p id="e0bc" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在自动编码器中应用约束。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="7adf" class="ng lz iq nt b gy nx ny l nz oa">encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias = <strong class="nt ir">True</strong>, activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage = 1.)) <br/>decoder = Dense(input_dim, activation="linear", use_bias = <strong class="nt ir">True</strong>)<br/><br/>autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)<br/><br/>autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()<br/><br/>autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=<strong class="nt ir">True</strong>,<br/>                verbose=0)</span></pre><p id="4a33" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">观察。</strong></p><p id="2de5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">3a。我们有较少相关的编码特征。施加这种惩罚更加困难。可以探索更强的约束函数。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="73a0" class="ng lz iq nt b gy nx ny l nz oa">encoder_layer = Model(inputs=autoencoder.inputs, outputs=autoencoder.layers[0].output)<br/>encoded_features = np.array(encoder_layer.predict(X_train_scaled))<br/>print('Encoded feature covariance\n', np.round(np.cov(encoded_features.T), 3))</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi om"><img src="../Images/dfe05de84dcd7b0942273b96710c625a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xFkXDkBsi79-nI516O45Q.png"/></div></div></figure><p id="cca2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> 4。约束:单位定额。</strong></p><p id="b2c8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><code class="fe on oo op nt b">UnitNorm</code>约束是在 Keras 中预先构建的。我们将在编码器和解码器层应用这种约束。值得注意的是，我们为编码器层保留了<code class="fe on oo op nt b">axis=0</code>，为解码器层保留了<code class="fe on oo op nt b">axis=1</code>。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="5882" class="ng lz iq nt b gy nx ny l nz oa">encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(<strong class="nt ir">axis=0</strong>)) <br/>decoder = Dense(input_dim, activation="linear", use_bias = True, kernel_constraint=UnitNorm(<strong class="nt ir">axis=1</strong>))</span><span id="d98a" class="ng lz iq nt b gy ob ny l nz oa">autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)</span><span id="7ee3" class="ng lz iq nt b gy ob ny l nz oa">autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()</span><span id="59e9" class="ng lz iq nt b gy ob ny l nz oa">autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=True,<br/>                verbose=0)</span></pre><p id="3af8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">观察。</strong></p><p id="76a7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">4.a .编码器和解码器沿编码轴的权重范数为，</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="c920" class="ng lz iq nt b gy nx ny l nz oa">w_encoder = np.round(autoencoder.layers[0].get_weights()[0], 2).T  # W in Figure 3.<br/>w_decoder = np.round(autoencoder.layers[1].get_weights()[0], 2)  # W' in Figure 3.</span><span id="f08d" class="ng lz iq nt b gy ob ny l nz oa">print('Encoder weights norm, \n', np.round(np.sum(w_encoder ** 2, axis = 1),3))<br/>print('Decoder weights norm, \n', np.round(np.sum(w_decoder ** 2, axis = 1),3))</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi oq"><img src="../Images/fcfbaea8e533ace12fa375855fae287a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nFNYIXuqKsbFcSpoyXkEEQ.png"/></div></div></figure><p id="b0bf" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如前所述，规范并不完全是 1.0，因为这不是一个硬约束。</p><h1 id="9890" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">把所有东西放在一起</h1><p id="af7a" class="pw-post-body-paragraph kg kh iq ki b kj of jr kl km og ju ko kp oh kr ks kt oi kv kw kx oj kz la lb ij bi translated">这里我们将把上述属性放在一起。根据问题的不同，这些属性的某种组合会比其他组合更好。</p><p id="314e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">同时应用几个约束有时会损害评估。例如，在此处使用的数据集中，结合联系层、权重正交性和单位范数效果最佳。</p><pre class="mr ms mt mu gt ns nt nu nv aw nw bi"><span id="c540" class="ng lz iq nt b gy nx ny l nz oa">encoder = Dense(encoding_dim, activation="linear", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0), kernel_constraint=UnitNorm(axis=0)) <br/>decoder = DenseTied(input_dim, activation="linear", tied_to=encoder, use_bias = False)</span><span id="bb2f" class="ng lz iq nt b gy ob ny l nz oa">autoencoder = Sequential()<br/>autoencoder.add(encoder)<br/>autoencoder.add(decoder)</span><span id="298b" class="ng lz iq nt b gy ob ny l nz oa">autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='sgd')<br/>autoencoder.summary()</span><span id="af05" class="ng lz iq nt b gy ob ny l nz oa">autoencoder.fit(X_train_scaled, X_train_scaled,<br/>                epochs=nb_epoch,<br/>                batch_size=batch_size,<br/>                shuffle=True,<br/>                verbose=0)</span><span id="310f" class="ng lz iq nt b gy ob ny l nz oa">train_predictions = autoencoder.predict(X_train_scaled)<br/>print('Train reconstrunction error\n', sklearn.metrics.mean_squared_error(X_train_scaled, train_predictions))<br/>test_predictions = autoencoder.predict(X_test_scaled)<br/>print('Test reconstrunction error\n', sklearn.metrics.mean_squared_error(X_test_scaled, test_predictions))</span></pre><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi or"><img src="../Images/89b2116b9dbcd55343deaf762ca93926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F9dxymx4cUdo3Khp3yMALA.png"/></div></div></figure><p id="357f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae kf" href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ki ir"> GitHub 库</strong> </a></p><p id="1827" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这里的完整地提到了模型调整的步骤和更多细节。</p><div class="os ot gp gr ou ov"><a href="https://github.com/cran2367/pca-autoencoder-relationship/blob/master/pca-autoencoder-relationship.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd ir gy z fp pa fr fs pb fu fw ip bi translated">cran 2367/PCA-自动编码器-关系</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">了解 PCA 和自动编码器之间的关系-cran 2367/PCA-自动编码器-关系</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">github.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj na ov"/></div></div></a></div><h1 id="4f84" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">关键要点</h1><h2 id="6e64" class="ng lz iq bd ma nh ni dn me nj nk dp mi kp nl nm mk kt nn no mm kx np nq mo nr bi translated"><strong class="ak">重建误差的改善</strong></h2><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi pk"><img src="../Images/5a5a6f478e6ae7f8416c1a92126651ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsdxHpK_LXbPfhdAyH1jDg.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Table 1. Summary of reconstruction errors.</figcaption></figure><ul class=""><li id="3406" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated">基线模型测试数据的重建误差为 0.027。</li><li id="05ee" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">将每个属性添加到 Autoencoder 减少了测试错误。改进范围从 19%的重量正交性到 67%的束缚重量。</li><li id="4188" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">不同的数据有不同的改进。</li><li id="e6c5" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">在我们的问题中，结合束缚权重、权重正交性和单位范数产生了具有最佳重构误差的最优模型。</li><li id="90e5" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">尽管最佳模型中的误差大于仅具有束缚权重的模型，但这更稳定，因此更可取。</li></ul><h2 id="7954" class="ng lz iq bd ma nh ni dn me nj nk dp mi kp nl nm mk kt nn no mm kx np nq mo nr bi translated"><strong class="ak">关键实施说明</strong></h2><p id="129d" class="pw-post-body-paragraph kg kh iq ki b kj of jr kl km og ju ko kp oh kr ks kt oi kv kw kx oj kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">绑重物</em> </strong></p><ul class=""><li id="ab9b" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated">在绑定权重层<code class="fe on oo op nt b">DenseTied</code>中，编码器和解码器中的偏差会有所不同。</li><li id="b8ae" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">要使所有重量完全相等，设置<code class="fe on oo op nt b">use_bias=False</code>。</li></ul><p id="2957" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">权重正交</em> </strong></p><ul class=""><li id="5837" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated"><code class="fe on oo op nt b">kernel_regularizer</code>用于在层的权重上添加约束或正则化。</li><li id="65c9" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">正交轴对于编码器应该是按行的，<code class="fe on oo op nt b">axis=0</code>，对于解码器应该是按列的，<code class="fe on oo op nt b">axis=1</code>。</li></ul><p id="3347" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">不相关的编码特征</em> </strong></p><ul class=""><li id="8f5e" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated"><code class="fe on oo op nt b">activity_regularizer</code>用于对层的输出特征应用约束。因此，这里使用它来将编码特征的非对角协方差约束为零。</li><li id="588f" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">这个约束不强。也就是说，它不会将非对角协方差元素推到非常接近零的程度。</li><li id="29f3" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">可以探索该约束的另一种定制。</li></ul><p id="9b17" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">单位定额</em> </strong></p><ul class=""><li id="6b53" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated"><code class="fe on oo op nt b">UnitNorm</code>编码器和解码器应该在不同的轴上。</li><li id="3a16" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">类似于权重正交性，这适用于编码器的行<code class="fe on oo op nt b">axis=0</code>和解码器的列<code class="fe on oo op nt b">axis=1</code>。</li></ul><p id="7ffb" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> <em class="lc">通用</em> </strong></p><ul class=""><li id="cb2c" class="ld le iq ki b kj kk km kn kp lf kt lg kx lh lb lr lj lk ll bi translated">有两个类，<code class="fe on oo op nt b">Regularizer</code>和<code class="fe on oo op nt b">Constraints</code>用于构建定制函数。实际上，对于我们的应用程序来说，两者是一样的。对于权重正交性和不相关特征，我们使用了<code class="fe on oo op nt b">Constraints</code>类。</li><li id="efc5" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">所有三个约束——单位范数、不相关编码特征和权重正交性——都是软约束。也就是说，它们可以使模型权重和特征接近所需属性，但并不精确。例如，权重几乎是单位范数，而不是精确的。</li></ul><h2 id="6673" class="ng lz iq bd ma nh ni dn me nj nk dp mi kp nl nm mk kt nn no mm kx np nq mo nr bi translated"><strong class="ak">在实践中</strong>，</h2><ul class=""><li id="c4be" class="ld le iq ki b kj of km og kp pl kt pm kx pn lb lr lj lk ll bi translated">合并这些属性的性能将因问题而异。</li><li id="aeaa" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">在不同的设置下，例如有和没有偏差，以及不同层上正交性和不相关特征约束的不同权重下，分别探索每个属性。</li><li id="3cc0" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">除此之外，在自动编码器中包括流行的正则化技术，例如<strong class="ki ir">丢弃</strong>层。</li></ul><h1 id="0d89" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">摘要</h1><ul class=""><li id="1109" class="ld le iq ki b kj of km og kp pl kt pm kx pn lb lr lj lk ll bi translated">在前传<a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b?sk=9c99493a4daf74807a84fd5c5290e0dc" rel="noopener">第一部分</a>中，我们学习了自动编码器应该从 PCA 继承的重要属性。</li><li id="7150" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">这里，我们实现了自定义层和约束来合并这些属性。</li><li id="02a2" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">在表 1 中，我们展示了这些特性显著改善了测试重构误差。</li><li id="14a2" class="ld le iq ki b kj lm km ln kp lo kt lp kx lq lb lr lj lk ll bi translated">正如在<strong class="ki ir">关键要点</strong>中提到的，我们需要反复试验来找到最佳设置。然而，这些试验是在一个具有解释意义的方向上进行的。</li></ul><p id="2952" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><a class="ae kf" href="https://medium.com/@cran2367/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b?sk=9c99493a4daf74807a84fd5c5290e0dc" rel="noopener"> <em class="lc">转到前传第一部</em> </a></p></div><div class="ab cl po pp hu pq" role="separator"><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt pu"/><span class="pr bw bk ps pt"/></div><div class="ij ik il im in"><h1 id="3bd8" class="ly lz iq bd ma mb pv md me mf pw mh mi jw px jx mk jz py ka mm kc pz kd mo mp bi translated">参考</h1><ol class=""><li id="26ac" class="ld le iq ki b kj of km og kp pl kt pm kx pn lb li lj lk ll bi translated"><a class="ae kf" href="https://stackoverflow.com/questions/53751024/tying-autoencoder-weights-in-a-dense-keras-layer" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/53751024/tieng-auto encoder-weights-in-a-dense-keras-layer</a></li></ol></div></div>    
</body>
</html>