<html>
<head>
<title>I trained a network to speak like me</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我训练一个网络像我一样说话</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/i-trained-a-network-to-speak-like-me-9552c16e2396?source=collection_archive---------19-----------------------#2019-09-03">https://towardsdatascience.com/i-trained-a-network-to-speak-like-me-9552c16e2396?source=collection_archive---------19-----------------------#2019-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4cdb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/inside-ai/home" rel="noopener">内部 AI </a></h2><div class=""/><div class=""><h2 id="78f0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">而结局真的很搞笑。语言生成指南。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/51e97e5ef966b0f8fecd01571b84ac3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QdjI4iG51HHZVwl47Cg9hw.png"/></div></div></figure><p id="cac4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi lz translated">在过去的几个月里，我在我的个人博客上写了 100 多篇文章:【https://maelfabien.github.io/】<span class="l ma mb mc bm md me mf mg mh di">。这是相当可观的内容。我想到了一个主意:</span></p><blockquote class="mj"><p id="159c" class="mk ml it bd mm mn mo mp mq mr ms ly dk translated">🚀训练一个语言生成模型<strong class="ak">像我一样说话</strong>。🚀</p></blockquote><p id="3f09" class="pw-post-body-paragraph ld le it lf b lg mt kd li lj mu kg ll lm mv lo lp lq mw ls lt lu mx lw lx ly im bi translated">或者更确切地说，要像我一样把<strong class="lf jd">写成</strong>。这是说明语言生成的主要概念、使用 Keras 的实现以及我的模型的局限性的最佳方式。</p><p id="ba1b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本文的全部代码可以在这个资源库中找到:</p><div class="my mz gp gr na nb"><a href="https://github.com/maelfabien/Machine_Learning_Tutorials" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd jd gy z fp ng fr fs nh fu fw jc bi translated">mael fabien/机器学习教程</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">在这个库中，我上传了代码、笔记本和来自我的个人博客 https://maelfabien.github.io/…的文章</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">github.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np lb nb"/></div></div></a></div><p id="cb8b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我们开始之前，我发现这个<a class="ae mi" href="https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>是理解语言生成算法结构的有用资源。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="9d32" class="nx ny it bd nz oa ob oc od oe of og oh ki oi kj oj kl ok km ol ko om kp on oo bi translated">语言生成</h1><blockquote class="op oq or"><p id="caeb" class="ld le os lf b lg lh kd li lj lk kg ll ot ln lo lp ou lr ls lt ov lv lw lx ly im bi translated">自然语言生成是一个旨在生成有意义的自然语言的领域。</p></blockquote><p id="26a9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">大多数情况下，内容是作为单个单词的序列生成的。对于这个伟大的想法，它是这样工作的:</p><ul class=""><li id="649f" class="ow ox it lf b lg lh lj lk lm oy lq oz lu pa ly pb pc pd pe bi translated">您训练一个模型来预测序列中的下一个单词</li><li id="b49c" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">你给训练好的模型一个输入</li><li id="1aa6" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">并迭代 N 次，从而生成接下来的 N 个单词</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/15eb4aa39821b26b6a8a954f7c82d8d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYbj4I4HYhISuBsrPi2QGg.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">Sequential Prediction Process</figcaption></figure><h2 id="0fa3" class="pp ny it bd nz pq pr dn od ps pt dp oh lm pu pv oj lq pw px ol lu py pz on iz bi translated">1.数据集创建</h2><p id="449b" class="pw-post-body-paragraph ld le it lf b lg qa kd li lj qb kg ll lm qc lo lp lq qd ls lt lu qe lw lx ly im bi translated">第一步是建立一个数据集，它可以被我们稍后将要建立的网络所理解。首先导入以下包:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><p id="bf64" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> a .加载数据</strong></p><p id="3953" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我写的每一篇文章的标题都遵循这个模板:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/45b96da5443f19b88878af1bdf42e2db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*U4IEda8Upeqv3oxfUqKdjw.png"/></div></figure><p id="bd29" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是我们通常不希望在最终数据集中出现的内容类型。我们将把重点放在文本本身。</p><p id="a732" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所有文章都写在一个单独的降价文件中。报头基本上携带关于标题、图片报头等的信息。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><p id="4391" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们需要指向包含文章的文件夹，在我的名为“maelfabien.github.io”的目录中。</p><p id="0558" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> b .句子标记化</strong></p><p id="312b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，打开每篇文章，并将每篇文章的内容追加到列表中。然而，由于我们的目的是生成句子而不是整篇文章，我们将把每篇文章分成一个句子列表，并将每个句子附加到列表“all_sentences”中:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="e8bf" class="pp ny it qj b gy qn qo l qp qq">all_sentences= []<br/><br/>for file in glob.glob("*.md"):<br/>    f = open(file,'r')<br/>    txt = f.read().replace("\n", " ")<br/>    try: <br/>        sent_text = nltk.sent_tokenize(''.join(txt.split("---")[2]).strip())<br/>        for k in sent_text :<br/>            all_sentences.append(k)<br/>    except : <br/>        pass</span></pre><p id="cbd0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">总的来说，我们有 6800 多个训练句子。到目前为止的过程如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qr"><img src="../Images/a7ef6376e8878fe6730a9fb64159008f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STVpAKTc3Se_0Aljd461iQ.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">Sentence split</figcaption></figure><p id="0b55" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> c. N-gram 创建</strong></p><p id="b624" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，想法是创建 N 个一起出现的单词。为此，我们需要:</p><ul class=""><li id="ab42" class="ow ox it lf b lg lh lj lk lm oy lq oz lu pa ly pb pc pd pe bi translated">在语料库上安装一个标记器，将一个索引与每个标记相关联</li><li id="8153" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">将语料库中的每个句子分解为一系列标记</li><li id="9b80" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">存储一起出现的令牌序列</li></ul><p id="0b84" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">可以用下面的方式来说明:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qs"><img src="../Images/d0b104a4f4e08b903e752039a96f8559.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xX3UjpURIGFhyMbPBeM84w.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">N-gram creation</figcaption></figure><p id="cd9f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">让我们实现这一点。我们首先需要安装标记器:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="d138" class="pp ny it qj b gy qn qo l qp qq">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(all_sentences)<br/>total_words = len(tokenizer.word_index) + 1</span></pre><p id="6fdb" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">变量‘total _ words’包含已经使用的不同单词的总数。这里，8976。然后，对于每个句子，获取相应的标记并生成 N 元语法:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><p id="88c8" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">变量“token_list”包含一系列标记形式的句子:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="5ecc" class="pp ny it qj b gy qn qo l qp qq">[656, 6, 3, 2284, 6, 3, 86, 1283, 640, 1193, 319]<br/>[33, 6, 3345, 1007, 7, 388, 5, 2128, 1194, 62, 2731]<br/>[7, 17, 152, 97, 1165, 1, 762, 1095, 1343, 4, 656]</span></pre><p id="3d76" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后,“n 元语法序列”创建 n 元语法。开头是前两个词，然后逐渐加词:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="c345" class="pp ny it qj b gy qn qo l qp qq">[656, 6]<br/>[656, 6, 3]<br/>[656, 6, 3, 2284]<br/>[656, 6, 3, 2284, 6]<br/>[656, 6, 3, 2284, 6, 3]<br/>...</span></pre><p id="ab55" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> d .填充</strong></p><p id="f76e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在面临以下问题:不是所有的序列都有相同的长度！我们如何解决这个问题？</p><p id="1da5" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们将使用衬垫。Paddings 在变量“input_sequences”的每一行之前添加 0 的序列，以便每一行与最长的一行具有相同的长度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/9abefe6ccac8f43d1570c7fd7b953a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AquKaNiUaTxha2V140xuzQ.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">Illustrating Padding</figcaption></figure><p id="b54e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了将所有句子填充到句子的最大长度，我们必须首先找到最长的句子:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="24e5" class="pp ny it qj b gy qn qo l qp qq">max_sequence_len = max([len(x) for x in input_sequences])</span></pre><p id="d3a4" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在我的例子中，它等于 792。对一个句子来说，这看起来太大了！由于我的博客包含一些代码和教程，我希望这句话实际上是用 Python 代码写的。让我们画出序列长度的直方图:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qf qg l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qu"><img src="../Images/6ae18ff1c667f44eb89a9adc38f2d6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QZetSUDpw3D1sMXFp2gkMQ.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">Length of sequences</figcaption></figure><p id="91b9" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">单个序列 200 +字的例子确实很少。将最大序列长度设置为 200 怎么样？</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="b5ab" class="pp ny it qj b gy qn qo l qp qq">max_sequence_len = 200<br/>input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))</span></pre><p id="856c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">它会返回如下内容:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="2ade" class="pp ny it qj b gy qn qo l qp qq">array([[   0,    0,    0, ...,    0,  656,    6],<br/>       [   0,    0,    0, ...,  656,    6,    3],<br/>       [   0,    0,    0, ...,    6,    3, 2284],<br/>       ...,</span></pre><p id="7e7f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> e .分割 X 和 y </strong></p><p id="51cd" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在有了固定长度的数组，它们中的大多数在实际序列之前都填充了 0。好吧，我们怎么把它变成训练集？我们需要分开 X 和 y！请记住，我们的目标是预测序列中的下一个单词。因此，我们必须把除了最后一个外的所有记号作为我们的 X，把最后一个作为我们的 y。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qv"><img src="../Images/7fbcd52a43dab40677568ba802091000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*snO0T9wL3cI3tg80RD8X5A.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">Split X and y</figcaption></figure><p id="be39" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在 Python 中，就这么简单:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="168c" class="pp ny it qj b gy qn qo l qp qq">X, y = input_sequences[:,:-1],input_sequences[:,-1]</span></pre><p id="362b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在将把这个问题看作一个多类分类任务。像往常一样，我们必须首先对<strong class="lf jd"> y </strong>进行一次热编码，以获得一个稀疏矩阵，该矩阵在对应于令牌的列中包含 1，在其他地方包含 0:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qr"><img src="../Images/2f7cd2c9f83fc110978f4313004015e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pJEutlEnXsy0-lrz4_SH_Q.png"/></div></div></figure><p id="663b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在 Python 中，使用 Keras Utils `to _ categorical `:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="f445" class="pp ny it qj b gy qn qo l qp qq">y = ku.to_categorical(y, num_classes=total_words)</span></pre><p id="bb84" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">x 现在的形状是(164496，199)，y 的形状是(164496，8976)。</p><p id="7555" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们有大约 165，000 个训练样本。x 是 199 列宽，因为它对应于我们允许的最长序列(200 - 1，要预测的标签)。y 有 8976 列，对应于所有词汇单词的稀疏矩阵。数据集现在准备好了！</p><h2 id="df98" class="pp ny it bd nz pq pr dn od ps pt dp oh lm pu pv oj lq pw px ol lu py pz on iz bi translated">2.建立模型</h2><p id="5d40" class="pw-post-body-paragraph ld le it lf b lg qa kd li lj qb kg ll lm qc lo lp lq qd ls lt lu qe lw lx ly im bi translated">我们将使用长短期记忆网络(LSTM)。LSTM 的重要优势是能够理解整个序列的相关性，因此，句子的开头可能会对第 15 个单词产生影响。另一方面，递归神经网络(RNNs)只意味着依赖于网络的前一状态，并且只有前一个词有助于预测下一个词。如果我们选择 RNNs，我们会很快错过上下文，因此，LSTMs 似乎是正确的选择。</p><p id="ff88" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> a .模型架构</strong></p><p id="c02d" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于训练可能非常(非常)(非常)(非常)(非常)(非常)(不开玩笑)长，我们将建立一个简单的 1 嵌入+ 1 LSTM 层+ 1 密集网络:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="7ab9" class="pp ny it qj b gy qn qo l qp qq">def create_model(max_sequence_len, total_words):<br/><br/>    input_len = max_sequence_len - 1<br/><br/>    model = Sequential()<br/>    <br/>    # Add Input Embedding Layer<br/>    model.add(Embedding(total_words, 10, input_length=input_len))<br/>    <br/>    # Add Hidden Layer 1 - LSTM Layer<br/>    model.add(LSTM(100))<br/>    model.add(Dropout(0.1))<br/>    <br/>    # Add Output Layer<br/>    model.add(Dense(total_words, activation='softmax'))<br/><br/>    model.compile(loss='categorical_crossentropy', optimizer='adam')<br/>    <br/>    return model<br/><br/>model = create_model(max_sequence_len, total_words)<br/>model.summary()</span></pre><p id="dcd0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">首先，我们添加一个嵌入层。我们将它传递到一个有 100 个神经元的 LSTM 中，添加一个 dropout 来控制神经元的共同适应，最后得到一个致密层。请注意，我们在最后一层应用了 softmax 激活函数，以获得输出属于每个类的概率。使用的损失是分类交叉熵，因为它是一个多类分类问题。</p><p id="44de" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该模式的总结是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qw"><img src="../Images/865a4d4c9d832d439f106ac0ae7aad4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wP8TQHhf7zAgU2kku-vx6Q.png"/></div></div><figcaption class="pl pm gj gh gi pn po bd b be z dk">Model Summary</figcaption></figure><p id="08e1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd"> b .训练模型</strong></p><p id="4879" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们现在(终于)准备好训练模型了！</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="84ac" class="pp ny it qj b gy qn qo l qp qq">model.fit(X, y, batch_size=256, epochs=100, verbose=True)</span></pre><p id="83ff" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后模型的训练将开始:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="a18e" class="pp ny it qj b gy qn qo l qp qq">Epoch 1/10<br/>164496/164496 [==============================] - 471s 3ms/step - loss: 7.0687<br/>Epoch 2/10<br/>73216/164496 [============&gt;.................] - ETA: 5:12 - loss: 7.0513</span></pre><p id="2176" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在 CPU 上，一个历元大约需要 8 分钟。在 GPU 上(例如在 Colab 中)，您应该修改所使用的 Keras LSTM 网络，因为它不能在 GPU 上使用。您可能需要这个:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="5b1b" class="pp ny it qj b gy qn qo l qp qq"># Modify Import<br/>from keras.layers import Embedding, LSTM, Dense, Dropout, CuDNNLSTM<br/><br/># In the Moddel<br/>...<br/>    model.add(CuDNNLSTM(100))<br/>...</span></pre><p id="340b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我倾向于在几个步骤停止训练，以进行样本预测，并在给定几个交叉熵值的情况下控制模型的质量。</p><p id="16a6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">以下是我的观察:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ca"><img src="../Images/9c690832961b85e01bd72e5b9d0f1a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2uGcVy5tNGCgD8Pf-vU1w.png"/></div></div></figure><h2 id="0119" class="pp ny it bd nz pq pr dn od ps pt dp oh lm pu pv oj lq pw px ol lu py pz on iz bi translated">3.生成序列</h2><p id="ef97" class="pw-post-body-paragraph ld le it lf b lg qa kd li lj qb kg ll lm qc lo lp lq qd ls lt lu qe lw lx ly im bi translated">如果你已经读到这里，这就是你所期待的:<strong class="lf jd">生成新句子！</strong>为了生成句子，我们需要对输入文本应用相同的转换。我们将构建一个循环，针对给定的迭代次数生成下一个单词:</p><pre class="ks kt ku kv gt qi qj qk ql aw qm bi"><span id="003d" class="pp ny it qj b gy qn qo l qp qq">input_txt = "Machine"<br/><br/>for _ in range(10):<br/>    <br/>    # Get tokens<br/>    token_list = tokenizer.texts_to_sequences([input_txt])[0]</span><span id="a534" class="pp ny it qj b gy qx qo l qp qq">    # Pad the sequence<br/>    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')</span><span id="7cff" class="pp ny it qj b gy qx qo l qp qq">    # Predict the class<br/>    predicted = model.predict_classes(token_list, verbose=0)<br/>    <br/>    output_word = ""<br/>    <br/>    # Get the corresponding work<br/>    for word,index in tokenizer.word_index.items():<br/>        if index == predicted:<br/>            output_word = word<br/>            break<br/>            <br/>    input_txt += " "+output_word</span></pre><p id="81b7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">当损失在 3.1 左右时，下面是它使用“Google”作为输入生成的句子:</p><blockquote class="op oq or"><p id="5eb4" class="ld le os lf b lg lh kd li lj lk kg ll ot ln lo lp ou lr ls lt ov lv lw lx ly im bi translated">谷歌是世界范围内产生的大量数据</p></blockquote><p id="bfac" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">它实际上没有任何意义，但它成功地将谷歌与大量数据的概念联系起来。这令人印象深刻，因为它仅仅依赖于单词的共现，而没有整合任何语法概念。</p><p id="3892" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">🚀如果我们在训练中多等一会儿，让损失减少到 2.5，并给它输入“随机森林”:</p><blockquote class="op oq or"><p id="56da" class="ld le os lf b lg lh kd li lj lk kg ll ot ln lo lp ou lr ls lt ov lv lw lx ly im bi translated">Random Forest 是一个完全托管的分布式服务，旨在支持大量初创公司的愿景基础架构</p></blockquote><p id="9ede" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">同样，生成的内容没有意义，但语法结构相当正确。</p><p id="5097" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">该损失在大约 50 个时期后偏离，并且从未低于 2.5。</p><p id="0d53" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我认为我们在这里达到了开发方法的极限:</p><ul class=""><li id="508a" class="ow ox it lf b lg lh lj lk lm oy lq oz lu pa ly pb pc pd pe bi translated">这个模型仍然非常简单</li><li id="43fa" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">训练数据不像它应该的那样干净</li><li id="2ed4" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">并且数据量非常有限</li></ul><p id="f53c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">也就是说，我发现结果非常有趣，例如，经过训练的模型可以很容易地部署在 Flask WebApp 上。</p><h1 id="082e" class="nx ny it bd nz oa qy oc od oe qz og oh ki ra kj oj kl rb km ol ko rc kp on oo bi translated">结论</h1><p id="a23e" class="pw-post-body-paragraph ld le it lf b lg qa kd li lj qb kg ll lm qc lo lp lq qd ls lt lu qe lw lx ly im bi translated">我希望这篇文章有用。我试图说明语言生成的主要概念、挑战和限制。与我们在本文中讨论的方法相比，更大的网络和更好的数据无疑是改进的来源。如果您有任何问题或意见，请留下您的评论:)</p><p id="0666" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">来源:</strong></p><ul class=""><li id="e515" class="ow ox it lf b lg lh lj lk lm oy lq oz lu pa ly pb pc pd pe bi translated">Kaggle 内核:<a class="ae mi" href="https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Shiva MB/初学者指南-文本生成-使用-lstms </a></li><li id="379c" class="ow ox it lf b lg pf lj pg lm ph lq pi lu pj ly pb pc pd pe bi translated">最初发布于此:<a class="ae mi" href="https://maelfabien.github.io/project/NLP_Gen/#generating-sequences" rel="noopener ugc nofollow" target="_blank">https://mael fabien . github . io/project/NLP _ Gen/# generating-sequences</a></li></ul></div></div>    
</body>
</html>