<html>
<head>
<title>Understanding dimensions in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解 PyTorch 中的维度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be?source=collection_archive---------5-----------------------#2019-07-11">https://towardsdatascience.com/understanding-dimensions-in-pytorch-6edf9972d3be?source=collection_archive---------5-----------------------#2019-07-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="760b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过对 3D 张量求和过程的可视化，对 PyTorch 维数有了更好的直觉</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0d9ba907017f3365daa04b81c5320bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*g8PjYqOligrJoIVE"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@crissyjarvis?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Crissy Jarvis</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="43e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我开始用 PyTorch 张量做一些基本运算时，比如求和，对于一维张量来说，它看起来很容易，也很简单:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="2beb" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; x <strong class="lt ir">=</strong> torch.tensor([1, 2, 3])</span><span id="fff7" class="lx ly iq lt b gy md ma l mb mc">&gt;&gt; torch.sum(x)</span><span id="ee5d" class="lx ly iq lt b gy md ma l mb mc">tensor(6)</span></pre><p id="78a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，一旦我开始摆弄 2D 和 3D 张量，并对行和列求和，我就对<code class="fe me mf mg lt b">torch.sum</code>的第二个参数<code class="fe me mf mg lt b">dim</code>感到困惑。</p><p id="8f99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从官方文件上说的开始:</p><blockquote class="mh mi mj"><p id="df3a" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated"><strong class="ky ir"> torch.sum(input，dim，keepdim=False，dtype=None) → Tensor </strong></p><p id="9b68" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated">返回给定维数 dim 中每一行输入张量的总和。</p></blockquote><p id="2b79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我不太理解这种解释。我们可以对多列求和，那么为什么有人提到它只是“返回每行的总和”呢？这是我第一次不理解。</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><p id="1cf8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，正如我所说，更重要的问题是每个维度的方向。我的意思是。当我们描述 2D 张量的形状时，我们说它包含一些<strong class="ky ir">行</strong>和一些<strong class="ky ir">列。</strong>所以对于一个<em class="mk"> 2x3 </em>张量，我们有 2 行 3 列:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="4cce" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; x = torch.tensor([<br/>     [1, 2, 3],<br/>     [4, 5, 6]<br/>   ])</span><span id="82d9" class="lx ly iq lt b gy md ma l mb mc">&gt;&gt; x.shape</span><span id="1146" class="lx ly iq lt b gy md ma l mb mc">torch.Size([2, 3])</span></pre><p id="36b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们首先指定行(2 行)，然后指定列(3 列)，对吗？这使我得出结论，第一维(<em class="mk"> dim=0 </em>)适用于行，第二维(<em class="mk"> dim=1 </em>)适用于列。按照维度<em class="mk"> dim=0 </em>表示行方式的推理，我期望<code class="fe me mf mg lt b"><em class="mk">torch.sum(x, dim=0)</em></code> <em class="mk"> </em>产生一个<strong class="ky ir"> <em class="mk"> 1x2 </em> </strong>张量(<code class="fe me mf mg lt b">1 + 2 + 3</code>和<code class="fe me mf mg lt b">4 + 5 + 6</code>产生一个<code class="fe me mf mg lt b">tensor[6, 15]</code>)。但结果我得到了不同的东西:一个<strong class="ky ir"> <em class="mk"> 1x3 </em> </strong>张量。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="cdbb" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; torch.sum(x, dim=0)</span><span id="04a4" class="lx ly iq lt b gy md ma l mb mc">tensor([5, 7, 9])</span></pre><p id="890b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我惊讶地看到现实与我预期的相反，因为我最终得到了结果<code class="fe me mf mg lt b">tensor[6, 15]</code>，但是在传递参数<em class="mk"> dim=1 </em>时:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="c8d8" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; torch.sum(x, dim=1)</span><span id="5c42" class="lx ly iq lt b gy md ma l mb mc">tensor([6, 15])</span></pre><p id="3508" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为什么会这样呢？我找到了<a class="ae kv" href="https://medium.com/@aerinykim/numpy-sum-axis-intuition-6eb94926a5d1" rel="noopener">一篇</a>关于<a class="mv mw ep" href="https://medium.com/u/1d8994ad0efc?source=post_page-----6edf9972d3be--------------------------------" rel="noopener" target="_blank"> Aerin Kim 的文章🙏</a>解决同样的困惑，但是对于 NumPy 矩阵，我们传递第二个参数，称为<em class="mk">轴</em>。除了 PyTorch 中的<em class="mk"> dim </em>在 NumPy 中被称为<em class="mk">轴</em>之外，NumPy sum 与 PyTorch 中的几乎相同:</p><blockquote class="mh mi mj"><p id="6879" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated">numpy.sum(a，axis=None，dtype=None，out=None，keepdims=False)</p></blockquote><p id="f002" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理解 PyTorch 中的<em class="mk"> dim </em>和 NumPy 中的<em class="mk">轴</em>如何工作的关键是 Aerin 文章中的这段话:</p><blockquote class="mh mi mj"><p id="1af9" class="kw kx mk ky b kz la jr lb lc ld ju le ml lg lh li mm lk ll lm mn lo lp lq lr ij bi translated">理解 numpy sum 的<strong class="ky ir">轴</strong>的方法是:它<strong class="ky ir"> <em class="iq">折叠</em> </strong>指定的轴。因此，当它折叠轴 0(行)时，它就变成了一行(按列求和)。</p></blockquote><p id="084d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">她很好地解释了<em class="mk">轴</em>参数在<em class="mk"> numpy.sum. </em>上的作用，然而，当我们引入第三维时，它变得更加棘手。当我们观察一个 3D 张量的形状时，我们会注意到新的维度被预先考虑并占据了第一个位置(在下面的<strong class="ky ir">粗体</strong>中)，即第三维度变成了<code class="fe me mf mg lt b">dim=0</code>。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="1e61" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; y = torch.tensor([<br/>     [<br/>       [1, 2, 3],<br/>       [4, 5, 6]<br/>     ],<br/>     [<br/>       [1, 2, 3],<br/>       [4, 5, 6]<br/>     ],<br/>     [<br/>       [1, 2, 3],<br/>       [4, 5, 6]<br/>     ]<br/>   ])</span><span id="9ffe" class="lx ly iq lt b gy md ma l mb mc">&gt;&gt; y.shape</span><span id="8f84" class="lx ly iq lt b gy md ma l mb mc">torch.Size([<strong class="lt ir">3</strong>, 2, 3])</span></pre><p id="9e84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是的，相当混乱。这就是为什么我认为对不同维度的求和过程进行一些基本的可视化会大大有助于更好的理解。</p><p id="ed06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个 3D 张量的第一维(<em class="mk"> dim=0 </em>)是最高的一维，包含 3 个二维张量。因此，为了对它进行总结，我们必须将它的 3 个元素一个接一个地折叠起来:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="81eb" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; torch.sum(y, dim=0)</span><span id="a2e6" class="lx ly iq lt b gy md ma l mb mc">tensor([[ 3,  6,  9],<br/>        [12, 15, 18]])</span></pre><p id="25a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它是这样工作的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/97a32642c33330388a52981ee1cc62e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/1*XvGIb1XxF6r9FYHQ3yYPTw.gif"/></div></figure><p id="593d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于第二维度(<em class="mk"> dim=1 </em>)，我们必须折叠行:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0931" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; torch.sum(y, dim=1)</span><span id="a9a1" class="lx ly iq lt b gy md ma l mb mc">tensor([[5, 7, 9],<br/>        [5, 7, 9],<br/>        [5, 7, 9]])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/1c85200728714c091fa610f5c63d25ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/1*e608vZqp7jNIdsarKbeunQ.gif"/></div></figure><p id="8e1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，第三维度在列上折叠:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="ef13" class="lx ly iq lt b gy lz ma l mb mc">&gt;&gt; torch.sum(y, dim=2)</span><span id="ecf4" class="lx ly iq lt b gy md ma l mb mc">tensor([[ 6, 15],<br/>        [ 6, 15],<br/>        [ 6, 15]])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e12e998fe203a59020a6bf8a95b3c1e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/1*pf9K7VX2M6_1p39JUmOkLQ.gif"/></div></figure><p id="3bf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你像我一样，最近开始学习 PyTorch 或 NumPy，我希望这些基本的动画示例能帮助你更好地理解维度是如何工作的，不仅是对于<em class="mk"> sum </em>也是对于其他方法。</p><p id="c9be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢阅读！</p><p id="0201" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考资料:</p><p id="91aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] A .金，<a class="ae kv" href="https://medium.com/@aerinykim/numpy-sum-axis-intuition-6eb94926a5d1" rel="noopener"> Numpy 和轴直觉</a></p></div></div>    
</body>
</html>