<html>
<head>
<title>How Are Convolutions Actually Performed Under the Hood?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积实际上是如何进行的？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf?source=collection_archive---------5-----------------------#2019-12-13">https://towardsdatascience.com/how-are-convolutions-actually-performed-under-the-hood-226523ce7fbf?source=collection_archive---------5-----------------------#2019-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0d07" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PyTorch &amp; TensorFlow 用来加速卷积的两个简单技巧。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/806cfef459256dd1385c3a74a70cf3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lum5Mp8oMmsmT_5XFvI4qQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@thekidnamedhosea?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Hosea Georgeson</a> on <a class="ae ky" href="https://unsplash.com/s/photos/under-the-hood?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="43ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卷积已经成为现代神经网络的一个基本部分，因为它们能够捕捉局部信息并通过权重共享减少参数的数量。由于几乎所有基于视觉的模型(和一些 NLP 模型)都使用这种或那种形式的卷积，显然我们希望尽可能快地进行这些运算。</p><p id="5404" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了强调对快速卷积的需求，下面是一个简单网络的分析器输出，该网络具有一个 2D 卷积层，后跟一个全连接层:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/acba7a42276e2b088ebda54f613f8814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qrddrN-7KN-6ErmOuzgGkg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Profiler Output for a Simple Conv Network</figcaption></figure><p id="814c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性层(<code class="fe lw lx ly lz b">addmm</code>)后面的卷积层负责大约 90%的总执行时间。因此，开发一些技巧来尽可能加快计算速度也就不足为奇了。</p><p id="8d06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我们将看看 PyTorch 和 TensorFlow 用来显著提高卷积速度的两个技巧。我们将使用 2D 卷积，因为这是最容易可视化的，但完全相同的概念适用于 1D 和三维卷积</p><h1 id="df02" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">朴素卷积实现</h1><p id="e368" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">让我们从 2D 卷积的一个简单实现开始。我们将使用一个简单的 2x2 内核和一个 3x3 输入矩阵(带一个通道):</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="ad7d" class="nb mb it lz b gy nc nd l ne nf"><strong class="lz iu">input_matrix<br/></strong>array([[3., 9., 0.],<br/>       [2., 8., 1.],<br/>       [1., 4., 8.]], dtype=float32)</span><span id="ba36" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">kernel<br/></strong>array([[8., 9.],<br/>       [4., 4.]], dtype=float32)</span><span id="d1ed" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">bias</strong><br/>array([0.06], dtype=float32)</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Naive 2D Convolution</figcaption></figure><p id="92fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单的实现很容易理解，我们简单地遍历输入矩阵并拉出与内核形状相同的“窗口”。对于每个窗口，我们对内核进行简单的元素级乘法，并对所有值求和。最后，在返回结果之前，我们将偏差项添加到输出的每个元素中。</p><p id="e7e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过用 PyTorch 自己的<code class="fe lw lx ly lz b">conv2d</code>层检查输出，我们可以快速验证我们得到了正确的结果。</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="c137" class="nb mb it lz b gy nc nd l ne nf">naive_conv_op = conv_2d(input_matrix, kernel, bias)<br/>print(naive_conv_op)</span><span id="ecc6" class="nb mb it lz b gy ng nd l ne nf">torch_conv = nn.Conv2d(1, 1, 2)<br/>torch_conv_op = torch_conv(input_matrix)<br/>print(torch_conv_op)</span><span id="2f4f" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>naive_conv_op<br/></strong>array([[145.06, 108.06],<br/>       [108.06, 121.06]])</span><span id="147a" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">torch_conv_op<br/></strong>tensor([[[[145.07, 108.07],<br/>          [108.07, 121.07]]]])</span></pre><p id="4981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是它们的执行时间:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="55c0" class="nb mb it lz b gy nc nd l ne nf">%%timeit<br/>conv_2d(input_matrix, kernel, bias)</span><span id="4e23" class="nb mb it lz b gy ng nd l ne nf">%%timeit<br/>torch_conv(input_matrix)</span><span id="d31b" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>Naive Conv:<br/></strong>26.9 µs ± 1.34 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)</span><span id="731b" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Torch Conv:<br/></strong>59.5 µs ± 935 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)</span></pre><p id="8bb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们检查当内核大小保持不变，输入矩阵的大小缓慢变化时，执行时间是如何变化的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/8f6e4ebf2d5d3fc1aeb82c0ef78079d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-gq7KwSvZLQU6HGCF9WqoQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Naive Convolution vs PyTorch Convolution</figcaption></figure><p id="b841" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们实现中的 2 个 for 循环负责 O(n)执行时间，当输入大小增加到 250 x 250 以上时，朴素 Conv 每个矩阵需要 1-3 秒。如果我们有一个像 Inception Net 这样的巨大网络，有数百个卷积和数千个大型输入矩阵，朴素卷积将是一个绝对可怕的想法。</p><p id="286b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，请注意 PyTorch 自己的实现可以很好地适应输入矩阵的大小。显然，PyTorch 做卷积的方式不同。</p><h1 id="eb7c" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">招数 1 : im2col</h1><p id="d8d7" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">在将每个窗口与内核相乘时，我们执行了两个操作:</p><ol class=""><li id="8623" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">增加了条款</li><li id="869f" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">把它们加在一起。</li></ol><p id="0556" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">….我们对输入矩阵中的每个窗口都这样做。</p><p id="e31f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在这里要问的重要问题是:<em class="ny">我们能对整个操作进行矢量化吗？</em></p><p id="c978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">答案是肯定的，这正是<code class="fe lw lx ly lz b">im2col</code>帮助我们做的(代表图像块到列)</p><p id="9d1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单地说，<code class="fe lw lx ly lz b">im2col</code>是一种技术，我们将每个窗口展平，然后将它们堆叠成矩阵中的列。现在，如果我们将内核展平为一个行向量，并在两者之间进行矩阵乘法，我们应该会在对输出进行整形后得到完全相同的结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/27e0c0dea443d10fb048291ef333141e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*tiNWuTqyB8jCku28y95jKA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Im2Col</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/83adbbef6b886c321bab1a1c2db10903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*DnJSZLVuw5utEBACZtOz0Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Im2Col-Reshaping</figcaption></figure><p id="3702" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试一试:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Naive Implementation of Im2Col</figcaption></figure><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="a5e1" class="nb mb it lz b gy nc nd l ne nf">im2col(input_matrix, kernel)</span><span id="9d37" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/></strong>array([[3, 9, 2, 8],<br/>       [9, 0, 8, 1],<br/>       [2, 8, 1, 4],<br/>       [8, 1, 4, 8]])</span></pre><p id="404d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们展平内核并进行矩阵乘法:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="4445" class="nb mb it lz b gy nc nd l ne nf">output_shape = (input_matrix.shape[0] - kernel.shape[0]) + 1</span><span id="17c0" class="nb mb it lz b gy ng nd l ne nf">im2col_matrix = im2col(input_matrix, kernel) <br/>im2col_conv = np.dot(kernel.flatten(), im2col_matrix) + bias<br/>im2col_conv = im2col_conv.reshape(output_shape,output_shape)<br/>print(im2col_conv)</span><span id="002b" class="nb mb it lz b gy ng nd l ne nf">torch_conv = nn.Conv2d(1, 1, 2)<br/>torch_conv_op = torch_conv(input_matrix)<br/>print(torch_conv_op)</span><span id="3b49" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>im2col_conv<br/></strong>array([[145.06, 108.06],<br/>       [108.06, 121.06]])</span><span id="3ef9" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">torch_conv_op<br/></strong>tensor([[[[145.07, 108.07],<br/>          [108.07, 121.07]]]])</span></pre><p id="5490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们来看看它是如何扩展的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c8dee85b6e5b55f6054fad17152fbcad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*H1lC49ULqww35qOdqUPrXA.png"/></div></figure><p id="a9a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">矢量化无疑有所帮助，但仍有改进的空间。在我们进入下一个技巧之前，让我们看看为什么矢量化会有帮助。</p><h2 id="5a4c" class="nb mb it bd mc ob oc dn mg od oe dp mk li of og mm lm oh oi mo lq oj ok mq ol bi translated">为什么会这样？</h2><p id="cc78" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">所有现代 CPU 和 GPU 都带有优化的矩阵代数库，允许代码利用硬件加速。这些库归入<a class="ae ky" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" rel="noopener ugc nofollow" target="_blank"> BLAS </a>或基本线性代数子程序的总称。当我们对代码进行矢量化并调用<code class="fe lw lx ly lz b">np.dot()</code>时，它允许 numpy 使用 BLAS 库，从而提高执行速度。</p><p id="537c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事实上，在早期的探查器输出中，您可能会看到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/dd26e3a2115a8c17471786c7664b4575.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*sp4hvwhRINxM8Lw_MKEvRA.png"/></div></figure><p id="1aa5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MKLDNN 代表深度神经网络的数学内核库，这是英特尔的 BLAS 库。自从我在英特尔 i7 上运行 PyTorch 模型后，PyTorch 自动调用了英特尔的 BLAS 库。如果你在 Nvidia GPU 上运行这个，PyTorch 会使用 cuBLAS (Nvidia 的 BLAS 库)。</p><p id="5220" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个技巧是去除 2- for 循环，高效地创建<code class="fe lw lx ly lz b">im2col</code>矩阵。</p><h1 id="edc2" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">诀窍 2:记忆跨越</h1><p id="2a7c" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">在<code class="fe lw lx ly lz b">im2col</code>中创建窗口时，我们仍然使用 2 for 循环来索引输入矩阵，这会降低执行速度。为了理解如何改进这一点，我们需要看看 numpy 数组是如何存储在内存中的。</p><p id="efb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就像所有其他数组一样，numpy 数组作为连续的块存储在内存中。每个 numpy 数组还有一个<code class="fe lw lx ly lz b">.strides</code>属性，告诉我们需要跳转多少字节来访问下一个元素。</p><p id="ceec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="9692" class="nb mb it lz b gy nc nd l ne nf">x = np.arange(10, dtype = 'int64')<br/>print(x)<br/>print(x.strides)</span><span id="7609" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>x<br/></strong>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</span><span id="d4e7" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">x.strides<br/></strong>(8,)</span></pre><p id="d9ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个元素都是<code class="fe lw lx ly lz b">int64</code>，即 64 位或 8 字节，这就是为什么<code class="fe lw lx ly lz b">x.strides</code>告诉我们需要跳跃 8 字节来访问数组中的下一个元素。</p><p id="9d78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当处理 2D 数组时，我们得到两个步幅值，告诉我们在列方向和行方向跳跃多少字节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/fe3ac65d524fe2313dfd50b30028421e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*84tvYSQe4PiLQvVBN2PaZw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Credit: AndyK <a class="ae ky" href="https://stackoverflow.com/questions/53097952/how-to-understand-numpy-strides-for-layman" rel="noopener ugc nofollow" target="_blank">on StackOverflow</a>[2]</figcaption></figure><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="2ff0" class="nb mb it lz b gy nc nd l ne nf">x = np.array([[1,2,3], [4,5,6], [7,8,9]])<br/>print(x)<br/>print(x.strides)</span><span id="1990" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>x<br/></strong>array([[1, 2, 3],<br/>       [4, 5, 6],<br/>       [7, 8, 9]])</span><span id="bdcd" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">x.strides<br/></strong>(24,8)</span><span id="cce1" class="nb mb it lz b gy ng nd l ne nf">#Jump 24bytes to access next row, 8bytes to access next column</span></pre><p id="c57e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在有趣的部分来了，numpy 让我们能够通过使用一个叫做<code class="fe lw lx ly lz b">np.lib.stride_tricks.as_strided</code>的函数来改变任何 numpy 数组的步长。基于我们提供的跨距值，这个函数简单地改变了我们在内存中查看数组的方式，并生成了一个新的“视图”。</p><p id="5382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个例子:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="4eb5" class="nb mb it lz b gy nc nd l ne nf">x = np.array([[1,2,3], [4,5,6], [7,8,9]])<br/>print(x)</span><span id="df03" class="nb mb it lz b gy ng nd l ne nf">x_newview = np.lib.stride_tricks.as_strided(x, shape = (5, 4), strides = (8,8))<br/>print(x_newview)</span><span id="b1fb" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:</strong></span><span id="8082" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">X</strong><br/>array([[1, 2, 3],<br/>       [4, 5, 6],<br/>       [7, 8, 9]])</span><span id="c01a" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">X_newview<br/></strong>array([[1, 2, 3, 4],<br/>       [2, 3, 4, 5],<br/>       [3, 4, 5, 6],<br/>       [4, 5, 6, 7],<br/>       [5, 6, 7, 8]])</span></pre><p id="2f6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当开始下一行时，我们使用<code class="fe lw lx ly lz b">as_strided</code>仅跳转 8 个字节(1 个元素),而不是跳转 24 个字节(3 个元素)来开始下一行。使用<code class="fe lw lx ly lz b">shape</code>参数，我们也可以根据需要设置输出形状。</p><p id="7bd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:如前所述，<code class="fe lw lx ly lz b">as_strided</code>改变了我们看待内存中数组的方式。这意味着如果我们改变“视图”中的值，它将改变内存中的值，从而改变原始矩阵中的元素。</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="be62" class="nb mb it lz b gy nc nd l ne nf">X_newview[1,3] = -99<br/>print(X_newview)<br/>print(X)</span><span id="87f3" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>X_newview<br/></strong>array([[  1,   2,   3,   4],<br/>       [  2,   3,   4, -99],<br/>       [  3,   4, -99,   6],<br/>       [  4, -99,   6,   7],<br/>       [-99,   6,   7,   8]])</span><span id="7208" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">X<br/></strong>array([[  1,   2,   3],<br/>       [  4, -99,   6],<br/>       [  7,   8,   9]])</span></pre><p id="0ec9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于<code class="fe lw lx ly lz b">as_strided</code>不使用任何循环来创建这些“视图”,我们可以使用它来有效地生成卷积窗口。我们需要做的就是计算正确的步幅值和输出形状，剩下的工作由<code class="fe lw lx ly lz b">as_strided</code>来完成。</p><p id="4972" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果我们提供了错误的步幅值，<code class="fe lw lx ly lz b">as_strided</code>将访问数组之外的内存位置并返回垃圾值。幸运的是，<code class="fe lw lx ly lz b">scikit-images</code>库中的<code class="fe lw lx ly lz b">view_as_windows</code>函数通过在后台使用<code class="fe lw lx ly lz b">as_strided</code>自动计算形状和步幅值，为我们完成了所有繁重的工作:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="de12" class="nb mb it lz b gy nc nd l ne nf">from skimage.util.shape import view_as_windows</span><span id="5cc0" class="nb mb it lz b gy ng nd l ne nf">input_matrix = np.array([[3,9,0], [2, 8, 1], [1,4,8]])<br/>print(input_matrix)</span><span id="001b" class="nb mb it lz b gy ng nd l ne nf">kernel = np.array([[8,9], [4,4]])<br/>print(kernel)</span><span id="6649" class="nb mb it lz b gy ng nd l ne nf">windows = view_as_windows(x, kernel.shape)<br/>print(windows)</span><span id="34f3" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>input_matrix<br/></strong>array([[3, 9, 0],<br/>       [2, 8, 1],<br/>       [1, 4, 8]])</span><span id="acc5" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">kernel<br/></strong>array([[8, 9],<br/>       [4, 4]])</span><span id="c146" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">windows<br/></strong>array([[[[3, 9],<br/>         [2, 8]],</span><span id="daa4" class="nb mb it lz b gy ng nd l ne nf">        [[9, 0],<br/>         [8, 1]]],<br/></span><span id="6726" class="nb mb it lz b gy ng nd l ne nf">       [[[2, 8],<br/>         [1, 4]],</span><span id="cb62" class="nb mb it lz b gy ng nd l ne nf">        [[8, 1],<br/>         [4, 8]]]])</span></pre><p id="7592" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们只是重塑:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="dd2b" class="nb mb it lz b gy nc nd l ne nf">output_shape = (input_matrix.shape[0] - kernel.shape[0]) + 1<br/>windows = windows.reshape(output_shape**2, kernel.shape[0]*2)<br/>print(windows)</span><span id="5a33" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>windows<br/></strong>array([[3, 9, 2, 8],<br/>       [9, 0, 8, 1],<br/>       [2, 8, 1, 4],<br/>       [8, 1, 4, 8]])</span></pre><p id="6340" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是完成所有这些工作的最后一个函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Im2Col with Memory Strides</figcaption></figure><p id="084e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们可以做矩阵乘法，方法和之前一样:</p><pre class="kj kk kl km gt mx lz my mz aw na bi"><span id="4d13" class="nb mb it lz b gy nc nd l ne nf">output_shape = (input_matrix.shape[0] - kernel.shape[0]) + 1<br/>mem_strided_mat = memory_strided_im2col(input_matrix, kernel) <br/>mem_strided_conv = np.dot(kernel.flatten(), mem_strided_mat) + bias</span><span id="f253" class="nb mb it lz b gy ng nd l ne nf">mem_strided_conv = mem_strided_conv.reshape(output_shape, output_shape)<br/>print(mem_strided_conv)</span><span id="d526" class="nb mb it lz b gy ng nd l ne nf">torch_conv = nn.Conv2d(1, 1, 2)<br/>torch_conv_op = torch_conv(input_matrix)<br/>print(torch_conv_op)</span><span id="6892" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">Output:<br/>mem_strided_conv<br/></strong>array([[145.06, 108.06],<br/>       [108.06, 121.06]])</span><span id="2fa6" class="nb mb it lz b gy ng nd l ne nf"><strong class="lz iu">torch_conv_op<br/></strong>tensor([[[[145.07, 108.07],<br/>          [108.07, 121.07]]]])</span></pre><p id="8ed7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看它与迄今为止所有其他实现相比如何:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/1af98b973ac753d853fdd64c3d35b1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JDX_u2I2KJwkNUQukcdq-g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Plot for Mem Strided Im2Col</figcaption></figure><p id="02cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用<code class="fe lw lx ly lz b">as_strided</code>大大提高了我们的实现速度！事实上，它几乎和 PyTorch 一样快。</p><p id="f780" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，如果您在分析器输出中注意到，PyTorch 在卷积之前使用自己的<code class="fe lw lx ly lz b">as_strided</code>函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/6d2827eacc93e06c89600bc4717d2a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*0EZnR_pF5uSeyEg5U-kiDA.png"/></div></figure><h1 id="fa95" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">内存权衡</h1><p id="6e61" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">因为我们需要为输入矩阵的每个窗口创建列，所以 im2col 矩阵最终会比简单的实现消耗更多的内存。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/09c72c1940f51d1d1da072fa2e94aa72.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*2pWWtxEiCT9pTSw7ci65Dw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Memory Consumption of Strided Im2Col</figcaption></figure><p id="f8c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，速度上的提升(<em class="ny">见下表</em>)远远超过了内存消耗增加带来的困难。</p><h1 id="6660" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">摘要</h1><p id="8b67" class="pw-post-body-paragraph kz la it lb b lc ms ju le lf mt jx lh li mu lk ll lm mv lo lp lq mw ls lt lu im bi translated">这里总结了所有实现的执行时间。当输入大小改变时，内核大小(2 x 2)保持不变。我在英特尔 i7 处理器上运行了所有这些程序。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/26b0fb30f375c9a0bf09d8fd43929dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*QSKnpWonSixqzS6MW6AhJA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Final Results Table</figcaption></figure><p id="b38f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">令人难以置信的是，仅用两个简单的技巧，我们就能获得比简单卷积快 150 倍的性能提升。PyTorch 实现仍然比我们的内存步进式 im2col 实现快 2 倍。这很可能是因为 PyTorch 有自己的张量实现，可能会针对更大的矩阵进行优化。事实上，对于 50 x 50 以下的矩阵，我们的实现比 PyTorch 更快。</p><p id="942f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然我们在这里只使用 PyTorch，<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>在执行卷积(<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank"> docs </a>)时也执行完全相同的一组操作。</p><p id="299c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，当使用填充、步长或 1D/3D 卷积时，我们的实现会发生怎样的变化:</p><p id="7176" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">填充</strong>:如果我们添加填充，对我们的实现没有影响，因为填充通常在卷积之前应用。然而，必须正确计算输出形状。</p><p id="abb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步幅</strong>:这里我们假设步幅为 1。更大的步幅只会以更大的跳跃滑动窗口，这意味着必须重新计算<code class="fe lw lx ly lz b">as_strided</code>中的<code class="fe lw lx ly lz b">strides</code>。然而，概念是相同的。(<em class="ny">事实上，</em> <code class="fe lw lx ly lz b"><em class="ny">view_as_windows</em></code> <em class="ny">有一个</em> <code class="fe lw lx ly lz b"><em class="ny">step</em></code> <em class="ny">参数，它也可以处理步幅。)</em></p><p id="47dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更多过滤器</strong>:在我们的例子中，我们假设内核只有一个过滤器。如果我们有更多的过滤器，每个过滤器将被拉平，给我们一个矩阵，而不是向量。接下来，我们将这个矩阵乘以 im2col 矩阵。这意味着我们将把矩阵<em class="ny">乘以矩阵</em>而不是矩阵乘以向量<em class="ny">来获得输出。</em></p><p id="41f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 1D 或 3D 卷积</strong>:im2 col 矩阵中的列会变得更短或更高，因为窗口的大小会改变(也取决于内核)。</p><p id="fbb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢并发现这很有用！如有任何问题或意见，请随时与我联系。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="8a36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Gist with all code:</strong><a class="ae ky" href="https://gist.github.com/anirudhshenoy/089a70deed944d0ca7ab0b6a5eb5a7f1" rel="noopener ugc nofollow" target="_blank">https://Gist . github . com/anirudhshenoy/089 a 70 deed 944d 0 ca 7 ab 0 b 6 a5 e b5 a 7 f 1</a></p><p id="95a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献:</strong></p><p id="2a23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1]第 11 讲 CS231N:费-李非&amp;安德烈·卡帕西&amp;贾斯廷·约翰逊<a class="ae ky" href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture11.pdf" rel="noopener ugc nofollow" target="_blank">http://cs 231n . Stanford . edu/slides/2016/winter 1516 _ Lecture 11 . pdf</a></p><p id="3711" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]<a class="ae ky" href="https://stackoverflow.com/questions/53097952/how-to-understand-numpy-strides-for-layman" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/53097952/how-to-understand-numpy-stamps-for-lender</a></p><p id="c0b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] TensorFlow Conv2D 文档: <a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/nn/conv2d </a></p></div></div>    
</body>
</html>