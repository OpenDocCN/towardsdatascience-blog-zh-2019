<html>
<head>
<title>Linear Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-models-4a3d14b8d368?source=collection_archive---------3-----------------------#2019-12-27">https://towardsdatascience.com/linear-regression-models-4a3d14b8d368?source=collection_archive---------3-----------------------#2019-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a38e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Python 和 SciKit 更好地理解线性回归模型的指南-学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f94dd7ada6d9abc4b34982ee62b81215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frgUePT6WpH4AsK8ALwx-A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@ifbdesign?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Benjamin Smith</a> on <a class="ae ky" href="https://unsplash.com/s/photos/graph?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="134e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以说，回归任务的最佳起点是线性模型:它们可以快速训练并易于解释。</p><p id="eefe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性模型使用输入要素的线性函数进行预测。在这里，我们将探索 Scikit-Learn 中一些流行的<a class="ae ky" href="https://scikit-learn.org/stable/modules/linear_model.html" rel="noopener ugc nofollow" target="_blank">线性模型。</a></p><p id="e19f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整的 Jupyter 笔记本可以在这里找到<a class="ae ky" href="https://github.com/terrah27/ml_guides/blob/master/linear_models_regression.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="5de6" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数据</h2><p id="f8ca" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这里我们将使用<a class="ae ky" href="https://scikit-learn.org/stable/datasets/index.html" rel="noopener ugc nofollow" target="_blank"> SciKit-Learn 糖尿病数据集</a>来回顾一些流行的线性回归算法。</p><p id="c544" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集包含 10 个特征(已经过均值居中和缩放)和一个目标值:基线后一年疾病进展的量度。</p><p id="d881" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们导入数据并为建模做准备:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0267" class="lv lw it mu b gy my mz l na nb">from sklearn.datasets import load_diabetes<br/>from sklearn.model_selection import train_test_split</span><span id="42f8" class="lv lw it mu b gy nc mz l na nb"># load regression dataset<br/>diabetes, target = load_diabetes(return_X_y=True)<br/>diabetes = pd.DataFrame(diabetes)</span><span id="999e" class="lv lw it mu b gy nc mz l na nb"># Prepare data for modeling<br/># Separate input features and target<br/>y = target<br/>X = diabetes</span><span id="638b" class="lv lw it mu b gy nc mz l na nb"># setting up testing and training sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)</span></pre><h2 id="cca2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">评估指标:R</h2><p id="d5ed" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">r 平方，或决定系数，是我们的模型所解释的目标变量的变化量。</p><p id="674d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">值的范围从 0 到 1。较高的值表示模型具有高度预测性。例如，R 值为 0.80 意味着模型解释了数据中 80%的可变性。</p><p id="f7de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，R 值越高越好。低值表明我们的模型不太擅长预测目标。然而，需要注意的是，过高的 R 可能是过度拟合的迹象。</p><p id="4c57" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用以下函数来获取模型的交叉验证分数:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1b43" class="lv lw it mu b gy my mz l na nb">from sklearn.model_selection import cross_val_score</span><span id="26bc" class="lv lw it mu b gy nc mz l na nb"># function to get cross validation scores<br/>def get_cv_scores(model):<br/>    scores = cross_val_score(model,<br/>                             X_train,<br/>                             y_train,<br/>                             cv=5,<br/>                             scoring='r2')<br/>    <br/>    print('CV Mean: ', np.mean(scores))<br/>    print('STD: ', np.std(scores))<br/>    print('\n')</span></pre><h2 id="2092" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">线性回归(普通最小二乘法)</h2><p id="ddb9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">线性回归找出使预测值和目标值之间的均方误差或残差最小的参数。</p><p id="68a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">均方误差定义为预测值和真实值之间的平方差之和除以样本总数。</p><p id="7d3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了生成线性回归，我们使用 Scikit-Learn 的<code class="fe nd ne nf mu b">LinearRegression</code>类:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="7578" class="lv lw it mu b gy my mz l na nb">from sklearn.linear_model import LinearRegression</span><span id="a143" class="lv lw it mu b gy nc mz l na nb"># Train model<br/>lr = LinearRegression().fit(X_train, y_train)</span><span id="80f6" class="lv lw it mu b gy nc mz l na nb"># get cross val scores<br/>get_cv_scores(lr)</span><span id="a505" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### CV Mean:  0.4758231204137221<br/>### STD:  0.1412116836029729</span></pre><p id="37ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们得到 0.48 的 R 值和 0.14 的标准偏差。低 R 值表明我们的模型不是很精确。标准偏差值表明我们可能过度拟合了训练数据。</p><p id="ea22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当模型对已知数据的预测比对未知数据的预测好得多时，就会发生过度拟合。该模型开始记忆训练数据，并且不能推广到看不见的测试数据。</p><p id="6664" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">克服过度拟合的一个选择是简化模型。我们将尝试通过引入正则化来简化我们的线性回归模型。</p><p id="7cda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正则化可以定义为明确地限制模型以防止过度拟合。</p><p id="8791" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于线性回归没有参数，所以没有办法控制模型的复杂程度。我们将在下面探索一些增加正则化的变化。</p><h2 id="2714" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">岭回归(L2 正则化)</h2><p id="eee4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">岭回归使用 L2 正则化来最小化系数的大小。它减少了系数的大小，有助于降低模型的复杂性。</p><p id="4cdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用正则化参数⍺.来控制模型的复杂性</p><p id="8390" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较高的⍺值迫使系数向零移动，并增加了对模型的限制。这降低了训练性能，但也增加了模型的可推广性。将⍺设置得太高可能会导致模型过于简单，数据不足。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/285b381f1c1edfff58c7ace56aee0555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnjLXQNl_7bok9XykPjQkw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Ridge Regression</figcaption></figure><p id="cd88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">⍺值越低，系数受到的限制就越少。当⍺很小时，模型变得更类似于上面的线性回归，我们有过度拟合的风险。</p><p id="46ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看是否可以使用 Scikit-Learn 的<code class="fe nd ne nf mu b">Ridge</code>类来提高性能:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="986d" class="lv lw it mu b gy my mz l na nb"><strong class="mu iu">from</strong> sklearn.linear_model <strong class="mu iu">import</strong> Ridge</span><span id="fec4" class="lv lw it mu b gy nc mz l na nb"><em class="nh"># Train model with default alpha=1<br/></em>ridge <strong class="mu iu">=</strong> Ridge(alpha<strong class="mu iu">=</strong>1).fit(X_train, y_train)</span><span id="7238" class="lv lw it mu b gy nc mz l na nb"><em class="nh"># get cross val scores<br/></em>get_cv_scores(ridge)</span><span id="016c" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### CV Mean:  0.3826248703036134<br/>### STD:  0.09902564009167607</span></pre><p id="4f02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">平均 R 值为 0.38 意味着我们只能用岭回归模型解释 38%的方差——与上面的线性回归相比，肯定没有改进。然而，我们的标准差降低了，这表明我们不太可能过度拟合。</p><p id="3b8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在上面使用了 alpha 的默认值，这可能不会给出最好的性能。α的最佳值将随每个数据集而变化。</p><p id="365f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看是否可以通过调整 alpha 值来提高 R 值。我们将使用<a class="ae ky" rel="noopener" target="_blank" href="/hyperparameter-tuning-c5619e7e6624">网格搜索</a>来找到一个最佳的 alpha 值:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="5db1" class="lv lw it mu b gy my mz l na nb"># find optimal alpha with grid search<br/>alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]<br/>param_grid = dict(alpha=alpha)</span><span id="cae1" class="lv lw it mu b gy nc mz l na nb">grid = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)<br/>grid_result = grid.fit(X_train, y_train)</span><span id="bb62" class="lv lw it mu b gy nc mz l na nb">print('Best Score: ', grid_result.best_score_)<br/>print('Best Params: ', grid_result.best_params_)</span><span id="fa76" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### Best Score:  0.4883436188936269<br/>### Best Params:  {'alpha': 0.01}</span></pre><p id="471a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们的 R 分数通过为 alpha 优化而增加！但是 0.48 的 R 分还是不太好。让我们看看是否可以使用其他类型的正则化来进一步改善这一点。</p><h2 id="5674" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">拉索回归(L1 正则化)</h2><p id="acd0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Lasso 回归使用 L1 正则化来强制某些系数恰好为零。这意味着模型完全忽略了一些特征。这可以被认为是一种自动特征选择！</p><p id="7ed7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们有大量的特征，但期望只有少数特征是重要的时，Lasso 可以是一个很好的模型选择。这样可以让模型更容易解读，揭示最重要的特征！</p><p id="131e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较高的⍺值会迫使更多的系数为零，从而导致拟合不足。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/533062462e94822ceedd2cfafcedbdd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*HAN654ziSGSqHYVcQGy6Ow.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Lasso Regression</figcaption></figure><p id="498b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">alpha 值越低，非零特征越少，并可能导致过度拟合。非常低的 alpha 值将导致模型类似于线性回归。</p><p id="cced" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们在糖尿病数据集上尝试 Lasso 回归:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="94b7" class="lv lw it mu b gy my mz l na nb">from sklearn.linear_model import Lasso</span><span id="9167" class="lv lw it mu b gy nc mz l na nb"># Train model with default alpha=1<br/>lasso = Lasso(alpha=1).fit(X_train, y_train)</span><span id="72f9" class="lv lw it mu b gy nc mz l na nb"># get cross val scores<br/>get_cv_scores(lasso)</span><span id="9731" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### CV Mean:  0.3510033961713952<br/>### STD:  0.08727927390128883</span></pre><p id="1c8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在上面使用了 alpha 的默认值，这可能不会给出最好的性能。α的最佳值将随每个数据集而变化。</p><p id="7d05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看可以通过调整阿尔法值来提高 R 值。我们将使用<a class="ae ky" rel="noopener" target="_blank" href="/hyperparameter-tuning-c5619e7e6624">网格搜索</a>来找到一个最佳的 alpha 值:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="996f" class="lv lw it mu b gy my mz l na nb"># find optimal alpha with grid search<br/>alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]<br/>param_grid = dict(alpha=alpha)</span><span id="f07b" class="lv lw it mu b gy nc mz l na nb">grid = GridSearchCV(estimator=lasso, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)<br/>grid_result = grid.fit(X_train, y_train)</span><span id="4736" class="lv lw it mu b gy nc mz l na nb">print('Best Score: ', grid_result.best_score_)<br/>print('Best Params: ', grid_result.best_params_)</span><span id="8d50" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### Best Score:  0.48813139496070573<br/>### Best Params:  {'alpha': 0.01}</span></pre><p id="40d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过优化 alpha，我们的分数提高了！</p><p id="59d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以检查这些系数，看看是否有任何系数被设置为零:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="ee63" class="lv lw it mu b gy my mz l na nb"># match column names to coefficients<br/>for coef, col in enumerate(X_train.columns):<br/>    print(f'{col}:  {lasso.coef_[coef]}')</span><span id="849a" class="lv lw it mu b gy nc mz l na nb">[out]<br/>age:  20.499547879943435<br/>sex:  -252.36006394772798<br/>bmi:  592.1488111417586<br/>average_bp:  289.434686266713<br/>s1:  -195.9273869617746<br/>s2:  0.0<br/>s3:  -96.91157736328506<br/>s4:  182.01914264519363<br/>s5:  518.6445047270033<br/>s6:  63.76955009503193</span></pre><p id="861d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nd ne nf mu b">s2</code>的系数为零。完全被模型忽略了！</p><p id="870f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将尝试最后一种类型的回归，看看我们是否可以进一步提高 R 分数。</p><h2 id="1db5" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">弹性网回归</h2><p id="f27c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">弹性网是一个线性回归模型，结合了套索和山脊的惩罚。</p><p id="c9be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<code class="fe nd ne nf mu b">l1_ratio</code>参数来控制 L1 和 L2 正则化的组合。当<code class="fe nd ne nf mu b">l1_ratio = 0</code>我们有 L2 正则化(岭)，当<code class="fe nd ne nf mu b">l1_ratio = 1</code>我们有 L1 正则化(套索)。介于 0 和 1 之间的值给出了 L1 正则化和 L2 正则化的组合。</p><p id="a02a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先用默认参数拟合弹性网，然后使用网格搜索找到<code class="fe nd ne nf mu b">alpha</code>和<code class="fe nd ne nf mu b">l1_ratio</code>的最佳值:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="bdd4" class="lv lw it mu b gy my mz l na nb">from sklearn.linear_model import ElasticNet</span><span id="7db1" class="lv lw it mu b gy nc mz l na nb"># Train model with default alpha=1 and l1_ratio=0.5<br/>elastic_net = ElasticNet(alpha=1, l1_ratio=0.5).fit(X_train, y_train)</span><span id="d53e" class="lv lw it mu b gy nc mz l na nb"># get cross val scores<br/>get_cv_scores(elastic_net)</span><span id="4a7e" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### CV Mean:  -0.05139208284143739<br/>### STD:  0.07297997198698156</span><span id="c32a" class="lv lw it mu b gy nc mz l na nb"># find optimal alpha with grid search<br/>alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]<br/>l1_ratio = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]<br/>param_grid = dict(alpha=alpha, l1_ratio=l1_ratio)</span><span id="46c3" class="lv lw it mu b gy nc mz l na nb">grid = GridSearchCV(estimator=elastic_net, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)<br/>grid_result = grid.fit(X_train, y_train)</span><span id="8145" class="lv lw it mu b gy nc mz l na nb">print('Best Score: ', grid_result.best_score_)<br/>print('Best Params: ', grid_result.best_params_)</span><span id="1bcb" class="lv lw it mu b gy nc mz l na nb">[out]<br/>### Best Score:  0.48993062619187755<br/>### Best Params:  {'alpha': 0.001, 'l1_ratio': 0.8}</span></pre><p id="2a44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，在找到最佳超参数值后，我们的 R 值增加了。</p><h2 id="f20c" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结论</h2><p id="6a47" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们探索了四种不同的线性回归模型:</p><ul class=""><li id="a1d6" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated">线性回归</li><li id="8ec7" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">山脉</li><li id="ed02" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">套索</li><li id="c56d" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">弹性网</li></ul><p id="8419" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们通过正则化简化了我们的模型。不幸的是，我们的 R 分数仍然很低。</p><p id="86ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在以后的文章中，我们将探索线性回归的假设和更多提高模型性能的方法。</p></div></div>    
</body>
</html>