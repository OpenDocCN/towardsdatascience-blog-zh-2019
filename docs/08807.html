<html>
<head>
<title>Lit BERT: NLP Transfer Learning In 3 Steps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Lit BERT: NLP 迁移学习的三个步骤</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lit-bert-nlp-transfer-learning-in-3-steps-272a866570db?source=collection_archive---------10-----------------------#2019-11-25">https://towardsdatascience.com/lit-bert-nlp-transfer-learning-in-3-steps-272a866570db?source=collection_archive---------10-----------------------#2019-11-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/de2f74a19eedb98a544879fe5a93cbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/1*S0pwe67pA780cdQETmGblw.gif"/></div></figure><p id="f1d3" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a> (Devlin 等人，2018)可能是最流行的迁移学习的 NLP 方法。由<a class="ae ks" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>实现的这个 API 提供了很多不错的特性，并抽象出了漂亮 API 背后的细节。</p><p id="b934" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><a class="ae ks" href="https://github.com/williamFalcon/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>是一个轻量级框架(实际上更像是重构你的 PyTorch 代码)，它允许任何使用<a class="ae ks" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>的人，如学生、研究人员和制作团队，轻松扩展深度学习代码，同时使其可复制。它还通过教练标志提供 42+高级研究功能。</p><p id="9a1f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">Lightning 没有在 PyTorch 上添加抽象，这意味着它可以很好地与其他伟大的软件包一起使用，比如 Huggingface！在本教程中，我们将使用他们的 BERT 实现在 Lightning 中执行微调任务。</p><p id="f456" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在本教程中，我们将分三步进行 NLP 的迁移学习:</p><ol class=""><li id="1750" class="kt ku iq jw b jx jy kb kc kf kv kj kw kn kx kr ky kz la lb bi translated">我们将从 huggingface 库导入 BERT。</li><li id="e092" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">我们将创建一个<a class="ae ks" href="https://pytorch-lightning.readthedocs.io/en/latest/LightningModule/RequiredTrainerInterface/" rel="noopener ugc nofollow" target="_blank">照明模块</a>，它使用 BERT 提取的特征进行微调</li><li id="6c95" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">我们将使用<a class="ae ks" href="https://github.com/williamFalcon/pytorch-lightning" rel="noopener ugc nofollow" target="_blank">灯光训练器</a>训练 BertMNLIFinetuner。</li></ol></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h1 id="d9cd" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">现场演示</h1><p id="765a" class="pw-post-body-paragraph ju jv iq jw b jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn mq kp kq kr ij bi translated">如果您更愿意在实际代码中看到这一点，<a class="ae ks" href="https://colab.research.google.com/drive/1DovlWenVCuXZ-EZT66wc3GVHZqREyblV" rel="noopener ugc nofollow" target="_blank">复制这个 colab 笔记本！</a></p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h1 id="35dd" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">微调(又名迁移学习)</h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/30b302232febddd02af362f35c88ab58.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*26KhEb6qbk-QYWvB5ZLlPg.gif"/></div></figure><p id="c98e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果你是一名试图改进<a class="ae ks" href="https://gluebenchmark.com/" rel="noopener ugc nofollow" target="_blank"> NYU 胶水</a>基准的研究人员，或者是一名试图理解产品评论以推荐新内容的数据科学家，你正在寻找一种方法来提取一段文本的表示，以便可以解决不同的任务。</p><p id="2515" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于迁移学习，通常有两个步骤。您使用数据集 X 来预训练您的模型。然后，您使用该预训练模型将该知识带入求解数据集 b。在这种情况下，BERT 已经在图书语料库和英语维基百科上接受了预训练<a class="ae ks" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。下游任务是你关心的解决胶合任务或分类产品评论。</p><p id="fee4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">预训练的好处是，我们在下游任务中不需要太多数据就能获得惊人的结果。</p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h1 id="b960" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">用 PyTorch 闪电微调</h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi mx"><img src="../Images/638788f591f069e8d012cd58e9eda82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMEaNthMSqMCV4Lp1EG-9g.png"/></div></div></figure><p id="9384" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一般来说，我们可以使用以下抽象方法对 PyTorch Lightning 进行微调:</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="f3ee" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于迁移学习，我们在 lightning 模块中定义了两个核心部分。</p><ol class=""><li id="076d" class="kt ku iq jw b jx jy kb kc kf kv kj kw kn kx kr ky kz la lb bi translated">预训练模型(即:特征提取器)</li><li id="bf23" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">finetune 模型。</li></ol><p id="0d4c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">您可以将预训练模型视为特征提取器。这可以让您以一种比布尔或一些表格映射更好的方式来表示对象或输入。</p><p id="3bbf" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">例如，如果您有一个文档集合，您可以通过预先训练的模型运行每个文档，并使用输出向量来相互比较文档。</p><p id="dc8e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">微调模型可以任意复杂。它可能是一个深度网络，也可能是一个简单的线性模型或 SVM。</p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h1 id="eb91" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">用 BERT 微调</h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="my mz di na bf nb"><div class="gh gi ne"><img src="../Images/df0f7dc3710a242a6f818fb58e169436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*69VKlkQBba9nGiatORvevw.png"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Huggingface</figcaption></figure><p id="0e7f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这里，我们将使用一个预训练的 BERT 来微调一个名为 MNLI 的任务。这实际上只是试图将文本分为三类。这是照明模块:</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="877c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在这种情况下，我们使用 huggingface 库中预先训练的 BERT，并添加我们自己的简单线性分类器，将给定的文本输入分类到三个类别之一。</p><p id="3a2c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然而，我们仍然需要定义验证循环来计算我们的验证准确性</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="bab6" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">以及计算我们测试精度的测试回路</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="a621" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最后，我们定义将要操作的优化器和数据集。该数据集应该是您试图求解的下游数据集。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="d58c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">完整的 lightning 模块看起来像这样。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="nc nd l"/></div></figure></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h1 id="c62f" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">摘要</h1><p id="801e" class="pw-post-body-paragraph ju jv iq jw b jx mm jz ka kb mn kd ke kf mo kh ki kj mp kl km kn mq kp kq kr ij bi translated">在这里，我们学习了在 LightningModule 中使用 Huggingface BERT 作为特征提取器。这种方法意味着您可以利用一个真正强大的文本表示来做如下事情:</p><ol class=""><li id="d586" class="kt ku iq jw b jx jy kb kc kf kv kj kw kn kx kr ky kz la lb bi translated">情感分析</li><li id="7d28" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">对聊天机器人的建议回复</li><li id="7660" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">使用 NLP 构建推荐引擎</li><li id="86f6" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated"><a class="ae ks" href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">改进谷歌搜索算法</a></li><li id="38de" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi">…</li><li id="4f0c" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">为文档创建嵌入以进行相似性搜索</li><li id="b124" class="kt ku iq jw b jx lc kb ld kf le kj lf kn lg kr ky kz la lb bi translated">任何你能创造性地想到的东西！</li></ol><p id="d84e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">你也看到了<a class="ae ks" href="https://github.com/williamFalcon/pytorch-lightning/" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>与包括<a class="ae ks" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank"> Huggingface </a>在内的其他库的配合有多好！</p></div></div>    
</body>
</html>