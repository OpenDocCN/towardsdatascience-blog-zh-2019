<html>
<head>
<title>Paper Summary. Unsupervised Learning by Competing Hidden Units</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文总结。竞争隐藏单元的无监督学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-summary-unsupervised-learning-by-competing-hidden-units-ac74bc6d8b4a?source=collection_archive---------32-----------------------#2019-06-17">https://towardsdatascience.com/paper-summary-unsupervised-learning-by-competing-hidden-units-ac74bc6d8b4a?source=collection_archive---------32-----------------------#2019-06-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a3d2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><a class="ae kf" href="https://www.pnas.org/content/pnas/116/16/7723.full.pdf" rel="noopener ugc nofollow" target="_blank">竞争隐单元无监督学习综述</a>。</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/9a8a8aa8afd5f1c74ec4c5326e55bb8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sME7VFsUV8ZCCPwVunwTSg.jpeg"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">This gem was found in <a class="ae kf" href="https://twitter.com/evolvingstuff/status/1012517941030502401" rel="noopener ugc nofollow" target="_blank">twitter.com/evolvingstuff/status/1012517941030502401</a></figcaption></figure><p id="aef6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于作者:</p><ul class=""><li id="5016" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">麻省理工学院，IBM 公司，普林斯顿大学</li><li id="bd7f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kf" href="http://pni.princeton.edu/john-hopfield" rel="noopener ugc nofollow" target="_blank">约翰·j·霍普菲尔德</a>普林斯顿神经科学研究所</li></ul><p id="42c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们之前在同一篇论文中工作过<a class="ae kf" href="https://arxiv.org/abs/1701.00939" rel="noopener ugc nofollow" target="_blank">密集联想记忆对敌对的输入具有鲁棒性</a>。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="c548" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">介绍了一种新的无监督学习技术。(几乎)没有反向传播，模型也不是为特定任务而训练的。两位作者来自神经科学和计算机科学背景，他们的工作基于两项生物学观察:</p><p id="f4a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1-突触变化是局部的:</p><blockquote class="mn mo mp"><p id="c05f" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated"><em class="iq">在生物学中，突触更新取决于突触前细胞和突触后细胞的活动，或许还取决于一些全局变量，如任务执行得如何。(第 1 页)</em></p></blockquote><p id="37ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用反向传播训练的 A 和 B 之间的单元的权重不仅取决于 A 和 B 的活动，还取决于前一层的活动和训练标签。所以它不依赖于 A，B 的活动，而是依赖于网络中的其他神经元。这是受<a class="ae kf" href="https://en.wikipedia.org/wiki/Hebbian_theory" rel="noopener ugc nofollow" target="_blank"> Hebb </a>的创意启发。</p><p id="364f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2-动物在没有标记数据和比用反向传播训练的神经网络更少的数据的情况下学习:</p><blockquote class="mn mo mp"><p id="772d" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated"><em class="iq">其次，高等动物需要丰富的感官经验，才能将早期的[…]视觉系统调整为成年系统。这种体验被认为主要是观察性的，很少或没有标签，因此没有明确的任务。(第 1 页)</em></p></blockquote><h1 id="71b4" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">无监督的本地训练</h1><p id="a6b3" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">作者成功地在 MNIST 和 CIFAR-10 上训练了他们的模型，只有前向传递，这意味着:—这项技术的计算要求较低，其计算复杂性与反向传播中前向传递的计算复杂性相当(<a class="ae kf" href="https://youtu.be/4lY-oAY0aQU?t=1581" rel="noopener ugc nofollow" target="_blank">来源</a>)。—不需要针对给定任务训练模型来根据数据进行有意义的表示。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/de98de3d8246ff916cce47bb23b12902.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*ZlQ__ugrQgwlv8LPPa0gsA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Figure 01</figcaption></figure><p id="ff5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">蓝色的矩形是作者的“生物学习算法”。首先，数据通过它，没有任何标签或任何关于它将用于什么任务的指示。一旦被训练，一个完全连接的网络被附加到它的顶部，以便专门化模型并做出期望的预测。使用反向传播来训练该部分。</p><p id="135c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常为了计算隐藏层<code class="fe ns nt nu nv b">hμ</code>的活动，我们通过将输入<code class="fe ns nt nu nv b">vi</code>乘以矩阵<code class="fe ns nt nu nv b">Wμi</code>来将输入<code class="fe ns nt nu nv b">vi</code>投影到隐藏层上，然后应用非线性。在这种新技术中，通过求解这个微分方程来计算<code class="fe ns nt nu nv b">hμ</code>活动:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/86cfc1379c3f5c9b93ec37511e94649d.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*mAQdC-G-fXfY8tcKr-xayQ.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Equation 08</figcaption></figure><ul class=""><li id="7889" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><code class="fe ns nt nu nv b">μ</code>是我们要更新的隐藏层的索引</li><li id="722b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><code class="fe ns nt nu nv b">τ</code>是流程的时间刻度</li><li id="3337" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><code class="fe ns nt nu nv b">Iμ</code>是输入电流</li><li id="aa65" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">第二项，所有其他隐藏层的总和，引入了神经元之间的竞争。较强的单位会抑制较弱的单位。没有它，所有的神经元都会在输入信号出现时被激活。注意，该术语引入了单元之间的横向连接，因为同一层内的单元可以彼此连接。</li><li id="4fb5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><code class="fe ns nt nu nv b">r</code>是一个 ReLU，<code class="fe ns nt nu nv b">winh</code>是一个超参数常数。</li></ul><p id="af84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于训练是局部的，只需要向前传递，这种结构不同于自动编码器。</p><h1 id="ab63" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">在活动</h1><p id="da7a" class="pw-post-body-paragraph kw kx iq ky b kz nm jr lb lc nn ju le lf no lh li lj np ll lm ln nq lp lq lr ij bi translated">在一个关于 MNIST 和 CIFAR-10 的实验中，作者使用他们的生物技术训练了 2000 个隐藏单元来寻找矩阵<code class="fe ns nt nu nv b">Wμi</code>:</p><ul class=""><li id="0009" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">隐藏单元用正态分布初始化</li><li id="41aa" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">隐藏的单位被训练(同样，没有明确的任务或标签)</li><li id="d618" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">这些单元然后被冻结并连接到感知器上</li><li id="fb99" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用 SGD 训练感知器权重</li></ul><p id="2126" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下图最右边的图中可以看到 MNIST 的训练误差(BP 代表反向传播，BIO 代表提出的方法)。我们可以看到，尽管训练误差较高，但测试误差非常接近端到端训练的模型。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/37d7fb59c8d60f9a4f7796226e0f51e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxffVl1IBs_xl9kkhRXVpQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Figure 03</figcaption></figure><p id="032d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 MNIST 上，我们可以看到，由提出的生物学习算法(左图)学习的特征不同于用反向传播(中图)训练的特征。</p><blockquote class="mn mo mp"><p id="9be3" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated"><em class="iq">网络通过多个隐藏单元学习数据的分布式表示。然而，这种表示与由端到端训练的网络学习到的表示非常不同，这从图 3 的左侧和中间的比较中可以清楚地看出。</em></p></blockquote><p id="ca15" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，对于 CIFAR-10:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ny"><img src="../Images/d5fa592a8aa5e3345e413c420b4fbbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_v_j5n_j57sTOCwdB7cTrw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Figure 07</figcaption></figure><h1 id="88f7" class="mu mv iq bd mw mx my mz na nb nc nd ne jw nf jx ng jz nh ka ni kc nj kd nk nl bi translated">tldr</h1><blockquote class="mn mo mp"><p id="48cb" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated"><em class="iq">没有自上而下的信息传播，仅使用自下而上的信号学习突触权重，并且算法不知道网络最终必须在顶层解决的任务(第 8 页)</em></p></blockquote><ul class=""><li id="0976" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">一种新的无监督训练技术，其中没有定义任务，训练集通过模型进行训练，没有反向传播。一个完全连接的感知器被附加在上面，用反向传播进行训练，较低的无监督子模型被冻结。</li><li id="a2fa" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">这种技术在 MNIST 和 CIFAR 上显示出较差但接近艺术概括状态的性能。</li><li id="510f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">没有向前/向后，每个单元都可能与其他单元相连，包括在它自己的层上。</li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="0a50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">补充资源:</p><ul class=""><li id="6e7a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">麻省理工学院的一位作者的视频演示。</li><li id="5771" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kf" href="https://github.com/DimaKrotov/Biological_Learning/blob/master/Unsupervised_learning_algorithm_MNIST.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>进行繁殖。</li><li id="3d7c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kf" href="https://www.ibm.com/blogs/research/2019/04/biological-algorithm/" rel="noopener ugc nofollow" target="_blank">在 IBM 的博客上发表博文</a>。</li></ul></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="00a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mq">最初发表于</em><a class="ae kf" href="https://data-soup.github.io/blog/" rel="noopener ugc nofollow" target="_blank">data-soup.github.io/blog/</a></p></div></div>    
</body>
</html>