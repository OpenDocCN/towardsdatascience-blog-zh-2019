<html>
<head>
<title>Uber Reviews Text Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优步评论文本分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/uber-reviews-text-analysis-11613675046d?source=collection_archive---------12-----------------------#2019-07-16">https://towardsdatascience.com/uber-reviews-text-analysis-11613675046d?source=collection_archive---------12-----------------------#2019-07-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3e36" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">探索性分析，词汇袋，逻辑回归</h2></div><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi ca"><img src="../Images/eef8c38ff528642919993f072ac26cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JevZF1feLuCydpjupOj4tw.jpeg"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk"><a class="ae kx" href="https://unsplash.com/photos/7nrsVjvALnA" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="b19f" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="6490" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi mm translated">他的项目概述了一个使用词袋和逻辑回归的文本挖掘分类模型。我们将尝试理解优步文本评论和游乐设备评级之间的关系。如果您对非结构化数据分析相对陌生，但有一些统计和/或其他分类经验，这是一个很好的起点。</p><p id="7dc8" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><strong class="ls iu">数据源</strong>:用户<a class="ae kx" href="https://www.kaggle.com/purvank" rel="noopener ugc nofollow" target="_blank"> Purvank </a>的<a class="ae kx" href="https://www.kaggle.com/purvank/uber-rider-reviews-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="efca" class="ky kz it bd la lb nh ld le lf ni lh li jz nj ka lk kc nk kd lm kf nl kg lo lp bi translated">目录</h1><ol class=""><li id="f3e2" class="nm nn it ls b lt lu lw lx lz no md np mh nq ml nr ns nt nu bi translated">初步分析</li><li id="c95d" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml nr ns nt nu bi translated">格式化/转换文本</li><li id="1bd4" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml nr ns nt nu bi translated">逻辑回归</li><li id="4990" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml nr ns nt nu bi translated">测试/结论</li></ol></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="92d7" class="ky kz it bd la lb nh ld le lf ni lh li jz nj ka lk kc nk kd lm kf nl kg lo lp bi translated">初步分析</h1><h2 id="8f36" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">输入数据</h2><p id="29bf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">首先，让我们引入数据并可视化数据框架:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="c698" class="oa kz it on b gy or os l ot ou">#importing modules<br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import random</span><span id="356e" class="oa kz it on b gy ov os l ot ou">#pulling in data<br/>df = pd.read_csv(r'C:\Users\Andrew\Desktop\Python Text Analysis\Uber_Ride_Reviews.csv')<br/>df</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi ow"><img src="../Images/663d00eb4048e1ae32bc1e0184308d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wI97UDLbzX2vrNrTh1ozDA.png"/></div></div></figure><p id="2850" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这是一个简单的数据集。我们有“骑行回顾”文本栏和“骑行评级”栏(范围从 1 到 5，最低评级和最高评级)。用户写下这些文字评论来描述他们的体验，分类 5 星评级总结了这一点。</p><h2 id="0ed3" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">基本统计</h2><p id="d175" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">让我们检查一下。describe()方法来查看我们正在处理的实例数量和其他基本统计数据:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="ad2b" class="oa kz it on b gy or os l ot ou">df.describe()</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/32a9f15fc8ecec4fe26f4055420261bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*15S_uSiUAhkgltmsj_KUZg.png"/></div></figure><p id="5c83" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">看来优步车手在这个样本中表现不佳。均值是 1.62 的评分，75%的标记是 1 星评论！这告诉我们有不成比例的 1 星评论。希望我们可以通过建立一个模型来深入了解这一点。</p><h2 id="23b2" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">模型的目的</h2><p id="eb31" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">提到这一点，让我们重申我们将建立的词汇袋/逻辑回归模型的效用:</p><ul class=""><li id="d36f" class="nm nn it ls b lt mv lw mw lz oy md oz mh pa ml pb ns nt nu bi translated">我们将能够衡量未来文本评论的情绪，并将它们分为“好”或“坏”的类别。</li><li id="b557" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">我们将能够找出对评级情绪有重大影响的特定词语。这些信息对优步来说可能是一笔财富(如果用于他们的整个内部数据集，而不是我们拥有的这个小样本)。例如，单词“rude”可能有一个负系数，促使我们的分类器将该评论标记为不好。单词“great”可能有一个正系数，促使我们的分类器将该评论标记为好。 目标是发现。我们不知道哪些有趣的话可能会对情绪产生影响，这就是有趣的地方。“伟大”这个词比“可怕”更重要吗？这是积极的还是消极的？很多类似这样的问题，都会用我们的模型来回答。</li></ul><h2 id="123b" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">零检查</h2><p id="4bd7" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">接下来，让我们检查数据集中的空值:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="45d7" class="oa kz it on b gy or os l ot ou">#checking for nulls<br/>null_count = df.isnull().sum()<br/>null_count</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/84ee903d0467854f0394e9c866a624d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*fP0C2VleL7xMXzkzEnrSPQ.png"/></div></figure><p id="766a" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">这次没有。如果我们有一些空值，我们需要探索如何处理它们。关于这个主题有很多好的资源，你可以<a class="ae kx" rel="noopener" target="_blank" href="/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b">研究</a>。</p><h2 id="c4a0" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">分布</h2><p id="e424" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">接下来，让我们将数据的评级分布可视化:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="8e8b" class="oa kz it on b gy or os l ot ou">#seperating by groups<br/>groups = df.groupby('ride_rating').count()<br/>Values = groups.ride_review<br/>colors = ['r', 'g', 'b', 'c', 'm']</span><span id="da0a" class="oa kz it on b gy ov os l ot ou">#making bar plot<br/>plt.bar(([1,2,3,4,5]), Values, color= colors)<br/>plt.title('Rating Distribution')<br/>plt.xlabel('Rating')<br/>plt.ylabel('Review Quantity')<br/>plt.show()</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi pe"><img src="../Images/0a3c83aad42b7b72297661b8a41d1352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_mQsNPiRduz1S0h98G9RQ.png"/></div></div></figure><blockquote class="pf pg ph"><p id="950a" class="lq lr pc ls b lt mv ju lv lw mw jx ly pi mx mb mc pj my mf mg pk mz mj mk ml im bi translated">正如我们之前发现的，这个样本中有很多 1 星评论。</p></blockquote><p id="f0e2" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">为了稍后执行逻辑回归，我们需要找出一种方法来将这 5 个评级类别转换为二元类(1 和 0)。记住，逻辑回归只处理“非此即彼”的目标变量。将星级划分为二进制等级的最佳方式(在我看来)是:</p><ul class=""><li id="9b91" class="nm nn it ls b lt mv lw mw lz oy md oz mh pa ml pb ns nt nu bi translated">将低于 3 颗星的评分设为 0 级(负面情绪)</li><li id="ede5" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">将高于 3 的评分设为 1 级(积极情绪)</li><li id="32f2" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">删除 3 星评分。三颗星是中性的，不提供任何情绪洞察。</li></ul><p id="66fa" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">因此，让我们创建一个新列，删除 3 颗星评级，并创建一个新列，将其他评级分为两类:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="f250" class="oa kz it on b gy or os l ot ou">#deleting all instances with ride_rating = 3<br/>df = df[df.ride_rating != 3]</span><span id="a684" class="oa kz it on b gy ov os l ot ou">#separating by groups<br/>groups = df.groupby('ride_rating').count()<br/>Values = groups.ride_review<br/>colors = ['r', 'g', 'b', 'c']</span><span id="651b" class="oa kz it on b gy ov os l ot ou">#making bar plot<br/>plt.bar(([1,2,4,5]), Values, color= colors)<br/>plt.title('Rating Distribution')<br/>plt.xlabel('Rating')<br/>plt.ylabel('Review Quantity')<br/>plt.show()</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi pl"><img src="../Images/cbd90988cde0698f5b1d71bbbcc3b803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c199mhptKBLoTgcVujK38w.png"/></div></div></figure><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="af21" class="oa kz it on b gy or os l ot ou">#creating new binary_class column<br/>df['binary_class'] = np.where(df['ride_rating'] &gt; 3, 1, 0)<br/>df</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi pm"><img src="../Images/ca50cd7df45d5107c8d8aea92eccb3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXghk7Eaxg0B6bkHxSV2gw.png"/></div></div></figure><p id="92fd" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我们准备好了！让我们准备好课文。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="7c74" class="ky kz it bd la lb nh ld le lf ni lh li jz nj ka lk kc nk kd lm kf nl kg lo lp bi translated">格式化/转换文本</h1><h2 id="21d4" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">训练/测试分割</h2><p id="dd61" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这个过程的第一步是将我们的数据分成训练集和测试集。我们将从训练数据中创建我们的模型，并保存一些实例，以便稍后进行测试。我们用 sklearn 来洗牌和分牌。在不干扰参数的情况下，它应该将我们的数据分成 75%的训练和 25%的测试。</p><p id="4933" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">通过调用 X_train.shape，我们可以检查这一点。此外，让我们打印一份随机回顾来验证它是否有效，并提醒我们自己我们在做什么。</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="8e83" class="oa kz it on b gy or os l ot ou">#splitting into train and test<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(df['ride_review'], df['binary_class'], random_state = 0)</span><span id="d0af" class="oa kz it on b gy ov os l ot ou">#setting random number between 1 and 1000<br/>number = random.randint(1,1000)</span><span id="3610" class="oa kz it on b gy ov os l ot ou">#printing random training text and X_train shape<br/>print ('Random Review:')<br/>print(' ')<br/>print(X_train[number])<br/>print(' ')<br/>print('X_train shape: ' + str(X_train.shape))</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi pn"><img src="../Images/683116031c04ae0c30721ded8809f4b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6hT4AJQjQNz6zVHXmN6Ew.png"/></div></div></figure><blockquote class="pf pg ph"><p id="6908" class="lq lr pc ls b lt mv ju lv lw mw jx ly pi mx mb mc pj my mf mg pk mz mj mk ml im bi translated">不出所料，970/1294 =我们样本的 75%。</p></blockquote><h2 id="e00f" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">把单词变成数字</h2><p id="7b47" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在到了激动人心的部分，将我们的文本数据转换成数字特征。为了稍后对这些数据进行回归，我们需要为样本中的每个单词建立一个特征。本质上，我们将把每个单词翻译成一个数字，然后计算这些单词/数字在矩阵中的使用频率。<strong class="ls iu">这样做的过程叫做“</strong><a class="ae kx" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"><strong class="ls iu"/></a><strong class="ls iu">”。值得注意的是，单词的顺序并不重要，单词袋只计算每个单词实例的使用频率。</strong>首先，我们将使用 Sklearn <a class="ae kx" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a></p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="03c3" class="oa kz it on b gy or os l ot ou">#importing countvectorizer<br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="9354" class="oa kz it on b gy ov os l ot ou">#creating variable which assigns X_train to numbers<br/>vect = CountVectorizer().fit(X_train)</span><span id="7dc5" class="oa kz it on b gy ov os l ot ou">#translates numbers back to text<br/>vect.get_feature_names()[1:10]</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi po"><img src="../Images/b3c1fcd86049479b438b16a3c43f1411.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*lcCOeHoKc0pgqxJAmmf8-A.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Words 1–10</figcaption></figure><p id="9e28" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我们可以看到使用 len(vect.get_feature_names())方法，所有评论中共有 6607 个单词:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="4929" class="oa kz it on b gy or os l ot ou">#length of total words<br/>len(vect.get_feature_names())</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/48e252c11a82c4217858c621f2d7dff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*qyPXEYgXUxWnh9O3qWSANw.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Total Words</figcaption></figure><p id="c760" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">现在，让我们将 X_train 数据转换成一个矩阵，该矩阵以行的形式包含文档(实例),以列的形式包含新功能的数量(6，607)。比如我们上面看到的第一个字(0)就是“弃”。这将是矩阵中的第一列。任何包含“废弃”的优步评论都会计算出它被使用的次数，并添加到那一栏。作为一个旁注，有人使用“放弃”，将有可能对评级产生负面影响。我们可以稍后再做实验。</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="05bd" class="oa kz it on b gy or os l ot ou">#creating matrix array for logistic regression<br/>X_train_vectorized = vect.transform(X_train)<br/>print (X_train_vectorized.toarray())</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi pq"><img src="../Images/b9e2f5968cf53830b4a9f45b9bcf0193.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*va8EGBCzT8BCWAH-MVvIGw.png"/></div></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">1294 instance X 6607 feature array matrix</figcaption></figure><blockquote class="pf pg ph"><p id="021f" class="lq lr pc ls b lt mv ju lv lw mw jx ly pi mx mb mc pj my mf mg pk mz mj mk ml im bi translated">因为单词太多，而大多数评论只有其中的一小部分，所以这个数组中的大部分数字都会是 0。</p></blockquote></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="3589" class="ky kz it bd la lb nh ld le lf ni lh li jz nj ka lk kc nk kd lm kf nl kg lo lp bi translated">逻辑回归</h1><h2 id="c4de" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">建筑模型</h2><p id="a016" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们终于成功回归了。在我们继续之前，如果您需要逻辑回归复习，请在此处查看<a class="ae kx" rel="noopener" target="_blank" href="/univariate-logistic-regression-example-in-python-acbefde8cc14"/>。</p><p id="2e94" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">同样，我们将使用 sklearn 来执行此模型:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="af5b" class="oa kz it on b gy or os l ot ou">#creating log regression<br/>from sklearn.linear_model import LogisticRegression<br/>model = LogisticRegression()<br/>model.fit(X_train_vectorized, y_train)</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="gh gi pr"><img src="../Images/c9f37335a9d10f9bc9fb7b5cc3c09746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4MQ3UYlIN5siersZw33ng.png"/></div></div></figure><p id="4e20" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">现在，我们将计算 AUC，看看它对测试数据的分类有多好。更多关于 ROC 曲线和 AUC 的信息可以在<a class="ae kx" rel="noopener" target="_blank" href="/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc">这里找到</a>:</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="cfab" class="oa kz it on b gy or os l ot ou">#calculating AUC<br/>from sklearn.metrics import roc_auc_score<br/>predictions = model.predict(vect.transform(X_test))<br/>print('AUC: ', roc_auc_score(y_test, predictions))</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/ff309d5d8091e5be7039206776dc9b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*SwMj1nP7SAPSxtPBTOShsQ.png"/></div></figure><blockquote class="pf pg ph"><p id="950f" class="lq lr pc ls b lt mv ju lv lw mw jx ly pi mx mb mc pj my mf mg pk mz mj mk ml im bi translated">这很好。理解这个指标的粗略方法是说我们有 75%正确分类的实例。</p></blockquote></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><h1 id="cd3d" class="ky kz it bd la lb nh ld le lf ni lh li jz nj ka lk kc nk kd lm kf nl kg lo lp bi translated">测试/结论</h1><h2 id="fdb7" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">积极和消极的话</h2><p id="b6ca" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们来深究一下哪些词对阶级分离的影响最大。在这里，我们将数字转换回文字，获得回归输出的系数，将它们添加到数据帧中，并根据系数对它们进行排序。</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="4f03" class="oa kz it on b gy or os l ot ou">#creating array variable of all the words<br/>feature_names = np.array(vect.get_feature_names())</span><span id="b02e" class="oa kz it on b gy ov os l ot ou">#creating array of all the regression coefficients per word<br/>coef_index = model.coef_[0]</span><span id="088b" class="oa kz it on b gy ov os l ot ou">#creating df with both arrays in it<br/>df = pd.DataFrame({'Word':feature_names, 'Coef': coef_index})</span><span id="e2e9" class="oa kz it on b gy ov os l ot ou">#sorting by coefficient<br/>df.sort_values('Coef')</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/ef9c2374478a83da51fd00983bfa4135.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*OiUkrNldsjna5ielgixJ8w.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Most negative Coefficients (Most correlated with negative review)</figcaption></figure><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/878ec75d07d7741b67b4aea1908df47a.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*oDR3iuZ3ovFnpZM9gEPBmg.png"/></div><figcaption class="kt ku gj gh gi kv kw bd b be z dk">Most positive Coefficients (Most correlated with positive review)</figcaption></figure><h2 id="1776" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">负面情绪:</h2><p id="0f7b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这让我着迷。</p><ul class=""><li id="e222" class="nm nn it ls b lt mv lw mw lz oy md oz mh pa ml pb ns nt nu bi translated">“带电”是最负相关的词。</li><li id="46ef" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">“Lyft”我认为也很有趣。如果客户有负面体验，我相信他们会倾向于评论“你的竞争对手比你好！”。</li><li id="aa33" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">“免费”这个词让我很困惑。我本以为“免费”会是一个积极向上的词。</li></ul><h2 id="7661" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">积极情绪:</h2><p id="2b17" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">当你向下移动数据框，单词变得更加正相关。</p><ul class=""><li id="614f" class="nm nn it ls b lt mv lw mw lz oy md oz mh pa ml pb ns nt nu bi translated">“伟大”是最高的，分数如此积极，甚至超过了“充电”。</li><li id="c271" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">“安全”、“方便”、“干净”……这些词很多都是直觉预期的。</li><li id="8d38" class="nm nn it ls b lt nv lw nw lz nx md ny mh nz ml pb ns nt nu bi translated">一个意想不到的正相关词是“她”。女司机是否对客户情绪有更积极的作用？</li></ul><blockquote class="pf pg ph"><p id="a7d0" class="lq lr pc ls b lt mv ju lv lw mw jx ly pi mx mb mc pj my mf mg pk mz mj mk ml im bi translated">这些结果可以为优步的利益相关者和领导层提供潜在的无价的商业洞察力。</p></blockquote><h2 id="45c1" class="oa kz it bd la ob oc dn le od oe dp li lz of og lk md oh oi lm mh oj ok lo ol bi translated">测试自定义评论</h2><p id="38ed" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">最后，我们可以试验和测试我们自己的定制评论。下面以粗体显示的是输入，下面是各自的输出(1 =正；0 =负)。最后一个应该是肯定的，但是我们的分类器把它标为否定的。其余的似乎是准确的。</p><pre class="ki kj kk kl gt om on oo op aw oq bi"><span id="bdca" class="oa kz it on b gy or os l ot ou">print(model.predict(vect.transform(['<strong class="on iu">abandoned great</strong>'])))</span><span id="f772" class="oa kz it on b gy ov os l ot ou">print(model.predict(vect.transform(['<strong class="on iu">great she the best</strong>'])))</span><span id="ddd7" class="oa kz it on b gy ov os l ot ou">print(model.predict(vect.transform(['<strong class="on iu">charged slow horrible</strong>'])))</span><span id="e2d7" class="oa kz it on b gy ov os l ot ou">print(model.predict(vect.transform(['<strong class="on iu">it was as average as a trip could be</strong>'])))</span><span id="7fc0" class="oa kz it on b gy ov os l ot ou">print(model.predict(vect.transform(['<strong class="on iu">my family felt safe we got to our destination with ease</strong>'])))</span><span id="905d" class="oa kz it on b gy ov os l ot ou">print(model.predict(vect.transform(['<strong class="on iu">i got to my destination quickly and affordably i had a smile on my face from start to finish</strong>'])))</span></pre><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/22d062d2b8eb608be2295d724bd2b60a.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/format:webp/1*nUNthfbG0LgIBLY3wSh4Zg.png"/></div></figure><p id="c7bf" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">感谢阅读。</p><p id="3892" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">我在 Kaggle 上有一个内核，里面有这个分析<a class="ae kx" href="https://www.kaggle.com/hershyandrew/uber-reviews-text-analysis" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="f4bd" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">还有，如果你想联系，我的 LinkedIn 是<a class="ae kx" href="https://www.linkedin.com/in/andrew-hershy-a7779199/" rel="noopener ugc nofollow" target="_blank">这里</a></p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="0227" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated">如果你觉得这有帮助，请订阅。如果你喜欢我的内容，请查看其他几个项目:</p><p id="d67a" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4"> <em class="pc">随机森林是否优于 Logistic 回归？</em>(一比较)</a></p><p id="64b9" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/gini-index-vs-information-entropy-7a7e4fed3fcb"> <em class="pc">基尼指数 vs 信息熵</em> </a></p><p id="68cf" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/linear-vs-polynomial-regression-walk-through-83ca4f2363a3"> <em class="pc">简单线性 vs 多项式回归</em> </a></p><p id="1cc5" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc"> <em class="pc">用 Python 中的逻辑回归预测癌症</em> </a></p><p id="1cb1" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/univariate-logistic-regression-example-in-python-acbefde8cc14">二元逻辑回归示例(python) </a></p><p id="87db" class="pw-post-body-paragraph lq lr it ls b lt mv ju lv lw mw jx ly lz mx mb mc md my mf mg mh mz mj mk ml im bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/r-squared-recipe-5814995fa39a"> <em class="pc">从头开始计算 R 平方(使用 python) </em> </a></p></div></div>    
</body>
</html>