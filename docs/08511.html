<html>
<head>
<title>How to apply machine learning and deep learning methods to audio analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将机器学习和深度学习方法应用于音频分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-apply-machine-learning-and-deep-learning-methods-to-audio-analysis-615e286fcbbc?source=collection_archive---------2-----------------------#2019-11-18">https://towardsdatascience.com/how-to-apply-machine-learning-and-deep-learning-methods-to-audio-analysis-615e286fcbbc?source=collection_archive---------2-----------------------#2019-11-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6f47" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">作者:Niko Laskaris，面向客户的数据科学家，</em> <a class="ae kp" href="http://www.comet.ml/" rel="noopener ugc nofollow" target="_blank"> <em class="ko"> Comet.ml </em> </a></p><p id="9fd2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要查看本文末尾的代码、训练可视化和关于 python 示例的更多信息，请访问<a class="ae kp" href="https://www.comet.ml/demo/urbansound8k/view/" rel="noopener ugc nofollow" target="_blank"> Comet 项目页面</a>。</p><h1 id="7492" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">介绍</h1><p id="f181" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">虽然许多关于深度学习的写作和文献都涉及计算机视觉和<a class="ae kp" href="https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>，但音频分析——一个包括<a class="ae kp" href="https://pdfs.semanticscholar.org/5129/350ec0bd8f1fe78a9b864865709f8d8de058.pdf" rel="noopener ugc nofollow" target="_blank">自动语音识别(ASR) </a>、数字信号处理以及音乐分类、标记和生成的领域——是深度学习应用的一个不断增长的子域。一些最受欢迎和最广泛的机器学习系统，虚拟助手 Alexa，Siri 和 Google Home，很大程度上是建立在可以从音频信号中提取信息的模型之上的产品。</p><p id="b649" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们在<a class="ae kp" href="http://comet.ml/" rel="noopener ugc nofollow" target="_blank"> Comet </a>的许多用户都在从事音频相关的机器学习任务，如音频分类、语音识别和语音合成，因此我们为他们构建了工具，使用 Comet 的元机器学习平台来分析、探索和理解音频数据。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/bdcb5456f42eb6c69536557fc381a1e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mVLKLCgxnlUIjbIl"/></div></div></figure><p id="554d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用<a class="ae kp" href="http://www.comet.ml/" rel="noopener ugc nofollow" target="_blank"> Comet </a>进行音频建模、训练和调试</p><p id="804f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章专注于展示数据科学家和人工智能从业者如何使用 Comet 在音频分析领域应用机器学习和深度学习方法。为了理解模型如何从数字音频信号中提取信息，我们将深入探讨一些用于音频分析的核心特征工程方法。然后，我们将使用<a class="ae kp" href="https://librosa.github.io/librosa/" rel="noopener ugc nofollow" target="_blank"> Librosa </a>，一个用于音频分析的伟大 python 库，来编写一个简短的 python 示例，在<a class="ae kp" href="https://urbansounddataset.weebly.com/urbansound8k.html" rel="noopener ugc nofollow" target="_blank"> UrbanSound8k </a>数据集上训练神经架构。</p><h1 id="8974" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">音频的机器学习:数字信号处理，滤波器组，梅尔频率倒谱系数</h1><p id="b3e6" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">构建机器学习模型来分类、描述或生成音频通常涉及输入数据是音频样本的建模任务。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mf"><img src="../Images/b311edebb28fbe317295803a11252abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XeN1wpbFXfg0V62J"/></div></div></figure><p id="2ec9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">来自 UrbanSound8k 的音频数据集样本的示例波形</p><p id="b4ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些音频样本通常表示为时间序列，其中 y 轴度量是波形的幅度。振幅通常作为最初拾取音频的麦克风或接收器设备周围的压力变化的函数来测量。除非有与音频样本相关联的元数据，否则这些时间序列信号通常将是拟合模型的唯一输入数据。</p><p id="45e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">查看下面的样本，这些样本取自 Urbansound8k 数据集中的 10 个类别中的每一个类别，从目测可以清楚地看出，波形本身可能不一定产生清晰的类别识别信息。考虑发动机怠速、汽笛和手提钻的波形，它们看起来非常相似。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mg"><img src="../Images/2b00e1af6c4132999ae76d1763e0a961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BkDPP21IROJaWAE3"/></div></div></figure><p id="bd6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">事实证明，从音频波形(以及一般的数字信号)中提取的最佳特征之一自 20 世纪 80 年代以来一直存在，并且仍然是最先进的:梅尔频率倒谱系数(MFCCs)，由<a class="ae kp" href="https://users.cs.northwestern.edu/~pardo/courses/eecs352/papers/Davis1980-MFCC.pdf" rel="noopener ugc nofollow" target="_blank"> Davis 和 Mermelstein 于 1980 年</a>提出。下面我们将从技术上讨论 MFCCs 是如何产生的，以及为什么它们在音频分析中有用。这部分有些技术性，所以在开始之前，我们先定义几个与数字信号处理和音频分析相关的关键术语。如果你想更深入地了解，我们将链接到维基百科和其他资源。</p><h1 id="6b64" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">混乱但有用的术语</h1><p id="e860" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Sampling_(signal_processing)" rel="noopener ugc nofollow" target="_blank">采样和采样频率</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c68effcf1b1ba9b3afb506a168c158d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*XmubNBSlN4n4csFJ"/></div></figure><p id="f86e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在信号处理中，<strong class="js iu">采样</strong>是将连续信号缩减为一系列离散值。<strong class="js iu">采样频率</strong>或<strong class="js iu">速率</strong>是在一定时间内采集的样本数量。高采样频率导致较少的信息损失，但是计算费用较高，而低采样频率具有较高的信息损失，但是计算快速且便宜。</p><p id="948e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Amplitude" rel="noopener ugc nofollow" target="_blank">振幅</a></p><p id="4be8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">声波的<strong class="js iu">振幅</strong>是其在一段时间(通常是时间)内变化的量度。振幅的另一个常见定义是变量极值之间的差值大小的函数。</p><p id="b7cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c086a8e596f79ae991f5d5695ea848d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/0*1f8Epv3yoxb6ItBl"/></div></figure><p id="c620" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">傅立叶变换</strong>将时间函数(信号)分解成组成频率。同样，音乐和弦可以通过其组成音符的音量和频率来表达，函数的傅立叶变换显示了基础函数(信号)中每个频率的振幅(量)。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/9d4c807f2fa8a191502ac9f6202cf722.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/0*gcTo2kMwFbvh7QQ-"/></div></figure><p id="6478" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">顶部:数字信号；下图:信号的傅立叶变换</p><p id="5497" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">傅立叶变换有多种变体，包括<a class="ae kp" href="https://en.wikipedia.org/wiki/Short-time_Fourier_transform" rel="noopener ugc nofollow" target="_blank">短时傅立叶变换</a>，它在 Librosa 库中实现，涉及将音频信号分割成帧，然后对每帧进行傅立叶变换。一般来说，在音频处理中，傅立叶变换是将音频信号分解成其组成频率的一种优雅且有用的方式。</p><p id="77aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">*资源:到目前为止，我找到的最好的傅立叶变换视频来自<a class="ae kp" href="https://www.youtube.com/watch?v=spUNpyF58BY&amp;t=1s" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown </a> *</p><p id="1ca6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Periodogram" rel="noopener ugc nofollow" target="_blank">周期图</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/f4959d227c6440c6b8da6c30c0d73b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*oaguDFaOXKufjqeo"/></div></figure><p id="db2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在信号处理中，<strong class="js iu">周期图</strong>是对信号频谱密度的估计。上面的周期图显示了大约 30Hz 和大约 50Hz 的两个正弦基函数的功率谱。傅立叶变换的输出可以被认为是(不完全)本质上的周期图。</p><p id="b6b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Spectral_density" rel="noopener ugc nofollow" target="_blank">光谱密度</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ml"><img src="../Images/4827977007e363454231ceff815fcfda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eVSlB_Plyg9HmrqZ"/></div></div></figure><p id="d423" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">时间序列的<strong class="js iu">功率谱</strong>是一种将功率分布描述为组成该信号的离散频率分量的方式。一个信号的统计平均值，通过它的频率含量来测量，被称为它的<strong class="js iu">频谱</strong>。数字信号的<strong class="js iu">频谱密度</strong>描述了信号的频率成分。</p><p id="6632" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Mel_scale" rel="noopener ugc nofollow" target="_blank">梅尔标度</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/ff4c054695946036019f5fca317926e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*n5cV1piWNSGbH5Qx"/></div></figure><p id="fd64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">梅尔标度是一种由听众判断彼此距离相等的音高标度。mel 标度和正常频率测量之间的参考点是通过将 1000 mels 的感知音调分配给 1000 Hz 来任意定义的。在大约 500 Hz 以上，越来越大的音程被听众判断为产生相等的音高增量。名称<strong class="js iu"> mel </strong>来自单词 melody，表示音阶基于音高比较。</p><p id="d2cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将 f 赫兹转换成 m 英里的公式是:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mn"><img src="../Images/9d5a2d1b72537c71df483d7f526d2d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/0*iFiepytIIzKTZs-j"/></div></div></figure><p id="5b70" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Cepstrum" rel="noopener ugc nofollow" target="_blank">倒谱</a></p><p id="fed6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">倒谱</strong>是对信号的估计功率谱的对数进行傅立叶变换的结果。</p><p id="ca59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Spectrogram" rel="noopener ugc nofollow" target="_blank">心电图</a></p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/08ff1c479165d8aadd41536152da6a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/0*7nKweTpgHrU2xiC3"/></div></figure><p id="8dae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Urbansound8k 数据集中音频样本的 Mel 频谱图</p><p id="dfc4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">频谱图</strong>是信号频谱随时间变化的直观表示。考虑频谱图的一个好方法是将一些时间间隔数字信号的周期图叠加起来。</p><p id="0abc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae kp" href="https://en.wikipedia.org/wiki/Cochlea" rel="noopener ugc nofollow" target="_blank">耳蜗</a></p><p id="bef8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">耳蜗内耳的螺旋腔，包含耳蜗器官，它对声音振动产生神经冲动。</p><h1 id="d0de" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">音频预处理:数字信号处理技术</h1><p id="4c62" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">数据集预处理、特征提取和特征工程是我们从底层数据中提取信息的步骤，这些信息在机器学习上下文中应该对预测样本的类别或某些目标变量的值有用。在音频分析中，这个过程很大程度上是基于寻找音频信号的成分，这些成分可以帮助我们将其与其他信号区分开来。</p><p id="767c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如上所述，MFCCs 仍然是从音频样本中提取信息的现有技术工具。尽管 Librosa 之类的库为我们提供了一个 python 命令行程序来计算音频样本的 MFCCs，但底层的数学有点复杂，所以我们将一步一步地讲解它，并包含一些有用的链接以供进一步学习。</p><h1 id="44f5" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">计算给定音频样本的 MFCCs 的步骤:</h1><ol class=""><li id="1594" class="mp mq it js b jt lo jx lp kb mr kf ms kj mt kn mu mv mw mx bi translated">将信号分割成短帧(时间)</li><li id="6827" class="mp mq it js b jt my jx mz kb na kf nb kj nc kn mu mv mw mx bi translated">计算每帧功率谱的周期图估计</li><li id="ea4f" class="mp mq it js b jt my jx mz kb na kf nb kj nc kn mu mv mw mx bi translated">将 mel 滤波器组应用于功率谱，并对每个滤波器中的能量求和</li><li id="cf4f" class="mp mq it js b jt my jx mz kb na kf nb kj nc kn mu mv mw mx bi translated">对对数滤波器组能量进行离散余弦变换(DCT)</li></ol><p id="31ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">关于 MFCC 推导和计算的精彩附加阅读可以在博客文章<a class="ae kp" href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae kp" href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><ol class=""><li id="aae1" class="mp mq it js b jt ju jx jy kb nd kf ne kj nf kn mu mv mw mx bi translated"><strong class="js iu">将信号分割成短帧</strong></li></ol><p id="9e6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将音频信号分割成短帧是有用的，因为它允许我们将音频采样成离散的时间步长。我们假设在足够短的时间尺度上，音频信号不会改变。短帧持续时间的典型值在 20-40 毫秒之间。通常每帧重叠 10-15 毫秒。</p><p id="c4d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">*注意，重叠的帧会使我们最终生成的特征高度相关。这就是为什么我们必须在最后进行离散余弦变换的基础。* </em></p><p id="513a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2。计算每一帧的功率谱</strong></p><p id="7a41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们有了帧，我们需要计算每个帧的功率谱。时间序列的功率谱描述了组成该信号的频率分量的功率分布。根据傅立叶分析，任何物理信号都可以分解成许多离散的频率，或连续范围内的频谱。根据频率成分分析的特定信号的统计平均值称为其频谱。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7a34f455d86fa2d8365d3f0b1a471fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/0*bGO60h8mN1fUn-lD"/></div></figure><p id="ee64" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">来源:<a class="ae kp" href="https://terpconnect.umd.edu/~toh/spectrum/HarmonicAnalysis.html" rel="noopener ugc nofollow" target="_blank">马里兰大学，谐波分析和傅立叶变换</a></p><p id="113e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将<strong class="js iu">短时傅立叶变换</strong>应用于每一帧，以获得每一帧的功率谱。</p><p id="e573" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3。将 mel 滤波器组应用于功率谱，并对每个滤波器的能量求和</strong></p><p id="4044" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们有了功率谱，我们还有一些工作要做。人的耳蜗不能很好地辨别附近的频率，并且这种影响只会随着频率的增加而变得更加明显。<strong class="js iu"> mel-scale </strong>是一种工具，它允许我们比线性频带更接近人类听觉系统的响应。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/96e6a7d2fdc43026698689fbece63e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/0*kjMma7khTYdTpdM0"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Source: <a class="ae kp" href="https://labrosa.ee.columbia.edu/doc/HTKBook21/node54.html" rel="noopener ugc nofollow" target="_blank">Columbia</a></figcaption></figure><p id="406b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面的可视化中可以看出，mel 滤波器随着频率的增加而变宽，我们不太关心较高频率下的变化。在低频时，差异对人耳来说更明显，因此在我们的分析中更重要，滤波器较窄。</p><p id="493a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过对我们的输入数据应用傅立叶变换得到的我们的功率谱的幅度，通过将它们与每个三角形 Mel 滤波器相关联而被<a class="ae kp" href="https://en.wikipedia.org/wiki/Data_binning" rel="noopener ugc nofollow" target="_blank">装箱</a>。通常应用该宁滨，使得每个系数乘以相应的滤波器增益，因此每个 Mel 滤波器保持代表该通道中频谱幅度的加权和。</p><p id="759e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们有了滤波器组能量，我们就取每个能量的对数。这是由人类听觉限制推动的另一个步骤:人类无法感知线性范围内的音量变化。要使声波的感知音量加倍，声波的能量必须增加 8 倍。如果一个声波已经是高音量(高能量)，那么该波能量的巨大变化听起来不会有很大的不同。</p><p id="2e9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 4。对对数滤波器组能量进行离散余弦变换</strong></p><p id="55fe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为我们的滤波器组能量是重叠的(见步骤 1)，所以它们之间通常有很强的相关性。进行离散余弦变换有助于去相关能量。</p><p id="6fc1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi">*****</p><p id="daf4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对我们来说，值得庆幸的是,<a class="ae kp" href="https://librosa.github.io/librosa/" rel="noopener ugc nofollow" target="_blank"> Librosa </a>的创建者已经抽象出了大量的这种数学，并使得为你的音频数据生成 MFCCs 变得很容易。让我们通过一个简单的 python 示例来展示这种分析的实际效果。</p><h1 id="14ac" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">范例项目:Urbansound8k + Librosa</h1><p id="c58b" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们将为 UrbanSound8k 数据集拟合一个简单的神经网络(keras + tensorflow backend)。首先，让我们加载我们的依赖项，包括 numpy、pandas、keras、scikit-learn 和 librosa。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="3804" class="nr kr it nn b gy ns nt l nu nv">#### Dependencies ####</span><span id="92e7" class="nr kr it nn b gy nw nt l nu nv">#### Import Comet for experiment tracking and visual tools<br/>from comet_ml import Experiment<br/>####</span><span id="bd3c" class="nr kr it nn b gy nw nt l nu nv">import IPython.display as ipd<br/>import numpy as np<br/>import pandas as pd<br/>import librosa<br/>import matplotlib.pyplot as plt<br/>from scipy.io import wavfile as wav</span><span id="1b37" class="nr kr it nn b gy nw nt l nu nv">from sklearn import metrics <br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split </span><span id="8a85" class="nr kr it nn b gy nw nt l nu nv">from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Activation<br/>from keras.optimizers import Adam<br/>from keras.utils import to_categorical</span></pre><p id="80fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们创建一个 Comet 实验作为我们所有工作的包装。我们将能够捕获任何和所有工件(音频文件、可视化、模型、数据集、系统信息、培训指标等。)自动。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="e61c" class="nr kr it nn b gy ns nt l nu nv">experiment = Experiment(api_key="API_KEY",<br/>                        project_name="urbansound8k")</span></pre><p id="3c1b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们加载数据集，并从数据集中为每个类获取一个样本。我们可以使用 Comet 从视觉和听觉上检查这些样本。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="aafb" class="nr kr it nn b gy ns nt l nu nv"># Load dataset<br/>df = pd.read_csv('UrbanSound8K/metadata/UrbanSound8K.csv')# Create a list of the class labels<br/>labels = list(df['class'].unique())</span><span id="a90b" class="nr kr it nn b gy nw nt l nu nv"># Let's grab a single audio file from each class<br/>files = dict()<br/>for i in range(len(labels)):<br/>    tmp = df[df['class'] == labels[i]][:1].reset_index()<br/>    path = 'UrbanSound8K/audio/fold{}/{}'.format(tmp['fold'][0], tmp['slice_file_name'][0])<br/>    files[labels[i]] = path</span></pre><p id="b911" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用 librosa 的 display.waveplot 函数查看每个样本的波形。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="5aac" class="nr kr it nn b gy ns nt l nu nv">fig = plt.figure(figsize=(15,15))# Log graphic of waveforms to Comet<br/>experiment.log_image('class_examples.png')<br/>fig.subplots_adjust(hspace=0.4, wspace=0.4)<br/>for i, label in enumerate(labels):<br/>    fn = files[label]<br/>    fig.add_subplot(5, 2, i+1)<br/>    plt.title(label)<br/>    data, sample_rate = librosa.load(fn)<br/>    librosa.display.waveplot(data, sr= sample_rate)<br/>plt.savefig('class_examples.png')</span></pre><p id="f7b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将把这个图形保存到我们的彗星实验中。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="5452" class="nr kr it nn b gy ns nt l nu nv"># Log graphic of waveforms to Comet<br/>experiment.log_image('class_examples.png')</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi mg"><img src="../Images/77eb8d1e3841134d22d53405ab566011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xI0u9IyWvbcVGTD6"/></div></div></figure><p id="49a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们将记录音频文件本身。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="b5ad" class="nr kr it nn b gy ns nt l nu nv"># Log audio files to Comet for debugging<br/>for label in labels:<br/>    fn = files[label]<br/>    experiment.log_audio(fn, metadata = {'name': label})</span></pre><p id="3674" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们将样本记录到 Comet，我们就可以直接从 UI 中监听样本、检查元数据等等。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ml"><img src="../Images/d3c9f44d66e4edf4d46986de5e75f1da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9bp4nS-ieHCpvw7P"/></div></div></figure><p id="1bdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">预处理</strong></p><p id="3a57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以从数据中提取特征。我们将使用 librosa，但是我们也将展示另一个实用程序 scipy.io，用于比较和观察正在发生的一些隐式预处理。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="e649" class="nr kr it nn b gy ns nt l nu nv">fn = 'UrbanSound8K/audio/fold1/191431-9-0-66.wav'<br/>librosa_audio, librosa_sample_rate = librosa.load(fn)<br/>scipy_sample_rate, scipy_audio = wav.read(fn)</span><span id="f792" class="nr kr it nn b gy nw nt l nu nv">print("Original sample rate: {}".format(scipy_sample_rate))<br/>print("Librosa sample rate: {}".format(librosa_sample_rate))</span></pre><p id="fc9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">原始采样率:48000 <br/> Librosa 采样率:22050</p><p id="e497" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Librosa 的 load 函数会自动将采样率转换为 22.05 KHz。它还将归一化-1 和 1 之间的位深度。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="d31e" class="nr kr it nn b gy ns nt l nu nv">print('Original audio file min~max range: {} to {}'.format(np.min(scipy_audio), np.max(scipy_audio)))print('Librosa audio file min~max range: {0:.2f} to {0:.2f}'.format(np.min(librosa_audio), np.max(librosa_audio)))</span></pre><p id="712f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">&gt;原始音频文件最小～最大范围:-1869 到 1665 <br/> &gt;自由音频文件最小～最大范围:-0.05 到-0.05</p><p id="86e9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Librosa 还将音频信号从立体声转换为单声道。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="f662" class="nr kr it nn b gy ns nt l nu nv">plt.figure(figsize=(12, 4))<br/>plt.plot(scipy_audio)<br/>plt.savefig('original_audio.png')<br/>experiment.log_image('original_audio.png')</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nx"><img src="../Images/4bd236c1ef72b65ad4a3047c863910c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PHNAUzx6RavGpf9W"/></div></div></figure><p id="c124" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">原始音频(注意是立体声——两个音频源)</em></p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="4228" class="nr kr it nn b gy ns nt l nu nv"># Librosa: mono track<br/>plt.figure(figsize=(12,4))<br/>plt.plot(librosa_audio)<br/>plt.savefig('librosa_audio.png')<br/>experiment.log_image('librosa_audio.png')</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ny"><img src="../Images/c195a63c42cbafdfdc31d5b38247115c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iRP2r4kqJp_3Mp6a"/></div></div></figure><p id="53ab" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko"> Librosa 音频:转换为单声道</em></p><p id="4e0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">使用 Librosa 从音频中提取 MFCCs】</strong></p><p id="484a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还记得我们之前为了理解梅尔频率倒谱系数而经历的所有数学运算吗？使用 Librosa，下面是如何从音频中提取它们(使用我们上面定义的 librosa_audio)</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="6545" class="nr kr it nn b gy ns nt l nu nv">mfccs = librosa.feature.mfcc(y=librosa_audio, sr=librosa_sample_rate, n_mfcc = 40)</span></pre><p id="a914" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就是这样！</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="37ba" class="nr kr it nn b gy ns nt l nu nv">print(mfccs.shape)</span></pre><p id="1426" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi">&gt; (40, 173)</p><p id="71f0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Librosa 计算了 173 帧音频样本的 40 个 MFCCs。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="8d58" class="nr kr it nn b gy ns nt l nu nv">plt.figure(figsize=(8,8))<br/>librosa.display.specshow(mfccs, sr=librosa_sample_rate, x_axis='time')<br/>plt.savefig('MFCCs.png')<br/>experiment.log_image('MFCCs.png')</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6e8a152d5e829d76999cc774e67e9710.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/0*-TUGSRTMUZc9aovy"/></div></figure><p id="164f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将定义一个简单的函数来提取数据集中每个文件的 MFCCs。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="abde" class="nr kr it nn b gy ns nt l nu nv">def extract_features(file_name):audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') <br/>    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)<br/>    mfccs_processed = np.mean(mfccs.T,axis=0)<br/>     <br/>    return mfccs_processed</span></pre><p id="d418" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们来提取特征。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="5bec" class="nr kr it nn b gy ns nt l nu nv">features = []</span><span id="4f1b" class="nr kr it nn b gy nw nt l nu nv"># Iterate through each sound file and extract the features <br/>for index, row in metadata.iterrows():file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row["fold"])+'/',str(row["slice_file_name"]))<br/>    <br/>    class_label = row["class"]<br/>    data = extract_features(file_name)<br/>    <br/>    features.append([data, class_label])</span><span id="3066" class="nr kr it nn b gy nw nt l nu nv"># Convert into a Panda dataframe <br/>featuresdf = pd.DataFrame(features, columns=['feature','class_label'])</span></pre><p id="2a49" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在有一个数据帧，其中每行有一个标签(类)和一个特征列，由 40 个 MFCCs 组成。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="c7c9" class="nr kr it nn b gy ns nt l nu nv">featuresdf.head()</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oa"><img src="../Images/cad03a94db5e845af14d781f13f42ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LlT7lGO3rUZh9rJK"/></div></div></figure><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="adf3" class="nr kr it nn b gy ns nt l nu nv">featuresdf.iloc[0]['feature']</span></pre><p id="82a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数组([-2.1579300e+02，7.1666122e+01，-1.3181377e+02，-5.2091331e+01，-2.2115969e+01，-2.1764181e+01，-1.1183747e+01，1.8912683e+01，6.7266388e+00，1.40</p><p id="2046" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们已经成功地从底层音频数据中提取了我们的特征，我们可以建立和训练一个模型。</p><p id="88fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">模型建立和训练</strong></p><p id="6150" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们首先将 MFCCs 转换成 numpy 数组，并对分类标签进行编码。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="8fc9" class="nr kr it nn b gy ns nt l nu nv">from sklearn.preprocessing import LabelEncoder<br/>from keras.utils import to_categorical</span><span id="d256" class="nr kr it nn b gy nw nt l nu nv"># Convert features and corresponding classification labels into numpy arrays<br/>X = np.array(featuresdf.feature.tolist())<br/>y = np.array(featuresdf.class_label.tolist())</span><span id="04e3" class="nr kr it nn b gy nw nt l nu nv"># Encode the classification labels<br/>le = LabelEncoder()<br/>yy = to_categorical(le.fit_transform(y))</span></pre><p id="0094" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的数据集将被分成训练集和测试集。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="2847" class="nr kr it nn b gy ns nt l nu nv"># split the dataset <br/>from sklearn.model_selection import train_test_split </span><span id="77d1" class="nr kr it nn b gy nw nt l nu nv">x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 127)</span></pre><p id="0202" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们定义并编译一个简单的前馈神经网络架构。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="34d0" class="nr kr it nn b gy ns nt l nu nv">num_labels = yy.shape[1]<br/>filter_size = 2def build_model_graph(input_shape=(40,)):<br/>    model = Sequential()<br/>    model.add(Dense(256))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.5))<br/>    model.add(Dense(256))<br/>    model.add(Activation('relu'))<br/>    model.add(Dropout(0.5))<br/>    model.add(Dense(num_labels))<br/>    model.add(Activation('softmax'))<br/>    # Compile the model<br/>    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')</span><span id="5c36" class="nr kr it nn b gy nw nt l nu nv">    return modelmodel = build_model_graph()</span></pre><p id="ae7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们来看一个模型总结，并计算训练前的准确性。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="476e" class="nr kr it nn b gy ns nt l nu nv"># Display model architecture summary <br/>model.summary()</span><span id="69de" class="nr kr it nn b gy nw nt l nu nv"># Calculate pre-training accuracy <br/>score = model.evaluate(x_test, y_test, verbose=0)<br/>accuracy = 100*score[1]</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ob"><img src="../Images/c7d89af3165d24fd9f686a11d9e144ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jfrEsXti_uvIt3hs"/></div></div></figure><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="aa33" class="nr kr it nn b gy ns nt l nu nv">print("Pre-training accuracy: %.4f%%" % accuracy)</span></pre><p id="6572" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练前准确率:12.2496%</p><p id="b73c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在是时候训练我们的模型了。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="a652" class="nr kr it nn b gy ns nt l nu nv">from keras.callbacks import ModelCheckpoint <br/>from datetime import datetime </span><span id="361c" class="nr kr it nn b gy nw nt l nu nv">num_epochs = 100<br/>num_batch_size = 32</span><span id="5e7b" class="nr kr it nn b gy nw nt l nu nv">model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), verbose=1)</span></pre><p id="49a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">及时完成的培训:</p><p id="cac9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">甚至在训练完成之前，Comet 就记录了我们实验的关键信息。我们可以从 Comet UI 中实时可视化我们的精度和损耗曲线(注意橙色旋转轮表示训练正在进行中)。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ml"><img src="../Images/89ef68104b8a2c61907ed85c726083b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*20NAeZUT1CkD4oxl"/></div></div></figure><p id="c6b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">彗星的实验可视化仪表板</p><p id="76b3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦经过训练，我们就可以在训练和测试数据上评估我们的模型。</p><pre class="lu lv lw lx gt nm nn no np aw nq bi"><span id="f52c" class="nr kr it nn b gy ns nt l nu nv"># Evaluating the model on the training and testing set<br/>score = model.evaluate(x_train, y_train, verbose=0)<br/>print("Training Accuracy: {0:.2%}".format(score[1]))</span><span id="54c2" class="nr kr it nn b gy nw nt l nu nv">score = model.evaluate(x_test, y_test, verbose=0)<br/>print("Testing Accuracy: {0:.2%}".format(score[1]))</span></pre><p id="3aa5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">训练准确率:93.00% <br/>测试准确率:87.35%</p><h1 id="c768" class="kq kr it bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated">结论</h1><p id="3ad7" class="pw-post-body-paragraph jq jr it js b jt lo jv jw jx lp jz ka kb lq kd ke kf lr kh ki kj ls kl km kn im bi translated">我们的模型已经训练得相当好了，但可能还有很大的改进空间，也许可以使用 Comet 的<a class="ae kp" href="https://www.comet.ml/docs/python-sdk/introduction-optimizer/" rel="noopener ugc nofollow" target="_blank">超参数优化</a>工具。在少量代码中，我们已经能够从音频数据中提取数学上复杂的 MFCC，建立和训练神经网络以基于这些 MFCC 对音频进行分类，并在测试数据上评估我们的模型。</p></div></div>    
</body>
</html>