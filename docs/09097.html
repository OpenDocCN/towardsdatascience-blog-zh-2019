<html>
<head>
<title>Digital Tribes: customer clustering with K-Means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数字部落:用 K 均值进行客户聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/digital-tribes-customer-clustering-with-k-means-6db8580a7a60?source=collection_archive---------16-----------------------#2019-12-03">https://towardsdatascience.com/digital-tribes-customer-clustering-with-k-means-6db8580a7a60?source=collection_archive---------16-----------------------#2019-12-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e989" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">使用 PYTHON 进行客户细分</h2><div class=""/><div class=""><h2 id="96a7" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">了解如何使用强大的机器学习技术来更好地细分您的客户群</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0f98bf08f39c4a1ec03215dbc7e9b86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_rUa0zVJ0wt0Sgea"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@perrygrone?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Perry Grone</a> on <a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="be9e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这篇文章是<a class="ae lh" rel="noopener" target="_blank" href="/divide-and-conquer-segment-your-customers-using-rfm-analysis-68aee749adf6"> <em class="me">分而治之:使用 RFM 分析</em> </a> <em class="me"> </em>故事细分你的客户的后续，在这里我们开始使用经典的 RFM 分析细分我们的用户群。</p><p id="ddc0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一次，我们将探索一些机器学习技术，以获得更多定制和微调的分组。一种方法是使用聚类算法，例如<strong class="lk jd"> K-Means </strong>。</p><h1 id="78f3" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">什么是集群？</h1><p id="d6fc" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">聚类模型是无监督机器学习算法组的一部分。在无监督学习问题中，没有明确的目标变量，例如，我们不试图将客户分为我们事先知道存在的不同类别。相反，该算法使用数据本身的模式来识别和分组相似的观察结果，并找到这些类别。</p><p id="c09a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> K-Means </strong>模型的工作原理是寻找数据集中具有相似属性的不同数据点组(聚类)。它通过以最小化所有点到包含它们的聚类的质心的距离的方式对点进行分组来做到这一点。</p><p id="9de1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">换句话说:该算法的主要目标是找到聚类，使得同一聚类中的数据点之间的距离小于不同聚类中任意两点之间的距离。这样，属于同一组的成员往往具有相似的特征，而与其他组的成员不同。</p><p id="a8c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">解决了这个问题，让我们回到数据分析上来，好吗？</p><h1 id="a140" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">一些特征工程和预处理</h1><p id="9955" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">由于我们试图了解更多关于客户行为模式的信息，我们可以使用这些行为的指示性特征来训练 K-Means 模型。例如，我们可以从使用我们在上一篇文章中计算的<strong class="lk jd">新近度</strong>、<strong class="lk jd">频率</strong>、<strong class="lk jd">货币值</strong>和<strong class="lk jd">任期</strong>开始。</p><p id="c8d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样，我们可以将志同道合的客户聚集在一起，然后分析这些不同的群体，以获得模式和趋势方面的见解，从而帮助制定未来的业务决策。</p><p id="b4ff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们首先从我们在上一篇文章中生成的<strong class="lk jd">客户</strong>数据集中抽取必要的列，并查看特性之间的相关性:</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="dc76" class="nh mg it nd b gy ni nj l nk nl"># retain only the recency, frequency, tenure and monetary columns<br/>rfm = customer[['customer_id', 'recency', 'frequency', 'tenure', 'mean_value', 'total_value']]<br/>rfm.set_index('customer_id', inplace=True)</span><span id="76e9" class="nh mg it nd b gy nm nj l nk nl"># build a feature correlation matrix<br/>rfm_corr = rfm.corr()</span><span id="eaae" class="nh mg it nd b gy nm nj l nk nl">fig, ax = plt.subplots(1, 2, figsize=(12,6))</span><span id="e171" class="nh mg it nd b gy nm nj l nk nl"># create a heatmap to display the correlations<br/>sns.heatmap(rfm_corr, annot=True, ax=ax[0], square=True)<br/>ax[0].set_ylim([5, 0])<br/>ax[0].set_title('Correlation heatmap')<br/>ax[0].set_xlabel('Variables')</span><span id="3f15" class="nh mg it nd b gy nm nj l nk nl"># plot the regrassion line to highlight the strong correlation<br/>sns.regplot(rfm.frequency, rfm.total_value, ax=ax[1])<br/>ax[1].set_title('Frequency x Total value')<br/>ax[1].set_xlabel('Frequency')<br/>ax[1].set_ylabel('Total Value')<br/># Turn off tick labels<br/>ax[1].set_yticklabels([])<br/>ax[1].set_xticklabels([])</span><span id="ddcc" class="nh mg it nd b gy nm nj l nk nl">fig.suptitle('Feature correlations', fontsize=20)<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/f3dd9373eb13fc25986508cfb3041094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJt9V_EFrc0x-T-53Vid7w.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 1: </strong>Feature correlations</figcaption></figure><p id="07d4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可以清楚地看到，<strong class="lk jd"> total_value </strong>和<strong class="lk jd"> frequency </strong>列是非常强相关的(系数为 0.9)。事后看来，你可能会认为这是显而易见的:从长远来看，购买越频繁的客户往往会花更多的钱。虽然，这表明在试图训练任何机器学习模型之前，仔细检查你的数据是多么重要！</p><p id="50b9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">丢弃高度相关的要素是一种很好的做法，因为它们会给数据增加一些冗余，并且可能会影响模型的最终结果。从现在开始，我们将删除<strong class="lk jd"> total_value </strong>，并在分析中使用<strong class="lk jd"> mean_value </strong>列。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="335a" class="nh mg it nd b gy ni nj l nk nl">rfm.drop('total_value', axis=1, inplace=True)</span></pre><p id="d937" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们必须采取的另一个步骤是从数据集中移除离群值。由于<strong class="lk jd"> K-Means </strong>算法<strong class="lk jd"> </strong>依赖于点与点之间距离的计算(大部分时间是简单的欧氏距离)，因此受离群点存在的影响较大。让我们从检查我们的四个特征的盒图开始。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="d9c7" class="nh mg it nd b gy ni nj l nk nl">fig, ax = plt.subplots(2, 2, figsize=(12,6))</span><span id="75de" class="nh mg it nd b gy nm nj l nk nl">sns.boxplot(rfm.recency, orient='v', ax=ax[0][0])<br/>ax[0][0].set_title('Recency')<br/>ax[0][0].set_ylabel('Values')<br/>ax[0][0].set_yticklabels([])<br/>ax[0][0].set_xticklabels([])</span><span id="e47a" class="nh mg it nd b gy nm nj l nk nl">sns.boxplot(rfm.frequency, orient='v', ax=ax[0][1])<br/>ax[0][1].set_title('Frequency')<br/>ax[0][1].set_ylabel('Values')<br/>ax[0][1].set_yticklabels([])<br/>ax[0][1].set_xticklabels([])</span><span id="a2d0" class="nh mg it nd b gy nm nj l nk nl">sns.boxplot(rfm.mean_value, orient='v', ax=ax[1][0])<br/>ax[1][0].set_title('Monetary')<br/>ax[1][0].set_ylabel('Values')<br/>ax[1][0].set_yticklabels([])<br/>ax[1][0].set_xticklabels([])</span><span id="0a82" class="nh mg it nd b gy nm nj l nk nl">sns.boxplot(rfm.tenure, orient='v', ax=ax[1][1])<br/>ax[1][1].set_title('Tenure')<br/>ax[1][1].set_ylabel('Values')<br/>ax[1][1].set_yticklabels([])<br/>ax[1][1].set_xticklabels([])</span><span id="94b9" class="nh mg it nd b gy nm nj l nk nl">plt.tight_layout()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/51f76e1e61b134e6ff8aee97fc29ba0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-8dj5q2MIHNJJxRNEhg12Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 2: </strong>boxplot of features before outlier removal</figcaption></figure><p id="47f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看到在<strong class="lk jd">平均值</strong>和<strong class="lk jd">频率</strong>列的分布中明显存在异常值，其中一些异常值距离其余的点非常远。我们将根据<strong class="lk jd"> Z 分数</strong>移除这些异常值:分数大于 3 的任何点都将被移除，即距离平均值超过 3 个标准偏差的任何点。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="17d0" class="nh mg it nd b gy ni nj l nk nl">from scipy import stats</span><span id="75ce" class="nh mg it nd b gy nm nj l nk nl"># remove outliers based on the Z-score</span><span id="532f" class="nh mg it nd b gy nm nj l nk nl"># For each column, first it computes the Z-score of each value in the column, relative to the column mean and standard deviation. <br/># Then is takes the absolute of Z-score because the direction does not matter, only if it is below the threshold. <br/># The .all(axis=1) ensures that for each row, all column satisfy the constraint. <br/># Finally, result of this condition is used to index the dataframe.<br/>is_inliner = (np.abs(stats.zscore(rfm)) &lt; 3).all(axis=1)</span><span id="4b68" class="nh mg it nd b gy nm nj l nk nl">rfm = rfm[is_inliner]</span></pre><p id="5b66" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后我们可以重复箱线图来检查新的数据分布</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/4eb6357d62258228fa4341214bd503f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loI9XNT-srtWDbgfF9L6Lg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 3:</strong> boxplot of features after outlier removal</figcaption></figure><p id="2e18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好多了！一些异常值将会保留，因为它们没有下降到均值的足够远的位置而被去除。但是，比较有攻击性的都没有了。</p><p id="9847" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们必须采取的最后一个预处理步骤是标准化数据。<strong class="lk jd"> K-Means </strong>算法更适合正态分布的数据。考虑到这一点，我们将使用一个<strong class="lk jd"> log </strong>函数来尝试消除一些数据的偏斜，然后使用来自<strong class="lk jd"> scikit-learn </strong>的<strong class="lk jd">标准定标器</strong>来转换分布，使<strong class="lk jd">均值</strong>接近 0，而<strong class="lk jd">标准差</strong>接近 1。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="e641" class="nh mg it nd b gy ni nj l nk nl"># standardize variables<br/>from sklearn.preprocessing import StandardScaler</span><span id="6e2d" class="nh mg it nd b gy nm nj l nk nl"># take the log to unskew the data<br/>log_rfm = np.log(rfm)<br/># use fit_transfome from StandardScaler to standardize the data (mean=0, std=1)<br/>scaler = StandardScaler()<br/>scaled_rfm_array = scaler.fit_transform(log_rfm)<br/>scaled_rfm = pd.DataFrame(<br/>    scaled_rfm_array,<br/>    columns=rfm.columns,<br/>    index=rfm.index,<br/>)</span></pre><p id="2c87" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们通过最后看一下我们的数据分布来完成预处理</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="22c9" class="nh mg it nd b gy ni nj l nk nl">fig, ax = plt.subplots(2, 2, figsize=(12,6))</span><span id="0b26" class="nh mg it nd b gy nm nj l nk nl">sns.distplot(scaled_rfm.recency, ax=ax[0][0])<br/>ax[0][0].set_title('Recency')<br/>ax[0][0].set_xlabel('Values')</span><span id="f96d" class="nh mg it nd b gy nm nj l nk nl">sns.distplot(scaled_rfm.frequency, ax=ax[0][1])<br/>ax[0][1].set_title('Frequency')<br/>ax[0][1].set_xlabel('Value')</span><span id="82fc" class="nh mg it nd b gy nm nj l nk nl">sns.distplot(scaled_rfm.mean_value, ax=ax[1][0])<br/>ax[1][0].set_title('Monetary')<br/>ax[1][0].set_xlabel('Value')</span><span id="c606" class="nh mg it nd b gy nm nj l nk nl">sns.distplot(scaled_rfm.tenure, ax=ax[1][1])<br/>ax[1][1].set_title('Tenure')<br/>ax[1][1].set_xlabel('Value')</span><span id="4f51" class="nh mg it nd b gy nm nj l nk nl">plt.tight_layout()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/589e10c2ffb0cd26e26eb325bca81d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dYg_4Ry_l9K7iBThKWflLw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 4:</strong> feature distributions</figcaption></figure><p id="a167" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">那看起来足够好了！让我们开始你们期待已久的精彩部分吧。</p><h1 id="3f38" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">最后，真正的交易</h1><p id="96c5" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们终于准备好安装<strong class="lk jd"> K-Means </strong>模型了！然而，K-Means 算法的一个缺点是，您必须预先提供您希望它生成的聚类数。该算法不能从数据中学习该参数。</p><p id="7d6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以从任意设置集群数为 3 开始。这是我们在上一篇文章中用来进一步细分<strong class="lk jd"> RFM 得分</strong>的层级数，感觉这是一个很好的经验法则。</p><p id="10f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> K-Means </strong>是一种迭代算法。预先确定聚类数后，该模型将每个数据点分配给质心最近的聚类。然后重新计算质心，并重复该过程，直到数据点分配没有变化。这意味着我们有效地缩短了距离。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="8d41" class="nh mg it nd b gy ni nj l nk nl"># import kmeans from sklearn<br/>from sklearn.cluster import KMeans</span><span id="5434" class="nh mg it nd b gy nm nj l nk nl"># run kmeans<br/>kmeans = KMeans(n_clusters=2, random_state=1)<br/>kmeans.fit(scaled_rfm)</span><span id="0127" class="nh mg it nd b gy nm nj l nk nl"># extract the cluster labels from thte fitted model<br/>cluster_labels = kmeans.labels_</span></pre><p id="9c37" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">搞定了。这就是训练一个<strong class="lk jd"> K-Means </strong>模型所要做的一切。但是等等，我们已经根据经验将聚类数设置为 3，但是我们如何发现这个数是否最适合我们的数据呢？</p><p id="d14b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以看看各种<strong class="lk jd"> K-Means </strong>模型拟合不同数量的<strong class="lk jd"> K </strong>的<strong class="lk jd">误差平方和(SSE) </strong>。<strong class="lk jd"> SSE </strong>是每个观察值与其聚类平均值之间的平方差之和。它可以用来衡量一个集群内的变化。</p><p id="50f4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> SSE </strong>函数将随着集群数量的增加而单调递减。然而，下降最初非常陡峭，在达到某个值<strong class="lk jd">K</strong><em class="me"/>后，下降开始放缓，形成我们所说的肘形曲线。图的“肘”上的值<strong class="lk jd"> K </strong>(即下降开始减缓的<strong class="lk jd"> K </strong>的值)为我们提供了给定数据集的最佳聚类数的指示。这种方法可以与特定领域的问题和需求知识相结合，以决定要使用的集群数量。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="4a9d" class="nh mg it nd b gy ni nj l nk nl"># find the ideal number of clusters<br/># Fit KMeans and calculate SSE for each k<br/>sse = {}<br/>for k in range(1, 11):<br/>    kmeans = KMeans(n_clusters=k, random_state=1)<br/>    kmeans.fit(scaled_rfm)<br/>    sse[k] = kmeans.inertia_ # sum of squared distances to closest cluster center<br/>    <br/># Plot SSE for each k<br/>plt.title('The Elbow Method')<br/>plt.xlabel('k')<br/>plt.ylabel('SSE') <br/>sns.pointplot(x=list(sse.keys()), y=list(sse.values()))<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e108e5181635cc6f7dcac3fa1144d0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*V8ro2c2DK_6sJko8WlYeOw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 4:</strong> Elbow plot</figcaption></figure><p id="5bc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">看起来 3 或 4 个分类将是更好地代表数据集的分类数。只是为了比较，我们将再运行<strong class="lk jd"> K-Means </strong> 3 次，使其适合 2、3 和 4 个集群。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="c728" class="nh mg it nd b gy ni nj l nk nl"># fit kmeans with 2 clustes<br/>kmeans = KMeans(n_clusters=2, random_state=1)<br/>kmeans.fit(scaled_rfm)<br/># extract the lables<br/>cluster_labels_k2 = kmeans.labels_<br/># assing the cluster labels to the dataset<br/>rfm_k2 = rfm.assign(cluster = cluster_labels_k2)</span><span id="cafc" class="nh mg it nd b gy nm nj l nk nl"># fit kmeans with 3 clustes<br/>kmeans = KMeans(n_clusters=3, random_state=1)<br/>kmeans.fit(scaled_rfm)<br/># extract the lables<br/>cluster_labels_k3 = kmeans.labels_<br/># assing the cluster labels to the dataset<br/>rfm_k3 = rfm.assign(cluster = cluster_labels_k3)</span><span id="d55b" class="nh mg it nd b gy nm nj l nk nl"># fit kmeans with 4 clustes<br/>kmeans = KMeans(n_clusters=4, random_state=1)<br/>kmeans.fit(scaled_rfm)<br/># extract the lables<br/>cluster_labels_k4 = kmeans.labels_<br/># assing the cluster labels to the dataset<br/>rfm_k4 = rfm.assign(cluster = cluster_labels_k4)</span></pre><p id="c3e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们最终得到 3 个不同的数据集:<strong class="lk jd"> rfm_k2 </strong>、<strong class="lk jd"> rfm_k3 </strong>和<strong class="lk jd"> rfm_k4 </strong>，每个数据集都包含每个不同值 o <strong class="lk jd"> K </strong>的聚类信息。</p><h1 id="bb57" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">从集群中构建客户角色</h1><p id="defb" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们可以做的第一件事是创建汇总表来检查每个集群中包含的样本的基本信息。与我们在上一篇文章中所做的类似，我们将包括一些汇总统计数据，比如每个特征的<strong class="lk jd">均值</strong>值，以及分配给每个聚类的样本数。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="f182" class="nh mg it nd b gy ni nj l nk nl"># group the rfm_k2 dataset by the clusters<br/>rfm_k2_summary = rfm_k2.groupby('cluster').agg(<br/>    recency=('recency',  'mean'),<br/>    frequency=('frequency', 'mean'),<br/>    tenure=('tenure', 'mean'),<br/>    monetary=('mean_value', 'mean'),<br/>    samples=('mean_value', 'count')<br/>).round(0)</span><span id="c0ce" class="nh mg it nd b gy nm nj l nk nl"># group the rfm_k3 dataset by the clusters<br/>rfm_k3_summary = rfm_k3.groupby('cluster').agg(<br/>    recency=('recency', 'mean'),<br/>    frequency=('frequency', 'mean'),<br/>    tenure=('tenure', 'mean'),<br/>    monetary=('mean_value', 'mean'),<br/>    samples=('mean_value', 'count')<br/>).round(0)</span><span id="5287" class="nh mg it nd b gy nm nj l nk nl"># group the rfm_k4 dataset by the clusters<br/>rfm_k4_summary = rfm_k4.groupby('cluster').agg(<br/>    recency=('recency',  'mean'),<br/>    frequency=('frequency', 'mean'),<br/>    tenure=('tenure', 'mean'),<br/>    monetary=('mean_value', 'mean'),<br/>    samples=('mean_value', 'count')<br/>).round(0)</span></pre><p id="a622" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一种更好地理解和比较分段的方法是创建所谓的<strong class="lk jd">蛇形图</strong>。该图来自市场研究技术，用于比较不同的细分市场，并提供每个细分市场属性的可视化表示。</p><p id="33cd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要创建此图，我们需要归一化数据(中心和刻度)，然后绘制每个属性的每个聚类的平均归一化值。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="ff5b" class="nh mg it nd b gy ni nj l nk nl"># assign the cluster labes for the scaled rfm<br/>scaled_rfm_k2 = scaled_rfm.copy()<br/>scaled_rfm_k2['cluster'] = rfm_k2.cluster</span><span id="2097" class="nh mg it nd b gy nm nj l nk nl">scaled_rfm_k3 = scaled_rfm.copy()<br/>scaled_rfm_k3['cluster'] = rfm_k3.cluster</span><span id="2367" class="nh mg it nd b gy nm nj l nk nl">scaled_rfm_k4 = scaled_rfm.copy()<br/>scaled_rfm_k4['cluster'] = rfm_k4.cluster</span><span id="57d7" class="nh mg it nd b gy nm nj l nk nl"># melt the dataframes to get the required format<br/>rfm_k2_melt = pd.melt(<br/>    scaled_rfm_k2.reset_index(), <br/>    id_vars=['customer_id', 'cluster'],<br/>    value_vars=['recency', 'frequency', 'mean_value', 'tenure'], <br/>    var_name='attribute',<br/>    value_name='value'<br/>)<br/>rfm_k3_melt = pd.melt(<br/>    scaled_rfm_k3.reset_index(), <br/>    id_vars=['customer_id', 'cluster'],<br/>    value_vars=['recency', 'frequency', 'mean_value', 'tenure'], <br/>    var_name='attribute',<br/>    value_name='value'<br/>)<br/>rfm_k4_melt = pd.melt(<br/>    scaled_rfm_k4.reset_index(), <br/>    id_vars=['customer_id', 'cluster'],<br/>    value_vars=['recency', 'frequency', 'mean_value', 'tenure'], <br/>    var_name='attribute',<br/>    value_name='value'<br/>)</span><span id="18ac" class="nh mg it nd b gy nm nj l nk nl"># plot the snakeplot for each dataset<br/>fig, ax = plt.subplots(1, 3, figsize=(15,5))</span><span id="e718" class="nh mg it nd b gy nm nj l nk nl">sns.lineplot(<br/>    x="attribute",<br/>    y="value",<br/>    hue='cluster',<br/>    data=rfm_k2_melt,<br/>    ax=ax[0],<br/>)<br/>ax[0].set_title('2 Clusters')<br/>ax[0].set_xlabel('Variables')</span><span id="616c" class="nh mg it nd b gy nm nj l nk nl">sns.lineplot(<br/>    x="attribute",<br/>    y="value",<br/>    hue='cluster',<br/>    data=rfm_k3_melt,<br/>    ax=ax[1],<br/>)<br/>ax[1].set_title('3 Clusters')<br/>ax[1].set_xlabel('Variables')</span><span id="869f" class="nh mg it nd b gy nm nj l nk nl">sns.lineplot(<br/>    x="attribute",<br/>    y="value",<br/>    hue='cluster',<br/>    data=rfm_k4_melt,<br/>    ax=ax[2],<br/>)<br/>ax[2].set_title('4 Clusters')<br/>ax[2].set_xlabel('Variables')</span><span id="4f21" class="nh mg it nd b gy nm nj l nk nl">fig.suptitle('Snake plot of standardized variables', fontsize=20)<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nt"><img src="../Images/69c35dc6360efc3b494ff9cd9b35a45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQUeY8btGggJKFP--VgjRA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 6: </strong>Snakeplot</figcaption></figure><p id="32fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了这个<strong class="lk jd">蛇形图</strong>，就更容易看出生成的集群之间的差异。</p><p id="b13f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们可以看到聚类之间差异的最后一件事是检查聚类属性相对于总体的相对重要性。由此，我们可以看出哪个特征在集群的形成中起了更重要的作用。首先，我们需要计算每个聚类的平均值，然后将它们除以总体平均值减 1。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="cd40" class="nh mg it nd b gy ni nj l nk nl"># relative importance of segment attributes<br/>cluster_avg_k2 = rfm_k2.groupby(['cluster']).mean()<br/>cluster_avg_k3 = rfm_k3.groupby(['cluster']).mean()<br/>cluster_avg_k4 = rfm_k4.groupby(['cluster']).mean()</span><span id="81be" class="nh mg it nd b gy nm nj l nk nl">population_avg = rfm.mean()<br/>relative_imp_k2 = cluster_avg_k2 / population_avg - 1<br/>relative_imp_k3 = cluster_avg_k3 / population_avg - 1<br/>relative_imp_k4 = cluster_avg_k4 / population_avg - 1</span><span id="2524" class="nh mg it nd b gy nm nj l nk nl">fig, ax = plt.subplots(1, 3, figsize=(11,5))</span><span id="ad01" class="nh mg it nd b gy nm nj l nk nl">sns.heatmap(<br/>    data=relative_imp_k2,<br/>    annot=True,<br/>    fmt='.2f',<br/>    cmap='RdYlGn',<br/>    linewidths=2,<br/>    square=True,<br/>    ax=ax[0],<br/>)<br/>ax[0].set_ylim([0, 2])<br/>ax[0].set_title('2 Clusters')<br/>ax[0].set_xlabel('Variables')</span><span id="a35c" class="nh mg it nd b gy nm nj l nk nl">sns.heatmap(<br/>    data=relative_imp_k3,<br/>    annot=True,<br/>    fmt='.2f',<br/>    cmap='RdYlGn',<br/>    linewidths=2,<br/>    square=True,<br/>    ax=ax[1],<br/>) <br/>ax[1].set_ylim([0, 3])<br/>ax[1].set_title('3 Clusters')<br/>ax[1].set_xlabel('Variables')</span><span id="364e" class="nh mg it nd b gy nm nj l nk nl">sns.heatmap(<br/>    data=relative_imp_k4,<br/>    annot=True,<br/>    fmt='.2f',<br/>    cmap='RdYlGn',<br/>    linewidths=2,<br/>    square=True,<br/>    ax=ax[2],<br/>) <br/>ax[2].set_ylim([0, 4])<br/>ax[2].set_title('4 Clusters')<br/>ax[2].set_xlabel('Variables')</span><span id="4b70" class="nh mg it nd b gy nm nj l nk nl">fig.suptitle('Relative importance of attributes', fontsize=20)<br/># plt.tight_layout()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/c9c97a4a6e4e37269fb4a346a9e89d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-I0LkV8u1ZtzAAvFfLc0lg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 7:</strong> Relative importance of the attributes</figcaption></figure><p id="e79f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，离 0 越远，该片段的特定特征就越重要。</p><h1 id="7dfc" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">可视化我们的结果</h1><p id="535a" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">这一切都很酷，但是除了查看每个集群的指标之外，我们如何检查我们的结果呢？如果我们能有办法在空间上可视化这些星团会怎么样？但是我们的特征矩阵有 4 个特征，我们不能在一个 4 维空间里画出一些东西…我们能吗？</p><p id="d05d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是也不是！实际上，您不能在 4 维上绘图，但是您可以使用一些巧妙的技巧将数据集的维度减少到 2，然后在常规平面上绘图！</p><p id="9b11" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中一项技术是古老的<strong class="lk jd">主成分分析(PCA) </strong>。有了它，我们可以围绕最高方差的轴旋转和变换数据，然后选择只保留我们需要的<em class="me"> k </em>主成分。在我们的例子中，我们将使用它来旋转多维数据集并将其转换为二维数据集。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="3f41" class="nh mg it nd b gy ni nj l nk nl">from sklearn.decomposition import PCA</span><span id="fcb5" class="nh mg it nd b gy nm nj l nk nl">pca = PCA(n_components=2)</span><span id="1b0e" class="nh mg it nd b gy nm nj l nk nl">pca_array = pca.fit_transform(scaled_rfm)<br/>pca_data = pd.DataFrame(pca_array, columns=['x', 'y'], index=scaled_rfm.index)</span><span id="d0e4" class="nh mg it nd b gy nm nj l nk nl">pca_data['k2'] = rfm_k2.cluster<br/>pca_data['k3'] = rfm_k3.cluster<br/>pca_data['k4'] = rfm_k4.cluster</span><span id="9ca7" class="nh mg it nd b gy nm nj l nk nl">fig, ax = plt.subplots(1, 3, figsize=(15,5))</span><span id="bfd2" class="nh mg it nd b gy nm nj l nk nl">sns.scatterplot(x='x', y='y', hue='k2', data=pca_data, ax=ax[0])<br/>ax[0].set_title('2 Clusters')<br/>ax[0].set_xlabel('X')<br/>ax[0].set_ylabel('Y')</span><span id="3031" class="nh mg it nd b gy nm nj l nk nl">sns.scatterplot(x='x', y='y', hue='k3', data=pca_data, ax=ax[1])<br/>ax[1].set_title('3 Clusters')<br/>ax[1].set_xlabel('X')<br/>ax[1].set_ylabel('Y')</span><span id="be4c" class="nh mg it nd b gy nm nj l nk nl">sns.scatterplot(x='x', y='y', hue='k4', data=pca_data, ax=ax[2])<br/>ax[2].set_title('4 Clusters')<br/>ax[2].set_xlabel('X')<br/>ax[2].set_ylabel('Y')</span><span id="64db" class="nh mg it nd b gy nm nj l nk nl">fig.suptitle('PCA plot of clusters', fontsize=20)<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/6fc84c90fa4567af0f2630f7760aefc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ChkJ286_oF6C_oOv9Ah-Ew.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 8:</strong> Dimensionality reduction with PCA</figcaption></figure><p id="63a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如我们在上面的图中看到的，看起来 3 毕竟是理想的集群数。</p><p id="e712" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">另一种可视化高维数据的技术是<strong class="lk jd">t-分布式随机邻居嵌入(t-SNE) </strong>。它是一种更复杂的非线性技术，涉及联合概率。对该过程内部工作的详细解释超出了本文的范围。然而，由于它也包含在<strong class="lk jd"> scikit-learn </strong>库中，使用它就像我们迄今为止使用的其他模型一样简单。</p><pre class="ks kt ku kv gt nc nd ne nf aw ng bi"><span id="b62c" class="nh mg it nd b gy ni nj l nk nl">from sklearn.manifold import TSNE</span><span id="248c" class="nh mg it nd b gy nm nj l nk nl">tsne = TSNE(learning_rate=300, perplexity=80, early_exaggeration=20)</span><span id="1487" class="nh mg it nd b gy nm nj l nk nl">tsne_array = tsne.fit_transform(scaled_rfm)<br/>tsne_data = pd.DataFrame(tsne_array, columns=['x', 'y'], index=scaled_rfm.index)</span><span id="851f" class="nh mg it nd b gy nm nj l nk nl">tsne_data['k2'] = rfm_k2.cluster<br/>tsne_data['k3'] = rfm_k3.cluster<br/>tsne_data['k4'] = rfm_k4.cluster</span><span id="8aa4" class="nh mg it nd b gy nm nj l nk nl">fig, ax = plt.subplots(1, 3, figsize=(15,5))</span><span id="c936" class="nh mg it nd b gy nm nj l nk nl">sns.scatterplot(x='x', y='y', hue='k2', data=tsne_data, ax=ax[0])<br/>ax[0].set_title('2 Clusters')<br/>ax[0].set_xlabel('X')<br/>ax[0].set_ylabel('Y')</span><span id="6d00" class="nh mg it nd b gy nm nj l nk nl">sns.scatterplot(x='x', y='y', hue='k3', data=tsne_data, ax=ax[1])<br/>ax[1].set_title('3 Clusters')<br/>ax[1].set_xlabel('X')<br/>ax[1].set_ylabel('Y')</span><span id="607b" class="nh mg it nd b gy nm nj l nk nl">sns.scatterplot(x='x', y='y', hue='k4', data=tsne_data, ax=ax[2])<br/>ax[2].set_title('4 Clusters')<br/>ax[2].set_xlabel('X')<br/>ax[2].set_ylabel('Y')</span><span id="9b15" class="nh mg it nd b gy nm nj l nk nl">fig.suptitle('t-SNE plot of clusters', fontsize=20)<br/># plt.tight_layout()<br/>plt.show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/a71b70ec9531f460813170b0de396467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wjra13tD2sqMtFPMa2KeA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><strong class="bd no">Figure 9:</strong> Dimensionality reduction with PCA</figcaption></figure><h1 id="b47f" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">结论</h1><p id="a497" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">这就是我们的客户细分之旅！我希望你喜欢它，并希望在这个过程中学到一些东西。如果你看到了，不要忘记留下一些掌声，请继续关注下一个系列。</p></div></div>    
</body>
</html>