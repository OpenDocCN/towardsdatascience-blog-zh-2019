<html>
<head>
<title>The Anatomy of K-means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K 均值的剖析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397?source=collection_archive---------6-----------------------#2019-02-22">https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397?source=collection_archive---------6-----------------------#2019-02-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e618" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">K 均值聚类算法完全指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/18e1eaaf741ca7488d2281dd543a2835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SWXXKmlOtKcWMx3Y"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@an_ku_sh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ankush Minda</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6995" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您想要根据内容和主题对数百(或数千)个文档进行分类，或者出于某种原因您希望将不同的图像组合在一起。或者更重要的是，让我们假设您已经对相同的数据进行了分类，但是您想要挑战这种标记。您想知道数据分类是否有意义，或者是否可以改进。</p><p id="1dab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，我的建议是你把你的数据聚集起来。信息经常被噪声和冗余所掩盖，将数据分组到具有相似特征的簇(聚类)是一种有效的方式来照亮一些信息。</p><p id="9177" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">聚类</strong>是一种广泛用于寻找具有相似特征的观察组(称为聚类)的技术。这个过程不是由特定的目的驱动的，这意味着您不必特别告诉您的算法如何对这些观察值进行分组，因为它会自己进行分组(组是有机形成的)。结果是同一组中的观察值(或数据点)之间比另一组中的其他观察值更相似。目标是获得相同组中尽可能相似的数据点，以及不同组中尽可能不相似的数据点。</p><p id="00c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means 非常适合探索性分析，是了解数据和提供几乎所有数据类型的见解的完美工具。无论是图像、图形还是文本，K-means 都非常灵活，几乎可以处理任何内容。</p><h1 id="48bd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">无监督学习领域的巨星之一</strong></h1><p id="fb0e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">聚类(包括 K 均值聚类)是一种用于数据分类的无监督学习技术。</p><p id="adc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">无监督学习</strong>意味着没有输出变量来指导学习过程(没有这个或那个，没有对错)，数据由算法探索，以找到模式。我们只观察特征，但没有既定的结果衡量标准，因为我们想找出它们。</p><p id="abc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与监督学习相反，在监督学习中，你的现有数据已经被标记，并且你知道你想要在你获得的新数据中确定哪种行为，无监督学习技术不使用标记数据，算法自行发现数据中的结构。</p><p id="7e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在聚类技术领域中，<strong class="lb iu"> K-means </strong>可能是最广为人知和最常用的方法之一。K-means 使用迭代优化方法，根据用户定义的聚类数(由变量<em class="ms"> K </em>表示)和数据集产生最终的聚类。例如，如果您将 K 设置为等于 3，那么您的数据集将被分组为 3 个簇，如果您将 K 设置为等于 4，那么您将数据分组为 4 个簇，依此类推。</p><p id="b9ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means 从任意选择的数据点开始，如建议的数据组的<strong class="lb iu"> means </strong>，并迭代地重新计算新的均值，以便收敛到数据点的最终聚类。</p><p id="ea3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果您只是提供一个值(K)，算法如何决定如何对数据进行分组呢？当你定义 K 的值时，你实际上是在告诉算法你想要多少个均值或<em class="ms">质心</em>(如果你设置 K=3，你创建了 3 个均值或质心，这占了 3 个聚类)。<strong class="lb iu">质心</strong>是代表聚类中心(平均值)的数据点，它不一定是数据集的成员。</p><p id="458b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法是这样工作的:</p><ol class=""><li id="0ea0" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">随机创建 K 个质心(基于预定义的 K 值)</li><li id="5f67" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">K-means 将数据集中的每个数据点分配到最近的质心(最小化它们之间的欧几里德距离)，这意味着如果一个数据点比任何其他质心都更靠近该聚类的质心，则该数据点被视为在特定的聚类中</li><li id="c109" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">然后，K-means 通过取分配给质心的聚类的所有数据点的平均值来重新计算质心，从而相对于前一步骤减少了总的聚类内方差。K-means 中的“means”指的是平均数据并找到新的质心</li><li id="e2e4" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">该算法在步骤 2 和 3 之间迭代，直到满足某些标准(例如，数据点和它们对应的质心之间的距离之和最小化、达到最大迭代次数、质心值没有变化或者没有数据点改变聚类)</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/33da2235484d5ad702c84376f3a54bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*GePJBQORYP8sLKRE"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">In this example, after 5 iterations the calculated centroids remain the same, and data points are not switching clusters anymore (the algorithm converges). Here, each centroid is shown as a dark coloured data point. Source: <a class="ae ky" href="http://ai.stanford.edu" rel="noopener ugc nofollow" target="_blank">http://ai.stanford.edu</a></figcaption></figure><p id="68d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行该算法的初始结果可能不是最佳结果，使用不同的随机起始质心重新运行该算法可能会提供更好的性能(不同的初始对象可能会产生不同的聚类结果)。因此，通常的做法是以不同的起点运行算法多次，并评估不同的启动方法(例如，Forgy 或 Kaufman 方法)。</p><p id="bc75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是另一个问题出现了:你如何知道 K 的正确值，或者要创建多少个质心？对此没有通用的答案，尽管质心或聚类的最佳数量不是先验已知的，但是存在不同的方法来尝试估计它。一种常用的方法是测试不同数量的聚类，并测量所得的误差平方和，选择 K 值，在该值处增加将导致误差和非常小的减少，而减少将急剧增加误差和。定义最佳聚类数的这个点被称为<strong class="lb iu">“肘点”</strong>，并且可以被用作寻找 k 值的最佳选择的视觉测量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f608c743ad206d1af2cbf564ee17c2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/0*k27usELCoFFnztEr"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">In this example, the elbow point is located in 3 clusters</figcaption></figure><p id="4589" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-means 是您的数据科学工具包中的必备工具，这有几个原因。首先，它易于实施并带来高效的 T2 性能。毕竟，您只需要定义一个参数(K 的值)就可以看到结果。它也是<strong class="lb iu">快</strong>，并且与<strong class="lb iu">大数据集</strong>配合得非常好，使它能够处理当前的海量数据。它非常灵活，几乎可以用于任何数据类型，而且它的结果比其他算法更容易解释。此外，该算法非常受欢迎，以至于你可以在几乎任何学科中找到用例及实现。</p><h1 id="2046" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">但是任何事情都有不利的一面</h1><p id="8603" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">然而，K-means 存在一些缺点。第一个是你需要定义<strong class="lb iu">个集群，</strong>这个决定会严重影响结果。此外，由于初始质心的位置是随机的，结果可能不可比，并显示出缺乏一致性。K-means 生成具有<strong class="lb iu">统一大小</strong>的聚类(每个聚类具有大致相等数量的观察值)，即使数据可能以不同的方式表现，并且它对异常值和噪声数据非常敏感。此外，它假设每个聚类中的数据点被建模为位于该聚类质心周围的球面内(<strong class="lb iu">球面限制</strong>)，但是当违反该条件(或任何先前的条件)时，该算法会以非直观的方式表现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/1d9bdbda240aa4b6618dda251f441fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0-eXD1OOacqurX8t"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example 1</figcaption></figure><p id="3b4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例 1: </strong>在左侧是数据的直观聚类，两组数据点之间有明显的分隔(一个小环被一个大环包围)。在右侧，相同的数据点由 K-means 算法聚类(K 值为 2)，其中每个质心用菱形表示。如您所见，该算法无法识别直观的聚类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/b6d66e548d4197b79f8327c7cb23d332.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*khUvajEXVSFgxbBQ"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example 2</figcaption></figure><p id="3d59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">例 2: </strong>左手边是两个可识别数据组的聚类。在右侧，相同数据点上的 K-means 聚类的结果不符合直观的聚类。与示例 1 的情况一样，由于算法的球面限制，K-means 创建的分区不能反映我们视觉识别的内容。它试图寻找周围有整齐数据球体的质心，当星团的几何形状偏离球体时，它的表现很差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/b64a16b18d46401798772db37984b463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i7gZBM79AcFvkYSi"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example 3</figcaption></figure><p id="951c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">示例 3: </strong>同样，在左侧有 K-means 未能识别的两个清晰的聚类(一个小且紧密的数据组和另一个较大且分散的数据组)(右侧)。在这里，为了平衡两个数据组之间的聚类内距离并生成大小一致的聚类，该算法混合两个数据组并创建两个不代表数据集的人工聚类。</p><p id="c09f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，K-means 不允许彼此远离的数据点共享同一个聚类，不管这些数据点之间的关系可能有多明显。</p><h1 id="87dd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">现在该怎么办？</strong></h1><p id="59af" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">事实是，现实生活中的数据几乎总是复杂的、杂乱的和嘈杂的。现实世界中的情况很少反映出应用现成算法的明确条件。在 K-means 算法的情况下，预计<em class="ms">至少有一个假设被违反，因此我们不仅需要识别这一点，还需要知道在这种情况下该做什么。</em></p><p id="d7fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好消息是有替代方案，缺陷可以被纠正。比如将数据转换为<a class="ae ky" href="https://www.r-bloggers.com/exploring-assumptions-of-k-means-clustering-using-r/" rel="noopener ugc nofollow" target="_blank">极坐标</a>可以解决我们在例 1 中描述的球面限制。如果您发现严重的局限性，也可以考虑使用其他类型的聚类算法。可能的方法是使用基于<a class="ae ky" href="https://www.datascience.com/blog/k-means-alternatives" rel="noopener ugc nofollow" target="_blank">密度的</a>或基于<a class="ae ky" href="https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html" rel="noopener ugc nofollow" target="_blank">层次的</a>算法，它们修复了一些 K-means 的局限性(但也有它们自己的局限性)。</p><p id="3529" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，K-means 是一个很棒的算法，有很多潜在的用途，所以它可以用于几乎任何类型的数据分组。但是从来没有免费的午餐:如果你不想被引导到错误的结果，你需要了解它的假设和运作方式。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="22e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢<a class="ae ky" href="https://medium.com/@Binsi" rel="noopener"> Sabrina Steinert </a>的宝贵意见</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><blockquote class="nt nu nv"><p id="9beb" class="kz la ms lb b lc ld ju le lf lg jx lh nw lj lk ll nx ln lo lp ny lr ls lt lu im bi translated">对这些话题感兴趣？在 Linkedin<a class="ae ky" href="https://www.linkedin.com/in/lopezyse/" rel="noopener ugc nofollow" target="_blank">或 Twitter </a>上关注我</p></blockquote></div></div>    
</body>
</html>