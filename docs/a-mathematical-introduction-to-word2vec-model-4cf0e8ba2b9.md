# word2vec 模型的数学介绍

> 原文：<https://towardsdatascience.com/a-mathematical-introduction-to-word2vec-model-4cf0e8ba2b9?source=collection_archive---------9----------------------->

![](img/803bd648b1198a727459249da55d0bd7.png)

Image by Gerhard Gellinger from Pixabay

作为 NLP 项目的一部分，我最近不得不处理 Mikolov 等人在 2013 年开发的著名的 word2vec 算法。在网上有很多关于这个主题的教程和指南，有些更侧重于理论，有些则有实现的例子。
然而，我感兴趣的不仅仅是学习这种方法背后的理论，还有理解算法的实现细节，这样我就可以自己编写并正确地重现算法了([此处](https://github.com/acapitanelli/word-embedding))。从这个意义上说，我找不到真正对我有帮助的东西，所以我准备了两篇有条理的文章:在这篇文章中，我讨论模型的理论，而[在这里](https://medium.com/@andrea.capitanelli/how-to-train-the-word2vec-model-24704d842ec3)我展示如何通过神经网络具体训练模型。

*注:我发现在 Medium 上使用 latex 是相当不满意和讨厌的，所以我选择了使用手绘配方(和方案)。我提前为我的书法道歉。*

*Word2vec* 已经成为一种非常流行的**单词嵌入**的方法。单词嵌入意味着单词用实值向量表示，因此可以像处理任何其他数学向量一样处理它们。一种从文本、基于字符串的域到向量空间的变换，只需要少量的规范运算(主要是和与减)。

![](img/a1af7811be30e5e7f6599546a36a1fc7.png)

A change of domain

## 为什么要嵌入单词？

通过单词嵌入，我们可以用数学方法处理文本中的单词。如果两个单词有相似的意思，它们的向量是接近的:单词嵌入是一种相似性的**度量**，这使得不仅单词，而且句子和整个文本都有可能相关联。这在各种 NLP 任务和系统中非常有用，例如信息检索、情感分析或推荐系统。

## 报纸

2013 年，Mikolov 等人提出了两篇开创性的论文[1] [2]和两种不同的单词嵌入模型，即*连续单词袋* (CBOW)和*跳格*模型。它们在本质上没有太大区别，在下面的讨论中，我将集中讨论后者。

两个模型的假设都是**分布假设:**出现在相同上下文中的单词往往有相似的意思(Harris，1954)。也就是说，一个词的特点是它的同伴。

假设我们的文本由一系列单词组成:

![](img/39aff7c27f92b4c365dbf9c1fa1e1dfd.png)

Text as a word sequence of words

那么对于单词 *wⱼ， *wⱼ* 的*上下文由其左右邻域给出:

![](img/c21fe95248c996b86cff4907509e0a8d.png)

其中 *M* 是上下文窗口的一半大小。

然后，给每个单词 *w* 分配一个向量表示 *v，*和概率，即 *wₒ* 在 *wᵢ* 的上下文中被定义为它们的向量乘积的 *softmax* :

![](img/091c3c38588bcf956b9baa7460daac21.png)

Output probability is given by softmax of vector product

跳格模型的目标是**预测中心词**的上下文。因此，训练模型意味着找到使目标函数最大化的一组 *v* :

![](img/55701cd047206c34b1a7d34c177c0735.png)

Objective function

等效地，这意味着最小化*损失*函数，该函数看起来是在语料库上平均的交叉熵(因为真实分布仅在中心词的上下文中等于 1):

![](img/80b961cf9c9fa0fc79be29f27431bf40.png)

到目前为止，这就是我们需要知道的关于 vanilla *wordvec* 的所有信息。

然而，这种需要考虑每个单词的上下文的所有概率的方法，对于通常包括数十亿个单词的相当大的语料库来说是不可分析处理的。因此 Mikolov 的小组提出了一些提高计算效率的有效措施。令人惊讶的是，它们还改善了嵌入结果。让我们看看他们。

## 单词子采样

这很简单。我们不是以简单的方式考虑文本中的所有单词，而是给每个单词分配一个概率，这个概率与它出现的频率成反比。保持概率由下式给出:

![](img/e783557b731609fb71f01059ec87eecb.png)

因此，在生成训练数据的过程中，**常用词将更有可能被丢弃**，从而减少要处理的数据量。由于这种词通常带来有限的信息量(想想像“the”、“of”、“to”这样的词)，从语料库中删除一些词还可以提高文本的质量，并允许专注于更有意义的术语。

这里显示的保持概率公式包含在 Google 代码实现中，它与原始文章中报告的略有不同。感谢克里斯·麦考密克指出了这一点。

## 负采样

相反，这种技术似乎有点晦涩难懂，但它已被证明非常有效。

负抽样只考虑少量样本来评估跳格概率。样本被称为*阴性*，因为它们是不属于 *wᵢ* 的上下文的单词(因此，模型应该理想地将概率分配为零)。

负采样源于*噪声对比估计* (NCE)技术；其基本思想是将多项分类问题转化为二元分类问题；

*   **多项式分类问题** *。*该模型使用 softmax 函数估计输出单词的真实概率分布。
*   **二元分类** **问题** *。*对于每个训练样本，模型被输入一个*正*对(一个中心单词和出现在其上下文中的另一个单词)和少量的*负*对(中心单词和从词汇表中随机选择的单词)。该模型学习区分真实对和负面对。

对于负采样，目标函数变为:

![](img/352ca32241937b167f6d42e68b30eecb.png)

Objective function with negative sampling

其中σ是逻辑( *sigmoid* )函数:

![](img/4dceca71f7ae7790824a9ba23e8bc7c2.png)

Sigmoid function

与 *softmax* 不同， *sigmoid* 是一个单参数函数，因此不需要同时评估所有输出值。

## 为什么负抽样效果这么好？

这个问题仍在讨论中。分布假设看起来是正确的，负采样通过增加中心词和上下文词之间的相似性来利用它。戈德堡和 Levy⁴认为，二次抽样也可能产生有益的影响。事实上，通过从语料库中消除频繁出现的单词，我们有选择地**扩展了上下文**的大小，因此我们可以掌握与信息性单词的关系，否则这些信息性单词将留在窗口之外。

## 参考

[1] Mikolov 等，2013，[向量空间中单词表示的高效估计。](https://arxiv.org/pdf/1301.3781.pdf)

[2] Mikolov 等，2013，[词和短语的分布式表征及其组合性](https://arxiv.org/pdf/1310.4546.pdf)。

[3][https://mccormickml . com/2017/01/11/word 2 vec-tutorial-part-2-negative-sampling/](https://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

[4] Goldberg，Levy，2014， [word2vec 解释:推导 Mikolov 等人的负采样单词嵌入法](https://arxiv.org/pdf/1402.3722.pdf)。