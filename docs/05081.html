<html>
<head>
<title>ConvNet Playground: An Interactive Visualization Tool for Exploring Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ConvNet Playground:探索卷积神经网络的交互式可视化工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/convnetplayground-979d441ebf82?source=collection_archive---------16-----------------------#2019-07-30">https://towardsdatascience.com/convnetplayground-979d441ebf82?source=collection_archive---------16-----------------------#2019-07-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5ed2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">探索应用于语义图像搜索任务的 CNN，并查看由预训练模型学习的模式的可视化。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/83fccd367643b4c7c2a5a7009dd28745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFxZLO8BNNt26gAHwIik5w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">ConvNet Playground is an interactive visualization for exploring Convolutional Neural Networks applied to the task of semantic image search. It allows you explore the performance of multiple pre-trained CNN architectures (and intermediate models based on each architecture) for feature extraction on images across various datasets.</figcaption></figure><p id="cbc3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated">对于数据科学家和工程师来说，重用预先训练的模型所学习的特征(迁移学习)是相当常见的。当这在实践中完成时，几个问题出现了——我使用什么模型架构？我是使用整个模型还是模型的子集？每个模型中的层学习什么模式，它们最适合什么任务？建立如何导航这些选择的直觉需要大量的实验，这可能是计算和时间密集型的。</p><p id="f7c2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了帮助解决这个问题，<a class="ae ma" href="http://convnetplayground.fastforwardlabs.com" rel="noopener ugc nofollow" target="_blank"> ConvNet Playground </a>提供了一个环境，以支持探索和学习的方式探索这些选择的组合(结果)。我们希望该工具将有助于对 CNN 如何工作建立直觉感兴趣的爱好者、教授 CNN 的教育者和对展示 CNN 在图像分析任务中的价值感兴趣的技术专家。ConvNet Playground 是<a class="ae ma" href="http://experiments.fastforwardlabs.com" rel="noopener ugc nofollow" target="_blank">我们在<a class="ae ma" href="https://www.cloudera.com/products/fast-forward-labs-research.html" rel="noopener ugc nofollow" target="_blank"> Cloudera 快进实验室</a>建造的几个原型</a>之一，以帮助我们的客户建立对机器学习如何工作的直觉。</p><p id="4d89" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意:与该工具的某些部分交互并理解它们需要一些机器学习概念的知识。如果你想了解 CNN 的一些背景知识，推荐<a class="ae ma" href="https://colah.github.io/posts/2014-07-Conv-Nets-Modular/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>和<a class="ae ma" href="http://cs231n.stanford.edu" rel="noopener ugc nofollow" target="_blank">课程</a>。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="4f3c" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">语义搜索——一个简单的实现</h1><p id="b957" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">ConvNet Playground 专注于使用 CNN 进行语义图像搜索的任务。我们的(相当简单的)方法分两个阶段实现:(1)我们使用预先训练的 CNN(想想 VGG16、InceptionV3 等)从我们的数据集中的所有图像中提取特征。在 i <a class="ae ma" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> mageNet </a>)上进行预训练(ii。)我们计算相似性作为这些特征之间距离的度量。<em class="nf">好的</em>模型将提取正确捕获<em class="nf">相似性</em>的特征——相似图像的特征靠得很近，不相似图像的特征很好..相距甚远。当然，相似性是一个不断变化的概念，对于每个任务和相应的数据集可能会有所不同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/646f359af9a41d9deaff4c65d4611f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfNVDveet0VNjN6l4mitVQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">To implement semantic search, we start by extracting features from images in our dataset using pre-trained CNN models. Similarity is computed as the distance between these features.</figcaption></figure><h2 id="9a1e" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">执行搜索查询</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/83fccd367643b4c7c2a5a7009dd28745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFxZLO8BNNt26gAHwIik5w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The user can select an image and view a list of the top 15 images which are most similar to the selected image.</figcaption></figure><p id="cf95" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用户可以执行搜索查询(单击默认数据集中的图像)，默认模型(InceptionV3)会确定与所选模型最相似的 15 个图像的列表。搜索界面还提供了一个<strong class="kx ir"><em class="nf"/></strong>搜索结果分数，它是与所选图像属于同一类别的返回结果的百分比，根据其在结果列表中的位置进行加权，即，与在列表末尾的正确结果相比，排列在结果列表顶部的正确结果得到更多分数。例如，对于是香蕉图像的搜索查询，如果所有返回的图像都属于同一类别(香蕉)，则搜索分数将是 100%。请注意，这个分数是保守的——一些图片可能属于不同的类别，但<em class="nf">也类似</em>(例如，轿车、甲壳虫、法拉利<em class="nf">都是</em>汽车)。搜索分数可用于轻松比较每个搜索配置的性能。</p><h2 id="0fc6" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">修改搜索配置(高级选项)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/82a8fa2dc4574975b79f4112690555e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-ODFShmHTUzTf2VoU3Tqg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Advanced options allows the user select datasets of varied complexity, select models and distance metrics for computing similarity. As each configuration is changed, the search results are updated in real time.</figcaption></figure><p id="8292" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了让用户进一步探索 CNN 的性能，用户可以使用<strong class="kx ir"> <em class="nf">高级选项</em> </strong>开关修改他们的搜索配置——数据集、模型架构、距离度量。随着每个配置选项的修改，用户可以实时查看搜索性能的变化。</p><p id="21c1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">数据集:</strong>用户可以从 4 个不同复杂程度(图像内容、图像大小)的数据集中选择一个。<strong class="kx ir"> <em class="nf"> Iconic200 </em> </strong>是从 Flickr (creative commons)收集的真实世界图像的数据集。它包含跨越 10 个关键词搜索的图像(拱门、香蕉、大众甲壳虫、埃菲尔铁塔、帝国大厦、法拉利、皮卡、轿车、巨石阵、拖拉机)。<strong class="kx ir"> <em class="nf"> Fashion200 </em> </strong>是来自<a class="ae ma" href="https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle 时尚产品图片数据集</a>的 200 张图片(10 个类别)的真实时尚单品的集合。图像的最大宽度为 300 像素。<strong class="kx ir"><em class="nf">tinyimagenet 200</em></strong>包含 64px * 64px 图像，是<a class="ae ma" href="https://tiny-imagenet.herokuapp.com/" rel="noopener ugc nofollow" target="_blank"> Tiny Imagenet 视觉识别挑战数据集的子集。</a> <strong class="kx ir"> <em class="nf"> Cifar10 </em> </strong>是流行的<a class="ae ma" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> cifar10 </a>数据集的子集(200 张图像)，包含来自 10 个随机选择的类的 20 张图像。每幅图像的尺寸为 32px 32px。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/73ae37e90b23913f6b8fff6e5fc687ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vQpCbmsa7iCa3ZVWTR0l9A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">ConvNet Playground allows users to perform search queries on 4 datasets of varying complexity (content, resolution).</figcaption></figure><p id="dfb9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">模型和层(中间模型):</strong>我们提供了九个模型(vgg16、vgg19、mobilenet、efficientnetb0、efficientnetb5、xception、resnet50、inceptionv3、densenet121)的结果，以及使用每个模型的 8 个层构建的一组中间模型。我们只用了八层，主要是为了减轻观众的认知负担，便于视觉比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/50428f36dd8c22a378bdaf47bf4e6e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HZ2L-qLHUwlU0PkbpaB6A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The user can chose which models or intermediate models are used for feature extraction. Intermediate models are a subset of the main pre-trained model constructed from a given layer.</figcaption></figure><p id="23d5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，选择使用哪些层的<em class="nf">取决于以下因素——首先，我们关注具有可训练参数的卷积层，并且我们在每个模型中包括第一个和最后一个卷积层。最后，我们选择中间的六个卷积层作为随机样本。这些模型按照复杂性(参数数量)递增的顺序呈现，并显示出它们在生成正确识别相似图像的特征的能力方面的显著差异。</em></p><p id="696b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">距离度量:</strong>我们提供了使用四种不同的距离度量来测量从每个数据集中的所有图像中提取的特征之间的相似性的结果。这些包括余弦、欧几里德、平方欧几里德和闵可维距离。</p><h2 id="801d" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">嵌入的 UMAP 可视化</h2><p id="afbc" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">为了探索各种模型如何捕捉每个数据集的相似性，用户可以查看使用每个模型提取的特征的<a class="ae ma" href="https://arxiv.org/abs/1802.03426" rel="noopener ugc nofollow" target="_blank"> UMAP </a>可视化。例如，很好地捕捉相似性的模型生成嵌入，其中每个类中的图像被聚集在一起，并且与其他不相同的类很好地分开。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/eb92c817915ac22bfc743bf104706c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yz_fg2Fy1nfIMDUyKiGCLA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Users can explore a UMAP visualization of embeddings extracted using each model. In the image above, the selected model (Inception V3, Layer 310) does a decent job of clustering each category in the dataset.</figcaption></figure><h2 id="594f" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">比较每个查询的模型性能</h2><p id="4877" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">compare models 视图提供了一个界面，用于跨所有模型体系结构比较给定查询的性能(搜索结果得分)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/14874015bcf8185b8dea14589edba4d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eR99mCkaf2rtNQrNVqrc6w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">For each query, users can view graphs of how how all other models (and intermediate models) perform.</figcaption></figure><h2 id="92bd" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">每个模型中各层学习到的模式的可视化</h2><p id="a00d" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">更好地理解为什么从模型中的层构造的中间模型对于不同的数据集表现不同的一种方法是检查在每个层学习的模式/特征。ConvNet Playground 通过<a class="ae ma" href="https://convnetplayground.fastforwardlabs.com/#/models" rel="noopener ugc nofollow" target="_blank"> Model Explorer </a>视图支持这种检查。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/5777f6145c1a4f77a6a9bf30e026b352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K_UxMjAJYKktgvgj-nmxFg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The <a class="ae ma" href="https://convnetplayground.fastforwardlabs.com/#/models" rel="noopener ugc nofollow" target="_blank">Model Explorer </a>view allows users to explore patterns learned by layers in a pre-trained CNN model. In the example above, we see that channels in layer 2 in the VGG16 model have learned to detect colors and edges.</figcaption></figure><h1 id="5612" class="mi mj iq bd mk ml ny mn mo mp nz mr ms jw oa jx mu jz ob ka mw kc oc kd my mz bi translated">一些反思</h1><h2 id="1ffe" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">你可以用预先训练好的模型走得很远</h2><p id="c9ac" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">ConvNet Playground 中的所有特征提取都是使用在<a class="ae ma" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> imageNet </a>上预先训练的模型架构来执行的。没有进行微调。由于预先训练的模型可以提供很好的价值，并作为实验的一个令人信服的起点，下一个问题是您使用模型的什么层或子集？</p><h2 id="1c71" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">我使用哪一层进行特征提取。</h2><p id="08c5" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">当重用表示(迁移学习)时，自由地使用整个模型(仅删除完全连接的层)或模型的子集可能会令人困惑。实现这一点的一种方法是考虑在每一层学到的模式对手头任务的适用性。例如，如果您的任务与低级特征相关(例如，检测形状、颜色、边缘)，则后续层(例如，眼睛、面部、物体的一部分)学习的高级概念不太可能有用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/8bb655e35b80cc09fc2bcf6dcf81ce62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkBzgHvpdDb8vFJgflcaiw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">a.) Layer 2 in the VGG model, which has learned to detect colors and edges, incorrectly returns images with similar colors. b.) Layer 7 in an Inceptionv3 model, which has also learned to detect colors and edges, correctly returns of shirts with similar colors and edges. Insight: when the meaning of similarity is simple (e.g. colors and edges), intermediate models constructed from early layers work well, have fewer parameters and should be considered.</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/8bb655e35b80cc09fc2bcf6dcf81ce62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mkBzgHvpdDb8vFJgflcaiw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">a.) Layer 2 in the VGG model, which has learned to detect colors and edges, incorrectly returns images with similar colors. b.) Layer 7 in an Inceptionv3 model, which has also learned to detect colors and edges, correctly returns of shirts with similar colors and edges. Insight: when the meaning of similarity is simple (e.g. colors and edges), intermediate models constructed from early layers work well, have fewer parameters and should be considered.</figcaption></figure><p id="7599" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，他们可以利用提取的嵌入的 UMAP 可视化来探索这些模型在捕捉各种数据集的相似性概念方面的表现。</p><h2 id="690f" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">AutoML/NAS 型号极具竞争力！！</h2><p id="3d5d" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">通常的做法是使用 VGG16(前 5 名精度为<strong class="kx ir"> 92.6 </strong>)作为图像分析任务的基础。在凉爽的<strong class="kx ir">138 米</strong>称重，VGG16 是参数低效。相比之下，<a class="ae ma" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"> EfficientNetB0 </a>，一个神经架构搜索(NAS，AutoML)模型，在<strong class="kx ir"> 5.3M </strong>参数(前 5 名精度<strong class="kx ir"> 93.2) </strong>中加权。ConvNet Playground 集成了两个 EfficientNet 模型变体，并将其应用于语义搜索(EfficientNetB0 5.3M 参数和 EfficientNetB5 30.6M 参数)..它们真的很好用，<a class="ae ma" href="http://convnetplayground.fastforwardlabs.com/" rel="noopener ugc nofollow" target="_blank">去试试吧</a>！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/109f86501443ff588d4a74ac1b03e7ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a9hMKXtjCqgqQT4y9GiBrQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Top performing models for image analysis between 2012 and 2019. We see that model architectures designed using neural architecture search approaches (autoML) models have become competitive in the last few years. Not just that, they achieve this with less parameters.</figcaption></figure><h2 id="9330" class="nh mj iq bd mk ni nj dn mo nk nl dp ms le nm nn mu li no np mw lm nq nr my ns bi translated">一些神经元/层学习非常奇怪的模式</h2><p id="e801" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">如果你对挖掘预训练模型中的层感兴趣，并探索每个架构中的层学习了什么样的模式，那么…有足够多的东西让你忙起来！如果你发现一些有趣的东西(比如下面的频道已经学会在 densenet121 中明确检测眼睛的<a class="ae ma" href="http://convnetplayground.fastforwardlabs.com/#/models?model=densenet121&amp;layer=conv4_block9_2_conv&amp;channel=5" rel="noopener ugc nofollow" target="_blank">模式)..一定要在推特上分享！</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/1b15564cd6c11ad9a8c433855ff58cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4N9jDlfOZTLWu5r83znwUQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This <a class="ae ma" href="http://convnetplayground.fastforwardlabs.com/#/models?model=densenet121&amp;layer=conv4_block9_2_conv&amp;channel=5" rel="noopener ugc nofollow" target="_blank">eye pattern learned by Channel 5, Layer 202</a> in densenet121. Check out the <a class="ae ma" href="http://convnetplayground.fastforwardlabs.com/#/models" rel="noopener ugc nofollow" target="_blank">Model Explorer view</a>, you might find some even more interesting patterns learned by other models.</figcaption></figure><p id="c6de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">请注意，该原型中的方法(使用来自预训练模型的特征)相对简单，因此具有局限性(规模、在搜索查询中匹配多个对象、准确性)。在实践中，我们可以通过微调给定数据集的专门模型来扩展这种实现，利用附加信息(例如文本描述、交互日志、购买日志等)来构建更有意义的嵌入，或者使用两阶段方法来匹配搜索查询中的多个对象(提取对象裁剪并用作搜索查询)。</p><h1 id="9e0d" class="mi mj iq bd mk ml ny mn mo mp nz mr ms jw oa jx mu jz ob ka mw kc oc kd my mz bi translated">承认</h1><p id="1b7c" class="pw-post-body-paragraph kv kw iq kx b ky na jr la lb nb ju ld le nc lg lh li nd lk ll lm ne lo lp lq ij bi translated">ConvNet Playground 建立在许多相关项目和工具的基础上，旨在通过交互式体验使神经网络更容易访问。这些相关的项目包括<a class="ae ma" href="https://distill.pub/2017/feature-visualization/" rel="noopener ugc nofollow" target="_blank">特征可视化、</a> <a class="ae ma" href="https://playground.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow 游乐场、</a> <a class="ae ma" href="https://teachablemachine.withgoogle.com/" rel="noopener ugc nofollow" target="_blank">可教机器、</a> <a class="ae ma" href="https://transcranial.github.io/keras-js/#/" rel="noopener ugc nofollow" target="_blank"> Keras.js </a>等等。ConvNet Playground 使用<a class="ae ma" href="https://github.com/tensorflow/lucid/tree/master/lucid" rel="noopener ugc nofollow" target="_blank"> lucid </a>和<a class="ae ma" href="https://github.com/totti0223/lucid4keras" rel="noopener ugc nofollow" target="_blank"> lucid4keras </a>库来可视化层中通道学习到的特征，<a class="ae ma" href="https://anseki.github.io/leader-line/" rel="noopener ugc nofollow" target="_blank"> leader-line </a>用于绘制 svg 线，<a class="ae ma" href="https://www.carbondesignsystem.com/" rel="noopener ugc nofollow" target="_blank"> carbon </a>设计系统用于布局。本项目中使用的预训练模型来自<a class="ae ma" href="https://keras.io/applications/" rel="noopener ugc nofollow" target="_blank"> Keras 应用</a>模块，EfficientNets 实施来自<a class="ae ma" href="https://github.com/qubvel/efficientnet" rel="noopener ugc nofollow" target="_blank">此处</a>。感谢格兰特·卡斯特对 Three.js 的宝贵指点，以及快进实验室团队的其他成员的反馈和指导！</p><p id="bebb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ConvNet Playground 是我们最近在 Cloudera Fast Forward Labs 进行的关于用于图像分析的<strong class="kx ir">深度学习</strong>的研究报告的一部分。我们的报告更加详细(什么任务用什么模型，如何选择一个好的模型，OSS 工具和框架，可解释性等等)。).<a class="ae ma" href="https://www.cloudera.com/products/fast-forward-labs-research.html" rel="noopener ugc nofollow" target="_blank">如果您有兴趣获取完整报告(可通过订阅我们的研究和咨询服务获得)，请联系</a>。</p><p id="8aff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你觉得这个工具有用或者有反馈要分享，请随时联系我们。</p></div></div>    
</body>
</html>