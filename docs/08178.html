<html>
<head>
<title>The Abundance of Entropies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熵的丰富</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-abundance-of-entropies-6cd613139c82?source=collection_archive---------21-----------------------#2019-11-08">https://towardsdatascience.com/the-abundance-of-entropies-6cd613139c82?source=collection_archive---------21-----------------------#2019-11-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7220" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解熵的不同类型。</h2></div><p id="b93b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我曾经在不恰当的时候谈论信息论和香农，一般来说，你会发现我在随机的社交活动中脱口而出关于香农的信息。</p><p id="4244" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我不应该说双关语，我显然不擅长！</p><h1 id="b3e0" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">信息信息无处不在！</h1><blockquote class="lt"><p id="dc39" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated">有些人认为，通过研究另一天的科学发现，我们可以学会如何做出发现。另一方面，一位圣人观察到我们从历史中学不到任何东西，除了我们从历史中学不到任何东西，亨利·福特断言历史是一派胡言。</p><p id="286c" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated"><em class="md"> -约翰·R·皮尔斯，</em> <a class="ae me" href="https://www.amazon.com/Introduction-Information-Theory-Symbols-Mathematics/dp/0486240614" rel="noopener ugc nofollow" target="_blank"> <em class="md">信息论导论</em> </a></p></blockquote><p id="01d8" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">约翰·R·皮尔斯的《信息论导论》这本书很美。它并没有很高的技术性，事实上，它是我爱上的一个数学理论的令人耳目一新的写照。</p><p id="bfde" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他认为，许多强大的科学发现不是通过对自然现象的研究产生的，而是通过对人造设备中的现象的研究产生的。</p><p id="be66" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mk">熵</em>。在任何与信息论相关的课程中，你都会经常听到这个神秘的词。事实上，如果你从事过机器学习，你一定非常熟悉交叉熵这个术语！！<br/>尽管如此，我们一定都在某个时候疑惑过— <em class="mk">熵不是在热力学中使用的吗？</em></p><h2 id="270e" class="ml lc iq bd ld mm mn dn lh mo mp dp ll ko mq mr ln ks ms mt lp kw mu mv lr mw bi translated">通信中使用的熵不同于统计力学或热力学中使用的熵。他们正在处理的问题完全不同。</h2><h1 id="5f30" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">热力学中的熵</h1><p id="e14c" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Rudolf_Clausius" rel="noopener ugc nofollow" target="_blank">鲁道夫·克劳修斯</a>在 19 世纪 50 年代为热力学定义了熵。气体的熵取决于它的温度、体积和质量。熵是可逆性的指标，即当熵没有变化时，过程是可逆的。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/28bf784f691c8edac38cde327a676a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*FPBZdZkaxXL-cCvXRO357A.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Boltzmann’s formula for entropy of gas.</figcaption></figure><p id="be5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不可逆的物理现象总是涉及熵的增加。有趣的是，这最初是由路德维希·玻尔兹曼在 1872 年到 1875 年间提出的！玻尔兹曼公式是热力学<a class="ae me" href="https://en.wikipedia.org/wiki/Entropy_(statistical_thermodynamics)" rel="noopener ugc nofollow" target="_blank">熵</a>最概括的公式。</p><blockquote class="lt"><p id="5d11" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated">因此，熵的增加意味着我们将热能转化为机械能的能力下降。熵的增加意味着可用能量的减少。</p><p id="f026" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated">——约翰·R·皮尔斯，<a class="ae me" href="https://www.amazon.com/Introduction-Information-Theory-Symbols-Mathematics/dp/0486240614" rel="noopener ugc nofollow" target="_blank">信息论导论</a></p></blockquote><h1 id="c918" class="lb lc iq bd ld le lf lg lh li lj lk ll jw no jx ln jz np ka lp kc nq kd lr ls bi translated">统计力学中的熵</h1><blockquote class="lt"><p id="6b4c" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated"><em class="md">虽然热力学给了我们熵的概念，但它并没有给出熵的详细物理图像，例如在分子的位置和速度方面。统计力学在特殊情况下确实给出了熵的详细的力学含义。</em></p><p id="01ef" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated">——约翰·R·皮尔斯，<a class="ae me" href="https://www.amazon.com/Introduction-Information-Theory-Symbols-Mathematics/dp/0486240614" rel="noopener ugc nofollow" target="_blank">信息论导论</a></p></blockquote><p id="25a1" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">在这里，我们认识到熵的增加意味着有序性的降低或不确定性的增加。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/040714ff3427821aec8d7ede9ff5de7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*GeMueZzqAJVsn9S_B_WxJw.png"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">where K is Boltzmann’s constant and Pi is the probability of being occupied.</figcaption></figure><h1 id="bd29" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">传播理论中的熵</h1><p id="282f" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">传播理论或者当时叫什么信息论:</p><ul class=""><li id="6abb" class="ns nt iq kh b ki kj kl km ko nu ks nv kw nw la nx ny nz oa bi translated">信息来源:作家，演讲者任何能产生信息的东西</li><li id="d717" class="ns nt iq kh b ki ob kl oc ko od ks oe kw of la nx ny nz oa bi translated">消息:数据，如文本文档或你想发送给某人的东西。</li></ul><p id="cded" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着关于实际将产生什么消息的不确定性变得越来越大，消息所传达的信息量也在增加。我们来看看。你有两个朋友。一个人(A 人)不停地给你发迷因，另一个人是一个普通人(B 人)，他有广泛的话题可聊。当你在手机上听到一声哔哔声，如果是 A 发来的，不确定性会减少，因为你知道信息的熵会减少。类似地，人 B 可以给你发送任何东西，因此你不知道信息会是关于什么的，不确定性是越来越高的熵。</p><p id="84f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果是人 A，你已经认为它是一个迷因，当你打开它发现它是一个迷因。这是可以预见的，你打开它会得到一些信息。另一方面，打开不可预测的来自 B 的信息会给你更多的信息。</p><p id="1a1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们也可以看看刽子手的例子。如果你能猜出名字中比“of”或“the”更罕见的词，你会更了解这是哪部电影/书。</p><p id="27f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，高熵意味着更多的信息</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a6a718981e8e681e9da1fd60a09be886.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*75l99SDKZBQDcKikuQ5hdA.png"/></div></figure><p id="8155" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/fb2fb51c3aef606b8350b7f27be79a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*CcOLqvZwXJjPiq_m1aEkRQ.png"/></div></figure><p id="90a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意到它们在数学上是多么的相似了吗？</p><h1 id="7ba4" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">交叉熵</h1><p id="aa8a" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">在机器学习中，交叉熵通常被用作损失函数。它是给定随机变量的两个概率分布之间的差值。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a77b6a7d082ea6cdc7f6253ed99a5d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*ADDCKsq8gGRuak8Kwot4eA.png"/></div></figure><blockquote class="oj ok ol"><p id="a67d" class="kf kg mk kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">交叉熵</em> </strong> <em class="iq">:代表事件的平均总位数，来自 Q 而不是 p</em></p><p id="b1eb" class="kf kg mk kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><em class="iq">Ref:</em><a class="ae me" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://machinelingmastery . com/cross-entropy-for-machine-learning/</em></a></p></blockquote><p id="1b6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以当我们用交叉熵作为损失函数时</p><blockquote class="oj ok ol"><p id="626e" class="kf kg mk kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><em class="iq">期望概率(y):数据集中一个例子的每个类标签的已知概率(P)。<br/>预测概率(yhat):由模型(Q)预测的每一类标签实例的概率。</em></p><p id="169a" class="kf kg mk kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><em class="iq">因此，我们可以使用上述交叉熵计算来估计单个预测的交叉熵；</em></p><p id="f86b" class="kf kg mk kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><em class="iq">其中</em> X <em class="iq">中的每个</em> x <em class="iq">是可以分配给该示例的类标签，并且</em> P(x) <em class="iq">对于已知标签将是 1，对于所有其他标签将是 0。</em></p><p id="bbaf" class="kf kg mk kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><em class="iq">Ref:</em><a class="ae me" href="https://machinelearningmastery.com/cross-entropy-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"><em class="iq">https://machinelingmastery . com/cross-entropy-for-machine-learning/</em></a></p></blockquote><h1 id="7b73" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">将信息与知识联系起来</h1><p id="470d" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">文字，人类不断玩弄文字。当信息与知识联系在一起时，就会出现一些问题。我们不顾一切地把这个和统计力学联系起来！通信理论起源于电子通信，而不是统计力学。</p><h1 id="7be5" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">外卖？</h1><ul class=""><li id="e1a6" class="ns nt iq kh b ki mx kl my ko op ks oq kw or la nx ny nz oa bi translated">尽管有些人可能认为研究科学发现的历史是多余的，但科学发现是通过观察人造设备中的现象而发生的</li><li id="0150" class="ns nt iq kh b ki ob kl oc ko od ks oe kw of la nx ny nz oa bi translated">通信中使用的熵之所以这么叫是因为数学类比，它与统计力学或热力学中定义的熵有很大不同。事实上，他们试图解决不同的问题。当热力学熵试图处理一个过程的可逆性时，它的统计力学对等物是根据分子的位置和速度来处理熵。通信中的熵是关于信息的，更多的熵，更多的信息</li><li id="78f6" class="ns nt iq kh b ki ob kl oc ko od ks oe kw of la nx ny nz oa bi translated">这里的信息与你传统的知识关联略有不同</li></ul></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="feb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mk">原载于 2019 年 11 月 8 日 https://megstalks.com</em><em class="mk">的</em><a class="ae me" href="https://megstalks.com/2019/11/08/origins-of-information-theory-abundance-of-entropies/" rel="noopener ugc nofollow" target="_blank">T22。</a></p></div></div>    
</body>
</html>