<html>
<head>
<title>Rules-of-thumb for building a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建神经网络的经验法则</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af?source=collection_archive---------5-----------------------#2019-07-22">https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af?source=collection_archive---------5-----------------------#2019-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="43ee" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在本文中，我们将获得一个构建初始神经网络的起点。我们将学习经验法则，例如隐藏层数、节点数、激活等。，并查看 TensorFlow 2 中的实现。</h2></div><p id="b5d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">&lt;<download the="" free="" book="" class="ae lb" href="https://www.understandingdeeplearning.com" rel="noopener ugc nofollow" target="_blank">了解深度学习，了解更多&gt; &gt;</download></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/f321e60b7eebca07477ae7353bfd7cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HNZiZrzJ9JcNJ_SuQx1ioQ.jpeg"/></div></div><figcaption class="lo lp gj gh gi lq lr bd b be z dk">Photo by <a class="ae lb" href="https://unsplash.com/@dewang?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Dewang Gupta</a> on <a class="ae lb" href="https://unsplash.com/search/photos/rules-of-thumb?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4811" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习提供了各种各样的模型。有了它们，我们可以建立极其精确的预测模型。然而，由于设置参数种类繁多，要找到一个起点可能会令人望而生畏。</p><p id="7a51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将找到一个构建神经网络的起点，更具体地说是一个多层感知器的例子，但它的大部分是普遍适用的。</p><p id="e9ed" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">这里的想法是回顾经验法则来建立第一个神经网络模型。如果第一个模型的性能合理(最低可接受的精度)，则对其进行调优和优化。否则，最好从问题、数据或不同的方法中寻找。</strong></p><p id="ba49" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面，我们有，</p><ul class=""><li id="dbf8" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">构建神经网络的经验法则，以及</li><li id="db08" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">它们在 TensorFlow 2 中用于二进制分类的实现代码。</li></ul><h1 id="3055" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">神经网络</h1><p id="931b" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">神经网络随着 CNN、RNNs 等有了巨大的进步。随着时间的推移，每种疾病又发展出几种亚型。随着每一项发展，我们都成功地提高了我们的预测能力。</p><p id="39ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但与此同时，我们成功地让寻找模型构建的起点变得更加困难。</p><p id="cd92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个模型都有自己的天价索赔。周围有这么多广告牌，很容易分心。</p><p id="417f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的内容中，我们将通过使用一些经验法则来构建第一个模型。</p><h1 id="8f32" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">经验法则</h1><p id="1f48" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">我们有各种各样的神经网络。其中，一个多层感知器就是深度学习的“hello world”。因此，当你学习或开发深度学习的新模型时，这是一个很好的起点。</p><p id="b934" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是建造 MLP 的经验法则。然而，它们中的大多数在其他深度学习模型上是适用的。</p><ol class=""><li id="51cb" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la nd ly lz ma bi translated"><strong class="kh ir">层数</strong>:从两个隐藏层开始(不包括最后一层)。</li><li id="22e0" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">中间层的节点数(大小)</strong>:来自 2 的几何级数的数，如 4，8，16，32，…第一层应该是输入数据要素数量的一半左右。下一层的大小是上一层的一半。</li><li id="f876" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">用于分类的输出层的节点数(大小):</strong>如果是二进制分类，则大小为 1。对于多类分类器，大小是类的数量。</li><li id="cc5b" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">回归输出层的大小</strong>:如果是单个响应，则大小为 1。对于多响应回归，大小是响应的数量。</li><li id="1f3d" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">中间层激活</strong>:使用<code class="fe ne nf ng nh b">relu</code>激活。</li><li id="8ea9" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">输出层激活:</strong>使用<code class="fe ne nf ng nh b">sigmoid</code>进行二元分类，<code class="fe ne nf ng nh b">softmax</code>进行多类分类器，使用<code class="fe ne nf ng nh b">linear</code>进行回归。对于自动编码器，如果输入数据是连续的，最后一层应该是<code class="fe ne nf ng nh b">linear</code>，否则，对于二进制或多级分类输入应该是<code class="fe ne nf ng nh b">sigmoid</code>或<code class="fe ne nf ng nh b">softmax</code>。</li><li id="6f62" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir"> Dropout layers: </strong>在除输入层(如果单独定义输入层)以外的每一层后添加 Dropout。将<strong class="kh ir">辍学率设置为 0.5 </strong>。辍学率&gt; 0.5 是适得其反的。如果您认为 0.5 的比率会正则化太多的结点，则增加图层的大小，而不是将辍学率降低到 0.5 以下。我更喜欢不在输入层设置任何下降。但是如果你觉得必须这么做，把辍学率设为 0.2。</li><li id="26db" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">数据预处理</strong>:我假设您的预测值<em class="ni"> X </em>是数值型的，并且您已经将任何分类列转换为 one-hot-encoding。在使用数据进行模型定型之前，请执行数据缩放。使用<code class="fe ne nf ng nh b">sklearn.preprocessing</code>中的<code class="fe ne nf ng nh b">MinMaxScaler</code>。如果这样做效果不好，请将<code class="fe ne nf ng nh b">StandardScaler</code>放在同一个库中。回归中的<em class="ni"> y </em>需要缩放。</li><li id="cf96" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">分割数据至训练，有效，测试</strong>:使用<code class="fe ne nf ng nh b">sklearn.model_selection</code>中的<code class="fe ne nf ng nh b">train_test_split</code>。参见下面的例子。</li><li id="b82d" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">类别权重:</strong>如果您有不平衡的数据，那么设置类别权重以平衡您的<code class="fe ne nf ng nh b">model.fit</code>中的损失。对于二元分类器，权重应为:{ 0:1 的数量/数据大小，1:0 的数量/数据大小}。对于极度不平衡的数据(罕见事件)，类权重可能不起作用。谨慎添加。</li><li id="88e7" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">优化器</strong>:使用默认学习率的<code class="fe ne nf ng nh b">adam</code>。</li><li id="3674" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">分类丢失:</strong>对于二元分类使用<code class="fe ne nf ng nh b">binary_crossentropy</code>。对于多类，如果标签是一个热编码，使用<code class="fe ne nf ng nh b">categorical_crossentropy</code>，否则如果标签是整数，使用<code class="fe ne nf ng nh b">sparse_categorical_crossentropy</code>。</li><li id="a0ea" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">回归损失</strong>:使用<code class="fe ne nf ng nh b">mse</code>。</li><li id="948f" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">分类标准:</strong>使用<code class="fe ne nf ng nh b">accuracy</code>显示正确分类的百分比。对于不平衡数据，还包括<code class="fe ne nf ng nh b">tf.keras.metrics.Recall()</code>和<code class="fe ne nf ng nh b">tf.keras.metrics.FalsePositives()</code>。</li><li id="c529" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">回归度量</strong>:使用<code class="fe ne nf ng nh b">tf.keras.metrics.RootMeanSquaredError()</code>。</li><li id="061c" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">历元</strong>:从 20 开始，查看模型训练是否显示损失减少和精度提高。如果 20 个时代没有最小的成功，继续前进。如果你取得了一些微小的成功，把 epoch 设为 100。</li><li id="2a7e" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">批量</strong>:从 2 的几何级数中选择批量。对于不平衡的数据集有更大的值，如 128，否则从 16 开始。</li></ol><h2 id="a9c2" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">高级从业者的额外费用很少</h2><ol class=""><li id="a60a" class="ls lt iq kh b ki my kl mz ko nv ks nw kw nx la nd ly lz ma bi translated"><strong class="kh ir">振荡损耗</strong>:如果你在训练时遇到振荡损耗，那么就有收敛问题。尝试降低学习率和/或更改批量大小。</li><li id="bc36" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">过采样和欠采样</strong>:如果数据不平衡，使用<code class="fe ne nf ng nh b"><a class="ae lb" href="https://imbalanced-learn.readthedocs.io/en/stable/api.html#module-imblearn.over_sampling" rel="noopener ugc nofollow" target="_blank">imblearn.over_sampling</a></code>中的<code class="fe ne nf ng nh b">SMOTE</code>。</li><li id="148b" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">曲线移动:</strong>如果你必须进行移动预测，例如早期预测，使用曲线移动。下面显示了一个实现<code class="fe ne nf ng nh b">curve_shift</code>。</li><li id="8148" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">自定义度量</strong>:不平衡二进制分类的一个重要度量是误报率。如下面的<code class="fe ne nf ng nh b">class FalsePositiveRate()</code>实现所示，您可以构建这个以及类似的其他定制指标。</li><li id="0766" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la nd ly lz ma bi translated"><strong class="kh ir">卢瑟激活</strong> : <code class="fe ne nf ng nh b">selu</code>激活被认为优于所有其他现有激活。我并不总是注意到这一点，但是如果你想使用<code class="fe ne nf ng nh b">selu</code>激活，那么就使用<code class="fe ne nf ng nh b">kernel_initializer=’lecun_normal’</code>和<code class="fe ne nf ng nh b">AlphaDropout</code>。在<code class="fe ne nf ng nh b">AlphaDropout</code>中使用速率为 0.1，<code class="fe ne nf ng nh b">AlphaDropout(0.1)</code>。示例实现如下所示。</li></ol><h1 id="3a42" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">TensorFlow 2 中的多层感知器(MLP)示例</h1><p id="857e" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">我已经在我以前的文章中使用的纸张断裂数据集上实现了 MLP 神经网络(参见<a class="ae lb" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">使用 Keras </a>中的自动编码器进行极端罕见事件分类)。</p><p id="a0c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个实现中，我们将看到我们在上面的经验法则中提到的元素的例子。</p><p id="77cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现在 TensorFlow 2 中完成。如果还没有迁移到 TensorFlow 2，我强烈推荐迁移到 tensor flow 2。它具有 Keras 的所有简单性和显著更好的计算效率。</p><p id="1e01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">按照<a class="ae lb" rel="noopener" target="_blank" href="/step-by-step-guide-to-install-tensorflow-2-0-67bc73e79b82">分步指南安装 Tensorflow 2 </a>进行安装。</p><p id="9112" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下文中，我并不试图找到最佳模式。想法是学习实现。为了简洁，没有跳过任何步骤。相反，这些步骤是冗长的，以帮助读者直接应用它们。</p><h2 id="cfd5" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">图书馆</h2><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="a7b4" class="nj mh iq nh b gy oc od l oe of">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="c3e5" class="nj mh iq nh b gy og od l oe of">import pandas as pd<br/>import numpy as np<br/>from pylab import rcParams</span><span id="1d5a" class="nj mh iq nh b gy og od l oe of">from collections import Counter</span><span id="c30c" class="nj mh iq nh b gy og od l oe of">import tensorflow as tf</span><span id="a9ff" class="nj mh iq nh b gy og od l oe of">from tensorflow.keras import optimizers<br/>from tensorflow.keras.models import Model, load_model, Sequential<br/>from tensorflow.keras.layers import Input, Dense, Dropout, AlphaDropout<br/>from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard</span><span id="6acb" class="nj mh iq nh b gy og od l oe of">from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix, precision_recall_curve<br/>from sklearn.metrics import recall_score, classification_report, auc, roc_curve<br/>from sklearn.metrics import precision_recall_fscore_support, f1_score</span><span id="1cee" class="nj mh iq nh b gy og od l oe of">from numpy.random import seed<br/>seed(1)</span><span id="1220" class="nj mh iq nh b gy og od l oe of">SEED = 123 #used to help randomly select the data points<br/>DATA_SPLIT_PCT = 0.2</span><span id="2576" class="nj mh iq nh b gy og od l oe of">rcParams['figure.figsize'] = 8, 6<br/>LABELS = ["Normal","Break"]</span></pre><p id="de97" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要测试您是否使用了正确的 TensorFlow 版本，请运行以下命令，</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="fc66" class="nj mh iq nh b gy oc od l oe of">tf.__version__</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oh"><img src="../Images/efd2b430807c70b7d692b7340c2a7cf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6EC6DaB0UhXKgYvOBhyrQ.png"/></div></div></figure><h2 id="e20f" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">读取和准备数据</h2><p id="e4ce" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">在这里下载数据<a class="ae lb" href="https://docs.google.com/forms/d/e/1FAIpQLSdyUk3lfDl7I5KYK_pw285LCApc-_RcoC0Tf9cnDnZ_TWzPAw/viewform" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="da0f" class="nj mh iq nh b gy oc od l oe of">'''<br/>Download data here:<br/><a class="ae lb" href="https://docs.google.com/forms/d/e/1FAIpQLSdyUk3lfDl7I5KYK_pw285LCApc-_RcoC0Tf9cnDnZ_TWzPAw/viewform" rel="noopener ugc nofollow" target="_blank">https://docs.google.com/forms/d/e/1FAIpQLSdyUk3lfDl7I5KYK_pw285LCApc-_RcoC0Tf9cnDnZ_TWzPAw/viewform</a><br/>'''<br/>df = pd.read_csv("data/processminer-rare-event-mts - data.csv") <br/>df.head(n=5)  # visualize the data.</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oi"><img src="../Images/2d47d35d1831ea2912fa499d0f62032f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ws2vd_USZMJWS5MjgZ76XA.png"/></div></div></figure><h2 id="7261" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">将分类列转换为一次性编码</h2><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="8be0" class="nj mh iq nh b gy oc od l oe of">hotencoding1 = pd.get_dummies(df['x28'])  # Grade&amp;Bwt<br/>hotencoding1 = hotencoding1.add_prefix('grade_')<br/>hotencoding2 = pd.get_dummies(df['x61'])  # EventPress<br/>hotencoding2 = hotencoding2.add_prefix('eventpress_')</span><span id="30f1" class="nj mh iq nh b gy og od l oe of">df=df.drop(['x28', 'x61'], axis=1)</span><span id="ce94" class="nj mh iq nh b gy og od l oe of">df=pd.concat([df, hotencoding1, hotencoding2], axis=1)</span></pre><h2 id="7313" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">曲线移动</h2><p id="fd78" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">这是一个时间序列数据，其中我们必须提前预测事件(y = 1)。在此数据中，连续的行相隔 2 分钟。我们将把列<code class="fe ne nf ng nh b">y</code>中的标签移动 2 行，进行 4 分钟预测。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="9e31" class="nj mh iq nh b gy oc od l oe of">sign = lambda x: (1, -1)[x &lt; 0]</span><span id="a6e5" class="nj mh iq nh b gy og od l oe of">def curve_shift(df, shift_by):<br/>    '''<br/>    This function will shift the binary labels in a dataframe.<br/>    The curve shift will be with respect to the 1s. <br/>    For example, if shift is -2, the following process<br/>    will happen: if row n is labeled as 1, then<br/>    - Make row (n+shift_by):(n+shift_by-1) = 1.<br/>    - Remove row n.<br/>    i.e. the labels will be shifted up to 2 rows up.<br/>    <br/>    Inputs:<br/>    df       A pandas dataframe with a binary labeled column. <br/>             This labeled column should be named as 'y'.<br/>    shift_by An integer denoting the number of rows to shift.<br/>    <br/>    Output<br/>    df       A dataframe with the binary labels shifted by shift.<br/>    '''</span><span id="40c1" class="nj mh iq nh b gy og od l oe of">vector = df['y'].copy()<br/>    for s in range(abs(shift_by)):<br/>        tmp = vector.shift(sign(shift_by))<br/>        tmp = tmp.fillna(0)<br/>        vector += tmp<br/>    labelcol = 'y'<br/>    # Add vector to the df<br/>    df.insert(loc=0, column=labelcol+'tmp', value=vector)<br/>    # Remove the rows with labelcol == 1.<br/>    df = df.drop(df[df[labelcol] == 1].index)<br/>    # Drop labelcol and rename the tmp col as labelcol<br/>    df = df.drop(labelcol, axis=1)<br/>    df = df.rename(columns={labelcol+'tmp': labelcol})<br/>    # Make the labelcol binary<br/>    df.loc[df[labelcol] &gt; 0, labelcol] = 1</span><span id="db7d" class="nj mh iq nh b gy og od l oe of">return df</span></pre><p id="c99c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上移两行，</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="9eb7" class="nj mh iq nh b gy oc od l oe of">df = curve_shift(df, shift_by = -2)</span></pre><p id="728d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在删除时间列。从这里开始就不需要了。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="b7de" class="nj mh iq nh b gy oc od l oe of">df = df.drop(['time'], axis=1)</span></pre><h2 id="2454" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">将数据分为训练、有效和测试</h2><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="f007" class="nj mh iq nh b gy oc od l oe of">df_train, df_test = train_test_split(df, test_size=DATA_SPLIT_PCT, random_state=SEED)<br/>df_train, df_valid = train_test_split(df_train, test_size=DATA_SPLIT_PCT, random_state=SEED)</span></pre><p id="f992" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">把 X 和 y 分开。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="511d" class="nj mh iq nh b gy oc od l oe of">x_train = df_train.drop(['y'], axis=1)<br/>y_train = df_train.y.values</span><span id="512a" class="nj mh iq nh b gy og od l oe of">x_valid = df_valid.drop(['y'], axis=1)<br/>y_valid = df_valid.y.values</span><span id="b662" class="nj mh iq nh b gy og od l oe of">x_test = df_test.drop(['y'], axis=1)<br/>y_test = df_test.y</span></pre><h2 id="84d0" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">数据缩放</h2><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="0733" class="nj mh iq nh b gy oc od l oe of">scaler = MinMaxScaler().fit(x_train)<br/># <em class="ni">scaler = StandardScaler().fit(x_train)</em><br/>x_train_scaled = scaler.transform(x_train)<br/>x_valid_scaled = scaler.transform(x_valid)<br/>x_test_scaled = scaler.transform(x_test)</span></pre><h2 id="82c8" class="nj mh iq bd mi nk nl dn mm nm nn dp mq ko no np ms ks nq nr mu kw ns nt mw nu bi translated">MLP 模型</h2><p id="408d" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated"><strong class="kh ir">自定义指标:false position verate()</strong></p><p id="8724" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将开发一个<code class="fe ne nf ng nh b">FalsePositiveRate()</code>指标，用于下面的每个模型。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="405f" class="nj mh iq nh b gy oc od l oe of">class FalsePositiveRate(tf.keras.metrics.Metric):<br/>    def __init__(self, name='false_positive_rate', **kwargs):<br/>        super(FalsePositiveRate, self).__init__(name=name, **kwargs)<br/>        self.negatives = self.add_weight(name='negatives', initializer='zeros')<br/>        self.false_positives = self.add_weight(name='false_negatives', initializer='zeros')<br/>        <br/>    def update_state(self, y_true, y_pred, sample_weight=None):<br/>        '''<br/>        Arguments:<br/>        y_true  The actual y. Passed by default to Metric classes.<br/>        y_pred  The predicted y. Passed by default to Metric classes.<br/>        <br/>        '''<br/>        # Compute the number of negatives.<br/>        y_true = tf.cast(y_true, tf.bool)<br/>        <br/>        negatives = tf.reduce_sum(tf.cast(tf.equal(y_true, False), self.dtype))<br/>        <br/>        self.negatives.assign_add(negatives)<br/>        <br/>        # Compute the number of false positives.<br/>        y_pred = tf.greater_equal(y_pred, 0.5)  # Using default threshold of 0.5 to call a prediction as positive labeled.<br/>        <br/>        false_positive_values = tf.logical_and(tf.equal(y_true, False), tf.equal(y_pred, True)) <br/>        false_positive_values = tf.cast(false_positive_values, self.dtype)<br/>        if sample_weight is not None:<br/>            sample_weight = tf.cast(sample_weight, self.dtype)<br/>            sample_weight = tf.broadcast_weights(sample_weight, values)<br/>            values = tf.multiply(false_positive_values, sample_weight)<br/>        <br/>        false_positives = tf.reduce_sum(false_positive_values)<br/>        <br/>        self.false_positives.assign_add(false_positives)<br/>        <br/>    def result(self):<br/>        return tf.divide(self.false_positives, self.negatives)</span></pre><p id="f5a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">自定义性能绘图功能</strong></p><p id="379a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将编写两个绘图函数来可视化损失进度和准确性度量。我们将在下面构建的模型中使用它们。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="b30e" class="nj mh iq nh b gy oc od l oe of">def plot_loss(model_history):<br/>    train_loss=[value for key, value in model_history.items() if 'loss' in key.lower()][0]<br/>    valid_loss=[value for key, value in model_history.items() if 'loss' in key.lower()][1]</span><span id="d564" class="nj mh iq nh b gy og od l oe of">fig, ax1 = plt.subplots()</span><span id="8c4e" class="nj mh iq nh b gy og od l oe of">color = 'tab:blue'<br/>    ax1.set_xlabel('Epoch')<br/>    ax1.set_ylabel('Loss', color=color)<br/>    ax1.plot(train_loss, '--', color=color, label='Train Loss')<br/>    ax1.plot(valid_loss, color=color, label='Valid Loss')<br/>    ax1.tick_params(axis='y', labelcolor=color)<br/>    plt.legend(loc='upper left')<br/>    plt.title('Model Loss')</span><span id="8a6a" class="nj mh iq nh b gy og od l oe of">plt.show()</span><span id="0cd1" class="nj mh iq nh b gy og od l oe of">def plot_model_recall_fpr(model_history):<br/>    train_recall=[value for key, value in model_history.items() if 'recall' in key.lower()][0]<br/>    valid_recall=[value for key, value in model_history.items() if 'recall' in key.lower()][1]</span><span id="18af" class="nj mh iq nh b gy og od l oe of">train_fpr=[value for key, value in model_history.items() if 'false_positive_rate' in key.lower()][0]<br/>    valid_fpr=[value for key, value in model_history.items() if 'false_positive_rate' in key.lower()][1]</span><span id="a777" class="nj mh iq nh b gy og od l oe of">fig, ax1 = plt.subplots()</span><span id="14aa" class="nj mh iq nh b gy og od l oe of">color = 'tab:red'<br/>    ax1.set_xlabel('Epoch')<br/>    ax1.set_ylabel('Recall', color=color)<br/>    ax1.set_ylim([-0.05,1.05])<br/>    ax1.plot(train_recall, '--', color=color, label='Train Recall')<br/>    ax1.plot(valid_recall, color=color, label='Valid Recall')<br/>    ax1.tick_params(axis='y', labelcolor=color)<br/>    plt.legend(loc='upper left')<br/>    plt.title('Model Recall and FPR')</span><span id="3de7" class="nj mh iq nh b gy og od l oe of">ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis</span><span id="4aad" class="nj mh iq nh b gy og od l oe of">color = 'tab:blue'<br/>    ax2.set_ylabel('False Positive Rate', color=color)  # we already handled the x-label with ax1<br/>    ax2.plot(train_fpr, '--', color=color, label='Train FPR')<br/>    ax2.plot(valid_fpr, color=color, label='Valid FPR')<br/>    ax2.tick_params(axis='y', labelcolor=color)<br/>    ax2.set_ylim([-0.05,1.05])</span><span id="ec95" class="nj mh iq nh b gy og od l oe of">fig.tight_layout()  # otherwise the right y-label is slightly clipped<br/>    plt.legend(loc='upper right')<br/>    plt.show()</span></pre><p id="19e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">型号 1。基线。</strong></p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="841a" class="nj mh iq nh b gy oc od l oe of">n_features = x_train_scaled.shape[1]<br/>mlp = Sequential()<br/>mlp.add(Input(shape=(n_features, )))<br/>mlp.add(Dense(32, activation='relu'))<br/>mlp.add(Dense(16, activation='relu'))<br/>mlp.add(Dense(1, activation='sigmoid'))<br/>mlp.summary()</span><span id="684b" class="nj mh iq nh b gy og od l oe of">mlp.compile(optimizer='adam',<br/>            loss='binary_crossentropy',<br/>            metrics=['accuracy', tf.keras.metrics.Recall(), FalsePositiveRate()]<br/>           )</span><span id="af07" class="nj mh iq nh b gy og od l oe of">history = mlp.fit(x=x_train_scaled,<br/>                  y=y_train,<br/>                  batch_size=128,<br/>                  epochs=100,<br/>                  validation_data=(x_valid_scaled, y_valid),<br/>                  verbose=0).history</span></pre><p id="bfcf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">查看模型拟合损失和精确度(召回和 FPR)进度。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="20e8" class="nj mh iq nh b gy oc od l oe of">plot_loss(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oj"><img src="../Images/28b73f5bcec5046a93465c41bfa4d98c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XIw2Qx5fjccSrt5ENQ6vQ.png"/></div></div></figure><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="2422" class="nj mh iq nh b gy oc od l oe of">plot_model_recall_fpr(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ok"><img src="../Images/1749e954c8325ec84803a172a3cd45ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RLU679Ykkqb_VPjU2DdLOA.png"/></div></div></figure><p id="62af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">型号 2。等级权重。</strong></p><p id="a8d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据经验法则定义类别权重。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="ae3d" class="nj mh iq nh b gy oc od l oe of">class_weight = {0: sum(y_train == 1)/len(y_train), 1: sum(y_train == 0)/len(y_train)}</span></pre><p id="04bb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们将训练模型。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="5553" class="nj mh iq nh b gy oc od l oe of">n_features = x_train_scaled.shape[1]</span><span id="268e" class="nj mh iq nh b gy og od l oe of">mlp = Sequential()<br/>mlp.add(Input(shape=(n_features, )))<br/>mlp.add(Dense(32, activation='relu'))<br/>mlp.add(Dense(16, activation='relu'))<br/>mlp.add(Dense(1, activation='sigmoid'))</span><span id="5611" class="nj mh iq nh b gy og od l oe of">mlp.summary()</span><span id="95fb" class="nj mh iq nh b gy og od l oe of">mlp.compile(optimizer='adam',<br/>            loss='binary_crossentropy',<br/>            metrics=['accuracy', tf.keras.metrics.Recall(), FalsePositiveRate()]<br/>           )</span><span id="1e26" class="nj mh iq nh b gy og od l oe of">history = mlp.fit(x=x_train_scaled,<br/>                  y=y_train,<br/>                  batch_size=128,<br/>                  epochs=100,<br/>                  validation_data=(x_valid_scaled, y_valid),<br/>                  class_weight=class_weight,<br/>                  verbose=0).history</span><span id="a6c1" class="nj mh iq nh b gy og od l oe of">plot_loss(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ol"><img src="../Images/9f258413b08177f7dd2628a97d4f5144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*48f4EzEa2VV8DgGdYdFmXQ.png"/></div></div></figure><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="efb1" class="nj mh iq nh b gy oc od l oe of">plot_model_recall_fpr(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi om"><img src="../Images/67b780a10148b7b49672ea5ad01c6a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xg_qamEJoy8P4LU3CfJT5A.png"/></div></div></figure><p id="4e59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">型号 3。退学正规化。</strong></p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="0770" class="nj mh iq nh b gy oc od l oe of">n_features = x_train_scaled.shape[1]</span><span id="7664" class="nj mh iq nh b gy og od l oe of">mlp = Sequential()<br/>mlp.add(Input(shape=(n_features, )))<br/>mlp.add(Dense(32, activation='relu'))<br/>mlp.add(Dropout(0.5))<br/>mlp.add(Dense(16, activation='relu'))<br/>mlp.add(Dropout(0.5))<br/>mlp.add(Dense(1, activation='sigmoid'))</span><span id="eb2e" class="nj mh iq nh b gy og od l oe of">mlp.summary()</span><span id="960c" class="nj mh iq nh b gy og od l oe of">mlp.compile(optimizer='adam',<br/>            loss='binary_crossentropy',<br/>            metrics=['accuracy', tf.keras.metrics.Recall(), FalsePositiveRate()]<br/>           )</span><span id="380e" class="nj mh iq nh b gy og od l oe of">history = mlp.fit(x=x_train_scaled,<br/>                  y=y_train,<br/>                  batch_size=128,<br/>                  epochs=100,<br/>                  validation_data=(x_valid_scaled, y_valid),<br/>                  class_weight=class_weight,<br/>                  verbose=0).history</span><span id="883d" class="nj mh iq nh b gy og od l oe of">plot_loss(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi on"><img src="../Images/47166cb439ab82bc0fc2fb90f04c90bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKcCErCF6EsI0EW9n4ieNg.png"/></div></div></figure><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="386c" class="nj mh iq nh b gy oc od l oe of">plot_model_recall_fpr(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oo"><img src="../Images/4ee150f5226851c92dfb0043ad4ca6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q3H69pN_R2adAmrJAC39iA.png"/></div></div></figure><p id="0af9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">型号 4。过采样-欠采样</strong></p><p id="5650" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 SMOTE 重采样器。</p><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="d28d" class="nj mh iq nh b gy oc od l oe of">from imblearn.over_sampling import SMOTE<br/>smote = SMOTE(random_state=212)<br/>x_train_scaled_resampled, y_train_resampled = smote.fit_resample(x_train_scaled, y_train)<br/>print('Resampled dataset shape %s' % Counter(y_train_resampled))</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi op"><img src="../Images/6f0269cb52f8f4b09c525daab54b082f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ctEMkIEl0Ol7KxQpmiIT_w.png"/></div></div></figure><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="3f05" class="nj mh iq nh b gy oc od l oe of">n_features = x_train_scaled.shape[1]</span><span id="55e4" class="nj mh iq nh b gy og od l oe of">mlp = Sequential()<br/>mlp.add(Input(shape=(n_features, )))<br/>mlp.add(Dense(32, activation='relu'))<br/>mlp.add(Dropout(0.5))<br/>mlp.add(Dense(16, activation='relu'))<br/>mlp.add(Dropout(0.5))<br/>mlp.add(Dense(1, activation='sigmoid'))</span><span id="0391" class="nj mh iq nh b gy og od l oe of">mlp.summary()</span><span id="5a48" class="nj mh iq nh b gy og od l oe of">mlp.compile(optimizer='adam',<br/>            loss='binary_crossentropy',<br/>            metrics=['accuracy', tf.keras.metrics.Recall(), FalsePositiveRate()]<br/>           )</span><span id="6734" class="nj mh iq nh b gy og od l oe of">history = mlp.fit(x=x_train_scaled_resampled,<br/>                  y=y_train_resampled,<br/>                  batch_size=128,<br/>                  epochs=100,<br/>                  validation_data=(x_valid, y_valid),<br/>                  class_weight=class_weight,<br/>                  verbose=0).history</span><span id="2182" class="nj mh iq nh b gy og od l oe of">plot_loss(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oo"><img src="../Images/1c4b9dfc4a816c48fc22ade87d73f5b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6dBWa5-GRHp8LOVhgUd7w.png"/></div></div></figure><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="bf4a" class="nj mh iq nh b gy oc od l oe of">plot_model_recall_fpr(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi oq"><img src="../Images/4f1817cb49f0c94369eb0b4dfde59436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxbRmUdJZGCsqSddL45cmQ.png"/></div></div></figure><p id="1ef3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">模型 5。卢瑟激活。</strong></p><p id="f2fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用<code class="fe ne nf ng nh b">selu</code>激活，这种激活因其自规范化特性而流行起来。</p><p id="465e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意:</p><ul class=""><li id="a292" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">我们用了一个<code class="fe ne nf ng nh b">kernel_initializer=’lecun_normal’</code>，</li><li id="d776" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">作为<code class="fe ne nf ng nh b">AlphaDropout(0.1)</code>辍学。</li></ul><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="2e58" class="nj mh iq nh b gy oc od l oe of">n_features = x_train_scaled.shape[1]</span><span id="6cca" class="nj mh iq nh b gy og od l oe of">mlp = Sequential()<br/>mlp.add(Input(shape=(n_features, )))<br/>mlp.add(Dense(32, kernel_initializer='lecun_normal', activation='selu'))<br/>mlp.add(AlphaDropout(0.1))<br/>mlp.add(Dense(16, kernel_initializer='lecun_normal', activation='selu'))<br/>mlp.add(AlphaDropout(0.1))<br/>mlp.add(Dense(1, activation='sigmoid'))</span><span id="fab5" class="nj mh iq nh b gy og od l oe of">mlp.summary()</span><span id="c15f" class="nj mh iq nh b gy og od l oe of">mlp.compile(optimizer='adam',<br/>            loss='binary_crossentropy',<br/>            metrics=['accuracy', tf.keras.metrics.Recall(), FalsePositiveRate()]<br/>           )</span><span id="140a" class="nj mh iq nh b gy og od l oe of">history = mlp.fit(x=x_train_scaled,<br/>                  y=y_train,<br/>                  batch_size=128,<br/>                  epochs=100,<br/>                  validation_data=(x_valid, y_valid),<br/>                  class_weight=class_weight,<br/>                  verbose=0).history</span><span id="4be5" class="nj mh iq nh b gy og od l oe of">plot_loss(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi or"><img src="../Images/89125789a53ade60388f3614be68cb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pyk6-JmC8giNaFAtgeqwOg.png"/></div></div></figure><pre class="ld le lf lg gt ny nh nz oa aw ob bi"><span id="659b" class="nj mh iq nh b gy oc od l oe of">plot_model_recall_fpr(history)</span></pre><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ol"><img src="../Images/c6e249ccebc2a6cd01a236d6bc362919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u90C5L42ZYmcO8ERskNIbw.png"/></div></div></figure><h1 id="7252" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><ul class=""><li id="91fc" class="ls lt iq kh b ki my kl mz ko nv ks nw kw nx la lx ly lz ma bi translated">凭借深度学习提供的所有预测建模能力，开始也可能会令人不知所措。</li><li id="351f" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">本文中的经验法则提供了构建初始神经网络的起点。</li><li id="8a86" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">由此构建的模型应该进一步调整以提高性能。</li><li id="cd24" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">如果用这些经验法则构建的模型性能没有一些最低性能。进一步调优可能不会带来太大的改善。尝试另一种方法。</li><li id="bbd5" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">本文展示了在 TensorFlow 2 中实现神经网络的步骤。</li><li id="756e" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">如果您没有 TensorFlow 2，建议迁移到它，因为它带来了(keras)实施的简易性和高性能。参见此处的说明，<a class="ae lb" rel="noopener" target="_blank" href="/step-by-step-guide-to-install-tensorflow-2-0-67bc73e79b82">安装 Tensorflow 2 </a>的分步指南。</li></ul></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><p id="3722" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="ni">阅读我的其他文章以获得更深入的学习理解、模型和实现</em> </strong>。</p><p id="72cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 Keras 中的自动编码器进行极端罕见事件分类。</p><p id="3633" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/simplified-math-behind-dropout-in-deep-learning-6d50f3f47275">理解辍学背后的简化数学</a>。</p><p id="04e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"> LSTM 用于 Keras 极端罕见事件分类的自动编码器</a>。</p><p id="852c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/step-by-step-understanding-lstm-autoencoder-layers-ffab055b6352">逐步了解 LSTM 自动编码器图层</a>。</p><p id="f83e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第一部分</a>。</p><p id="ce31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第二部分</a>。</p></div></div>    
</body>
</html>