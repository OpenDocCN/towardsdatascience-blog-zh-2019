<html>
<head>
<title>Review: M²FCN — Multi-stage Multi-recursive-input Fully Convolutional Networks (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:FCN——多级多递归输入全卷积网络(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-m%C2%B2fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1?source=collection_archive---------10-----------------------#2019-04-12">https://towardsdatascience.com/review-m%C2%B2fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1?source=collection_archive---------10-----------------------#2019-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9928" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 EM 图像分割方面优于 U-Net 和 CUMedVision1</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/92d41c12fded76525558d67ab2158a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66y20N9D56stBAdMIFMTmA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Neuronal Structure Segmentation: An EM Image (Left), the Ground Truths for its Neuronal Boundary Detection Result (Middle), and Segmentation Result (Right)</strong></figcaption></figure><p id="7a37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这个故事中，简单回顾了<strong class="ky ir">M·FCN</strong>(<strong class="ky ir">多级多递归输入全卷积网络</strong>)，作者<strong class="ky ir">上海大学</strong>和<strong class="ky ir">约翰·霍普金斯大学</strong>。发表于<strong class="ky ir"> 2017 ICCV </strong>。(<a class="mb mc ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----4f8d5e3f07f1--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="29af" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">概述</h1><ol class=""><li id="bda3" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr nj nk nl nm bi translated"><strong class="ky ir">问题和贡献</strong></li><li id="7efe" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">米 FCN 模型建筑</strong></li><li id="3163" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">消融研究</strong></li><li id="3bdb" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="ab50" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak"> 1。问题和贡献</strong></h1><p id="a263" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">在分割电子显微镜(EM)图像方面有三个主要挑战:</p><ol class=""><li id="3b3d" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr nj nk nl nm bi translated"><strong class="ky ir">膜的厚度变化很大</strong>，薄如细丝，厚如水滴。</li><li id="aa9c" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">电磁采集的噪声</strong>使得膜的对比度很低，导致一些膜的边界甚至看不见。</li><li id="6481" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">混杂结构</strong>的存在，如线粒体和囊泡，也增加了膜检测的难度。</li></ol><p id="2e8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的主要贡献<strong class="ky ir">包括:</strong></p><ol class=""><li id="08c4" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr nj nk nl nm bi translated"><strong class="ky ir">端到端的多级网络架构</strong>，其中每一级通过对不同级别施加监督来产生多个边输出，并将它们作为多个递归输入馈入下一级。</li><li id="9474" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">使用多个递归输入而不是单个递归输入</strong>不仅可以提高神经元边界检测的性能，而且在生物学上也是合理的。</li><li id="c207" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">该网络在两个公开可用的 EM 分割数据集上取得了<strong class="ky ir">有希望的结果</strong>。</li></ol></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="00fd" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak"> 2。M FCN 模型架构</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/cb43cd5703edd60b9a9c2acaeb3b9fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cJ6J7wsLi0ZJCY_y0IDgA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">The Proposed M²FCN Architecture</strong></figcaption></figure><h2 id="1b85" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">2.1.体系结构</h2><ul class=""><li id="6ad7" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr ol nk nl nm bi translated">一个<strong class="ky ir">整体嵌套边缘检测器(HED)网络</strong>作为默认子网，由<strong class="ky ir"> VGG-16 </strong>网络转换而来。</li><li id="dbbd" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated">共有 5 个级别，步长分别为 1、2、4、8 和 16，<strong class="ky ir">感受野大小分别为 5、14、40、92、196 </strong>。</li><li id="d1c7" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated"><strong class="ky ir">在每个阶段内，每个子网由多个层次</strong>组成，每个层次由一个卷积层、一个 ReLU 层和一个最终池层的若干组合组成。</li><li id="d5be" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated"><strong class="ky ir">每侧输出层连接到每级</strong>的最后一个卷积层，由 1×1 卷积层和解卷积层组成，保证每侧输出的分辨率与输入的原图像相同。</li><li id="1a62" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated"><strong class="ky ir">在【0，1】范围内，每个侧输出层</strong>应用一个 s 形层。</li></ul><h2 id="778d" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">2.2.训练阶段</h2><ul class=""><li id="d37d" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr ol nk nl nm bi translated">因此，<em class="om"> m </em>级的输入 X^(m 为:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/3137a55710d97ab0528a517c5c0ffebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*DgXY74dnR3Kv_zLhh_3ZUg.png"/></div></figure><ul class=""><li id="dbe7" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">其中<em class="om"> X </em>是输入图像，那些<em class="om"> S </em>是第 1 到第<em class="om"> n </em>级的第(<em class="om"> m </em> -1)级的侧输出，⨁是沿信道维度的级联。</li><li id="b3fc" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated">单侧输出的交叉熵损失函数:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/688a84b4056a53d5bcec91bb7846dcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*7s4h7ttrcDnnLyRtiiRkmA.png"/></div></figure><ul class=""><li id="2ccb" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">其中|B|和|B(bar)|分别是边界和非边界地面真值标签集，β=|B(bar)|/|B|，以实现正/负类平衡权重来消除由于不平衡类引起的偏差。</li><li id="5877" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated">所有侧输出<em class="om"> Ls </em>的损失函数为:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/201193408e887ff1e71d63aab26bf8fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*lLSg1VTgv1FwfvCeyS6flg.png"/></div></figure><ul class=""><li id="3ee1" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">此外，<em class="om"> m </em>级有一个保险丝输出:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ca608dab03dd1fbed3d6094ff950f4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*xToyOoBsg-4KvxWC8S5Tgg.png"/></div></figure><ul class=""><li id="d356" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">其中<em class="om"> h </em>为融合重量。与上面类似，<em class="om"> m </em>级融合输出的类平衡损失函数为:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/c85844c553b421aac84a32457833358d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*lDMA5yW0GUoS0r4K5D1PwQ.png"/></div></figure><ul class=""><li id="4fe4" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">所有熔断输出<em class="om"> Lf </em>的损失函数为:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ba8ab3102ff1dcd40cca01778665172b.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*c89TUqh-g-oxuLY-ezoAHQ.png"/></div></figure><ul class=""><li id="f4d1" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">反向传播使<em class="om"> Ls </em>和<em class="om"> Lf </em>最小化:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a1fe8c668669cb5bf62f2fe050ec152b.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*Qdfht5-IaN9i6tHs5VWUAw.png"/></div></figure><ul class=""><li id="4a95" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">为了训练上述级联网络，首先，训练单级网络。然后初始化为第一级网络，其余随机初始化。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="6b8f" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak"> 3。消融研究</strong></h1><h2 id="499a" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">3.1.评估指标</h2><ul class=""><li id="12cf" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr ol nk nl nm bi translated">Rand 合并分数(左)和 Rand 分割分数(右):</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/73eb0d24286530c671dfceb0d7195130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*Zh4bF8uIDN1x4G9VMgUFBw.png"/></div></figure><ul class=""><li id="3515" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">其中<em class="om"> nij </em>表示建议分割的第<em class="om"> i </em>段和基础事实分割的第<em class="om"> j </em>段中体素的数量。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/d3d47b076ef68a3a14b3307a2b4deac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*5ZRVjBX5x8OMxo6Iac4R2w.png"/></div></figure><ul class=""><li id="8f4a" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">然后<strong class="ky ir">使用 Rand 合并分数和 Rand 分割分数将 Rand F 分数用作评估指标</strong>。</li></ul><h2 id="fc85" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">3.2.替代网络设计</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/38511e6dd32fe2cd9e7648f47f12b567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*6y9opKvKizeRo1_JbVI3fg.png"/></div></figure><ul class=""><li id="ec67" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated"><strong class="ky ir"> AD_I </strong>:仅 1 级，作为<strong class="ky ir">基线</strong>。</li><li id="d962" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated"><strong class="ky ir"> AD_II 到 ADVI </strong> : 2 个阶段，4 级(较小的感受野)对 5 级(较大的感受野)，逐级对端到端，单递归对多递归，以便通过表格中的相互比较来显示每个项目的贡献。</li><li id="59fe" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated"><strong class="ky ir"> AD_VIII </strong>:提议作为<strong class="ky ir">终网</strong>的那个，比 AD_I 到 AD_VI 都强。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/6db37233fc98bf5946a4b782902a0639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eetGKtYxVzhQwIuIVtnKEQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Qualitative Results for 1-stage, 2-stage and 3-stage Network</strong></figcaption></figure><ul class=""><li id="b28c" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">红色箭头表示假阳性被更多阶段抑制。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="9710" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">4.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="a33b" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">4.1.小鼠梨形皮质 EM 数据集</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/fa8262619897cc3a967a74c3f9b81aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*rvstdl0iKYLpr7UBA1_GVw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Mouse Piroform Cortex EM dataset</strong></figcaption></figure><ul class=""><li id="77b6" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">有三个阶段的 FCN 是最好的。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/0afbe5e435f586d563aa9e843dc2ca12.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*V6CCThW2rFDpmmGK2vx3qg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Precision (Rand Merge) Recall (Rand Split) Curves</strong></figcaption></figure><ul class=""><li id="fb50" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">即使在高召回率的情况下，M FCN 也能达到高精度。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/db933ccb7cc482b54bce59d79a36102e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6basR5dQcmdWBqUZ1ZLHaw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Qualitative Results: Original Images (Left), Ground-Truth Label Maps (2nd Left), Predicted Boundary Maps (2nd Right), Predicted Label Maps (Right)</strong></figcaption></figure><h2 id="e272" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">4.2.ISBI 2012 EM 分段数据集</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/3da1ea4174d109730c4100c93efdf427.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*6O2WcMC3U4A2b2RkJ9AQxg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">ISBI 2012 EM Segmentation Dataset</strong></figcaption></figure><ul class=""><li id="90d3" class="nc nd iq ky b kz la lc ld lf nv lj nw ln nx lr ol nk nl nm bi translated">M FCN 胜过 U-Net 和 CUMedVision1 。</li><li id="62ee" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated">此外，<strong class="ky ir"> M FCN 使用 VGGNet 作为主干，其性能与使用 ResNet 作为主干的 PolyMtl 和 FusionNet 类似</strong>。</li><li id="2dc7" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr ol nk nl nm bi translated">作者认为，如果 FCN 人变得更有骨气，他们会有更好的结果。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h2 id="b702" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">参考</h2><p id="0c71" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">【2017 ICCV】【M·FCN】<br/><a class="ae pb" href="https://arxiv.org/abs/1703.08493" rel="noopener ugc nofollow" target="_blank">用于神经元边界检测的多级多递归输入全卷积网络</a></p><h2 id="b26f" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测<br/></strong><a class="ae pb" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae pb" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae pb" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae pb" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae pb" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae pb" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae pb" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae pb" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae pb" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae pb" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae pb" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolo v3</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>[<a class="ae pb" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">语义切分<br/></strong><a class="ae pb" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae pb" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae pb" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae pb" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae pb" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae pb" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae pb" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae pb" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae pb" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae pb" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物医学图像分割<br/> </strong> [ <a class="ae pb" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累计视觉 1 </a> ] [ <a class="ae pb" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">累计视觉 2/DCAN</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae pb" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae pb" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实例分割<br/></strong>[<a class="ae pb" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">Hypercolumn</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">deep mask</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">sharp mask</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath net</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae pb" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>[<a class="ae pb" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a>]</p><p id="58de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"/><br/><a class="ae pb" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae pb" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae pb" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>