<html>
<head>
<title>Reinforcement Learning — TD(λ) Introduction(3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习— TD(λ)简介(3)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-3-f329bdbf872a?source=collection_archive---------18-----------------------#2019-09-14">https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-3-f329bdbf872a?source=collection_archive---------18-----------------------#2019-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b5ff" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 Sarsa(λ)扩展 Q 函数上的 TD(λ)</h2></div><p id="55ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上一篇<a class="ae le" href="https://medium.com/@zhangyue9306/reinforcement-learning-td-%CE%BB-introduction-2-f0ea427cd395" rel="noopener">文章</a>中，我们学习了 TD(λ)与合格迹的思想，这是 n 步 TD 方法的组合，并将其应用于随机游走的例子。在这篇文章中，让我们将 lambda 的思想扩展到更一般的用例——不是学习状态值函数，而是学习状态的 Q 函数，动作值。在本文中，我们将:</p><ol class=""><li id="bcf5" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">了解 Sarsa(λ)的概念</li><li id="72f7" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">将其应用于山地车实例</li></ol><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/3178f4b7956f2a57dda33d9f784dffd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpMttKW3rJsBVFenzAjeRQ.jpeg"/></div></div></figure><h1 id="2648" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">萨尔萨(λ)</h1><p id="10f6" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">与我们已经阐述的许多扩展一样，将价值函数<code class="fe nc nd ne nf b">V(S)</code>扩展到 Q 函数<code class="fe nc nd ne nf b">Q(S, A)</code>是非常自然的，因为所有的公式和逻辑都是相同的，当公式化问题时，将只考虑动作。回想一下 TD(λ)这里介绍的<a class="ae le" rel="noopener" target="_blank" href="/reinforcement-learning-td-λ-introduction-686a5e4f4e60"/>，更新过程类似:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ng"><img src="../Images/2b1d039cf3dd72d90232acf379388607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6hzEf9Ydt5bzMI56B6xonw.png"/></div></div></figure><p id="d447" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">唯一不同的是<code class="fe nc nd ne nf b">∇V</code>被<code class="fe nc nd ne nf b">∇q</code>取代，并且<a class="ae le" href="https://medium.com/@zhangyue9306/reinforcement-learning-td-%CE%BB-introduction-2-f0ea427cd395" rel="noopener">合格跟踪</a>将扩展为:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nh"><img src="../Images/e5c48ee26010a9f6699285b33d83d21b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nF4uxYyKRpMkK4-xM8_mtQ.png"/></div></div></figure><p id="6f81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仍在这里的<code class="fe nc nd ne nf b">∇V</code>换成了<code class="fe nc nd ne nf b">∇q</code>。</p><p id="ebcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们得到 Sarsa(λ)的备份图:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi ni"><img src="../Images/82930557a2b7102eaf74db75c7401dc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HA4Y8GxAv6HCf2NFsO98ig.png"/></div></div></figure><p id="09f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中每列是 n 步 Sarsa，<code class="fe nc nd ne nf b">1-λ</code>，<code class="fe nc nd ne nf b">(1-λ)λ</code> …是权重。</p><p id="e874" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">扩展的算法将是:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nj"><img src="../Images/ce40d697305ec5bf99fe06eb05b046fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcb9rZn27woQaliD7h4DLQ.png"/></div></div></figure><p id="5006" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，这里的算法是专门为二进制特征表示而设计的，即每个状态、动作对将被表示为二进制特征，例如<code class="fe nc nd ne nf b">(3.2, 1)</code>可以被表示为<code class="fe nc nd ne nf b">[1, 0, 0, 1]</code>(更具体地，我们已经广泛地讨论了如何在<a class="ae le" rel="noopener" target="_blank" href="/reinforcement-learning-tile-coding-implementation-7974b600762b">瓦片编码</a>中将连续状态表示为二进制特征)</p><p id="005a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">乍一看，你可能觉得算法有点复杂，但实际上和<code class="fe nc nd ne nf b">TD(λ)</code>是一样的，我们试着这样理解一下:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/7384c96d2241e8daa2ac6222b1b6912a.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*8TpTbOOKS4jKU5VYoRBZfA.png"/></div></figure><p id="031d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们来关注一下底部。你应该对此很熟悉，因为它看起来几乎与<code class="fe nc nd ne nf b">TD(λ)</code> — <code class="fe nc nd ne nf b">δ</code>相同，这是时间上的差异，权重<code class="fe nc nd ne nf b">w</code>基于<code class="fe nc nd ne nf b">δ</code>和资格跟踪<code class="fe nc nd ne nf b">z</code>进行更新，其中<code class="fe nc nd ne nf b">z</code>跟踪先前的状态。<code class="fe nc nd ne nf b">z</code>提供了两个更新:</p><ol class=""><li id="4877" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated"><strong class="kk iu">积累痕迹</strong>:<code class="fe nc nd ne nf b">z = z + 1</code>；那么<code class="fe nc nd ne nf b">1</code>从何而来？这其实是<code class="fe nc nd ne nf b">q(S, A)</code>的衍生！请记住，这里我们使用状态、动作的二进制表示，活动特征的导数是 1。其实也可以写成<code class="fe nc nd ne nf b">z = z + ∇q</code>，代入<code class="fe nc nd ne nf b">w</code> updates 时，跟踪之前的导数。</li><li id="15fd" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated"><strong class="kk iu">替换轨迹:</strong> <code class="fe nc nd ne nf b">z = 1</code>，仅使用当前导数更新<code class="fe nc nd ne nf b">w</code>。</li></ol><p id="507e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne nf b">F(s, a)</code>在这里你可以把它想成是 tile 编码函数或者任何其他给出状态、动作二进制表示的函数。</p><p id="f9ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个算法就这么多了。忽略其他部分，将其与我们在之前的讲座中所学的内容联系起来，现在让我们开始实施一个示例。</p><h1 id="1ca3" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">山地车上的 Sarsa(λ)</h1><p id="843e" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">我们已经在 n 步 Sarsa 示例<a class="ae le" rel="noopener" target="_blank" href="/reinforcement-learning-on-policy-function-approximation-2f47576f772d">中讨论了山地车，这里是</a>，设置是(如果你对下面的实现有任何困惑，我强烈建议你阅读以前的帖子以更好地理解这个过程):</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nl"><img src="../Images/c6d4eaa93ca70e59ae9ce885a9a8c568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*AP3UJKPBxWoKIJxZsnhnUg.png"/></div></div></figure><p id="a38f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如图所示，考虑驾驶一辆动力不足的汽车爬上陡峭的山路。困难在于重力比汽车的发动机更强，即使在全油门的情况下，汽车也无法加速上陡坡。唯一的解决办法是先离开目标，然后爬上左边的反坡。</p><p id="bc5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nm">这个问题中的奖励在所有时间步长上都是-1，直到汽车在山顶移动经过它的目标位置，这就结束了这一集。有三种可能的操作:全油门向前(+1)、全油门向后(-1)和零油门(0)。汽车根据简化的物理学原理行驶。其位置、</em>、<code class="fe nc nd ne nf b"><em class="nm">x_t</em></code>、<em class="nm">和速度、</em>、<code class="fe nc nd ne nf b"><em class="nm">x_ ̇t</em></code>、<em class="nm">，均由</em>更新</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi nn"><img src="../Images/734e3f3d8537f4af6cdc9c24e9402a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IeFZpdv0U-qia_9R_c9dAw.png"/></div></div></figure><p id="0b35" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nm">其中绑定操作强制执行</em> <code class="fe nc nd ne nf b"><em class="nm">-1.2 &lt;= x_t+1 &lt;= 0.5</em></code> <em class="nm">和</em> <code class="fe nc nd ne nf b"><em class="nm">-0.07 &lt;= x_ ̇t+1 &lt;= 0.07</em></code> <em class="nm">。另外，当</em> <code class="fe nc nd ne nf b"><em class="nm">x_t+1</em></code> <em class="nm">到达左界时，</em> <code class="fe nc nd ne nf b"><em class="nm">x_ ̇t+1</em></code> <em class="nm">被复位为零。到了右边界，目标达到，插曲终止。每集从</em> <code class="fe nc nd ne nf b"><em class="nm">[-0.6, -0.4)</em></code> <em class="nm">中的任意位置</em> <code class="fe nc nd ne nf b"><em class="nm">x_t</em></code> <em class="nm">和零速度开始。</em></p><p id="68a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的过程将与我们在 n 步 Sarsa 中陈述的大部分相同，所以我将主要集中于解释不同之处。</p><h2 id="9f2a" class="no mg it bd mh np nq dn ml nr ns dp mp kr nt nu mr kv nv nw mt kz nx ny mv nz bi translated">价值函数</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="36ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<code class="fe nc nd ne nf b">init</code>函数中，区别在于<code class="fe nc nd ne nf b">self.z</code>的初始化，T7 是合格跟踪向量，在开始时设置为 0。</p><p id="00ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne nf b">value</code>函数返回值给定状态，动作配对。主要的区别是在<code class="fe nc nd ne nf b">update</code>函数中，在更新权重之前，我们首先更新<code class="fe nc nd ne nf b">self.z</code>。</p><ol class=""><li id="3c46" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">在<code class="fe nc nd ne nf b">accumulating</code>方法中，使用<code class="fe nc nd ne nf b">z = γλz + 1</code>更新活动图块</li><li id="8ca9" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">在<code class="fe nc nd ne nf b">replacing</code>方法中，活动图块更新为:<code class="fe nc nd ne nf b">z = 1</code></li></ol><p id="f0b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne nf b">costToGo</code>功能相同。</p><h2 id="184a" class="no mg it bd mh np nq dn ml nr ns dp mp kr nt nu mr kv nv nw mt kz nx ny mv nz bi translated">一般功能</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="8a75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的功能与之前的实现相同，所以我将简单介绍一下。<code class="fe nc nd ne nf b">reset</code>函数在代理到达目标或最左侧状态时将代理复位到初始状态；<code class="fe nc nd ne nf b">takeAction</code>函数接收一个动作，并根据预定义的公式返回代理的下一个状态；<code class="fe nc nd ne nf b">chooseAction</code>函数根据ϵ-greedy 策略的当前估计选择操作；并且<code class="fe nc nd ne nf b">giveReward</code>根据代理的状态给予奖励。</p><h2 id="387d" class="no mg it bd mh np nq dn ml nr ns dp mp kr nt nu mr kv nv nw mt kz nx ny mv nz bi translated">玩游戏</h2><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="oa ob l"/></div></figure><p id="238d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后是<code class="fe nc nd ne nf b">play</code>功能，启动游戏。逻辑简单明了——一个代理从<code class="fe nc nd ne nf b">current state</code> → <code class="fe nc nd ne nf b">takes an action</code> → <code class="fe nc nd ne nf b">reaches next state</code> → <code class="fe nc nd ne nf b">receives reward</code> → <code class="fe nc nd ne nf b">takes next action</code> …，和<code class="fe nc nd ne nf b">target = reward + Q(nextState, nextAction)</code>开始，它被传递给我们上面定义的值函数来更新资格跟踪<code class="fe nc nd ne nf b">z</code>和权重<code class="fe nc nd ne nf b">w</code>。</p><h2 id="a2e0" class="no mg it bd mh np nq dn ml nr ns dp mp kr nt nu mr kv nv nw mt kz nx ny mv nz bi translated">结果</h2><p id="e292" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">整个游戏设置与我们在 n 步 Sarsa 上介绍的完全相同，因此我们比较 Sarsa(λ)和 n 步 Sarsa 之间的学习结果:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi oc"><img src="../Images/4c477a93f2f101244444f6df6d5aeb95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bll0mFGnU8fMyrD3rRQ6IA.png"/></div></div><figcaption class="od oe gj gh gi of og bd b be z dk">Image from Reinforcement Learning an Introduction</figcaption></figure><p id="55b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用相同数量的镶嵌和其他参数。参考 Sutton 的书，Sarsa(λ)比 n 步 Sarsa 更有竞争力，因为它学习更快以达到目标(更多说明，请参考这里的完整实现<a class="ae le" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/MountainCar(Lambda)/MountainCar.py" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="eccf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考</strong>:</p><ul class=""><li id="4d3a" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld oh ll lm ln bi translated"><a class="ae le" href="http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="d9ae" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oh ll lm ln bi translated"><a class="ae le" href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/Shang tong Zhang/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>