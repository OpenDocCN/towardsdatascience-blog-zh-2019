<html>
<head>
<title>Sentence classification using Bi-LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于双 LSTM 的句子分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentence-classification-using-bi-lstm-b74151ffa565?source=collection_archive---------1-----------------------#2019-03-28">https://towardsdatascience.com/sentence-classification-using-bi-lstm-b74151ffa565?source=collection_archive---------1-----------------------#2019-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b630" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于双向 LSTM 模型的句子分类及其与其他基线模型的比较</h2></div><p id="96bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，有各种各样的方法来进行句子分类，如单词袋方法或神经网络等。在这篇文章中，我将主要讨论使用深度学习模型(特别是双 LSTM)的句子分类任务</p><p id="d693" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文主要关注一些基本的介绍，然后直接进入实现。如果您需要更深入的信息，我在参考资料中提供了链接。</p><p id="29ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我把文章按不同的类别组织起来，</p><ol class=""><li id="e7d5" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">介绍</li><li id="2124" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">实现:(预处理和基线模型)</li><li id="aaf5" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">双 LSTM 模型</li><li id="2f57" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">结果和结论</li></ol><p id="117b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随意跳到一个特定的类别。</p><p id="a606" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">一、简介</strong></p><p id="1c00" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于句子分类，我们主要有两种方法:</p><ol class=""><li id="ce80" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">单词袋模型(BOW)</li><li id="2f9d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">深度神经网络模型</li></ol><p id="cfab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">BOW 模型的工作原理是分别对待每个单词，并对每个单词进行编码。对于 BOW 方法，我们可以使用 TF-IDF 方法，但它不能保留句子中每个单词的上下文。</p><p id="6987" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了更好地完成命名实体抽取、情感分析等任务，我们使用深度神经网络。</p><h2 id="89a5" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated"><strong class="ak">二世。实施</strong></h2><p id="b74c" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated"><strong class="kk iu">数据集:</strong></p><p id="91a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我使用了 Reddit - <a class="ae mq" href="https://emoclassifier.github.io/" rel="noopener ugc nofollow" target="_blank">数据集</a> [2]，它基于四种情绪类别，如愤怒、快乐、血腥和毛骨悚然。</p><p id="5513" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于深度神经模型，我们需要文本的嵌入。嵌入捕捉了单词在更高维度平面中的表示。通过嵌入，我们创建了单词的向量表示，通过理解单词的上下文来学习单词。我们可以使用预先训练的嵌入，如 glove、fasttext，它们在数十亿个文档上进行训练，或者我们可以使用 gensim 包等创建自己的嵌入(在我们自己的语料库上进行训练)。</p><p id="0cbd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我使用了预先训练好的<a class="ae mq" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> glove-twitter 嵌入</a>，它适用于我们的社交网络数据环境。此外，我选择 100 维嵌入，它表现很好，不需要花太多时间训练。可以选择其他(25、50、300 D 也可以)。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="a060" class="ls lt it mw b gy na nb l nc nd">embedding_path = "~/glove.twitter.27B.100d.txt" ## change <br/># create the word2vec dict from the dictionary<br/>def get_word2vec(file_path):<br/>    file = open(embedding_path, "r")<br/>    if (file):<br/>        word2vec = dict()<br/>        split = file.read().splitlines()<br/>        for line in split:<br/>            key = line.split(' ',1)[0] # the first word is the key<br/>            value = np.array([float(val) for val in line.split(' ')[1:]])<br/>            word2vec[key] = value<br/>        return (word2vec)<br/>    else:<br/>        print("invalid fiel path")</span><span id="8245" class="ls lt it mw b gy ne nb l nc nd">w2v = get_word2vec(embedding_path)</span></pre><p id="18a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">文本预处理:</strong></p><p id="b49c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以他们的数据有四个代表四种不同情绪的文件，所以我们需要合并这些文件来完成多类别分类任务。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="59a1" class="ls lt it mw b gy na nb l nc nd">df_rage = pd.read_csv(os.path.join(dir_path,'processed_rage.csv'))<br/>df_happy =  pd.read_csv(os.path.join(dir_path,'processed_happy.csv'))<br/>df_gore =  pd.read_csv(os.path.join(dir_path,'processed_gore.csv'))<br/>df_creepy =  pd.read_csv(os.path.join(dir_path,'processed_creepy.csv'))</span><span id="63b5" class="ls lt it mw b gy ne nb l nc nd"># create a random balances dataset of all of the categories<br/>length = np.min([len(df_rage),len(df_happy),len(df_creepy),len(df_gore)])</span><span id="4b60" class="ls lt it mw b gy ne nb l nc nd">df_final = pd.concat([df_rage[:length], df_happy[:length], df_gore[:length], df_creepy[:length]], ignore_index=True)</span></pre><p id="d3e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">标记化:</strong></p><p id="d83e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了将句子分解成更简单的记号或单词，我们对文本进行记号化。在这里，我们将使用 nltk Tweet tokenizer，因为它可以很好地处理社交网络数据。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="9e7b" class="ls lt it mw b gy na nb l nc nd">import nltk<br/>from nltk.corpus import stopwords<br/>stopwords = set(stopwords.words('english'))<br/>nltk.download('wordnet')<br/>nltk.download('stopwords')<br/>from nltk.tokenize import TweetTokenizer<br/>from nltk.corpus import wordnet as wn<br/>tknzr = TweetTokenizer()</span><span id="faa1" class="ls lt it mw b gy ne nb l nc nd">def get_tokens(sentence):<br/>#     tokens = nltk.word_tokenize(sentence)  # now using tweet tokenizer<br/>    tokens = tknzr.tokenize(sentence)<br/>    tokens = [token for token in tokens if (token not in stopwords and len(token) &gt; 1)]<br/>    tokens = [get_lemma(token) for token in tokens]<br/>    return (tokens)</span><span id="8419" class="ls lt it mw b gy ne nb l nc nd">def get_lemma(word):<br/>    lemma = wn.morphy(word)<br/>    if lemma is None:<br/>        return word<br/>    else:<br/>        return lemma</span><span id="492e" class="ls lt it mw b gy ne nb l nc nd">token_list = (df_final['title'].apply(get_tokens))</span></pre><p id="3c80" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">准备输入变量</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="5ea7" class="ls lt it mw b gy na nb l nc nd"># integer encode the documents<br/>encoded_docs = t.texts_to_sequences(sentences)<br/># pad documents to a max length of 4 words<br/>max_length = max_len<br/>X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')</span></pre><p id="c0bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输出变量:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="fbdf" class="ls lt it mw b gy na nb l nc nd">from sklearn import preprocessing<br/>le = preprocessing.LabelEncoder()<br/>Y_new = df_final['subreddit']<br/>Y_new = le.fit_transform(Y_new)</span></pre><p id="7dcf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将数据拆分为培训和测试</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="d901" class="ls lt it mw b gy na nb l nc nd">## now splitting into test and training data<br/>from sklearn.model_selection import train_test_split<br/>X_train,X_test, Y_train, Y_test =  train_test_split(X, y,test_size =0.20,random_state= 4 )</span></pre><p id="0fda" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基准模型</strong></p><p id="9230" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在获得 LSTM 模型的分数之前，我已经从我们的基线模型中获得了一些指标:</p><p id="d8f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于基线模型，我们可以简单地计算 word2vec 嵌入的平均值。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="4958" class="ls lt it mw b gy na nb l nc nd"># the object is a word2vec dictionary with value as array vector,<br/># creates a mean of word vecotr for sentences<br/>class MeanVect(object):<br/>    def __init__(self, word2vec):<br/>        self.word2vec = word2vec<br/>        # if a text is empty we should return a vector of zeros<br/>        # with the same dimensionality as all the other vectors<br/>        self.dim = len(next(iter(word2vec.values())))<br/>        <br/>    # pass a word list<br/>    def transform(self, X):<br/>        return np.array([<br/>            np.mean([self.word2vec[w] for w in words if w in self.word2vec]<br/>                    or [np.zeros(self.dim)], axis=0)<br/>            for words in (X)<br/>        ])</span></pre><p id="cc5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SVM</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="40c3" class="ls lt it mw b gy na nb l nc nd">def svm_wrapper(X_train,Y_train):<br/>    param_grid = [<br/>    {'C': [1, 10], 'kernel': ['linear']},<br/>    {'C': [1, 10], 'gamma': [0.1,0.01], 'kernel': ['rbf']},]<br/>    svm = GridSearchCV(SVC(),param_grid)<br/>    svm.fit(X_train, Y_train)<br/>    return(svm)</span></pre><p id="cd9a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">韵律学</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="e4b8" class="ls lt it mw b gy na nb l nc nd"># svm<br/>svm = svm_wrapper(X_train,Y_train)<br/>Y_pred = svm.predict(X_test)<br/>score = accuracy_score(Y_test,Y_pred)<br/>print("accuarcy :", score)</span><span id="e221" class="ls lt it mw b gy ne nb l nc nd">0.70</span></pre><p id="406d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于基线，你可以进一步应用其他分类器(如随机森林等)，但我用 SVM 得到了最好的 F1 分数。</p><p id="69ea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于语言环境中的神经模型，最流行的是 LSTMs(长短期记忆),这是一种 RNN(递归神经网络),它保留了文本的长期依赖性。我在参考文献中加入了链接，这些参考文献似乎详细解释了 LSTM 的观点。</p><p id="f131" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">三世。双向 LSTM: </strong></p><p id="f1a5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于双向 LSTM，我们有一个嵌入层，而不是加载随机权重，我们将从手套嵌入中加载权重</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="1d61" class="ls lt it mw b gy na nb l nc nd"># get the embedding matrix from the embedding layer<br/>from numpy import zeros<br/>embedding_matrix = zeros((vocab_size, 100))<br/>for word, i in t.word_index.items():<br/> embedding_vector = w2v.get(word)<br/> if embedding_vector is not None:<br/>  embedding_matrix[i] = embedding_vector</span></pre><p id="dd7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还想计算神经模型的 vocab 大小。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="6a71" class="ls lt it mw b gy na nb l nc nd">from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/># prepare tokenizer<br/>t = Tokenizer()<br/>t.fit_on_texts(token_list)<br/>vocab_size = len(t.word_index) + 1</span><span id="bd3f" class="ls lt it mw b gy ne nb l nc nd"># integer encode the documents<br/>encoded_docs = t.texts_to_sequences(sentences)<br/># pad documents to a max length of 4 words<br/>max_length = max_len<br/>X = pad_sequences(encoded_docs, maxlen=max_length, padding='post')<br/>y = Y_new</span></pre><p id="0c7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最终模型</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="c009" class="ls lt it mw b gy na nb l nc nd"># main model<br/>input = Input(shape=(max_len,))<br/>model = Embedding(vocab_size,100,weights=[embedding_matrix],input_length=max_len)(input)<br/>model =  Bidirectional (LSTM (100,return_sequences=True,dropout=0.50),merge_mode='concat')(model)<br/>model = TimeDistributed(Dense(100,activation='relu'))(model)<br/>model = Flatten()(model)<br/>model = Dense(100,activation='relu')(model)<br/>output = Dense(3,activation='softmax')(model)<br/>model = Model(input,output)<br/>model.compile(loss='sparse_categorical_crossentropy',optimizer='adam', metrics=['accuracy'])</span></pre><p id="c24f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于我们的神经模型，上面的 max_len 必须是固定的，它可以是具有最大字数的句子，也可以是静态值。我把它定义为 60。</p><p id="fa16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模型总结-</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="5c98" class="ls lt it mw b gy na nb l nc nd">Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_32 (InputLayer)        (None, 60)                0         <br/>_________________________________________________________________<br/>embedding_34 (Embedding)     (None, 60, 100)           284300    <br/>_________________________________________________________________<br/>bidirectional_29 (Bidirectio (None, 60, 200)           160800    <br/>_________________________________________________________________<br/>time_distributed_28 (TimeDis (None, 60, 100)           20100     <br/>_________________________________________________________________<br/>flatten_24 (Flatten)         (None, 6000)              0         <br/>_________________________________________________________________<br/>dense_76 (Dense)             (None, 100)               600100    <br/>_________________________________________________________________<br/>dense_77 (Dense)             (None, 3)                 303       <br/>=================================================================<br/>Total params: 1,065,603<br/>Trainable params: 1,065,603<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="22f5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，在针对 ner 模型的通用架构的论文[1]中，他们在双 LSTM 之上使用了 CRF 层，但是对于简单的多类别句子分类，我们可以跳过它。</p><p id="9836" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使训练数据符合模型:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="6f58" class="ls lt it mw b gy na nb l nc nd">model.fit(X_train,Y_train,validation_split=0.25, nb_epoch = 10, verbose = 2)</span></pre><p id="92b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">四:成绩</strong></p><p id="c178" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">评估模式</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="da2e" class="ls lt it mw b gy na nb l nc nd"># evaluate the model<br/>loss, accuracy = model.evaluate(X_test, Y_test, verbose=2)<br/>print('Accuracy: %f' % (accuracy*100))</span><span id="5c75" class="ls lt it mw b gy ne nb l nc nd">Accuracy: 74.593496</span></pre><p id="a3ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">分类报告</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="8f75" class="ls lt it mw b gy na nb l nc nd">from sklearn.metrics import classification_report,confusion_matrix<br/>Y_pred = model.predict(X_test)<br/>y_pred = np.array([np.argmax(pred) for pred in Y_pred])<br/>print('  Classification Report:\n',classification_report(Y_test,y_pred),'\n')</span><span id="27d3" class="ls lt it mw b gy ne nb l nc nd">Classification Report:<br/>               precision    recall  f1-score   support<br/><br/>           0       0.74      0.72      0.73       129<br/>           1       0.75      0.64      0.69       106<br/>           2       0.76      0.89      0.82       127<br/>           3       0.73      0.72      0.72       130<br/><br/>   micro avg       0.75      0.75      0.75       492<br/>   macro avg       0.75      0.74      0.74       492<br/>weighted avg       0.75      0.75      0.74       492</span></pre><figure class="mr ms mt mu gt ng gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nf"><img src="../Images/5f8e0901769c2c9ce1f5ee3755cbe6f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DydI6ilg2zJ_AkQWYmiiBQ.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Comparison with other models</figcaption></figure><p id="66d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论:</strong></p><p id="6820" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们看到，对于保留文本序列及其上下文的神经模型，我们得到了比 BOW 模型更好的分数，但这取决于上下文和应用。因此，在某些情况下，与复杂的神经模型相比，简单的模型可能是有益的。此外，我们有一个较小的数据集，所以训练时间很短，但对于较大的数据集(&gt; 100k)，可能需要 1 个多小时的训练。</p><p id="cf37" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢，如果你有任何疑问或意见，请随时添加到评论区。谢了。</p><p id="02c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="5629" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]:兰普尔、纪尧姆、米格尔·巴列斯特罗斯、桑迪普·苏布拉曼尼安、川上和也和克里斯·戴尔。"命名实体识别的神经架构."<em class="nr"> arXiv 预印本 arXiv:1603.01360 </em> (2016)。</p><p id="6471" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] Duong、Chi Thang、Remi Lebret 和 Karl Aberer。"用于分析社交媒体的多模态分类."2017 年第 27 届欧洲机器学习和数据库知识发现原理与实践会议(ECML-PKDD)</p><div class="ns nt gp gr nu nv"><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">了解 LSTM 网络——colah 的博客</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">这些循环使得循环神经网络看起来有点神秘。然而，如果你想得更多一点，事实证明…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">colah.github.io</p></div></div></div></a></div><div class="ns nt gp gr nu nv"><a href="https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">如何用 Keras 在 Python 中开发用于序列分类的双向 LSTM</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">双向 LSTMs 是传统 LSTMs 的扩展，可以提高模型在序列分类上的性能</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">machinelearningmastery.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj nl nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">嵌入|机器学习速成班|谷歌开发者</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">嵌入是一个相对低维的空间，您可以将高维向量转换到其中。嵌入…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">developers.google.com</p></div></div><div class="oe l"><div class="ok l og oh oi oe oj nl nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a href="http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/" rel="noopener  ugc nofollow" target="_blank"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd iu gy z fp oa fr fs ob fu fw is bi translated">用 Word2Vec 进行文本分类</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">在上一篇文章中，我谈到了主题模型对于非 NLP 任务的有用性，这次又回到了 NLP 领域。我…</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">nadbordrozd.github.io</p></div></div></div></a></div></div></div>    
</body>
</html>