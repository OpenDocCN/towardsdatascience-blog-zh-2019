<html>
<head>
<title>An Introduction to Apache, PySpark and Dataframe Transformations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache、PySpark 和 Dataframe 转换简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-apache-pyspark-and-dataframe-transformations-2a6d4229f0e3?source=collection_archive---------5-----------------------#2019-06-12">https://towardsdatascience.com/an-introduction-to-apache-pyspark-and-dataframe-transformations-2a6d4229f0e3?source=collection_archive---------5-----------------------#2019-06-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="20eb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">掌握大数据分析的全面指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4bac02f80d59bbd777a63a26d3bcf534.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LQ-DqOaz85RlNqHc"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Picture from <a class="ae ky" href="https://unsplash.com/photos/klWUhr-wPJ8" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="5441" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">简介:大数据问题</h1><p id="5e10" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Apache 是作为数据分析的新引擎和编程模型出现的。它的起源可以追溯到 2009 年，它在最近几年变得如此重要的主要原因是由于经济因素的变化，这些因素强调了计算机应用程序和硬件。</p><p id="903a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从历史上看，计算机的能力只会随着时间的推移而增长。每年，新的处理器都能够更快地执行操作，运行在其上的应用程序也自动变得更快。</p><p id="ca91" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">所有这一切在 2005 年发生了变化，当时散热的限制导致从提高单个处理器的速度转向探索 CPU 内核的并行化。这意味着应用程序和运行它们的代码也必须改变。所有这些都为 Apache Spark 等新模型奠定了基础。</p><p id="debf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，传感器和存储单元的成本仅在过去几年有所下降。如今收集和储存大量信息是完全不可能的。</p><p id="d8bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有如此多的数据可用，处理和分析数据的方式也必须彻底改变，通过在计算机集群上进行大规模并行计算。这些集群能够同时协同组合这些计算机的能力，并使处理数据处理等昂贵的计算任务变得更加容易。</p><p id="d361" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这就是 Apache Spark 发挥作用的地方。</p><h1 id="c6c6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是 Apache Spark</h1><p id="5a23" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如伟大的著作《火花——权威指南》所述:</p><blockquote class="ms"><p id="a772" class="mt mu it bd mv mw mx my mz na nb mm dk translated">“Apache Spark 是一个统一的计算引擎和一组用于在计算机集群上进行并行数据处理的库”</p></blockquote><p id="bfda" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">如今，Apache Spark 是最流行的大数据处理开源引擎。主要原因是:</p><ul class=""><li id="b351" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">它支持广泛使用的编程语言，如:Python、Scala、Java 和 r。</li><li id="2cb4" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">它支持 SQL 任务。</li><li id="dacc" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">它支持数据流。</li><li id="8576" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">它有机器学习和深度学习的库。</li><li id="6d57" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">它可以在单台机器上运行，也可以在一群计算机上运行。</li></ul><p id="1b9d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是一个示意图，展示了 Spark 生态系统中可用的不同库。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/933e146455c384cf3cd541dce9323d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wli21z6ZETBdHvod2xBYKQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure by the Author</figcaption></figure><h1 id="1aa5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">如何设置和运行 Apache Spark</h1><p id="eb8a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这一系列文章中，我们将重点关注 Apache Spark Python 的库 PySpark。如前所述，Spark 既可以在本地运行，也可以在计算机集群中运行。有几种方法可以配置我们的机器在本地运行 Spark，但是不在本文的讨论范围之内。</p><p id="f188" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用 PsyPark 并释放其 inmense 处理能力的最简单、最快速的方法之一是使用免费网站 Databricks，具体来说就是使用它的社区版。</p><p id="f95f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要开始，我们只需访问:</p><div class="nw nx gp gr ny nz"><a href="https://databricks.com/try-databricks" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd iu gy z fp oe fr fs of fu fw is bi translated">尝试数据块</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">无限集群，可扩展至任何规模的作业调度程序，为生产管道执行作业完全交互式…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">databricks.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on ks nz"/></div></div></a></div><p id="481f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">并选择其社区版:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/45cae1be451cbde73b59aacfe7b4aec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L0yB_NoTdxnso80hcCNUfg.png"/></div></div></figure><p id="6a3f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们必须创造和说明。</p><h2 id="5ba2" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">运行临时集群</h2><p id="8f1d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">一旦我们创建了一个帐户，为了能够开始工作，我们应该创建一个临时集群。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/78ab42f80cbfa393642d619380a1e67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K-jQhSLH-dyQDVbweHI2nQ.png"/></div></div></figure><p id="3d1d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">由于是免费版本，这些集群的默认内存为 6 Gb，每个集群可以运行 6 个小时。为了开发工业项目或使用数据管道，建议使用 premiun 平台。</p><p id="99f7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">但是对于这些教程来说，社区版已经足够了。</p><h2 id="a16a" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">添加数据</h2><p id="20d9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了添加要使用的数据:</p><ul class=""><li id="0d0b" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">单击数据选项卡</li><li id="1b41" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">然后添加数据</li></ul><p id="e15a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可以使用其他用户上传的可用数据，也可以使用从您的计算机上传的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/af3b694eb8ae441917ced43621465c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*tarQhVGX4hh8r-yooRw-Wg.png"/></div></figure><p id="789f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完成后，我们可以在笔记本中创建一个表格，这样我们就都设置好了！</p><h1 id="5226" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Pyspark 应用和分区</h1><p id="39b1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了理解 Apache Spark 如何工作，我们应该讨论 Spark 应用程序的核心组件:驱动程序、执行器和集群管理器。</p><p id="8dc8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面是一个 Spark 应用程序架构的示意图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/80ae995c68aa0a135a901c7083b1d8fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMcPY0MTVaUVeXJkLH_KcQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure by the Author</figcaption></figure><h2 id="f769" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">驾驶员</h2><p id="9f1a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">该驱动程序位于计算机集群的一个节点中，并执行三个主要任务:</p><ol class=""><li id="29f4" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm pe nn no np bi translated">保存关于 Spark 应用程序的信息</li><li id="8688" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm pe nn no np bi translated">响应输入，例如用户的程序</li><li id="ea3b" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm pe nn no np bi translated">分析、分配和安排执行者要完成的任务。</li></ol><h2 id="68eb" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">实施者</h2><p id="b3a2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">执行者是实际执行驱动程序分配的工作的人。他们做两件事:</p><ol class=""><li id="ca3d" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm pe nn no np bi translated">执行分配给他们的代码。</li><li id="35b1" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm pe nn no np bi translated">向驱动程序报告计算的状态。</li></ol><h2 id="7ef6" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">集群管理器</h2><p id="0f16" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">集群管理器负责:</p><ol class=""><li id="d0a9" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm pe nn no np bi translated">控制物理计算机</li><li id="8e17" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm pe nn no np bi translated">将资源分配给 Spark 应用</li></ol><p id="c57b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可以有几个 Spark 应用程序同时在同一个集群上运行，所有这些应用程序都将由集群管理器管理。</p><h1 id="2d5f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">PySpark 数据帧</h1><p id="2ad8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Apache Spark 使用几个数据抽象，每个抽象都有一个特定的接口。最常见的抽象是:</p><ul class=""><li id="440d" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">数据集</li><li id="08e3" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">数据帧</li><li id="e96c" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">SQL 表</li><li id="b5e2" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">弹性分布式数据集</li></ul><p id="deac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在本系列中，我们将重点关注在 Apache Spark 中表示和存储数据的最常见的单元 Dataframes。</p><p id="5c5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据帧是具有行和列的数据表，理解它们最接近的类比是具有带标签的列的电子表格。</p><p id="5f93" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据帧的一个重要特征是它们的模式。数据帧的模式是一个列表，其中包含列名和每列存储的数据类型。</p><p id="39e5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">数据帧的其他相关属性是它们不位于一台简单的计算机中，事实上它们可以被分割到数百台机器中。这是因为优化了信息处理，并且当数据太大而不适合单台机器时。</p><h1 id="3e99" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">Apache 分区</h1><p id="2531" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如前所述，执行器执行驱动程序分配的工作，并且它们以并行方式执行，为了能够做到这一点，将数据火花分割到不同的分区。</p><p id="9ef1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些分区是位于群集内单台计算机中的行的集合。当我们谈论 Dataframe 的分区时，我们谈论的是数据如何分布在我们集群上的所有机器上。</p><p id="6038" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">大多数情况下，我们不会明确指定如何在集群中进行分区，但通过我们的代码，我们将传输数据的高级转换，Spark 将自行意识到哪种方式是执行这些分区的最佳方式。总是寻求获得最大的处理效率。</p><p id="e3b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">执行这些操作的低级 API 超出了本系列的范围。</p><h1 id="fab7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">数据框架转换</h1><p id="e508" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">首先，我们必须明白，转换是我们指定对数据帧进行的修改。</p><p id="5497" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些转换是以一种高级的方式指定的，并且直到我们明确地调用一个动作时才会被执行。</p><p id="f3e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种工作方式叫懒评，目的是提高效率。当我们要求进行转换时，Spark 会设计一个计划来优化执行这些任务，直到最后一分钟我们要求一个动作(如。显示()或。收集())</p><h1 id="6ab5" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">苹果股价</h1><p id="8449" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们将探讨一些最常见的操作和转换。我们将从 2010 年到 2016 年研究苹果股价的数据。我们将执行一些探索性的数据分析、数据转换、处理缺失值并执行分组和聚合。</p><h2 id="1038" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">导入数据框架</h2><p id="d28d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">要初始化和显示数据帧，代码如下:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="8989" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># File location and type</strong><br/>file_location = "/FileStore/tables/appl_stock.csv"<br/>file_type = "csv"</span><span id="d3db" class="op la it pg b gy po pl l pm pn"># CSV options<br/>infer_schema = "true"<br/>first_row_is_header = "true"<br/>delimiter = ","</span><span id="b298" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># The applied options are for CSV files. For other file types, these will be ignored.</strong><br/>df = spark.read.format(file_type) \<br/>  .option("inferSchema", infer_schema) \<br/>  .option("header", first_row_is_header) \<br/>  .option("sep", delimiter) \<br/>  .load(file_location)</span><span id="6a5a" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Display Dataframe</strong><br/>display(df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/488459de69fb4d4916f15b5835f2cf00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPHar6jPgAOCnqv2UY-C5g.png"/></div></div></figure><h2 id="34f3" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">获取数据框架的模式</h2><p id="24df" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">数据帧的模式是数据结构的描述，它是 StructField 对象的集合，并提供关于数据帧中数据类型的信息。</p><p id="55fe" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">显示数据帧的模式非常简单:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="9402" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Display Dataframe's Schema<br/></strong>df.printSchema()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/c7bceb7d00e331900be682a460076896.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*IJMqrSHrP6vSVB3aO2bq7Q.png"/></div></figure><h2 id="1a32" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">执行过滤和转换</h2><p id="f47f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了过滤我们的数据，只获取那些收盘价低于$500 的行，我们可以运行下面一行代码:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="a9c4" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Filter data usign pyspark</strong><br/>df.filter(" Close &lt; 500").show())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/c142d00f88692c87640f1fbf405806a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*088gmPpI3rfRBB93VTvh-w.png"/></div></div></figure><p id="031b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们还可以过滤以仅获取某些列:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="a7ae" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Filter data by columns</strong><br/>df.filter("Close &lt; 500").select(['Open', 'Close']).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/86e9b30441e24923c88cb252c0af15bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*QGC9mFnyI-puxm0QVLtcoQ.png"/></div></figure><p id="aa8c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要按一列过滤并显示另一列，我们将使用。选择()方法。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="4cf5" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Filter by one column and show other</strong><br/>df.filter(df['Close'] &lt; 500).select('Volume').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/3918558f187a86268f406c48cc1921d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*ZcIAdWYZnB_B-NPc21mS0w.png"/></div></div></figure><p id="3eb2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要按多个条件过滤:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="f540" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Filter by multiple conditions: closing price &lt; $200 and opening price &gt; $200</strong><br/>df.filter( (df['Close'] &lt; 200) &amp; (df['Open'] &gt; 200) ).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/720974469c5243f2c363d898d62b9791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7eHFShJCdjksceBg5BK7A.png"/></div></div></figure><p id="ed8b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">获取数据的统计汇总</strong></p><p id="c6c2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与 Pandas 等其他库类似，我们可以通过简单地运行。describe()方法。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="cac3" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Display Statistic Summary</strong><br/>df.describe().show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/f20a0134371519c3e29dcf07853cd7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N2sApC8iLwwbNvFX57urgg.png"/></div></div></figure><h2 id="ac7f" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">添加和重命名列</h2><p id="4684" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">要向 dataframe 添加新列，我们将使用。withColumn()方法如下。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="c855" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Display Dataframe with new column</strong><br/>df.withColumn('Doubled Adj Close', df['Adj Close']*2).select('Adj Close', 'Doubled Adj Close').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/4165fc24446a130afbcde6dd1a62ee61.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*FSzcuZgyPFdXIGoySDLonA.png"/></div></figure><p id="5355" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要重命名现有的列，我们将使用。withColumnRenamed()方法。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="b518" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Display Dataframe with renamed column</strong><br/>df.withColumnRenamed('Adj Close', 'Adjusted Close Price').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/ef4e1ccd01067f582763e946fe8f7b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suzHQsnMgWZVqhXm6Xn1gw.png"/></div></div></figure><h1 id="03f4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">分组和聚合数据</h1><p id="298c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在，我们将对我们的数据进行一些整理和汇总，以获得有意义的见解。但是首先，我们应该导入一些库</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="75f8" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Import relevant libraries</strong><br/>from pyspark.sql.functions import dayofmonth,hour,dayofyear,weekofyear,month,year,format_number,date_format,mean, date_format, datediff, to_date, lit</span></pre><p id="366b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们创建一个新列，每行包含年份:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="1a84" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># To know th average closing price per year</strong><br/>new_df = df.withColumn('Year', year(df['Date']))<br/>new_df.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/c936bd9920dab6d5347b1931f7d07e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7FNqT3sTPEwRquDkuFtJhg.png"/></div></div></figure><p id="ba43" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们按最近创建的“年度”列进行分组，并按每年的最高、最低和平均价格进行聚合，以获得对价格状态和演变的有意义的见解。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="3e61" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Group and aggregate data</strong><br/>new_df.groupBy('Year').agg(f.max('Close').alias('Max Close'), f.min('Close').alias('Min Close'), f.mean('Close').alias('Average Close')).orderBy('Year').show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/96564d6508fa690adcbe2668aa98c42b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*QwKHERwLCz4xixL7ffyd5A.png"/></div></figure><p id="28d7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们已经实现了我们的目标！然而，我们仍然有一些非常难读的数据。事实上，我们有比我们需要的更多的小数。</p><p id="c734" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">考虑到我们正在处理数百美元的价格，超过两位小数并不能为我们提供相关信息。</p><p id="8003" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，让我们利用这一优势，学习如何格式化结果，以显示我们想要的小数位数。</p><h2 id="0e2b" class="op la it bd lb oq or dn lf os ot dp lj ma ou ov ll me ow ox ln mi oy oz lp pa bi translated">格式化我们的数据</h2><p id="991f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了格式化我们的数据，我们将使用 format_number()函数，如下所示:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="d536" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Import relevant functions</strong><br/>from pyspark.sql.functions import forman_number, col</span><span id="ec71" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Select the appropiate columns to format<br/></strong>cols<strong class="pg iu"> = </strong>['Max Close', 'Min Close', 'Average Close']</span><span id="8ef8" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Format the columns<br/></strong>formatted_df = new_df.select('Year', *[format_number(col(col_name), 2).name(col_name) for col_name in cols])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/f5ea51ac67f607747094d36f136fba30.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*mwiYodNKZaWZoBkM0mbfjQ.png"/></div></figure><h1 id="9b66" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">用户定义的函数</h1><p id="0ed4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在让我们学习如何将我们定义的函数应用到我们的数据帧中。我们将在本例中使用它来获取一个列，其中记录了每行的月份。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="2dc7" class="op la it pg b gy pk pl l pm pn"><strong class="pg iu"># Import relevant functions</strong><br/>from pyspark.sql.functions import date_format, datediff, to_date, lit, UserDefinedFunction, month<br/>from pyspark.sql.types import StringType<br/>from pyspark.sql import functions as F</span><span id="b613" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Create month list<br/></strong>month_lst = ['January', 'Feburary', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']</span><span id="039b" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Define the function<br/></strong>udf = UserDefinedFunction(lambda x: month_lst[int(x%12) - 1], StringType())</span><span id="aa61" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Add column to df with the number of the month of the year<br/></strong>df = df.withColumn('moy_number', month(df.Date))</span><span id="7dbf" class="op la it pg b gy po pl l pm pn"><strong class="pg iu"># Apply function and generate a column with the name of the month of the year<br/></strong>df = df.withColumn('moy_name', udf("moy_number"))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/3e6811385f985c178c09acd78111c75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ByqYfQGlZFUoZhtw44qHEA.png"/></div></div></figure><p id="f3c5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">成功！</p><h1 id="09c3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="20bb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们讨论了:</p><ul class=""><li id="ba19" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">Apache Spark 的基础</li><li id="a368" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">我们对它的重要性和运作方式有了直觉</li><li id="b4e3" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">使用 PySpark 和 Dataframes 执行分析操作</li></ul><p id="7c2c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在接下来的文章中，我们将学习如何在 PySpark 中应用机器学习，并将这些知识应用到一些项目中。敬请期待！</p><h1 id="8541" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最后的话</h1><p id="aa13" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="qb">如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章</em> <a class="ae ky" href="https://medium.com/@rromanss23" rel="noopener"> <em class="qb">这里</em> </a> <em class="qb">。</em></p><p id="47db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="qb">如果你想了解更多关于机器学习、数据科学和人工智能的知识</em> <strong class="lt iu"> <em class="qb">请在 Medium </em> </strong> <em class="qb">上关注我，敬请关注我的下一篇帖子！</em></p></div></div>    
</body>
</html>