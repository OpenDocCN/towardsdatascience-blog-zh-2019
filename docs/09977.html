<html>
<head>
<title>Cluster analysis: theory and implementation of unsupervised algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">聚类分析:无监督算法的理论和实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cluster-analysis-theory-and-implementation-of-unsupervised-algorithms-87823c4c5e03?source=collection_archive---------21-----------------------#2019-12-30">https://towardsdatascience.com/cluster-analysis-theory-and-implementation-of-unsupervised-algorithms-87823c4c5e03?source=collection_archive---------21-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a51f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">包括 k-means、分层和 DBSCAN 的优缺点</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c54e105c6768c73f38b9f89c77a19697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c96QqKjrcyCLYNAM"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@arnaudmariat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Arnaud Mariat</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a102" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单地说，聚类就是根据某些属性分离观察值。用更专业的术语来说，聚类是一种无监督的机器学习算法，是一种将观察值(数据)以相似的观察值彼此更接近的方式分组的过程。这是一种“非监督”算法，因为与监督算法(如随机森林)不同，你不必用标记的数据来训练它，而是将你的数据与一些指令(如你想要的聚类数)一起放入“聚类机器”，机器将找出其余的，并根据底层模式和属性对数据进行聚类。</p><p id="b300" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">李·RCT(1981):</p><blockquote class="lv lw lx"><p id="81ae" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">聚类分析是一种新发展起来的面向计算机的数据分析技术。它是许多研究领域的产物:统计学、计算机科学、运筹学和模式识别。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/923e7954ca6d52a69dec17b820afd1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*TCILsPRt1EG7eCv6jnUAKQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">What does it mean for data to be clustered?</figcaption></figure><p id="72e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的目的是强调一些行业应用，并讨论最常用的聚类算法的优缺点。在第二部分中，我将演示 K-means 聚类的一个实现，作为 Python 环境中的一个例子。<strong class="lb iu">最后，我将为数据科学家留下一些额外的技术笔记。</strong></p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="552b" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">行业应用</h2><p id="d378" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">为什么聚类在统计学和机器学习领域如此受欢迎？这是因为在广泛的商业应用案例中，聚类分析是一种强大的数据挖掘工具。这里只是众多应用中的几个:</p><ul class=""><li id="f932" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><strong class="lb iu">探索性数据分析(EDA) </strong>:聚类是最基本的数据分析技术的一部分，用于理解和解释数据，并开发对数据特征和模式的初步直觉。</li><li id="a784" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">统计分析</strong>:常用于识别同一变量在不同样本中的(不)相似性(如 A 城市<em class="ly">vs</em>B 城市儿童的数学成绩)。</li><li id="1d98" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">城市规划</strong>:集群有助于识别具有相似特征的家庭和社区，以实施适当的社区发展政策。</li><li id="ba58" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">异常检测:</strong>保险行业使用聚类来识别异常和潜在的欺诈交易。</li><li id="88a4" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">客户细分:</strong>聚类广泛用于开发营销策略，例如，针对不同类别的客户进行不同种类的促销。</li><li id="2ddf" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">计算机视觉</strong>:在计算机视觉/图像分割中，聚类用于根据模式识别过程将数据划分为不相交的组。</li><li id="2582" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">在生物学中</strong>:聚类是遗传学和分类学分类以及理解现存和灭绝生物进化的基本工具。</li><li id="da72" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">和许多其他的</strong>:聚类有广泛的其他应用，如建筑推荐系统、社会媒体网络分析、土地利用分类中的空间分析等。</li></ul><h2 id="1c5f" class="mk ml it bd mm mn mo dn mp mq mr dp ms li mt mu mv lm mw mx my lq mz na nb nc bi translated">许多不同的聚类算法</h2><p id="6d36" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">聚类算法家族有几种变体:K-means、hierarchical、DBSCAN、spectral、gaussian、birch、mean shift 和 affinity propagation 就是其中的一些。下面，我将重点介绍前三种算法的一些要点，它们是最常用的算法。</p><p id="1ece" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">K-表示</strong>:首先，“<em class="ly"> K </em>表示你想要的集群数。也就是说，<em class="ly"> K = n </em>表示要识别的聚类的数量<em class="ly"> n </em>。还有一种叫做“质心”的东西，它是一个假想的/人为的数据点(数据点的平均值)，每个数据聚类都围绕着它进行分区。因此<em class="ly"> K = 2 </em>意味着该算法将把观测值(数据)分成两个群，使得质心和观测值之间的距离最小化。</p><blockquote class="lv lw lx"><p id="d016" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">优点:<em class="it">简单易懂，易于实现</em></p><p id="e9df" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">缺点:<em class="it">有时难以选择</em>K<em class="it">；离群值可以在它们的方向上拖动质心；缩放数据可以改变群集</em></p></blockquote><p id="c2d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">层次聚类:</strong>层次聚类以两种不同的方式工作:第一种称为“自下而上”或聚集聚类，其中每个观察值得到自己的聚类，然后每对聚类合并在一起形成另一个聚类，以此类推。另一个(<em class="ly">又名</em>)。“自上而下”或分裂聚类)的工作方向相反，<em class="ly">即</em>，所有的观测值都是从一个集群开始，然后重复划分成更小的集群大小。</p><blockquote class="lv lw lx"><p id="c494" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">优点:<em class="it">易于实现；通过查看树状图，很容易确定聚类的数量；比 K-均值聚类更具信息量</em></p><p id="c27b" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">缺点:<em class="it">对异常值高度敏感；对于大型数据集可能非常耗时</em></p></blockquote><p id="7162" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> DBSCAN: </strong> <a class="ae ky" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.121.9220" rel="noopener ugc nofollow" target="_blank">于 1996 年</a>提出，它是一种基于密度的算法，在给定最小点数的情况下，根据观测值彼此的接近程度对它们进行聚类。它需要两个参数:(I)<em class="ly">ε</em>(<em class="ly">ε</em>)—确定点应该在一个聚类中的半径；以及(ii) <em class="ly"> minPts </em> —指定形成密集空间/聚类的最小点数。有趣的是，1996 年提出该算法的论文获得了 2014 年 KDD 大会的“<a class="ae ky" href="https://www.kdd.org/News/view/2014-sigkdd-test-of-time-award" rel="noopener ugc nofollow" target="_blank">时间检验奖</a>”。</p><blockquote class="lv lw lx"><p id="5ada" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">优点<em class="it">:与 K-means 和层次聚类不同，DBSCAN 在异常值存在的情况下是健壮的；因此可以用于异常(即异常值)检测。</em></p><p id="bc12" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">缺点:i <em class="it"> t 对参数值比较敏感(</em> ε <em class="it">和</em>T24】min pts)；无法在变化的数据密度中正确识别任何群集。</p></blockquote><h1 id="c565" class="nw ml it bd mm nx ny nz mp oa ob oc ms jz od ka mv kc oe kd my kf of kg nb og bi translated">5 步实施</h1><p id="a234" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">实现聚类算法的整个过程非常简单，因为与其他受监督的机器学习算法相比，在过程和参数调整中需要较少的人工决策。在这一节中，我将使用<code class="fe oh oi oj ok b">sklearn</code>库演示 Python 环境中的 K-means 集群实现。</p><p id="2d41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第一步。安装依赖关系</strong></p><p id="4224" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上你需要三个库:<code class="fe oh oi oj ok b">pandas</code>处理数据，<code class="fe oh oi oj ok b">seaborn</code>可视化，<code class="fe oh oi oj ok b">sklearn</code>输入预处理和建模。</p><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="1e0f" class="mk ml it ok b gy op oq l or os"># to import and work with data<br/>import pandas as pd <br/># to visualize data and results<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt <br/># to pre-process model inputs<br/>from sklearn import preprocessing <br/># clustering algorithm<br/>from sklearn.cluster import KMeans</span></pre><p id="cd83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二步。数据</strong></p><p id="a2d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在这个演示中使用的数据是著名的<em class="ly"> iris </em>数据集。我之所以选择这个数据集，是因为在散点图中可以很容易/直观地将聚类分开。</p><p id="f824" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在许多情况下，导入数据后，您可能需要做一些处理，如编码分类变量等。还需要确保没有<code class="fe oh oi oj ok b">NaN</code>价值观。</p><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="b9a9" class="mk ml it ok b gy op oq l or os"># import data<br/>df = pd.read_csv("iris.csv")<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ab4f9eb54526629895026ae82b75c01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*yzO3aSEQxKu8ouhkacILuQ.png"/></div></figure><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="1d91" class="mk ml it ok b gy op oq l or os"># select data<br/>df = df[["petal_length", "petal_width"]]</span><span id="ad1c" class="mk ml it ok b gy ou oq l or os"># see if there's NA values<br/>df.isna().sum()</span></pre><p id="d109" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第三步。准备模型输入</strong></p><p id="b9d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">选择数据集后，接下来以模型可以使用的方式预处理/格式化输入。在这个阶段会发生两件事:所选特征的标准化(减少数据的可变性)和将数据框转换成<code class="fe oh oi oj ok b">numpy</code>数组。</p><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="6868" class="mk ml it ok b gy op oq l or os"># data normalization<br/>df = preprocessing.scale(df)<br/>df = pd.DataFrame(df)</span><span id="cd3e" class="mk ml it ok b gy ou oq l or os"># input<br/>import numpy as np<br/>X = df.iloc[:, [0,1]].values</span></pre><p id="637b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第四步。确定集群数量</strong></p><p id="b533" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 K-means 算法中，你需要定义你想要的聚类数。所谓的“<a class="ae ky" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘法</a>可以通过最小化误差平方和来帮助确定。</p><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="4469" class="mk ml it ok b gy op oq l or os">##########################<br/># The "elbow" method #<br/>##########################</span><span id="bc7c" class="mk ml it ok b gy ou oq l or os">k_range = range(1,10)<br/>sse = []<br/>for k in k_range:<br/>    km = KMeans(n_clusters = k)<br/>    km.fit(X)<br/>    sse.append(km.inertia_)</span><span id="212d" class="mk ml it ok b gy ou oq l or os">plt.xlabel("K")<br/>plt.ylabel("Sum of squared errors")<br/>plt.plot(k_rng, sse, marker='o')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/839bfd03f5cec8833419d0cc70f04f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*-jY9Y4pkKgPgadfCzAX_aQ.png"/></div></figure><p id="69d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第五步。模型实现</strong></p><p id="f629" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦您在上一步中确定了唯一需要的参数，您就可以拟合模型，在二维图中可视化聚类数，并进一步分析以回答您正在寻找的研究问题。</p><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="fad6" class="mk ml it ok b gy op oq l or os"># model<br/>km = KMeans(n_clusters = 3)<br/>y_km=km.fit_predict(X)</span><span id="ff29" class="mk ml it ok b gy ou oq l or os"># plot the 3 clusters<br/>plt.scatter(<br/>    X[y_km == 0, 0], X[y_km == 0, 1],<br/>    s=50, c='lightgreen',<br/>    marker='s', edgecolor='black',<br/>    label='cluster 1'<br/>)</span><span id="0740" class="mk ml it ok b gy ou oq l or os">plt.scatter(<br/>    X[y_km == 1, 0], X[y_km == 1, 1],<br/>    s=50, c='orange',<br/>    marker='o', edgecolor='black',<br/>    label='cluster 2'<br/>)</span><span id="3f06" class="mk ml it ok b gy ou oq l or os">plt.scatter(<br/>    X[y_km == 2, 0], X[y_km == 2, 1],<br/>    s=50, c='lightblue',<br/>    marker='v', edgecolor='black',<br/>    label='cluster 3'<br/>)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/8c90b968c6a9c1f80cf24b07e73c7d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*9KJFVD7LS8K4hU9oZLGjhA.png"/></div></figure><pre class="kj kk kl km gt ol ok om on aw oo bi"><span id="b219" class="mk ml it ok b gy op oq l or os"># value counts in different clusters<br/>frame = pd.DataFrame(X)<br/>frame['cluster'] = y_km<br/>frame['cluster'].value_counts()</span></pre><h1 id="ef5d" class="nw ml it bd mm nx ny nz mp oa ob oc ms jz od ka mv kc oe kd my kf of kg nb og bi translated">数据科学家的技术笔记</h1><ol class=""><li id="08b4" class="ni nj it lb b lc nd lf ne li ox lm oy lq oz lu pa no np nq bi translated">就像其他机器学习算法一样，如果有分类特征，就需要用数字特征编码。</li><li id="f535" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu pa no np nq bi translated">需要重申的是，聚类算法对尺度敏感，因为该算法使用欧几里德距离。因此，请确保输入数据经过适当的预处理。</li><li id="7913" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu pa no np nq bi translated">请注意，与监督分类实现不同，在监督分类实现中，模型首先被拟合，然后用于预测，在聚类中，模型拟合和预测一起发生(因此使用了<code class="fe oh oi oj ok b">fit_predict())</code></li><li id="598f" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu pa no np nq bi translated">如果您对 K-means 实现中的哪些有效哪些无效感兴趣，请查看这个<a class="ae ky" href="https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means/133841#133841" rel="noopener ugc nofollow" target="_blank"> StackOverflow 页面</a>。</li><li id="44a5" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu pa no np nq bi translated">如果你对<code class="fe oh oi oj ok b">rstat</code>感兴趣，你可以看看这个<a class="ae ky" href="https://uc-r.github.io/kmeans_clustering" rel="noopener ugc nofollow" target="_blank"> UC 商业分析</a>页面，里面有代码和一些有用的讨论。</li></ol></div></div>    
</body>
</html>