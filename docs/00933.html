<html>
<head>
<title>Predict malignancy in cancer tumors with your own neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用你自己的神经网络预测癌症肿瘤的恶性程度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941?source=collection_archive---------4-----------------------#2019-02-12">https://towardsdatascience.com/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941?source=collection_archive---------4-----------------------#2019-02-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4495" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这个系列的最后一部分，我们使用我们从头编码的网络来预测乳腺癌肿瘤的恶性程度。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/72466212422375103346b40ec2b2953e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*--FDKBYGmG5oSBGCjkQRSQ.gif"/></div></figure><p id="6552" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><a class="ae lm" rel="noopener" target="_blank" href="/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504"> <strong class="ks iu">在本系列</strong></a>的第 1 部分中，我们深入了解了<strong class="ks iu">我们的神经网络</strong>的架构。<a class="ae lm" rel="noopener" target="_blank" href="/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2"> <strong class="ks iu">在第二部</strong></a><strong class="ks iu"/>中，我们<strong class="ks iu">用 Python 构建了它。</strong>我们还深入了解了反向传播和梯度下降优化算法。</p><p id="0853" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在<strong class="ks iu"/><strong class="ks iu">最终第 3 部分中，</strong>我们将使用<strong class="ks iu">威斯康星癌症数据集。我们将学会准备我们的数据，通过我们的网络运行它，并分析</strong>结果。</p><p id="bdfc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">是时候探索我们网络的损失情况了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ln lo l"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Navigating the Loss Landscape within deep learning training processes. Variations include: Std SGD, LR annealing, large LR or SGD+momentum. Loss values modified &amp; scaled to facilitate visual contrast. Visuals by Javier Ideami@ideami.com</figcaption></figure><h1 id="af57" class="lt lu it bd lv lw lx ly lz ma mb mc md jz me ka mf kc mg kd mh kf mi kg mj mk bi translated">打开网络</h1><p id="5a90" class="pw-post-body-paragraph kq kr it ks b kt ml ju kv kw mm jx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">为了打开我们的网络，<strong class="ks iu">我们需要一些燃料，</strong>我们需要<strong class="ks iu">数据</strong>。</p><ul class=""><li id="f886" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">我们将使用与乳腺癌肿瘤检测相关的真实数据集。</li><li id="6862" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">数据来自于<a class="ae lm" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.names" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu">威斯康辛癌症数据集</strong> </a> <strong class="ks iu">。</strong></li><li id="b5b0" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">这些数据是由麦迪逊的威斯康星大学医院和威廉·h·沃尔伯格博士收集的。</li><li id="da6b" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">应数据所有者的要求，我们提及与数据集相关的一项研究:O. L. Mangasarian 和 W. H. Wolberg:“通过线性规划进行癌症诊断”，《暹罗新闻》，第 23 卷，第 5 期，1990 年 9 月，第 1 和 18 页。</li><li id="b4a4" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">csv 格式的数据可以通过<a class="ae lm" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data" rel="noopener ugc nofollow" target="_blank">链接</a>下载</li><li id="ae26" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">在这个<a class="ae lm" href="https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu"> Github 链接</strong> </a>，可以访问项目的所有代码和数据。</li></ul><div class="ne nf gp gr ng nh"><a href="https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">javismiles/深度学习预测乳腺癌肿瘤恶性肿瘤</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">用 Python 从头开始编码的 2 层神经网络预测癌症恶性程度。…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ko nh"/></div></div></a></div><p id="6b19" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">首先，我们将数据下载到我们的机器上。然后，我们使用 pandas 创建一个<strong class="ks iu">数据帧，并查看它的第一行。</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="5c8e" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">df = pd.read_csv('wisconsin-cancer-dataset.csv',header=None)</strong><br/>df.head(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi og"><img src="../Images/0eccc8c62e6e50d9d195d31d05e7b619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9T3x5KjRDcLZFkvHGWpvNg.jpeg"/></div></div></figure><p id="89af" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">dataframe 是一种 python 数据结构，它允许我们非常容易地工作和可视化数据。</p><p id="529d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们需要做的第一件事是理解数据的结构。我们在它的网站上找到了关于它的关键信息。</p><ul class=""><li id="03ac" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated"><strong class="ks iu">共有 699 行，属于 699 名患者</strong></li><li id="7dc3" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">第一列是标识每个患者的<strong class="ks iu"> ID </strong>。</li><li id="2bf7" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">下面的<strong class="ks iu"> 9 列是特征</strong>，表示与检测到的肿瘤相关的不同类型的信息。它们代表与以下相关的数据:团块厚度、细胞大小的均匀性、细胞形状的均匀性、边缘粘附、单个上皮细胞大小、裸露的细胞核、平淡的染色质、正常的核仁和有丝分裂。</li><li id="2e6d" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">最后一列是肿瘤的类别，它有两个可能的值:<strong class="ks iu"> 2 </strong>表示肿瘤被发现为<strong class="ks iu">良性</strong>。<strong class="ks iu"> 4 </strong>表示发现为<strong class="ks iu">恶性</strong>。</li><li id="0e88" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">我们还被告知有几行包含丢失的数据。<strong class="ks iu">缺失数据</strong>在数据集中用<strong class="ks iu">表示。</strong>性格。</li><li id="1846" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">在数据集中的 699 名患者中，类别分布为:<strong class="ks iu">良性:458 名(65.5%)和恶性:241 名(34.5%) </strong></li></ul><p id="ccf3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这是有用的信息，可以让我们得出一些结论。</p><ul class=""><li id="bbd4" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">我们的目标是训练我们的神经网络，根据数据提供的特征预测肿瘤是良性还是恶性，。</li><li id="1534" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">网络的<strong class="ks iu">输入将由 9 个特征<strong class="ks iu">组成，9 列</strong>表示肿瘤的不同<strong class="ks iu">特征</strong>。</strong></li><li id="ea7e" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">我们将不使用保存患者 ID 的第一列。</li><li id="f563" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">我们将<strong class="ks iu">从数据集中删除任何包含丢失数据</strong>的<strong class="ks iu">行。性格)。</strong></li><li id="5763" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">在二进制分类的情况下，从两个类中获得较大比例的数据是有益的。我们有一个<strong class="ks iu"> 65%-35%的分布，</strong>这已经足够好了。</li><li id="3081" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">良性和恶性<strong class="ks iu">类</strong>用数字 2 和 4 标识<strong class="ks iu">。我们网络的最后一层通过它的<strong class="ks iu"> Sigmoid </strong>函数输出 0 到 1 之间的值。此外，当数据设置在从 0 到 1 的范围内时，神经网络往往工作得更好。因此，我们将更改 class 列的值，对于良性情况，将值保持为 0 而不是 2，对于恶性情况，将值保持为 1 而不是 4。(我们也可以改为缩放 Sigmoid 的输出)。</strong></li></ul><p id="4392" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们开始做这些改变。首先，我们将类值(在第 10 列)从 2 更改为 0，从 4 更改为 1</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9bc4" class="ob lu it nx b gy oc od l oe of">df.iloc[:,10].replace(2, 0,inplace=True)<br/>df.iloc[:,10].replace(4, 1,inplace=True)</span></pre><p id="a412" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，我们继续删除所有包含缺失值的行(由？character)位于第 6 列，我们已将其标识为包含它们的列。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="2d5d" class="ob lu it nx b gy oc od l oe of">df = df[~df[6].isin(['?'])]</span></pre><p id="743f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">那个“？”字符导致 Python 将第 6 列解释为由字符串组成。其他列由整数组成。我们将整个数据帧设置为由浮点数组成。这有助于我们的网络执行复杂的计算。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="4a43" class="ob lu it nx b gy oc od l oe of">df = df.astype(float)</span></pre><p id="0cad" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">接下来，让我们处理数据中值的范围。请注意 9 个特征中的数据是如何由超出 0 到 1 范围的数字组成的。真实的数据集通常是杂乱的，并且它们的值具有很大的范围差异:负数、列内的巨大范围差异等等。</p><p id="2947" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这就是为什么<strong class="ks iu">数据标准化</strong>是深度学习过程的<strong class="ks iu">特征工程</strong>阶段中关键的第一步。</p><p id="8b7e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">将<strong class="ks iu">数据</strong>规格化</strong>意味着<strong class="ks iu">以一种网络更容易消化</strong>的方式准备 it <strong class="ks iu">。我们正在帮助网络更容易、更快地收敛到我们所寻求的最小值。通常，神经网络对 0 到 1 范围内的数值数据集反应良好，对平均值为 0、标准偏差为 1 的数据也反应良好。</strong></p><p id="13fe" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">特征工程和标准化</strong>不是本文的重点，但让我们快速提及特征工程过程的这个阶段中的一些方法:</p><ul class=""><li id="8572" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">标准化方法的一个例子是通过对每个特征列应用<strong class="ks iu">最小-最大</strong>方法来重新调整我们的数据，使其符合 0 到 1 的范围。<br/><strong class="ks iu">new _ x =(x-min _ x)/(max _ x-min _ x)</strong></li><li id="ece3" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">我们还可以应用<strong class="ks iu">标准化</strong>，它将每个特性列的值居中，设置平均值为 0，标准偏差为 1。<br/> <strong class="ks iu"> new_x = (x 均值)/std.dev </strong></li></ul><p id="add0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">一些数据集和场景将从这些技术中获益更多。在我们的例子中，经过一些测试后，我们决定使用 sklearn 库应用最小-最大归一化:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="74fb" class="ob lu it nx b gy oc od l oe of">names = df.columns[0:10]<br/>scaler = MinMaxScaler() <br/><strong class="nx iu">scaled_df </strong>= scaler.fit_transform(<strong class="nx iu">df</strong>.iloc[:,0:10]) <br/><strong class="nx iu">scaled_df </strong>= pd.DataFrame(<strong class="nx iu">scaled_df</strong>, columns=names)</span></pre><p id="d2df" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们来看看所有这些变化之后的相同的 15 行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi ol"><img src="../Images/8e1975c4fe95fe9700ca707aae5c9a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B_K7pvFSMAtwq6HdkA90aQ.jpeg"/></div></div></figure><ul class=""><li id="8b3b" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">更改后，我们有 683 行。16 个缺失数据已被删除。</li><li id="4d64" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">所有的列现在都由浮点数组成，它们的值在 0 和 1 之间被规范化。(当我们稍后构建训练集时，将忽略列 0，即 id)。</li><li id="991b" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">最后一列 class 现在对良性肿瘤使用 0 值，对恶性肿瘤使用 1 值。</li><li id="cf30" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">请注意，我们没有对 class 列进行规范化，因为它已经保存了 0 到 1 范围内的值，并且它的值应该保持设置为 0 或 1。</li><li id="afd8" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">注意，最后一列，我们将用作目标的那一列，不需要是浮点型。它可以是整数，因为我们的输出只能是 1 或 0。(当我们训练网络时，我们将从原始的<strong class="ks iu"> df </strong> dataframe 中选取该列，该列被设置为 0 或 1)。</li><li id="8ba5" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">因此，我们的<strong class="ks iu"> scaled_df </strong> dataframe 包含所有规范化的列，我们将从数据集的非规范化版本<strong class="ks iu"> df </strong> dataframe 中选择 class 列。</li></ul><p id="072a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">随着我们探索更多的数据，这一过程可能会继续下去。</p><ul class=""><li id="e375" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">这 9 个特征都是必不可少的吗？我们要把他们都包括在培训过程中吗？</li><li id="2e40" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">我们有足够的高质量数据来产生好的训练、验证和测试集吗？(稍后详细介绍)</li><li id="6a54" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">研究这些数据，我们是否发现了任何有意义和有用的见解，可以帮助我们更有效地训练网络？</li></ul><p id="f5a8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这些以及更多都是培训开始前进行的<strong class="ks iu">功能工程流程</strong>的一部分。</p><p id="0ae5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">另一件有用的事情是构建图表，以不同的方式分析数据。myplotlib python 库帮助我们通过不同种类的图表来研究数据。</p><p id="9802" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们先把要研究的规格化列和 class 列结合起来，然后开始探索。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="babf" class="ob lu it nx b gy oc od l oe of">scaled_df[10]= df[10]</span><span id="387c" class="ob lu it nx b gy om od l oe of">scaled_df.iloc[0:13,1:11].plot.bar();<br/>scaled_df.iloc[0:13,1:11].plot.hist(alpha=0.5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi on"><img src="../Images/ac696cf841f9c5851013423f0560f2ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qGXn1jpat-TVV2UcFMwqhA.jpeg"/></div></div></figure><p id="85bc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">点击此链接，探索 Panda <a class="ae lm" href="https://pandas.pydata.org/pandas-docs/stable/visualization.html" rel="noopener ugc nofollow" target="_blank">提供的所有可视化选项</a></p><p id="1aef" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了加快文章的速度，在我们的例子中，我们认为 9 个特性是有用的。<strong class="ks iu">我们的目标是精确预测“类”列</strong>。</p><p id="459f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">那么，<strong class="ks iu">描述我们的 683 个样本和它们的输出之间的联系的函数</strong>会有多复杂呢？</p><p id="672f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">9 个特征和输出之间的关系显然是多维的和非线性的。</p><p id="18a8" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们通过网络运行数据，看看会发生什么。</p><p id="fcd9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在此之前，我们需要考虑一个关键话题:</p><ul class=""><li id="9557" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">如果我们使用 683 个样本，我们所有的样本，来创建我们的训练集，并获得良好的结果，我们将不得不面对一个关键问题。</li><li id="923e" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">如果</strong>网络以完全匹配训练期间使用的样本的方式设置其权重，而<strong class="ks iu">未能推广到训练集</strong>之外的新样本，该怎么办？</li><li id="a1dc" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">当<strong class="ks iu">使用训练数据时，或者当<strong class="ks iu">使用从未见过的数据时</strong>，我们的最终目标</strong>是否达到很高的<strong class="ks iu">准确度</strong>？显然，第二种情况。</li></ul><p id="359e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这就是深度学习实践者通常考虑三种数据集的原因:</p><ul class=""><li id="a197" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated"><strong class="ks iu">训练集:</strong>你用来训练网络的数据。它包含输入要素和目标标注。</li><li id="e716" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">验证集:</strong>一个单独的、不同的数据批次，理想情况下应该来自与训练集相同的分布。您将使用它来验证培训的质量。验证集也有目标标签。</li><li id="1329" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">测试集</strong>:另一批单独的数据，用于测试网络中的新相关数据，这些数据最好来自与验证集相同的分布。通常，测试集不带有目标标签。</li></ul><p id="9d5a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">不同集合相对于彼此的大小</strong>是另一个需要花些时间来描述的话题。出于我们的目的，考虑大部分数据形成了训练集，其中一小部分通常被提取(并从训练集中消除)成为验证集。</p><p id="ea24" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> 20% </strong>是一个典型的数字，通常被选为构成我们验证集的数据的百分比。</p><p id="a481" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">要估计网络的训练质量，比较训练集和验证集的性能是很有用的:</p><ul class=""><li id="a07b" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">如果在验证集上获得的<strong class="ks iu">损失值提高，然后开始变得更差</strong>，则网络<strong class="ks iu">过度拟合</strong>，这意味着网络已经学习了非常适合训练数据的函数，然而<strong class="ks iu">没有将</strong> <strong class="ks iu">足够好地推广到验证集</strong>。</li><li id="e48d" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">过度拟合的反义词是<strong class="ks iu">欠拟合</strong>，当网络的训练性能不够好时，我们在训练集和验证集中获得的损失值都太高(例如，训练损失比验证损失更严重)。</li><li id="edb8" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">理想情况下，您希望在两个数据集中获得相似的性能。</li><li id="eae3" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">当我们有<strong class="ks iu">过拟合</strong>时，我们可以应用<strong class="ks iu">正则化</strong>。正则化是一种对优化算法进行更改以使网络更好地泛化的技术。正则化技术包括剔除、L1 和 L2 正则化、提前停止和数据增强技术。</li></ul><p id="a369" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">总的来说，认识到验证集的成功是你真正的目标。如果网络不能很好地处理它以前没有见过的新数据，那么让网络在训练数据上表现得非常好也没有用。</p><p id="bf5a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，您真正的目标是达到一个良好的损失值，并通过验证集实现良好的准确性。</p><p id="6c76" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了达到这一点，过度拟合是我们需要防止的最重要的问题之一，这就是为什么正规化如此重要。让我们快速简单地回顾一下<strong class="ks iu"> 4 种广泛使用的正则化技术。</strong></p><p id="d92a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">退出</strong>:在每一轮训练中，我们会随机关闭一些隐藏的网络单位。这可以防止网络过分强调任何特定的权重，并有助于网络更好地推广。这就好像我们通过不同的网络架构运行数据，然后平均它们的影响，这有助于防止过度拟合。</p><p id="384e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu"> L1 和 L2 </strong>:我们在成本函数中增加了额外的项，当权重变得太大时，这些项会惩罚网络。这些技术鼓励网络在损失值和权重比例之间找到一个好的平衡。</p><p id="358a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">过早停止:</strong>过度适应可能是训练时间过长的结果。如果我们监控我们的验证错误，当验证错误停止改善时，我们可以停止训练过程。</p><p id="f1bd" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">数据扩充:</strong>通常，更多的训练数据意味着更好的网络性能，但是获取更多的数据并不总是可能的。相反，我们可以通过人为创建数据的变体来扩充现有数据。例如，在图像的情况下，我们可以应用旋转、平移、裁剪和其他技术来产生它们的新变体。</p><p id="05d1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">回到我们的数据。是时候挑选我们的训练集和验证集了。我们将选择 683 行中的一部分作为训练集，选择数据集的另一部分作为我们的<strong class="ks iu">验证集。</strong></p><p id="7fb1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">培训结束后，<strong class="ks iu">我们将通过验证集再次运行流程来验证我们网络的质量</strong>。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="588f" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">x=scaled_df.iloc[0:500,1:10].values.transpose()<br/>y=df.iloc[0:500,10:].values.transpose()</strong></span><span id="60ac" class="ob lu it nx b gy om od l oe of"><strong class="nx iu">xval=scaled_df.iloc[501:683,1:10].values.transpose()<br/>yval=df.iloc[501:683,10:].values.transpose()</strong></span></pre><p id="8286" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们决定用 683 行中的 500 行来构建我们的训练集，并且我们从标准化的<strong class="ks iu"> scaled_df </strong>数据帧中挑选它们。我们还确保删除第一列(id ),并且不包括网络的<strong class="ks iu">输入 x </strong>中的最后一列(class)</p><p id="ac1d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们使用对应于相同的 500 行的类列声明目标输出 y 。我们从原始的非规范化<strong class="ks iu"> df </strong> dataframe 中选择 class 列(因为 class 值应该保持为 0 或 1)。</p><p id="4867" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，我们为验证集选择接下来的 183 行，并将它们存储在变量<strong class="ks iu"> xval </strong>和<strong class="ks iu"> yval </strong>中。</p><p id="b2f9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们准备好了。我们将首先用我们的 x，y 训练集的 500 行来训练网络。<strong class="ks iu">之后</strong>，<strong class="ks iu">我们将使用我们的 xval，yval 验证集</strong>的 183 行来测试训练好的网络，以查看网络对它以前从未见过的数据的概括能力如何。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="a211" class="ob lu it nx b gy oc od l oe of">nn = dlnet(x,y)<br/>nn.lr=0.01<br/>nn.dims = [9, 15, 1]</span><span id="2435" class="ob lu it nx b gy om od l oe of">nn.gd(x, y, iter = 15000)</span></pre><p id="e928" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们声明我们的网络，设置一个学习率和每层的节点数(输入有 9 个节点，因为我们使用的是 9 个特征，不算网络的一层。第一隐藏层具有 15 个隐藏单元，第二和最后一层具有单个输出节点)。</p><p id="5e27" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，我们通过几千次迭代运行梯度下降算法。让我们用几秒钟的梯度下降来感受一下网络的训练效果。</p><p id="b5ca" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">每 x 次迭代，我们显示网络的损耗值。如果训练进展顺利，<strong class="ks iu">损失值将在每个周期后下降。</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="6dc3" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">Cost after iteration 0: 0.673967<br/>Cost after iteration 500: 0.388928<br/>Cost after iteration 1000: 0.231340<br/>Cost after iteration 1500: 0.171447<br/>Cost after iteration 2000: 0.146433<br/>Cost after iteration 2500: 0.133993<br/>Cost after iteration 3000: 0.126808<br/>Cost after iteration 3500: 0.122107<br/>Cost after iteration 12500: 0.101980<br/>Cost after iteration 13000: 0.101604<br/>Cost after iteration 14500: 0.100592</strong></span></pre><p id="7466" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">经过多次迭代后，我们的损失开始稳定在一个较低的水平。我们绘制了一个图表，通过迭代跟踪网络的损耗。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi og"><img src="../Images/1c6afd6b56b9747e7cc31800b57fdbca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nt9xFqN8dDtStKTSC909aw.jpeg"/></div></div></figure><p id="f52a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们的网络似乎训练得相当好，达到了低损耗值(我们的预测和目标输出之间的距离很小)。但是，<strong class="ks iu">有多好？</strong>最重要的是，它有多好，不仅仅是对整个训练集，更重要的是，<strong class="ks iu">对我们的验证集</strong>？</p><p id="291c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了找到答案，我们创建了一个新函数，<strong class="ks iu"> pred() </strong>，它通过网络运行一组输入，然后系统地将每个获得的输出与其对应的目标输出进行比较，以便<strong class="ks iu">产生一个平均精度值。</strong></p><p id="f464" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">请注意下面函数是如何研究预测值是高于还是低于 0.5 的。我们正在进行二元分类，默认情况下，我们认为高于 0.5 的输出值意味着结果属于其中一个类，反之亦然。</p><p id="d425" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">在这种情况下，因为 1 是恶性肿瘤的类值，我们认为高于 0.5 的输出预测恶性结果，低于 0.5 则相反。<strong class="ks iu">我们稍后将讨论如何、何时以及为什么要更改这个 0.5 的阈值。</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="28b2" class="ob lu it nx b gy oc od l oe of">def pred(self,x, y):  <br/>        self.X=x<br/>        self.Y=y<br/>        comp = np.zeros((1,x.shape[1]))<br/>        pred, loss= self.forward()    <br/>    <br/>        for i in range(0, pred.shape[1]):<br/>            if pred[0,i] &gt; 0.5: comp[0,i] = 1<br/>            else: comp[0,i] = 0<br/>    <br/>        print("Acc: " + str(np.sum((comp == y)/x.shape[1])))<br/>        <br/>        return comp</span></pre><p id="18f6" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，我们通过调用两次<strong class="ks iu"> pred </strong>函数，一次使用我们的训练集，另一次使用我们的验证集，来比较使用训练集和验证集时网络的准确性。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="0669" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">pred_train = nn.pred(x, y)<br/>pred_test = nn.pred(xval, yval)</strong></span></pre><p id="b826" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们得到了这两个结果。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9a1f" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">Acc: 0.9620000000000003<br/>Acc: 1.0</strong></span></pre><p id="d613" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">该网络在训练集(前 500 行)上的准确率为 96%，在使用验证集(接下来的 183 行)时的准确率为 100%。</p><p id="c7f9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">验证集上的准确率更高。这意味着网络没有过度拟合，并且<strong class="ks iu">泛化得足够好</strong>能够适应它以前从未见过的数据。</p><p id="53c5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们现在可以使用 nn.forward()函数直接比较与目标输出相关的验证集输出的前几个值:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ab1e" class="ob lu it nx b gy oc od l oe of">nn.X,nn.Y=xval, yval <br/>yvalh, loss = nn.forward()<br/>print("\ny",np.around(yval[:,0:50,], decimals=0).astype(np.int))       <br/>print("\nyh",np.around(yvalh[:,0:50,], decimals=0).astype(np.int),"\n")</span></pre><p id="0eb0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们得到了</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="968f" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">y [[0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0<br/>  0 0 0 0 0 0 0 0 0 0 0 0 0 1]]</strong></span><span id="8c1b" class="ob lu it nx b gy om od l oe of"><strong class="nx iu">yh [[0 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0<br/>  0 0 0 0 0 0 0 0 0 0 0 0 0 1]]</strong></span></pre><p id="08cf" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">两个<strong class="ks iu">完全匹配</strong>，因为我们已经在验证集上实现了<strong class="ks iu"> 100%的准确性</strong>。</p><p id="8e4e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，该函数很好地学习了<strong class="ks iu">以适应训练集和验证集。</strong></p><p id="6bd0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">分析准确性的一个很好的方法是绘制一个混淆矩阵。首先，我们声明一个自定义绘图函数。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="bab1" class="ob lu it nx b gy oc od l oe of">def plotCf(a,b,t):<br/>    cf =confusion_matrix(a,b)<br/>    plt.imshow(cf,cmap=plt.cm.Blues,interpolation='nearest')<br/>    plt.colorbar()<br/>    plt.title(t)<br/>    plt.xlabel('Predicted')<br/>    plt.ylabel('Actual')<br/>    tick_marks = np.arange(len(set(expected))) # length of classes<br/>    class_labels = ['0','1']<br/>    tick_marks<br/>    plt.xticks(tick_marks,class_labels)<br/>    plt.yticks(tick_marks,class_labels)<br/>    # plotting text value inside cells<br/>    thresh = cf.max() / 2.<br/>    for i,j in itertools.product(range(cf.shape[0]),range(cf.shape[1])):<br/>        plt.text(j,i,format(cf[i,j],'d'),horizontalalignment='center',color='white' if cf[i,j] &gt;thresh else 'black')<br/>    plt.show();</span></pre><p id="21dc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">(这个自定义的混淆矩阵函数来自<a class="ae lm" href="https://www.kaggle.com/jprakashds/confusion-matrix-in-python-binary-class" rel="noopener ugc nofollow" target="_blank">JP 创建的</a>这个公共 Kaggle)</p><p id="c427" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，我们再次运行 pred 函数两次，并为训练集和验证集绘制混淆矩阵。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="cb45" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">nn.X,nn.Y=x, y <br/>target=np.around(np.squeeze(y), decimals=0).astype(np.int)<br/>predicted=np.around(np.squeeze(nn.pred(x,y)), decimals=0).astype(np.int)<br/>plotCf(target,predicted,'Cf Training Set')</strong></span><span id="f844" class="ob lu it nx b gy om od l oe of"><strong class="nx iu">nn.X,nn.Y=xval, yval <br/>target=np.around(np.squeeze(yval), decimals=0).astype(np.int)<br/>predicted=np.around(np.squeeze(nn.pred(xval,yval)), decimals=0).astype(np.int)<br/>plotCf(target,predicted,'Cf Validation Set')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi oo"><img src="../Images/288a38184c9708531b8b0abdb76ce1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDKnhGGcvHha8LgrfeDiQA.jpeg"/></div></div></figure><p id="2a35" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我们可以更清楚地看到，我们的验证集在其 183 个样本上具有完美的准确性。至于训练集，500 个样本中有 19 个错误。</p><p id="7045" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，在这一点上，你可能会说，在像诊断肿瘤这样微妙的主题中，如果乙状结肠输出给出高于 0.5 的值，则将我们的预测设置为 1 并不是很好。在给出恶性肿瘤的预测之前，网络应该非常有信心。</p><p id="7b56" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我完全同意，那是非常正确的。这些都是你需要根据挑战的性质和你要处理的主题做出的决定。</p><p id="301e" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">然后，让我们创建一个名为<strong class="ks iu">阈值</strong>的新变量。它将控制我们的<strong class="ks iu">置信阈值，在我们确定肿瘤是恶性肿瘤之前，网络的输出需要多接近 1。默认情况下，我们将其设置为 0.5</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="9077" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">self.threshold=0.5</strong></span></pre><p id="7830" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">外部预测函数现在被更新以使用<strong class="ks iu">置信度阈值。</strong></p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="8378" class="ob lu it nx b gy oc od l oe of">def pred(self,x, y):  <br/>        self.X=x<br/>        self.Y=y<br/>        comp = np.zeros((1,x.shape[1]))<br/>        pred, loss= self.forward()    <br/>    <br/>        for i in range(0, pred.shape[1]):<br/>            <strong class="nx iu">if pred[0,i] &gt; self.threshold: comp[0,i] = 1</strong><br/>            else: comp[0,i] = 0<br/>    <br/>        print("Acc: " + str(np.sum((comp == y)/x.shape[1])))<br/>        <br/>        return comp</span></pre><p id="4f94" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，让我们随着置信度阈值的逐渐提高来比较我们的结果。</p><p id="e2a2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">置信度阈值:0.5。</strong>输出值需要高于 0.5 才能被视为恶性输出。如前所述，验证准确率为 100%，训练准确率为 96%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi op"><img src="../Images/461d86d78235396310cc628269e7598f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7xN6eL73jzEPBDAXW-TLw.jpeg"/></div></div></figure><p id="3e59" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">置信度阈值:0.7。</strong>输出值需要高于 0.7 才能被视为恶性输出。验证准确率保持在 100%，训练准确率下降到 95%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi op"><img src="../Images/5f48e8076d0a83905c22410d6825ca76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Yimg7tvAi1pDdNd99bmkg.jpeg"/></div></div></figure><p id="c685" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">置信度阈值:0.8。</strong>输出值需要高于 0.8 才能被视为恶性输出。第一次验证准确率非常非常轻微地下降到 99.45%。在混淆矩阵中，我们看到 183 个样本中有 1 个没有被正确识别。训练精度下降更多，直到 94.2%</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi op"><img src="../Images/519c32a4dc971062e642c57bcd87c813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WOwoaw8YdKGUn6d_nmppfQ.jpeg"/></div></div></figure><p id="0599" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">置信度阈值:0.9。</strong>最后，在 0.9 的情况下，输出值需要高于 0.9 才能被视为恶性输出。我们正在寻找几乎完全的信心。验证准确度稍微下降，直到 98.9%。在混淆矩阵中，我们看到 183 个样本中有 2 个没有被正确识别。训练准确率进一步下降到 92.6%。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi op"><img src="../Images/720fcac12c5002fe88cbdd1bea67dc9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1giuJUOakwG73GjbVKyGzQ.jpeg"/></div></div></figure><p id="8775" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，通过控制置信度阈值，我们可以适应挑战的特定需求。</p><p id="744d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果我们想要降低与我们的训练集相关的损失值(因为我们未能识别一小部分训练样本)，我们可以尝试训练更长时间，并且还可以使用不同的学习率。</p><p id="f340" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">例如</strong>，如果我们设置学习率为 0.07，训练 65000 次迭代，我们得到:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="7760" class="ob lu it nx b gy oc od l oe of"><strong class="nx iu">Cost after iteration 63500: 0.017076<br/>Cost after iteration 64000: 0.016762<br/>Cost after iteration 64500: 0.016443<br/>Acc: 0.9980000000000003<br/>Acc: 0.9945054945054945</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi oq"><img src="../Images/40f23e47488a5be4ff04303f906be6a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t14kqiCkVfkbYEU-qRztDQ.jpeg"/></div></div></figure><p id="8151" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">现在，将我们的置信阈值设置为 0.5，网络对两组中的每个样本都是准确的，除了每组中的一个样本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi or"><img src="../Images/48e8dd2d20bfca95c2d07c954e3ef5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nOdwU5StCJ4pwnvMErsdQ.jpeg"/></div></div></figure><p id="11a0" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果我们将置信度阈值提高到 0.7，性能仍然很好，只有 1 个验证样本和 2 个训练样本没有被正确预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi or"><img src="../Images/b6299496a7045004d0782dd49d42f7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N5UY1eP4MuMp617uD2SazA.jpeg"/></div></div></figure><p id="0fec" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最后，如果我们真的要求很高，并且将置信度阈值设置为 0.9，则网络无法正确猜测 1 个验证样本和 10 个训练样本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi or"><img src="../Images/9cde379c27391c35d2e90f7029517ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-G8gmrQLZzYrt0KTQhYuKA.jpeg"/></div></div></figure><p id="d947" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">虽然我们做得很好，但考虑到我们使用的是没有正规化的基本网络，当您处理更复杂的数据时，事情通常会变得更加困难。</p><p id="a778" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">通常情况下，亏损局面<strong class="ks iu">会变得非常复杂</strong>，而且<strong class="ks iu">更容易</strong>陷入错误的局部最小值，或者<strong class="ks iu">无法收敛</strong>到足够好的亏损。</p><p id="347a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">此外，根据网络的初始条件，我们可能会收敛到一个好的极小值，或者我们可能会在某处停滞不前，无法摆脱它。在这个阶段，再次描绘我们的初始动画是很有用的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/f6f184a98710c7a6a575d31f771e6435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Dk3GumlgjMnBbYhyE8jYhQ.gif"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Navigating the Loss Landscape. Values have been modified and scaled up to facilitate visual contrast.</figcaption></figure><p id="323a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">想象一下这样的风景，到处都是山丘和山谷，有些地方损耗很高，有些地方损耗很低。与复杂场景相关的损失函数的情况通常不一致(虽然可以使用不同的方法使其更加平滑，但这是一个完全不同的主题)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/fa01393523ebd17f22953e3dc1403580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tSIAOcrX933i8dWtqHX6w.jpeg"/></div></div></figure><p id="c14c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">到处都是深浅不一、角度各异的<strong class="ks iu">山丘和山谷</strong>。<strong class="ks iu">运行梯度下降算法时，您可以通过改变网络的损失值来改变地形</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/260761c64a82795b74e7ca3a4675efea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CqIKRaWFXFmNNOpUNt-ZGQ.jpeg"/></div></div></figure><p id="1c91" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">并且你移动的速度由<strong class="ks iu">学习率控制:</strong></p><ul class=""><li id="8722" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">如果你移动得非常慢，不知何故到达了一个不够低的高原或山谷，你可能会被困在那里。</li><li id="bf88" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">如果你走得太快，你可能会到达一个足够低的山谷，但穿过它，并以同样快的速度离开它。</li></ul><p id="6a58" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，有些非常微妙的问题会对您的网络性能产生巨大影响。</p><ul class=""><li id="44dc" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated"><strong class="ks iu">初始条件:</strong>在流程开始时，你把球丢在景观的哪个部分？</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/dbb1cc5352450fb3f3a2f6a7aec46c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tPl6_ruoy-ACCbnQbhhT-A.jpeg"/></div></div></figure><ul class=""><li id="5424" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated"><strong class="ks iu">你移动球的速度</strong>，学习率。</li></ul><p id="ed26" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最近<strong class="ks iu">在提高神经网络训练速度方面取得的许多进展</strong>都与不同的技术有关，这些技术<strong class="ks iu">动态地管理学习率</strong>，也与以更好的方式设置初始条件的新方法有关。</p><p id="1c75" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">关于初始条件:</strong></p><ul class=""><li id="0226" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated">记住，每一层计算前一层的权重和输入的组合(输入的<strong class="ks iu">加权和</strong>),并将该计算传递给该层的激活函数。</li><li id="76c9" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">这些激活函数的形状可以加速或停止神经元的动态变化，这取决于输入范围和它们对该范围的反应方式之间的组合。</li><li id="00d5" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">例如，如果 sigmoid 函数接收的值触发了接近其输出范围极值的结果，则激活函数在该范围部分的输出会变得非常平坦。如果它在一段时间内保持不变，导数，在那一点的变化率变为零或者非常小。</li><li id="a23e" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">回想一下<strong class="ks iu">是导数帮助我们决定下一步</strong>的走向。因此，如果导数没有给我们提供有意义的信息，网络将很难知道从该点开始下一步的方向。</li><li id="e7b1" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">就好像你已经到达了风景中的一个高原，你真的不知道下一步该去哪里，你只是不停地绕着那个点转圈。</li><li id="887a" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated">ReLU 也可能发生这种情况，尽管 ReLU 只有 1 个平面，而不是 2 个乙状结肠和 Tanh。<strong class="ks iu"> Leaky-ReLU </strong>是 ReLU 的一个变种，它稍微修改了函数的那一面(平坦的那一面),试图防止渐变消失。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi ot"><img src="../Images/57bfcff30b8f16466a7ae679df5ff596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N37fK-ELmeAE2IpABIt4dQ.jpeg"/></div></div></figure><p id="4b47" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因此，<strong class="ks iu">以可能的最佳方式</strong>设置我们的权重的初始值是至关重要的，以便在训练过程开始时单元的计算产生落入我们的激活函数的最佳可能范围内的输出。</p><p id="00e2" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这可能会使从开始的整个<strong class="ks iu">区别成为一个真正高的损失或更低的损失。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/fa01393523ebd17f22953e3dc1403580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tSIAOcrX933i8dWtqHX6w.jpeg"/></div></div></figure><p id="d812" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">管理学习率</strong>以防止训练过程太慢或太快，并<strong class="ks iu">使其值适应过程和每个参数的变化条件</strong>，是另一个复杂的挑战</p><p id="1199" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">谈论处理初始条件和学习率的许多方法需要几篇文章。我将简要描述其中的一些方法，让专家们了解一些应对这些挑战的方法。</p><ul class=""><li id="d504" class="mq mr it ks b kt ku kw kx kz ms ld mt lh mu ll mv mw mx my bi translated"><strong class="ks iu"> Xavier 初始化:</strong>一种初始化我们的权重的方法，这样神经元就不会开始处于饱和状态(陷入输出范围的微妙部分，在那里导数无法为网络提供足够的信息来知道下一步去哪里)。</li><li id="386b" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">学习率退火</strong>:高学习率会推动算法绕过并错过损失景观处的良好最小值。逐渐降低学习速度可以防止这种情况。有不同的方法来实现这种减少，包括:指数衰减、阶跃衰减和 1/t 衰减。</li><li id="8aab" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">fast . ai Lr _ find()</strong>:fast . ai 库的一种算法，为学习率寻找理想的取值范围。<strong class="ks iu"> Lr_find </strong>通过几次迭代训练模型。它首先尝试使用一个非常低的学习速率，并在每个小批量中逐渐改变速率，直到它达到一个非常高的值。每次迭代都会记录损失，一个图表可以帮助我们将损失与学习速度进行对比。然后，我们可以决定以最有效的方式减少损失的学习率的最佳值。</li><li id="c761" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">不同的学习率</strong>:在我们网络的不同部分使用不同的学习率。</li><li id="4bdb" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu"> SGDR，带重启的随机梯度下降</strong>:每 x 次迭代重置我们的学习率。如果我们陷入其中，这可以帮助我们走出不够低的高原或局部极小值。典型的过程是从高学习率开始。然后在每一个小批量中逐渐减少。经过 x 个周期后，您将其重置回初始高值，并再次重复相同的过程。这个概念是，从高速率逐渐移动到较低的速率是有意义的，因为我们首先从景观的高点(初始高损耗值)快速向下移动，然后缓慢移动以防止绕过景观的最小值(低损耗值区域)。但是，如果我们在某个不够低的高原或山谷中停滞不前，那么每 x 次迭代就将我们的速率重新设置为一个较高的值，这将有助于我们跳出这种情况，继续探索这一领域。</li><li id="1ab9" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu"> 1 周期策略:</strong>les lie n . Smith 提出的一种动态改变学习速率的方式，我们从一个较低的速率值开始，逐渐增加，直到达到最大值。然后，我们继续逐渐减少它，直到过程结束。最初的逐渐增加允许我们探索大面积的损失景观，增加我们到达不颠簸的低区域的机会；在循环的第二部分，我们在我们到达的低平地区安顿下来。</li><li id="8c0f" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">动量</strong>:随机梯度下降的一种变化，有助于加速通过损失景观的路径，同时保持总体方向受控。回想一下，SGD 可能很吵。动量平均化路径中的变化，使路径变得平滑，并加速向目标的移动。</li><li id="93ed" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">自适应学习率:</strong>为网络的不同参数计算和使用不同学习率的方法。</li><li id="53f3" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu"> AdaGrad </strong> ( <strong class="ks iu">自适应梯度算法):</strong> <br/>结合上一点，AdaGrad 是 SGD 的变体，它不是对所有参数使用单一的学习速率，而是对每个参数使用不同的速率。</li><li id="e193" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">均方根传播</strong> (RMSProp):像 Adagrad 一样，RMSProp 对每个参数使用不同的学习速率，并根据它们变化的平均速度来调整这些速率(这在处理嘈杂的环境时很有帮助)。</li><li id="4ca4" class="mq mr it ks b kt mz kw na kz nb ld nc lh nd ll mv mw mx my bi translated"><strong class="ks iu">亚当</strong>:它结合了 RMSprop 和 SGDR 的一些方面与动力。像 RMSprop 一样，它使用平方梯度来缩放学习速率，并且它还使用梯度的平均值来利用动量。</li></ul><p id="bcd5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果你对这些名字都不熟悉，不要不知所措。<strong class="ks iu">在它们大多数的背后是非常相同的根:反向传播和梯度下降。</strong></p><p id="4cb5" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">此外，在现代框架(如 fast.ai 库)中，许多这些方法都是自动为您选择的。理解它们是如何工作的是非常有用的，因为这样你就能更好地做出自己的决定，甚至研究和测试不同的变化和选择。</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h1 id="0ad1" class="lt lu it bd lv lw pb ly lz ma pc mc md jz pd ka mf kc pe kd mh kf pf kg mj mk bi translated">理解意味着更多的选择</h1><p id="7ca9" class="pw-post-body-paragraph kq kr it ks b kt ml ju kv kw mm jx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">当我们理解了网络的核心，基本的反向传播算法和基本的梯度下降过程，每当我们面临艰难的挑战时，我们就有更多的选择去探索和实验。</p><p id="0420" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">因为我们了解这个过程，所以我们意识到，例如在深度学习中，<strong class="ks iu">我们在损失范围内的初始位置是关键。</strong></p><p id="3425" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">一些初始位置会很快推动球(训练过程)卡在景观的某个部分。其他人会很快把我们逼到一个很好的最小值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/eb541cbaaa2e4e982e6b7200f69c5d21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hv5_KJNYnIldqmt9fIp9lQ.jpeg"/></div></div></figure><p id="f41d" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当神秘函数变得更加复杂时，就是时候加入我前面提到的一些高级解决方案了。现在也是时候更深入地研究整个网络的架构，并更深入地研究不同的超参数。</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h1 id="32fa" class="lt lu it bd lv lw pb ly lz ma pc mc md jz pd ka mf kc pe kd mh kf pf kg mj mk bi translated">浏览风景</h1><p id="ea88" class="pw-post-body-paragraph kq kr it ks b kt ml ju kv kw mm jx ky kz mn lb lc ld mo lf lg lh mp lj lk ll im bi translated">我们的损失状况在很大程度上受到网络架构设计以及超参数的影响，如学习率、我们的批量大小、我们使用的优化算法等。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/de69d0c5eb6264812793c4b860ec8362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d84KZNSMo7b-uh8-MwcErQ.jpeg"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="oh oi di oj bf ok"><div class="gh gi os"><img src="../Images/f328a8abd85d479362d988fa35deb092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kl7WSV3SdRUjGNj70x9isA.jpeg"/></div></div></figure><p id="03e3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">有关这些影响的讨论，请查看这篇论文:<a class="ae lm" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">李浩、、加文·泰勒、克里斯托夫·斯图德、汤姆·戈尔茨坦的《可视化神经网络的损失景观》</a>。</p><p id="d508" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">最近的研究得出了一个非常有趣的观点，即神经网络中的<strong class="ks iu">跳过连接</strong>模型如何平滑我们的损失景观，并使它变得更加简单和凸，增加我们收敛到好结果的机会。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/96d2548d0dc1758cbafc7958d3742cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*mM8p8jxWlvQhm9iZhPV2fg.gif"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Navigating the Loss Landscape. Values have been modified and scaled up to facilitate visual contrast.</figcaption></figure><p id="b2ae" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">跳过连接对训练非常深的网络帮助很大。</strong>基本上，跳过连接是链接不同层的节点的额外连接，跳过中间的一个或多个非线性层。</p><p id="2dd1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">当我们<strong class="ks iu">用不同的架构和参数实验</strong>时，我们<strong class="ks iu">正在修改我们的损失场景</strong>，使其更加崎岖或平滑，增加或减少局部最优解的数量。当我们优化初始化网络参数的方式时，我们正在提高我们的起点。</p><p id="bda9" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">让我们继续探索新的方法来应对世界上最迷人的挑战。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/7e70d8ed832e63e8bbe0cc8bd8d09da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*JYx0i-6AXnxGrdCVknUZvg.gif"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">Navigating the Loss Landscape. Values have been modified and scaled up to facilitate visual contrast.</figcaption></figure><p id="7363" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这篇文章涵盖了基础知识，从这里开始，<strong class="ks iu">前途无量</strong>！</p><p id="a94f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><strong class="ks iu">链接到本文的 3 个部分:</strong> <br/> <a class="ae lm" rel="noopener" target="_blank" href="/the-keys-of-deep-learning-in-100-lines-of-code-907398c76504"> <strong class="ks iu">第 1 部分</strong> </a> <strong class="ks iu"> </strong> | <a class="ae lm" rel="noopener" target="_blank" href="/coding-a-2-layer-neural-network-from-scratch-in-python-4dd022d19fd2"> <strong class="ks iu">第 2 部分</strong>|</a>|<a class="ae lm" rel="noopener" target="_blank" href="/predict-malignancy-in-breast-cancer-tumors-with-your-own-neural-network-and-the-wisconsin-dataset-76271a05e941"><strong class="ks iu">第 3 部分</strong> </a></p><p id="9a2f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated"><a class="ae lm" href="https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy" rel="noopener ugc nofollow" target="_blank"> <strong class="ks iu"> Github 仓库里有这个项目的所有代码</strong> </a></p><div class="ne nf gp gr ng nh"><a href="https://github.com/javismiles/Deep-Learning-predicting-breast-cancer-tumor-malignancy" rel="noopener  ugc nofollow" target="_blank"><div class="ni ab fo"><div class="nj ab nk cl cj nl"><h2 class="bd iu gy z fp nm fr fs nn fu fw is bi translated">javismiles/深度学习预测乳腺癌肿瘤恶性肿瘤</h2><div class="no l"><h3 class="bd b gy z fp nm fr fs nn fu fw dk translated">用 Python 从头开始编码的 2 层神经网络预测癌症恶性程度。…</h3></div><div class="np l"><p class="bd b dl z fp nm fr fs nn fu fw dk translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv ko nh"/></div></div></a></div></div></div>    
</body>
</html>