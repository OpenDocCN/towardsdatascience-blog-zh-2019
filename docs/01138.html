<html>
<head>
<title>[RL] Train the Robotic Arm to Reach a Ball — Part 01</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[RL]训练机械臂去够球—第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/rl-train-the-robotic-arm-to-reach-a-ball-part-01-1cecd2e1cfb8?source=collection_archive---------18-----------------------#2019-02-21">https://towardsdatascience.com/rl-train-the-robotic-arm-to-reach-a-ball-part-01-1cecd2e1cfb8?source=collection_archive---------18-----------------------#2019-02-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7f2aaa590178a0fe3f54ab5ae11d25c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xadG70WbZahpxJ6E"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@lamppidotco?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">James Pond</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="3400" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">比较 DDPG、D4PG 和 A2C 的学习效率</h2></div><h1 id="a607" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">1.摘要</h1><h2 id="7a04" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">1.1 激励机制</h2><p id="7c9e" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">看到我们如何利用强化学习的力量，训练机器人通过自我试验和错误来学习如何与环境互动，总是令人着迷。在这个任务中，我将实验和比较强化学习中不同流行算法的效率。这不仅有助于我们对这些算法有一个总体的了解，而且也让我们进一步相信强化学习在不久的将来会有多大的潜力。</p><p id="d052" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">为了恰当地处理这个话题，整个讨论将分为两个系列。本文是第一部分，主要介绍训练环境和一种学习算法——DDPG，这是最古老的方法。</p><h2 id="6410" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">1.2 培训环境</h2><p id="35ee" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在强化学习领域，组件由<em class="na">一个环境、一个或多个代理</em>和<em class="na">两者之间交换的信息</em>组成。首先，环境将当前情况的信息发送给代理，然后代理以其最好的可能方式作出响应，再次，环境将奖励和下一个情况的信息发送给代理，以此类推。</p><p id="da6e" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">在这里，为了给代理创建一个游乐场，我将使用 Unity 开发并由 Udacity 修改的环境[ <a class="ae jd" href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher" rel="noopener ugc nofollow" target="_blank"> <strong class="me jh"> Reacher </strong> </a> ]进行实验。省去了我们搭建环境和配置奖励机制的麻烦。当我们刚刚开始熟悉这个领域时，这使我们能够专注于实现、调整和比较 RL 算法。</p><p id="0a22" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">在<strong class="me jh"> Reacher </strong>任务中，机械臂需要学习如何控制和移动一个球。它粘球控制球的时间越长，积累的奖励就越多。环境的观测状态由 33 个变量组成，并且都在连续空间中。</p><p id="fc49" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">关于机械臂，有两种情况，一种是单个机器人代理，另一种是多个机器人代理，总共有 20 个代理，每个代理都有自己的环境副本。因为我们现在有了单个和多个代理的场景，所以我们可以探索和比较两者之间的学习效率。</p><p id="84f0" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">每个代理有 4 个动作变量，都在-1 和 1 之间的连续空间内。此外，单代理和多代理环境都可以在[<a class="ae jd" href="https://github.com/udacity/deep-reinforcement-learning/tree/master/p2_continuous-control" rel="noopener ugc nofollow" target="_blank"><strong class="me jh">uda city Github</strong></a>]中下载。</p><h2 id="3aba" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">1.3 实验的模型/算法</h2><p id="7150" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在单代理情况下，使用[<a class="ae jd" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"><strong class="me jh">【DDPG】</strong></a>]和[ <a class="ae jd" href="https://arxiv.org/abs/1804.08617" rel="noopener ugc nofollow" target="_blank"> <strong class="me jh">分布式确定性策略梯度(D4PG) </strong> </a> ]的算法。当在单个代理上训练时，最大的问题之一是过渡状态/经历的顺序将是相关的，因此像 DDPG/D4PG 这样的非策略将更适合这种情况。在这些模型中，对<strong class="me jh">重放存储器</strong>进行采样的做法打破了转换之间的相关性。</p><p id="c9e1" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">在多个代理的情况下，使用[ <a class="ae jd" href="https://blog.openai.com/baselines-acktr-a2c/" rel="noopener ugc nofollow" target="_blank"> <strong class="me jh">【同步优势演员评论家(A2C) </strong> </a>】。A2C 是[ <a class="ae jd" href="https://arxiv.org/abs/1602.01783" rel="noopener ugc nofollow" target="_blank"> <strong class="me jh"> A3C </strong> </a>的同步版。A2C 更容易实现，在一些研究中，它的性能甚至优于 A3C。在对多个智能体进行训练的情况下，由于每个智能体都有其独特的过渡经验，集体的过渡经验自然会解决顺序关联的问题。此外，我们甚至可以享受基于策略的学习算法带来的训练稳定性。</p><h2 id="4f0d" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">1.4 快速总结</h2><p id="22c1" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">那么，这个任务的目标是什么呢？在任务中，如果代理人的手臂在目标位置，那么给予+0.1 的奖励。代理能够将手臂保持在目标位置的时间越长，累积的奖励就越多。为了完成任务，代理人需要平均每集获得 30 英镑以上的奖励。</p><p id="abf5" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">从后面显示的结果中，我们可以清楚地看出<strong class="me jh"> A2C </strong>在训练速度方面明显优于其他两种算法，这并不奇怪，尽管如此，这种改进确实令人印象深刻。具体来说，在这个案例中，A2C 成功地训练代理人在不到 5 <strong class="me jh"> 00 的训练集</strong>内积累超过 30 的奖励(目标奖励)。而 D4PG 需要大约<strong class="me jh"> 5000 集</strong>，DDPG 甚至<strong class="me jh">无论花多少时间训练经纪人都无法完成任务。</strong></p><p id="f128" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">下面是这篇文章结构的快速指南。</p><h2 id="a704" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">1.5 报告的结构</h2><ul class=""><li id="a364" class="nb nc jg me b mf mg mi mj ls nd lv ne ly nf mu ng nh ni nj bi translated"><strong class="me jh"><em class="na">1 . 5 . 1</em></strong><a class="ae jd" href="#536e" rel="noopener ugc nofollow"><strong class="me jh"><em class="na">智能体如何与环境交互</em> </strong> </a> <br/>在本节中，我将显示如何使智能体与环境交互的代码(假设在单智能体场景中)，向环境发送其动作值，并从环境接收奖励和下一次观察状态的反馈。</li><li id="536e" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated"><strong class="me jh"><em class="na">1 . 5 . 2</em></strong><a class="ae jd" href="#2899" rel="noopener ugc nofollow"><strong class="me jh"><em class="na">在单个智能体场景上训练— DDPG </em> </strong> </a> <br/>在单个智能体的情况下，我将对两个算法进行实验— DDPG 和 D4PG。这一部分将详细演示 DDPG 模型是如何逐步建立起来的，包括演员和评论家的网络结构、重放记忆的设置、动作探索和损失函数等。最后，对训练结果进行了讨论。</li></ul></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="abb6" class="kv kw jg bd kx ky nw la lb lc nx le lf km ny kn lh kp nz kq lj ks oa kt ll lm bi translated">2.代理如何与环境交互</h1><p id="3a47" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated"><a class="ae jd" href="#a704" rel="noopener ugc nofollow"> ︽ </a>在这里，我将简单介绍一下 Unity 环境的样子，以及代理如何用代码与环境交互。</p><h2 id="7d2b" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">2.1 环境背景</h2><p id="0a65" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">注意到环境包含多个大脑。大脑根据接收到的动作输入来控制环境的反应。用不同的大脑指定环境，会产生不同的反馈。在这个任务中，使用默认的大脑(第一个)。</p><p id="9583" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated"><code class="fe ob oc od oe b"><strong class="me jh">env:</strong></code>包含每个大脑的环境对象。它包括，</p><ul class=""><li id="21ca" class="nb nc jg me b mf mv mi mw ls of lv og ly oh mu ng nh ni nj bi translated"><code class="fe ob oc od oe b">brain_name = env.brain_names[0]:</code>默认使用 env brain。</li><li id="e7fd" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated"><code class="fe ob oc od oe b">env.brains[brain_name].vector_action_space_size:</code>行动的维度。</li><li id="fb00" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated"><code class="fe ob oc od oe b">env[brain_name].agents:</code>内部代理数量信息。</li><li id="eb38" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated"><code class="fe ob oc od oe b">env[brain_name].vector_observations:</code>维数(代理数，观察状态数)。</li></ul><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Activate the Environment</figcaption></figure><figure class="oi oj ok ol gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/9a38b5339518cae805a1f244df931518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQGP6kokfbt53ccyXh2kOg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Preview of the Environment (Single Agent)</figcaption></figure><h2 id="dac6" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">2.2 代理与环境交互</h2><p id="e23b" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">对于每个新的情节，环境需要是<code class="fe ob oc od oe b">reset()</code> <strong class="me jh"> </strong>并因此给出初始观察状态。然后，代理通过<code class="fe ob oc od oe b">act()</code>对其从环境接收的状态做出响应。环境使用<code class="fe ob oc od oe b">step()</code>接收来自代理的当前动作，并用<em class="na">新状态</em>、<em class="na">奖励</em>和<em class="na">完成(一集结束的标记)</em>更新自身。</p><p id="b5ce" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">既然我们已经了解了所有必需的背景知识，我们就可以开始建模了。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Agent Interact with Environment</figcaption></figure></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="7c13" class="kv kw jg bd kx ky nw la lb lc nx le lf km ny kn lh kp nz kq lj ks oa kt ll lm bi translated">3.在单个代理场景下训练——DDPG</h1><h2 id="abf8" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.1 DDPG</h2><p id="b662" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated"><a class="ae jd" href="#a704" rel="noopener ugc nofollow"> ︽ </a>首先，我在训练开始前导入一些自定义模块来配置整个设置。<a class="ae jd" href="https://github.com/TomLin/RLND-project/tree/master/p2-continuous-control" rel="noopener ugc nofollow" target="_blank">模块</a>包括:</p><ol class=""><li id="b79a" class="nb nc jg me b mf mv mi mw ls of lv og ly oh mu op nh ni nj bi translated"><code class="fe ob oc od oe b">ddpg_model:</code>包含 DDPG 演员和评论家神经网络结构类的模块文件。</li><li id="2513" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu op nh ni nj bi translated"><code class="fe ob oc od oe b">noise:</code>奥恩斯坦-乌伦贝克噪声过程用于 DDPG 代理的勘探目的。</li><li id="5696" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu op nh ni nj bi translated"><code class="fe ob oc od oe b">replay_memory:</code>在训练中收集和取样进行过渡的经验。</li><li id="f96f" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu op nh ni nj bi translated"><code class="fe ob oc od oe b">ddpg_agent:</code>定义 DDPG 代理如何与环境交互并实施培训的模块文件。</li></ol><p id="f13e" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">下面是一系列代码片段，突出了每个自定义模块中最重要的部分。它帮助我们快速了解每个模块的核心。</p><h2 id="0659" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.2 模型结构</h2><p id="c1cf" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">网络的大部分结构遵循了最初[<a class="ae jd" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">DDGP 论文 T10 中实现的内容，脚本的主要部分参考了[</a><a class="ae jd" href="https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum" rel="noopener ugc nofollow" target="_blank">uda city 的 DDPG 模板 T12。</a></p><p id="3772" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">值得注意的一点是，critic 网络在计算 Q 值时采用了最后一个隐含层的动作。</p><p id="8094" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">但是，为了提高计算效率，在这种情况下需要调整一些超参数。例如，演员和评论家网络都包含两个隐藏层(每个具有<strong class="me jh"> 128 </strong>和<strong class="me jh"> 64 </strong>单位的大小)。下面显示的代码是 Critic Network 的初始化。</p><p id="6ce5" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">演员网络共享相似的结构。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Initialize DDPG Critic Network (excerpted)</figcaption></figure><h2 id="bc1f" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.3 重量初始化</h2><p id="4093" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在初始化时，评论家和演员网络的权重都采用<strong class="me jh">x 服务器初始化</strong>。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Function — Xavier Weights Initialization</figcaption></figure><h2 id="9364" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.3 重放记忆</h2><p id="d259" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">这里，为了减轻计算压力，重放存储器的大小仅设置为<strong class="me jh"> 100，000 </strong>。回放内存在<em class="na">均匀随机采样</em>中采样，并发送给【cuda 或 cpu 训练模型。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Create Replay Memory Object</figcaption></figure><h2 id="0e7b" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.4 行动探索</h2><p id="9c5a" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">奥恩斯坦-乌伦贝克噪声被添加到动作探索的动作值中。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Action Exploration of DDPG Agent</figcaption></figure><h2 id="9d5b" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.5 损失函数</h2><p id="577a" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">确定性政策梯度不同于随机政策梯度，因为它不包括行动的对数概率。</p><p id="a6b8" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">也就是说，策略(参与者)梯度是批评家网络相对于动作的梯度，乘以参与者网络相对于网络参数的梯度。在这篇[ <a class="ae jd" href="https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html" rel="noopener ugc nofollow" target="_blank">帖子</a>中阅读更多详情。</p><p id="d2b7" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">另一方面，批评值的损失只是常规的时间差异(TD)误差。因此，保单损失和批评价值损失定义为:</p><figure class="oi oj ok ol gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/84f35f0dfda3ff030b9e4d213637a88b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f7Bdtdlo9op4208mSBfbsw.png"/></div></div></figure><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Compute Loss for Critic and Actor for DDPG Agent</figcaption></figure><h2 id="3de9" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.6 重量更新</h2><p id="2e63" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在训练过程中，模型中的权重被<strong class="me jh">软更新</strong>。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Code — Implement Soft-Updated for Network Parameters</figcaption></figure><h2 id="94fb" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.7 简而言之，超参数</h2><p id="4e9b" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">以下是超参数设置的概述。</p><ul class=""><li id="49a3" class="nb nc jg me b mf mv mi mw ls of lv og ly oh mu ng nh ni nj bi translated">学习率(演员/评论家):1e-4</li><li id="3f02" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">重量衰减:1e-2 # L2 重量衰减</li><li id="31c7" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">批量:64</li><li id="b350" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">缓冲区大小:100000</li><li id="6352" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">Gamma: 0.99 #奖励折扣系数</li><li id="9d03" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">Tau: 1e-3 #目标网络中参数的软更新</li><li id="3e96" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">每次重复学习:10</li><li id="da41" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">每时间步发生的学习:20</li><li id="bf5d" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">评论家剪辑的最大渐变:1</li><li id="7c3f" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">隐藏层 1 尺寸:128</li><li id="01e3" class="nb nc jg me b mf nk mi nl ls nm lv nn ly no mu ng nh ni nj bi translated">隐藏层 2 尺寸:64</li></ul><h2 id="83dc" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.8 构建培训功能</h2><p id="ed60" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在本节中，我定义了一个训练函数<code class="fe ob oc od oe b">train_ddpg()</code>来监控进度，并在训练完成后保存模型。</p><figure class="oi oj ok ol gt is"><div class="bz fp l di"><div class="om on l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Function — Control and Monitor the Training Progress</figcaption></figure><h2 id="3002" class="ln kw jg bd kx lo lp dn lb lq lr dp lf ls lt lu lh lv lw lx lj ly lz ma ll mb bi translated">3.9 培训结果—失败</h2><p id="eaff" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">在实验中，我设置了在每个<strong class="me jh"> 20 </strong>时间点触发的训练。在每次训练期间，权重更新将迭代<strong class="me jh"> 10 </strong>次。</p><p id="f47e" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">另外，<strong class="me jh">临界梯度被削波</strong>，最大值为 1，以增强训练的稳定性。正如你将在下面看到的，在第一个 1000 集(如下图所示)中，情节奖励徘徊在<strong class="me jh"> 0.04 </strong>到<strong class="me jh"> 0.05 </strong>左右，这很大程度上意味着代理根本没有从这些经历中学到任何东西。</p><p id="e9bd" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">除了这里展示的演示实验，我还尝试了不同的训练长度，跨度从 1000 到 5000 集，但没有一次成功。</p><figure class="oi oj ok ol gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi or"><img src="../Images/938d64f3df3cfa22f0d18019426a96c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SeL9he9kNsRQsgnf-9iAbg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Average Episodic Score for DDPG Agent (task not solved)</figcaption></figure></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><h1 id="425c" class="kv kw jg bd kx ky nw la lb lc nx le lf km ny kn lh kp nz kq lj ks oa kt ll lm bi translated">待续</h1><p id="6d5f" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">现在我们已经看到 DDPG 不能理解任务的复杂性。所以在下一部分，我将转向其他算法——D4PG 和 A2C，看看这些新发布的方法是否更强大，是否能够解决手头的任务。</p><p id="12cc" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">文章中的完整代码可以在这个<a class="ae jd" href="https://github.com/TomLin/RLND-project/tree/master/p2-continuous-control" rel="noopener ugc nofollow" target="_blank">链接</a>中找到。此外，如果你喜欢这个帖子，欢迎指出我犯的任何错误或在评论框中留下反馈。</p><h1 id="f921" class="kv kw jg bd kx ky kz la lb lc ld le lf km lg kn lh kp li kq lj ks lk kt ll lm bi translated">参考</h1><p id="11b4" class="pw-post-body-paragraph mc md jg me b mf mg kh mh mi mj kk mk ls ml mm mn lv mo mp mq ly mr ms mt mu ij bi translated">[1] Unity Technologies，<a class="ae jd" href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher" rel="noopener ugc nofollow" target="_blank"> ml-agents </a> (2018)，Github</p><p id="ee8d" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">[2] Udacity，<a class="ae jd" href="https://github.com/udacity/deep-reinforcement-learning/tree/master/ddpg-pendulum" rel="noopener ugc nofollow" target="_blank">深度强化学习</a> (2018)，Github</p><p id="1049" class="pw-post-body-paragraph mc md jg me b mf mv kh mh mi mw kk mk ls mx mm mn lv my mp mq ly mz ms mt mu ij bi translated">[3] P. Emami，<a class="ae jd" href="https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html" rel="noopener ugc nofollow" target="_blank">tensor flow 中的深度确定性策略梯度</a> (2016)，Github</p></div></div>    
</body>
</html>