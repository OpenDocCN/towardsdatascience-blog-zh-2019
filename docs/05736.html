<html>
<head>
<title>Short Text Topic Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">短文本主题建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883?source=collection_archive---------0-----------------------#2019-08-22">https://towardsdatascience.com/short-text-topic-modeling-70e50a57c883?source=collection_archive---------0-----------------------#2019-08-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c469" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">直觉和代码来理解和实现短文本的主题建模，比如社交媒体(Tweets，Reddit 的帖子…)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d1ab055f18ebf87528ebe8fa3154676f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*21o5eeyguePYV4LO1IloNw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@helloimnik?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Hello I’m Nik 🇬🇧</a> on <a class="ae ky" href="https://unsplash.com/search/photos/candies?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4bee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主题建模</strong>的目的是在一个文本语料库(如邮件或新闻文章)中找到主题(或聚类),而不需要先知道那些主题。这就是主题建模的真正力量所在，你不需要任何带标签或带注释的数据，只需要<strong class="lb iu">原始文本</strong>，从这种混乱的主题建模算法中会找到你的文本所涉及的主题！</p><p id="82aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将描述最流行的主题建模方法<strong class="lb iu"> LDA </strong>背后的直觉和逻辑，并了解它对短文本的<strong class="lb iu">限制。鉴于这篇文章是关于<strong class="lb iu">短文本主题建模</strong> ( <strong class="lb iu"> STTM </strong>)我们将不深究 LDA 的细节。想要加深对 LDA 的了解的读者可以在这里和这里找到关于 LDA <a class="ae ky" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158">的优秀文章和有用资源。<br/>然后，在第二部分中，我们将介绍一种新的 STTM 方法，最后在第三部分中了解如何在玩具数据集上轻松应用它(拟合/预测✌️)并评估其性能。</a></strong></p><p id="c925" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">已经熟悉 LDA 和主题建模的读者可能想跳过第一部分，直接进入第二和第三部分，这两部分介绍了一种新的<strong class="lb iu">短文本主题建模</strong>及其<strong class="lb iu"> Python 编码的方法🐍</strong>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8e3e" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">一.潜在的狄利克雷分配</h2><p id="f777" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">最流行的话题建模算法是<strong class="lb iu"> LDA，潜在狄利克雷分配</strong>。让我们首先解开这个令人印象深刻的名字，对它的作用有一个直觉。</p><ul class=""><li id="46c1" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><strong class="lb iu">潜伏</strong>因为题目很“隐蔽”。我们有一堆文本，我们希望算法将它们分成对我们有意义的簇。例如，如果我们的文本数据来自新闻内容，通常发现的聚类可能是关于中东政治、计算机、空间…但我们还不知道它。</li><li id="29b8" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><strong class="lb iu">狄利克雷</strong>代表<a class="ae ky" href="https://stats.stackexchange.com/questions/244917/what-exactly-is-the-alpha-in-the-dirichlet-distribution" rel="noopener ugc nofollow" target="_blank">狄利克雷</a>分布，该模型将其用作生成文档-主题和词-主题分布的先验。</li><li id="67aa" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">因为我们想给我们的文章分配主题。</li></ul><p id="8a71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的图 1 描述了 LDA 步骤如何在文档语料库中清晰地找到主题。</p><blockquote class="no"><p id="4eec" class="np nq it bd nr ns nt nu nv nw nx lu dk translated"><em class="ny">“通过对这些主题的混合物进行采样，然后从该混合物中对单词进行采样，生成一个文档”(吴恩达、大卫·布莱和迈克尔·乔丹，来自 LDA 最初的</em> <a class="ae ky" href="https://ai.stanford.edu/~ang/papers/nips01-lda.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ny">论文</em> </a> <em class="ny">)。</em></p></blockquote><figure class="oa ob oc od oe kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/5c4cac8789e1c35d3f184b3159142c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Ip3crqYQxPa3eUVcC9GKw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1: LDA documents generation process using Dirichlet distribution.</figcaption></figure><p id="2be8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://u" rel="noopener ugc nofollow" target="_blank"> <em class="of"> NB </em> </a>:在上面的图 1 中，为了便于说明，我们在词汇表中设置了 K=3 个主题和 N=8 个单词。为了便于说明，我们还将这些主题命名为计算机、空间和中东政治(而不是称它们为主题 1、主题 2 和主题 3)。事实上，我们的任务是理解 3 个发现的主题是关于计算机、空间和中东政治的内容(我们将在第三部分 STTM 管道的主题归属中更深入地了解这一部分)。</p><p id="c8fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，LDA 通过使用狄利克雷分布作为先验知识来生成由主题组成的文档，然后更新它们，直到它们匹配基本事实。</p><h2 id="8644" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">二。短文本主题建模(STTM)</h2><p id="e312" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">尽管 LDA 在中型或大型文本(&gt; 50 个单词)上取得了很好的效果，但通常邮件和新闻文章都在这个大小范围内，<strong class="lb iu"> LDA 在短文本</strong>上表现不佳，如 Tweets、Reddit 帖子或 StackOverflow titles 的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/066f14677c3eb1d4aedb299e026d3fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vt_YKZvuN5RbeLGrVOrNCA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2: Example of short texts and the topic they discuss.</figcaption></figure><p id="fa5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看上面图 2 中的短文本示例，很明显，假设文本是主题的混合物(记住图 1 中的第一步)不再正确。我们现在假设一篇短文仅由<strong class="lb iu">的一个主题</strong>组成。</p><p id="0720" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Gibbs 抽样 Dirichlet 混合模型</strong> ( <strong class="lb iu"> GSDMM </strong>)是一种“改变的”LDA 算法，在 STTM 任务上表现出很好的结果，这使得最初的假设:<strong class="lb iu"> 1 主题↔️1 文档</strong>。文档中的单词是使用相同的唯一主题生成的，而不是像在原始 LDA 中那样来自主题的混合。</p><p id="f39a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在深入代码和实践方面之前，让我们先了解一下<strong class="lb iu"> GSDMM </strong>和一个称为<strong class="lb iu">电影组过程</strong>的等效过程，它将帮助我们了解 STTM 的不同步骤和过程，以及如何<strong class="lb iu">有效地调优其超参数</strong>(我们记得 LDA 部分的 alpha 和 beta)。</p><p id="9494" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想象一下，一群学生在餐馆里，随机坐在 K 张桌子旁。他们都被要求在纸上写下他们最喜欢的电影(但必须是一个简短的名单)。目标是以这样一种方式将他们聚集在一起，使得同一组中的学生分享相同的电影兴趣。为此，学生必须根据以下两条规则逐一选择新的桌子:</p><ul class=""><li id="644b" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">规则一:选择学生多的桌子。这条规则提高了完整性，所有对同一部电影感兴趣的学生都被分配到同一个桌子上。</li><li id="59a4" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">规则 2:选择一张桌子，让学生分享相似电影的兴趣。这个规则旨在增加<strong class="lb iu">的同质性</strong>，我们只希望成员们在一张桌子上分享同一部电影的兴趣。</li></ul><p id="46e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重复这个过程后，我们预计一些表格会消失，另一些会变得更大，最终会有一群学生符合他们对电影的兴趣。这就是 GSDMM 算法所做的事情！</p><h2 id="7142" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">三。STTM 实施</h2><p id="7321" class="pw-post-body-paragraph kz la it lb b lc mv ju le lf mw jx lh li mx lk ll lm my lo lp lq mz ls lt lu im bi translated">在这一部分中，我们将使用 Scikit-learn 中用于文本主题建模的 20 个新闻组<a class="ae ky" href="https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html" rel="noopener ugc nofollow" target="_blank">数据集</a>,从一个具体示例中构建<strong class="lb iu">完整的 STTM 管道</strong>。</p><p id="563d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要从 Github 下载 STTM 脚本到我们的项目文件夹中。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="efa6" class="mc md it oi b gy om on l oo op">cd sttm_project<br/>git clone <a class="ae ky" href="https://github.com/rwalk/gsdmm.git" rel="noopener ugc nofollow" target="_blank">https://github.com/rwalk/gsdmm.git</a></span></pre><p id="536e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以开始实现 STTM 管道(这是我使用的<a class="ae ky" href="https://github.com/Matyyas/short_text_topic_modeling/blob/master/notebook_sttm_example.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>的静态版本)。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="4d43" class="mc md it oi b gy om on l oo op"># Useful libs<br/>from sklearn.datasets import fetch_20newsgroups<br/>import pickle<br/>import pandas as pd<br/>import numpy as np</span><span id="bb94" class="mc md it oi b gy oq on l oo op"># STTM lib from Github<br/>from gsdmm import MovieGroupProcess</span><span id="c60f" class="mc md it oi b gy oq on l oo op"># Custom python scripts for preprocessing, prediction and<br/># visualization that I will define more in depth later<br/>from preprocessing import tokenize<br/>from topic_allocation import top_words, topic_attribution<br/>from visualisation import plot_topic_notebook</span><span id="de6b" class="mc md it oi b gy oq on l oo op"># Load the 20NewsGroups dataset from sklearn<br/>cats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']<br/>newsgroups_train = fetch_20newsgroups(subset=’train’, categories=cats)</span></pre><p id="921c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是在这个练习中，我们不会用新闻的全部内容来从中推断一个话题，而是<strong class="lb iu">只考虑新闻</strong>的主题和第一句话(见下图 3)。事实上，我们需要短文本来进行短文本主题建模…很明显🙏</p><p id="4926" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，为了便于说明，我们将只查看 3 个主题(均匀分布在数据集中)。这些主题如下:</p><ul class=""><li id="7bef" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">中东政治🌍</li><li id="5426" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">空间👾</li><li id="e34a" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">Windows X 🖥</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/91909419bc884ac42aea738da8ee5027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8IsI3CLCm7QIRfQvKo7uUA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 3: Example of a news about the topic: <strong class="bd os">Windows X</strong>. We concatenate these 2 highlighted sentences together to have a document.</figcaption></figure><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="f1db" class="mc md it oi b gy om on l oo op"># Preprocessing and tokenization<br/>tokenized_data = tokenize(df, form_reduction='stemming', predict=False)</span></pre><p id="de0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是我在这项任务中遵循的<a class="ae ky" href="https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908" rel="noopener">预处理</a>配方:</p><ul class=""><li id="ea97" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">使用<a class="ae ky" href="https://spacy.io/usage/spacy-101" rel="noopener ugc nofollow" target="_blank"> spaCy </a>记号化器进行记号化。</li><li id="8225" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">删除停用词和 1 个字符的词。</li><li id="913a" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">使用<a class="ae ky" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> nltk </a>库的词干分析器进行词干分析(根据我的经验，我观察到<a class="ae ky" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">词干分析</a>比<a class="ae ky" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank">词干分析</a>在短文本上给出了更好的聚类)。</li><li id="b2dc" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">删除空文档和超过 30 个令牌的文档。</li><li id="73e4" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">移除唯一令牌(术语频率= 1)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/d2346488b0b0111cb06419e718e193c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TEaFNXtSlhO4nGsWGl_mpg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 4: We do match the Short Text statistics regarding the number of token in our documents, referring to the <a class="ae ky" href="https://arxiv.org/pdf/1904.07695.pdf" rel="noopener ugc nofollow" target="_blank">STTM survey paper p.10</a>.</figcaption></figure><p id="0a16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，必须记住<strong class="lb iu">预处理是数据相关的</strong>，如果使用不同的数据集，应考虑采用其他预处理方法。</p><p id="b229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们的数据已经被清理并处理成适当的输入格式，我们就可以训练模型了🚀</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="04cb" class="mc md it oi b gy om on l oo op"># Train STTM model</span><span id="22cb" class="mc md it oi b gy oq on l oo op"># Init of the Gibbs Sampling Dirichlet Mixture Model algorithm<br/># K = number of potential topic (which we don't know a priori)<br/># alpha = <br/># beta = <br/># n_iters = number of iterations to <br/>mgp = MovieGroupProcess(K=10, alpha=0.1, beta=0.1, n_iters=30)</span><span id="a4f0" class="mc md it oi b gy oq on l oo op">vocab = set(x for doc in docs for x in doc)<br/>n_terms = len(vocab)</span><span id="45e5" class="mc md it oi b gy oq on l oo op">y = mgp.fit(docs, n_terms)</span><span id="0f01" class="mc md it oi b gy oq on l oo op"># Save model<br/>with open(‘dumps/trained_models/v1.model’, “wb”) as f:<br/> pickle.dump(mgp, f)<br/> f.close()</span></pre><p id="0839" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们深入了解 gsdmm:模型的超参数机制</p><ul class=""><li id="4694" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated"><strong class="lb iu"> K </strong> = 10。在实际情况下，我们不知道主题的确切数量，所以我们希望选择一个更高的值。理论上，GSDMM 应该清空无用的簇，并最终找到簇的确切数目。这里不会出现这种情况，但没什么可担心的，我们稍后会更深入地解释这种情况。</li><li id="95bf" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated"><strong class="lb iu">阿尔法</strong>= 0.1<strong class="lb iu">贝塔</strong> = 0.1。在这里，我们保留了默认参数(对于几个数据集来说，这些参数工作得很好)。然而，人们可能希望对它们进行调优，以改善关于集群的完整性和同质性的主题分配。不要犹豫参考<a class="ae ky" href="https://dl.acm.org/doi/10.1145/2623330.2623715" rel="noopener ugc nofollow" target="_blank">原文</a>📖为了理解这两个参数之间的平衡。</li><li id="17f8" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">n_iters  = 30。根据原始论文，GSDMM 在几个数据集上收敛得相当快(大约 5 次迭代)，并且保持非常稳定。因此，对于任何类型的数据集，30 次迭代都是一个很好的默认值。</li></ul><p id="19da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦模型被训练，我们想要探索发现的主题，并检查它们在内容上是否一致🔎</p><p id="4815" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们的模型已经将文档收集到 10 个主题中，我们必须给它们一个对其内容有意义的名称。因此，让我们深入研究我们的模型所发现的主题。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="14f2" class="mc md it oi b gy om on l oo op">doc_count = np.array(mgp.cluster_doc_count)<br/>print('Number of documents per topic :', doc_count)<br/>print('*'*20)</span><span id="ab10" class="mc md it oi b gy oq on l oo op"># Topics sorted by the number of document they are allocated to<br/>top_index = doc_count.argsort()[-10:][::-1]<br/>print('Most important clusters (by number of docs inside):', top_index)<br/>print('*'*20)</span><span id="bf5d" class="mc md it oi b gy oq on l oo op"># Show the top 5 words in term frequency for each cluster <br/>top_words(mgp.cluster_word_distribution, top_index, 5)</span></pre><p id="4b8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码显示了下面的统计数据，这些数据让我们了解我们的集群是由什么组成的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/cb570b76a3e0eb13a7b39f46faea4277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXEA2hm7igOtKKl1K9fkhA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 5: 1) Number of documents by cluster (or topic) index. 2) The sorted list of clusters regarding the number of documents they contain. 3) The top 5 words regarding their frequency inside a cluster.</figcaption></figure><p id="ec9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理想情况下，GSDMM 算法应该找到正确的主题数，这里是 3，而不是 10。我想到三种解释:</p><ul class=""><li id="acd9" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">寻找其他超参数以清空较小的集群(参考<a class="ae ky" href="https://dl.acm.org/doi/10.1145/2623330.2623715" rel="noopener ugc nofollow" target="_blank">原始论文</a>以更深入地理解α和β参数)。</li><li id="cc6f" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">我们既有小数据集又有词汇表(大约 1700 个文档和 2100 个单词)，这对于模型来说可能难以推断和区分主题之间的显著差异。像往常一样，数据越多越好。</li><li id="d66e" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">该算法可能会在主题中找到主题。让我解释一下。众所周知，其中一个话题是关于中东的新闻。然而，该算法将该主题分成 3 个子主题:以色列和真主党之间的紧张局势(聚类 7)、土耳其政府和亚美尼亚之间的紧张局势(聚类 5)或以色列的犹太复国主义(聚类 0)。</li></ul><p id="bfa9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，即使找到了 3 个以上的集群，我们也很清楚如何将它们分配到各自的主题中。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="d199" class="mc md it oi b gy om on l oo op"># Must be hand made so the topic names match the above clusters <br/># (Figure 5) regarding their content</span><span id="aa4e" class="mc md it oi b gy oq on l oo op">topic_dict = {}<br/>topic_names = ['x',<br/>               'mideast',<br/>               'x',<br/>               'space',<br/>               'space',<br/>               'mideast',<br/>               'space',<br/>               'space',<br/>               'mideast',<br/>               'space']</span><span id="a65a" class="mc md it oi b gy oq on l oo op">for i, topic_num in enumerate(top_index):<br/>    topic_dict[topic_num]=topic_names[i] </span><span id="6674" class="mc md it oi b gy oq on l oo op">df_pred = topic_attribution(tokenized_data, mgp, topic_dict, threshold=0.4) </span></pre><p id="f338" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有人可能会问，<em class="of">topic _ attribute</em>函数的<em class="of">阈值</em>输入参数是什么。实际上，主题被分配给给定概率的文本，并且<em class="of">topic _ attribute</em>是一个定制函数，它允许选择考虑哪个阈值(置信度)以便属于一个主题。例如，查看主题到文本的最高概率分配，如果该概率低于 0.4，则该文本将被分配到“其他”主题中。</p><p id="392b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://u" rel="noopener ugc nofollow" target="_blank"> <em class="of"> NB </em> </a> <em class="of"> : </em>这个定制的<em class="of">topic _ attribute</em>函数建立在 GSDMM 包中的原始函数之上:<em class="of"> choose_best_label，</em>输出最有可能属于某个文档的主题。</p><p id="5330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在是时候将找到的主题分配给文档，并将它们与地面真相进行比较(✅对❌)</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="1459" class="mc md it oi b gy om on l oo op">df_pred[['content', 'topic_name', 'topic_true_name']].head(20)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/1507f5186f4b0ef458c22f79652630e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3YqRjHGRUEMGitdaAGZh7g.png"/></div></div></figure><p id="14c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">天真地比较预测的主题和真实的主题，我们会有 82%的准确率！🎯<br/>仅用一个<strong class="lb iu"> 9 字平均</strong>的文档，一个 1705 个文档的小语料库和很少的超参数调优！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/6e7a3de9754d4e35c07d5b107824ba4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3p7eqyYEYC5IJZW0sN1DA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">🙌</figcaption></figure><p id="9c5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拥有一个高效的模型当然很好，但如果我们能够简单地展示其结果并与之互动，那就更好了。为此，<a class="ae ky" href="http://stat-graphics.org/movies/ldavis.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> pyLDAvis </strong> </a>是一个非常强大的主题建模可视化工具，允许在二维空间维度中动态显示集群及其内容。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/092cacb4ff29be055fb84225e8dc2e3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zCSWUqETVMVGlQ1oInF4-Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Screen-shot of pyLDAvis ability. Check out this <a class="ae ky" href="http://stat-graphics.org/movies/ldavis.html" rel="noopener ugc nofollow" target="_blank">video</a> to see its full power.</figcaption></figure><p id="d13d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在轮到你用自己的数据(社交媒体评论、在线聊天的回答……)来尝试了💪</p><p id="7e70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊！👌</p><p id="dd9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢拉贾·埃尔·海姆达尼对我的评价和反馈。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="e72c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考资料和其他有用的资源<br/></strong>——GSD mm<br/>的<a class="ae ky" href="https://dl.acm.org/doi/10.1145/2623330.2623715" rel="noopener ugc nofollow" target="_blank">原文</a>——实现 STTM 的好看的<a class="ae ky" href="https://github.com/rwalk/gsdmm" rel="noopener ugc nofollow" target="_blank"> python 包</a>。pyLDAvis 库漂亮地可视化了一堆文本中的主题(或者任何单词包一样的数据)。<br/> -最近对 STTM 的一项对比调查<a class="ae ky" href="https://arxiv.org/pdf/1904.07695.pdf" rel="noopener ugc nofollow" target="_blank"/>看其他策略。</p><p id="2bb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://n" rel="noopener ugc nofollow" target="_blank"> <em class="of"> PS </em> </a>:对于那些愿意深入 STTM 的人来说，有一种有趣的更进一步的方法(我现在还没有亲自探索过)叫做<a class="ae ky" href="https://www.ntu.edu.sg/home/axSun/paper/sigir16text.pdf" rel="noopener ugc nofollow" target="_blank"> GPU-DMM </a>，它显示了 SOTA 在短文本主题建模任务上的结果。简而言之，GPU-DMM 正在使用<a class="ae ky" rel="noopener" target="_blank" href="/word-embeddings-intuition-and-some-maths-to-understand-end-to-end-skip-gram-model-cab57760c745">预先训练的单词嵌入</a>作为外部知识来源来影响单词的采样，以生成主题和文档。</p></div></div>    
</body>
</html>