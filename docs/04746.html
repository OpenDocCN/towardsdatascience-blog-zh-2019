<html>
<head>
<title>Why Deep Learning Works — Step by Step Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习为什么有效——循序渐进教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-deep-learning-works-289f17cab01a?source=collection_archive---------7-----------------------#2019-07-19">https://towardsdatascience.com/why-deep-learning-works-289f17cab01a?source=collection_archive---------7-----------------------#2019-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6aed" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一开始是神经元:理解梯度下降、反向传播、线性回归、逻辑回归、自动编码器、卷积神经网络和 VGG16。借助 Python &amp; Keras 中的可视化辅助工具和实际动手编码。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b53f535031fa714deb9a69ef2f4b8f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hzqhsKFIZNWJKhvq.jpg"/></div></div></figure><p id="a793" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这篇文章是为那些错过了通过深度学习黑盒的无痛之旅的人写的。我们用一个关于一个过于关心未来的农民的故事来激励和吸引读者。我们通过使用人工神经网络解决简单的线性和逻辑回归来开始研究他的问题。我们通过观察梯度下降、链式法则和反向传播来逐步建立我们的理解。稍后，我们通过构建一个自动编码器并添加卷积层来消除 MNIST 手写数字的噪声，进行了更深入的研究。在解释了为什么这样的网络在计算机视觉中如此成功之后，我们进一步从预训练的 VGG16 网络中提取特征。</p><blockquote class="lq"><p id="3526" class="lr ls it bd lt lu lv lw lx ly lz lp dk translated">通过阅读这篇文章，你将最终破解深度学习的奥秘。</p><p id="4539" class="lr ls it bd lt lu lv lw lx ly lz lp dk translated">你也会有一些有意义的开放式问题。这就是(深度)学习的全部意义，对吗？</p></blockquote><p id="405f" class="pw-post-body-paragraph ku kv it kw b kx ma ju kz la mb jx lc ld mc lf lg lh md lj lk ll me ln lo lp im bi translated">先说一个故事。</p><h1 id="ed4d" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">生活在未来的人</h1><p id="8a99" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">这是一个想知道自己未来的老人的故事。他是个农民。他很幸运，他的田地每年都能产出越来越多的庄稼。他的粮仓里有太多的粮食，不得不把多余的粮食分给朋友和邻居。尽管如此，他还是担心未来。他不知道给慈善机构捐多少，自己留多少。实际上，这取决于他下一年的收成。他需要对未来几年的产量有一个准确的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/6e013c77c6728d52ee6d2b4838407306.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*luJLD2d3aqjKMiBeRS8Acw.png"/></div></figure><p id="2b97" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，和深度学习有什么联系呢？请继续读下去，你会很高兴的。农夫的大儿子想出了下面这个实验。他取了一些谷物样本，并把大约相当于前一年每年收获量的数量放在一个袋子里。按照时间升序排列，每个袋子代表一年。然后，他让他最小的兄弟姐妹把一根棍子放在袋子的顶部，这样棍子就不会掉下来，并且保持平衡，正如这篇文章中的图片所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/af62cd100a3108b3b4b36bbc5beca09d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bShQZ5S7qQ7kgZJ9KnvHxQ.png"/></div></div></figure><p id="a4ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这一景象让老人对未来几年的收成有了一个概念，假设趋势保持不变。他的儿子刚刚发明了线性回归，比弗朗西斯·高尔顿早了几百年吗(对不起，先生)？</p><blockquote class="ne nf ng"><p id="5c33" class="ku kv nh kw b kx ky ju kz la lb jx lc ni le lf lg nj li lj lk nk lm ln lo lp im bi translated">如果我们可以做线性回归，为什么我们需要人工神经网络？</p></blockquote><p id="b2f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">继续阅读，我们将很快解决深度学习问题。人们对未来很着迷。我们想知道明天是否会下雨，我们明年是否会结婚，我们是否以及何时会最终变得富有。过去，这些是甲骨文(而非软件公司甲骨文)最喜欢问的问题。据推测，神谕是具有特殊技能的人，他们会根据现实世界的数据来预测未来，比如手上的线条、星座或血液的味道。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/f78e31d0e10ec50a46d11aedecaf5783.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/0*ReCX6rawzD-IFZGs.jpg"/></div></figure><p id="89b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">能够预测未来并不总是一件好事。特洛伊的卡珊德拉有预见的天赋。对她自己不利的是，她被阿波罗诅咒没有人会相信她的预言。因此，她关于特洛伊毁灭的警告被忽视了，事情对她来说并不顺利。如今，企业、组织和政府都在通过雇佣数据科学家——现代的先知——来寻找自己的卡珊德拉。因为一些数据科学家和机器学习者希望尽可能地模糊，以便获得更多的重要性，就像过去的神谕一样，他们选择使用所谓的<strong class="kw iu">深度学习</strong>技术，一直如此。也是因为管用。不仅仅是为了预测未来，也是为了理解现在，进行推理和分类。你可以看看<a class="ae nm" href="https://blog.statsbot.co/deep-learning-achievements-4c563e034257" rel="noopener ugc nofollow" target="_blank">这篇文章</a>自己看看，我们可以用深度学习做些什么惊人的事情。</p><p id="8636" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">深度学习为什么有效？为什么我们农民的儿子需要深度学习，尽管他已经发明了线性回归？这些是我们将在接下来的章节中尝试解决的问题。</p><h1 id="e1a5" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated"><strong class="ak">用神经元进行线性回归</strong></h1><p id="db41" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们先来看看农夫儿子的线性模型。他用年份来预测收成的数量(数量)。通过画出一条最佳匹配线，这条线跟随多年来的数量趋势，他能够预测接下来几年的收获量。在数学上，他找到了一条多年来的直线<em class="nh">ŷ</em>x，截距为<em class="nh"> β0 </em>，斜率为<em class="nh"> β1 </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e2e3565d4d76d05a0ec68c88e07058de.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*N-tXTCXOfQZDV-Kk8AwRMg.png"/></div></figure><h2 id="0890" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">线性回归的解析解</h2><p id="1454" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">给定<em class="nh"> Y </em>的真实世界测量值和相应的预测值<em class="nh"> X </em>，我们能够计算斜率和截距，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8fae27f42b131514e533725e8efdc303.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*yjlLfNY8-mJNHMFBCKx5Ag.png"/></div></figure><p id="a914" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一张图片胜过千言万语，Python 代码也是如此。为了做出漂亮的剧情，探索模式，我们将重现农夫之子使用的数据。让我们沿着线性方程所表示的平面中的直线创建 100 组人工随机测量值。我们将这些点沿直线均匀分布在-2 和 3 之间的区间内。然后，我们向每个 Y 值添加一个标准偏差为 1 且平均值为 0 的随机正态分布值。我们最后使用上面的公式来计算截距和斜率。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/a69d8619962923bbbb6fcb0e079946bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*WapIlDDsBeRo-eSv2BKSeQ.png"/></div></figure><p id="682a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们在上面的图像中看到的，农民的儿子的想法是一个赢家。我们可以预测<em class="nh"> X </em>的任意值的<em class="nh"> Y </em>，因为我们知道支配<em class="nh"> Y </em>行为的回归线。那么，我们能用人工神经网络做同样的事情吗？首先，我们需要定义什么是真正的人工神经网络。</p><h2 id="6847" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">生物神经元</h2><p id="9aba" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">什么先出现的？神经科学还是人工神经网络？不管怎样，人工神经元的想法似乎来自我们大脑中的神经元。我们的大脑由 1000 亿个神经元组成。当电信号通过树突流入神经元的细胞核时，电荷就积累起来了。当细胞达到一定的电荷水平，即阈值时，它就会激活，通过轴突发出电信号。神经元在决定何时触发时，对不同的输入信号有不同的权重。一些人认为，人工神经网络模拟了为不同的树突(输入)分配不同权重的过程。</p><h2 id="f0f7" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">人工神经元</h2><p id="d691" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">单个人工神经元接受输入<em class="nh"> X </em>，使用<strong class="kw iu">权重</strong>W<em class="nh">应用(线性)<strong class="kw iu">仿射变换</strong>以产生临时值<em class="nh"> h </em>。然后，它将该值通过<strong class="kw iu">激活功能</strong>来产生输出。在我们的线性回归例子中，<em class="nh"> X </em>可以是一个数，比如说 2。仿射变换会使用向量<em class="nh"> W </em>中存储的截距和斜率对输入进行变换，为了返回，姑且说<em class="nh"> h </em> =5。后一个值将经过激活，在我们的例子中是线性函数，输出，比如说<em class="nh"> Y </em> = <em class="nh"> h </em> =5。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ddb9271b80f753a0aa2dba9806e1f3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*rN9EyHPX7HcwyDrPBG9xWQ.png"/></div></figure><h2 id="3762" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">人工神经元和梯度下降</h2><p id="c21a" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们之前看到，回归线的截距和斜率可以使用解析公式直接计算。像我们刚刚描述的那样，一个人工神经元可以不使用任何公式找到那些值吗？我们可以使用以下策略:我们可以选择截距和斜率的随机值，假设<em class="nh"> W </em> =(-1，3)，然后，神经元可以计算相应的回归线，并将其与真实的<em class="nh"> Y </em>值进行比较。然后，我们将能够计算出神经元的输出与真实的<em class="nh"> Y </em>之间的差异，即损耗。在我们的例子中，均方误差是一个好的损失函数，我们称之为 L。我们将重复选择随机值的过程，直到损失足够小，或者直到我们厌倦尝试。这种策略可能对一个神经元有效。然而，如果我们有两个或几千个神经元，我们可以花一生的时间来寻找。这就是为什么这个策略听起来如此疯狂，以至于人们想出了一个更好主意，叫做梯度下降。这就像在自由式滑雪中从山顶通过最短的路径下降到山谷。不会被杀。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4eb9dd300038584fa8488eed8c8f334c.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*oDnZ9PtIXKapIGu20OMfkw.png"/></div></figure><p id="365d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">梯度下降是一种工具，通过查看损失函数<em class="nh"> L </em>相对于<em class="nh"> W </em>的导数，找到使损失函数<em class="nh">L</em>最小的<em class="nh"> W </em>的值。假设我们已经选择了<em class="nh"> W </em> =(-1，3)，并且我们已经计算了这些值的损失，现在我们必须决定尝试<em class="nh"> W </em>的哪个下一个值，以便具有减少的损失。我们的想法是计算损失的导数，比如说截距，并检查其符号。如果导数为正，说明损失在增加，需要尝试更小的截距。如果导数是负的，损失在减少，所以我们可以继续尝试更大的斜率值。如果导数为零，我们就达到了损失的极小值，希望是全局的。我们可以停下来考虑一下<em class="nh"> W </em>是使损失最小的最佳重量。梯度下降需要计算导数，如果我们没有像 Matlab 这样的好工具，这可能会成为一个噩梦。感谢<strong class="kw iu">反向传播</strong>，我们甚至不需要这样的工具，我们将在后面看到。</p><h2 id="3444" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">带 Keras 的线性回归</h2><p id="a06b" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">让我们实际看看我们的神经元将如何执行线性回归任务。我们使用<a class="ae nm" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>，这是一个 Python 库，也是深度学习实验的好伙伴。我们创建一个具有单个神经元、标量输入、标量输出、具有权重和偏差的仿射变换以及线性输出的模型。接下来，我们使用随机梯度下降优化器在数据点上训练我们的神经元。损失函数是均方误差。优化器将执行 5 次迭代(历元)。为了简单起见，我们不对数据进行缩放。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2ccf0fcfdeefa07bb165092e5609148a.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*r8TypiTe51I8igxZl56PAA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6a89e8e2893b125504acfe3e1920196b.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*3lAZmNE0xV4eKthkaM4Gmg.png"/></div></figure><p id="07f8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">瞧啊。单个神经元能够预测几乎相同的回归线(绿色)，这是我们之前分析计算的(橙色)。如上图所示，斜率和截距的预测值非常接近。在这两种情况下，衡量健康的 R2 分数看起来也很相似。看来人工神经网络真的管用！如果我们让它变得更难呢？</p><h1 id="0668" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">神经元非线性回归</h1><p id="419c" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们的农民很快意识到生活中只有少数事情是线性的。收成趋势可能会达到一个稳定水平，然后开始下降。下面，我们设想一个新的数据集，它呈现出所谓的“单峰”趋势。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2cdf72acf65eff87b6957fe998e32f4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*Wz6G1imG7t3ZFwVkpm7ZfA.png"/></div></figure><h2 id="aab4" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">三神经元模型</h2><p id="8e51" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">显然，线性激活的单个神经元将无法捕捉到这一新数据中的行为。第一个想法是改变线性激活函数，并使用类似于<em class="nh"> S </em>的形状函数。sigmoid 函数是深度学习中非常常用的一个很好的候选函数。一个 sigmoid 将完美地捕捉到我们的数据中观察到的增长趋势。镜像的 s 形曲线也可以捕捉到下降趋势。因此，我们需要两个 sigmoid，因此需要两个神经元，以及将两个 sigmoid 相加的第三个神经元。相应的架构如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/d9fb3243092f7e405851216c3d027094.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zGOzdamgnaf_SnAnWSZPFw.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="f199" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">第一个神经元将采用标量<em class="nh"> X </em>作为输入，然后使用权重<em class="nh"> W1 </em>和偏差应用仿射变换。结果将通过一个 sigmoid 激活产生<em class="nh"> h1 </em>。第二个神经元也会发生类似的事情，产生<em class="nh"> h2 </em>。现在，数字<em class="nh"> h1 </em>和<em class="nh"> h2 </em>都将被馈入第三个神经元，该神经元将通过应用权重<em class="nh"> W31 </em>到<em class="nh"> h1 </em>和<em class="nh"> W32 </em>到<em class="nh"> h2 </em>来执行仿射变换，最后添加一个偏置<em class="nh">。</em>产生的<em class="nh"> q </em>将通过线性激活产生单峰形状。</p><h2 id="4860" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">人工神经网络前向传递</h2><p id="b0ac" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">让我们用 Python 从头开始构建这个网络，没有 Keras。正如我们在上一节中看到的，网络总共有 7 个权重(和偏差)。我们手动给 7 个权重赋值，使用驼峰函数的数据执行正向传递，并绘制网络的输出，与真实的<em class="nh"> Y </em>值绘制在相同的图中。我们手动调整权重，直到图尽可能匹配。通过这样做，我们模拟了所谓的前向传递，这是由 Keras 在训练网络时执行的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/b682c254ff397e9570b5c1e04761214b.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*ia2lMZXKahEoGvBMZiTLcg.png"/></div></figure><h2 id="0467" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">带 Keras 的非线性回归</h2><p id="5b90" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">像特洛伊的卡珊德拉一样，我们能够猜测出<em class="nh"> W </em>的完美值，因此网络<strong class="kw iu">的输出完美地超过了</strong>的数据。虽然大多数数据科学家都能做到这样的神奇，但并不是所有人都有这种天赋。幸运的是，对于普通人来说，有另一种方法可以做到这一点。这是使用一个库，如 Keras 来运行一个梯度下降优化器，尽最大努力为我们找到这 7 个权重。这就是我们在下面的代码中演示的内容。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/f231d95d4bbdfd2b8fccd7e8dcde5242.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*_MzCoTCzjVicOUnhygK4bg.png"/></div></figure><p id="29cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">经过 50000 次的训练，网络达到了 96.69%的 R2 评分。上面的图显示了它的预测，与我们之前通过手动权重调整获得的预测进行了比较。看起来很神奇，不是吗？给定 3 个神经元，<em class="nh"> X </em>和<em class="nh"> Y </em>数据点，网络能够微调自己以适应数据。它已经学习了数据的形状，而没有任何进一步的洞察力。下面，我们可以直观地看到对最佳<em class="nh"> W </em>的搜索如何收敛到最低损失。如果我们让优化器运行额外的时期，我们最终会达到 100%适合。这篇文章的一些读者已经在咬牙切齿地考虑过度拟合和缩放。好吧，让我们先高兴一下，过一会儿再回到这个热门话题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/417d9e95cf69c8738ba9f0fabe42241e.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*_lOuE7gpSsZBF2cffPocIw.png"/></div></figure><p id="ac8c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这一结果进一步证明了人工神经网络具有捕捉数据中非线性相关性的能力。这就是深度学习奏效的原因吗？好吧，我们在这里作弊了一点。我们基于我们对数据的双 s 形形状的先验知识来设计网络，因此，通过选择 s 形作为激活函数。来自真实场景的数据并不总是具有如此清晰的形状，我们最终将四处寻找合适的网络架构。迟早，我们会把我们对世界的看法引入网络架构，把它变成一个有偏见的模型。你可以看看我另一篇关于<a class="ae nm" rel="noopener" target="_blank" href="/wild-wide-ai-responsible-data-science-16b860e1efe9">负责任的数据科学</a>的文章。现在，让我们暂时假设我们都是好人。然而，深度学习为什么有效的问题仍然没有答案。</p><h2 id="e205" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">使用 Keras 的逻辑回归</h2><p id="1a13" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们的农民对驼峰模型不太满意，尤其是它的第二部分，告诉他预计收成会下降。他更想知道哪一年有好收成。下面我们创建一个反映这种情况的数据集。低于 0 的年份被归类为歉收年份，而 0 之后的年份则是丰收年份。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f18eee0bc69f20a168e83a1305c171d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*72u4uWzC2gT__w1l6UCyMw.png"/></div></figure><p id="db93" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们看看一个简单的神经元是否能够将一个给定的年份归类为收获的好年份。这是一个逻辑回归问题。下面我们将数据输入 Keras 模型，使用随机梯度下降作为二进制交叉熵损失的优化器(太多的关键字让你吃惊)。我们实现了 81%的分类准确率，正如你在下图中看到的，sigmoid 激活函数做得很好。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/aa402312a165001a906851e61941fbd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*Kf0awIMtgY3tHsnC9_ShYw.png"/></div></figure><h2 id="7002" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">逻辑回归中的梯度下降</h2><p id="5ab9" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们刚刚解决的分类问题是进入梯度下降和反向传播的黑暗世界的一个很好的例子。我们记得，在我们上面的神经元中，输入经过仿射变换，结果被输入到 sigmoid 激活中。因此，输出是 0 或 1 之间的值，这是丰收的可能性。下面我们看到这个输出的表达式<em class="nh"> P(Y=1) </em>。注意，<em class="nh"> X </em>上的仿射变换取决于两个参数<em class="nh"> β0 </em>和<em class="nh"> β1 </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/0660251b47c06fc960cd2f3e1546b6a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*IIbtUy9nYpkodtEdU9tD5Q.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="10af" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在训练期间，对于任何输入<em class="nh"> Xi </em>，神经元能够计算似然<em class="nh"> Pi </em>并将其与真实值<em class="nh"> Yi </em>进行比较。通常使用<strong class="kw iu">二进制交叉熵</strong>计算误差。它的作用与我们之前介绍的均方误差相同。二进制交叉熵测量预测值<em class="nh"> Pi </em>与真实值<em class="nh"> Yi </em>(为 0 或 1)的距离。下面给出用于计算二元交叉熵的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/23af3ea2b57d128839dda58b72108260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7JVdq30VwGnM7rhMzvKifQ.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="a36b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，损失取决于权重<em class="nh"> β0 </em>和<em class="nh"> β1。</em>在梯度下降优化器选择了随机权重后，它挑选一个随机的<em class="nh"> Xi </em>，然后向前传递以计算损失。如果我们使用的是基本梯度下降，优化器会对所有输入数据点重复这种计算，如果我们使用的是随机版本，优化器只会对少量数据重复这种计算。总损失是将所有单项损失相加得出的。在优化器计算了损失之后，它将计算它相对于权重的导数。基于导数的符号，它将正向或负向更新权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/f1119774f8a991508f6df61e41402e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*Ufs2js9xyyh9gfsZZJOtXg.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="fdd0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们知道，我们想要向导数的相反方向前进，我们知道，我们想要向导数成比例地前进一步。<strong class="kw iu">学习率</strong> λ是控制每个权重<em class="nh"> W </em> ( <em class="nh"> β0 </em>或β1 )的比例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/556606cdb7258309ba90143f21487d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*LjvXtAsZj0QIUXKvNAiLqg.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><h2 id="3d08" class="no mg it bd mh np nq dn ml nr ns dp mp ld nt nu mr lh nv nw mt ll nx ny mv nz bi translated">逻辑回归中的反向传播</h2><p id="5b5b" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">如上图所示，更新权重需要计算损失相对于每个权重的偏导数。我们如何做到这一点？嗯，我们可以用高中数学来做这个。在下图中，我们试图计算相对于<em class="nh"> β0 </em>和<em class="nh"> β1 </em>的交叉熵损失函数的导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/9a257e05d7ef94e6b25ffb5a0b26e72c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFMpl9rCb_JMA6tuU3MZIw.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="08e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于只有一个神经元的网络来说，这是一个很好的解决方案。但是想象一下，一个我们通常在深度学习中遇到的有数百个神经元的网络。我敢打赌，你不想计算结果的导数。即使你成功地做到了这一点，你也必须在每次网络结构改变时更新你的公式，哪怕只是一点点。这就是<strong class="kw iu">反向传播</strong>发挥作用的地方。反向传播算法最初是在 20 世纪 70 年代提出的，但直到 1986 年 Geoffrey Hinton 的一篇论文才充分认识到它的重要性。</p><p id="c277" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">反向传播使用<strong class="kw iu">链规则</strong>，这是编写嵌套函数的导数的方便的助记符。例如，如果我们有一个网络，其中一个神经元馈入第二个神经元，最后馈入第三个神经元，以获得输出。总损失函数<em class="nh"> f </em>是前两个神经元的损失函数<em class="nh"> g </em>的函数，类似地<em class="nh"> g </em>是第一个神经元的损失函数<em class="nh"> h </em>的函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/dc997d4481ffef5a36b630be3665353e.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*w3k8VQwPabiMZTkQGHlEBQ.png"/></div></div></figure><p id="8d13" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基本上，当我们从输入开始计算每个神经元的输出，直到最后一个神经元，我们也已经计算了导数的微小分量。在上面的例子中，当向前通过第一个神经元时，我们已经能够计算出<em class="nh"> dh/dx </em>。接下来，当向前通过第二个神经元时，我们能够计算<em class="nh"> dg/dh </em>。最后，我们开始计算<em class="nh"> df/dg </em>,通过神经元回溯并重用所有已经计算过的元素。这就是反向传播这个名字的由来。这种技术有几种实现和风格。为了简单起见，我们在这里保持简单。</p><p id="7e3a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了说明链式法则和反向传播是如何工作的，让我们回到具有 sigmoid 激活的 1-神经元网络的损失函数。损失函数被定义为二进制交叉熵，它可以被分成两部分 A 和 B，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2fa4193134d2eff6b0a072c3cc84c99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*THQqRo4Gu6qB1mSM6uu0zw.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="e529" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们仔细看看损失函数的 A 部分。它可以分成几个块，在下图中用红框突出显示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/1139067090a7b073fe37ae22490e4e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*gEqmR7BCllAuIGqBzOhj7w.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="d745" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">反向传播需要计算该函数在任何给定数据点<em class="nh"> X </em>处对于任何给定权重<em class="nh"> W </em>的导数。这是通过计算每个块的导数，并使用链式法则将所有块放在一起完成的。下面我们来看看这对于<em class="nh"> X=3 </em>和<em class="nh"> W=3 </em>是如何工作的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/b3445d8ddfecd719723ea40118fb1619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5LX5k6Iq0L3xpYh4WXxTmA.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><p id="2a18" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们所需要的是能够计算小块(上面称为变量)的导数。这种块是已知的，因为激活函数通常是已知的。它们可以是 s 形的、线形的、ReLu 形的等等。这些是导数已知的可微函数。您可以在此找到最新激活功能的完整列表<a class="ae nm" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6">。因此，上述计算可以在运行时使用计算图来构建。Keras 能够查看您的网络架构和每个神经元使用的激活函数，以便在模型编译期间构建计算图。该图在训练期间用于执行正向传递和反向传播。交叉熵损失函数的计算图的例子如下所示。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/10f6732f5709050d6e2f71bb2b741c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bdLS_B5vdhr4ToPJ12TlA.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">source: P. Protopapas, Harvard</figcaption></figure><h1 id="f8ce" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">神经元聚类</h1><p id="f338" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">理解梯度下降和反向传播是如何工作的，是理解深度学习为什么实际工作的重要一步。嗯，我们还没到那种程度。到目前为止，我们只用了 3 个神经元。我们的农民可以争辩说，年份和收获量之间的关系太复杂了，不能只用两类来表示:好或坏。在这一节中，我们将探讨人工神经网络的聚类问题。让我们模拟一些分布在三组代表收割质量的数据。我们考虑坐标为(-2，0)，(0，1.7)和(2.1，0)的(x，y)平面中的三个点。在这三个中心的周围，我们创建了一个由 100 个随机生成的点组成的云。对于这些点中的任何一个到其中心的径向距离，我们使用随机正态分布。对于任何一个“云”点的角坐标，我们使用均匀分布。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/e42f8d77379439a79c36e5085621b0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*nOJoveNRd8NWsCrs_nRAoQ.png"/></div></figure><p id="5c77" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在将创建一个两层人工神经网络，它可以预测平面中随机生成的点是属于云 0、1 还是 2。输入层有 50 个神经元(我们越来越深入)！它接收两个值(<em class="nh"> x，y </em>)。ReLU 激活函数被应用于输入的仿射变换。结果被输入到具有 3 个神经元的输出层，该层使用<strong class="kw iu"> softmax </strong>激活函数来预测三类云之一。随机梯度优化器用于通过最小化用作损失函数的<strong class="kw iu">分类交叉熵</strong>来解决估计 303 权重和偏差的分类问题。我们将数据分成 50 个数据点用于训练分类器，50 个数据点用于验证。我们在 20 个时期内分批训练 5 个点。训练之后，我们还对测试集进行预测。在显示之前，我们使用 argmax 将预测的分类向量转换为原始标签。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/1fb6067ed435745c5240a642be2aedd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4gIFrx_eo5Ut8QPvPvJgw.png"/></div></div></figure><p id="3e48" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如上所示，神经网络在分离测试集中的类方面做得很好(右图)。在原始数据(左)中发现的一些异常值被网络(右)分配了不同的标签，以产生清晰的边界。我们在测试数据上取得了 95.33%的分类准确率。</p><p id="3255" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我们在平面上随机生成全新的看不见的坐标。然后我们用训练好的网络来预测类(云)。正如我们在图中看到的，网络清楚地将所有三个云分开，尽管有几个点非常接近边界，这是假阳性/阴性的良好候选。总的来说，似乎我们开始做深度学习了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/45094148c03766c54ed9dd3d53ba1283.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*kqagEP8YoOH6lv4VXpEW4g.png"/></div></figure><p id="4d61" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们让我们农民的孙子的孙子的孙子对深度学习感到兴奋！直奔<a class="ae nm" href="http://playground.tensorflow.org" rel="noopener ugc nofollow" target="_blank">http://playground.tensorflow.org</a>通过在你的浏览器中可视化地修补层、神经元、激活功能、训练和测试来构建你自己的人工神经网络。当你完成后，回到这里，更多令人兴奋的东西来了。</p><h1 id="0117" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">用于图像去噪的自动编码器</h1><p id="b06a" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">没有被现在打动的读者，已经知道深度学习为什么有效了。对于我们其余的人，我们现在来看一个网络架构的例子，这个例子有点深奥。自动编码器由一对两个相连的网络组成:编码器模型和解码器模型。自动编码器的目标是找到一种将输入编码成压缩形式(潜在空间)的方法，使得解码版本尽可能接近输入。<strong class="kw iu">编码器模型</strong>将输入转换成小而密集的表示。<strong class="kw iu">解码器模型</strong>可以被视为能够生成特定特征的生成模型。编码器和解码器通常作为一个整体来训练。损失函数惩罚网络产生不同于输入的输出。因此，编码器学会在有限的潜在空间中保存尽可能多的相关信息，并巧妙地丢弃不相关的部分，例如噪声。解码器学习获取压缩的潜在信息，并将其重构为完全无错误的输入。自动编码器可以用于图像的降维和去噪，但也可以在无监督的机器翻译中获得成功(哇！).</p><p id="5b8d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我们定义一个自动编码器的最简单的架构，它处理有噪声的手写数字的图像。我们向网络提供原始图像<em class="nh"> x </em>，以及它们的嘈杂版本<em class="nh"> x~ </em>。网络试图重建其输出<em class="nh">x’</em>以尽可能接近原始图像。通过这样做，它学会了如何去噪任何手写数字。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/d6453feb801b272c7b4c6859cae21e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qgxeODdkcdMIfR--.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk"><a class="ae nm" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="41eb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在接下来的几段中，我们将通过使用 Keras 实现这样一个网络的一个小版本来了解它是如何工作的。</p><p id="f25d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">输入</strong>:28x 28 灰度图像。因此，输入是一个 784 元素的向量。</p><p id="e04c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">编码器</strong>:64 个神经元的单一致密层。因此，潜在空间将具有尺寸 64。所用的激活函数是整流单元(ReLu)，选自最佳实践。激活函数被附加到层中的每个神经元，并基于每个神经元的输入是否与自动编码器的预测相关来确定它是否应该被激活(“激发”)。激活函数还有助于将每个神经元的输出标准化到 1 到 0 之间的范围。</p><p id="fc70" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">解码器</strong>:具有 784 个神经元的单一致密层，对应于 28x28 灰度图像。使用 sigmoid 激活函数作为分类任务的最佳实践(编码器的输入与解码器的输出)。</p><p id="a0fe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">优化器:</strong>我们使用二进制交叉熵作为损失函数，使用 Adadelta 作为优化器来最小化损失函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3ded" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">数据:</strong>我们使用<a class="ae nm" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST 数字</a>通过应用高斯噪声矩阵生成合成噪声数字，并在 0 和 1 之间裁剪图像。下面我们展示一些有噪声的数字。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/02d26308fb98429c8a9d49a9331e1f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMFVJYMKPd5cTSTAM8scGw.png"/></div></div></figure><p id="4c78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们仍然可以识别数字，但很难。因此，我们想使用我们的自动编码器来学习恢复原始数字。我们通过在 100 个时期内拟合自动编码器来实现这一点，同时使用噪声数字作为输入，原始去噪数字作为目标。因此，自动编码器将最小化噪声图像和干净图像之间的差异。通过这样做，它将学习如何从任何看不见的手写数字中去除噪声，这些数字是由类似的噪声产生的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/dc3a97900006200121f8f2c9d1752e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*PLBqcw1kPpsjMcij3VO6pA.png"/></div></figure><p id="a8b7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面，我们将有噪声的输入图像与干净的输入图像进行对比。</p><ul class=""><li id="8c11" class="pj pk it kw b kx ky la lb ld pl lh pm ll pn lp po pp pq pr bi translated">整体噪声消除得非常好。在输入图像上人为引入的白点已经从清洗后的图像中消失。数字可以被视觉识别。例如，有噪声的数字“4”根本不可读，现在，我们可以读取它的干净版本。</li><li id="83c7" class="pj pk it kw b kx ps la pt ld pu lh pv ll pw lp po pp pq pr bi translated">去噪对信息质量有负面影响。重建的数字有些模糊。解码器添加了一些原始图像中不存在的特征，例如，下面的第 8 位和第 9 位数字几乎无法识别。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/6e758eaedcfa88dedf634ed4c30a9eea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pzPbp765q8NlgJWg2PXXRQ.png"/></div></div></figure><p id="e432" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">希望这个去噪的例子已经说明了深度学习的机会。我们通过添加层得到的越深，模型变得越复杂，它能实现的魔力就越大。然而，复杂性是以过度适应训练数据和泛化能力下降为代价的。这就是正规化和其他技巧发挥作用的地方。在下一节中，我们将通过添加卷积层来升级我们的自动编码器。</p><h1 id="27bc" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">卷积神经网络</h1><p id="044c" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">前面几节温和地介绍了用神经元来近似线性和非线性现象的思想。我们还演示了两层神经元如何足以学习手写数字中的正确特征，并消除有噪声的特征。在这一部分中，我们将探讨如何将具有数百个神经元的几个层堆叠起来，这些神经元在噪声图像中学习低级特征，以减少潜在空间中的信息损失，并提高生成数字的质量。</p><p id="7e30" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，让我们建立一个深度网络。我们从第一个卷积层 Conv2D 开始，它有 16 个滤波器，每个滤波器的尺寸为 3×3。填充相同，步幅为 1，因此输出要素地图的形状为 28x28x16。我们对这个输出应用一个 ReLu 激活函数，在它上面加上一个偏置。卷积导致来自图像相邻区域的信息重复，导致高维数。<strong class="kw iu"> Pooling </strong>取卷积层输出的一个区域，并返回一个聚合值:通常是 max，但可以使用 average、min 或任何函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi gj"><img src="../Images/1dfc8ae0aac30f06faeaa72704a793c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XJ9rbW9sHPc3n7Z5.gif"/></div></div></figure><p id="8588" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们添加一个 MaxPooling2D 层，大小为 2x2，步幅为 1，填充相同。这意味着我们将取 2x2 块的最大值，并移动 1 步到下一个块。池化的输出将用零填充，以具有与输入相同的维度。将激活函数应用于卷积后，总会进行池化。</p><p id="c353" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当考虑多个尺度时，CNN 工作得最好。因此，我们再次复制卷积和最大池层。得到的编码器以 7x7x32 的形状输出图像的表示。这是因为输入图像 28x28x1 由第一 Conv2D 层中的 32 个 3x3 滤波器中的每一个进行卷积，返回维数为 28x28x32 的特征空间。MaxPooling 将这种表示减少到 14x14x32 的一半。第二个 Conv2D 层使用一组 32 个 3x3 滤波器对每个 14x14x32 表示进行卷积。这导致 14×14×32 维的特征空间。最后一个 MaxPooling 图层将制图表达缩小了一半，从而得到形状 7x7x32。</p><p id="09ac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">解码器与编码器具有相同的架构，但各层以相反的顺序排列。我们使用上采样层来恢复由编码器中的 MaxPooling 执行的维度减少。最后，我们添加一个卷积层，将上次表示中的通道数恢复为 1。Sigmoid 激活函数用于解决分类问题，其中二进制结果反映了解码图像与输入图像的相似程度。我们可以观察自动编码器的解码部分如何恢复原始图像形状(28x28x1)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/9b6e56587972c58c9882ebab1fee81f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*phpzbMuw-Mdjgsjgq64ckg.png"/></div></figure><p id="67fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">整个网络有 28，353 个权重和偏差要训练。我们使用二进制交叉熵作为损失函数，使用 Adadelta 作为优化器来最小化损失函数。在 20 个时期之后，CNN 自动编码器具有大约 0.098 的训练/测试损失值，相比之下，密集自动编码器具有更高的损失 0.1235。我们的 CNN 自动编码器在看不见的图像上表现很好。与上一节中带有 1 个密集隐藏层的简单自动编码器相比，去噪后的图像看起来要好得多。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/5d5d10d6b4b1a712a8631ce9942e3f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zXlffracBosWn_ScNTpdqg.png"/></div></div></figure><h1 id="16e0" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">越来越深</h1><p id="efd3" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">将卷积层堆叠在一起的方法与其说是科学，不如说是艺术。不要害怕看下面的 VGG 网络，它是在 2014 年提出的，用于以 92.7%的测试准确度将 1400 万张图像分类到 1000 个类别。它接受了 2 周的训练，以估计其 14，714，688 的体重。恐怖！还有更多这样的超大质量网络:GoogLeNet、ResNet、DenseNet、MobileNet、Xception、ResNeXt，仅举几例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/084ccf8c57e80ef39c2a2a0c5c8153b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/0*8UhP6K5T4OipNZSF.png"/></div></figure><p id="b15a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">深度学习的一个推荐且高效的方法是利用那些预先训练好的网络之一。预先训练的网络只是先前在大型数据集上训练的保存的网络，并且可以有效地充当真实世界的通用模型。特征提取是使用由预训练网络学习的表示的一种方式，它采用预训练网络的卷积基，通过它运行新数据，并使用新的小数据集在输出之上训练新的简单分类器，如下所示。另一种技术包括冻结预训练网络的基础，附加一个简单的分类器并训练整体。另一种替代方案，称为微调，包括在训练前解冻基地中的特定层。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="8df2" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="7892" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">在本文中，我们假设了一种深度学习的神经网络架构方法。我们没有直接解释为什么深度学习算法会在复杂输入通过每个神经网络层时逐步学习复杂输入的表示，而是从单个神经元开始。通过动手的例子和 Python 代码，我们用人工神经网络解决了基本的回归任务，并逐步建立了一个具有卷积层的自动编码器。我们通过梯度下降和反向传播展示了学习过程。我们很快介绍了使用预先训练的超大质量人工神经网络(如 VGG16)的迁移学习。</p><p id="cac4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们没有触及像风格转换这样的主题，对于物体识别来说，纹理，而不是物体形状，似乎是物体最重要的方面。我们解释的概念，对于递归神经网络、序列对序列网络、长短期记忆网络和许多其他奇特的深度学习外星发明，将略有不同。</p><p id="bd86" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在一篇文章中涵盖深度学习是一个笑话，我们并没有试图这样做。经典书籍<strong class="kw iu"><strong class="kw iu">伊恩·古德菲勒</strong>的《深度学习》</strong>和<strong class="kw iu">的《用 Python 进行深度学习》</strong>的<strong class="kw iu">弗朗索瓦·乔莱</strong>都是很好的起点。</p><p id="b39b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">深度学习为什么有效？这是一个开放式的问题。谢天谢地。</p><p id="c27a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">感谢您的阅读。看看我下面这篇关于机器学习中贝叶斯方法的文章。</p><div class="qb qc gp gr qd qe"><a rel="noopener follow" target="_blank" href="/bayesian-nightmare-how-to-start-loving-bayes-1622741fa960"><div class="qf ab fo"><div class="qg ab qh cl cj qi"><h2 class="bd iu gy z fp qj fr fs qk fu fw is bi translated">贝叶斯噩梦。解决了！</h2><div class="ql l"><h3 class="bd b gy z fp qj fr fs qk fu fw dk translated">通过 Python PyMC3 中的示例和 cod 温和地介绍贝叶斯数据分析。</h3></div><div class="qm l"><p class="bd b dl z fp qj fr fs qk fu fw dk translated">towardsdatascience.com</p></div></div><div class="qn l"><div class="qo l qp qq qr qn qs ks qe"/></div></div></a></div></div></div>    
</body>
</html>