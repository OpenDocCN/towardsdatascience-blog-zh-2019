<html>
<head>
<title>A Step-by-Step Implementation of Gradient Descent and Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降和反向传播的逐步实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-by-step-implementation-of-gradient-descent-and-backpropagation-d58bda486110?source=collection_archive---------4-----------------------#2019-05-30">https://towardsdatascience.com/a-step-by-step-implementation-of-gradient-descent-and-backpropagation-d58bda486110?source=collection_archive---------4-----------------------#2019-05-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e6fe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从头开始构建神经网络的一个例子</h2></div><p id="53f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章背后的初衷仅仅是我在神经网络中梳理数学，因为我喜欢精通算法的内部工作，并获得事物的本质。然后我想，与其一遍又一遍地重温记事本上的公式，我还不如编一个故事。尽管您可能会找到一些从头构建简单神经网络的教程。不同的人看待事物的角度不同，学习的侧重点也不同。另一种思维方式可能在某种意义上增强理解。所以让我们开始吧。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/fe943db241d2e6ee3c8a4858a0d547ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DVq-6bM9GMQhZnaIcrn9Q.jpeg"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Photo from Unsplash</figcaption></figure><p id="dd7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lr">概括地说神经网络</em> </strong></p><p id="19b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络的核心是一个大函数，它将一些输入映射到所需的目标值，在中间步骤中执行生成网络的操作，这是通过在一个反复执行此操作的流水线场景中乘以权重并添加偏差来实现的。训练神经网络的过程是确定一组参数，使期望值和模型输出之间的差异最小化。这是使用梯度下降(又名反向传播)来完成的，根据定义，梯度下降包括两个步骤:计算损失/误差函数的梯度，然后响应于梯度更新现有参数，这就是下降是如何完成的。重复这个循环，直到达到损失函数的最小值。这个学习过程可以用简单的等式来描述:W(t+1) = W(t) — dJ(W)/dW(t)。</p><p id="62b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lr">数学直觉</em> </strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/7cead379b8a68921634aa047e7ca2ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9agaYlewb0vzuJIOupf3w.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Photo from <a class="ae lt" href="https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75" rel="noopener ugc nofollow" target="_blank">https://blog.goodaudience.com/artificial-neural-networks-explained-436fcf36e75</a></figcaption></figure><p id="f2de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出于我自己的练习目的，我喜欢使用一个如图所示的只有一个隐藏层的小网络。在该布局中，X 表示输入，下标 I、j、k 分别表示输入、隐藏和输出层中的单元数量；w_ij 表示连接输入到隐藏层的权重，w_jk 是连接隐藏到输出层的权重。</p><p id="a07f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，模型输出计算如下:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lu"><img src="../Images/099d887f6b299d2133279b221eb9cd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QHM97X0-NvgRkCwplDtrXQ.png"/></div></div></figure><p id="50ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">损失函数的选择通常是误差平方和。这里，我使用 sigmoid 激活函数，为了简单起见，假设偏差 b 为 0，这意味着权重是影响模型输出的唯一变量。让我们推导出计算隐藏到输出权重 w_jk 的梯度的公式。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lv"><img src="../Images/fe1e84d61a9f0ecdfca355397fee79f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCHvviwoArAoB92kawUBmg.png"/></div></div></figure><p id="fa67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">确定隐权输入的复杂性在于它间接影响输出误差。每个隐藏单元输出影响模型输出，因此对隐藏权重 w_ij 的输入取决于它所连接的所有单元的误差。推导开始也一样，只是把 z_k 处的链式法则展开到子函数上。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lw"><img src="../Images/22048a2971022553cfdb4c093301d066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NWz132H10NIQ5ZBiWripmg.png"/></div></div></figure><p id="b70e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lr">更多思想:</em> </strong></p><p id="71fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，两个权重的梯度具有相似的形式。误差通过激活函数的导数反向传播，然后通过来自前一层的输入(激活输出)加权。在第二个公式中，来自输出层的反向传播误差被进一步投影到 w_jk 上，然后重复相同的反向传播方式并通过输入进行加权。这种反向传播过程一直重复到任意层神经网络的第一层。<em class="lr"> </em> <strong class="kh ir"> <em class="lr">“因此，关于每个参数的梯度被认为是参数对误差的贡献，并且应该在学习期间被否定。”</em> </strong></p><p id="a9d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将上述过程编写成代码:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lx"><img src="../Images/618474d65b9cd806807f64b0278b1bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4mspxFowlO3vmXxUa3EX7w.png"/></div></div></figure><p id="a2ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是完整的示例:</p><pre class="lc ld le lf gt ly lz ma mb aw mc bi"><span id="54f4" class="md me iq lz b gy mf mg l mh mi">import numpy as np</span><span id="4946" class="md me iq lz b gy mj mg l mh mi">class NeuralNetwork:<br/>    def __init__(self):<br/>        np.random.seed(10) # for generating the same results<br/>        self.wij   = np.random.rand(3,4) # input to hidden layer weights<br/>        self.wjk   = np.random.rand(4,1) # hidden layer to output weights<br/>        <br/>    def sigmoid(self, x, w):<br/>        z = np.dot(x, w)<br/>        return 1/(1 + np.exp(-z))<br/>    <br/>    def sigmoid_derivative(self, x, w):<br/>        return self.sigmoid(x, w) * (1 - self.sigmoid(x, w))<br/>    <br/>    def gradient_descent(self, x, y, iterations):<br/>        for i in range(iterations):<br/>            Xi = x<br/>            Xj = self.sigmoid(Xi, self.wij)<br/>            yhat = self.sigmoid(Xj, self.wjk)<br/>            # gradients for hidden to output weights<br/>            g_wjk = np.dot(Xj.T, (y - yhat) * self.sigmoid_derivative(Xj, self.wjk))<br/>            # gradients for input to hidden weights<br/>            g_wij = np.dot(Xi.T, np.dot((y - yhat) * self.sigmoid_derivative(Xj, self.wjk), self.wjk.T) * self.sigmoid_derivative(Xi, self.wij))<br/>            # update weights<br/>            self.wij += g_wij<br/>            self.wjk += g_wjk<br/>        print('The final prediction from neural network are: ')<br/>        print(yhat)</span><span id="cc9c" class="md me iq lz b gy mj mg l mh mi">if __name__ == '__main__':<br/>    neural_network = NeuralNetwork()<br/>    print('Random starting input to hidden weights: ')<br/>    print(neural_network.wij)<br/>    print('Random starting hidden to output weights: ')<br/>    print(neural_network.wjk)<br/>    X = np.array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])<br/>    y = np.array([[0, 1, 1, 0]]).T<br/>    neural_network.gradient_descent(X, y, 10000)</span></pre><p id="5e31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考资料:</p><ol class=""><li id="4002" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated"><a class="ae lt" href="https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://theclevermachine . WordPress . com/2014/09/06/derivation-error-back propagation-gradient-descent-for-neural-networks/</a></li><li id="c852" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated"><a class="ae lt" rel="noopener" target="_blank" href="/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6">https://towards data science . com/how-to-build-your-own-your-own-neural-network-in-python-68998 a08e 4 f 6</a></li></ol></div></div>    
</body>
</html>