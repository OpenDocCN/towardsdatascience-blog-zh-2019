<html>
<head>
<title>How to Make Computers Dream</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何让电脑做梦</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-make-computers-dream-3b4b10e4463a?source=collection_archive---------26-----------------------#2019-11-24">https://towardsdatascience.com/how-to-make-computers-dream-3b4b10e4463a?source=collection_archive---------26-----------------------#2019-11-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8a90" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">生成模型的软介绍</h2></div><blockquote class="ki kj kk"><p id="9c63" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">“当一个人理解了原因，所有消失的图像可以很容易地通过原因的印象在大脑中重新找到。<br/>这才是真正的记忆艺术……”<br/>勒内·笛卡尔，私人思考。</p></blockquote><p id="91fe" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">闭上眼睛(当你试图说服人们阅读你的文章时，这是一个糟糕的开场白)，想象一张脸。</p><p id="4533" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">现在想象一只猫。一只狗，房子，汽车，水槽，一瓶啤酒，桌子，树。</p><p id="5986" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这一切都在你的头脑中，很容易在你的内眼、内耳和内心世界中想象出来。但是你有没有想过，当你想象一个前所未见、前所未闻的东西时，会发生什么？</p><p id="fbef" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们都有能力想出不存在的新奇事物。在电车上看到一则广告后，我们塑造了对未来的愿景，并突然体验到自己在遥远的土地上。</p><p id="beaa" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">当我们晚上睡觉时，闭上眼睛，远离世界上所有的模糊事物和所有的感官输入，我们会梦见一个新的世界，里面住着和我们在现实生活中熟悉的人极其相似的人。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/923c63d2eb60dd0f419474852560174b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmE5QCY66yToq-ruNxD7lg.jpeg"/></div></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">Salvador Dali: Dream Caused by the Flight of a Bumblebee around a Pomegranate a Second Before Awakening (fair use).</figcaption></figure><h2 id="17e1" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">我们能从中学到什么</h2><p id="b434" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku li mw kx ky lj mx lb lc lk my lf lg lh im bi translated">神经科学家和人工智能研究人员都意识到，我们如此强大的想象能力是我们智力的一个重要方面。因此，在构建类人智能时，对它进行建模可能是对我们工具箱的重要扩展。</p><p id="cd42" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">但是我们能教机器如何做梦吗？这难道不是人类独有的东西，与计算机的精确和机械决定论如此不同吗？</p><p id="ff38" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">答案是肯定的。看看这些快乐的人们:</p><div class="lm ln lo lp gt ab cb"><figure class="mz lq na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/d584906b4d1b746bd2c73aca0f06c8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZCQNHt2W_RtwOtPfEsQmlg.jpeg"/></div></figure><figure class="mz lq na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/56bf7dc56ec42c34936c8870592e268a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2t_GM4frEbIz4uR3kE8mKg.jpeg"/></div></figure></div><div class="ab cb"><figure class="mz lq na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/9f802c013acba03441a5242c2b88f86e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*cl6pGW3clYumFV4pknPa8Q.jpeg"/></div></figure><figure class="mz lq na nb nc nd ne paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/fddde427fa687f151a424a63deb7baf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oo0PR8E6McBbJWg7bjLacw.jpeg"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk nf di ng nh">George, Barbara, Aditya and Mikey, as imagined by a computer (via <a class="ae ni" href="https://arxiv.org/abs/1812.04948" rel="noopener ugc nofollow" target="_blank">StyleGAN</a>).</figcaption></figure></div><p id="898a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">你可以看着他们，感受他们的生活。他们在哪里长大？他们上的是哪所高中？他们的性格是怎样的？他们是相当严肃的人，还是厚颜无耻的微笑的痕迹？</p><p id="7ea3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">他们是你想象中的那种人。也许他们会出现在你的梦里。</p><p id="a764" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">问题是他们四个都不存在。它们完全是我五分钟前让我的电脑生成的虚构。<strong class="ko iu"> <em class="kn">那是一台电脑想象出来的，就像你想象出来的一样。</em> </strong></p><p id="8605" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu"> <em class="kn">这怎么可能？</em> </strong></p><h2 id="261d" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">不同程度的随机性</h2><p id="1fe8" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku li mw kx ky lj mx lb lc lk my lf lg lh im bi translated">让我们后退几步。</p><p id="248b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">图片由成千上万的像素组成，其中每个像素包含单独的颜色信息(例如，用 RGB 编码)。如果您随机生成一张图片(每个像素都是独立于所有其他像素生成的)，它看起来像这样:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e6c2462b503031fc51a4cd4ebd82b3ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*S65MaMFfnudlUsy9_DfKBg.jpeg"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">A truly random picture. If you see a face in this, you should consider seeing a doctor.</figcaption></figure><p id="02c8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">如果你每秒钟都以这种方式生成一张图片，直到一张脸出现，你不会在几十亿年后更接近你的目标，当太阳在你脸上爆炸时，你仍然会生成图片。</p><p id="b94a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu"> <em class="kn">大部分随机图片都不是人脸的图片，人脸的随机图片非常不随机。</em> </strong></p><h2 id="2a0f" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">潜在结构</h2><p id="14d7" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku li mw kx ky lj mx lb lc lk my lf lg lh im bi translated">每张脸后面都有一个潜在的结构。在这种潜在结构的范围内，有一定程度的灵活性，但它受到的约束要比不受约束多得多。</p><p id="ebc8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">人类非常擅长理解潜在结构。我们不会根据像素来考虑人脸:我们会考虑嘴巴、鼻子和耳朵等特征，以及眉毛之间的距离(或没有距离)，我们会在脑海中建立一个抽象的面孔表示(更多信息请见我的文章<a class="ae ni" rel="noopener" target="_blank" href="/the-geometry-of-thought-700047775956">思维的几何学</a>)，这使我们可以很容易地想象一张脸，无论是在脑海中还是通过绘制。</p><p id="acfa" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">总的来说，在世界中寻找潜在结构不仅对在世界中导航和交流非常重要，因此对我们的生存也非常重要，而且从某种意义上说，寻找潜在结构是所有科学实践的基石，可以说，也是智力的基石(正如我在关于<a class="ae ni" rel="noopener" target="_blank" href="/why-intelligence-might-be-simpler-than-we-think-1d3d7feb5d34">为什么智力可能比我们想象的更简单</a>的文章中所详述的)。</p><p id="1f10" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">当我们从外部世界收集数据时，数据是由我们通常看不到的过程产生的。建立一个世界模型意味着寻找这些产生我们观察到的数据的隐藏过程。</p><p id="a4c9" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">物理定律，无论是牛顿定律还是薛定谔方程，都是这些潜在原理的浓缩、抽象的表现。就像牛顿的例子一样，意识到一个<strong class="ko iu"> <em class="kn">落下的苹果遵循与行星</em> </strong>轨道相同的规律意味着理解这个世界比它看起来有序得多，随机得少。</p><h2 id="fb8f" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">生成模型</h2><blockquote class="ki kj kk"><p id="8b58" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">"我不能创造的东西，我不明白。"<br/>理查德·费曼</p></blockquote><p id="c53b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">生成模型的目标是学习一些输入数据<strong class="ko iu"> x <em class="kn"> </em> </strong>背后的潜在结构的有效表示(例如在我们的声音文件的大量人脸图片上进行训练)，理解控制其分布的规律，并使用它来生成共享输入数据 x 的关键特征的新输出<strong class="ko iu">x’</strong></p><p id="e652" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">您的输入数据不是真正随机的这一事实意味着 x 后面有一个结构，这意味着有某些<strong class="ko iu"> <em class="kn">非平凡(平凡只是随机点)概率分布负责生成数据</em> </strong>。但是这些概率分布对于高维输入通常是极其复杂的，并且通常很难解析出来。</p><p id="d7ad" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这就是深度学习的救援之处，它已经一次又一次地被证明在捕捉数据中各种复杂的非线性相关性方面非常成功，并允许我们很好地利用它们。</p><p id="7994" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">生成模型可以采取许多形状和形式，但我认为<strong class="ko iu"> <em class="kn">变型自动编码器</em> </strong>是一个非常有启发性的例子，所以我们现在将仔细看看它们是如何工作的。</p><h2 id="aed9" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">潜在变量</h2><p id="96ff" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku li mw kx ky lj mx lb lc lk my lf lg lh im bi translated">一个<a class="ae ni" href="https://arxiv.org/abs/1312.6114" rel="noopener ugc nofollow" target="_blank">变分自动编码器</a>通常使用<strong class="ko iu"> <em class="kn">两个深度神经网络构建。</em>T9】</strong></p><p id="165a" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">第一深度神经网络<strong class="ko iu"> <em class="kn">学习</em> </strong>输入数据<strong class="ko iu"> x </strong>的潜在(通常是低维)表示。</p><p id="0624" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">它<strong class="ko iu"> <em class="kn">将</em> </strong>这种潜在结构编码在<strong class="ko iu"> <em class="kn">关于一些潜在变量的概率分布</em> </strong>中，我们用<strong class="ko iu"> z 来表示</strong>那么<strong class="ko iu"> </strong>的主要任务就是在给定我们的数据的情况下，找到所谓的潜在变量的后验，在<strong class="ko iu"> <em class="kn">概率论</em> </strong>的语言中写成<strong class="ko iu"> p(z|x) </strong>。相应地，这个步骤被称为<strong class="ko iu"> <em class="kn">编码器</em> </strong>。</p><p id="84b5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">请注意，这有点类似于<strong class="ko iu"> <em class="kn">辨别神经网络</em> </strong>在监督分类任务中所做的事情:它被训练成在与标签相关的数据中找到结构，例如，允许它区分猫和狗的图片。</p><p id="3f93" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">只是在生成模型的情况下，我们在寻找数据本身的<strong class="ko iu"><em class="kn"/></strong>概率分布，对于自动编码器，我们将其编码在潜在变量中。</p><p id="3d88" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">对于技术上更感兴趣的人来说:实现这一点的一种方法是通过在潜在变量上引入一类近似先验分布<a class="ae ni" href="https://arxiv.org/abs/1906.02691" rel="noopener ugc nofollow" target="_blank">(例如<strong class="ko iu"><em class="kn"/></strong>高斯分布的组合)并训练网络找到这些分布的参数(例如<strong class="ko iu"> <em class="kn">表示</em> </strong>和<strong class="ko iu"> <em class="kn">协方差</em> </strong>)，它们尽可能接近真实先验(由<strong class="ko iu">测量)</strong></a></p><p id="c7c8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">一旦模型已经学习了潜在变量<strong class="ko iu"> z </strong>上的<strong class="ko iu"> <em class="kn">概率分布</em> </strong>，它可以通过从<strong class="ko iu"> p(z|x) </strong>采样并执行第一个网络的反向任务来使用这一知识生成新数据<strong class="ko iu">x’</strong>，这意味着寻找由<strong class="ko iu"> p(x'|z) </strong>给出的以潜在变量为条件的数据的后验概率。</p><p id="f493" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">换句话说:给定我们之前通过使用第一个网络了解到的潜在变量<strong class="ko iu"> p(z|x) </strong>的分布，新数据会是什么样子？</p><p id="c53f" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这个步骤然后被称为<strong class="ko iu"> <em class="kn">解码器</em> </strong>或<strong class="ko iu"> <em class="kn">生成模型</em> </strong>。</p><p id="9bc0" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">我们可以通过同样训练一个<strong class="ko iu"> <em class="kn">神经网络</em> </strong>将随机变量<strong class="ko iu"> z </strong>映射到新数据<strong class="ko iu">x’上来构建它。</strong></p><p id="72dd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">总结一下<strong class="ko iu"> <em class="kn">变型自动编码器</em> </strong>正在做的事情:</p><ol class=""><li id="0c5d" class="nk nl it ko b kp kq ks kt li nm lj nn lk no lh np nq nr ns bi translated">从输入数据中学习后验 x→z:<strong class="ko iu">p(z | x)</strong></li><li id="86c4" class="nk nl it ko b kp nt ks nu li nv lj nw lk nx lh np nq nr ns bi translated">从模型生成新数据 z→x ':<strong class="ko iu">p(x ' | z)</strong></li></ol><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/665ad2f814e509349cfa8851f0425d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*wKE69-fX180Q_gkzYzGbwg.png"/></div><figcaption class="lx ly gj gh gi lz ma bd b be z dk">The variational autoencoder. Credit to Chervinskii [CC BY-SA 4.0]</figcaption></figure><h2 id="b7e6" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">亥姆霍兹机器会梦到电羊吗？</h2><blockquote class="ki kj kk"><p id="df7d" class="kl km kn ko b kp kq ju kr ks kt jx ku kv kw kx ky kz la lb lc ld le lf lg lh im bi translated">“电器也有它们的寿命。这些生命是微不足道的。”菲利普·K·蒂克，机器人会梦见电动绵羊吗</p></blockquote><p id="d996" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这里缺少一个关键因素:我们如何训练生成模型？</p><p id="68a8" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">这可能相当棘手。训练它们通常比训练有鉴别能力的模型要困难得多。因为在你自己创作之前，你真的真的需要理解一些东西:认识一首贝多芬的交响曲比你自己创作一首容易得多。 </p><p id="53a3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">为了训练模型，我们需要一些损失函数来进行训练，并需要一个算法来实现它，同时需要训练多个网络。</p><p id="7ea5" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">最早的生成模型之一叫做<strong class="ko iu"> <em class="kn">亥姆霍兹机器，</em> </strong> <a class="ae ni" href="http://www.gatsby.ucl.ac.uk/~dayan/papers/hm95.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ko iu"> <em class="kn">由达扬、辛顿、尼尔</em> </strong> </a> <strong class="ko iu"> <em class="kn">于 1995 年开发。</em>T56】</strong></p><p id="a2d3" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">亥姆霍兹机器<strong class="ko iu"> <em class="kn"> </em> </strong>可以使用所谓的<a class="ae ni" href="https://en.wikipedia.org/wiki/Wake-sleep_algorithm" rel="noopener ugc nofollow" target="_blank">唤醒-睡眠算法</a>进行训练。</p><p id="4f5e" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu"> <em class="kn">在唤醒阶段</em> </strong>，网络从世界中查看数据 x，并试图推断潜在状态的后验<strong class="ko iu"> p(z|x) </strong>。</p><p id="9efd" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu"> <em class="kn">在睡眠阶段</em> </strong>，网络基于其内部的世界模型，从<strong class="ko iu"> p(x'|z) </strong>中生成(“梦境”)新数据，并试图使其梦境与现实趋同。</p><p id="877c" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">在这两个步骤中，机器被训练以最小化模型的自由能(也称为“惊奇”)。通过逐步减少惊奇(然后可以通过梯度下降等方法合并)，生成的数据和真实数据变得越来越相似。</p><h2 id="4632" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated"><strong class="ak"> <em class="nz">不同的生成模型</em> </strong></h2><p id="02a2" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku li mw kx ky lj mx lb lc lk my lf lg lh im bi translated">现代深度学习中使用了几种类型的生成模型，它们建立在亥姆霍兹机器上，但克服了它们的一些问题(例如唤醒-睡眠算法效率低下/不收敛)。</p><p id="25e2" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">在上面介绍的<strong class="ko iu"> <em class="kn">变分自动编码器</em> </strong>中，目的是尽可能好地重构输入数据。这对于实际应用非常有用，例如数据去噪或重建数据的丢失部分。它是通过最小化所谓的<a class="ae ni" href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="noopener ugc nofollow" target="_blank"> ELBO(证据下限)</a>来训练的。</p><p id="c1aa" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">另一个强大的方法是由<a class="ae ni" href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="noopener ugc nofollow" target="_blank">通用对抗网络</a> (GAN)给出的，它被用来生成你之前看到的人脸。</p><p id="c575" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">在 GANs 中，在<strong class="ko iu"> <em class="kn">生成模型</em> </strong>之上引入了一个<strong class="ko iu"> <em class="kn">鉴别器网络</em> </strong>，然后对其进行训练以区分其输入是真实数据 x 还是生成数据 x’。不使用编码器网络，但随机采样 z，并训练生成模型，以使鉴别器网络尽可能难以辨别输出数据是真是假。</p><p id="2b5b" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">请注意，生成模型背后的思想非常抽象，因此非常灵活。你可以在各种数据(不仅仅是图片)上训练它们，比如在时间序列数据上的<a class="ae ni" href="https://arxiv.org/abs/1902.07186" rel="noopener ugc nofollow" target="_blank">递归神经网络(RNNs)，例如 fMRI 数据或来自大脑的尖峰脉冲串</a>。在推断出数据背后的潜在结构之后，可以对训练的模型进行分析，以提高对大脑中潜在过程的理解(例如<a class="ae ni" href="https://www.nature.com/articles/s41380-019-0365-9" rel="noopener ugc nofollow" target="_blank">与精神疾病</a>有关的动力系统属性等)。).</p><h2 id="c304" class="mb mc it bd md me mf dn mg mh mi dp mj li mk ml mm lj mn mo mp lk mq mr ms mt bi translated">生成模型与认知</h2><p id="4704" class="pw-post-body-paragraph kl km it ko b kp mu ju kr ks mv jx ku li mw kx ky lj mx lb lc lk my lf lg lh im bi translated">生成模型所能实现的已经令人印象深刻，但它们也能让我们更进一步了解我们的大脑是如何工作的。他们不仅被动地对世界进行分类，还主动捕捉其中的基本结构，并将其整合到模型本身中。就像我们都生活在我们大脑创造的自己的内心世界一样，生成模型创造了他们自己微小的内心世界。</p><p id="ffaa" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated">正如贝叶斯大脑假说的支持者所说，这是我们认知器官的一个关键特征。我们的大脑不断建立潜在的内部表征，以某种方式反映现实世界的概率分布，但也简化它们并专注于最重要的事情(因为这个世界太复杂了，你的大脑无法完全模拟)。</p><p id="a017" class="pw-post-body-paragraph kl km it ko b kp kq ju kr ks kt jx ku li kw kx ky lj la lb lc lk le lf lg lh im bi translated"><strong class="ko iu"> <em class="kn">在精神生成模型中，一旦你能创造出你知道它是如何运作的东西。因此，制造能够做梦和想象的机器可能会让我们在理解我们如何做梦和想象自己方面走得更远。</em> </strong></p></div></div>    
</body>
</html>