<html>
<head>
<title>How to Improve Training your Deep Neural Network in Tensorflow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在 Tensorflow 2.0 中改进训练您的深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-improve-training-your-deep-neural-network-in-tensorflow-2-0-a3f62d6367c1?source=collection_archive---------29-----------------------#2019-10-30">https://towardsdatascience.com/how-to-improve-training-your-deep-neural-network-in-tensorflow-2-0-a3f62d6367c1?source=collection_archive---------29-----------------------#2019-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/20c00f6743dbef40def61d82425b64a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NCqB30SgphDQnjGk.jpg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Taken from <a class="ae kf" href="http://www.merzpraxis.de/index.php/2016/06/13/der-suchende/" rel="noopener ugc nofollow" target="_blank">http://www.merzpraxis.de/index.php/2016/06/13/der-suchende/</a></figcaption></figure><p id="dfd1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当涉及到建立和训练深度神经网络时，你需要设置大量的超参数。正确设置这些参数对你的网的成功有巨大的影响，也对你花在加热空气上的时间，也就是训练你的模型有巨大的影响。其中一个你必须选择的参数是所谓的学习率(也称为更新率或步长)。在很长一段时间里，选择这种权利更像是试错法或黑色艺术。然而，有一种<a class="ae kf" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">聪明而简单的技术</a>可以找到一个合适的学习率，我猜这种技术通过在<a class="ae kf" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fastai </a>中的使用变得非常流行。在本文中，我将向您简要介绍这种方法，并向您展示 Tensorflow 2 中的一个实现，它也可以通过我的<a class="ae kf" href="https://github.com/Shawe82/tf2-utils" rel="noopener ugc nofollow" target="_blank">回购</a>获得。所以让我们开始吧。</p><h2 id="c233" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">问题是</h2><p id="f615" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">学习率<em class="mc"> l </em>是一个单一的浮点数，它决定了你向负梯度方向移动多远，以更新和优化你的网络。正如在介绍中已经说过的，正确地选择它极大地影响了你花在训练你的模型上的时间，直到你得到好的结果并停止咒骂。为什么会这样呢？如果你选择的太小，你的模型将需要很长时间才能达到最佳状态，因为你只需要一小步一小步的更新。如果你选择的太大，你的模型将会反弹，跳过最优值，最终无法达到最优值。</p><h2 id="9bcd" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">解决方案</h2><p id="04df" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">莱斯利·n·史密斯提出了一个非常聪明和简单的方法，在短时间内系统地找到一个让你非常快乐的学习率。前提是你有一个模型，并且你有一个被分成<em class="mc"> n </em>批的训练集。</p><ol class=""><li id="999c" class="md me it ki b kj kk kn ko kr mf kv mg kz mh ld mi mj mk ml bi translated">你将你的学习初始化为一个小值<em class="mc"> l </em> = <em class="mc"> l_min </em>，例如<em class="mc"> l_min </em> =0.00001</li><li id="9239" class="md me it ki b kj mm kn mn kr mo kv mp kz mq ld mi mj mk ml bi translated">你用一批训练集来更新你的模型</li><li id="9876" class="md me it ki b kj mm kn mn kr mo kv mp kz mq ld mi mj mk ml bi translated">计算损失并记录损失和使用的学习率</li><li id="ede7" class="md me it ki b kj mm kn mn kr mo kv mp kz mq ld mi mj mk ml bi translated">你成倍地提高了当前的学习速度</li><li id="2e20" class="md me it ki b kj mm kn mn kr mo kv mp kz mq ld mi mj mk ml bi translated">如果学习率已经达到预定义的最大值<em class="mc"> l_max </em>或者如果损失增加太多，则<strong class="ki iu">返回步骤 2 </strong>或者<strong class="ki iu">停止</strong>搜索</li><li id="2aab" class="md me it ki b kj mm kn mn kr mo kv mp kz mq ld mi mj mk ml bi translated">最佳学习速率是导致两次连续试验之间损失最大程度减少的速率。选择最佳学习率的另一种方法是找到导致最小损失的学习率，并将其降低一到两个数量级。</li></ol><p id="6965" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了让这一切更加直观，我给你看了在对数标度上绘制的学习率的平滑损失。红线标记计算出的最佳学习率。</p><figure class="ms mt mu mv gt ju gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b88c99263b698dda2836d499a13f0a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*5fw63oAku9tDLWv2oXyX4Q.png"/></div></figure><h2 id="688e" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">实施</h2><p id="8c93" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">由于我目前正在学习<a class="ae kf" href="https://www.tensorflow.org/guide/effective_tf2" rel="noopener ugc nofollow" target="_blank"> Tensorflow 2 </a> (TF2)，我认为通过使用新的 TF2 概念实现学习率查找器来练习它是一个好主意。除此之外，我(希望你也是)现在有了所有我(或你)想要追求的即将到来的 TF2 项目的 LR Finder。在这里发布的代码中，我用<em class="mc">步骤 1–6</em>标记了相应的行，以引用上面的清单。这里显示的代码经过了一点简化和重构，以增加在介质上的可读性。你可以在我的<a class="ae kf" href="https://github.com/Shawe82/tf2-utils" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上找到完整的代码以及一个小例子和绘图功能。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0e78" class="le lf it mx b gy nb nc l nd ne"><strong class="mx iu">from</strong> dataclasses <strong class="mx iu">import</strong> dataclass, field<br/><strong class="mx iu">from</strong> typing <strong class="mx iu">import</strong> List</span><span id="bedf" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">import</strong> numpy as np<br/><strong class="mx iu">import</strong> tensorflow <strong class="mx iu">as</strong> tf<br/><strong class="mx iu">from</strong> tqdm <strong class="mx iu">import</strong> tqdm</span><span id="68a4" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">def</strong> lr_finder(<br/>    mdl: tf.keras.Model,<br/>    opt: tf.keras.optimizers.Optimizer,<br/>    lossf: tf.keras.losses.Loss,<br/>    dataset,<br/>    learn_rates: LrGenerator,<br/>    losses: SmoothedLoss,<br/>) -&gt; Lr:<br/>    # Step 1 &amp; 4 init and decrease, Step 2 take batch<br/>    <strong class="mx iu">for</strong> lr, (source, target) <strong class="mx iu">in</strong> zip(learn_rates(), dataset): <br/>        tf.keras.backend.set_value(optimizer.lr, lr)<br/>        # Step 2 model update, Step 3 Calculate loss<br/>        loss = step(mdl, opt, lossf, source, target).numpy()<br/>        # Step 3 record losses<br/>        losses.update(loss) <br/>        # Step 5 check if done<br/>        <strong class="mx iu">if</strong> losses.no_progress:<br/>            <strong class="mx iu">break</strong></span><span id="feb7" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">return</strong> Lr(learn_rates, losses)</span><span id="5ab9" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@tf.function</strong><br/><strong class="mx iu">def</strong> step(mld, opt, lossf, src: Tensor, trg: Tensor) -&gt; Tensor:<br/>    <strong class="mx iu">with</strong> tf.GradientTape() <strong class="mx iu">as</strong> tape:<br/>        # Step 3 Compute loss<br/>        loss = lossf(trg, mld(src)) <br/>        grads = tape.gradient(loss, mld.trainable_weights)<br/>    # Step 2 Update the model<br/>    opt.apply_gradients(zip(grads, mld.trainable_weights))<br/>    <strong class="mx iu">return</strong> loss</span><span id="c29e" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@dataclass</strong><br/><strong class="mx iu">class</strong> SmoothedLoss:<br/>    sm: float = 1<br/>    _losses:List[float]=field(init=False, default_factory=list)<br/>    _sm: List[float] = field(init=False, default_factory=list)<br/>    _avg: float = field(init=False, default=0)<br/>    _best_loss: float = field(init=False, default=None)</span><span id="5c9f" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">def</strong> update(self, loss):<br/>        # Step 3 record losses<br/>        self._avg = self.sm * self._avg_loss + (1 - self.sm) * loss<br/>        smooth_loss = self._avg / (<br/>            1 - self.sm ** (len(self._sm) + 1)<br/>        )<br/>        self._best_loss = (<br/>            loss<br/>            <strong class="mx iu">if </strong>len(self._losses) == 0 or loss &lt; self._best_loss<br/>            <strong class="mx iu">else</strong> self._best_loss<br/>        )</span><span id="10ac" class="le lf it mx b gy nf nc l nd ne">self._losses.append(loss)<br/>        self._sm.append(smooth_loss)</span><span id="752f" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@property</strong><br/>    <strong class="mx iu">def</strong> no_progress(self):<br/>        <strong class="mx iu">return</strong> self._sm[-1] &gt; 4 * self._best_loss</span><span id="01b1" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@dataclass</strong><br/><strong class="mx iu">class</strong> LrGenerator:<br/>    min_lr: float<br/>    max_lr: float<br/>    n_steps: int<br/>    _lrs: List[float] = field(init=False)</span><span id="4595" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">def</strong> __call__(self):<br/>        self._lrs = [] <br/>        frac = self.max_lr / self.min_lr<br/>        <strong class="mx iu">for</strong> step <strong class="mx iu">in</strong> tqdm(range(self.n_steps)):<br/>            # Step 1 and 4 update lr and init<br/>            lr = self.min_lr*frac**(step/(self.n_steps - 1))<br/>            # Step 3 record learning rates<br/>            self._lrs.append(lr)<br/>            <strong class="mx iu">yield</strong> lr</span><span id="baf3" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@dataclass<br/>class</strong> Lr:<br/>    lr: LrGenerator<br/>    loss: SmoothedLoss<br/>    _opt_idx: int = field(init=False, default=None)</span><span id="a0ea" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@property<br/>    def </strong>opt_idx(self):<br/>        # Step 6 get best learning rate<br/>        cut = 3<br/>        <strong class="mx iu">if</strong> self._opt_idx <strong class="mx iu">is</strong> None:<br/>            sls = np.array(self.loss._smoothed_losses)<br/>            self._opt_idx = np.argmin(sls[1 + cut :] - sls[cut:-1]) <br/>        <strong class="mx iu">return</strong> self._opt_idx + 1 + cut</span><span id="ac8d" class="le lf it mx b gy nf nc l nd ne"><strong class="mx iu">@property<br/>    def</strong> lr_opt(self):<br/>        # Step 6 get best learning rate<br/>        <strong class="mx iu">return</strong> self.lr._lrs[self.opt_idx]</span></pre><h2 id="38fb" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">结束了</h2><p id="0f0e" class="pw-post-body-paragraph kg kh it ki b kj lx kl km kn ly kp kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">感谢您关注我的小文章和我的第一个 TensorFlow 2 实现。如有任何问题、意见或建议，请随时联系我。</p></div></div>    
</body>
</html>