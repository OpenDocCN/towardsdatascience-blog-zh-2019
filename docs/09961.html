<html>
<head>
<title>Deep Q Learning for the CartPole</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ç”¨äºæ¨ªç«¿çš„æ·±åº¦ Q å­¦ä¹ </h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f?source=collection_archive---------5-----------------------#2019-12-30">https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f?source=collection_archive---------5-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b4a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯ä»‹ç»æ·±åº¦ Q å­¦ä¹ çš„æ¦‚å¿µï¼Œå¹¶ç”¨å®ƒæ¥è§£å†³ OpenAI å¥èº«æˆ¿çš„ CartPole ç¯å¢ƒã€‚</p><p id="78c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">è¯¥å‘˜é¢å°†ç”±ä»¥ä¸‹éƒ¨åˆ†ç»„æˆ:</p><ol class=""><li id="7451" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">å¼€æ”¾å¼äººå·¥æ™ºèƒ½å¥èº«æˆ¿ç¯å¢ƒä»‹ç»</li><li id="5540" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">éšæœºåŸºçº¿ç­–ç•¥</li><li id="ea8e" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">æ·±åº¦ Q å­¦ä¹ </li><li id="cb59" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">å…·æœ‰é‡æ”¾è®°å¿†çš„æ·±åº¦ Q å­¦ä¹ </li><li id="de08" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">åŒé‡æ·±åº¦ Q å­¦ä¹ </li><li id="c67e" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">è½¯æ›´æ–°</li></ol><h2 id="9eb9" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">ç¯å¢ƒ</h2><p id="7309" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated"><a class="ae ma" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">æ¨ªæ‹‰æ†ç¯å¢ƒ</a>ç”±ä¸€æ ¹æ²¿æ— æ‘©æ“¦è½¨é“ç§»åŠ¨çš„æ†å­ç»„æˆã€‚é€šè¿‡å¯¹æ¨è½¦æ–½åŠ +1 æˆ–-1 çš„åŠ›æ¥æ§åˆ¶è¯¥ç³»ç»Ÿã€‚é’Ÿæ‘†å¼€å§‹ç›´ç«‹ï¼Œç›®æ ‡æ˜¯é˜²æ­¢å®ƒç¿»å€’ã€‚çŠ¶æ€ç©ºé—´ç”±å››ä¸ªå€¼è¡¨ç¤º:å°è½¦ä½ç½®ã€å°è½¦é€Ÿåº¦ã€ç£æè§’åº¦å’Œç£æå°–ç«¯çš„é€Ÿåº¦ã€‚åŠ¨ä½œç©ºé—´ç”±ä¸¤ä¸ªåŠ¨ä½œç»„æˆ:å‘å·¦ç§»åŠ¨æˆ–å‘å³ç§»åŠ¨ã€‚æ†ä¿æŒç›´ç«‹çš„æ¯ä¸ªæ—¶é—´æ­¥é•¿æä¾›+1 çš„å¥–åŠ±ã€‚å½“æŸ±å­åç¦»å‚ç›´æ–¹å‘è¶…è¿‡ 15 åº¦ï¼Œæˆ–è€…æ‰‹æ¨è½¦åç¦»ä¸­å¿ƒè¶…è¿‡ 2.4 ä¸ªå•ä½æ—¶ï¼Œè¯¥é›†ç»“æŸã€‚</p><p id="9bf0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ä¸‹é¢çš„å•å…ƒæ ¼ç»˜åˆ¶äº†ç¯å¢ƒä¸­çš„ä¸€ç»„ç¤ºä¾‹å¸§:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="50c1" class="lc ld it mg b gy mk ml l mm mn"># Demonstration<br/>env = gym.envs.make("CartPole-v1")</span><span id="cbfa" class="lc ld it mg b gy mo ml l mm mn">def get_screen():<br/>    ''' Extract one step of the simulation.'''<br/>    screen = env.render(mode='rgb_array').transpose((2, 0, 1))<br/>    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255.<br/>    return torch.from_numpy(screen)</span><span id="21ad" class="lc ld it mg b gy mo ml l mm mn"># Speify the number of simulation steps<br/>num_steps = 2</span><span id="4236" class="lc ld it mg b gy mo ml l mm mn"># Show several steps<br/>for i in range(num_steps):<br/>    clear_output(wait=True)<br/>    env.reset()<br/>    plt.figure()<br/>    plt.imshow(get_screen().cpu().permute(1, 2, 0).numpy(),<br/>               interpolation='none')<br/>    plt.title('CartPole-v0 Environment')<br/>    plt.xticks([])<br/>    plt.yticks([])<br/>    plt.show()</span></pre><p id="1318" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">æ ¹æ®å‰§é›†çš„æ•°é‡ï¼Œè¾“å‡ºå¦‚ä¸‹æ‰€ç¤º:</p><figure class="mb mc md me gt mp"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Untrained Agent</figcaption></figure><p id="4737" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œä»£ç†è¿˜æ²¡æœ‰ç»è¿‡è®­ç»ƒï¼Œæ‰€ä»¥å®ƒåªèƒ½èµ°å‡ æ­¥ã€‚æˆ‘ä»¬å°†å¾ˆå¿«æ¢è®¨ä¸€äº›èƒ½æ˜¾è‘—æé«˜æ€§èƒ½çš„ç­–ç•¥ã€‚ä½†æ˜¯é¦–å…ˆï¼Œè®©æˆ‘ä»¬å®šä¹‰ç»˜å›¾å‡½æ•°ï¼Œå®ƒå°†å¸®åŠ©æˆ‘ä»¬åˆ†æç»“æœ:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="2402" class="lc ld it mg b gy mk ml l mm mn">def plot_res(values, title=''):   <br/>    ''' Plot the reward curve and histogram of results over time.'''<br/>    # Update the window after each episode<br/>    clear_output(wait=True)<br/>    <br/>    # Define the figure<br/>    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))<br/>    f.suptitle(title)<br/>    ax[0].plot(values, label='score per run')<br/>    ax[0].axhline(195, c='red',ls='--', label='goal')<br/>    ax[0].set_xlabel('Episodes')<br/>    ax[0].set_ylabel('Reward')<br/>    x = range(len(values))<br/>    ax[0].legend()<br/>    # Calculate the trend<br/>    try:<br/>        z = np.polyfit(x, values, 1)<br/>        p = np.poly1d(z)<br/>        ax[0].plot(x,p(x),"--", label='trend')<br/>    except:<br/>        print('')<br/>    <br/>    # Plot the histogram of results<br/>    ax[1].hist(values[-50:])<br/>    ax[1].axvline(195, c='red', label='goal')<br/>    ax[1].set_xlabel('Scores per Last 50 Episodes')<br/>    ax[1].set_ylabel('Frequency')<br/>    ax[1].legend()<br/>    plt.show()</span></pre><p id="4daa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">æœ€ç»ˆçš„å‰§æƒ…ç”±ä¸¤ä¸ªæ”¯çº¿å‰§æƒ…ç»„æˆã€‚ç¬¬ä¸€ä¸ªå›¾æ˜¾ç¤ºäº†ä»£ç†åœ¨ä¸€æ®µæ—¶é—´å†…ç´¯ç§¯çš„æ€»å¥–åŠ±ï¼Œè€Œå¦ä¸€ä¸ªå›¾æ˜¾ç¤ºäº†ä»£ç†åœ¨è¿‡å» 50 é›†çš„æ€»å¥–åŠ±çš„ç›´æ–¹å›¾ã€‚å½“æˆ‘ä»¬åˆ†ææˆ‘ä»¬çš„ç­–ç•¥æ—¶ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°ä¸€äº›å›¾è¡¨ã€‚</p><h2 id="e656" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">åŸºçº¿éšæœºæ¨¡å‹</h2><p id="c955" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">åœ¨å®ç°ä»»ä½•æ·±åº¦å­¦ä¹ æ–¹æ³•ä¹‹å‰ï¼Œæˆ‘å†™äº†ä¸€ä¸ªç®€å•çš„ç­–ç•¥ï¼Œå…¶ä¸­åŠ¨ä½œæ˜¯ä»åŠ¨ä½œç©ºé—´ä¸­éšæœºé‡‡æ ·çš„ã€‚è¿™ç§æ–¹æ³•å°†ä½œä¸ºå…¶ä»–ç­–ç•¥çš„åŸºçº¿ï¼Œå¹¶ä½¿å…¶æ›´å®¹æ˜“ç†è§£å¦‚ä½•ä½¿ç”¨å¼€æ”¾çš„äººå·¥æ™ºèƒ½å¥èº«æˆ¿ç¯å¢ƒä¸ä»£ç†åˆä½œã€‚</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="ddbe" class="lc ld it mg b gy mk ml l mm mn">def random_search(env, episodes, <br/>                  title='Random Strategy'):<br/>    """ Random search strategy implementation."""<br/>    final = []<br/>    for episode in range(episodes):<br/>        state = env.reset()<br/>        done = False<br/>        total = 0<br/>        while not done:<br/>            # Sample random actions<br/>            action = env.action_space.sample()<br/>            # Take action and extract results<br/>            next_state, reward, done, _ = env.step(action)<br/>            # Update reward<br/>            total += reward<br/>            if done:<br/>                break<br/>        # Add to the final reward<br/>        final.append(total)<br/>        plot_res(final,title)<br/>    return final</span></pre><p id="7e30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ä¸€ä¸ªç¯å¢ƒæ­¥éª¤è¿”å›å‡ ä¸ªå€¼ï¼Œæ¯”å¦‚<code class="fe mw mx my mg b">next_state</code>ã€<code class="fe mw mx my mg b">reward</code>ï¼Œä»¥åŠæ¨¡æ‹Ÿæ˜¯å¦ä¸º<code class="fe mw mx my mg b">done</code>ã€‚ä¸‹å›¾æ˜¾ç¤ºäº† 150 é›†(æ¨¡æ‹Ÿè¿è¡Œ)çš„æ€»ç´¯ç§¯å¥–åŠ±:</p><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/373d5740044f10952a9d1e22d303931f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTDwkwdu6nYRsPfOR6Pa6Q.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Random Strategy</figcaption></figure><p id="bc23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ä¸Šé¢çš„å›¾å±•ç¤ºäº†éšæœºç­–ç•¥ã€‚ä¸å‡ºæ‰€æ–™ï¼Œç”¨è¿™ç§æ–¹æ³•è§£å†³ç¯å¢ƒé—®é¢˜æ˜¯ä¸å¯èƒ½çš„ã€‚ä»£ç†æ²¡æœ‰ä»ä»–ä»¬çš„ç»éªŒä¸­å­¦ä¹ ã€‚å°½ç®¡æœ‰æ—¶å¾ˆå¹¸è¿(è·å¾—å·®ä¸å¤š 75 çš„å¥–åŠ±)ï¼Œä½†ä»–ä»¬çš„å¹³å‡è¡¨ç°ä½è‡³ 10 æ­¥ã€‚</p><h2 id="0995" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">æ·±åº¦ Q å­¦ä¹ </h2><p id="7e8e" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">Q-learning èƒŒåçš„ä¸»è¦æ€æƒ³æ˜¯æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡½æ•°ğ‘„:ğ‘†ğ‘¡ğ‘ğ‘¡ğ‘’Ã—ğ´ğ‘ğ‘¡ğ‘–ğ‘œğ‘›â†’â„ï¼Œå®ƒå¯ä»¥å‘Šè¯‰ä»£ç†ä»€ä¹ˆè¡Œä¸ºä¼šå¯¼è‡´ä»€ä¹ˆå›æŠ¥ã€‚å¦‚æœæˆ‘ä»¬çŸ¥é“ğ‘„çš„ä»·å€¼ï¼Œæˆ‘ä»¬å°±æœ‰å¯èƒ½åˆ¶å®šä¸€ä¸ªæœ€å¤§åŒ–å›æŠ¥çš„æ”¿ç­–:</p><p id="4d77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ğœ‹(ğ‘ )=argmaxğ‘ ğ‘„(ğ‘ ,ğ‘)</p><p id="94ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œæˆ‘ä»¬æ— æ³•è·å¾—å…¨éƒ¨ä¿¡æ¯ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦æƒ³å‡ºè¿‘ä¼¼ğ‘„.çš„æ–¹æ³•ä¸€ç§ä¼ ç»Ÿçš„æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªæŸ¥æ‰¾è¡¨ï¼Œå…¶ä¸­ğ‘„çš„å€¼åœ¨ä»£ç†çš„æ¯ä¸ªåŠ¨ä½œä¹‹åè¢«æ›´æ–°ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¾ˆæ…¢ï¼Œå¹¶ä¸”ä¸èƒ½æ‰©å±•åˆ°å¤§çš„åŠ¨ä½œå’ŒçŠ¶æ€ç©ºé—´ã€‚ç”±äºç¥ç»ç½‘ç»œæ˜¯é€šç”¨å‡½æ•°é€¼è¿‘å™¨ï¼Œæˆ‘å°†è®­ç»ƒä¸€ä¸ªå¯ä»¥é€¼è¿‘ğ‘„.çš„ç½‘ç»œ</p><p id="0b41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">DQL ç±»çš„å®ç°åŒ…æ‹¬ä¸€ä¸ªåœ¨ PyTorch ä¸­å®ç°çš„ç®€å•ç¥ç»ç½‘ç»œï¼Œå®ƒæœ‰ä¸¤ä¸ªä¸»è¦çš„æ–¹æ³•â€”â€”é¢„æµ‹å’Œæ›´æ–°ã€‚ç½‘ç»œå°†ä»£ç†çš„çŠ¶æ€ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æ¯ä¸ªåŠ¨ä½œçš„ğ‘„å€¼ã€‚ä»£ç†é€‰æ‹©æœ€å¤§ğ‘„å€¼æ¥æ‰§è¡Œä¸‹ä¸€ä¸ªæ“ä½œ:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4525" class="lc ld it mg b gy mk ml l mm mn">class DQL():<br/>    ''' Deep Q Neural Network class. '''<br/>    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):<br/>            self.criterion = torch.nn.MSELoss()<br/>            self.model = torch.nn.Sequential(<br/>                            torch.nn.Linear(state_dim, hidden_dim),<br/>                            torch.nn.LeakyReLU(),<br/>                            torch.nn.Linear(hidden_dim, hidden_dim*2),<br/>                            torch.nn.LeakyReLU(),<br/>                            torch.nn.Linear(hidden_dim*2, action_dim)<br/>                    )<br/>            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)</span><span id="ee53" class="lc ld it mg b gy mo ml l mm mn">def update(self, state, y):<br/>        """Update the weights of the network given a training sample. """<br/>        y_pred = self.model(torch.Tensor(state))<br/>        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))<br/>        self.optimizer.zero_grad()<br/>        loss.backward()<br/>        self.optimizer.step()</span><span id="0b64" class="lc ld it mg b gy mo ml l mm mn">def predict(self, state):<br/>        """ Compute Q values for all actions using the DQL. """<br/>        with torch.no_grad():<br/>            return self.model(torch.Tensor(state))</span></pre><p id="ce22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">q_learning å‡½æ•°æ˜¯åé¢æ‰€æœ‰ç®—æ³•çš„ä¸»å¾ªç¯ã€‚<br/>å®ƒæœ‰è®¸å¤šå‚æ•°ï¼Œå³:</p><p id="52ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <code class="fe mw mx my mg b">env</code>ä»£è¡¨æˆ‘ä»¬è¦è§£å†³çš„å¼€æ”¾ Ai å¥èº«æˆ¿ç¯å¢ƒ(CartPoleã€‚)<br/> - <code class="fe mw mx my mg b">episodes</code>ä»£è¡¨æˆ‘ä»¬æƒ³ç©çš„æ¸¸æˆæ•°é‡ã€‚<br/> - <code class="fe mw mx my mg b">gamma</code>æ˜¯ä¸€ä¸ªè´´ç°å› å­ï¼Œä¹˜ä»¥æœªæ¥å¥–åŠ±ï¼Œä»¥æŠ‘åˆ¶è¿™äº›å¥–åŠ±å¯¹ä»£ç†äººçš„å½±å“ã€‚å®ƒçš„ç›®çš„æ˜¯è®©æœªæ¥çš„å¥–åŠ±ä¸å¦‚çœ¼å‰çš„å¥–åŠ±æœ‰ä»·å€¼ã€‚<br/> - <code class="fe mw mx my mg b">epsilon</code>è¡¨ç¤ºéšæœºè¡ŒåŠ¨ç›¸å¯¹äºè¡ŒåŠ¨è€…åœ¨äº‹ä»¶ä¸­ç§¯ç´¯çš„ç°æœ‰â€œçŸ¥è¯†â€æ‰€å‘ŠçŸ¥çš„è¡ŒåŠ¨çš„æ¯”ä¾‹ã€‚è¿™ç§ç­–ç•¥è¢«ç§°ä¸ºâ€œè´ªå©ªæœç´¢ç­–ç•¥â€åœ¨ç©æ¸¸æˆä¹‹å‰ï¼Œä»£ç†æ²¡æœ‰ä»»ä½•ç»éªŒï¼Œå› æ­¤é€šå¸¸ä¼šå°† epsilon è®¾ç½®ä¸ºè¾ƒé«˜çš„å€¼ï¼Œç„¶åé€æ¸é™ä½å…¶å€¼ã€‚<br/> - <code class="fe mw mx my mg b">eps_decay</code>è¡¨ç¤ºä»£ç†å­¦ä¹ æ—¶Îµå‡å°çš„é€Ÿåº¦ã€‚0.99 æ¥è‡ªæœ€åˆçš„ DQN è®ºæ–‡ã€‚</p><p id="617d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ç¨åå½“æˆ‘ä»¬åˆ°è¾¾ç›¸åº”çš„ä»£ç†æ—¶ï¼Œæˆ‘å°†è§£é‡Šå…¶ä»–å‚æ•°ã€‚</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="eb2f" class="lc ld it mg b gy mk ml l mm mn">def q_learning(env, model, episodes, gamma=0.9, <br/>               epsilon=0.3, eps_decay=0.99,<br/>               replay=False, replay_size=20, <br/>               title = 'DQL', double=False, <br/>               n_update=10, soft=False):<br/>    """Deep Q Learning algorithm using the DQN. """<br/>    final = []<br/>    memory = []<br/>    for episode in range(episodes):<br/>        if double and not soft:<br/>            # Update target network every n_update steps<br/>            if episode % n_update == 0:<br/>                model.target_update()<br/>        if double and soft:<br/>            model.target_update()<br/>        <br/>        # Reset state<br/>        state = env.reset()<br/>        done = False<br/>        total = 0<br/>        <br/>        while not done:<br/>            # Implement greedy search policy<br/>            if random.random() &lt; epsilon:<br/>                action = env.action_space.sample()<br/>            else:<br/>                q_values = model.predict(state)<br/>                action = torch.argmax(q_values).item()<br/>            <br/>            # Take action and add reward to total<br/>            next_state, reward, done, _ = env.step(action)<br/>            <br/>            # Update total and memory<br/>            total += reward<br/>            memory.append((state, action, next_state, reward, done))<br/>            q_values = model.predict(state).tolist()<br/>             <br/>            if done:<br/>                if not replay:<br/>                    q_values[action] = reward<br/>                    # Update network weights<br/>                    model.update(state, q_values)<br/>                break</span><span id="fa46" class="lc ld it mg b gy mo ml l mm mn">if replay:<br/>                # Update network weights using replay memory<br/>                model.replay(memory, replay_size, gamma)<br/>            else: <br/>                # Update network weights using the last step only<br/>                q_values_next = model.predict(next_state)<br/>                q_values[action] = reward + gamma *       torch.max(q_values_next).item()<br/>                model.update(state, q_values)</span><span id="fb8c" class="lc ld it mg b gy mo ml l mm mn">state = next_state<br/>        <br/>        # Update epsilon<br/>        epsilon = max(epsilon * eps_decay, 0.01)<br/>        final.append(total)<br/>        plot_res(final, title)<br/>    return final</span></pre><p id="4041" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">æœ€ç›´æ¥çš„ä»£ç†åŸºäºå…¶æœ€è¿‘çš„è§‚å¯Ÿæ›´æ–°å…¶ Q å€¼ã€‚å®ƒæ²¡æœ‰ä»»ä½•è®°å¿†ï¼Œä½†å®ƒé€šè¿‡é¦–å…ˆæ¢ç´¢ç¯å¢ƒï¼Œç„¶åé€æ¸é™ä½å…¶Îµå€¼æ¥åšå‡ºæ˜æ™ºçš„å†³å®šã€‚è®©æˆ‘ä»¬æ¥è¯„ä¼°è¿™æ ·ä¸€ä¸ªä»£ç†çš„æ€§èƒ½:</p><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ng"><img src="../Images/c18589ea9d4b2db9f0abe0207e979afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DDvLqjBCtSbCRr1FsL7fiQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Deep Q Learning</figcaption></figure><p id="8b92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ä¸Šå›¾æ˜¾ç¤ºä»£ç†çš„æ€§èƒ½æœ‰äº†æ˜¾è‘—çš„æé«˜ã€‚å®ƒè¾¾åˆ°äº† 175 æ­¥ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„ï¼Œè¿™å¯¹äºä¸€ä¸ªéšæœºçš„ä»£ç†äººæ¥è¯´æ˜¯ä¸å¯èƒ½çš„ã€‚è¶‹åŠ¿çº¿ä¹Ÿæ˜¯æ­£çš„ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ€§èƒ½éšç€æ—¶é—´çš„æ¨ç§»è€Œå¢åŠ ã€‚åŒæ—¶ï¼Œä»£ç†åœ¨ 150 ä¸ªçºªå…ƒåæ²¡æœ‰æˆåŠŸè¾¾åˆ°ç›®æ ‡çº¿ä»¥ä¸Šï¼Œå…¶å¹³å‡æ€§èƒ½ä»åœ¨ 15 æ­¥å·¦å³ï¼Œå› æ­¤æœ‰è¶³å¤Ÿçš„æ”¹è¿›ç©ºé—´ã€‚</p><h2 id="935b" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">é‡æ”¾è®°å¿†</h2><p id="1713" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">ä¸€æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬çš„ğ‘„è¿‘ä¼¼ä¸æ˜¯å¾ˆæœ‰æ•ˆã€‚ä¸Šé¢çš„å›¾è¡¨å¾ˆå¥½åœ°è¯´æ˜äº†è¿™ä¸€ç‚¹ã€‚ä¸éšæœºä»£ç†ç›¸æ¯”ï¼Œç½‘ç»œè®¾æ³•å®ç°äº†æ›´å¥½çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå®ƒæ— æ³•åˆ°è¾¾ 195 çº§å°é˜¶çš„é—¨æ§›çº¿ã€‚æˆ‘å®ç°äº†ç»éªŒé‡æ”¾ï¼Œä»¥æé«˜ç½‘ç»œç¨³å®šæ€§ï¼Œå¹¶ç¡®ä¿ä»¥å‰çš„ç»éªŒä¸ä¼šè¢«ä¸¢å¼ƒï¼Œè€Œæ˜¯ç”¨äºåŸ¹è®­ã€‚</p><p id="5d6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ä½“éªŒå›æ”¾å°†ä»£ç†çš„ä½“éªŒå­˜å‚¨åœ¨å†…å­˜ä¸­ã€‚æˆæ‰¹çš„ç»éªŒæ˜¯ä»è®°å¿†ä¸­éšæœºæŠ½å–çš„ï¼Œå¹¶ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œã€‚è¿™ç§å­¦ä¹ åŒ…æ‹¬ä¸¤ä¸ªé˜¶æ®µâ€”â€”è·å¾—ç»éªŒå’Œæ›´æ–°æ¨¡å‹ã€‚é‡æ”¾çš„å¤§å°æ§åˆ¶äº†ç”¨äºç½‘ç»œæ›´æ–°çš„ä½“éªŒçš„æ•°é‡ã€‚å†…å­˜æ˜¯ä¸€ä¸ªæ•°ç»„ï¼Œå­˜å‚¨ä»£ç†çš„çŠ¶æ€ã€å¥–åŠ±å’ŒåŠ¨ä½œï¼Œä»¥åŠåŠ¨ä½œæ˜¯å¦å®Œæˆæ¸¸æˆå’Œä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="ed63" class="lc ld it mg b gy mk ml l mm mn"># Expand DQL class with a replay function.<br/>class DQN_replay(DQN):<br/>    def replay(self, memory, size, gamma=0.9):<br/>        """ Add experience replay to the DQN network class. """<br/>        # Make sure the memory is big enough<br/>        if len(memory) &gt;= size:<br/>            states = []<br/>            targets = []<br/>            # Sample a batch of experiences from the agent's memory<br/>            batch = random.sample(memory, size)<br/>            <br/>            # Extract information from the data<br/>            for state, action, next_state, reward, done in batch:<br/>                states.append(state)<br/>                # Predict q_values<br/>                q_values = self.predict(state).tolist()<br/>                if done:<br/>                    q_values[action] = reward<br/>                else:<br/>                    q_values_next = self.predict(next_state)<br/>                    q_values[action] = reward + gamma * torch.max(q_values_next).item()</span><span id="d8cd" class="lc ld it mg b gy mo ml l mm mn">targets.append(q_values)</span><span id="97e0" class="lc ld it mg b gy mo ml l mm mn">self.update(states, targets)</span></pre><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nh"><img src="../Images/645a64dece4038994ca8a062449a26c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pwO3jYClwwX5vHEEpcGGA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">DQL with Replay</figcaption></figure><p id="02dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼Œä¸åªè®°ä½æœ€åä¸€ä¸ªåŠ¨ä½œçš„ç¥ç»ç½‘ç»œç›¸æ¯”ï¼Œå…·æœ‰é‡æ”¾åŠŸèƒ½çš„ç¥ç»ç½‘ç»œä¼¼ä¹æ›´åŠ å¥å£®å’Œæ™ºèƒ½ã€‚å¤§çº¦ 60 é›†ä¹‹åï¼Œä»£ç†äººè®¾æ³•è¾¾åˆ°äº†è·å¥–é—¨æ§›ï¼Œå¹¶ä¿æŒåœ¨è¿™ä¸€æ°´å¹³ã€‚å®ƒè¿˜è®¾æ³•è·å¾—äº†å¯èƒ½çš„æœ€é«˜å¥–åŠ±â€”â€”500 è‹±é•‘ã€‚</p><h2 id="686b" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">åŒé‡æ·±åº¦ Q å­¦ä¹ </h2><p id="a50d" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">ä¼ ç»Ÿçš„æ·±åº¦ Q å­¦ä¹ å¾€å¾€ä¼šé«˜ä¼°å›æŠ¥ï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œè´¨é‡ç­–ç•¥è¾ƒä½ã€‚è®©æˆ‘ä»¬è€ƒè™‘ Q å€¼çš„ç­‰å¼:</p><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ni"><img src="../Images/62eb6cb24053e1ca322d92e6f7408ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylvPuOJ81DZmNH14rcmwuA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">The Bellman Equation. Source: <a class="ae ma" href="https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/" rel="noopener ugc nofollow" target="_blank">Link</a></figcaption></figure><p id="4368" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ç­‰å¼çš„æœ€åä¸€éƒ¨åˆ†æ˜¯å¯¹æœ€å¤§å€¼çš„ä¼°è®¡ã€‚è¿™ä¸€ç¨‹åºå¯¼è‡´ç³»ç»Ÿæ€§é«˜ä¼°ï¼Œä»è€Œå¼•å…¥æœ€å¤§åŒ–åå·®ã€‚ç”±äº Q-learning æ¶‰åŠä»ä¼°è®¡ä¸­å­¦ä¹ ä¼°è®¡ï¼Œè¿™æ ·çš„é«˜ä¼°å°¤å…¶ä»¤äººæ‹…å¿§ã€‚</p><p id="61be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ä¸ºäº†é¿å…è¿™ç§æƒ…å†µï¼Œæˆ‘å°†å®šä¹‰ä¸€ä¸ªæ–°çš„ç›®æ ‡ç½‘ç»œã€‚Q å€¼å°†å–è‡ªè¿™ä¸ªæ–°ç½‘ç»œï¼Œè¿™æ„å‘³ç€åæ˜ ä¸» DQN çš„çŠ¶æ€ã€‚ç„¶è€Œï¼Œå®ƒæ²¡æœ‰ç›¸åŒçš„æƒé‡ï¼Œå› ä¸ºå®ƒåªåœ¨ä¸€å®šæ•°é‡çš„é›†åæ›´æ–°ã€‚è¿™ä¸ªæƒ³æ³•åœ¨<a class="ae ma" href="https://dl.acm.org/citation.cfm?id=3016191" rel="noopener ugc nofollow" target="_blank"> Hasselt et al .ï¼Œ2015 </a>ä¸­é¦–æ¬¡æå‡ºã€‚<br/>æ·»åŠ ç›®æ ‡ç½‘ç»œå¯èƒ½ä¼šé™ä½è®­ç»ƒé€Ÿåº¦ï¼Œå› ä¸ºç›®æ ‡ç½‘ç»œä¸ä¼šæŒç»­æ›´æ–°ã€‚ç„¶è€Œï¼Œéšç€æ—¶é—´çš„æ¨ç§»ï¼Œå®ƒåº”è¯¥å…·æœ‰æ›´ç¨³å¥çš„æ€§èƒ½ã€‚</p><p id="92a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mw mx my mg b">q_learning</code>å¾ªç¯ä¸­çš„<code class="fe mw mx my mg b">n_update</code>æŒ‡å®šæ›´æ–°ç›®æ ‡ç½‘ç»œçš„æ—¶é—´é—´éš”ã€‚</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="282e" class="lc ld it mg b gy mk ml l mm mn">class DQN_double(DQN):<br/>    def __init__(self, state_dim, action_dim, hidden_dim, lr):<br/>        super().__init__(state_dim, action_dim, hidden_dim, lr)<br/>        self.target = copy.deepcopy(self.model)<br/>        <br/>    def target_predict(self, s):<br/>        ''' Use target network to make predicitons.'''<br/>        with torch.no_grad():<br/>            return self.target(torch.Tensor(s))<br/>        <br/>    def target_update(self):<br/>        ''' Update target network with the model weights.'''<br/>        self.target.load_state_dict(self.model.state_dict())<br/>        <br/>    def replay(self, memory, size, gamma=1.0):<br/>        ''' Add experience replay to the DQL network class.'''<br/>        if len(memory) &gt;= size:<br/>            # Sample experiences from the agent's memory<br/>            data = random.sample(memory, size)<br/>            states = []<br/>            targets = []<br/>            # Extract datapoints from the data<br/>            for state, action, next_state, reward, done in data:<br/>                states.append(state)<br/>                q_values = self.predict(state).tolist()<br/>                if done:<br/>                    q_values[action] = reward<br/>                else:<br/>                    # The only difference between the simple replay is in this line<br/>                    # It ensures that next q values are predicted with the target network.<br/>                    q_values_next = self.target_predict(next_state)<br/>                    q_values[action] = reward + gamma * torch.max(q_values_next).item()</span><span id="1067" class="lc ld it mg b gy mo ml l mm mn">targets.append(q_values)</span><span id="cfcf" class="lc ld it mg b gy mo ml l mm mn">self.update(states, targets)</span></pre><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nj"><img src="../Images/5ff9a17d67fb876766e91f1d03cc4cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3MuqL1A_HhSBbWQXqeGyw.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Double DQL with Replay</figcaption></figure><p id="82aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">åŒ DQL é‡æ”¾å·²ç»è¶…è¿‡äº†ä»¥å‰çš„ç‰ˆæœ¬ï¼Œå¹¶ä¸€ç›´æ‰§è¡Œ 300 æ­¥ä»¥ä¸Šã€‚ç”±äºåŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼°çš„åˆ†ç¦»ï¼Œæ€§èƒ½ä¼¼ä¹ä¹Ÿæ›´åŠ ç¨³å®šã€‚æœ€åï¼Œè®©æˆ‘ä»¬æ¢ç´¢ä¸€ä¸‹å¯¹ DQL ä»£ç†çš„æœ€åä¿®æ”¹ã€‚</p><h2 id="6e12" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">è½¯ç›®æ ‡æ›´æ–°</h2><p id="e255" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">ä¸Šé¢å®ç°çš„ç”¨äºæ›´æ–°ç›®æ ‡ç½‘ç»œçš„æ–¹æ³•åœ¨æœ€åˆçš„ DQN è®ºæ–‡ä¸­ä»‹ç»è¿‡ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦ä¸€ç§å®Œå–„çš„æ›´æ–°ç›®æ ‡ç½‘ç»œæƒé‡çš„æ–¹æ³•ã€‚æˆ‘ä»¬å°†åœ¨æ¯æ¬¡è¿è¡Œåä½¿ç”¨ä»¥ä¸‹å…¬å¼é€’å¢åœ°æ›´æ–°ç›®æ ‡ç½‘ç»œï¼Œè€Œä¸æ˜¯åœ¨ä¸€å®šæ•°é‡çš„æ­¥éª¤åæ›´æ–°æƒé‡:</p><p id="c949" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ç›®æ ‡æƒé‡=ç›®æ ‡æƒé‡*(1-Ï„)+æ¨¡å‹æƒé‡*Ï„</p><p id="41d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">å…¶ä¸­ 0 &lt; TAU &lt; 1</p><p id="90ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">This method of updating the target network is known as â€œsoft target network updatesâ€ and was introduced in <a class="ae ma" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank"> Lillicrap ç­‰äººï¼Œ2016 </a>ã€‚è¯¥æ–¹æ³•çš„å®ç°å¦‚ä¸‹æ‰€ç¤º:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="e026" class="lc ld it mg b gy mk ml l mm mn">class DQN_double_soft(DQN_double):<br/>    def target_update(self, TAU=0.1):<br/>        ''' Update the targer gradually. '''<br/>        # Extract parameters  <br/>        model_params = self.model.named_parameters()<br/>        target_params = self.target.named_parameters()<br/>        <br/>        updated_params = dict(target_params)</span><span id="3302" class="lc ld it mg b gy mo ml l mm mn">for model_name, model_param in model_params:<br/>            if model_name in target_params:<br/>                # Update parameter<br/>                updated_params[model_name].data.copy_((TAU)*model_param.data + (1-TAU)*target_params[model_param].data)</span><span id="b2ca" class="lc ld it mg b gy mo ml l mm mn">self.target.load_state_dict(updated_params)</span></pre><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nk"><img src="../Images/17eab5d1f99ab0c2bea99f3cc86eaf0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWB2JaBa-advYnP99uOSGQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">DDQL with Soft Update</figcaption></figure><p id="9fa7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">å…·æœ‰è½¯ç›®æ ‡æ›´æ–°çš„ç½‘ç»œè¡¨ç°ç›¸å½“å¥½ã€‚ä½†æ˜¯ï¼Œå¥½åƒå¹¶ä¸æ¯”ç¡¬æƒæ›´æ–°å¥½ã€‚</p><p id="1354" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">è¿™å¼  gif å›¾å±•ç¤ºäº†ä¸€åè®­ç»ƒæœ‰ç´ çš„ç‰¹å·¥çš„è¡¨ç°:</p><figure class="mb mc md me gt mp"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Trained Agent</figcaption></figure><h2 id="c977" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">ç»“è®º</h2><p id="8051" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">ç»éªŒé‡æ”¾å’Œç›®æ ‡ç½‘ç»œçš„å®ç°æ˜¾è‘—æé«˜äº†å¼€æ”¾äººå·¥æ™ºèƒ½å¹³å°ç¯å¢ƒä¸‹æ·±åº¦ Q å­¦ä¹ ä»£ç†çš„æ€§èƒ½ã€‚å¯¹ä»£ç†çš„ä¸€äº›å…¶ä»–ä¿®æ”¹ï¼Œå¦‚å†³æ–—ç½‘ç»œæ¶æ„(<a class="ae ma" href="https://arxiv.org/pdf/1511.06581.pdf" rel="noopener ugc nofollow" target="_blank">ç‹ç­‰ï¼Œ2015 </a>)ï¼Œå¯ä»¥æ·»åŠ åˆ°è¯¥å®ç°ä¸­ï¼Œä»¥æé«˜ä»£ç†çš„æ€§èƒ½ã€‚è¯¥ç®—æ³•ä¹Ÿå¯æ¨å¹¿åˆ°å…¶ä»–ç¯å¢ƒã€‚è¯·éšæ„æµ‹è¯•å®ƒè§£å†³å…¶ä»–ä»»åŠ¡çš„èƒ½åŠ›ï¼</p><p id="4573" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ç¬”è®°æœ¬é“¾æ¥:<a class="ae ma" href="https://github.com/ritakurban/Practical-Data-Science/blob/master/DQL_CartPole.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/ritakurban/Practical-Data-Science/blob/master/DQL _ å¡ç‰¹æ³¢å°”. ipynb </a></p><h2 id="be6d" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">å‚è€ƒ</h2><p id="3955" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">(1)å¼ºåŒ– Qâ€”â€”ç”¨ OpenAI Gym åœ¨ Python ä¸­ä»å¤´å­¦ä¹ ã€‚(2019).Learndatasci.comã€‚æ£€ç´¢äº 2019 å¹´ 12 æœˆ 9 æ—¥ï¼Œæ¥è‡ª<a class="ae ma" href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/" rel="noopener ugc nofollow" target="_blank">https://www . learn data sci . com/tutorials/reinforcement-q-learning-scratch-python-open ai-gym/</a></p><p id="5201" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(2)å¸•å…¹å…‹ï¼Œa .(2019)ã€‚å¼ºåŒ–å­¦ä¹ (DQN)æ•™ç¨‹ã€‚æ£€ç´¢è‡ª:<a class="ae ma" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/intermediate/reinforcement _ q _ learning . html</a></p><p id="4a8f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(3) Lillicrapï¼ŒT. P .ï¼ŒHuntï¼ŒJ. J .ï¼ŒPritzelï¼Œa .ï¼ŒHeessï¼Œn .ï¼ŒErezï¼Œt .ï¼ŒTassaï¼Œy .ï¼Œâ€¦ &amp; Wierstraï¼ŒD. (2015)ã€‚æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„è¿ç»­æ§åˆ¶ã€‚arXiv é¢„å°æœ¬ arXiv:1509.02971ã€‚</p><p id="c31f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(4)èŒƒÂ·å“ˆç‘Ÿå°”ç‰¹(2016 å¹´ 3 æœˆå‡ºç‰ˆ)ã€‚åŒ q å­¦ä¹ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€‚åœ¨ç¬¬ä¸‰åå±Š AAAI äººå·¥æ™ºèƒ½ä¼šè®®ä¸Šã€‚</p><p id="e6bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(5)ç‹ï¼Œz .ï¼Œç»å°”ï¼Œt .ï¼Œèµ«å¡å°”ï¼Œm .ï¼ŒèŒƒå“ˆç‘Ÿå°”ç‰¹ï¼Œh .ï¼Œå…°æ‰˜ç‰¹ï¼Œm .ï¼Œ&amp;å¾·å¼—è±å¡”æ–¯ï¼ŒN. (2015)ã€‚ç”¨äºæ·±åº¦å¼ºåŒ–å­¦ä¹ çš„å†³æ–—ç½‘ç»œæ¶æ„ã€‚arXiv é¢„å°æœ¬ arXiv:1511.06581ã€‚</p><p id="ac6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(6)åŒ DQN å®ç°è§£å†³ OpenAI å¥èº«æˆ¿çš„æ¨ªæ’‘ v-0ã€‚(2019).ä¸­ç­‰ã€‚æ£€ç´¢äº 2019 å¹´ 12 æœˆ 20 æ—¥ï¼Œæ¥è‡ª<a class="ae ma" href="https://medium.com/@leosimmons/double-dqn-implementation-to-solve-openai-gyms-cartpole-v-0-df554cd0614d" rel="noopener">https://medium . com/@ Leo Simmons/double-dqn-implementation-to-solve-open ai-gyms-cart pole-v-0-df 554 CD 0614d</a></p></div></div>    
</body>
</html>