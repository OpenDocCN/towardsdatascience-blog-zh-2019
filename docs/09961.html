<html>
<head>
<title>Deep Q Learning for the CartPole</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于横竿的深度 Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f?source=collection_archive---------5-----------------------#2019-12-30">https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f?source=collection_archive---------5-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b4a8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的目的是介绍深度 Q 学习的概念，并用它来解决 OpenAI 健身房的 CartPole 环境。</p><p id="78c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该员额将由以下部分组成:</p><ol class=""><li id="7451" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">开放式人工智能健身房环境介绍</li><li id="5540" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">随机基线策略</li><li id="ea8e" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">深度 Q 学习</li><li id="cb59" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">具有重放记忆的深度 Q 学习</li><li id="de08" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">双重深度 Q 学习</li><li id="c67e" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">软更新</li></ol><h2 id="9eb9" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">环境</h2><p id="7309" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated"><a class="ae ma" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">横拉杆环境</a>由一根沿无摩擦轨道移动的杆子组成。通过对推车施加+1 或-1 的力来控制该系统。钟摆开始直立，目标是防止它翻倒。状态空间由四个值表示:小车位置、小车速度、磁极角度和磁极尖端的速度。动作空间由两个动作组成:向左移动或向右移动。杆保持直立的每个时间步长提供+1 的奖励。当柱子偏离垂直方向超过 15 度，或者手推车偏离中心超过 2.4 个单位时，该集结束。</p><p id="9bf0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的单元格绘制了环境中的一组示例帧:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="50c1" class="lc ld it mg b gy mk ml l mm mn"># Demonstration<br/>env = gym.envs.make("CartPole-v1")</span><span id="cbfa" class="lc ld it mg b gy mo ml l mm mn">def get_screen():<br/>    ''' Extract one step of the simulation.'''<br/>    screen = env.render(mode='rgb_array').transpose((2, 0, 1))<br/>    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255.<br/>    return torch.from_numpy(screen)</span><span id="21ad" class="lc ld it mg b gy mo ml l mm mn"># Speify the number of simulation steps<br/>num_steps = 2</span><span id="4236" class="lc ld it mg b gy mo ml l mm mn"># Show several steps<br/>for i in range(num_steps):<br/>    clear_output(wait=True)<br/>    env.reset()<br/>    plt.figure()<br/>    plt.imshow(get_screen().cpu().permute(1, 2, 0).numpy(),<br/>               interpolation='none')<br/>    plt.title('CartPole-v0 Environment')<br/>    plt.xticks([])<br/>    plt.yticks([])<br/>    plt.show()</span></pre><p id="1318" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据剧集的数量，输出如下所示:</p><figure class="mb mc md me gt mp"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Untrained Agent</figcaption></figure><p id="4737" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们所看到的，代理还没有经过训练，所以它只能走几步。我们将很快探讨一些能显著提高性能的策略。但是首先，让我们定义绘图函数，它将帮助我们分析结果:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="2402" class="lc ld it mg b gy mk ml l mm mn">def plot_res(values, title=''):   <br/>    ''' Plot the reward curve and histogram of results over time.'''<br/>    # Update the window after each episode<br/>    clear_output(wait=True)<br/>    <br/>    # Define the figure<br/>    f, ax = plt.subplots(nrows=1, ncols=2, figsize=(12,5))<br/>    f.suptitle(title)<br/>    ax[0].plot(values, label='score per run')<br/>    ax[0].axhline(195, c='red',ls='--', label='goal')<br/>    ax[0].set_xlabel('Episodes')<br/>    ax[0].set_ylabel('Reward')<br/>    x = range(len(values))<br/>    ax[0].legend()<br/>    # Calculate the trend<br/>    try:<br/>        z = np.polyfit(x, values, 1)<br/>        p = np.poly1d(z)<br/>        ax[0].plot(x,p(x),"--", label='trend')<br/>    except:<br/>        print('')<br/>    <br/>    # Plot the histogram of results<br/>    ax[1].hist(values[-50:])<br/>    ax[1].axvline(195, c='red', label='goal')<br/>    ax[1].set_xlabel('Scores per Last 50 Episodes')<br/>    ax[1].set_ylabel('Frequency')<br/>    ax[1].legend()<br/>    plt.show()</span></pre><p id="4daa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最终的剧情由两个支线剧情组成。第一个图显示了代理在一段时间内累积的总奖励，而另一个图显示了代理在过去 50 集的总奖励的直方图。当我们分析我们的策略时，我们会看到一些图表。</p><h2 id="e656" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">基线随机模型</h2><p id="c955" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">在实现任何深度学习方法之前，我写了一个简单的策略，其中动作是从动作空间中随机采样的。这种方法将作为其他策略的基线，并使其更容易理解如何使用开放的人工智能健身房环境与代理合作。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="ddbe" class="lc ld it mg b gy mk ml l mm mn">def random_search(env, episodes, <br/>                  title='Random Strategy'):<br/>    """ Random search strategy implementation."""<br/>    final = []<br/>    for episode in range(episodes):<br/>        state = env.reset()<br/>        done = False<br/>        total = 0<br/>        while not done:<br/>            # Sample random actions<br/>            action = env.action_space.sample()<br/>            # Take action and extract results<br/>            next_state, reward, done, _ = env.step(action)<br/>            # Update reward<br/>            total += reward<br/>            if done:<br/>                break<br/>        # Add to the final reward<br/>        final.append(total)<br/>        plot_res(final,title)<br/>    return final</span></pre><p id="7e30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个环境步骤返回几个值，比如<code class="fe mw mx my mg b">next_state</code>、<code class="fe mw mx my mg b">reward</code>，以及模拟是否为<code class="fe mw mx my mg b">done</code>。下图显示了 150 集(模拟运行)的总累积奖励:</p><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi mz"><img src="../Images/373d5740044f10952a9d1e22d303931f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iTDwkwdu6nYRsPfOR6Pa6Q.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Random Strategy</figcaption></figure><p id="bc23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的图展示了随机策略。不出所料，用这种方法解决环境问题是不可能的。代理没有从他们的经验中学习。尽管有时很幸运(获得差不多 75 的奖励)，但他们的平均表现低至 10 步。</p><h2 id="0995" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">深度 Q 学习</h2><p id="7e8e" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">Q-learning 背后的主要思想是我们有一个函数𝑄:𝑆𝑡𝑎𝑡𝑒×𝐴𝑐𝑡𝑖𝑜𝑛→ℝ，它可以告诉代理什么行为会导致什么回报。如果我们知道𝑄的价值，我们就有可能制定一个最大化回报的政策:</p><p id="4d77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">𝜋(𝑠)=argmax𝑎 𝑄(𝑠,𝑎)</p><p id="94ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，在现实世界中，我们无法获得全部信息，这就是为什么我们需要想出近似𝑄.的方法一种传统的方法是创建一个查找表，其中𝑄的值在代理的每个动作之后被更新。然而，这种方法很慢，并且不能扩展到大的动作和状态空间。由于神经网络是通用函数逼近器，我将训练一个可以逼近𝑄.的网络</p><p id="0b41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">DQL 类的实现包括一个在 PyTorch 中实现的简单神经网络，它有两个主要的方法——预测和更新。网络将代理的状态作为输入，并返回每个动作的𝑄值。代理选择最大𝑄值来执行下一个操作:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="4525" class="lc ld it mg b gy mk ml l mm mn">class DQL():<br/>    ''' Deep Q Neural Network class. '''<br/>    def __init__(self, state_dim, action_dim, hidden_dim=64, lr=0.05):<br/>            self.criterion = torch.nn.MSELoss()<br/>            self.model = torch.nn.Sequential(<br/>                            torch.nn.Linear(state_dim, hidden_dim),<br/>                            torch.nn.LeakyReLU(),<br/>                            torch.nn.Linear(hidden_dim, hidden_dim*2),<br/>                            torch.nn.LeakyReLU(),<br/>                            torch.nn.Linear(hidden_dim*2, action_dim)<br/>                    )<br/>            self.optimizer = torch.optim.Adam(self.model.parameters(), lr)</span><span id="ee53" class="lc ld it mg b gy mo ml l mm mn">def update(self, state, y):<br/>        """Update the weights of the network given a training sample. """<br/>        y_pred = self.model(torch.Tensor(state))<br/>        loss = self.criterion(y_pred, Variable(torch.Tensor(y)))<br/>        self.optimizer.zero_grad()<br/>        loss.backward()<br/>        self.optimizer.step()</span><span id="0b64" class="lc ld it mg b gy mo ml l mm mn">def predict(self, state):<br/>        """ Compute Q values for all actions using the DQL. """<br/>        with torch.no_grad():<br/>            return self.model(torch.Tensor(state))</span></pre><p id="ce22" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">q_learning 函数是后面所有算法的主循环。<br/>它有许多参数，即:</p><p id="52ef" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">- <code class="fe mw mx my mg b">env</code>代表我们要解决的开放 Ai 健身房环境(CartPole。)<br/> - <code class="fe mw mx my mg b">episodes</code>代表我们想玩的游戏数量。<br/> - <code class="fe mw mx my mg b">gamma</code>是一个贴现因子，乘以未来奖励，以抑制这些奖励对代理人的影响。它的目的是让未来的奖励不如眼前的奖励有价值。<br/> - <code class="fe mw mx my mg b">epsilon</code>表示随机行动相对于行动者在事件中积累的现有“知识”所告知的行动的比例。这种策略被称为“贪婪搜索策略”在玩游戏之前，代理没有任何经验，因此通常会将 epsilon 设置为较高的值，然后逐渐降低其值。<br/> - <code class="fe mw mx my mg b">eps_decay</code>表示代理学习时ε减小的速度。0.99 来自最初的 DQN 论文。</p><p id="617d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">稍后当我们到达相应的代理时，我将解释其他参数。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="eb2f" class="lc ld it mg b gy mk ml l mm mn">def q_learning(env, model, episodes, gamma=0.9, <br/>               epsilon=0.3, eps_decay=0.99,<br/>               replay=False, replay_size=20, <br/>               title = 'DQL', double=False, <br/>               n_update=10, soft=False):<br/>    """Deep Q Learning algorithm using the DQN. """<br/>    final = []<br/>    memory = []<br/>    for episode in range(episodes):<br/>        if double and not soft:<br/>            # Update target network every n_update steps<br/>            if episode % n_update == 0:<br/>                model.target_update()<br/>        if double and soft:<br/>            model.target_update()<br/>        <br/>        # Reset state<br/>        state = env.reset()<br/>        done = False<br/>        total = 0<br/>        <br/>        while not done:<br/>            # Implement greedy search policy<br/>            if random.random() &lt; epsilon:<br/>                action = env.action_space.sample()<br/>            else:<br/>                q_values = model.predict(state)<br/>                action = torch.argmax(q_values).item()<br/>            <br/>            # Take action and add reward to total<br/>            next_state, reward, done, _ = env.step(action)<br/>            <br/>            # Update total and memory<br/>            total += reward<br/>            memory.append((state, action, next_state, reward, done))<br/>            q_values = model.predict(state).tolist()<br/>             <br/>            if done:<br/>                if not replay:<br/>                    q_values[action] = reward<br/>                    # Update network weights<br/>                    model.update(state, q_values)<br/>                break</span><span id="fa46" class="lc ld it mg b gy mo ml l mm mn">if replay:<br/>                # Update network weights using replay memory<br/>                model.replay(memory, replay_size, gamma)<br/>            else: <br/>                # Update network weights using the last step only<br/>                q_values_next = model.predict(next_state)<br/>                q_values[action] = reward + gamma *       torch.max(q_values_next).item()<br/>                model.update(state, q_values)</span><span id="fb8c" class="lc ld it mg b gy mo ml l mm mn">state = next_state<br/>        <br/>        # Update epsilon<br/>        epsilon = max(epsilon * eps_decay, 0.01)<br/>        final.append(total)<br/>        plot_res(final, title)<br/>    return final</span></pre><p id="4041" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最直接的代理基于其最近的观察更新其 Q 值。它没有任何记忆，但它通过首先探索环境，然后逐渐降低其ε值来做出明智的决定。让我们来评估这样一个代理的性能:</p><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ng"><img src="../Images/c18589ea9d4b2db9f0abe0207e979afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DDvLqjBCtSbCRr1FsL7fiQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Deep Q Learning</figcaption></figure><p id="8b92" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图显示代理的性能有了显著的提高。它达到了 175 步，正如我们之前看到的，这对于一个随机的代理人来说是不可能的。趋势线也是正的，我们可以看到性能随着时间的推移而增加。同时，代理在 150 个纪元后没有成功达到目标线以上，其平均性能仍在 15 步左右，因此有足够的改进空间。</p><h2 id="935b" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">重放记忆</h2><p id="1713" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">一次使用一个样本的𝑄近似不是很有效。上面的图表很好地说明了这一点。与随机代理相比，网络设法实现了更好的性能。然而，它无法到达 195 级台阶的门槛线。我实现了经验重放，以提高网络稳定性，并确保以前的经验不会被丢弃，而是用于培训。</p><p id="5d6b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">体验回放将代理的体验存储在内存中。成批的经验是从记忆中随机抽取的，并用于训练神经网络。这种学习包括两个阶段——获得经验和更新模型。重放的大小控制了用于网络更新的体验的数量。内存是一个数组，存储代理的状态、奖励和动作，以及动作是否完成游戏和下一个状态。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="ed63" class="lc ld it mg b gy mk ml l mm mn"># Expand DQL class with a replay function.<br/>class DQN_replay(DQN):<br/>    def replay(self, memory, size, gamma=0.9):<br/>        """ Add experience replay to the DQN network class. """<br/>        # Make sure the memory is big enough<br/>        if len(memory) &gt;= size:<br/>            states = []<br/>            targets = []<br/>            # Sample a batch of experiences from the agent's memory<br/>            batch = random.sample(memory, size)<br/>            <br/>            # Extract information from the data<br/>            for state, action, next_state, reward, done in batch:<br/>                states.append(state)<br/>                # Predict q_values<br/>                q_values = self.predict(state).tolist()<br/>                if done:<br/>                    q_values[action] = reward<br/>                else:<br/>                    q_values_next = self.predict(next_state)<br/>                    q_values[action] = reward + gamma * torch.max(q_values_next).item()</span><span id="d8cd" class="lc ld it mg b gy mo ml l mm mn">targets.append(q_values)</span><span id="97e0" class="lc ld it mg b gy mo ml l mm mn">self.update(states, targets)</span></pre><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nh"><img src="../Images/645a64dece4038994ca8a062449a26c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pwO3jYClwwX5vHEEpcGGA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">DQL with Replay</figcaption></figure><p id="02dd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如预期的那样，与只记住最后一个动作的神经网络相比，具有重放功能的神经网络似乎更加健壮和智能。大约 60 集之后，代理人设法达到了获奖门槛，并保持在这一水平。它还设法获得了可能的最高奖励——500 英镑。</p><h2 id="686b" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">双重深度 Q 学习</h2><p id="a50d" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">传统的深度 Q 学习往往会高估回报，导致训练不稳定，质量策略较低。让我们考虑 Q 值的等式:</p><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi ni"><img src="../Images/62eb6cb24053e1ca322d92e6f7408ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylvPuOJ81DZmNH14rcmwuA.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">The Bellman Equation. Source: <a class="ae ma" href="https://www.freecodecamp.org/news/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc/" rel="noopener ugc nofollow" target="_blank">Link</a></figcaption></figure><p id="4368" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">等式的最后一部分是对最大值的估计。这一程序导致系统性高估，从而引入最大化偏差。由于 Q-learning 涉及从估计中学习估计，这样的高估尤其令人担忧。</p><p id="61be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了避免这种情况，我将定义一个新的目标网络。Q 值将取自这个新网络，这意味着反映主 DQN 的状态。然而，它没有相同的权重，因为它只在一定数量的集后更新。这个想法在<a class="ae ma" href="https://dl.acm.org/citation.cfm?id=3016191" rel="noopener ugc nofollow" target="_blank"> Hasselt et al .，2015 </a>中首次提出。<br/>添加目标网络可能会降低训练速度，因为目标网络不会持续更新。然而，随着时间的推移，它应该具有更稳健的性能。</p><p id="92a9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe mw mx my mg b">q_learning</code>循环中的<code class="fe mw mx my mg b">n_update</code>指定更新目标网络的时间间隔。</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="282e" class="lc ld it mg b gy mk ml l mm mn">class DQN_double(DQN):<br/>    def __init__(self, state_dim, action_dim, hidden_dim, lr):<br/>        super().__init__(state_dim, action_dim, hidden_dim, lr)<br/>        self.target = copy.deepcopy(self.model)<br/>        <br/>    def target_predict(self, s):<br/>        ''' Use target network to make predicitons.'''<br/>        with torch.no_grad():<br/>            return self.target(torch.Tensor(s))<br/>        <br/>    def target_update(self):<br/>        ''' Update target network with the model weights.'''<br/>        self.target.load_state_dict(self.model.state_dict())<br/>        <br/>    def replay(self, memory, size, gamma=1.0):<br/>        ''' Add experience replay to the DQL network class.'''<br/>        if len(memory) &gt;= size:<br/>            # Sample experiences from the agent's memory<br/>            data = random.sample(memory, size)<br/>            states = []<br/>            targets = []<br/>            # Extract datapoints from the data<br/>            for state, action, next_state, reward, done in data:<br/>                states.append(state)<br/>                q_values = self.predict(state).tolist()<br/>                if done:<br/>                    q_values[action] = reward<br/>                else:<br/>                    # The only difference between the simple replay is in this line<br/>                    # It ensures that next q values are predicted with the target network.<br/>                    q_values_next = self.target_predict(next_state)<br/>                    q_values[action] = reward + gamma * torch.max(q_values_next).item()</span><span id="1067" class="lc ld it mg b gy mo ml l mm mn">targets.append(q_values)</span><span id="cfcf" class="lc ld it mg b gy mo ml l mm mn">self.update(states, targets)</span></pre><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nj"><img src="../Images/5ff9a17d67fb876766e91f1d03cc4cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3MuqL1A_HhSBbWQXqeGyw.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Double DQL with Replay</figcaption></figure><p id="82aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">双 DQL 重放已经超过了以前的版本，并一直执行 300 步以上。由于动作选择和评估的分离，性能似乎也更加稳定。最后，让我们探索一下对 DQL 代理的最后修改。</p><h2 id="6e12" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">软目标更新</h2><p id="e255" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">上面实现的用于更新目标网络的方法在最初的 DQN 论文中介绍过。在本节中，我们将探索另一种完善的更新目标网络权重的方法。我们将在每次运行后使用以下公式递增地更新目标网络，而不是在一定数量的步骤后更新权重:</p><p id="c949" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">目标权重=目标权重*(1-τ)+模型权重*τ</p><p id="41d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 0 &lt; TAU &lt; 1</p><p id="90ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">This method of updating the target network is known as “soft target network updates” and was introduced in <a class="ae ma" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank"> Lillicrap 等人，2016 </a>。该方法的实现如下所示:</p><pre class="mb mc md me gt mf mg mh mi aw mj bi"><span id="e026" class="lc ld it mg b gy mk ml l mm mn">class DQN_double_soft(DQN_double):<br/>    def target_update(self, TAU=0.1):<br/>        ''' Update the targer gradually. '''<br/>        # Extract parameters  <br/>        model_params = self.model.named_parameters()<br/>        target_params = self.target.named_parameters()<br/>        <br/>        updated_params = dict(target_params)</span><span id="3302" class="lc ld it mg b gy mo ml l mm mn">for model_name, model_param in model_params:<br/>            if model_name in target_params:<br/>                # Update parameter<br/>                updated_params[model_name].data.copy_((TAU)*model_param.data + (1-TAU)*target_params[model_param].data)</span><span id="b2ca" class="lc ld it mg b gy mo ml l mm mn">self.target.load_state_dict(updated_params)</span></pre><figure class="mb mc md me gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="na nb di nc bf nd"><div class="gh gi nk"><img src="../Images/17eab5d1f99ab0c2bea99f3cc86eaf0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWB2JaBa-advYnP99uOSGQ.png"/></div></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">DDQL with Soft Update</figcaption></figure><p id="9fa7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">具有软目标更新的网络表现相当好。但是，好像并不比硬权更新好。</p><p id="1354" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这张 gif 图展示了一名训练有素的特工的表现:</p><figure class="mb mc md me gt mp"><div class="bz fp l di"><div class="mq mr l"/></div><figcaption class="ms mt gj gh gi mu mv bd b be z dk">Trained Agent</figcaption></figure><h2 id="c977" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">结论</h2><p id="8051" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">经验重放和目标网络的实现显著提高了开放人工智能平台环境下深度 Q 学习代理的性能。对代理的一些其他修改，如决斗网络架构(<a class="ae ma" href="https://arxiv.org/pdf/1511.06581.pdf" rel="noopener ugc nofollow" target="_blank">王等，2015 </a>)，可以添加到该实现中，以提高代理的性能。该算法也可推广到其他环境。请随意测试它解决其他任务的能力！</p><p id="4573" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">笔记本链接:<a class="ae ma" href="https://github.com/ritakurban/Practical-Data-Science/blob/master/DQL_CartPole.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/ritakurban/Practical-Data-Science/blob/master/DQL _ 卡特波尔. ipynb </a></p><h2 id="be6d" class="lc ld it bd le lf lg dn lh li lj dp lk kb ll lm ln kf lo lp lq kj lr ls lt lu bi translated">参考</h2><p id="3955" class="pw-post-body-paragraph jq jr it js b jt lv jv jw jx lw jz ka kb lx kd ke kf ly kh ki kj lz kl km kn im bi translated">(1)强化 Q——用 OpenAI Gym 在 Python 中从头学习。(2019).Learndatasci.com。检索于 2019 年 12 月 9 日，来自<a class="ae ma" href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/" rel="noopener ugc nofollow" target="_blank">https://www . learn data sci . com/tutorials/reinforcement-q-learning-scratch-python-open ai-gym/</a></p><p id="5201" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(2)帕兹克，a .(2019)。强化学习(DQN)教程。检索自:<a class="ae ma" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/intermediate/reinforcement _ q _ learning . html</a></p><p id="4a8f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(3) Lillicrap，T. P .，Hunt，J. J .，Pritzel，a .，Heess，n .，Erez，t .，Tassa，y .，… &amp; Wierstra，D. (2015)。深度强化学习的连续控制。arXiv 预印本 arXiv:1509.02971。</p><p id="c31f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(4)范·哈瑟尔特(2016 年 3 月出版)。双 q 学习的深度强化学习。在第三十届 AAAI 人工智能会议上。</p><p id="e6bb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(5)王，z .，绍尔，t .，赫塞尔，m .，范哈瑟尔特，h .，兰托特，m .，&amp;德弗莱塔斯，N. (2015)。用于深度强化学习的决斗网络架构。arXiv 预印本 arXiv:1511.06581。</p><p id="ac6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(6)双 DQN 实现解决 OpenAI 健身房的横撑 v-0。(2019).中等。检索于 2019 年 12 月 20 日，来自<a class="ae ma" href="https://medium.com/@leosimmons/double-dqn-implementation-to-solve-openai-gyms-cartpole-v-0-df554cd0614d" rel="noopener">https://medium . com/@ Leo Simmons/double-dqn-implementation-to-solve-open ai-gyms-cart pole-v-0-df 554 CD 0614d</a></p></div></div>    
</body>
</html>