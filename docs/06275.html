<html>
<head>
<title>Everything You Need To Know About Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于线性回归你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-about-linear-regression-b791e8f4bd7a?source=collection_archive---------4-----------------------#2019-09-10">https://towardsdatascience.com/everything-you-need-to-know-about-linear-regression-b791e8f4bd7a?source=collection_archive---------4-----------------------#2019-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="a97a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线性回归是机器学习领域的第一块敲门砖。如果你是机器学习新手或数学极客，想知道线性回归背后的所有数学知识，那么你和我 9 个月前的情况一样。在这里，我们将看看线性回归的数学，并了解其背后的机制。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/7f8a2966cb3549675bc80ae72fb3b1ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*QiU6DcP_r9qWLznMw0-M_Q.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Linear Regression (Source: <a class="ae la" href="https://datumguy.com/blog/blog/view/5ce138213122e?utm_content=buffer09809&amp;utm_medium=social&amp;utm_source=facebook.com&amp;utm_campaign=buffer" rel="noopener ugc nofollow" target="_blank">https://datumguy.com/blog/blog/view/5ce138213122e?utm_content=buffer09809&amp;utm_medium=social&amp;utm_source=facebook.com&amp;utm_campaign=buffer</a>)</figcaption></figure><h2 id="2327" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">介绍</h2><p id="aea5" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">线性回归。分解之后，我们得到两个词‘线性’和‘回归’。当我们从数学角度思考时,“线性”一词似乎与直线有关，而“回归”一词意味着<em class="lz">一种确定两个或多个变量之间统计关系的技术</em>。</p><p id="90c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简单地说，线性回归就是找到一条几乎与给定数据相符合的直线的方程，这样它就可以预测未来的值。</p><h2 id="cc58" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">假设</h2><p id="2a42" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">这个假设是什么？这只不过是我们正在谈论的直线方程。让我们看看下面的等式。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/c33ff0203bc2a68a1dc9a56e70fe9202.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*hFg5W_eV5b-qyyfDb9HbUw.png"/></div></figure><p id="0d9d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你觉得这个眼熟吗？这是一条直线的方程式。这是一个假设。让我们用类似的方式重写一下。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/6dbbf764dbd28dcb83e5b80df327ed5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*-0_jhjes5pxxr8Dz3MNxKg.png"/></div></figure><p id="c557" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们刚刚分别将<em class="lz"> y </em>替换为<em class="lz"> h(x) </em>和<em class="lz"> c </em>，将<em class="lz"> m </em>替换为<em class="lz">θ₀</em>和<em class="lz">θ₁</em>。<em class="lz"> h(x) </em>将是我们的预测值。这是机器学习中最常见的写假设的方式。</p><p id="1b6d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了理解这个假设，我们以房价为例。假设您收集了您所在地区不同房屋的大小及其各自的价格。该假设可以表示为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/8ec616946badcdcff67c5f8534f46b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*Y2-jjNyPe4maxKETrfh6_g.png"/></div></figure><p id="6b95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在你所要做的就是根据你的数据集找到合适的基价和θ₁的价值，这样你就可以在给定房子大小的情况下预测它的价格。</p><p id="4b3a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更专业地说，我们必须调整<em class="lz">θ₀</em>&amp;<em class="lz">θ₁</em>的值，以使我们的线尽可能地适合数据集。现在我们需要一些度量来确定“最佳”线，我们已经有了。这叫做成本函数。让我们调查一下。</p><h2 id="bca1" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">成本函数 J(θ)</h2><p id="5534" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">线性回归的成本函数为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi md"><img src="../Images/0c3f8f0e04aeae0b72c83b6ea1bd0511.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*Ufonqa20VceIYU0i2UfMnQ.png"/></div></figure><p id="d970" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了让它对我们的大脑来说看起来更漂亮，我们可以把它重写为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi me"><img src="../Images/87fb2b77daea65ec745df496e4696040.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*mDEzcRicu3Oc2rrvB7A-uw.png"/></div></figure><p id="c225" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的<em class="lz"> m </em>表示数据集中的样本总数。在我们的例子中，<em class="lz"> m </em>将是我们数据集中房屋的总数。</p><p id="a59f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在仔细看看我们的成本函数，我们需要所有<em class="lz"> m </em>个例子的<em class="lz">预测值</em>，即<em class="lz"> h(x) </em>。让我们再看看我们的<em class="lz">预测值</em>或<em class="lz">预测价格</em>是什么样子的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/8ec616946badcdcff67c5f8534f46b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*Y2-jjNyPe4maxKETrfh6_g.png"/></div></figure><p id="b27a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了计算我们的成本函数，我们需要的是所有<em class="lz"> m </em>个例子的<em class="lz"> h(x) </em>，即<em class="lz"> m 个预测价格</em>对应于<em class="lz"> m </em>个房屋。</p><p id="a898" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，为了计算<em class="lz"> h(x) </em>，我们需要一个<em class="lz">底价</em>和<em class="lz">θ₁.的价值</em>请注意，这些是我们将调整以找到最合适的值。我们需要一些东西来开始，所以我们将随机初始化这两个值。</p><h2 id="bd85" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">成本函数的解释</h2><p id="cdf5" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">如果你仔细观察成本函数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi me"><img src="../Images/87fb2b77daea65ec745df496e4696040.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*mDEzcRicu3Oc2rrvB7A-uw.png"/></div></figure><p id="8add" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你会发现我们所做的只是在所有的<em class="lz"> m </em>个例子中平均预测值和实际值之间的距离的平方。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/3bae4d17db5db2dfb44d962b27da88e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*dnFljsilN6-wpo0orHg3Bw.png"/></div></figure><p id="fdbf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看上图，这里<em class="lz"> m = 4。</em>蓝色线上的点是预测值，红色点是实际值。绿线是实际值和预测值之间的距离。</p><p id="dcac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这条线路的费用是</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/c66cf4ee703c89702ddce61adcb40039.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*zaDOKX7g8nI6NW_wyHexkQ.png"/></div></figure><p id="e67e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以成本函数计算的只是绿线长度平方的平均值。我们还将它除以 2，以简化我们将看到的一些未来的计算。</p><p id="c11c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">线性回归试图通过找到θ₀和θ₁.的合适值来最小化这个成本如何？通过使用梯度下降。</p><h2 id="5cc3" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">梯度下降</h2><p id="1f5a" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">对于机器学习来说，梯度下降是一种非常重要的算法。从线性回归到神经网络，它无处不在。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/37c647634b61eaf4609a767b21399014.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*3qlkq6_wWNgno2g162udzQ.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f85b85206431f600adf4d497dd395541.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*YEGHkUxBENGkBswPQcF91w.png"/></div></figure><p id="2d4e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是我们更新权重的方法。这个更新规则是在一个循环中执行的&amp;它帮助我们达到成本函数的最小值。<em class="lz"> α </em>是一个恒定的学习速率，我们一会儿会谈到它。</p><h2 id="448d" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated"><span class="l mi mj mk bm ml mm mn mo mp di"> U </span>理解梯度下降</h2><p id="c4a4" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">所以基本上我们是在更新我们的权重，用我们的成本函数对权重的偏导数减去它。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/89c1c97a70d6e7f9a1813a86d08d158b.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*qC0s5MMe_LY4TFEyAnsGtQ.png"/></div></figure><p id="9a13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是这怎么能把我们带到最小的成本呢？让我们想象一下。为了便于理解，我们假设<em class="lz">θ₀</em>现在为 0。</p><p id="4601" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以假设变成了</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/43713eb80306a835d03c8433b75cdd21.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*DRG2hyI6bCTWVNJev9wqZA.png"/></div></figure><p id="01d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和成本函数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/782859ad342b7ada8f14aa0e7c8c93c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*iCfUr2NCQnd-1Pp2thwViA.png"/></div></figure><p id="d33d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们看看成本如何依赖于θ₁.的价值因为这是一个二次方程，所以<em class="lz">θ₁</em>对 j(θ)<em class="lz"/>的图形将是一条抛物线，并且看起来像这样，其中<em class="lz">θ₁</em>在<em class="lz"> x 轴</em>上，<em class="lz">j(</em>θ<em class="lz">)在</em>y 轴<em class="lz">上。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/352076ec4e12b74a284ce26862ac4e73.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*bKvOVKxuEr9E6p4wahOH-A.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Source: Machine Learning by Andrew Ng</figcaption></figure><p id="6621" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的目标是达到成本函数的最小值，当我们的<em class="lz">θ₁</em>等于<em class="lz">θₘᵢₙ.时，我们将得到这个最小值</em></p><p id="1bb6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，首先我们将随机初始化我们的<em class="lz">θ₁.</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2e9cb7cfa80ccaa76ca71c411de2034f.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*jCGprYKqkUVUdZZmHH1arw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Source: Machine Learning by Andrew Ng</figcaption></figure><p id="f300" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设<em class="lz">θ₁</em>初始化，如图所示。当前<em class="lz">θ₁</em>对应的成本等于图上的蓝点。</p><p id="2186" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，让我们使用梯度下降更新θ<em class="lz">₁</em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f85b85206431f600adf4d497dd395541.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*YEGHkUxBENGkBswPQcF91w.png"/></div></figure><p id="4015" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们正在减去成本函数 w . r . t<em class="lz">θ₁</em>乘以某个常数的导数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi md"><img src="../Images/a86dc3796abfde1626810a49b4423967.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*geANPO24pSzd6MuwkXVQsw.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Source: Machine Learning by Andrew Ng</figcaption></figure><p id="a161" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">成本函数 w . r . t<em class="lz">θ₁</em>的导数给出了曲线在该点的斜率。这在这些情况下是积极。所以我们从θ₁.的当前值中减去正的量这将迫使<em class="lz">θ₁</em>向左移动，并慢慢发散到<em class="lz">θₘᵢₙ</em>的值，在那里我们的成本函数最小。<em class="lz"> α </em>的作用来了，就是我们的学习率。学习率决定了我们想要在一次迭代中下降多少。此外，这里需要注意的一点是，随着我们向最小值移动，曲线的斜率也变得不那么陡峭，这意味着，随着我们达到最小值，我们将采取越来越小的步骤。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi md"><img src="../Images/c78d58f91df674510278ef74c2b88084.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*GBXAHp59M6M7LjPN53iQEA.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Source: Machine Learning by Andrew Ng</figcaption></figure><p id="51f5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最终，斜率将在曲线的最小值处变为零，然后<em class="lz">θ</em>₁<em class="lz"/>将不会更新。</p><p id="158e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样想吧。假设一个人在山谷的顶端，他想到达谷底。因此，当坡度较陡时，他走较大的步子，当坡度不太陡时，他走较小的步子。他根据当前位置决定下一个位置，当他到达他的目标山谷底部时停下来。</p><p id="d38b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似地，如果<em class="lz">θ₁</em>在最小值的左侧被初始化，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/feaaac35c815bfa858cfea8df128016e.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*AgOL9owi1yRbCMmNiEGPdg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Source: Machine Learning by Andrew Ng</figcaption></figure><p id="0b80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一点的斜率将是负的。在梯度下降中，我们减去斜率，但这里斜率是负的。所以，否定之否定就会变成肯定。因此，我们将继续增加，直到达到成本最低。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/d13ee43f51e5adf1c9b6afefc2bfa6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Gradient Descent (Source: <a class="ae la" href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"><strong class="bd mw">https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/</strong></a><strong class="bd mw">)</strong></figcaption></figure><p id="b8cf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图很好的描绘了梯度下降。注意当我们到达最小值时，步长是如何变得越来越小的。</p><p id="4ef7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似地，<em class="lz">θ₀</em>的值也将使用梯度下降进行更新。我没有展示它，因为我们需要同时更新<em class="lz">θ₀</em>和<em class="lz">θ₁</em>的值，这将导致一个三维图形(一个轴上是成本，<em class="lz">θ₀</em>在一个轴上，而<em class="lz">θ₁</em>在另一个轴上)变得有点难以想象。</p><h2 id="8636" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">成本函数的导数</h2><p id="dfb8" class="pw-post-body-paragraph jq jr it js b jt lu jv jw jx lv jz ka kb lw kd ke kf lx kh ki kj ly kl km kn im bi translated">我们在梯度下降中使用成本函数的导数。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/37c647634b61eaf4609a767b21399014.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*3qlkq6_wWNgno2g162udzQ.png"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f85b85206431f600adf4d497dd395541.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*YEGHkUxBENGkBswPQcF91w.png"/></div></figure><p id="188a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们来看看微分后得到的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/c9ccb55f364e5ff02ef5d52c52efb8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*RQE3W8GM2lxlt3MS1WjMbw.png"/></div></figure><p id="2c38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">同样，对于<em class="lz">θ₁</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/4d76073de90e90400d0e97fc36ef3bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*-FyZrL28am05wyIOP4eTDg.png"/></div></figure><h2 id="4246" class="lb lc it bd ld le lf dn lg lh li dp lj kb lk ll lm kf ln lo lp kj lq lr ls lt bi translated">线性回归可视化</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/241c96830a613b2c47c8935050a04525.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/1*p3gOraOrKuOE33fmjq47Kg.gif"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Linear Regression Visualization</figcaption></figure><p id="6674" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在此可视化效果中，您可以看到线是如何拟合到数据集的。注意，最初，这条线很快就覆盖了这段距离。但是随着成本的降低，线路变得更慢。</p><p id="c65e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我的<a class="ae la" href="https://github.com/sushantPatrikar/Linear-Regression-Simulator" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上有上面可视化的代码。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><blockquote class="ng nh ni"><p id="3cf9" class="jq jr lz js b jt ju jv jw jx jy jz ka nj kc kd ke nk kg kh ki nl kk kl km kn im bi translated">有问题吗？需要帮助吗？联系我！</p></blockquote><p id="918a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">电子邮件:sushantpatrikarml@gmail.com</p><p id="bb3e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">github:<a class="ae la" href="https://github.com/sushantPatrikar" rel="noopener ugc nofollow" target="_blank">https://github.com/sushantPatrikar</a></p><p id="424e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">领英:<a class="ae la" href="https://www.linkedin.com/in/sushant-patrikar/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/sushant-patrikar/</a></p><p id="0f9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">网址:【https://sushantpatrikar.github.io/ T2】T3</p></div></div>    
</body>
</html>