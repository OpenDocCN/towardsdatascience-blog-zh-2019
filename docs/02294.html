<html>
<head>
<title>Review: Hikvision — 1st Runner Up in ILSVRC 2016 (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:海康威视——2016 年国际机器人视觉大会(物体检测)亚军</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-hikvision-1st-runner-up-in-ilsvrc-2016-object-detection-1f0a42cda767?source=collection_archive---------35-----------------------#2019-04-15">https://towardsdatascience.com/review-hikvision-1st-runner-up-in-ilsvrc-2016-object-detection-1f0a42cda767?source=collection_archive---------35-----------------------#2019-04-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2c4c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">ILSVRC 2016 物体探测挑战赛单个模型结果第一名</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/0915149960227053426915f0bad0a341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*75N7cKNjtI40pgXe-3Cb1w.gif"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Hikvision CCTV Product</strong></figcaption></figure><p id="26e0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi lo"><span class="l lp lq lr bm ls lt lu lv lw di">T</span>his time, the approach by <a class="ae lx" href="https://www.hikvision.com" rel="noopener ugc nofollow" target="_blank"><strong class="ku ir">Hikvision (</strong>海康威视<strong class="ku ir">)</strong></a>, in <strong class="ku ir">ILSVRC 2016 object detection challenge</strong>, is briefly reviewed. Hikvision was launched in 2001 based at Hangzhou in China. Hikvision advances the core technologies of audio and video encoding, video image processing, and related data storage, as well as forward-looking technologies such as cloud computing, big data, and deep learning.</p><p id="fea6" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">海康威视在 ILSVRC 2016 中赢得了多项比赛:</p><ul class=""><li id="ece1" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated"><strong class="ku ir">物体检测:第二名，65.27%地图</strong></li><li id="2a52" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">物体定位:第二名，误差 8.74%</li><li id="4750" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">场景分类:第一名，误差 9.01%</li><li id="32ae" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">场景解析:第七名，IoU 和像素准确率平均 53.5%</li></ul><p id="a799" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">在这个故事中，我只关注探测挑战。</strong>虽然海康威视在检测任务上拥有最先进的成果，但没有多少创新技术或新颖性。可能由于这个原因，他们还没有发表任何关于它的论文或技术报告。</p><p id="2b65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">相反，<strong class="ku ir">他们只在 2016 年 ECCV </strong>的 ImageNet 和 COCO 联合研讨会上分享了他们的方法和成果。(<a class="mm mn ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----1f0a42cda767--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @ Medium)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mo mp l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="ak">Hikvision VCA video analytics</strong> (<a class="ae lx" href="https://www.youtube.com/watch?v=2I16iVJtg9M" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=2I16iVJtg9M</a>)</figcaption></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="8f1b" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">概述</h1><ol class=""><li id="286c" class="ly lz iq ku b kv np ky nq lb nr lf ns lj nt ln nu me mf mg bi translated"><strong class="ku ir">级联 RPN </strong></li><li id="2328" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln nu me mf mg bi translated"><strong class="ku ir">全球背景</strong></li><li id="09b6" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln nu me mf mg bi translated"><strong class="ku ir">其他技术</strong></li><li id="e3cd" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln nu me mf mg bi translated"><strong class="ku ir">物体检测要素概要</strong></li><li id="5a48" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln nu me mf mg bi translated"><strong class="ku ir">结果</strong></li></ol></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="1472" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated"><strong class="ak"> 1。级联 RPN </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/ec3bed6897d519dd67c0e8aeda6ebeec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oNvJY9SCS5D4zECwIDEfyg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Cascaded RPN</strong></figcaption></figure><ul class=""><li id="4067" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated">级联区域建议网络(RPN)用于生成建议。</li><li id="362d" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">原始 RPN:批量为 256，阴性/阳性样本比率通常大于 10</li><li id="5042" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">级联 RPN:批量大小为 32，最大 N/P 比率仅为 1.5。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/8fe2a9d53d27234c7d4a938167aaf304.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*HDRDzg3PPyYFMcfE_O4zkA.png"/></div></figure><ul class=""><li id="6d96" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated">通过级联 RPN 和更好的 N/P 比，召回率得到提高。</li><li id="90dd" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">0.7 增益时召回率为 9.5%。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="10f2" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">2.全球背景</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/c816b79eada07ecc7e371a8bea63876e.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*wVYKqqM3XpJfnitN55W3mQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Global Context</strong></figcaption></figure><ul class=""><li id="7a25" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated">利用全局上下文，提取全局特征并与 ROI 特征连接，以获得更好的分类精度。</li><li id="a6c7" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">获得了 3.8%的映射增益。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="5d5c" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">3.其他技术</h1><ul class=""><li id="d286" class="ly lz iq ku b kv np ky nq lb nr lf ns lj nt ln md me mf mg bi translated"><strong class="ku ir">ImageNet LOC</strong>预训练:0.5%地图增益。</li><li id="90f1" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated"><strong class="ku ir">平衡采样</strong>:2007 年 VOC 0.7% mAP。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="7aa0" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated"><strong class="ak"> 4。物体探测要素概述</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi oc"><img src="../Images/ebb3832f3e6311c8acbb827d782a724c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G9-cyZYko775dJIHdPkvSg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Object Detection Elements</strong></figcaption></figure><ul class=""><li id="374c" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated"><a class="ae lx" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">使用预激活 ResNet </a>。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="0533" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">5.结果</h1><h2 id="c56d" class="od my iq bd mz oe of dn nd og oh dp nh lb oi oj nj lf ok ol nl lj om on nn oo bi translated">5.1.ILSVRC 2016 检测挑战</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/a72613015f0a523595eb59d73927cad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*srZ3zBAU2u5sydnGfuYWpg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ILSVRC 2016 Detection Challenge</strong></figcaption></figure><ul class=""><li id="71aa" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated">使用单一模型，实际上，海康威视获得排名 1，优于 CUImage 团队，使用<a class="ae lx" rel="noopener" target="_blank" href="/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac"> GBD 网</a>。</li><li id="bc21" class="ly lz iq ku b kv mh ky mi lb mj lf mk lj ml ln md me mf mg bi translated">然而，使用集合模型，<a class="ae lx" rel="noopener" target="_blank" href="/review-gbd-net-gbd-v1-gbd-v2-winner-of-ilsvrc-2016-object-detection-d625fbeadeac"> GBD 网</a>获得了更好的结果。</li></ul><h2 id="7117" class="od my iq bd mz oe of dn nd og oh dp nh lb oi oj nj lf ok ol nl lj om on nn oo bi translated">5.2.ILSVRC 2016 本地化挑战</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/05629fe6518359bf022158011f0d12e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*t3sAEZmigoS5SGafIGc3og.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">ILSVRC 2016 Localization Challenge</strong></figcaption></figure><ul class=""><li id="ffd0" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated">海康威视以 3.7%的分类误差和 8.7%的定位误差获得 Rank 2。</li></ul><h2 id="2f94" class="od my iq bd mz oe of dn nd og oh dp nh lb oi oj nj lf ok ol nl lj om on nn oo bi translated">5.3.帕斯卡 VOC 2012</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/904331be38edc6c1d06b65b669c3d169.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*c7VBNzfrAZXLHVZqFVjPVw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">PASCAL VOC 2012</strong></figcaption></figure><ul class=""><li id="1d83" class="ly lz iq ku b kv kw ky kz lb ma lf mb lj mc ln md me mf mg bi translated">海康威视超越<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>。</li></ul></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><p id="8552" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">通过将不同的技术结合在一起，海康威视能够在 ILSVRC 2016 检测挑战赛中获得第二名。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="752b" class="od my iq bd mz oe of dn nd og oh dp nh lb oi oj nj lf ok ol nl lj om on nn oo bi translated">参考</h2><p id="b5fd" class="pw-post-body-paragraph ks kt iq ku b kv np jr kx ky nq ju la lb os ld le lf ot lh li lj ou ll lm ln ij bi translated">【2016 ECCV】【海康威视】(仅幻灯片)<br/> <a class="ae lx" href="http://image-net.org/challenges/talks/2016/Hikvision_at_ImageNet_2016.pdf" rel="noopener ugc nofollow" target="_blank">走向良好实践表彰&amp;检测</a></p><h2 id="c16d" class="od my iq bd mz oe of dn nd og oh dp nh lb oi oj nj lf ok ol nl lj om on nn oo bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph ks kt iq ku b kv np jr kx ky nq ju la lb os ld le lf ot lh li lj ou ll lm ln ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">物体检测<br/></strong><a class="ae lx" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lx" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lx" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae lx" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae lx" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lx" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae lx" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae lx" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">yolo v1</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">yolo v2/yolo 9000</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolo v3</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>[<a class="ae lx" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">语义切分<br/></strong><a class="ae lx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae lx" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae lx" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae lx" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae lx" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lx" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae lx" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">生物医学图像分割<br/></strong>[<a class="ae lx" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lx" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lx" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lx" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae lx" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN</a><a class="ae lx" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"/></p><p id="3134" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">实例分割<br/> </strong> [ <a class="ae lx" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例化</a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"/><br/><a class="ae lx" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae lx" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae lx" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>