<html>
<head>
<title>Layman’s Introduction to KNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">外行人对 KNN 的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/laymans-introduction-to-knn-c793ed392bc2?source=collection_archive---------9-----------------------#2019-05-20">https://towardsdatascience.com/laymans-introduction-to-knn-c793ed392bc2?source=collection_archive---------9-----------------------#2019-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0f04" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">k-最近邻算法是大多数人在开始机器学习时的起点。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/acd8a541b51aa0d8adc35c7f789ac06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vRLpxQrSr0F4-zgC"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@the_roaming_platypus?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">timJ</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e9cb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">kNN 代表 k-最近邻。它是一个<strong class="lb iu">监督学习</strong>算法。这意味着我们在监督下训练它。我们使用已有的标签数据来训练它。给定一个由观察值<strong class="lb iu"> <em class="lv"> (x，y)</em></strong>组成的带标签的数据集，我们希望捕获<strong class="lb iu"> x —数据</strong>和<strong class="lb iu"> y —标签</strong>之间的关系。更正式地说，我们想要学习一个函数<strong class="lb iu"> <em class="lv"> g : X→Y </em> </strong>以便给定一个看不见的观察值<strong class="lb iu"> X </strong>，<strong class="lb iu"> <em class="lv"> g(x) </em> </strong>可以自信地预测相应的输出<strong class="lb iu"> Y </strong>。</p><p id="41d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">监督学习算法的其他例子包括<a class="ae ky" href="https://link.medium.com/tJY9BkPMmW" rel="noopener">随机森林</a>、<a class="ae ky" href="https://link.medium.com/3h3C1XczsU" rel="noopener">线性回归</a>和<a class="ae ky" href="https://link.medium.com/bDfhu5lzsU" rel="noopener">逻辑回归</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/1122f26ec51a673d1e2b8f496c2a8c83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*neNUJerAYRn70tm3QDMd8w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://www.edureka.co/blog/what-is-machine-learning/" rel="noopener ugc nofollow" target="_blank">Edureka</a></figcaption></figure><p id="9352" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">kNN 实现起来非常简单，在任何机器学习设置中被广泛用作第一步。它通常被用作更复杂分类器的基准，如人工神经网络(ANN)和支持向量机(SVM)。尽管简单，k-NN 可以胜过更强大的分类器，并用于各种应用，如经济预测，数据压缩和遗传学。</p><ul class=""><li id="f8ac" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">遗传学</li></ul><div class="mg mh gp gr mi mj"><a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-7-S1-S11" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">一个基于回归的 K 最近邻算法用于基因功能预测。</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">随着各种功能基因组学和蛋白质组学技术的出现，越来越需要…</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">bmcbioinformatics.biomedcentral.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ks mj"/></div></div></a></div><ul class=""><li id="0f65" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">农业</li></ul><div class="mg mh gp gr mi mj"><a href="http://www.scielo.br/scielo.php?pid=S0001-37652018000100295&amp;script=sci_arttext" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">用现场数据、航空数据估算火炬松人工林的林分高度和树木密度</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">准确的森林库存对于优化纸浆和造纸行业的整个供应链管理具有重要的经济意义</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">www.scielo.br</p></div></div></div></a></div><ul class=""><li id="24ce" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">航空— <a class="ae ky" href="https://ieeexplore.ieee.org/abstract/document/8630470" rel="noopener ugc nofollow" target="_blank">空中交通流量预测</a></li></ul><div class="mg mh gp gr mi mj"><a href="https://ieeexplore.ieee.org/abstract/document/8630470" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">基于 k 近邻回归的空中交通流量预测</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">为了提高空中交通流量的预测精度，提出了将 k 近邻回归应用于空中交通流量预测的方法</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">ieeexplore.ieee.org</p></div></div></div></a></div><p id="58f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如 20 世纪初的大多数技术进步一样，KNN 算法也诞生于为军队所做的研究。美国空军航空医学院的两个办公室——<strong class="lb iu">Fix 和 Hodges (1951) </strong>写了一份技术报告，介绍了一种用于模式分类的<em class="lv">非参数方法</em>，这种方法后来成为流行的 k-最近邻(kNN)算法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/9d04a1f2239ba0f6215c33bf338cec57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ouwIsQxI0SBVWObtp6gM2A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a800276.pdf" rel="noopener ugc nofollow" target="_blank">DTIC</a> — The Declassified Paper in full.</figcaption></figure><h1 id="1897" class="mz na it bd nb nc nd ne nf ng nh ni nj jz nk ka nl kc nm kd nn kf no kg np nq bi translated">它是如何工作的？</h1><p id="1579" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">假设我们有一个包含两种点的数据集，即标签 1 和标签 2。现在给定这个数据集中的一个新点，我们想找出它的标签。kNN 的做法是通过其最近的 k 个邻居<strong class="lb iu">的多数投票。k 可以取 1 到无穷大之间的任何值，但在大多数实际情况下，k 小于 30。</strong></p><h2 id="1301" class="nw na it bd nb nx ny dn nf nz oa dp nj li ob oc nl lm od oe nn lq of og np oh bi translated">蓝色圆圈 v/s 橙色三角形</h2><p id="ca00" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">假设我们有两组点——<strong class="lb iu">蓝色圆圈</strong>和<strong class="lb iu">橙色三角形</strong>。我们要将<strong class="lb iu">测试点= </strong> <em class="lv">带问号的黑色圆圈，</em>归类为蓝色圆圈或橙色三角形。</p><p id="7383" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目标:给黑圈贴标签。</strong></p><p id="806d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 K = 1，我们将着眼于第一个最近的邻居。因为我们采取多数投票，并且只有一个投票人，所以我们将其标签分配给我们的黑色测试点。我们可以看到，对于 k=1，测试点将被归类为蓝色圆圈。</p><div class="kj kk kl km gt ab cb"><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/13d122a5fc11a014a89b6b2eeca98272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*6YK2xQ4wxBGGrCaegT9JfA.png"/></div></figure><figure class="oi kn oj ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3dfc5f729a86c716d73dc078d636bfae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*z-y9I2aHAGj4GtMI5cR1OA.png"/></div></figure></div><p id="b0c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将我们的搜索半径扩大到 K=3 也保持了相同的结果，只是这次不是绝对多数，是 3 中的 2。仍然在 k=3 的情况下，测试点被预测为具有蓝圈类别→因为大多数点是蓝色的。</p><p id="62ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看看 k=5 和 K =9 是怎么做的。为了查看最近的邻居，我们以测试点为中心画圆，当 5 个点落在圆内时停止。</p><p id="fb92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们观察 5 以及随后的 K = 9 时，我们测试点的大多数最近邻都是橙色三角形。这表明测试点必须是一个橙色的三角形。</p><div class="kj kk kl km gt ab cb"><figure class="oi kn oo ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/077a6dba03897bfb3a30a23e1b082222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7tSKxmXPca1IlgjRHtwOGg.png"/></div></figure><figure class="oi kn op ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/325e5b362b98f262ddebb52cacac799b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*_EYdoVX941aZXa5BH6XnHQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk oq di or os">Left→K=5, Majority = Orange | Right→K =9, Majority = Orange</figcaption></figure></div><p id="30f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经标记了这一个测试点，我们对所有未知点重复相同的过程(即<strong class="lb iu">测试集)</strong>。一旦使用 k-NN 标记了所有的测试点，我们尝试使用判定边界来分离它们。决策边界显示了训练集的分离程度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/c4fb01b987a15d228b1c529ecca1a488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1OXyF2Li7sHftOUsV3mRyw.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">k-NN Live!</figcaption></figure></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h2 id="b808" class="nw na it bd nb nx ny dn nf nz oa dp nj li ob oc nl lm od oe nn lq of og np oh bi translated">这就是 k-NN 如何发生的要点。让我们从一个机器学习工程师大脑的角度来看。</h2><p id="b40a" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated"><strong class="lb iu">首先</strong>他们会选择<strong class="lb iu">k。</strong>我们在上面已经看到，k 越大<strong class="lb iu"> k </strong>的票数就越多。这意味着正确的几率更高。但是你说，代价是什么？</p><p id="39f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">获得 k 个最近邻意味着对距离进行排序。那是一个昂贵的手术。需要非常高的处理能力，这意味着更长的处理时间或更昂贵的处理器。K 值越高，整个过程的成本越高。但是过低的 k 会导致过度拟合。</p><blockquote class="pb"><p id="7435" class="pc pd it bd pe pf pg ph pi pj pk lu dk translated">非常低的 k 将不能概括。非常高的 k 值是昂贵的。</p></blockquote><div class="pl pm pn po pp ab cb"><figure class="oi kn pq ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/f3d19b29773edd186793bb08f08c00ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*3HgD_elK5E0I8w2_luc3NQ.png"/></div></figure><figure class="oi kn pq ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e155712e437cfdd4306fd59366a3dc87.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*YBomoaS2qizrGT_HM2E2VA.png"/></div></figure><figure class="oi kn pq ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/04b0acaa0aec7ffa6f18f97fe0dcf491.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*1phwk7N1x1HAL-Mhzm6j_w.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk pr di ps os">K = 1, 5, 15. Source code below.</figcaption></figure></div><p id="8548" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们达到更高的 K 值时，边界变得平滑。蓝色和红色区域大致分开。一些蓝红战士被留在了敌后。它们是附带损害。它们说明了训练准确性的损失，但是导致了更好的概括和高测试准确性，即<strong class="lb iu">新点的正确标记的高准确性。</strong></p><p id="4ba3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相对于 K 绘制的验证误差图通常如下所示。我们可以看到，在 K = 8 左右，误差最小。它在任何一边上升。</p><div class="kj kk kl km gt ab cb"><figure class="oi kn pt ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/52d785c2b46112693ac5afe13ec10331.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*iTBrCkyLL5veOlyTgkD2gQ.png"/></div></figure><figure class="oi kn pu ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/81583a7025910d5379aef456b501b12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*PatUdrQTOPTmztcXw0Dpqg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk oq di or os">Left : Training error increases with K | Right : Validation error is best for K = 8 | Source: <a class="ae ky" href="https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/" rel="noopener ugc nofollow" target="_blank">AnalyticsVidhya</a></figcaption></figure></div><p id="f32a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">然后</strong>他们会尝试找出点与点之间的距离。我们如何决定哪些邻居是附近的，哪些不是？</p><ul class=""><li id="9b65" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">欧几里德距离—最常见的距离度量</li><li id="32da" class="lx ly it lb b lc pv lf pw li px lm py lq pz lu mc md me mf bi translated">切比雪夫距离— <a class="ae ky" href="https://en.wikipedia.org/wiki/Lp_space" rel="noopener ugc nofollow" target="_blank"> L∞ </a>距离</li><li id="975c" class="lx ly it lb b lc pv lf pw li px lm py lq pz lu mc md me mf bi translated">曼哈顿距离-L1 距离:它们的坐标的(绝对)差之和。</li></ul><blockquote class="qa qb qc"><p id="291a" class="kz la lv lb b lc ld ju le lf lg jx lh qd lj lk ll qe ln lo lp qf lr ls lt lu im bi translated">在国际象棋中，棋盘上<strong class="lb iu">车</strong>的方格之间的距离用曼哈顿距离来度量；<strong class="lb iu">国王和王后</strong>使用切比雪夫距离——维基百科</p></blockquote><div class="kj kk kl km gt ab cb"><figure class="oi kn qg ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/56873e60c491ee594d1a9ddb285c1874.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*0o0l5b600N5R7VwlBorrUQ.png"/></div></figure><figure class="oi kn qh ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e7e1da1fbfd611aafad0c6c2690db63f.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*VzR3lgJXoZbNjSl2rXiXsw.png"/></div></figure><figure class="oi kn qi ok ol om on paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/dae2682ec7e979c2492046d4cfa067dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*v1hhyZo6S62k8pwLnsDV3g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk qj di qk os">All 3 distance metrics with their mathematical formulae. Source: <a class="ae ky" href="https://lyfat.wordpress.com/2012/05/22/euclidean-vs-chebyshev-vs-manhattan-distance/" rel="noopener ugc nofollow" target="_blank">Lyfat</a></figcaption></figure></div><p id="156f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">一旦</strong>我们知道如何根据距离来比较点，我们就会训练我们的模型。<strong class="lb iu">k-NN 最好的一点是没有明确的训练步骤。</strong>我们已经知道了关于数据集的所有信息——它的标签。本质上，算法的训练阶段仅包括存储训练样本的<a class="ae ky" href="https://en.wikipedia.org/wiki/Feature_vector" rel="noopener ugc nofollow" target="_blank"> <em class="lv">特征向量</em> </a>和类别标签。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ql"><img src="../Images/a76da2ce3110db2a5bd775a0602828e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zEQR_coyg8VYy0N-ugon9g.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Tinkersons, December 2, 2017</figcaption></figure><p id="8fbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">K-NN 是一个懒惰的学习器，因为它不从训练数据中学习判别函数，而是记忆训练数据集。</p><blockquote class="pb"><p id="9e0f" class="pc pd it bd pe pf pg ph pi pj pk lu dk translated">一个渴望学习的人有一个模型拟合或训练步骤。懒惰的学习者没有训练阶段。</p></blockquote><h2 id="941e" class="nw na it bd nb nx qm dn nf nz qn dp nj li qo oc nl lm qp oe nn lq qq og np oh bi translated">最后，为什么是 k-NN？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qr"><img src="../Images/a34609ed4da3463563188ff4ef33a960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NQjgAltSfP6EIA5h7cxzHw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://xkcd.com/1445/" rel="noopener ugc nofollow" target="_blank">xkcd</a></figcaption></figure><ul class=""><li id="7728" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">快速实现:这也是它作为基准算法受欢迎的原因。</li><li id="65bc" class="lx ly it lb b lc pv lf pw li px lm py lq pz lu mc md me mf bi translated">较少的培训时间:更快的周转时间</li><li id="ce33" class="lx ly it lb b lc pv lf pw li px lm py lq pz lu mc md me mf bi translated">可比较的准确性:正如许多研究论文中指出的，它的预测准确性对于许多应用来说是相当高的。</li></ul><p id="27c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当一个人必须快速交付一个具有相当准确结果的解决方案时，k-NN 是一个救命稻草。在大多数工具中，如 MATLAB、python、R，它是作为一个单行命令给出的。尽管如此，这是非常容易实现和有趣的尝试。</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="74f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">R 图的源代码</p><pre class="kj kk kl km gt qs qt qu qv aw qw bi"><span id="a36a" class="nw na it qt b gy qx qy l qz ra">library(ElemStatLearn)<br/>require(class)</span><span id="f1ba" class="nw na it qt b gy rb qy l qz ra"><strong class="qt iu">#Data Extraction</strong><br/>KVAL &lt;- 15<br/>x &lt;- mixture.example$x<br/>g &lt;- mixture.example$y<br/>xnew &lt;- mixture.example$xnew</span><span id="c046" class="nw na it qt b gy rb qy l qz ra"><strong class="qt iu">#KNN and boundary extraction</strong><br/>mod15 &lt;- knn(x, xnew, g, k=KVAL, prob=TRUE)<br/>prob &lt;- attr(mod15, "prob")<br/>prob &lt;- ifelse(mod15=="1", prob, 1-prob)<br/>px1 &lt;- mixture.example$px1<br/>px2 &lt;- mixture.example$px2<br/>prob15 &lt;- matrix(mod15, length(px1), length(px2))</span><span id="024b" class="nw na it qt b gy rb qy l qz ra"><strong class="qt iu">#Plotting Boundary</strong><br/>par(mar=rep(2,4))<br/>contour(px1, px2, prob15, levels=0.5, labels="", xlab="x", ylab="y", main="", axes=TRUE)</span><span id="5c02" class="nw na it qt b gy rb qy l qz ra"><strong class="qt iu">#Plotting red and blue points</strong><br/>points(x, pch = 20, col=ifelse(g==1, "coral", "cornflowerblue"))gd &lt;- expand.grid(x=px1, y=px2)<br/>points(gd, pch=".", cex=0.001, col=ifelse(prob15&gt;0.5, "coral", "cornflowerblue"))<br/>legend("topright", pch = c(19, 19), col = c( "red","blue"), legend = c( "Class 1", "Class 2"))<br/>title(main="Boundary for K=15", sub="ii", xlab="Feature 1", ylab="Feature 2")<br/>box()</span></pre></div></div>    
</body>
</html>