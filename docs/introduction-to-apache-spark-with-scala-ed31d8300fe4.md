# Scala Apache Spark 简介

> 原文：<https://towardsdatascience.com/introduction-to-apache-spark-with-scala-ed31d8300fe4?source=collection_archive---------4----------------------->

本文是 3 月份 Scala-Lagos 会议的后续笔记，在该会议上，我们讨论了 Apache Spark、它的功能和用例，以及一个简单的例子，其中 Scala API 用于 Tweets 上的样本数据处理。它旨在很好地介绍 Apache Spark 的优势以及这些优势背后的理论。

# **Spark——一个包罗万象的数据处理平台**

![](img/cbaded5d0fc762fac8a0890af4828de8.png)

> “如果有一个收获的话，那就是小赢也没什么。小胜是好事，他们会复合。如果你做得对，最终结果将是巨大的。”—安迪·约翰斯

Apache Spark 是一个高度开发的引擎，用于在数千个并行计算引擎上进行大规模数据处理。这允许最大化这些计算引擎的处理器能力。Spark 能够处理多种数据处理任务，包括复杂的数据分析、流分析、图形分析以及对大量数据进行可扩展的机器学习，这些数据的数量级为 TB、Zettabytes 等等。

Apache Spark 的成功得益于其开发背后的基本理念，即打破 MapReduce 的限制，MapReduce 是 Hadoop 的一个关键组件，迄今为止，其处理能力和分析能力比 MapReduce 好几个数量级，100 倍，并且具有**内存处理能力**的优势，因为它能够将其数据保存在计算引擎的内存(RAM)中，并对存储在内存中的数据进行数据处理，从而消除了从磁盘写入/读取数据的连续输入/输出(I/O)需求。

为了有效地做到这一点，Spark 依赖于使用一种称为**弹性分布式数据集(RDD)** 的专门数据模型，它可以有效地存储在内存中，并允许各种类型的操作。RDD 是不可变的，即存储在内存中的数据项的只读格式，并且有效地分布在机器集群中，人们可以将 RDD 视为原始数据格式(如 String、Int)的数据抽象，这使得 Spark 可以很好地工作。

![](img/9f77d5df34713dd4404a4357a394dde5.png)

除了 RDD，Spark 还利用直接无环图(DAG)来跟踪 rdd 上的计算，这种方法通过利用作业流来适当分配性能优化来优化数据处理，这还有一个额外的优势，即通过有效的回滚机制帮助 Spark 在作业或操作失败时管理错误。因此，在出现错误的情况下，Spark 不需要从头开始计算，它可以很容易地利用错误之前计算的 RDD，并通过固定操作传递它。这就是 Spark 被指定为容错处理引擎的原因。

Spark 还利用集群管理器在一个机器集群上正确地运行它的作业，集群管理器以一种**主-工**的方式帮助分配资源和调度作业。主设备为集群中的工人分配作业和必要的资源，并协调工人的活动，以便在工人不可用的情况下，将作业重新分配给另一个工人。

凭借使用 RDD 抽象、DAG 计算范式、资源分配和集群管理器调度的内存处理理念，Spark 已成为快速大数据处理领域不断进步的引擎。

# **星火数据处理能力。**

**使用基本 SQL 进行复杂分析的结构化 SQL**

Apache Spark 一个众所周知的功能是，它允许数据科学家轻松地以类似 SQL 的格式对大量数据进行分析。利用 spark-core 内部和底层 RDD 上的抽象，spark 提供了所谓的 DataFrames，这是一种将关系处理与 Spark 的函数式编程 API 集成在一起的抽象。这是通过使用具有列名的模式向数据添加结构信息以向数据提供半结构或完整结构来实现的，并且通过这种方式，可以使用列名直接查询数据集，从而为数据处理打开了另一个级别。

从 Spark 1.6 版开始，结构化 SQL API 提供了数据集 API，它为 Spark-core 的低级 RDD 提供了类似 SQL 的高级功能。从字面上看，数据集 API 是一种抽象，它通过使用优化的 SQL 执行引擎为 spark RDD 提供了 SQL 感觉和执行优化，同时也不会失去 RDD 附带的功能操作。数据集 API 和数据帧 API 都构成了结构化 SQL API。

**用于实时分析的火花流**

Spark 还提供了一个扩展，通过以离散流的形式提供底层 RDD 的抽象，可以轻松地操作流数据。使用底层 RDD Spark 内核有两个主要优势；它允许 Spark 的其他核心功能在流数据上得到利用，并有利于可以在 rdd 上执行的数据核心操作。

离散化数据流是指实时小批量获得的 RDD 数据。

**用于预测建模的 MLLib/ML 机器学习**

Spark 还通过提供机器学习算法、数据特征化算法和流水线功能来提供机器学习功能，这些功能经过优化，可以在大量数据上进行扩展。Spark 机器学习库的目标概括如下:

> 让实用的机器学习变得可扩展和简单。

**GraphX 图形处理引擎。**

第四种数据处理能力是其固有的对图形数据进行分析的能力，例如在社交网络分析中。Spark 的 GraphX API 是 ETL 处理操作和图形算法的集合，针对数据的大规模实现进行了优化。

# **初始化火花**

根据使用情况，有多种方法可以初始化 Spark 应用程序，应用程序可以利用 RDD、Spark 流、带有数据集或数据帧的结构化 SQL。因此，理解如何初始化这些不同的 Spark 实例非常重要。

1. **RDD 与星火语境:**

spark-core 的操作是通过创建一个 spark context 来启动的，该 context 是通过许多配置来创建的，例如主位置、应用程序名称、执行器的内存大小等等。

这里有两种启动 spark 上下文的方法，以及如何使用创建的 spark 上下文创建 RDD。

**2。带 Spark 会话的数据帧/数据集:**

如上所述，Spark 的入口点可能是通过使用 *Spark 上下文*，然而，Spark 允许通过 *Spark 会话*与结构化 SQL API 直接交互。它还包括指定 Spark 应用程序的配置。

下面是启动 Spark 会话并使用该会话创建数据集和数据帧的方法。

**3。带火花流的数据流:**

Spark 的另一个入口点是在与实时数据交互时使用*流上下文*。流式上下文的实例可以从 Spark 配置或 Spark 上下文中创建。如下所示

# **对 RDD、数据集和数据帧的操作**

已经很好地了解了 spark 的功能，展示一些可以应用于各种 Spark 抽象的操作是很重要的。

**1。RDD**

RDD 是 spark 的主要抽象，也是 Spark 核心的核心，它有两个基本操作。

***转换*** —转换操作应用于现有的 RDD，以创建新的和已更改的 rdd。这种操作例子包括 ***映射******过滤*** 和 ***平面映射*** 等等。spark 中转换操作的完整列表可以在[这里](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations)找到

一旦 spark 上下文用于创建 RDD，这些操作就可以应用于 RDD，如下面的代码示例所示。值得注意的是，这些操作是延迟计算的，因为它们在应用操作之前不会被直接计算。

***动作*** —动作操作触发 Spark 中的实际计算，它驱动计算向驱动程序返回值。动作操作的思想是将集群中的所有计算返回给驱动程序，以产生一个实际数据类型的单一结果，而不是 spark 的 RDD 抽象。启动动作操作时必须小心，因为驱动程序有足够的内存来管理这些数据是很重要的。动作操作的例子包括 ***【减少】******收集******取*** 等等。完整列表可在[这里](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)找到

**2。数据集/数据帧**

如前所述，数据集是结构化 SQL 的类似 RDD 的优化抽象，既允许 SQL 中的关系操作，也允许函数操作，如 ***映射*** 、 ***过滤*** 和许多其他类似的操作，这些操作在 RDD 中都是可能的。需要强调的是，并不是所有的 DataFrame 类似 SQL 的功能都完全适用于 Dataset，但是有许多基于列的函数仍然非常适用于 Dataset。此外，*在特定于领域的对象中编码*数据集还有一个额外的优势，即把数据集映射到一个*类型 T* 这有助于扩展 Spark Dataset 可能实现的功能，还增加了执行强大的*λ*操作的能力。

Spark DataFrame 可以进一步视为以命名列组织的数据集，并呈现为一个等效的关系表，您可以使用类似 SQL 的查询甚至 HQL。因此，在 Spark DataFrame 上，执行任何类似 SQL 的操作，例如 **SELECT COLUMN-NAME** 、 **GROUPBY** 和 **COUNT** 等等变得相对容易。关于 Spark DataFrame 的另一个有趣的事情是，这些操作可以使用任何可用的 spark APIs 以编程方式完成——Java、Scala、Python 或 R，以及将 DataFrame 转换为一个*临时* *SQL 表*，在该表中可以执行纯 SQL 查询。

**结论。**

总结一下 Spark 的介绍，一个示例 scala 应用 tweets 上的字数统计是[提供的](https://github.com/LagosScala/introduction-scala-spark)，它是在 scala API 中开发的。该应用程序可以在您喜欢的 IDE 中运行，如 InteliJ 或笔记本电脑，如 Databricks 或 Apache Zeppelin。

在本文中，涵盖了一些要点:

*   Spark 作为下一代数据处理引擎的描述
*   赋予 spark 能力的非延迟技术
*   Spark 中存在的数据处理 API
*   关于如何使用数据处理 API 的知识
*   体验 spark 处理能力的一个简单例子。

*本文原载* [*此处*](https://adekunleba.github.io/scalalang/sparkscala)