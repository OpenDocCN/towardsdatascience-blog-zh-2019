<html>
<head>
<title>How to initialize a Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何初始化神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-initialize-a-neural-network-27564cfb5ffc?source=collection_archive---------4-----------------------#2019-06-18">https://towardsdatascience.com/how-to-initialize-a-neural-network-27564cfb5ffc?source=collection_archive---------4-----------------------#2019-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f3ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">权重初始化技术的实际应用</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/58c6e344195af062a2bef95a0914e3c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PNCySq7ZgoTM7GKQnj3VA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image source: <a class="ae kv" href="https://pixabay.com/photos/sport-tracks-running-run-sprint-1201014/" rel="noopener ugc nofollow" target="_blank">https://pixabay.com/photos/sport-tracks-running-run-sprint-1201014/</a></figcaption></figure><p id="18ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练神经网络远不是一项简单的任务，因为最轻微的错误都会在没有任何警告的情况下导致非最佳结果。训练取决于许多因素和参数，因此需要<a class="ae kv" href="http://karpathy.github.io/2019/04/25/recipe/" rel="noopener ugc nofollow" target="_blank">深思熟虑的方法</a>。</p><p id="956f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">众所周知，训练的开始(即最初的几次迭代)<a class="ae kv" href="https://arxiv.org/pdf/1901.09321.pdf" rel="noopener ugc nofollow" target="_blank">非常重要</a>。如果操作不当，结果会很糟糕——有时，网络甚至什么也学不到！因此，初始化神经网络权重的方式是良好训练的关键因素之一。</p><p id="9393" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文的目标是解释为什么初始化会产生影响，并给出有效实现它的不同方法。我们将通过实际例子来检验我们的方法。<br/>代码使用了<a class="ae kv" href="https://github.com/fastai" rel="noopener ugc nofollow" target="_blank"> fastai 库</a>(基于 pytorch)。所有的实验笔记本都可以在这个 github 库中找到。</p><h1 id="2c54" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为什么初始化很重要？</h1><p id="b4e0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">神经网络训练基本上包括重复以下两个步骤:</p><ul class=""><li id="1ced" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">前进的一步在于权重和输入/激活之间的大量矩阵乘法(我们称<em class="my">激活为</em>将成为下一层输入的层的输出，即隐藏激活)</li><li id="bd0c" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">向后的步骤，包括更新网络的权重，以便最小化损失函数(使用参数的梯度)</li></ul><p id="5738" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在向前的步骤中，激活(然后是梯度)可以很快变得很大或很小——这是因为我们重复了许多矩阵乘法。更具体地说，我们可能会得到:</p><ul class=""><li id="2904" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">非常大的激活，因此大梯度射向无限</li><li id="d646" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">非常小的激活，因此梯度极小，由于数值精度，梯度可能被抵消为零</li></ul><p id="20e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两种影响对训练都是致命的。下面是在第一次向前传递时，使用随机初始化的权重进行爆炸的示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/7c814bd4708193e2410e550da9bdc473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HHdcqNEB4XN5Zk2F"/></div></div></figure><p id="a094" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个特殊的例子中，平均值和标准偏差在第 10 层已经很大了！</p><p id="ffe8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让事情变得更加棘手的是，在实践中，即使在避免爆炸或消失效果的同时，经过长时间的训练，你仍然可以获得非最佳的结果。这在下面一个简单的 convnet 上有所说明(实验将在文章的第二部分详述):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/caaa9626e25c9595960df1504ec0db00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Hh0V_Wy_9PxgRXM2"/></div></div></figure><p id="3774" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，默认的 pytorch 方法并不是最好的方法，随机初始化并不能学到很多东西(另外:这只是一个 5 层网络，这意味着更深层次的网络不会学到任何东西)。</p><h1 id="c1bd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何初始化您的网络</h1><p id="da05" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">回想一下，良好初始化的目标是:</p><ul class=""><li id="2223" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">获取随机权重</li><li id="48dc" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">在第一个正向过程中，将激活保持在<em class="my">良好范围</em>内(反向过程中的梯度也是如此)</li></ul><p id="4f1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">什么是<em class="my">练好的靶场</em>？定量地说，这意味着使矩阵与输入向量相乘的输出产生平均值接近 0 且标准偏差接近 1 的输出向量(即激活)。那么每一层将在所有层中传播这些统计数据。<br/>即使在一个深度网络中，你也会在第一次迭代中获得稳定的统计数据。</p><p id="2eef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在讨论两种实现方法。</p><h1 id="d4a0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数学方法:明凯初始化</h1><p id="2e81" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们想象一下这个问题。如果初始化的权重在训练开始时太大，那么每个矩阵乘法将指数地增加激活，导致我们所说的<em class="my">梯度爆炸</em>。<br/>相反，如果权重太小，那么每次矩阵乘法将减少激活，直到它们完全消失。</p><p id="8326" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，这里的关键是缩放权重矩阵，以获得均值约为 0、标准差为 1 的矩阵乘法输出。</p><p id="6025" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是如何定义权重的范围呢？因为每个权重(以及输入)都是独立的，并且按照正态分布分布，所以我们可以通过计算得到帮助。</p><p id="f684" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">两篇著名论文基于这一思想提出了一个很好的初始化方案:</p><ul class=""><li id="74de" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">“Xavier 初始化”，在 2010 年的论文<a class="ae kv" href="http://proceedings.mlr.press/v9/glorot10a.html" rel="noopener ugc nofollow" target="_blank">中提出理解训练深度前馈神经网络的难度</a></li><li id="1c07" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">“明凯初始化”，2015 年在论文<a class="ae kv" href="https://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank">中提出，深入研究整流器:在 ImageNet 分类上超越人类水平的性能</a></li></ul><p id="c390" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实际上，这两种方案非常相似:“主要”区别在于明凯初始化考虑了每次矩阵乘法之后的 ReLU 激活函数。</p><p id="273c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，大多数神经网络使用 ReLU(或类似 leaky ReLU 的类似函数)。这里，我们只关注明凯初始化。</p><p id="731f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简化公式(对于标准 ReLU)是通过以下方式对随机权重(取自标准分布)进行缩放:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/77447072cc9e7b03c76d0ed617aa83c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eitrcER_ahD2J16mBDYtNw.png"/></div></div></figure><p id="93bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，如果我们有一个大小为 512 的输入:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/55e69c88b476c447f5f9e829715dd4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*ldAxqLjm0gA_mVMPvGknAA.png"/></div></figure><p id="318c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，所有偏置参数应初始化为零。</p><p id="4763" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，对于 Leaky ReLU，公式有一个额外的部分，我们在这里没有考虑(我们让读者参考原始论文)。</p><p id="8696" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看这个方法在前面的例子中是如何工作的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/c800c9691bddc0ab3e0df9c69f21c6d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r-iWSblIlB3OqvlK"/></div></div></figure><p id="a1f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，现在我们在初始化后获得了平均值为 0.64、标准偏差为 0.87 的激活。显然，这并不完美(怎么可能是随机数呢？)，但比正态分布的随机权重好得多。</p><p id="1716" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">50 层之后，我们得到的平均值为 0.27，标准差为 0.464，因此不再有爆炸或消失的效果。</p><h2 id="1813" class="nj lt iq bd lu nk nl dn ly nm nn dp mc lf no np me lj nq nr mg ln ns nt mi nu bi translated">可选:明凯公式的快速解释</h2><p id="cbe4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">明凯的论文中提供了导致 math.sqrt 的神奇缩放数(2 /输入向量的大小)的数学推导。此外，我们在下面提供了一些有用的代码，读者可以完全跳过这些代码进入下一节。注意，代码要求理解如何做矩阵乘法以及什么是方差/标准差。</p><p id="959e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了理解公式，我们可以考虑矩阵乘法结果的方差是什么。在本例中，我们将一个 512 矢量乘以一个 512x512 矩阵，得到一个 512 矢量的输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/fe7e06834b135abf3f6d2ed5426cfd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gcJZ-od33CVIclAv"/></div></div></figure><p id="0713" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，矩阵乘法输出的方差大约等于输入向量的大小。根据定义，标准差是它的平方根。</p><p id="75a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么将权重矩阵除以输入向量大小的平方根(本例中为 512)会得到标准差为 1 的结果。</p><p id="519f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是“2”的分子从何而来？这只是考虑到了 ReLU 层。</p><p id="0ad8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如你所知，ReLU 将负数设置为 0(只有 max(0，input))。所以，因为我们有以平均值 0 为中心的数字，它基本上消除了一半的方差。这就是为什么我们加一个分子 2。</p><h2 id="31af" class="nj lt iq bd lu nk nl dn ly nm nn dp mc lf no np me lj nq nr mg ln ns nt mi nu bi translated">明凯 init 的缺点是</h2><p id="d90b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">明凯 init 在实践中非常有效，那么为什么要考虑另一种方法呢？事实证明，Kaming init 有一些缺点:</p><ul class=""><li id="fca7" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">一层之后的平均值不是 0，而是大约 0.5。这是因为 ReLU 激活函数，它删除了所有的负数，有效地改变了它的平均值</li><li id="ec06" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">明凯初始化只适用于 ReLU 激活函数。因此，如果你有一个更复杂的架构(不仅仅是 matmult → ReLU 层)，那么这将不能在所有层上保持大约 1 的标准偏差</li><li id="2042" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">一层之后的标准偏差不是 1，而是接近 1。在深度网络中，这不足以使标准差始终接近 1。</li></ul><h1 id="9ca9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">算法途径:LSUV</h1><p id="7434" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">那么，在不为更复杂的架构手动定制明凯初始化的情况下，我们能做些什么来获得一个好的初始化方案呢？</p><p id="0774" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2015 年的论文<a class="ae kv" href="https://arxiv.org/abs/1511.06422" rel="noopener ugc nofollow" target="_blank">展示了一种有趣的方法。它被称为 LSUV(层序单位方差)。</a></p><p id="11eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决方案在于使用简单的算法:首先，用正交初始化来初始化所有层。然后，取一个小批量输入，并为每一层计算其输出的标准偏差。将每层除以产生的偏差，然后将其重置为 1。下面是论文中解释的算法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/5deeb92b639e2c5b2f155f72ae453461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fma-u_Qhy4RnL6MQ"/></div></div></figure><p id="7a98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">经过一些测试，我发现正交初始化比在 ReLU 之前进行明凯初始化给出了相似(有时更差)的结果。</p><p id="d9f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">杰瑞米·霍华德在<a class="ae kv" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fastai MOOC </a>中展示了另一种实现方式，它对权重进行了更新，以保持均值在 0 左右。在我的实验中，我还发现将平均值保持在 0 左右会得到更好的结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="1478" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们比较这两种方法的结果。</p><h1 id="2cae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">初始化方案的性能</h1><p id="adbe" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们将在两个架构上检查不同初始化方案的性能:一个具有 5 层的“简单”convnet 和一个更复杂的类似 resnet 的架构。<br/>任务是在<a class="ae kv" href="https://github.com/fastai/imagenette" rel="noopener ugc nofollow" target="_blank">Imagenet 数据集</a>(Imagenet 数据集的 10 个类的子集)上进行图像分类。</p><h2 id="3e6b" class="nj lt iq bd lu nk nl dn ly nm nn dp mc lf no np me lj nq nr mg ln ns nt mi nu bi translated">简单的建筑</h2><p id="c045" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这个实验可以在这个笔记本里找到<a class="ae kv" href="https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Simple%20model.ipynb" rel="noopener ugc nofollow" target="_blank">。请注意，由于随机性，每次的结果可能会略有不同(但不会改变顺序和整体情况)。</a></p><p id="6c29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它使用一个简单的模型，定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/b2bf5e318b67b39a081d5f43f852579a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zyOiWlkYvUv2dtVF"/></div></div></figure><blockquote class="oa ob oc"><p id="b712" class="kw kx my ky b kz la jr lb lc ld ju le od lg lh li oe lk ll lm of lo lp lq lr ij bi translated">#ConvLayer 是一个 Conv2D 层，后跟一个 ReLU <br/> nn。Sequential(ConvLayer(3，32，ks=5)，ConvLayer(32，64)，ConvLayer(64，128)，ConvLayer(128，128)，nn。AdaptiveAvgPool2d(1)，Flatten()，nn。线性(128，data.c))</p></blockquote><p id="91db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是 3 个初始化方案的比较:Pytorch 默认的初始化(它是一个明凯初始化，但有一些特定的参数)，明凯初始化和 LSUV 初始化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/c05c83b846e6c8adefa8ee1ea37a30a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*s2JzrOr3YdMawGuI"/></div></div></figure><p id="9086" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，随机初始化的性能太差了，我们从下面的结果中删除了它。</p><p id="ee5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">【init 后的激活统计<br/> 第一个问题是在第一次迭代向前传递后的激活统计是什么？我们越接近平均值 0 和标准差 1，就越好。</p><p id="95be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该图显示了初始化后(训练前)每一层的激活状态。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/7c328734a07969c121ffe58e6b753ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9aU5Hc015SxaaJ0G"/></div></div></figure><p id="bff4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于标准差(右图)，LSUV 和明凯初始值都接近 1(LSUV 更接近)。但是对于 pytorch 默认值，标准差要低得多。</p><p id="db50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是对于平均值，明凯初始结果更差。这是可以理解的，因为明凯 init 没有考虑平均的 ReLU 效应。所以平均值在 0.5 左右而不是 0。</p><h2 id="fd09" class="nj lt iq bd lu nk nl dn ly nm nn dp mc lf no np me lj nq nr mg ln ns nt mi nu bi translated">复杂架构(resnet50)</h2><p id="30c0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在让我们检查一下在一个更复杂的架构上是否得到了类似的结果。</p><p id="cd4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">架构是 xresnet-50，<a class="ae kv" href="https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py" rel="noopener ugc nofollow" target="_blank">在 fastai 库</a>中实现。它比我们之前的简单模型多了 10 倍的层数。</p><p id="6272" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将分两步进行检查:</p><ul class=""><li id="c2a2" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">没有规范化层:batchnorm 将被禁用。因为这一层将逐批修改 stats mini，所以它应该会降低初始化的影响</li><li id="13d3" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">使用规范化图层:batchnorm 将被启用</li></ul><p id="29c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第一步:没有 batchnorm <br/> </strong>这个实验可以在这个笔记本里找到<a class="ae kv" href="https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20no%20batchnorm.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="8e49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果没有 batchnorm，10 个时期的结果是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/0b142fa4625b3499a184df8d3eae2784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p222oGtYChG_8eD1"/></div></div></figure><p id="69d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该图显示 LSUV 的精度(y 轴)为 67%,明凯初始化为 57 %, py torch 默认为 48%。差别是巨大的！</p><p id="089c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在训练前检查激活统计:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/8c749082317ffb824abc25af27063c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EIqhX8_ww86-Jzrz"/></div></div></figure><p id="2da2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们缩放以获得更好的比例:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/feb7e0b1276500692fa10bc1701b9080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*thkK3qIJk67zcw6u"/></div></div></figure><p id="8bf7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到一些层的 stats 为 0:这是 xresnet50 的设计，与 init 方案无关。这是论文<a class="ae kv" href="https://arxiv.org/abs/1812.01187" rel="noopener ugc nofollow" target="_blank">中的一个技巧，用于使用卷积神经网络</a>进行图像分类(在 fastai 库中实现)。</p><p id="f5f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们认为:</p><ul class=""><li id="699e" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated">Pytorch 默认初始化:标准差和均值接近于 0。这是不好的，显示了一个正在消失的问题</li><li id="68a1" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">明凯·尼特:我们得到了一个很大的均值和标准差</li><li id="e8f2" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">LSUV init:我们得到了很好的统计数据，虽然不完美，但比其他方案要好</li></ul><p id="1e4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，即使在 10 个完整的时期之后，这个例子的最佳初始化方案对于完整的训练给出了好得多的结果。这显示了在第一次迭代中保持各层良好统计的重要性。</p><h2 id="68a3" class="nj lt iq bd lu nk nl dn ly nm nn dp mc lf no np me lj nq nr mg ln ns nt mi nu bi translated">步骤 2:使用 batchnorm 图层</h2><p id="1dc6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这个实验可以在这个笔记本里找到<a class="ae kv" href="https://github.com/tchambon/How-to-initialize-a-neural-net/blob/master/Complex%20model%20-%20with%20batchnorm.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7f8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为 batchnorm 正在规范化一个层的输出，我们应该期望 init 方案的影响较小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/0561619b965e874bdee55e535354244d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RrCRDmTpKczNhqNz"/></div></div></figure><p id="b558" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果显示，所有 init 方案的准确率接近 88%。请注意，在每次运行时，最佳初始化方案可能会根据随机生成器而变化。</p><p id="408c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它表明 batchnom 层使网络对初始化方案不太敏感。</p><p id="1e84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训前的激活统计如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/676bc76fbbb6f8806a2aa3c9797e1cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JsmeoAMypJm5qFoa"/></div></div></figure><p id="b630" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像以前一样，最好的似乎是 LSUV init(只有它能保持平均值在 0 左右，标准差接近 1)。</p><p id="7dbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是结果显示这对准确性没有影响，至少对于这个体系结构和这个数据集是这样。不过它证实了一件事:batchnorm 使网络对初始化的质量不太敏感。</p><h1 id="e216" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="bedb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">从这篇文章中要记住什么？</p><ul class=""><li id="26e3" class="mp mq iq ky b kz la lc ld lf mr lj ms ln mt lr mu mv mw mx bi translated"><strong class="ky ir">第一次迭代非常重要</strong>,会对整个培训产生持久的影响。</li><li id="66e0" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">一个好的初始化方案应该在网络的所有层(对于第一次迭代)保持关于激活的输入统计(平均值为 0，标准偏差为 1)。</li><li id="ea3f" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">批处理层降低了神经网络对初始化方案的敏感性。</li><li id="e98a" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated"><strong class="ky ir">使用明凯 init + LSUV 似乎是一个不错的方法</strong>，尤其是当网络缺少标准化层的时候。</li><li id="e668" class="mp mq iq ky b kz mz lc na lf nb lj nc ln nd lr mu mv mw mx bi translated">其他类型的架构在初始化方面可能有不同的行为。</li></ul></div></div>    
</body>
</html>