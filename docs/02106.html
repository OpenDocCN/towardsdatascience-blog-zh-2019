<html>
<head>
<title>A Beginners Introduction into MapReduce</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MapReduce 初学者入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-introduction-into-mapreduce-2c912bb5e6ac?source=collection_archive---------2-----------------------#2019-04-07">https://towardsdatascience.com/a-beginners-introduction-into-mapreduce-2c912bb5e6ac?source=collection_archive---------2-----------------------#2019-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/63eeebc057aec33fa5567d1ebb6d36a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eX-HaxcDk5h_jgmDmkAXbQ.jpeg"/></div></div></figure><p id="026a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">很多时候，作为数据科学家，我们必须处理海量数据。在这种情况下，许多方法都不起作用或不可行。大量的数据是好的，非常好，我们希望尽可能地利用这些数据。</p><p id="1ca7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我想介绍 MapReduce 技术，这是一种用于处理大量数据的广泛技术。MapReduce 的实现有很多，包括著名的 Apache Hadoop。这里，我就不说实现了。我将尝试以最直观的方式介绍这个概念，并给出玩具和现实生活中的例子。</p><p id="f22c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们从一些简单的任务开始。给你一个字符串列表，你需要返回最长的字符串。在 python 中这很容易做到:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="8b67" class="lf lg iq lb b gy lh li l lj lk">def find_longest_string(list_of_strings):<br/>    longest_string = None<br/>    longest_string_len = 0 </span><span id="bda9" class="lf lg iq lb b gy ll li l lj lk">    for s in list_of_strings:<br/>        if len(s) &gt; longest_string_len:<br/>            longest_string_len = len(s)<br/>            longest_string = s</span><span id="345c" class="lf lg iq lb b gy ll li l lj lk">    return longest_string</span></pre><p id="cdc5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们一个接一个地检查字符串，计算长度并保留最长的字符串，直到我们完成。</p><p id="73b5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于小列表，它的工作速度相当快:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="cce7" class="lf lg iq lb b gy lh li l lj lk">list_of_strings = ['abc', 'python', 'dima']</span><span id="9c8b" class="lf lg iq lb b gy ll li l lj lk">%time max_length = print(find_longest_string(list_of_strings))</span><span id="acab" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>python<br/>CPU times: user 0 ns, sys: 0 ns, total: 0 ns<br/>Wall time: <strong class="lb ir">75.8 µs</strong></span></pre><p id="0dd1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">即使对于包含 3 个以上元素的列表，它也能很好地工作，这里我们尝试使用 3000 个元素:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="16f2" class="lf lg iq lb b gy lh li l lj lk">large_list_of_strings = list_of_strings*<strong class="lb ir">1000</strong></span><span id="89a0" class="lf lg iq lb b gy ll li l lj lk">%time print(find_longest_string(large_list_of_strings))</span><span id="f4d7" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>python<br/>CPU times: user 0 ns, sys: 0 ns, total: 0 ns<br/>Wall time: <strong class="lb ir">307 µs</strong></span></pre><p id="64ab" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是如果我们尝试 3 亿个元素呢？</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="0fa0" class="lf lg iq lb b gy lh li l lj lk">large_list_of_strings = list_of_strings*<strong class="lb ir">100000000</strong><br/>%time max_length = max(large_list_of_strings, key=len)</span><span id="3a4c" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>python<br/>CPU times: user 21.8 s, sys: 0 ns, total: 21.8 s<br/>Wall time: <strong class="lb ir">21.8 s</strong></span></pre><p id="4378" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一个问题，在大多数应用中，20 秒的响应时间是不可接受的。改善计算时间的一个方法是购买更好更快的 CPU。通过引入更好更快的硬件来扩展您的系统被称为“垂直扩展”。这当然不会永远有效。不仅找到一个工作速度快 10 倍的 CPU 不是一件小事，而且，我们的数据可能会变得更大，我们不想每次代码变慢时都升级我们的 CPU。我们的解决方案不可扩展。相反，我们可以进行“水平扩展”，我们将设计我们的代码，以便它可以并行运行，当我们添加更多处理器和/或 CPU 时，它将变得更快。</p><p id="67f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，我们需要将代码分解成更小的组件，看看如何并行执行计算。直觉如下:1)将我们的数据分成许多块，2)对每个块并行执行<code class="fe lm ln lo lb b">find_longest_string</code>函数，3)在所有块的输出中找到最长的字符串。</p><p id="1bf8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的代码非常具体，很难破坏和修改，所以我们不使用<code class="fe lm ln lo lb b">find_longest_string</code>函数，而是开发一个更通用的框架，帮助我们在大数据上并行执行不同的计算。</p><p id="622e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在代码中做的两件主要事情是计算字符串的<code class="fe lm ln lo lb b">len</code>,并将其与迄今为止最长的字符串进行比较。我们将把代码分成两步:1)计算所有字符串的<code class="fe lm ln lo lb b">len</code>，2)选择<code class="fe lm ln lo lb b">max</code>值。</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="a9a9" class="lf lg iq lb b gy lh li l lj lk">%%time</span><span id="504c" class="lf lg iq lb b gy ll li l lj lk"># step 1:<br/>list_of_string_lens = [len(s) for s in list_of_strings]<br/>list_of_string_lens = zip(list_of_strings, list_of_string_lens)</span><span id="e34c" class="lf lg iq lb b gy ll li l lj lk">#step 2:<br/>max_len = max(list_of_string_lens, key=lambda t: t[1])<br/>print(max_len)</span><span id="af4b" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>('python', 6)<br/>CPU times: user 51.6 s, sys: 804 ms, total: 52.4 s<br/>Wall time: 52.4 s</span></pre><p id="17a8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">(我正在计算字符串的长度，然后<code class="fe lm ln lo lb b">zip</code>将它们放在一起，因为这比在一行中计算并复制字符串列表要快得多)</p><p id="afc7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这种状态下，代码实际上比以前运行得更慢，因为我们不是对所有的字符串执行一次，而是执行两次，首先计算<code class="fe lm ln lo lb b">len</code>，然后找到<code class="fe lm ln lo lb b">max</code>的值。为什么这对我们有好处？因为现在我们的“步骤 2”得到的输入不是原始的字符串列表，而是一些预处理过的数据。这允许我们使用另一个“步骤二”的输出来执行步骤二！我们稍后会更好地理解这一点，但首先，让我们给这些步骤起个名字。我们称“第一步”为“映射器”,因为它将一些值映射到另一些值，我们称“第二步”为缩减器，因为它得到一个值列表并产生一个值(在大多数情况下)。以下是映射器和缩减器的两个辅助函数:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="9ddc" class="lf lg iq lb b gy lh li l lj lk">mapper = len</span><span id="3259" class="lf lg iq lb b gy ll li l lj lk">def reducer(p, c):<br/>    if p[1] &gt; c[1]:<br/>        return p<br/>    return c</span></pre><p id="71b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">映射器就是<code class="fe lm ln lo lb b">len</code>函数。它获取一个字符串并返回它的长度。缩减器获取两个元组作为输入，并返回长度最大的一个。</p><p id="8313" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们使用<code class="fe lm ln lo lb b">map</code>和<code class="fe lm ln lo lb b">reduce</code>重写我们的代码，在 python 中甚至有内置函数(在 python 3 中，我们必须从<code class="fe lm ln lo lb b">functools</code>导入它)。</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="eee7" class="lf lg iq lb b gy lh li l lj lk">%%time</span><span id="925d" class="lf lg iq lb b gy ll li l lj lk">#step 1<br/>mapped = map(mapper, list_of_strings)<br/>mapped = zip(list_of_strings, mapped)</span><span id="a3eb" class="lf lg iq lb b gy ll li l lj lk">#step 2:<br/>reduced = reduce(reducer, mapped)</span><span id="77a5" class="lf lg iq lb b gy ll li l lj lk">print(reduced)</span><span id="464a" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>('python', 6)<br/>CPU times: user 57.9 s, sys: 0 ns, total: 57.9 s<br/>Wall time: 57.9 s</span></pre><p id="c2d4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">代码做完全相同的事情，它看起来有点花哨，但它更通用，将帮助我们并行化。让我们更仔细地看看它:</p><p id="14af" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">步骤 1 使用 mapper 函数将我们的字符串列表映射到元组列表中(这里我再次使用<code class="fe lm ln lo lb b">zip</code>以避免重复字符串)。</p><p id="0517" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">步骤 2 使用 reducer 函数，检查来自步骤 1 的元组并逐个应用它。结果是一个具有最大长度的元组。</p><p id="3666" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，让我们将输入分成块，并在进行任何并行化之前理解它是如何工作的(我们将使用<code class="fe lm ln lo lb b">chunkify</code>将一个大列表分成大小相等的块):</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="c63b" class="lf lg iq lb b gy lh li l lj lk">data_chunks = chunkify(list_of_strings, number_of_chunks=30)</span><span id="3688" class="lf lg iq lb b gy ll li l lj lk">#step 1:<br/>reduced_all = []<br/>for chunk in data_chunks:<br/>    mapped_chunk = map(mapper, chunk)<br/>    mapped_chunk = zip(chunk, mapped_chunk)<br/>    <br/>    reduced_chunk = reduce(reducer, mapped_chunk)<br/>    reduced_all.append(reduced_chunk)<br/>    <br/>#step 2:<br/>reduced = reduce(reducer, reduced_all)</span><span id="d885" class="lf lg iq lb b gy ll li l lj lk">print(reduced)</span><span id="873b" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>('python', 6)</span></pre><p id="9311" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第一步中，我们检查我们的块，并使用 map 和 reduce 找到该块中最长的字符串。在第二步中，我们获取第一步的输出，这是一个缩减值的列表，并执行最终的缩减以获得最长的字符串。我们使用<code class="fe lm ln lo lb b">number_of_chunks=36</code>是因为这是我机器上的 CPU 数量。</p><p id="f4a1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们几乎可以并行运行我们的代码了。我们唯一可以做得更好的是将第一个<code class="fe lm ln lo lb b">reduce</code>步骤添加到单个映射器中。我们这样做是因为我们想把我们的代码分成两个简单的步骤，因为第一个<code class="fe lm ln lo lb b">reduce</code>在单个块上工作，我们也想把它并行化。它看起来是这样的:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="f26f" class="lf lg iq lb b gy lh li l lj lk">def chunks_mapper(chunk):<br/>    mapped_chunk = map(mapper, chunk) <br/>    mapped_chunk = zip(chunk, mapped_chunk)<br/>    return reduce(reducer, mapped_chunk)</span><span id="c58a" class="lf lg iq lb b gy ll li l lj lk">%%time</span><span id="15b3" class="lf lg iq lb b gy ll li l lj lk">data_chunks = chunkify(list_of_strings, number_of_chunks=30)</span><span id="aae5" class="lf lg iq lb b gy ll li l lj lk">#step 1:<br/>mapped = map(chunks_mapper, data_chunks)</span><span id="8400" class="lf lg iq lb b gy ll li l lj lk">#step 2:<br/>reduced = reduce(reducer, mapped)</span><span id="b53b" class="lf lg iq lb b gy ll li l lj lk">print(reduced)</span><span id="16c0" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>('python', 6)<br/>CPU times: user 58.5 s, sys: 968 ms, total: 59.5 s<br/>Wall time: 59.5 s</span></pre><p id="e062" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在我们有了一个好看的两步代码。如果我们按原样执行它，我们将获得相同的计算时间，但是，现在我们可以通过使用<code class="fe lm ln lo lb b">pool.map</code>函数而不是常规的<code class="fe lm ln lo lb b">map</code>函数，使用<code class="fe lm ln lo lb b">multiprocessing</code>模块来并行化步骤 1:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="c5a7" class="lf lg iq lb b gy lh li l lj lk">from multiprocessing import Pool</span><span id="296a" class="lf lg iq lb b gy ll li l lj lk">pool = Pool(8)</span><span id="99f0" class="lf lg iq lb b gy ll li l lj lk">data_chunks = chunkify(large_list_of_strings, number_of_chunks=8)</span><span id="3c64" class="lf lg iq lb b gy ll li l lj lk">#step 1:<br/>mapped = pool.map(mapper, data_chunks)</span><span id="29fe" class="lf lg iq lb b gy ll li l lj lk">#step 2:<br/>reduced = reduce(reducer, mapped)</span><span id="c904" class="lf lg iq lb b gy ll li l lj lk">print(reduced)</span><span id="1d00" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>('python', 6)<br/>CPU times: user 7.74 s, sys: 1.46 s, total: 9.2 s<br/>Wall time: 10.8 s</span></pre><p id="1d3b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以看到它运行速度快了两倍！这不是一个巨大的改进，但好消息是我们可以通过增加进程的数量来改进它！我们甚至可以在多台机器上进行，如果我们的数据非常大，我们可以使用几十台甚至几千台机器，使我们的计算时间尽可能短(几乎)。</p><p id="c482" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们的架构是使用两个函数构建的:<code class="fe lm ln lo lb b">map</code>和<code class="fe lm ln lo lb b">reduce</code>。每个计算单元映射输入数据并执行初始归约。最后，某个集中式单元执行最终的归约并返回输出。看起来是这样的:</p><figure class="kw kx ky kz gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lp"><img src="../Images/95835abc8b788a5e22b8a7fb0d934667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOdiH_em5zWcZivW7ZCYxw.png"/></div></div></figure><p id="bf55" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种架构有两个重要优势:</p><ol class=""><li id="eff0" class="lq lr iq ka b kb kc kf kg kj ls kn lt kr lu kv lv lw lx ly bi translated">它是可扩展的:如果我们有更多的数据，我们唯一需要做的就是添加更多的处理单元。不需要更改代码！</li><li id="5118" class="lq lr iq ka b kb lz kf ma kj mb kn mc kr md kv lv lw lx ly bi translated">它是通用的:这种架构支持各种各样的任务，我们可以用几乎任何东西代替我们的<code class="fe lm ln lo lb b">map</code>和<code class="fe lm ln lo lb b">reduce</code>函数，这样就可以以可扩展的方式计算许多不同的东西。</li></ol><p id="ba35" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">需要注意的是，在大多数情况下，我们的数据会非常大并且是静态的。这意味着每次分割成块是低效的，实际上是多余的。因此，在现实生活中的大多数应用程序中，我们从一开始就将数据存储在块(或碎片)中。然后，我们将能够使用 MapReduce 技术进行不同的计算。</p><p id="fac2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在来看一个更有趣的例子:字数！</p><p id="32d1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设我们有一个非常大的新闻文章集，我们想找到不包括停用词的前 10 个常用词，我们该如何做呢？首先，让我们得到数据:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="2f18" class="lf lg iq lb b gy lh li l lj lk">from sklearn.datasets import fetch_20newsgroups<br/>data = news.data*10</span></pre><p id="891d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这篇文章中，我把数据放大了 10 倍，这样我们就可以看到不同之处。</p><p id="aa92" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于数据集中的每个文本，我们希望对其进行标记化、清理、删除停用词，并最终计算单词数:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="798c" class="lf lg iq lb b gy lh li l lj lk">def clean_word(word):<br/>    return re.sub(r'[^\w\s]','',word).lower()</span><span id="9124" class="lf lg iq lb b gy ll li l lj lk">def word_not_in_stopwords(word):<br/>    return word not in ENGLISH_STOP_WORDS and word and word.isalpha()<br/>    <br/>    <br/>def find_top_words(data):<br/>    cnt = Counter()<br/>    for text in data:<br/>        tokens_in_text = text.split()<br/>        tokens_in_text = map(clean_word, tokens_in_text)<br/>        tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)<br/>        cnt.update(tokens_in_text)<br/>        <br/>    return cnt.most_common(10)</span></pre><p id="b3d5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看看不使用 MapReduce 需要多长时间:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="9034" class="lf lg iq lb b gy lh li l lj lk">%time find_top_words(data)</span><span id="0df2" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:</span><span id="9c23" class="lf lg iq lb b gy ll li l lj lk">[('subject', 122520),<br/> ('lines', 118240),<br/> ('organization', 111850),<br/> ('writes', 78360),<br/> ('article', 67540),<br/> ('people', 58320),<br/> ('dont', 58130),<br/> ('like', 57570),<br/> ('just', 55790),<br/> ('university', 55440)]</span><span id="37aa" class="lf lg iq lb b gy ll li l lj lk">CPU times: user 51.7 s, sys: 0 ns, total: 51.7 s<br/>Wall time: 51.7 s</span></pre><p id="9cc1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，让我们写出我们的<code class="fe lm ln lo lb b">mapper</code>、<code class="fe lm ln lo lb b">reducer</code>和<code class="fe lm ln lo lb b">chunk_mapper</code>:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="95db" class="lf lg iq lb b gy lh li l lj lk">def mapper(text):<br/>    tokens_in_text = text.split()<br/>    tokens_in_text = map(clean_word, tokens_in_text)<br/>    tokens_in_text = filter(word_not_in_stopwords, tokens_in_text)<br/>    return Counter(tokens_in_text)</span><span id="23f8" class="lf lg iq lb b gy ll li l lj lk">def reducer(cnt1, cnt2):<br/>    cnt1.update(cnt2)<br/>    return cnt1</span><span id="a051" class="lf lg iq lb b gy ll li l lj lk">def chunk_mapper(chunk):<br/>    mapped = map(mapper, chunk)<br/>    reduced = reduce(reducer, mapped)<br/>    return reduced</span></pre><p id="3f4d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><code class="fe lm ln lo lb b">mapper</code>获取一个文本，将它分割成记号，清理它们，过滤无用的单词和非单词，最后，它计算这个文本文档中的单词数。<code class="fe lm ln lo lb b">reducer</code>函数获取 2 个计数器并将它们合并。<code class="fe lm ln lo lb b">chunk_mapper</code>得到一个块，并对其进行 MapReduce 操作。现在，让我们使用我们构建的框架来运行它，看看:</p><pre class="kw kx ky kz gt la lb lc ld aw le bi"><span id="3dfc" class="lf lg iq lb b gy lh li l lj lk">%%time</span><span id="37b6" class="lf lg iq lb b gy ll li l lj lk">data_chunks = chunkify(data, number_of_chunks=36)</span><span id="b13d" class="lf lg iq lb b gy ll li l lj lk">#step 1:<br/>mapped = pool.map(chunk_mapper, data_chunks)</span><span id="dd60" class="lf lg iq lb b gy ll li l lj lk">#step 2:<br/>reduced = reduce(reducer, mapped)</span><span id="a979" class="lf lg iq lb b gy ll li l lj lk">print(reduced.most_common(10))</span><span id="114b" class="lf lg iq lb b gy ll li l lj lk">OUTPUT:<br/>[('subject', 122520),<br/> ('lines', 118240),<br/> ('organization', 111850),<br/> ('writes', 78360),<br/> ('article', 67540),<br/> ('people', 58320),<br/> ('dont', 58130),<br/> ('like', 57570),<br/> ('just', 55790),<br/> ('university', 55440)]</span><span id="e017" class="lf lg iq lb b gy ll li l lj lk">CPU times: user 1.52 s, sys: 256 ms, total: 1.77 s<br/>Wall time: <strong class="lb ir">4.67 s</strong></span></pre><p id="6a5d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这快了 10 倍！在这里，我们能够真正利用我们的计算能力，因为任务要复杂得多，需要更多。</p><p id="43e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">综上所述，MapReduce 是一种令人兴奋的、对于大型数据处理来说必不可少的技术。它可以处理大量的任务，包括计数、搜索、监督和非监督学习等等。今天有很多实现和工具可以让我们的生活更加舒适，但我认为理解基础知识非常重要。</p></div></div>    
</body>
</html>