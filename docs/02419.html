<html>
<head>
<title>Review: Suggestive Annotation — Deep Active Learning Framework (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:提示性注释—深度主动学习框架(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6?source=collection_archive---------15-----------------------#2019-04-20">https://towardsdatascience.com/review-suggestive-annotation-deep-active-learning-framework-biomedical-image-segmentation-e08e4b931ea6?source=collection_archive---------15-----------------------#2019-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0b80" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">减少生物医学专家(如放射技师)的注释工作和成本</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/cb1f22c2244227de7be46c844c070758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2g925cNbDAduuSzb4xkq4A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Glands Segmentation in Colon Histology Images (Left) &amp; Lymph Nodes Segmentation in Ultrasound Images (Right)</strong></figcaption></figure><p id="bf68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这个故事中，<strong class="ky ir">暗示性的注解(SA) </strong>被回顾。例如，结肠癌和淋巴结癌(淋巴瘤)是导致死亡的两种常见癌症。<strong class="ky ir">精确分割对于了解注释对象的大小/形状至关重要，例如用于诊断或癌症分级/分期</strong>。传统上，<strong class="ky ir">注释医学图像</strong>，需要生物医学领域的专家。<strong class="ky ir">需要很高的努力和成本。</strong></p><p id="42ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于标注代价昂贵，因此<strong class="ky ir">深度主动学习框架</strong>被应用于生物医学领域，以便<strong class="ky ir">用较少的标注样本训练深度神经网络</strong>。而这是一篇<strong class="ky ir"> 2017 MICCAI </strong>超过<strong class="ky ir"> 40 次引用</strong>的论文。(<a class="mb mc ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----e08e4b931ea6--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="b8f8" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">概述</h1><ol class=""><li id="6982" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr nj nk nl nm bi translated"><strong class="ky ir">生物医学影像专家标注的问题</strong></li><li id="73d9" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">什么是主动学习？</li><li id="a9da" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">从人工标注到深度主动学习框架</strong></li><li id="8dd8" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">提出使用提示性标注的深度主动学习框架</strong></li><li id="d984" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated"><strong class="ky ir">结果</strong></li></ol></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="80b1" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak"> 1。生物医学成像中的专家注释问题</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/a1bdcfb72a7cb1b6f3b28299d2cddc5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZhWap-kYPz48W2xIbfLFw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Annotation in Biomedical Imaging by Experts</strong></figcaption></figure><ul class=""><li id="1481" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated"><strong class="ky ir">只有经过培训的生物医学专家才能对数据进行注释</strong>。</li><li id="19ba" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">大量的人工工作(<strong class="ky ir">时间&amp;成本</strong>)。</li><li id="c8fb" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">人为失误</strong>。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="237c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated"><strong class="ak"> 2。什么是主动学习？</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1110f497657f30db5c0beefaa4af7788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*mfFS-kaz5LwUJ-0fSiRUfw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Active Learning</strong></figcaption></figure><ul class=""><li id="a869" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">注释/标记是一项昂贵的活动，尤其是在生物医学领域。</li><li id="b4c9" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">主动学习的建议</strong>来自 2010 年的一份技术报告，“<a class="ae ny" href="http://burrsettles.com/pub/settles.activelearning.pdf" rel="noopener ugc nofollow" target="_blank">主动学习文献调查</a>”，引用超过 3000 条。</li><li id="116d" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">如上图所示，<strong class="ky ir">人工标注来自未标注库</strong>的一些样本，<strong class="ky ir">输入这些标注样本用于训练</strong>。</li><li id="5188" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">在训练之后，<strong class="ky ir">机器学习模型输出一些具有高度不确定性的样本，回到未标记池。</strong></li><li id="3960" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">因此，<strong class="ky ir">人类可以避免注释那些由机器学习模型预测的高确定性</strong>的样本，从而节省人类注释者的努力和成本。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="776c" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">3.<strong class="ak">从人工标注到深度主动学习框架</strong></h1><h2 id="bd40" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">3.1.雇用年轻人做注解</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/8acee2bc38a2b79b75f0b4d1f3cf70bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4j6BcutcONEllF5R7cXvQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Employ Junior for Annotation</strong></figcaption></figure><ul class=""><li id="d0b7" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">如上所示，类似于前一节中的主动学习框架，训练有素的初级标注来自未标注样本池的样本。</li><li id="a453" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">然后选择不确定性高的，请他/她的学长，也就是专家来批注。</li><li id="3b1f" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">有了专家的注解，初级可以学到更多，成为一个更好训练的初级。</li><li id="d066" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">通过更好的学习/训练，受过训练的初级应该具有更高的标注能力来标注来自池中的剩余未标注样本。</li><li id="76ae" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">有了上述框架，我们可以节省专家的精力和成本。</strong></li></ul><h2 id="09a4" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">3.2.雇用更多的初级人员进行注释</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/9b877c910b05425a4f4255c2639d4ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1keGBOIhCaPrfNYIbF_gvw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Employ More Juniors for Annotation</strong></figcaption></figure><ul class=""><li id="1189" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">为了加快注释的速度，我们可以雇用更多的初级人员来完成注释任务。</li><li id="f0b8" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">只有那些在所有训练好的初级中不确定的样本，才被送到专家那里进行标注。</strong></li><li id="da48" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">因此，我们可以进一步节省专家的精力和成本。</li></ul><h2 id="5713" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">3.3.FCN 替换青年队</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/9dfe02f4e233efd5c20a9ee2701f4c12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObSSSiQfckcu6009YC5idg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">FCN Replacing Juniors</strong></figcaption></figure><ul class=""><li id="ad7d" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">为了更加自动化，<strong class="ky ir">全卷积网络(fcn)将取代人</strong>。</li><li id="a2f9" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">现在，它变成了一个主动的学习框架。借助深度学习，本文称之为<strong class="ky ir">深度主动学习框架</strong>。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="e193" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">4.使用建议性注释的深度主动学习框架</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/effb96610bad13ba5d17d6482cbe8cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SD03WWneA6e6zj5bNmzcGw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Deep Active Learning Framework</strong></figcaption></figure><ul class=""><li id="d23a" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">如上图所示有三个主要部分:<strong class="ky ir"> FCN 架构</strong>、<strong class="ky ir">不确定性度量</strong>和<strong class="ky ir">相似性估计</strong>。</li></ul><h2 id="b7aa" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">4.1.FCN 建筑</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/304c0da40b9652dd86bf1b2537324792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a5yiR1sjlwX70vtzdZBDPw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">FCN Architecture</strong></figcaption></figure><ul class=""><li id="2e41" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">输入:<strong class="ky ir">未标注图像</strong></li><li id="a35c" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">输出:<strong class="ky ir">标注标签图(</strong>我们想要的)和<strong class="ky ir"> 1024-d 图像描述符</strong>，用于测量不确定度。</li><li id="2c18" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">所使用的架构是一个使用残差块的<a class="ae ny" href="http://FCN" rel="noopener ugc nofollow" target="_blank"> FCN </a>式架构。</li><li id="c8ec" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">使用自举(带替换的采样)；这样每个 FCN 将有不同的训练数据</li><li id="c16b" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">简单情况</strong> : <strong class="ky ir"> </strong>多个 fcn 会得出<strong class="ky ir">相似的输出</strong></li><li id="f7ae" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">疑难案例</strong>:多个 fcn 会有<strong class="ky ir">不同的输出</strong></li><li id="e3b9" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">由于使用了 4 个 NVidia Tesla P100 GPU，SA 中使用了</strong>4 个 fcn。</li></ul><h2 id="640e" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">4.2.不确定性度量</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/afb8a0be5d927e9d814eb0f3a384526f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMZKHGyhY-Mc5mTS25gV8A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Uncertainty Measure</strong></figcaption></figure><ul class=""><li id="f25a" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated"><strong class="ky ir">当一个像素的不确定性(标准偏差)较低时，该像素的精度较高</strong>，反之亦然。</li><li id="1807" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">为了测量图像的不确定性，使用所有像素的平均不确定性。</strong></li></ul><h2 id="3ca0" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">4.3.相似性估计</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/37421d7449d459ed0366ab5aea1dac4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJHnkPvwndLTP3mMvEWItA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e54eff97a4e5c5f1ace31bc7ceb5479a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*hahIpKKc-C19wHWMBNiYsA.png"/></div></figure><ul class=""><li id="ff2d" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">如前所述，还有另一个输出，1024-d 图像描述符。<strong class="ky ir">该描述符包含丰富而准确的形状信息</strong>。</li><li id="4962" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><strong class="ky ir">余弦相似度</strong>用于相似度估计。</li></ul><h2 id="3ee6" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">4.4.暗示性注释</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/5c74fa028f4df1633f442504932b9d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vejx1hMO8sWaxBRPCUHTw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Suggestive Annotation (SA)</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/6ccf2225f76a225b317cd5a9264ab1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*WRg6yf1nQT2zpqjk0MLhVw.png"/></div></figure><ul class=""><li id="c484" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">在所有未标注的图像<em class="ot">苏</em>中，我们使用不确定性度量<strong class="ky ir">选择 K 个不确定性得分最高的图像<em class="ot"> K </em>作为<em class="ot">Sc</em>T9(<strong class="ky ir">T11】K= 16</strong>)。</strong></li><li id="3384" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">因此，我们<strong class="ky ir">选择了<em class="ot"> K 个</em>图像，fcn 具有不同的输出</strong>。</li><li id="84fc" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">在这些<em class="ot"> K </em>图像中，使用贪婪方法找到<em class="ot"> Sa </em>(我们想建议专家注释的一组)。</li><li id="a9e2" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">最初<em class="ot"> Sa </em>是空集，即<em class="ot"> Sa </em>和<em class="ot"> F </em> ( <em class="ot"> Sa </em>，<em class="ot"> Su </em> ) = 0。</li><li id="77d0" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">迭代添加<em class="ot">Ii</em>∑<em class="ot">Sc</em>，最大化<em class="ot">F</em>(<em class="ot">Sa</em>∩<em class="ot">Ii</em>，<em class="ot"> Su </em>)直到<em class="ot"> Sa </em>包含<em class="ot"> k </em>图像(<em class="ot"> k </em> =8)</li><li id="a4ef" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">因此，选择一组具有不确定输出的图像<em class="ot"> Sa </em>，但也类似于未标注的图像。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="b6d9" class="mk ml iq bd mm mn mo mp mq mr ms mt mu jw mv jx mw jz mx ka my kc mz kd na nb bi translated">5.结果</h1><h2 id="d594" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">5.1.2015 MICCAI 腺体挑战数据集</h2><ul class=""><li id="c15b" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr nw nk nl nm bi translated">85 幅训练图像</li><li id="b047" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated">80 张测试图像，其中 60 张在 A 部分(正常腺体)，20 张在 B 部分(异常腺体)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/9134c6aeac78a691926506934556b543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mpefm_WDlkQWcXbdNaIS3w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Comparison with full training data for gland segmentation</strong></figcaption></figure><ul class=""><li id="0b30" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">当使用 100%训练数据时，SA(我们的方法)优于<a class="ae ny" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>和<a class="ae ny" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener"> CUMedVision2 / DCAN </a>，证明了 FCN 架构的有效性。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/67ad55f5e68d174ae2c979fee2a6063b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1f5iG2HDYV3SitZkY8jvTg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Comparison using limited training data for gland segmentation</strong></figcaption></figure><ul class=""><li id="7712" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">当使用 50%的训练数据时，它已经接近甚至优于 SOTA(绿色)结果。</li></ul><h2 id="8183" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">5.2.淋巴结数据集</h2><ul class=""><li id="f673" class="nc nd iq ky b kz ne lc nf lf ng lj nh ln ni lr nw nk nl nm bi translated">37 幅训练图像和 37 幅测试图像</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/6c78067f3f0edb6c3f93b7ea381f0c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*How3z9FW09BOZhoRmEf0tw.png"/></div></div></figure><ul class=""><li id="ba33" class="nc nd iq ky b kz la lc ld lf nt lj nu ln nv lr nw nk nl nm bi translated">仅用 50%的训练数据，该框架就能比<a class="ae ny" href="http://U-Net" rel="noopener ugc nofollow" target="_blank"> U-Net </a>、<a class="ae ny" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener"> CUMedVision1 </a>和<a class="ae ny" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener"> CFS-FCN </a>有更好的分割性能。</li><li id="38a5" class="nc nd iq ky b kz nn lc no lf np lj nq ln nr lr nw nk nl nm bi translated"><a class="ae ny" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener"> CFS-FCN </a>需要对中间标签图进行额外的标记工作，这些标签图可以被视为 200%的训练数据。</li></ul></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="f56a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用暗示性标注的深度主动学习框架，有助于提高小数据集的预测精度。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h2 id="b3e9" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">参考</h2><p id="fa5a" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ox lh li lj oy ll lm ln oz lp lq lr ij bi translated">【2017 MICCAI】【SA】<br/><a class="ae ny" href="https://arxiv.org/abs/1706.04737" rel="noopener ugc nofollow" target="_blank">提示性标注:生物医学图像分割的深度主动学习框架</a></p><h2 id="5a1f" class="nz ml iq bd mm oa ob dn mq oc od dp mu lf oe of mw lj og oh my ln oi oj na ok bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ox lh li lj oy ll lm ln oz lp lq lr ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测<br/></strong><a class="ae ny" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae ny" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae ny" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae ny" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae ny" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae ny" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae ny" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae ny" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae ny" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae ny" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae ny" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae ny" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae ny" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae ny" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae ny" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">语义切分<br/></strong><a class="ae ny" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae ny" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae ny" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae ny" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae ny" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae ny" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae ny" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae ny" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae ny" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae ny" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物医学图像分割<br/></strong>[<a class="ae ny" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae ny" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae ny" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae ny" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae ny" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN</a><a class="ae ny" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"/></p><p id="3134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实例分割<br/> </strong> [ <a class="ae ny" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例化</a> ] [ <a class="ae ny" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"/><br/><a class="ae ny" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae ny" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae ny" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>