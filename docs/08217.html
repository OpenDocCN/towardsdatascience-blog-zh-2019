<html>
<head>
<title>Paper review: DenseNet -Densely Connected Convolutional Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文综述:dense net-密集连接的卷积网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-review-densenet-densely-connected-convolutional-networks-acf9065dfefb?source=collection_archive---------4-----------------------#2019-11-10">https://towardsdatascience.com/paper-review-densenet-densely-connected-convolutional-networks-acf9065dfefb?source=collection_archive---------4-----------------------#2019-11-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="70ef" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">CVPR 2017，<strong class="ak">最佳论文奖</strong>获得者</h2></div><p id="3cba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们进入 DenseNets 之前，这里有一本免费的<strong class="kh ir">实用自然语言处理</strong>书籍，你可能会喜欢</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">More information in video description.</figcaption></figure><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/e60e2423e447935996d88a69517ac10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*TeHVqikNc68QC98Vm9M98Q.gif"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Dense connections</figcaption></figure><blockquote class="lt"><p id="59c8" class="lu lv iq bd lw lx ly lz ma mb mc la dk translated"><strong class="ak">“简单的模型和大量的数据胜过基于更少数据的更复杂的模型。”彼得·诺维格</strong></p></blockquote><h1 id="f12c" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">关于报纸</h1><p id="1177" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated"><strong class="kh ir">密集连接的卷积网络</strong>在 IEEE 计算机视觉和模式识别大会上获得<strong class="kh ir">最佳论文奖</strong>(<strong class="kh ir">CVPR</strong>)<strong class="kh ir">2017</strong>。论文可以在这里<a class="ae na" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank">阅读</a>。</p><p id="ee65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一作者，<a class="ae na" href="http://www.gaohuang.net/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">黄高</strong> </a>曾在康乃尔大学做博士后，目前在清华大学做助理教授。他的研究重点是计算机视觉的深度学习。</p><h1 id="1516" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">我是如何看到这张纸的？</h1><p id="17f9" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我在研究专注于通过重建提高图像质量(在分辨率或动态范围方面)的神经网络实现时，看到了这篇论文。虽然这篇论文展示了该体系结构在图像分类方面的能力，但密集连接的思想已经激发了许多其他深度学习领域的优化，如图像超分辨率、图像分割、医疗诊断等。</p><h1 id="4145" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">DenseNet 架构的主要贡献</h1><ul class=""><li id="38ad" class="ne nf iq kh b ki mv kl mw ko ng ks nh kw ni la nj nk nl nm bi translated">缓解<strong class="kh ir">消失渐变问题</strong></li><li id="b684" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">更强的<strong class="kh ir">特征传播</strong></li><li id="cd8e" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated"><strong class="kh ir">特征重用</strong></li><li id="45e8" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated"><strong class="kh ir">减少参数</strong>计数</li></ul><blockquote class="ns nt nu"><p id="57f1" class="kf kg nv kh b ki kj jr kk kl km ju kn nw kp kq kr nx kt ku kv ny kx ky kz la ij bi translated"><strong class="kh ir">阅读前:</strong> <br/>理解这篇帖子需要对深度学习概念有一个基本的了解。</p></blockquote><h1 id="6347" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">论文评论</h1><p id="3e7c" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">本文首先讨论<strong class="kh ir">消失梯度问题，即</strong>随着网络越来越深，梯度如何不能充分地反向传播到网络的初始层。随着梯度向后移动到网络中，梯度变得越来越小，结果，初始层失去了学习基本低级特征的能力。</p><p id="4716" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">已经开发了几种架构来解决这个问题。这些包括—资源网、高速公路网、分形网、随机深度网。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi nz"><img src="../Images/11e7301ceba8bc35a697f07c858f5e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwxLN83uT8bgd4nGiR6JZQ.jpeg"/></div></div></figure><p id="717b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不管这些网络的架构设计如何，它们都试图为信息在初始层和最终层之间的流动创建通道。出于同样的目的，DenseNets 在网络的各层之间创建路径。</p><h1 id="6f41" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">相关作品</h1><ul class=""><li id="a042" class="ne nf iq kh b ki mv kl mw ko ng ks nh kw ni la nj nk nl nm bi translated">高速公路网络(使更深层次模型的训练变得容易的最初尝试之一)</li><li id="e6d5" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">ResNet(通过使用标识映射求和来绕过连接)</li><li id="4c93" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">随机深度(在训练期间随机丢弃层)</li><li id="849e" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">GoogLeNet(初始模块——增加网络宽度)</li><li id="de97" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">分形网络</li><li id="2958" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">网络中的网络</li><li id="5bac" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">深度监督网络</li><li id="fc3b" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">梯形网络</li><li id="4084" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated">深度融合网络</li></ul><h1 id="a130" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">密集连接</h1><p id="1039" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">根据网络的前馈特性，密集块中的每一层都从所有前面的层接收要素地图，并将其输出传递给所有后面的层。从其他层接收的特征地图通过<strong class="kh ir">连接</strong>进行融合，而不是通过求和(如在 ResNets 中)。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0aaee349a141912076d1dcfa70d12f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*StSq7XyHcuxUOfWuuALkdw.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Concatenation of feature maps</figcaption></figure><p id="4a76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些连接形成了密集的路径回路，让<strong class="kh ir">有更好的梯度流动</strong>。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi lm"><img src="../Images/e60e2423e447935996d88a69517ac10a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*TeHVqikNc68QC98Vm9M98Q.gif"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Dense connections</figcaption></figure><blockquote class="ns nt nu"><p id="e983" class="kf kg nv kh b ki kj jr kk kl km ju kn nw kp kq kr nx kt ku kv ny kx ky kz la ij bi translated">每一层都可以直接访问损失函数的梯度和原始输入信号。</p></blockquote><p id="8abb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于这些密集的连接，该模型需要更少的层，因为不需要学习冗余的特征地图，允许重复使用<strong class="kh ir">集体知识</strong>(由网络集体学习的特征)。所提出的架构具有窄层，其为低至 12 个通道特征图提供了最先进的结果。更少更窄的层意味着模型需要学习的参数更少，更容易训练。作者还谈到了作为串联特征图结果的层输入变化的重要性，这防止了模型过度拟合训练数据。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ob"><img src="../Images/96f483a19bfd6101633b918af40f4185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04TJTANujOsauo3foe0zbw.jpeg"/></div></div></figure><p id="5694" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DenseNet 模型的许多变体已经在论文中提出。我选择用他们的标准网络(DenseNet-121)来解释这些概念。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi oc"><img src="../Images/990b69172fa7d09ca4d099e0f3f5863a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rkra9kVPl754-vjRGsOqvw.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Some of the variants of the DenseNet architecture</figcaption></figure><h1 id="7984" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">复合函数</h1><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4089bf18ebf5cf30ea165b8b12b1f6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*qA2rLVBRB-wZoI3nAEMdkA.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Composite function</figcaption></figure><p id="6afc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">*论文(和博客)中网络表述的每个 CONV 块对应于一个操作—</p><p id="1ee0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> BatchNorm→ReLU→Conv* </strong></p><h1 id="f17d" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">密集块</h1><p id="e556" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">密集连接的概念已经在密集块中描述。一个密集块包括<em class="nv"> n </em>个密集层。这些密集层使用密集电路连接，使得每个密集层接收来自所有先前层的特征地图，并将其特征地图传递给所有后续层。特征的尺寸(宽度、高度)在密集块中保持不变。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi oe"><img src="../Images/da9309f636bd42ceea67fce49ede94f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfrliiHwn_L4kKcO61Oxgw.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk"><strong class="bd of">Dense block (DB) with six Dense Layers (DL)</strong></figcaption></figure><h1 id="4d66" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">致密层</h1><p id="70b1" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">每个密集层由两个卷积运算组成</p><ul class=""><li id="43b4" class="ne nf iq kh b ki kj kl km ko og ks oh kw oi la nj nk nl nm bi translated"><strong class="kh ir">1×1 CONV</strong>(用于提取特征的常规 conv 操作)</li><li id="7a9e" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated"><strong class="kh ir"> 3 X 3 CONV </strong>(降低特征深度/通道数)</li></ul><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a03b691abb2758ff3d88d7a137b1ddb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*cRXqccOxYkZbWpfmyXzjog.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk"><strong class="bd of">Dense layer of DB-1</strong></figcaption></figure><p id="b2e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DenseNet-121 在致密块中包含 6 个这样的致密层。每个致密层的输出深度等于致密块体的生长速度。</p><h1 id="83c2" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">增长率(k)</h1><p id="e5b8" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">这是一个你会在报纸上经常遇到的术语。基本上就是一个密集层输出的通道数(<em class="nv"> 1x1 conv → 3x3 conv </em>)。作者在实验中使用了一个值<em class="nv"> k = 32 </em>。这意味着密集层(<em class="nv"> l </em>)从前一密集层(<em class="nv"> l-1 </em>)接收的特征数量是 32。这被称为增长率，因为在每一层之后，32 个通道特征被连接并作为输入馈送到下一层。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ok"><img src="../Images/0cb637eb03d13bb1d1de296c7121aab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NIQenf9KTillNhYfuySfMw.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Dense block with channel count (C) of features entering and exiting the layers</figcaption></figure><h1 id="3e45" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">转变层</h1><p id="214a" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">在每个密集块的末尾，特征图的数量累积到一个值— <em class="nv">输入特征+(密集层数 x 增长率)。</em>因此，对于进入增长率为 32 的 6 个密集层的密集块的 64 个通道特征，在块的末端累积的通道数将是—<br/>64+(6×32)= 256。为了减少这个通道数，在两个密集块之间增加了一个<strong class="kh ir">过渡层</strong>(或块)。过渡层包括-</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/440a0558841a2996637e291879d43338.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*LoFEV57u5kCjsqZRX7EAKw.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk"><strong class="bd of">Transition layer/block</strong></figcaption></figure><ul class=""><li id="49ad" class="ne nf iq kh b ki kj kl km ko og ks oh kw oi la nj nk nl nm bi translated"><strong class="kh ir"> 1 X 1 CONV </strong>操作</li><li id="12f7" class="ne nf iq kh b ki nn kl no ko np ks nq kw nr la nj nk nl nm bi translated"><strong class="kh ir">2×2 AVG 池</strong>操作</li></ul><p id="85fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1 X 1 CONV </strong>操作将通道数减半。<br/><strong class="kh ir">2 X 2 AVG 池</strong>图层负责根据宽度和高度对特征进行下采样。</p><h1 id="b34a" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">全网络</h1><p id="3793" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">从下图中可以看出，作者为三个密集区块中的每一个选择了不同数量的密集层。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi om"><img src="../Images/ae0f0f09a98a26805ead18bc16b0297a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CE11_lfEz00aoOjLiw5sdw.png"/></div></div><figcaption class="li lj gj gh gi lk ll bd b be z dk"><strong class="bd of">Full DenseNet architecture</strong></figcaption></figure><h1 id="7f4f" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">与 DenseNet 的比较</h1><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi on"><img src="../Images/bf9719ca16f2ed5ee7fb671d73f58948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*cXAoIC_ig5R8nE4hKj2OeQ.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">ResNet DenseNet comparison</figcaption></figure><p id="ec6a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，即使参数数量减少，DenseNet 模型的验证误差也比参数数量相同的 ResNet 模型低得多。这些实验是在具有更适合 ResNet 的超参数的两个模型上进行的。作者声称，DenseNet 在广泛的超参数搜索后会表现得更好。</p><blockquote class="lt"><p id="adb5" class="lu lv iq bd lw lx oo op oq or os la dk translated">具有 20M 参数的 DenseNet-201 模型与具有超过 40M 参数的 101 层 ResNet 产生类似的验证误差。</p></blockquote><h1 id="cfba" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">检查代码</h1><p id="4333" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我相信仔细阅读代码会更容易理解这种架构的实现。研究论文(在深度学习的背景下)可能很难理解，因为它们更多地是关于驱动神经网络设计决策的因素。检查代码(通常是网络/模型代码)可以降低这种复杂性，因为有时我们感兴趣的只是实现。有些人喜欢先看到实现，然后试图找出网络设计决策背后的原因。不管怎样，在之前或之后阅读代码总是有帮助的。</p><p id="8e2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DenseNet 实现的代码可以在这里找到。由于我对 PyTorch 更熟悉，我将试着解释这个模型的 PyTorch 实现，它可以在<a class="ae na" href="https://github.com/gpleiss/efficient_densenet_pytorch" rel="noopener ugc nofollow" target="_blank">这里</a>找到。最重要的文件是<strong class="kh ir"> models/densenet.py </strong>，它保存了 densenet 的网络架构。</p><p id="380c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码被分成这些类，其中每种类型的块由一个类表示。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/723143d970da3a8e5bd53dc4b8400bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*RgsliXFElnLQ5uh5ZCDBAg.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk">Class hierarchy in code</figcaption></figure><h1 id="be29" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">致密层</h1><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ou lh l"/></div></figure><p id="f9f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> _DenseLayer </strong>类可用于初始化密集层的组成操作——</p><p id="cb99" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">巴特诺姆→雷卢→ Conv (1X1) →巴特诺姆→雷卢→ Conv (3X3) </strong></p><p id="0d31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> _bn_function_factory() </strong>函数负责将前几层的输出连接到当前层。</p><h1 id="3eb1" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated"><strong class="ak"> DenseBlock </strong></h1><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ou lh l"/></div></figure><p id="2b5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">_ <strong class="kh ir"> DenseBlock </strong>类包含一定数量的<strong class="kh ir">_ dense layer</strong>(<em class="nv">num _ layers</em>)。<br/>该类由<strong class="kh ir"> DenseNet </strong>类初始化，取决于网络中使用的密集块的数量。</p><h1 id="9b95" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">过渡块</h1><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ou lh l"/></div></figure><h1 id="a533" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated"><strong class="ak"> DenseNet </strong></h1><p id="1448" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">由于这部分代码有点太大，不适合写在这篇博客里，所以我将只使用一部分代码，这将有助于理解网络的要点。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="ou lh l"/></div></figure><p id="76cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我在网上找到了这张图片，它帮助我更好地理解了网络。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="ab gu cl ov"><img src="../Images/f05dc6bdd91c8e96c30cb23efc797394.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vIZhPImFr9Gjpx6ZB7IOJg.png"/></div><figcaption class="li lj gj gh gi lk ll bd b be z dk"><a class="ae na" rel="noopener" target="_blank" href="/understanding-and-visualizing-densenets-7f688092391a">Source</a></figcaption></figure><h1 id="ebdb" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">受本文启发的其他作品</h1><ul class=""><li id="dd6c" class="ne nf iq kh b ki mv kl mw ko ng ks nh kw ni la nj nk nl nm bi translated"><a class="ae na" href="https://arxiv.org/abs/1802.08797" rel="noopener ugc nofollow" target="_blank">图像超分辨率残差密集网络</a> (2018)</li></ul><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="gh gi ow"><img src="../Images/ca285745dd7dd9ed4f4aff7af38a2b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6NS5NPZoU3iQXIJOu6jruQ.png"/></div></div></figure><ul class=""><li id="705b" class="ne nf iq kh b ki kj kl km ko og ks oh kw oi la nj nk nl nm bi translated"><a class="ae na" href="http://www.statnlp.org/research/lg/zhijiang_zhangyan19tacl-gcn.pdf" rel="noopener ugc nofollow" target="_blank">用于图形到序列学习的密集连接图形卷积网络</a> (2019)</li></ul><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/53cadea0298c444f9e6035d887222a0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*pa45Gjp8H1D9NnXJE6Vypw.png"/></div></figure><h1 id="1be5" class="md me iq bd mf mg mh mi mj mk ml mm mn jw nb jx mp jz nc ka mr kc nd kd mt mu bi translated">结论</h1><p id="75ab" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">DenseNet 是一个网络，它描述了在使用密集块的网络中拥有密集连接的重要性。这有助于特征重用、更好的梯度流、减少的参数计数和跨网络更好的特征传输。这样的实现可以帮助使用更少的计算资源和更好的结果来训练更深的网络。</p></div><div class="ab cl oy oz hu pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="ij ik il im in"><p id="4b33" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了了解更多关于机器学习和数据科学领域的信息，请查看并订阅<a class="ae na" href="https://www.youtube.com/channel/UC66w1T4oMv66Jn1LR5CW2yg" rel="noopener ugc nofollow" target="_blank">深度神经笔记本播客</a>，在这里，我采访了专家和从业者，了解他们的专业状态、他们迄今为止的旅程以及未来的道路。</p><figure class="lb lc ld le gt lf"><div class="bz fp l di"><div class="lg lh l"/></div></figure><p id="a793" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也可以在所有主要的播客流媒体平台上使用，包括<a class="ae na" href="https://podcasts.apple.com/in/podcast/deep-neural-notebooks/id1488705711" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">苹果播客</strong> </a>、<a class="ae na" href="https://open.spotify.com/show/2eq1jD7V5K19aZUUJnIz5z" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> Spotify </strong> </a>和<a class="ae na" href="https://podcasts.google.com/?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8xMDZkYzIzOC9wb2RjYXN0L3Jzcw&amp;episode=aHR0cHM6Ly9hbmNob3IuZm0vZGVlcC1uZXVyYWwtbm90ZWJvb2tzL2VwaXNvZGVzL0tub3dsZWRnZS0tVGVjaC0tTGVhZGVyc2hpcC0tS25vd2xlZGdlV2hhcnRvbi0tSW50ZXJ2aWV3LXdpdGgtTXVrdWwtUGFuZHlhLWVicXM0aw&amp;ved=0CAIQkfYCahcKEwjQx9n87bDoAhUAAAAAHQAAAAAQAQ" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">谷歌播客</strong> </a>。</p></div></div>    
</body>
</html>