<html>
<head>
<title>BERT at EMNLP 2019</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特在 EMNLP 2019</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-at-emnlp-2019-46db6c2e59b2?source=collection_archive---------34-----------------------#2019-12-02">https://towardsdatascience.com/bert-at-emnlp-2019-46db6c2e59b2?source=collection_archive---------34-----------------------#2019-12-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f290" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">ml 评论</h2><div class=""/><div class=""><h2 id="8133" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">EMNLP 2019 上与 BERT 相关的论文</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2b9a10c6acb0f42716b03bec3bf62c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zA4tMb0zlk_rJwCyIyluCA.jpeg"/></div></div></figure><p id="ee9b" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">自然语言处理经验方法会议(EMNLP)于 2019 年 11 月 3 日至 11 月 7 日在香港举行。有很多有趣的论文，但我想强调一下关于伯特的论文。</p><h2 id="3d53" class="lw lx iq bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn iw bi translated">揭露伯特的黑暗秘密</h2><p id="e862" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated"><a class="ae mt" href="http://arxiv.org/abs/1908.08593" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1908.08593</a></p><p id="f67f" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">在这篇论文中，来自马萨诸塞大学洛厄尔分校的研究人员研究了跨 BERT 层和头部的自我注意机制。为此使用了 GLUE 任务的子集:MRPC、STS-B、SST-2、QQP、RTE、QNLI、MNLI。</p><p id="7f86" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">实验:</p><ul class=""><li id="b51c" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">BERT 中的特定关系中心。</li><li id="3632" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">微调后自我注意模式的变化。</li><li id="9509" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">注意语言特征。</li><li id="f736" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">象征性的关注。</li><li id="caf8" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">禁用自我关注头。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ni"><img src="../Images/50349ed185af845c3377cfd3a0d7e5d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SfFrrOxZUYBOsZ2PnLsQrQ.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Typical self-attention classes used for training a neural network. Both axes on every image represent BERT tokens of an input example, and colors denote absolute attention weights (darker colors stand for greater weights). The first three types are most likely associated with language model pre-training, while the last two potentially encode semantic and syntactic information.</figcaption></figure><p id="2f28" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">有趣的发现:</p><p id="8670" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">伯特模型明显参数化过度。不同人的注意力模式是有限的。因此，禁用某些磁头不会导致精度下降，但会提高性能。</p><p id="65b6" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">很有意思。这就是为什么蒸馏伯特有意义。</p><h2 id="d479" class="lw lx iq bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn iw bi translated">可视化和理解 BERT 的有效性</h2><p id="4099" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">http://arxiv.org/abs/1908.05620<a class="ae mt" href="http://arxiv.org/abs/1908.05620" rel="noopener ugc nofollow" target="_blank"/></p><p id="335c" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">微软研究院的另一篇关于用酷炫的可视化理解 BERT 的表现的论文。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/059512bbf059affd4a4957f49d61aac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2eiXC0HG1-x1RO_cUxwQ3A.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Training loss surfaces of training from scratch (top) and fine-tuning BERT (bottom) on four datasets. Pre-training leads to wider optima, and eases optimization compared with random initialization.</figcaption></figure><p id="c4ef" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">上图清楚地展示了这篇论文的主要思想:</p><ul class=""><li id="0506" class="mu mv iq lc b ld le lg lh lj mw ln mx lr my lv mz na nb nc bi translated">微调 BERT 的训练损失趋于沿优化方向单调减少，这使得优化变得容易并加速了训练收敛。</li><li id="c615" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">微调过程对于过拟合是鲁棒的；</li><li id="5b30" class="mu mv iq lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">预训练模型获得更平坦、更宽的最优解；</li></ul><p id="281e" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">所以，不要为了你的任务而从零开始训练 BERT。微调比较好。</p><h2 id="3484" class="lw lx iq bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn iw bi translated">伯特模型压缩的病人知识蒸馏</h2><p id="3699" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated">【http://arxiv.org/abs/1908.09355 T4】</p><p id="5d6a" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">微软的另一篇论文是关于知识提炼的。提出了一种通过病人知识蒸馏将大型 BERT 模型压缩成浅层模型的新方法。这种方法声称是第一次不仅在蒸馏中使用了输出分布，而且还使用了“老师”的隐藏状态。此外,“学生”试图模仿的只是[CLS]表征。与其他蒸馏方法相比，伯特-PKD 法优于蒸馏法，但不如蒂尼伯特法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b210f914190e69a9be913ab86f30b788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KcZJMcKDJqhqe1jviK83-g.jpeg"/></div></div></figure><h2 id="4a00" class="lw lx iq bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn iw bi translated">句子伯特:使用连体伯特网络的句子嵌入</h2><p id="ee2b" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated"><a class="ae mt" href="http://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1908.10084</a></p><p id="f410" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">代号:<a class="ae mt" href="https://github.com/UKPLab/sentence-transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/UKPLab/sentence-transformers</a></p><p id="35cc" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">问题如下:来自 BERT 的嵌入适合语义相似性搜索吗？本文表明，BERT 开箱即用将句子映射到一个向量空间，该向量空间不太适合用于常见的相似性度量，如余弦相似性。该性能比普通手套嵌入的性能差。为了克服这个缺点，人们提出了句子-BERT (SBERT)。SBERT 在连体或三联体网络架构中微调 BERT。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3e595b0f8a94fa996426114318de1772.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*SivlUr_NgcbrJ_x4TkpOmQ.png"/></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">SBERT architecture with classification objective function, e.g., for fine-tuning on SNLI dataset. The two BERT networks have tied weights (siamese network structure).</figcaption></figure><h2 id="f334" class="lw lx iq bd ly lz ma dn mb mc md dp me lj mf mg mh ln mi mj mk lr ml mm mn iw bi translated">贝托，本茨，贝卡斯:伯特惊人的跨语言效果</h2><p id="cff6" class="pw-post-body-paragraph la lb iq lc b ld mo ka lf lg mp kd li lj mq ll lm ln mr lp lq lr ms lt lu lv ij bi translated"><a class="ae mt" href="http://arxiv.org/abs/1904.09077" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1904.09077</a></p><p id="96a8" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">本文探讨了多语言 BERT 作为零镜头语言迁移模型的跨语言潜力。</p><p id="70c9" class="pw-post-body-paragraph la lb iq lc b ld le ka lf lg lh kd li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">长话短说:BERT 有效地学习了一种良好的多语言表示，在各种任务中具有很强的跨语言零镜头迁移性能。</p></div></div>    
</body>
</html>