# 引入 k-最近邻

> 原文：<https://towardsdatascience.com/introducing-k-nearest-neighbors-7bcd10f938c5?source=collection_archive---------20----------------------->

## 理解这种分类模型的基础

![](img/a44c3ab56b58df10cf9e9b0daf1266e7.png)

k-最近邻算法基于一个非常简单的前提:距离很近的事物有很多共同点。这个前提全世界都可以看到:

*   大学生最亲密的邻居通常是其他大学生。
*   我们倾向于和我们的浪漫伴侣有更多的共同点，他们和我们共享一个家，而不是和隔壁城镇的人。
*   在杂货店里，每一种产品都储存在一组里，而不是分布在整个商店里。

k-最近邻建模方法使用这一前提来假设我们可以获取关于一个点的知识，并得出关于相邻点的结论。由于相似的事物通常聚集在一起，我们可以利用这种接近性对我们不知道的数据点做出假设。在杂货店里，苹果旁边的商品通常是另一个苹果。更具体地说，节日苹果旁边的物品通常是另一个节日苹果。人们的政治取向相似；随着越来越多的人聚集到政治上两极分化的城镇，如果我们知道一个城市中的一个人是保守派或自由派，我们可以合理地假设他们的邻居也是保守派或自由派。

因为它是基于这样的假设而不是复杂的统计模型，所以 k-最近邻是最简单的预测模型。

## k 近邻模型是如何工作的？

为了使用 k-最近邻算法，您需要两样东西:

*   **相信附近数据点彼此相似的假设**。请记住，这并不总是一个有用的假设，你必须小心应用它。例如，如果你想知道口香糖贩卖机中的一个物品是什么，那么知道它旁边的物品是口香糖是非常有用的。然而，如果你想知道相邻的口香糖球是什么颜色，那么知道一个口香糖球是红色是没有用的；每个容器中通常有大约 5 种颜色的口香糖混合在一起。
*   **两个数据点间距离的某种近似方法**。在最简单的情况下，以及我们到目前为止讨论的所有例子中，这是两个项目之间的实际距离。然而，使用 k-最近邻算法，这也可以是数据集中两点之间的理论距离。例如，如果你想预测不同家庭的卧室数量，你可以使用平方英尺作为你的数据。那么这个距离就是两个不同住宅之间的平方英尺的差异。

通过结合相似项被分组的假设和距离的度量，您可以创建一个算法，通过在数据集中搜索最近的邻居来预测未知事例的事实。一旦找到这些最近的邻居，您就可以得出结论，您的未知点与这些邻居具有相同的特征。

在 k-最近邻模型中，k 是用于定义如何应用该模型的变量。它说明了执行分析时要检查的相邻要素的数量。使用 k-最近邻算法时，您可以更改 k，这可能会产生显著不同的结果。通过尝试不同的值并测试模型的预测能力来选择 k 的值。这意味着您必须开发、验证和测试几个模型。更多信息可以在[如何在多个型号中选择](/how-to-choose-between-multiple-models-a0c274b4228a)中找到。

## 我怎样才能更好地了解这个算法？

k-最近邻算法是一种常见的分类工具，在互联网上可以找到关于该主题的大量信息。对于那些想要更多信息和高度可信来源的人，我强烈推荐[从头开始的数据科学:Python 的基本原理](https://www.amazon.com/gp/product/1492041130/ref=as_li_tl?ie=UTF8&camp=1789&creative=9325&creativeASIN=1492041130&linkCode=as2&tag=petergrantpub-20&linkId=fd514f3035abb73af2aefe53ba8aa613)。

## 关闭

k-最近邻算法是一种常见的分类模型，它基于这样的假设，即数据集中靠在一起的项目通常是相似的。为了使用它，你必须既相信那个假设，又有两个数据点之间距离的度量。然后，该模型使用距离计算来确定与您尝试预测的位置最近的 k 个数据点，并使用来自这些点的信息来得出结论。