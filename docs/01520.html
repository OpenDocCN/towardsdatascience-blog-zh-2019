<html>
<head>
<title>Rosenblatt’s perceptron, the first modern neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">罗森布拉特的感知机，第一个现代神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a?source=collection_archive---------6-----------------------#2019-03-11">https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a?source=collection_archive---------6-----------------------#2019-03-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6695" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">初学者深度学习快速入门。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1457e4485364b32bcc22166ebe42afa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3yza9S1OCTAANFXGkgYQg.jpeg"/></div></div></figure><p id="d1ca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">在过去的十年里，机器学习已经在许多领域产生了变革性的影响，如认知神经科学、图像分类、推荐系统或工程。最近，<em class="lz">神经网络</em>和<em class="lz">深度学习</em>吸引了更多的关注，它们的成功被科学和主流媒体定期报道，例如 Deep Mind 的 AlphaGo 和 AlphaGo Zero 或最近的<a class="ae ma" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" rel="noopener ugc nofollow" target="_blank"> AlphaStar </a>。这种新的兴趣部分是由于对开源库的访问，例如<a class="ae ma" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>、<a class="ae ma" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>、<a class="ae ma" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>或<a class="ae ma" href="https://github.com/FluxML/Flux.jl" rel="noopener ugc nofollow" target="_blank"> Flux.jl </a>等等。</p><p id="bc25" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然这种对高效和多功能库的更多访问通过减少实现深度学习算法所需的计算机科学知识，打开了创新应用的大门，但仍然需要对基础数学理论的良好理解，以提出用于所考虑任务的高效神经网络架构。不幸的是，社会对数学的形象可能会吓跑学生(参见纪录片<a class="ae ma" href="https://www.youtube.com/watch?time_continue=23&amp;v=jSt6Opt5nm4" rel="noopener ugc nofollow" target="_blank"> <em class="lz">我是如何开始讨厌数学的</em> </a> <em class="lz"> </em>以获得例证)。缺乏数学素养也可能是政治和非技术行业经常对深度学习的表现和能力持怀疑态度或过于乐观的原因之一。此外，<a class="mb mc ep" href="https://medium.com/u/a2db0221ff40?source=post_page-----37a3ec09038a--------------------------------" rel="noopener" target="_blank">苏珊娜·沙特克</a>最近发表了一篇文章，讨论为什么<a class="ae ma" rel="noopener" target="_blank" href="/people-dont-trust-ai-we-need-to-change-that-d1de5a4a0021">人们不信任人工智能</a>以及为什么业界可能不愿意采用它。她列举的一个关键原因(尽管不是唯一的原因)如下:</p><blockquote class="md me mf"><p id="85ac" class="ku kv lz kw b kx ky ju kz la lb jx lc mg le lf lg mh li lj lk mi lm ln lo lp im bi translated">在 IBM2018 年的一项研究中，63%的受访者认为缺乏技术技能是人工智能实施的障碍。</p></blockquote><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/people-dont-trust-ai-we-need-to-change-that-d1de5a4a0021"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">人们不信任人工智能。我们需要改变这种情况。</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">94%的高管认为人工智能是业务的关键，但只有 18%的高管大规模采用了人工智能。问题是对人工智能的不信任——我们…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na ks mm"/></div></div></a></div><h2 id="3d46" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">本系列的历史观点和目标</h2><p id="c58d" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">尽管深度学习只是在最近才成为主流媒体，但它的历史可以追溯到 20 世纪 40 年代初，由麦卡洛克和皮茨建立的第一个人工神经元数学模型。从那以后，科学文献中提出了许多架构，从 Frank Rosenblatt (1958 年)的单层感知器到最近的<a class="ae ma" href="https://arxiv.org/abs/1806.07366" rel="noopener ugc nofollow" target="_blank">神经常微分方程</a> (2018 年)，以解决各种任务(例如，下围棋、时间序列预测、图像分类、模式提取等)。下面的时间线(由<a class="mb mc ep" href="https://medium.com/u/e8ec6fa4d7d4?source=post_page-----37a3ec09038a--------------------------------" rel="noopener" target="_blank">法维奥·巴斯克斯</a>提供)提供了深度学习历史的一幅相当准确的图片。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/d24955fe8dee326186f2b99218bc222f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFhP6ogHTx9d-w-A7IQUWw.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk">Thanks to <a class="mb mc ep" href="https://medium.com/u/e8ec6fa4d7d4?source=post_page-----37a3ec09038a--------------------------------" rel="noopener" target="_blank">Favio Vázquez</a> for this amazing figure. Check out his posts, they are really good!</figcaption></figure><p id="da27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如你所看到的，这段历史很复杂。因此，在数量有限的博文中涵盖所有这些不同的架构是不现实的。此外，这些神经网络结构中的一些可以从高级数学领域或者甚至从统计物理学中提取。这些系列不是详细讨论这些架构中的每一个，而是旨在逐步向初学者介绍深度学习背后的数学理论，它使用的基本算法，以及提供一些关于其发展的历史观点。为此，我们将从简单的线性分类器开始，如 Rosenblatt 的单层感知或逻辑回归，然后转到完全连接的神经网络和其他广泛的架构，如卷积神经网络或 LSTM 网络。其他各种主题，如凸和非凸优化，通用近似定理，或技术和道德的良好做法也将在路上解决。因为我们的目标是帮助初学者理解深度学习算法的内部工作原理，所以所有将要介绍的实现基本上都依赖于 SciPy 和 NumPy，而不是像 TensorFlow 这样高度优化的库，至少在可能的情况下。此外，为了教学和科学推广，本系列中使用的所有代码都可以在 GitHub [ <a class="ae ma" href="https://github.com/loiseaujc/TowardsDataScience/tree/master" rel="noopener ugc nofollow" target="_blank">此处</a> ]上免费获得。事不宜迟，让我们开始吧！</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h1 id="9516" class="ol nc it bd nd om on oo ng op oq or nj jz os ka nm kc ot kd np kf ou kg ns ov bi translated">麦卡洛克和皮茨的人工神经元模型(1943 年)</h1><p id="1990" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">人工神经元的第一个数学模型是由沃伦·麦卡洛克(1898-1969，美国神经生理学家)和小沃尔特·h·皮茨(1923-1969，美国逻辑学家)在 1943 年提出的阈值逻辑单元。然而，在深入研究他们的模型之前，让我们先快速回顾一下生物神经元实际上是如何工作的。</p><div class="kj kk kl km gt ab cb"><figure class="ow kn ox oy oz pa pb paragraph-image"><img src="../Images/275e2cb9a4513dc2a4c0eb35ceece2f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*d8VOpWUia97VA_RCACgpPg.jpeg"/></figure><figure class="ow kn pc oy oz pa pb paragraph-image"><img src="../Images/376ea4ea2ee8e6a8827c1592be401cfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/1*MDLWwvcWcNiG2qjvMp9Gpg.gif"/><figcaption class="oa ob gj gh gi oc od bd b be z dk pd di pe pf">Left: Warren S. McCulloch. Right: Walter H. Pitts Jr.</figcaption></figure></div><h2 id="dc4e" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">生物神经元的高级描述</h2><p id="7c34" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">神经元是大脑的组成部分。简而言之，神经元是电可兴奋的细胞，通过专门的连接与其他细胞进行交流。存在不同的生物模型来描述它们的属性和行为，例如</p><ul class=""><li id="752d" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated">早在 1907 年，路易斯·拉皮克(1866-1952，法国神经科学家)就提出了整合-发射模型。</li><li id="bc7d" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated"><a class="ae ma" href="https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model" rel="noopener ugc nofollow" target="_blank">霍奇金-赫胥黎模型</a>，以获得 1963 年诺贝尔生理学和医学奖的<a class="ae ma" href="https://en.wikipedia.org/wiki/Alan_Lloyd_Hodgkin" rel="noopener ugc nofollow" target="_blank">艾伦·a·霍奇金</a>(1914–1998，英国生理学家和生物物理学家)和<a class="ae ma" href="https://en.wikipedia.org/wiki/Andrew_Huxley" rel="noopener ugc nofollow" target="_blank">安德鲁·f·赫胥黎</a>(1917–2012，英国生理学家和生物物理学家)命名。</li><li id="3637" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">以理查德·菲茨休(1922–2007，美国生物物理学家)和 j·南云(日本工程师)命名的<a class="ae ma" href="https://en.wikipedia.org/wiki/FitzHugh%E2%80%93Nagumo_model" rel="noopener ugc nofollow" target="_blank">菲茨休-南云模型</a>，基本上是霍奇金-赫胥黎模型的简化。</li><li id="59ae" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">或者是尤金·m·伊兹基科维奇(生于 1967 年，俄罗斯数学家)最近的脉冲神经元模型<a class="ae ma" href="https://www.izhikevich.org/publications/spikes.htm" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><p id="8753" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管这些模型中的一些开始被采用作为复杂神经网络的构建模块(例如，参见脉冲神经网络)，我们此后将限制我们自己对神经元的非常高级的描述。下图显示了示意图。出于我们的目的，我们只对以下元素感兴趣:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/eed60dbcc89a1ba20ac8f01283d0c9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*3Aktb4kThvYHPLD5vQOzxg.png"/></div><figcaption class="oa ob gj gh gi oc od bd b be z dk">Schematic representation of <strong class="bd pv">biological neuron</strong>. From <a class="ae ma" href="https://en.wikipedia.org/wiki/Neuron" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>.</figcaption></figure><ul class=""><li id="32df" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated"><strong class="kw iu">树突</strong>，也称为<em class="lz">树突</em>，是一个神经细胞的分支原生质延伸，它将从其他神经细胞接收到的电化学刺激传播到细胞体(或<strong class="kw iu">细胞体</strong>)。</li><li id="eeb7" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated"><strong class="kw iu">胞体</strong>是从树突接收到的信号汇合并传递的地方。它包含许多细胞器以及细胞核。</li><li id="5ede" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated"><strong class="kw iu">轴突丘</strong>是连接<strong class="kw iu">轴突</strong>的细胞体的特殊部分。正是它控制着神经元的放电。如果它接收到的信号的总强度超过了阈值，神经元就会向轴突发出一个信号(称为动作电位)。</li><li id="4a55" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated"><strong class="kw iu">轴突</strong>是从胞体向下延伸到末端的细长纤维。它的作用是通过它的<strong class="kw iu">突触</strong>将神经信号传递给其他神经元。</li><li id="f8f4" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated"><strong class="kw iu">突触</strong>是位于连接神经元和其他神经细胞的轴突末梢最末端的小间隙。在那里，神经递质被用来将信号通过突触传递给其他神经元。</li></ul><p id="330a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">生物神经元的工作原理可以总结如下。首先，它从其树突(即，从其他神经元)获取输入。第二步，在 soma 中对这些输入进行加权求和。结果然后传递给轴突小丘。如果这个加权和大于阈值限制，神经元将会触发。否则，它将保持静止。我们神经元的状态(开或关)然后通过其轴突传播，并通过其突触传递给其他连接的神经元。虽然非常简单，但这种对生物神经元工作原理的高级描述足以理解麦卡洛克和皮茨在 1943 年提出的人工神经元的数学模型。</p><h2 id="0194" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">人工神经元的数学模型</h2><p id="fe19" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">基于对神经元工作原理的基本理解，麦卡洛克和皮茨在他们的开创性论文<em class="lz">中提出了第一个人工神经元的数学模型，这是早在 1943 年神经活动中固有思想的逻辑演算。尽管非常简单，他们的模型已经被证明是非常通用和容易修改的。今天，他们最初模型的变体现在已经成为大多数神经网络的基本构建模块，从简单的单层感知器一直到微软用来赢得 2016 年 ImageNet 竞赛的 152 层深度神经网络。</em></p><p id="cb5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">麦卡洛克&amp;皮茨的神经元模型，以下简称为 MCP 神经元，可由以下规则定义:</p><ul class=""><li id="2838" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated">它有一个二进制输出<em class="lz"> y </em> ∈ {0，1}，其中<em class="lz"> y </em> =1 表示神经元启动，<em class="lz"> y </em> =0 表示它处于静止状态。</li><li id="7843" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">它有 n 个兴奋性二进制输入<em class="lz"> xₖ </em> ∈ {0，1}。</li><li id="f4e0" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">它有一个单一的抑制输入<em class="lz"> i </em>。如果它是开着的，神经元就不能激发。</li><li id="0e1a" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">它有一个阈值θ。如果它的输入总和大于这个临界值，神经元就会触发。否则，它将保持静止。</li></ul><p id="a78e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定输入<strong class="kw iu"><em class="lz">x</em></strong>=【<em class="lz">x₁、x₂、x₃、</em> … <em class="lz">、xₙ </em> ]ᵀ，抑制输入<em class="lz"> i </em>和阈值θ，输出<em class="lz"> y </em>计算如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/f712ff9442313e5acbe765f3450945dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*B9YzrlmPeoHdGWcRKqMRJA.png"/></div></figure><p id="79e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于任何具有神经网络基础知识的人来说，这样的模型看起来可疑地像现代人工神经元，这正是因为它是！</p><p id="eb88" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">许多不同的论文和博客帖子已经展示了如何使用 MCP 神经元来实现不同的布尔函数，如 OR、and 或 NOT。下面用马文·明斯基的符号来说明这些。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/d8f76bce45dc4bc1f100767e00697dec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZIh9LhqwMlWKO7DJ6iJFg.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk">Three boolean functions are modeled using MCP neurons. For more details, see the post mentioned below by <a class="mb mc ep" href="https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------" rel="noopener" target="_blank">Akshay Chandra Lagandula</a></figcaption></figure><p id="bb00" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">必须强调的是，通过堆叠多个 MCP 神经元，还可以表示更复杂的功能(例如，触发器、除以 2 等)。尽管有这种灵活性，MCP 神经元仍有很大的局限性，即</p><ul class=""><li id="8abe" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated">单个 MCP 神经元不能代表 XOR 布尔函数或任何其他非线性函数。</li><li id="e534" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">所有的突触权重都被设置为 1，这意味着所有的输入对输出的贡献是相等的。</li><li id="0bbc" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">所考虑的功能需要由用户硬编码。无法从数据中得知。</li><li id="b490" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">绝对抑制规则(即如果抑制输入<em class="lz"> i </em>开启，神经元不能触发)限制性太强。</li></ul><p id="8454" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管如此，MCP 神经元在当时的研究界引起了极大的兴奋，半个多世纪后，引发了现代深度学习。在这个过程中，最重要的改进之一，解决了 MCP 神经元的一些限制，来自于弗兰克·罗森布拉特和他的感知机。</p><p id="a50f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">注:</strong> <a class="mb mc ep" href="https://medium.com/u/202534492f47?source=post_page-----37a3ec09038a--------------------------------" rel="noopener" target="_blank">阿克谢·钱德拉·拉甘杜拉</a>去年夏天出版了一本关于麦卡洛克·皮茨的《神经元》的精彩介绍。最值得注意的是，他举例说明了布尔函数(例如 AND、OR 等)是如何使用这个模型实现的。对于更深入的细节(和漂亮的数字)，强烈鼓励感兴趣的读者去看看。</p><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/mcculloch-pitts-model-5fdf65ac5dd1"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">麦卡洛克-皮茨神经元——人类第一个生物神经元的数学模型</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">众所周知，深度神经网络的最基本单元被称为人工神经元/感知器…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="py l mx my mz mv na ks mm"/></div></div></a></div><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/perceptron-the-artificial-neuron-4d8c70d5cc8d"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">感知器:人工神经元(麦卡洛克-皮茨神经元的本质升级)</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">深度神经网络的最基本单元被称为人工神经元，它接受输入，处理它…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="pz l mx my mz mv na ks mm"/></div></div></a></div></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h1 id="4df5" class="ol nc it bd nd om on oo ng op oq or nj jz os ka nm kc ot kd np kf ou kg ns ov bi translated">罗森布拉特的单层感知器(1957 年)</h1><p id="0622" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">在麦卡洛克和皮茨之后大约 15 年，美国心理学家弗兰克·罗森布拉特(1928-1971)受到突触可塑性(即学习过程中大脑神经元的适应)的<a class="ae ma" href="https://en.wikipedia.org/wiki/Hebbian_theory" rel="noopener ugc nofollow" target="_blank">赫比理论</a>的启发，提出了<em class="lz">感知器</em>，这是对 MCP 神经元模型的一个重大改进。这项发明让他获得了国际认可，迄今为止，电气和电子工程师协会(IEEE)，“<em class="lz">世界上最大的专业协会，致力于推动技术创新和卓越，造福人类</em>”，以他的名字命名其年度奖项。</p><div class="kj kk kl km gt ab cb"><figure class="ow kn qa oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/56a92b145fe5112487d811a77006824d.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*TMGB3f3v4hPiPx2bvgRDjQ.jpeg"/></div></figure><figure class="ow kn qb oy oz pa pb paragraph-image"><img src="../Images/9ffb9ce126b844a9a8836a87eb0c29be.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*lC7w-cbJ26_FEGjnYskahw.jpeg"/><figcaption class="oa ob gj gh gi oc od bd b be z dk qc di qd pf">Left: <strong class="bd pv">Frank Rosenblatt</strong>, from <a class="ae ma" href="https://en.wikipedia.org/wiki/Frank_Rosenblatt" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>. Right: <strong class="bd pv">Mark I Perceptron machine</strong>, the first implementation of the perceptron algorithm. From <a class="ae ma" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank">Wikipedia</a> as well.</figcaption></figure></div><p id="59c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">罗森布拉特的主要成就是表明，通过放松 MCP 的一些规则(即绝对抑制、所有输入的平等贡献以及它们的整数性质)，人工神经元实际上可以从数据中学习。更重要的是，他为这种改进的 MCP 神经元模型提出了一种<em class="lz">监督</em>学习算法，使人工神经元能够自己直接从训练数据中计算出正确的权重。在深入机器学习有趣的东西之前，让我们快速讨论一下感知器可以解决的问题类型。</p><h2 id="1e13" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">二元分类</h2><p id="896a" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">二进制(或二项式)分类是基于规定的规则将给定集合的元素分类成两组(例如，分类图像是描绘猫还是狗)的任务。下图描述了此类问题的两个实例。在左边，任务是识别两个<em class="lz">线性可分的</em>类之间的分界线(即分界线是简单的直线)，而在右边，两个类是<em class="lz">非线性可分的</em>(即分界线是<strong class="kw iu">而不是</strong>简单的直线)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/8decf52ff8c1ccdaf983f7e18acc1348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*negXjEfhGskviBEjYnG9nw.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk">Examples of linear and nonlinear binary classification problems. In the rest of this post, we will consider only example (a).</figcaption></figure><p id="504a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们将看到的，罗森布拉特的感知器只能处理线性可分类的分类任务。然而，必须注意的是，右图中的例子也可能被感知器处理，尽管它需要一个称为<em class="lz">特征工程</em>的输入预处理，以将其转换成一个线性可分问题<em class="lz">。</em>这将在以后的帖子中解决(希望如此)。</p><p id="f999" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了更好地理解感知器处理二元分类问题的能力，让我们考虑它所依赖的人工神经元模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qf"><img src="../Images/12c6ec9c98ced4a75ee2f54ea5727530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofVdu6L3BDbHyt1Ro8w07Q.png"/></div></div><figcaption class="oa ob gj gh gi oc od bd b be z dk">Artificial neuron used by the perceptron. From <a class="ae ma" href="https://commons.wikimedia.org/wiki/File:Perceptron_moj.png#/media/File:Perceptron_moj.png" rel="noopener ugc nofollow" target="_blank">Wikipedia</a>.</figcaption></figure><p id="530a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如你所看到的，这个神经元与麦卡洛克&amp;皮茨在 1943 年提出的非常相似。然而，它有一些主要的区别，即</p><ul class=""><li id="c53b" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated">神经元接受与突触权重<em class="lz"> b </em>相关的额外恒定输入(在上图中表示为θ)，也称为偏差。关于 MCP 神经元，偏差<em class="lz"> b </em>仅仅是激活阈值的负值。</li><li id="23f7" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">突触权重<em class="lz"> wₖ </em>不限于一，因此允许一些输入比其他输入对神经元的输出有更大的影响。</li><li id="ad78" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">它们也不局限于严格为正。因此，一些输入可能具有抑制性影响。</li><li id="af0d" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">绝对抑制法则不再适用。</li></ul><p id="18a7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在数学术语中，感知器所依赖的人工神经元的非线性是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/11f2ef05fec87521f29729536397973f.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*xSzatNPrLQnVrAr2eZv0kA.png"/></div></figure><p id="9abc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该函数对应于 Heaviside 函数(即<em class="lz"> H </em> ( <em class="lz"> z </em> ) = 0，如果<em class="lz"> z </em> &lt; 0，否则<em class="lz"> H </em> ( <em class="lz"> z </em> ) = 1)。请注意，感知器的等效公式(其中二进制输出定义为<em class="lz"> y </em> ∈ {-1，1})考虑的是符号函数，而不是亥维赛函数，即</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/c51c897ce66fde4c789831bc8c75ccc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*tVI8T9W4WL11Kq3j6kGe6g.png"/></div></figure><p id="ba6c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">无论公式是什么，感知器(和许多其他线性分类器)的决策边界是这样的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/673311e26dbcbc5ad66f60834779a741.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*-ta2VNXqjAJ_LtpLob2AcQ.png"/></div></figure><p id="cf41" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">或者，使用我们简洁的数学符号</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/937aa4f9763d595d2fc1c4381cce5c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*Mc7fqeR7Qt7hY47nsKPUeQ.png"/></div></figure><p id="4fc8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该决策函数线性依赖于输入<em class="lz"> xₖ </em>，因此得名<em class="lz">线性分类器</em>。此外，这个方程是一个<em class="lz">超平面</em>(1D 的一个简单点，2D 的一条直线，3D 中的一个规则平面，等等)的方程。突触权重的向量<strong class="kw iu"> <em class="lz"> w </em> </strong>垂直于该平面，而偏差<em class="lz"> b </em>是从原点的偏移。既然我们对罗森布拉特的感知器为什么可以用于线性分类有了更好的理解，那么有待回答的问题是</p><blockquote class="md me mf"><p id="5c2d" class="ku kv lz kw b kx ky ju kz la lb jx lc mg le lf lg mh li lj lk mi lm ln lo lp im bi translated">给定一组 m 个例子(<strong class="kw iu"> x </strong> ₘ，yₘ)，感知器如何学习正确的突触权重<strong class="kw iu"> w </strong>和 bias b 以正确区分两类？</p></blockquote><h2 id="563e" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">感知机学习算法</h2><p id="e7d1" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">如前所述，Rosenblatt 的主要成就不仅表明他对 MCP 神经元的修改实际上可以用于执行二进制分类，而且还提出了一种相当简单但相对有效的算法，使感知器能够从示例中学习正确的突触权重<strong class="kw iu"> <em class="lz"> w </em> </strong>。该算法如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qk"><img src="../Images/05e829e78c05060787333ddd5a4d56ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wje5-soLhLjDApnuPMwm2A.png"/></div></div></figure><p id="c13e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在继续讨论 Python 实现之前，让我们考虑四个简单的思想实验来说明它是如何工作的。</p><ul class=""><li id="9173" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated">假设 mᵗʰ示例<strong class="kw iu"> <em class="lz"> x </em> </strong> <em class="lz"> ₘ </em>属于类别<em class="lz"> yₘ </em> =0，并且感知器正确预测<em class="lz"> ŷₘ </em> =0 <em class="lz">。</em>在这种情况下，重量修正由δ<strong class="kw iu"><em class="lz">w</em></strong>=(0-0)<strong class="kw iu"><em class="lz">x</em></strong><em class="lz">ₘ</em>给出，即我们不改变重量。这同样适用于偏见。</li><li id="dde9" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">同样，如果 mᵗʰ例子<strong class="kw iu"> <em class="lz"> x </em> </strong> <em class="lz"> ₘ </em>属于类<em class="lz"> yₘ </em> =1，感知器正确预测<em class="lz"> ŷₘ </em> =1，那么权重修正为δ<strong class="kw iu"><em class="lz">w</em></strong>= 0。这同样适用于偏差。</li><li id="3927" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">现在假设 mᵗʰ例子<strong class="kw iu"> <em class="lz"> x </em> </strong> <em class="lz"> ₘ </em>属于类别<em class="lz"> yₘ </em> =0，并且感知机错误地预测<em class="lz"> ŷₘ </em> =1 <em class="lz">。</em>在这种情况下，权重修正由δ<strong class="kw iu"><em class="lz">w</em></strong>=(0–1)<strong class="kw iu"><em class="lz">x</em></strong><em class="lz">ₘ=</em>–<strong class="kw iu"><em class="lz">x</em></strong><em class="lz">ₘ</em>给出，而偏差更新为<em class="lz">b</em>=<em class="lz">b</em>–1。</li><li id="7f47" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">最后，如果 mᵗʰ例子<strong class="kw iu"> <em class="lz"> x </em> </strong> <em class="lz"> ₘ </em>属于类<em class="lz"> yₘ </em> =1 而感知器错误预测<em class="lz"> ŷₘ </em> =0，<em class="lz"> </em>权重修正为δ<strong class="kw iu"><em class="lz">w</em></strong>=<strong class="kw iu"><em class="lz">x</em></strong><em class="lz">ₘ</em>。偏差也根据<em class="lz"> b </em> = <em class="lz"> b+ </em> 1 更新。</li></ul><p id="24de" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，这个算法非常简单。然而，乍一看，为什么这样一个简单的算法实际上可以收敛到一组有用的突触权重，这可能还不清楚。虽然相对简单，收敛的证明将不会在这里提出，实际上将是一个即将到来的职位的主题。对于这篇文章的其余部分，只要做出一个信念的飞跃，相信我，它确实会收敛。与此同时，如果你是一个怀疑论者或者仅仅是不相信，你可以看看阿克谢·钱德拉·拉甘杜拉的帖子，从几何学的角度直观地了解它为什么有效。</p><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/perceptron-learning-algorithm-d5db0deab975"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">感知器学习算法:其工作原理的图形解释</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">本帖将讨论 Minsky 和 Papert 在 1969 年提出的著名的感知器学习算法。这是一个…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="ql l mx my mz mv na ks mm"/></div></div></a></div><p id="5a3c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在让我们继续有趣的事情，用 Python 实现这个简单的学习算法。假设您已经熟悉 Python，下面的代码应该是不言自明的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qm qn l"/></div></figure><p id="192e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请注意，为了清晰和易用，我们将在整个课程中尽量坚持使用 scikit-learn API。在我的 towards data science Github repo(<a class="ae ma" href="https://github.com/loiseaujc/TowardsDataScience/tree/master" rel="noopener ugc nofollow" target="_blank">此处</a>)上可以免费获得该代码的扩展版本(带有各种健全性检查和其他内容)。我们的模型学习到的最终决策边界如下所示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/a3fe0ffa2559452272f6aa6e881c78da.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*TwJnmqKzm2a-Rm77TGENOQ.png"/></div><figcaption class="oa ob gj gh gi oc od bd b be z dk">Linear decision boundary learned by the perceptron.</figcaption></figure><p id="9cf6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于这个特定的例子，我们的感知器在整个数据集上经过三次，才正确地学习到这个决策边界。虽然它正确地对我们训练数据集中的所有例子进行了分类，但我们将在后面的帖子中看到，感知器的泛化能力相当有限，特别是由于它的边缘很小，并且对有噪声的数据非常敏感，这甚至可能阻止学习算法收敛。尽管如此，请不要犹豫，从 Github 下载相应的脚本，并使用这个简单的实现来建立您对它为什么工作、如何工作以及它的局限性的直觉。毕竟，</p><blockquote class="md me mf"><p id="c4a8" class="ku kv lz kw b kx ky ju kz la lb jx lc mg le lf lg mh li lj lk mi lm ln lo lp im bi translated">锻造造就铁匠，航海造就水手，熟能生巧。</p></blockquote><h2 id="4df3" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">感知器的消亡</h2><p id="0c1b" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">这种学习算法对于线性可分问题的简单性和效率是它在 20 世纪 50 年代末和 60 年代初如此流行的一些关键原因。然而，这种受欢迎程度导致罗森布拉特夸大了他的感知学习能力，在科学界引起了不切实际的期望，媒体也报道了这一点。正如我们将在后面的文章中看到的，这种感知机确实有很大的局限性，极大地限制了它在现实生活中的应用。<em class="lz">的奇招</em>来自<a class="ae ma" href="https://en.wikipedia.org/wiki/Marvin_Minsky" rel="noopener ugc nofollow" target="_blank">马文·明斯基</a>(1927–2016，美国认知科学家)和<a class="ae ma" href="https://en.wikipedia.org/wiki/Seymour_Papert" rel="noopener ugc nofollow" target="_blank">西蒙·派珀特</a>(1928–2016，南非裔美国数学家)，他们在 1969 年出版了臭名昭著的名著<em class="lz">感知机:计算几何导论</em>。在本书中，作者展示了 Rosenblatt 的感知器(以及任何其他单层感知器)实际上是多么有限，而且值得注意的是，它无法学习简单的逻辑异或函数。有人认为，这本书的出版和对感知机极限的展示引发了 20 世纪 80 年代所谓的人工智能冬天…</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h1 id="2774" class="ol nc it bd nd om on oo ng op oq or nj jz os ka nm kc ot kd np kf ou kg ns ov bi translated">结论</h1><p id="d005" class="pw-post-body-paragraph ku kv it kw b kx nu ju kz la nv jx lc ld nw lf lg lh nx lj lk ll ny ln lo lp im bi translated">本文是我在法国巴黎国立高等艺术学院教授的深度学习入门系列文章的第一篇。由于我们必须先学会走，然后才能跑，因此我们的注意力一直集中在深度学习的非常初步的方面，从历史和数学的角度来看，即麦卡洛克&amp;皮茨的人工神经元模型和罗森布拉特的单层感知器。因为这些是现代神经网络的非常基本的构建模块，所以在进入现代深度学习之前，不要犹豫，尽可能多地阅读它们，并玩 Jupyter 笔记本，以确保您完全掌握它们的属性和限制。我知道在单层感知器上给一个帖子贴上深度学习的标签可能有些牵强。然而，即使可以在网上找到大量的教程(有些真的很好，有些有点可疑)来运行深度学习库，如 TensorFlow，而不需要对底层数学的深刻理解(没有双关语)，拥有这样的见解将被证明是非常有价值的，并防止你后来屈服于深度学习的常见陷阱。所以我们一步一步来，好吗？</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="01e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在接下来的几篇文章中，我们将讨论以下主题:</p><ul class=""><li id="6e63" class="pg ph it kw b kx ky la lb ld pi lh pj ll pk lp pl pm pn po bi translated">感知器收敛定理。</li><li id="01eb" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">罗森布拉特感知器的极限，走向灭亡的途径。</li></ul><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/improving-upon-rosenblatts-perceptron-d0517d3c5939"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">罗森布拉特感知机的改进</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">自适应线性神经元和 Delta 规则</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="qp l mx my mz mv na ks mm"/></div></div></a></div><p id="8231" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，你会在下面找到一个关于麦卡洛克&amp;皮茨神经元和罗森布拉特感知器的历史和数学的附加在线资源列表。不要犹豫，看看这些，因为它们可能会处理一些我们只见过的方面！</p><p id="a6e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">PS:如果你知道任何其他相关链接，不要犹豫给我发消息，我会编辑帖子来添加它:)</p><h2 id="c73f" class="nb nc it bd nd ne nf dn ng nh ni dp nj ld nk nl nm lh nn no np ll nq nr ns nt bi translated">其他在线资源</h2><ul class=""><li id="49d0" class="pg ph it kw b kx nu la nv ld qq lh qr ll qs lp pl pm pn po bi translated">Sebastian Raschka 关于<a class="ae ma" href="https://sebastianraschka.com/Articles/2015_singlelayer_neurons.html#adaptive-linear-neurons-and-the-delta-rule" rel="noopener ugc nofollow" target="_blank"> <em class="lz">单层神经网络和梯度下降</em> </a>的博文。</li><li id="8494" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">Jonty Sinai 的博客文章简单地命名为<a class="ae ma" href="https://jontysinai.github.io/jekyll/update/2017/11/11/the-perceptron.html" rel="noopener ugc nofollow" target="_blank"> <em class="lz">感知器</em> </a>。</li><li id="5c54" class="pg ph it kw b kx pp la pq ld pr lh ps ll pt lp pl pm pn po bi translated">Amanda Gefter <a class="mb mc ep" href="https://medium.com/u/c731395f912d?source=post_page-----37a3ec09038a--------------------------------" rel="noopener" target="_blank"> Nautilus </a>关于 Walter Pitts 不可思议的历史。</li></ul></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><blockquote class="md me mf"><p id="175f" class="ku kv lz kw b kx ky ju kz la lb jx lc mg le lf lg mh li lj lk mi lm ln lo lp im bi translated"><em class="it">想阅读更多此类内容吗？</em>查看我其他关于<a class="ae ma" href="https://loiseau-jc.medium.com/list/lowrank-structure-and-datadriven-modeling-8f39635a90ea" rel="noopener">低秩结构和数据驱动建模</a> <em class="it">的文章或者干脆我的</em> <a class="ae ma" href="https://loiseau-jc.medium.com/list/machine-learning-basics-0baf10d8f8b5" rel="noopener"> <em class="it">机器学习基础知识</em> </a>！</p></blockquote><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/binary-cross-entropy-and-logistic-regression-bf7098e75559"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd iu gy z fp mr fr fs ms fu fw is bi translated">二元交叉熵和逻辑回归</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">有没有想过我们为什么使用它，它来自哪里，如何有效地优化它？这里有一个解释(代码…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="qp l mx my mz mv na ks mm"/></div></div></a></div></div></div>    
</body>
</html>