<html>
<head>
<title>Sentiment Analysis using LSTM (Step-by-Step Tutorial)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 LSTM 进行情感分析(循序渐进教程)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948?source=collection_archive---------0-----------------------#2019-02-18">https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948?source=collection_archive---------0-----------------------#2019-02-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="38b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 PyTorch 框架进行深度学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7e27007888c144bd555bcab2fcbb9ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*O778gXEk9fEbjn63.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Sentiment Analysis, Image by: <a class="ae ky" href="https://monkeylearn.com/sentiment-analysis/" rel="noopener ugc nofollow" target="_blank">Monkeylearn</a></figcaption></figure><h1 id="03d0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">什么是情绪分析:</strong></h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/511f17baff23ccfbed6048747dd1c601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*Up6sMLbRg79TlRP2rP1tsw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Sentiment Analysis from Dictionary</figcaption></figure><p id="2593" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">我认为谷歌词典的这个结果给出了一个非常简洁的定义。我不用再强调情感分析变得多么重要了。因此，这里我们将在<a class="ae ky" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB 电影数据集</a>上建立一个分类器，使用一种叫做 RNN 的<a class="ae ky" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>技术。</p><p id="11a9" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">我概述了如何使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆(LSTM) </a>架构实现<a class="ae ky" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络(RNN) </a>的逐步过程:</p><ol class=""><li id="315a" class="mo mp it lu b lv lw ly lz mb mq mf mr mj ms mn mt mu mv mw bi translated">加载并可视化数据</li><li id="ee2f" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">数据处理—转换为小写</li><li id="6cea" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">数据处理—删除标点符号</li><li id="307b" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">数据处理—创建评论列表</li><li id="db8a" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">Tokenize 创建 Vocab 到 Int 映射字典</li><li id="69da" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">标记化—对单词进行编码</li><li id="2df2" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">标记化—对标签进行编码</li><li id="d7d9" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">分析评论长度</li><li id="dd80" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">去除离群值——去除过长或过短的评论</li><li id="40c7" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">填充/截断剩余数据</li><li id="846c" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">训练、验证、测试数据集分割</li><li id="3978" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">数据加载器和批处理</li><li id="a3bf" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">定义 LSTM 网络架构</li><li id="205e" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">定义模型类</li><li id="e5f0" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">训练网络</li><li id="b189" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">测试(针对测试数据和用户生成的数据)</li></ol></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><blockquote class="nj nk nl"><p id="ecae" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated">1 <strong class="lu iu"> <em class="it">)载入并可视化数据</em> </strong></p></blockquote><p id="844f" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">我们正在使用<a class="ae ky" href="http://ai.stanford.edu/~amaas/data/sentiment/" rel="noopener ugc nofollow" target="_blank"> IMDB 电影评论数据集</a>。如果它以 txt 文件的形式存储在你的机器中，那么我们只需把它加载进来</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5ce5" class="nv la it nr b gy nw nx l ny nz"><strong class="nr iu"># read data from text files</strong><br/>with open(‘data/reviews.txt’, ‘r’) as f:<br/> reviews = f.read()<br/>with open(‘data/labels.txt’, ‘r’) as f:<br/> labels = f.read()</span><span id="d6bf" class="nv la it nr b gy oa nx l ny nz">print(reviews[:50])<br/>print()<br/>print(labels[:26])</span><span id="bc9d" class="nv la it nr b gy oa nx l ny nz"><strong class="nr iu">--- Output ---</strong></span><span id="1fb4" class="nv la it nr b gy oa nx l ny nz">bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years</span><span id="33d6" class="nv la it nr b gy oa nx l ny nz">positive<br/>negative<br/>positive</span></pre><blockquote class="nj nk nl"><p id="8f7e" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated">2 <strong class="lu iu"> <em class="it">)数据处理—转换成小写</em> </strong></p></blockquote><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8ddb" class="nv la it nr b gy nw nx l ny nz">reviews = reviews.lower()</span></pre><blockquote class="nj nk nl"><p id="bbd8" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated">3 <strong class="lu iu"> <em class="it">数据处理—去掉标点符号</em> </strong></p></blockquote><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="baf7" class="nv la it nr b gy nw nx l ny nz">from string import punctuation<br/>print(punctuation)</span><span id="62c5" class="nv la it nr b gy oa nx l ny nz"><strong class="nr iu">--- Output ---</strong></span><span id="d818" class="nv la it nr b gy oa nx l ny nz">!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`{|}~</span></pre><p id="a480" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">我们看到了 python 中预定义的所有标点符号。为了去掉所有这些标点符号，我们将简单地使用</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="1f39" class="nv la it nr b gy nw nx l ny nz">all_text = ''.join([c for c in reviews if c not in punctuation])</span></pre><blockquote class="nj nk nl"><p id="33e6" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 4)数据处理—创建评论列表</em> </strong></p></blockquote><p id="bed3" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">我们把所有的字符串都放在一个大字符串中。现在，我们将分离出单独的评论，并将它们存储为单独的列表元素。像【点评 _1，点评 _2，点评 _3……】。复习 _n]</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="4e04" class="nv la it nr b gy nw nx l ny nz">reviews_split = all_text.split(‘\n’)<br/>print ('Number of reviews :', len(reviews_split))</span></pre><p id="c6dd" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">评论数量:25001</p><blockquote class="nj nk nl"><p id="4b57" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 5)标记化—创建 Vocab 到 Int 的映射字典</em> </strong></p></blockquote><p id="a828" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">在大多数 NLP 任务中，您将创建一个索引映射字典，以便为经常出现的单词分配较低的索引。最常见的方法之一是使用<code class="fe ob oc od nr b">Collections</code>库中的<code class="fe ob oc od nr b">Counter</code>方法。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="5ef7" class="nv la it nr b gy nw nx l ny nz">from collections import Counter</span><span id="4889" class="nv la it nr b gy oa nx l ny nz">all_text2 = ' '.join(reviews_split)<br/># create a list of words<br/>words = all_text2.split()</span><span id="ce93" class="nv la it nr b gy oa nx l ny nz"># Count all the words using Counter Method<br/>count_words = Counter(words)<br/><br/>total_words = len(words)<br/>sorted_words = count_words.most_common(total_words)</span></pre><p id="a240" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">让我们来看看我们创建的这些对象</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="a6b6" class="nv la it nr b gy nw nx l ny nz">print (count_words)</span><span id="7363" class="nv la it nr b gy oa nx l ny nz"><strong class="nr iu">--- Output ---</strong></span><span id="fc4e" class="nv la it nr b gy oa nx l ny nz">Counter({'the': 336713, 'and': 164107, 'a': 163009, 'of': 145864</span></pre><p id="9626" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">为了创建一个 vocab 到 int 的映射字典，您只需这样做</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="05db" class="nv la it nr b gy nw nx l ny nz">vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}</span></pre><p id="1626" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">这里有一个小技巧，在这个映射中，索引将从 0 开始，即“the”的映射将是 0。但是稍后我们将为较短的评论填充，填充的常规选择是 0。所以我们需要从 1 开始索引</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="07e5" class="nv la it nr b gy nw nx l ny nz">vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}</span></pre><p id="a4ff" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">让我们来看看这个映射字典。我们可以看到“the”的映射现在是 1</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c58e" class="nv la it nr b gy nw nx l ny nz">print (vocab_to_int)</span><span id="f236" class="nv la it nr b gy oa nx l ny nz"><strong class="nr iu">--- Output ---</strong></span><span id="45bc" class="nv la it nr b gy oa nx l ny nz">{'the': 1, 'and': 2, 'a': 3, 'of': 4,</span></pre><blockquote class="nj nk nl"><p id="c4bd" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 6)标记化——对单词进行编码</em> </strong></p></blockquote><p id="69c9" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">到目前为止，我们已经使用来自所有评论的 vocab 创建了 a)评论列表和 b)索引映射字典。所有这些都是为了创建评论的编码(用整数代替评论中的单词)</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7297" class="nv la it nr b gy nw nx l ny nz">reviews_int = []<br/>for review in reviews_split:<br/>    r = [vocab_to_int[w] for w in review.split()]<br/>    reviews_int.append(r)<br/>print (reviews_int[0:3])</span><span id="4f61" class="nv la it nr b gy oa nx l ny nz"><strong class="nr iu">--- Output ---</strong></span><span id="cc0c" class="nv la it nr b gy oa nx l ny nz">[[21025, 308, 6, 3, 1050, 207, 8, 2138, 32, 1, 171, 57, 15, 49, 81, 5785, 44, 382, 110, 140, 15, .....], [5194, 60, 154, 9, 1, 4975, 5852, 475, 71, 5, 260, 12, 21025, 308, 13, 1978, 6, 74, 2395, 5, 613, 73, 6, 5194, 1, 24103, 5, ....], [1983, 10166, 1, 5786, 1499, 36, 51, 66, 204, 145, 67, 1199, 5194.....]]</span></pre><p id="bd3c" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">注意:我们现在创建的是一个列表列表。每个评论都是一个整数值列表，所有这些都存储在一个巨大的列表中</p><blockquote class="nj nk nl"><p id="ed08" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 7)标记化——对标签进行编码</em> </strong></p></blockquote><p id="14ec" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">这很简单，因为我们只有 2 个输出标签。因此，我们将“正”标记为 1，“负”标记为 0</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f06f" class="nv la it nr b gy nw nx l ny nz">encoded_labels = [1 if label =='positive' else 0 for label in labels_split]<br/>encoded_labels = np.array(encoded_labels)</span></pre><blockquote class="nj nk nl"><p id="a8da" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 8)分析评论长度</em> </strong></p></blockquote><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="8027" class="nv la it nr b gy nw nx l ny nz">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="d142" class="nv la it nr b gy oa nx l ny nz">reviews_len = [len(x) for x in reviews_int]<br/>pd.Series(reviews_len).hist()<br/>plt.show()</span><span id="9c16" class="nv la it nr b gy oa nx l ny nz">pd.Series(reviews_len).describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/90aaeb2e1f6502057220793ef76c7863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z2oGK_xacbKu7xSZtrr9KA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Review Length Analysis</figcaption></figure><p id="bf4e" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated"><strong class="lu iu">观察结果</strong> : a)平均评论长度= 240 b)部分评论长度为 0。保留此评论对我们的分析没有任何意义 c)大多数评论少于 500 字或更多 d)有相当多的评论非常长，我们可以手动调查它们，以检查我们是否需要将它们包括在我们的分析中或从我们的分析中排除</p><blockquote class="nj nk nl"><p id="a751" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 9)剔除异常值——剔除过长或过短的评论</em> </strong></p></blockquote><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="c30d" class="nv la it nr b gy nw nx l ny nz">reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l&gt;0 ]<br/>encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l&gt; 0 ]</span></pre><blockquote class="nj nk nl"><p id="6f9f" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 10)填充/截断剩余数据</em> </strong></p></blockquote><p id="13c5" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">为了处理短评论和长评论，我们会将所有评论填充或截断到特定长度。我们用<strong class="lu iu">序列长度来定义这个长度。</strong>该序列长度与 LSTM 层的时间步数相同。</p><p id="7644" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">对于比<code class="fe ob oc od nr b">seq_length</code>短的评论，我们会用 0 填充。对于长于<code class="fe ob oc od nr b">seq_length</code>的评论，我们会将其截断为第一个 seq_length 单词。</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="f110" class="nv la it nr b gy nw nx l ny nz">def pad_features(reviews_int, seq_length):<br/>    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.<br/>    '''<br/>    features = np.zeros((len(reviews_int), seq_length), dtype = int)<br/>    <br/>    for i, review in enumerate(reviews_int):<br/>        review_len = len(review)<br/>        <br/>        if review_len &lt;= seq_length:<br/>            zeroes = list(np.zeros(seq_length-review_len))<br/>            new = zeroes+review</span><span id="b47f" class="nv la it nr b gy oa nx l ny nz">        elif review_len &gt; seq_length:<br/>            new = review[0:seq_length]<br/>        <br/>        features[i,:] = np.array(new)<br/>    <br/>    return features</span></pre><p id="06ed" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">注意:我们正在创建/维护一个 2D 数组结构，就像我们为<code class="fe ob oc od nr b">reviews_int</code>创建的一样。输出将如下所示</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="551a" class="nv la it nr b gy nw nx l ny nz">print (features[:10,:])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/fcfd70162c29726b475a36f4652fa893.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*2PkHxtYH1e95NTd-KBa57A.png"/></div></figure><blockquote class="nj nk nl"><p id="d298" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 11)训练、验证、测试数据集拆分</em> </strong></p></blockquote><p id="ade7" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">一旦我们把数据整理好，我们将把它分成训练集、验证集和测试集</p><p id="f8f7" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">训练= 80% |有效= 10% |测试= 10%</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="37dc" class="nv la it nr b gy nw nx l ny nz">split_frac = 0.8<br/>train_x = features[0:int(split_frac*len_feat)]<br/>train_y = encoded_labels[0:int(split_frac*len_feat)]</span><span id="f93f" class="nv la it nr b gy oa nx l ny nz">remaining_x = features[int(split_frac*len_feat):]<br/>remaining_y = encoded_labels[int(split_frac*len_feat):]</span><span id="06e4" class="nv la it nr b gy oa nx l ny nz">valid_x = remaining_x[0:int(len(remaining_x)*0.5)]<br/>valid_y = remaining_y[0:int(len(remaining_y)*0.5)]</span><span id="ae4a" class="nv la it nr b gy oa nx l ny nz">test_x = remaining_x[int(len(remaining_x)*0.5):]<br/>test_y = remaining_y[int(len(remaining_y)*0.5):]</span></pre><blockquote class="nj nk nl"><p id="f4bf" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 12)数据加载器和批处理</em> </strong></p></blockquote><p id="6e37" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">在创建我们的训练、测试和验证数据之后。下一步是为这些数据创建数据加载器。我们可以使用生成器函数将我们的数据分批，而不是使用<a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset" rel="noopener ugc nofollow" target="_blank"> TensorDataset </a>。这是 PyTorch 中一个非常有用的实用程序，可以像使用<a class="ae ky" href="https://pytorch.org/docs/stable/torchvision/datasets.html" rel="noopener ugc nofollow" target="_blank"> torchvision 数据集</a>一样轻松地使用<a class="ae ky" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank">数据加载器</a>的数据</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="895a" class="nv la it nr b gy nw nx l ny nz">import torch<br/>from torch.utils.data import DataLoader, TensorDataset</span><span id="d7b3" class="nv la it nr b gy oa nx l ny nz"># create Tensor datasets<br/>train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))<br/>valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))<br/>test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))</span><span id="f01e" class="nv la it nr b gy oa nx l ny nz"># dataloaders<br/>batch_size = 50</span><span id="9add" class="nv la it nr b gy oa nx l ny nz"># make sure to SHUFFLE your data<br/>train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)<br/>valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)<br/>test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)</span></pre><p id="fb6b" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">为了获得一批可视化的训练数据，我们将创建一个数据迭代器</p><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="137c" class="nv la it nr b gy nw nx l ny nz"># obtain one batch of training data<br/>dataiter = iter(train_loader)<br/>sample_x, sample_y = dataiter.next()</span><span id="997e" class="nv la it nr b gy oa nx l ny nz">print('Sample input size: ', sample_x.size()) # batch_size, seq_length<br/>print('Sample input: \n', sample_x)<br/>print()<br/>print('Sample label size: ', sample_y.size()) # batch_size<br/>print('Sample label: \n', sample_y)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/d9c53d029356a40b84edcbe9467b05fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5dxwwFIx7xzcqtxkdAy1w.png"/></div></div></figure><p id="1326" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">这里，50 是批量大小，200 是我们定义的序列长度。现在，我们的数据准备步骤已经完成，接下来我们将查看 LSTM 网络架构，以开始构建我们的模型</p><blockquote class="nj nk nl"><p id="fd43" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated">13 <strong class="lu iu"> <em class="it">)定义 LSTM 网络架构</em> </strong></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/a96838abe1abeec9e9e4a249faf1e573.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SICYykT7ybua1gVJDNlajw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">LSTM Architecture for Sentiment Analysis. Image by Author</figcaption></figure><p id="58b3" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">这些层如下所示:</p><p id="1cb7" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">0.令牌化:这不是 LSTM 网络的一个层，而是将我们的单词转换成令牌(整数)的一个强制步骤</p><ol class=""><li id="fc8b" class="mo mp it lu b lv lw ly lz mb mq mf mr mj ms mn mt mu mv mw bi translated">嵌入层:将单词标记(整数)转换成特定大小的嵌入</li><li id="2ce2" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">LSTM 层:由隐藏状态变暗和层数定义</li><li id="93fb" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">全连接图层:将 LSTM 图层的输出映射到所需的输出大小</li><li id="c71a" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">Sigmoid 激活层:将所有输出值转换为 0 到 1 之间的值</li><li id="dea7" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn mt mu mv mw bi translated">输出:最后一个时间步长的 Sigmoid 输出被认为是该网络的最终输出</li></ol><p id="13eb" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">注:如果你想更多地了解这些 LSTM 层，并获得事物的微观观点。读读这个—</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/reading-between-the-layers-lstm-network-7956ad192e58"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">层间阅读(LSTM 网络)</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">使用 PyTorch 框架进行深度学习</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div><blockquote class="nj nk nl"><p id="7bf5" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 14)定义模型类</em> </strong></p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa pb l"/></div></figure><blockquote class="nj nk nl"><p id="dead" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 15)训练网络</em> </strong></p></blockquote><ul class=""><li id="1115" class="mo mp it lu b lv lw ly lz mb mq mf mr mj ms mn pc mu mv mw bi translated">实例化网络</li></ul><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="7cfc" class="nv la it nr b gy nw nx l ny nz"># Instantiate the model w/ hyperparams<br/>vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding<br/>output_size = 1<br/>embedding_dim = 400<br/>hidden_dim = 256<br/>n_layers = 2</span><span id="4e8d" class="nv la it nr b gy oa nx l ny nz">net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)</span><span id="c095" class="nv la it nr b gy oa nx l ny nz">print(net)</span><span id="a442" class="nv la it nr b gy oa nx l ny nz">SentimentLSTM(<br/>  (embedding): Embedding(74073, 400)<br/>  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)<br/>  (dropout): Dropout(p=0.3)<br/>  (fc): Linear(in_features=256, out_features=1, bias=True)<br/>  (sig): Sigmoid()<br/>)</span></pre><ul class=""><li id="3588" class="mo mp it lu b lv lw ly lz mb mq mf mr mj ms mn pc mu mv mw bi translated">训练循环</li></ul><p id="f800" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">training loop 中的大多数代码都是非常标准的深度学习训练代码，您可能会在所有使用 PyTorch 框架的实现中经常看到这些代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa pb l"/></div></figure><blockquote class="nj nk nl"><p id="8319" class="ls lt nm lu b lv lw ju lx ly lz jx ma nn mc md me no mg mh mi np mk ml mm mn im bi translated"><strong class="lu iu"> <em class="it"> 16)测试</em> </strong></p></blockquote><ul class=""><li id="3dcd" class="mo mp it lu b lv lw ly lz mb mq mf mr mj ms mn pc mu mv mw bi translated">关于测试数据</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa pb l"/></div></figure><ul class=""><li id="870e" class="mo mp it lu b lv lw ly lz mb mq mf mr mj ms mn pc mu mv mw bi translated">关于用户生成的数据</li></ul><p id="2c6a" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">首先，我们将定义一个负责预处理步骤的<code class="fe ob oc od nr b">tokenize</code>函数，然后我们将创建一个<code class="fe ob oc od nr b">predict</code>函数，它将在解析用户提供的评论后给出最终输出。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pa pb l"/></div></figure><h1 id="60e3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">结果:</strong></h1><pre class="kj kk kl km gt nq nr ns nt aw nu bi"><span id="b329" class="nv la it nr b gy nw nx l ny nz">test_review = 'This movie had the best acting and the dialogue was so good. I loved it.'</span><span id="2ef0" class="nv la it nr b gy oa nx l ny nz">seq_length=200 # good to use the length that was trained on</span><span id="d4c2" class="nv la it nr b gy oa nx l ny nz">predict(net, test_review_neg, seq_length)</span></pre><p id="01d8" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated"><strong class="lu iu">阳性审查检出</strong></p><h2 id="84c1" class="nv la it bd lb pd pe dn lf pf pg dp lj mb ph pi ll mf pj pk ln mj pl pm lp pn bi translated"><strong class="ak">感谢阅读！</strong></h2><ul class=""><li id="da62" class="mo mp it lu b lv po ly pp mb pq mf pr mj ps mn pc mu mv mw bi translated">如果你喜欢这个，<a class="ae ky" href="https://samarthagrawal86.medium.com" rel="noopener">关注我的 medium </a>了解更多。</li><li id="430c" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn pc mu mv mw bi translated">你们的掌声对写更多、写得更好是一个巨大的鼓励和帮助。</li><li id="45e5" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn pc mu mv mw bi translated">有兴趣合作吗？我们在<a class="ae ky" href="https://www.linkedin.com/in/samarth-agrawal-2501/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上连线吧。</li><li id="14cf" class="mo mp it lu b lv mx ly my mb mz mf na mj nb mn pc mu mv mw bi translated">请随意写下您的想法/建议/反馈。</li></ul><p id="99b8" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated"><strong class="lu iu">更新</strong>:另一篇文章给你一个在层内发生的事情的微观视角。</p><div class="oi oj gp gr ok ol"><a rel="noopener follow" target="_blank" href="/reading-between-the-layers-lstm-network-7956ad192e58"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd iu gy z fp oq fr fs or fu fw is bi translated">层间阅读(LSTM 网络)</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">使用 PyTorch 框架进行深度学习</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">towardsdatascience.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz ks ol"/></div></div></a></div></div></div>    
</body>
</html>