<html>
<head>
<title>An Alternative To Batch Normalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">批处理规范化的替代方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-alternative-to-batch-normalization-2cee9051e8bc?source=collection_archive---------6-----------------------#2019-11-08">https://towardsdatascience.com/an-alternative-to-batch-normalization-2cee9051e8bc?source=collection_archive---------6-----------------------#2019-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7214f8cfdab31ae9becf82633be0a542.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gg1Te-7SJfk9E2D-mORfw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image from Experfy</figcaption></figure><p id="9fc8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">批处理规范化(BN) </strong>作为一种规范化技术的发展是深度学习模型发展的转折点，它使各种网络能够训练和收敛。</p><p id="a569" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">尽管 BN 取得了巨大的成功，但它也表现出了一些缺点，这些缺点是由它沿批维度归一化的独特行为引起的。BN 的一个主要缺点是它需要足够大的批量来产生好的结果<em class="ld"> (for-eg 32，64)。</em>这个<em class="ld"> </em>禁止人们探索更高容量的模型，这将受到记忆的限制。为了解决这个问题<strong class="kh iu">脸书人工智能研究所【FAIR】</strong>开发了一种新的归一化技术<strong class="kh iu">【GN】</strong>。</p><blockquote class="le lf lg"><p id="ef6e" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">在本文中，我们将主要关注<strong class="kh iu">组标准化(GN) </strong>以及它如何作为<strong class="kh iu">批标准化(BN) </strong>和其他标准化变体(<strong class="kh iu">层标准化(LN)、实例标准化(IN) </strong>)的替代方案。</p></blockquote><h1 id="3fc4" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">群体规范化</h1><p id="3dd0" class="pw-post-body-paragraph kf kg it kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated"><strong class="kh iu">组归一化(GN) </strong> <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>提出 GN 作为将通道分成组并归一化每个组内的特征的层。GN 与批量大小无关，它不像 BN 那样利用批量维度。GN 在很大的批量范围内保持稳定。</p><blockquote class="le lf lg"><p id="090a" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">在本文的后半部分，你会更深刻地理解这几行字想表达的意思。所以抓紧了！！</p></blockquote><h1 id="b9f7" class="lk ll it bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">组规范化和其他规范化变体</h1><blockquote class="le lf lg"><p id="292e" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">在本节中，我们将首先描述特征规范化的一般公式，然后在此公式中表示组规范化和其他变体。</p></blockquote><p id="44c0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (1)通式</strong></p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/6c0fba327d6cb8d760f1a8513cef14c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*M4hw-BHn4KRj_e2MZXhQQQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Equation-1</figcaption></figure><p id="f085" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里<em class="ld"> x </em>是由层计算的特征，而<em class="ld"> i </em>是索引。对于一幅<em class="ld"> 2d </em>图像，<em class="ld"> i </em> = <em class="ld"> (i_N，i_C，i_H，i_W) </em>是一个<em class="ld"> 4d </em>向量，其形式为<em class="ld"> (N，C，H，W) </em>，其中<em class="ld"> N </em>是批次大小，<em class="ld"> C </em>是通道数，<em class="ld"> H </em>和<em class="ld"> W 这里和σ是平均值和标准差，计算公式如下:</em></p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ea4e08dfc0ee786f6bf2266ffb8ea356.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*Gwfs4vTrrhVSDnYXkk18zA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Equation-2</figcaption></figure><p id="b63d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里和σ是在由<em class="ld"> S_i. </em> <strong class="kh iu">定义的一组像素上计算的。所有这些归一化变量彼此不同，仅基于如何为它们中的每一个定义<em class="ld">S _ I</em></strong>。<em class="ld"> </em>变量<em class="ld"> m </em>和<em class="ld">ε</em>分别定义集合的大小和一个小常数<em class="ld">(for-eg 0.00001)</em><em class="ld">。添加ε</em>是为了确保我们在计算<em class="ld"> x_i </em>时不会被零除，但它也可以略微增加每批的方差。</p><p id="1691" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> (2)分组规范化和其他变量的制定</strong></p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/56cb3992de73d166eb6510290d5a9569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*FH6LP-y_He6oogmEQwXd_Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mv">Normalization Variants</strong>. Image from <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">Group Normalization paper</a></figcaption></figure><blockquote class="le lf lg"><p id="3d21" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">在上面的图像中，每个子图显示一个特征图张量，其中 N 为批次轴，C 为通道轴，而(H，W)为空间轴。蓝色像素使用相同的平均值和方差进行归一化，通过聚合这些像素的值来计算。</p></blockquote><ul class=""><li id="0135" class="mw mx it kh b ki kj km kn kq my ku mz ky na lc nb nc nd ne bi translated">在<strong class="kh iu">批定额</strong>、<strong class="kh iu">、</strong>套<em class="ld"> S_i </em>被<strong class="kh iu">、</strong>定义为:</li></ul><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/d88a0a84d0f65005e9fa7847890042ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*2OgzQ09F_I_nGXvOhuTJrA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Condition-1</figcaption></figure><p id="0293" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其中<em class="ld"> i_C </em>(和<em class="ld"> k_C </em>)表示 I(和 k)沿<em class="ld"> C </em>轴的分指数。这意味着共享相同通道索引的像素被一起归一化。这里和σ沿<em class="ld"> (N，H，W) </em>轴计算。</p><ul class=""><li id="c424" class="mw mx it kh b ki kj km kn kq my ku mz ky na lc nb nc nd ne bi translated">在<strong class="kh iu">图层规范</strong>中，集合<em class="ld"> S_i </em>定义为:</li></ul><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/1f7b603f30654ce35874036b3a1a31a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*vcP6-6BXfhHd9Kbl5MqTNA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Condition-2</figcaption></figure><p id="a710" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其中<em class="ld"> i_N </em>(和<em class="ld"> k_N </em>)表示 I(和 k)沿 N 轴的子索引。这意味着共享相同批索引的像素被一起归一化。这里和σ是沿着<em class="ld"> (C，H，W) </em>轴计算的。</p><ul class=""><li id="5f69" class="mw mx it kh b ki kj km kn kq my ku mz ky na lc nb nc nd ne bi translated">在<strong class="kh iu">实例规范</strong>中，集合 S_i 定义为:</li></ul><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/55ff3581a7cf660688f7543ded750d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*2cV9ptpayIQlyT3q4yMDGA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Condition-3</figcaption></figure><p id="c23c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于每个样本和每个通道，沿着<em class="ld"> (H，W) </em>轴计算这里的和σ。</p><ul class=""><li id="7dac" class="mw mx it kh b ki kj km kn kq my ku mz ky na lc nb nc nd ne bi translated">在<strong class="kh iu">组规范</strong>中，集合 S_i 定义为:</li></ul><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2495fa9c604a59e2a528a7cc607fcf13.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*l98kR9bjArBvvjGOWvTS5A.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Condition-4</figcaption></figure><p id="3807" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里<em class="ld"> G </em>是组数，是预定义的超参数<em class="ld">(默认 G = 32)</em>。<em class="ld"> C/G </em>是每组的通道数。看起来像支架的东西是地板操作。GN 沿着<em class="ld"> (H，W) </em>轴和一组<em class="ld"> C/G </em>通道计算和σ。For-eg 如果<strong class="kh iu"> <em class="ld"> G = 2 </em> </strong>和<strong class="kh iu"> <em class="ld">通道数= 6 </em> </strong>在<em class="ld">规格化变体图像(最右边)</em>中会有<strong class="kh iu"> <em class="ld"> 2 组</em> </strong>，每组有<em class="ld"> </em> <strong class="kh iu"> <em class="ld"> 3 个通道</em> </strong> <em class="ld"> (C/G</em></p><p id="1212" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此时，我们有一个标准化的值，表示为<em class="ld"> x_i. </em>，但是我们没有直接使用它，而是将它乘以一个<strong class="kh iu"> <em class="ld">伽马</em> </strong>值，然后加上一个<strong class="kh iu"> <em class="ld">贝塔</em> </strong>值。<em class="ld"> gamma </em>和<em class="ld"> beta </em>都是网络和服务的可学习参数，分别用于缩放和移动归一化值。因为它们就像权重一样是可以学习的，所以它们给了你的网络一些额外的旋钮来在训练中调整，以帮助它学习它试图逼近的函数。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bbc07977992b9aabbe444d1239015f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/1*KQCTjs5k1wEiVTSt-XzD9Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Equation-3</figcaption></figure><p id="4c59" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，我们有了层的最终批量标准化输出，然后我们将它传递给一个非线性激活函数，如 sigmoid、tanh、ReLU、Leaky ReLU 等。</p></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="c21a" class="lk ll it bd lm ln nr lp lq lr ns lt lu lv nt lx ly lz nu mb mc md nv mf mg mh bi translated">履行</h1><p id="e367" class="pw-post-body-paragraph kf kg it kh b ki mi kk kl km mj ko kp kq mk ks kt ku ml kw kx ky mm la lb lc im bi translated">到目前为止，我们已经讨论了组规范化以及它与其他规范化变体的不同之处。现在，是时候讨论实现细节了。在 Pytorch 和 Tensorflow 中，用几行代码就可以轻松实现 GN。所以，让我们开始吧…</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/1d43f50f5bdeb0eb4c35116c45f5c9c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*7wnHQ-Mo1tGS7NHTxuT3Og.jpeg"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Python code on Group Norm based on Tensorflow. Image from <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">Group Normalization paper</a>.</figcaption></figure><p id="75e9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">解释</strong></p><ul class=""><li id="ce99" class="mw mx it kh b ki kj km kn kq my ku mz ky na lc nb nc nd ne bi translated">这里的<em class="ld"> x </em>是形状为<em class="ld"> (N，C，H，W) </em>的输入特征。Gamma 和 beta:比例和形状偏移<em class="ld"> (1，C，1，1) </em>和<em class="ld"> G </em>是 GN 的组数。</li><li id="44ab" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated">对于每一批，我们以<em class="ld">【N，G，C//G，H，W】</em><em class="ld">(其中 C//G 是整数除法，定义了每组的通道数)</em>的形式对特征向量<em class="ld"> x </em>进行整形</li><li id="4b24" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated"><em class="ld"> tf.nn.moments </em>帮助计算每个批次沿轴<em class="ld">【C//G，H，W】的平均值和方差。</em>如果<em class="ld"> Keep_dims </em>为<em class="ld"> true </em>，则表示保留其长度为 1 的缩减尺寸。</li><li id="5039" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated">在归一化特征向量<em class="ld"> x(基于等式-1 的图像中的公式)</em>之后，它被整形为其初始形状<em class="ld">【N，C，H，W】。</em></li><li id="2c51" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated">此时，归一化值被表示为<em class="ld"> x. </em>，但是我们没有直接使用它，而是将它乘以一个<em class="ld"> gamma </em>值，然后加上一个<em class="ld"> beta </em>值<em class="ld">(shape(1，C，1，1)) </em>，并返回最终向量。</li></ul><p id="8e94" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果你对组规范化的 Pytorch 实现感兴趣，可以在这里找到<a class="ae mn" href="https://github.com/vjrahil/pytorch-groupnorm" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="b163" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu">注意:</strong>通过对代码做一些小的调整，GN 实现可以转换成 LN 或 in。</p><ul class=""><li id="7c66" class="mw mx it kh b ki kj km kn kq my ku mz ky na lc nb nc nd ne bi translated">在层标准化(LN)的情况下，设置组数(G) = 1。</li><li id="cf7e" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated">在实例规范化(In)的情况下，设置组数(G) =通道数(C)。</li></ul></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="aeda" class="lk ll it bd lm ln nr lp lq lr ns lt lu lv nt lx ly lz nu mb mc md nv mf mg mh bi translated">比较结果</h1><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d5b72e7afe93513cf93e500bb811e597.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/1*IlcXFIKLykwxNGEwHtci3Q.gif"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Comparison of error curves. Image from <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">Group Normalization paper</a></figcaption></figure><blockquote class="le lf lg"><p id="0e01" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">以下结果是通过在 ImageNet 训练集上以 32 个<strong class="kh iu">图像/GPU </strong>的批量训练 ResNet-50 模型获得的。左图是 ImageNet 训练错误，右图是 ImageNet 验证错误。默认情况下，GN 的 g 为 32。</p></blockquote><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2cbffa74d77a294ae42362d9ce598fab.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*OZAcNJ4zaoemKlzWLr2ZVQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Comparison of error rates. Image from <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">Group Normalization paper</a></figcaption></figure><p id="f614" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从上图我们可以推断出 BN 是最好的。第二好的归一化技术是 GN，其退化为<em class="ld"> 0.5%。</em></p><p id="6a8e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，让我们尝试在不同的批处理大小上训练 ImageNet 训练集。保持所有其他超参数不变，看看我们得到了什么。</p><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f2ad01b78310645a2c5d4db76422b6d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*ePWrdatGy7T9or2ONDKCnw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Sensitivity to batch size.Range of batch size = [32,16,8,4,2]. Image from <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">Group Normalization paper</a></figcaption></figure><blockquote class="le lf lg"><p id="2ebe" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">左图显示了在 ImageNet 训练中对批量大小的敏感度，右图显示了在 ImageNet 验证错误中对批量大小的敏感度。默认情况下，GN 的 g 为 32。</p></blockquote><figure class="mp mq mr ms gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/1f2e561684d38af57b1b1f2a75239c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*yzZt1bxaAh0lANiOrBxDBA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Sensitivity to batch size error comparison. Image from <a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">Group Normalization paper</a></figcaption></figure><p id="8fb2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从上图中，我们可以推断出，随着批量的减少，BN 的性能急剧下降。然而，GN 在很大的批量范围内保持稳定。因此，在您将处理小批量图像的地方，例如，<strong class="kh iu"> Fast/er </strong>和<strong class="kh iu"> Mask R-CNN </strong>框架，由于高分辨率，您将使用 1 或 2 个图像的批量大小，BN 将是无用的。因此，相反，我们可以在这样的框架中使用组归一化(GN)作为批量归一化(BN)的替代，在大批量时给出与 BN 相当的结果。</p><blockquote class="le lf lg"><p id="36b6" class="kf kg ld kh b ki kj kk kl km kn ko kp lh kr ks kt li kv kw kx lj kz la lb lc im bi translated">如果你想比较其他数据集的结果，你可以查看原始的<a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>以获得更多信息。</p></blockquote></div><div class="ab cl nk nl hx nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="im in io ip iq"><h1 id="3dda" class="lk ll it bd lm ln nr lp lq lr ns lt lu lv nt lx ly lz nu mb mc md nv mf mg mh bi translated">结论</h1><ul class=""><li id="1829" class="mw mx it kh b ki mi km mj kq og ku oh ky oi lc nb nc nd ne bi translated">GN 是一个有效的标准化层，不需要利用批次维度。</li><li id="e2b3" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated">GN 与 LN 和 IN 相关，这两种归一化方法在训练<strong class="kh iu">递归(RNN/LSTM) </strong>或<strong class="kh iu">生成(GAN)模型时特别成功。</strong></li><li id="2852" class="mw mx it kh b ki nx km ny kq nz ku oa ky ob lc nb nc nd ne bi translated">GN 在检测、分割和视频分类方面的改进表明，在这些任务中，GN 是强大的且目前占主导地位的 BN 技术的强有力替代。</li></ul><p id="07c2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">本文推导的所有结果都是基于<a class="ae mn" href="https://arxiv.org/pdf/1803.08494.pdf" rel="noopener ugc nofollow" target="_blank">群归一化</a>的原始论文。</p></div></div>    
</body>
</html>