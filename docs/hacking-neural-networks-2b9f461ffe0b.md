# 神经网络的安全漏洞

> 原文：<https://towardsdatascience.com/hacking-neural-networks-2b9f461ffe0b?source=collection_archive---------5----------------------->

## 当你的网络认为一切都是鸵鸟的时候怎么办。

神经网络无处不在，它们是可以被黑客攻击的。本文将深入研究神经网络和机器学习模型的对抗性机器学习和网络安全。

本文借用了哈佛大学 AC 209 b 讲座的内容，很大程度上归功于哈佛大学 IACS 系的讲师[帕夫洛斯·普罗托帕帕斯](https://iacs.seas.harvard.edu/people/pavlos-protopapas)。

*“人们担心计算机会变得太聪明并接管世界，但真正的问题是它们太笨了，它们已经接管了世界。”* ― **佩德罗·多明戈斯**

![](img/d6355b842ff91a887d5e7c72e781d3a3.png)

神经网络在现代世界中变得越来越普遍，并且它们的实现经常没有考虑到它们潜在的安全缺陷。这导致了一个新的网络安全领域，它着眼于神经网络的脆弱性，以及我们如何保护它们免受黑客的攻击。虽然这是一个相对严肃的问题，但在本文中，我将采用幽默的方式概述神经网络的弱点，希望读者会觉得有趣并有所收获。

![](img/ede87ca4e1d1f061f8efd224ce0d828e.png)

Adding strategic noise to an image can be used to fool neural networks. **Source:** Goodfellow et al., 2015.

# 概观

在这篇文章中，我将向读者全面介绍 [**对抗性机器学习**](https://en.wikipedia.org/wiki/Adversarial_machine_learning) 领域，其中试图愚弄、毒害或操纵机器学习模型，以规避其内在目的，通常是出于恶意目的。

我在本教程中要回答的问题是:

*   什么是对抗性机器学习？
*   我的机器学习模型怎么会被攻击？
*   我如何保护我的模型免受攻击？
*   这些攻击在现实世界中会产生什么后果？

由于神经网络的基础架构和学习过程，这个问题是神经网络最关心的问题，因此本文的大部分内容都集中在这些网络上，尽管也给出了其他示例。

# **现实世界中的神经网络**

你可能从来不知道，但你可能每天都在不假思索地使用神经网络。问过 Siri 一个问题吗？有一个神经网络。自动驾驶汽车？神经网络。你相机上的人脸识别？更多的神经网络。Alexa？再一次，神经网络…

这些只是有形的应用，人们每天都在使用神经网络的大量无形应用。无论是你在工作中使用的软件程序，还是仅仅为了寻找那天晚上去吃饭的地方，你都可能使用某种形式的神经网络(我这样说是因为有大量不同类型的网络)。

![](img/4bcb7e6718c12ebe09d975bfe9663976.png)

Some of the most common uses of neural networks today.

这些网络的普及性只会增加，这是有充分理由的。神经网络是 [**通用函数逼近器**](https://en.wikipedia.org/wiki/Universal_approximation_theorem) 。那是什么意思？这意味着，如果我的神经网络有足够大的容量(即足够多的节点——权重和偏差——可以修改)，那么我就能够使用神经网络逼近任何非线性函数。这是一个非常有力的陈述，因为我基本上可以学习任何函数，不管它有多复杂。

然而，我们为此付出了代价，那就是网络可能对你给它们的输入相当敏感。因此，可以说，如果你知道按哪个按钮是对的，就比较容易愚弄网络。通过操纵图像的某些节点，很容易激活与特定特征相关的神经元，这可以使网络给出虚假的输出。

这可能看起来相对无辜，毕竟，正如 Ian Goodfellow 在他的开创性论文中所示，看到神经网络随机将熊猫变成长臂猿是非常滑稽的。然而，这类攻击的实际应用可能会令人望而生畏。

我们以 Alexa 为例，假设你定期使用 Alexa 买东西。有一天，一个特别聪明的黑客坐在你的房子外面(在网络安全世界中通常被称为 wardriving ),入侵你的 Wifi——如果你没有妥善保护你的路由器或者仍然有默认密码，使用 **aircrack-ng** 这出奇地容易。

黑客现在可以访问 Alexa，它具有代表你进行交易的安全特权，给予你口头批准。如果黑客足够聪明，可以想象他们可以欺骗 Alexa 把你所有的钱都给黑客，只要按下神经网络上的正确按钮。

这可能会吓到你，你可能会去扔掉 Alexa。然而，仅仅因为这是可能的，并不意味着它是容易的。有些“攻击”可以在神经网络上实施，大多数拥有聪明工程师的公司至少已经考虑到这可能是一个问题。

我们现在将深入探讨如何攻击神经网络，可能的攻击类型，以及工程师和数据科学家如何帮助保护他们的网络免受这些潜在漏洞的影响。

# **对抗性攻击**

在这一节中，我将向读者介绍对抗性攻击的分类。

## **白盒攻击与黑盒攻击**

从本质上讲，对神经网络的攻击包括引入策略性放置的噪声，旨在通过错误地刺激对产生特定结果很重要的激活电位来欺骗网络。为了真正理解我说的“有策略地放置噪音”是什么意思，请考虑以下由谷歌大脑开发的图像，以展示同样的效果如何愚弄人类。你以为左边和右边的图都是猫吗？还是狗？还是各一个？

![](img/94fa575c68e79f366cd32d25b1eb3abd.png)

就具有决策边界的分类算法而言，这里是网络如何被引入的策略噪声破坏的图示。

![](img/8fe261507162f91f5ec91d0c9842274d.png)

Illustration of an adversarial attack in the feature space. **Source:** Panda et al. 2019

有两种主要类型的攻击是可能的:白盒攻击和黑盒攻击。灰箱攻击出现在网络安全领域，但不存在于神经攻击中。

当有人能够访问底层网络时，就会发生白盒攻击。因此，他们会知道网络的架构。这类似于对公司 IT 网络的白盒渗透测试——这是企业界测试公司 IT 基础设施防御能力的常规测试。一旦黑客了解了您的 IT 网络是如何构建的，就更容易进行破坏。在这种情况下,“知识就是力量”这句话尤其适用，因为了解网络的结构可以帮助您选择最具破坏性的攻击来执行，也有助于揭示与网络结构相关的弱点。

![](img/000d8b884677abe3a77c34afd0b3ee00.png)

White box attack. Architecture is known and individual neurons can be manipulated.

当攻击者对底层网络一无所知时，就会发生黑盒攻击。在神经网络的意义上，该架构可以被认为是一个黑盒。虽然对黑盒进行攻击更加困难，但它仍然不是不受影响的。黑盒攻击的底层程序首先由 [Papernot et al. (2015)](https://arxiv.org/pdf/1602.02697.pdf) 描述。

假设我们能够在网络上测试尽可能多的样本，我们可以通过将一组训练样本传入网络并获得输出来开发一个推断网络。然后，我们可以使用这些标记的训练样本作为我们的训练数据，并训练一个新的模型，以获得与原始模型相同的输出。

![](img/edb5bc88ce9f366b7e6e2c80fd2aa811.png)

一旦我们有了新的网络，我们可以为我们推断的网络开发对抗性的例子，然后使用这些例子对原始模型进行对抗性攻击。

![](img/2074e290377090407f9d299aca9177c1.png)

该模型不依赖于对网络体系结构的了解，尽管这将使攻击更容易进行。

![](img/b963a6b01658496c9d287ed539825fd9.png)

## **物理攻击**

您可能已经意识到，这些攻击都是软件攻击，但实际上也有可能是物理攻击网络。我不知道这种情况是否真的发生过，但是一些研究已经在研究使用“敌对标签”来欺骗网络。下面是一个例子。

![](img/cb1cf16dca7fa667db9b20133ef30e11.png)![](img/00eeb56c015946d698856cf89834044e.png)

Physical attacks on neural networks using adversarial stickers.

显然，这给自动驾驶汽车的大规模采用带来了一个潜在问题。没有人会希望自己的车无视停车标志，继续驶向另一辆车、一栋建筑或一个人。不过，不要太惊慌，有一些方法可以保护网络免受所有这些类型的攻击，我将在后面介绍。

## **闪避和毒药攻击**

到目前为止，我们讨论的所有攻击都是规避攻击，即它们涉及“愚弄”系统。一个很好的例子是欺骗垃圾邮件检测器来保护电子邮件帐户，这样你就可以将垃圾邮件发送到某人的收件箱中。垃圾邮件检测器通常使用某种形式的机器学习模型(如朴素贝叶斯分类器)，可用于单词过滤。如果一封电子邮件包含太多通常与垃圾邮件相关的“流行语”(给定你的电子邮件历史作为训练数据)，那么它将被归类为垃圾邮件。然而，如果我知道这些单词，我可以故意改变它们，使检测器不太可能将我的电子邮件视为垃圾邮件，我将能够欺骗系统。

另一个很好的例子是在计算机安全领域，机器学习算法通常在入侵检测系统(IDSs)或入侵防御系统(IPSs)中实现。当一个网络数据包到达我的电脑时，如果它带有恶意软件的特征签名，我的算法就会在它做出任何恶意行为之前阻止它。然而，黑客可以使用混淆代码来“迷惑”网络，使其无法发现问题。

最后一个例子是，麻省理工学院的一些研究人员开发了一只 3D 打印的乌龟，它的纹理能够骗过谷歌的物体检测算法，并使其将乌龟归类为步枪。鉴于谷歌的算法目前在许多行业用于商业目的，最后一个问题有点令人担忧。

中毒攻击包括损害算法的学习过程。这是一个比规避攻击稍微微妙和阴险的过程，但只对参与在线学习的模型有效，即他们在工作中学习，并在新的经验(数据)对他们可用时重新训练自己。这听起来可能不是太大的问题，直到我们考虑一些中毒攻击的例子。

回到我们的 IDSs 示例，由于新的病毒总是在开发中，所以这些都是通过在线学习不断更新的。如果希望防止零日攻击，有必要让这些系统具备在线学习的能力。攻击者可以通过注入精心设计的样本来破坏训练数据，最终危及整个学习过程。一旦发生这种情况，您的 IDS 基本上就变得毫无用处，您面临着更大的潜在病毒风险，甚至可能意识不到这一点。因此，中毒可被视为训练数据的敌对污染。我们的垃圾邮件检测器示例也是如此。

这一节给了我们一个大概的概述，介绍了我们可能会遇到的各种问题。在下一节中，我们将更仔细地研究如何处理白盒和黑盒攻击，最常见的敌对攻击类型，以及人们可以在其神经网络中使用的防御措施，以改善这些安全问题。

# **具体攻击类型**

Ian Goodfellow(生成对抗网络的创始人，也是创造这个术语的人)发表了第一篇研究神经网络潜在安全漏洞的论文。他决定把这叫做‘对抗性机器学习’，相对容易和生成性对抗性网络混淆。先说清楚，它们不一样。

伊恩描述了第一种攻击，快速梯度步进法。正如我们到目前为止所讨论的，这通过引入策略噪声来操纵分类器所使用的清晰的决策边界。

![](img/239a79570511cdbc564e1d4bf8fe1554.png)

When your neural network suddenly thinks everything is an ostrich… **Source:** Szegedy et al. (2013)

## 后 Goodfellow 2015 攻击

在过去几年中，出现了许多新的攻击媒介，主要有:

*   JSMA(基于雅可比的显著图)[ [Papernot et。艾尔，2016](https://arxiv.org/pdf/1511.07528.pdf)
*   C&W [ [卡里尼和瓦格纳，2016](https://arxiv.org/pdf/1608.04644.pdf)
*   库拉金等人。al，2017
*   I-FGSM [ [Tramer et。阿尔，2018](https://arxiv.org/pdf/1705.07204.pdf)

为了避免这篇文章拖得太长，我将不深入讨论每个算法的具体细节。但是，如果您希望我在以后的文章中更详细地介绍这些内容，请随意发表评论。

# 网络防御

已经开发了许多方法来保护神经网络免受我们已经讨论过的各种类型的攻击媒介。

## 对抗训练

防御对抗性攻击的最好方法是通过对抗性训练。也就是说，你主动生成对立的例子，调整它们的标签，并把它们添加到训练集中。然后，您可以在这个更新的训练集上训练新的网络，这将有助于使您的网络在对抗实例中更加健壮。

![](img/311888efabff6b7e86cbadcefa96ff36.png)

Adversarial training in action.

## **平滑决策边界**

正规化永远是答案。从这个意义上说，我们正则化的是数据的导数。这有助于平滑类之间的决策边界，并使使用策略噪声注入来操纵网络分类变得不那么容易。

![](img/707a4499a4bcd434c5a6a62ec39f78e4.png)

(Left) Hard decision boundary, (right) smoothened decision boundary.

## **搞混了**

Mixup 是一个简单的过程，乍一看似乎有点奇怪，它涉及通过某个因子λ混合两个训练样本，该因子介于 0 和 1 之间，并为这些训练样本分配非整数分类值。这有助于增加训练集，并降低网络的乐观分类倾向。它本质上扩散和平滑了类别之间的边界，并减少了分类对少量神经元激活电位的依赖。

![](img/f8ebbc7e392bff1a13af6492c1bfed9d.png) [## 混淆:超越经验风险最小化

### 大型深度神经网络是强大的，但表现出不良行为，如记忆和敏感性…

arxiv.org](https://arxiv.org/abs/1710.09412) 

## **挂钩**

对于那些不熟悉聪明的汉斯的故事的人，我建议你谷歌一下。主要的故事是关于一匹马，据说它能够通过跺一定次数的脚来做基本的算术。但后来发现，这匹马其实是在作弊，是在回应周围人群的言语和视觉线索。

![](img/99642e8f372c34d34964ce57a6a04724.png)[](https://github.com/tensorflow/cleverhans) [## 张量流/cleverhans

### 一个用于构建攻击、构建防御和基准测试的对抗性示例库…

github.com](https://github.com/tensorflow/cleverhans)  [## 欢迎来到 cleverhans 博客

### 与 cleverhans 相关的 Jekyll 博客

www.cleverhans.io](http://www.cleverhans.io/) 

本着同样的精神，CleverHans 是一个 Python 库，它被开发来测试机器学习系统对敌对例子的脆弱性。如果你正在开发一个神经网络，想看看它有多健壮，用 CleverHans 测试一下，你会发现对它的脆弱程度有一个概念。这类似于使用 Burp 套件来测试代码注入漏洞。

## **渗透测试**

就像任何形式的网络安全一样，你总是可以花钱请人来黑你，看看他们造成了多大的破坏。然而，你可以让他们签署一份文件，规定限制攻击者可以做什么。这让你知道自己在实际网络攻击中的脆弱程度。我不知道渗透测试公司目前是否提供这些类型的服务，但这不会让我感到惊讶。

# 最终意见

我希望你喜欢这篇文章，现在对这个有趣的网络安全新子领域有了更好的理解，它涉及通过各种攻击媒介损害机器学习模型。

这个领域仍然非常新，可以找到大量的研究论文，其中大部分可以在 arxiv 上找到，这意味着公众可以免费查看它们。我建议你读一读其中的一些——参考资料中的文章是一个很好的起点。

## 时事通讯

关于新博客文章和额外内容的更新，请注册我的时事通讯。

[](https://mailchi.mp/6304809e49e7/matthew-stewart) [## 时事通讯订阅

### 丰富您的学术之旅，加入一个由科学家，研究人员和行业专业人士组成的社区，以获得…

mailchi.mp](https://mailchi.mp/6304809e49e7/matthew-stewart) 

# 参考

[1] Panda，p .，Chakraborty，I .，和 Roy，k .，针对对抗性攻击的安全机器学习的离散化解决方案。2019.

[2]张，h .，西塞，m .，多芬，y .，洛佩兹-帕兹，d .2017.

[3] Goodfellow，I .，Shlens，j .，和 Szegedy，c.《解释和利用对立的例子》。2014.

[4]n . paper not，p . McDaniel，I . good fellow，Jha，s .和 z .切利克，Swami，a .对机器学习的实用黑盒攻击。2015.

[5] Papernot，n .，McDaniel，p .，Jha，s .，弗雷德里克松，m .，切利克，Z.B .，和 Swami，a.《对抗环境中深度学习的局限性》。《第一届 IEEE 欧洲安全与隐私研讨会论文集》，第 372–387 页，2016 年。

[6] Kurakin，a .，Goodfellow，I .，和 Bengio，s,《大规模的对抗性机器学习》。2016.

[7] Tramer `，f ,, Kurakin，a ,, paper not，n ,, good fellow，I ,, Boneh，d ,,和 McDaniel，p,《整体对抗训练:攻击和防御》。2018.