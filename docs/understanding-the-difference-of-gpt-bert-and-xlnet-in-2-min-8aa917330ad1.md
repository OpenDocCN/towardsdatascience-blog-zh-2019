# XLNet 如何结合 GPT 和伯特的精华

> 原文：<https://towardsdatascience.com/understanding-the-difference-of-gpt-bert-and-xlnet-in-2-min-8aa917330ad1?source=collection_archive---------11----------------------->

## 在 3 分钟内理解 GPT、伯特和 XLNet 的概念差异

![](img/2eda3106d7e707c07deec804e28f1285.png)

Centre Pompidou, Paris, France

XLNet 是一个新的预训练模型，它在 20 个任务上持续优于 BERT，通常是大幅度优于 BERT。

**什么？！为什么呢？**

如果不了解机器学习，就不难认为我们捕捉到的上下文越多，预测就越准确。因此，一个模型能够最深入、最有效地捕捉上下文的能力就是制胜的秘诀。

让我们来玩一个游戏——在下面的语境中**【猜测 1】**和**【猜测 2】**是什么？

['自然'，'语言'，'处理'，'是'，'一'，'婚姻'，'的'，**【guess 1】**，**【guess 2】**，'和'，'语言学']

考虑到 3 分钟的时间限制，让我来揭晓答案，相反，我会问你:你认为哪个模型(GPT、伯特、XLNet)最有助于找到答案。

**回答:** ['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，**，'机器'，**，**，'学习'，**，'和'，'语言学']

我们将继续使用 Pr(Guess|Context)符号。字面意思是根据上下文猜测的概率。

**GPT——我们从左向右阅读，所以我们不知道“机器”、“学习”之后的上下文:**

Pr('机器' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'])

Pr('学习' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'机器'])

知道“机器”实际上有助于你猜测“学习”，因为随着机器学习的流行，“学习”经常跟在“机器”后面。

**伯特——我们知道与 GPT 相反的两个方面，但我们是基于相同的上下文猜测“机器”和“学习”:**

Pr('机器' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'])

Pr('学习' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'])

拥有“语言学”实际上可以帮助你猜测“机器”的“学习”，因为你知道自然语言处理是机器学习和语言学的完美结合。即使你不知道，有了“语言学”的存在，你至少知道它不是“语言学”。

你可以看到 BERT 的明显缺点是，它不能说明“机器”和“学习”是一个相当常见的术语。

我们如何结合 GPT 和伯特的优点？

**XLNet——两者之长:**

排列！排列的力量在于，即使我们只从左向右阅读，排列也能让我们捕捉到两边的上下文(从左向右阅读，从右向左阅读)。

允许我们捕捉双方语境的排列之一:['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'，**，'机器'**，**，'学习'** ]

Pr('机器' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'])

Pr('学习' |['自然'，'语言'，'处理'，'是'，'婚姻'，'的'，'和'，'语言学'，'机器'])

这一次，你有了完整的上下文，在猜测“机器”之后，你可以立即猜测“学习”。你可以清楚地看到 XLNet 结合了 GPT 和伯特的优点。

仅此而已，希望这只是一个 3 分钟的阅读。如果你喜欢这篇文章，请鼓掌并分享！当然，如果你想知道更多，请阅读 XLNet [的论文](https://arxiv.org/pdf/1906.08237.pdf)。