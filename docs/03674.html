<html>
<head>
<title>Introduction to Multilayer Neural Networks with TensorFlow’s Keras API</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 的 Keras API 介绍多层神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-multilayer-neural-networks-with-tensorflows-keras-api-abf4f813959?source=collection_archive---------11-----------------------#2019-06-11">https://towardsdatascience.com/introduction-to-multilayer-neural-networks-with-tensorflows-keras-api-abf4f813959?source=collection_archive---------11-----------------------#2019-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/688e361cc211d8830790baaf326f5193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LkKz4wtZNBo5i-Vc8DWhTA.png"/></div></div></figure><div class=""/><p id="928e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">了解如何使用 TensorFlow 的高级 API Keras 构建和训练多层感知器！</p><p id="e070" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe kz la lb lc b">Keras</code>的开发始于 2015 年初。时至今日，它已经发展成为构建在<code class="fe kz la lb lc b">Theano</code>和<code class="fe kz la lb lc b">TensorFlow</code>之上的最流行和最广泛使用的库之一。它的一个突出特点是，它有一个非常直观和用户友好的 API，允许我们只用几行代码来实现神经网络。</p><p id="ed07" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe kz la lb lc b">Keras</code>也从 1.1.0 版本集成到了<code class="fe kz la lb lc b">TensorFlow</code>中。它是<code class="fe kz la lb lc b">contrib</code>模块的一部分(包含由<code class="fe kz la lb lc b">TensorFlow</code>贡献者开发的包，被认为是实验代码)。</p><p id="d2bb" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本教程中，我们将通过浏览以下内容来了解这个高级<code class="fe kz la lb lc b">TensorFlow</code> API:</p><ul class=""><li id="18eb" class="ld le je kd b ke kf ki kj km lf kq lg ku lh ky li lj lk ll bi translated">前馈神经网络的基础</li><li id="7d1a" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated">加载和准备流行的 MNIST 数据集</li><li id="07f0" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated">构建图像分类器</li><li id="c80d" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated">训练神经网络并评估其准确性</li></ul><p id="dfa1" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们开始吧！</p><blockquote class="lr ls lt"><p id="cdf1" class="kb kc lu kd b ke kf kg kh ki kj kk kl lv kn ko kp lw kr ks kt lx kv kw kx ky im bi translated"><em class="je">本教程改编自 Next Tech 的</em> <strong class="kd jf"> <em class="je"> Python 机器学习</em> </strong> <em class="je">系列的</em> Part 4 <em class="je">，带你从 0 到 100 用 Python 进行机器学习和深度学习算法。它包括一个浏览器内沙盒环境，预装了所有必要的软件和库，以及使用公共数据集的项目。这里</em>   <em class="je">可以免费上手</em> <a class="ae ly" href="https://c.next.tech/2YrZOMz" rel="noopener ugc nofollow" target="_blank"> <strong class="kd jf"> <em class="je">！</em></strong></a></p></blockquote></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h1 id="1bf6" class="mg mh je bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">多层感知器</h1><p id="c6aa" class="pw-post-body-paragraph kb kc je kd b ke ne kg kh ki nf kk kl km ng ko kp kq nh ks kt ku ni kw kx ky im bi translated">多层前馈神经网络是一种特殊类型的具有多个单个神经元的<em class="lu">全连接</em>网络。它们也被称为<strong class="kd jf">多层感知器</strong> ( <strong class="kd jf"> MLP </strong>)。下图说明了由三层组成的 MLP 的概念:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="ab gu cl nn"><img src="../Images/43ecec36de0f055abfd072e7477a4014.png" data-original-src="https://miro.medium.com/v2/format:webp/1*vWRGnasRs2zo3GhTHlmIfg.jpeg"/></div></figure><p id="563f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">上图中描绘的 MLP 有一个输入图层、一个隐藏图层和一个输出图层。隐藏层中的单元完全连接到输入层，输出层完全连接到隐藏层。如果这样的网络有不止一个隐含层，我们也称之为<strong class="kd jf">深度人工神经网络</strong>。</p><p id="d050" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们可以向 MLP 添加任意数量的隐藏层来创建更深层次的网络架构。实际上，我们可以将神经网络中的层数和单元数视为我们希望针对给定问题任务进行优化的附加超参数。</p><p id="380d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如上图所示，我们将第<em class="lu"> i </em>层中的第<em class="lu"> i </em>个激活单元表示为<em class="lu"> a_i^(l).</em>为了使数学和代码实现更加直观，我们将使用上标中的<em class="lu">作为输入层，使用<em class="lu"> h </em>上标作为隐藏层，使用<em class="lu"> o </em>上标作为输出层。</em></p><p id="9298" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">例如，<em class="lu"> a_i^(in) </em>是指输入层的第<em class="lu"> i </em>个值，<em class="lu"> a_i^(h) </em>是指隐藏层的第<em class="lu"> i </em>个单元，<em class="lu"> a_i^(out) </em>是指输出层的第<em class="lu"> i </em>个单元。这里，激活单元<em class="lu"> a_0^(in) </em>和<em class="lu"> a_0^(out) </em>是<strong class="kd jf">偏置单元</strong>，我们设置为等于<em class="lu"> 1 </em>。输入层中单元的激活仅仅是其输入加上偏置单元:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/351cf421d80890280af91d5517374d0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*utpugbLTvJmLH1Y6JB8ppQ.png"/></div></figure><p id="8d61" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">层<em class="lu"> l </em>中的每个单元通过一个权重系数连接到层<em class="lu"> l + 1 </em>中的所有单元。例如，层<em class="lu"> l </em>中第<em class="lu"> k </em>个单元到层<em class="lu"> l + 1 </em>中第<em class="lu"> j </em>个单元的连接将写成<em class="lu"> w_{k，j}^(l) </em>。回头参考前面的图，我们将连接输入到隐藏层的权重矩阵表示为<em class="lu"> W^(h) </em>，将连接隐藏层到输出层的矩阵表示为<em class="lu"> W^(out) </em>。</p><p id="c220" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们通过矩阵总结了连接输入层和隐藏层的权重:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/74bb42cff8d45e8a5f4147c53842b8b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*sxd64uqvQTp6SeyE-PCHug.png"/></div></figure><p id="73ff" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中<em class="lu"> d </em>是隐藏单元的数量，<em class="lu"> m </em>是包括偏置单元在内的输入单元的数量。因为理解这种符号对于理解本教程后面的概念很重要，所以让我们在一个简化的 3–4–3 多层感知器的描述性图示中总结一下我们刚刚学到的内容:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="ab gu cl nn"><img src="../Images/c72a5396e973e05f0d098511cdc98116.png" data-original-src="https://miro.medium.com/v2/format:webp/1*9Cfkv8E7r0pWN_EPB1BbKA.jpeg"/></div></figure><h1 id="9267" class="mg mh je bd mi mj nq ml mm mn nr mp mq mr ns mt mu mv nt mx my mz nu nb nc nd bi translated">MNIST 数据集</h1><p id="2e47" class="pw-post-body-paragraph kb kc je kd b ke ne kg kh ki nf kk kl km ng ko kp kq nh ks kt ku ni kw kx ky im bi translated">为了查看通过<code class="fe kz la lb lc b">tensorflow.keras</code> ( <code class="fe kz la lb lc b">tf.keras</code>)高级 API 进行的神经网络训练是什么样的，让我们实现一个多层感知器来对来自流行的混合国家标准与技术研究所(<strong class="kd jf"> MNIST </strong>)数据集的手写数字进行分类，该数据集用作机器学习算法的流行基准数据集。</p><p id="5b90" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了跟随本教程中的代码片段，您可以使用这个 Next Tech <a class="ae ly" href="https://c.next.tech/2YUIiAQ" rel="noopener ugc nofollow" target="_blank">沙箱</a>，它已经安装了 MNIST 数据集和所有必要的包。否则，您可以使用您的本地环境，在这里下载数据集<a class="ae ly" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="2f66" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">MNIST 数据集分为四个部分，如下所示:</p><ul class=""><li id="1fa3" class="ld le je kd b ke kf ki kj km lf kq lg ku lh ky li lj lk ll bi translated"><strong class="kd jf">训练集图像</strong> : <code class="fe kz la lb lc b">train-images-idx3-ubyte.gz</code> — 6 万个样本</li><li id="0fc5" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated"><strong class="kd jf">训练集标签</strong>:<code class="fe kz la lb lc b">train-labels-idx1-ubyte.gz</code>—60000 个标签</li><li id="3c59" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated"><strong class="kd jf">测试集图像</strong> : <code class="fe kz la lb lc b">t10k-images-idx3-ubyte.gz</code> — 1 万个样本</li><li id="6306" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated"><strong class="kd jf">测试集标签</strong>:<code class="fe kz la lb lc b">t10k-labels-idx1-ubyte.gz</code>—10000 个标签</li></ul><p id="2460" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">训练集由来自 250 个不同人的手写数字组成(50%是高中生，50%是人口普查局的员工)。测试集包含来自不同人的手写数字。</p><p id="9e22" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意<code class="fe kz la lb lc b">TensorFlow</code>也提供了相同的数据集，如下所示:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="2a42" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是，我们将使用 MNIST 数据集作为外部数据集，分别学习数据预处理的所有步骤。通过这种方式，您可以了解需要如何处理自己的数据集。</p><p id="2d08" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">第一步是通过在终端中运行以下命令来解压缩 MNIST 数据集的四个部分:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="880d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe kz la lb lc b">load_mnist</code>函数返回两个数组，第一个是一个<em class="lu"> n x m </em>维<code class="fe kz la lb lc b">NumPy</code>数组(<code class="fe kz la lb lc b">images</code>)，其中<em class="lu"> n </em>是样本的数量，<em class="lu"> m </em>是特征的数量(这里是像素)。MNIST 数据集中的图像由 28 x 28 像素组成，每个像素由一个灰度强度值表示。这里，我们将 28 x 28 像素展开成一维的行向量，表示我们的<code class="fe kz la lb lc b">images</code>数组中的行(每行或每幅图像 784)。由<code class="fe kz la lb lc b">load_mnist</code>函数返回的第二个数组(<code class="fe kz la lb lc b">labels</code>)包含相应的目标变量，即手写数字的类标签(整数 0-9)。</p><p id="528a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，按如下方式加载和准备数据集:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><pre class="nj nk nl nm gt nx lc ny nz aw oa bi"><span id="fc38" class="ob mh je lc b gy oc od l oe of">[Out:]<br/> Rows: 60000,  Columns: 784<br/> Rows: 10000,  Columns: 784<br/> (60000, 784) (60000,)<br/> (10000, 784) (10000,)</span></pre><p id="64e5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了了解 MNIST 的这些图像，让我们通过 Matplotlib 的<code class="fe kz la lb lc b">imshow</code>函数来可视化数字 0-9 的例子:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="8e81" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们现在应该可以看到一个 2 x 5 子图形的图，显示了每个唯一数字的代表性图像:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="ab gu cl nn"><img src="../Images/1c64889686cb5b6dd6205a09ed6c2e6e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*FzRxkCQ72duZnzClZf-Vdg.png"/></div></figure><p id="a855" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在让我们开始建立我们的模型！</p><h1 id="b508" class="mg mh je bd mi mj nq ml mm mn nr mp mq mr ns mt mu mv nt mx my mz nu nb nc nd bi translated">使用 TensorFlow 的 Keras API 构建 MLP</h1><p id="0234" class="pw-post-body-paragraph kb kc je kd b ke ne kg kh ki nf kk kl km ng ko kp kq nh ks kt ku ni kw kx ky im bi translated">首先，让我们为<code class="fe kz la lb lc b">NumPy</code>和<code class="fe kz la lb lc b">TensorFlow</code>设置随机种子，以便获得一致的结果:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="9486" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了继续准备训练数据，我们需要将类标签(整数 0–9)转换为独热格式。幸运的是，<code class="fe kz la lb lc b">Keras</code>为此提供了一个方便的工具:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><pre class="nj nk nl nm gt nx lc ny nz aw oa bi"><span id="d92a" class="ob mh je lc b gy oc od l oe of">First 3 labels:  [5 0 4]</span><span id="52c6" class="ob mh je lc b gy og od l oe of">First 3 labels (one-hot):<br/> [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]<br/> [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]<br/> [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]</span></pre><p id="f9a4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，让我们实现我们的神经网络！简而言之，我们将有三层，其中前两层(输入层和隐藏层)各有 50 个带有<code class="fe kz la lb lc b">tanh</code>激活函数的单元，最后一层(输出层)有 10 层，用于 10 个类别标签，并使用<code class="fe kz la lb lc b">softmax</code>给出每个类别的概率。<code class="fe kz la lb lc b">Keras</code>让这些任务变得非常简单:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="da7d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们使用<code class="fe kz la lb lc b">Sequential</code>类初始化一个新模型，以实现一个前馈神经网络。然后，我们可以添加尽可能多的层。然而，由于我们添加的第一层是输入层，我们必须确保<code class="fe kz la lb lc b">input_dim</code>属性匹配训练集中的特征(列)数量(在神经网络实现中为 784 个特征或像素)。</p><p id="cca5" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，我们必须确保两个连续层的输出单元(<code class="fe kz la lb lc b">units</code>)和输入单元(<code class="fe kz la lb lc b">input_dim</code>)的数量匹配。我们的前两层各有 50 个单元和一个偏置单元。输出图层中单元的数量应等于唯一分类标注的数量-一次性编码分类标注数组中的列数。</p><p id="17e3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意，我们使用<code class="fe kz la lb lc b">glorot_uniform</code> to 作为权重矩阵的初始化算法。<a class="ae ly" href="https://keras.io/initializers/#glorot_uniform" rel="noopener ugc nofollow" target="_blank"> <strong class="kd jf"> Glorot 初始化</strong> </a>是深度神经网络更鲁棒的初始化方式。偏差被初始化为零，这更常见，事实上是<code class="fe kz la lb lc b">Keras</code>中的默认设置。</p><p id="3afc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在编译我们的模型之前，我们还必须定义一个优化器。我们选择了<strong class="kd jf">随机梯度下降</strong>优化。此外，我们可以设置权重衰减常数和动量学习的值，以调整每个时期的学习速率。最后，我们将成本(或损失)函数设置为<code class="fe kz la lb lc b">categorical_crossentropy</code>。</p><p id="d419" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">二元交叉熵只是逻辑回归中成本函数的技术术语，分类交叉熵是通过<a class="ae ly" href="%5Bhttps://en.wikipedia.org/wiki/Softmax_function%5D(https://en.wikipedia.org/wiki/Softmax_function)" rel="noopener ugc nofollow" target="_blank"> softmax </a>对多类预测的推广。</p><p id="2371" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">编译完模型后，我们现在可以通过调用<code class="fe kz la lb lc b">fit</code>方法来训练它。这里，我们使用小批量随机梯度，批量大小为每批 64 个训练样本。我们训练 MLP 超过 50 个历元，我们可以通过设置<code class="fe kz la lb lc b">verbose=1</code>在训练过程中跟踪代价函数的优化。</p><p id="00f3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><code class="fe kz la lb lc b">validation_split</code>参数特别方便，因为它将在每个时期后保留 10%的训练数据(这里是 6000 个样本)用于验证，这样我们可以监控模型在训练期间是否过度拟合:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="614a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">打印成本函数的值在训练期间非常有用，可以快速发现训练期间成本是否在下降，并尽早停止算法。否则，需要调整超参数值。</p><p id="48f3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了预测类标签，我们可以使用<code class="fe kz la lb lc b">predict_classes</code>方法直接返回整数形式的类标签:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><pre class="nj nk nl nm gt nx lc ny nz aw oa bi"><span id="999d" class="ob mh je lc b gy oc od l oe of">First 3 predictions: [5 0 4]</span></pre><p id="3748" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，让我们在训练集和测试集上打印模型准确性:</p><figure class="nj nk nl nm gt iv"><div class="bz fp l di"><div class="nv nw l"/></div></figure><pre class="nj nk nl nm gt nx lc ny nz aw oa bi"><span id="2bdb" class="ob mh je lc b gy oc od l oe of">Training accuracy: 98.81<br/>Test accuracy: 96.27</span></pre></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="9a3d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我希望你喜欢这篇关于使用<code class="fe kz la lb lc b">TensorFlow</code>的<code class="fe kz la lb lc b">keras</code> API 来建立和训练一个用于图像分类的多层神经网络的教程！注意，这只是一个非常简单的神经网络，没有优化的调整参数。</p><p id="045c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在实践中，你需要知道如何通过调整学习率、动量、重量衰减和隐藏单元的数量来优化模型。您还需要学习如何处理消失梯度问题，即随着网络中图层的增加，误差梯度会变得越来越小。</p><p id="00bf" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="lu">我们将在 Next Tech 的</em> <strong class="kd jf"> <em class="lu"> Python 机器学习(第四部分)</em> </strong> <em class="lu">课程中讨论这些主题，以及:</em></p><ul class=""><li id="c4b5" class="ld le je kd b ke kf ki kj km lf kq lg ku lh ky li lj lk ll bi translated"><em class="lu">分解</em> <code class="fe kz la lb lc b"><em class="lu">TensorFlow</em></code> <em class="lu">的机制，如张量、激活函数、计算图、变量、占位符</em></li><li id="12b9" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated"><em class="lu">低级</em> <code class="fe kz la lb lc b"><em class="lu">TensorFlow</em></code> <em class="lu">和另一个高级 API，</em> <code class="fe kz la lb lc b"><em class="lu">Layers</em></code></li><li id="d23b" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated"><em class="lu">建模时序数据使用</em> <strong class="kd jf"> <em class="lu">递归神经网络</em></strong><em class="lu">【RNN】和</em> <strong class="kd jf"> <em class="lu">长短期记忆</em></strong><em class="lu">【LSTM】网络</em></li><li id="3b23" class="ld le je kd b ke lm ki ln km lo kq lp ku lq ky li lj lk ll bi translated"><em class="lu">用深度</em> <strong class="kd jf"> <em class="lu">卷积神经网络</em> </strong> <em class="lu"> (CNN)对图像进行分类。</em></li></ul><p id="10f3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="lu">这里</em>  <em class="lu">可以免费上手</em> <a class="ae ly" href="https://c.next.tech/2YrZOMz" rel="noopener ugc nofollow" target="_blank"> <em class="lu">！</em></a></p></div></div>    
</body>
</html>