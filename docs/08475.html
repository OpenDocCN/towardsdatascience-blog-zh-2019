<html>
<head>
<title>Calculate Similarity — the most relevant Metrics in a Nutshell</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算相似性——简而言之，最相关的指标</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/calculate-similarity-the-most-relevant-metrics-in-a-nutshell-9a43564f533e?source=collection_archive---------2-----------------------#2019-11-17">https://towardsdatascience.com/calculate-similarity-the-most-relevant-metrics-in-a-nutshell-9a43564f533e?source=collection_archive---------2-----------------------#2019-11-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/1d69f131ed612dad9f4dc80d0cc15593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4c9g_ahJp87wgkSs59be9g.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://www.pexels.com/de-de/@karolina-grabowska" rel="noopener ugc nofollow" target="_blank">Karolina Grabowska</a> on <a class="ae kf" href="http://www.pexels.com" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="756b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">许多数据科学技术都基于测量对象之间的相似性和不相似性。例如，K-最近邻使用相似性来分类新的数据对象。在无监督学习中，K-Means 是一种聚类方法，它使用欧几里德距离来计算聚类质心与其指定的数据点之间的距离。推荐引擎使用基于邻居的协作过滤方法，该方法基于与其他用户的相似性/不相似性来识别个人的邻居。</p><p id="f56a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇博文中，我将看看实践中最相关的相似性度量。可以用多种方式来测量对象之间的相似性。</p><p id="d281" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，我们可以将相似性度量分为两个不同的组:</p><ol class=""><li id="356c" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">基于相似性的指标:</li></ol><ul class=""><li id="e981" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld ln lk ll lm bi translated">皮尔逊相关</li><li id="1def" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld ln lk ll lm bi translated">斯皮尔曼相关</li><li id="d22b" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld ln lk ll lm bi translated">肯德尔氏τ</li><li id="5386" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld ln lk ll lm bi translated">余弦相似性</li><li id="e9f6" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld ln lk ll lm bi translated">雅克卡相似性</li></ul><p id="7194" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.基于距离的指标:</p><ul class=""><li id="90dd" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld ln lk ll lm bi translated">欧几里得距离</li><li id="c682" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld ln lk ll lm bi translated">曼哈顿距离</li></ul><h1 id="22a8" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">基于相似性的度量</h1><p id="acba" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">基于相似性的方法确定具有最高值的最相似的对象，因为这意味着它们生活在更近的邻域中。</p><h2 id="626e" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">皮尔逊相关</h2><p id="de37" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">相关性是一种研究两个定量连续变量之间关系的技术，例如年龄和血压。皮尔逊相关系数是与线性关系的强度和方向相关的度量。我们用以下方式计算向量<em class="ni"> x </em>和<em class="ni"> y </em>的度量:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/ba18256d0037b7d177357825afab8345.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*4JSKpD-YjekoSMxHdTdCOg.png"/></div></figure><p id="40f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi no"><img src="../Images/2d4b18e91b63a450a4686c090146b35c.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*5p2VYIyB3SfAN5vToNL0pw.png"/></div></div></figure><p id="c538" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">皮尔逊相关可以取值范围从-1 到+1。只有直接相关的增加或减少不会导致 1 或-1 的皮尔逊相关。</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi np"><img src="../Images/f95cf7a6555e91f054c59c2d137a784b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1mGeKY3dxkonA98qp0ZprA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: Wikipedia</figcaption></figure><p id="a211" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现:</strong></p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="9c03" class="mw lu it nr b gy nv nw l nx ny">import numpy as np<br/>from scipy.stats import pearsonr<br/>import matplotlib.pyplot as plt</span><span id="e65f" class="mw lu it nr b gy nz nw l nx ny"># seed random number generator<br/>np.random.seed(42)<br/># prepare data<br/>x = np.random.randn(15)<br/>y = x + np.random.randn(15)</span><span id="0622" class="mw lu it nr b gy nz nw l nx ny"># plot x and y<br/>plt.scatter(x, y)<br/>plt.plot(np.unique(x), np.poly1d(np.polyfit(x, y, 1))(np.unique(x)))<br/>plt.xlabel('x')<br/>plt.ylabel('y')<br/>plt.show()</span></pre><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a8d8b9d663d24abff858972d1766b9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*8tYJKMCMlpX1ChG63hQruQ.png"/></div></figure><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="a9b8" class="mw lu it nr b gy nv nw l nx ny"># calculate Pearson's correlation<br/>corr, _ = pearsonr(x, y)<br/>print('Pearsons correlation: %.3f' % corr)</span></pre><p id="349a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">皮尔逊相关系数:0.810</p><h2 id="e22a" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">斯皮尔曼相关</h2><p id="1fb1" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">Spearman 相关就是所谓的非参数统计，即分布不依赖于参数的统计(服从正态分布或二项式分布的统计就是参数统计的例子)。通常，非参数统计对数据进行排序，而不是取初始值。斯皮尔曼相关系数就是如此，其计算方法与皮尔逊相关类似。这些指标的区别在于，Spearman 的相关性使用每个值的排名。</p><p id="2eaa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了计算 Spearman 相关性，我们首先需要将我们的每个数据映射到排名数据值:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/804cc42582a65c8f8ff8c80068b41b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/format:webp/1*tW_YU4GZkU96kDzElpyBBA.png"/></div></figure><p id="eb4b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果原始数据为[0，-5，4，7]，则排名值将为[2，1，3，4]。我们可以用下面的方法计算 Spearman 相关性:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b6509b07c35e88925641778c45b8052a.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*E-_hI7m6UlM4YVdWxthnxA.png"/></div></figure><p id="15d2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/6dd693a4bb60d37b62ea3c2611ce780c.png" data-original-src="https://miro.medium.com/v2/resize:fit:208/format:webp/1*vAPxKMgyiDAPWIrHHTDO-A.png"/></div></figure><p id="9f4e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">斯皮尔曼的相关性基准单调关系，因此它可以有完美的非线性关系。它可以取值范围从-1 到+1。下面的情节阐明了皮尔逊和斯皮尔曼相关性之间的区别。</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/416a5afed825a3850a76706238889abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*gJ61bhv1MwpFpJk-yfO_HA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: Wikipedia</figcaption></figure><p id="c122" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于数据探索，我建议计算皮尔逊和斯皮尔曼的相关性。两者的比较会产生有趣的发现。如果 S&gt;P(如上图)，说明我们是单调关系，不是线性关系。由于线性简化了将回归算法拟合到数据集的过程，我们可能希望使用对数变换来修改非线性的单调数据，使其看起来是线性的。</p><p id="7f21" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现:</strong></p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="bd46" class="mw lu it nr b gy nv nw l nx ny">from scipy.stats import spearmanr<br/># calculate Spearman's correlation<br/>corr, _ = spearmanr(x, y)<br/>print(‘Spearmans correlation: %.3f’ % corr)</span></pre><p id="4dd5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">斯皮尔曼相关系数:0.836</p><h2 id="5cbd" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">肯德尔氏τ</h2><p id="b403" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">肯德尔的τ和斯皮尔曼的相关系数很相似。这两个度量都是关系的非参数度量。具体来说，Spearman 和 Kendall 的系数都是基于排名数据而不是原始数据计算的。</p><p id="e3da" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与 Pearson 和 Spearman 的相关性类似，Kendall 的 Tau 总是在-1 和+1 之间，其中-1 表示两个变量之间的强负关系，1 表示两个变量之间的强正关系。</p><p id="12b6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管 Spearman 和 Kendall 的测量值非常相似，但选择 Kendall 的测量值具有统计学优势，因为在使用较大样本量时，Kendall 的 Tau 具有较小的可变性。然而，Spearman 的度量在计算上更高效，因为 Kendall 的 Tau 是 O(n)，而 Spearman 的相关性是 O(nLog(n))。</p><p id="2745" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一步是对数据进行分级:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/804cc42582a65c8f8ff8c80068b41b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:128/format:webp/1*tW_YU4GZkU96kDzElpyBBA.png"/></div></figure><p id="31fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们计算肯德尔的τ值为:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi of"><img src="../Images/d597814f7ee73feb57b649c503349e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*aaH8Lhdkchpdp7q2GrM-fQ.png"/></div></div></figure><p id="d465" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 sgn 采用与排序值的差异相关联的符号。我们可以写</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/dbc32708a7ae200fb9dea54cb5730f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:206/format:webp/1*3U9SzqlSde188Bfr_0ajlw.png"/></div></figure><p id="d3bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如下所示:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9eef39cb0a0d1131be885312ea78b159.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*RXL6uLU4_gg1G9J0qWqu_w.png"/></div></figure><p id="b4af" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">的可能结果</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d54fe937f2919ac976b5cb229b3d9b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*N_hDfNokEcG-PmEJ9YJ7vQ.png"/></div></figure><p id="0b00" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是-1，0，+1。这些相加给出了 x 和 y 的等级指向相同方向的次数的比例。</p><p id="4f60" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现</strong></p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="1ef8" class="mw lu it nr b gy nv nw l nx ny"># calculate Pearson’s correlation<br/>from scipy.stats import kendalltau<br/>corr, _ = kendalltau(x, y)<br/>print(‘Kendalls tau: %.3f’ % corr)</span></pre><p id="2aa0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">肯德尔斯τ:0.695</p><h2 id="a689" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">余弦相似性</h2><p id="28f5" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">余弦相似度计算两个向量之间角度的余弦。为了计算余弦相似性，我们使用以下公式:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a4d84542cd63e8560f4f8c28f0ffee2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*DC_5-iSBziDMbLk6RpFgxQ.png"/></div></figure><p id="0fb4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下余弦函数:左边的红色向量指向不同的角度，右边的图形显示了结果函数。</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ce498b948f05c88f6137a559aa0e3d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*0BK4SLxnkQleq9pYWGyhMw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: mathonweb</figcaption></figure><p id="93a3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，余弦相似度可以取-1 和+1 之间的值。如果向量指向完全相同的方向，余弦相似度为+1。如果向量指向相反的方向，余弦相似度为-1。</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7883ff46e09865d5e48237885ce60770.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*ElwLV9oIJNyHkvw39nxV0w.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: O’Reilly</figcaption></figure><p id="7b03" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">余弦相似度在文本分析中非常普遍。它用于确定文档之间的相似程度，而不考虑文档的大小。TF-IDF 文本分析技术有助于将文档转换成向量，其中向量中的每个值对应于文档中一个单词的 TF-IDF 得分。每个单词都有自己的轴，余弦相似度决定了文档的相似程度。</p><p id="4a5e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现</strong></p><p id="9e73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要使用<em class="ni">来改变向量 x 和 y 的形状。reshape(1，-1) </em>计算单个样本的余弦相似度。</p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="a74f" class="mw lu it nr b gy nv nw l nx ny">from sklearn.metrics.pairwise import cosine_similarity<br/>cos_sim = cosine_similarity(x.reshape(1,-1),y.reshape(1,-1))<br/>print('Cosine similarity: %.3f' % cos_sim)</span></pre><p id="577f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">余弦相似度:0.773</p><h2 id="2425" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">雅克卡相似性</h2><p id="b70d" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">余弦相似度用于比较两个<em class="ni">实值向量</em>，而 Jaccard 相似度用于比较两个<em class="ni">二元向量(集)</em>。</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/1ee11b890256ca6cbf13e8464c016888.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*8xipdQq6-Ptw2aBuszYnDA.png"/></div></figure><p id="8bee" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在集合论中，看到公式的可视化通常是有帮助的:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7af7c4aa25f182897fca95675c656055.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*N2QFsvWRmN07ldlWRgzmVg.png"/></div></figure><p id="1f7d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到 Jaccard 相似性将交集的大小除以样本集的并集的大小。</p><p id="2d0b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">余弦相似度和雅克卡相似度都是计算文本相似度的常用度量。计算 Jaccard 相似性在计算上更昂贵，因为它将一个文档的所有术语与另一个文档匹配。Jaccard 相似性在检测重复项时非常有用。</p><p id="4bf3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现</strong></p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="744c" class="mw lu it nr b gy nv nw l nx ny">from sklearn.metrics import jaccard_score<br/>A = [1, 1, 1, 0]<br/>B = [1, 1, 0, 1]<br/>jacc = jaccard_score(A,B)<br/>print(‘Jaccard similarity: %.3f’ % jacc)</span></pre><p id="9032" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">雅克卡相似度:0.500</p><h1 id="9bf1" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">基于距离的度量</h1><p id="8520" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">基于距离的方法优先考虑具有最低值的对象，以检测它们之间的相似性。</p><h2 id="40e3" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">欧几里得距离</h2><p id="aeab" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">欧几里德距离是两个向量之间的直线距离。</p><p id="1774" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于两个矢量 x 和 y，可以计算如下:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/632d5393ad007443318ce5e793ce6190.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*J85lRNHSFqXCxcAzdrUI3Q.png"/></div></figure><p id="f73f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与余弦和 Jaccard 相似性相比，欧几里德距离在 NLP 应用的上下文中并不经常使用。它适用于连续的数值变量。欧几里德距离不是比例不变的，因此建议在计算距离之前缩放数据。此外，欧氏距离会成倍增加数据集中冗余信息的影响。如果我有五个高度相关的变量，我们将这五个变量都作为输入，那么我们将用五来加权这个冗余效应。</p><p id="927f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现</strong></p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="0d98" class="mw lu it nr b gy nv nw l nx ny">from scipy.spatial import distance<br/>dst = distance.euclidean(x,y)<br/>print(‘Euclidean distance: %.3f’ % dst)</span></pre><p id="0a68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">欧几里德距离:3.273</p><h2 id="b80c" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">曼哈顿距离</h2><p id="9ff7" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">与欧几里得距离不同的是曼哈顿距离，也称为“城市街区”，即一个向量到另一个向量的距离。当您无法穿过建筑物时，您可以将此度量想象为计算两点之间距离的一种方法。</p><p id="f913" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们计算曼哈顿距离如下:</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/e95184cb6424592fb39ca2a833cb4f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*DLNIK-YPHeOWqE7CcXyluA.png"/></div></figure><p id="f526" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">绿线给出的是欧几里德距离，而紫线给出的是曼哈顿距离。</p><figure class="nk nl nm nn gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/626cf68bc1d2e8f8d7d88ceef52ae0e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*8PYcXZ3oyqYkQOXZrl7lRQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Source: Quora</figcaption></figure><p id="de83" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在许多 ML 应用中，欧几里得距离是选择的度量。然而，对于高维数据，曼哈顿距离是优选的，因为它产生更稳健的结果。</p><p id="aa76" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">用 Python 实现</strong></p><pre class="nk nl nm nn gt nq nr ns nt aw nu bi"><span id="9e96" class="mw lu it nr b gy nv nw l nx ny">from scipy.spatial import distance<br/>dst = distance.cityblock(x,y)<br/>print(‘Manhattan distance: %.3f’ % dst)</span></pre><p id="2b34" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">曼哈顿距离:10.468</p><h1 id="8657" class="lt lu it bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">摘要</h1><p id="dd47" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">这篇博文概述了实践中使用的最相关的相似性度量。没有简单的如果-那么流程图来选择合适的相似性度量。我们首先需要理解和研究这些数据。然后，对于给定的数据科学问题，找到量化相似性的正确方法总是一个案例一个案例的决定。</p><h2 id="7b5d" class="mw lu it bd lv mx my dn lz mz na dp md kr nb nc mh kv nd ne ml kz nf ng mp nh bi translated">参考资料:</h2><p id="571f" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated"><a class="ae kf" href="https://bib.dbvis.de/uploadedFiles/155.pdf" rel="noopener ugc nofollow" target="_blank">https://bib.dbvis.de/uploadedFiles/155.pdf</a></p><p id="b2db" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="http://text2vec.org/similarity.html" rel="noopener ugc nofollow" target="_blank">http://text2vec.org/similarity.html</a></p><p id="a99d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="http://www.ijaret.org/2.10/COMPARATIVE%20ANALYSIS%20OF%20JACCARD%20COEFFICIENT%20AND%20COSINE%20SIMILARITY%20FOR%20WEB%20DOCUMENT%20SIMILARITY%20MEASURE.pdf" rel="noopener ugc nofollow" target="_blank">http://www.ijaret.org/2.10/COMPARATIVE%20ANALYSIS%20OF%<em class="ni">20 JAC card</em>% 20 系数% 20 余弦% 20 相似度% 20FOR % 20WEB %文档% 20 相似度%20MEASURE.pdf </a></p></div></div>    
</body>
</html>