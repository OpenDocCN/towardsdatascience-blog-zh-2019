<html>
<head>
<title>Object-Oriented Machine Learning Pipeline with mlflow for Pandas and Koalas DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向对象的机器学习流水线与熊猫和考拉数据框架的 mlflow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/object-oriented-machine-learning-pipeline-with-mlflow-for-pandas-and-koalas-dataframes-ef8517d39a12?source=collection_archive---------12-----------------------#2019-10-25">https://towardsdatascience.com/object-oriented-machine-learning-pipeline-with-mlflow-for-pandas-and-koalas-dataframes-ef8517d39a12?source=collection_archive---------12-----------------------#2019-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c219" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用熊猫、考拉、scikit-learn 和 mlflow 在 Python 中开发支持 Spark 的机器学习管道的端到端流程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bd8d6ff223148d76cbbe8ee6c6c96f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t-EB042Z8x5YUCD0cZiGzA.jpeg"/></div></div></figure><p id="2805" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在文章<a class="ae lq" rel="noopener" target="_blank" href="/python-data-preprocessing-using-pandas-dataframe-spark-dataframe-and-koalas-dataframe-e44c42258a8f"> Python 数据预处理使用熊猫数据帧、Spark 数据帧和考拉数据帧</a>中，我使用了一个公共数据集来评估和比较机器学习的典型数据预处理步骤中<a class="ae lq" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank">熊猫</a>、<a class="ae lq" href="https://spark.apache.org/releases/spark-release-2-4-4.html" rel="noopener ugc nofollow" target="_blank"> Spark </a>和<a class="ae lq" href="https://github.com/databricks/koalas" rel="noopener ugc nofollow" target="_blank">考拉</a>数据帧的基本功能。考拉的主要优势是支持类似 Spark 上熊猫的易用 API。</p><p id="f087" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我使用一个更具挑战性的数据集，Kaggle 竞赛的<a class="ae lq" href="https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem" rel="noopener ugc nofollow" target="_blank">面试出席问题</a>来演示使用熊猫、考拉、<a class="ae lq" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>和<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a>用 Python 为熊猫和考拉数据帧开发面向对象的机器学习管道的端到端过程。这是通过以下方式实现的:</p><ul class=""><li id="5b4e" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">使用 Pandas DataFrame 和 scikit-learn pipeline API 开发数据预处理管道</li><li id="738d" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">结合 scikit-learn pipeline API 和 Koalas DataFrame 为 Spark 开发数据预处理管道</li><li id="05b5" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">结合 scikit-learn 和 mlflow 开发机器学习流水线</li></ul><p id="5267" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">端到端开发流程基于<a class="ae lq" href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining" rel="noopener ugc nofollow" target="_blank">数据挖掘的跨行业标准流程</a>。如下图所示，它包括六个主要阶段:</p><ul class=""><li id="879a" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">商业理解</li><li id="5c25" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">数据理解</li><li id="7358" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">数据准备</li><li id="8d21" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">建模</li><li id="7181" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">估价</li><li id="200a" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">部署</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/79773541615f492636568420a6d1eca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*eapYiAVmG5HC9JU0QLZWEw.png"/></div></figure><p id="e7c4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">图 1: </strong> CRISP-DM 流程图(参考维基百科中的<a class="ae lq" href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining" rel="noopener ugc nofollow" target="_blank">来源)</a></p><p id="f1a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为方便讨论，假设在 Mac 等本地机器上安装了以下 Python 库:</p><ul class=""><li id="044c" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">Anaconda (conda 4.7.10)和 Python 3.6、Numpy、Pandas、Matplotlib 和 Scikit-Learn</li><li id="100b" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://spark.apache.org/releases/spark-release-2-4-4.html" rel="noopener ugc nofollow" target="_blank"> pyspark 2.4.4 </a></li><li id="873f" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://github.com/databricks/koalas" rel="noopener ugc nofollow" target="_blank">考拉</a></li><li id="4af5" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://www.mlflow.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a></li></ul><p id="a951" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用 Python 3.6 的原因是当前版本的<a class="ae lq" href="https://www.mlflow.org/docs/latest/index.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a>的某些功能(如部署)无法与 Python 3.7 兼容。</p><h1 id="1097" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">1.商业理解</h1><p id="ee93" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">第一阶段是业务理解。这个阶段的关键点是理解要解决的业务问题。作为一个例子，下面是对 Kaggle 面试出勤率问题的简要描述:</p><p id="4927" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定招聘人员在安排与候选人的面试时提出的一组问题，如何使用候选人对这些问题的回答来预测预期出席者是否会参加安排的面试(是、否或不确定)。</p><h1 id="4dd9" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">2.数据理解</h1><p id="4908" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">一旦理解了业务问题，下一步就是确定在哪里(即数据源)以及我们应该如何收集数据，从这些数据中可以构建问题的机器学习解决方案。</p><p id="6da0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">研究人员在 2014 年 9 月至 2017 年 1 月的两年多时间里，从印度的招聘行业收集了 Kaggle 面试出勤问题的数据集。</p><p id="76ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个数据集是用标签收集的(<em class="nd">观察到的出勤</em>的列保存标签)，因此它适用于监督机器学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/0aaf36454e7d621a67d203edaa7eb346.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jVpAifUQejwP8DaveV-Udg.png"/></div></div></figure><p id="b8b8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下代码为本文中的所有源代码导入必要的 Python 库，并将数据集加载到考拉数据帧中，并显示数据帧的前五行，如上表所示。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="e0be" class="nk mh it ng b gy nl nm l nn no">import numpy as np<br/>import pandas as pd<br/>import databricks.koalas as ks<br/>import matplotlib.pyplot as plt<br/>import matplotlib as mpl<br/>from   datetime import datetime<br/>import os</span><span id="ee85" class="nk mh it ng b gy np nm l nn no">from sklearn.base import BaseEstimator, TransformerMixin<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.externals import joblib</span><span id="a188" class="nk mh it ng b gy np nm l nn no">import mlflow<br/>import mlflow.sklearn</span><span id="8b95" class="nk mh it ng b gy np nm l nn no">from sklearn.model_selection import train_test_split<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.model_selection import GridSearchCV</span><span id="24c5" class="nk mh it ng b gy np nm l nn no">from sklearn.metrics import make_scorer<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import f1_score<br/>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score</span><span id="92e6" class="nk mh it ng b gy np nm l nn no">%matplotlib inline</span><span id="7fc3" class="nk mh it ng b gy np nm l nn no">ks_df = ks.read_csv('Interview_Attendance_Data.csv')<br/>ks_df.head()</span></pre><h1 id="2973" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">3.数据准备</h1><p id="db3a" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">数据准备的主要目标是清理收集的原始数据集并将其转换为适当的格式，以便转换后的数据可以被目标机器学习模型有效地使用。</p><p id="7ddf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在面试考勤原始数据集中，<em class="nd"> Name(Cand ID) </em>列包含候选人的唯一标识符，该标识符没有太大的预测能力，因此可以删除。此外，所有列(即对于考拉数据帧从<em class="nd"> _c22 </em>到<em class="nd"> _c26 </em>的列，或者对于熊猫数据帧从<em class="nd">未命名:22 </em>到<em class="nd">未命名:26 </em>的列)都没有数据，因此也可以安全地删除。</p><p id="3daf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">除了采访日期，数据集中的所有其他列都有分类(文本)值。为了使用机器学习来解决问题，必须将这些分类值转换为数值，因为机器学习模型只能使用数值数据。</p><p id="3a5e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">面试的<em class="nd">日期一栏应拆分为日、月、年，以提高预测能力，因为与作为整体的一串日期相比，个别日、月、年的信息往往与季节性工作更密切相关。</em></p><p id="cce1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">技能组合的<em class="nd">性质</em>和<em class="nd">候选人籍贯</em>列有大量唯一条目。这些将在一键编码后引入大量新的衍生功能。在数据集不够大的情况下，太多的特征会导致<a class="ae lq" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维数灾难</a>问题。为了缓解这一问题，这两列的值被重新划分到更少的存储桶中。</p><p id="933c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上述数据预处理/转换可以概括为以下步骤:</p><ul class=""><li id="0f69" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">桶装技能组合</li><li id="5179" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">存储候选人本地位置</li><li id="8bbd" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">解析面试日期</li><li id="66fb" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">将分类值改为大写并删除不太有用的特征</li><li id="fcb1" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">一次性编码分类值</li></ul><p id="e8c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些步骤是通过将熊猫和考拉数据帧与 scikit-learn 管道 API(即<em class="nd"> BaseEstimator </em>、<em class="nd"> TransformerMixin </em>和<em class="nd">管道</em>)相结合，为熊猫和考拉数据帧开发面向对象的数据预处理管道来实现的。</p><h2 id="ea61" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.1 转换列值</h2><p id="3544" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">几个数据预处理步骤共享一个转换数据帧中特定列的值的通用操作。但是，如<a class="ae lq" href="https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.Series.iloc.html#databricks.koalas.Series.iloc" rel="noopener ugc nofollow" target="_blank">考拉系列</a>所述，考拉系列不支持一些常见的熊猫数据帧和系列索引机制，如<em class="nd">df . iloc</em>【0】。因此，没有简单的方法来遍历和更改考拉数据帧中的列值。</p><p id="2bc3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一个困难是，考拉不允许从头构建一个新的考拉系列对象，然后将它作为一个新列添加到现有的考拉数据框架中。它只允许从考拉数据帧的现有列构建新的考拉系列对象。</p><p id="be5d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过定义一个全局函数来调用考拉系列对象的<em class="nd"> apply </em>()方法，可以避免上述困难。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="7be5" class="nk mh it ng b gy nl nm l nn no">def transformColumn(column_values, func, func_type):</span><span id="4b43" class="nk mh it ng b gy np nm l nn no">    def transform_column(column_element) -&gt; func_type:<br/>        return func(column_element)<br/>    <br/>    cvalues = column_values<br/>    cvalues = cvalues.apply(transform_column)<br/>    return cvalues</span></pre><h2 id="a079" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.2 铲斗技能组合</h2><p id="6f5c" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">为了缓解维数灾难问题，<em class="nd"> BucketSkillset </em>转换类的<em class="nd">转换</em>()方法通过将出现次数少于 9 次的值更改为<em class="nd"> Others </em>的一个相同字符串值，将 Skillset 列的唯一值划分为更小数量的桶。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="68e2" class="nk mh it ng b gy nl nm l nn no">class BucketSkillset(BaseEstimator, TransformerMixin):<br/>    def __init__(self):<br/>        self.skillset = ['JAVA/J2EE/Struts/Hibernate', 'Fresher', 'Accounting Operations', 'CDD KYC', 'Routine', 'Oracle', <br/>          'JAVA/SPRING/HIBERNATE/JSF', 'Java J2EE', 'SAS', 'Oracle Plsql', 'Java Developer', <br/>          'Lending and Liabilities', 'Banking Operations', 'Java', 'Core Java', 'Java J2ee', 'T-24 developer', <br/>          'Senior software engineer-Mednet', 'ALS Testing', 'SCCM', 'COTS Developer', 'Analytical R &amp; D', <br/>          'Sr Automation Testing', 'Regulatory', 'Hadoop', 'testing', 'Java', 'ETL', 'Publishing']       <br/>    <br/>    def fit(self, X, y=None):<br/>        return self<br/>    <br/>    def transform(self, X, y=None):  <br/>      <br/>        func = lambda x: x if x in self.skillset else 'Others'<br/>               <br/>        X1 = X.copy()<br/>        cname = 'Nature of Skillset'<br/>        cvalue = X1[cname]<br/>        <br/>        if type(X1) == ks.DataFrame:  <br/>            cvalue = transformColumn(cvalue, func, str)<br/>            X1[cname] = cvalue<br/>        elif type(X1) == pd.DataFrame:<br/>            X2 = map(func, cvalue)<br/>            X1[cname] = pd.Series(X2)<br/>        else:<br/>            print('BucketSkillset: unsupported dataframe: {}'.format(type(X1)))<br/>            pass<br/>            <br/>        return X1</span></pre><h2 id="77f6" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.3 存储候选人本地位置</h2><p id="7688" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">与 bucketing skillset 类似，为了缓解维数灾难问题，<em class="nd">bucket location</em>transformer 类的<em class="nd"> transform </em>()方法通过将那些出现次数少于 12 次的值更改为<em class="nd"> Others </em>的一个相同值，将候选本地位置列的唯一值划分为更小数量的桶。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="0690" class="nk mh it ng b gy nl nm l nn no">class BucketLocation(BaseEstimator, TransformerMixin):<br/>    def __init__(self):<br/>        self.candidate_locations = ['Chennai', 'Hyderabad', 'Bangalore', 'Gurgaon', 'Cuttack', 'Cochin', <br/>                          'Pune', 'Coimbatore', 'Allahabad', 'Noida', 'Visakapatinam', 'Nagercoil',<br/>                          'Trivandrum', 'Kolkata', 'Trichy', 'Vellore']<br/>        <br/>    <br/>    def fit(self, X, y=None):<br/>        return self<br/>    <br/>    def transform(self, X, y=None):            <br/>        X1 = X.copy()<br/>        <br/>        func = lambda x: x if x in self.candidate_locations else 'Others'<br/>        <br/>        cname = 'Candidate Native location'<br/>        cvalue = X1[cname]<br/>        if type(X1) == ks.DataFrame: <br/>            cvalue = transformColumn(cvalue, func, str)<br/>            X1[cname] = cvalue<br/>        elif type(X1) == pd.DataFrame:<br/>            X2 = map(func, cvalue)<br/>            X1[cname] = pd.Series(X2)<br/>        else:<br/>            print('BucketLocation: unsupported dataframe: {}'.format(type(X1)))<br/>            pass<br/>            <br/>        return X1</span></pre><h2 id="f917" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.4 解析面试日期</h2><p id="1567" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated"><em class="nd">采访日期</em>一栏的值很乱，因为使用了各种格式。例如，不仅使用不同的分隔符来分隔日、月和年，而且还遵循日、月和年的不同顺序。这由<em class="nd"> ParseInterviewDate </em>转换器类的 _ <em class="nd"> parseDate </em>()和<em class="nd"> transform_date </em>()方法来处理。<em class="nd"> transform </em>()方法的总体功能是将采访日期字符串分成各个日、月和年的值。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="0015" class="nk mh it ng b gy nl nm l nn no">class ParseInterviewDate(BaseEstimator, TransformerMixin):<br/>    def __init__(self):<br/>        pass   <br/>    def __parseDate(self, string, delimit):<br/>        try:<br/>            if ('&amp;' in string):<br/>                subs = tuple(string.split('&amp;'))<br/>                string = subs[0]<br/>        except:<br/>            print ('TypeError: {}'.format(string))<br/>            return None<br/>        <br/>        string = string.strip()<br/>        <br/>        try:<br/>            d = datetime.strptime(string, '%d{0}%m{0}%Y'.format(delimit))<br/>        except:<br/>            try:<br/>                d = datetime.strptime(string, '%d{0}%m{0}%y'.format(delimit))<br/>            except:<br/>                try:<br/>                     d = datetime.strptime(string, '%d{0}%b{0}%Y'.format(delimit))<br/>                except:<br/>                    try:<br/>                         d = datetime.strptime(string, '%d{0}%b{0}%y'.format(delimit))<br/>                    except:<br/>                        try:<br/>                            d = datetime.strptime(string, '%b{0}%d{0}%Y'.format(delimit))<br/>                        except:<br/>                            try:<br/>                                d = datetime.strptime(string, '%b{0}%d{0}%y'.format(delimit))<br/>                            except:<br/>                                d = None<br/>        return d<br/>    <br/>    def fit(self, X, y=None):<br/>        return self<br/>    <br/>    def transform(self, X, y=None):<br/>        <br/>        def transform_date(ditem):<br/>            if (isinstance(ditem, str) and len(ditem) &gt; 0):<br/>                if ('.' in ditem):<br/>                    d = self.__parseDate(ditem, '.')<br/>                elif ('/' in ditem):<br/>                    d = self.__parseDate(ditem, '/')<br/>                elif ('-' in ditem):<br/>                    d = self.__parseDate(ditem, '-')<br/>                elif (' ' in ditem):<br/>                    d = self.__parseDate(ditem, ' ')<br/>                else:<br/>                    d = None<br/>                    <br/>                if (d is None):<br/>                    return 0, 0, 0<br/>                else:<br/>                    return d.day, d.month, d.year<br/>                <br/>        def get_day(column_element) -&gt; int:<br/>            try:<br/>                day, month, year = transform_date(column_element)<br/>                return int(day)<br/>            except:<br/>                return 0<br/>        <br/>        def get_month(column_element) -&gt; int:<br/>            try:<br/>                day, month, year = transform_date(column_element)<br/>                return int(month)<br/>            except:<br/>                return 0<br/>        <br/>        def get_year(column_element) -&gt; int:<br/>            try:<br/>                day, month, year = transform_date(column_element)<br/>                return int(year)<br/>            except:<br/>                return 0<br/>            <br/>        def pandas_transform_date(X1):<br/>            days = []<br/>            months = []<br/>            years = []<br/>            ditems = X1['Date of Interview'].values<br/>            for ditem in ditems:<br/>                if (isinstance(ditem, str) and len(ditem) &gt; 0):<br/>                    if ('.' in ditem):<br/>                        d = self.__parseDate(ditem, '.')<br/>                    elif ('/' in ditem):<br/>                        d = self.__parseDate(ditem, '/')<br/>                    elif ('-' in ditem):<br/>                        d = self.__parseDate(ditem, '-')<br/>                    elif (' ' in ditem):<br/>                        d = self.__parseDate(ditem, ' ')<br/>                    else:<br/>                        d = None<br/>                    <br/>                    if (d is None):<br/>                        # print("{}, invalid format of interview date!".format(ditem))<br/>                        days.append(0) # 0 - NaN<br/>                        months.append(0)<br/>                        years.append(0)<br/>                    else:<br/>                        days.append(d.day) <br/>                        months.append(d.month)<br/>                        years.append(d.year)<br/>                else:<br/>                    days.append(0)<br/>                    months.append(0)<br/>                    years.append(0)<br/>        <br/>            X1['Year'] = years<br/>            X1['Month'] = months<br/>            X1['Day'] = days<br/>            <br/>            return X1<br/>        <br/>        X1 = X.copy()<br/>        <br/>        if type(X1) == ks.DataFrame: <br/>            X1['Year'] = X1['Date of Interview']<br/>            X1['Month'] = X1['Date of Interview']<br/>            X1['Day'] = X1['Date of Interview']<br/>        <br/>            func_map = {'Year' : get_year, 'Month' : get_month, 'Day' : get_day}<br/>            for cname in func_map:<br/>                cvalue = X1[cname]<br/>                cvalue = cvalue.apply(func_map[cname])<br/>                X1[cname] = cvalue<br/>        elif type(X1) == pd.DataFrame:<br/>            X1 = pandas_transform_date(X1)<br/>        else:<br/>            print('ParseInterviewDate: unsupported dataframe: {}'.format(type(X1)))<br/>            pass <br/>         <br/>        return X1</span></pre><h2 id="08cf" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.5 将分类值改为大写，并删除不太有用的特性</h2><p id="1861" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated"><em class="nd">features super case</em>transformer 类的<em class="nd"> transform </em>()方法是将分类特性的值改为大写，同时丢弃不太有用的特性。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="3617" class="nk mh it ng b gy nl nm l nn no">class FeaturesUppercase(BaseEstimator, TransformerMixin):<br/>    def __init__(self, feature_names, drop_feature_names):<br/>        self.feature_names      = feature_names<br/>        self.drop_feature_names = drop_feature_names<br/>    <br/>    def fit(self, X, y=None):<br/>        return self<br/>        <br/>    def transform(self, X, y=None):<br/>        <br/>        func = lambda x: x.strip().upper()<br/>        <br/>        X1 = X.copy()<br/>        <br/>        for fname in self.feature_names:<br/>            values = X1[fname]<br/>            values = values.fillna('NaN')<br/>            if type(X1) == ks.DataFrame: <br/>                values = transformColumn(values, func, str)<br/>            elif type(X1) == pd.DataFrame:<br/>                values = map(lambda x: x.strip().upper(), values)<br/>            else:<br/>                print('FeaturesUppercase: unsupported dataframe: {}'.format(type(X1)))   <br/>            X1[fname] = values<br/>            <br/>        # drop less important features<br/>        X1 = X1.drop(self.drop_feature_names, axis=1)<br/>            <br/>        return X1</span></pre><h2 id="c9f2" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.6 一次性编码分类值</h2><p id="d39b" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated"><em class="nd">onehotencodata</em>transformer 类的<em class="nd"> transform </em>()方法调用 DataFrame 的<em class="nd"> get_dummies </em>()方法对分类值的值进行一次性编码。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="d3de" class="nk mh it ng b gy nl nm l nn no">class OneHotEncodeData(BaseEstimator, TransformerMixin):<br/>    def __init__(self):<br/>        self.one_hot_feature_names = ['Client name', <br/>                        'Industry', <br/>                        'Location', <br/>                        'Position to be closed', <br/>                        'Nature of Skillset',<br/>                        'Interview Type', <br/>                        'Gender', <br/>                        'Candidate Current Location',<br/>                        'Candidate Job Location', <br/>                        'Interview Venue', <br/>                        'Candidate Native location',<br/>                        'Have you obtained the necessary permission to start at the required time',<br/>                        'Hope there will be no unscheduled meetings',<br/>                        'Can I Call you three hours before the interview and follow up on your attendance for the interview',<br/>                        'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',<br/>                        'Have you taken a printout of your updated resume. Have you read the JD and understood the same',<br/>                        'Are you clear with the venue details and the landmark.',<br/>                        'Has the call letter been shared', <br/>                        'Marital Status']<br/>        self.label_encoders   = None<br/>        self.one_hot_encoders = None<br/>        <br/>    def fit(self, X, y=None):       <br/>        return self<br/>    <br/>    def transform(self, X, y=None):  <br/>        X1 = X.copy()<br/>        if type(X1) == ks.DataFrame: <br/>            X1 = ks.get_dummies(X1)<br/>        elif type(X1) == pd.DataFrame:<br/>            X1 = pd.get_dummies(X1)<br/>        else:<br/>            print('OneHotEncodeData: unsupported dataframe: {}'.format(type(X1)))<br/>            pass<br/>        <br/>        return X1</span></pre><h2 id="1c69" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">3.7 将变压器合并到管道中</h2><p id="7fff" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">在<em class="nd"> PredictInterview </em>类的<em class="nd"> PreprocessData </em>()方法中，所有的数据预处理转换器被组合成一个 scikit-learn 管道(详见第 4.3 节)。一旦调用管道对象的<em class="nd"> fit_transform </em>()方法，这些变形器的<em class="nd"> fit </em>()和<em class="nd"> transform </em>()方法将被依次执行。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="8e55" class="nk mh it ng b gy nl nm l nn no">self.pipeline = Pipeline([<br/>            ('bucket_skillset', BucketSkillset()),<br/>            ('bucket_location', BucketLocation()),<br/>            ('parse_interview_date', ParseInterviewDate()),<br/>            ('features_to_uppercase', FeaturesUppercase(self.feature_names, self.drop_feature_names)),<br/>            ('one_hot_encoder', self.oneHotEncoder)<br/>        ])</span></pre><h1 id="cf45" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">4.建模</h1><p id="a088" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">准备好数据集后，下一步是建模。建模的主要目标包括:</p><ul class=""><li id="f5c0" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">识别机器学习模型</li><li id="8e2a" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">训练机器学习模型</li><li id="3dc8" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">调整机器学习模型的超参数</li></ul><h2 id="d71e" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">4.1 识别机器学习模型</h2><p id="65b5" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">有三种主要的高级类型的机器学习和深度学习算法/模型:</p><ul class=""><li id="c743" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">监督机器学习和深度学习</li><li id="b883" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">无监督机器学习和深度学习</li><li id="933f" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">强化学习</li></ul><p id="c02d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有监督的机器学习和深度学习可以分为回归和分类等子类型。每个子类型包括各种机器学习和深度学习算法/模型。例如，监督机器学习分类模型包括决策树分类器、随机森林分类器、GBM 分类器等。</p><p id="0e3b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一般来说，给定一个业务问题，有许多不同类型的模型可以用作可能的解决方案。需要对这些不同的模型进行比较，以确定最有希望的模型作为目标业务问题的解决方案。因此，模型识别不能孤立地进行。它依赖于模型训练和模型性能度量的评估/比较。</p><p id="7572" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我们简单地选择 sci kit-learn<em class="nd">RandomForestClassifier</em>模型进行演示。</p><h2 id="eec6" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">4.2 训练模型和调整超参数</h2><p id="0a96" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">一旦模型(例如，<em class="nd"> RandomForestClassifier </em>)被识别，通常有多个超参数要被调整。超参数是在模型训练可以开始之前需要设置的参数，并且这种超参数值在模型训练期间不会改变。例如，随机森林分类器具有多个超参数，例如估计器的<em class="nd">数量</em>、<em class="nd">最大深度</em>等。</p><p id="b5b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">sciket-learn<a class="ae lq" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a>是一个流行的库，通过多次自动执行模型的实例来搜索给定模型的超参数的最佳组合。每次执行对应于所选超参数值的唯一组合。<em class="nd"> GridSearch </em>类将使用这个库来寻找估计器的<em class="nd">数量和<em class="nd">最大深度</em>的最佳组合:</em></p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="b404" class="nk mh it ng b gy nl nm l nn no">class GridSearch(object):<br/>    def __init__(self, cv=10):<br/>        self.grid_param = [<br/>            {'n_estimators': range(68,69), # range(60, 70) <br/>             'max_depth'   : range(8,9)}   # range(5, 10)} <br/>        ]<br/>        self.cv = cv<br/>        self.scoring_function = make_scorer(f1_score, greater_is_better=True) <br/>        self.gridSearch = None<br/>        <br/>    def fit(self, X, y):<br/>        rfc = RandomForestClassifier()<br/>        self.gridSearchCV = GridSearchCV(rfc, self.grid_param, cv=self.cv, scoring=self.scoring_function)<br/>        self.gridSearchCV.fit(X, y)<br/>        return self.gridSearchCV.best_estimator_</span></pre><h2 id="5752" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">4.3 跟踪模型超参数和性能指标</h2><p id="ca16" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated"><a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a>的一个设计功能是跟踪和比较不同模型执行的超参数和性能指标。</p><p id="b9b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="nd"> PredictInterview </em>类的<em class="nd"> mlFlow </em>()的方法是训练一个模型，使用训练好的模型预测结果，获得各种模型性能指标，然后调用<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a> API 来跟踪超参数和性能指标，同时将训练好的模型记录到一个文件中，供以后使用，如部署。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="f12b" class="nk mh it ng b gy nl nm l nn no">def mlFlow(self):<br/>        np.random.seed(40)<br/>        with mlflow.start_run():<br/>            self.loadData()<br/>            self.PreprocessData()<br/>            self.trainModel()<br/>            self.predictClasses()<br/>            accuracy_score, f1_score, rmse_score, mae_score, r2_score = self.getModelMetrics()</span><span id="17b4" class="nk mh it ng b gy np nm l nn no">            best_params = self.gridSearch.gridSearchCV.best_params_</span><span id="db49" class="nk mh it ng b gy np nm l nn no">            mlflow.log_param("n_estimators", best_params["n_estimators"])<br/>            mlflow.log_param("max_depth", best_params["max_depth"])            <br/>            mlflow.log_metric("rmse", rmse_score)<br/>            mlflow.log_metric("r2", r2_score)<br/>            mlflow.log_metric("mae", mae_score)<br/>            mlflow.log_metric("accuracy", accuracy_score)<br/>            mlflow.log_metric("f1", f1_score)</span><span id="cb62" class="nk mh it ng b gy np nm l nn no">            mlflow.sklearn.log_model(self.rfc, "random_forest_model")</span></pre><p id="13c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的<em class="nd"> PredictInterview </em>类的 Jupyter 记事本和本文中的所有其他源代码都可以在 Github [6]中找到。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="edb5" class="nk mh it ng b gy nl nm l nn no">class PredictInterview(object):<br/>    def __init__(self, use_koalas=True):<br/>        self.use_koalas = use_koalas<br/>        self.dataset_file_name = 'Interview_Attendance_Data.csv'<br/>        self.feature_names = ['Date of Interview', <br/>                       'Client name', <br/>                       'Industry', <br/>                       'Location', <br/>                       'Position to be closed', <br/>                       'Nature of Skillset',<br/>                       'Interview Type', <br/>                       'Gender', <br/>                       'Candidate Current Location',<br/>                       'Candidate Job Location', <br/>                       'Interview Venue', <br/>                       'Candidate Native location',<br/>                       'Have you obtained the necessary permission to start at the required time',<br/>                       'Hope there will be no unscheduled meetings',<br/>                       'Can I Call you three hours before the interview and follow up on your attendance for the interview',<br/>                       'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',<br/>                       'Have you taken a printout of your updated resume. Have you read the JD and understood the same',<br/>                       'Are you clear with the venue details and the landmark.',<br/>                       'Has the call letter been shared', 'Marital Status']<br/>        <br/>        if self.use_koalas:<br/>            self.drop_feature_names = [<br/>                'Name(Cand ID)',<br/>                'Date of Interview', <br/>                '_c22',<br/>                '_c23',<br/>                '_c24',<br/>                '_c25',<br/>                '_c26']<br/>        else: # use Pandas<br/>            self.drop_feature_names = [<br/>                'Unnamed: 22',<br/>                'Unnamed: 23',<br/>                'Unnamed: 24',<br/>                'Unnamed: 25',<br/>                'Unnamed: 26']<br/>        <br/>        self.dataset = None<br/>        self.rfc     = None<br/>        self.gridSearch = None<br/>        self.X_train = None<br/>        self.y_train = None<br/>        self.X_test  = None<br/>        self.y_test  = None<br/>        self.y_pred  = None<br/>        self.X_clean = None<br/>        self.y_clean = None<br/>        self.X_train_encoded = None<br/>        self.X_test_encoded  = None<br/>        self.y_train_encoded = None<br/>        self.accuracy_score  = None <br/>        self.f1_score        = None<br/>        self.oneHotEncoder   = None<br/>        self.X_test_name_ids = None<br/>        self.pipeline = None<br/>        <br/>        <br/>    def loadData(self, path=None):<br/>        if (path != None):<br/>            path = os.path.join(path, self.dataset_file_name)<br/>        else:<br/>            path = self.dataset_file_name<br/>         <br/>        if self.use_koalas:<br/>            dataset = ks.read_csv(path)  <br/>        else:<br/>            dataset = pd.read_csv(path)<br/>        <br/>        # shuffle data <br/>        self.dataset = dataset.sample(frac=1.0) <br/>        <br/>        return self.dataset     <br/>    <br/>    def PreprocessData(self):<br/>        y = self.dataset['Observed Attendance']      # extract labels y<br/>        if self.use_koalas:<br/>            X = self.dataset.drop('Observed Attendance') # extract features X<br/>        else:<br/>            X = self.dataset.drop(['Observed Attendance'], axis=1) <br/>        <br/>        self.oneHotEncoder = OneHotEncodeData()<br/>        <br/>        self.pipeline = Pipeline([<br/>            ('bucket_skillset', BucketSkillset()),<br/>            ('bucket_location', BucketLocation()),<br/>            ('parse_interview_date', ParseInterviewDate()),<br/>            ('features_to_uppercase', FeaturesUppercase(self.feature_names, self.drop_feature_names)),<br/>            ('one_hot_encoder', self.oneHotEncoder)<br/>        ])<br/>        <br/>        X_1hot = self.pipeline.fit_transform(X)<br/>        <br/>        # fill up missing labels and then change labels to uppercase<br/>        y = y.fillna('NaN')<br/>        <br/>        if self.use_koalas:<br/>            func = lambda x: x.strip().upper()<br/>            y_uppercase = transformColumn(y, func, str) <br/>        else:<br/>            y_uppercase = map(lambda x: x.strip().upper(), y.values)<br/>            y_uppercase = pd.Series(y_uppercase)<br/>        <br/>        # separate labeled records from unlabeled records<br/>        self.X_train_encoded = X_1hot[y_uppercase != 'NAN']<br/>        self.X_test_encoded  = X_1hot[y_uppercase == 'NAN']<br/>        <br/>        # save Names/ID for reporting later one<br/>        self.X_test_name_ids = self.dataset['Name(Cand ID)'].loc[y_uppercase == 'NAN']<br/>        <br/>        y_train = y_uppercase.loc[y_uppercase != 'NAN']<br/>        <br/>        # encode labels as follows: 0 - NO, 1 - YES, NAN - NAN<br/>        if self.use_koalas:<br/>            func = lambda x: 1 if x == 'YES' else 0<br/>            y = transformColumn(y_train, func, int)<br/>        else:<br/>            y = map(lambda x: 1 if x == 'YES' else 0, y_train)<br/>            y = pd.Series(y)<br/>        <br/>        self.y_train_encoded = y<br/>        <br/>        self.X_clean = X_1hot<br/>        self.y_clean = y_uppercase<br/>        <br/>        return None<br/>    <br/>    def __splitData(self):<br/>        if self.use_koalas:<br/>            X_train_encoded = self.X_train_encoded.to_numpy()<br/>            y_train_encoded = self.y_train_encoded.to_numpy()<br/>        else:<br/>            X_train_encoded = self.X_train_encoded.values<br/>            y_train_encoded = self.y_train_encoded.values<br/>            <br/>        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X_train_encoded, <br/>                                                                                y_train_encoded, <br/>                                                                                test_size = 0.25, random_state = 0)<br/>        return (self.X_train, self.X_test, self.y_train, self.y_test)<br/>    <br/>    def trainModel(self):<br/>        X_train, X_test, y_train, y_test = self.__splitData()<br/>        self.gridSearch = GridSearch()<br/>        self.rfc = self.gridSearch.fit(X_train, y_train)<br/>        return self.rfc<br/>    <br/>    def predictClasses(self):<br/>        if (self.rfc is None):<br/>            print("No trained model available, please train a model first!")<br/>            return None<br/>        <br/>        self.y_pred = self.rfc.predict(self.X_test)<br/>        return self.y_pred<br/>    <br/>    def getModelMetrics(self):<br/>        if (self.y_test is None or self.y_pred is None):<br/>            print('Failed to get model performance metrics because y_test is null or y_pred is null!')<br/>            return None<br/>        <br/>        self.accuracy_score = accuracy_score(self.y_test, self.y_pred)<br/>        self.f1_score = f1_score(self.y_test, self.y_pred)<br/>        <br/>        pred = self.predictAttendanceProbability(self.X_test)[:, 1]<br/>        actual = self.y_test.astype(float)<br/>        <br/>        self.rmse_score = np.sqrt(mean_squared_error(actual, pred))<br/>        self.mae_score = mean_absolute_error(actual, pred)<br/>        self.r2_score = r2_score(actual, pred)<br/>        <br/>        return (self.accuracy_score, self.f1_score, self.rmse_score, self.mae_score, self.r2_score)<br/>    <br/>    def predictNullAttendanceProbability(self):<br/>        y_pred = self.rfc.predict_proba(self.X_test_encoded.to_numpy())<br/>        return y_pred<br/>    <br/>    def predictNullAttendanceClasses(self):<br/>        y_pred = self.rfc.predict(self.X_test_encoded.to_numpy())<br/>        return y_pred<br/>    <br/>    def predictAttendanceProbability(self, X):<br/>        y_pred = self.rfc.predict_proba(X)<br/>        return y_pred<br/>    <br/>    def predictAttendanceClass(self, X):<br/>        y_pred = self.rfc.predict(X)<br/>        return y_pred<br/>    <br/>    def mlFlow(self):<br/>        np.random.seed(40)<br/>        with mlflow.start_run():<br/>            self.loadData()<br/>            self.PreprocessData()<br/>            self.trainModel()<br/>            self.predictClasses()<br/>            accuracy_score, f1_score, rmse_score, mae_score, r2_score = self.getModelMetrics()</span><span id="ca24" class="nk mh it ng b gy np nm l nn no">            best_params = self.gridSearch.gridSearchCV.best_params_</span><span id="1e92" class="nk mh it ng b gy np nm l nn no">            mlflow.log_param("n_estimators", best_params["n_estimators"])<br/>            mlflow.log_param("max_depth", best_params["max_depth"])            <br/>            mlflow.log_metric("rmse", rmse_score)<br/>            mlflow.log_metric("r2", r2_score)<br/>            mlflow.log_metric("mae", mae_score)<br/>            mlflow.log_metric("accuracy", accuracy_score)<br/>            mlflow.log_metric("f1", f1_score)</span><span id="03f8" class="nk mh it ng b gy np nm l nn no">            mlflow.sklearn.log_model(self.rfc, "random_forest_model")</span></pre><p id="9f4a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的代码显示了如何实例化<em class="nd"> PredictInterview </em>类的一个对象，然后调用它的<em class="nd"> mlFlow </em>()方法。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="847a" class="nk mh it ng b gy nl nm l nn no">predictInterview = PredictInterview(use_koalas=True)<br/>predictInterview.mlFlow()</span></pre><h2 id="0de6" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">4.4 比较模型超参数和性能指标</h2><p id="c437" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">一旦在 mlflow 中跟踪了模型的超参数和性能指标，我们可以使用终端或 Jupyter 笔记本启动如下的<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a> UI(用户界面),查看模型执行的历史记录:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="7eeb" class="nk mh it ng b gy nl nm l nn no">!mlflow ui # for jupyter notebook</span></pre><p id="d06b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">假设<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a> UI 在本地机器上启动，以下 IP 地址和端口号可用于在 Web 浏览器中查看结果:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="bca5" class="nk mh it ng b gy nl nm l nn no">http://127.0.0.1:5000</span></pre><p id="c69f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下图是<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a> UI 中模型执行历史的快照:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/a6d98a638de5c4ee6cdbaace4521b15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfjlQixYhzQ_6k-2yto-eQ.png"/></div></div></figure><p id="57e6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">图 2: </strong>在 mlflow UI 中跟踪超参数和指标</p><h1 id="bd13" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">5.估价</h1><p id="b297" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">一旦机器学习模型被训练成具有预期的性能，下一步就是在受控的接近真实的设置中评估模型的预测结果，以获得模型有效、可靠并且满足部署的业务需求的信心。</p><p id="9c89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">例如，对于 Kaggle 面试出勤项目，一种可能的评估方法是使用<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a>将模型部署为 Web 服务，然后开发客户端程序来调用模型 Web 服务，以在经过数据准备后获得测试数据集的预测结果。然后，这些预测结果可用于生成报告(例如，表格或<em class="nd"> csv </em>文件),供招聘行业领域专家审阅。</p><p id="4eaf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">出于演示目的，下面的代码使用阈值为 0.5 的预测结果来为“观察到的出席率”列中缺少值的每个候选人生成概率和预测，并将结果形成为 Pandas 数据帧。</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="08e2" class="nk mh it ng b gy nl nm l nn no">pred_probs   = predictInterview.predictNullAttendanceProbability()<br/>pred_classes = predictInterview.predictNullAttendanceClasses()</span><span id="4cf8" class="nk mh it ng b gy np nm l nn no">x = predictInterview.X_test_name_ids.to_numpy() <br/>z = zip(x, pred_probs, pred_classes)<br/>answers = ('no', 'yes')</span><span id="03a1" class="nk mh it ng b gy np nm l nn no">result = [[x1, p1[1], answers[c]] for x1, p1, c in z]<br/>result_df = pd.DataFrame(np.array(result), columns=['Names/ID', 'Probability', 'Yes/No'])<br/>result_df.to_csv('interview_prediction.csv')<br/>result_df.head(15)</span></pre><p id="e1f0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是数据帧的前 15 行:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7531257a32c78456e7d41cbb73e83917.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*2NiX5sDQw1qlmeUFFvugIw.png"/></div></figure><h1 id="7b50" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">6.部署</h1><p id="e81b" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">一旦模型评估得出模型可以部署的结论，最后一步就是将评估后的模型部署到生产系统中。如<a class="ae lq" href="http://data-science-for-biz.com/" rel="noopener ugc nofollow" target="_blank">商业数据科学</a>一书中所述，部署的细节取决于目标生产系统。</p><p id="14fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以 Kaggle 面试考勤项目为例，一种可能的场景是将模型部署为服务器上的 Web 服务，可以被目标生产系统中的其他组件调用，以获得预测结果来辅助工作面试安排。在目标生产系统的开发基于不同于建模语言(例如，Python)的编程语言(例如，Java)的更复杂的情况下，模型有可能需要作为生产系统的组件在目标编程语言中重新实现。</p><h2 id="bbe6" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">6.1 将模型部署为 Web 服务</h2><p id="162c" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">如前所述，在<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a>中跟踪模型执行的过程中，一个经过训练的模型已经被记录到一个文件中。以下屏幕快照显示了已记录模型的信息:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/2c53fbd8f9baf01410b15676d4f8fabd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*drUXcQA-RqUGFEbm35PyyQ.png"/></div></div></figure><p id="24fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">图 3:</strong>ml flow UI 中的测井训练模型</p><p id="cf02" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html#serving-the-model" rel="noopener ugc nofollow" target="_blank"> mlflow 教程</a>类似，下面的代码将使用<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank"> mlflow </a>内置功能来启动一个日志模型作为 Web 服务:</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="2d75" class="nk mh it ng b gy nl nm l nn no">mlflow models serve -m /Users/xyz/machine-learning-spark/mlruns/0/258301f3ac5f42fb99e885968ff17c2a/artifacts/random_forest_model -p 1234</span></pre><h2 id="dcec" class="nk mh it bd mi nq nr dn mm ns nt dp mq ld nu nv ms lh nw nx mu ll ny nz mw oa bi translated">6.2 调用模型 Web 服务来预测结果</h2><p id="14c7" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">为简单起见，在本节中，假设<em class="nd"> test_df </em>是只有一行测试数据的 Pandas 数据帧(面试出席特征向量):</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="4b69" class="nk mh it ng b gy nl nm l nn no">test_df.head() </span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/48bd8eebc85138c60dc73737a506c677.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oSM_m9HjN-8qvr268V-lWA.png"/></div></div></figure><p id="7038" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下代码可用于将测试数据行发送到模型 Web 服务，以获得预测的面试出席率(1 -是，0 -否):</p><pre class="kj kk kl km gt nf ng nh ni aw nj bi"><span id="d741" class="nk mh it ng b gy nl nm l nn no">import requests<br/>import json</span><span id="5409" class="nk mh it ng b gy np nm l nn no">headers = {'Content-Type': 'application/json',<br/>           'Format': 'pandas-split'}</span><span id="1e96" class="nk mh it ng b gy np nm l nn no">url = '<a class="ae lq" href="http://127.0.0.1:1234/invocations'" rel="noopener ugc nofollow" target="_blank">http://127.0.0.1:1234/invocations'</a></span><span id="bcdf" class="nk mh it ng b gy np nm l nn no">headers_json_str = json.dumps(headers)<br/>headers_json_obj = json.loads(headers_json_str)<br/>data_json_obj = test_df.to_json(orient='split')</span><span id="fd54" class="nk mh it ng b gy np nm l nn no">response = requests.post(url, data=data_json_obj, headers = headers_json_obj)</span><span id="1896" class="nk mh it ng b gy np nm l nn no">response.text</span></pre><h1 id="5ecf" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">摘要</h1><p id="2902" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">在本文中，我使用了一个接近真实的具有挑战性的数据集，即 Kaggle 竞赛的<a class="ae lq" href="https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem" rel="noopener ugc nofollow" target="_blank">面试出勤问题</a>，通过将熊猫和考拉数据帧与 scikit-learn pipeline API 和 mlflow 相结合，演示了一个用 Python 为熊猫和考拉数据帧开发面向对象的机器学习管道的端到端过程。这个端到端的开发过程遵循<a class="ae lq" href="https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining" rel="noopener ugc nofollow" target="_blank">数据挖掘的跨行业标准过程</a>。为标准流程的每个阶段(除了第一阶段)提供了简短的描述和示例实现代码。Github [6]中提供了一个 Jupyter 笔记本和相应的 Python 源代码文件。</p><h1 id="34a4" class="mg mh it bd mi mj mk ml mm mn mo mp mq jz mr ka ms kc mt kd mu kf mv kg mw mx bi translated">参考</h1><p id="6e7c" class="pw-post-body-paragraph ku kv it kw b kx my ju kz la mz jx lc ld na lf lg lh nb lj lk ll nc ln lo lp im bi translated">[1]教务长，f .，福塞特，T. (2013)。业务数据科学，奥赖利，2013 年 7 月</p><p id="a0fa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2]杰龙，A. (2017)。使用 Scikit-Learn &amp; TensorFlow 进行动手机器学习，奥赖利，2017 年 3 月</p><p id="7e47" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3] mlflow 1.3.0 教程:<a class="ae lq" href="https://www.mlflow.org/docs/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank">https://www.mlflow.org/docs/latest/tutorial.html</a></p><p id="76e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4]面试考勤问题:<a class="ae lq" href="https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem/data" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/vishnusraghavan/The-Interview-Attendance-Problem/data</a></p><p id="407d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5]张，于(2019)。使用熊猫数据帧、Spark 数据帧和考拉数据帧的 Python 数据预处理:<a class="ae lq" rel="noopener" target="_blank" href="/python-data-preprocessing-using-pandas-dataframe-spark-dataframe-and-koalas-dataframe-e44c42258a8f">https://towards Data science . com/python-Data-预处理-使用-熊猫-数据帧-Spark-数据帧-和-考拉-数据帧-e4c 42258 a8f</a></p><p id="8d4e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6]张，于(2019)。<a class="ae lq" href="https://github.com/yzzhang/machine-learning/tree/master/Random_Forest/interview_attendance" rel="noopener ugc nofollow" target="_blank">Github 中的 Jupyter 笔记本</a></p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="e29d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">披露声明:2019 首创一。观点是作者个人的观点。除非本帖中另有说明，否则 Capital One 不隶属于所提及的任何公司，也不被这些公司认可。使用或展示的所有商标和其他知识产权是其各自所有者的财产。</p></div></div>    
</body>
</html>