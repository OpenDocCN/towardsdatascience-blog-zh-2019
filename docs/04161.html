<html>
<head>
<title>Generating Beatles Lyrics with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习生成披头士歌词</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-beatles-lyrics-with-machine-learning-1355635d5c4e?source=collection_archive---------6-----------------------#2019-06-30">https://towardsdatascience.com/generating-beatles-lyrics-with-machine-learning-1355635d5c4e?source=collection_archive---------6-----------------------#2019-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/35b49a7a32c6a00c0e56df1dec760589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5Oyn62pTisTUX4uQPcFzw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">(<a class="ae jg" href="https://www.rollingstone.com/music/music-album-reviews/review-the-beatles-sgt-peppers-anniversary-editions-reveal-wonders-121092/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><div class=""/><div class=""><h2 id="4266" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">语言模型和 OpenAI 的 GPT-2 的高级入门</h2></div><p id="1d11" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">甲壳虫乐队是一个巨大的文化现象。他们永恒的音乐至今仍在年轻人和老年人中引起共鸣。我个人是个超级粉丝。依我拙见，他们是有史以来最伟大的乐队。他们的歌曲充满了有趣的歌词和深刻的思想。以这些酒吧为例:</p><blockquote class="md me mf"><p id="062f" class="ky kz mg la b lb lc kk ld le lf kn lg mh li lj lk mi lm ln lo mj lq lr ls lt im bi translated">当你超越自己时，你会发现内心的平静在那里等着你</p></blockquote><p id="3352" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">强大的东西。然而，甲壳虫乐队的伟大之处在于他们的多才多艺。他们的一些歌曲深刻而有思想，而另一些则有趣而轻松。不出所料，贯穿他们歌词的最大主题是<em class="mg">爱情</em>。这里有一段这样的诗句:</p><blockquote class="md me mf"><p id="a43d" class="ky kz mg la b lb lc kk ld le lf kn lg mh li lj lk mi lm ln lo mj lq lr ls lt im bi translated">我亲爱的小宝贝，你难道看不出，当你属于我时是多么美好。要是我能永远做你的爱人就好了，我永远得不到我的心，我得不到我的思想。</p></blockquote><p id="b2b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，这些歌词不是你可能认识的任何一个甲壳虫乐队成员写的。不是列侬，或麦卡特尼，或哈里森，甚至，上帝保佑，林哥·斯塔尔(只是开玩笑，林戈是好的)。它们实际上是由一个机器学习模型生成的，即 OpenAI 的 GPT-2 [1]。虽然这使用了他们最小的模型，但结果是相当惊人的。</p><p id="3452" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是在我们走得太远之前，让我们后退一步，看看这一切是如何工作的。和往常一样，我的 Github 上有完整的工作代码<a class="ae jg" href="https://github.com/EugenHotaj/beatles" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="591e" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">语言建模</h1><p id="c0fe" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">语言模型试图学习一种语言的结构(例如英语或披头士的歌词)。它们是使用监督学习训练的生成模型。像其他监督学习任务一样，语言模型试图预测给定一些特征的标签。然而，与大多数监督学习任务不同，没有明确的标签，而是语言本身既充当特征又充当标签。</p><p id="069b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在高层次上，语言模型试图做的是，在给定一系列前一个单词的情况下，预测下一个单词。例如，一个好的语言模型可能预测到<em class="mg">“milk”</em>是短语<em class="mg">“购买一加仑 ____”的逻辑结论</em></p><p id="79a0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过尝试猜测下一个单词，我们真正做的是学习词汇的概率分布，这取决于我们到目前为止看到的单词。也就是说，我们想要学习</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/13b322736120818ac3589113e2f5ea79.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*canPje2h8hit_SehEKsKSw.png"/></div></figure><p id="019f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的词汇表中的单词在哪里？</p><p id="969d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为我们显式地对这个分布建模，所以我们可以用它做一些很酷的事情，比如用它来生成我们以前没有见过的单词。我们可以通过重复地从这个分布中抽取下一个单词，然后当我们抽取下一个单词时，用它作为条件，以此类推。具体来说，让我们看看这在 Python 中可能是什么样子。如果我们有一个带有<code class="fe nm nn no np b">sample</code>方法的<code class="fe nm nn no np b">model</code>对象，那么我们可以通过如下方式生成新的样本:</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">How we might generate sentences from a language model.</figcaption></figure><p id="61c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，我跳过了一些细节，但希望随着我们的继续，这些会变得更加清晰。现在，让我们考虑世界上最简单的语言模型，unigram。</p><p id="d8ed" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">unigram 模型忽略任何条件作用，只是从训练数据中随机选择下一个单词。这相当于把我们的训练数据扔进搅拌机，在高空搅拌 10 分钟后把里面的东西洒出来。可以说，我们不会产生任何类似英语的东西(当然，除非我们有一万亿只猴子和一万亿台搅拌机，或者是<a class="ae jg" href="https://en.wikipedia.org/wiki/Infinite_monkey_theorem" rel="noopener ugc nofollow" target="_blank">打字机</a>)。</p><h1 id="714e" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">二元模型</h1><p id="c237" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">一元模型之上的一步是二元模型。正如你可能已经从名字中猜到的，二元模型学习一个分布，这个分布只取决于<em class="mg">前一个单词，即</em></p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/8c5b313ec03faa48c1fd88b13253021c.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*6SLpkVSF5W9WJQrEjyDBZQ.png"/></div></figure><p id="a771" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为它非常简单，所以二元模型很容易在 Python 中实现，并且会让我们更深入地理解语言模型是如何工作的。</p><h2 id="e4c2" class="nt ml jj bd mm nu nv dn mq nw nx dp mu lh ny nz mw ll oa ob my lp oc od na oe bi translated">收集数据</h2><p id="fe27" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在实现之前，我们首先需要一些数据。我们的最终目标是创作出让披头士感到自豪的歌曲，所以让我们从收集他们所有已知的歌词开始吧。</p><p id="1dc5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我发现<a class="ae jg" href="http://toti.eu.com/beatles/index.asp" rel="noopener ugc nofollow" target="_blank">这个网站</a>把他们发行的每首歌的歌词分类。它也有一个有用的索引页面，上面有我们可以用来抓取网站的单首歌曲的链接。我编写了一个简单的脚本<a class="ae jg" href="https://github.com/EugenHotaj/beatles/blob/master/scraper.py" rel="noopener ugc nofollow" target="_blank">来迭代页面上的每个链接，解析它的 HTML 来提取歌词，并将歌词转储到一个文件中，每行一首歌。如果你打算继续下去，或者只是自己想要披头士的歌词，我强烈推荐使用它，因为抓取 HTML 是相当乏味和烦人的，即使使用像</a><a class="ae jg" href="https://www.crummy.com/software/BeautifulSoup/" rel="noopener ugc nofollow" target="_blank"> Beautiful Soup </a>这样的工具。</p><p id="8fb6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们有了一个好的、干净的格式的数据，剩下的就容易了。但是不要只相信我的话，从这张图表中来看:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/34e33e8e9692fa3a33df2d2fedc524d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VMAtWKEx71I8nf2o_ffTA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Data cleaning and organizing accounts for the biggest chunk of data science projects (<a class="ae jg" href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#2b97379b6f63" rel="noopener ugc nofollow" target="_blank">source</a>).</figcaption></figure><h2 id="6c14" class="nt ml jj bd mm nu nv dn mq nw nx dp mu lh ny nz mw ll oa ob my lp oc od na oe bi translated">构建模型</h2><p id="b534" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">如上所述，二元模型只是在前一个词的条件下对下一个词进行采样。我们可以做到这一点的一个简单方法是跟踪哪些单词跟随当前单词，以及出现的频率。也就是说，我们为训练数据中的每个单词<code class="fe nm nn no np b">current_word</code>保留一个字典，然后每当我们看到一个<code class="fe nm nn no np b">next_word</code>，我们就更新<code class="fe nm nn no np b">current_word[next_word] += 1</code>。然后，为了生成单词，我们只需在<code class="fe nm nn no np b">current_word</code>字典中查找所有单词和计数，并以与其计数成比例的概率对单词进行采样。下面是 Python 中完整模型的草图:</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">A sketch of the bigram language model.</figcaption></figure><p id="0009" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后要注意的是，我们可能想通过添加一些特殊的标记来表示歌词和歌曲的开始/结束，从而对歌词进行预处理。这是为了迫使我们的模型在生成新歌词时保持歌曲的一些结构，否则模型将会不断地吐出大块的文本。在我的代码中，我用<code class="fe nm nn no np b">XXSL, XXEL, XXSS,</code>和<code class="fe nm nn no np b">XXES</code>分别表示开始行、结束行、开始歌曲和结束歌曲。</p><p id="1eb5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，为了生成歌曲，我们可以从<code class="fe nm nn no np b">XXSS</code>标记开始，并一直调用<code class="fe nm nn no np b">model.predict()</code>，直到我们达到一个<code class="fe nm nn no np b">XXES</code>标记。</p><figure class="ni nj nk nl gt iv"><div class="bz fp l di"><div class="nq nr l"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Generating a song with the bigram model.</figcaption></figure><p id="2146" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">理论上，一旦循环停止，我们将生成一首前所未见的披头士歌曲。但是这有什么好处吗？</p><h2 id="56d5" class="nt ml jj bd mm nu nv dn mq nw nx dp mu lh ny nz mw ll oa ob my lp oc od na oe bi translated">一首前所未见的披头士歌曲</h2><p id="5fbd" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">这是 bigram 模型生成的一首歌曲的一小段:</p><blockquote class="md me mf"><p id="2db3" class="ky kz mg la b lb lc kk ld le lf kn lg mh li lj lk mi lm ln lo mj lq lr ls lt im bi translated">她是如此的我爱她的心。<br/>他们很好；他们说我这么多，<br/>她一点也不惊讶<br/>当你是我的时候<br/>悲伤和阿姆斯特丹的希尔顿<br/>他们让我走，<br/>是的我等一个男孩生下来就和一个有钱人在一起，<br/>大家一起分享</p></blockquote><p id="283a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">生成的样本听起来像一个疯子的胡言乱语，只有在我们非常幸运的情况下才有意义。</p><p id="c606" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以继续扩展二元模型来考虑前面的两个词。这就是所谓的三元模型。你可能会发现三元模型实际上会产生更好听的歌词。这是因为少了三个单词组合，所以模型在每一步都有较少的选择，在某些步骤中只有一个选择。一般来说，我们可以通过考虑前面的<em class="mg"> n </em>个单词来创建一个任意的<em class="mg"> n-gram </em>模型。当<em class="mg"> n </em>等于一首完整歌曲的长度时，你可能会发现这个模型非常适合生成披头士的歌曲。不幸的是，它生成的歌曲已经存在。</p><h2 id="5244" class="nt ml jj bd mm nu nv dn mq nw nx dp mu lh ny nz mw ll oa ob my lp oc od na oe bi translated">走向更好的模式</h2><p id="7733" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">二元模型的一个最明显的问题是，它将只使用它在训练数据中看到的单词和短语。虽然我们希望生成听起来像是披头士写的歌词，但我们不想只局限于他们使用的词。例如，如果甲壳虫乐队从未使用过“游行”这个词，那么 bigram 模型将不会生成任何关于“游行”的歌曲。当然，由于我们只训练披头士的歌词，我们不可能指望我们的模型使用从未见过的词。我们需要的是在巨大的语料库上进行训练，比如<a class="ae jg" href="https://www.wikipedia.org/" rel="noopener ugc nofollow" target="_blank">维基百科</a>或者<a class="ae jg" href="https://www.reddit.com/" rel="noopener ugc nofollow" target="_blank"> Reddit </a>。</p><p id="44cc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，即使我们在所有的维基百科上训练，并且看到英语中的每一个单词，我们的二元模型仍然太死板了。比如说“高个子”这个短语。每一个对英语有基本了解的人都会意识到“高”只是“人”的修饰词，与“人”无关。相反，“高”可以用来修饰无数其他事物，如“女人”、“男孩”、“建筑物”、“长颈鹿”等。然而，我们的二元模型无法了解这一点，而是必须在使用“tall”之前至少看到它的一次使用。所以如果模特只见过“高个子”、“高个子男生”、“高个子女生”，而没有见过“高个子女生”，就它而言，“高个子女生”这个词组甚至不存在。</p><p id="82e1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，我们想要的是一个具有更丰富的词汇表和对词汇表中单词之间关系的更深入理解的模型。幸运的是，聪明的研究人员已经发明了如此强大的模型，我们可以用它们来创作更好的歌曲。</p><h1 id="3a4f" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">GPT-2 模型</h1><p id="aa46" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">OpenAI 的 GPT-2 模型[1]最近因为“太危险而不能发布”而成为头条新闻该模型生成了如此令人信服的文本，以至于作者认为它可能被用于恶意 purposes⁴.相反，他们发布了两个较小的版本供人们玩和试验。我们将使用两者中最小的一个来生成披头士的歌词。</p><p id="22bf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GPT-2 是一个基于<a class="ae jg" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">变形金刚</a>的模型，它是在数百个 GPU 小时的海量 Reddit 数据上训练的。在训练期间，它能够学习一种非常好的英语语言模型(或者至少是 Reddit 上使用的英语版本)。这意味着它能够理解“高”可以用于人类、建筑物或长颈鹿。此外，因为它在 Reddit 的很大一部分上进行了训练，所以它可能看到了英语中 99.9%的单词和短语。这对我们来说是个好消息，因为这正是我们想要的:丰富的词汇和对如何使用这些词汇的深刻理解。</p><p id="491a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，如果我们打开模型，让它生成一些东西，它极不可能得出类似披头士的歌词(即使<a class="ae jg" href="https://www.reddit.com/r/beatles/" rel="noopener ugc nofollow" target="_blank">r/披头士</a>存在)。这是因为模型不知道我们关心的是生成披头士的歌词，毕竟这不是它被训练要做的。相反，我们需要推动模型做我们想让它做的事情。我们可以做到这一点的一个方法是通过迁移学习。</p><h2 id="57dc" class="nt ml jj bd mm nu nv dn mq nw nx dp mu lh ny nz mw ll oa ob my lp oc od na oe bi translated">迁移学习</h2><p id="7bc9" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">迁移学习是指我们可以通过做一件事来利用我们所学的信息，并应用它来解决一件相关的事情。例如，当你开始阅读这篇文章时，你不必重新学习什么是单词，哪些单词跟随哪些其他单词，或者它们如何组合在一起构成句子。想象一下那会有多乏味。相反，你利用花在阅读 AP 文学书籍上的所有时间来理解我现在在说什么(我猜<a class="ae jg" href="https://en.wikipedia.org/wiki/Adventures_of_Huckleberry_Finn" rel="noopener ugc nofollow" target="_blank">哈克芬</a>终究派上了用场)。</p><p id="a366" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以类似的方式，我们可以利用 GPT 2 号通过数百小时阅读 Reddit 帖子学到的知识，并将其转移到我们生成披头士歌词的任务中。高层次的想法是采取预先训练的模型，并继续训练它一段时间。然而，代替 Reddit 的帖子，我们将只使用刮掉的披头士的歌词<em class="mg"/>。这将使模型严重偏向于生成类似披头士的歌曲。</p><p id="5f71" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将在这里跳过具体如何做，因为这将需要另一个类似长度的帖子来解释一切。相反，如果你对确切的细节感兴趣，我会让你参考[2]。这是一篇很棒的博文，一步一步地指导你如何将 GPT-2 模型应用到你关心的任何语言任务中。这也是我得到这里显示的结果所遵循的。</p><h2 id="8802" class="nt ml jj bd mm nu nv dn mq nw nx dp mu lh ny nz mw ll oa ob my lp oc od na oe bi translated">新甲壳虫乐队</h2><p id="cf64" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">像所有好的深度学习结果一样，我在简介中发布的歌词是经过精心挑选的。生成的歌曲并不都一样好，它们的质量取决于你在微调阶段的位置。当模型仍然严重不符合训练数据时，在对大约 100 个小批量进行微调后，您可能会得到以下结果:</p><blockquote class="md me mf"><p id="0256" class="ky kz mg la b lb lc kk ld le lf kn lg mh li lj lk mi lm ln lo mj lq lr ls lt im bi translated">我永远爱你，列侬，麦卡特尼，你永远爱我。你永远爱我，你也永远爱我。我永远都是</p></blockquote><p id="44cd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这样又持续了 10-15 行。至少比<a class="ae jg" href="https://en.wikipedia.org/wiki/Lil_Pump" rel="noopener ugc nofollow" target="_blank"> Lil' Pump </a>要好。</p><p id="3e96" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">玩笑归玩笑，我最感兴趣的是前两行。在训练数据中，每首歌曲的第一行是标题，第二行是作者，后面几行是实际的歌词。即使在这个早期阶段，模型已经设法学习我们的数据结构:第一行和第二行是特殊的；在第二行中，可能出现的单词组合很少，最有可能是 Lennon &amp; McCartney。</p><p id="a7e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们对大约 350 个小批量进行微调，该模型就会开始生成更加可信的歌词，比如介绍中的这首，或者这首:</p><blockquote class="md me mf"><p id="6238" class="ky kz mg la b lb lc kk ld le lf kn lg mh li lj lk mi lm ln lo mj lq lr ls lt im bi translated">黑衣女人<br/>列侬&amp;麦卡特尼<br/>我要大吵大闹<br/>如果你不想让我出现<br/>你最好离我远点。<br/>我濒临死亡，我恋爱了</p></blockquote><p id="9aa1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不完美，但还不错。最后，如果我们长时间保持微调(大约 2800 分钟一批)，就会发生这种情况:</p><blockquote class="md me mf"><p id="7b67" class="ky kz mg la b lb lc kk ld le lf kn lg mh li lj lk mi lm ln lo mj lq lr ls lt im bi translated">回来(回来)<br/>列侬&amp;麦卡特尼<br/>黄色潜水艇<br/>列侬&amp;麦卡特尼<br/>在一个周六的夜晚，当太阳照在我身上的时候<br/>太阳出来了，船帆清晰<br/>太阳出来了，船帆清晰<br/>哦——嘿</p></blockquote><p id="36c1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型开始过度拟合，并且生成的样本是很可能出现在训练数据中的东西，例如重复的“Lennon &amp; McCartney”线、“黄色潜水艇”等。我发现微调 300-500 步可以产生最好的歌词。</p><h1 id="73dc" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">结论</h1><p id="a1cb" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">希望您现在对语言模型如何工作以及我们如何利用最先进的模型来极大地改进下游任务有了更好的了解。</p><p id="d971" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也就是说，GPT-2 模型还有很多值得探索的地方。我生成的样本使用了默认的(可能)次优超参数。如果花更多的时间进行微调，看看生成的歌词会有多好，这将是一件有趣的事情。我也只使用了最小的型号，因为我是用我的笔记本电脑来训练的。我相信更大的模型会产生更壮观的结果。最后，OpenAI 最近发布了<a class="ae jg" href="https://openai.com/blog/musenet/" rel="noopener ugc nofollow" target="_blank"> MuseNet </a>，它能够生成非常逼真的声音音乐。将 GPT-2 和 MuseNet 放在一起(它们基本上是同一个模型)并生成歌词和伴奏音乐会有多神奇？如果我有更多的时间、金钱或任何关于我在做什么的想法，我想用机器学习来创作一首成熟的歌曲，然后让真正有才华的人来演奏它。</p><p id="5cd6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p><p id="09cf" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Eugen Hotaj，<br/>2019 年 6 月 30 日</p><p id="b0a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">附言:如果你喜欢这篇文章，那就关注我，让我通知你有新的帖子！一如既往，所有代码和数据都可以在<a class="ae jg" href="https://github.com/EugenHotaj/beatles" rel="noopener ugc nofollow" target="_blank"> my GitHub </a>上获得。</p><h1 id="e3a4" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">脚注</h1><p id="c242" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">尽管我认为最佳独唱音乐家必须是另一个 60 年代的传奇人物，鲍勃·迪伦。他的单曲<a class="ae jg" href="https://www.youtube.com/watch?v=IwOfCgkyEj0" rel="noopener ugc nofollow" target="_blank">像滚石</a>，可能是有史以来写得最好的歌曲，这是<a class="ae jg" href="https://www.rollingstone.com/music/music-lists/500-greatest-songs-of-all-time-151127/bob-dylan-like-a-rolling-stone-2-54028/" rel="noopener ugc nofollow" target="_blank">不仅仅是我的看法</a>。</p><p id="0228" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">来自<a class="ae jg" href="http://go/wiki/Within_You_Without_You" rel="noopener ugc nofollow" target="_blank"> <em class="mg">无你之内</em> </a> <em class="mg">，</em>我最喜欢的披头士专辑里我最喜欢的披头士的歌。很奇怪，但又很好。</p><p id="8240" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了简洁起见，我跳过了一些次要的细节。你可以在我的 GitHub 上看到所有血淋淋的细节<a class="ae jg" href="https://github.com/EugenHotaj/beatles" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="9fb0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">⁴有些人认为这只是一个巨大的宣传噱头，但那是无关紧要的。</p><h1 id="85d6" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">参考</h1><p id="a217" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">[1] A .拉德福德等人，<a class="ae jg" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习器</a> (2019)</p><p id="41ea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] S .托多洛夫，<a class="ae jg" href="https://svilentodorov.xyz/blog/gpt-finetune" rel="noopener ugc nofollow" target="_blank">根据 Facebook Messenger 的数据微调 OpenAI 的 GPT-2，产生虚假对话</a> (2019)</p></div></div>    
</body>
</html>