<html>
<head>
<title>Confidence in k-means</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">k 均值的置信度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/confidence-in-k-means-d7d3a13ca856?source=collection_archive---------18-----------------------#2019-07-15">https://towardsdatascience.com/confidence-in-k-means-d7d3a13ca856?source=collection_archive---------18-----------------------#2019-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2b4c34ac05348ec450e08d63a26d83bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pKwC_KDilv1QN7kR2ETOLw.png"/></div></div></figure><div class=""/><p id="041d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以你用 k-means 做了一些聚类；您已经缩放了数据，应用了 PCA，使用肘和剪影方法检查了聚类，并且您非常确信您的模型为您提供了您所能获得的绝对最佳的聚类。但是你能对单个数据点的分类有多信任呢？这就是我要在这里讨论的。</p><p id="47f6" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">大多数用于评估聚类算法的指标都是对模型的全局成功的度量。但是，如果您不希望对每个数据点进行分类，而是希望识别明确适合特定聚类的特定数据点，该怎么办呢？例如，你可能不想把一个病人归为某一特定疾病的低风险人群，仅仅因为这是最坏的一类；你需要确信它们绝对是低风险的。同样，你可能不希望为没有表现出你的用户细分所定义的强烈行为特征的客户提供个性化的数字体验。</p><p id="de24" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有一个非常简单的方法可以计算出你的算法对每个分类的置信度。然后，您可以过滤掉模型不确定的预测，并根据剩余的、有把握的结果做出决策。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="0dfc" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">软 k 均值算法概述</h1><p id="b949" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">软 k-均值算法与硬 k-均值算法类似，它基于数据点和聚类中心之间的欧几里德距离。与将每个数据点仅分配给一个聚类的普通 k-means 不同，软 k-means 计算一个权重，该权重描述每个数据点属于每个聚类的可能性。较高的值表示强的或有把握的分类，较低的值表示弱的或不太可能的分类。</p><p id="344a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">尽管确实存在可以执行软 k 均值的预写算法，但是它们没有得到很好的支持。还有很多原因可以解释为什么你不想改变你的实际模型。幸运的是，您可以使用硬 k 均值聚类中心追溯计算软 k 均值权重。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="f5a3" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">虹膜数据集上的 k-均值</h1><p id="11cb" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">让我们在具有 3 个聚类的 iris 数据集上训练一个非常直接的 k-means 算法。绘制花瓣长度与花瓣宽度的关系图，我们可以看到该算法工作得很好，但是在绿色和蓝色聚类相遇的地方有一些不正确的分类(右图)。在这种情况下，我们可以很容易地使用视觉检查发现这一点，因为我们知道每个虹膜是什么物种的基本事实(左图)。理想情况下，我们希望知道，在没有真实类别的先验知识的情况下，该区域中的点很难分类，因此不太可信。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mk"><img src="../Images/6713d0e417f89f75d8a675c817f5d43a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXslEWyT1F_DFoXdgUuJQg.png"/></div></div></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="b310" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">软 k 均值权重</h1><p id="c01b" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">设<strong class="kd jf">x</strong>T4】ᵢ为数据点。数据点属于聚类<em class="mj"> j </em>的可能性可以使用下面给出的权重等式来计算:</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/aba9638566e8b7095eb2971091e19765.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*fQtde9Qf5oM1IGReX73S7g.png"/></div><figcaption class="mq mr gj gh gi ms mt bd b be z dk">The wij are used in the soft k-means algorithm to assign a probability of a point belonging to a cluster</figcaption></figure><p id="7fd8" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里的<strong class="kd jf"> c </strong> ⱼ是第<em class="mj"> j </em>星团中心的坐标。<em class="mj"> m </em>的值是控制算法模糊性的参数，典型的默认值为 2。对于每个数据点，权重的总和是 1，这就是为什么它们作为可能性或概率工作得很好。这里有一个很好的 python 函数来为您计算权重:</p><figure class="ml mm mn mo gt iv"><div class="bz fp l di"><div class="mu mv l"/></div></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="fc2a" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">权重过滤</h1><p id="93c7" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">一旦我们计算了每个聚类的软 k 均值权重，我们就可以确定与硬 k 均值算法分配的聚类相关联的权重。然后我们可以过滤掉那些低值，只留下我们有把握的数据点。下面我为我的 iris 数据集聚类做了这个，用一个过滤器去除权重低于 0.9 的点。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mk"><img src="../Images/9fde1cd11d8be427de3d903e6788ff5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBbK79p3JF6iJ1dWV--QFQ.png"/></div></div></figure><p id="7237" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">过滤器已经移除了蓝色和绿色聚类之间的不确定区域中的许多数据点。已经确定，16%的数据点太难分类，无法过滤掉。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="b5af" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">结论</h1><p id="0d9b" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">只需几行代码，我们就可以计算出每个数据点的置信度得分，表明我们的 k-means 算法在分类中的置信度，而无需事先了解真实类别。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><h1 id="8080" class="lg lh je bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">密码</h1><p id="0b3f" class="pw-post-body-paragraph kb kc je kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">soft_cluster_weights 函数和附带的笔记本在<a class="ae mw" href="https://github.com/mattcrooksphd/Medium-SoftClusteringWeights" rel="noopener ugc nofollow" target="_blank">这个 GitHub repo </a>中提供。</p></div></div>    
</body>
</html>