<html>
<head>
<title>Topic Modeling in Python: Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的主题建模:潜在狄利克雷分配(LDA)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0?source=collection_archive---------0-----------------------#2019-04-15">https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0?source=collection_archive---------0-----------------------#2019-04-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/53e00a83f9294e063fd222060315836b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8cspD8Tj7uPo1XJBw1Arbg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><a class="ae jd" href="http://getwallpapers.com/image/398564" rel="noopener ugc nofollow" target="_blank">http://getwallpapers.com/image/398564</a></figcaption></figure><h2 id="3874" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/towards-data-science/in-depth-analysis/home" rel="noopener">深入分析</a></h2><div class=""/><div class=""><h2 id="d127" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">如何开始使用 Python 中的 LDA 进行主题建模</h2></div></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="88a7" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">前言:</strong>本文旨在提供潜在主题的综合信息，不应被视为原创作品。这些信息和代码通过一些在线文章、研究论文、书籍和开源代码被重新利用</p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="650f" class="mh mi jg bd mj mk ml mm mn mo mp mq mr kv ms kw mt ky mu kz mv lb mw lc mx my bi translated">介绍</h1><p id="8456" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">简而言之，主题模型是一种统计语言模型，用于揭示文本集合中隐藏的结构。在实际和更直观的情况下，你可以把它想象成一项任务:</p><p id="96ea" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">降维</strong>，不要将文本<em class="ne"> T </em>在其特征空间中表示为{Word_i: count(Word_i，T) for Word_i in Vocabulary}，而是在主题空间中表示为{Topic_i: Weight(Topic_i，T) for Topic_i in Topics}</p><p id="ddac" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">无监督学习</strong>，可以比作聚类，在聚类的情况下，主题的数量和聚类的数量一样，是一个输出参数。通过进行主题建模，我们建立了单词簇，而不是文本簇。因此，文本是所有主题的混合，每个主题都有特定的权重</p><p id="7495" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">标记</strong>，抽象文档集合中出现的最能代表其中信息的“主题”。</p><p id="5d6b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">有几种现有的算法可以用来执行主题建模。最常见的有<em class="ne">潜在语义分析(LSA/LSI)、概率潜在语义分析(pLSA)和潜在狄利克雷分配(LDA) </em></p><p id="15ce" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在本文中，我们将仔细研究 LDA，并使用<em class="ne"> python 2.7 </em>中的<em class="ne"> sklearn </em>实现来实现我们的第一个主题模型</p><h1 id="17e6" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated">理论概述</h1><p id="c986" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">LDA 是一个生成概率模型，它假设每个主题是一组潜在单词的混合，每个文档是一组主题概率的混合。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/2443fdaa04a2bbf08fc6fa1ab8824ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2g0ARjCpTodoOwSso9XbLg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><a class="ae jd" href="http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4" rel="noopener ugc nofollow" target="_blank">http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4</a></figcaption></figure><p id="8d3e" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">我们可以将 LDA 的生成过程描述为，给定<em class="ne"> M </em>个文档，<em class="ne"> N </em>个单词，以及之前<em class="ne"> K </em>个主题，模型训练输出:</p><p id="a886" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="ne"> psi </em>，每个题目的字数分布<em class="ne"> K </em></p><p id="d53b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="ne">φ</em>，每个文档的主题分布<em class="ne"> i </em></p><h1 id="fba7" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated"><strong class="ak">LDA 的参数</strong></h1><blockquote class="np nq nr"><p id="2643" class="ll lm ne ln b lo lp kq lq lr ls kt lt ns lv lw lx nt lz ma mb nu md me mf mg ij bi translated"><strong class="ln jq"> Alpha 参数</strong>是代表文档-主题密度的狄利克雷先验集中参数——随着<em class="jg">更高的 Alpha，文档被假定为由更多的主题组成，并导致每个文档更具体的主题分布。</em></p><p id="251d" class="ll lm ne ln b lo lp kq lq lr ls kt lt ns lv lw lx nt lz ma mb nu md me mf mg ij bi translated"><strong class="ln jq"> Beta 参数</strong>是表示主题-词密度的相同的先验集中参数— <em class="jg">在高 Beta 的情况下，主题被假定为由大多数词组成，并且导致每个主题的更具体的词分布。</em></p></blockquote></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><h1 id="f4f0" class="mh mi jg bd mj mk ml mm mn mo mp mq mr kv ms kw mt ky mu kz mv lb mw lc mx my bi translated">LDA 实施</h1><p id="64d8" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">完整的代码可以在 GitHub 的<a class="ae jd" href="https://github.com/kapadias/medium-articles/blob/master/natural-language-processing/topic-modeling/Introduction%20to%20Topic%20Modeling.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本上找到</a></p><ol class=""><li id="660c" class="nv nw jg ln b lo lp lr ls lu nx ly ny mc nz mg oa ob oc od bi translated">加载数据</li><li id="f146" class="nv nw jg ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">数据清理</li><li id="3d7d" class="nv nw jg ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">探索性分析</li><li id="fda2" class="nv nw jg ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">为 LDA 分析准备数据</li><li id="b3e2" class="nv nw jg ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">LDA 模型训练</li><li id="653f" class="nv nw jg ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">分析 LDA 模型结果</li></ol><h1 id="19fa" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated">加载数据</h1><p id="10ce" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">对于本教程，我们将使用在 NeurIPS (NIPS)会议上发表的论文数据集，该会议是机器学习社区中最负盛名的年度活动之一。CSV 数据文件包含了从 1987 年到 2016 年(29 年！).这些论文讨论了机器学习中的各种主题，从神经网络到优化方法，等等。</p><p id="97b2" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">让我们从查看文件的内容开始</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="ae72" class="oo mi jg ok b gy op oq l or os"># Importing modules<br/>import pandas as pd<br/>import os</span><span id="dd6a" class="oo mi jg ok b gy ot oq l or os">os.chdir('..')</span><span id="854f" class="oo mi jg ok b gy ot oq l or os"># Read data into papers<br/>papers = pd.read_csv('./data/NIPS Papers/papers.csv')</span><span id="82e7" class="oo mi jg ok b gy ot oq l or os"># Print head<br/>papers.head()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ou"><img src="../Images/b0be61d6c934ef98b6fb0b786bf2ebea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTtUMPkoFYnuVfy6_Rn0SA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Sample of raw data</figcaption></figure><h1 id="1bc4" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated">数据清理</h1><p id="658c" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">因为这个分析的目标是执行主题建模，所以让我们只关注每篇论文的文本数据，忽略其他元数据列。此外，为了演示，我们将只查看 100 篇论文</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="6642" class="oo mi jg ok b gy op oq l or os"># Remove the columns<br/>papers = papers.drop(columns=['id', 'event_type', 'pdf_name'], axis=1).sample(100)</span><span id="2f2e" class="oo mi jg ok b gy ot oq l or os"># Print out the first rows of papers<br/>papers.head()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/e2ce66462b79a54216f63d9bf8c56934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fo4WgizEBAXc02ADEXsd1A.png"/></div></div></figure><p id="c22a" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">去掉标点/小写</strong></p><p id="3ed0" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">接下来，让我们对<em class="ne"> paper_text </em>列的内容进行简单的预处理，使它们更易于分析，并得到可靠的结果。为了做到这一点，我们将使用一个正则表达式来删除任何标点符号，然后<em class="ne">小写</em>文本</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="5503" class="oo mi jg ok b gy op oq l or os"># Load the regular expression library<br/>import re</span><span id="9091" class="oo mi jg ok b gy ot oq l or os"># Remove punctuation<br/>papers['paper_text_processed'] = \<br/>papers['paper_text'].map(lambda x: re.sub('[,\.!?]', '', x))</span><span id="acee" class="oo mi jg ok b gy ot oq l or os"># Convert the titles to lowercase<br/>papers['paper_text_processed'] = \<br/>papers['paper_text_processed'].map(lambda x: x.lower())</span><span id="cd8d" class="oo mi jg ok b gy ot oq l or os"># Print out the first rows of papers<br/>papers['paper_text_processed'].head()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ow"><img src="../Images/487bfa7d989189d4500d9f25eb8ec308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIPiEdF-tQnUm5rgx4U2WA.png"/></div></div></figure><h1 id="ad3a" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated"><strong class="ak">探索性分析</strong></h1><p id="9f57" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">为了验证预处理是否有效，我们将使用<a class="ae jd" href="https://github.com/amueller/word_cloud" rel="noopener ugc nofollow" target="_blank"> wordcloud </a>包制作一个单词云，以获得最常见单词的可视化表示。这是理解数据和确保我们在正确的轨道上的关键，以及在训练模型之前是否需要更多的预处理。</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="5050" class="oo mi jg ok b gy op oq l or os"># Import the wordcloud library<br/>from wordcloud import WordCloud</span><span id="9c51" class="oo mi jg ok b gy ot oq l or os"># Join the different processed titles together.<br/>long_string = ','.join(list(papers['paper_text_processed'].values))</span><span id="c775" class="oo mi jg ok b gy ot oq l or os"># Create a WordCloud object<br/>wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')</span><span id="0e2f" class="oo mi jg ok b gy ot oq l or os"># Generate a word cloud<br/>wordcloud.generate(long_string)</span><span id="19c1" class="oo mi jg ok b gy ot oq l or os"># Visualize the word cloud<br/>wordcloud.to_image()</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ox"><img src="../Images/3295df90e926164ea832ee24c7da5fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AdB-eAXNZVYj51_cP8r3mg.png"/></div></div></figure><h1 id="a846" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated"><strong class="ak">为 LDA 分析准备数据</strong></h1><p id="d79d" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">接下来，让我们将文本数据转换成一种格式，作为训练 LDA 模型的输入。我们从标记文本和删除停用词开始。接下来，我们将标记化的对象转换成语料库和词典。</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="c234" class="oo mi jg ok b gy op oq l or os">import gensim<br/>from gensim.utils import simple_preprocess<br/>import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords</span><span id="df59" class="oo mi jg ok b gy ot oq l or os">stop_words = stopwords.words('english')<br/>stop_words.extend(['from', 'subject', 're', 'edu', 'use'])</span><span id="abf8" class="oo mi jg ok b gy ot oq l or os">def sent_to_words(sentences):<br/>    for sentence in sentences:<br/>        # deacc=True removes punctuations<br/>        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))</span><span id="d113" class="oo mi jg ok b gy ot oq l or os">def remove_stopwords(texts):<br/>    return [[word for word in simple_preprocess(str(doc)) <br/>             if word not in stop_words] for doc in texts]</span><span id="b9ff" class="oo mi jg ok b gy ot oq l or os">data = papers.paper_text_processed.values.tolist()<br/>data_words = list(sent_to_words(data))</span><span id="4768" class="oo mi jg ok b gy ot oq l or os"># remove stop words<br/>data_words = remove_stopwords(data_words)</span><span id="31d2" class="oo mi jg ok b gy ot oq l or os">print(data_words[:1][0][:30])</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/1ef173e73f83c9b05e44f522f9d521a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNUBI1YKCMdvgNSRCyELYw.png"/></div></div></figure><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="e0d2" class="oo mi jg ok b gy op oq l or os">import gensim.corpora as corpora</span><span id="ab6c" class="oo mi jg ok b gy ot oq l or os"># Create Dictionary<br/>id2word = corpora.Dictionary(data_words)</span><span id="a948" class="oo mi jg ok b gy ot oq l or os"># Create Corpus<br/>texts = data_words</span><span id="51a6" class="oo mi jg ok b gy ot oq l or os"># Term Document Frequency<br/>corpus = [id2word.doc2bow(text) for text in texts]</span><span id="90f4" class="oo mi jg ok b gy ot oq l or os"># View<br/>print(corpus[:1][0][:30])</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/b966048e934134f6202fcd53147dfc34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GKz6jdvGMBho_4Q23zXhpw.png"/></div></div></figure><h1 id="dcbc" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated"><strong class="ak"> LDA 模型训练</strong></h1><p id="1e9d" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">为了简单起见，除了输入主题数量之外，我们将保持所有参数的默认值。对于本教程，我们将建立一个有 10 个主题的模型，其中每个主题是关键字的组合，每个关键字对主题有一定的权重。</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="51b7" class="oo mi jg ok b gy op oq l or os">from pprint import pprint</span><span id="def9" class="oo mi jg ok b gy ot oq l or os"># number of topics<br/>num_topics = 10</span><span id="5b22" class="oo mi jg ok b gy ot oq l or os"># Build LDA model<br/>lda_model = gensim.models.LdaMulticore(corpus=corpus,<br/>                                       id2word=id2word,<br/>                                       num_topics=num_topics)</span><span id="d1bc" class="oo mi jg ok b gy ot oq l or os"># Print the Keyword in the 10 topics<br/>pprint(lda_model.print_topics())<br/>doc_lda = lda_model[corpus]</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oy"><img src="../Images/542028a3c7a0390e555857ca355bbbe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLhfJ7AW1MSb1EdhlujC-A.png"/></div></div></figure><h1 id="c948" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated"><strong class="ak">分析 LDA 模型结果</strong></h1><p id="0a5a" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">现在我们已经有了一个训练好的模型，让我们来想象一下主题的可解释性。为此，我们将使用一个流行的可视化软件包，<a class="ae jd" href="https://github.com/bmabey/pyLDAvis" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a>，该软件包旨在以交互方式帮助:</p><ol class=""><li id="b48a" class="nv nw jg ln b lo lp lr ls lu nx ly ny mc nz mg oa ob oc od bi translated">更好地理解和解释单个主题，以及</li><li id="75fb" class="nv nw jg ln b lo oe lr of lu og ly oh mc oi mg oa ob oc od bi translated">更好地理解主题之间的关系。</li></ol><p id="8a9d" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对于(1)，您可以使用不同的λ参数值，手动选择每个主题来查看其最常用和/或“相关”的术语。当你试图给每个主题指定一个人类可以理解的名称或“含义”时，这很有帮助。</p><p id="c2c3" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对于(2)，探索<em class="ne">主题间距离图</em>可以帮助你了解主题如何相互关联，包括主题组之间潜在的更高层次的结构。</p><pre class="nl nm nn no gt oj ok ol om aw on bi"><span id="5da7" class="oo mi jg ok b gy op oq l or os">import pyLDAvis.gensim<br/>import pickle <br/>import pyLDAvis</span><span id="dc47" class="oo mi jg ok b gy ot oq l or os"># Visualize the topics<br/>pyLDAvis.enable_notebook()</span><span id="2ebb" class="oo mi jg ok b gy ot oq l or os">LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))</span><span id="6b30" class="oo mi jg ok b gy ot oq l or os"># # this is a bit time consuming - make the if statement True<br/># # if you want to execute visualization prep yourself<br/>if 1 == 1:<br/>    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)<br/>    with open(LDAvis_data_filepath, 'wb') as f:<br/>        pickle.dump(LDAvis_prepared, f)</span><span id="1590" class="oo mi jg ok b gy ot oq l or os"># load the pre-prepared pyLDAvis data from disk<br/>with open(LDAvis_data_filepath, 'rb') as f:<br/>    LDAvis_prepared = pickle.load(f)</span><span id="b587" class="oo mi jg ok b gy ot oq l or os">pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')</span><span id="9542" class="oo mi jg ok b gy ot oq l or os">LDAvis_prepared</span></pre><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oz"><img src="../Images/4060eaab89b8528b824e9a3430ae364d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q13vXASYqTlR_THV4eUHcg.png"/></div></div></figure><h1 id="85e1" class="mh mi jg bd mj mk nf mm mn mo ng mq mr kv nh kw mt ky ni kz mv lb nj lc mx my bi translated">结束语</h1><p id="d4bd" class="pw-post-body-paragraph ll lm jg ln b lo mz kq lq lr na kt lt lu nb lw lx ly nc ma mb mc nd me mf mg ij bi translated">在过去十年中，机器学习变得越来越受欢迎，计算可用性的最新进展导致人们寻求如何整合新方法来推进自然语言处理领域的方法呈指数增长。</p><p id="81dd" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">通常，我们将主题模型视为黑盒算法，但是希望这篇文章能够阐明底层的数学、其背后的直觉，以及让您开始处理任何文本数据的高级代码。</p><p id="5b1f" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在下一篇文章中，我们将更深入地了解如何评估主题模型的性能，调整其超参数以获得更直观和可靠的结果。</p><p id="d30b" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><strong class="ln jq">参考文献:</strong></p><p id="f1c6" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[1]主题模型—维基百科。<a class="ae jd" href="https://en.wikipedia.org/wiki/Topic_model" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Topic_model</a></p><p id="14f4" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[2]主题建模的分布式策略。<a class="ae jd" href="https://www.ideals.illinois.edu/bitstream/handle/2142/46405/ParallelTopicModels.pdf?sequence=2&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">https://www . ideas . Illinois . edu/bitstream/handle/2142/46405/paralleltopicmodels . pdf？sequence=2 &amp; isAllowed=y </a></p><p id="f00a" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[3]主题地图—软件—资源— Amaral 实验室。<a class="ae jd" href="https://amaral.northwestern.edu/resources/software/topic-mapping" rel="noopener ugc nofollow" target="_blank">https://amaral . northwestern . edu/resources/software/topic-mapping</a></p><p id="e166" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">[4]文本挖掘中的主题建模综述。<a class="ae jd" href="https://thesai.org/Downloads/Volume6No1/Paper_21-A_Survey_of_Topic_Modeling_in_Text_Mining.pdf" rel="noopener ugc nofollow" target="_blank">https://thesai . org/Downloads/volume 6 no 1/Paper _ 21-A _ Survey _ of _ Topic _ Modeling _ in _ Text _ mining . pdf</a></p></div><div class="ab cl le lf hu lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="ij ik il im in"><p id="10a5" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">感谢阅读。<em class="ne">如果你有任何反馈，欢迎评论这篇文章，在</em> <a class="ae jd" href="https://www.linkedin.com/in/shashankkapadia/" rel="noopener ugc nofollow" target="_blank"> <em class="ne"> LinkedIn </em> </a> <em class="ne">上给我发消息，或者给我发邮件(shmkapadia[at]gmail.com) </em></p><p id="86ed" class="pw-post-body-paragraph ll lm jg ln b lo lp kq lq lr ls kt lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated"><em class="ne">如果你喜欢这篇文章，请访问我在 NLP 上的其他文章</em></p><div class="ip iq gp gr ir pa"><a rel="noopener follow" target="_blank" href="/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd jq gy z fp pf fr fs pg fu fw jp bi translated">评估主题模型:潜在狄利克雷分配(LDA)</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">构建可解释主题模型的分步指南</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">towardsdatascience.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po ix pa"/></div></div></a></div><div class="ip iq gp gr ir pa"><a href="https://medium.com/@shashank.kapadia/introduction-to-natural-language-processing-nlp-2a8fae09ed03" rel="noopener follow" target="_blank"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd jq gy z fp pf fr fs pg fu fw jp bi translated">自然语言处理(NLP)简介</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">自然语言处理简介</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">medium.com</p></div></div><div class="pj l"><div class="pp l pl pm pn pj po ix pa"/></div></div></a></div><div class="ip iq gp gr ir pa"><a rel="noopener follow" target="_blank" href="/building-blocks-text-pre-processing-641cae8ba3bf"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd jq gy z fp pf fr fs pg fu fw jp bi translated">构建块:文本预处理</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">本文是关于自然语言处理的后续文章的第二篇。这一系列…的目的</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">towardsdatascience.com</p></div></div><div class="pj l"><div class="pq l pl pm pn pj po ix pa"/></div></div></a></div><div class="ip iq gp gr ir pa"><a rel="noopener follow" target="_blank" href="/introduction-to-language-models-n-gram-e323081503d9"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd jq gy z fp pf fr fs pg fu fw jp bi translated">语言模型简介:N-Gram</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">本文是关于自然语言处理的第三篇文章。这一系列…的目的</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">towardsdatascience.com</p></div></div><div class="pj l"><div class="pr l pl pm pn pj po ix pa"/></div></div></a></div></div></div>    
</body>
</html>