<html>
<head>
<title>K-Means Clustering with scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 scikit-learn 的 K-Means 聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c?source=collection_archive---------0-----------------------#2019-05-31">https://towardsdatascience.com/k-means-clustering-with-scikit-learn-6b47a369a83c?source=collection_archive---------0-----------------------#2019-05-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="414b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">了解流行的 k-means 聚类算法背后的基础知识和数学知识，以及如何在<code class="fe ko kp kq kr b">scikit-learn</code>中实现它！</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi ks"><img src="../Images/dbeef63256e9b8ac6b8125f461b94218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0DDt5Xp9z6ecj5eL6FNAfQ.png"/></div></div></figure><p id="3f2b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">聚类</strong>(或<strong class="js iu">聚类分析</strong>)是一种技术，它允许我们找到相似对象的组，这些对象彼此之间的相关性比其他组中的对象更大。面向业务的集群应用程序的示例包括按不同主题对文档、音乐和电影进行分组，或者基于共同的购买行为来寻找具有相似兴趣的客户，以此作为推荐引擎的基础。</p><p id="d3fb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本教程中，我们将了解最流行的聚类算法之一，<strong class="js iu"> k-means </strong>，它在学术界和工业界都有广泛的应用。我们将涵盖:</p><ul class=""><li id="258b" class="le lf it js b jt ju jx jy kb lg kf lh kj li kn lj lk ll lm bi translated">k-均值聚类的基本概念</li><li id="45eb" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">k-means 算法背后的数学</li><li id="0a99" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">k-means 的优缺点</li><li id="d0fb" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">如何使用<code class="fe ko kp kq kr b">scikit-learn</code>在样本数据集上实现算法</li><li id="93a9" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">如何可视化集群</li><li id="6d57" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">如何用肘法选择最佳的<em class="ls"> k </em></li></ul><p id="d507" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们开始吧！</p><blockquote class="lt lu lv"><p id="0b1f" class="jq jr ls js b jt ju jv jw jx jy jz ka lw kc kd ke lx kg kh ki ly kk kl km kn im bi translated"><em class="it">本教程改编自 Next Tech 的</em> <strong class="js iu"> <em class="it"> Python 机器学习</em> </strong> <em class="it">系列的</em> Part 3 <em class="it">，带你从 0 到 100 用 Python 进行机器学习和深度学习算法。它包括一个浏览器内沙盒环境，预装了所有必要的软件和库，以及使用公共数据集的项目。你可以在这里</em><a class="ae lz" href="https://c.next.tech/2VsyVXb" rel="noopener ugc nofollow" target="_blank"><em class="it"/></a><em class="it">免费上手！</em></p></blockquote></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><h1 id="4e89" class="mh mi it bd mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne bi translated">K-均值聚类的基础</h1><p id="1696" class="pw-post-body-paragraph jq jr it js b jt nf jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj nj kl km kn im bi translated">正如我们将看到的，与其他聚类算法相比，k-means 算法非常容易实现，并且计算效率非常高，这可能是它受欢迎的原因。k-means 算法属于<strong class="js iu">基于原型聚类</strong>的范畴。</p><p id="c403" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">基于原型的聚类是指每个聚类由一个原型表示，该原型可以是具有连续特征的相似点的<strong class="js iu">形心</strong> ( <em class="ls">平均值</em>)，也可以是分类特征的<strong class="js iu">形心</strong>(最具<em class="ls">代表性的</em>或最频繁出现的点)。</p><p id="4605" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">虽然 k-means 非常善于识别具有球形形状的聚类，但是这种聚类算法的一个缺点是我们必须先验地指定聚类的数量<em class="ls"> k </em>。对<em class="ls"> k </em>的不适当选择会导致较差的聚类性能——我们将在本教程的后面讨论如何选择<em class="ls"> k </em>。</p><p id="303e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管 k-means 聚类可以应用于更高维度的数据，但是为了可视化的目的，我们将使用一个简单的二维数据集来完成下面的示例。</p><p id="80f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以通过使用 Next Tech <a class="ae lz" href="https://c.next.tech/2HLYRt2" rel="noopener ugc nofollow" target="_blank">沙箱</a>跟随本教程中的代码，沙箱已经预装了所有必要的库，或者如果你愿意，你可以在你自己的本地环境中运行代码片段。</p><p id="1df7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦你的沙盒加载完毕，让我们从<code class="fe ko kp kq kr b">scikit-learn</code>导入玩具数据集并可视化数据点:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/42e539eaca8e448aada5eb841d7de502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*0Qtlh02GkKleLQPWO3wghQ.png"/></div></figure><p id="d04f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们刚刚创建的数据集由 150 个随机生成的点组成，这些点大致分为三个密度较高的区域，通过二维散点图进行可视化。</p><p id="8e2f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在聚类的实际应用中，我们没有关于这些样本的任何基本事实类别信息(作为经验证据而不是推断提供的信息)；否则，它将属于监督学习的范畴。因此，我们的目标是根据样本的特征相似性对样本进行分组，这可以通过使用 k-means 算法来实现，k-means 算法可以概括为以下四个步骤:</p><ol class=""><li id="c222" class="le lf it js b jt ju jx jy kb lg kf lh kj li kn nn lk ll lm bi translated">从样本点中随机选取<em class="ls"> k 个</em>形心作为初始聚类中心。</li><li id="27c2" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn nn lk ll lm bi translated">将每个样本分配到最近的质心<em class="ls"> μ^(j)，j ∈ {1，…，k}。</em></li><li id="cabc" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn nn lk ll lm bi translated">将质心移动到指定给它的样本的中心。</li><li id="71e1" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn nn lk ll lm bi translated">重复第 2 步和第 3 步，直到聚类分配不变或达到用户定义的容差或最大迭代次数。</li></ol><p id="7247" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，下一个问题是<em class="ls">我们如何测量对象之间的相似性</em>？我们可以将相似度定义为距离的反义词，一种常用的对具有连续特征的样本进行聚类的距离是<em class="ls"> m </em>维空间中两点<strong class="js iu"> <em class="ls"> x </em> </strong>和<strong class="js iu"> <em class="ls"> y </em> </strong>之间的<strong class="js iu">平方欧氏距离</strong>:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3043569488ec38135eaa6a671b7dcd1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Lr1Xw4nkONoNz1dmBlYjfA.png"/></div></figure><p id="792f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意，在前面的等式中，索引<em class="ls"> j </em>是指样本点<strong class="js iu"> <em class="ls"> x </em> </strong>和<strong class="js iu"> <em class="ls"> y </em> </strong>的第<em class="ls"> j </em>维(特征列)。我们将使用上标<em class="ls"> i </em>和<em class="ls"> j </em>分别指代样本索引和集群索引。</p><p id="42d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">基于这种欧几里德距离度量，我们可以将 k-means 算法描述为一个简单的优化问题，一种用于最小化组内<strong class="js iu">误差平方和</strong> ( <strong class="js iu"> SSE </strong>)的迭代方法，有时也称为<strong class="js iu">组</strong> <strong class="js iu">惯性</strong>:</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a6febd631f7cc3e153bc839d83ac1937.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*kFwKdOb2ko1SaqnOVBApow.png"/></div></figure><p id="bc80" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b28bbd1ebc19ac7a3e90dae5122898d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*Gj4quhatOb6NJfFwyhUpwA.png"/></div></figure><p id="2d27" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和</p><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/831f28fbad4365de88bebeb371a839b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*HGnGIS92CJ0cu1SiukGPCA.png"/></div></figure><p id="b1fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请注意，当我们使用欧几里德距离度量将 k-means 应用于真实世界的数据时，我们希望确保在相同的尺度上测量特征，并在必要时应用<em class="ls"> z </em>分数标准化或最小-最大缩放。</p><h1 id="928b" class="mh mi it bd mj mk ns mm mn mo nt mq mr ms nu mu mv mw nv my mz na nw nc nd ne bi translated">使用<code class="fe ko kp kq kr b">scikit-learn</code>的 k-均值聚类</h1><p id="819c" class="pw-post-body-paragraph jq jr it js b jt nf jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj nj kl km kn im bi translated">现在我们已经了解了 k-means 算法是如何工作的，让我们使用来自<code class="fe ko kp kq kr b">scikit-learn</code>的<code class="fe ko kp kq kr b">cluster</code>模块的<code class="fe ko kp kq kr b">KMeans</code>类将它应用于我们的样本数据集:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="3eda" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用前面的代码，我们将期望的集群数量设置为<code class="fe ko kp kq kr b">3</code>。我们设置<code class="fe ko kp kq kr b">n_init=10</code>使用不同的随机质心独立运行 k-means 聚类算法 10 次，以选择最终模型作为 SSE 最低的模型。通过<code class="fe ko kp kq kr b">max_iter</code>参数，我们指定每次运行的最大迭代次数(这里是<code class="fe ko kp kq kr b">300</code>)。</p><p id="5632" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意，如果在达到最大迭代次数之前收敛，那么<code class="fe ko kp kq kr b">scikit-learn</code>中的 k-means 实现会提前停止。然而，对于特定的运行，k-means 可能不会达到收敛，如果我们为<code class="fe ko kp kq kr b">max_iter</code>选择相对较大的值，这可能会有问题(计算开销)。</p><p id="90c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">处理收敛问题的一种方法是为<code class="fe ko kp kq kr b">tol</code>选择更大的值，T10 是一个参数，它控制关于组内平方和误差变化的容限，以表明收敛。在前面的代码中，我们选择了公差<code class="fe ko kp kq kr b">1e-04</code> (= 0.0001)。</p><p id="0f7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">k-means 的一个问题是一个或多个聚类可能是空的。然而，在<code class="fe ko kp kq kr b">scikit-learn</code>中的当前 k-means 实现中考虑到了这个问题。如果一个聚类是空的，该算法将搜索离空聚类的质心最远的样本。然后，它会将质心重新分配到这个最远的点。</p><p id="259f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然我们已经预测了聚类标签<code class="fe ko kp kq kr b">y_km</code>，让我们将 k-means 在数据集中识别的聚类与聚类质心一起可视化。这些存储在适合的<code class="fe ko kp kq kr b">KMeans</code>对象的<code class="fe ko kp kq kr b">cluster_centers_</code>属性下:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/73198f9d0c10bab5773d61e1588e64cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*TEYPlUQfggUVnqu26QiQ3g.png"/></div></figure><p id="9171" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在生成的散点图中，我们可以看到 k-means 将三个质心放置在每个球体的中心，对于给定的数据集，这看起来是一个合理的分组。</p><h1 id="0e3a" class="mh mi it bd mj mk ns mm mn mo nt mq mr ms nu mu mv mw nv my mz na nw nc nd ne bi translated">肘法</h1><p id="cb3c" class="pw-post-body-paragraph jq jr it js b jt nf jv jw jx ng jz ka kb nh kd ke kf ni kh ki kj nj kl km kn im bi translated">虽然 k-means 在这个玩具数据集上工作得很好，但重要的是要重申，k-means 的一个缺点是，在我们知道最优的<em class="ls"> k </em>是什么之前，我们必须指定聚类的数量<em class="ls"> k </em>。在现实世界的应用程序中，要选择的聚类数量可能并不总是那么明显，尤其是当我们处理无法可视化的高维数据集时。</p><p id="c1a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">肘方法</strong>是一个有用的图形工具，可以估计给定任务的最佳集群数量<em class="ls"> k </em>。直观地，我们可以说，如果<em class="ls"> k </em>增加，组内 SSE(<strong class="js iu">失真</strong>)将减少。这是因为样本将更接近它们被分配到的质心。</p><p id="0522" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">肘方法背后的思想是确定失真开始下降最快的<em class="ls"> k </em>的值，如果我们绘制不同的<em class="ls"> k </em>值的失真，这将变得更加清楚:</p><figure class="kt ku kv kw gt kx"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="kt ku kv kw gt kx gh gi paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gh gi nx"><img src="../Images/f57eee84c8114a06777ca481892d19dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*Q9OzABjrHuY1uFIcEe8tlg.png"/></div></div></figure><p id="719e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们在结果图中看到的，肘部位于<em class="ls"> k </em> = 3 处，这证明<em class="ls"> k </em> = 3 确实是这个数据集的一个好选择。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="63e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望你喜欢这个关于 k-means 算法的教程！我们探讨了 k-means 算法背后的基本概念和数学，如何实现 k-means，以及如何选择最佳数量的聚类，<em class="ls"> k </em>。</p><p id="dd67" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您想了解更多，Next Tech 的<strong class="js iu"> Python 机器学习(第三部分)</strong>课程将进一步探索聚类算法和技术，例如:</p><ul class=""><li id="8221" class="le lf it js b jt ju jx jy kb lg kf lh kj li kn lj lk ll lm bi translated"><strong class="js iu">剪影图</strong>，另一种用于选择最优<em class="ls"> k 的方法</em></li><li id="fb8a" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated"><strong class="js iu"> k-means++ </strong>，k-means 的一个变种，通过对初始聚类中心更巧妙的播种来改善聚类结果。</li><li id="d8df" class="le lf it js b jt ln jx lo kb lp kf lq kj lr kn lj lk ll lm bi translated">其他类别的聚类算法，如<strong class="js iu">分层</strong>和<strong class="js iu">基于密度的聚类</strong>，不需要我们预先指定聚类的数量或假设我们的数据集中的球形结构。</li></ul><p id="2f28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本课程还探讨了回归分析、情感分析以及如何将动态机器学习模型部署到 web 应用程序中。这里可以开始<a class="ae lz" href="https://c.next.tech/2VsyVXb" rel="noopener ugc nofollow" target="_blank">！</a></p></div></div>    
</body>
</html>