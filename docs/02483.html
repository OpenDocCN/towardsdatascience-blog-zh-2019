<html>
<head>
<title>Review: GCN — Global Convolutional Network, Large Kernel Matters (Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:GCN——全球卷积网络，大核问题(语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=collection_archive---------16-----------------------#2019-04-23">https://towardsdatascience.com/review-gcn-global-convolutional-network-large-kernel-matters-semantic-segmentation-c830073492d2?source=collection_archive---------16-----------------------#2019-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b4b1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">胜过<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-8s </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c"> CRF-RNN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩容网</a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv1 &amp; DeepLabv2 </a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/6a815d4b9d3ed49126132b70ca2aa050.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnK3HWNW7xFzc3x6U7nw1g.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Large Kernel Matters</strong></figcaption></figure><p id="bde4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这篇论文中，对<strong class="kz ir">全球卷积网络(GCN) </strong>，由<strong class="kz ir">清华大学</strong>和<strong class="kz ir">旷视科技公司(Face++) </strong>所做的综述。在诸如 VGGNet 的惯例中，使用小的 3×3 核的堆叠，以便获得大的有效感受野。然而，我们发现<strong class="kz ir">大内核也扮演着重要的角色。</strong>以上图为例，在 A 中，感受野大到足以覆盖该鸟进行分割。但是如果像 B 中那样放大图像，感受野就不够大。在 C 中，使用建议的 GCN，感受野可以扩大。最后:</p><ul class=""><li id="9a3d" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated"><strong class="kz ir"> GCN </strong>被提出来解决语义分割的分类和定位问题。</li><li id="1eb1" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated"><strong class="kz ir">边界细化(BR) </strong>也被提出来进一步细化对象边界。</li></ul><p id="d686" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">并且发表在<strong class="kz ir"> 2017 CVPR </strong>上<strong class="kz ir"> 100 多篇引用</strong>。(<a class="mq mr ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----c830073492d2--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="c621" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">概述</h1><ol class=""><li id="b3d5" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls nw mi mj mk bi translated"><strong class="kz ir">分类与定位的矛盾</strong></li><li id="e9d0" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls nw mi mj mk bi translated"><strong class="kz ir">全球卷积网络(GCN) &amp;边界细化(BR) </strong></li><li id="3969" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls nw mi mj mk bi translated"><strong class="kz ir">消融研究</strong></li><li id="0a7b" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls nw mi mj mk bi translated"><strong class="kz ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="401e" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">1.分类与本土化的矛盾</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/6d692def968603d2c717e61d82827855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFS4AUYvxNI2pyNc-AWbUg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Classification (Left), Segmentation/Localization (Middle), GCN (Right)</strong></figcaption></figure><ul class=""><li id="a103" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">对于<strong class="kz ir">分类</strong>任务，要求模型对于各种变换<strong class="kz ir">不变，如平移和旋转</strong>。</li><li id="cf81" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">但是对于<strong class="kz ir">定位</strong>任务，模型应该是<strong class="kz ir">变换敏感的</strong>，<strong class="kz ir">，即精确定位每个语义类别的每个像素</strong>。</li></ul><h2 id="88bb" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">全球卷积网络(GCN)</h2><ul class=""><li id="4329" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">同时应对上述两项挑战。作者遵循两个设计原则:</li><li id="ff11" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated"><strong class="kz ir"> 1)从本地化视图</strong>来看，模型结构应该是<strong class="kz ir">全卷积</strong>以保持本地化性能，并且<strong class="kz ir">不应该使用全连接或全局池层</strong>，因为这些层将丢弃本地化信息；</li><li id="ef18" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated"><strong class="kz ir"> 2)从分类的角度来看</strong>，<strong class="kz ir">在网络架构中应该采用大的核尺寸</strong>，以<strong class="kz ir">实现特征图和逐像素分类器</strong>之间的密集连接，这<strong class="kz ir">增强了处理不同变换的能力。</strong></li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="d9f6" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 2。全球卷积网络(GCN) &amp;边界细化(BR) </strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ok"><img src="../Images/be249cbb049f5b9a35ad2f1431818f68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VRH-f6OaHxqyjUviJtpfg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Overall Architecture (GCN), GCN Module (Top Right), and BR module (Bottom Right)</strong></figcaption></figure><ul class=""><li id="9c90" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">如上图，<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>作为主干。特别是，在最先进的比较过程中，使用 ImageNet 上预训练的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-152 </a>。</li><li id="b114" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">如上图所示，插入 GCN 模块，然后插入 BR 模块。</li><li id="df26" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">使用去卷积层对较低分辨率的得分图进行上采样，然后与较高分辨率的得分图相加，以生成新的得分图。</li></ul><h2 id="4884" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">2.1.全球卷积网络(GCN)模块</h2><ul class=""><li id="0a27" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">如图右上方所示，<strong class="kz ir">GCN 模块没有直接使用更大的核或全局卷积，而是使用了 1×k+k×1 和 k×1+1×k 卷积</strong>的组合，使得<strong class="kz ir">能够在特征图</strong>中的一个大 k×k 区域内进行密集连接。</li><li id="b81f" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">不同于<a class="ae kf" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>使用的非对称核，卷积层后<strong class="kz ir">没有非线性。</strong></li><li id="b30b" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">与琐碎的 k×k 卷积相比，GCN 结构仅涉及 O(2/k)计算成本和参数数量<strong class="kz ir">，这对于大核尺寸更实用。</strong></li></ul><h2 id="9250" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">2.2.边界细化(BR)模块</h2><ul class=""><li id="398f" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">如图右下方所示，边界对齐建模为<strong class="kz ir">残差结构</strong>，其中<strong class="kz ir">ŝ=s+r(s)</strong>，s 为粗分图，r()为残差分支。</li><li id="78f1" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">它可以作为<strong class="kz ir">作者</strong>定制的附加残差块，在 GCN 之后使用，在反褶积过程中使用。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="b2ed" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">3.<strong class="ak">消融研究</strong></h1><ul class=""><li id="a97e" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">PASCAL VOC 2012 有 1464 幅图像用于训练，1449 幅图像用于验证，1456 幅图像用于测试，属于 20 个对象类和一个背景类。</li><li id="8d36" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">语义边界数据集也被用作辅助数据集，产生 10，582 幅图像用于训练。</li><li id="2048" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">PASCAL VOC 2012 验证集用于评估。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ol"><img src="../Images/020c030c036382c500e61e72dfb57203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDn9l2EKJpFM1JWV8EHEMw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">GCN (Leftmost), 1×1 Conv (2nd Left), Trivial k×k Conv (2nd Right), Stacks of 3×3 Conv (Rightmost)</strong></figcaption></figure><h2 id="9c23" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.1.大内核问题</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi om"><img src="../Images/a38568246eb56afe1f960a0a1507acad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YWIjKw4AgoL-ODVRQc3XA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Different k values for GCN on PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="0104" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">基线:使用简单的 1×1 Conv 的简单基线。</li><li id="c8da" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">用<em class="on"> k </em> = 15，大致等于 16×16 的特征图大小。</li><li id="6632" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">性能随着内核大小<em class="on"> k </em>不断增加。</li><li id="1a9b" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">特别是 GCN ( <em class="on"> k </em> = 15)以 5.5%的显著优势超过了最小的一个。</li></ul><h2 id="5d8f" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.2.参数多了有帮助吗？</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oo"><img src="../Images/02e03cdf41818c4a60ee0ff9aaca7fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ydavApVqh8fhuQs5VK1dGQ.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">GCN vs Trivial k×k Conv on PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="d910" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">对于<strong class="kz ir">琐碎的<em class="on"> k </em> × <em class="on"> k </em> Conv </strong>，如果 k≤5，更大的内核会带来更好的性能，而对于 k≥7，性能会下降。</li><li id="8701" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">一个假设是<strong class="kz ir">太多的参数使训练遭受过度拟合</strong>，这削弱了较大内核的好处。</li><li id="ae1f" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">作者发现，寻找琐碎的大核实际上使网络<strong class="kz ir">难以收敛</strong>。</li><li id="f812" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">而拟议中的 GCN 没有这个问题。</li></ul><h2 id="2dba" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.3.GCN vs 一堆 3×3 的小回旋</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi op"><img src="../Images/e849969dbe6c8b33bcc12b4ed106430f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7PkkmVpLwq7pI2aTTp8xEg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">GCN vs Stacks of Small 3×3 Convolutions on PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="e777" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">在这里，对于小的 3×3 卷积的堆叠，非线性被去除，以便与 GCN 进行公平的比较。</li><li id="c73c" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">同样，<strong class="kz ir">小的 3×3 卷积</strong>的堆叠带来比 GCN 更多的参数，并且当感受野增加时导致<strong class="kz ir">过拟合</strong>。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oq"><img src="../Images/a79ec54b168c90771ee1e3766949e0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXLrO7i-cWUNBp5Frv7DKg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Different Number of Feature Maps (m) on PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="3dec" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">还测试了不同数量的特征图(<em class="on"> m </em>)，以便减少小的 3×3 卷积的堆叠的参数数量。</li><li id="f952" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">然而，它的性能随着参数的减少而下降。</li></ul><h2 id="9e87" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.4.GCN 和 BR 如何对分割结果做出贡献？</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi or"><img src="../Images/47e3d7c440be429fe2f706a5bf5c31cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwHgT0j3bKd-z7_-RC7Vxg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">GCN &amp; BR on PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="f135" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">位于大物体中心的像素可能从 GCN 中受益更多，因为它非常接近“纯”分类问题。</li><li id="761e" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">至于物体的<strong class="kz ir">边界像素，其性能<strong class="kz ir">主要受定位能力</strong>的影响。</strong></li><li id="f2e5" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">为了验证上述推论，分割得分图被分成两部分:<strong class="kz ir"> a)边界区域</strong>，其<strong class="kz ir">像素位于靠近物体边界(距离≤7) </strong>，以及<strong class="kz ir"> b)内部区域作为其他像素</strong>。</li><li id="0b4e" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">如上所示，<strong class="kz ir"> BR 主要提高边界区域的精度，而 GCN 有助于提高内部区域的精度</strong>。</li></ul><h2 id="1c1a" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">3.5.GCN vs <a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">雷斯内特</a></h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi os"><img src="../Images/9aa131e93ac39910b91f31b081d7c5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVaQ5lonUYHBj-q1UCuvXA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Original </strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd kw">ResNet</strong></a><strong class="bd kw"> Bottleneck Module (Left), and ResNet-GCN Module (Right)</strong></figcaption></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ot"><img src="../Images/527f846882f726e7787180f1e8063e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObEETflyBj4_z-7srsFkYg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Detailed Architecture of </strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd kw">ResNet50</strong></a> <strong class="bd kw">and ResNet50-GCN</strong></figcaption></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ou"><img src="../Images/c77f4b584a672ba4836200b7ac5abe1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ooqJsJ8XnU5mHI8TYF6ztw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Original </strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd kw">ResNet</strong></a><strong class="bd kw"> vs ResNet-GCN on PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="75a3" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">我们可能会想，在主干上，为什么不用 GCN 块(右)代替原来的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>块(左)来提高精度呢？作者使用<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-50 </a>研究了上述两种结构。</li><li id="ba47" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">作为 ImageNet 分类模型，总部位于 GCN 的 ResNet 比最初的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>稍逊一筹。</li><li id="ab09" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">对于 GCN 和 BR，增益仍然很小。</li></ul></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="f373" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated"><strong class="ak"> 4。与最先进方法的比较</strong></h1><h2 id="bcb7" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">4.1.帕斯卡 VOC 2012</h2><ul class=""><li id="d0b3" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">使用 COCO 女士预训练模型。</li><li id="a9ae" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">培训阶段分为三个阶段:</li><li id="bbbd" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">(1)在阶段-1 中，使用来自 SBD COCO 和标准 PASCAL VOC 2012 的所有图像，产生用于训练的 109，892 个图像。</li><li id="ab7c" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">(2)在第二阶段，仅使用 SBD 和标准帕斯卡 VOC 2012 图像，与上一节相同。</li><li id="41a2" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">(3)对于阶段 3，仅使用标准 PASCAL VOC 2012 数据集。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/38117a739b96f908a4ce3700ed1d88f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*ERdU_pbHq-Ihk2KNkI_5_g.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">PASCAL VOC 2012 validation set</strong></figcaption></figure><ul class=""><li id="d7e2" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">在阶段 3，利用 GCN+溴，获得 80.3%的平均 IoU。</li><li id="1e0e" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">利用多尺度(MS)和条件随机场(CRF)，获得了 81.0%的平均 IoU。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/13ad426512e5c646f5616d61dc42e7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*7rk3Z7cmt9hmc5LNsTve3Q.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">PASCAL VOC 2012 test set</strong></figcaption></figure><ul class=""><li id="1a12" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">最后，在测试集上获得了 82.2%的平均 IoU，优于<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>【37】，以及<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a>【6，7】。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ox"><img src="../Images/d2193165aefe34cb58eef78362041a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L_AtREz3YjmAa4kIR-uhaA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Qualitative Results</strong></figcaption></figure><h2 id="511d" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">4.2.城市景观</h2><ul class=""><li id="603e" class="mc md iq kz b la nr ld ns lg nt lk nu lo nv ls mh mi mj mk bi translated">它包含来自 50 个不同条件的城市的 24998 幅图像，属于 30 类，没有背景类。</li><li id="543a" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">根据图像的标记质量，将图像分成两组。其中 5，000 个是精细注释的，而另外 19，998 个是粗略注释的。5000 个精细注释图像被进一步分组为 2975 个训练图像、500 个验证图像和 1525 个测试图像。</li><li id="0e0c" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">Cityscapes 中的图像具有 1024×2048 的固定大小，这对于我们的网络架构来说太大了。因此，在训练阶段，我们将图像随机裁剪为 800×800。GCN 的 k<em class="on">也从 15 增加到 25，最终特征图为 25×25。</em></li><li id="24a9" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">培训阶段分为两个阶段:</li><li id="2a3b" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">(1)在阶段-1 中，粗糙的带注释的图像和训练集被混合，产生 22，973 个图像。</li><li id="186b" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">(2)对于阶段 2，网络仅在训练集上进行微调。</li><li id="4df4" class="mc md iq kz b la ml ld mm lg mn lk mo lo mp ls mh mi mj mk bi translated">在评估阶段，图像被分成四个 1024×1024 的作物，它们的得分图被融合。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/78d3d9109d34972c5a6898992747246a.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*oNW5aDelPtyC685xEhbsgg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Cityscapes validation set</strong></figcaption></figure><ul class=""><li id="ec11" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">利用多尺度(MS)和条件随机场(CRF)，获得了 77.4%的平均 IoU。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/fc50563029e05e1f2278aff59cf22c4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*gZscRUFbZ7LbXfjvOjhROQ.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Cityscapes test set</strong></figcaption></figure><ul class=""><li id="723d" class="mc md iq kz b la lb ld le lg me lk mf lo mg ls mh mi mj mk bi translated">最终，在测试集上获得了 76.9%的平均 IoU，优于<a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN-8s</a>【29】、<a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>【37】、<a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">扩容网</a>【36】、<a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">DeepLabv2</a>【7】。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pa"><img src="../Images/2bf006923f9c63dfb70a2391a87d7499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vLw7DOl6H_RgF_EfCi371w.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Qualitative Results</strong></figcaption></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h2 id="87e6" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">参考</h2><p id="df7c" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg pb li lj lk pc lm ln lo pd lq lr ls ij bi translated">【2017 CVPR】【GCN】<br/><a class="ae kf" href="https://arxiv.org/abs/1703.02719" rel="noopener ugc nofollow" target="_blank">大内核事项——通过全局卷积网络改进语义分割</a></p><h2 id="4cab" class="ny na iq bd nb nz oa dn nf ob oc dp nj lg od oe nl lk of og nn lo oh oi np oj bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg pb li lj lk pc lm ln lo pd lq lr ls ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/></strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN<br/></a></p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">实例分割<br/> </strong> [ <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"/><br/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">【DeepPose】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">【汤普森 NIPS'14】</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">【汤普森 CVPR'15】</a></p></div></div>    
</body>
</html>