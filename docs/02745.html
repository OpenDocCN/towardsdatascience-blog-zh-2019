<html>
<head>
<title>Principal Component Analysis Example In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的主成分分析示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-part-15-dimensionality-reduction-with-principal-component-analysis-a5b3bb7353bc?source=collection_archive---------9-----------------------#2019-05-04">https://towardsdatascience.com/machine-learning-part-15-dimensionality-reduction-with-principal-component-analysis-a5b3bb7353bc?source=collection_archive---------9-----------------------#2019-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1824c024e5471765fedb11cd6bc6911c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cdg8jL6-A7As6vmh"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="c56b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主成分分析或 PCA 用于在不损失太多信息的情况下减少特征的数量。拥有过多维度的问题在于，它使得数据难以可视化，并且使得训练模型在计算上更加昂贵。</p><p id="f67a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了让我们对 PCA 有一个直观的理解，假设我们想要建立一个预测房价的模型。我们将从收集该地区房屋的数据开始。假设我们收集了数以千计的样本，每个样本都包含了一个给定房屋的属性信息。这些属性包括卧室数量、浴室数量和面积。常识会让我们相信，卧室的数量、浴室的数量和房子的面积之间存在某种关系。换句话说，我们怀疑房子的面积越大，包含的卧室和浴室就越多。如果变量高度相关，我们真的有必要为同一基本特征(即大小)设置三个独立的变量吗？主成分分析用于确定哪个变量(即浴室数量或平方英尺)占房价的最大差异，并将它们结合起来。</p><p id="72c0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们绘制了一个任意变量的三个样本。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi le"><img src="../Images/c8bfa62e1e0e1e7805767579b79619fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*osGBmfsp6Q5RYq5sC5VrpQ.png"/></div></figure><p id="ad00" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">平均值等于所有数据点的总和除以样本总数。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lj"><img src="../Images/72730b45350be19c73a540f8e0e3f13d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGkAdeuovazPCxotjRhwDw.png"/></div></div></figure><p id="e090" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单个数据点的方差就是它与平均值的距离。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/d9b4919c1ed20b9de14e5f3aebadd52b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*mbtFi_Y6dmXM_iAhTbMImA.png"/></div></figure><p id="1020" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">整个数据集的方差是所有这些距离的平方和除以样本总数。我们最终将这些值平方，因为在坐标系中，平均值左侧的距离将为负，并且会与平均值右侧的距离抵消。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/608a69e3aff84256a6a25a80bf034fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*chwyH_A9CRJ8RKVrwNfi-g.png"/></div></figure><p id="085f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在二维空间中，为了计算其中一个变量的方差，我们将数据点投影到它的轴上，然后遵循与前面相同的过程。一个特征(即薪水)的均值和方差是相同的，不管它所绘制的其他特征是什么。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lm"><img src="../Images/3de1895f10ac1ad6cc758a5f4f925a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-fe4TLK7XeIXtYKPzTOnRg.png"/></div></div></figure><p id="694a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可惜的是，<em class="ln"> </em> <strong class="ki jk"> <em class="ln"> x </em> </strong>和<em class="ln"> </em> <strong class="ki jk"> <em class="ln"> y </em> </strong>方差本身并没有包含足够的信息。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lo"><img src="../Images/9948a0f76563a55a15825e8c2ed61b16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_-oUKPfPbEjC7hWqok0vA.png"/></div></div></figure><p id="6d9e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管两个图有明显的差异，但它们导致相同的<strong class="ki jk"> <em class="ln"> x </em> </strong>和<em class="ln"> </em> <strong class="ki jk"> <em class="ln"> y </em> </strong>方差。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lp"><img src="../Images/586459f6d86da399f1b52e564eaae104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfE-oMh1-5UdwIRXVFWdCQ.png"/></div></div></figure><p id="6368" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们利用另一个叫做协方差的性质。计算协方差时，我们将 x 和 y 坐标相乘，而不是取平均值的距离的平方。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lq"><img src="../Images/e831a03a430063ea657a090627a6a8ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LsARCfCfBLuegndIRjqwvA.png"/></div></div></figure><p id="f439" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">协方差和相关性是不同的概念。然而，它们都描述了两个变量之间的关系。更具体地说，两个变量之间的相关性实际上是协方差除以第一个变量的方差的平方根乘以第二个变量的方差。因此，当相关或协方差为负时，x 的增加导致 y 的减少，当相关或协方差为正时，x 的增加导致 y 的增加。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lr"><img src="../Images/6210953a84116bc21df3842ffe2d9988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PRI9Aj5aDiuJMVdxOuLcsg.png"/></div></div></figure><p id="cd1f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个变量<strong class="ki jk">的<strong class="ki jk">协方差矩阵</strong>由左上角第一个变量的方差、右下角第二个变量的方差和对角线上剩余两个位置的协方差组成。</strong></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/c9b4feda158f134f8d8c4637ea448f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*yXz3o5b6YyqseyjYkm2U_Q.png"/></div></figure><p id="a3f7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为 PCA 算法的一部分，协方差矩阵用于计算特征值和特征向量。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lt"><img src="../Images/37ea5ca2921ff92f75605671844bba20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4A0kqMN1VrIaTYrQS47aw.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lu"><img src="../Images/b56210c1cd778ccb6ba532710d3608d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ht3lJxmMW6hAfimx-ZZ2mQ.png"/></div></div></figure><p id="748e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们画出了两个不同变量之间的关系。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lv"><img src="../Images/fefd235f3ebc48a23f7960a96dee4307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*mA39YIKp91gPMtXYVqrVHA.png"/></div></div></figure><p id="f73e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们将数据居中。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lw"><img src="../Images/7d5884f24b09208a787d8d5c8a4faf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ioZJKTDNOgYFtEHZqyFRg.png"/></div></div></figure><p id="c2a8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们在特征向量的方向上画两个向量，它们的大小与特征值相等。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lx"><img src="../Images/3f8ea378868ac0888cdcf2fa3feb536d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16yhHmbIGDB5hUTFs3A91Q.png"/></div></div></figure><p id="0111" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我们选择方差最大的一个，因为当我们删除另一个维度时，它会导致最少的信息损失。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ly"><img src="../Images/89893891fa717a9381d6d82b370e1e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpie-eebwurOH7M-Jv8T6A.png"/></div></div></figure><p id="b534" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，数据点被投影到直线上。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lz"><img src="../Images/c1d6b6d624dbd301dc1f437d58beb833.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XiIrtnbR8hJH1HeOGgOYZw.png"/></div></div></figure><p id="8b8a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后者用作新特征的一维图。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ma"><img src="../Images/a8d852ad56431e7fdcbd04394a6d4dba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWowCEc801IY1efUOfSL_Q.png"/></div></div></figure><h1 id="8c28" class="mb mc jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">密码</h1><p id="ba63" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">让我们看看如何用 python 实现主成分分析。首先，导入所有必需的库。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="f38e" class="nj mc jj nf b gy nk nl l nm nn">import pandas as pd<br/>import numpy as np<br/>from matplotlib import pyplot as plt<br/>from sklearn.decomposition import PCA<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.datasets import load_iris</span></pre><p id="3879" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本例中，我们将使用 iris 数据集，该数据集可以通过 sklearn API 轻松导入。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="1fe4" class="nj mc jj nf b gy nk nl l nm nn">iris = load_iris()</span><span id="131b" class="nj mc jj nf b gy no nl l nm nn">X = pd.DataFrame(iris.data, columns=iris.feature_names)<br/>y = pd.Categorical.from_codes(iris.target, iris.target_names)</span></pre><p id="e0d0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你所见，有 4 个特点。凭直觉，我们可以预测萼片长度和萼片宽度之间有很强的相关性，花瓣长度和花瓣宽度之间也有很强的相关性。因此，我们应该能够将维数从 4 减少到 2。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="f7a2" class="nj mc jj nf b gy nk nl l nm nn">X.head()</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/8fd7c87490361bd774e274bb95802631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P8vVYbYSWbnxbeOYVGWtjw.png"/></div></div></figure><p id="b813" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们前面看到的，方差是通过从平均值中取距离的平方和来计算的。因此，如果一个要素的尺度比另一个要素大得多，那么即使相对离差可能较小，它的方差也会大得多。因此，我们必须扩展数据。在缩放数据时，平均值设置为 0，标准偏差设置为 1。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="5343" class="nj mc jj nf b gy nk nl l nm nn">scaler = StandardScaler()<br/>X = scaler.fit_transform(X)</span></pre><p id="0e2d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将使用 PCA 将维数从 4 减少到 2。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="31d1" class="nj mc jj nf b gy nk nl l nm nn">pca = PCA(n_components=2)</span><span id="9afc" class="nj mc jj nf b gy no nl l nm nn">principal_components = pca.fit_transform(X)</span><span id="d3c6" class="nj mc jj nf b gy no nl l nm nn">new_X = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])</span></pre><p id="f2d3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看这两个新特性。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="491a" class="nj mc jj nf b gy nk nl l nm nn">new_X.head()</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f23e5796f99517142544ae1128507bd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*SQ89vFexIDrw8aHZmIzFvQ.png"/></div></figure><p id="3848" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以使用一个 scree 图来可视化每个主成分解释的方差的百分比。</p><pre class="lf lg lh li gt ne nf ng nh aw ni bi"><span id="333c" class="nj mc jj nf b gy nk nl l nm nn">per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)</span><span id="e177" class="nj mc jj nf b gy no nl l nm nn">labels = ['PC' + str(x) for x in range(1, len(per_var) + 1)]</span><span id="1a33" class="nj mc jj nf b gy no nl l nm nn">plt.bar(x=range(1, len(per_var)+1), height=per_var, tick_label=labels)<br/>plt.ylabel('percentange of explained variance')<br/>plt.xlabel('principal component')<br/>plt.title('scree plot')<br/>plt.show()</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5f4d6ca1ae39fc9f172cccc0734c324d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*KywAOd0LlaMwzGqBc05rqA.png"/></div></figure><h1 id="f451" class="mb mc jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">最后的想法</h1><p id="7131" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">总而言之，我们采用一个多维数据集并绘制它(虽然我们不能可视化任何大于 3 维的东西，但数学仍然会解决)。然后我们计算协方差矩阵和特征向量/特征值。后者将告诉我们哪些特征是高度相关的，并且可以在没有太大信息损失的情况下被压缩。产生的特征可用于训练我们的模型。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/e0feb9deedd5940831635fbc71a1417e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TIo1GXZw5lXDsJzxin95ag.png"/></div></div></figure></div></div>    
</body>
</html>