<html>
<head>
<title>Steps to basic modern NN model from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始到基本现代神经网络模型的步骤</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/steps-to-basic-modern-nn-model-from-scratch-1e86b7c042?source=collection_archive---------20-----------------------#2019-11-09">https://towardsdatascience.com/steps-to-basic-modern-nn-model-from-scratch-1e86b7c042?source=collection_archive---------20-----------------------#2019-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="557a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">该博客将包括创建初级神经网络的步骤，从理解矩阵乘法开始，到构建你的训练循环。除了构建神经网络，我还将讨论各种定制技术。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/42ca5b092462e4c697c8c857714fcb04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w03uvoQUwhNKye4HFvdzKg.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@skraidantisdrambliukas?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Gertrūda Valasevičiūtė</a> on <a class="ae kv" href="https://unsplash.com/s/photos/artificial-intelligence?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2be1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">步骤 1 — MatMul</h1><p id="53b0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">今天，我们将学习构建神经网络的第一步，即初级矩阵乘法。有许多方法可以做到这一点，我们将看到每一种方法，并将比较它们以获得最佳结果。</p><p id="ecb4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们需要神经网络的线性层中的矩阵乘法。要做矩阵乘法，我们需要一个数据集。Fastai 很好地提供了各种数据集，我们将使用 MNIST 数据集来做操作。</p><p id="2b90" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以，让我们抓住数据集。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><ul class=""><li id="ccaa" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">首先，我正在导入我将在整个系列中使用的库。</li><li id="b91d" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">之后，我正在下载带有扩展名<code class="fe nf ng nh ni b">.gz</code>的 MNIST 数据集。</li><li id="5635" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">因为下载的文件是 pickle 格式的；因此，我使用函数<code class="fe nf ng nh ni b">pickle.load</code>来访问数据集。</li><li id="682b" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">下载的数据集是 numpy 数组，我们希望 PyTorch 张量执行关于神经网络的操作。因此，我使用<code class="fe nf ng nh ni b">map</code>函数将 numpy 数组映射到 torch 张量。</li><li id="75f3" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">这样，我们的数据集就准备好了。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bdb0da828e2f4daa9d4b5f82ea5d6bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*gIvFg5QtsjTJymRpUoYeVA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Training dataset</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/2188d3bcd4098e8cbcc58a830b794a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*4RXkUbEwXXZNLam1ns2_mg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Validation dataset</figcaption></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="4c53" class="np kx iq ni b gy nq nr l ns nt">weights = torch.randn(784,10)<br/>bias = torch.zeros(10)</span><span id="f91d" class="np kx iq ni b gy nu nr l ns nt">m1 = x_valid[:5]<br/>m2 = weights</span><span id="55bb" class="np kx iq ni b gy nu nr l ns nt">m1.shape,m2.shape<strong class="ni ir"><br/>  = (torch.Size([5, 784]), torch.Size([784, 10]))</strong></span></pre><h2 id="0ac5" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">类型 1:简单的 Python 程序</h2><p id="434d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以用简单的 python 程序做矩阵乘法。但是 python 程序需要很多时间来实现，让我们来看看如何实现。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="aee4" class="np kx iq ni b gy nq nr l ns nt">%timeit -n 10 t1=matmul(m1, m2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/a11b1b4b5a43f2569aa1c615fcedf25e.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*4S96nuZwYApfTB5Vy6rEdg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">847 ms</figcaption></figure><ul class=""><li id="77a9" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">当验证数据集中只有 5 行时，847 ms 是一段很长的时间。</li></ul><h2 id="06e0" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">类型 2:元素式操作</h2><p id="09a0" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Pytorch 为我们提供了一种毫不费力的方法来做矩阵乘法，它被称为元素操作。让我们来理解一下。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><ul class=""><li id="8744" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">我们已经消除了最后一个循环。</li><li id="f476" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">在逐元素运算中，乘法单元被视为秩为 1 的张量。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="56e9" class="np kx iq ni b gy nq nr l ns nt">m2[:, 1].shape<br/>  <strong class="ni ir">=</strong> <strong class="ni ir">torch.Size([784])</strong></span><span id="4dfc" class="np kx iq ni b gy nu nr l ns nt">m1[1, :].shape<br/>  <strong class="ni ir">=</strong> <strong class="ni ir">torch.Size([784])</strong></span></pre><ul class=""><li id="1244" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">上面的代码在 c 中运行，让我们看看它执行所用的时间。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="4dfa" class="np kx iq ni b gy nq nr l ns nt">%timeit -n 10 t1=matmul(m1, m2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/168e38b4b23bd94e8aeb4fb545156c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*dAnwZCyTfz8Ud6TC70R0Aw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">1.31 ms &lt; 847 ms</figcaption></figure><h2 id="6327" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">类型 3:广播</h2><p id="f157" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">广播是矩阵乘法的另一种方式。</p><p id="264d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据 Scipy 文档，术语 broadcasting 描述了 numpy 在算术运算中如何处理不同形状的数组。在某些约束条件下，较小的阵列在较大的阵列中“广播”,以便它们具有兼容的形状。广播提供了一种向量化数组操作的方法，因此循环在 C 而不是 Python 中发生。这样做不会产生不必要的数据副本，并且通常会导致高效的算法实现。广播发生在一个 C 速度和 CUDA 速度的 GPU 上。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="3621" class="np kx iq ni b gy nq nr l ns nt">m1[2].unsqueeze(1).shape<strong class="ni ir"><br/>  = torch.Size([784, 1])</strong></span></pre><ul class=""><li id="e010" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">在广播中，尺寸为[1 * 784]的输入的整个列被压缩为[784 * 1]，然后乘以权重。</li><li id="2cda" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">最后，在列中取总和作为<code class="fe nf ng nh ni b">sum(dim=0)</code>，并存储在<code class="fe nf ng nh ni b">c</code>中。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="041e" class="np kx iq ni b gy nq nr l ns nt">%timeit -n 10 _=matmul(m1, m2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c1304e87f4e13528e5c812f95c9b55d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*4-tHXP3qj8i-JbGIKGq1UA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">265 us &lt;&lt; 1.31 ms &lt; 847 ms</figcaption></figure><h2 id="8d43" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">第四类:爱因斯坦求和</h2><p id="9746" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">爱因斯坦求和(<code class="fe nf ng nh ni b">einsum</code>)是一种以一般方式组合乘积和求和的紧凑表示。</p><p id="dc6f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">来自 numpy docs: <br/>"下标字符串是一个逗号分隔的下标标签列表，其中每个标签都指向相应操作数的一个维度。"</p><ul class=""><li id="3af7" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">这是一种更紧凑的表示。</li><li id="c59b" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">输入的数量代表输入的秩<code class="fe nf ng nh ni b">ik</code>表示秩 2 张量，<code class="fe nf ng nh ni b">kj</code>表示秩 2 张量。</li><li id="30d5" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">矩阵的维数表示为<code class="fe nf ng nh ni b">i*k</code>、<code class="fe nf ng nh ni b">k*j</code>和<code class="fe nf ng nh ni b">i*j</code>。</li><li id="91f4" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">每当你看到重复的维度，就在那个维度上做点积。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a686" class="np kx iq ni b gy nq nr l ns nt">%timeit -n 10 _=matmul(m1, m2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/18a8c233d33dbc275284c351abbbf28d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z7PSf4mcnd-STJSplVRVbQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">39.5 us &lt;&lt;&lt; 265 us &lt;&lt; 1.31 ms &lt; 847 ms</figcaption></figure><h2 id="cb5b" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">类型 5: PyTorch 操作</h2><p id="8274" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以直接用 PyTorch 的函数或算子进行矩阵乘法。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="c4a8" class="np kx iq ni b gy nq nr l ns nt">%timeit -n 10 t2 = m1.matmul(m2)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/126837b5b1097c34d61c9eed9ff62c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*t0NoP_D2utdJhlE--oX8Rw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">15.5 us &lt; 39.5 us &lt;&lt;&lt; 265 us &lt;&lt; 1.31 ms &lt; 847 ms</figcaption></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="0461" class="np kx iq ni b gy nq nr l ns nt">%timeit -n 10 t2 = m1@m2</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d3664b0a569ed85e020f7cdb55acfd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*18uad0V6xbCnM7q6XtWTEg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">10.9 us &lt; 15.5 us &lt; 39.5 us &lt;&lt;&lt; 265 us &lt;&lt; 1.31 ms &lt; 847 ms</figcaption></figure><p id="42e5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，我们可以很容易地比较各种代码的时序。这也是我们不喜欢用 Python 语言编写代码的原因，因为它的性能很慢。因此，大多数 python 库都是用 c 实现的。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><h1 id="b1c1" class="kw kx iq bd ky kz os lb lc ld ot lf lg jw ou jx li jz ov ka lk kc ow kd lm ln bi translated">步骤 2 和 3 — Relu/init &amp;向前传递</h1><p id="240e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在我们定义了矩阵乘法策略之后，是时候为神经网络定义 ReLU 函数和前向传递了。我想请读者浏览一下本系列的第 1 部分，以了解下面使用的数据的背景。</p><p id="4e02" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">神经网络定义如下:</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="609b" class="np kx iq ni b gy nq nr l ns nt">output = MSE(Linear(ReLU(Linear(X))))</span></pre><p id="7429" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">基础架构</strong></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="e11c" class="np kx iq ni b gy nq nr l ns nt">n,m = x_train.shape<br/>c = y_train.max()+1<br/><strong class="ni ir">n,m,c</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/845bcabb8490561e00026dd670d8b90b.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*kL5DBefdUzZ65W4UXuoVeg.png"/></div></figure><p id="437a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们解释一下矩阵乘法的权重。</p><p id="dec8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我将创建一个两层的神经网络。</p><ul class=""><li id="ead4" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">第一线性层将进行输入与 w1 的矩阵乘法。</li><li id="ed48" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">第一个线性图层的输出将作为第二个线性运算的输入，该输入将乘以 w2。</li><li id="78d8" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">我将获得单个输出，并使用 MSE 来计算损失，而不是针对单个输入获得十个预测。</li><li id="6c25" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">让我们宣布权重和偏差。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="dd62" class="np kx iq ni b gy nq nr l ns nt">w1 = torch.randn(m,nh)<br/>b1 = torch.zeros(nh)</span><span id="1507" class="np kx iq ni b gy nu nr l ns nt">w2 = torch.randn(nh,1)<br/>b2 = torch.zeros(1)</span></pre><ul class=""><li id="ad68" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">当我们使用 PyTorch <code class="fe nf ng nh ni b">randn</code>声明权重和偏差时，获得的权重和偏差被归一化，即它们的<strong class="lq ir">均值</strong>为 0，而<strong class="lq ir">标准差</strong>为 1。</li><li id="2ad2" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">我们需要对权重和偏差进行归一化，以便它们不会在对神经网络的输入进行线性运算后产生实质性的值。因为大量的输出对计算机来说变得难以处理。因此，我们倾向于将输入标准化。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/811e07d718044cae3f278cc80e2d1382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*ja2ZRiJuW7f6lZM0VrxxbA.png"/></div></figure><ul class=""><li id="102f" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">出于同样的原因，我们希望 out 输入矩阵的均值<strong class="lq ir">为 0，标准差<strong class="lq ir">为 1，但目前并非如此。让我们看看。</strong></strong></li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="3fc6" class="np kx iq ni b gy nq nr l ns nt">train_mean,train_std = x_train.mean(),x_train.std()<br/>train_mean,train_std</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/2435f44522a01230de7232038948311d.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*QLCbFBLvgoWco4qobI2acw.png"/></div></figure><ul class=""><li id="1bee" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">让我们定义一个函数来归一化输入矩阵。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="afdc" class="np kx iq ni b gy nq nr l ns nt">def normalize(x, m, s): return (x-m)/s</span><span id="297a" class="np kx iq ni b gy nu nr l ns nt">x_train = <strong class="ni ir">normalize</strong>(x_train, train_mean, train_std)<br/>x_valid = <strong class="ni ir">normalize</strong>(x_valid, train_mean, train_std)</span></pre><ul class=""><li id="fd40" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">现在，我们根据相同的均值和标准差对训练数据集和验证数据集进行归一化，以便我们的训练数据集和验证数据集具有相同的特征定义和比例。现在，让我们重新检查一下<strong class="lq ir">平均值</strong>和<strong class="lq ir">标准差</strong>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/aff245e4be6929ffcf2ee85eaea118e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*cAPB8Sc76ZIjRj7yAWxbfA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Mean approx. to 0 &amp; SD approx. to 1</figcaption></figure><ul class=""><li id="3ce5" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">现在，我们有了标准化的权重、偏差和输入矩阵。</li></ul><p id="db8a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们为神经网络定义<strong class="lq ir">线性层</strong>并执行操作。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="ce26" class="np kx iq ni b gy nq nr l ns nt">def lin(x, w, b): return x@w + b</span><span id="3c94" class="np kx iq ni b gy nu nr l ns nt">t = lin(x_valid, w1, b1)<br/>t.mean(),t.std()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/c76c75e63e4e86e7a387f5354d449717.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*99U46qF9C334bxh61si5Cw.png"/></div></figure><p id="5c03" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，线性运算后获得的平均值和标准偏差再次是非归一化的。现在，问题还是单纯的。如果保持这种状态，更多的线性运算将导致重要的和实质性的价值，这将是一个挑战。因此，我们希望线性运算后的激活也能被规范化。</p><h2 id="122b" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">简化的明凯初始化或氦初始化</h2><p id="13df" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">为了处理线性神经网络操作的非标准化行为，我们定义了将被明凯初始化的权重。尽管明凯归一化或初始化被定义为处理 ReLu/泄漏 ReLu 操作，我们仍然可以将其用于线性操作。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/1ec2be7b2a5ef61e79bdd7c9838cb156.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*yKzDsaYg8P77yM2wVo3BFg.png"/></div></figure><p id="075b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们将权重除以 math.sqrt(x ),其中 x 是行数。</p><p id="7e8e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">完成上述琐事后，我们得到归一化的平均值和标准差。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="82a7" class="np kx iq ni b gy nq nr l ns nt">def lin(x, w, b): return x@w + b</span><span id="5599" class="np kx iq ni b gy nu nr l ns nt">t = lin(x_valid, w1, b1)<br/>t.mean(),t.std()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/c96cd90dab290168c1f523c5fd1597cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*d0NYScfDGBTzdZ8ACevHDQ.png"/></div></figure><p id="dd8b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们为神经网络定义<strong class="lq ir">ReLU</strong>T10】层并执行操作。现在，为什么我们将 ReLU 定义为非线性激活函数，我希望你知道<strong class="lq ir"> <em class="pe">通用逼近定理。</em>T15】</strong></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a1e1" class="np kx iq ni b gy nq nr l ns nt">def relu(x): return x.clamp_min(0.)<br/>t = relu(lin(x_valid, w1, b1))</span><span id="ffed" class="np kx iq ni b gy nu nr l ns nt">t.mean(),t.std()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/25dc6092eb74c6cdc2ac51430201b3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*eHIcHpT2bz2a9pisZi14Ww.png"/></div></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="b08d" class="np kx iq ni b gy nq nr l ns nt">t = relu(lin(x_valid, w1, b1))<br/>t = relu(lin(t, w2, b2))</span><span id="0779" class="np kx iq ni b gy nu nr l ns nt">t.mean(),t.std()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/0a30c3658cf9bf9b1cb3b0ff9c12bbfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*paUHCEsbWF4qXyVl9nsgPw.png"/></div></figure><h2 id="03de" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">你注意到什么奇怪的事情了吗？</h2><p id="fcd8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">请注意，我们的标准偏差是线性运算后得到的一半，如果它在一层后减半，想象在八层后它将达到 1/2 ⁸，这非常非常小。如果我们的神经网络有 10000 层😵，算了吧。</p><p id="5519" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">来自 PyTorch docs: <br/> a:该层之后使用的整流器的负斜率(ReLU 默认为 0)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/ef2de9dfefac49d7c4786d8d01821ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*sKB7utoama8w6L_D-NdbiA.png"/></div></figure><p id="ebbf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是在描述 Imagenet 获奖方法的论文中介绍的，作者是何等人:<a class="ae kv" href="https://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank">深入研究整流器</a>，这也是第一篇声称 Imagenet 具有“超人性能”的论文(最重要的是，它介绍了 ResNets！)</p><p id="331c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，按照相同的策略，我们将把我们的权重乘以<code class="fe nf ng nh ni b">math.sqrt(2/m)</code>。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a0c6" class="np kx iq ni b gy nq nr l ns nt">w1 = torch.randn(m,nh)*math.sqrt(2/m)</span><span id="ce5f" class="np kx iq ni b gy nu nr l ns nt">t = relu(lin(x_valid, w1, b1))<br/>t.mean(),t.std()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/dcde7ea56226b20b62aa80301300526b.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*hnkkD8Xvgz_mxe_ZHEh-Aw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">It is still better than (0.1276, 0.5803).</figcaption></figure><p id="cc47" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">虽然我们有了更好的结果，但平均水平还是不太好。根据 fastai 文档，我们可以通过下面的调整来处理平均值。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="88ce" class="np kx iq ni b gy nq nr l ns nt">def relu(x): return x.clamp_min(0.) - 0.5</span><span id="dac6" class="np kx iq ni b gy nu nr l ns nt">w1 = torch.randn(m,nh)*math.sqrt(2./m )<br/>t1 = relu(lin(x_valid, w1, b1))<br/>t1.mean(),t1.std()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c0b0f9f478f741313ef54048aefc181d.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*6J-WYXmxTido8k5BmrU3dA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Now, it is so much better.</figcaption></figure><p id="fe6e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们结合上述所有代码和策略，创建我们的神经网络的<strong class="lq ir">正向传递。PyTorch 定义了明凯归一化的方法，即<code class="fe nf ng nh ni b">kaiming_normal_</code>。</strong></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="7b7a" class="np kx iq ni b gy nq nr l ns nt">def model(xb):<br/>  l1 = lin(xb, w1, b1)<br/>  l2 = relu(l1)<br/>  l3 = lin(l2, w2, b2)<br/>  return l3</span><span id="99e4" class="np kx iq ni b gy nu nr l ns nt">%timeit -n 10 _=<strong class="ni ir">model(x_valid)</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/4d94d4b68dd078f703e767e1938e3907.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*G5xwkEAZZhGl5XM9ObEcng.png"/></div></figure><p id="2a9c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于正向传递，最后要定义的是<strong class="lq ir">损失函数:MSE。</strong></p><p id="c021" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据我们以前的知识，我们通常使用<code class="fe nf ng nh ni b">CrossEntroyLoss</code>作为单标签分类函数的损失函数。我稍后会谈到这个问题。目前，我使用 MSE 来理解操作。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="781a" class="np kx iq ni b gy nq nr l ns nt">def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()</span></pre><p id="6b31" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们对训练数据集执行上述操作。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="4404" class="np kx iq ni b gy nq nr l ns nt">preds = model(x_train)<br/>preds.shape, preds</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/c50e37a5c6d45f45ce9bbdc8766a0996.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*JzuzAnNEJTAZQNMBo5ge9Q.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/d67e1551775b3f7a0291eea67459cd4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*6-Ew9x6mCQJIa_0HGMqzuQ.png"/></div></figure><p id="0e70" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了执行 MSE，我们需要浮动。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="88b2" class="np kx iq ni b gy nq nr l ns nt">y_train,y_valid = y_train.float(),y_valid.float()</span><span id="3074" class="np kx iq ni b gy nu nr l ns nt"><strong class="ni ir">mse(preds, y_train)</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/5fbb84e6e527611a23084f617908a323.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*I9bVVCwNYYwpCSxOsET2dw.png"/></div></figure><p id="e272" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在所有上述操作之后，仍有一个问题没有得到实质性的回答，它是</p><h1 id="8bfc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么我们需要明凯初始化？</h1><p id="1503" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们再来理解一下。</p><p id="de4a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如下初始化两个张量。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="3380" class="np kx iq ni b gy nq nr l ns nt">import torch</span><span id="f661" class="np kx iq ni b gy nu nr l ns nt"><strong class="ni ir">x</strong> = torch.randn(512)<br/><strong class="ni ir">a</strong> = torch.randn(512,512)</span></pre><p id="779f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对于神经网络，主要步骤是矩阵乘法，如果我们有一个大约 100 层的深度神经网络，那么让我们看看所获得的激活的标准偏差和平均值。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="7066" class="np kx iq ni b gy nq nr l ns nt">for i in range(100):<br/>    <strong class="ni ir">x = x @ a</strong></span><span id="ab42" class="np kx iq ni b gy nu nr l ns nt">x.<strong class="ni ir">mean()</strong>, x.<strong class="ni ir">std()</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/ba3133d40504bd82b5d66fc91a735378.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*Hl2D8a71ancsIWy2IekS3Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">mean=nan, sd=nan</figcaption></figure><p id="88b6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们可以很容易地看到平均值，标准差不再是一个数字。这也是合理的。计算机不能储存那么多的数字；它无法解释如此庞大的数字。出于同样的原因，它限制了从业者训练这样的深度神经网络。</p><p id="1fc8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">你会遇到的问题是激活爆炸:很快，你的激活将去南。我们甚至可以在第一次出现这种情况时要求循环中断:</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="5408" class="np kx iq ni b gy nq nr l ns nt">for i in range(100):<br/>    <strong class="ni ir">x</strong> = x @ a<br/>    if x.std() != x.std(): <strong class="ni ir">break</strong></span><span id="15ab" class="np kx iq ni b gy nu nr l ns nt">i</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi po"><img src="../Images/6dc3ba4765a96bea7b9cc7a76b16f246.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*qtUj-lwrXLftn7kcKG0pew.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Just after 28 loops</figcaption></figure><p id="93b7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">因此，这样的问题导致了明凯初始化的发明。最终想出这个主意肯定花了几十年时间。</p><p id="09ba" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi pp translated"><span class="l pq pr ps bm pt pu pv pw px di"> S </span> o，这就是我们如何定义神经网络的 ReLu 和反向传递。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><h1 id="ca1e" class="kw kx iq bd ky kz os lb lc ld ot lf lg jw ou jx li jz ov ka lk kc ow kd lm ln bi translated">步骤 4 —向后传球</h1><p id="1fac" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">到目前为止，我们已经理解了矩阵乘法、ReLu 函数和神经网络前向传递背后的思想。现在，我们将讨论向后传球。</p><p id="e9fe" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在进入后传之前，让我们先了解一个问题。</p><h2 id="ae85" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">什么是反向传播？</h2><p id="b869" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这是深度学习实践者的花哨术语，尤其是在你和某人讨论的时候。但是，更简单地说，反向传播只是通过链式法则计算梯度。我们发现关于权重/参数的梯度。就是这么简单。反向传播正好与正向传播的梯度相反。</p><p id="4c99" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们一步一步地寻找梯度。</p><p id="53fb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">—<strong class="lq ir">MSE 层的梯度</strong></p><p id="ed74" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe nf ng nh ni b">def mse(output, targ): return (output.squeeze(-1) — targ).pow(2).mean()</code></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="0f2e" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">mse_grad</strong>(inp, targ):<br/>    <strong class="ni ir"># grad of loss with respect to output of previous layer</strong><br/>    inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]</span></pre><ul class=""><li id="7b7f" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">我们计算输入层的梯度，这是前一层的输出。MSE 的梯度定义为<br/> <code class="fe nf ng nh ni b">2(predicted — target)/mean</code>。</li><li id="3fa9" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">由于计算梯度形成链规则，我们需要存储每一层的梯度，这得到乘以前一层。出于这个原因，我们将渐变保存在<code class="fe nf ng nh ni b">inp.g</code>输入层，因为它是前一层的输出层。</li></ul><p id="933a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">—<strong class="lq ir">ReLU 的坡度</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi py"><img src="../Images/b53ebeed5ba10a5ac18ae1e46a5b3223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8dvlB0LpublP8w87aWf-w.png"/></div></div></figure><p id="813a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，对于任何大于 0 的值，我们都要用 0 来代替，对于小于 0 的值，我们要保持它们为 0。</p><p id="da12" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe nf ng nh ni b">def relu(x): return x.clamp_min(0.)</code></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="eb59" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">relu_grad</strong>(inp, out):<br/>    <strong class="ni ir"># grad of relu with respect to input activations</strong><br/>    inp.g = (inp&gt;0).float() * out.g</span></pre><ul class=""><li id="4dde" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">我们将前一层的梯度相乘，这是 ReLU 的输出层，即<code class="fe nf ng nh ni b">out.g</code>。</li><li id="1586" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">这定义了链式法则。</li></ul><p id="1f3a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">— <strong class="lq ir">线性层的渐变</strong></p><p id="2453" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我发现线性图层的渐变比其他图层更难理解。但我会尽量简化。</p><p id="c228" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe nf ng nh ni b">def lin(x, w, b): return x@w + b</code></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a937" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">lin_grad</strong>(inp, out, w, b):<br/>    <strong class="ni ir"># grad of matmul with respect to weights</strong><br/>    <strong class="ni ir">inp.g</strong> = out.g @ w.t() <br/>    <strong class="ni ir">w.g</strong> = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)<br/>    <strong class="ni ir">b.g</strong> = out.g.sum(0)</span></pre><ul class=""><li id="99cc" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">inp.g —首先，我们计算关于参数的线性层的梯度。</li><li id="6d3b" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">w.g —重量梯度</li><li id="b2ef" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">b . g——与重量相关的偏差梯度。</li></ul><p id="98c1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，让我们把向前传球和向后传球结合起来。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="653c" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">forward_and_backward</strong>(inp, targ):</span><span id="2cbc" class="np kx iq ni b gy nu nr l ns nt"><strong class="ni ir"># forward pass:</strong><br/>    <strong class="ni ir">l1</strong> = inp @ w1 + b1<br/>    <strong class="ni ir">l2</strong> = relu(l1)<br/>    <strong class="ni ir">out</strong> = l2 @ w2 + b2  <br/>    <strong class="ni ir"># we don't actually need the loss in backward!</strong><br/>    <strong class="ni ir">loss</strong> = mse(out, targ)</span><span id="0f80" class="np kx iq ni b gy nu nr l ns nt"><strong class="ni ir"># backward pass:</strong><br/>    <strong class="ni ir">mse_grad</strong>(out, targ)<br/>    <strong class="ni ir">lin_grad</strong>(l2, out, w2, b2)<br/>    <strong class="ni ir">relu_grad</strong>(l1, l2)<br/>    <strong class="ni ir">lin_grad</strong>(inp, l1, w1, b1)</span></pre><p id="1d2a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><code class="fe nf ng nh ni b">forward_and_backward(x_train, y_train)</code></p><p id="9026" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我们把我们计算的梯度和 PyTorch 计算的梯度进行比较。</p><p id="1219" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在比较之前，我们需要存储我们的梯度。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="0578" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">w1g</strong> = w1.g.clone()<br/><strong class="ni ir">w2g</strong> = w2.g.clone()<br/><strong class="ni ir">b1g</strong> = b1.g.clone()<br/><strong class="ni ir">b2g</strong> = b2.g.clone()</span></pre><p id="591d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="pe">我们稍微作弊，用 PyTorch 签名来检查我们的结果。</em></p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="6ce3" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">xt2</strong> = x_train.clone().requires_grad_(True)<br/><strong class="ni ir">w12</strong> = w1.clone().requires_grad_(True)<br/><strong class="ni ir">w22</strong> = w2.clone().requires_grad_(True)<br/><strong class="ni ir">b12</strong> = b1.clone().requires_grad_(True)<br/><strong class="ni ir">b22</strong> = b2.clone().requires_grad_(True)</span></pre><p id="2aed" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们使用 PyTorch 定义正向函数来计算梯度。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="c730" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">forward</strong>(inp, targ):<br/>    <strong class="ni ir"># forward pass:</strong><br/>    <strong class="ni ir">l1</strong> = inp @ w12 + b12 <br/>    <strong class="ni ir">l2</strong> = relu(l1) <br/>    <strong class="ni ir">out</strong> = l2 @ w22 + b22<br/>    <strong class="ni ir"># we don't actually need the loss in backward!</strong><br/>    <strong class="ni ir">return</strong> mse(out, targ)</span><span id="9515" class="np kx iq ni b gy nu nr l ns nt">loss = forward(xt2, y_train)<br/>loss.backward()</span></pre><p id="a56d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们比较一下 w2 的梯度。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="fba1" class="np kx iq ni b gy nq nr l ns nt">w22.grad.T, w2g.T</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/269ac2e33015e66f554ef7a62a0f7520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*be1k_zP1CJQjYbdVCXBVHQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">They are almost the same.</figcaption></figure><p id="622a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi pp translated">现在，我们有很多方法可以重构上面写的代码，但这不是我们所关心的。我们需要理解神经网络反向传递背后的语义。这就是我们在模型中定义向后传球的方式。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><h1 id="3dc4" class="kw kx iq bd ky kz os lb lc ld ot lf lg jw ou jx li jz ov ka lk kc ow kd lm ln bi translated">步骤 4(b) —卷积网络的明凯初始化</h1><p id="ecec" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在前一部分中，我们寻找了稳定非线性激活动作效果的<strong class="lq ir">明凯初始化</strong>的需求。现在，主要是看明凯初始化如何用于卷积网络。我还会告诉你它是如何在 PyTorch 中实现的。因此，让我们开始学习之旅。</p><h2 id="2ccf" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">背景</h2><p id="abef" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">从上一章中，我们得到了下面的值。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="4dd7" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">x_train.shape, y_train.shape, x_valid.shape, y_valid.shape</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/b82ff4e961c48873a73c398da2993906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*T7eKUc_e1asCQ9DCw49i-w.png"/></div></figure><p id="a302" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们看看数据集的形状。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="74d9" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">x_train</strong>[:100].shape, <strong class="ni ir">x_valid</strong>[:100].shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/f088d1cbd3da5fdefcb30b505ce65a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*toTFhZc5Lrk9tO2Sr9EM4Q.png"/></div></figure><p id="7af5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据我们对<a class="ae kv" rel="noopener" target="_blank" href="/cnn-resnets-a-more-liberal-understanding-a0143a3ddac9">卷积网络</a>的了解，让我们使用 PyTorch 创建一个简单的卷积神经网络。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="5e7e" class="np kx iq ni b gy nq nr l ns nt">import torch.nn as <strong class="ni ir">nn</strong></span><span id="2643" class="np kx iq ni b gy nu nr l ns nt"><strong class="ni ir">nh</strong>=32<br/><strong class="ni ir">l1</strong> = nn.<strong class="ni ir">Conv2d</strong>(1, nh, 5)</span></pre><ul class=""><li id="f684" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">CNN 的输入层数是 1。</li><li id="5000" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">输出滤波器或层的数量是 32。</li><li id="d085" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">5</code>代表仁的大小。</li></ul><p id="edd4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当我们谈到明凯初始化时，首先想到的是计算卷积神经网络权重的均值和标准差。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="96d3" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">stats</strong>(x): return x.mean(),x.std()</span></pre><p id="9426" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">l1 在其中定义了权重。在计算统计数据之前，让我们先了解它们。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="3f07" class="np kx iq ni b gy nq nr l ns nt">l1.weight.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/e56502d52d6ba6cc667bc5afea19dd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*jLrtXt-d_nlprYTBZkV49g.png"/></div></figure><p id="8495" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">按重量计:</p><ul class=""><li id="a5ef" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">32</code>表示输出层/过滤器的数量。</li><li id="9276" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">1</code>代表输入层数</li><li id="5a49" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">5, 5</code>代表内核大小。</li></ul><p id="cfe5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们需要关注卷积神经网络的输出。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="3dbb" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">x = x_valid[:100] </strong><br/># <em class="pe">you may train the neural network using the training dataset but for now, I am taking vaidation daaset. There is no specific reason behind it.</em></span><span id="7b95" class="np kx iq ni b gy nu nr l ns nt">x.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/68a310f20f1b7b3191b88e2a41136b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*71A4AeRd5K5KqNGo6AZvMQ.png"/></div></figure><ul class=""><li id="4f00" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">100</code> —图像数量</li><li id="d1b7" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">1</code> —输入层。</li><li id="383a" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">28, 28</code> —输入图像的尺寸。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a293" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">t</strong> = l1(x)<br/>t.shape</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/6a2c1cac05fcaa1a8ab8656f76f3185d.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*J-c7AP4QgtHd-Tffl-w9sQ.png"/></div></figure><ul class=""><li id="945e" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">100</code> —图片数量</li><li id="d0ad" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">32</code> —输出层数</li><li id="1c2e" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">24, 24</code> —果仁大小</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="5dfe" class="np kx iq ni b gy nq nr l ns nt">stats(t)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/90eeff4fb2eb12d331fd9affef5e5768.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*5hO-kFjVGeDiUREzvn11FQ.png"/></div></figure><p id="97eb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是，我们希望标准差为 1，而不是 0.6，尽管我们的平均值为 0。因此，让我们将明凯初始化应用于权重。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="1368" class="np kx iq ni b gy nq nr l ns nt">init.<strong class="ni ir">kaiming_normal_</strong>(l1.weight, a=1.)<br/><strong class="ni ir">stats</strong>(l1(x))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/60b75a64aba2f2e3128496b3eeaed146.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*Ism-zV5-f-RXxBltQRI7ZA.png"/></div></figure><p id="53cf" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，更好了。平均值几乎为 0，标准差大约为 1。</p><p id="45c6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是，明凯初始化被引入来处理非线性激活函数。让我们来定义一下。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="530f" class="np kx iq ni b gy nq nr l ns nt">import torch.nn.functional as <strong class="ni ir">F</strong></span><span id="d215" class="np kx iq ni b gy nu nr l ns nt">def f1(x,a=0): return F.<strong class="ni ir">leaky_relu</strong>(l1(x),a)</span><span id="881b" class="np kx iq ni b gy nu nr l ns nt">init.<strong class="ni ir">kaiming_normal_</strong>(l1.weight, a=0)<br/><strong class="ni ir">stats</strong>(f1(x))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/19485712125c394af54808431ef36396.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*m9f21lwkBeKruxHszr2dXQ.png"/></div></figure><p id="35b5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">— Mean 不在 0 附近，但 SD 几乎等于 1。</p><p id="b751" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">没有明凯初始化，让我们找到统计数据。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="0856" class="np kx iq ni b gy nq nr l ns nt">l1 = nn.<strong class="ni ir">Conv2d</strong>(1, nh, 5)<br/>stats(f1(x))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/f6d399b66218224a79edb70c135a6f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*ljjGJKMNJOY6_8dXU-60kQ.png"/></div></figure><ul class=""><li id="afb8" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">现在，你可以很容易地比较有和没有明凯的统计数据。</li><li id="986b" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">在明凯，结果要好得多。</li></ul><h1 id="8aaa" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">比较</h1><p id="6bfb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，让我们将我们的结果与 PyTorch 进行比较。在此之前，我们需要看到 PyTorch 代码。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a6a9" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">torch.nn.modules.conv._ConvNd.reset_parameters</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qi"><img src="../Images/5fd62bc8979d97ad0279cc15963210ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOLTrRwvtX46ohPBjN2iSg.png"/></div></div></figure><h2 id="1f1b" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">明凯制服</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qj"><img src="../Images/dfdc69eece98fffc19ad83128f9a5896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3h9TglSNT4ni5j9_rZRGaw.png"/></div></div></figure><h2 id="09bd" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">计算 _ 校正 _ 风扇</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qk"><img src="../Images/cc947c657e27a49f79666f9ab9643e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ao3KoMwV4-GEHuG1ff4iHQ.png"/></div></div></figure><h2 id="d2ff" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">计算扇入扇出</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ql"><img src="../Images/1772e96a57f572ad04c8fd7e850e2e55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PjpeIv59LM_bsSoOIMa1hw.png"/></div></div></figure><p id="a7c9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们了解一下上面的方法。</p><ul class=""><li id="c897" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">PyTorch 使用明凯制服，而不是明凯正常。明凯统一在价值边界上不同于后者。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qm"><img src="../Images/eaf003ac23d65fe410eb01a60506e354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1o2VK-0g4OHdo6XlU8A8A.png"/></div></div></figure><ul class=""><li id="1319" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">calculate_fan_in_fan_out()中有一个变量<code class="fe nf ng nh ni b">receptive_field_size</code>。计算如下。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="09c1" class="np kx iq ni b gy nq nr l ns nt">l1.weight.shape<br/> <strong class="ni ir"> = torch.Size([32, 1, 5, 5])</strong></span><span id="97aa" class="np kx iq ni b gy nu nr l ns nt">l1.weight[0,0].<strong class="ni ir">shape<br/>  = torch.Size([5, 5])</strong></span><span id="f876" class="np kx iq ni b gy nu nr l ns nt">rec_fs = <strong class="ni ir">l1.weight[0,0].numel()</strong><br/><strong class="ni ir">rec_fs</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/69dcb8431d5d8805933218244b383046.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*0eW1FyWLJtcN_3o-sGv5Sw.png"/></div></figure><ul class=""><li id="441f" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">fan_in(number of input parameters)</code>和<code class="fe nf ng nh ni b">fan_out(number of output filters)</code>计算如下。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="f50b" class="np kx iq ni b gy nq nr l ns nt">nf,ni,*_ = <strong class="ni ir">l1.weight.shape</strong><br/>nf,ni</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qo"><img src="../Images/ab0791798d5f9d97b32bf5b963759223.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*lX_vBw2UVG67qmgMYHYdqQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">32 is output filters/layers, 1 is the input layer</figcaption></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="74ee" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">fan_in</strong>  = ni*rec_fs<br/><strong class="ni ir">fan_out</strong> = nf*rec_fs</span><span id="5514" class="np kx iq ni b gy nu nr l ns nt">fan_in,fan_out</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/df2dde924129d1cdff8693e2d8056586.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*8w5Z24Q8f2mHa7yf_un4LQ.png"/></div></figure><ul class=""><li id="46bb" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">明凯 _ 制服()里多了一个参数<code class="fe nf ng nh ni b">gain</code>。它是非线性激活函数中的泄漏量。其定义如下。</li></ul><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="5dc3" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">gain</strong>(a): return math.sqrt(2.0 / (1 + a**2))</span></pre><ul class=""><li id="0282" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">对于 ReLU，a 的值为 0。因此，ReLU 的 gain 值是 math.sqrt(2.0)。查看下面的链接。</li></ul><div class="qq qr gp gr qs qt"><a href="https://github.com/pytorch/pytorch/blob/fd4f22e4eaae17b8ee8c7de8b0b2b0e202fdf147/torch/nn/init.py#L60" rel="noopener  ugc nofollow" target="_blank"><div class="qu ab fo"><div class="qv ab qw cl cj qx"><h2 class="bd ir gy z fp qy fr fs qz fu fw ip bi translated">皮托赫/皮托赫</h2><div class="ra l"><h3 class="bd b gy z fp qy fr fs qz fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="rb l"><p class="bd b dl z fp qy fr fs qz fu fw dk translated">github.com</p></div></div><div class="rc l"><div class="rd l re rf rg rc rh kp qt"/></div></div></a></div><p id="5f75" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从所有上述知识，我们可以创建我们的明凯 _ 制服如下。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="17d5" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">kaiming2</strong>(x,a, use_fan_out=False):<br/>    <strong class="ni ir">nf,ni</strong>,*_ = x.shape<br/>    <strong class="ni ir">rec_fs</strong> = x[0,0].shape.numel()<br/>    <strong class="ni ir">fan</strong> = nf*rec_fs if use_fan_out else ni*rec_fs<br/>    <strong class="ni ir">std</strong> = gain(a) / math.sqrt(fan)<br/>    <strong class="ni ir">bound</strong> = math.sqrt(3.) * std<br/>    <strong class="ni ir">x.data.uniform_</strong>(-bound,bound)</span></pre><p id="b7ff" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们来计算一下统计数据。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="3cbb" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">kaiming2</strong>(l1.weight, a=0);<br/><strong class="ni ir">stats</strong>(f1(x))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ri"><img src="../Images/cfae24d1f8c044d3ba13ae83f89b2163.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*ZZc2jaQDkIKrj1vYPlh8xA.png"/></div></figure><p id="d46d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">成绩还是比较好的。</p><p id="08e3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这就是明凯概念在卷积神经网络中的应用。</p></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><h1 id="9098" class="kw kx iq bd ky kz os lb lc ld ot lf lg jw ou jx li jz ov ka lk kc ow kd lm ln bi translated">步骤 5 —训练循环</h1><p id="8427" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">现在，我们已经到了需要了解交叉熵损失的地步，因为交叉熵损失主要用于单分类或多分类问题。因为我们使用 MNIST 数据集，所以我们需要创建一个神经网络来预测 10 个数字，即从 0 到 9。之前，我们使用 MSE 损失并预测单一结果，这是我们通常不做的。</p><p id="2447" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所以在深入学习损失函数之前，让我们使用 PyTorch nn.module 创建一个神经网络。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="0912" class="np kx iq ni b gy nq nr l ns nt">from torch import nn<br/>class <strong class="ni ir">Model</strong>(<strong class="ni ir">nn</strong>.Module):<br/>    def __init__(self, n_in, nh, n_out):<br/>        super().__init__()<br/>        <strong class="ni ir">self.layers</strong> = [nn.<strong class="ni ir">Linear</strong>(n_in,nh), nn.<strong class="ni ir">ReLU</strong>(),      nn.<strong class="ni ir">Linear</strong>(nh,n_out)]<br/>    <br/>    def __call__(self, x):<br/>        for l in self.layers: x = <strong class="ni ir">l(x)</strong><br/>            return <strong class="ni ir">x</strong></span></pre><p id="1924" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们有下面定义的变量。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="2691" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">n,m</strong> = x_train.shape<br/><strong class="ni ir">c</strong> = y_train.max()+1<br/><strong class="ni ir">nh</strong> = 50</span></pre><p id="578b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们再次定义权重。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="1854" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">w1</strong> = torch.randn(m,nh)/math.sqrt(m)<br/><strong class="ni ir">b1</strong> = torch.zeros(nh)<br/><strong class="ni ir">w2</strong> = torch.randn(nh,10)/math.sqrt(nh)<br/><strong class="ni ir">b2</strong> = torch.zeros(10)</span></pre><p id="7e4b" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">您可以观察到重量初始化的差异。这一次，我们需要十个预测，输出中的每个数字一个。这就是我把 w2 初始化为(nh，10)的原因。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="5024" class="np kx iq ni b gy nq nr l ns nt">model = Model(m, nh, 10)<br/>pred = model(x_train)</span><span id="4431" class="np kx iq ni b gy nu nr l ns nt"><strong class="ni ir">pred.shape</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rj"><img src="../Images/9c2cf1bcc4294ae2bb58747da7f59a4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*STH0olKwekRnCk111tutVA.png"/></div></figure><h2 id="7e7e" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">—交叉熵损失</h2><p id="531f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">同样，在沉迷于交叉熵损失之前，我们需要对我们的预测或激活进行 softmax。我们在这种情况下做 softmax 我们想要单一标签分类。在实践中，当我们计算损失时，我们将需要 softmax 的对数，因为它有助于进一步计算交叉熵损失。</p><p id="10a7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Softmax 定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rk"><img src="../Images/3619d19511138acb63b7ea0bc0b8f0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*hx7CCdDij0C_0S6erfhSgQ.png"/></div></figure><p id="03c4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">或者更简洁地说:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rl"><img src="../Images/cbd64fae8d5cf7c0ed59036f1a755e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*Z4G2ivzdgNABX0J8iST-5A.png"/></div></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="aa57" class="np kx iq ni b gy nq nr l ns nt">def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()</span></pre><ul class=""><li id="9153" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated"><code class="fe nf ng nh ni b">x.exp().sum(-1,keepdim=True)</code>将对该行激活的指数求和。</li><li id="9967" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">如果<code class="fe nf ng nh ni b">keepdim</code>是<code class="fe nf ng nh ni b">True</code>，则输出张量的大小与<code class="fe nf ng nh ni b">input</code>相同，除了在维度<code class="fe nf ng nh ni b">dim</code>中其大小为 1。否则，<code class="fe nf ng nh ni b">dim</code>被压缩，导致输出张量比<code class="fe nf ng nh ni b">input</code>少一个维度。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi po"><img src="../Images/bbbef0bd4a7b89df783bbccdeaa349b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*fZGS08W6jb8bicFx_ppiYA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Dimensions are squeezed</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rm"><img src="../Images/20bc26752c424de5782e103cc316ef57.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*hUcPoKbUS1SX2afVVshyTw.png"/></div></figure><p id="5a45" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">既然我们已经定义了<code class="fe nf ng nh ni b">log_softmax</code>，那就让我们取预测的<code class="fe nf ng nh ni b">log_softmax</code>。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="bb69" class="np kx iq ni b gy nq nr l ns nt">sm_pred = log_softmax(pred)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rn"><img src="../Images/4a46552261cba1a342c2d4ea01664491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*tf-k60d7C0imb-H8kKVnIQ.png"/></div></figure><p id="6246" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">交叉熵损失定义如下:</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="7522" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">def</strong> CrossEntropy(yHat, y):<br/>    <strong class="ni ir">if</strong> y == 1:<br/>      <strong class="ni ir">return</strong> -log(yHat)<br/>    <strong class="ni ir">else</strong>:<br/>      <strong class="ni ir">return</strong> -log(1 - yHat)</span></pre><p id="7a02" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在二进制分类中，类的数量 M 等于 2，交叉熵可以计算为:</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="43b8" class="np kx iq ni b gy nq nr l ns nt">−(<em class="pe">𝑦</em>log(<em class="pe">𝑝</em>)+(1−<em class="pe">𝑦</em>)log(1−<em class="pe">𝑝</em>))</span></pre><p id="5cb7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果 M&gt;2(即多类分类)，我们计算每个观察的每个类标签的单独损失，并对结果求和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ro"><img src="../Images/d02715b48664dbba54f86b5afc3348b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*rzD3og2DLsu5U3tQCm26sg.png"/></div></figure><p id="3f29" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们的问题是多类分类，所以我们的交叉熵损失函数将是后一个。多类分类的交叉熵损失函数也可以使用 numpy 风格的<a class="ae kv" href="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#integer-array-indexing" rel="noopener ugc nofollow" target="_blank">整数数组索引</a>来完成。让我们用它来实现。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="6636" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">nll</strong>(input, target): return -input[range(target.shape[0]), target].mean()<br/><strong class="ni ir">loss</strong> = nll(sm_pred, y_train)</span></pre><p id="158d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">请注意，公式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rp"><img src="../Images/de309d3659c7463ea5d5e647ba24a081.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*x46kfXI63OH5WTkF5Lalnw.png"/></div></figure><p id="4126" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当我们计算 log softmax 时，给出了一个简化，之前定义为<code class="fe nf ng nh ni b">(x.exp()/(x.exp().sum(-1,keepdim=True))).log()</code>。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="c0c1" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">log_softmax</strong>(x): return x - x.exp().sum(-1,keepdim=True).log()<br/><strong class="ni ir">loss</strong> = <strong class="ni ir">nll</strong>(log_softmax(pred), y_train)</span></pre><p id="dbc5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然后，有一种更稳定的计算指数之和的对数的方法，叫做<a class="ae kv" href="https://en.wikipedia.org/wiki/LogSumExp" rel="noopener ugc nofollow" target="_blank"> LogSumExp 窍门</a>。这个想法是使用下面的公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rq"><img src="../Images/1c763422b079a587796b548c8cf8cc44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*_mIm3TmXIFHQGBsW6WUeeg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">where a is the maximum of the <em class="rr">𝑥𝑗</em>.</figcaption></figure><ul class=""><li id="2cb3" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">从预测中取出最大值。</li><li id="a4eb" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">从预测的指数中减去最大值。</li><li id="5d43" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">最后，将最大值添加到操作日志中，如上图所示。</li><li id="7f24" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">它有助于计算，并使它更快，而不影响任何情况下的输出。</li></ul><p id="9913" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们定义我们的 LogSumExp。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="bb0a" class="np kx iq ni b gy nq nr l ns nt"><strong class="ni ir">m</strong> = <strong class="ni ir">pred</strong>.max(-1)[0]<br/>m[:,None]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rs"><img src="../Images/f3542b481b9c14b134f8982849b17bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*kQn3gy58NSEvJpuhyf9psQ.png"/></div></figure><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="a6b2" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">logsumexp</strong>(x):<br/>    m = x.max(-1)[0]<br/>    return m + (x-m[:,None]).exp().sum(-1).log()</span></pre><p id="d45d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="pe">因此</em>我们可以将它用于我们的<code class="fe nf ng nh ni b">log_softmax</code>功能。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="db16" class="np kx iq ni b gy nq nr l ns nt">def <strong class="ni ir">log_softmax</strong>(x): return x - x.<strong class="ni ir">logsumexp</strong>(-1,keepdim=True)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rt"><img src="../Images/c49b6a9553aa015f27ee7c4ecd6d3b66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*hquoDrZKxFemXihOB-sQMQ.png"/></div></figure><p id="f42c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们看看 PyTorch 中的上述实现。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="685d" class="np kx iq ni b gy nq nr l ns nt">import torch.nn.functional as F<br/>F.<strong class="ni ir">nll_loss</strong>(F.<strong class="ni ir">log_softmax</strong>(pred, -1), y_train)</span></pre><blockquote class="ru"><p id="3935" class="rv rw iq bd rx ry rz sa sb sc sd mj dk translated">在 PyTorch 中，<code class="fe nf ng nh ni b">F.log_softmax</code>和<code class="fe nf ng nh ni b">F.nll_loss</code>被组合在一个优化函数<code class="fe nf ng nh ni b">F.cross_entropy</code>中。</p></blockquote><h2 id="ca8f" class="np kx iq bd ky nv se dn lc nx sf dp lg lx sg oa li mb sh oc lk mf si oe lm of bi translated">—基本训练循环</h2><p id="80f4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">训练循环重复以下步骤:</p><ul class=""><li id="4ddb" class="mr ms iq lq b lr mk lu ml lx mt mb mu mf mv mj mw mx my mz bi translated">根据一批输入获取模型的输出</li><li id="5057" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">将输出与我们拥有的标签进行比较，并计算损失</li><li id="72ec" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">计算损失相对于模型每个参数的梯度</li><li id="efbd" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">用这些梯度更新参数/权重，使它们更好一点。</li><li id="fbe4" class="mr ms iq lq b lr na lu nb lx nc mb nd mf ne mj mw mx my mz bi translated">在称为<code class="fe nf ng nh ni b">epochs</code>的循环中重复上述步骤。</li></ul><p id="8adb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们结合上述所有概念，创建我们的循环。</p><pre class="kg kh ki kj gt nl ni nm nn aw no bi"><span id="11af" class="np kx iq ni b gy nq nr l ns nt">bs = 64<br/>lr = 0.5   # learning rate<br/>epochs = 1 # how many epochs to train for</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mp mq l"/></div></figure><p id="1875" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi pp translated"><span class="l pq pr ps bm pt pu pv pw px di"> S </span> o，这就是我们在神经回路中定义训练回路的方式。现在，在 PyTorch 中，它有一些语法差异，你现在可以理解了。随着时间的推移，我将在这个系列中添加更多的步骤。在那之前，请随意探索 fastai。</p><h2 id="968c" class="np kx iq bd ky nv nw dn lc nx ny dp lg lx nz oa li mb ob oc lk mf od oe lm of bi translated">参考</h2><ul class=""><li id="aa51" class="mr ms iq lq b lr ls lu lv lx sj mb sk mf sl mj mw mx my mz bi translated"><a class="ae kv" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai </a></li></ul></div></div>    
</body>
</html>