<html>
<head>
<title>What is XLNet and why it outperforms BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是 XLNet，为什么它的性能优于 BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335?source=collection_archive---------1-----------------------#2019-06-24">https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335?source=collection_archive---------1-----------------------#2019-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b264" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">XLNet 基础知识直观了解 XLNet 和 BERT 的区别</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/be6747941764add4ecde218ec4e676b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvyhsLKzXQtPIBQGs13SgQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@rickyc678?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Richard Cohen</a> on <a class="ae ky" href="https://unsplash.com/t/wallpapers?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b399" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">发布后不到一周，似乎我周围 NLP 领域的每个人都在谈论 XLNet。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lv lw l"/></div></figure><p id="6d2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，“<strong class="lb iu">在 20 个任务上改进了伯特”</strong>确实吸引了我们的目光。但更重要的是理解它是如何工作的，以及它为什么优于 BERT。所以我写这篇博客来分享我读后的想法。</p><p id="a160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内容结构如下。</p><ul class=""><li id="4d11" class="lx ly it lb b lc ld lf lg li lz lm ma lq mb lu mc md me mf bi translated">XLNet 是什么？</li><li id="b394" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">XLNet 和 BERT 有什么区别？</li><li id="83fe" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">XLNet 如何工作？</li></ul><p id="184e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对 XLNet 中的双流自我关注感兴趣，可以查阅我的另一篇帖子，<a class="ae ky" rel="noopener" target="_blank" href="/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3">什么是 XLNet 中的双流自我关注</a>。</p><h1 id="1ae6" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated"><strong class="ak">什么是</strong> XLNet <strong class="ak">？</strong></h1><p id="a70a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">首先，XLNet 是一个类似 BERT 的模型，而不是一个完全不同的模型。但这是一个非常有前途和潜力的机会。总之，<strong class="lb iu"> XLNet 是一种广义自回归预训练方法。</strong></p><p id="a285" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么什么是<strong class="lb iu">自回归(AR)语言模型</strong>？</p><p id="2693" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> AR 语言模型</strong>是一种利用上下文单词预测下一个单词的模型。但是这里上下文词被限制在两个方向，或者向前<strong class="lb iu">或者向后</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/99090f109580c9c0e436772a8a171a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*3MPbLNx3it_I5pQbgPL_GQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2b8ac843c0e99d664f3c780ae731b127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*VmQ159sh2WFpkdFBVjvySQ.png"/></div></figure><p id="20f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/huggingface/pytorch-openai-transformer-lm#pytorch-implementation-of-openais-finetuned-transformer-language-model" rel="noopener ugc nofollow" target="_blank"> GPT </a>和<a class="ae ky" href="https://github.com/graykode/gpt-2-Pytorch#gpt2-pytorch-with-text-generator" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>都是<strong class="lb iu"> AR 语言模型。</strong></p><p id="f690" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AR 语言模型<strong class="lb iu">的<strong class="lb iu">优势</strong>在于</strong> <strong class="lb iu">擅长</strong> <strong class="lb iu">生成性 NLP 任务。</strong>因为在生成上下文时，通常是正向的。AR 语言模型在这样的 NLP 任务上自然很好用。</p><p id="68ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是 AR 语言模型<strong class="lb iu">有一些缺点</strong>，它只能使用前向上下文或者后向上下文，也就是说<strong class="lb iu">不能同时使用前向和后向上下文</strong>。</p><h1 id="ab1e" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">【XLNet 和 BERT 有什么区别？</h1><p id="8382" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">与 AR 语言模型不同，BERT 被归类为<strong class="lb iu">自动编码器(AE)语言模型。</strong></p><p id="edba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">AE 语言模型</strong>旨在<strong class="lb iu">从损坏的输入中重建原始数据。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d94ac2eb97fc8bf2b28e5167fce0738f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*bmSZYhV6XlzRFcStRs1iDw.png"/></div></figure><p id="1bbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">被破坏的输入意味着我们在预训练阶段使用<code class="fe nl nm nn no b">[MASK]</code>替换原始令牌<code class="fe nl nm nn no b">into</code>。而目标是预测<code class="fe nl nm nn no b">into</code>得到原来的判决。</p><p id="5df1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AE 语言模型的<strong class="lb iu">优势</strong>在于可以看到<strong class="lb iu">上</strong> <strong class="lb iu">前后两个方向的上下文。</strong></p><p id="1300" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是 AE 语言模型也有它的缺点。在<strong class="lb iu">预训练</strong>中使用<code class="fe nl nm nn no b">[MASK]</code>，但是在<strong class="lb iu">微调时</strong>实际数据中没有这种人为符号，导致<strong class="lb iu">预训练-微调不一致。</strong>【屏蔽】的另一个缺点是<strong class="lb iu">它假定预测(屏蔽)的记号是相互独立的，给定未屏蔽的记号</strong>。例如，我们有一句话“它表明，住房危机变成了银行危机”。我们掩盖了“银行业”和“危机”。注意这里，我们知道被掩盖的“银行业”和“危机”彼此包含着隐含的关系。但是 AE 模型试图在给定非屏蔽令牌的情况下预测“银行业”，并在给定非屏蔽令牌的情况下单独预测“危机”。它忽略了“银行业”和“危机”之间的关系。换句话说，它假设<strong class="lb iu">预测的(屏蔽的)令牌是相互独立的。</strong>但是我们知道模型应该学习预测的(屏蔽的)记号之间的这种相关性，以预测记号之一。</p><p id="1f3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作者想强调的是，XLNet 提出了一种新的方法<strong class="lb iu">让 AR 语言模型从双向语境中学习</strong>以避免 AE 语言模型中 MASK 方法带来的弊端。</p><h1 id="9ce2" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">XLNet 如何工作？</h1><p id="57c1" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">AR 语言模型<strong class="lb iu">只能向前或向后使用上下文，</strong>那么如何让它从双向上下文中学习呢？</p><p id="7377" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">语言模型由两个阶段组成，训练前阶段和微调阶段。XLNet 专注于训练前阶段。在预训练阶段，它提出了一个新的目标叫做<strong class="lb iu">置换语言建模。</strong>从这个名字我们可以知道基本的想法，它使用<strong class="lb iu">排列。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/30d2feff55a37297bc6e7aebc2341ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RGdAU7tXXKqckbDjhoIWmQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Illustration from paper</figcaption></figure><p id="fffa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们举一个例子来说明。序列顺序是<code class="fe nl nm nn no b">[x1, x2, x3, x4]</code>。这种序列的所有排列如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3dc5f68df696d6ae278f8085bea6e06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*-ulAWcF9TzHwu5txfbiOZA.png"/></div></figure><p id="f607" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以对于这个 4 个记号(N)的句子，有 24 个(N！)排列。</p><p id="731b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">场景是我们想要预测<code class="fe nl nm nn no b">x3</code>。所以 24 种排列中有 4 种模式，<code class="fe nl nm nn no b">x3</code>在第 1 位，第 2 位，第 3 位，第 4 位。</p><pre class="kj kk kl km gt nr no ns nt aw nu bi"><span id="7240" class="nv mm it no b gy nw nx l ny nz">[x3, xx, xx, xx]<br/>[xx, x3, xx, xx]<br/>[xx, xx, x3, xx]<br/>[xx, xx, xx, x3]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/49badae00f5c680b0e3127e790937252.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*xTAR_am2FL7F89u20A3SLQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">four patterns</figcaption></figure><p id="393f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们将<code class="fe nl nm nn no b">x3</code>的位置设为<code class="fe nl nm nn no b">t-th</code>位置，<code class="fe nl nm nn no b"><strong class="lb iu">t-1</strong></code> <strong class="lb iu">记号为预测</strong> <code class="fe nl nm nn no b"><strong class="lb iu">x3</strong></code> <strong class="lb iu">的上下文词。</strong></p><p id="53e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nl nm nn no b">x3</code>之前的单词在序列中有每一个可能的单词和长度。直觉上，模型将学习<strong class="lb iu">从两边的所有位置收集信息。</strong></p><p id="5b42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实现远比上面的解释复杂，这里就不说了。但是你应该得到关于 XLNet 最基本最重要的想法。如果对 XLNet 中的双流自我关注感兴趣，可以查阅我的另一篇帖子，<a class="ae ky" rel="noopener" target="_blank" href="/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3">什么是 XLNet 中的双流自我关注</a>。</p><h1 id="924f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">来自 XLNet 的启示</h1><p id="9f93" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">就像 BERT 将掩码方法公之于众一样，XLNet 表明置换方法作为语言模型目标是一个很好的选择。可以预见，未来将会有更多的工作来探索语言模型目标。</p><blockquote class="ob oc od"><p id="899d" class="kz la oe lb b lc ld ju le lf lg jx lh of lj lk ll og ln lo lp oh lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae ky" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="lb iu"> <em class="it">中等</em> </strong> </a> <strong class="lb iu"> <em class="it">同</em> </strong> <a class="ae ky" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> <em class="it">一个分类查看</em> </strong> </a> <strong class="lb iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae ky" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="it">bramble Xu</em></strong></a><strong class="lb iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae ky" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="it">徐亮</em> </strong> </a> <strong class="lb iu"> <em class="it"> <br/>博客:</em></strong><a class="ae ky" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="it">bramble Xu</em></strong></a></p></blockquote><h1 id="38a5" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">参考</h1><ul class=""><li id="0162" class="lx ly it lb b lc nd lf ne li oi lm oj lq ok lu mc md me mf bi translated">论文:<a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1906.08237</a></li><li id="b52c" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated">代码:<a class="ae ky" href="https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/modeling_xlnet.py?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">py torch _ transformers/modeling _ xlnet . py</a></li><li id="e0db" class="lx ly it lb b lc mg lf mh li mi lm mj lq mk lu mc md me mf bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/what-is-two-stream-self-attention-in-xlnet-ebfe013a0cf3">XLNet 中的双流自我关注是什么</a></li></ul></div></div>    
</body>
</html>