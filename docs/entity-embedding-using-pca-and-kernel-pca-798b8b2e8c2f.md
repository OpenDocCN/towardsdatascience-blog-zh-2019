# 使用主成分分析和核主成分分析的实体嵌入

> 原文：<https://towardsdatascience.com/entity-embedding-using-pca-and-kernel-pca-798b8b2e8c2f?source=collection_archive---------25----------------------->

## 使用 scikit-learn 的无监督实体嵌入

![](img/b6d1009eca969bc62506ff47f9d1c5be.png)

Photo by [Zara Walker](https://unsplash.com/@mojoblogs?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/search/photos/curve?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

在关于这个主题的[上一篇文章](/spectral-encoding-of-categorical-features-b4faebdf4a)中，我们介绍了一种基于分类特征值的相似性来学习分类特征嵌入的方法。提醒你一下，我们要解决的问题是，很难找到一个好的分类特征变量到向量空间的映射(或嵌入)。常用的 1-hot 编码方法会产生许多额外的列，而分类特征对模型的影响太大，降低了模型的泛化能力。另一方面，顺序编码不会增加列数，但是如果选择不当，会降低数据的线性可分性，并损害模型性能。

理想的分类特征嵌入将避免创建太多的新列，但是从另一方面保持大多数数据的线性可分性。一种方法是在处理表格数据的神经网络中使用实体嵌入层。在[之前的故事](/spectral-encoding-of-categorical-features-b4faebdf4a)中，我们提出使用谱图理论来寻找分类特征的最佳嵌入。然而，该方法仅使用类别相似性，而不使用类别的概率分布。但是根据我们的经验，我们知道类别的概率分布在寻找最佳嵌入时起着非常重要的作用。例如，如果几个类别只有很少的记录，将它们绑定到单个值“other”中是有意义的，因为这将减少列的数量，并且不会对模型性能造成太大影响。

自动化这个决定的一个方法是使用 [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) 算法，或者类似的算法 [MCA](https://en.wikipedia.org/wiki/Multiple_correspondence_analysis) 。想法是在 1-hot 编码完成后使用 PCA(使用 MCA 需要额外的转换)。选择最高特征值向量，我们可以在低维向量空间中表示类别。主成分分析会将高频类别放得很远，但会将低频类别放得很近。让我们在人工例子上演示一下。和以前一样，我们考虑一周中的某一天特性，其中 0 表示星期一，6 表示星期日。假设大部分数据在周末出现，那么我们将创建一个合成数据集来演示我们的概念。

```
array([ 23,  21,  23,  19,  20, 456, 438])
```

我们看到周六和周日的数据比平日多得多。我们将在这个分类变量上拟合 PCA，但是只留下两个具有最高特征值的分量(即那些捕获最大方差的分量)

让我们绘制这些组件，看看这些类别是如何映射到二维向量空间的:

![](img/0b587ddd5f3341224b44da049f7fdc3e.png)

我们可以看到，周日和周六的向量彼此相距很远，但也与一周中的其他日子相距很远(周一和周四的向量相同，因此它们在图中标记为一个点)。

我们的方法是对每个分类变量进行 PCA 分析，而不是对整个数据集。如果分类变量是相对独立的，这种策略是合理的(在特征工程中我们应该总是做依赖性测试！).如果两个分类变量之间有很强的相关性，那么我们就可以把这些变量的联合概率分布表示为两个分类分布的乘积。在这种情况下，我们将不得不对两个类别的组合进行 1-hot 编码和 PCA，如果有许多组合只有很少的观察值或没有观察值，这将提供很差的结果。

# 核主成分分析

这里我们将发展我们在前面的工作中提出的思想，即使用类别相似度函数来影响类别嵌入。为此，我们可以使用一种众所周知的方法，称为[核 PCA](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis) ，核函数将被表示为对称矩阵。请注意，内核矩阵必须是[正定义的](https://en.wikipedia.org/wiki/Definiteness_of_a_matrix)。

与之前的笔记本类似，让我们将周六和周日与工作日区分开来，看看这是如何影响类别嵌入的:

```
array([[1\. , 0.9, 0.8, 0.7, 0.5, 0.1, 0.3],
       [0.9, 1\. , 0.9, 0.8, 0.5, 0.1, 0.2],
       [0.8, 0.9, 1\. , 0.9, 0.5, 0.1, 0.2],
       [0.7, 0.8, 0.9, 1\. , 0.6, 0.1, 0.2],
       [0.5, 0.5, 0.5, 0.6, 1\. , 0.7, 0.5],
       [0.1, 0.1, 0.1, 0.1, 0.7, 1\. , 0.8],
       [0.3, 0.2, 0.2, 0.2, 0.5, 0.8, 1\. ]])
```

```
array([[ 1.03305649, -0.19636025],
       [ 1.05001289, -0.38425859],
       [ 1.0480194 , -0.38846218],
       [ 1.01195609, -0.38461521],
       [ 0.29211243, -0.54331321],
       [-0.25333403, -0.22797924],
       [ 0.04688558,  0.32797451]])
```

![](img/753994b8dff7e564383911da67ecc75b.png)

我们在这里可以看到，除了星期二和星期三之外，工作日的间隔更远，这两天的值几乎相同(在图上用一个点表示)。

# 结论

我们看到核分类嵌入可以通过使用 PCA 而不是拉普拉斯公式来改进。仅仅对单个分类变量使用 PCA 是非常有用的，因为它允许将类别映射到低维向量空间，而不牺牲线性可分性。当在类别相似性的概念上增强时，应用核 PCA 来捕获概率分布和类别的相似性变得可能，这甚至进一步提高了线性可分性。

有几种方法来定义核函数。如果未知*先验*，则可以通过测量给定类别值的数据概率分布之间的 Wasserstein 距离从数据中导出，然后，例如，取负指数函数以得到正定义的矩阵:

![](img/2aabf6303e808c1f2b30a01874837268.png)