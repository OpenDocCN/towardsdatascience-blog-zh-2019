<html>
<head>
<title>Review: Hypercolumn (Instance Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:超列(实例分段)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=collection_archive---------16-----------------------#2019-03-25">https://towardsdatascience.com/review-hypercolumn-instance-segmentation-367180495979?source=collection_archive---------16-----------------------#2019-03-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f059" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">超柱，一个来自神经科学的术语，优于 SDS </strong></h2></div><p id="d7fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir">超柱</strong>被点评。<strong class="kh ir">“超柱”(Hypercolumn)</strong>、<strong class="kh ir">这一术语从神经科学</strong>中借来，用来描述排列在柱状结构中的一组对多个方向和多个频率的边缘敏感的 V1 神经元。通过借用超柱的思想，提高了预测精度，发表在<strong class="kh ir"> 2015 CVPR </strong>上，引用超过<strong class="kh ir"> 800 次。本书在 CVPR 出版时，第一作者 Bharath Hariharan 博士正在加州大学伯克利分校攻读博士学位。当 Hypercolumn 后来在<strong class="kh ir">扩展到 2017 TPAMI </strong>时，Hariharan 博士已经成为脸书人工智能研究(FAIR)的博士后研究员。之后，另一篇著名论文《特征金字塔网络》(<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>)于 2017 年在 CVPR 发表。现在，他成为康奈尔大学的助理教授，从事三维空间的计算机视觉研究。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----367180495979--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="8715" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="d83a" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la mt mu mv mw bi translated"><strong class="kh ir">超柱概念</strong></li><li id="c3aa" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">使用超柱的像素分类</strong></li><li id="1f1d" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">高效超柱</strong></li><li id="3679" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">快速超柱(2017 年 TPAMI) </strong></li><li id="22bc" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="3096" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">1.超圆柱概念</h1><h2 id="1bc5" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">1.1.超圆柱表示</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/87484a49abdce3c09d89c88ddc957101.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*0NuhiNOGRxCezIdl8aAz7w.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Hypercolumn Representation</strong></figcaption></figure><ul class=""><li id="9d7b" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">此时，卷积神经网络(CNN)通常使用最后一层的输出<strong class="kh ir">作为特征表示。然而，该层中的信息<strong class="kh ir">在空间上过于粗糙，不允许精确定位</strong>。</strong></li><li id="5823" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><strong class="kh ir">一个像素的超列是该像素上方所有 CNN 单元的激活向量</strong>，如上图所示。</li><li id="19c6" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">通过这种方式，<strong class="kh ir">空间位置信息可以从更早的层</strong>中获得，并且具有更准确的预测结果。</li></ul><h2 id="7873" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">1.2.超列问题设置</h2><ul class=""><li id="62ce" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la oe mu mv mw bi translated">首先，假设在非最大抑制(NMS)之后，我们从对象检测系统获得了一组检测。</li><li id="22a9" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">然后，检测的包围盒被稍微扩展并在这个扩展的盒子上预测热图。</li><li id="546b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">对于分割，热图编码了特定位置在对象内部的概率。由于它是在检测之后被分割的，因此，我在标题处将其声明为一种<strong class="kh ir">实例分割</strong>方法。</li><li id="7af0" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">它也适用于<strong class="kh ir">零件标记/关键点预测</strong>，为每个零件/关键点预测一个单独的热图，其中每个热图是一个位置属于该零件/关键点的概率。</li><li id="e350" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">在每种情况下，预测一个<strong class="kh ir"> 50×50 的热图</strong> <strong class="kh ir">，然后调整</strong>到扩展的边界框的大小，并将其放到图像上。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="37d6" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">2.使用<strong class="ak">超柱</strong>的像素分类</h1><ul class=""><li id="21c6" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la oe mu mv mw bi translated">该位置处的超柱是一个长向量，其连接了来自网络中的一些或所有特征地图的特征。</li><li id="23ef" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">例如，使用来自<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>架构的 pool2 (256 通道)、conv4 (384 通道)和 fc7 (4096 通道)将导致 4736 维向量。</li><li id="95f9" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">位置很重要，例如:对于被检测的人，头部应该在盒子的顶部。因此，最简单的方法是为 50×50 个位置中的每个位置训练单独的分类器，即在每个位置可以使用每个超列上的 1×1 卷积或全连接(FC)层。</li><li id="5fd9" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">但是有三个问题:1。通过一个点的数据量很少，这可能会导致过度拟合。2.训练如此多的分类器是计算昂贵的。3.相邻的像素应该彼此相似。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="9729" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.<strong class="ak">高效超柱</strong></h1><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/959dcd37fb6bbdfc3c2dee63c3d48c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*KO1B4jefoEAeidxTNbxjSQ.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Efficient Hypercolumn</strong></figcaption></figure><ul class=""><li id="deba" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">一种解决方案是使用卷积和上采样(调整大小)。</li><li id="a005" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">1×1 卷积被 n×n 卷积代替。这相当于不仅查看像素正上方的单元，而且查看该单元的邻域。</li><li id="04f5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">有几段谈到了如何将超列概念转变为高效的超列实现。(如果有兴趣，请看论文。)</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/53dfb75d23ca5b10db9bf3d2b6166209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*efV26111sCLZKRRGU4-WmQ.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Hypercolumn Classifiers As a Neural Network</strong></figcaption></figure><ul class=""><li id="1c24" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">最后，超列分类器如上图所示。</li><li id="8bf6" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">在使用双线性插值进行卷积和上采样之后，来自不同层的特征图被加在一起，并经过 sigmoid 函数。</li><li id="1db6" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">然后结合热图给出最终输出。</li><li id="e3db" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">它看起来像 FCN。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="708c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">4.<strong class="ak">快速超柱(2017 TPAMI 中)</strong></h1><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/29696450d0e8c15ea5621bed0baf7ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*Vr91k6pFGaeJLbx9oxp3MQ.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Fast Hypercolumn Prediction Using the SPP Idea</strong></figcaption></figure><ul class=""><li id="2a5c" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">CNN(橙色)的卷积层只在整个图像上运行一次。因为卷积特征可以在所有盒子之间共享。</li><li id="9294" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">然后，对于每个盒子，空间金字塔池(SPP)图层使用空间金字塔格网来计算固定长度的矢量，然后将其传递给完全连接的图层(红色)。(SPP 层在<a class="ae lk" href="https://medium.com/coinmonks/review-sppnet-1st-runner-up-object-detection-2nd-runner-up-image-classification-in-ilsvrc-906da3753679" rel="noopener"> SPPNet </a>中提出。)</li><li id="c34b" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">主要的加速来自所有机器共享的卷积特性。(橙色)</li><li id="5d89" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">完全连接的图层要素仍按每个框单独计算。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="9fcd" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">5.结果</h1><h2 id="d71f" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">5.1.检测后分割</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi oi"><img src="../Images/8f6dc1ce153ff2451644d3db5ed4bffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_OhV3Bos5bCIuy7WiPAM9w.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Ablation Study on VOC2012 Val</strong></figcaption></figure><ul class=""><li id="558c" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated"><a class="ae lk" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"><strong class="kh ir">SDS</strong></a><strong class="kh ir">【22】</strong>:基线，47.7% mAP。</li><li id="021f" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><strong class="kh ir"> Hypercolumn (Hyp) </strong>:使用 10×10 网格，根据位置是否在原始候选区域内，还添加了额外的 1/0，51.2%地图。</li><li id="b73c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">用包围盒回归(<strong class="kh ir"> bbox-reg </strong>)来细化盒子:51.9%贴图。</li><li id="ab85" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">带微调(<strong class="kh ir"> FT </strong> ): 52.8% mAP，远高于<a class="ae lk" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a>。</li><li id="3ca7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">随着<strong class="kh ir">一个或两个上采样路径被丢弃</strong>，mAP 也被丢弃。</li><li id="9bb7" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">使用<strong class="kh ir">不同的网格尺寸</strong>来离散检测盒:使用 1×1 网格已经优于<a class="ae lk" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a>。使用 5×5 网格已经恢复了 10×10 网格的全部性能。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi on"><img src="../Images/42e83c5de32f468d9a919b1dfd7fb8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*rIQ0YD97EB_vAD8QHYqV6A.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Heatmap,Top Row: Baseline, Bottom Row: Hypercolumn</strong></figcaption></figure><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/7ae20daacd6d1e3fb9605639f2c23b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*sOGpQoP2LZ5TNM50cB9ojg.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">VOC2012 Val</strong></figcaption></figure><ul class=""><li id="b5ad" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">T-Net: <a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-alexnet-caffenet-winner-in-ilsvrc-2012-image-classification-b93598314160" rel="noopener"> AlexNet </a>，44.0%地图。</li><li id="cd5a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">o 网:<a class="ae lk" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>，52.6%地图。</li><li id="898a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">O-Net，Hyp: 56.5%图。</li><li id="11bc" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">O-Net，Hyp+Rescore: 60.0% mAP。(围绕 NMS 阈值和区域重叠重新计分播放)</li></ul><h2 id="b3d2" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">5.2.关键点预测</h2><ul class=""><li id="76de" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la oe mu mv mw bi translated">只有“人”的范畴。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi op"><img src="../Images/ad7064c37679c2c6da093dff53834a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YApHoVil5wO0oetjDYzfJw.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">VOC2009 Val</strong></figcaption></figure><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/584f83f81b1268fd87d6dbdc27d4a4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*4WB3e6ZsvHrhK0E70ana5Q.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Heatmap, Top Row: Baseline Only FC7, Bottom Row: Hypercolumns</strong></figcaption></figure><ul class=""><li id="c7d8" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">经过微调的超柱可获得最佳结果。</li></ul><h2 id="e6cd" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">5.3.零件标签</h2><ul class=""><li id="c020" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la oe mu mv mw bi translated">人、马、牛、羊、猫、狗和鸟。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/cccaed5d8e7bedc04f0f10a6cec3a98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*UiJhk4iCQyG3dgWdyVf9BQ.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">PASCAL VOC</strong></figcaption></figure><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi os"><img src="../Images/af152c91615b3ed683d7b39a3dc12317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQxOCE66caauzhygIprutg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Heatmap, Top Row: Baseline Only FC7, Bottom Row: Hypercolumns</strong></figcaption></figure><ul class=""><li id="e422" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">除了 bird，超圆柱获得了最好的结果。</li></ul><h2 id="c3fa" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">5.4.快速超圆柱</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/501bc93c2417f2b459af92ec4d2b26e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*TpUrAUqT-pnOe6G2aHXD3w.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Time Against Number of Boxes</strong></figcaption></figure><ul class=""><li id="c199" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated"><strong class="kh ir">慢速</strong>系统所花费的时间随着箱子数量的增加而线性增加(256 个箱子可达<strong class="kh ir"> 6 秒)。</strong></li><li id="afd6" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">无论我们在多少台机器上运行，使用 SPP 层的<strong class="kh ir"> fast </strong>系统所花费的时间保持不变。特别是，它能够在不到 250 毫秒内分割<strong class="kh ir"> 256 个盒子。</strong></li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="8fbe" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">参考</h2><p id="adc6" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ou kq kr ks ov ku kv kw ow ky kz la ij bi translated">【2015 CVPR】【超柱】<br/> <a class="ae lk" href="https://arxiv.org/abs/1411.5752" rel="noopener ugc nofollow" target="_blank">用于对象分割和细粒度定位的超柱</a></p><p id="ed8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">【2017 TPAMI】【超柱】<br/> <a class="ae lk" href="https://ieeexplore.ieee.org/document/7486965" rel="noopener ugc nofollow" target="_blank">使用超柱的对象实例分割和细粒度定位</a></p><h2 id="194b" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ou kq kr ks ov ku kv kw ow ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(没)(有)(什)(么)(好)(的)(情)(情)(感)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [ </a><a class="ae lk" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lk" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae lk" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割 <br/> </strong> <a class="ae lk" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">DeepMask </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">SharpMask </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">MultiPathNet </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC </a>】 <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">InstanceFCN </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS </a></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(T38) 人类姿势估计 (T39) <br/> [(T41) 汤普森 NIPS'14 [T42)]</p></div></div>    
</body>
</html>