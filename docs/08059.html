<html>
<head>
<title>Understanding NLP and Topic Modeling Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解 NLP 和主题建模第 1 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-nlp-and-topic-modeling-part-1-257c13e8217d?source=collection_archive---------25-----------------------#2019-11-05">https://towardsdatascience.com/understanding-nlp-and-topic-modeling-part-1-257c13e8217d?source=collection_archive---------25-----------------------#2019-11-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b307291756841f5edc72c4f16ffd88b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHX-tadiZ1yD2VajrjQT0g.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://www.pexels.com/@ivo-rainha-527110?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Ivo Rainha </a>from <a class="ae jg" href="https://www.pexels.com/photo/assorted-books-on-shelf-1290141/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><div class=""/><div class=""><h2 id="7ec2" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">我们探索如何通过自然语言处理提取主题帮助我们更好地数据科学</h2></div><p id="79b9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">自然语言处理(NLP)是数据科学的一个前沿领域。它的终端应用有很多——聊天机器人、推荐系统、搜索、虚拟助手等等。</p><p id="c3b6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，对于初露头角的数据科学家来说，至少了解 NLP 的基础知识是有益的，即使他们的职业生涯将他们带到了一个完全不同的方向。谁知道呢，通过 NLP 提取的一些主题可能会给你的下一个模型带来额外的分析提升。<strong class="la jk">今天，在这篇文章中，我们试图理解为什么主题建模是重要的，以及它如何帮助我们这些数据科学家。</strong></p><p id="b819" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">主题建模，顾名思义，就是使用一种算法来发现最能描述给定文本文档的主题或主题集。你可以把每个话题想象成一个单词或者一组单词。</strong></p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="b379" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">主题建模的目标</h1><p id="4a25" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我第一次和 NLP 一起工作时，我对自己说:</p><blockquote class="nh ni nj"><p id="c717" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">" NLP 仅仅是 EDA(探索性数据分析)的另一种形式吗？"</p></blockquote><p id="81bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是因为在那之前，我主要是带着明确的目标构建模型——用 X 来预测或解释 y。NLP 远没有那么结构化和清晰。甚至当我最终成功运行我的主题建模算法时，掉出的主题产生的问题比答案还多。在这里，看看我对 Reddit 的 NLP 分析得出的一些主题:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="66db" class="nx ml jj nt b gy ny nz l oa ob">Topic 10:<br/>book read reading history series love author first people novel world finished<br/>Topic 11:<br/>like feel something people look seem seems stuff actually right always question<br/>Topic 12:<br/>story writing write character main novel chapter short plot first scene advice<br/>Topic 13:<br/>think wait bit better people bad true might worth thinking put sell<br/>Topic 14:<br/>need financial house information relevant situation invest question money making ask consider<br/>Topic 15:<br/>car insurance loan vehicle damage accident hit month payment driver title pay</span></pre><p id="205c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有些主题有意义，有些没有意义。不管怎样，我应该对这些话题做些什么呢？</p><p id="8890" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是数据科学家的生活——通常只有在你最终清理了数据和调试了代码之后，真正的工作才开始。然后，终于，是时候去寻找那些恼人的难以捉摸的见解了。这正是自然语言处理和主题建模的要点。它本身可能不是目的，但通过 NLP 提取主题让我们更接近于生成有用的东西，就像降维技术在数据科学世界的数字方面帮助我们一样。</p><blockquote class="oc"><p id="8341" class="od oe jj bd of og oh oi oj ok ol lt dk translated">主题建模允许我们穿过噪音(处理文本数据的高维度)并识别我们的文本数据的信号(主要主题)。</p></blockquote><p id="090b" class="pw-post-body-paragraph ky kz jj la b lb om kk ld le on kn lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">有了这个经过提炼的信号，我们就可以开始真正的工作，产生真知灼见。让我们一步一步来。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="b806" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">维度的诅咒</h1><p id="845a" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在许多数据科学应用中，高维数据被视为诅咒。如果你想更详细地了解为什么，这是我以前写的一篇关于维度诅咒的文章。但是对于那些时间紧迫的人来说，这里有一个 TLDR:</p><ol class=""><li id="cf16" class="or os jj la b lb lc le lf lh ot ll ou lp ov lt ow ox oy oz bi translated">如果我们的特征比观察值多，我们就会冒模型过度拟合的风险——这通常会导致糟糕的样本外性能。</li><li id="4487" class="or os jj la b lb pa le pb lh pc ll pd lp pe lt ow ox oy oz bi translated">当我们有太多的特征时，观察变得更难聚类-信不信由你，<strong class="la jk">过多的维度会导致数据集中的每个观察看起来与所有其他观察等距。</strong>因为聚类使用一种距离度量<a class="ae jg" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">，比如欧几里德距离</a>，来量化观察值之间的相似性，这是一个大问题。如果距离都近似相等，那么所有的观察结果看起来都一样(也一样不同)，并且不能形成有意义的聚类。</li></ol><p id="d3d2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">什么时候我们最有可能遇到高维数据(也就是太多的特征)？文本数据。要了解原因，想象一下我们将如何对以下句子的数据进行编码，以便算法可以对其进行处理:</p><blockquote class="nh ni nj"><p id="5d39" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">"那个男人穿着一件有金色星星的夹克。"</p></blockquote><p id="4f19" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种自然的方法是所谓的<strong class="la jk">单词包</strong>方法— <strong class="la jk">单词包将一个给定的文档表示为一个不同单词及其频率的列表。</strong>所以上面的句子应该是这样的:</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pf"><img src="../Images/728e22d47b1076169b72ee19e66e9402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdtJAppcTU2s8qOCwwcsPA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Bag of Words Example</figcaption></figure><p id="2e93" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">因此，为了捕获给定的文档，我们需要为文档中的每个独特的单词提供一个特征。</strong>对于给定的文档，每个特征的值是该单词在文档中出现的次数(因此对于我们之前的示例，除了“a”出现两次之外，每个单词都出现一次)。</p><p id="51ce" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在想象我们的文档不是孤立的，而是一个更大的语料库的一部分。让我们首先弄清楚行话:</p><ul class=""><li id="7744" class="or os jj la b lb lc le lf lh ot ll ou lp ov lt pg ox oy oz bi translated"><strong class="la jk">文档</strong>是你定义的单个观察(也就是一袋单词)的任何东西。它可以是一句话，也可以是一整篇文章，甚至是一整本书——你如何定义它取决于你分析的目标。</li><li id="907d" class="or os jj la b lb pa le pb lh pc ll pd lp pe lt pg ox oy oz bi translated"><strong class="la jk">语料库</strong>是所有文本数据的总和——换句话说，是数据集中的所有文档。</li></ul><p id="6c04" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">如果你的语料库很大，那么你大概会有至少上万个独特的单词在里面</strong>(如果你的语料库包括很多名字的话会更多)。仅仅是试图想象那袋单词就让我头疼。试图在它上面运行算法将会非常慢，并且可能没有帮助——很可能你会有更多的特征(独特的单词)而不是观察(文档)。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="bda1" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">NLP 来救援了</h1><p id="c84b" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">现在，让我们用一个实际的例子来看看 NLP 如何帮助筛选维度来揭示信号。假设我们想向一个朋友推荐几本书。我们将如何着手做那件事？</p><p id="1d04" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种方法是问:</p><blockquote class="nh ni nj"><p id="113d" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">“嘿，说出几本你最近读过的、你真正喜欢的书。”</p></blockquote><p id="ee5e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后根据回复，推荐几本我们最喜欢的、与他或她列出的书最相似的书。我们刚刚描述了一个简单的推荐系统。</p><p id="5cc1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了在算法上做我们刚刚描述的事情，<strong class="la jk">我们需要能够弄清楚如何衡量两本书是否相似。</strong>我们可以把这两本书描述成一袋袋的单词，并尝试使用像<a class="ae jg" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>这样的距离度量来比较它们，但这真的有帮助吗？答案是否定的，原因有几个:</p><p id="c546" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一个原因是<strong class="la jk">停用词</strong>(真正常见的词有“The”、“a”、“it”、“and”等。)—这些单词在几乎所有的文档中出现得非常频繁，它们会给我们的相似度分数注入许多无意义的噪音(知道两本书都包含许多单词“the”的实例根本没有帮助)。</p><p id="c279" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">而且即使我们去掉了所有的停用词，</strong><a class="ae jg" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-50dc6e49aa1e"><strong class="la jk"/></a><strong class="la jk">的维度诅咒仍然影响着我们。一本书里有如此多不同的单词，其中许多与书的实际主题毫无关联。因此，我们的相似性度量很有可能锁定这些干扰词中的一个——这基本上是<strong class="la jk">伪相关的文本版本。例如，我们可以有一本关于消防员的书和一本关于钓鲑鱼的书，但将它们列为高度相似，因为我们的算法注意到“杆”和“引擎”这两个词在这两本书中都频繁出现。这是一个偶然的和无意义的相似性，如果我们采取行动，这将是有问题的。</strong></strong></p><h2 id="b529" class="nx ml jj bd mm ph pi dn mq pj pk dp mu lh pl pm mw ll pn po my lp pp pq na pr bi translated">主题建模</h2><p id="0d18" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">这就是主题建模的用武之地。<strong class="la jk">主题建模是使用定量算法梳理出文本主体所涉及的关键主题的实践。</strong>它与<a class="ae jg" rel="noopener" target="_blank" href="/understanding-pca-fae3e243731d">PCA 之类的东西有很多相似之处，它识别你的特征中的关键定量趋势(解释最大的差异)。</a>PCA 的输出是总结我们特征的一种方式——例如，它允许我们从大约 500 个特征到 10 个总结特征。这 10 个概要特征基本上是主题。</p><p id="e1c3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在 NLP 中，它的工作方式几乎完全相同。<strong class="la jk">我们想要将我们的全部书籍和它的 100，000 个特征(不同的单词)提取成 7 个主题</strong>(我任意选择了 7 个主题)。<strong class="la jk">一旦我们知道了主题以及它们由什么组成，我们就可以将语料库中的每本书从嘈杂的单词包转换成一个干净的主题加载组合:</strong></p><figure class="no np nq nr gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ps"><img src="../Images/6092828677ad86425e42127f959e3d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8xw10K_kdVa8w6Kh9-F-w.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">We want to express each book in our corpus as a portfolio of topics</figcaption></figure><blockquote class="oc"><p id="923b" class="od oe jj bd of og pt pu pv pw px lt dk translated">现在我们开始做生意了。使用每本书的主题负载计算的相似性分数比使用原始单词包计算的分数有用得多，因为虚假的相似性现在不太可能了。</p></blockquote><p id="0c94" class="pw-post-body-paragraph ky kz jj la b lb om kk ld le on kn lg lh oo lj lk ll op ln lo lp oq lr ls lt im bi translated">甚至一本书的<strong class="la jk">描述性统计在“主题空间”中也比在“词汇袋空间”</strong>中更有意义——我们现在可以说一本书在数据科学主题上负载很重，而不是去纠结为什么在我们的词汇袋中最频繁出现的两个词是“森林”和“随机”。</p><h2 id="4a4a" class="nx ml jj bd mm ph pi dn mq pj pk dp mu lh pl pm mw ll pn po my lp pp pq na pr bi translated">思维练习</h2><p id="3624" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在前面的例子中，我们决定将我们的书表达为 7 个主题的负载。但是我们可以选择任何数量的主题(选择主题数量更多的是艺术而不是科学，并且很大程度上取决于你的数据——因此充分了解你的数据是至关重要的)。10 也行，100 也行。但是想想<strong class="la jk">随着我们不断增加主题的数量，每个主题变得越来越细，算法开始失去看到全局的能力，会发生什么。</strong></p><p id="6e4a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">例如，假设我们有三本书——第一本是法国旅游指南，第二本是中国旅游指南，第三本是中国城市化的经济史。在我们的 7 主题 NLP 模型中，我们将第 1 本书和第 2 本书归类为旅游书籍(并将其评分为彼此相似)，将第 3 本书归类为商务书籍(并将其评分为与其他书籍不相似)。</p><p id="c2b4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有 5000 个主题，我们可以将第一本书归类为“骑行乡村法国”，第二本书归类为“旅游城市中国”，第三本书归类为“历史城市中国”。现在还不太清楚我们该如何给它们打分——算法可能只是举手表决，将所有 3 本书评为相同/不同。这不一定是错误的(取决于应用程序)<strong class="la jk">但它确实显示了高维数据(5000 个主题肯定是)会以意想不到的方式引入干扰，扭曲我们的分析。</strong></p><blockquote class="oc"><p id="4cce" class="od oe jj bd of og oh oi oj ok ol lt dk translated">主题建模的一般经验法则是，只要你的最终应用程序要求你做的那样具体，不要再多了。</p></blockquote></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="14f1" class="mk ml jj bd mm mn mo mp mq mr ms mt mu kp mv kq mw ks mx kt my kv mz kw na nb bi translated">下次</h1><p id="67a9" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我意识到，到目前为止，我对我们实际上是如何提出我们的主题一直含糊其辞。那是因为我想充分探索为什么 NLP 很重要。下一次，我将(用 Python 代码)介绍两种主题建模算法— <strong class="la jk"> LDA(潜在狄利克雷分配)</strong>和<strong class="la jk"> NMF(非负矩阵分解)</strong>。</p><p id="c276" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在那之前，感谢阅读，干杯！</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="4bee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> <em class="nk">更多数据科学与分析相关帖子由我:</em> </strong></p><p id="83f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/got-data-science-jobs-552e39d48da2">T5【维度之祸】T6</a></p><p id="10de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/understanding-pca-fae3e243731d?source=post_page---------------------------"> <em class="nk">了解 PCA </em> </a></p><p id="f272" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/business-strategy-for-data-scientists-25e3ca0af5ee"> <em class="nk">数据科学家的商业战略</em> </a></p><p id="1925" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/business-simulations-with-python-a70d6cba92c8"> <em class="nk">用 Python 进行业务模拟</em> </a></p><p id="b2be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/understanding-bayes-theorem-7e31b8434d4b"> <em class="nk">理解贝叶斯定理</em> </a></p><p id="1688" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/understanding-the-naive-bayes-classifier-16b6ee03ff7b"> <em class="nk">理解朴素贝叶斯分类器</em> </a></p><p id="11dd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/fun-with-the-binomial-distribution-96a5ecabf65b"> <em class="nk">二项分布</em> </a></p></div></div>    
</body>
</html>