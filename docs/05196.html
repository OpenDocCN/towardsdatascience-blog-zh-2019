<html>
<head>
<title>Linear Discriminant Analysis In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的线性判别分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2?source=collection_archive---------0-----------------------#2019-08-04">https://towardsdatascience.com/linear-discriminant-analysis-in-python-76b8b17817c2?source=collection_archive---------0-----------------------#2019-08-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9143d44be5b15a97eb7d3c6a5eb76c60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O0RCJa1Sr6GugLOS"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/woman-in-red-long-sleeve-writing-on-chalk-board-3769714/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/woman-in-red-long-sleeve-writing-on-chalk-board-3769714/</a></figcaption></figure><div class=""/><p id="21b4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性判别分析(LDA)是一种降维技术。顾名思义，降维技术减少了数据集中的维数(即变量)，同时保留了尽可能多的信息。例如，假设我们绘制了两个变量之间的关系，其中每种颜色代表一个不同的类。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi le"><img src="../Images/97f584cffd6ab003ede965ac82691ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o2TKovc_lkJ9_ISxZxrbog.png"/></div></div></figure><p id="8e56" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想把维数减少到 1，一种方法是把所有东西都投影到 x 轴上。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lj"><img src="../Images/98b832c782cbe0728ead2202aef67b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lugB_AavKEr3ghDGC6hOA.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/448b1a33c4146435035bb974aa482d56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*Z202fIHoHkW5KhxxXcQ8jA.png"/></div></figure><p id="c4a1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这很糟糕，因为它忽略了第二个特性提供的任何有用信息。另一方面，线性判别分析(LDA)使用来自两个特征的信息来创建新的轴，并将数据投影到新的轴上，从而最小化方差并最大化两个类的平均值之间的距离。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ll"><img src="../Images/aa2c6d9a76164c63030f1d110c124beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fz3JQ80No5Nnbap28EGRTg.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lm"><img src="../Images/f47fd7e0df9a799e33424fec0866d520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lhckC2RQzq28zNL7WtU5A.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ln"><img src="../Images/9d85d1b38436b515215eb036f4cb2c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W48aQ0LkZ5dm1_uow6FD2w.png"/></div></div></figure><h1 id="2c44" class="lo lp jj bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">密码</h1><p id="239a" class="pw-post-body-paragraph kg kh jj ki b kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">让我们看看如何使用 Python 从头开始实现线性判别分析。首先，导入以下库。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="c28f" class="mw lp jj ms b gy mx my l mz na">from sklearn.datasets import load_wine<br/>import pandas as pd<br/>import numpy as np<br/>np.set_printoptions(precision=4)<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>sns.set()<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix</span></pre><p id="af32" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在接下来的教程中，我们将使用可以从 UCI 机器学习知识库中获得的葡萄酒数据集。幸运的是，<code class="fe nb nc nd ms b">scitkit-learn</code>库提供了一个包装器函数，用于下载和</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="b5bf" class="mw lp jj ms b gy mx my l mz na">wine = load_wine()</span><span id="c3ca" class="mw lp jj ms b gy ne my l mz na">X = pd.DataFrame(wine.data, columns=wine.feature_names)<br/>y = pd.Categorical.from_codes(wine.target, wine.target_names)</span></pre><p id="ffce" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集包含 178 行，每行 13 列。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="ce1f" class="mw lp jj ms b gy mx my l mz na">X.shape</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/28bbfa59f25a8ff56076a49555219e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:198/format:webp/1*jXW8TKVRHfLOPQhTY29EaA.png"/></div></figure><p id="1166" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">特征由各种特征组成，例如酒的镁和酒精含量。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="764d" class="mw lp jj ms b gy mx my l mz na">X.head()</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/9a06441b6fbcdc1589bde7f8f2ea26aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rKQO0jUeltnuv8lQdc9uaw.png"/></div></div></figure><p id="5d37" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有三种不同的酒。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="6117" class="mw lp jj ms b gy mx my l mz na">wine.target_names</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/955614b13e37fbc63269a79d68445820.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*Hwv1Dlc2mMIESnso3NsOMw.png"/></div></figure><p id="b115" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们创建一个包含特性和类的数据框架。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="1a85" class="mw lp jj ms b gy mx my l mz na">df = X.join(pd.Series(y, name='class'))</span></pre><p id="f087" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">线性判别分析</strong>可以分解为以下步骤:</p><ol class=""><li id="b64b" class="ni nj jj ki b kj kk kn ko kr nk kv nl kz nm ld nn no np nq bi translated">计算类内和类间散布矩阵</li><li id="3416" class="ni nj jj ki b kj nr kn ns kr nt kv nu kz nv ld nn no np nq bi translated">计算散布矩阵的特征向量和相应的特征值</li><li id="c670" class="ni nj jj ki b kj nr kn ns kr nt kv nu kz nv ld nn no np nq bi translated">对特征值排序，选择最上面的<strong class="ki jk"> <em class="nw"> k </em> </strong></li><li id="d52f" class="ni nj jj ki b kj nr kn ns kr nt kv nu kz nv ld nn no np nq bi translated">创建一个包含映射到<strong class="ki jk"> <em class="nw"> k </em> </strong>特征值的特征向量的新矩阵</li><li id="d5d5" class="ni nj jj ki b kj nr kn ns kr nt kv nu kz nv ld nn no np nq bi translated">通过对步骤 4 中的数据和矩阵进行点积，获得新的特征(即 LDA 成分)</li></ol><h2 id="40ed" class="mw lp jj bd lq nx ny dn lu nz oa dp ly kr ob oc mc kv od oe mg kz of og mk oh bi translated">类内散布矩阵</h2><p id="082d" class="pw-post-body-paragraph kg kh jj ki b kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">我们使用以下公式计算类别散布矩阵中的<em class="nw">。</em></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7641efcf1840d6091e72ffc977903dbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*UDfb4tlOc7dLwSxYjECC7g.png"/></div></figure><p id="934a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="nw"> c </em> </strong>是不同类的总数，并且</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/d8cfad06d9e90ea4fd45d0267c9c7222.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*Y9Lgpn-LDXpEonGEkMNg3g.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/d2d24c2ed32d318de3acc680db36259b.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*b378Tkt4ECTTpqQvMeQYjg.png"/></div></div></figure><p id="baba" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="nw"> x </em> </strong>为样本(即行)<strong class="ki jk"> <em class="nw"> n </em> </strong>为给定类别的样本总数。</p><p id="8156" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个类，我们用每个特征的平均值创建一个向量。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="cd35" class="mw lp jj ms b gy mx my l mz na">class_feature_means = pd.DataFrame(columns=wine.target_names)</span><span id="533a" class="mw lp jj ms b gy ne my l mz na">for c, rows in df.groupby('class'):<br/>    class_feature_means[c] = rows.mean()</span><span id="7362" class="mw lp jj ms b gy ne my l mz na">class_feature_means</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ab016cdc8cb7fbbc1fa75bbff0c3e18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*Txft7wejaEzQrBBxqbXESQ.png"/></div></figure><p id="c60f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们将均值向量(<strong class="ki jk"> <em class="nw"> mi </em> </strong>)代入之前的等式，以获得类内散布矩阵。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="4e19" class="mw lp jj ms b gy mx my l mz na">within_class_scatter_matrix = np.zeros((13,13))</span><span id="ac4d" class="mw lp jj ms b gy ne my l mz na">for c, rows in df.groupby('class'):</span><span id="35b8" class="mw lp jj ms b gy ne my l mz na">rows = rows.drop(['class'], axis=1)<br/>    <br/>    s = np.zeros((13,13))</span><span id="2fe7" class="mw lp jj ms b gy ne my l mz na">for index, row in rows.iterrows():<br/>        x, mc = row.values.reshape(13,1), class_feature_means[c].values.reshape(13,1)<br/>        <br/>        s += (x - mc).dot((x - mc).T)<br/>    <br/>    within_class_scatter_matrix += s</span></pre><h2 id="0a9f" class="mw lp jj bd lq nx ny dn lu nz oa dp ly kr ob oc mc kv od oe mg kz of og mk oh bi translated">类间散布矩阵</h2><p id="0ce1" class="pw-post-body-paragraph kg kh jj ki b kj mm kl km kn mn kp kq kr mo kt ku kv mp kx ky kz mq lb lc ld im bi translated">接下来，我们使用下面的公式计算类间散布矩阵的<em class="nw">。</em></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi om"><img src="../Images/bf40df576a4922c723fe1c998c5e614f.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*zytvTtJbhZw320Qx9OOBQw.png"/></div></figure><p id="de3d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在哪里</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/d2d24c2ed32d318de3acc680db36259b.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*b378Tkt4ECTTpqQvMeQYjg.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/82601db78a1e05a722961ea1fb07d447.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*--DhGe1_VLzeH4ePFCLd0w.png"/></div></figure><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="81b4" class="mw lp jj ms b gy mx my l mz na">feature_means = df.mean()</span><span id="254a" class="mw lp jj ms b gy ne my l mz na">between_class_scatter_matrix = np.zeros((13,13))</span><span id="362d" class="mw lp jj ms b gy ne my l mz na">for c in class_feature_means:    <br/>    n = len(df.loc[df['class'] == c].index)<br/>    <br/>    mc, m = class_feature_means[c].values.reshape(13,1), feature_means.values.reshape(13,1)<br/>    <br/>    between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)</span></pre><p id="51dd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们求解广义特征值问题</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1348d6096b926f952854a14cd1001c03.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*MnuFuRp7gVPiM1-5nC6pEg.png"/></div></figure><p id="f058" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以获得线性判别式。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="89d3" class="mw lp jj ms b gy mx my l mz na">eigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))</span></pre><p id="1e09" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有最高特征值的特征向量携带关于数据分布的最多信息。于是，我们将特征值从最高到最低排序，选择第一个<strong class="ki jk"> <em class="nw"> k </em> </strong>特征向量。为了保证排序后特征值映射到同一个特征向量，我们把它们放在一个临时数组中。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="2199" class="mw lp jj ms b gy mx my l mz na">pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]</span><span id="19af" class="mw lp jj ms b gy ne my l mz na">pairs = sorted(pairs, key=lambda x: x[0], reverse=True)</span><span id="a716" class="mw lp jj ms b gy ne my l mz na">for pair in pairs:<br/>    print(pair[0])</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6f9f3fd25fe56aa8ae19432f068929a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*1pA2mBwV0LcHCtsHCBjJyQ.png"/></div></figure><p id="a7ef" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只看这些值，很难确定每个组件解释了多少差异。因此，我们用百分比来表示。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="8631" class="mw lp jj ms b gy mx my l mz na">eigen_value_sums = sum(eigen_values)</span><span id="bb6b" class="mw lp jj ms b gy ne my l mz na">print('Explained Variance')<br/>for i, pair in enumerate(pairs):<br/>    print('Eigenvector {}: {}'.format(i, (pair[0]/eigen_value_sums).real))</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/8841552c5a7f2131f05c440050a5e0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8OC1TMFtpaIvINcQ2YjwQA.png"/></div></div></figure><p id="8ea1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们用前两个特征向量创建一个矩阵<strong class="ki jk"> <em class="nw"> W </em> </strong>。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="a6cd" class="mw lp jj ms b gy mx my l mz na">w_matrix = np.hstack((pairs[0][1].reshape(13,1), pairs[1][1].reshape(13,1))).real</span></pre><p id="e4aa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们把<strong class="ki jk"> <em class="nw"> X </em> </strong>和<strong class="ki jk"> <em class="nw"> W </em> </strong>的点积保存成一个新的矩阵<strong class="ki jk"> <em class="nw"> Y </em> </strong>。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0f979f87458e917b221b8a9dc8bdef09.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*2h5xd7oKKVfirf-mwsgcpg.png"/></div></figure><p id="1c0a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="ki jk"> <em class="nw"> X </em> </strong>是一个<strong class="ki jk"> <em class="nw"> n×d </em> </strong>矩阵，具有<strong class="ki jk"> <em class="nw"> n </em> </strong>样本和<strong class="ki jk"> <em class="nw"> d </em> </strong>维度，<strong class="ki jk"> <em class="nw"> Y </em> </strong>是一个<strong class="ki jk"> <em class="nw"> n×k </em> </strong>矩阵，具有<strong class="ki jk"> <em class="nw"> n </em> </strong>样本和换句话说，<strong class="ki jk"><em class="nw"/></strong>是由 LDA 组件组成的，或者换一种说法，新的特征空间。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="b22d" class="mw lp jj ms b gy mx my l mz na">X_lda = np.array(X.dot(w_matrix))</span></pre><p id="b539" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nb nc nd ms b">matplotlib</code>不能直接处理分类变量。因此，我们将每个类别编码为一个数字，这样我们就可以将类别标签合并到我们的绘图中。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="94dc" class="mw lp jj ms b gy mx my l mz na">le = LabelEncoder()</span><span id="d8c9" class="mw lp jj ms b gy ne my l mz na">y = le.fit_transform(df['class'])</span></pre><p id="2466" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们将数据绘制为两个 LDA 分量的函数，并为每个类别使用不同的颜色。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="7f8f" class="mw lp jj ms b gy mx my l mz na">plt.xlabel('LD1')<br/>plt.ylabel('LD2')</span><span id="27a7" class="mw lp jj ms b gy ne my l mz na">plt.scatter(<br/>    X_lda[:,0],<br/>    X_lda[:,1],<br/>    c=y,<br/>    cmap='rainbow',<br/>    alpha=0.7,<br/>    edgecolors='b'<br/>)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/f2affc929d43e737e02dc7600a6c0750.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*a8-OMinND-8apNLRfssm1A.png"/></div></figure><p id="f9f7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用由<code class="fe nb nc nd ms b">scikit-learn</code>库提供的预定义的<code class="fe nb nc nd ms b">LinearDiscriminantAnalysis </code>类，而不是每次都从头开始实现线性判别分析算法。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="aa6e" class="mw lp jj ms b gy mx my l mz na">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span><span id="79dc" class="mw lp jj ms b gy ne my l mz na">lda = LinearDiscriminantAnalysis()</span><span id="aeb5" class="mw lp jj ms b gy ne my l mz na">X_lda = lda.fit_transform(X, y)</span></pre><p id="dd86" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以访问下面的属性来获得每个组件解释的方差。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="0c7a" class="mw lp jj ms b gy mx my l mz na">lda.explained_variance_ratio_</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/7e664f0b9c9d3b24734c6e7867f519b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*inEjLGSVwtzA9zO-0krJZA.png"/></div></figure><p id="3d43" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与之前一样，我们绘制了两个 LDA 组件。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="106c" class="mw lp jj ms b gy mx my l mz na">plt.xlabel('LD1')<br/>plt.ylabel('LD2')</span><span id="41e4" class="mw lp jj ms b gy ne my l mz na">plt.scatter(<br/>    X_lda[:,0],<br/>    X_lda[:,1],<br/>    c=y,<br/>    cmap='rainbow',<br/>    alpha=0.7,<br/>    edgecolors='b'<br/>)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5f352f5245c24150bca42212db6cfc5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*BUmzM1DL23uqqSn5Vzsuhw.png"/></div></figure><p id="ad6a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们来看看 LDA 与主成分分析或 PCA 相比如何。我们首先创建并装配一个<code class="fe nb nc nd ms b">PCA</code>类的实例。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="366c" class="mw lp jj ms b gy mx my l mz na">from sklearn.decomposition import PCA</span><span id="faec" class="mw lp jj ms b gy ne my l mz na">pca = PCA(n_components=2)</span><span id="1aa7" class="mw lp jj ms b gy ne my l mz na">X_pca = pca.fit_transform(X, y)</span></pre><p id="f681" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以访问<code class="fe nb nc nd ms b">explained_variance_ratio_</code>属性来查看每个组件解释的方差的百分比。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="d529" class="mw lp jj ms b gy mx my l mz na">pca.explained_variance_ratio_</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/83ec070221e050fdfb2ab07267df71dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*CyLt-NH0XZteu2FNxQDvuQ.png"/></div></figure><p id="917d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，PCA 选择了导致最高分布(保留最多信息)的成分，而不一定是最大化类间分离的成分。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="cef3" class="mw lp jj ms b gy mx my l mz na">plt.xlabel('PC1')<br/>plt.ylabel('PC2')</span><span id="8119" class="mw lp jj ms b gy ne my l mz na">plt.scatter(<br/>    X_pca[:,0],<br/>    X_pca[:,1],<br/>    c=y,<br/>    cmap='rainbow',<br/>    alpha=0.7,<br/>    edgecolors='b'<br/>)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/208beed509e4c234f6ab12deb4a97644.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*vHNcJsdkBwT-Nz_BolZgZg.png"/></div></figure><p id="025d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们看看是否可以创建一个模型，使用 LDA 组件作为特征对进行分类。首先，我们将数据分成训练集和测试集。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="118d" class="mw lp jj ms b gy mx my l mz na">X_train, X_test, y_train, y_test = train_test_split(X_lda, y, random_state=1)</span></pre><p id="d247" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们建立并训练一个决策树。在预测了测试集中每个样本的类别后，我们创建一个混淆矩阵来评估模型的性能。</p><pre class="lf lg lh li gt mr ms mt mu aw mv bi"><span id="458f" class="mw lp jj ms b gy mx my l mz na">dt = DecisionTreeClassifier()</span><span id="7f43" class="mw lp jj ms b gy ne my l mz na">dt.fit(X_train, y_train)</span><span id="9230" class="mw lp jj ms b gy ne my l mz na">y_pred = dt.predict(X_test)</span><span id="32a6" class="mw lp jj ms b gy ne my l mz na">confusion_matrix(y_test, y_pred)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/15468ac6d5648c46cd129c9372324454.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*Gg3IkVAqPvkyKvyDzOlGVg.png"/></div></figure><p id="b014" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，决策树分类器正确地对测试集中的所有内容进行了分类。</p></div></div>    
</body>
</html>