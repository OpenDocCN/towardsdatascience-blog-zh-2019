<html>
<head>
<title>Fine-tune Albert with Pre-training on Custom Corpus</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自定义语料库上的预训练来微调 Albert</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fine-tune-albert-with-pre-training-on-custom-corpus-f56ea3cfdc82?source=collection_archive---------10-----------------------#2019-12-29">https://towardsdatascience.com/fine-tune-albert-with-pre-training-on-custom-corpus-f56ea3cfdc82?source=collection_archive---------10-----------------------#2019-12-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/5320dc624404db02cc53b8f8a68a2305.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ylpIWkcQHnLGEv0AEqsHrA.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by Esperanza Zhang on Unsplash</figcaption></figure><div class=""/><div class=""><h2 id="bddd" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">使用特定领域文本的自定义语料库对 Albert 进行预训练，并针对应用任务进一步微调预训练模型</h2></div><h1 id="aee8" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">介绍</h1><p id="9e58" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">这篇<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus" rel="noopener ugc nofollow" target="_blank">帖子</a>展示了在自定义语料库上预训练最先进的 Albert[1] NLP 模型，并在特定的下游任务上进一步微调预训练的 Albert 模型的简单步骤。自定义语料库可以是特定领域或外语等，我们没有现有的预训练阿尔伯特模型。下游任务是应用驱动的，它可以是文档分类问题(例如，情感分析)，或者标记问题(例如，NER)等。这篇文章展示了一个自定义实体识别下游任务。本文中的示例用例是提取餐馆评论中的菜名，例如，在评论“我非常喜欢马拉汽船！”中，将“马拉汽船”标记为菜名</p><p id="1ef8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">然而，艾伯特模型的细节和它是如何工作的并没有在这篇文章中讨论。这篇文章假设你对变形金刚(<a class="ae mi" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">)和阿尔伯特模型有粗略的了解，并且能够从 git repos 克隆和运行脚本。</a></p><p id="0635" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">我的包含插图笔记本的 Github repo 可以在这里找到:<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus" rel="noopener ugc nofollow" target="_blank">https://Github . com/LydiaXiaohongLi/Albert _ fine tune _ with _ pre train _ on _ Custom _ Corpus</a>。我在同一个 repo 中包含了一个<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/Albert_Finetune_with_Pretrain_on_Custom_Corpus_ToyModel.ipynb" rel="noopener ugc nofollow" target="_blank"> colab 笔记本</a>，它演示了玩具数据集的 E2E 步骤，包括构建 vocab、预训练 Albert 和微调 Albert，由于简单的数据和训练所需的较少步骤，它适合 colab 的 CPU。</p><h2 id="a772" class="mo kv jf bd kw mp mq dn la mr ms dp le lv mt mu lg lz mv mw li md mx my lk mz bi translated">玩具数据集</h2><p id="8917" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">本帖中使用的玩具数据集包括:</p><ol class=""><li id="e5d7" class="na nb jf lo b lp mj ls mk lv nc lz nd md ne mh nf ng nh ni bi translated"><a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/data_toy/restaurant_review_nopunct.txt" rel="noopener ugc nofollow" target="_blank">餐厅评论语料库</a>:在这个玩具例子中由两个评论句子组成，用于预训练艾伯特模型。在实际应用中，一般我们可以用 100M+的句子进行训练。</li></ol><figure class="nk nl nm nn gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/435b83b1b660c484eb024739f7105975.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*wKXFEY6CLecUrPZtu7I9-A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Restaurant Review Corpus</figcaption></figure><p id="47c8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">2.从餐馆评论中提取菜名以微调 Albert 进行菜名识别。<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/data_toy/restaurant_review_train" rel="noopener ugc nofollow" target="_blank">训练集</a>由两个相同的复习句子组成，提取菜名。</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ff292d139b66cfadce57fb9dd7f93e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*ikUYUwhgra9V2sLI0JumOA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Dish Name Extraction — Train Set</figcaption></figure><p id="be58" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/data_toy/dish_name_val.csv" rel="noopener ugc nofollow" target="_blank">评估集</a>由两个相似的评论句子组成，但是两个评论中的菜肴交换了，并且文本上下文略有不同，包含未收录的单词(不在餐馆评论语料库中的单词)，以验证 Albert 模型的效率。</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e2afb2a5587d43a27588da9c992c7611.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*2hfEIFmNEWSOUzi6Rkpx1w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Evaluation Set</figcaption></figure><h1 id="395a" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">系统模型化</h1><h2 id="4ba3" class="mo kv jf bd kw mp mq dn la mr ms dp le lv mt mu lg lz mv mw li md mx my lk mz bi translated">构建 Vocab</h2><p id="305f" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">第一步是为餐馆评论语料库建立词汇。</p><p id="ed30" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">Google 并没有开源 WordPiece unsupervised tokenizer，但是，我们可以在开源的 SentencePiece 或 t2t text encoder 子词生成器上进行修改，以生成与 Bert/Albert 模型兼容的词汇。</p><p id="ea1c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">修改 t2t 文本编码器子字生成器的开源实现可以在 https://github.com/kwonmha/bert-vocab-builder<a class="ae mi" href="https://github.com/kwonmha/bert-vocab-builder" rel="noopener ugc nofollow" target="_blank"/>【2】找到</p><ol class=""><li id="8dac" class="na nb jf lo b lp mj ls mk lv nc lz nd md ne mh nf ng nh ni bi translated">克隆 repo(创建一个环境并相应地安装需求)</li><li id="1c4b" class="na nb jf lo b lp nq ls nr lv ns lz nt md nu mh nf ng nh ni bi translated">准备语料库文件，在本例中是餐馆评论语料库</li><li id="ebc7" class="na nb jf lo b lp nq ls nr lv ns lz nt md nu mh nf ng nh ni bi translated">运行下面的命令来创建 Albert 兼容的词汇文件</li></ol><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="79d2" class="mo kv jf nw b gy oa ob l oc od">python subword_builder.py --corpus_filepattern “{corpus_for_vocab}” --output_filename {name_of_vocab} --min_count {minimum_subtoken_counts}</span></pre><h2 id="7c07" class="mo kv jf bd kw mp mq dn la mr ms dp le lv mt mu lg lz mv mw li md mx my lk mz bi translated">预先训练艾伯特</h2><p id="3d87" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">下一步是用自定义语料库对 Albert 进行预训练。官方阿尔伯特回购可以在<a class="ae mi" href="https://github.com/google-research/ALBERT" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/ALBERT</a>找到</p><p id="c4ed" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">克隆回购</em> </strong></p><p id="8985" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">创建一个环境并相应地安装需求。</p><p id="0a20" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">创建艾伯特预培训文件</em> </strong></p><p id="ddf2" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">使用在构建 Vocab 步骤中使用/创建的自定义语料库和词汇文件运行以下命令。该步骤首先基于输入的词汇文件对语料库进行标记和编码。然后，它为 Albert 创建训练输入文件，以在掩蔽语言模型和句子顺序预测任务上进行训练。</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="a642" class="mo kv jf nw b gy oa ob l oc od">python create_pretraining_data.py --input_file “{corpus}” --output_file {output_file} --vocab_file {vocab_file} --max_seq_length=64</span></pre><p id="5077" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">预训练艾伯特模型</em> </strong></p><ul class=""><li id="4267" class="na nb jf lo b lp mj ls mk lv nc lz nd md ne mh of ng nh ni bi translated">准备<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/models_toy/albert_config.json" rel="noopener ugc nofollow" target="_blank"> Albert 配置文件</a>，Albert 配置文件中的 vocab_size 应该与构建 vocab 步骤中创建的<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/models_toy/vocab.txt" rel="noopener ugc nofollow" target="_blank"> vocab 文件</a>中的词汇总数相同。另一个示例 Albert 配置文件可以在 https://tfhub.dev/google/albert_base/2 的<a class="ae mi" href="https://tfhub.dev/google/albert_base/2" rel="noopener ugc nofollow" target="_blank">找到，这是用于构建 Albert base v2 模型的配置文件。</a></li><li id="60cc" class="na nb jf lo b lp nq ls nr lv ns lz nt md nu mh of ng nh ni bi translated">运行以下命令开始预训练。所有的模型参数将被存储在 output_dir 中。(下面的玩具模型仅运行 300 步的预训练，批量大小为 2)</li></ul><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="1a8d" class="mo kv jf nw b gy oa ob l oc od">python run_pretraining.py --input_file={albert_pretrain_file} --output_dir={models_dir} --albert_config_file={albert_config_file} -- train_batch_size=2 --num_warmup_steps=100 --num_train_steps=300 --max_seq_length=64</span></pre><p id="cf5c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">预训练中的 max_seq_length 应与创建预训练数据步骤相同，以便 tensorflow TFRecord 正确解析数据。</p><p id="90ca" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">如果您的数据很大，并且存储在 GCS 桶中，您可以相应地添加 TPU 相关参数，如下所示。</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="a270" class="mo kv jf nw b gy oa ob l oc od">python run_pretraining.py — input_file=”gs://{bucket_name}/{folder_names}/{albert_pretrain_file_name}” — output_dir=”gs://{bucket_name}/{folder_names}” — albert_config_file=”gs://{bucket_name}/{folder_names}/albert_config_file” — use_tpu=True — tpu_name={tpu_name} — tpu_zone={tpu_zones} — num_tpu_cores=8</span></pre><p id="cae8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">对于实际应用，我们应该有更大的训练批量、训练步数和预热步数。因此，最初的 Albert 论文使用 4096 批量和 125k 训练步数，预热步数为 3125。在这里，我们可以使用谷歌的免费 300 美元启动一个 TPU(v2–8)进行训练，如果只使用一个 v2–8 TPU，你应该用更小的批量(例如 512)和更多的训练步骤进行训练，否则可能会遇到 OOM 问题。</p><p id="7c04" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">或者，在样本 E2E <a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/Albert_Finetune_with_Pretrain_on_Custom_Corpus_ToyModel.ipynb" rel="noopener ugc nofollow" target="_blank"> colab 笔记本</a>中，我用 pytorch 重写了训练前的步骤。</p><h2 id="58df" class="mo kv jf bd kw mp mq dn la mr ms dp le lv mt mu lg lz mv mw li md mx my lk mz bi translated">微调艾伯特</h2><p id="4c11" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">最后一步是微调预训练的 Albert 来执行菜名识别任务。参考<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/albert_finetuning_toy.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>了解该步骤。这篇文章中的代码片段并没有连接到完整的笔记本，它们只是为了说明的目的。如果不清楚，请阅读完整的笔记本。</p><p id="cf80" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">使用 huggingface 的变形金刚实现的 pytorch 版本的微调步骤，可以在同一个 repo 中找到，参考这个<a class="ae mi" href="https://github.com/LydiaXiaohongLi/Albert_Finetune_with_Pretrain_on_Custom_Corpus/blob/master/albert_finetuning_toy_pytorch_multiGPU.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><p id="a676" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">流程输入</em> </strong></p><p id="6863" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了获得统一长度的输入序列，我们设置了 50 个标记的最大长度，如果超过，则截断该序列，否则填充该序列。为此，我们可以使用 Keras 的 pad_sequences。</p><p id="8833" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">Albert models 采用 input _ ids[batch _ size * max _ seq _ len]—标记化和编码的文本序列(在这种情况下，input _ mask[batch _ size * max _ seq _ len]—如果标记是根据原始序列编码的，则给出 1；如果填充，则仅根据原始序列的标记计算损失；segment _ ids[batch _ size * max _ seq _ len]—零张量数组(在这种情况下不使用，labels[batch _ size * max _ seq _ len]—如果标记是菜肴名称 0 的一部分，则给出 1；否则。我们将以这种格式处理输入。</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="8c77" class="mo kv jf nw b gy oa ob l oc od">df_data[‘text_tokens’] = df_data.text.apply(tokenizer.tokenize)<br/> df_data[‘text_labels’] = df_data.apply(lambda row: label_sent(row[‘name’].lower().split()<br/> , row[‘text_tokens’]), axis=1)<br/> df_data_sampled = df_data[[np.sum(label)&gt;0 for label in df_data.text_labels]]<br/> input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in df_data_sampled[‘text_tokens’]],<br/> maxlen=MAX_LEN, dtype=”long”, truncating=”post”, padding=”post”)<br/> labels = pad_sequences(df_data_sampled[‘text_labels’],<br/> maxlen=MAX_LEN, padding=”post”,<br/> dtype=”long”, truncating=”post”)<br/> # create the mask to ignore the padded elements in the sequences.<br/> input_mask = [[int(i&gt;0) for i in ii] for ii in input_ids]</span></pre><p id="1927" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">批量输入张量。</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="d9ea" class="mo kv jf nw b gy oa ob l oc od">d = tf.data.Dataset.from_tensor_slices(<br/> ({“input_ids”: tf.constant( input_ids, shape=[num_examples, seq_length], dtype=tf.int32),<br/> “input_mask”: tf.constant( input_mask, shape=[num_examples, seq_length], dtype=tf.int32),<br/> “segment_ids”: tf.zeros(shape=[num_examples, seq_length], dtype=tf.int32),}<br/> ,tf.constant(labels, shape=[num_examples, seq_length], dtype=tf.int32),))<br/>d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)</span></pre><p id="540b" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">创建阿尔伯特微调模型</em> </strong></p><p id="718c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在预训练的艾伯特模型之后增加一层，以预测输入序列中的每个标记是否是菜名的一部分。</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="daa3" class="mo kv jf nw b gy oa ob l oc od">def create_model(albert_config, mode, input_ids, input_mask, segment_ids,labels, num_labels):<br/> “””Creates a classification model.”””<br/> is_training = mode == tf.estimator.ModeKeys.TRAIN<br/> model = modeling.AlbertModel(<br/> config=albert_config,<br/> is_training=is_training,<br/> input_ids=input_ids,<br/> input_mask=input_mask,<br/> token_type_ids=segment_ids)<br/> output_layer = model.get_sequence_output()<br/> hidden_size = output_layer.shape[-1].value<br/> output_weight = tf.get_variable(<br/> “output_weights”, [num_labels, hidden_size],<br/> initializer=tf.truncated_normal_initializer(stddev=0.02))<br/> output_bias = tf.get_variable(<br/> “output_bias”, [num_labels], initializer=tf.zeros_initializer())</span></pre><p id="0dc9" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在这种情况下使用对数丢失。</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="9503" class="mo kv jf nw b gy oa ob l oc od">logits = tf.matmul(output_layer, output_weight, transpose_b=True)<br/>logits = tf.nn.bias_add(logits, output_bias)<br/>logits = tf.reshape(logits, [-1, MAX_LEN, num_labels])</span><span id="8c12" class="mo kv jf nw b gy og ob l oc od">log_probs = tf.nn.log_softmax(logits, axis=-1)<br/>one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)<br/>per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)<br/>loss = tf.reduce_sum(per_example_loss)</span></pre><p id="c716" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">带有权重衰减的 Adam 优化器在这种情况下用于学习</p><p id="4516" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">列车微调模式</em> </strong></p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="6b39" class="mo kv jf nw b gy oa ob l oc od">train_op = optimization.create_optimizer(<br/> total_loss, learning_rate, num_train_steps, num_warmup_steps,<br/> use_tpu, optimizer)</span><span id="ec20" class="mo kv jf nw b gy og ob l oc od">output_spec = tf.contrib.tpu.TPUEstimatorSpec(<br/> mode=mode,<br/> loss=total_loss,<br/> train_op=train_op,<br/> scaffold_fn=scaffold_fn)</span><span id="997e" class="mo kv jf nw b gy og ob l oc od">train_input_fn = input_fn_builder(<br/> file = “data_toy/dish_name_train.csv”,<br/> tokenizer = tokenizer,<br/> seq_length=MAX_LEN,<br/> drop_remainder=True)</span><span id="0779" class="mo kv jf nw b gy og ob l oc od">estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)</span></pre><p id="0968" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在这一步中，如果你没有用 TPU 估算器训练，它将自动检测没有 TPU 可用，并降级到 CPU。</p><p id="77f7" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="oe">评估微调模型</em> </strong></p><p id="4c4b" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">报告损失和准确性以供评估</p><pre class="nk nl nm nn gt nv nw nx ny aw nz bi"><span id="b0d7" class="mo kv jf nw b gy oa ob l oc od">def metric_fn(per_example_loss, label_ids, logits):<br/> predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)<br/> accuracy = tf.metrics.mean(tf.math.equal(label_ids,predictions))<br/> loss = tf.metrics.mean(values=per_example_loss)<br/> #<br/> return {<br/> “eval_accuracy”:accuracy,<br/> “eval_loss”: loss,<br/> }</span></pre><p id="904f" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">通过 100 个时期的训练(在笔记本中，批量大小被设置为 1，并且训练步长被设置为 200，这对于 100 个时期的训练是有效的)，该模型能够在评估集中提取正确的菜肴名称，即使上下文文本与甚至超出词汇表的单词略有不同</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/43d06f39d936a45b76cd8a46043748c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JNdJVDr4r-Iz9E8uf-YOCg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Prediction on Evaluation Set</figcaption></figure><h1 id="5bc3" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">结论</h1><p id="bf36" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">简而言之，Albert 是 NLP 的一大突破，由于开源，我们可以非常直接地在最先进的模型上构建定制的应用程序。</p><h1 id="6caf" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">参考</h1><p id="bee4" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">[1] <a class="ae mi" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lan%2C+Z" rel="noopener ugc nofollow" target="_blank">钟真 L </a>。，<a class="ae mi" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen%2C+M" rel="noopener ugc nofollow" target="_blank">明达 C </a>。、<a class="ae mi" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goodman%2C+S" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安</a> G、<a class="ae mi" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gimpel%2C+K" rel="noopener ugc nofollow" target="_blank">凯文 G </a>。，<a class="ae mi" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sharma%2C+P" rel="noopener ugc nofollow" target="_blank">皮尤什年代</a>。，<a class="ae mi" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Soricut%2C+R" rel="noopener ugc nofollow" target="_blank">拉杜 S </a>。，<a class="ae mi" href="https://arxiv.org/abs/1909.11942" rel="noopener ugc nofollow" target="_blank">用于语言表示的自我监督学习的 Lite BERT</a>(2019)，谷歌研究</p><p id="fda2" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[2]托马斯·w、弗拉达利·d、维克多·s、朱利安·c、克莱门特·d、安东尼·m、皮尔里奇·c、蒂姆·R、雷米·l、摩根·f、杰米·b，《拥抱脸的变形金刚:最先进的自然语言处理》(2019 年)</p><p id="8d19" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[3] M. H. Kwon，<a class="ae mi" href="https://github.com/kwonmha/bert-vocab-builder" rel="noopener ugc nofollow" target="_blank"> Bert-Vocab-Builder </a>，Github Repo</p><p id="fae6" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">[4]塞伯坦 AI，<a class="ae mi" href="https://github.com/cybertronai/pytorch-lamb" rel="noopener ugc nofollow" target="_blank">羔羊优化器</a>，Github Repo</p></div></div>    
</body>
</html>