<html>
<head>
<title>Regularization for Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型的正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-for-machine-learning-models-9173c2e90449?source=collection_archive---------25-----------------------#2019-07-25">https://towardsdatascience.com/regularization-for-machine-learning-models-9173c2e90449?source=collection_archive---------25-----------------------#2019-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="59ef" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">L1·拉索、L2·海岭和 L1+L2 弹性网正规化解释</h2></div><p id="b1ca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习中的一个常见问题是<strong class="kk iu">过拟合</strong>，其中模型错误地概括了训练数据中的噪声:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/276e5b5dbc3dc87dd3d759499ae61942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/1*rrjJaesIzo46XTT3y-SYXA.gif"/></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><a class="ae lq" rel="noopener" target="_blank" href="/over-fitting-and-regularization-64d16100f45c">https://towardsdatascience.com/over-fitting-and-regularization-64d16100f45c</a></figcaption></figure><p id="dfd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">补救这个问题并使模型更健壮的一个流行方法是<strong class="kk iu">正则化</strong>:一个<strong class="kk iu">惩罚项</strong>被添加到算法的损失函数中。这改变了最小化损失函数所产生的模型权重。</p><p id="310c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最流行的正则化技术是<strong class="kk iu">套索、脊</strong>(又名吉洪诺夫)和<strong class="kk iu">弹性网</strong>。对于只有一个<strong class="kk iu">权重参数<em class="lr"> w </em> </strong>(线性拟合的斜率)的简单线性回归的示例情况，它们的罚项看起来像这样(包括一个<strong class="kk iu">比例参数<em class="lr"> λ </em> </strong>):</p><ul class=""><li id="d8b1" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated"><strong class="kk iu">拉索(L1) </strong> : <em class="lr"> λ |w| </em></li><li id="36e9" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">山脊(L2) </strong> : <em class="lr"> λ w </em></li><li id="1106" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">弹性网(L1+L2)</strong>:<em class="lr">λ</em>₁<em class="lr">| w |+λ</em>₂<em class="lr">w</em></li></ul><p id="9343" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不同的项具有不同的效果:<strong class="kk iu">与 L1 相比，二次 L2 正则化在小权重(接近零)时变得可以忽略，但是在大权重时变得更强。这导致了以下行为，随便说说:</strong></p><ul class=""><li id="da8f" class="ls lt it kk b kl km ko kp kr lu kv lv kz lw ld lx ly lz ma bi translated"><strong class="kk iu">无正则化的线性回归</strong>:“我附和一切。”</li><li id="1ea7" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated">拉索:“一开始我持怀疑态度，但是要顺应重大趋势。”</li><li id="017d" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">岭</strong>:“我很容易被说服，但有点迟钝。”</li><li id="2e44" class="ls lt it kk b kl mb ko mc kr md kv me kz mf ld lx ly lz ma bi translated"><strong class="kk iu">弹力网:</strong>“我感觉在山脊和套索之间的某个地方。”</li></ul></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="fcd3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看<strong class="kk iu">在实践</strong>中是什么样子。对于十个随机数，我们将使用上述四种方法中的每一种方法进行线性拟合(出于演示目的，对岭使用增加的λ参数):</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/a37996ea66e0cee1bf1749afc7549c67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zd_AED0qTGrne104M7Ng6Q.png"/></div></div></figure><p id="4fce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，你会在数据中得到一个微小的随机趋势，这无疑是由线性回归得到的。岭回归也显示了这一趋势，但更弱。对于套索和弹性网，当最小化损失函数时，线性 L1 罚项足够高以迫使权重(即斜率)为零。</p><p id="cd8a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我们向数据点添加一个小的线性分量，并重新运行拟合程序:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/177839e74ac0c60002e81d18d2a20f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdft4k41rndb8C3sHWfAmw.png"/></div></div></figure><p id="e2d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这已经足够让 Lasso 不再完全“忽略”坡度系数了。</p><p id="e483" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们进一步增加增加的线性分量，我们得到这个:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ms"><img src="../Images/9d3dd696f1d3d8a1fb3f9608cd965ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5q_D2lnfkw47ikO4khu2nA.png"/></div></div></figure><p id="438e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">套索和弹性网现在几乎完全“接受”显著趋势，而对于山脊，二次惩罚项导致较低的斜率。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="cb9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你错过什么了吗？在评论中分享你的观点或问题吧！</p></div></div>    
</body>
</html>