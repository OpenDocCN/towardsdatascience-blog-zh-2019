<html>
<head>
<title>KNN visualization in just 13 lines of code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">仅用 13 行代码实现 KNN 可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knn-visualization-in-just-13-lines-of-code-32820d72c6b6?source=collection_archive---------5-----------------------#2019-09-24">https://towardsdatascience.com/knn-visualization-in-just-13-lines-of-code-32820d72c6b6?source=collection_archive---------5-----------------------#2019-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="64ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">是啊！就这么简单。让我们用数据集来想象当“k”变化时，决策边界是如何变化的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/296c178817de5c33e2eca0c270397856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*X1KBJctko0RH6BWBsu-XjA.png"/></div></figure><p id="21c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们快速回顾一下…</p><h2 id="c3f5" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">K-NN 是什么？它是如何工作的？</h2><p id="23d7" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">k 最近邻(KNN)算法是一种非常简单、易于理解、通用性强的机器学习算法。在 k-NN 分类中，输出是一个类成员。通过其邻居的多个投票对对象进行分类，将该对象分配到其 k 个最近邻居中最常见的类别(k 是正整数，通常很小)。如果 k = 1，那么该对象被简单地分配到该单个最近邻的类中。</p><p id="bb33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 KNN，K 是最近邻居的数量。邻居的数量是核心决定因素。如果类的数量是 2，k 通常是奇数。当 K=1 时，该算法称为最近邻算法。这是最简单的情况。假设 P1 是需要预测标签的点。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/d5edb2e5516fbabf792794742af16cc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/0*sYMSaIon56Qng2hF.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Basic steps in KNN.</figcaption></figure><p id="b922" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">KNN 有三个基本步骤。<br/> 1。计算距离。<br/> 2。找出 k 个最近的邻居。<br/> 3。为班级投票</p><h2 id="40cc" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">K 的重要性</h2><p id="116d" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">你不能为 k 取任何随机值，整个算法都是基于 k 值的。即使对 k 的微小改变也可能导致大的变化。像大多数机器学习算法一样，KNN 的 K 是一个超参数。您可以将 K 视为预测模型的控制变量。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="b2e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇博客中，我们将看到决策边界如何随 k 变化。为此，我将使用不同类型的玩具数据集。你可以很容易地从下面的链接下载所有这些数据集。</p><div class="mg mh gp gr mi mj"><a href="https://www.kaggle.com/deepthiar/toydatasets" rel="noopener  ugc nofollow" target="_blank"><div class="mk ab fo"><div class="ml ab mm cl cj mn"><h2 class="bd iu gy z fp mo fr fs mp fu fw is bi translated">可视化 KNN</h2><div class="mq l"><h3 class="bd b gy z fp mo fr fs mp fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。</h3></div><div class="mr l"><p class="bd b dl z fp mo fr fs mp fu fw dk translated">www.kaggle.com</p></div></div><div class="ms l"><div class="mt l mu mv mw ms mx ku mj"/></div></div></a></div><h2 id="c64d" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">了解数据集</h2><ol class=""><li id="0fba" class="my mz it js b jt lp jx lq kb na kf nb kj nc kn nd ne nf ng bi translated">u 形的</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/01e3167880390c8786cd9d9b95a19ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*EuYh3kBfY5ewpWJBmbhtkg.png"/></div></figure><p id="e8ac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个数据集中，我们可以观察到这些类是一个互锁的 u 形。这是一个非线性数据集。<br/>蓝色点属于 0 类，橙色点属于 1 类。</p><p id="7347" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2.两组同心圆</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/bf15563e0f5e23ab074bc5c6cf9674c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*WEE7yBtKxG0DFk3RUqHi8A.png"/></div></figure><p id="15cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个数据集中，我们可以看到这些点形成了两组同心圆。这是一个非线性数据集。<br/>蓝色点属于 0 类，橙色点属于 1 类。</p><p id="f38b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">3.异或运算</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6ca90a87103133ef95be42245c72ae1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*1Rs16dKS4AphdbKOsEKLbg.png"/></div></figure><p id="fdd2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该数据集类似于 2 变量 XOR k 图。这是一个非线性线性数据集。</p><p id="a9ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">蓝色点属于 1 类，橙色点属于 1 类。</p><p id="a52c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.线性可分的</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5df0842b6c7aafd4c635699a54f33040.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*LRT7nPgxo3mX9CrOKs15-Q.png"/></div></figure><p id="c368" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个线性数据集，其中的点可以线性分离。</p><p id="d4fd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">蓝色点属于 0 类，橙色点属于 1 类。</p><p id="c74d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">5.极端值</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0a14bbb02856bf232c6809d3c23ce2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*vdlSWk294GM6Oq7-fX2ONA.png"/></div></figure><p id="3301" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这个数据集中，我们可以观察到两个类中都有异常点。我们将看到异常值的存在如何影响决策边界。这是一个线性数据集。<br/>蓝色点属于 0 类，橙色点属于 1 类。</p><p id="4f11" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我们知道了我们的外观，我们现在将继续下去，看看决策边界如何随着 k 值的变化而变化。在这里，我将 1，5，20，30，40 和 60 作为 k 值。您可以尝试任何一组值</p><p id="5c95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，让我们导入库。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="2880" class="kw kx it nl b gy np nq l nr ns">import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from sklearn import datasets, neighbors<br/>from mlxtend.plotting import plot_decision_regions</span></pre><p id="2d39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">需要以下核心功能。</p><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="551b" class="kw kx it nl b gy np nq l nr ns">def knn_comparison(data, k):<br/> x = data[[‘X’,’Y’]].values<br/> y = data[‘class’].astype(int).values<br/> clf = neighbors.KNeighborsClassifier(n_neighbors=k)<br/> clf.fit(x, y)</span><span id="0801" class="kw kx it nl b gy nt nq l nr ns"># Plotting decision region<br/> plot_decision_regions(x, y, clf=clf, legend=2)</span><span id="1439" class="kw kx it nl b gy nt nq l nr ns"># Adding axes annotations<br/> plt.xlabel(‘X’)<br/> plt.ylabel(‘Y’)<br/> plt.title(‘Knn with K=’+ str(k))<br/> plt.show()</span></pre><p id="2b2f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以观察到它只是一行代码，做了所有的事情— <br/> <em class="nu"> clf = neighbors。KNeighborsClassifier(n _ neighbors = k)</em></p><p id="f4cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就是这么简单优雅。现在，我们只需加载我们的 CSV 文件，并将其与 k 一起传递给该函数。我们将对所有数据集逐一执行此操作。</p><p id="f4d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们开始吧…</p><h2 id="0c29" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">1.u 形</h2><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="4c13" class="kw kx it nl b gy np nq l nr ns">data1 = pd.read_csv(‘ushape.csv’)<br/>for i in [1,5,20,30,40,80]:<br/>    knn_comparison(data1, i)</span></pre><div class="kp kq kr ks gt ab cb"><figure class="nv kt nw nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/3f9018784341734bc8c41caf19b48cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*7dKJHWbZsspNHGUMoGPtYA.png"/></div></figure><figure class="nv kt of nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/ec36dc90facf83f0e2f292c3baae6e99.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*AaxUnrs90KU46y3G4437eQ.png"/></div></figure><figure class="nv kt og nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/d8d17b439a75ed439349f89ed06eed9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Wy_zQtQFCrjQrI41Mnm2VQ.png"/></div></figure></div><div class="ab cb"><figure class="nv kt oh nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/21ea0e71b6ed369b775f8d5f89281b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*xdO0y2-iTOYuxjCgzJWKGw.png"/></div></figure><figure class="nv kt oi nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/d2877ffa037652a50e469eb4f1eed70d.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*H05vZc-00o9q1rlmzN7d8w.png"/></div></figure><figure class="nv kt oj nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/d41e5227f80e6ae6f53e6d79e7cf6057.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*UsQUKH40tLru4q1Mi2R9bw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk ok di ol om">KNN visualization for the U-shaped dataset</figcaption></figure></div><h2 id="55b2" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">2.两组同心圆</h2><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="49cf" class="kw kx it nl b gy np nq l nr ns">data2 = pd.read_csv(‘concertriccir2.csv’)<br/>for i in [1,5,20,30,40,60]:<br/>    knn_comparison(data2, i)</span></pre><div class="kp kq kr ks gt ab cb"><figure class="nv kt on nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/99c48a1ce5d908b2e671fafd08b18e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*i42uBGwq8XnkLydoZLBXLA.png"/></div></figure><figure class="nv kt oo nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/2f113f253279f4e68309ddc695ca5b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*5rlY_3IFCEjzSGbflg2Kjg.png"/></div></figure><figure class="nv kt op nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/89ec739249f055be56171691d07a7528.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*1kZO1LehO9bofDIFwgo-GA.png"/></div></figure></div><div class="ab cb"><figure class="nv kt oq nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/76967b8fb6d14a9a51274b141c6a69c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*fd1mV96-eAj8ErXYslMBjQ.png"/></div></figure><figure class="nv kt or nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/82799a2163cf5b4ebddad8e55ad535b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*OTtvEDH41cfpoS58c8zOuQ.png"/></div></figure><figure class="nv kt os nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/ff78b79a9f693ed7c6dd85a95c8f5543.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*cAWQMYgHhIcaQgcLx2F1mw.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk ot di ou om">KNN visualization for the two concentric circles dataset</figcaption></figure></div><h2 id="6a19" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">3.异或运算</h2><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="6b58" class="kw kx it nl b gy np nq l nr ns">data3 = pd.read_csv(‘xor.csv’)<br/>for i in [1,5,20,30,40,60]:<br/>   knn_comparison(data3, i)</span></pre><div class="kp kq kr ks gt ab cb"><figure class="nv kt ov nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/bd0a7066d8aef1df96565f11bdac8a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*mzKVPXkTX7kkcbJq-VIEiw.png"/></div></figure><figure class="nv kt ow nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/10f0a759790800ffaf4b8b098cc99b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*HcOgZeQUUGOP6CScszgw5w.png"/></div></figure><figure class="nv kt ox nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/0451422fe4856279dab36842c84886d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*nHUH9NQRCrKmN17GqUyGAA.png"/></div></figure></div><div class="ab cb"><figure class="nv kt oy nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/89724f9bc2636876c985f64cac181a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*t3sRdZ7OuOZwt04LChBjfw.png"/></div></figure><figure class="nv kt oz nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/4bd28b2246cc332929b5fff781758ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*Hd-Q9rSsFyhsEaV8I7yjmg.png"/></div></figure><figure class="nv kt og nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/bfd0a48f1380c47c1b957e77fd95bbc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*kd8zPCmO7VzJt56fhCXmmA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk pa di pb om">KNN visualization for the XOR dataset</figcaption></figure></div><h2 id="1a85" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">4.线性可分的</h2><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="0a26" class="kw kx it nl b gy np nq l nr ns">data4 = pd.read_csv(‘linearsep.csv’)<br/>for i in [1,5,20,30,40,60]:<br/>    knn_comparison(data4, i)</span></pre><div class="kp kq kr ks gt ab cb"><figure class="nv kt pc nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/13927b9562de6030efffde30461a6b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Lrlid2T4biyyDhXYStjXuA.png"/></div></figure><figure class="nv kt pd nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/9bfb5b26a767899453812401a32fe6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*gNXsvLo8u50JflIvavgDaw.png"/></div></figure><figure class="nv kt pe nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/4c70be1582d5bf550b4b119ae62f54a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*y7xe1T1OfImdoOnl_0Jg8w.png"/></div></figure></div><div class="ab cb"><figure class="nv kt pf nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/5865f5b58ae1ceadf21f9f3345279beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*paoKU269vuZwtZ1GHvkOEg.png"/></div></figure><figure class="nv kt pg nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/14f6d8f4816a4ba22cee4095202bec2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*Asj8jNQQCxJiKcTi_oUdLQ.png"/></div></figure><figure class="nv kt ph nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/c97acd2eaa6a83c71699f235e83490d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*C4Pd6xLQJ5E1adOez6lc7A.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk pa di pb om">KNN visualization for the linearly separable dataset</figcaption></figure></div><h2 id="bb5e" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">5.极端值</h2><pre class="kp kq kr ks gt nk nl nm nn aw no bi"><span id="0656" class="kw kx it nl b gy np nq l nr ns">data5 = pd.read_csv(‘outlier.csv’)<br/>for i in [1, 5,20,30,40,60]:<br/>    knn_comparison(data5, i)</span></pre><div class="kp kq kr ks gt ab cb"><figure class="nv kt pf nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/71dbc9b904f871dfab15c31858721712.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*k8qnIzbONL7eV-wh57HGnA.png"/></div></figure><figure class="nv kt pi nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/0c738bdebbee4f4a29165dcaaa2314e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*Jxm3BUwfxvR-4H8607dnpQ.png"/></div></figure><figure class="nv kt pj nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/0a7ec75dd5e04a905e0f963ceacbfa14.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*kxNZPClli3DjTZXHoYIj5A.png"/></div></figure></div><div class="ab cb"><figure class="nv kt pk nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/8c0fcdf9ec6fd9a5088493b8561ac7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*2xyJiVMGrdrtc-V1avZYSA.png"/></div></figure><figure class="nv kt pl nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/7ef449a01047f1c12ff25363c8072c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*bRwGuCA542uJQA_1-R41TA.png"/></div></figure><figure class="nv kt pi nx ny nz oa paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><img src="../Images/6bd42153ef5c23509734c8a1b5c24b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*fp6KpctUxWymoFhyzJF8FA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk pm di pn om">KNN visualization for the outliers dataset</figcaption></figure></div><h2 id="3501" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">观察结果:</h2><p id="696b" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">在所有数据集中，我们可以观察到，当 k=1 时，我们过度拟合模型。也就是说，每个点都被正确分类，你可能会认为这是一件好事，正如俗话所说的“太多太坏”，过度拟合本质上意味着我们的模型训练得太好，以至于对模型产生负面影响。</p><p id="1af9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在所有的数据集中，我们可以观察到当 k = 60(一个很大的数字)时，我们对模型拟合不足。欠拟合指的是既不能对训练数据建模也不能推广到新数据的模型。欠拟合的机器学习模型不是合适的模型。在离群数据集的情况下，对于 k=60，我们的模型做得相当好。我们怎么知道这个？因为我们知道那些点是异常值，数据是线性可分的，但并不是每次都是这样，我们无法确定。此外，在 XOR 数据集中，与其他模型不同，k 的值不会在很大程度上影响模型。</p><p id="fd7a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">研究表明，不存在适合所有类型数据集的超参数(k)的最佳值。每个数据集都有自己的要求。在小 k 的情况下，噪声将对结果具有更高的影响，而大 k 将使其计算代价昂贵。研究还表明，小 k 是最灵活的拟合，它将具有低偏差但高方差，而大 k 将具有更平滑的决策边界，这意味着较低的方差但较高的偏差。</p><p id="12ae" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">过拟合和欠拟合是机器学习算法性能差的两个最大原因。</p><h2 id="b28d" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">结论:</h2><p id="70a2" class="pw-post-body-paragraph jq jr it js b jt lp jv jw jx lq jz ka kb lr kd ke kf ls kh ki kj lt kl km kn im bi translated">当 K 较小时，我们会限制给定预测的区域，并迫使我们的分类器对整体分布“视而不见”。K 值越小，拟合越灵活，偏差越小，但方差越大。从图形上看，我们的决策边界将会像上面观察到的那样更加参差不齐。另一方面，K 值越高，每次预测的平均点数越多，因此对异常值的适应能力越强。K 值越大，决策边界越平滑，意味着方差越低，但偏差越大。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><h2 id="fefd" class="kw kx it bd ky kz la dn lb lc ld dp le kb lf lg lh kf li lj lk kj ll lm ln lo bi translated">参考</h2><ol class=""><li id="0905" class="my mz it js b jt lp jx lq kb na kf nb kj nc kn nd ne nf ng bi translated"><a class="ae po" href="https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn" rel="noopener ugc nofollow" target="_blank">www . data camp . com/community/tutorials/k-nearest-neighbor-class ification-sci kit-learn</a></li><li id="c3b5" class="my mz it js b jt pp jx pq kb pr kf ps kj pt kn nd ne nf ng bi translated"><a class="ae po" href="http://scott.fortmann-roe.com/docs/BiasVariance.html" rel="noopener ugc nofollow" target="_blank">scott.fortmann-roe.com/docs/BiasVariance.html</a></li><li id="c878" class="my mz it js b jt pp jx pq kb pr kf ps kj pt kn nd ne nf ng bi translated"><a class="ae po" href="http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/#references" rel="noopener ugc nofollow" target="_blank">rasbt . github . io/mlx tend/user _ guide/plotting/plot _ decision _ regions/# references</a></li></ol></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="ae7e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你还在等什么？继续玩数据集吧！！</p></div></div>    
</body>
</html>