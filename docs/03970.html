<html>
<head>
<title>Cross-entropy: From an Information theory point of view</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵:从信息论的角度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cross-entropy-from-an-information-theory-point-of-view-456b34fd939d?source=collection_archive---------10-----------------------#2019-06-22">https://towardsdatascience.com/cross-entropy-from-an-information-theory-point-of-view-456b34fd939d?source=collection_archive---------10-----------------------#2019-06-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1fa28f32e369d253e8eba3bece308371.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4mARXEPSLLxrnXuieMNwow.png"/></div></div></figure><div class=""/><p id="ea82" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">交叉熵是分类问题机器学习中广泛使用的损失函数。信息论被广泛使用，但是在课堂上并没有解释使用它背后的基本原理。在这篇博客中，我们对信息论进行了直观的理解，并最终将其与交叉熵函数联系起来。</p><p id="df22" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">历史:</strong>电信是通过不可靠的信道实现的，在这些信道中，传输的信号往往不等于接收的信号，因为它们被噪声破坏了。Shannon 提出，可以开发一种编码器-解码器系统，该系统可以在讹误量不确定的情况下以最小讹误检索传输信号。这可以通过冗余来实现，即如果一个信号是 aaa，发送三个信号，每个 aaa，如果一个被破坏为 aab，另两个可以通过<em class="kz">共识来确认传输的信号是 aaa。</em>所有这些都是以比特和信道速率<strong class="kd jf">发送的<em class="kz">r</em>T7】=发送的有用比特数/总比特数。香农证明了误差可能是 0，即使<strong class="kd jf"> <em class="kz"> r </em> </strong>是&gt; &gt; 0(他证明了它是 0.5)。</strong></p><p id="e153" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">香农在等式(1)中定义了信号<strong class="kd jf"> <em class="kz">的<strong class="kd jf">信息内容</strong>和信息</em></strong>x<em class="kz"/>。他声称这是测量信息内容的正确方法。从(1)可以看出，对于一个不确定事件，信息量是最大的。等式(2)给出了发射信号总体的信息内容。Shannon 声称 h(x)应该是压缩文件的长度，我们应该渴望对信息进行编码，以便唯一识别。他证明了我们不能将信息编码成许多比特。更高的不确定性=更高的 h(x)值，这意味着需要更多的冗余来编码信息(这是有道理的，对吧)。例如，通过研究一座倒塌的桥(一种罕见的事件)来了解为什么一座桥会倒塌，比研究完美的桥要多得多。</p><p id="637f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">(1)</strong><em class="kz">x = h(x = a)= ln(1/P(x = a))的信息内容</em></p><p id="3e38" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf"> (2) </strong> h(x) = <em class="kz">求和超过所有 a</em>(<strong class="kd jf">P(x = a)ln(1/P(x = a))</strong>)</p><p id="34ac" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当所有人都具有<strong class="kd jf">相等的概率</strong>时，h(x)对于系综是最大的。在我们继续之前，让我们举一个例子。例如，如果我在想一个介于 1-100 之间的数字，并要求您在最少的尝试次数中猜出它，“最佳优先”问题是“如果该数字小于 50”。这是最好的问题，因为它给出了关于我的号码的最大信息(排除了 50 个选项)。如果你的第一个问题是“这个数字是否小于 1”，如果答案是“否”，你很可能没有获得足够的信息(因为你只剩下 99 个选项)。因此，当我们选择一个数字 50 将集合分成<strong class="kd jf">等概率时，信息内容最大化。</strong>根据香农原理，提供 1 比特的信息可以将不确定性降低 1/2。</p><p id="c0a7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">熵:</strong>定义为期望的信息量(等式 2)。它也是信息不确定性的一种度量。如果不确定性高，熵就高。例如，如果我们有一个装满的硬币，每次都是正面朝上，那么熵为 0，因为没有来自硬币投掷的信息(因为它总是正面朝上)。</p><p id="d9ac" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们必须编码一条长度为<em class="kz"> l </em>的信息，我们不能用比熵少的位数来编码它(等式 2)。用于传递信息的代码的平均长度不能少于所发送信息的“比特数”。为了唯一识别长度为<em class="kz"> l、</em>的信号，我们可以使用 2^ <em class="kz"> l </em>位(0 或 1)。一条信息的理想长度<em class="kz"> i </em>如等式(3)所示。为了唯一可识别(基于 Kraft 不等式)，编码的长度如等式(4)所示。</p><p id="2f8f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">(3)</strong><em class="kz">l _ I = h(x _ I)= ln(1/p _ I)</em></p><p id="5f9e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">(4)</strong><em class="kz">l _ I = ln(1/q _ I)-ln(Z)</em>其中 Z 为归一化常数&gt; 0 且≤ 1</p><p id="6655" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所需总位数之和<strong class="kd jf"> <em class="kz"> L </em> </strong> =所有位之和<em class="kz">I</em>(<em class="kz">p _ I</em>x<em class="kz">L _ I</em>)。使用来自(4)的<em class="kz"> l_i </em>值，我们得到<strong class="kd jf"><em class="kz">L≥</em></strong>h(X)+KL(p||q)其中 KL(p | | q)是 Kullback-Liebler 散度。因此，KL 散度是表示一组信息相对于理想编码位所需的额外编码长度。现在我们已经了解了 KL 背离背后的历史，让我们来看一下统计数据。</p><p id="6e01" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf"> Kullback-Liebler </strong>散度是“<em class="kz">P 与 Q 的不同程度的度量”</em>。它是使用等式(2)的两个分布中包含的信息之间的差异，通常从期望分布的角度来测量(分类问题中的真实类别)。从真实分布的角度来看，KL 散度是对新分布进行编码所需的信息内容+额外比特。现在，设置好这些部分后，我们来看看交叉熵。</p><p id="a8cc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">交叉熵</strong> = <strong class="kd jf"> <em class="kz">总和超过所有 I</em></strong><em class="kz">(</em><strong class="kd jf"><em class="kz">Y _ I</em></strong><em class="kz"/>x<strong class="kd jf"><em class="kz">ln(Y _ I/Y _ hat _ I</em></strong><em class="kz">)</em><strong class="kd jf">。</strong>对所有<em class="kz"> i </em>求和，并通过分离和丢弃<em class="kz"> ln() </em>项内的常数分子进行简化，交叉熵保持在<strong class="kd jf"><em class="kz">sum over all I</em></strong><em class="kz">(</em><strong class="kd jf"><em class="kz">Y _ I</em></strong>x<strong class="kd jf"><em class="kz">ln(1/Y _ hat _ I)</em></strong>)</p><p id="950c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果真实分布是<em class="kz"> Y </em>预测是<em class="kz"> Y_hat，</em>和<em class="kz"> Y 不等于 Y_hat，</em>由于我们永远不知道真实分布，交叉熵不等于熵。</p><p id="5b40" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">差=<strong class="kd jf"><em class="kz">-对所有 I 求和(Y _ I(log(Y _ hat _ I)-log(Y _ I))= KL(Y | | Y _ hat)</em></strong></p><p id="f2a0" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，交叉熵实际上是真实分布和来自机器学习模型的分布之间的熵+ KL 散度的度量。</p><p id="b199" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd jf">参考文献:</strong></p><ol class=""><li id="83b5" class="la lb je kd b ke kf ki kj km lc kq ld ku le ky lf lg lh li bi translated">大卫·麦凯教授信息论系列讲座</li><li id="3bbb" class="la lb je kd b ke lk ki ll km lm kq ln ku lo ky lf lg lh li bi translated"><a class="ae lj" href="https://www.youtube.com/watch?v=ErfnhcEV1O8" rel="noopener ugc nofollow" target="_blank">关于 KL 散度+交叉熵的 Youtube 视频</a></li></ol></div></div>    
</body>
</html>