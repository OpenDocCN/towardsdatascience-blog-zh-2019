<html>
<head>
<title>Omni-benchmarking Image Classification (and much more)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">全方位基准图像分类(以及更多)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/omni-benchmarking-image-classification-and-much-more-5e0d08bb2a7?source=collection_archive---------37-----------------------#2019-09-24">https://towardsdatascience.com/omni-benchmarking-image-classification-and-much-more-5e0d08bb2a7?source=collection_archive---------37-----------------------#2019-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/aa15b55d63134e88b89dacf28a85a5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVz01yW2a0KxfJq9dLK34w.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Performance/accuracy exploration of image classification models on hardware ranging from IoT-class to server-class platforms from the submissions to the 1st <a class="ae jg" href="http://cknowledge.org/request-cfp-asplos2018.html" rel="noopener ugc nofollow" target="_blank">ACM ReQuEST tournament at ASPLOS’18</a>.</figcaption></figure><div class=""/><p id="a986" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图像分类是研究最大似然模型的效率/质量权衡的流行领域。例如，<a class="ae jg" href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="noopener ugc nofollow" target="_blank">最先进的</a> <a class="ae jg" href="https://arxiv.org/abs/1906.06423" rel="noopener ugc nofollow" target="_blank"> FixResNeXt-101 </a>模型在<a class="ae jg" href="http://image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>验证集上达到了 86.4%的前 1 准确率，但 829M 参数仅适用于<a class="ae jg" href="https://github.com/facebookresearch/FixRes#cluster-settings" rel="noopener ugc nofollow" target="_blank">集群使用</a>。作为另一个例子，适用于移动设备<a class="ae jg" href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet#imagenet--checkpoints" rel="noopener ugc nofollow" target="_blank">的<a class="ae jg" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> MobileNets-v2 </a>型号系列的 Top-1 精度范围</a>从 6.1M 参数的 75.0%到 1.7M 参数的 45.5%。</p><p id="f718" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，“集群”和“移动”之间的区别纯粹是<em class="le">定性的</em>。就功耗而言，差异很容易达到一千倍或一百万倍。然而，执行<a class="ae jg" href="https://www.sigarch.org/artifact-evaluation-for-reproducible-quantitative-research/" rel="noopener ugc nofollow" target="_blank"> <em class="le">定量</em> </a>评估可能相当棘手。</p><p id="86d1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">诚实地回答你自己，你上一次能够在几分钟甚至几小时内拿着一份令人兴奋的新报纸转一圈是什么时候？我们的意思是，<em class="le">对于真正的</em>:下载所有的工件(代码、数据等)。)，安装所有依赖项(如 TensorFlow)和依赖项的依赖项(如 Java 和 Bazel)，移植到新平台(Android，有人吗？)、用新数据集测试等等？</p><p id="97e9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从来没有发生在我们身上。一百万年后也不会。我们最多能在几天内完成。更常见的是，几周之内。即使假设这张纸真的有藏物。(令人失望的是，7 中只有<a class="ae jg" href="https://medium.com/atlas-ml/state-of-deep-learning-h2-2018-review-cc3e490f1679" rel="noopener"> 1 毫升的纸有。)</a></p><p id="6b0c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，想象一下打开一篇论文，描述一种在移动设备上进行图像分类的新方法，这种方法很方便地带有一个工件。想象一下，您可以在阅读介绍的同时，在几分钟内安装并定制该神器，并在手机上再现实验，同时得出结论？只需点击一个图，就可以与社区共享您的结果，并立即看到任何异常和异常值，这可能需要进一步的调查。</p><p id="b060" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">是的，这听起来像是一场梦。然而，我们相信快速的工件和知识共享对于加速 ML/系统的研究和开发是至关重要的。今天，我们描述了我们与社区一起朝着这个梦想迈出的一小步。</p><h1 id="30e6" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">ACM ReQueEST:可再生质量/高效系统锦标赛</h1><p id="f633" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">2018 年，我们<a class="ae jg" href="https://portalparts.acm.org/3230000/3229762/fm/frontmatter.pdf" rel="noopener ugc nofollow" target="_blank">在<a class="ae jg" href="https://asplos-conference.org/" rel="noopener ugc nofollow" target="_blank"> ASPLOS </a>联合举办了</a>第一届<a class="ae jg" href="http://cknowledge.org/request" rel="noopener ugc nofollow" target="_blank">ACM ReQuEST 锦标赛</a>，我们<a class="ae jg" rel="noopener" target="_blank" href="/acm-request-1st-open-and-reproducible-tournament-to-co-design-pareto-efficient-deep-learning-ea8e5a13d777">邀请社区</a>提交论文，并附上完整的工件(代码、数据、脚本等)。).<a class="ae jg" href="https://portalparts.acm.org/3230000/3229762/fm/frontmatter.pdf" rel="noopener ugc nofollow" target="_blank">我们的目标</a>是让每一个<em class="le"> </em>提交的工件通过完善的<a class="ae jg" href="http://ctuning.org/ae/reviewing.html" rel="noopener ugc nofollow" target="_blank">工件评估</a>过程。换句话说，我们关注的是<a class="ae jg" href="https://www.sigarch.org/artifact-evaluation-for-reproducible-quantitative-research/" rel="noopener ugc nofollow" target="_blank">实际的定量再现性</a>而不是提交材料的纯粹新颖性。</p><p id="ce6c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们对提交的涵盖所有级别的 ML/软件/硬件堆栈(一位分析师<a class="ae jg" href="https://twitter.com/jameskobielus/status/994175022586613760" rel="noopener ugc nofollow" target="_blank">称之为</a> <em class="le">全方位基准</em>)的多样性感到非常惊讶:</p><ul class=""><li id="dddc" class="mi mj jj ki b kj kk kn ko kr mk kv ml kz mm ld mn mo mp mq bi translated"><strong class="ki jk">型号:</strong> MobileNets，ResNet-18，ResNet-50，Inception-v3，VGG16，AlexNet。</li><li id="9cb1" class="mi mj jj ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated"><strong class="ki jk">框架和库:</strong> TensorFlow，Caffe，MXNet，Keras，<a class="ae jg" href="https://github.com/ARM-software/ComputeLibrary/" rel="noopener ugc nofollow" target="_blank"> Arm 计算库</a>，cuDNN，<a class="ae jg" href="https://tvm.ai/" rel="noopener ugc nofollow" target="_blank"> TVM </a>，NNVM。</li><li id="8b96" class="mi mj jj ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated"><strong class="ki jk">平台:</strong> Xilinx Pynq-Z1 FPGA，Arm Cortex CPUs 和 Arm Mali gp GPU(Linaro hikey 960 和 T-Firefly RK3399)，树莓 Pi 设备的农场，NVIDIA Jetson TX1 和 TX2，以及亚马逊、微软和谷歌云中的英特尔至强服务器。</li><li id="8b9a" class="mi mj jj ki b kj mr kn ms kr mt kv mu kz mv ld mn mo mp mq bi translated"><strong class="ki jk">数据类型:</strong> 8 位整数，16 位浮点(半)，32 位浮点(浮点)。</li></ul><p id="2d5a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每一个<em class="le"> </em>工件，我们都创建了一个自动化的、可定制的、可重复的<a class="ae jg" href="https://github.com/ctuning/ck" rel="noopener ugc nofollow" target="_blank">集体知识</a>工作流，以统一评估准确性、延迟(每张图像的秒数)、吞吐量(每秒图像数)、峰值功耗(瓦特)、价格和其他指标。然后我们在<a class="ae jg" href="https://github.com/ctuning/ck-request-asplos18-results" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上发布了统一的工作流程，并在<a class="ae jg" href="https://dl.acm.org/citation.cfm?doid=3229762" rel="noopener ugc nofollow" target="_blank"> ACM 数字图书馆</a>上添加了快照。</p><p id="af69" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们决定在一个公开的<a class="ae jg" href="http://cknowledge.org/dashboard/request.asplos18" rel="noopener ugc nofollow" target="_blank">交互式仪表盘</a>上显示所有结果，而不是宣布一个获胜者或者<a class="ae jg" href="https://en.wikipedia.org/wiki/Name_and_shame" rel="noopener ugc nofollow" target="_blank">点名羞辱</a>。使用仪表板，您可以应用自己的标准来探索解决方案空间，并寻找<a class="ae jg" href="https://en.wikipedia.org/wiki/Pareto_efficiency" rel="noopener ugc nofollow" target="_blank">帕累托最优</a>解决方案(例如，找到达到所需精确度的最节能的解决方案)。基本上，如何通过自己的喜好和要求解读结果，就看你自己了！</p><h1 id="23af" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">视力</h1><p id="afee" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated">从纯粹的基准测试角度来看，<a class="ae jg" href="http://cknowledge.org/request" rel="noopener ugc nofollow" target="_blank"> ReQuEST </a>可以被视为<a class="ae jg" href="https://www.mlperf.org/" rel="noopener ugc nofollow" target="_blank"> MLPerf </a>开放部门的先驱，在这里提交者对他们使用的工作负载和系统没有限制，只要他们的提交是可复制的。</p><p id="3f0c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，我们的长期目标是创建<a class="ae jg" href="https://reuseresearch.com/components" rel="noopener ugc nofollow" target="_blank">可重用 ML/SW/HW 组件的开放库</a>。这将允许研究人员在他们的可复制和交互式论文中建立这样的组件，同时最终解决跨最新软件和硬件的基准测试技术的棘手问题。</p><figure class="mx my mz na gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/2d93eb110402e339d81f074ec0609cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WNzMpIH32BFqhXAPq6MFqw.png"/></div></div></figure><p id="ae3b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们预计，最终社区将对完整的 ML/SW/HW 堆栈进行众包基准测试、共同设计和优化(例如，参见<a class="ae jg" href="http://cknowledge.org/android-apps.html" rel="noopener ugc nofollow" target="_blank">我们的 Android 众包基准测试演示</a>)，这反过来将大大减少现实产品所需的工作和上市时间。</p><p id="420e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae jg" href="http://cknowledge.org/contacts.html" rel="noopener ugc nofollow" target="_blank">如果你想了解更多，请联系</a>并加入<a class="ae jg" href="http://cknowledge.org/partners.html" rel="noopener ugc nofollow" target="_blank">我们成长中的社区</a>！</p><h1 id="f88e" class="lf lg jj bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">作者</h1><p id="0f85" class="pw-post-body-paragraph kg kh jj ki b kj md kl km kn me kp kq kr mf kt ku kv mg kx ky kz mh lb lc ld im bi translated"><a class="ae jg" href="https://towardsdatascience.com/@dividiti?source=follow_footer--------------------------follow_footer-" rel="noopener" target="_blank">安东·洛克莫托夫</a>是<a class="ae jg" href="http://dividiti.com" rel="noopener ugc nofollow" target="_blank">divideti</a>的联合创始人兼首席执行官。研究员、工程师和企业家，热衷于优化使用宝贵的资源，如计算机系统…和人才。</p><p id="93f2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Grigori Fursin 是 CodeReef.ai 的联合创始人兼首席技术官。开源贡献者和可复制性专家，清理人工智能创新中的混乱。</p></div></div>    
</body>
</html>