<html>
<head>
<title>Similar Images Recommendations using FastAi and Annoy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 FastAi 和 airy 的相似图片推荐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/similar-images-recommendations-using-fastai-and-annoy-16d6ceb3b809?source=collection_archive---------14-----------------------#2019-07-15">https://towardsdatascience.com/similar-images-recommendations-using-fastai-and-annoy-16d6ceb3b809?source=collection_archive---------14-----------------------#2019-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f0761b11ca8161547a41c4e18a0d3124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xJ0MjCfbwQ4v2dhXe3JCIg.png"/></div></div></figure><p id="3b9c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">基于图像相似性的推荐，如上所示，是我们将要考虑的，给定一个基础图像，推荐视觉上相似的图像。基于图像的推荐在许多情况下非常方便，特别是在涉及用户视觉品味的情况下——服装、珠宝、配饰等。我们将使用 DeepFashion 数据集，其中有<strong class="kd iu"> 289，222 张服装图片</strong>分布在<strong class="kd iu"> 42 个类别</strong>。让我们快速看一下这个项目的工具箱。</p><p id="f736" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu"> FastAi </strong> </a> <strong class="kd iu">，【PyTorch 上的一个包装器，它使深度学习任务惊人地简单，并附带了所有最佳实践。</strong></p><p id="a4e9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://github.com/pytorch/pytorch" rel="noopener ugc nofollow" target="_blank"> <strong class="kd iu"> PyTorch </strong> </a> <strong class="kd iu">，</strong>深度学习库由脸书提供，我们将在我们的项目中使用 PyTorch 的一些功能。</p><p id="7777" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://github.com/spotify/annoy" rel="noopener ugc nofollow" target="_blank"><strong class="kd iu"/></a><strong class="kd iu">、</strong>近似最近邻由 Spotify 开源实现，能够高效地索引和搜索特征向量。</p><p id="3308" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个项目的代码可以在这个<a class="ae kz" href="https://jvn.io/gautham20/e6bd87b3597e4a12bb601216b4d2289d" rel="noopener ugc nofollow" target="_blank"> jupyter 笔记本</a>中找到。</p><h1 id="86de" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">嵌入或特征向量</h1><p id="9b6b" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">嵌入或特征向量可以被认为是对象上的简洁的 n 维向量形式，其目的是捕获包含在对象中的大部分信息。在其 n 维特征空间中，当比较不同的对象时，相似的对象彼此更接近。</p><p id="0b21" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于一幅图像，嵌入可以从<strong class="kd iu">卷积神经网络</strong> (CNN)中获得。CNN 能够理解/学习图像中包含的信息，在其每一层的不同细节层次上。初始卷积层理解诸如角、边等低级形状。而末端层理解图像中的复杂结构，人脸，建筑物等。基于任务。CNN 的末端层通常是完全连接的线性层。这些完全连接的层的输出被用作图像嵌入。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7612f94076729d93d237c19b7f26b697.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEdS3QDWnB2wKUdnYO56_Q.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Image Embedding. CNN image source-<a class="ae kz" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">en.wikipedia.org/wiki/Convolutional_neural_network</a></figcaption></figure><h1 id="a2df" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">使用 FastAI 微调预训练的 Resnet152 模型</h1><p id="978b" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">我们使用的 CNN 模型是在<a class="ae kz" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> Imagenet 数据集</a>上训练的<a class="ae kz" href="https://pytorch.org/docs/stable/torchvision/models.html" rel="noopener ugc nofollow" target="_blank">预训练<strong class="kd iu"> Resnet152 </strong> </a> <strong class="kd iu"> </strong>用于图像分类。虽然这个模型可以直接用于获得嵌入，但使用 FastAi 对模型进行微调以适应深度时尚数据集将有助于模型更好地理解我们数据集的细微差别。这个过程叫做<a class="ae kz" rel="noopener" target="_blank" href="/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"> <strong class="kd iu">迁移学习</strong> </a>。</p><p id="147b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在 DeepFashion 类别预测任务上训练模型，取得了 88.4% 的<strong class="kd iu">前 3 名准确率和 93.98% </strong>的<strong class="kd iu">前 5 名准确率，优于<a class="ae kz" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> DeepFashion 白皮书</a>【1】中公布的基准分数。</strong></p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/c8aca0aa23f8a63dcf6b5f8fcfe3fb4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*SdvD4xK5aItGu00fJ5X9lQ.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Results of training resnet152 for 10 epochs</figcaption></figure><p id="8975" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从这个微调的模型中提取特征向量。</p><h1 id="37b3" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">使用 PyTorch 钩子从模型中提取特征向量</h1><p id="0e6a" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">根据 resnet152 架构，我们使用倒数第二个全连接层的输出作为嵌入，它们是维数为(512，1)的向量。<br/>为了从 PyTorch 中的模型获得中间层的输出，我们使用了一个名为 Hook 的功能。可以添加钩子来从正向传递或反向传递中提取模型的中间值。</p><p id="b2fc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用 PyTorch 钩子的最佳方法可以改编自<a class="ae kz" href="https://github.com/fastai/fastai/blob/master/fastai/callbacks/hooks.py" rel="noopener ugc nofollow" target="_blank"> FastAi 库代码</a>本身。</p><figure class="md me mf mg gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="852e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用 Pytorch 钩子，我们为训练和有效数据集中的所有图像生成特征向量。</p><h1 id="cf32" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">使用特征向量推荐相似图像</strong></h1><p id="24ca" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">既然我们有了所有图像的特征向量，我们将不得不在给定一个基础图像的情况下得到相似的图像。最初，尝试一种简单的方法，给定一个基础图像，让计算它与数据集中所有其他图像的相似性得分，并获得前 n 个图像。</p><figure class="md me mf mg gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="0136" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这种方法不可扩展，每个查询的复杂度为 O(N ),对于 249，222 张图像,大约需要<strong class="kd iu"> 10 秒的时间。为了将这种复杂性降低到亚线性时间，我们将使用<strong class="kd iu">近似最近邻算法。</strong>近似最近邻算法可以看作是精度和性能之间的权衡。《烦人》的作者 Eric Bernhardsson 在<a class="ae kz" href="https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html" rel="noopener ugc nofollow" target="_blank">的博客上发表了各种人工神经网络算法的基准比较。</a></strong></p><h1 id="ba8b" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">使用骚扰获得相似的图像</h1><p id="e6d0" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated"><strong class="kd iu">angry</strong>(<strong class="kd iu">A</strong>approximate<strong class="kd iu">N</strong>earest<strong class="kd iu">N</strong>eighbor<strong class="kd iu">O</strong>h<strong class="kd iu">Y</strong>eah)是基于二叉树的 ANN 实现。对其工作原理的最佳解释可以在埃里克的文章中找到。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/994996ab0d14f74e00ce322a01e6fb61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02dgOrY3FTDCkwx0fAaz0w.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Annoy — binary tree traversal for a query, source-<a class="ae kz" href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html" rel="noopener ugc nofollow" target="_blank">erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html</a></figcaption></figure><p id="14a5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">精度-性能的权衡由我们构建的二叉树的数量来控制，越多的树意味着越好的精度。我们将使用与数据集中的类数量相同的树。下面的代码用于构建索引。</p><figure class="md me mf mg gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="20e1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在让我们尝试查询恼人的指数。</p><figure class="md me mf mg gt ju"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="e0ca" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">使用 Annoy 的近似查询时间减少到了<strong class="kd iu"> 2ms </strong>，比简单的方法少了几个数量级。</p><h1 id="ec48" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated"><strong class="ak">相似图片推荐</strong></h1><p id="b9c0" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">下面再来几个通过查询骚扰指数得到的推荐。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/89a53b3b0cd24337fcd355e426d54375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VsqJH1aC52scEzJtGhg0uQ.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Bridal Dresses</figcaption></figure><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/34cfa2d1b434b5e11489439e9defe08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdOKtIOiFGZQozinIDgDPg.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Hoodies</figcaption></figure><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/653f6be108da3c3a579a919bd15bffaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w8dv7nhtfnpZSv9XfPUt6g.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Girls tees</figcaption></figure><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/91fc9f50584f7f1106f787151cbb157d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cHzeOJIz2fQNAOnamGBOLg.png"/></div></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk">Men's pants</figcaption></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="337d" class="la lb it bd lc ld mw lf lg lh mx lj lk ll my ln lo lp mz lr ls lt na lv lw lx bi translated">资源</h1><p id="7bfe" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">[1] <a class="ae kz" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank">刘，罗，邱，王，唐。Deepfashion:通过丰富的注释支持强大的服装识别和检索。2016 年在 CVPR。</a></p><p id="574f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://erikbern.com/2015/10/01/nearest-neighbors-and-vector-models-part-2-how-to-search-in-high-dimensional-spaces.html" rel="noopener ugc nofollow" target="_blank">恼人的解释—最近邻和向量模型—第 2 部分—算法和数据结构</a></p><p id="43df" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae kz" href="https://erikbern.com/2018/02/15/new-benchmarks-for-approximate-nearest-neighbors.html" rel="noopener ugc nofollow" target="_blank"> ANN 算法基准</a></p><p id="45cf" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">希望这篇文章对你有所帮助，请在下面留下你的评论和想法。😊</p></div></div>    
</body>
</html>