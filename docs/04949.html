<html>
<head>
<title>Machine Learning for Content Moderation — Challenges</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于内容审核的机器学习—挑战</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-for-content-moderation-challenges-ce9c227c8b30?source=collection_archive---------32-----------------------#2019-07-25">https://towardsdatascience.com/machine-learning-for-content-moderation-challenges-ce9c227c8b30?source=collection_archive---------32-----------------------#2019-07-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4019" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用机器学习系统进行内容审核的挑战</h2></div><h1 id="06f6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">概观</h1><p id="7466" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">有关内容审核的机器学习主题的介绍，请阅读本系列的简介:</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/machine-learning-for-content-moderation-introduction-4e9353c47ae5"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">用于内容审核的机器学习—简介</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">用于在线内容审核的机器学习系统综述</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn mo lz"/></div></div></a></div><p id="f75d" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">现在，我们已经对用于自动内容审核的机器学习系统进行了概述，我们可以解决这些系统面临的主要挑战。这些潜在的问题可能导致在评估模型、确定接近的分类器阈值以及公平地使用它而没有无意的偏差方面的困难。由于内容审核系统作用于复杂的社会现象，因此它们面临着在其他机器学习环境中不一定会遇到的问题。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="3613" class="ki kj it bd kk kl nb kn ko kp nc kr ks jz nd ka ku kc ne kd kw kf nf kg ky kz bi translated">不同的定义</h1><p id="3959" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于内容调节的许多应用，很难提供感兴趣现象的明确定义。这些话题往往是非常复杂的社会现象，其定义是学术界不断争论的话题。例如，网络欺凌在学术文本中有各种各样的定义，因此很难创造一个所有人都同意的包罗万象的定义。由于这个原因，提供给手动内容贴标机的说明可能不够清楚，不能产生非常可靠的标签。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/a3f5c6165d35713c29391442ea0d0ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kLqC5UaginrIdQ7D.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk"><a class="ae nv" href="https://indigenousx.com.au/is-the-definition-of-racism-racist/" rel="noopener ugc nofollow" target="_blank">https://indigenousx.com.au/is-the-definition-of-racism-racist/</a></figcaption></figure><p id="47c6" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">这就导致了两个问题。</p><p id="671c" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">首先，如果用户认为违反规则的一些内容被删除而另一些没有被删除，那么对用户来说，内容审核系统似乎是不一致的。这可能会导致用户不信任内容审核机制，或者认为它不公平地针对某些用户。从用户的角度来看，这些系统是模糊的黑匣子，很难解释为什么会出现这种不一致。</p><p id="c329" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">另一个问题是，标记的训练数据可能有矛盾的数据点。如果标注不一致导致两个非常相似的数据点具有相反的标注，那么模型的性能可能会受到影响，因为它要努力学习数据中的正确属性和模式。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="a992" class="ki kj it bd kk kl nb kn ko kp nc kr ks jz nd ka ku kc ne kd kw kf nf kg ky kz bi translated">数据标签可用性</h1><p id="0fa4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">获取标记数据通常是一个成本高昂的过程。对于许多内容审核任务来说，标记任务相对复杂，并且需要训练标记员。他们必须理解特定社会现象的某种定义。这与简单得多的标记任务形成对比，例如确定图像是否包含特定对象。</p><p id="f528" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">此外，与图像分类等任务相比，这些任务的公共数据集非常少，因为它们通常被视为收集数据的公司的财产。公司不太可能想要分享这些数据，因为他们认为这是他们竞争优势的一部分。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nw"><img src="../Images/7038e1d3b0437d6fbc6b44900ffd9702.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/0*dLfBhEg6WWslngBg.png"/></div></div></figure><p id="e328" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">由于成本很高，可能很难获得足够大的数据集来轻松训练有监督的机器学习模型。为了解决这个问题，机器学习实践者必须经常求助于半监督或弱监督方法来扩充他们的数据集。通常，他们利用一小组手动标记的数据(“黄金标准”数据)来获取其他未标记的提交内容的标签。通过这种方法，他们能够从最初较小的手动标记的数据点池中创建大型数据集。</p><p id="90f5" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">虽然这些方法很强大，但它们也有自己的问题。“黄金标准”数据中存在的偏差或误差很可能会传播到整个数据集。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="4e8f" class="ki kj it bd kk kl nb kn ko kp nc kr ks jz nd ka ku kc ne kd kw kf nf kg ky kz bi translated">算法偏差</h1><p id="9f5b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当创建作用于社交内容的机器学习模型时，总是存在引入算法偏差的风险。<a class="ae nv" href="https://en.wikipedia.org/wiki/Algorithmic_bias" rel="noopener ugc nofollow" target="_blank">算法偏差</a>指的是一种算法(机器学习模型)创造了一种与某种种族或社会经济因素相关的不公平情况，例如通过不公平地惩罚某些人群，或者通过将人群从平台的其余部分中分割出来。</p><p id="a98c" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">创建内容审核模型时，尽最大努力避免算法偏差是很重要的。如果您的模型考虑了用户配置文件特征，如位置或人口统计特征，则必须非常小心，以确保模型不会根据这些特征进行辨别。</p><p id="9935" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">例如，在网络欺凌检测模型中，如果模型的输入是性别，则它可以学习在涉及某个性别的情况下增加网络欺凌的概率。然而，由于网络欺凌通常更多地发生在该性别的人群中，该模型只是继承了社会的整体偏见。</p><p id="aae1" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">在这种情况下，机器学习从业者必须小心谨慎，要么不使用某些人口统计特征，要么验证<em class="nx">有用</em>，非歧视性模式正在用它们学习。否则，这种模式可能成为歧视性的，并加剧先前存在的社会不平等。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="ce0f" class="ki kj it bd kk kl nb kn ko kp nc kr ks jz nd ka ku kc ne kd kw kf nf kg ky kz bi translated">对抗性</h1><p id="aaa8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">自然，对于那些内容被认为违反规则的人来说，这些系统被认为扼杀了他们在平台上的表达自由。因此，这些人通常会尽最大努力继续发布相同类型的内容，同时规避机器学习系统的标记机制。</p><p id="58f3" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">这与其他机器学习环境非常不同，在其他机器学习环境中，输入数据通常只是原始的传感器数据或金融数据。一旦你将对手引入其中，学习的任务就会变得更加困难和微妙，因为这些对手会不断改变他们的方法以逃避检测。</p><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d6e42902dcd5eaaf1a921cf8b0309771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*CUPr_LkfeSOyubzi.jpg"/></div></figure><p id="3ebd" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">一个常见的例子是垃圾邮件和诈骗/网络钓鱼检测<strong class="lc iu">。</strong>为了避免被发现，犯罪分子会尝试许多技术，例如改变文本的大小写和间距，用同义词替换单词，用数字或类似的非标准字符替换字符，以及将其实际内容嵌入到更大的非犯罪内容中。</p><p id="26b6" class="pw-post-body-paragraph la lb it lc b ld mp ju lf lg mq jx li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">这些对手愿意不断研究新方法来逃避检测，这意味着机器学习从业者必须不断重新评估他们的模型，以确保它们仍然有效。有时，这可能需要训练新的模型来处理新类别的规避方法。</p></div></div>    
</body>
</html>