<html>
<head>
<title>Generating Text with TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TensorFlow 2.0 生成文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-text-with-tensorflow-2-0-6a65c7bdc568?source=collection_archive---------10-----------------------#2019-05-12">https://towardsdatascience.com/generating-text-with-tensorflow-2-0-6a65c7bdc568?source=collection_archive---------10-----------------------#2019-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/0f7c1359bf3825bc70bf4b478267a197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Xa_xhnb2-KITNqfw"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@aussieactive?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">AussieActive</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><h1 id="d319" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">序文</h1><p id="f046" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在工作中，我大部分时间都在处理表格数据，所以扩展到相邻的机器学习领域有助于拓宽我的视野，识别潜在的应用……去年，我不得不钻研计算机视觉，以便完成<a class="ae jd" href="https://academy.microsoft.com/en-us/professional-program/tracks/artificial-intelligence/" rel="noopener ugc nofollow" target="_blank">微软在 AI </a>期末项目中的专业开发计划，今年我决定开始学习更多关于文本生成的知识。</p><p id="5bcd" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我选择了伊利亚特作为我的训练文本，因此有了上面的卫城图片。虽然我也在 PyTorch、Keras(带有 TensorFlow 后端)和 TensorFlow 中实现了递归神经网络(RNN)文本生成模型，但我发现 TensorFlow 2.0 的到来非常令人兴奋，并且对机器学习的未来充满希望，因此我将在本文中重点讨论这个框架。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="183e" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">代码和文本可以在我的<a class="ae jd" href="https://github.com/mlai-demo/TextGen-tf2" rel="noopener ugc nofollow" target="_blank">公共 Github 库</a>中找到。我已经从<a class="ae jd" href="http://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">古腾堡项目</a>中提取了文本，并在上传到资源库之前进行了预处理。你也可以在这里找到关于 RNN 文本生成的优秀教程<a class="ae jd" href="https://www.tensorflow.org/alpha/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="7ce6" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">设置</h1><p id="5850" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">让我们下载项目需要的东西，并确认一切都如预期的那样，GPU 可用(您肯定希望在这个项目上使用 GPU，这会节省您很多时间):</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="601f" class="mu ke jg mq b gy mv mw l mx my">from __future__ import absolute_import, division, print_function, unicode_literals</span><span id="55f3" class="mu ke jg mq b gy mz mw l mx my">!pip install tensorflow-gpu==2.0.0-alpha0<br/>import tensorflow as tf</span><span id="7ca3" class="mu ke jg mq b gy mz mw l mx my">import numpy as np<br/>import os<br/>import datetime</span><span id="9230" class="mu ke jg mq b gy mz mw l mx my">from tensorflow.python.client import device_lib<br/>print(device_lib.list_local_devices())</span><span id="0b20" class="mu ke jg mq b gy mz mw l mx my">print(“TensorFlow version: “, tf.__version__)</span></pre><p id="ee88" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果一切正常，你会看到你正在使用 TensforFlow 2.0.0-alpha0，如果在谷歌的 Colab 上运行代码，你会看到一个名为特斯拉 T4 的漂亮的 GPU 机器。我喜欢检查设备列表，尤其是在 Colab 上，因为有时我会忘记更改运行时类型，所以这是一个提醒。在这种情况下，由于 tensorflow-gpu 下载，Colab 会将您放在 GPU 运行时上，否则默认为 CPU。</p><p id="d9ab" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">要在<a class="ae jd" href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>中运行代码，要么通过文件&gt;上传笔记本菜单直接在 Colab 网站上传笔记本，要么只需点击笔记本左上角的相应图标。</p><p id="0a70" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">要将文本下载到 Colab，请使用以下代码片段:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="559d" class="mu ke jg mq b gy mv mw l mx my">from google.colab import files</span><span id="8f5a" class="mu ke jg mq b gy mz mw l mx my">uploaded = files.upload()</span><span id="7d30" class="mu ke jg mq b gy mz mw l mx my">for fn in uploaded.keys():<br/>    print(‘User uploaded file “{name}” with length {length}    bytes’.format(name=fn, length=len(uploaded[fn])))</span></pre><p id="fd68" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">注意:Colab 中的“代码片段”是迷你解决方案的重要资源。</p><p id="9315" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">您将在“文件”选项卡下找到上传的文本文件，考虑到文件上传到笔记本本身时会通知您，您甚至不需要查找它。您可能知道，您在 Colab 中的工作和文件是短暂的，在您下次登录时就会消失，因此在您注销之前将您的工作保存到其他地方是很重要的。</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="342a" class="mu ke jg mq b gy mv mw l mx my">import os<br/>path = os.getcwd()</span><span id="2623" class="mu ke jg mq b gy mz mw l mx my">text = open(path + ‘/Iliad_v3.txt’,  ‘rb’).read().decode(encoding=’utf-8')<br/>print(“Text is {} characters long”.format(len(text)))</span></pre><p id="ac0b" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">文本应该有 886，809 个字符长，所以不是一个大样本。出于好奇，我们也可以查一下字数，这样我就知道高中的时候这篇课文是否值得跳过:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="96c5" class="mu ke jg mq b gy mv mw l mx my">words = [w for w in text.split(‘ ‘) if w.strip() != ‘’ or w == ‘\n’]<br/>print(“Text is {} words long”.format(len(words)))</span></pre><p id="e7a6" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">应该有 153，260 个单词，所以真的没有当时看起来那么长。</p><p id="65c1" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">为了确保机器正在读取预期的内容，请快速检查前 100 个字符:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="24b4" class="mu ke jg mq b gy mv mw l mx my">print(text[:100])</span></pre></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><pre class="mp mq mr ms aw mt bi"><span id="273a" class="mu ke jg mq b gy na nb nc nd ne mw l mx my">achilles' wrath, to greece the direful spring<br/>of woes unnumber'd, heavenly goddess, sing!<br/>that</span></pre><p id="6878" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">对，就是这样！</p><h1 id="e9bd" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">准备课文</h1><p id="073d" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">创建一个由唯一字符排序的向量—在此文本中应该有 34 个字符:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="39d4" class="mu ke jg mq b gy mv mw l mx my">vocab = sorted(set(text))<br/>print (‘There are {} unique characters’.format(len(vocab)))<br/>char2int = {c:i for i, c in enumerate(vocab)}<br/>int2char = np.array(vocab)<br/>print(‘Vector:\n’)<br/>for char,_ in zip(char2int, range(len(vocab))):<br/>    print(‘ {:4s}: {:3d},’.format(repr(char), char2int[char]))</span></pre><p id="85a5" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们来看一个数字映射示例，看看文本的数字表示:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="9f86" class="mu ke jg mq b gy mv mw l mx my">text_as_int = np.array([char2int[ch] for ch in text], dtype=np.int32)<br/>print (‘{}\n mapped to integers:\n {}’.format(repr(text[:100]), text_as_int[:100]))</span></pre><p id="3621" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">从文本中创建训练和验证数据(确保训练片段可被批量大小整除，在本例中为 64)，并查看形状是否符合预期:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="58ab" class="mu ke jg mq b gy mv mw l mx my">tr_text = text_as_int[:704000] <br/>val_text = text_as_int[704000:] </span><span id="5ed8" class="mu ke jg mq b gy mz mw l mx my">print(text_as_int.shape, tr_text.shape, val_text.shape)</span></pre><h1 id="cd55" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">建立模型</h1><p id="1127" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我喜欢将(大部分)可调参数放在一个地方，以便在需要进行多次调整时可以方便地访问:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="3654" class="mu ke jg mq b gy mv mw l mx my">batch_size = 64<br/>buffer_size = 10000<br/>embedding_dim = 256<br/>epochs = 50<br/>seq_length = 200<br/>examples_per_epoch = len(text)//seq_length<br/>#lr = 0.001 #will use default for Adam optimizer<br/>rnn_units = 1024<br/>vocab_size = len(vocab)</span></pre><p id="48a6" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">准备训练和验证数据集，然后检查形状:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="bbff" class="mu ke jg mq b gy mv mw l mx my">tr_char_dataset = tf.data.Dataset.from_tensor_slices(tr_text)<br/>val_char_dataset = tf.data.Dataset.from_tensor_slices(val_text)</span><span id="f09e" class="mu ke jg mq b gy mz mw l mx my">tr_sequences = tr_char_dataset.batch(seq_length+1, drop_remainder=True)<br/>val_sequences = val_char_dataset.batch(seq_length+1, drop_remainder=True)<br/>def split_input_target(chunk):<br/>    input_text = chunk[:-1]<br/>    target_text = chunk[1:]<br/>    return input_text, target_text</span><span id="d1a7" class="mu ke jg mq b gy mz mw l mx my">tr_dataset = tr_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)</span><span id="42c1" class="mu ke jg mq b gy mz mw l mx my">val_dataset = val_sequences.map(split_input_target).shuffle(buffer_size).batch(batch_size, drop_remainder=True)</span><span id="69fa" class="mu ke jg mq b gy mz mw l mx my">print(tr_dataset, val_dataset)</span></pre><p id="a681" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">最后，建立模型——我使用了两个 LSTM 层和辍学:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="a3a1" class="mu ke jg mq b gy mv mw l mx my">def build_model(vocab_size, embedding_dim, rnn_units, batch_size):<br/>     model = tf.keras.Sequential([<br/>     tf.keras.layers.Embedding(vocab_size, embedding_dim,<br/>     batch_input_shape=[batch_size, None]),<br/>     tf.keras.layers.Dropout(0.2),<br/>     tf.keras.layers.LSTM(rnn_units,<br/>     return_sequences=True,<br/>     stateful=True,<br/>     recurrent_initializer=’glorot_uniform’),<br/>     tf.keras.layers.Dropout(0.2), <br/>     tf.keras.layers.LSTM(rnn_units,<br/>     return_sequences=True,<br/>     stateful=True,<br/>     recurrent_initializer=’glorot_uniform’),<br/>     tf.keras.layers.Dropout(0.2),<br/>     tf.keras.layers.Dense(vocab_size)<br/> ])<br/> <br/>return model</span><span id="801a" class="mu ke jg mq b gy mz mw l mx my">model = build_model(<br/>    vocab_size = len(vocab),<br/>    embedding_dim=embedding_dim,<br/>    rnn_units=rnn_units,<br/>    batch_size=batch_size)</span></pre><p id="66ca" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">顺便说一句，如果在谷歌云平台(GCP)上设置这个模型，你可能会得到一个警告<em class="nf">"&lt;tensor flow . python . keras . layers . recurrent . unified lstm object…&gt;:注意，这个图层没有针对性能进行优化。请使用 tf.keras.layers.CuDNNLSTM 在 GPU 上获得更好的性能。</em>但是，CuDNNLSTM 不可用，因此不确定这是否是以前 TensorFlow 版本的剩余警告，而 LSTM 在 2.0 中已经针对性能进行了优化，或者 CuDNNLSTM 只是尚未可用。</p><h1 id="947f" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">运行模型</h1><p id="989e" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">检查输出形状和模型，并定义损耗:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="c6e3" class="mu ke jg mq b gy mv mw l mx my">model.summary()</span><span id="b9fb" class="mu ke jg mq b gy mz mw l mx my">for input_example_batch, target_example_batch in tr_dataset.take(1):<br/>    example_batch_predictions = model(input_example_batch)<br/>    print(example_batch_predictions.shape)</span><span id="73de" class="mu ke jg mq b gy mz mw l mx my">def loss(labels, logits):<br/>    return tf.keras.losses.sparse_categorical_crossentropy(labels,    logits, from_logits=True)</span><span id="865a" class="mu ke jg mq b gy mz mw l mx my">example_batch_loss  = loss(target_example_batch, example_batch_predictions)<br/>print("Loss:      ", example_batch_loss.numpy().mean())</span></pre><p id="0624" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们使用 Adam optimizer，并在验证错误没有改善的 10 个时期之后停止训练:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="013f" class="mu ke jg mq b gy mv mw l mx my">optimizer = tf.keras.optimizers.Adam()<br/>model.compile(optimizer=optimizer, loss=loss)<br/>patience = 10<br/>early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)</span></pre><p id="dad7" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">创建目录来保存我们的检查点，然后…运行！</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="2a43" class="mu ke jg mq b gy mv mw l mx my">checkpoint_dir = ‘./checkpoints’+ datetime.datetime.now().strftime(“_%Y.%m.%d-%H:%M:%S”)<br/>checkpoint_prefix = os.path.join(checkpoint_dir, “ckpt_{epoch}”)</span><span id="df3c" class="mu ke jg mq b gy mz mw l mx my">checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(<br/>    filepath=checkpoint_prefix,<br/>    save_weights_only=True)</span><span id="752e" class="mu ke jg mq b gy mz mw l mx my">history = model.fit(tr_dataset, epochs=epochs, callbacks=[checkpoint_callback, early_stop] , validation_data=val_dataset)</span><span id="0391" class="mu ke jg mq b gy mz mw l mx my">print (“Training stopped as there was no improvement after {} epochs”.format(patience))</span></pre><p id="266f" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在我的例子中，训练在第 25 个时期停止，这意味着验证误差的最后一次改善是在第 15 个时期。趋势(下图)是人们可以预期的:首先，验证误差甚至略好于训练误差(即，一个时期的验证损失是使用改进的训练模型计算的，因为它是在该时期的末尾，而训练损失是使用各批损失的平均值评估的——在开始时，这可能导致验证误差降低)，但随着时间的推移，训练和验证集出现分歧——训练误差持续下降，而验证变平，然后开始恶化:</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/f70e180eb6c1e5d75c2169daceea8f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1C6BelnTL944kW1luSHD3g.png"/></div></div></figure><h1 id="ded9" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">生成文本</h1><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/e63b17c5020d327b3b505fdd3d5aecde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ngvmD_Vl_osNhxpx"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@dearseymour?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ksenia Makagonova</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="9ce6" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们从最近的检查点加载权重(或者将 load.weights 行调整到任何其他检查点),并生成一个一千字符的文本:</p><pre class="ml mm mn mo gt mp mq mr ms aw mt bi"><span id="9fb6" class="mu ke jg mq b gy mv mw l mx my">model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)<br/>model.load_weights(tf.train.latest_checkpoint(checkpoint_dir)) <br/>model.build(tf.TensorShape([1, None]))</span><span id="5459" class="mu ke jg mq b gy mz mw l mx my">def generate_text(model, start_string):<br/>    <br/>    print('Generating with seed: "' + start_string + '"')<br/>  <br/>    num_generate = 1000</span><span id="d219" class="mu ke jg mq b gy mz mw l mx my">    input_eval = [char2int[s] for s in start_string]<br/>    input_eval = tf.expand_dims(input_eval, 0)</span><span id="3e99" class="mu ke jg mq b gy mz mw l mx my">    text_generated = []</span><span id="95d5" class="mu ke jg mq b gy mz mw l mx my">    temperature = 1.0</span><span id="c3d6" class="mu ke jg mq b gy mz mw l mx my">    model.reset_states()<br/>    for i in range(num_generate):<br/>        predictions = model(input_eval)<br/>        predictions = tf.squeeze(predictions, 0)<br/>        predictions = predictions / temperature<br/>        predicted_id = tf.random.categorical(predictions,      num_samples=1)[-1,0].numpy()<br/>        input_eval = tf.expand_dims([predicted_id], 0)<br/>        text_generated.append(int2char[predicted_id])</span><span id="0438" class="mu ke jg mq b gy mz mw l mx my">    return (start_string + ''.join(text_generated))</span><span id="4c32" class="mu ke jg mq b gy mz mw l mx my">print(generate_text(model, start_string="joy of gods"))</span></pre><p id="5bc4" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">以下是我得到的信息:</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/cedc2b380c1d34fdd62834897ee09433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tRagYjV6xUBGeEAxgdvaKQ.png"/></div></div></figure><p id="3828" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">对于不到一百万字符的文本样本的 10 分钟单 GPU 训练来说，这是一个相当好的输出！</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="7a2f" class="pw-post-body-paragraph lb lc jg ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">感谢您的阅读，我希望这段代码将成为您进一步探索文本生成的良好起点！</p></div></div>    
</body>
</html>