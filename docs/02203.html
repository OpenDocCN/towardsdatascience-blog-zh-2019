<html>
<head>
<title>Linear regression and a quality bottle of wine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归和一瓶优质葡萄酒</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-and-a-quality-bottle-of-wine-b053ab768a53?source=collection_archive---------9-----------------------#2019-04-11">https://towardsdatascience.com/linear-regression-and-a-quality-bottle-of-wine-b053ab768a53?source=collection_archive---------9-----------------------#2019-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b529" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个线性回归像品酒师一样预测一瓶酒的质量？请继续阅读，寻找答案。</h2></div><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/ce623f2c303918495928478d73aec64d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Tq0xF_xMpE3bWsfE"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Photo by <a class="ae lg" href="https://unsplash.com/@scottiewarman?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Scott Warman</a> on <a class="ae lg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="15ed" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">统计数据比我们想象的更能左右我们的生活。在一次糟糕的表现之后，你可能会表现得更好。这是弗朗西斯·高尔顿创造的一个术语，叫做“回归”。他比较了祖先和后代的身高，发现他们的身高更接近平均值。对于一个人在数学考试中的分数也可以做出同样的推论。如果你的数学考试成绩很差，那么根据回归分析，你应该在下一次数学考试中取得好成绩。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/5835daea5927c0c7e6401156be9988fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*qxFIvyxGWGyvoktcw2Cn3A.jpeg"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Bell curve</figcaption></figure><p id="01f5" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">钟形曲线表示样本 n 的正态分布，其中 u 是平均值。<em class="me">回归表示 X 轴上的数据倾向于向平均值回归。</em></p><p id="f7d7" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">今天，回归算法被用来解决众多问题，这些问题涉及从统计学(这是其母领域)到股票市场前沿人工智能技术的各种应用。也许回归的最佳用途是在数据分析领域。</p><blockquote class="mf mg mh"><p id="cea7" class="lh li me lj b lk ll ju lm ln lo jx lp mi lr ls lt mj lv lw lx mk lz ma mb mc im bi translated">同样的模型也可以用来预测葡萄酒的质量。遵循以下方程的广义线性回归:</p></blockquote><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/a2be84f42c3731e12e92f6c97bed7445.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*CONLDhxUSWVsOZVnnn2Eew.jpeg"/></div></figure><p id="1780" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">β0 是截距，β1…βn 是回归系数。现在，请记住，因为我们在这里使用多个变量，这意味着我们在多维超平面上解释数据。</p><blockquote class="mm"><p id="b517" class="mn mo it bd mp mq mr ms mt mu mv mc dk translated">广义线性回归假设因变量(本例中为 Y)与自变量(X1)具有线性关系..p)。因此，如果我们假设一个二维数据集(X1 和 Y)，它会像下面的图像。</p></blockquote><figure class="mx my mz na nb kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi mw"><img src="../Images/6061121f99036a718b7fbafe9733bfc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*87aMm1RRoaxS4Sy8Q-XMDg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Linear regression for one dependent variable and independent variable</figcaption></figure><p id="9831" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">记住<em class="me">‘红线’</em>是假设线，数据点是实际数据点。当模型拟合时，假设关系是线性的，这意味着假设数据拟合在<em class="me">红线附近。</em>首先，我们需要从 UCI 存储库中收集<a class="ae lg" href="https://archive.ics.uci.edu/ml/datasets/wine" rel="noopener ugc nofollow" target="_blank">数据集</a>。先决条件是您已经安装了<em class="me"> python、spyder </em>和下面指定的其他软件包:</p><p id="e967" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了提取数据，我们使用 pandas 并将我们的特征(自变量)分配给 X，将因变量分配给 y。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f91a6809c1c9da2cdf53e0d025052e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*5nE6Z1AsH8n0AVrpksiLAw.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Extracting variables using pandas</figcaption></figure><p id="a42f" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj iu"> <em class="me">相关性</em> </strong>是一种统计技术，可以显示变量对的相关程度。我们使用这些变量来绘制一张<em class="me">热图</em>，用来表示预测关系。我们发现每个特征是如何相互关联的。必须注意，特征之间的相关性并不意味着因果关系。例如，特征与其自身之间的<em class="me">相关系数(r) </em>等于 1。在所有其他情况下，相关性往往小于 1。相关系数也可以在特征之间具有相反的关系，在这种情况下，相关系数将等于-1。r 越接近+1 或-1，这两个变量的关系就越密切。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a5dad7b3cad3aa85c7fd07dbacf3d118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*g5grDZxYaTcs3idv-6sqdg.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Heat-map of both dependent and independent variables</figcaption></figure><p id="24f1" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们使用<em class="me"> seaborn </em>来绘制 x 中所有可用特性的特性与质量图。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/234e83ed99afd3ffe1aa35f10ec04bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*xhWB59y_ASqxjuWZpAXP6A.png"/></div></figure><div class="kr ks kt ku gt ab cb"><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/40cad0744d2a01f99f6784140a3b3d7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6K2ipPqU9V2CsLpqhQTFYw.png"/></div></figure><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/c1f349ed72fe20aa88eafbf833bfd666.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*8L0BDefj9EmPFwd2F8Qnag.png"/></div></figure><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/70674751750193cd508cf35b43f189db.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*OxOKmL8ZvC4OIOVungAaRw.png"/></div></figure></div><div class="ab cb"><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/986da0dabc6421a3fae6cb270c6d6966.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*XsQx0-9e7R9-gdyjDB_1wg.png"/></div></figure><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/53763603118043e99c3dc4a04d0a5065.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*VshspxZsin412dGF5CMGkg.png"/></div></figure><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/94173f5b040680ef35effea97b4b05e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*CT3YxRRRl-stMF-6XBqRng.png"/></div></figure></div><div class="ab cb"><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/b461969caa88488457c4d1c91b4d8bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*dv3QlHyTUqlHAYxqUlcRBw.png"/></div></figure><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/f90f16242995c1ca963bb1b49ff8ffa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*tXZZAG7uWxrWbNUrJpnlKg.png"/></div></figure><figure class="ne kv nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><img src="../Images/fb27783df4e5f2d30c48ca6b4b7426b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*kt7DLCgz1xEXyih8fDHn7A.png"/></div></figure></div><div class="ab cb"><figure class="ne kv nk ng nh ni nj paragraph-image"><img src="../Images/fb2838f5720d9a34d09ee0deaf8dbcaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*E5d5Rp75i5j3IbCAB4aG5A.png"/></figure><figure class="ne kv nk ng nh ni nj paragraph-image"><img src="../Images/a79caf17da0b437d818661cc46cd8926.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*hbFpdmDdZCAJQm7ws8phDw.png"/><figcaption class="lc ld gj gh gi le lf bd b be z dk nl di nm nn">Feature vs Quality plots for features of X</figcaption></figure></div><p id="ad78" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们已经<em class="me">假设</em>模型<em class="me">是线性的</em>。现在数据集需要分成训练和测试数据，为此我们使用了<em class="me"> Scikitlearn 的 model_selection </em>模块。之后我们剩下要做的就是训练我们的线性回归模块。可以通过<em class="me"> x_train </em>使用 LinearRegression 类对其进行训练，也可以通过<em class="me"> x_test </em>对其进行测试。我们使用拟合方法来确定数据的中心，因为数据已经缩放，我们在这里不应用变换方法。</p><p id="ffd8" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性模型的估计是借助均方根误差或均方根偏差来完成的。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9526cae0c1ac7f4c81265c3211610170.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*ySeyceSp_ymLIXjL7I6Syw.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">RMSE for sample n</figcaption></figure><p id="ac94" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性回归技术的<em class="me">精度</em>和<em class="me"> RMSE(均方根误差)</em>或<em class="me"> RMSD </em>(均方根偏差)用于决定算法的平滑程度。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/8ad482993ff5851f7c93ae9160c4622a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Hly-rVRCuxZ5jvDZt_gdgQ.png"/></div></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/723b657a110458838f05735b7879a9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*yTQp04vV0rbzvQ5F1cuszA.png"/></div></figure><p id="44ac" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以 RMSE 是 0.4，老实说，也很糟糕，这个模型的精度只有 34.4%。这个模型突出了一些主要的瓶颈，这些瓶颈是在人工智能的早期科学家们无法根据质量对特征进行分类的时候研究人员遇到的。一些特征与因变量 Y 有很强的相关性，但这并不意味着该特征会导致 Y 的改善。也许，这就是我们在方法中所遗漏的。我们采用这些特征，并认为因为热图暗示了一种关系，所以该特征一定很强，但<em class="me">相关性并不暗示因果关系</em>。</p><p id="0b6a" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="me">酒的质量是一个</em><strong class="lj iu"><em class="me"/></strong>的定性变量，这也是算法没有做好的另一个原因。重要的是要注意，线性回归模型与定量方法相对比，与定性方法更好。当我们不能决定一个变量是否比另一个更好时，变量可以被归类为定性变量。比如:蓝色是否大于绿色？不。这意味着像决策树、随机森林这样的算法可以很好地用于葡萄酒质量的分类。让我们把它们留到下次吧。</p><p id="76fc" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="me">干杯！</em></p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="a6f3" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="me">注意:作者不拥有 UCI 知识库提供的数据集。作者感谢数据集的所有者保持数据集开源。</em></p></div></div>    
</body>
</html>