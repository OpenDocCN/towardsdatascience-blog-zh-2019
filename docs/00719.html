<html>
<head>
<title>Implementing multi-class text classification with Doc2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Doc2Vec 实现多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d?source=collection_archive---------9-----------------------#2019-02-03">https://towardsdatascience.com/implementing-multi-class-text-classification-with-doc2vec-df7c3812824d?source=collection_archive---------9-----------------------#2019-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/8ec03ba38a32452bae40bf7c63f88d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VgIFaKQN-JBaf7J19AWqWg.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Image by <a class="ae jd" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1668918" rel="noopener ugc nofollow" target="_blank">Gerd Altmann </a>from <a class="ae jd" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1668918" rel="noopener ugc nofollow" target="_blank">Pixabay</a> (<a class="ae jd" href="https://pixabay.com/illustrations/film-negative-photographs-slides-1668918/" rel="noopener ugc nofollow" target="_blank">Image Link</a>)</figcaption></figure><div class=""/><h1 id="7454" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="8a7f" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在本文中，您将学习如何在使用 Doc2Vec 表示文档时将文本文档分类到不同的类别。我们将通过一个简单易懂的例子来了解这一点，这个例子使用 Doc2vec 作为特征表示，使用逻辑回归作为分类算法，通过流派对电影情节进行分类。<a class="ae jd" href="https://github.com/RaRe-Technologies/movie-plots-by-genre" rel="noopener ugc nofollow" target="_blank"> <strong class="ld jh">电影数据集</strong> </a>包含简短的电影情节描述，它们的标签代表类型。数据集中有六个<strong class="ld jh">流派</strong>:</p><ol class=""><li id="d4ad" class="lz ma jg ld b le mb li mc lm md lq me lu mf ly mg mh mi mj bi translated">科幻小说</li><li id="859f" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">行动</li><li id="3d01" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">喜剧</li><li id="a9e1" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">幻想</li><li id="14fa" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">动画</li><li id="bcb6" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">浪漫性</li></ol><h1 id="3424" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">什么是 Doc2Vec，我们为什么需要它？</h1><p id="ffa5" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">那么，为什么要选择<strong class="ld jh"> doc2vec </strong>表示法而不是使用广为人知的 bag-of-words( <strong class="ld jh"> BOW </strong>)方法呢？对于复杂的文本分类算法，<strong class="ld jh"> BOW </strong>不适合，因为它缺乏捕捉文本中单词的语义和句法顺序的能力。因此，将它们用作机器学习算法的特征输入不会产生显著的性能。另一方面，Doc2Vec 能够检测单词之间的关系，并理解文本的语义。Doc2Vec 是一种无监督算法，它为段落/文档/文本学习固定长度的特征向量。为了理解<strong class="ld jh"> doc2vec </strong>的基本工作，需要理解<strong class="ld jh"> word2vec </strong>如何工作，因为它使用相同的逻辑，除了文档特定向量是添加的特征向量。关于这个的更多细节，你可以阅读这个<a class="ae jd" href="https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e" rel="noopener">博客</a>。现在我们知道了为什么使用它，以及 doc2vec 将如何在这个程序中使用，我们可以进入下一个阶段，实际实现分类器。</p><p id="aa3d" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">让我们开始构建一个电影情节分类器吧！！</p><h1 id="13ce" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">实施</strong></h1><p id="5a29" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这涉及到以下三个主要部分:</p><ol class=""><li id="d255" class="lz ma jg ld b le mb li mc lm md lq me lu mf ly mg mh mi mj bi translated">加载和准备文本数据</li><li id="80f0" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">使用<strong class="ld jh"> doc2vec </strong>模型获取特征向量</li><li id="2a68" class="lz ma jg ld b le mk li ml lm mm lq mn lu mo ly mg mh mi mj bi translated">训练分类器</li></ol><p id="145f" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">以下是输入 csv 文件的前三行:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="da86" class="nb ke jg mx b gy nc nd l ne nf">,movieId,plot,tag</span><span id="3eea" class="nb ke jg mx b gy ng nd l ne nf">0,1,"A little boy named Andy loves to be in his room, playing with his toys, especially his doll named ""Woody"". But, what do the toys do when Andy is not with them, they come to life. Woody believes that he has life (as a toy) good. However, he must worry about Andy's family moving, and what Woody does not know is about Andy's birthday party. Woody does not realize that Andy's mother gave him an action figure known as Buzz Lightyear, who does not believe that he is a toy, and quickly becomes Andy's new favorite toy. Woody, who is now consumed with jealousy, tries to get rid of Buzz. Then, both Woody and Buzz are now lost. They must find a way to get back to Andy before he moves without them, but they will have to pass through a ruthless toy killer, Sid Phillips.",animation</span><span id="9454" class="nb ke jg mx b gy ng nd l ne nf">1,2,"When two kids find and play a magical board game, they release a man trapped for decades in it and a host of dangers that can only be stopped by finishing the game.",fantasy</span></pre><p id="09d2" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">导入所需的库:</p><p id="8c21" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">我们使用多处理技术来利用所有内核，以便通过 Doc2Vec 进行更快的训练。tqdm 包用于在训练时显示进度条。我们对 Doc2Vec 使用 gensim 包。出于分类目的，使用来自 scikit-learn 的逻辑回归。NLTK 包用于标记化任务。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8390" class="nb ke jg mx b gy nc nd l ne nf">from gensim.test.utils import common_texts<br/>from gensim.models.doc2vec import Doc2Vec, TaggedDocument<br/>from sklearn.metrics import accuracy_score, f1_score<br/>from sklearn.model_selection import train_test_split</span><span id="a01c" class="nb ke jg mx b gy ng nd l ne nf">from sklearn.linear_model import LogisticRegression<br/>from sklearn import utils<br/>import csv<br/>from tqdm import tqdm<br/>import multiprocessing</span><span id="7109" class="nb ke jg mx b gy ng nd l ne nf">import nltk<br/>from nltk.corpus import stopwords</span></pre><h2 id="848f" class="nb ke jg bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">1.阅读和准备文本数据</h2><p id="eddc" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">以下代码用于从 csv 读取数据，并用于标记化功能，在创建培训和测试文档作为<strong class="ld jh"> doc2vec </strong>模型的输入时，将使用这些代码。数据有 2448 行，我们选择前 2000 行用于训练，其余的用于测试。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0737" class="nb ke jg mx b gy nc nd l ne nf">tqdm.pandas(desc="progress-bar")</span><span id="b75b" class="nb ke jg mx b gy ng nd l ne nf"># Function for tokenizing</span><span id="dc0f" class="nb ke jg mx b gy ng nd l ne nf">def tokenize_text(text):<br/>    tokens = []<br/>    for sent in nltk.sent_tokenize(text):<br/>        for word in nltk.word_tokenize(sent):<br/>            if len(word) &lt; 2:<br/>                continue<br/>            tokens.append(word.lower())<br/>    return tokens</span><span id="cd8f" class="nb ke jg mx b gy ng nd l ne nf"># Initializing the variables</span><span id="b8e2" class="nb ke jg mx b gy ng nd l ne nf">train_documents = []<br/>test_documents = []<br/>i = 0</span><span id="614a" class="nb ke jg mx b gy ng nd l ne nf"># Associating the tags(labels) with numbers</span><span id="66e9" class="nb ke jg mx b gy ng nd l ne nf">tags_index = {'sci-fi': 1 , 'action': 2, 'comedy': 3, 'fantasy': 4, 'animation': 5, 'romance': 6}</span><span id="828f" class="nb ke jg mx b gy ng nd l ne nf">#Reading the file</span><span id="9fb4" class="nb ke jg mx b gy ng nd l ne nf">FILEPATH = 'data/tagged_plots_movielens.csv'<br/>with open(FILEPATH, 'r') as csvfile:<br/>with open('data/tagged_plots_movielens.csv', 'r') as csvfile:<br/>    moviereader = csv.reader(csvfile, delimiter=',', quotechar='"')<br/>    for row in moviereader:<br/>        if i == 0:<br/>            i += 1<br/>            continue<br/>        i += 1<br/>        if i &lt;= 2000:</span><span id="b05e" class="nb ke jg mx b gy ng nd l ne nf">            train_documents.append(          TaggedDocument(words=tokenize_text(row[2]), tags=[tags_index.get(row[3], 8)] ))</span><span id="8c0e" class="nb ke jg mx b gy ng nd l ne nf">        else:<br/>            test_documents.append( TaggedDocument(words=tokenize_text(row[2]),<br/> tags=[tags_index.get(row[3], 8)]))</span><span id="2927" class="nb ke jg mx b gy ng nd l ne nf">print(train_documents[0])</span></pre><p id="8817" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">第一行的 training_document 输出是 TaggedDocument 对象。这将标记显示为 TaggedDocument 的第一个参数，labelID 显示为第二个参数(5: Animation)。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8bda" class="nb ke jg mx b gy nc nd l ne nf">TaggedDocument(['little', 'boy', 'named', 'andy', 'loves', 'to', 'be', 'in', 'his', 'room', 'playing', 'with', 'his', 'toys', 'especially', 'his', 'doll', 'named', '``', 'woody', "''", 'but', 'what', 'do', 'the', 'toys', 'do', 'when', 'andy', 'is', 'not', 'with', 'them', 'they', 'come', 'to', 'life', 'woody', 'believes', 'that', 'he', 'has', 'life', 'as', 'toy', 'good', 'however', 'he', 'must', 'worry', 'about', 'andy', "'s", 'family', 'moving', 'and', 'what', 'woody', 'does', 'not', 'know', 'is', 'about', 'andy', "'s", 'birthday', 'party', 'woody', 'does', 'not', 'realize', 'that', 'andy', "'s", 'mother', 'gave', 'him', 'an', 'action', 'figure', 'known', 'as', 'buzz', 'lightyear', 'who', 'does', 'not', 'believe', 'that', 'he', 'is', 'toy', 'and', 'quickly', 'becomes', 'andy', "'s", 'new', 'favorite', 'toy', 'woody', 'who', 'is', 'now', 'consumed', 'with', 'jealousy', 'tries', 'to', 'get', 'rid', 'of', 'buzz', 'then', 'both', 'woody', 'and', 'buzz', 'are', 'now', 'lost', 'they', 'must', 'find', 'way', 'to', 'get', 'back', 'to', 'andy', 'before', 'he', 'moves', 'without', 'them', 'but', 'they', 'will', 'have', 'to', 'pass', 'through', 'ruthless', 'toy', 'killer', 'sid', 'phillips'], [5])</span></pre><h2 id="c405" class="nb ke jg bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">2.从 doc2vec 模型中获取特征向量</h2><p id="7aa7" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">接下来，我们初始化<a class="ae jd" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank"> gensim doc2vec 模型</a>并训练 30 个时期。这个过程非常简单。Doc2Vec 体系结构也有两个类似 word2vec 的算法，它们是这两个算法的对应算法，即“连续单词包”(CBOW)和“Skip-Gram”(SG)。doc2vec 中的一种算法称为段落向量分布单词袋(PV-DBOW ),它类似于 word2vec 中的 SG 模型，只是增加了额外的段落 id 向量。这里训练神经网络来预测给定段落中周围单词的向量和基于段落中给定单词的段落 id 向量。第二种算法是段落向量(PV-DM)，类似于词向量中的 CBOW。</p><p id="e4b2" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><strong class="ld jh"> doc2vec </strong>模型的几个重要参数包括:</p><p id="ac17" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><code class="fe ns nt nu mx b">dm</code> ({0，1}，可选)1: PV-DM，0: PV-DBOW</p><p id="41a4" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><code class="fe ns nt nu mx b">vector_size</code>特征向量的维数(我们选择 300)</p><p id="3748" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><code class="fe ns nt nu mx b">workers</code>这是我们分配了核心数的线程数</p><p id="ee0e" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">其余参数详情可在<a class="ae jd" href="https://radimrehurek.com/gensim/models/doc2vec.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。初始化之后，我们使用<code class="fe ns nt nu mx b">train_documents</code>构建词汇表</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5107" class="nb ke jg mx b gy nc nd l ne nf">cores = multiprocessing.cpu_count()<br/><br/>model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)<br/>model_dbow.build_vocab([x for x in tqdm(train_documents)])</span><span id="c477" class="nb ke jg mx b gy ng nd l ne nf">train_documents  = utils.shuffle(train_documents)<br/>model_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)</span><span id="e45a" class="nb ke jg mx b gy ng nd l ne nf">def vector_for_learning(model, input_docs):<br/>    sents = input_docs<br/>    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])<br/>    return targets, feature_vectors</span><span id="442c" class="nb ke jg mx b gy ng nd l ne nf">model_dbow.save('./movieModel.d2v')</span></pre><h2 id="2c42" class="nb ke jg bd kf nh ni dn kj nj nk dp kn lm nl nm kr lq nn no kv lu np nq kz nr bi translated">3.训练分类器</h2><p id="74cf" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">最后，使用上述特征向量构建函数训练逻辑回归分类器。这里，我们在训练时使用为<code class="fe ns nt nu mx b">train_documents</code>生成的特征向量，并在预测阶段使用<code class="fe ns nt nu mx b">test_documents</code>的特征向量。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="3b42" class="nb ke jg mx b gy nc nd l ne nf">y_train, X_train = vector_for_learning(model_dbow, train_documents)<br/>y_test, X_test = vector_for_learning(model_dbow, test_documents)<br/><br/>logreg = LogisticRegression(n_jobs=1, C=1e5)<br/>logreg.fit(X_train, y_train)<br/>y_pred = logreg.predict(X_test)</span><span id="5dd1" class="nb ke jg mx b gy ng nd l ne nf">print('Testing accuracy for movie plots%s' % accuracy_score(y_test, y_pred))<br/>print('Testing F1 score for movie plots: {}'.format(f1_score(y_test, y_pred, average='weighted')))</span></pre><p id="74c5" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated"><code class="fe ns nt nu mx b">dm=1</code>时的输出如下:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9e05" class="nb ke jg mx b gy nc nd l ne nf">Testing accuracy 0.42316258351893093<br/>Testing F1 score: 0.41259684559985876</span></pre><p id="3267" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">这种准确性只能通过很短的文本的少量记录来获得，因此，这可以通过添加更好的功能来提高，如 n-grams，使用停用词来消除噪声。</p><h1 id="55b3" class="kd ke jg bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">进一步发展</strong></h1><p id="1e6f" class="pw-post-body-paragraph lb lc jg ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在此基础上，您可以更容易地使用其他数据集进行实验，并且更改 doc2vec 的算法就像更改<code class="fe ns nt nu mx b">dm</code>参数一样简单。希望这有助于您使用 Doc2Vec 开始您的第一个文本分类项目！</p><p id="c798" class="pw-post-body-paragraph lb lc jg ld b le mb lg lh li mc lk ll lm mp lo lp lq mq ls lt lu mr lw lx ly ij bi translated">作者:<a class="ae jd" href="https://www.linkedin.com/in/dipika-baad-154a2858/" rel="noopener ugc nofollow" target="_blank">迪皮卡·巴德</a></p></div></div>    
</body>
</html>