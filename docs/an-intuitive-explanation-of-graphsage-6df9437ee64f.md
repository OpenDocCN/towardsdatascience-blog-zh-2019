# GraphSAGE 的直观解释

> 原文：<https://towardsdatascience.com/an-intuitive-explanation-of-graphsage-6df9437ee64f?source=collection_archive---------6----------------------->

## 归纳学习在动态数据集中很有用。这里我们讨论一个关于图的归纳学习算法。

# 介绍

在之前的故事中，我们谈到了 DeepWalk，一种学习节点表示的算法。如果不熟悉 DeepWalk，可以查一下之前的故事。

[](https://medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72) [## 对 DeepWalk 的直观解释

### 网络上的机器学习有明显的优势。在这个故事中，我们讨论 DeepWalk 来学习节点嵌入。

medium.com](https://medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72) 

DeepWalk 是一种*直推式*算法，这意味着，它需要整个图都可以用来学习节点的嵌入。因此，当一个新节点被添加到现有节点时，需要重新运行它来为新节点生成嵌入。

在这个故事中，我们介绍了 GraphSAGE[1]，一种适用于动态图的表示学习技术。GraphSAGE 能够预测新节点的嵌入，而不需要重新训练过程。为此，GraphSAGE 学习了*聚合器函数*，该函数可以在给定新节点的特征和邻域的情况下诱导新节点的嵌入。这叫做*归纳学习。*

我们可以将 GraphSAGE 分为三个主要部分，即*上下文构建、信息聚合、*和*损失函数。*下面我们分别描述各个部分。

# 语境建构

与 word2vec 和 DeepWalk 类似，GraphSAGE 也有一个基于上下文的相似性假设。

> GraphSAGE 假设位于相同邻域的节点应该具有相似的嵌入。

与 DeepWalk 类似，上下文的定义是参数化的。该算法具有控制邻域深度的参数 *K* 。如果 *K* 为 1，则只有相邻节点被认为是相似的。如果 *K* 为 2，则距离为 2 的节点也会出现在相同的邻域中。

注意， *k* = 2 意味着距离为 4 的节点可以通过中间的节点影响彼此的嵌入。因此，增加步行长度会在节点之间引入不期望的信息共享。K*太大*甚至会导致所有节点都有相同的嵌入！

![](img/33a16917cfc0ff71a51f11b6ca9e762c.png)

Neighborhood exploration and information sharing in GraphSAGE. [1]

# 信息聚合

定义了邻居之后，现在我们需要邻居之间的信息共享程序。*聚合函数或聚合器*接受一个邻域作为输入，并将每个邻域的嵌入与权重相结合，以创建一个邻域嵌入。换句话说，它们聚集了来自节点邻居的信息。根据函数，聚合器权重可以是学习的，也可以是固定的。

为了学习聚合器的嵌入，我们首先初始化所有节点到节点特性的嵌入。依次地，对于直到 *K，*的每个邻域深度，我们用聚合器函数为每个节点创建邻域嵌入，并将其与节点的现有嵌入连接。我们通过神经网络层传递连接的向量来更新节点嵌入。当每个节点被处理时，我们归一化嵌入以具有单位范数。伪代码可以在下面找到。

![](img/5e5fbee97ddf0b565e39a4ebaed9d73f.png)

Pseudocode of GraphSAGE algorithm. [1]

学习聚合器函数来生成节点嵌入而不是学习嵌入本身的优点是归纳性。

> 当聚集器权重被学习时，一个看不见的节点的嵌入可以从它的特征和邻域中产生。

结果，当新的节点被引入到图中时，聚合器消除了重新训练的必要性。请注意，这在社交网络、web、引用网络等等中相当普遍。

# 损失函数

到目前为止，我们已经描述了生成节点嵌入的过程。然而，为了学习聚合器和嵌入的权重，我们需要一个可微分的损失函数。基于我们的直觉，我们希望相邻节点具有相似的嵌入，独立节点具有较远的嵌入向量。下面的函数用两项满足这两个条件。

![](img/abc33170e218c463f8b0c75e9b0866d6.png)

Loss function of GraphSAGE. [1]

这里 *u* 和 *v* 是两个邻居，计算的损失为 *u* 。第一项促进了我们所期望的最大化 *u* 和 *v* 嵌入的相似性。在第二项中，我们有一个变量 ***Q*** *，*它是负样本的数量，***是从负样本分布中抽取的负样本。在这种情况下，负样本意味着非邻居节点。这个术语试图将这两个节点的嵌入分开。最后， *σ* 通常用于表示 sigmoid 函数。***

***请注意，这是一个无监督的损失函数，可以在没有标签的情况下最小化。要在受监督的上下文中使用 GraphSAGE，我们有两种选择。我们可以首先学习节点嵌入，然后学习嵌入和标签之间的映射，或者我们可以在损失函数中添加监督损失项，并采用端到端的学习过程。这种灵活性很有价值。***

# ***关于聚合器的更多信息***

***GraphSAGE 的归纳性归功于它的聚合函数。我们可以定义各种参数或非参数的聚合器。作为一个非参数函数，我们可以使用简单的平均。换句话说，我们可以平均邻域中所有节点的嵌入来构造邻域嵌入。***

***参数函数可以是 LSTM 单元。然而，LSTM 细胞是为顺序操作而设计的，并且有记忆。因此，邻居被馈送到 LSTM 的顺序会影响邻居嵌入，尽管没有明显的顺序。为了缓解这种情况，可以将节点的随机排列提供给 LSTM。当损失函数最小化时，LSTM 参数将被学习。***

***另一个可学习的聚合器是一个单层神经网络，后面跟一个最大池算子。为了做到这一点，我们从一个非线性层传递每个邻居的嵌入，并对它们的结果应用元素式的最大值运算。在本文中，通过实验证明该功能是最有前途的功能。***

***虽然可以设计更复杂的聚合器，但简单是可取的，因为聚合器会显著影响训练时间。理想的聚合器应该是简单的、可学习的和对称的。换句话说，它应该学会如何聚合邻居嵌入，并且对邻居顺序漠不关心，同时不产生巨大的训练开销。***

# ***结论***

***GraphSAGE 是一种归纳表示学习算法，对于随时间增长的图形尤其有用。与转导技术相比，使用 GraphSAGE 为新节点创建嵌入要快得多。此外，GraphSAGE 不会因速度而影响性能。在需要节点分类、节点聚类和跨图泛化的三个不同数据集上进行了测试，结果优于现有解决方案。***

***如今，GraphSAGE 已经扩展到异构网络，并且有了新的归纳方法。然而，GraphSAGE 在归纳图表示学习中发挥了开拓性和影响力的作用。***

## ***参考***

***[1] Hamilton，Will，之桃·英，Jure Leskovec。"大型图上的归纳表示学习."神经信息处理系统的进展。2017.***