# å˜¿ï¼Œæ¨¡ç‰¹ï¼Œä½ ä¸ºä»€ä¹ˆè¯´è¿™æ˜¯åƒåœ¾é‚®ä»¶ï¼Ÿ

> åŸæ–‡ï¼š<https://towardsdatascience.com/hey-model-why-do-you-say-this-is-spam-7c945cc531f?source=collection_archive---------16----------------------->

## å°†åŸå› é™„åŠ åˆ°æ¨¡å‹é¢„æµ‹

![](img/d0ec9de42d94ed30b3c36ec0d5700a6e.png)

Shapley å€¼åœ¨æœºå™¨å­¦ä¹ ä¸­ç”¨äºè§£é‡Šå¤æ‚é¢„æµ‹æ¨¡å‹çš„é¢„æµ‹ï¼Œ*åˆå*â€œé»‘ç›’â€ã€‚åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ Shapley å€¼æ¥ç¡®å®š YouTube è¯„è®ºçš„å…³é”®æœ¯è¯­ï¼Œè¿™äº›æœ¯è¯­è§£é‡Šäº†ä¸ºä»€ä¹ˆä¸€ä¸ªè¯„è®ºè¢«é¢„æµ‹æ¨¡å‹é¢„æµ‹ä¸ºåƒåœ¾æˆ–åˆæ³•ã€‚ç‰¹å®šå…³é”®æœ¯è¯­çš„â€œè”ç›Ÿâ€å¯ä»¥è¢«è®¤ä¸ºæ˜¯æ¨¡å‹è¿”å›ç»™å®šé¢„æµ‹çš„â€œåŸå› â€ã€‚æ­¤å¤–ï¼Œæˆ‘å°†ä½¿ç”¨èšç±»æ¥è¯†åˆ«åœ¨å¦‚ä½•ä½¿ç”¨è¿™äº›å…³é”®æœ¯è¯­æ–¹é¢æœ‰ç›¸ä¼¼ä¹‹å¤„çš„è¯„è®ºç»„ã€‚æœ€åï¼Œæˆ‘å°†è¿›ä¸€æ­¥æ¦‚æ‹¬é¢„æµ‹åŸå› ï¼Œä»¥ä¾¿ä½¿ç”¨ä»£è¡¨åŸå› ç±»åˆ«çš„æ›´å°‘å…³é”®æœ¯è¯­çš„å­—å…¸å¯¹è¯„è®ºç»„è¿›è¡Œåˆ†ç±»ã€‚

## åºæ–‡

éµå¾ªè¿™ç¯‡æ–‡ç« ä¸­çš„ä»£ç ç‰‡æ®µéœ€è¦ Python å’Œ rã€‚åŒ…å«æ‰€æœ‰ä»£ç ç‰‡æ®µçš„å®Œæ•´ Jupyter ç¬”è®°æœ¬å¯ä»¥åœ¨[è¿™é‡Œ](https://gist.github.com/alessiot/769ebe433adf79725b08687cf889f4cb)æ‰¾åˆ°ã€‚è¿è¡Œä»£ç æ‰€éœ€çš„ Python åº“å¦‚ä¸‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ *rpy2* åº“åœ¨ Python ä¸­è¿è¡Œ R çš„å®ä¾‹ã€‚

```
import rpy2.ipythonfrom rpy2.robjects import pandas2ripandas2ri.activate()%reload_ext rpy2.ipythonfrom sklearn import model_selection, preprocessing, metrics, svm
from sklearn.feature_extraction.text import CountVectorizer
from sklearn import decomposition, ensembleimport pandas as pd
import numpy as npimport stringimport matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inlineimport xgboostimport shap, time
```

## *æ²™æ™®åˆ©æ³•*

æ²™æ™®åˆ©å€¼æ˜¯åœ¨åˆä½œåšå¼ˆç†è®ºä¸­è¿›è¡Œçš„ä¸€é¡¹ç ”ç©¶çš„ç»“æœã€‚ä»–ä»¬å‘Šè¯‰å¦‚ä½•åœ¨ç©å®¶ä¹‹é—´å…¬å¹³åˆ†é…â€œå¥–é‡‘â€ã€‚å½“ä½¿ç”¨å…ˆå‰è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹æ•°æ®å®ä¾‹çš„æ ‡ç­¾æ—¶ï¼Œå¯ä»¥é€šè¿‡å‡è®¾æ•°æ®å®ä¾‹çš„æ¯ä¸ªç‰¹å¾å€¼æ˜¯æ¸¸æˆä¸­çš„ç©å®¶æ¥è§£é‡Šè¯¥é¢„æµ‹ï¼Œå…¶ä¸­è¯¥é¢„æµ‹æ˜¯æ”¯å‡ºã€‚Shapley å€¼æ˜¯æ‰€æœ‰å¯èƒ½çš„è¦ç´ ç»„åˆä¸­æŸä¸ªè¦ç´ å€¼çš„å¹³å‡è¾¹é™…è´¡çŒ®ã€‚Christoph Molnar çš„åœ¨çº¿ä¹¦ç±æä¾›äº†æ›´å¤šçš„ç»†èŠ‚ã€‚

æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥ä»‹ç» Shapley æ–¹æ³•:XOR è¡¨ã€‚æˆ‘ä»¬å°†åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­è¿›è¡Œæ›´è¯¦ç»†çš„è®¨è®ºã€‚

```
data = {'F1':[0,0,1,1], 'F2':[0,1,0,1], "Target":[0,1,1,0]} #XOR
df = pd.DataFrame(data)X = df[["F1","F2"]]
y = df["Target"].values.tolist()df
```

![](img/b57b06f7481358643412ee28449bfc45.png)

ä¸ºäº†è®¡ç®— Shapley å€¼ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è®­ç»ƒä¸€ä¸ªæ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨å¾„å‘åŸºå‡½æ•°æ ¸çš„ SVMã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨ Python åº“ *shap* è®¡ç®— Shapley å€¼ã€‚

```
clf = sklearn.svm.SVC(kernel='rbf', probability=False).fit(X, y) df["Prediction"] = clf.predict(X)# Explaining the probability prediction results
explainer = shap.KernelExplainer(clf.predict, X)
shap_values = explainer.shap_values(X)pd.concat([df, pd.DataFrame(shap_values, 
           columns='shap_'+ X.columns.values)], axis=1)
```

![](img/405ada786d1e1553268f57ca21e8ffaa.png)

ä»ä¸Šè¿°ç»“æœå¯ä»¥çœ‹å‡ºï¼Œå½“æ¯ä¸ªæ•°æ®å®ä¾‹çš„ä¸¤ä¸ªç‰¹å¾çš„è®¡ç®—è´¡çŒ®(Shapley å€¼)éƒ½ä¸ºè´Ÿæ—¶ï¼Œé¢„æµ‹ä¸º 0ï¼Œå½“ Shapley å€¼éƒ½ä¸ºæ­£æ—¶ï¼Œé¢„æµ‹ä¸º 1ã€‚è¯·æ³¨æ„ï¼Œè¿™ä¸¤ä¸ªè´¡çŒ®åŠ èµ·æ¥å°±æ˜¯è¯¥å®ä¾‹*çš„é¢„æµ‹ä¸é¢„æœŸé¢„æµ‹ğ¸(ğ‘“ä¹‹é—´çš„åˆå§‹å·®å€¼ï¼Œå®ƒæ˜¯å®é™…ç›®æ ‡å€¼çš„å¹³å‡å€¼ï¼Œå¦‚ä¸‹æ‰€ç¤º:*

![](img/28c9ea688f3374a250b97fb1a9a5b8c7.png)

## YouTube åƒåœ¾è¯„è®º

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ•°æ®é›†ï¼Œä» 5 ä¸ªä¸åŒçš„ YouTube è§†é¢‘ä¸­æ”¶é›† [1956 æ¡è¯„è®ºã€‚è¿™äº›è¯„è®ºæ˜¯é€šè¿‡ YouTube API ä» 2015 å¹´ä¸ŠåŠå¹´ YouTube ä¸Šè§‚çœ‹æ¬¡æ•°æœ€å¤šçš„åä¸ªè§†é¢‘ä¸­çš„äº”ä¸ªæ”¶é›†çš„ã€‚æ•°æ®é›†åŒ…å«è¢«æ ‡è®°ä¸ºåˆæ³•é‚®ä»¶æˆ–åƒåœ¾é‚®ä»¶çš„éç¼–ç é‚®ä»¶ã€‚ç”±äºæˆ‘å°†åœ¨æœ¬æ–‡ç¨åå†æ¬¡ä½¿ç”¨ Rï¼Œæˆ‘å†³å®šä½¿ç”¨æˆ‘åœ¨æœç´¢æ•°æ®é›†æ—¶æ‰¾åˆ°çš„](http://www.dt.fee.unicamp.br/~tiago//youtubespamcollection/) [R ç‰‡æ®µ](https://github.com/christophM/interpretable-ml-book/blob/master/R/get-SpamTube-dataset.R)ä¸‹è½½æ•°æ®é›†ã€‚

ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¼šå°† csv æ ¼å¼çš„æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°æ–‡ä»¶å¤¹ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨*ç†ŠçŒ«*å›¾ä¹¦é¦†æ¥æ¢ç´¢å†…å®¹ã€‚

```
youtube_data = pd.read_csv("youtube_data/TubeSpam.csv")
youtube_data.head()
```

![](img/d059b4b1043defddae225625cdefa1a8.png)

æˆ‘ä»¬å¯ä»¥é€‰æ‹©ç¨åå°†ç”¨äºå»ºæ¨¡çš„åˆ—å†…å®¹å’Œç±»ï¼Œå¹¶åˆ é™¤ç¼ºå°‘å†…å®¹çš„è¡Œã€‚è¿è¡Œä¸‹é¢çš„ä»£ç ç‰‡æ®µå°†è¿”å› 1954 è¡Œï¼Œå¹¶æ˜¾ç¤ºæ ‡ç­¾ 0(åˆæ³•è¯„è®º)å’Œ 1(åƒåœ¾è¯„è®º)åœ¨åˆ é™¤é‡å¤è¡Œåå‡ºç°çš„æ¬¡æ•°å¤§è‡´ç›¸åŒã€‚

```
youtube_df = pd.DataFrame()
youtube_df['text'] = youtube_data['CONTENT']
youtube_df['label'] = youtube_data['CLASS']
youtube_df.dropna(inplace=True)youtube_df.groupby('label').describe()
```

![](img/a508976d1b7d0862d960a04abfcf7dec.png)

```
youtube_df.drop_duplicates(inplace=True)
```

![](img/e4b3162f1a297dd8e6c88435a2231396.png)

\

æˆ‘ä»¬å¯ä»¥çœ‹çœ‹åˆæ³•è¯„è®ºå’Œåƒåœ¾è¯„è®ºåœ¨é•¿åº¦ä¸Šæ˜¯å¦æœ‰æ˜æ˜¾çš„åŒºåˆ«

```
youtube_df['length'] = youtube_df['text'].apply(len)
youtube_df.hist(column='length', by ='label', bins=50, figsize = (10,4))
```

![](img/5e39ca407dc4e94e8c767824f005334f.png)

æ˜¾ç„¶ï¼Œåƒåœ¾è¯„è®ºå¹³å‡æ¥è¯´è¦é•¿ä¸€äº›ã€‚æœ€é•¿çš„æ³¨é‡Šä¹‹ä¸€æ˜¯ï¼Œä¾‹å¦‚:

```
youtube_df[youtube_df['length'] == 1077]['text'].iloc[0]
```

![](img/9d1cdc4355e035772ebc024877bd1899.png)

## é¢„å¤„ç†æ³¨é‡Š

åœ¨ä½¿ç”¨æ–‡æœ¬æ•°æ®è¿›è¡Œå»ºæ¨¡ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€äº›æ ‡å‡†çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æŠ€æœ¯å¯¹å…¶è¿›è¡Œæ¸…ç†ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†åˆ é™¤æ•°å­—ï¼Œæ ‡ç‚¹ç¬¦å·ï¼Œé¢å¤–çš„ç©ºæ ¼ï¼Œå°†æ‰€æœ‰å•è¯è½¬æ¢ä¸ºå°å†™ï¼Œåˆ é™¤åœç”¨è¯ï¼Œè¯å¹²å’Œè¯æ¡ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Python åº“ *nltk* ã€‚

```
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords # nltk.download('stopwords')
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer #nltk.download('wordnet')import string, restop_words = list(set(stopwords.words('english')))
stemmer= PorterStemmer()
lemmatizer=WordNetLemmatizer()translate_table = dict((ord(char), None) for char in string.punctuation)def nltk_text_preproc(text_in):
    text_out = re.sub(r'\d+', '', text_in) # rm numbers
    text_out = text_out.translate(translate_table) # rm punct
    text_out = text_out.strip() # rm white spaces return text_outdef nltk_token_processing(tokens):
    tokens = [i.lower() for i in tokens]
    tokens = [i for i in tokens if not i in stop_words]
    tokens = [stemmer.stem(i) for i in tokens]
    tokens = [lemmatizer.lemmatize(i) for i in tokens]

    return tokens
```

ä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹æ•°æ®é›†ä¸­çš„å‰ 10 æ¡è¯„è®º

```
pd.options.display.max_colwidth = 1000
youtube_df['text'].head(10)
```

![](img/7a3045e04320429e2781679deec2c803.png)

ä¹‹åï¼Œä½¿ç”¨æˆ‘ä»¬çš„é¢„å¤„ç†æ­¥éª¤

```
youtube_df['text'].head(10).map(lambda x: nltk_text_preproc(x)).map(lambda x: nltk_token_processing(word_tokenize(''.join(x)))).map(lambda x: ' '.join(x))
```

![](img/6c265d382ca051ce0185f83055da05a5.png)

åœ¨ç»§ç»­ä¹‹å‰ï¼Œæˆ‘ä»¬ç°åœ¨å‡†å¤‡å¤„ç†æ•°æ®é›†ä¸­çš„æ‰€æœ‰æ–‡æœ¬ã€‚

```
youtube_df['text'] = youtube_df['text'].map(lambda x: nltk_text_preproc(x))
youtube_df['text'] = youtube_df['text'].map(lambda x: nltk_token_processing(word_tokenize(''.join(x))))
youtube_df['text'] = youtube_df['text'].map(lambda x: ' '.join(x))
```

æ–°æ•°æ®é›†åŒ…å« 1735 è¡Œã€‚

## ä»æ–‡æœ¬åˆ›å»ºè¦ç´ 

åœ¨è¿™ä¸€éƒ¨åˆ†ï¼Œé¢„å¤„ç†çš„æ–‡æœ¬æ•°æ®å°†è¢«è½¬æ¢æˆç‰¹å¾å‘é‡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ *nltk* Python åº“çš„*è®¡æ•°çŸ¢é‡å™¨*æ–¹æ³•ã€‚è¿™å°†æ–‡æœ¬è¡Œè½¬æ¢æˆä¸€ä¸ªçŸ©é˜µï¼Œå…¶ä¸­æ¯ä¸€åˆ—ä»£è¡¨æ‰€æœ‰æ–‡æœ¬ä¸­çš„ä¸€ä¸ªæœ¯è¯­ï¼Œæ¯ä¸ªå•å…ƒæ ¼ä»£è¡¨ç‰¹å®šæœ¯è¯­åœ¨ç»™å®šè¡Œä¸­å‡ºç°çš„æ¬¡æ•°ã€‚

```
count_vect = CountVectorizer(min_df=0.01, 
                             max_df=1.0, ngram_range=(1,3)) 
count_vect.fit(youtube_df['text'])
youtube_df_text = count_vect.transform(youtube_df['text'])
```

è¿™é‡Œï¼Œæˆ‘ä»¬è¯·æ±‚åˆ é™¤å‡ºç°åœ¨ä¸åˆ° 1%çš„è¯„è®ºæˆ–æ‰€æœ‰è¯„è®ºä¸­çš„æœ¯è¯­ï¼Œå¹¶è¦æ±‚ CountVectorizer è®¡ç®—æœ€å¤š 2 ä¸ªè¿ç»­æœ¯è¯­çš„ n å…ƒè¯­æ³•ã€‚

ä¾‹å¦‚ï¼Œç¬¬äºŒä¸ªæ³¨é‡Š(è¡Œ)

```
text_example_orig = youtube_df['text'][1]
print(text_example_orig)
```

![](img/ab50f4f9faff2426eba6d7516b30117e.png)

æˆä¸º

```
text_example = count_vect.transform([text_example])
print(text_example)
```

![](img/70ddba85ef0a5bd558f53e7792ad3cbf.png)

å¦‚æœæˆ‘ä»¬æƒ³çŸ¥é“å“ªäº›é¡¹å¯¹åº”äº CountVectorizer è¿”å›çš„ç´¢å¼•ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç 

```
for row, col in zip(*text_example_transf.nonzero()):
    val = text_example_transf[row, col]
    #print((row, col), val)
    print(count_vect.get_feature_names()[col])
```

![](img/44224f3a98fea8acb8d65aad713efebd.png)

è®¡æ•°çŸ©é˜µä¸æ˜¯æ ‡å‡†çŸ©é˜µï¼Œè€Œæ˜¯ç¨€ç–çŸ©é˜µ

```
print ('Shape of Sparse Matrix: ', youtube_df_text.shape)
print ('Amount of Non-Zero occurences: ', youtube_df_text.nnz)
print ('sparsity: %.2f%%' % (100.0 * youtube_df_text.nnz /
                             (youtube_df_text.shape[0] * youtube_df_text.shape[1])))
```

![](img/97306e9c640afb7d8edd92328b101d72.png)

ç¨€ç–çŸ©é˜µæ˜¯åŒ…å«æå°‘éé›¶å…ƒç´ çš„çŸ©é˜µçš„æœ€ä½³è¡¨ç¤ºã€‚äº‹å®ä¸Šï¼Œç”¨äºŒç»´æ•°ç»„è¡¨ç¤ºç¨€ç–çŸ©é˜µä¼šå¯¼è‡´å¤§é‡å†…å­˜æµªè´¹ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæœ‰ 1ï¼Œ735 è¡Œ* 166 ä¸ªé¡¹ï¼Œè¿™å°†å¯¼è‡´å…·æœ‰ 288ï¼Œ010 ä¸ªå…ƒç´ çš„äºŒç»´çŸ©é˜µï¼Œä½†æ˜¯åªæœ‰ 7ï¼Œ774 ä¸ªå…·æœ‰éé›¶å‡ºç°(2.7%ç¨€ç–åº¦)ï¼Œå› ä¸ºä¸æ˜¯æ‰€æœ‰è¡Œéƒ½åŒ…å«æ‰€æœ‰é¡¹ã€‚

## å»ºæ¨¡

æˆ‘ä»¬å°†æŠŠæ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ã€‚

```
train_x, valid_x, train_y, valid_y, index_train, index_val = model_selection.train_test_split(youtube_df_text, youtube_df['label'], range(len(youtube_df['label'])), stratify=youtube_df['label'], random_state=1, train_size=0.8)
```

ç°åœ¨å±äºè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆæ³•è¯„è®ºå’Œåƒåœ¾è¯„è®ºçš„æ•°é‡æ˜¯

```
np.unique(train_y, return_counts=True)
```

![](img/057212adf54a22dc728cbc7ff0579e73.png)

ç°åœ¨æ˜¯æ—¶å€™è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹äº†

```
def train_model(classifier, feature_vector_train, label, feature_vector_valid, valid_label):
    # fit the training dataset on the classifier
    classifier.fit(feature_vector_train, label)

    # predict the labels on validation dataset
    predictions = classifier.predict(feature_vector_valid)

    return classifier, metrics.accuracy_score(predictions, valid_label), metrics.classification_report(predictions, valid_label)
```

çœ‹çœ‹å®ƒåœ¨éªŒè¯é›†ä¸Šçš„è¡¨ç°ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ XGBoost åˆ†ç±»å™¨ã€‚

```
classifier, accuracy, confusion_matrix = train_model(xgboost.XGBClassifier(), 
            train_x, train_y, valid_x, valid_y)
print("Xgb, Accuracy: ", accuracy)
print(confusion_matrix)
```

![](img/66519f60805eeb3f840b08a178120e9d.png)

å¯¹äºè¿™ç¯‡æ–‡ç« æ¥è¯´ï¼Œä¸€ä¸ªé«˜æ€§èƒ½çš„æ¨¡å‹æ˜¯ä¸å¿…è¦çš„ã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾ä¸€ä¸ªå…³äºç‰¹æ€§å’Œæ ‡ç­¾å¦‚ä½•ç›¸äº’å…³è”çš„ä¸€èˆ¬æ€§æè¿°ã€‚äº‹å®ä¸Šï¼Œè¿™é‡Œæˆ‘ä»¬åªéœ€è¦ä¸€ä¸ªåƒæ ·çš„æ¨¡å‹ã€‚

## ç”¨ SHAP è®¡ç®—æ²™æ™®åˆ©å€¼

Python åº“[SHAP](https://github.com/slundberg/shap)(SHapley Additive explaints)å¯ä»¥è§£é‡Šä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹çš„è¾“å‡ºï¼Œç‰¹åˆ«æ˜¯å®ƒä¸º[æ ‘é›†æˆæ–¹æ³•](https://arxiv.org/abs/1802.03888)æä¾›äº†é«˜é€Ÿç²¾ç¡®ç®—æ³•ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„æ¨¡å‹æ˜¯ XGBoost æ¨¡å‹çš„åŸå› ä¹‹ä¸€ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œè®¡ç®— SHAP å€¼åªéœ€è¦å‡ åˆ†ä¹‹ä¸€ç§’ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬è®¡ç®—æ•´ä¸ªæ•°æ®é›†çš„ SHAP å€¼ã€‚è¯·æ³¨æ„ï¼Œå°† XGBoost ä¸é€»è¾‘ç›®æ ‡å‡½æ•°ç»“åˆä½¿ç”¨æ—¶ï¼ŒSHAP å€¼æ˜¯å¯¹æ•°ä¼˜åŠ¿ã€‚è¦å°†å¯¹æ•°ä¼˜åŠ¿å·®é¢è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…¬å¼ odds = exp(å¯¹æ•°ä¼˜åŠ¿)ï¼Œå…¶ä¸­ p = odds/(1+odds)ã€‚åœ¨æœ¬èŠ‚çš„æœ€åï¼Œæˆ‘ä»¬å°†åšè¿™ä¸ªç»ƒä¹ ï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬é¦–å…ˆè®¡ç®— SHAP å€¼å¹¶ç ”ç©¶å®ƒä»¬ã€‚

```
t0 = time.time()
explainer = shap.TreeExplainer(classifier)
shap_values_train = explainer.shap_values(youtube_df_text)
t1 = time.time()
timeit=t1-t0
print('time to compute Shapley values (s):', timeit)
```

![](img/07c25fc00ec013f323fbbe8eca4caa35.png)

å°†ç¨€ç–çŸ©é˜µè½¬æ¢æˆå¯†é›†çŸ©é˜µæ˜¯å¾ˆæ–¹ä¾¿çš„ã€‚

```
txt_dense_df = pd.DataFrame(youtube_df_text.todense(), 
                            columns=count_vect.get_feature_names())
shap_values_train_df = pd.DataFrame(shap_values_train, 
                                    columns=txt_dense_df.columns)
```

æœ‰äº†è¿™ä¸ªæ•°æ®æ¡†ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—æ•´ä½“è¦ç´ çš„é‡è¦æ€§

```
shap_sum = np.abs(shap_values_train_df).mean(axis=0)
importance_df = pd.DataFrame([txt_dense_df.columns.tolist(), 
                              shap_sum.tolist()]).T
importance_df.columns = ['column_name', 
                         'shap_importance (log-odds)']
importance_df = importance_df.sort_values('shap_importance (log-odds)', ascending=False)
importance_df['shap_importance (%)'] = importance_df['shap_importance (log-odds)'].apply(lambda x: 100*x/np.sum(importance_df['shap_importance (log-odds)']))
```

å¹¶ä¸”é€‰æ‹©ä¾‹å¦‚å‰ 20 ä¸ªç‰¹å¾

```
topN = 20
top20 = importance_df.iloc[0:topN]["column_name"]print('Cumulative Importance', 
      np.sum(importance_df.iloc[0:topN]["shap_importance (%)"]))shap_values_imp = shap_values_train_df[top20]shap.summary_plot(shap_values_train_df, 
                  txt_dense_df, plot_type="bar")importance_df.iloc[0:topN]
```

![](img/223eaf3160a9980d57694279d4cb3780.png)![](img/bf1a3682b7e4b8e4431e1cd9d4a13dc4.png)

å…¶ç´¯è®¡â€œé‡è¦æ€§â€çº¦ä¸º 94%ã€‚ä¸Šé¢çš„æ¡å½¢å›¾æ˜¾ç¤ºäº†æ¯ä¸ªç‰¹å¾çš„ SHAP å€¼çš„å¹³å‡ç»å¯¹å€¼ã€‚

ä¹Ÿå¯ä»¥ä½¿ç”¨æ›´å¤šçš„å¯è§†åŒ–æ¥è§£é‡Šæ¨¡å‹ç»“æœã€‚ä¸¾ä¸ªä¾‹å­ï¼Œ

```
j = 1
youtube_df.iloc[j]
```

![](img/3f386efc0a95e86c7f3117d04511d139.png)

æ˜¯å…¸å‹çš„åƒåœ¾è¯„è®ºã€‚æˆ‘ä»¬å¯ä»¥æ˜¾ç¤ºæœ‰åŠ©äºå°†æ¨¡å‹è¾“å‡ºä»åŸºç¡€å€¼(æˆ‘ä»¬ä½¿ç”¨çš„è®­ç»ƒæ•°æ®é›†çš„å¹³å‡æ¨¡å‹è¾“å‡º)æ¨è‡³æ¨¡å‹è¾“å‡ºçš„é¡¹ã€‚å°†é¢„æµ‹æ¨å‘æ›´é«˜çš„å¯¹æ•°ä¼˜åŠ¿å€¼çš„æœ¯è¯­æ˜¾ç¤ºä¸ºçº¢è‰²ï¼Œå°†é¢„æµ‹æ¨å‘æ›´ä½çš„å¯¹æ•°ä¼˜åŠ¿å€¼çš„æœ¯è¯­æ˜¾ç¤ºä¸ºè“è‰²ã€‚åœ¨è¿™ä¸ªç‰¹æ®Šçš„ä¾‹å­ä¸­ï¼Œæœ‰ä¸€ä¸ªå…¸å‹çš„åƒåœ¾è¯„è®ºçš„æ‰€æœ‰â€œæˆåˆ†â€ã€‚äº‹å®ä¸Šï¼Œæ‰€æœ‰æœ¯è¯­çš„ SHAP å€¼éƒ½å°†æ¨¡å‹è¾“å‡ºæ¨è‡³ä¸€ä¸ªæ¯”å¹³å‡è¾“å‡ºå€¼é«˜å¾—å¤šçš„å€¼ã€‚

```
shap.initjs()# visualize the j-prediction's explanation (use matplotlib=True to avoid Javascript)
shap.force_plot(explainer.expected_value, shap_values_imp.iloc[j].to_numpy(), 
                txt_dense_df.iloc[j][top20])
```

![](img/253e126f7c3e3040eed875cd88b3b6bb.png)

å¯¹äºåˆæ³•çš„è¯„è®ºï¼Œå¦‚

![](img/7d954dd18f963e2e9168ceab08f8c7a0.png)

ä¸Šé¢ä½¿ç”¨çš„å¯è§†åŒ–å˜æˆäº†

![](img/efdd78f9757fe352f41b15f903a3f09c.png)

åœ¨æˆ‘çš„[ç¬”è®°æœ¬](https://gist.github.com/alessiot/769ebe433adf79725b08687cf889f4cb)ä¸­ï¼Œæˆ‘æ·»åŠ äº†æ›´å¤šçš„å¯è§†åŒ–å†…å®¹ï¼Œè¿™å¹¶ä¸æ˜¯è¿™ç¯‡æ–‡ç« çš„ä¸»è¦ç›®çš„ã€‚

æœ€åï¼Œè®©æˆ‘ä»¬åšä¸€ä¸ªå°†å¯¹æ•°å‡ ç‡è½¬æ¢æˆæ¦‚ç‡çš„ç»ƒä¹ ï¼Œçœ‹çœ‹ SHAP å€¼ä¸å®ƒä»¬æœ‰ä»€ä¹ˆå…³ç³»ã€‚

```
j = 1# log odds margin to prob
# odds = exp(log odds), p = odds/(1+odds)log_odds = np.sum(shap_values_train_df.iloc[j].to_numpy())
avg_model_output = np.mean(youtube_df['label']) # prob
log_odds_avg_model_output = np.log(avg_model_output/(1-avg_model_output))
predicted_prob = classifier.predict_proba(youtube_df_text.tocsc())[j][1] #target=1
predicted_log_odds = np.log(predicted_prob/(1-predicted_prob))print("Sum of Shaphley values (log-odds)for j-instance:", log_odds, 
      'prob:', np.exp(log_odds)/(1.0+np.exp(log_odds)))
print("Average model output:", avg_model_output)
print("Predicted probability value for j-instance:", predicted_prob,
      "Predicted value:", classifier.predict(youtube_df_text.tocsc())[j])print('log_odds:', log_odds, 'is expected to be equal to pred-expected:', predicted_log_odds-log_odds_avg_model_output)
print('pred-expected (prob):', predicted_prob-avg_model_output)
```

![](img/7a2e01d1a89c0652e2b3512843eecd63.png)

å¯¹äºè¯„è®º j = 1ï¼Œæˆ‘ä»¬åŸºäºå¯¹æ•°-èµ”ç‡(SHAP å€¼)åˆ°æ¦‚ç‡çš„è½¬æ¢ï¼Œè®¡ç®—äº† SHAP å€¼(6.59)ï¼Œå¯¹åº”äºè¯¥è¯„è®ºæ˜¯åƒåœ¾é‚®ä»¶(æ ‡ç­¾= 1)çš„æ¦‚ç‡ä¸º 99.86%ã€‚ä»æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡æ˜¯ 99.85%ï¼Œç•¥æœ‰ä¸åŒï¼Œä½†è¶³å¤Ÿæ¥è¿‘ã€‚æ­£å¦‚æˆ‘åœ¨æœ¬æ–‡å¼€å¤´æ‰€è¯´çš„ï¼Œæˆ‘ä»¬æœŸæœ› SHAP å€¼(å¯¹æ•°ä¼˜åŠ¿)ç­‰äºé¢„æµ‹å€¼å‡å»é¢„æœŸæ¦‚ç‡ï¼Œè¿™æ˜¯æ‰€æœ‰å®é™…ç›®æ ‡çš„å¹³å‡å€¼(0.48)ã€‚

## èšç±» SHAP å€¼

æœ¬æ–‡ä¹‹å‰é€‰å‡ºçš„å‰ 20 ä¸ªæœ¯è¯­å¯ä»¥ç”¨å­—å…¸æ€»ç»“æˆæ›´é€šç”¨çš„â€œä¸»é¢˜â€:

```
top20_dict = {'ACTION': ['check','subscrib','view','share','follow','help', 'visit'],
 'REQUEST': ['plea','hey'],
 'VALUE': ['money','free','billion', 'new', "good", 'best'],
 'YOUTUBE': ['channel', 'song', 'onlin'],
 'COMMENT': ['love', 'like comment']             
}
```

å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªè¯„è®ºï¼Œæˆ‘ä»¬å¯ä»¥ç¡®å®šè¿è¡Œä»¥ä¸‹ä»£ç ç‰‡æ®µçš„é¦–è¦åŸå› ã€‚

```
from itertools import chainntopreason = 1 #change this to allow more reasons to be capturedtop20_dict_values = list(top20_dict.values())
top20_dict_keys = list(top20_dict.keys())shap_values_imp_r = shap_values_imp.copy()
target_values_r = pd.Series(predictions)# Create summarizing labels
top_reasons_all = []
for i in range(shap_values_imp_r.shape[0]):

    shap_feat = shap_values_imp_r.iloc[i]
    shap_feat = shap_feat.iloc[np.lexsort([shap_feat.index, shap_feat.values])]

    topN = shap_feat.index.to_list()[-1:] 
    topN_value = shap_feat.values[-1:]

    topN_idx = []
    for topn in topN:
        for idx in range(len(top20_dict_values)):
            if topn in top20_dict_values[idx] and idx not in topN_idx:
                topN_idx.append(idx)

    #topN_idx = [idx for idx in range(len(top20_dict_values)) for topn in topN if topn in top20_dict_values[idx]]

    #Ordered by increasing importance
    top_reasons = [top20_dict_keys[x] for x in topN_idx]#print(i, topN, topN_idx, top_reasons)top_reasons_all.append(','.join(top_reasons))

shap_values_imp_r['target'] = target_values_r
shap_values_imp_r['top_reasons'] = top_reasons_all
```

åœ¨ä¸€ä¸ªå•ç‹¬çš„ Jupyter å•å…ƒæ ¼ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ R æ¥é€‰æ‹©ç”¨äºå¯¹è¯„è®ºçš„ SHAP å€¼è¿›è¡Œåˆ†ç»„çš„æœ€ä½³èšç±»æ•°ã€‚

```
%%R -i shap_values_imp_r -w 800 -h 800library(gplots)d_shap <- dist(shap_values_imp_r[1:(ncol(shap_values_imp_r)-2)]) 
hc <- hclust(d_shap, method = "ward.D2")
dend <- as.dendrogram(hc)library(dendextend)
library(colorspace)## Find optimal number of clusters
clusters_test = list()
for (ncl in 2:50){
    clusters_test[[ncl]] <- cutree(hc, k=ncl)
}
```

è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨å±‚æ¬¡èšç±»ï¼Œèšç±»æ•°é‡åœ¨ 2 åˆ° 50 ä¹‹é—´ã€‚å›åˆ° Pythonï¼Œæˆ‘ä»¬è®¡ç®—æ¯ä¸ªèšç±»ç»“æœçš„*è½®å»“*åˆ†æ•°ã€‚é€šè¿‡æŸ¥çœ‹è½®å»“åˆ†æ•°å’Œèšç±»çš„å¯è§†åŒ–æ¥å†³å®šè¦ä½¿ç”¨çš„èšç±»æ•°é‡ã€‚åœ¨é€‰æ‹©é›†ç¾¤æ•°é‡æ—¶ï¼Œæ€»æ˜¯éœ€è¦åšå‡ºå†³å®šã€‚

```
h_clusters = rpy2.robjects.r['clusters_test']h_clusters_sil = []
cl_id = 0
for cl in h_clusters:
    if cl is not rpy2.rinterface.NULL:
        sil = metrics.silhouette_score(shap_values_imp_r.drop(shap_values_imp_r.columns[len(shap_values_imp_r.columns)-2:], axis=1, inplace=False), 
                                       cl, metric='euclidean')
        h_clusters_sil.append(sil)
        #print(cl_id, sil)
        cl_id += 1
    else:
        cl_id += 1plt.plot(range(2, 51), h_clusters_sil)
plt.title('Silhouette')
plt.xlabel('Number of clusters')
plt.ylabel('Silhouette Index') #within cluster sum of squares
plt.show()
```

![](img/b142c79a96c75bd6f8120feaf1a91ccd.png)

æˆ‘ä»¬ä½¿ç”¨ Ward çš„æ–¹æ³•è¿›è¡Œèšç±»ï¼Œè¯¥æ–¹æ³•ä½¿æ€»çš„ç»„å†…æ–¹å·®æœ€å°åŒ–ã€‚åœ¨æ¯ä¸€æ­¥ï¼Œå…·æœ‰æœ€å°èšç±»é—´è·ç¦»çš„èšç±»å¯¹è¢«åˆå¹¶ã€‚åˆå¹¶çš„é«˜åº¦(è§ä¸‹å›¾)è¡¨ç¤ºä¸¤ä¸ªå®ä¾‹ä¹‹é—´çš„ç›¸ä¼¼æ€§ã€‚åˆå¹¶çš„é«˜åº¦è¶Šé«˜ï¼Œå®ä¾‹è¶Šä¸ç›¸ä¼¼ã€‚æ•°æ®çš„å¹³å‡[è½®å»“](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_silhouette_method)æ˜¯è¯„ä¼°æœ€ä½³èšç±»æ•°çš„æ ‡å‡†ã€‚æ•°æ®å®ä¾‹çš„è½®å»“æ˜¯ä¸€ç§åº¦é‡ï¼Œå®ƒä¸å®ƒçš„åˆ†ç±»ä¸­çš„æ•°æ®åŒ¹é…å¾—æœ‰å¤šç´§å¯†ï¼Œä¸ç›¸é‚»åˆ†ç±»çš„æ•°æ®åŒ¹é…å¾—æœ‰å¤šæ¾æ•£ã€‚å‰ªå½±å¾—åˆ†èŒƒå›´åœ¨-1 å’Œ 1 ä¹‹é—´ï¼Œæœ€å¥½ä¸º 1ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹© 30 ä¸ªèšç±»ï¼Œä»¥ä¾¿åœ¨è¯„è®ºç±»å‹ä¸­æœ‰è¶³å¤Ÿçš„ç²’åº¦ï¼Œå¹¶è·å¾—å¤§çº¦ 0.70 çš„è‰¯å¥½è½®å»“åˆ†æ•°ã€‚

```
%%R -i shap_values_imp_r -w 800 -h 800library(gplots)d_shap <- dist(shap_values_imp_r[1:(ncol(shap_values_imp_r)-2)], 'euclidean') 
hc <- hclust(d_shap, method = "ward.D2")
dend <- as.dendrogram(hc)library(dendextend)
library(colorspace)n_clust <- 30dend <- color_branches(dend, k=n_clust) #, groupLabels=iris_species)clusters <- cutree(hc, k=n_clust)
#print(head(clusters))print(format(prop.table(table(clusters,shap_values_imp_r$top_reasons, shap_values_imp_r$target), margin=1), 
             digit=2, scientific = F))
print(format(prop.table(table(clusters,shap_values_imp_r$top_reasons), margin=1), 
             digit=2, scientific = F))heatmap.2(as.matrix(shap_values_imp_r[1:(ncol(shap_values_imp_r)-2)]), 
          dendrogram = "row",
          Rowv = dend,
          Colv = "NA", # this to make sure the columns are not ordered
          key.xlab = "Predicted - Average log-odds",
          #hclustfun=function(d) hclust(d, method="complete"), 
          srtCol=45,  adjCol = c(1,1))
```

![](img/357d192d7b58d3d65e9095bb1006a8eb.png)

åœ¨ä¸Šé¢çš„çƒ­å›¾ä¸­ï¼Œæˆ‘ä»¬å°†åˆ†å±‚èšç±»(å·¦ä¾§ y è½´)ä¸å‰ 20 ä¸ªæœ¯è¯­çš„é‡è¦æ€§(x è½´ï¼Œæ ¹æ®è¯„è®ºä¸­ç›¸åº”æœ¯è¯­å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“è€Œç€è‰²çš„å•å…ƒæ ¼)ç›¸ç»“åˆï¼Œå‰è€…æ˜¾ç¤ºäº†æ›´æ¥è¿‘çš„ç›¸ä¼¼è¯„è®º(å³ä¾§ y è½´ä¸Šçš„æ•°å­—æ˜¯è¯„è®ºè¡Œå·)ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡æ±‡æ€»æ¯ä¸ªé›†ç¾¤ä¸­çš„è¯„è®ºæ¥è¿›ä¸€æ­¥æ€»ç»“ä¸Šé¢çš„çƒ­å›¾ã€‚ä¸ºäº†ä½¿è¿™ä¸€æ­¥æ›´é€šç”¨ï¼Œæˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„æ•°æ®é›†æ˜¯ä¸€ä¸ªæ ‡ç­¾æœªçŸ¥çš„æ–°æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨ k=1 çš„ k-æœ€è¿‘é‚»åˆ†ç±»å™¨åŸºäºèšç±»ç»“æœé¢„æµ‹æ ‡ç­¾ã€‚ç„¶åï¼Œåœ¨æŒ‰é¢„æµ‹æ ‡ç­¾åˆ†ç»„åï¼Œæˆ‘ä»¬å¯ä»¥èšåˆæ¯ä¸ªèšç±»ä¸­çš„ SHAP å€¼ã€‚

```
from sklearn.neighbors import KNeighborsClassifiercluster_model = KNeighborsClassifier(n_neighbors=1)
cluster_model.fit(shap_values_imp,rpy2.robjects.r['clusters'])predicted = cluster_model.predict(shap_values_imp_r[shap_values_imp_r.columns[:-2]]) grouped = pd.concat([shap_values_imp, pd.Series(youtube_df['label'].tolist(), name='avg_tgt')], axis=1).groupby(predicted)# compute average impact to model prediction output. 
sums = grouped.apply(lambda x: np.mean(x))
```

ä¸‹é¢çš„çƒ­å›¾æ˜¾ç¤ºäº† 30 ä¸ªåˆ†ç±»(y è½´)ä¸­çš„æ¯ä¸ªåˆ†ç±»çš„å‰ 20 ä¸ªæœ¯è¯­å¯¹æ¨¡å‹é¢„æµ‹çš„å½±å“(z è½´ä¸Šæ˜¾ç¤ºçš„å¯¹æ•°ä¼˜åŠ¿)ã€‚

```
import plotly
import plotly.graph_objs as go
plotly.offline.init_notebook_mode()data = [go.Heatmap(z=sums[sums.columns[1:-1]].values.tolist(), 
                   y=sums.index,
                   x=sums.columns[1:-1],
                   colorscale='Blues')]plotly.offline.iplot(data, filename='pandas-heatmap')
```

![](img/458e24d61559d7d7bcbdf4f5b39a9867.png)

ä¾‹å¦‚ï¼Œä»çƒ­å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œèšç±» 30 ä¸»è¦æ˜¯â€œå–œæ¬¢çš„è¯„è®ºâ€2 å…ƒè¯­æ³•ï¼Œå¦‚æœæˆ‘ä»¬æŸ¥çœ‹å¹³å‡é¢„æµ‹æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™æ˜¯åˆæ³•è¯„è®ºçš„èšç±»(â€œæ€»å’Œâ€æ•°æ®å¸§çš„â€œavg_tgtâ€å€¼)ã€‚èšç±» 8 å¹³å‡æ¥è¯´æ˜¯åƒåœ¾é‚®ä»¶èšç±»ï¼Œå¹¶ä¸”ä»¥æœ¯è¯­â€œè§†å›¾â€ä¸ºä¸»ã€‚

æˆ‘ä»¬è¿˜å¯ä»¥èšé›†æ¯ä¸ªèšç±»ä¸­è¯„è®ºçš„ SHAP å€¼ï¼Œä»¥æ˜¾ç¤ºæ¯ä¸ªèšç±»ä¸­ä»€ä¹ˆâ€œä¸»é¢˜â€å ä¸»å¯¼åœ°ä½ã€‚

```
agg_sums = pd.DataFrame({k: sums[v].mean(axis=1) for (k, v) in top20_dict.items()})data = [go.Heatmap(z=agg_sums[agg_sums.columns].values.tolist(), 
                   y=agg_sums.index,
                   x=agg_sums.columns,
                   colorscale='Blues')]plotly.offline.iplot(data, filename='pandas-heatmap')
```

![](img/d032ff5906eb560fdd1c980925384cb1.png)

ç¬¬ 30 ç»„çš„æ„è§æ˜¯

```
pd.options.display.max_colwidth = 1000cluster_no = 30ex_2 = youtube_df.iloc[rpy2.robjects.r['clusters']==cluster_no]
ex_2_pred = pd.Series(predictions[rpy2.robjects.r['clusters']==cluster_no])
ex_2_top = shap_values_imp_r.iloc[rpy2.robjects.r['clusters']==cluster_no]['top_reasons']ex_2
```

![](img/bc42c2df2bb37232c559ec8c7ec1c745.png)

ç¬¬ 8 ç»„çš„æ„è§æ˜¯

![](img/960d03f75b5de9138b9c32dd87c7e1f1.png)

## ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Shapley å€¼å‘æ¨¡å‹åˆ†æ•°æ·»åŠ ä¿¡æ¯ï¼Œè¿™æ˜¯è·å¾—é¢„æµ‹æ ‡ç­¾çš„â€œåŸå› â€ã€‚äº‹å®ä¸Šï¼Œä¸€ä¸ªæ¨¡å‹åˆ†æ•°åº”è¯¥æ€»æ˜¯è·Ÿéšç€å¯¹è¿™ä¸ªåˆ†æ•°çš„è§£é‡Šã€‚æ­¤å¤–ï¼Œèšç±» Shapley å€¼å¯ç”¨äºè¯†åˆ«æ•°æ®å®ä¾‹ç»„ï¼Œè¿™äº›æ•°æ®å®ä¾‹å¯ä»¥ç”¨ä»£è¡¨å¤šç§åŸå› çš„æ›´é€šç”¨çš„ä¸»é¢˜æ¥è§£é‡Šã€‚

æˆ‘è¦æ„Ÿè°¢ [Sundar Krishnan](https://medium.com/u/b61b6673cdce?source=post_page-----7c945cc531f--------------------------------) å’Œ [Praveen Thoranathula](https://medium.com/u/4d9e48e82766?source=post_page-----7c945cc531f--------------------------------) çš„æœ‰ç›Šè®¨è®ºã€‚