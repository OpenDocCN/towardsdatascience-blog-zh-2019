<html>
<head>
<title>Using USE (Universal Sentence Encoder) to Detect Fake News</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 USE(通用语句编码器)检测假新闻</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-use-universal-sentence-encoder-to-detect-fake-news-dfc02dc32ae9?source=collection_archive---------26-----------------------#2019-11-19">https://towardsdatascience.com/using-use-universal-sentence-encoder-to-detect-fake-news-dfc02dc32ae9?source=collection_archive---------26-----------------------#2019-11-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/7bfe9d846a0fb05c8a5550880c115e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*f-zUSTZqRgSdb37r1Nm4aw.jpeg"/></div></figure><p id="9b13" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi kv translated">自从引进<a class="ae le" href="https://www.aclweb.org/anthology/D18-2029" rel="noopener ugc nofollow" target="_blank">通用语句编码器</a>(使用)以来，它已经成为<a class="ae le" href="https://www.tensorflow.org/hub" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>中下载量最多的预训练文本模块。通用句子编码器系列包括:</p><ul class=""><li id="ee4c" class="lf lg it jz b ka kb ke kf ki lh km li kq lj ku lk ll lm ln bi translated"><a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder/2" rel="noopener ugc nofollow" target="_blank">通用语句编码器</a></li><li id="215c" class="lf lg it jz b ka lo ke lp ki lq km lr kq ls ku lk ll lm ln bi translated"><a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-large/3" rel="noopener ugc nofollow" target="_blank">通用句子编码器大号</a></li><li id="981e" class="lf lg it jz b ka lo ke lp ki lq km lr kq ls ku lk ll lm ln bi translated"><a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-lite/2" rel="noopener ugc nofollow" target="_blank">通用语句编码器 lite </a></li><li id="064a" class="lf lg it jz b ka lo ke lp ki lq km lr kq ls ku lk ll lm ln bi translated"><a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-multilingual/1" rel="noopener ugc nofollow" target="_blank">通用语句编码器多语言</a></li><li id="76bf" class="lf lg it jz b ka lo ke lp ki lq km lr kq ls ku lk ll lm ln bi translated"><a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1" rel="noopener ugc nofollow" target="_blank">通用语句编码器多语大号</a></li></ul><p id="90c8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">通用句子编码器具有用于语义相似性和问答检索的不同模块。</p><p id="95fa" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们将使用<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-large/3" rel="noopener ugc nofollow" target="_blank">通用句子编码器 large </a>进行假新闻检测，这是一个文本分类问题。假新闻(也称为垃圾新闻、伪新闻或恶作剧新闻)是一种黄色新闻或宣传，由通过传统新闻媒体(印刷和广播)或在线社交媒体传播的故意虚假信息或骗局组成</p><p id="3b2a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">我们使用<a class="ae le" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python </a>和<a class="ae le" href="http://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter Notebook </a>来开发我们的系统，我们将使用的库包括<a class="ae le" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank">Keras</a>T6、T8】Numpy、T10】Pandas、<a class="ae le" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Sklearn </a>、<a class="ae le" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a>、<a class="ae le" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>和<a class="ae le" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> Tensorflow Hub </a>。完整的代码和数据可以从<a class="ae le" href="https://github.com/saadarshad102/Fake-News-Detection-Universal-Sentence-Encoder" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="b16a" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">数据探索</h2><p id="8ff4" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">首先，我们将看看我们的数据。我们的数据有三列，即标题、文本和标签。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="7cd7" class="ma mb it nd b gy nh ni l nj nk">data = pd.read_csv('fake_or_real_news.csv')<br/>data = data[['title', 'text', 'label']]<br/>data.columns =  ['Title', 'Text', 'Label']<br/>data.head()</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/748e674cec344d88b5378d4314990287.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*4P1a_21mdFyOQ7m_lyIA_g.jpeg"/></div></figure><p id="0b10" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在我们检查标题和文本的最大字数。</p><p id="1cd0" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最大标题长度:289 <br/>最大文本长度:115372</p><p id="418b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">由于新闻文章的文本更重要，而且我们也有内存限制，所以我们将只使用文本进行分类。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="2ce9" class="ma mb it nd b gy nh ni l nj nk">print("Max title length:", data.Title.str.len().max())<br/>print("Max text length:", data.Text.str.len().max())</span></pre><p id="dfd6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在我们看到了阶级分布。我们有 3171 个真的和 3164 个假的例子。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="cbe0" class="ma mb it nd b gy nh ni l nj nk">data.Label.value_counts()</span></pre><p id="08d8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为了得到数据集的形状，我们使用熊猫数据框的形状属性。我们总共有 6335 个例子。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="f10f" class="ma mb it nd b gy nh ni l nj nk">data.shape</span></pre><p id="3afe" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">因为我们的问题是二元分类。我们需要给我们的模型传递一个二维输出向量。为此，我们在数据框中添加了两个 one hot 编码列。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="854e" class="ma mb it nd b gy nh ni l nj nk">pos = []<br/>neg = []<br/>for l in data.Label:<br/>    if l == 'FAKE':<br/>        pos.append(0)<br/>        neg.append(1)<br/>    elif l == 'REAL':<br/>        pos.append(1)<br/>        neg.append(0)</span><span id="c202" class="ma mb it nd b gy nm ni l nj nk">data['Pos']= pos<br/>data['Neg']= neg</span></pre><p id="a528" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在让我们看看我们的数据框架。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6a5f" class="ma mb it nd b gy nh ni l nj nk">data.head()</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/9d4bf9672880fba5e64b78c7b4beedcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r8fp6bs5jIfGwtwZeF_GSg.jpeg"/></div></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="5128" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">将数据分为测试和训练</h2><p id="ff69" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">现在，我们将数据集分为训练集和测试集。我们将使用 90 %的数据进行训练，10 %的数据进行测试。我们使用随机状态，所以每次我们都得到相同的训练和测试数据。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6db9" class="ma mb it nd b gy nh ni l nj nk">data_train, data_test = train_test_split(data, <br/>                                         test_size=0.10, <br/>                                         random_state=42)</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ns"><img src="../Images/fbef90a35db6852bde1dcd3520dd633a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmFN4XOoEjDeMMtTqCq97g.png"/></div></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="aab0" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">加载通用语句编码器</h2><p id="f8d2" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">下一步是加载通用句子编码器。使用 tensorflow_hub 很容易做到。这一步可能需要一些时间。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="edef" class="ma mb it nd b gy nh ni l nj nk">module_url = "<a class="ae le" href="https://tfhub.dev/google/universal-sentence-encoder-large/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder-large/2</a>" <br/>embed = hub.Module(module_url)</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="6df5" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">获取嵌入</h2><p id="68f2" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">现在我们将把训练句子转换成嵌入。这是通过简单地将整个句子传递给 embed object 来实现的。我们为每个句子得到一个 512 维的向量。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6c63" class="ma mb it nd b gy nh ni l nj nk">with tf.Session() as session:<br/>    session.run([tf.global_variables_initializer(), <br/>                 tf.tables_initializer()])<br/>    training_embeddings = session.run(embed(data_train.Text.to_list()))</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nt"><img src="../Images/11989c044f8f2c76f819b2d24d84480d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Na0jdXipJq9rmwkzcM_OVg.png"/></div></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="bc35" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">定义神经网络</h2><p id="800c" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">我们现在将设计一个总共有两层的浅层神经网络。嵌入被传递到具有“Relu”激活和 128 个单元的密集层。接下来是我们的输出层。这一层有两个单元，因为我们的模型是一个二元分类器。Softmax 用作输出层的激活函数。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="eb9f" class="ma mb it nd b gy nh ni l nj nk">model = Sequential()<br/>model.add(Dense(128, activation = 'relu'))<br/>model.add(Dense(2, activation = 'softmax'))</span><span id="64ce" class="ma mb it nd b gy nm ni l nj nk">model.compile(loss='binary_crossentropy', <br/>              optimizer='adam',<br/>              metrics=['acc'])</span></pre><p id="671a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">model.summary()将打印所有图层的摘要以及输出的形状。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="6250" class="ma mb it nd b gy nh ni l nj nk">model.summary()</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/70ea230b165c7b8c03515d7ebf516e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*4yguDUSo3y-Wm3dIW2R3QQ.jpeg"/></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="26ab" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">培养</h2><figure class="my mz na nb gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nv"><img src="../Images/ec84a0c62f224b7a227d6ee6ad1672a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTUn5UIJqcbOP62LjHtdJg.jpeg"/></div></div></figure><p id="c997" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">历元数是您的模型将循环和学习的数量，批量大小是您的模型在单个时间看到的数据量。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="48fc" class="ma mb it nd b gy nh ni l nj nk">num_epochs = 30<br/>batch_size = 32</span><span id="a6b3" class="ma mb it nd b gy nm ni l nj nk">history = model.fit(training_embeddings, <br/>                    data_train[label_names].values, <br/>                    epochs=num_epochs, <br/>                    validation_split=0.1, <br/>                    shuffle=True, <br/>                    batch_size=batch_size)</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="285f" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">绘制准确度和损失图</h2><p id="4a83" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">首先，我们将绘制测试和训练精度相对于历元数的曲线。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="8771" class="ma mb it nd b gy nh ni l nj nk">plt.plot(history.history['acc'])<br/>plt.plot(history.history['val_acc'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'], loc='upper left')<br/>plt.show()</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/dc329b390c7b41f1d503ac5b8be7ae1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*FniubbzKxX1rk3zvQh8lsA.jpeg"/></div></figure><p id="b984" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">现在，我们将绘制测试和训练损失与历元数的关系图。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="ea3e" class="ma mb it nd b gy nh ni l nj nk">plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('model loss')<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'test'], loc='upper left')<br/>plt.show()</span></pre><figure class="my mz na nb gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/05b2ecfcdd8b7f6cee7a5e7dbf5c2697.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*KQvSzZxm0AtB_zqU2ukMYQ.jpeg"/></div></figure></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="810f" class="ma mb it bd mc md me dn mf mg mh dp mi ki mj mk ml km mm mn mo kq mp mq mr ms bi translated">测试</h2><p id="7ea8" class="pw-post-body-paragraph jx jy it jz b ka mt kc kd ke mu kg kh ki mv kk kl km mw ko kp kq mx ks kt ku im bi translated">现在我们将把测试例子转换成嵌入。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="dc15" class="ma mb it nd b gy nh ni l nj nk">with tf.Session() as session:<br/>    session.run([tf.global_variables_initializer(), <br/>                 tf.tables_initializer()])<br/>    test_embeddings = session.run(embed(data_test.Text.to_list()))</span></pre><p id="6a0d" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">哇！只需三十次迭代和一个小数据集，我们就能获得 90 %的准确率。请记住，我们没有对数据进行预处理，因此这种准确性可以得到认可。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="79c8" class="ma mb it nd b gy nh ni l nj nk">predictions = model.predict(test_embeddings, <br/>                            batch_size=1024, <br/>                            verbose=1)<br/>labels = ['REAL', 'FAKE']</span><span id="3a93" class="ma mb it nd b gy nm ni l nj nk">prediction_labels=[]<br/>for p in predictions:<br/>    prediction_labels.append(labels[np.argmax(p)])</span><span id="75fb" class="ma mb it nd b gy nm ni l nj nk">print("Accuracy:", sum(data_test.Label==prediction_labels)/len(prediction_labels))</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><blockquote class="ny nz oa"><p id="bae5" class="jx jy ob jz b ka kb kc kd ke kf kg kh oc kj kk kl od kn ko kp oe kr ks kt ku im bi translated">如果你有任何问题或建议，欢迎在下面留言。你也可以在<a class="ae le" href="https://www.linkedin.com/in/saadarshad102/" rel="noopener ugc nofollow" target="_blank"><strong class="jz iu"><em class="it">Linkedin</em></strong><em class="it">上和我联系。</em>T9】</a></p></blockquote></div></div>    
</body>
</html>