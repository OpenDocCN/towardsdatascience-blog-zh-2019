# 理解神经网络

> 原文：<https://towardsdatascience.com/understanding-neural-networks-19020b758230?source=collection_archive---------1----------------------->

![](img/9c54dd5b039156a57962a8559b801829.png)

## 我们探索神经网络如何运作，以建立对深度学习的直观理解

深度学习是目前的热门话题。但到底是什么让它与众不同，让它有别于机器学习的其他方面呢？这是一个深奥的问题(原谅我的双关语)。为了开始回答这个问题，我们需要学习神经网络的基础知识。

神经网络是深度学习的主力。虽然它们可能看起来像黑匣子，但在内心深处(对不起，我将停止可怕的双关语)，它们正试图完成与任何其他模型相同的事情——做出正确的预测。

在这篇文章中，我们将探索一个简单的神经网络的来龙去脉。到最后，希望你(和我)会对神经网络如何工作有更深更直观的理解。

# 30，000 英尺的视野

让我们从一个非常高层次的概述开始，这样我们就知道我们在做什么。**神经网络是神经元的多层网络(下图中的蓝色和洋红色节点)，我们用它来对事物进行分类、做出预测等**。下面是一个简单的神经网络图，它有五个输入、五个输出和两个隐藏的神经元层。

![](img/b00533a02f42c4436fb25258e6a9880a.png)

Neural network with two hidden layers

从左边开始，我们有:

1.  橙色的是我们模型的输入层。
2.  我们第一个隐藏的蓝色神经元层。
3.  我们第二层隐藏的洋红色神经元。
4.  模型的输出层(也叫预测层)是绿色的。

连接这些点的箭头显示了所有神经元是如何相互连接的，以及数据是如何从输入层一直传输到输出层的。

稍后我们将逐步计算每个输出值。我们还将观察神经网络如何使用反向传播过程从错误中学习。

# 找到我们的方向

但首先让我们找到方向。神经网络到底想做什么？像任何其他模型一样，它试图做出一个好的预测。我们有一组输入和一组目标值，我们正试图获得尽可能与这些目标值匹配的预测。

暂时忘记我在上面画的看起来更复杂的神经网络图，把注意力放在下面这张更简单的图上。

![](img/d188242737b29ec512e8b4acdf41c47b.png)

Logistic regression (with only one feature) implemented via a neural network

这是一个通过神经网络表达的单特征逻辑回归(我们只给模型一个 X 变量)(如果你需要逻辑回归的复习，[我在这里写了这个](/understanding-logistic-regression-using-a-simple-example-163de52ea900))。为了了解它们之间的联系，我们可以用神经网络颜色代码重写逻辑回归方程。

![](img/dda50056a8e26a3308df487c46180bdb.png)

Logistic regression equation

让我们检查每个元素:

1.  x(橙色)是我们的输入，这是我们为了计算预测而赋予模型的唯一特征。
2.  B1(以蓝绿色表示，也称为蓝绿色)是我们的逻辑回归的估计斜率参数，B1 告诉我们 Log_Odds 随着 X 的变化而变化的程度。**注意 B1 位于蓝绿色线上，该线将输入 X 连接到隐藏层 1** 中的蓝色神经元。
3.  B0(蓝色)是偏差，非常类似于回归中的截距项。关键区别在于，在神经网络中，每个神经元都有自己的偏差项(而在回归中，模型有一个奇异的截距项)。
4.  蓝色神经元还包括一个[乙状结肠激活功能](https://en.wikipedia.org/wiki/Sigmoid_function)(由蓝色圆圈内的曲线表示)。记住，sigmoid 函数是我们使用[从对数几率到概率的函数(在我之前的帖子中搜索“sigmoid”)](/understanding-logistic-regression-using-a-simple-example-163de52ea900)。
5.  最后，我们通过将 sigmoid 函数应用于量(B1*X + B0)来获得我们的预测概率。

不算太糟吧？让我们回顾一下。一个超级简单的神经网络仅由以下组件组成:

*   一个连接(虽然在实践中，通常会有多个连接，每个连接都有自己的权重，进入一个特定的神经元)，权重“生活在它里面”，它转换你的输入(使用 B1)并将其提供给神经元。
*   一种神经元，包括一个偏置项(B0)和一个激活函数(本例中为 sigmoid)。

**这两个对象是神经网络**的基本构建模块。更复杂的神经网络只是具有更多隐藏层的模型，这意味着更多的神经元和神经元之间更多的连接。这种更复杂的连接网络(以及权重和偏差)允许神经网络“学习”隐藏在我们数据中的复杂关系。

# 现在让我们增加一点复杂性

现在我们有了基本框架，让我们回到稍微复杂一点的神经网络，看看它是如何从输入到输出的。这里再次供参考:

![](img/b00533a02f42c4436fb25258e6a9880a.png)

Our slightly more complicated neural network

第一个隐藏层由两个神经元组成。因此，为了将所有五个输入连接到隐藏层 1 中的神经元，我们需要十个连接。下图显示了输入 1 和隐藏层 1 之间的连接。

![](img/9b977262d3329f57623ac35d205fd0b2.png)

The connections between Input 1 and Hidden Layer 1

请注意我们对存在于连接中的权重的标注——W1，1 表示存在于输入 1 和神经元 1 之间的连接中的权重，w1，2 表示输入 1 和神经元 2 之间的连接中的权重。所以我将遵循的一般符号是 W **a，b** 表示输入 **a** (或神经元 **a** )和神经元 **b** 之间连接的权重。

现在让我们计算隐藏层 1 中每个神经元的输出(称为激活)。我们使用下面的公式( **W** 表示重量，中的**表示输入)。**

> Z1 = W1 * In1+W2 * In2+W3 * In3+W4 * In4+W5 * In5+Bias _ neuro n1
> 
> 神经元 1 激活=(Z1)

我们可以使用矩阵数学来总结这种计算(记住我们的符号规则，例如，W4，2 表示输入 4 和神经元 2 之间的连接中的权重):

![](img/ed4d3940a61156aae66676d3c51527b5.png)

Matrix math makes our life easier

对于神经网络的任何层，其中前一层为 **m** 元素深，当前层为 **n** 元素深，这概括为:

> **[W] @ [X] + [Bias] = [Z]**

其中[W]是您的 **n 乘 m** 权重矩阵(前一层和当前层之间的连接)，[X]是您的 **m** **乘 1** 前一层的开始输入或激活矩阵，[Bias]是您的 **n 乘 1** 神经元偏差矩阵，[Z]是您的 **n 乘 1** 中间输出矩阵。**在前面的等式中，我遵循 Python 符号，用@表示矩阵乘法**。一旦我们有了[Z],我们可以将激活函数(在我们的例子中是 sigmoid)应用于[Z]的每个元素，这就给出了当前层的神经元输出(激活)。

最后，在我们继续之前，让我们直观地将这些元素映射回我们的神经网络图，以便将它们联系起来([Bias]嵌入在蓝色神经元中)。

![](img/b58f3ee5ae2fb21aa49714e92781018a.png)

Visualizing [W], [X], and [Z]

通过重复计算[Z]并对每个连续层应用激活函数，我们可以从输入移动到输出。这个过程被称为正向传播。现在我们知道了输出是如何计算的，是时候开始评估输出的质量并训练我们的神经网络了。

# 神经网络学习的时间到了

这将是一个很长的帖子，所以现在请随意休息一下。还和我在一起吗？厉害！现在我们知道了神经网络的输出值是如何计算的，是时候训练它了。

> 神经网络的训练过程，在高层次上，就像许多其他数据科学模型一样— **定义一个成本函数，并使用** [**梯度下降优化**](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html) **将其最小化**。

首先让我们考虑一下我们可以使用什么杠杆来最小化成本函数。在传统的线性或逻辑回归中，我们寻找β系数(B0，B1，B2 等。)最小化成本函数。对于神经网络来说，我们正在做同样的事情，但是规模更大，更复杂。

在传统回归中，我们可以孤立地改变任何特定的贝塔系数，而不会影响其他贝塔系数。因此，通过对每个β系数施加小的孤立冲击，并测量其对成本函数的影响，可以相对简单地计算出我们需要向哪个方向移动，以降低并最终最小化成本函数。

![](img/fadae5031c61aefbd77aeb8299fa2587.png)

Five feature logistic regression implemented via a neural network

**在神经网络中，改变任何一个连接的权重(或一个神经元的偏差)都会对所有其他神经元及其在后续层中的激活产生回响效应**。

**这是因为神经网络中的每个神经元都像它自己的小模型**。例如，如果我们想要一个五特征逻辑回归，我们可以通过一个神经网络来表达它，就像左边的那个，只用一个单一的神经元！

> 因此，神经网络的每一个隐藏层基本上都是一堆模型**(该层中的每一个单独的神经元就像它自己的模型一样)**，其输出馈入更下游的更多模型**(神经网络的每一个连续的隐藏层包含更多的神经元)。**

## **成本函数**

**考虑到所有这些复杂性，我们能做什么呢？其实也没那么差。让我们一步一步来。首先，让我清楚地说明我们的目标。给定一组训练输入(我们的特征)和结果(我们试图预测的目标):**

> **我们希望找到一组权重(请记住，神经网络中任何两个元素之间的每条连接线都包含一个权重)和偏差(每个神经元都包含一个偏差)，以最小化我们的成本函数，其中成本函数是我们的预测相对于目标结果有多错误的近似值。**

**为了训练我们的神经网络，我们将使用[均方误差(MSE)](https://en.wikipedia.org/wiki/Mean_squared_error) 作为成本函数:**

> ****MSE = Sum [(预测-实际)] * (1 /数量 _ 观察值)****

**一个模型的 MSE 告诉我们平均起来我们错了多少，但是有一个转折——通过在平均之前平方我们预测的误差，我们对远远偏离的预测的惩罚比那些稍微偏离的预测更严重。线性回归和逻辑回归的成本函数以非常相似的方式运作。**

**好的，我们有一个成本函数来最小化。是时候点火了[梯度下降](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)对吗？**

**没那么快—要使用梯度下降，我们需要知道我们的成本函数的梯度，即指向最大陡度方向的向量(我们希望在梯度的相反方向上重复采取步骤，以最终达到最小值)。**

**除了在神经网络中，我们有如此多的可变权重和相互关联的偏差。我们如何计算所有这些的梯度呢？在下一节中，我们将看到反向传播如何帮助我们处理这个问题。**

## **梯度下降快速回顾**

**函数的梯度是向量，其元素是它对每个参数的偏导数。例如，如果我们试图最小化一个成本函数 C(B0，B1)，只有两个可变参数 B0 和 B1，则梯度为:**

> ****C(B0，B1) = [ [dC/dB0]，[dC/dB1] ]** 的梯度**

**因此，梯度的每个元素都告诉我们，如果我们对特定的参数进行微小的改变，成本函数会如何改变——这样我们就知道要调整什么以及调整多少。总而言之，我们可以通过以下步骤向最小值迈进:**

**![](img/172a1741813b3251eeaff1d5c9b7de6b.png)**

**Illustration of Gradient Descent**

1.  **计算我们“当前位置”的梯度(使用我们当前的参数值计算梯度)。**
2.  **按照与其渐变元素成比例且与其渐变元素方向相反的量来修改每个参数。例如，如果我们的成本函数相对于 B0 的偏导数为正但很小，相对于 B1 的偏导数为负且很大，那么我们希望将 B0 减少很小的量，将 B1 增加很大的量，以降低我们的成本函数。**
3.  **重新计算梯度使用我们新的调整参数值，并重复前面的步骤，直到我们达到最小值。**

# **反向传播**

**我会遵从这本[伟大的教科书(在线免费！)对于详细的数学(如果你想更深入的了解神经网络，一定要去查一下)](http://neuralnetworksanddeeplearning.com/)。相反，我们将尽最大努力建立对反向传播如何以及为什么工作的直观理解。**

**请记住，前向传播是通过神经网络向前移动的过程(从输入到最终输出或预测)。反向传播是相反的。除了代替信号，我们通过我们的模型向后移动误差。**

**当我试图理解反向传播过程时，一些简单的可视化帮助很大。下面是我脑海中一个简单的神经网络从输入到输出的图片。该过程可以总结为以下步骤:**

*   **输入被输入到神经元的蓝色层，并通过每个神经元中的权重、偏置和 sigmoid 进行修改，以获得激活。例如:*Activation _ 1 = Sigmoid(Bias _ 1+W1 * Input _ 1)***
*   **来自蓝色层的激活 1 和激活 2 被馈入品红色神经元，品红色神经元使用它们来产生最终的输出激活。**

**前向传播的目的是计算每个连续隐藏层的每个神经元的激活，直到我们得到输出。**

**![](img/692bebb7a3180e2a307a372d70620312.png)**

**Forward propagation in a neural network**

**现在让我们把它反过来。如果你沿着红色的箭头(在下面的图片中)，你会注意到我们现在从洋红色神经元的输出开始。这是我们的输出激活，我们用它来做预测，也是我们模型中误差的最终来源。**然后，我们通过我们用于前向传播信号**的相同权重和连接，在我们的模型中向后移动该误差(因此，代替激活 1，现在我们有误差 1——归因于顶部蓝色神经元的误差)。**

**还记得我们说过正向传播的目标是一层一层地计算神经元激活，直到我们得到输出吗？我们现在可以用类似的方式陈述反向传播的目的:**

> **我们想要计算归因于每个神经元的误差(我将把这个误差量称为神经元的误差，因为一遍又一遍地说“归因”是没有意思的)，从最接近输出的层开始，一直回到我们的模型的起始层。**

**![](img/e785de43aace25fbc97af76294687e07.png)**

**Backpropagation in a neural network**

**那么，我们为什么要关心每个神经元的误差呢？请记住，神经网络的两个构建模块是将信号传递到特定神经元的连接(每个连接中都有一个权重)和神经元本身(有一个偏差)。**整个网络中的这些权重和偏差也是我们调整以改变模型**所做预测的刻度盘。**

**这一部分非常重要:**

> ****特定神经元的误差大小**(相对于所有其他神经元的误差)**与该神经元的输出(又名激活)对我们的成本函数**的影响成正比。**

**因此，每个神经元的误差代表了成本函数相对于该神经元输入的偏导数。这具有直观的意义——如果一个特定的神经元比所有其他神经元有更大的误差，那么调整我们违规神经元的权重和偏差将比摆弄任何其他神经元对我们模型的总误差产生更大的影响。**

**并且关于每个权重和偏差的偏导数是组成我们的成本函数的梯度向量的单独元素。**因此，基本上反向传播允许我们计算归因于每个神经元的误差，进而允许我们计算偏导数并最终计算梯度，以便我们可以利用梯度下降**。万岁！**

## **一个有帮助的类比——责备游戏**

**有很多东西需要消化，所以希望这个类比会有所帮助。几乎每个人在生活中的某个时候都有一个糟糕的同事——当事情出错时，他总是玩推卸责任的游戏，把同事或下属扔到公共汽车底下。**

**神经元，通过反向传播，是责备游戏的大师。当错误被反向传播到特定的神经元时，**该神经元将迅速有效地将矛头指向上游同事(或同事)，他们是造成错误的最大原因**(即，第 4 层神经元将矛头指向第 3 层神经元，第 3 层神经元指向第 2 层神经元，等等)。**

**![](img/e17ae883d8c37c457fb208333952d4fc.png)**

**Neurons blame the most active upstream neurons**

**当神经元不能直接观察到其他神经元的错误时，每个神经元如何知道该责备谁？**他们只是根据最高和最频繁的激活来看谁给他们发送了最多的信号**。就像在现实生活中一样，那些安全行事的懒人(低激活率和不频繁激活率)会逃避责任，而那些工作最多的神经元会受到指责，并修改其权重和偏好。玩世不恭是的，但也非常有效地让我们得到一套最佳的权重和偏见，使我们的成本函数最小化。左边是神经元如何把彼此扔到巴士下面的图像。**

**简而言之，这就是反向传播过程背后的直觉。在我看来，这是反向传播的三个关键点:**

1.  ****它是将误差一层一层向后移位，并将正确的误差量归属于神经网络中的每个神经元的过程。****
2.  ****归因于特定神经元的误差是改变该神经元的权重(来自通向神经元的连接)和偏差将如何影响成本函数的良好近似。****
3.  ****当向后看时，更活跃的神经元(非懒惰的)是那些被反向传播过程指责和调整的神经元。****

# **把这一切联系在一起**

**如果你一路读到这里，那么我对你表示感谢和钦佩(对你的坚持)。**

**我们从一个问题开始，“是什么让深度学习变得特别？”我现在将尝试回答这个问题(主要从基本神经网络的角度，而不是像 CNN、RNNs 等更高级的表亲的角度)。).依我拙见，以下几个方面使神经网络变得特别:**

*   **每个神经元都是自己的微型模型，有自己的偏好和一组输入特征和权重。**
*   **每个单独的模型/神经元通过模型的所有隐藏层馈入许多其他单独的神经元。因此，我们最终将模型插入到其他模型中，其总和大于其组成部分。这使得神经网络能够拟合我们数据的所有角落和缝隙，包括非线性部分(但要小心过度拟合——并且一定要考虑[正则化](https://en.wikipedia.org/wiki/Regularization_(mathematics))，以保护你的模型在面对新的和超出样本的数据时不会表现不佳)。**
*   **许多互连模型方法的多功能性以及反向传播过程高效且最优地设置每个模型的权重和偏差的能力，使得神经网络能够以许多其他算法无法实现的方式从数据中稳健地“学习”。**

*****作者注*** *:神经网络和深度学习是极其复杂的学科。我仍然处于了解它们的早期阶段。写这篇博客是为了发展我自己的理解，也是为了帮助你，读者。我期待您所有的评论、建议和反馈。干杯！***

**如果你总体上喜欢这篇文章和我的写作，请考虑通过我的推荐链接注册 Medium 来支持我的写作。谢谢！**

*****来源:*****

**[*神经网络和深度学习作者迈克尔·a·尼尔森*](http://neuralnetworksanddeeplearning.com/index.html)**

**[*维基百科:反向传播*](https://en.wikipedia.org/wiki/Backpropagation#Finding_the_derivative_of_the_error)**