<html>
<head>
<title>Reinforcement Learning — Model Based Planning Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——基于模型的规划方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8?source=collection_archive---------6-----------------------#2019-07-06">https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-5e99cae0abb8?source=collection_archive---------6-----------------------#2019-07-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1a03" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习环境模型的例子</h2></div><p id="523d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在以前的文章中，我们已经谈到了基于无模型方法的强化学习方法，这也是 RL 学习的关键优势之一，因为在大多数情况下，学习环境的模型可能是棘手和艰难的。但是，如果我们想学习一个环境模型，或者如果我们已经有了一个环境模型，我们如何利用它来帮助学习过程呢？在这篇文章中，我们将一起探讨以环境为模型的 RL 方法。以下内容的结构如下:</p><ol class=""><li id="0c68" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">从如何建模环境的基本概念开始</li><li id="7d41" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">用我们刚刚学到的理论用 Python 实现一个例子</li><li id="1ee3" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">将理论扩展到更一般情况的进一步想法</li></ol><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/499b598903e6646b44a714e35fbc58ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kbl-s1V9jOHXm4-37_rAJg.jpeg"/></div></div></figure><h1 id="5a56" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">模拟环境</h1><p id="4190" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">一个代理从一个状态开始，通过在那个状态采取一个可用的动作，环境给它反馈，相应地代理进入下一个状态并接收奖励(如果有的话)。在这个一般的设定中，环境给了一个代理两个信号，一个是它在设定中的下一个状态，另一个是奖励。因此，当我们说对环境建模时，我们是在对从<code class="fe my mz na nb b">(state, action)</code>到<code class="fe my mz na nb b">(nextState, reward)</code>的函数映射建模。例如，考虑一个网格世界设定中的情况，一个代理用头撞墙，作为回应，代理呆在原地，获得奖励 0，那么最简单的格式，模型函数会是<code class="fe my mz na nb b">(state, action)--&gt;(state, 0)</code>，表示这个特定状态和动作的代理，代理会呆在原地，获得奖励 0。</p><h2 id="3f6d" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">算法</h2><p id="ac60" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">现在让我们来看看一个环境模型是如何帮助改善 Q-learning 过程的。我们首先介绍一种最简单的算法，叫做<strong class="kh ir"> Dyna-Q </strong>:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi no"><img src="../Images/0b1b70c44cdf17c91bb341ed8e333ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_rbF8SRhVD4Iqwrv4yzbqA.png"/></div></div></figure><p id="c470" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Q-learning 利用模型来备份策略的方式简单而直接。首先，<code class="fe my mz na nb b">a, b, c, d</code>步骤与一般 Q-learning 步骤完全相同(如果你不熟悉 Q-learning，请查看我这里的例子<a class="ae np" rel="noopener" target="_blank" href="/implement-grid-world-with-q-learning-51151747b455"/>)。唯一的区别在于步骤<code class="fe my mz na nb b">e</code>和<code class="fe my mz na nb b">f</code>，在步骤<code class="fe my mz na nb b">e</code>中，基于确定性环境的假设记录环境的模型(对于非确定性和更复杂的环境，可以根据具体情况制定更一般的模型)。<strong class="kh ir">步骤</strong> <code class="fe my mz na nb b"><strong class="kh ir">f</strong></code> <strong class="kh ir">可以简单概括为应用正在学习的模型并更新 Q 函数</strong> <code class="fe my mz na nb b">n</code> <strong class="kh ir">乘以</strong>，其中<code class="fe my mz na nb b">n</code>是预定义的参数。步骤<code class="fe my mz na nb b">f</code>中的备份与步骤<code class="fe my mz na nb b">d</code>中的备份完全相同，并且<strong class="kh ir">您可能认为这是重复代理已经经历过几次的事情，以便强化学习过程</strong>。</p><blockquote class="nq nr ns"><p id="bad4" class="kf kg nt kh b ki kj jr kk kl km ju kn nu kp kq kr nv kt ku kv nw kx ky kz la ij bi translated">典型地，如在<strong class="kh ir"> Dyna-Q </strong>中，相同的强化学习方法既用于从真实经验中学习，也用于从模拟经验中规划。因此，强化学习方法是学习和规划的“最终共同路径”。</p></blockquote><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nx"><img src="../Images/90e328e469a6dc9f8a7de4eead42d885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Da0U79-88o4Vm1AmxlAXrw.png"/></div></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk">The general Dyna Architecture</figcaption></figure><p id="39b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上图更直接地显示了 Dyna 方法的一般结构。请注意<code class="fe my mz na nb b">Policy/value functons</code>中的两个向上的箭头，在大多数情况下是我们之前讨论过的 Q 函数，其中一个箭头来自<code class="fe my mz na nb b">direct RL update</code>到<code class="fe my mz na nb b">real experience</code>，在这种情况下，它相当于代理在环境中探索，另一个来自<code class="fe my mz na nb b">planning update</code>到<code class="fe my mz na nb b">simulated experience</code>，在这种情况下，它重复代理从<code class="fe my mz na nb b">real experience</code>学习的模型。<strong class="kh ir">因此，在每次采取行动时，通过更新来自实际行动和模型模拟的 Q 函数来加强学习过程</strong>。</p><h1 id="fafd" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">实施 Dyna Maze</h1><p id="a392" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我认为理解算法的最好方法是实现一个实际的例子。我将从<em class="nt">强化学习 an introduction </em>中取例子，用 Python 实现并与一般的没有规划步骤的 Q 学习(模型模拟)进行比较。</p><h2 id="ffee" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">游戏设置</h2><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1dfcbeb187a3a99c0bb3b8d4021cedd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7Ju1zTNg0dJB3lxS2x_syQ.png"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk">Dyna Maze Board</figcaption></figure><p id="a790" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑图中插图所示的简单迷宫。在 47 个状态中的每一个状态中都有四个动作，<code class="fe my mz na nb b">up</code>、<code class="fe my mz na nb b">down</code>、<code class="fe my mz na nb b">right</code>和<code class="fe my mz na nb b">left</code>，它们将代理确定性地带到相应的邻近状态，除非当移动被障碍物或迷宫的边缘阻挡时，在这种情况下代理保持在它原来的位置。在所有转换中，奖励为零，除了那些进入目标状态的转换，它是<code class="fe my mz na nb b">+1</code>。在到达目标状态<code class="fe my mz na nb b">(G)</code>后，代理返回到开始状态<code class="fe my mz na nb b">(S)</code>以开始新的一集。</p><p id="7926" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该实现的整个结构有两个类，第一个类代表电路板，也是环境，它能够</p><ol class=""><li id="0947" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">接受一个动作并输出代理的下一个状态(或位置)</li><li id="a913" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">给予相应的奖励</li></ol><p id="15ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二类代表代理，它能够</p><ol class=""><li id="a8d8" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">在棋盘上探索</li><li id="a730" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">跟踪环境的模型</li><li id="ca88" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">一路更新 Q 函数。</li></ol><h2 id="acb5" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">董事会实施</h2><p id="099d" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">第一类的棋盘设置与我们之前谈过的许多棋盘游戏相似，你可以在这里查看完整的实现<a class="ae np" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py" rel="noopener ugc nofollow" target="_blank"/>。我将在这里删除我的解释(您可以查看我以前的文章以查看更多示例)，因此，我们将有一个类似于以下内容的板:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi od"><img src="../Images/d36dcbc4d0083bf4082dc66db0a704cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qwdM8Y1h8zowBauQUCHpZw.png"/></div></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk">Board Implementation</figcaption></figure><p id="b9a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">棋盘用一个 numpy 数组表示，其中<code class="fe my mz na nb b">z</code>表示块，<code class="fe my mz na nb b">*</code>表示代理的当前位置，<code class="fe my mz na nb b">0</code>表示空的和可用的位置。</p><h2 id="089f" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">代理实现</h2><h2 id="1eb3" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">初始化</h2><p id="2b6a" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">首先，在<code class="fe my mz na nb b">init</code>函数中，我们将初始化算法所需的所有参数。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="514d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了那些通用的 Q-learning 设置(learning rate，state_actions，…)之外，<code class="fe my mz na nb b">(state, action) -&gt; (reward, state)</code>的一个模型也被初始化为 python 字典，该模型只会随着 agent 在环境中的探索而更新。<code class="fe my mz na nb b">self.steps</code>是模型在每一个动作采取中用于更新 Q 函数的次数，<code class="fe my mz na nb b">self.steps_per_episode</code>用于记录每一集的步数(在下面的算法比较中我们会把它作为一个关键的度量)。</p><h2 id="1b47" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">选择操作</h2><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="5f1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe my mz na nb b">chooseAction</code>函数中，代理仍将采取ϵ-greedy 行动，其中它有<code class="fe my mz na nb b">self.exp_rate</code>概率采取随机行动，有<code class="fe my mz na nb b">1 — self.exp_rate</code>概率采取贪婪行动。</p><h2 id="ced8" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">模型学习和策略更新</h2><p id="e7e1" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">现在，让我们使用随着代理的探索而学习的模型来了解策略更新的关键点。</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="oe of l"/></div></figure><p id="60a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个实现完全遵循我们上面列出的算法。在每一集(玩游戏)中，第一轮 Q 函数更新后，模型也会用<code class="fe my mz na nb b">self.model[self.state][action]=(reward, nxtState)</code>更新，之后 Q 函数会重复更新<code class="fe my mz na nb b">self.steps</code>的次数。注意，在循环中，<code class="fe my mz na nb b">state</code>和<code class="fe my mz na nb b">action</code>都是从之前的观察中随机选择的。</p><h2 id="8cc9" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">尝试不同的步骤</h2><p id="9dff" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">当步数设置为 0 时，Dyna-Q 方法本质上是 Q 学习。我们来对比一下 0、5、50 步的学习过程。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi og"><img src="../Images/cf5b94c1420ca7989ce8c1a1e4747c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*QtgYp-8RZYClFbg4XIJ-ag.png"/></div><figcaption class="ny nz gj gh gi oa ob bd b be z dk">Dyna-Q with different steps</figcaption></figure><p id="72d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">x 轴是集数，y 轴是达到目标的步数。任务是尽可能快地达到目标。从学习曲线中，我们观察到规划代理人(具有模拟模型)的学习曲线比非规划代理人稳定得更快。提到萨顿书中的话:</p><blockquote class="nq nr ns"><p id="9587" class="kf kg nt kh b ki kj jr kk kl km ju kn nu kp kq kr nv kt ku kv nw kx ky kz la ij bi translated">在没有计划的情况下(n = 0)，每一集只向策略添加一个额外的步骤，因此到目前为止只学习了一个步骤(最后一个)。有了计划，在第一集期间也只学习了一个步骤，但是在第二集期间已经开发了一个广泛的策略，在该集结束时将几乎回到开始状态</p></blockquote><p id="5136" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">额外的模型模拟和备份进一步增强了代理的体验，从而导致更快和更稳定的学习过程。(结帐<a class="ae np" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py" rel="noopener ugc nofollow" target="_blank">完全实现</a>)</p><h1 id="b8b0" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">如何概括这个想法？</h1><p id="6b1d" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我们在这里探索的例子肯定有有限的用途，因为状态是离散的，而动作是确定的。但是模拟环境以加速学习过程的想法有着无限的用途。</p><h2 id="1643" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">对于具有非确定性动作的离散状态</h2><p id="d022" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">可以学习概率模型，而不是我们上面介绍的直接的一对一映射。概率模型应该在学习过程中不断更新，并且在备份阶段，可以用概率分布非确定性地选择<code class="fe my mz na nb b">(reward, nextState)</code>。</p><h2 id="b59f" class="nc mc iq bd md nd ne dn mh nf ng dp ml ko nh ni mn ks nj nk mp kw nl nm mr nn bi translated">对于连续状态</h2><p id="ce34" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">Q 函数的更新将略有不同(我将在以后的文章中介绍)，关键是学习一个更复杂和通用的环境参数模型。这个过程可以包括一般的监督学习算法，其中当前状态、动作作为输入，下一个状态和奖励作为输出。</p><p id="a4e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的<a class="ae np" rel="noopener" target="_blank" href="/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb">帖子</a>中，我们将进一步学习改进 Dyna 方法的想法，并讨论模型错误时的情况！</p><p id="8289" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，请在这里查看完整的代码<a class="ae np" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/DynaMaze/DynaMaze.py" rel="noopener ugc nofollow" target="_blank"/>。欢迎您投稿，如果您有任何问题或建议，请在下面发表评论！</p><p id="de76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考</strong>:</p><ul class=""><li id="a422" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la oh lh li lj bi translated"><a class="ae np" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="a655" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la oh lh li lj bi translated"><a class="ae np" href="https://github.com/JaeDukSeo/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>