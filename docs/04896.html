<html>
<head>
<title>Classification of unbalanced datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据集的分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-of-unbalanced-datasets-8576e9e366af?source=collection_archive---------5-----------------------#2019-07-24">https://towardsdatascience.com/classification-of-unbalanced-datasets-8576e9e366af?source=collection_archive---------5-----------------------#2019-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c779" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当数据集不平衡时，如何使用 sklearn 正确地进行分类分析，并改进其结果。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d85ab7a5d251964113d68d3700caf024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dgi4FHro1_RQ5c5F"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Brett Jordan</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ee4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">假设您有一个包含十几个特征的数据集，并且需要对每个观察值进行分类。可以是两类问题(你的输出不是 1 就是 0；对或错)或多类问题(可能有两个以上的选择)。然而，在这种情况下，有一个转折。数据不平衡。想想那些可能患有或可能没有癌症的病人(大多数可能不会)或延长信用额度的决定(大多数银行客户都获得了延期)。你的机器学习算法会对一个类“曝光过度”，对另一个类“曝光不足”。网上有很多关于这个问题的文章，采用了不同的方法。在这里，我将结合其中的一些来得到一个健壮的解决方案。</p><p id="e9e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从更直接的线性模型开始，然后添加一些调整，移动到提升树，最后到神经网络。</p><blockquote class="me mf mg"><p id="08f1" class="kz la mh lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">你可以从<a class="ae ky" href="https://github.com/nastyh/Unbalanced-Dataset" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集和练习册。</p></blockquote><p id="7cc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将遵循以下步骤:</p><ul class=""><li id="9e13" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">加载数据和一些最需要的依赖项</li><li id="fe99" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">做一些最简单的预处理</li><li id="d5e8" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">建立几个简单的模型，作为进一步尝试的基础</li><li id="0495" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">花些时间在特征工程上</li><li id="4d34" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">运用各种方法来帮助我们处理不平衡的数据</li></ul><h2 id="25f8" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">装载和理解</h2><p id="f310" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">在这个练习中，我们将使用来自 Kaggle 的<a class="ae ky" href="https://www.kaggle.com/amanajmera1/framingham-heart-study-dataset" rel="noopener ugc nofollow" target="_blank">弗雷明汉心脏研究数据集</a>。它提出了一个二元分类问题，其中我们需要预测变量“TenYearCHD”(零或一)的值，该值显示患者是否会患心脏病。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="edcf" class="mz na it ny b gy oc od l oe of">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import scipy.stats as st<br/>import seaborn as sns<br/>import pandas_profiling<br/>%matplotlib inline</span><span id="74ad" class="mz na it ny b gy og od l oe of">df = pd.read_csv(r'path to dataset')</span></pre><p id="2854" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使初步的数据探索更方便一点。我最近看到了一篇由 Parul Pandey 撰写的<a class="ae ky" rel="noopener" target="_blank" href="/10-simple-hacks-to-speed-up-your-data-analysis-in-python-ec18c6396e6b">文章</a>名为“用 Python 加速数据分析的 10 个简单技巧”,并安装了一个<a class="ae ky" href="https://github.com/pandas-profiling/pandas-profiling" rel="noopener ugc nofollow" target="_blank">剖析</a>包。</p><p id="919e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只需一个命令，它就能做一些非常方便的事情:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="5b00" class="mz na it ny b gy oc od l oe of"># looking at stats<br/>pandas_profiling.ProfileReport(df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/bc1735e78fdf2918e6adfe9a53fb4fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*OYVI6lBYypbE4yuA9hmFRA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The summary output after using Profiling</figcaption></figure><p id="8674" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，数据相当干净:我们在这里和那里有丢失的值，但是我们将很快处理它们。</p><p id="c54e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图中的特征之间的相关性也没有说太多:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/624ac06c493a8f8d0f99e02ee5eab142.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xBIBD2miotd9gm3SF8OIwA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pearson correlation from Profiling</figcaption></figure><p id="28ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mh">吸烟和舒张压/收缩压周围的两个红色矩形相当明显:通常吸烟和高血压是相关的</em></p><p id="b1b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数特性的名称都很好地解释了每个变量的含义，但有几个不太明显:</p><p id="1828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“教育”可以是:</p><p id="7967" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1 —对于某些高中来说</p><p id="68f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2 —高中普通教育</p><p id="fb9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3 —一些大学或职业学校</p><p id="dcf2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">4 —学院</p><p id="3228" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BPMeds 是一个二元变量，其中</p><p id="ebae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0-表示患者不服用任何降压药</p><p id="7173" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1-表示相反的意思</p><p id="3006" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们手动检查目标变量:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="3e38" class="mz na it ny b gy oc od l oe of">df['TenYearCHD'].value_counts(normalize = True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/32f15400898e6bb8849da1910d9e1bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*RjIi8S4UnuasDhC6ekWXug.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Only 15% of patients have been diagnosed with a decease</figcaption></figure><p id="3545" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你是个视觉型的人:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="2a29" class="mz na it ny b gy oc od l oe of">sns.countplot(x='TenYearCHD',data=df)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/89aa75e3426a3d869d470d365e3b23b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*TESmGQNzSnZe1TSXjM46eQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Same information but shown as a bar chart</figcaption></figure><p id="8391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这不是一个<em class="mh">严重</em>不平衡的集合，但可能会扭曲线性和非线性算法。</p><h2 id="399c" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">数据预处理</h2><p id="a154" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">我能想到几个迫在眉睫的问题:</p><ul class=""><li id="edbd" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">缺失值和 NaNs</li><li id="ffdc" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">“教育”列可能被视为序数，也可能不被视为序数</li><li id="edfa" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">数据的规范化或标准化(通常属于这一部分，但我将在以后做，一旦我们开始构建一个需要规范化特征的模型)</li></ul><p id="c744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“BPMeds”有一些缺失值，但是大约 96%的列为零(没有服用降压药)。所以，用零填充 NaNs 应该是公平的。</p><p id="0fda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“葡萄糖”和“总胆固醇”都是连续变量，我们将使用平均列值来填充空单元格。</p><p id="4c5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“身体质量指数”和“心率”似乎也能很好地适应平均值。</p><p id="e1ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“吸烟日”需要一个两步走的方法。它有 29 个缺失值。你可能认为使用平均值就可以了。然而，“cigsPerDay”与另一个二元特征“currentSmoker”相关联。所以，你可以把一个不吸烟的人叫做 NaN，然后给这个人分配一些平均数量的香烟。我们希望避免这种情况。让我们看看如何做到这一点。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="06e5" class="mz na it ny b gy oc od l oe of">df['cigsPerDay'].value_counts(normalize = True).plot(kind="bar")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/a9a78b9192dbbbb0f28bc653883a352e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SsIGTK7Jqxl7h502TM1Mtw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Distribution of the number of cigarettes smoked</figcaption></figure><p id="5a15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数人不吸烟，我们不想给他们分配香烟。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="09c0" class="mz na it ny b gy oc od l oe of">df['cigsPerDay'][df['currentSmoker']==0].isna().sum()</span></pre><p id="4f9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该命令返回零，因此似乎我们没有针对任何非吸烟者的 NaNs。但是利用。mean()还是不好，因为它会偏向零。我们希望只对吸烟者应用 fillna()命令:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="9371" class="mz na it ny b gy oc od l oe of"># creating a boolean array of smokers<br/>smoke = (df['currentSmoker']==1)</span><span id="5b87" class="mz na it ny b gy og od l oe of"># applying mean to NaNs in cigsPerDay but using a set of smokers only<br/>df.loc[smoke,'cigsPerDay'] = df.loc[smoke,'cigsPerDay'].fillna(df.loc[smoke,'cigsPerDay'].mean())</span></pre><p id="5b8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，现在吸烟的平均数量刚刚超过 18 支:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="1c67" class="mz na it ny b gy oc od l oe of">df['cigsPerDay'][df['currentSmoker']==1].mean()</span></pre><p id="48ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们决定走一条更简单的路，不排除不吸烟者，这个值将是 9 左右。</p><p id="50f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以肯定的是，不吸烟的人不会被赋予任何价值:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6fbd" class="mz na it ny b gy oc od l oe of">df['cigsPerDay'][df['currentSmoker']==0].mean()</span></pre><p id="2b38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“教育”是一个有趣的话题。虽然它本身是一个分类变量<em class="mh">，</em>它已经为我们编码了，更高的教育水平对应着更大的数字。这里的一个潜在问题是，“大学”和“某所大学或职业学校”之间的“间隔”可能与“高中或 GED”和“某所大学或职业学校”之间的“间隔”不同然而，数字说的是相反的:四和三之间的距离与二和三之间的距离完全相同。你可以通过谷歌搜索<a class="ae ky" href="https://www.google.com/search?q=ordinal+variables+in+logistic+regression&amp;rlz=1C1CHBF_enUS841US841&amp;oq=ordinal+varables+in+&amp;aqs=chrome.4.69i57j0l5.6763j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank">“逻辑回归中的序数值”</a>或者浏览<a class="ae ky" href="https://www3.nd.edu/~rwilliam/stats3/OrdinalIndependent.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>来了解这个问题。现在让我们假设距离是相似的。也就是说，我们将去掉“教育”中的 NaNs，以便回归可以与列一起工作。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="3ddb" class="mz na it ny b gy oc od l oe of">df['education'].value_counts(normalize = True).plot(kind="bar")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/861aad9bbe89711d5cbf2fc185aeb747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qz7x0OINh6V11R94vZDyng.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Breakdown of “education”</figcaption></figure><p id="c65c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里用“1”代替 NaNs 似乎是公平的。</p><p id="50e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">立刻做出所有该做的改变:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="089c" class="mz na it ny b gy oc od l oe of"># Filling out missing values<br/>df['BPMeds'].fillna(0, inplace = True)<br/>df['glucose'].fillna(df.glucose.mean(), inplace = True)<br/>df['totChol'].fillna(df.totChol.mean(), inplace = True)<br/>df['education'].fillna(1, inplace = True)<br/>df['BMI'].fillna(df.BMI.mean(), inplace = True)<br/>df['heartRate'].fillna(df.heartRate.mean(), inplace = True)</span></pre><p id="293b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们检查它是否通过:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="a0ce" class="mz na it ny b gy oc od l oe of">df.isna().sum()</span></pre><p id="e033" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们准备好了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/107a7925c076cc47c28d9ba8c6b14ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*QtefMUupELMQ6UXhYf3kDw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">No NaNs left</figcaption></figure><h2 id="1fcd" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">基本型号</h2><p id="fb76" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">我们将从一个随机的森林开始，也将看看我们的功能的重要性指标。</p><p id="a8f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不要忘记将特征与目标变量分开:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="3970" class="mz na it ny b gy oc od l oe of">features = df.iloc[:,:-1]<br/>result = df.iloc[:,-1]</span></pre><p id="533b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型本身在下面。</p><p id="7ffc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">依赖关系优先:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="7acf" class="mz na it ny b gy oc od l oe of">import sklearn<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.metrics import roc_auc_score<br/>from sklearn.preprocessing import StandardScaler</span></pre><p id="fa40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练/测试分割:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="7956" class="mz na it ny b gy oc od l oe of">X_train, X_test, y_train, y_test = train_test_split(features, result, test_size = 0.2, random_state = 14)</span></pre><p id="89f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">配件:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6982" class="mz na it ny b gy oc od l oe of">rf = RandomForestClassifier()<br/>rf.fit(X_train, y_train)</span><span id="717a" class="mz na it ny b gy og od l oe of"># Making predictions on unseen data<br/>predictions_rf = rf.predict(X_test)</span></pre><p id="c6bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征的重要性(有时，为了加快速度，您希望使用较少的特征构建模型):</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6009" class="mz na it ny b gy oc od l oe of"># what features are the most important?<br/>plt.plot(rf.feature_importances_)<br/>plt.xticks(np.arange(X_train.shape[1]), X_train.columns.tolist(), rotation=90)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4b1622fea80cd17b2c2a7421e8bcad3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*YI5K2a7La-9_phYDYyTdjg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Some features are definitely more important</figcaption></figure><p id="306b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您希望它是一个列表:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="fba9" class="mz na it ny b gy oc od l oe of"># View a list of the features and their importance scores<br/>list(zip(features, rf.feature_importances_))</span></pre><p id="e385" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">评估模型:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="908b" class="mz na it ny b gy oc od l oe of">print(classification_report(y_test, predictions_rf))<br/>print(confusion_matrix(y_test, predictions_rf))</span><span id="5123" class="mz na it ny b gy og od l oe of"># Under ROC curve<br/>prob_rf = rf.predict_proba(X_test)<br/>prob_rf = [p[1] for p in prob_rf]<br/>print(roc_auc_score(y_test, prob_rf))</span></pre><p id="ff75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有两个结论:</p><ul class=""><li id="c3bd" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated">84.90%的准确率几乎无法击败随机猜测。记住，84.81%的数据被标记为零。所以我们比猜测高出 0.09%。</li><li id="4245" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated">此外，混淆矩阵显示了大量的假阴性:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/731fe8d6e25e4bc17ac61f75cc6e0954.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*0m1m9ZRkMtEWBHfrFFXoAg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Lots of type II errors</figcaption></figure><p id="1b11" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望避免这种情况，因为模型错误地指出患者没有问题，而实际上他或她有问题！我们将很快解决这个问题，但是现在，让我们只使用重要的特征来重建模型。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="ddd7" class="mz na it ny b gy oc od l oe of">from sklearn.feature_selection import SelectFromModel<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="8ad3" class="mz na it ny b gy og od l oe of"># Create a selector object that will use the random forest classifier to identify<br/># features that have an importance of more than 0.12<br/>sfm = SelectFromModel(clf, threshold=0.12)</span><span id="3e90" class="mz na it ny b gy og od l oe of"># Train the selector<br/>sfm.fit(X_train_std, y_train)</span><span id="0a07" class="mz na it ny b gy og od l oe of">feat_labels = list(features.columns.values) # creating a list with features' names<br/>for feature_list_index in sfm.get_support(indices=True):<br/>    print(feat_labels[feature_list_index])</span><span id="e65a" class="mz na it ny b gy og od l oe of">importances = clf.feature_importances_<br/>std = np.std([tree.feature_importances_ for tree in clf.estimators_],<br/>             axis=0)<br/>indices = np.argsort(importances)[::-1]</span><span id="c5a5" class="mz na it ny b gy og od l oe of">print("Feature ranking:")</span><span id="4cf0" class="mz na it ny b gy og od l oe of">for f in range(X_train_std.shape[1]):<br/>    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))</span><span id="2846" class="mz na it ny b gy og od l oe of"># Plot the feature importances of the forest<br/>plt.figure()<br/>plt.title("Feature importances")<br/>plt.bar(range(X_train_std.shape[1]), importances[indices],<br/>       color="r", yerr=std[indices], align="center")<br/>plt.xticks(range(X_train_std.shape[1]), indices)<br/>plt.xlim([-1, X_train_std.shape[1]])<br/>plt.show()</span><span id="bdaa" class="mz na it ny b gy og od l oe of"># with only important features. Can check X_important_train.shape[1]<br/>X_important_train = sfm.transform(X_train_std)<br/>X_important_test = sfm.transform(X_test_std)</span><span id="d396" class="mz na it ny b gy og od l oe of">clf_important = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)<br/>clf_important.fit(X_important_train, y_train)</span><span id="fb57" class="mz na it ny b gy og od l oe of">predictions_y_4 = clf_important.predict(X_important_test)<br/>print(classification_report(y_test, predictions_y_4))<br/>print(confusion_matrix(y_test, predictions_y_4))<br/>accuracy_score(y_test, predictions_y_4)<br/># Under ROC curve<br/>prob_y_4 = clf_important.predict_proba(X_important_test)<br/>prob_y_4 = [p[1] for p in prob_y_4]<br/>print(roc_auc_score(y_test, prob_y_4))</span></pre><p id="e052" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我选择了重要性大于 0.12 的特性，并仅使用这些列重建了随机森林。结果非常相似，但我们能够节省一些计算能力。</p><h2 id="28a4" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">逻辑回归</h2><p id="cb97" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">回归需要标准化的特征:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="030f" class="mz na it ny b gy oc od l oe of">scaler = StandardScaler()<br/>X_train_std = scaler.fit_transform(X_train)<br/>X_test_std = scaler.fit_transform(X_test)</span></pre><p id="ac4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型是:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="759b" class="mz na it ny b gy oc od l oe of">logmodel = LogisticRegression(solver='liblinear')<br/>logmodel.fit(X_train_std, y_train)<br/>predictions_y_2 = logmodel.predict(X_test_std)</span></pre><p id="ae04" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和评估结果:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="3457" class="mz na it ny b gy oc od l oe of">print(classification_report(y_test, predictions_y_2))<br/>print(confusion_matrix(y_test, predictions_y_2))</span><span id="cfa6" class="mz na it ny b gy og od l oe of"># Under ROC curve<br/>prob_y_2 = logmodel.predict_proba(X_test_std)<br/>prob_y_2 = [p[1] for p in prob_y_2]<br/>print(roc_auc_score(y_test, prob_y_2))</span></pre><p id="b03d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果与 random forest 非常相似:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/00de365c567f1ddfa523f5ab496c6084.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*XKpD7PtHeKWxYghOPRkAKw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Just two fewer type II errors</figcaption></figure><p id="b62d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它在假阴性上表现稍好。但这仍然不够。</p><p id="60d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LogisticRegression 有一个参数<em class="mh"> class_weight </em>，这将有助于提高准确性。让我们看看:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="65c3" class="mz na it ny b gy oc od l oe of">logmodel = LogisticRegression(solver='liblinear', class_weight='balanced')<br/>logmodel.fit(X_train_std, y_train)<br/>predictions_y_3 = logmodel.predict(X_test_std)</span><span id="7ba6" class="mz na it ny b gy og od l oe of"><br/>print(classification_report(y_test, predictions_y_3))<br/>print(confusion_matrix(y_test, predictions_y_3))<br/>accuracy_score(y_test, predictions_y_3)<br/># Under ROC curve<br/>prob_y_3 = logmodel.predict_proba(X_test_std)<br/>prob_y_3 = [p[1] for p in prob_y_3]<br/>print(roc_auc_score(y_test, prob_y_3))</span></pre><p id="87b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这实际上并没有什么帮助:我们减少了第二类错误的数量，但却牺牲了整体的准确性。看来我们需要转向一些不同的东西。</p><p id="2f73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以手动实现<em class="mh"> class_weight </em>，通过传递一个 0 和 1 值的分解，您可以使用<em class="mh"> value_counts: </em>快速获得</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="fa9b" class="mz na it ny b gy oc od l oe of">df['TenYearCHD'].value_counts(normalize = True)</span><span id="c480" class="mz na it ny b gy og od l oe of">weights = {0 : '0.848113', 1 : '0.151887'}<br/>logmodel_auto = LogisticRegression(class_weight = weights, solver = 'liblinear')<br/>logmodel_auto.fit(X_train_std, y_train)<br/>predictions_std_auto = logmodel_auto.predict(X_test_std)</span><span id="03bc" class="mz na it ny b gy og od l oe of">print(classification_report(y_test, predictions_std_auto))<br/>print(confusion_matrix(y_test, predictions_std_auto))<br/>accuracy_score(y_test, predictions_std_auto)<br/># Under ROC curve<br/>prob_y_4 = logmodel.predict_proba(X_test_std)<br/>prob_y_4 = [p[1] for p in prob_y_4]<br/>print(roc_auc_score(y_test, prob_y_4))</span></pre><p id="f4fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，在第一类错误上做得很好:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/7cedd5ce0334d53a3b59493069e64327.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*tKXwCrFo20dx9zqMj0mVeA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Passing weights manually removes all Type I errors</figcaption></figure><p id="2778" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以尝试不同的组合。通过反复试验(或在 GridSearch 的帮助下)，您可以找到一个符合您目的的方法，例如:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6f85" class="mz na it ny b gy oc od l oe of">weights = {0 : '0.09042', 1 : '0.90958'}</span></pre><p id="9a94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您决定使用 GridSearch 来寻找合适的权重，这是它的实现方式:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="b0b8" class="mz na it ny b gy oc od l oe of">from sklearn.model_selection import GridSearchCV<br/>weights = np.linspace(0.03, 0.97, 55)</span><span id="2491" class="mz na it ny b gy og od l oe of">scaler = StandardScaler()<br/>features_std = scaler.fit_transform(features)</span><span id="a106" class="mz na it ny b gy og od l oe of">gsc = GridSearchCV(<br/>    estimator=LogisticRegression(solver='liblinear'),<br/>    param_grid={<br/>        'class_weight': [{0: x, 1: 1.0-x} for x in weights]<br/>    },<br/>    scoring='roc_auc',<br/>    cv=3<br/>)<br/>grid_result = gsc.fit(features_std, result)</span></pre><p id="5d22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我们测试了变量 TenYearCHD <strong class="lb iu"> </strong>从 3-97%崩溃到 97-3%崩溃的 0 和 1 的不同组合。<em class="mh"> GridSearchCV </em>可以评估不同的评估者，它们是在现场选择和控制的<em class="mh">评分。</em>通常，您会使用类似于分类的精确度和回归的 R 平方，但在我们的情况下，精确度不会提供太多信息，所以我决定使用 AUC 曲线下的面积。此处描述了所有可用的评分参数<a class="ae ky" href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="22ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以打印出最佳参数，并将其传回模型:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="4788" class="mz na it ny b gy oc od l oe of">print("Best parameters : %s" % grid_result.best_params_)</span><span id="d245" class="mz na it ny b gy og od l oe of"># passing weights found above<br/>rf_w = RandomForestClassifier(class_weight = {0:0.882962962962963, 1:0.11703703703703705})<br/>rf_w.fit(X_train, y_train)</span><span id="5773" class="mz na it ny b gy og od l oe of">print(classification_report(y_test, predictions_rf_w))<br/>print(confusion_matrix(y_test, predictions_rf_w))<br/>accuracy_score(y_test, predictions_rf_w)</span></pre><p id="095c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者到逻辑回归:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="b839" class="mz na it ny b gy oc od l oe of">weights = {0 : '0.882962962962963', 1 : '0.11703703703703705'}<br/>logmodel_auto_gridsearch = LogisticRegression(class_weight = weights, solver = 'liblinear')<br/>logmodel_auto_gridsearch.fit(X_train_std, y_train)<br/>predictions_std_auto_gridsearch = logmodel_auto_gridsearch.predict(X_test_std)</span><span id="fed6" class="mz na it ny b gy og od l oe of">print(classification_report(y_test, predictions_std_auto_gridsearch))<br/>print(confusion_matrix(y_test, predictions_std_auto_gridsearch))<br/>accuracy_score(y_test, predictions_std_auto_gridsearch)<br/># Under ROC curve<br/>prob_y_3_gridsearch = logmodel_auto_gridsearch.predict_proba(X_test_std)<br/>prob_y_3_gridsearch= [p[1] for p in prob_y_3_gridsearch]<br/>print(roc_auc_score(y_test, prob_y_3_gridsearch))</span></pre><h2 id="10e8" class="mz na it bd nb nc nd dn ne nf ng dp nh li ni nj nk lm nl nm nn lq no np nq nr bi translated">专门解决不平衡的数据</h2><p id="2f09" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">最流行的方法之一是上(下)采样。你可能还记得，我们的数据集的问题是，在训练阶段，算法看到的负面(零)情况远远多于正面，这使得模型不太准确。下一个合乎逻辑的事情是在相同数量的正面(1)和负面(0)情况下训练模型。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6c9b" class="mz na it ny b gy oc od l oe of">from sklearn.utils import resample</span><span id="0181" class="mz na it ny b gy og od l oe of">df_minority = df[df.TenYearCHD==1]<br/>df_majority = df[df.TenYearCHD==0]</span><span id="2e53" class="mz na it ny b gy og od l oe of">df['TenYearCHD'].value_counts()</span></pre><p id="e29b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我们将现有的数据集分成两部分:患心脏病的观察数据和其他数据。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="86f0" class="mz na it ny b gy oc od l oe of"># sample with replacement to match majority class and get #reproducible results<br/>df_minority_upsampled = resample(df_minority, <br/>                                 replace=True,     <br/>                                 n_samples=3596,    <br/>                                 random_state=123)<br/> <br/># Display new class counts<br/>df_upsampled.TenYearCHD.value_counts()</span></pre><p id="c681" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您现在所见，<em class="mh"> df_upsampled </em>具有相同数量的零和一观察值。</p><p id="a9de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将对其进行标准化，并再次将其放入模型中:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="20f3" class="mz na it ny b gy oc od l oe of"># Train/test, normalize the new data set<br/>features_upsampled = df_upsampled.iloc[:,:-1]<br/>result_upsampled = df_upsampled.iloc[:,-1]</span><span id="b4a2" class="mz na it ny b gy og od l oe of">X_train_upsampled, X_test_upsampled, y_train_upsampled, y_test_upsampled = train_test_split(features_upsampled, result_upsampled, test_size = 0.2, random_state = 14)</span><span id="1bf8" class="mz na it ny b gy og od l oe of">X_train_std_upsampled = scaler.fit_transform(X_train_upsampled)<br/>X_test_std_upsampled = scaler.fit_transform(X_test_upsampled)</span><span id="57b6" class="mz na it ny b gy og od l oe of"># new log model for upsampled data<br/>logmodel_upsampled = LogisticRegression(solver='liblinear')<br/>logmodel_upsampled.fit(X_train_std_upsampled, y_train_upsampled)<br/>predictions_y_2_upsampled = logmodel_upsampled.predict(X_test_std_upsampled)</span><span id="d849" class="mz na it ny b gy og od l oe of"><br/>print(classification_report(y_test_upsampled, predictions_y_2_upsampled))<br/>print(confusion_matrix(y_test_upsampled, predictions_y_2_upsampled))<br/>accuracy_score(y_test_upsampled, predictions_y_2_upsampled)<br/># Under ROC curve<br/>prob_y_2_upsampled = logmodel_upsampled.predict_proba(X_test_std_upsampled)<br/>prob_y_2_upsampled = [p[1] for p in prob_y_2_upsampled]<br/>print(roc_auc_score(y_test_upsampled, prob_y_2_upsampled))</span></pre><p id="b50a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就我们而言，结果已经恶化。为什么这样我们将在下次探讨它。不过，通常会有帮助。</p><p id="9037" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经提到，在这种情况下，错误 II 比错误 I 更严重。另一种解决方法是移动阈值(现在设置为 0.5)。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6583" class="mz na it ny b gy oc od l oe of">logmodel_lowering = LogisticRegression(solver='liblinear')<br/>logmodel_lowering.fit(X_train_std, y_train)</span><span id="1bbc" class="mz na it ny b gy og od l oe of">from sklearn.preprocessing import binarize<br/>for i in range(1,7):<br/>    cm2=0<br/>    predictions_y_2_lowering = logmodel_lowering.predict_proba(X_test_std)<br/>    y_pred2_lowering=binarize(predictions_y_2_lowering,i/10)[:,1]<br/>    cm2=confusion_matrix(y_test,y_pred2_lowering)<br/>    print ('With',i/10,'threshold the Confusion Matrix is ','\n',cm2,'\n',<br/>            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\n\n',<br/>          'Sensitivity: ',cm2[1,1]/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]/(float(cm2[0,0]+cm2[0,1])),'\n\n\n')</span></pre><p id="5fba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里写了一个循环，从 10%的阈值到 70%的阈值，然后显示结果。为了这个练习，我在这里拟合基本的逻辑回归模型。</p><p id="8f1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">迄今为止，我们只取得了有限的进展。可能是时候转向 XGBoost 了——一个最 Kaggle 比赛的首选武器。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="7a69" class="mz na it ny b gy oc od l oe of">import xgboost as xgb<br/>from sklearn.metrics import mean_squared_error</span><span id="1fdc" class="mz na it ny b gy og od l oe of">xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.05,<br/>max_depth = 9, alpha = 10, n_estimators = 20)</span><span id="a53b" class="mz na it ny b gy og od l oe of">eval_set = [(X_test_std, y_test)]<br/>xg_reg.fit(X_train_std, y_train, eval_metric="error", eval_set = eval_set, verbose = True)</span><span id="27fb" class="mz na it ny b gy og od l oe of">rmse = np.sqrt(mean_squared_error(y_test, prediction_y_5))<br/>print("RMSE: %f" % (rmse))</span></pre><p id="8e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与其他方法不同，XGBoost 可以在 fit 阶段报告和评估测试集的性能。我已经创建了<em class="mh"> eval_set，</em>将其传递给 fit 方法，并设置<em class="mh"> verbose = True </em>以实时查看详细程度。</p><p id="7b1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开箱即用，它返回的均方根误差(RMSE)为 0.3678。让我们努力降低它。为此，我们需要调整算法中传递的众多参数中的一些。你真的需要在<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html#" rel="noopener ugc nofollow" target="_blank">官方文档</a>中阅读它们，因为这是实现你的模型的卓越性能的关键。今天，我将重点介绍三个可用参数。同样，<em class="mh"> GridSearchCV </em>会为我们做这件事。</p><p id="b3ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将测试这些参数的各种输入:</p><ul class=""><li id="fe44" class="ml mm it lb b lc ld lf lg li mn lm mo lq mp lu mq mr ms mt bi translated"><em class="mh"> n_estimators </em>(定义要训练的森林的大小)</li><li id="facb" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><em class="mh"> max_depth </em>(基础学习者的最大树深度)</li><li id="ea06" class="ml mm it lb b lc mu lf mv li mw lm mx lq my lu mq mr ms mt bi translated"><em class="mh"> learning_rate </em>(嗯，是学习率)</li></ul><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="3556" class="mz na it ny b gy oc od l oe of">n_estimators = [10, 20, 30, 40, 50, 60]<br/>max_depth = [2, 4, 5, 6, 7, 8]<br/>learning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]<br/>param_grid = dict(max_depth = max_depth, n_estimators = n_estimators, learning_rate=learning_rate)<br/>kfold = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 10)<br/>grid_search_xg = GridSearchCV(xg_reg, param_grid, scoring = 'roc_auc', n_jobs = -1, cv=kfold, verbose = 1)<br/>result_gcv_xgb = grid_search_xg.fit(X_train_std, y_train)</span></pre><p id="b614" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里使用<em class="mh"> StratifiedKFold </em>和<em class="mh"> GridSearchCV </em>迭代参数进行交叉验证。您想要测试的参数越多，您的计算机运行的排列就越多，花费的时间就越多。如果你是在笔记本电脑上做，要小心。</p><p id="b1db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该过程完成后，让我们看看最佳参数是什么:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="7a2c" class="mz na it ny b gy oc od l oe of">print("Best: %f using %s" % (result_gcv_xgb.best_score_, result_gcv_xgb.best_params_))<br/>means = result_gcv_xgb.cv_results_['mean_test_score']<br/>stds = result_gcv_xgb.cv_results_['std_test_score']<br/>params = result_gcv_xgb.cv_results_['params']</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/c79dd22235c09d4e53002734cc2cf286.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8G8H9Bz1Go4ww2J9UMHYg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">GridSearchCV found the optimal parameters</figcaption></figure><p id="1dde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，您可以使用上面得到的内容重新运行模型:</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="2ce7" class="mz na it ny b gy oc od l oe of"># rebuild using best params<br/>xg_reg = xgb.XGBRegressor(objective ='binary:logistic', colsample_bytree = 0.3, learning_rate = 0.1,<br/>max_depth = 2, alpha = 10, n_estimators = 50)<br/>eval_set = [(X_test_std, y_test)]<br/>xg_reg.fit(X_train_std, y_train, eval_metric="error", eval_set = eval_set, verbose = False)<br/>prediction_y_5 = xg_reg.predict(X_test_std)<br/>rmse = np.sqrt(mean_squared_error(y_test, prediction_y_5))<br/>print("RMSE: %f" % (rmse))</span></pre><p id="d7bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到一个进步:RMSE 下降到 0.3384。</p><p id="d43c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost 返回概率，而不是实际预测。不过，我们需要实际的预测来建立一个混淆矩阵。</p><pre class="kj kk kl km gt nx ny nz oa aw ob bi"><span id="6720" class="mz na it ny b gy oc od l oe of">prediction_y_5_01 = prediction_y_5<br/>prediction_y_5_01[prediction_y_5 &gt; 0.5] = 1<br/>prediction_y_5_01[prediction_y_5 &lt;= 0.5] = 0</span><span id="e4fb" class="mz na it ny b gy og od l oe of">print(classification_report(y_test, prediction_y_5_01))<br/>print(confusion_matrix(y_test, prediction_y_5_01))<br/>accuracy_score(y_test, prediction_y_5_01)</span></pre><p id="b486" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经尝试使用一些传统的和更具体的方法来建立一个准确的预测模型。根据数据的性质，它可能已经足以提高模型的性能。在文章的第二部分，我们将继续我们的旅程，构建一条学习曲线，几个模型的集合，最后使用 Keras 重新运行分析。</p></div></div>    
</body>
</html>