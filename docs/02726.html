<html>
<head>
<title>Validation Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">验证方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/validation-methods-e4eefcbee720?source=collection_archive---------17-----------------------#2019-05-03">https://towardsdatascience.com/validation-methods-e4eefcbee720?source=collection_archive---------17-----------------------#2019-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="974c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本帖中，我们将讨论以下概念，它们都旨在评估分类模型的性能:</p><ol class=""><li id="e81f" class="ko kp it js b jt ju jx jy kb kq kf kr kj ks kn kt ku kv kw bi translated">模型的交叉验证。</li><li id="2b90" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">混乱矩阵。</li><li id="0be1" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">ROC 曲线。</li><li id="529c" class="ko kp it js b jt kx jx ky kb kz kf la kj lb kn kt ku kv kw bi translated">科恩的<em class="lc"> κ </em>分数。</li></ol><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="69c8" class="lm ln it li b gy lo lp l lq lr">import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>import warnings</span><span id="963d" class="lm ln it li b gy ls lp l lq lr">warnings.filterwarnings('ignore')</span></pre><p id="8faf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们首先创建一些具有三个特征和二进制标签的简单数据集。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="0601" class="lm ln it li b gy lo lp l lq lr">from sklearn.model_selection import train_test_split</span><span id="0aff" class="lm ln it li b gy ls lp l lq lr"># Creating the dataset<br/>N = 1000 # number of samples<br/>data = {'A': np.random.normal(100, 8, N),<br/>        'B': np.random.normal(60, 5, N),<br/>        'C': np.random.choice([1, 2, 3], size=N, p=[0.2, 0.3, 0.5])}<br/>df = pd.DataFrame(data=data)</span><span id="7396" class="lm ln it li b gy ls lp l lq lr"># Labeling <br/>def get_label(A, B, C):<br/>    if A &lt; 95:<br/>        return 1<br/>    elif C == 1:<br/>        return 1<br/>    elif B &gt; 68 or B &lt; 52:<br/>        return 1<br/>    return 0</span><span id="760a" class="lm ln it li b gy ls lp l lq lr">df['label'] = df.apply(lambda row: get_label(row['A'],row['B'],row['C']),axis=1)</span><span id="eb82" class="lm ln it li b gy ls lp l lq lr"># Dividing to train and test set<br/>X = np.asarray(df[['A', 'B', 'C']])<br/>y = np.asarray(df['label'])<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)</span></pre><p id="7667" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了演示的目的，让我们尝试使用一个简单的逻辑回归。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="c6a1" class="lm ln it li b gy lo lp l lq lr">from sklearn import linear_model<br/>from sklearn.model_selection import cross_val_score</span><span id="808c" class="lm ln it li b gy ls lp l lq lr">clf = linear_model.LogisticRegression()<br/>clf.fit(X_train, y_train)</span><span id="598c" class="lm ln it li b gy ls lp l lq lr">print("&gt;&gt; Score of the classifier on the train set is: ", round(clf.score(X_test, y_test),2))</span><span id="0a1c" class="lm ln it li b gy ls lp l lq lr">&gt;&gt; Score of the classifier on the train set is:  0.74</span></pre><p id="5b9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">交叉验证</strong></p><p id="f123" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">交叉验证背后的想法很简单——我们选择某个数字<em class="lc"> k </em>，通常是<em class="lc"> k </em> =5 或者<em class="lc"> k </em> =10 (5 是 sklearn 中的默认值，参见[1])。我们将数据分成<em class="lc"> k </em>个大小相等的部分，在这些部分的<em class="lc">k</em>1 上训练模型，并在剩余部分上检查其性能。我们这样做<em class="lc"> k </em>次，我们可以对分数进行平均以获得一个 CV 分数。</p><p id="e793" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">优点:使用交叉验证可以给你一个暗示，告诉你你的模型做得有多好，它的优点是非常健壮(与简单的训练-测试分割相反)。它还可以用于参数的超调:对于给定的参数，使用 CV 分数以稳健的方式优化其值。</p><p id="c4a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看我们的例子的 CV 分数:</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="4318" class="lm ln it li b gy lo lp l lq lr">scores = cross_val_score(clf, X_train, y_train, cv=10)<br/>print('&gt;&gt; Mean CV score is: ', round(np.mean(scores),3))<br/>pltt = sns.distplot(pd.Series(scores,name='CV scores distribution'), color='r')</span><span id="8305" class="lm ln it li b gy ls lp l lq lr">&gt;&gt; Mean CV score is:  0.729</span></pre><figure class="ld le lf lg gt lu gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/16306cf0fd4d6e6bdcd8d9aea2725c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*fOMd2-LmQ3JAQUL3f2vy5g.png"/></div></figure><p id="3743" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还可以使用 CV 分数的值来导出置信区间，在该置信区间中，我们可以确保找到实际分数的概率很高。</p><p id="2968" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">混淆矩阵</strong></p><p id="195b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这个想法相当简单。我们希望显示真阳性(TP)，真阴性(TN)，假阳性(FP)和假阴性(FN)。更一般地，当有几个标签时，我们显示属于标签<em class="lc"> i </em>但被归类为<em class="lc"> j </em>的数据点的数量。这个数字被定义为混淆矩阵的(<em class="lc"> i </em>，<em class="lc"> j </em>)条目。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="2a40" class="lm ln it li b gy lo lp l lq lr">from sklearn.metrics import confusion_matrix<br/>C = confusion_matrix(clf.predict(X_test),y_test)<br/>df_cm = pd.DataFrame(C, range(2),range(2))<br/>sns.set(font_scale=1.4)<br/>pltt = sns.heatmap(df_cm, annot=True,annot_kws={"size": 16}, cmap="YlGnBu",  fmt='g')</span></pre><figure class="ld le lf lg gt lu gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/6d1cc48ee5621693f5834f628b08ed3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*YH9SQwqxcyLDElFtV3_fgQ.png"/></div></figure><p id="422f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> ROC 曲线</strong></p><p id="86ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们仔细看看混淆矩阵。我们讨论了真阳性(TP)和假阳性(FP)的概念。很明显，如果我们允许 FP 为 1，那么 TP 也将等于 1；一般来说，如果 TP 和 FP 相等，我们的预测和随机猜测一样好。</p><p id="fc56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ROC 曲线被定义为 TP 作为 FP 的函数的图。因此，根据以上讨论，ROC 曲线将位于直线<em class="lc"> y </em> = <em class="lc"> x </em>上方。</p><p id="e53d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">ROC 曲线的构建来自我们的分类器分配给每个点的概率；对于用标签<em class="lc"> li </em> ∈{0，1}预测的每个数据点<em class="lc"> xi </em>，我们有一个概率<em class="lc"> pi </em> ∈[0，1】使得<em class="lc"> yi </em> = <em class="lc"> li </em>。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="a990" class="lm ln it li b gy lo lp l lq lr">from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve<br/>pltt = plot_ROC(y_train, clf.predict_proba(X_train)[:,1], y_test, clf.predict_proba(X_test)[:,1])</span></pre><figure class="ld le lf lg gt lu gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/30e33033a7d11716d6cfec816160d288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*3Dyk69_r2A-RuzkY-icNPg.png"/></div></figure><p id="82b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">考虑到 ROC 曲线，有几个重要的概念:</p><p id="d3bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(ROC 曲线下面积(AUC)，是分类器质量的重要度量。对于一个随机的猜测，我们有 AUC=∫ <em class="lc"> x </em> d <em class="lc"> x </em> =1/2，所以我们排除了一个知情分类器的分数&gt; 1/2。概率解释如下:随机正数比均匀抽取的随机负数排在前面(根据模型的概率)的概率。ROC AUC 是机器学习中常用的一种([3])。</p><p id="457a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(2)图上标记的点是 TP 和 FP 的比率，正如我们前面在混淆矩阵中看到的。</p><p id="a2e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(3)如果 ROC 曲线位于线<em class="lc"> y </em> = <em class="lc"> x </em>之下，这意味着通过反演分类器的结果，我们可以得到一个信息性的分类器。拥有一个总是给出错误答案的算法和拥有一个好的算法一样好！下面是绘制 ROC 曲线的代码(摘自[2])。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="44eb" class="lm ln it li b gy lo lp l lq lr">def plot_ROC(y_train_true, y_train_prob, y_test_true, y_test_prob):<br/>    '''<br/>    a funciton to plot the ROC curve for train labels and test labels.<br/>    Use the best threshold found in train set to classify items in test set.<br/>    '''<br/>    fpr_train, tpr_train, thresholds_train = roc_curve(y_train_true, y_train_prob, pos_label =True)<br/>    sum_sensitivity_specificity_train = tpr_train + (1-fpr_train)<br/>    best_threshold_id_train = np.argmax(sum_sensitivity_specificity_train)<br/>    best_threshold = thresholds_train[best_threshold_id_train]<br/>    best_fpr_train = fpr_train[best_threshold_id_train]<br/>    best_tpr_train = tpr_train[best_threshold_id_train]<br/>    y_train = y_train_prob &gt; best_threshold</span><span id="499b" class="lm ln it li b gy ls lp l lq lr">    cm_train = confusion_matrix(y_train_true, y_train)<br/>    acc_train = accuracy_score(y_train_true, y_train)<br/>    auc_train = roc_auc_score(y_train_true, y_train)</span><span id="846a" class="lm ln it li b gy ls lp l lq lr">    fig = plt.figure(figsize=(10,5))<br/>    ax = fig.add_subplot(121)<br/>    curve1 = ax.plot(fpr_train, tpr_train)<br/>    curve2 = ax.plot([0, 1], [0, 1], color='navy', linestyle='--')<br/>    dot = ax.plot(best_fpr_train, best_tpr_train, marker='o', color='black')<br/>    ax.text(best_fpr_train, best_tpr_train, s = '(%.3f,%.3f)' %(best_fpr_train, best_tpr_train))<br/>    plt.xlim([0.0, 1.0])<br/>    plt.ylim([0.0, 1.0])<br/>    plt.xlabel('False Positive Rate')<br/>    plt.ylabel('True Positive Rate')<br/>    plt.title('ROC curve (Train), AUC = %.4f'%auc_train)</span><span id="460c" class="lm ln it li b gy ls lp l lq lr">    fpr_test, tpr_test, thresholds_test = roc_curve(y_test_true, y_test_prob, pos_label =True)</span><span id="ebf6" class="lm ln it li b gy ls lp l lq lr">    y_test = y_test_prob &gt; best_threshold</span><span id="d626" class="lm ln it li b gy ls lp l lq lr">    cm_test = confusion_matrix(y_test_true, y_test)<br/>    acc_test = accuracy_score(y_test_true, y_test)<br/>    auc_test = roc_auc_score(y_test_true, y_test)</span><span id="9bae" class="lm ln it li b gy ls lp l lq lr">    tpr_score = float(cm_test[1][1])/(cm_test[1][1] + cm_test[1][0])<br/>    fpr_score = float(cm_test[0][1])/(cm_test[0][0]+ cm_test[0][1])</span><span id="8555" class="lm ln it li b gy ls lp l lq lr">    ax2 = fig.add_subplot(122)<br/>    curve1 = ax2.plot(fpr_test, tpr_test)<br/>    curve2 = ax2.plot([0, 1], [0, 1], color='navy', linestyle='--')<br/>    dot = ax2.plot(fpr_score, tpr_score, marker='o', color='black')<br/>    ax2.text(fpr_score, tpr_score, s = '(%.3f,%.3f)' %(fpr_score, tpr_score))<br/>    plt.xlim([0.0, 1.0])<br/>    plt.ylim([0.0, 1.0])<br/>    plt.xlabel('False Positive Rate')<br/>    plt.ylabel('True Positive Rate')<br/>    plt.title('ROC curve (Test), AUC = %.4f'%auc_test)<br/>    plt.savefig('ROC', dpi = 500)<br/>    plt.show()</span><span id="1d66" class="lm ln it li b gy ls lp l lq lr">    return best_threshold</span></pre><p id="86bc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">科恩的<em class="lc"> κ </em>得分</strong></p><p id="7e28" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Cohen 的<em class="lc"> κ </em>分数给出了两个分类器对相同数据的一致性。定义为<em class="lc">κ</em>= 1(1-<em class="lc">po)</em>/(1-<em class="lc">PE)</em>，其中<em class="lc"> po </em>为观察到的符合概率，<em class="lc"> pe </em>为符合的随机概率。</p><p id="1c33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看一个例子。我们需要再使用一个分类器。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="5fd5" class="lm ln it li b gy lo lp l lq lr">from sklearn import svm</span><span id="ad20" class="lm ln it li b gy ls lp l lq lr">clf2 = svm.SVC()<br/>clf2.fit(X_train, y_train)</span><span id="0fec" class="lm ln it li b gy ls lp l lq lr">print("&gt;&gt; Score of the classifier on the train set is: ", round(clf2.score(X_test, y_test),2))</span><span id="c9dc" class="lm ln it li b gy ls lp l lq lr">&gt;&gt; Score of the classifier on the train set is:  0.74</span></pre><p id="766b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们计算列车组上的<em class="lc"> κ </em>。</p><pre class="ld le lf lg gt lh li lj lk aw ll bi"><span id="cc10" class="lm ln it li b gy lo lp l lq lr">y = clf.predict(X_test)<br/>y2 = clf2.predict(X_test)<br/>n = len(y)</span><span id="f79e" class="lm ln it li b gy ls lp l lq lr">p_o = sum(y==y2)/n # observed agreement</span><span id="20a5" class="lm ln it li b gy ls lp l lq lr">p_e = sum(y)*sum(y2)/(n**2)+sum(1-y)*sum(1-y2)/(n**2) # random agreement: both 1 or both 0</span><span id="6f81" class="lm ln it li b gy ls lp l lq lr">kappa = 1-(1-p_o)/(1-p_e)</span><span id="c873" class="lm ln it li b gy ls lp l lq lr">print("&gt;&gt; Cohen's Kappa score is: ", round(kappa,2))</span><span id="913f" class="lm ln it li b gy ls lp l lq lr">&gt;&gt; Cohen's Kappa score is:  0.4</span></pre><p id="ea89" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这表明两个分类器之间存在某种一致性。<em class="lc"> κ </em> =0 表示不一致，而<em class="lc"> κ </em> &lt; 0 也可以发生在两个分类器不一致的时候。</p><p id="ab11" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">结论</strong></p><p id="2e8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们讨论了对您的模型进行评分并将其与其他模型进行比较的几种基本策略。在应用机器学习算法和比较它们的性能时，记住这些概念很重要。</p><p id="85d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参考文献</strong></p><p id="dd1b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[1]<a class="ae lz" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/cross _ validation . html</a></p><p id="4901" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]此处提供评估 ROC 曲线的函数:<a class="ae lz" href="https://github.com/bc123456/ROC" rel="noopener ugc nofollow" target="_blank">https://github.com/bc123456/ROC</a></p><p id="ffa3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]<a class="ae lz" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Receiver _ operating _ character istic</a></p><p id="cf71" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae lz" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Cohen%27s_kappa</a></p></div></div>    
</body>
</html>