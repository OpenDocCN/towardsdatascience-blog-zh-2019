<html>
<head>
<title>Let’s Build a Fashion-MNIST CNN, PyTorch Style</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让我们建立一个时尚-MNIST 有线电视新闻网，PyTorch 风格</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582?source=collection_archive---------2-----------------------#2019-10-23">https://towardsdatascience.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582?source=collection_archive---------2-----------------------#2019-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b32" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何使用 Google Colab 和 TensorBoard 从头开始构建 PyTorch ML 项目的逐行指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f2ea5451cfa450d03fee3c36ffde0d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cz0q6YuI0H8dSd5N9Km5_A.png"/></div></div></figure><p id="4c07" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">当谈到技术中的框架时，一件有趣的事情是，从一开始，似乎总是有各种各样的选择。但随着时间的推移，竞争将演变为只剩下两个强有力的竞争者。典型的例子有“PC vs Mac”、“iOS vs Android”、“React.js vs Vue.js”等。而现在，我们在机器学习方面有了‘py torch vs tensor flow’。</p><p id="0182" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由谷歌支持的<a class="ae lz" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>无疑是这方面的领跑者。它于 2015 年作为开源机器学习框架发布，迅速获得了大量关注和接受，特别是在生产准备和部署非常关键的行业。<a class="ae lz" href="https://github.com/pytorch/pytorch" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>由脸书在 2017 年推出，但由于其动态计算图和'<a class="ae lz" href="https://legacy.python.org/dev/peps/pep-0020/" rel="noopener ugc nofollow" target="_blank">python 式</a>'风格，很快获得了从业者和研究人员的喜爱。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ma"><img src="../Images/8f467add53fb4ec27a654b070f6d119b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfvNjEbtpC33GS_sqcW1Kg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Image from <a class="ae lz" href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/" rel="noopener ugc nofollow" target="_blank">The Gradient</a></figcaption></figure><p id="c526" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lz" href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/" rel="noopener ugc nofollow" target="_blank"> The Gradient </a>最近的研究表明，PyTorch 在研究人员中表现出色，TensorFlow 在业界占据主导地位:</p><blockquote class="mf"><p id="fcc6" class="mg mh it bd mi mj mk ml mm mn mo lp dk translated">2019 年，ML 框架的战争还有两个主要竞争者:PyTorch 和 TensorFlow。我的分析表明，研究人员正在放弃 TensorFlow，成群结队地涌向 PyTorch。与此同时，在行业中，Tensorflow 是目前的首选平台，但这种情况可能不会持续太久。— <a class="ae lz" href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/" rel="noopener ugc nofollow" target="_blank">坡度</a></p></blockquote><p id="3eaf" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">最近发布的 PyTorch 1.3 引入了 PyTorch Mobile、quantization 和其他好东西，它们都朝着缩小差距的正确方向前进。如果你对神经网络基础有点熟悉，但想尝试 PyTorch 作为一种不同的风格，那么请继续阅读。我将尝试解释如何使用 PyTorch 为时尚-MNIST 数据集从头构建一个卷积神经网络分类器。如果你没有强大的本地环境，这里的代码可以在 Google Colab 和 Tensor Board 上使用。事不宜迟，我们开始吧。您可以在下面找到 Google Colab 笔记本和 GitHub 链接:</p><p id="9a75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lz" href="https://colab.research.google.com/drive/1YWzAjpAnLI23irBQtLvDTYT1A94uCloM" rel="noopener ugc nofollow" target="_blank">📙<strong class="kw iu">谷歌 Colab 笔记本</strong> </a></p><p id="5f71" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">👽<a class="ae lz" href="https://github.com/wayofnumbers/SideProjects/blob/master/PyTorch_Tutorial_Basic_v1.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> GitHub </strong> </a></p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="0c4b" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">导入</h1><p id="d397" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">首先，让我们导入必要的模块。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="bbac" class="od nc it nz b gy oe of l og oh"># import standard PyTorch modules<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from torch.utils.tensorboard import SummaryWriter # TensorBoard support<br/><br/># import torchvision module to handle image manipulation<br/>import torchvision<br/>import torchvision.transforms as transforms<br/><br/># calculate train time, writing train data to files etc.<br/>import time<br/>import pandas as pd<br/>import json<br/>from IPython.display import clear_output<br/><br/>torch.set_printoptions(linewidth=120)<br/>torch.set_grad_enabled(True)     # On by default, leave it here for clarity</span></pre><p id="c477" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">PyTorch 模块非常简单。</p><h2 id="48a4" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">火炬</h2><p id="deea" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated"><code class="fe ot ou ov nz b">torch</code>是主模块，包含了<strong class="kw iu">张量</strong>计算所需的所有东西。您可以单独使用张量计算来构建一个全功能的神经网络，但这不是本文要讨论的内容。我们将利用更强大、更方便的<code class="fe ot ou ov nz b">torch.nn</code>、<code class="fe ot ou ov nz b">torch.optim</code>和<code class="fe ot ou ov nz b">torchvision</code>类来快速构建我们的 CNN。对于那些有兴趣知道如何从“从<em class="ow">抓到</em>开始做这件事的人，请访问这个<a class="ae lz" href="https://pytorch.org/tutorials/beginner/nn_tutorial.html" rel="noopener ugc nofollow" target="_blank">奇妙的 PyTorch 官方 tutoria </a> l 作者<a class="ox oy ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----efb297e22582--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>。</p><h2 id="5771" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">torch.nn 和 torch.nn 功能</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/af347b724f99ed5f02a39918018680be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zmhp4AMV_HCUa5rH"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Photo by <a class="ae lz" href="https://unsplash.com/@duck58cth?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alphacolor</a> on <a class="ae lz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="538f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b">torch.nn</code>模块提供了许多构建神经网络的类和函数。你可以把它想象成神经网络的基本构件:模型、各种层、激活函数、参数类等等。它允许我们像组装乐高玩具一样来建造模型。</p><h2 id="c890" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">火炬. optim</h2><p id="dc1a" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated"><code class="fe ot ou ov nz b">torch.optim</code>提供所有优化器，如 SGD、ADAM 等。，这样就不用从头开始写了。</p><h2 id="ffd9" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">火炬视觉</h2><p id="4925" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated"><code class="fe ot ou ov nz b">torchvision</code>包含大量流行的数据集、模型架构和计算机视觉的常见图像转换。我们从中获取时尚 MNIST 数据集，并使用其变换。</p><h2 id="0743" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">摘要记录器(张量板)</h2><p id="280b" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated"><code class="fe ot ou ov nz b">SummaryWriter</code>使 PyTorch 能够为张量板生成报告。我们将使用 Tensor Board 查看我们的训练数据，比较结果并获得直觉。张量板曾经是 TensorFlow 相对于 PyTorch 的最大优势，但是现在从 v1.2 开始 PyTorch 正式支持张量板。</p><p id="2e94" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还引入了其他一些实用模块，如<code class="fe ot ou ov nz b">time</code>、<code class="fe ot ou ov nz b">json</code>、<code class="fe ot ou ov nz b">pandas</code>等。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="1703" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">资料组</h1><p id="1aae" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated"><code class="fe ot ou ov nz b">torchvision</code>已经有了时尚 MNIST 数据集。如果你不熟悉时尚 MNIST 数据集:</p><blockquote class="pa pb pc"><p id="d5e5" class="ku kv ow kw b kx ky ju kz la lb jx lc pd le lf lg pe li lj lk pf lm ln lo lp im bi translated"><code class="fe ot ou ov nz b">Fashion-MNIST</code>是一个由<a class="ae lz" href="https://jobs.zalando.com/tech/" rel="noopener ugc nofollow" target="_blank"> Zalando </a>的文章图像组成的数据集，由 60，000 个示例的训练集和 10，000 个示例的测试集组成。每个示例都是 28x28 灰度图像，与 10 个类别的标签相关联。我们打算将<code class="fe ot ou ov nz b">Fashion-MNIST</code>作为原始<a class="ae lz" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST 数据集</a>的直接替代，用于机器学习算法的基准测试。它共享训练和测试分割的相同图像大小和结构。— <a class="ae lz" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">来自 Github </a></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/5b2eb440c08f4773c113766533f98674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RCXpLibVCgoRYckEd2kU8Q.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Fashion MNIST Dataset — <a class="ae lz" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">From GitHub</a></figcaption></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b418" class="od nc it nz b gy oe of l og oh"># Use standard FashionMNIST dataset<br/>train_set = torchvision.datasets.FashionMNIST(<br/>    root = './data/FashionMNIST',<br/>    train = True,<br/>    download = True,<br/>    transform = transforms.Compose([<br/>        transforms.ToTensor()                                 <br/>    ])<br/>)</span></pre><p id="2cc1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个不需要过多解释。我们指定根目录来存储数据集，抓取训练数据，如果本地机器上没有，允许下载它，然后应用<code class="fe ot ou ov nz b">transforms.ToTensor</code>将图像转换成<strong class="kw iu">张量</strong>，这样我们就可以在我们的网络中直接使用它。数据集存储在名为<code class="fe ot ou ov nz b">train_set.</code>的<code class="fe ot ou ov nz b">dataset</code>类中</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="3a1f" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">网络</h1><p id="f784" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi lq translated">在 PyTorch 中构建实际的神经网络既有趣又简单。我假设你对卷积神经网络的工作原理有一些基本的概念。如果没有，可以参考 deeplizard 的这个视频:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ph pi l"/></div></figure><p id="eff8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">时尚 MNIST 只有 28x28 px 大小，所以我们实际上不需要非常复杂的网络。我们可以像这样建立一个简单的 CNN:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/948c7a17db8d399075d43517bbeefcbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PceBlGfN_G7xyIkAfI7BuA.png"/></div></div></figure><p id="2526" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们有两个卷积层，每个卷积层有 5x5 个内核。在每个卷积层之后，我们有一个跨度为 2 的最大池层。这允许我们从图像中提取必要的特征。然后我们展平张量，将它们放入密集层，通过多层感知器(MLP)来执行我们的 10 个类别的分类任务。</p><p id="8d87" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们已经清楚了网络的结构，让我们看看如何使用 PyTorch 来构建它:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="552d" class="od nc it nz b gy oe of l og oh"># Build the neural network, expand on top of nn.Module<br/>class Network(nn.Module):<br/>  def __init__(self):<br/>    super().__init__()<br/><br/>    # define layers<br/>    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)<br/>    self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)<br/><br/>    self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)<br/>    self.fc2 = nn.Linear(in_features=120, out_features=60)<br/>    self.out = nn.Linear(in_features=60, out_features=10)<br/><br/>  # define forward function<br/>  def forward(self, t):<br/>    # conv 1<br/>    t = self.conv1(t)<br/>    t = F.relu(t)<br/>    t = F.max_pool2d(t, kernel_size=2, stride=2)<br/><br/>    # conv 2<br/>    t = self.conv2(t)<br/>    t = F.relu(t)<br/>    t = F.max_pool2d(t, kernel_size=2, stride=2)<br/><br/>    # fc1<br/>    t = t.reshape(-1, 12*4*4)<br/>    t = self.fc1(t)<br/>    t = F.relu(t)<br/><br/>    # fc2<br/>    t = self.fc2(t)<br/>    t = F.relu(t)<br/><br/>    # output<br/>    t = self.out(t)<br/>    # don't need softmax here since we'll use cross-entropy as activation.<br/><br/>    return t</span></pre><p id="f85c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先 PyTorch 中所有的网络类都是在基类上扩展的:<code class="fe ot ou ov nz b">nn.Module</code>。它包含了所有的基础知识:<strong class="kw iu">权重、偏差、正向方法</strong>以及一些实用属性和方法，如<code class="fe ot ou ov nz b">.parameters()</code>和<code class="fe ot ou ov nz b">.zero_grad()</code>，我们也将使用它们。</p><p id="70f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的网络结构在<code class="fe ot ou ov nz b">__init__</code> dunder 函数中定义。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="a889" class="od nc it nz b gy oe of l og oh">def __init__(self): <br/>  super().__init__() </span><span id="86c3" class="od nc it nz b gy pk of l og oh">  # define layers <br/>  self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)<br/>  self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)<br/>  self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)<br/>  self.fc2 = nn.Linear(in_features=120, out_features=60)<br/>  self.out = nn.Linear(in_features=60, out_features=10)</span></pre><p id="2100" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b">nn.Conv2d</code>和<code class="fe ot ou ov nz b">nn.Linear</code>是在<code class="fe ot ou ov nz b">torch.nn</code>模块中定义的两个标准 PyTorch 层。这些都是不言自明的。需要注意的一点是，我们在这里只定义了实际的层。激活和最大池化<strong class="kw iu">操作</strong>包含在下面解释的转发功能中。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="3d0d" class="od nc it nz b gy oe of l og oh"># define forward function  <br/>def forward(self, t):  <br/>  # conv 1  <br/>  t = self.conv1(t)  <br/>  t = F.relu(t)  <br/>  t = F.max_pool2d(t, kernel_size=2, stride=2)   </span><span id="fdcc" class="od nc it nz b gy pk of l og oh">  # conv 2  <br/>  t = self.conv2(t)   <br/>  t = F.relu(t)  <br/>  t = F.max_pool2d(t, kernel_size=2, stride=2)   </span><span id="f7ea" class="od nc it nz b gy pk of l og oh">  # fc1   <br/>  t = t.reshape(-1, 12*4*4)  <br/>  t = self.fc1(t)  <br/>  t = F.relu(t)   </span><span id="81a2" class="od nc it nz b gy pk of l og oh">  # fc2  <br/>  t = self.fc2(t)  <br/>  t = F.relu(t)  </span><span id="c46e" class="od nc it nz b gy pk of l og oh">  # output  <br/>  t = self.out(t)  </span><span id="e193" class="od nc it nz b gy pk of l og oh">  # don't need softmax here since we'll use cross-entropy as activation.   <br/>  return t</span></pre><p id="1dd0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦定义了层，我们就可以使用层本身来计算每一层的转发结果，加上激活函数(ReLu)和最大池操作，我们可以很容易地编写我们的网络的转发函数，如上所述。注意，在<code class="fe ot ou ov nz b">fc1</code>(完全连接第 1 层)上，我们使用 PyTorch 的张量运算<code class="fe ot ou ov nz b">t.reshape</code>来展平张量，这样它就可以传递给后面的致密层。此外，我们没有在输出层添加 softmax 激活函数，因为 PyTorch 的<strong class="kw iu"> CrossEntropy </strong>函数会为我们处理这个问题。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="f212" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">超参数</h1><p id="62d0" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi lq translated">通常情况下，我们可以只手动选择一组超参数，并用它们做一些实验。在这个例子中，我们想通过引入一些结构化来做更多的事情。我们将构建一个系统来生成不同的超参数组合，并使用它们来执行训练“运行”。每次“运行”使用一组超参数组合。将每次跑步的训练数据/结果导出到 Tensor Board，以便我们可以直接比较并查看哪个超参数集表现最佳。</p><p id="53a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将所有的超参数存储在一个<a class="ae lz" href="https://www.geeksforgeeks.org/ordereddict-in-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">有序指令</strong> </a>中:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="2427" class="od nc it nz b gy oe of l og oh"># put all hyper params into a OrderedDict, easily expandable<br/>params = OrderedDict(<br/>    lr = [.01, .001],<br/>    batch_size = [100, 1000],<br/>    shuffle = [True, False]<br/>)<br/>epochs = 3</span></pre><p id="99f4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b">lr</code>:学习率。我们想为我们的模型尝试 0.01 和 0.001。</p><p id="04d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b">batch_size</code>:批量，加速训练过程。我们会用 100 和 1000。</p><p id="c7f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b">shuffle</code> : Shuffle toggle，我们是否在训练前洗牌。</p><p id="21c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦参数下降。我们使用两个助手类:<code class="fe ot ou ov nz b">RunBuilder</code>和<code class="fe ot ou ov nz b">RunManager</code>来管理我们的超参数和训练过程。</p><h2 id="ed79" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">运行生成器</h2><p id="b810" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">类<code class="fe ot ou ov nz b">RunBuilder</code>的主要目的是提供一个静态方法<code class="fe ot ou ov nz b">get_runs</code>。它将 OrderedDict(其中存储了所有超参数)作为一个参数，并生成一个名为元组 <code class="fe ot ou ov nz b">Run</code>的<a class="ae lz" href="https://www.youtube.com/watch?v=GfxJYp9_nJA" rel="noopener ugc nofollow" target="_blank">，每个<code class="fe ot ou ov nz b">run</code>元素代表超参数的一种可能组合。这个命名元组稍后由训练循环使用。代码很容易理解。</a></p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="6d7d" class="od nc it nz b gy oe of l og oh"># import modules to build RunBuilder and RunManager helper classes<br/>from collections  import OrderedDict<br/>from collections import namedtuple<br/>from itertools import product<br/><br/># Read in the hyper-parameters and return a Run namedtuple containing all the <br/># combinations of hyper-parameters<br/>class RunBuilder():<br/>  @staticmethod<br/>  def get_runs(params):<br/><br/>    Run = namedtuple('Run', params.keys())<br/><br/>    runs = []<br/>    for v in product(*params.values()):<br/>      runs.append(Run(*v))<br/>    <br/>    return runs</span></pre><h2 id="06a3" class="od nc it bd nd oi oj dn nh ok ol dp nl ld om on nn lh oo op np ll oq or nr os bi translated">运行管理器</h2><p id="d472" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated"><code class="fe ot ou ov nz b">RunManager </code>类有四个主要目的。</p><ol class=""><li id="8249" class="pl pm it kw b kx ky la lb ld pn lh po ll pp lp pq pr ps pt bi translated">计算并记录每个时期和运行的持续时间。</li><li id="07e3" class="pl pm it kw b kx pu la pv ld pw lh px ll py lp pq pr ps pt bi translated">计算每个历元和运行的训练损失和准确度。</li><li id="94f1" class="pl pm it kw b kx pu la pv ld pw lh px ll py lp pq pr ps pt bi translated">记录训练数据(如损耗、准确度、重量、梯度、计算图等)。)并运行，然后将它们导出到张量板以供进一步分析。</li><li id="d7f4" class="pl pm it kw b kx pu la pv ld pw lh px ll py lp pq pr ps pt bi translated">将所有训练结果保存在<code class="fe ot ou ov nz b">csv</code>和<code class="fe ot ou ov nz b">json</code>中，以备将来参考或提取 API。</li></ol><p id="2fd0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如你所见，它帮助我们处理后勤工作，这对我们成功训练模型也很重要。让我们看看代码。这有点长，请原谅:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="c773" class="od nc it nz b gy oe of l og oh"># Helper class, help track loss, accuracy, epoch time, run time, <br/># hyper-parameters etc. Also record to TensorBoard and write into csv, json<br/>class RunManager():<br/>  def __init__(self):<br/><br/>    # tracking every epoch count, loss, accuracy, time<br/>    self.epoch_count = 0<br/>    self.epoch_loss = 0<br/>    self.epoch_num_correct = 0<br/>    self.epoch_start_time = None<br/><br/>    # tracking every run count, run data, hyper-params used, time<br/>    self.run_params = None<br/>    self.run_count = 0<br/>    self.run_data = []<br/>    self.run_start_time = None<br/><br/>    # record model, loader and TensorBoard <br/>    self.network = None<br/>    self.loader = None<br/>    self.tb = None<br/><br/>  # record the count, hyper-param, model, loader of each run<br/>  # record sample images and network graph to TensorBoard  <br/>  def begin_run(self, run, network, loader):<br/><br/>    self.run_start_time = time.time()<br/><br/>    self.run_params = run<br/>    self.run_count += 1<br/><br/>    self.network = network<br/>    self.loader = loader<br/>    self.tb = SummaryWriter(comment=f'-{run}')<br/><br/>    images, labels = next(iter(self.loader))<br/>    grid = torchvision.utils.make_grid(images)<br/><br/>    self.tb.add_image('images', grid)<br/>    self.tb.add_graph(self.network, images)<br/><br/>  # when run ends, close TensorBoard, zero epoch count<br/>  def end_run(self):<br/>    self.tb.close()<br/>    self.epoch_count = 0<br/><br/>  # zero epoch count, loss, accuracy, <br/>  def begin_epoch(self):<br/>    self.epoch_start_time = time.time()<br/><br/>    self.epoch_count += 1<br/>    self.epoch_loss = 0<br/>    self.epoch_num_correct = 0<br/><br/>  # <br/>  def end_epoch(self):<br/>    # calculate epoch duration and run duration(accumulate)<br/>    epoch_duration = time.time() - self.epoch_start_time<br/>    run_duration = time.time() - self.run_start_time<br/><br/>    # record epoch loss and accuracy<br/>    loss = self.epoch_loss / len(self.loader.dataset)<br/>    accuracy = self.epoch_num_correct / len(self.loader.dataset)<br/><br/>    # Record epoch loss and accuracy to TensorBoard <br/>    self.tb.add_scalar('Loss', loss, self.epoch_count)<br/>    self.tb.add_scalar('Accuracy', accuracy, self.epoch_count)<br/><br/>    # Record params to TensorBoard<br/>    for name, param in self.network.named_parameters():<br/>      self.tb.add_histogram(name, param, self.epoch_count)<br/>      self.tb.add_histogram(f'{name}.grad', param.grad, self.epoch_count)<br/>    <br/>    # Write into 'results' (OrderedDict) for all run related data<br/>    results = OrderedDict()<br/>    results["run"] = self.run_count<br/>    results["epoch"] = self.epoch_count<br/>    results["loss"] = loss<br/>    results["accuracy"] = accuracy<br/>    results["epoch duration"] = epoch_duration<br/>    results["run duration"] = run_duration<br/><br/>    # Record hyper-params into 'results'<br/>    for k,v in self.run_params._asdict().items(): results[k] = v<br/>    self.run_data.append(results)<br/>    df = pd.DataFrame.from_dict(self.run_data, orient = 'columns')<br/><br/>    # display epoch information and show progress<br/>    clear_output(wait=True)<br/>    display(df)<br/><br/>  # accumulate loss of batch into entire epoch loss<br/>  def track_loss(self, loss):<br/>    # multiply batch size so variety of batch sizes can be compared<br/>    self.epoch_loss += loss.item() * self.loader.batch_size<br/><br/>  # accumulate number of corrects of batch into entire epoch num_correct<br/>  def track_num_correct(self, preds, labels):<br/>    self.epoch_num_correct += self._get_num_correct(preds, labels)<br/><br/>  @torch.no_grad()<br/>  def _get_num_correct(self, preds, labels):<br/>    return preds.argmax(dim=1).eq(labels).sum().item()<br/>  <br/>  # save end results of all runs into csv, json for further analysis<br/>  def save(self, fileName):<br/><br/>    pd.DataFrame.from_dict(<br/>        self.run_data, <br/>        orient = 'columns',<br/>    ).to_csv(f'{fileName}.csv')<br/><br/>    with open(f'{fileName}.json', 'w', encoding='utf-8') as f:<br/>      json.dump(self.run_data, f, ensure_ascii=False, indent=4)</span></pre><p id="24ad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b"><strong class="kw iu">__init__</strong></code>:初始化必要的属性，如计数、丢失、正确预测数、开始时间等。</p><p id="c2fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b"><strong class="kw iu">begin_run</strong></code>:记录运行开始时间，以便当运行结束时，可以计算运行的持续时间。创建一个<code class="fe ot ou ov nz b">SummaryWriter</code>对象来存储我们在运行过程中想要导出到张量板的所有内容。将网络图和样本图像写入<code class="fe ot ou ov nz b">SummaryWriter</code>对象。</p><p id="b04c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b"><strong class="kw iu">end_run</strong></code>:运行完成后，关闭 SummaryWriter 对象，并将历元计数重置为 0(准备下一次运行)。</p><p id="94a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b"><strong class="kw iu">begin_epoch</strong></code>:记录历元开始时间，以便在历元结束时计算历元持续时间。复位<code class="fe ot ou ov nz b">epoch_loss</code>和<code class="fe ot ou ov nz b">epoch_num_correct</code>。</p><p id="7375" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个函数是大多数事情发生的地方。当一个时期结束时，我们将计算时期持续时间和运行持续时间(直到这个时期，而不是最后的运行持续时间，除非是运行的最后一个时期)。我们将计算这个时期的总损耗和精确度，然后将损耗、精确度、权重/偏差、梯度导出到张量板中。为了便于在 Jupyter 笔记本中跟踪，我们还创建了一个<strong class="kw iu"> OrderedDict </strong>对象<code class="fe ot ou ov nz b">results</code>，并将我们所有的运行数据(损失、准确度、运行计数、历元计数、运行持续时间、历元持续时间、所有超参数)放入其中。然后我们将使用<strong class="kw iu">熊猫</strong>来读取它，并以整洁的表格格式显示出来。</p><p id="5496" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b"><strong class="kw iu">track_loss</strong></code> <strong class="kw iu">、</strong> <code class="fe ot ou ov nz b"><strong class="kw iu">track_num_correct</strong></code> <strong class="kw iu">、</strong> <code class="fe ot ou ov nz b"><strong class="kw iu">_get_num_correct</strong></code>:这些是累计损失的效用函数，每批的正确预测数，以便以后计算历元损失和精度。</p><p id="6952" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe ot ou ov nz b"><strong class="kw iu">save</strong></code>:将所有运行数据(所有运行的<code class="fe ot ou ov nz b">results</code> <strong class="kw iu"> OrderedDict </strong>对象列表)保存为<code class="fe ot ou ov nz b">csv</code>和<code class="fe ot ou ov nz b">json</code>格式，以便进一步分析或 API 访问。</p><p id="ec0e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这门课有很多东西要学。恭喜你走到这一步！最困难的部分已经过去了。从现在开始，一切都会变得有意义。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="fe58" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">培养</h1><p id="6093" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi lq translated">终于，我们准备好进行一些训练了！在我们的<code class="fe ot ou ov nz b">RunBuilder </code>和<code class="fe ot ou ov nz b">RunManager</code>课程的帮助下，培训过程变得轻而易举:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="42d3" class="od nc it nz b gy oe of l og oh">m = RunManager()<br/><br/># get all runs from params using RunBuilder class<br/>for run in RunBuilder.get_runs(params):<br/><br/>    # if params changes, following line of code should reflect the changes too<br/>    network = Network()<br/>    loader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size)<br/>    optimizer = optim.Adam(network.parameters(), lr=run.lr)<br/><br/>    m.begin_run(run, network, loader)<br/>    for epoch in range(epochs):<br/>      <br/>      m.begin_epoch()<br/>      for batch in loader:<br/>        <br/>        images = batch[0]<br/>        labels = batch[1]<br/>        preds = network(images)<br/>        loss = F.cross_entropy(preds, labels)<br/><br/>        optimizer.zero_grad()<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>        m.track_loss(loss)<br/>        m.track_num_correct(preds, labels)<br/><br/>      m.end_epoch()<br/>    m.end_run()<br/><br/># when all runs are done, save results to files<br/>m.save('results')</span></pre><p id="6743" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们使用<code class="fe ot ou ov nz b">RunBuilder</code>创建超参数的迭代器，然后遍历每个超参数组合来执行我们的训练:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="bbb0" class="od nc it nz b gy oe of l og oh">for run in RunBuilder.get_runs(params):</span></pre><p id="3677" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，我们从上面定义的<code class="fe ot ou ov nz b">Network</code>类创建我们的<code class="fe ot ou ov nz b">network</code>对象。<code class="fe ot ou ov nz b">network = Network()</code>。这个<code class="fe ot ou ov nz b">network</code>物体承载了我们需要训练的所有重量/偏差。</p><p id="412d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们还需要创建一个<code class="fe ot ou ov nz b">DataLoader </code>对象。它是一个 PyTorch 类，保存我们的训练/验证/测试数据集，它将遍历数据集，并按指定的<code class="fe ot ou ov nz b">batch_size</code>批量给我们训练数据。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="d28b" class="od nc it nz b gy oe of l og oh">loader = torch.utils.data.DataLoader(train_set, batch_size = run.batch_size)</span></pre><p id="e236" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">之后，我们将使用<code class="fe ot ou ov nz b">torch.optim</code>类创建一个优化器。<code class="fe ot ou ov nz b">optim</code>类获取网络参数和学习率作为输入，将帮助我们逐步完成训练过程并更新梯度等。我们将使用 Adam 作为优化算法。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="31f2" class="od nc it nz b gy oe of l og oh">optimizer = optim.Adam(network.parameters(), lr=run.lr)</span></pre><p id="f156" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好的。现在我们已经创建了网络，准备好了数据加载器，选择了优化器。让我们开始训练吧！</p><p id="0989" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将遍历所有我们想要训练的纪元(这里是 3 个),所以我们将所有东西都放在一个“纪元”循环中。我们还使用我们的<code class="fe ot ou ov nz b">RunManager</code>类的<code class="fe ot ou ov nz b">begin_run</code>方法来开始跟踪跑步训练数据。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="396d" class="od nc it nz b gy oe of l og oh">m.begin_run(run, network, loader)    <br/>for epoch in range(epochs):</span></pre><p id="9220" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于每个时期，我们将遍历每批图像来进行训练。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="f239" class="od nc it nz b gy oe of l og oh">m.begin_epoch()    <br/>for batch in loader:              <br/>  images = batch[0]      <br/>  labels = batch[1]      <br/>  preds = network(images)      <br/>  loss = F.cross_entropy(preds, labels)<br/>         <br/>  optimizer.zero_grad()  <br/>  loss.backward()      <br/>  optimizer.step()<br/>      <br/>  m.track_loss(loss)      <br/>  m.track_num_correct(preds, labels)</span></pre><p id="5e33" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上面的代码是真正的训练发生的地方。我们从批处理中读入图像和标签，使用<code class="fe ot ou ov nz b">network</code>类进行前向传播(还记得上面的<code class="fe ot ou ov nz b">forward</code>方法吗？)并得到预测。通过预测，我们可以使用<code class="fe ot ou ov nz b">cross_entropy</code>函数计算该批次的损失。一旦计算出损失，我们用<code class="fe ot ou ov nz b">.zero_grad()</code>重置梯度(否则 PyTorch 会累积梯度，这不是我们想要的)，使用<code class="fe ot ou ov nz b">loss.backward()</code>方法进行一次反向传播，以计算权重/偏差的所有梯度。然后，我们使用上面定义的优化器来更新权重/偏差。现在网络已经为当前批次更新，我们将计算正确预测的损失和数量，并使用我们的<code class="fe ot ou ov nz b">RunManager</code>类的<code class="fe ot ou ov nz b">track_loss</code>和<code class="fe ot ou ov nz b">track_num_correct</code>方法累积/跟踪它们。</p><p id="7f50" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一旦全部完成，我们将使用<code class="fe ot ou ov nz b">m.save('results')</code>将结果保存在文件中。</p><p id="d848" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">笔记本中运行的输出如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/20a7dba510027e44f8a02993d059b2be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgEb42V3ubSA9P8BKryv2w.png"/></div></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="4779" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">张量板</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/b670ba8b9acdf3c9b98659c116f6f755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dOWdO9wmi8tNgjKF.gif"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Image from Tensorboard.org</figcaption></figure><p id="3030" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">传感器板是一个 TensorFlow 可视化工具，现在 PyTorch 也支持它。我们已经努力将所有内容输出到。/runs '文件夹，Tensor Board 将在其中查找要使用的记录。我们现在需要做的只是启动张量板并进行检查。由于我在 Google Colab 上运行这个模型，我们将使用一个名为<code class="fe ot ou ov nz b">ngrok</code>的服务来代理和访问我们在 Colab 虚拟机上运行的张量板。先安装<code class="fe ot ou ov nz b">ngrok </code>:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="2d96" class="od nc it nz b gy oe of l og oh">!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip</span><span id="9519" class="od nc it nz b gy pk of l og oh">!unzip ngrok-stable-linux-amd64.zip</span></pre><p id="ab19" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后，指定我们要从中运行张量板的文件夹，并启动张量板 web 界面。/runs 是默认值):</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="181a" class="od nc it nz b gy oe of l og oh">LOG_DIR = './runs'</span><span id="2dfa" class="od nc it nz b gy pk of l og oh">get_ipython().system_raw(</span><span id="f7e6" class="od nc it nz b gy pk of l og oh">'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &amp;'</span><span id="c228" class="od nc it nz b gy pk of l og oh">.format(LOG_DIR)</span><span id="a756" class="od nc it nz b gy pk of l og oh">)</span></pre><p id="c5bd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">启动<code class="fe ot ou ov nz b">ngrok</code>代理:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="8468" class="od nc it nz b gy oe of l og oh">get_ipython().system_raw('./ngrok http 6006 &amp;')</span></pre><p id="5215" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">生成一个 URL，以便我们可以从 Jupyter 笔记本中访问我们的张量板:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b09c" class="od nc it nz b gy oe of l og oh">! curl -s http://localhost:4040/api/tunnels | python3 -c \</span><span id="527f" class="od nc it nz b gy pk of l og oh">"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])"</span></pre><p id="6a3b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们在下面看到的，TensorBoard 是一个非常方便的可视化工具，让我们可以深入了解我们的训练，并可以极大地帮助超参数调整过程。我们可以很容易地找出哪个超参数组件表现最好，然后用它来做我们真正的训练。</p><div class="kj kk kl km gt ab cb"><figure class="qb kn qc qd qe qf qg paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/23a6b096511ceb3324d05ca28106aa54.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*L5xNeZ1A4cENWc0tmvbwWg.png"/></div></figure><figure class="qb kn qh qd qe qf qg paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/082aca40dbaad8ee63b4f96d7565ff73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*jjAFS756-DntXAgu_ryMdA.png"/></div></figure></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qi"><img src="../Images/9ed4dc0229aadf0d21ca6a0fcc235c59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m_ap-x42L9h53Phfa47ZtA.png"/></div></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="a41c" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">结论</h1><p id="1162" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">正如你所看到的，PyTorch 作为一个机器学习框架是灵活的、强大的和富有表现力的。你只需要写 Python 代码。由于本文的主要重点是展示如何使用 PyTorch 构建一个卷积神经网络，并以结构化的方式对其进行训练，因此我没有完成整个训练时期，精度也不是最佳的。你可以自己试试，看看模型表现如何。</p><p id="b4ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这篇文章很大程度上受到了 deeplizard 在 YouTube 上的 PyTorch 视频系列的启发。甚至大部分代码片段都是直接抄袭过来的。我想感谢他们的伟大内容，如果你觉得有必要深入研究，随时去看看，并订阅他们的频道。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="eaf3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">觉得这篇文章有用？在 Medium 上关注我(<a class="ox oy ep" href="https://medium.com/u/72c98619a048?source=post_page-----efb297e22582--------------------------------" rel="noopener" target="_blank">李立伟</a>)或者你可以在 Twitter <a class="ae lz" href="https://twitter.com/lymenlee" rel="noopener ugc nofollow" target="_blank"> @lymenlee </a>或者我的博客网站<a class="ae lz" href="https://wayofnumbers.com" rel="noopener ugc nofollow" target="_blank">wayofnumbers.com</a>上找到我。你也可以看看我下面最受欢迎的文章！</p><div class="qj qk gp gr ql qm"><a rel="noopener follow" target="_blank" href="/this-is-cs50-a-pleasant-way-to-kick-off-your-data-science-education-d6075a6e761a"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd iu gy z fp qr fr fs qs fu fw is bi translated">“这是 CS50”:开始数据科学教育的愉快方式</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">为什么 CS50 特别适合巩固你的软件工程基础</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">towardsdatascience.com</p></div></div><div class="qv l"><div class="qw l qx qy qz qv ra ks qm"/></div></div></a></div><div class="qj qk gp gr ql qm"><a rel="noopener follow" target="_blank" href="/two-sides-of-the-same-coin-fast-ai-vs-deeplearning-ai-b67e9ec32133"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd iu gy z fp qr fr fs qs fu fw is bi translated">一枚硬币的两面:杰瑞米·霍华德的 fast.ai vs 吴恩达的 deeplearning.ai</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">如何不通过同时参加 fast.ai 和 deeplearning.ai 课程来“过度适应”你的人工智能学习</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">towardsdatascience.com</p></div></div><div class="qv l"><div class="rb l qx qy qz qv ra ks qm"/></div></div></a></div><div class="qj qk gp gr ql qm"><a href="https://medium.com/datadriveninvestor/thoughts-on-andrew-ngs-machine-learning-course-7724df76320f" rel="noopener follow" target="_blank"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd iu gy z fp qr fr fs qs fu fw is bi translated">我完成了吴恩达的机器学习课程，感觉棒极了！</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">好的，坏的，美丽的</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">medium.com</p></div></div><div class="qv l"><div class="rc l qx qy qz qv ra ks qm"/></div></div></a></div></div></div>    
</body>
</html>