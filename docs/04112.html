<html>
<head>
<title>Building an ML application using MLlib in Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Pyspark 中使用 MLlib 构建 ML 应用程序</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-an-ml-application-with-mllib-in-pyspark-part-1-ac13f01606e2?source=collection_archive---------3-----------------------#2019-06-28">https://towardsdatascience.com/building-an-ml-application-with-mllib-in-pyspark-part-1-ac13f01606e2?source=collection_archive---------3-----------------------#2019-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="71cb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">本教程将指导您如何在 apache spark 中创建 ML 模型，以及如何与它们交互</h2></div><h1 id="35cf" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="618b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Apache Spark 是一种按需大数据工具，全球许多公司都在使用它。它的内存计算和并行处理能力是这个工具流行的主要原因。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/1ad82342d49d0c8c0c164e0af863c688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*A8HoNRfZxh-NklyeI043Lw.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Spark Stack</figcaption></figure><p id="fde7" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">MLlib 是一个可扩展的机器学习库，它与 Spark SQL、Spark Streaming 和 GraphX 等其他服务一起出现在 Spark 之上。</p><h1 id="c13a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">数据集相关介绍</strong></h1><p id="775f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本文中，我们将专注于一个叫做笔画数据集的数据集。中风是一种流向大脑的血流停止或血流过多的情况。中风的危险因素有</p><ul class=""><li id="44f6" class="mk ml iq kz b la mf ld mg lg mm lk mn lo mo ls mp mq mr ms bi translated">吸烟</li><li id="9c9c" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">高血压</li><li id="1a84" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">糖尿病</li><li id="2eb7" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">血液胆固醇水平高</li><li id="753c" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">酗酒</li><li id="c8e5" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">高脂肪(尤其是饱和脂肪)和高盐，但低纤维、水果和蔬菜的饮食</li><li id="a341" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">缺乏经常锻炼</li><li id="d15f" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">肥胖</li></ul><p id="dc2c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">所以我这里得到了一个不错的数据集:<a class="ae my" href="https://bigml.com/user/francisco/gallery/model/508b2008570c6372100000b1#info" rel="noopener ugc nofollow" target="_blank"><em class="mz">https://bigml . com/user/Francisco/gallery/model/508 b 2008570 c 6372100000 B1 # info</em></a></p><p id="b27d" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">以下是数据集:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/c1a06497f992bb18614cf6dd0a3a33e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P4peJgrljDHdYJrE-BXjag.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Stroke Dataset</figcaption></figure><p id="62b8" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">该数据集几乎包含了上述中风的所有风险因素。因此，选择具有适当风险因素的数据集非常重要。</p><p id="263f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们将把列的字符串值变成数字值。这样做的原因将在后面解释。使用 Excel 中的替换功能，我将数据集更改为以下内容</p><ol class=""><li id="54ef" class="mk ml iq kz b la mf ld mg lg mm lk mn lo mo ls nf mq mr ms bi translated">性别列—男性=1，女性=0</li></ol><p id="17dc" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">2.吸烟史——从未=0，曾经=0.25，当前= 0.5，以前= 0.75，当前= 1.0</p><h1 id="101c" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">使用的服务和库</strong></h1><ol class=""><li id="4f63" class="mk ml iq kz b la lb ld le lg ng lk nh lo ni ls nf mq mr ms bi translated">Google cloud——我们将在 Dataproc 中建立我们的 spark 集群，并在 Jupyter 笔记本中编写代码</li><li id="6f0c" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls nf mq mr ms bi translated">Jpmml(pyspark2pmml) —用于将我们的模型转换成 pmml 文件。</li><li id="71db" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls nf mq mr ms bi translated">open scoring——一个为 PMML 模型评分的 REST web 服务。</li><li id="1dbd" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls nf mq mr ms bi translated">VS 代码——我们将使用 React JS 构建一个与 REST 服务器通信的交互式网站。</li></ol><h1 id="1ec4" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">架构图:</strong></h1><p id="d304" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下图展示了我们整个项目的简要架构。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nj"><img src="../Images/1b6fa38efe91bae3816e8c23d234519b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9h4n88719evf9s1NwxgDg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">The architecture diagram of our project</figcaption></figure><h1 id="8f6f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">步骤 1:设置谷歌云</strong></h1><p id="8b80" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Google cloud 有一个名为 Dataproc 的服务，用于创建预装 Apache Spark 的集群。我们可以随时调整集群的大小。谷歌云提供免费的 300 美元信用作为入门优惠。因此，我们将使用这些免费配额来建立我们的集群。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/39770577b908ac42628bb61d2765ef90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*adBptbRv4e8OFkm9lm8ptA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Google Cloud Console</figcaption></figure><p id="151d" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">点击“激活”获得 300 美元的免费点数。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/2f3dafb4e340dda3832df4e35dd91e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FmJGozlZ-b7W8UvuBGmeNA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Registration Step-1</figcaption></figure><p id="b0fc" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">选择您的国家，然后点击“继续”。在下一页，您将被提示输入您的帐单细节和信用卡或借记卡细节。填写它们，然后点击底部的按钮。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/d1b30416be612a2f8eefbc3bd0f9086d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkpszRIeuw_Rp4wZNuLHiw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Google Cloud Console - Dataproc</figcaption></figure><p id="cfad" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">将打开控制台页面。在页面顶部，在搜索栏中键入 Dataproc，上面的页面就会打开。单击 create a cluster 开始创建集群。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/39e3d1f108c2b34aea4e19ddca16412e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NguSjr96j7MIMeLZTtx5nA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">GC — Creating a cluster-1</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/9d47ed6042a92ac7aa6cd270619bec52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_Rlg0Tcn-tL70Nn9qQ9NIw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">GC — Creating a cluster-2</figcaption></figure><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/75a52eed19250f50ee06751ad36ececb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAdhn5DZG5rdmwmd4qCXJg.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">GC — Creating a cluster-3</figcaption></figure><p id="5a18" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">请确保您输入了与上述相同的设置。点击高级选项，按照上面的图像设置，然后点击创建。创建一个集群可能需要 2 到 3 分钟。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/bfe569e30eb841f939b1dfd684719400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mREa47otVvLjqB4eMKKu5Q.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Google cloud — Dataproc clusters</figcaption></figure><p id="bf76" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">导航到群集，然后单击虚拟机实例。在 VM 实例下，我们可以看到创建了一个主节点和两个工作节点。主节点的作用是它通常请求集群中的资源，并使它们对 spark 驱动程序可用。它监视和跟踪工作节点的状态，这些工作节点的工作是托管 executor 进程，该进程存储来自任务的输出数据并托管 JVM。详细描述可以在<a class="ae my" href="http://www.informit.com/articles/article.aspx?p=2928186" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="22eb" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在单击主节点的 SSH 按钮。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/eddb2f6dfa976603db2f3785b823dfe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNGxwH6GjvKpO3sFb9NGsA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">SSH</figcaption></figure><p id="3a1f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">一个新的终端在一个新的 chrome 标签中打开。这是命令行界面，通过它我们可以与我们的集群进行交互。键入“pyspark”检查 spark 上的安装及其版本。确保 spark 版本在 2.2 以上，python 版本为 3.6。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/8d3ef1cc2486bd39d03b910633b5a1d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X__jz-e4Cv4WEEJp_sfQjw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Firewall Rules</figcaption></figure><p id="e11b" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在设置 jupyter 笔记本，我们需要创建一个防火墙规则。按照图片设置新的防火墙规则。确保在协议和端口中选择“全部允许”。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/344158270934ec2965afcf7e3071b980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nyfqn2Huq0spRcXDGHhvIA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Firewall Rules</figcaption></figure><p id="00fa" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">点击保存并导航至“外部 IP 地址”。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/4b4f7755f7b116de643d2602355becee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rCfetbUb8RvuFqdqoTc_GA.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">External IP addresses</figcaption></figure><p id="b42b" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">将“spark-cluster-m”的类型改为静态。给出任意一个名字，点击“保留”。</p><p id="d658" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在导航到“SSH”并键入以下命令。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="6ee9" class="nq kg iq nm b gy nr ns l nt nu">sudo nano ~/.jupyter_notebook_<a class="ae my" href="https://www.youtube.com/redirect?event=comments&amp;stzid=UgwMLhVicKWXmwMzyJ54AaABAg&amp;q=http%3A%2F%2Fconfig.py%2F&amp;redir_token=7XHzrHJ0cqu2HG4iRpSCumF2asJ8MTU2MDUzMTgyMEAxNTYwNDQ1NDIw" rel="noopener ugc nofollow" target="_blank">config.py</a></span></pre><p id="cfc4" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">复制下面的线并粘贴它。按 CTRL+o，回车，CTRL+x。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="d6f6" class="nq kg iq nm b gy nr ns l nt nu">c=get_config()</span><span id="a1c1" class="nq kg iq nm b gy nv ns l nt nu">c.NotebookApp.ip=’*’</span><span id="b3b6" class="nq kg iq nm b gy nv ns l nt nu">c.NotebookApp.open_browser=False</span><span id="a838" class="nq kg iq nm b gy nv ns l nt nu">c.NotebookApp.port=5000</span></pre><p id="7c7e" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在我们可以通过下面的命令打开 jupyter 笔记本</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="cedf" class="nq kg iq nm b gy nr ns l nt nu">jupyter-notebook --no-browser — port=5000</span></pre><p id="e524" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">在 SSH 中键入上述命令，然后打开一个新的选项卡，并在 google chrome 中键入“https://localhost:5000”以打开 Jupyter notebook。在我的例子中，本地主机是 35.230.35.117</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/97c8ba9c875bfd9c9cad38e436cb7921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FSbm89a9-w4tq-kcw1i5A.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Jupyter Notebook</figcaption></figure><h1 id="fbb3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">第二步:在 Jupyter 笔记本的 Pyspark 中编码</strong></h1><p id="2710" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在进入这一部分之前，我们需要安装一些外部库。</p><p id="2d77" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们需要 Imblearn 库来执行 SMOTE，因为我们的数据集高度不平衡。更多关于 smote 的信息可以在这个<a class="ae my" href="https://medium.com/coinmonks/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167" rel="noopener">链接</a>中找到。</p><p id="6480" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">在 SSH 中，键入</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="511f" class="nq kg iq nm b gy nr ns l nt nu">sudo -i</span></pre><p id="5170" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">然后键入下面一行</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="b4a1" class="nq kg iq nm b gy nr ns l nt nu">conda install -c glemaitre imbalanced-learn</span></pre><p id="c46f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">退出根文件夹，然后打开 Jupyter 笔记本。开始编码吧。</p><p id="f4a0" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated"><strong class="kz ir">导入重要库</strong></p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="ae7c" class="nq kg iq nm b gy nr ns l nt nu">from pyspark.sql import SQLContext<br/>from pyspark.sql import DataFrameNaFunctions<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import DecisionTreeClassifier<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.feature import Binarizer<br/>from pyspark.ml.feature import OneHotEncoder, VectorAssembler, StringIndexer, VectorIndexer<br/>from pyspark.ml.classification import RandomForestClassifier<br/>from pyspark.sql.functions import avg</span><span id="be4f" class="nq kg iq nm b gy nv ns l nt nu">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="d50c" class="nq kg iq nm b gy nv ns l nt nu">from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/>from pyspark.mllib.evaluation import MulticlassMetrics<br/>from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="f62c" class="nq kg iq nm b gy nv ns l nt nu">from imblearn.over_sampling import SMOTE<br/>from imblearn.combine import SMOTEENN<br/>from sklearn.model_selection import train_test_split<br/>from collections import Counter</span></pre><p id="8aac" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在我们需要创建一个 spark 会话。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="6bef" class="nq kg iq nm b gy nr ns l nt nu">from pyspark.context import SparkContext<br/>from pyspark.sql.session import SparkSession<br/>sc = SparkContext(‘local’)<br/>spark = SparkSession(sc)</span></pre><p id="2d44" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们需要从存储中访问我们的数据文件。导航到 google 云控制台中的“bucket”并创建一个新的 bucket。我命名为“data-stroke-1 ”,并上传修改后的 CSV 文件。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/c11403e22972c6a3596afa01c98a266d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cu-eiv8NXiR-cUNr5vi1vw.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Google Cloud Bucket</figcaption></figure><p id="b221" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在我们需要加载已经上传到我们的 bucket 中的 CSV 文件。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="038a" class="nq kg iq nm b gy nr ns l nt nu">input_dir = ‘gs://data-stroke-1/’<br/>df = spark.read.format(‘com.databricks.spark.csv’).options(header=’true’, inferschema=’true’).load(input_dir+’stroke.csv’)<br/>df.columns</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nw"><img src="../Images/989dbdfca84f7a6300464ab39fbbe87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zs7vG-3XLbSX-_zO0rqFmw.png"/></div></div></figure><p id="7369" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们可以通过使用下图所示的命令打印数据帧来检查它。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nx"><img src="../Images/6080f905d09774ca942afefd430f3902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpkX_xA3nlijhpydlREXxg.png"/></div></div></figure><p id="f416" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在，我们需要创建一个列，其中包含所有负责预测中风发生的特征。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="9708" class="nq kg iq nm b gy nr ns l nt nu">featureColumns = [‘gender’,’age’,‘diabetes’,‘hypertension’,<br/> ‘heart disease’,‘smoking history’,‘BMI’]</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ny"><img src="../Images/3af29087175391f2d167f30254ad8cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0pfam5BCv2NUOhPobO_JYA.png"/></div></div></figure><p id="adfd" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">确保所有列都是双精度值。接下来，让我们删除所有 2 岁以下的条目。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="d2cf" class="nq kg iq nm b gy nr ns l nt nu">df = df.filter(df.age &gt;2)<br/>df.count()</span></pre><p id="c4c1" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在让我们打印一个条形图来检查数据中存在的类的类型</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="8812" class="nq kg iq nm b gy nr ns l nt nu">responses = df.groupBy(‘stroke’).count().collect()<br/>categories = [i[0] for i in responses]<br/>counts = [i[1] for i in responses]<br/> <br/>ind = np.array(range(len(categories)))<br/>width = 0.35<br/>plt.bar(ind, counts, width=width, color=’r’)<br/> <br/>plt.ylabel(‘counts’)<br/>plt.title(‘Stroke’)<br/>plt.xticks(ind + width/2., categories)</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nz"><img src="../Images/d7351f2fdae99bf396e9d067b67a21c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IqX5RmNit2a-1OOxYYECaw.png"/></div></div></figure><h1 id="faee" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">步骤 3:数据预处理</h1><p id="edf4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="mz">步骤 3A。缺失数据管理</em></p><p id="ef7d" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在，进行适当的缺失数据管理以最终得到一个非常好的模型是非常重要的。使用“df.na.drop()”并不总是好的，它会删除所有丢失数据的行。用适当合理的价值观来填充它们是我们可以实现的一个想法。</p><p id="bad1" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">如我们所见，我们在身体质量指数列和吸烟史列中缺少值。填充这些身体质量指数值的一种可能方法是使用年龄值来填充它们。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oa"><img src="../Images/1b0558a45478f786465eaf7af07c621e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Br4Dn6oeLgoZ8BL4WhF5xQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">taken from — <a class="ae my" href="https://dqydj.com/bmi-distribution-by-age-calculator-for-the-united-states/" rel="noopener ugc nofollow" target="_blank"><em class="ob">https://dqydj.com/bmi-distribution-by-age-calculator-for-the-united-states/</em></a></figcaption></figure><p id="15d2" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">对于吸烟史，很难找到合理的数值来填补。通常情况下，16 岁以下的人对吸烟并没有那么上瘾，因此我们可以用 0 来填充那些年龄组的人的值。年龄在 17 到 24 岁之间的人一生中可能至少尝试过一次吸烟，所以我们可以给这些人 0.25 的价值。现在，一些人过了一定年龄就戒烟了，即使他们有健康问题，也很少有人继续吸烟。我们不能决定给它们取什么值，所以默认情况下，我们给它们赋值 0。</p><p id="b41c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated"><em class="mz">我们既可以删除这些列中缺少值的所有行，也可以按照上面的逻辑填充这些行。但是出于本教程的目的，我已经用上面的逻辑填充了丢失的行，但是实际上篡改数据而没有数据驱动的逻辑来备份通常不是一个好主意。</em></p><p id="157a" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们将对此数据帧执行一些操作，而 spark 数据帧不支持任何操作。因此，我们将我们的数据帧复制到熊猫数据帧，然后执行操作。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="f964" class="nq kg iq nm b gy nr ns l nt nu">imputeDF = df</span><span id="886e" class="nq kg iq nm b gy nv ns l nt nu">imputeDF_Pandas = imputeDF.toPandas()</span></pre><p id="7a5f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们将根据年龄将完整的数据帧分成许多数据帧，并用合理的值填充它们，然后，将所有数据帧合并成一个数据帧，并将其转换回 spark 数据帧。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="222f" class="nq kg iq nm b gy nr ns l nt nu">df_2_9 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=2 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 9)]<br/>values = {‘smoking history’: 0, ‘BMI’:17.125}<br/>df_2_9 = df_2_9.fillna(value = values)</span><span id="ae80" class="nq kg iq nm b gy nv ns l nt nu">df_10_13 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=10 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 13)]<br/>values = {‘smoking history’: 0, ‘BMI’:19.5}<br/>df_10_13 = df_10_13.fillna(value = values)</span><span id="a86a" class="nq kg iq nm b gy nv ns l nt nu">df_14_17 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=14 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 17)]<br/>values = {‘smoking history’: 0, ‘BMI’:23.05}<br/>df_14_17 = df_14_17.fillna(value = values)</span><span id="83d7" class="nq kg iq nm b gy nv ns l nt nu">df_18_24 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=18 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 24)]<br/>values = {‘smoking history’: 0, ‘BMI’:27.1}<br/>df_18_24 = df_18_24.fillna(value = values)</span><span id="399a" class="nq kg iq nm b gy nv ns l nt nu">df_25_29 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=25 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 29)]<br/>values = {‘smoking history’: 0, ‘BMI’:27.9}<br/>df_25_29 = df_25_29.fillna(value = values)</span><span id="7860" class="nq kg iq nm b gy nv ns l nt nu">df_30_34 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=30 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 34)]<br/>values = {‘smoking history’: 0.25, ‘BMI’:29.6}<br/>df_30_34 = df_30_34.fillna(value = values)</span><span id="1ed4" class="nq kg iq nm b gy nv ns l nt nu">df_35_44 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=35 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 44)]<br/>values = {‘smoking history’: 0.25, ‘BMI’:30.15}<br/>df_35_44 = df_35_44.fillna(value = values)</span><span id="a229" class="nq kg iq nm b gy nv ns l nt nu">df_45_49 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=45 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 49)]<br/>values = {‘smoking history’: 0, ‘BMI’:29.7}<br/>df_45_49 = df_45_49.fillna(value = values)</span><span id="7ae0" class="nq kg iq nm b gy nv ns l nt nu">df_50_59 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=50 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 59)]<br/>values = {‘smoking history’: 0, ‘BMI’:29.95}<br/>df_50_59 = df_50_59.fillna(value = values)</span><span id="1062" class="nq kg iq nm b gy nv ns l nt nu">df_60_74 = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;=60 ) &amp; (imputeDF_Pandas[‘age’] &lt;= 74)]<br/>values = {‘smoking history’: 0, ‘BMI’:30.1}<br/>df_60_74 = df_60_74.fillna(value = values)</span><span id="1b05" class="nq kg iq nm b gy nv ns l nt nu">df_75_plus = imputeDF_Pandas[(imputeDF_Pandas[‘age’] &gt;75 )]<br/>values = {‘smoking history’: 0, ‘BMI’:28.1}<br/>df_75_plus = df_75_plus.fillna(value = values)</span></pre><p id="bc10" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">组合所有数据帧</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="2fdb" class="nq kg iq nm b gy nr ns l nt nu">all_frames = [df_2_9, df_10_13, df_14_17, df_18_24, df_25_29, df_30_34, df_35_44, df_45_49, df_50_59, df_60_74, df_75_plus]<br/>df_combined = pd.concat(all_frames)<br/>df_combined_converted = spark.createDataFrame(df_combined)<br/>imputeDF = df_combined_converted</span></pre><p id="9a9d" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated"><em class="mz">步 3B。处理不平衡数据</em></p><p id="9375" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们将执行<a class="ae my" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html" rel="noopener ugc nofollow" target="_blank"> SMOTE </a>技术来处理不平衡数据。SMOTE 可以从这里引用:</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="bd9c" class="nq kg iq nm b gy nr ns l nt nu">X = imputeDF.toPandas().filter(items=[‘gender’, ‘age’, ‘diabetes’,’hypertension’,’heart disease’,’smoking history’,’BMI’])<br/>Y = imputeDF.toPandas().filter(items=[‘stroke’])</span><span id="9c2a" class="nq kg iq nm b gy nv ns l nt nu">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=0)</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oc"><img src="../Images/bf9be4b8403639bcf484225468d986d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LB-OSbanYii35WqAcf42kA.png"/></div></div></figure><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="2372" class="nq kg iq nm b gy nr ns l nt nu">sm = SMOTE(random_state=12, ratio = ‘auto’, kind = ‘regular’)</span><span id="98aa" class="nq kg iq nm b gy nv ns l nt nu">x_train_res, y_train_res = sm.fit_sample(X_train, Y_train)</span><span id="df42" class="nq kg iq nm b gy nv ns l nt nu">print(‘Resampled dataset shape {}’.format(Counter(y_train_res)))</span></pre><p id="91c8" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">请参考此<a class="ae my" href="https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html" rel="noopener ugc nofollow" target="_blank">链接</a>了解参数</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi od"><img src="../Images/3262abe2a757c1813a1f612f00610039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppooTNczYrEopKZz4Y-dfw.png"/></div></div></figure><p id="e4f7" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">X_train 包含除 Stroke 列之外的所有数据列。</p><p id="1648" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">Y_train 包含笔画列数据。</p><p id="28b2" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">将重新采样的数据组合成一个火花数据帧</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="3cc1" class="nq kg iq nm b gy nr ns l nt nu">dataframe_1 = pd.DataFrame(x_train_res,columns=[‘gender’, ‘age’, ‘diabetes’, ‘hypertension’, ‘heart disease’, ‘smoking history’, ‘BMI’])<br/>dataframe_2 = pd.DataFrame(y_train_res, columns = [‘stroke’])</span><span id="2472" class="nq kg iq nm b gy nv ns l nt nu"># frames = [dataframe_1, dataframe_2]<br/>result = dataframe_1.combine_first(dataframe_2)</span></pre><p id="486f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">将其改回火花数据帧</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="5b59" class="nq kg iq nm b gy nr ns l nt nu">imputeDF_1 = spark.createDataFrame(result)</span></pre><p id="5101" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">检查重新采样的数据。这与我们之前使用的代码相同。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oe"><img src="../Images/000d3ce1c825e70cf026a1327bbace97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ai3mMFSdHz2kgzpZc7fn9Q.png"/></div></div></figure><p id="92eb" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">如我们所见，我们成功地对数据进行了重新采样。现在我们将进入下一部分。</p><h1 id="558e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">第四步。构建 Spark ML 管道</strong></h1><p id="ade4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下面是一个 spark ml 项目的通用管道，除了我们没有使用字符串索引器和 oneHotEncoder。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi of"><img src="../Images/9fa039b033b8443f6c7858f44d3aff0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ywifmmWYMwiwnZ9QlpC1qQ.png"/></div></div><figcaption class="mb mc gj gh gi md me bd b be z dk">Spark ML Pipeline</figcaption></figure><p id="54f7" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在要构建一个汇编器，为此，我们需要一个二进制化器。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="3988" class="nq kg iq nm b gy nr ns l nt nu">binarizer = Binarizer(threshold=0.0, inputCol=”stroke”, outputCol=”label”)<br/>binarizedDF = binarizer.transform(imputeDF_1)</span><span id="6aff" class="nq kg iq nm b gy nv ns l nt nu">binarizedDF = binarizedDF.drop(‘stroke’)</span></pre><p id="3ed6" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">这将创建一个名为“label”的新列，其值与 stroke 列中的值相同。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="00dd" class="nq kg iq nm b gy nr ns l nt nu">assembler = VectorAssembler(inputCols = featureColumns, outputCol = “features”)<br/>assembled = assembler.transform(binarizedDF)</span><span id="2b09" class="nq kg iq nm b gy nv ns l nt nu">print(assembled)</span></pre><p id="394f" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">汇编程序将预测笔画所需的所有列组合起来，产生一个称为特征的向量。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi og"><img src="../Images/8db2662d404b99a3780abf8b50bd3c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wFT3-4IczRpK1-55uw15Hg.png"/></div></div></figure><p id="587e" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated"><strong class="kz ir">现在开始拆分数据</strong></p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="15df" class="nq kg iq nm b gy nr ns l nt nu">(trainingData, testData) = assembled.randomSplit([0.7, 0.3], seed=0)<br/>print(“Distribution of Ones and Zeros in trainingData is: “, trainingData.groupBy(“label”).count().take(3))</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oh"><img src="../Images/c0759fd4ac447c5a4ff21501490ec691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNDGkj47WoPHWnybTDPcUg.png"/></div></div></figure><p id="8bc0" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">培养</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="2f15" class="nq kg iq nm b gy nr ns l nt nu">dt = DecisionTreeClassifier(labelCol="label", featuresCol="features", maxDepth=25, minInstancesPerNode=30, impurity="gini")<br/>pipeline = Pipeline(stages=[dt])<br/>model = pipeline.fit(trainingData)</span></pre><p id="ef08" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">测试</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="8b9f" class="nq kg iq nm b gy nr ns l nt nu">predictions = model.transform(testData)</span></pre><p id="dc96" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">AUC-ROC</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="dd28" class="nq kg iq nm b gy nr ns l nt nu">from pyspark.mllib.evaluation import BinaryClassificationMetrics as metric<br/>results = predictions.select(['probability', 'label'])<br/> <br/>## prepare score-label set<br/>results_collect = results.collect()<br/>results_list = [(float(i[0][0]), 1.0-float(i[1])) for i in results_collect]<br/>scoreAndLabels = sc.parallelize(results_list)<br/> <br/>metrics = metric(scoreAndLabels)<br/>print("Test Data Aread under ROC score is : ", metrics.areaUnderROC)</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi oi"><img src="../Images/b473a52263abd8e95b885e19c1347604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iw9S4H0C4WbZriXu9mIWCg.png"/></div></div></figure><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="d37e" class="nq kg iq nm b gy nr ns l nt nu">from sklearn.metrics import roc_curve, auc<br/> <br/>fpr = dict()<br/>tpr = dict()<br/>roc_auc = dict()<br/> <br/>y_test = [i[1] for i in results_list]<br/>y_score = [i[0] for i in results_list]<br/> <br/>fpr, tpr, _ = roc_curve(y_test, y_score)<br/>roc_auc = auc(fpr, tpr)<br/> <br/>%matplotlib inline<br/>plt.figure()<br/>plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)<br/>plt.plot([0, 1], [0, 1], 'k--')<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('Receiver operating characteristic Graph')<br/>plt.legend(loc="lower right")<br/>plt.show()</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/11dcedef3e984011007986464721abce.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*0MDK9xhwdyUhjKLb0rZcKg.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">AUC — ROC Curve</figcaption></figure><p id="a5b6" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">正如我们所看到的，我们得到了 98 左右的 AUC-ROC 分数，这是非常好的。由于 SMOTE 技术的使用，模型可能会过拟合。(<em class="mz">但是逻辑回归对这个数据集很有效。但是 pyspark2pmml 库中似乎有一些错误，不能正确导出逻辑回归模型。</em>)因此，出于演示目的，我将使用决策树模型文件。</p><h1 id="d752" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">第五步。保存模型文件</h1><p id="5bb8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为此，我们将使用一个名为 PySPark2PMML 的库，它的细节可以在这里找到(<a class="ae my" href="https://github.com/jpmml/pyspark2pmml" rel="noopener ugc nofollow" target="_blank">https://github.com/jpmml/pyspark2pmml</a>)</p><p id="94b4" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">保存 jupyter 文件并退出 jupyter 笔记本。</p><p id="399e" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">从<a class="ae my" href="https://github.com/jpmml/jpmml-sparkml/releases" rel="noopener ugc nofollow" target="_blank">https://github.com/jpmml/jpmml-sparkml/releases</a>下载<em class="mz">jpmml-spark ml-executable-1 . 5 . 3 . jar</em>文件</p><p id="4df8" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">上传到 SSH</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/c611155bf8f6dc093d701c1d226c9a22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wRbvclhPIz-gyXN5VIfQqA.png"/></div></div></figure><p id="ada9" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">上传后，如果我们运行“ls”命令检查，我们会看到我们的文件。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/b66b7d9488a05bcf0d2f74a0c3020e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*I0qNVufAmof1F4t5YImNgg.png"/></div></figure><p id="3364" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在我们需要设置 jupyter notebook，当我们在 ssh 中键入 pyspark 时，我们需要打开 jupyter notebook。为此，我们需要更改环境变量。</p><p id="c201" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">更新 PySpark 驱动程序环境变量:</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/988437572616905058981add98bad743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*rs-BxGaAmxCNNunKlXFiDQ.png"/></div></figure><p id="f478" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">将下面几行添加到您的<code class="fe om on oo nm b">~/.bashrc</code>(或<code class="fe om on oo nm b">~/.zshrc</code>)文件中。按“I”插入新行。复制下面的代码并使用“CTRL+V”粘贴它。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="0baa" class="nq kg iq nm b gy nr ns l nt nu">export PYSPARK_DRIVER_PYTHON=jupyter<br/>export PYSPARK_DRIVER_PYTHON_OPTS='notebook'</span></pre><p id="eb83" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">要保存并退出 vi 编辑器，请按“ESC”和“:wq”保存。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/575f219733f97af9f1ba4d6f360437fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4TpU0cyMA0lQUxK9ixEsA.png"/></div></div></figure><p id="0e28" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">重启你的终端，然后输入“pyspark”。你应该可以运行 jupyter 笔记本。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nk"><img src="../Images/650f3585095c310ab1dfe1454d24b241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x6_RYy5OxJkaNjhVL45cHQ.png"/></div></div></figure><p id="2d99" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">您应该能够在其中一行中看到端口号。</p><p id="31d0" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在要使用 pmml 库，通过调用下面的命令打开 jupyter 笔记本。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="a014" class="nq kg iq nm b gy nr ns l nt nu">pyspark --jars /home/yashwanthmadaka_imp24/jpmml-sparkml-executable-1.5.3.jar</span></pre><p id="999e" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">打开 jupyter 笔记本后，运行我们之前写的所有单元格。现在，添加下面这段代码。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="7fa2" class="nq kg iq nm b gy nr ns l nt nu">trainingData = trainingData.drop(“features”)</span><span id="b958" class="nq kg iq nm b gy nv ns l nt nu">from pyspark.ml.feature import RFormula<br/>formula = RFormula(formula = "label ~ .")<br/>classifier = DecisionTreeClassifier(maxDepth=25, minInstancesPerNode=30, impurity="gini")<br/>pipeline = Pipeline(stages = [formula, classifier])<br/>pipelineModel = pipeline.fit(trainingData)</span><span id="92de" class="nq kg iq nm b gy nv ns l nt nu">from pyspark2pmml import PMMLBuilder<br/>pmmlBuilder = PMMLBuilder(sc, trainingData, pipelineModel) \<br/> .putOption(classifier, "compact", True)</span><span id="7edb" class="nq kg iq nm b gy nv ns l nt nu">pmmlBuilder.buildFile("dt-stroke.pmml")</span></pre><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi op"><img src="../Images/e4e02c7bf7ada2e4de6f49bd85476186.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1MeI5Ir6qbswsAfY8Y40w.png"/></div></div></figure><p id="998b" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">运行上述代码后，将在提到的位置创建一个新文件。将这个文件下载到您的本地桌面，让我们开始构建一个网站来与我们的模型文件进行交互。</p><p id="8c9a" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">整个 jupyter 笔记本可以在<a class="ae my" href="https://github.com/yashwanthmadaka24/Stroke-Classification---Decision-Tree" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="64ff" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">第六步。构建一个前端 ReactJS 应用程序与 PMML 文件交互。</h1><p id="f899" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="mz">步骤 6a。从我们的模型文件构建一个 REST 服务器:</em></p><p id="9dd7" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">对于与模型文件交互的应用程序，我们需要将应用程序公开为 REST web 服务。为此，我们将借助 Openscoring 库。</p><p id="5dcf" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们需要使用 maven 安装 Openscoring。确保将我们从 Google clouds 虚拟机下载的模型文件放入<em class="mz">PATH/open scoring/open scoring-client/target 文件夹</em>。</p><p id="92df" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">其中 PATH = open scoring 文件所在的路径。</p><p id="1147" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">安装后，我们需要按照下面的命令启动服务器。</p><p id="d16c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">首先，通过进入服务器文件夹并键入下面的命令来启动服务器</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="bf9b" class="nq kg iq nm b gy nr ns l nt nu">cd openscoring-server/target</span><span id="d786" class="nq kg iq nm b gy nv ns l nt nu">java -jar openscoring-server-executable-2.0-SNAPSHOT.jar</span></pre><p id="633b" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">接下来，打开客户端文件夹，输入下面的命令。接下来，打开一个新的 cmd 并键入以下命令。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="f27b" class="nq kg iq nm b gy nr ns l nt nu">cd openscoring-client/target</span><span id="81db" class="nq kg iq nm b gy nv ns l nt nu">java -cp openscoring-client-executable-2.0-SNAPSHOT.jar org.openscoring.client.Deployer --model <a class="ae my" href="http://localhost:8080/openscoring/model/stroke" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/openscoring/model/stroke</a> --file dt-stroke.pmml</span></pre><p id="fa0c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">当我们访问<a class="ae my" href="http://localhost:8080/openscoring/model/stroke" rel="noopener ugc nofollow" target="_blank">http://localhost:8080/open scoring/model/stroke</a>时可以看到下面的结构</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/4ceb166a12fde7f365017221b7debfcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*TdwDSutyTNKhpkSJrmL_aw.png"/></div></figure><p id="13a3" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated"><em class="mz">步骤 6b。下载 ReactJS 前端并运行:</em></p><p id="d5ce" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在访问 this <a class="ae my" href="https://github.com/yashwanthmadaka24/React-Js-Website" rel="noopener ugc nofollow" target="_blank"> Github 链接</a>并克隆这个项目。</p><p id="356c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">下载后，使用 VS 代码打开这个项目文件夹。打开里面的终端，输入</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="2245" class="nq kg iq nm b gy nr ns l nt nu">npm install</span></pre><p id="48b3" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">在启动 ReactJS 应用程序之前，我们需要启用 CORS。为此，我们可以添加一个<a class="ae my" href="https://chrome.google.com/webstore/detail/allow-cors-access-control/lhobafahddgcelffkeicbaginigeejlf?hl=en" rel="noopener ugc nofollow" target="_blank"> chrome 扩展</a>。</p><p id="305b" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">打开 CORS 后，在 VS 代码终端中键入以下命令。</p><pre class="lu lv lw lx gt nl nm nn no aw np bi"><span id="fd68" class="nq kg iq nm b gy nr ns l nt nu">npm start</span></pre><p id="87b4" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">将打开一个 web 界面，如下所示。</p><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div class="gh gi or"><img src="../Images/194e978dd68b3704a0e75ab49d465855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*PMt7UqTrg5P_q4Nf5A38LQ.png"/></div><figcaption class="mb mc gj gh gi md me bd b be z dk">ReactJS Frontend</figcaption></figure><p id="10d0" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">我们可以输入任何值并测试它。我们会得到预测，中风发生的概率和不发生中风的概率。</p><p id="bb69" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">所有与 REST 服务器交互的方法都编码在 index.js 文件中。</p></div><div class="ab cl os ot hu ou" role="separator"><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox oy"/><span class="ov bw bk ow ox"/></div><div class="ij ik il im in"><h1 id="c06c" class="kf kg iq bd kh ki oz kk kl km pa ko kp jw pb jx kr jz pc ka kt kc pd kd kv kw bi translated"><strong class="ak">附言</strong></h1><blockquote class="pe"><p id="3bfc" class="pf pg iq bd ph pi pj pk pl pm pn ls dk translated">现实模型的 98 分是不可能达到的，这个博客的主要意义是展示如何与 pyspark 制作的 ML 模型进行交互。</p><p id="673e" class="pf pg iq bd ph pi pj pk pl pm pn ls dk translated">我们的数据预处理越好，我们的模型就越好。模型的质量直接取决于我们使用的数据的质量和多样性。因此，最好花更多的时间进行适当的数据清理和数据过滤技术。</p></blockquote><h1 id="c04a" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw po jx kr jz pp ka kt kc pq kd kv kw bi translated">有用的链接</h1><ol class=""><li id="efdc" class="mk ml iq kz b la lb ld le lg ng lk nh lo ni ls nf mq mr ms bi translated">SMOTE—<a class="ae my" href="https://medium.com/coinmonks/smote-and-adasyn-handling-imbalanced-data-set-34f5223e167" rel="noopener">https://medium . com/coin monks/SMOTE-and-adasyn-handling-unbalanced-data-set-34f 5223 e167</a></li><li id="62f2" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls nf mq mr ms bi translated">pyspark 2 pmml—<a class="ae my" href="https://github.com/jpmml/pyspark2pmml" rel="noopener ugc nofollow" target="_blank">https://github.com/jpmml/pyspark2pmml</a></li><li id="04e3" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls nf mq mr ms bi translated">开场得分—<a class="ae my" href="https://github.com/openscoring/openscoring" rel="noopener ugc nofollow" target="_blank">https://github.com/openscoring/openscoring</a></li><li id="56ee" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls nf mq mr ms bi translated">ReactJs 前端—<a class="ae my" href="https://github.com/yashwanthmadaka24/React-Js-Website" rel="noopener ugc nofollow" target="_blank">https://github.com/yashwanthmadaka24/React-Js-Website</a></li></ol><p id="d10c" class="pw-post-body-paragraph kx ky iq kz b la mf jr lc ld mg ju lf lg mh li lj lk mi lm ln lo mj lq lr ls ij bi translated">现在，是休息的时候了😇</p><figure class="lu lv lw lx gt ly"><div class="bz fp l di"><div class="pr ps l"/></div></figure></div></div>    
</body>
</html>