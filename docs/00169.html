<html>
<head>
<title>The Vanishing Gradient Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消失梯度问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484?source=collection_archive---------1-----------------------#2019-01-08">https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484?source=collection_archive---------1-----------------------#2019-01-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="67a5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">问题、原因、意义及其解决方案</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/534f37e2a7a295fae3113069fbafaeca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmhzHazJEUBdOcLrnNoK0g.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Title Image // <a class="ae kv" href="https://commons.wikimedia.org/wiki/File:Vanishing_Point_of_Railway.jpg" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="fb9c" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">问题:</strong></h2><p id="5e53" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">随着使用某些激活函数的更多层被添加到神经网络，损失函数的梯度接近零，使得网络难以训练。</p><h2 id="dd43" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">为什么:</strong></h2><p id="a864" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">某些激活函数，如 sigmoid 函数，将大的输入空间压缩成 0 到 1 之间的小输入空间。因此，sigmoid 函数输入的大变化会导致输出的小变化。因此，导数变小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ml"><img src="../Images/5322fd93bcf07db675f811b59e6f2c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A3A_rt4YmumHusvTvVTxw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 1: The sigmoid function and its derivative // <a class="ae kv" href="https://isaacchanghau.github.io/img/deeplearning/activationfunction/sigmoid.png" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="a99f" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">例如，图 1 是 sigmoid 函数及其导数。请注意，当 sigmoid 函数的输入变大或变小时(当|x|变大时)，导数变得接近于零。</p><h2 id="ef9e" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">意义何在:</strong></h2><p id="943b" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">对于只有几层使用这些激活的浅层网络，这不是一个大问题。然而，当使用更多的层时，它会导致梯度太小而不能有效地进行训练。</p><p id="06b2" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">使用反向传播找到神经网络的梯度。简单地说，反向传播通过从最后一层一层地移动到初始层来找到网络的导数。根据链式法则，每层的导数沿网络向下相乘(从最后一层到初始层),以计算初始层的导数。</p><p id="cb36" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">然而，当<em class="mr"> n </em>个隐藏层使用类似 sigmoid 函数的激活时，<em class="mr"> n </em>个小导数相乘在一起。因此，当我们向下传播到初始层时，梯度呈指数下降。</p><p id="58ad" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">小的梯度意味着初始层的权重和偏差不会随着每次训练而有效地更新。由于这些初始层通常对识别输入数据的核心元素至关重要，因此会导致整个网络的整体不准确。</p><h2 id="6ec3" class="kw kx iq bd ky kz la dn lb lc ld dp le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated"><strong class="ak">解决方案:</strong></h2><p id="b412" class="pw-post-body-paragraph ls lt iq lu b lv lw jr lx ly lz ju ma lf mb mc md lj me mf mg ln mh mi mj mk ij bi translated">最简单的解决方法是使用其他激活函数，比如 ReLU，它不会引起一个小的导数。</p><p id="7def" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">剩余网络是另一种解决方案，因为它们提供直接到早期层的剩余连接。如图 2 所示，残差连接直接将块开头的值<strong class="lu ir"> x </strong>加到块的结尾(F(x)+x)。这种剩余连接不经过激活函数，激活函数“挤压”导数，导致块的整体导数更高。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/78cb0d060af67e52f12ec1210bbbc9fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*mxJ5gBvZnYPVo0ISZE5XkA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 2: A residual block</figcaption></figure><p id="34e1" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">最后，批量规范化图层也可以解决这个问题。如前所述，当一个大的输入空间被映射到一个小的输入空间，导致导数消失时，问题就出现了。在图 1 中，这在|x|较大时最为明显。批量标准化通过简单地标准化输入来减少这个问题，这样|x|就不会到达 sigmoid 函数的外部边缘。如图 3 所示，它对输入进行了归一化处理，使大部分输入落在绿色区域，这里的导数不会太小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/d85e16726d86d630ee9a1447ff3a0fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XCtAytGsbhRQnu-x7Ynr0Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image 3: Sigmoid function with restricted inputs</figcaption></figure></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="6df2" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">如果您有任何问题或建议，请在下面留下您的评论:)</p><p id="54f6" class="pw-post-body-paragraph ls lt iq lu b lv mm jr lx ly mn ju ma lf mo mc md lj mp mf mg ln mq mi mj mk ij bi translated">阅读这些文章了解更多信息:</p><ul class=""><li id="0a9d" class="nb nc iq lu b lv mm ly mn lf nd lj ne ln nf mk ng nh ni nj bi translated"><a class="ae kv" href="https://www.quora.com/What-is-the-vanishing-gradient-problem" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/什么是消失的渐变问题</a></li><li id="5ea2" class="nb nc iq lu b lv nk ly nl lf nm lj nn ln no mk ng nh ni nj bi translated">https://en.wikipedia.org/wiki/Vanishing_gradient_problem<a class="ae kv" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank"/></li><li id="42bc" class="nb nc iq lu b lv nk ly nl lf nm lj nn ln no mk ng nh ni nj bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/intuit-and-implement-batch-normalization-c05480333c5b">https://towards data science . com/intuit-and-implement-batch-normalization-c 05480333 C5 b</a></li></ul></div></div>    
</body>
</html>