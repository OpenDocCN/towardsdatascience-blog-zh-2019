<html>
<head>
<title>Back Propagation, the Easy Way (Part 3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播，简单的方法(第 3 部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-propagation-the-easy-way-part-3-cc1de33e8397?source=collection_archive---------11-----------------------#2019-01-20">https://towardsdatascience.com/back-propagation-the-easy-way-part-3-cc1de33e8397?source=collection_archive---------11-----------------------#2019-01-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d1ca" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何处理矩阵的维数</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c9706ff5b4eb88f707cd2085864bdec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dyil2q-yhYYahheF"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@toddswrittenword?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Todd Brogowski</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="19dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae kv" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="a918" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在系列的第 2 部分<a class="ae kv" rel="noopener" target="_blank" href="/back-propagation-the-easy-way-part-2-bea37046c897">中，我们认为我们的神经网络一次处理一个输入样本。然而，这是没有效率的。这需要太多的计算，并且不能从矩阵计算中使用的大量优化技术中获益。</a></p><p id="d25b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数时候，用于训练神经网络的数据是以充满行的文件格式出现的，其中每行代表一个样本，每列代表一个特征。</p><p id="c27a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面我们来考虑一下网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/047444aff742ca25923cb51b9f07fcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZnUGT63XwmJbeasoKotKA.png"/></div></div></figure><h2 id="e6a7" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">正向输送</h2><p id="b813" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">我们将从关注前两层开始。</p><p id="c9e9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输入是一个包含数据行的文件，其中每一列都包含进入神经网络的一个输入条目的值。<br/>例如，具有值(X11，X21，…，Xm1)的列 X1 进入神经网络的 X1 输入，类似于列 X2。</p><p id="1260" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人们很自然地认为，输入到神经网络中的行数将在输出端收集。也就是说，如果我们有两个样本一次一个地输入网络，我们应该期望在输出端有两个结果(一次一个)。<br/>同样，如果我们将数据作为包含代表两个样本的两行的矩阵输入，我们应该会得到包含两行结果的矩阵。</p><p id="8541" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这也适用于每一层的输出。<br/>让我们考虑一个包含 X1 和 X2 的不同样本的文件。<br/>在第 2 层的输出端，对于输入到输入层的每个样本，我们有值 A1、A2、A3、A4。对于每个样本都是如此，因此对于<strong class="ky ir"> m </strong>个样本，我们将有<strong class="ky ir"> m </strong>个输出，例如(A11，A12，A13，A14)，(A21，A22，A23，A24)，…。(Am1、Am2、Am3、Am4)。所以第 2 层输出的 A 矩阵的维数是(m，4)。z 的导数也是如此。</p><p id="8246" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，权重矩阵的维数不随样本数而改变，它仅取决于输入端的节点数(在本例中为 2)和输出端的节点数(在本例中为 4)，这使得它为(2，4)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/c619b5f11fde366ae6d338780e029489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x56O6547ZnD8bv4NtPCr0A.png"/></div></div></figure><p id="3cdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者，我们可以通过线性代数获得相同的结果。<br/>我们已经知道，在 feed froward 中我们计算 A 和 dZ，例如<br/> <strong class="ky ir"> A = f(X.W) </strong>和<strong class="ky ir"> dZ = f'(X.W) </strong>。<br/>所以要计算 A 和 dZ 的维数，看一下 X 和 W 的维数就够了，它们分别是(m，2)和(2，4)。<br/>结果是(m，2) x (2，4) = (m，4)。<br/>下图描绘了如何填充 A 和 dZ 以形成维数为(m，4)的矩阵。</p><div class="kg kh ki kj gt ab cb"><figure class="ms kk mt mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/2002c802fe7f8dde14d1eb939864534c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4SAMfm40wXWAIdRGL5ea5w.png"/></div></figure><figure class="ms kk my mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/510744c7bbb681f43b90bd830b7d7149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*pS1xUEGbI4RComz4ouqpGw.png"/></div></figure></div><div class="ab cb"><figure class="ms kk mz mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/5bbd4aacae06f1235c770568b83a6656.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*wruNzcyQNyoifAhZMTCatg.png"/></div></figure><figure class="ms kk na mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/c588a5a0668cd67cbbbecdb01e011c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*BEZbeZUsC-ina3KoG-Wjnw.png"/></div></figure></div><h2 id="5740" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">反向传播</h2><p id="5718" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">到目前为止，我们计算了前馈阶段每层输出的维数。我们来看看反向传播。</p><p id="a1eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">提醒一下，反向传播中的公式是:</p><p id="43ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">𝚫ᴸ = (Aᴸ - Y) * dZᴸ</p><p id="bd5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">𝚫ⁱ = (𝚫ⁱ⁺ .ᵀ)* dZⁱ西部</p><p id="af35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(其中*是成员的乘法运算)</p><p id="ebb8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在知道 A 和 dZ 都具有维度(m，n ),其中<strong class="ky ir"> m </strong>是样本的数量，<strong class="ky ir"> n </strong>是层中节点的数量。所以𝚫ᴸ将有相同的维数(m，n)</p><p id="1df1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的例子中，最后一层(L)只有一个节点，所以层 L 的 A 和 dZ 的维数为(m，1)。由此可见，𝚫ᴸ也有维数(m，1)。</p><p id="d42e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了计算内层的增量，让我们以示例中的第 2 层和第 3 层为例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/c6f5374f9463dd2a78f61204154e1bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IbetYwemBgXXBQWOra1Ww.png"/></div></div></figure><p id="ac4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">层 2 和层 3 之间的权重𝞱具有维度(4，3)，因为层 2 的 4 个节点连接到层 3 的 3 个节点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/257eb2619876eae515cd9164753a0404.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDxrR4Y_AA53ksZKY0y4Tw.png"/></div></div></figure><p id="55cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，由于第 2 层的增量是来自第 3 层的增量和权重𝞱的点积，我们最终得到以下配置。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/26f8ef5aadb0c029fb6ec5447d0fcb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8MeVFpSZ0ZhEWVO9CX-Cg.png"/></div></div></figure><p id="4748" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不可能进行点积并在维度为(m，4)的第 2 层获得 delta！<br/>为了解决这个问题，我们用𝞱ᵀ转置了𝞱.</p><p id="a344" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算将是可能的，如下图所示</p><div class="kg kh ki kj gt ab cb"><figure class="ms kk ne mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/a9c483dc03576ca009fdabf75cf7ffbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*-R7gh8NQq7huI0dK3ibxNg.png"/></div></figure><figure class="ms kk nf mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/26a1cae7f0d00b592a88d8adb192b6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*M_A3oAe6_c0oe9sT0mGkWw.png"/></div></figure></div><h2 id="6e81" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">更新权重</h2><p id="b391" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">更新层<strong class="ky ir"> <em class="ng"> i </em> </strong>的权重，包括获取该层的输入，对该层的增量执行点积，然后使用结果来更新权重。</p><p id="3bc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们以第 1 层和第 2 层为例，我们在第 1 层有一个维度为(m，2)的输入，在第 2 层有一个维度为(m，4)的增量。<br/>然而，连接层 1 和层 2 的权重矩阵具有维度<br/> (2，4)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/beeba5b02ca3329cb0b7cc700215e173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XfCC_5vXywAg7ndzBCgQWA.png"/></div></div></figure><p id="ea30" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了能够更新权重矩阵，x 和𝚫之间的点积应该产生(2，3)矩阵。<br/>这是通过得到 x 的转置，即 x 的<strong class="ky ir"> ᵀ </strong>，然后执行点积 x 的<strong class="ky ir"> ᵀ。</strong>我们称之为𝞭.的𝚫</p><div class="kg kh ki kj gt ab cb"><figure class="ms kk ni mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/57da926c67fe1d7a6801b77634011f0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*KpVvYUrRxMRewJq47h2s6g.png"/></div></figure><figure class="ms kk nj mu mv mw mx paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/84f2d44000ee505512cd72aa74ab3786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*iru81nnJ10frFsr3unF45w.png"/></div></figure></div><p id="0467" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以使用梯度下降公式更新权重矩阵</p><p id="f472" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Wⁿ⁺ = Wⁿ - 𝝰 * 𝞭(这里 n 是 w 的版本，而不是图层索引)</p><h2 id="a7d3" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">结论</h2><p id="f98c" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">您从头开始进行反向传播的可能性很小，这要感谢大量已经这样做的库。然而，如果你想自己做这个练习，你应该注意尺寸，否则你会陷入计算错误的迷宫。</p><h2 id="5dd1" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated"><strong class="ak">源代码</strong></h2><p id="4696" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">下面是前一篇文章中开发的 XOR 示例的源代码，它使用样本矩阵，而不是一次一个样本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h2 id="99a1" class="lt lu iq bd lv lw lx dn ly lz ma dp mb lf mc md me lj mf mg mh ln mi mj mk ml bi translated">相关文章</h2><p id="e9d6" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf mo lh li lj mp ll lm ln mq lp lq lr ij bi translated">第一部分:<a class="ae kv" rel="noopener" target="_blank" href="/back-propagation-the-easy-way-part-1-6a8cde653f65">反向传播</a> <br/>的简单详细说明第二部分:<a class="ae kv" rel="noopener" target="_blank" href="/back-propagation-the-easy-way-part-2-bea37046c897">反向传播</a>的实际实现</p></div></div>    
</body>
</html>