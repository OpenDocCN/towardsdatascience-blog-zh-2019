<html>
<head>
<title>LightGBM with the Focal Loss for imbalanced datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据集中焦点损失的 LightGBM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lightgbm-with-the-focal-loss-for-imbalanced-datasets-9836a9ae00ca?source=collection_archive---------3-----------------------#2019-10-06">https://towardsdatascience.com/lightgbm-with-the-focal-loss-for-imbalanced-datasets-9836a9ae00ca?source=collection_archive---------3-----------------------#2019-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b6ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">焦损失</a>(以下简称 FL)是由宗-林逸等人在他们 2018 年的论文<em class="kp">“密集物体探测的焦损失”</em> [1]中引入的。它被设计成解决具有极端不平衡类别的情况，例如前景和背景类别之间的不平衡可以是例如 1:1000 的一阶段对象检测。</p><p id="eb83" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇文章中，我将展示如何为<a class="ae ko" href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf" rel="noopener ugc nofollow" target="_blank">light GBM</a>【2】(以下简称 LGB)编写 FL，并演示如何使用它。配套的 github repo 可以在这里找到<a class="ae ko" href="https://github.com/jrzaurin/LightGBM-with-Focal-Loss" rel="noopener ugc nofollow" target="_blank"/>。不幸的是，我找不到公开的真正具有挑战性的不平衡数据集。python 中的<a class="ae ko" href="https://imbalanced-learn.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank">不平衡学习</a> [3]包附带的数据集相对容易，LGB 不需要任何技术来处理不平衡数据集就能产生良好的结果。另一个选择是 Kaggle 的<a class="ae ko" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">信用卡欺诈检测数据集</a>。然而，这也是一个非常容易的数据集，LGB 产生了非常好的结果“<em class="kp">开箱即用</em>”。</p><p id="f524" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">尽管如此，因为这篇文章的目标是展示 LGB 的 FL 代码以及如何使用它，所以我简单地挑选了两个众所周知的数据集，然后继续。这些是<a class="ae ko" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">成人普查</a>数据集和已经提到的信用卡欺诈检测数据集。</p><p id="c771" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要遵循这个代码中的代码的小数学是这个帖子可以写成如下。考虑二元分类问题，我们可以将 p_t 定义为:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi kq"><img src="../Images/03debc6bd5285ac47af2e20b83c38577.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*Sdliy1z_4DWjI4Y4-EJ2JQ@2x.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Eq 1 (Eq 2 in Tsung-Yi Lin et al., 2018 paper)</figcaption></figure><p id="057a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中，y ∈ { ∓ 1}指定地面实况类，p ∈ [0，1]是标注 y = 1 的类的模型估计概率。那么交叉熵可以写成:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/7ae6e295d2d4e2e646beedf281b45187.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*PVvbJrAAZ04vUuRqFL0sWA@2x.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Eq2. Binary Cross Entropy</figcaption></figure><p id="1a40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据该公式，<strong class="js iu">焦点损失</strong>定义为:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ld"><img src="../Images/54d16a3f7f6eb9b115821efa4de944e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*UETfYaZNurUtPK5l7-z__g@2x.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Eq3 (Eq 5 in their paper)</figcaption></figure><p id="ab9f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看它的行为，如图 1 所示。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi le"><img src="../Images/cd240f310551592b0131ac06724b0745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nz3rZGJxF2df77-BTzKTTA@2x.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Figure 1 (Figure 1 in their paper). The figure shows the Focal Loss plotted against pt. Note that for γ=0 the FL is equal to the CE. We can see that the factor (1 − pt) γo the standard cross entropy criterion. Setting γ &gt; 0 reduces the relative loss for well-classified examples (pt &gt; .5).</figcaption></figure><p id="7e0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如我们在图中看到的，设置γ &gt; 0 减少了分类良好的例子的相对损失(pt &gt; .5)，把更多的注意力放在困难的、错误分类的例子上。引用作者的话:<em class="kp">“当γ = 2 时，与 CE 相比，pt = 0.9 的示例的损耗低 100 倍，pt ≈ 0.968 的示例的损耗低 1000 倍”。</em></p><p id="1f0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是我们现在需要的所有数学知识。</p><h1 id="19c4" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">光焦度损失</h1><p id="1337" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">使用 LGB 时，为了编写您自己的损失函数，您需要损失数学表达式及其梯度和 hessian(即一阶和二阶导数)。<strong class="js iu">光 GBM </strong>的<strong class="js iu">焦损</strong>可以简单编码为:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk"><strong class="ak">Focal Loss implementation to be used with LightGBM</strong></figcaption></figure><p id="81c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果只有一段代码可以从这篇文章中“拯救”出来，那就是上面的代码片段。</p><p id="d88c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你要将 FL 和 LGB 一起使用，你可能需要编写相应的评估函数。在这种情况下，该函数需要返回名称、目标函数的值以及一个布尔值，该值指示值越高越好:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Evaluation Focal Loss function to be used with LightGBM</figcaption></figure><p id="5b95" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">例如，如果不使用 F1 作为目标函数，您更喜欢 F1 分数这样的指标，您可以使用以下代码:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">f1 score with custom loss (Focal Loss in this case)</figcaption></figure><p id="8874" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意第 2 行中的<em class="kp"> sigmoid </em>函数。这是因为当使用您自己的损失函数时，直接来自算法的原始预测不会通过 sigmoid 来表示概率。</p><p id="4f33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要将 FL 与 LGB 一起用于训练，只需:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">How to use the Focal Loss for training with LightGBM</figcaption></figure><p id="0d7b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者通过 F1 分数和交叉验证:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">How to use the Focal Loss for LightGBM with cv and F1 as the evaluation metric</figcaption></figure><h1 id="e73f" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">结果</h1><p id="935d" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">如前所述，我使用了两个数据集，人口普查数据集和信用卡欺诈检测数据集。对于每个数据集，我运行两个实验:1)使用设置为<code class="fe mo mp mq mr b">True</code>的<code class="fe mo mp mq mr b">is_unbalance</code>参数(如果少数类是正类，这相当于使用<code class="fe mo mp mq mr b">scale_pos_weight</code>)和 2)使用 FL。我使用<a class="ae ko" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank"> Hyperopt </a>进行超参数优化，针对 F1 分数进行优化，每个实验运行 100 次迭代，每次迭代都进行交叉验证(3 次，即每个实验 300 次拟合)。我根本没有使用过/欠采样。本文引用的所有代码都可以在<a class="ae ko" href="https://github.com/jrzaurin/LightGBM-with-Focal-Loss" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Table 1. Performance metrics for the experiments run in this post with and without Focal Loss</figcaption></figure><p id="8b12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">结果如表 1 所示。需要强调的是，正如本文前面提到的，<strong class="js iu"> <em class="kp">这两个数据集都不适合这里的</em> </strong>练习。事实上，成人人口普查数据甚至没有不平衡。因此，先验地，人们不会期望在有和没有 FL 的情况下获得的度量之间有大的变化。然而，表中的结果说明了 FL 的潜力。例如，两个数据集的 F1 分数都增加了 2%。此外，在信用卡欺诈检测数据集的情况下，所有的<em class="kp"> F1、精度</em>和<em class="kp">召回</em>提高了 2%或更多。</p><p id="994b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对此，我可以补充一点，我已经在几个真实数据集和相关的改进中使用了 FL，在不平衡率为 2:100 的数据集上，所有性能指标都一致提高了约 5%。我的一些同事告诉我，他们看到了更大的改善。</p><p id="694e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了结束这一部分，让我们来看看聚焦损耗中的α和γ参数:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Table 2. Focal loss α and γ parameters</figcaption></figure><p id="e3a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">较高的γ值将较大地减少分类良好的样本的相对损失，将更多的注意力放在难以分类的样本上。因此，这些结果可以解释如下。成人数据集虽然不是不平衡的，但对于算法来说，它比信用卡欺诈数据集<em class="kp">【更具挑战性】</em>。为了获得最好的 F1 分数，我们需要较大的γ值，将大量的“<em class="kp">焦点</em>放在硬的、错误分类的样本上。</p><p id="04a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"/><code class="fe mo mp mq mr b"><strong class="js iu">is_unbalance</strong></code><strong class="js iu">参数</strong></p><p id="7027" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，当使用 FL 时，我已经将<code class="fe mo mp mq mr b">is_unbalance </code>参数设置为<code class="fe mo mp mq mr b">False. </code>，注意也可以同时使用 FL 和<code class="fe mo mp mq mr b">is_unbalance=True</code>。为了理解使用两者的含义，让我们首先假设树集成方法的目标函数的一般形式:</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/f65ba0d37e7e301a3600a5d48f80c8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Ledoz7Y7CmWDo8boShhFpg@2x.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Eq 4. This is actually Eq 3 in Tianqi Chen &amp; Carlos Guestrin 2016 XGBoost paper [4].</figcaption></figure><p id="b9a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中<em class="kp"> f </em> _t 是树在第<em class="kp"> t </em>次迭代时的函数，<em class="kp"> g </em>和<em class="kp"> h </em>分别是梯度和 Hessian。</p><p id="f6d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们看一下 c++代码的这里的<a class="ae ko" href="https://github.com/microsoft/LightGBM/blob/a3a353d6832b3653f222886f912293998ff8ae69/src/objective/binary_objective.hpp#L95" rel="noopener ugc nofollow" target="_blank">，我们可以看到设置<code class="fe mo mp mq mr b">is_unbalance = True</code>仅仅意味着，当计算损失时，少数类的标签权重将是两个类之间的比率(<em class="kp">count _ majority/count _ minority</em>)。如果我们再看这里的</a><a class="ae ko" href="https://github.com/microsoft/LightGBM/blob/a0d7313b81a85754a0549bbd0354f9e74d832672/python-package/lightgbm/sklearn.py#L80" rel="noopener ugc nofollow" target="_blank"/>，在 sklearns API 中，我们看到在计算目标函数时，这些权重直接乘以梯度和 Hessian。</p><p id="e6b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在假设我们有一个不平衡比率为 1:9 的数据集，我们取两个样本，一个正(少数)样本和一个负(多数)样本，预测概率 p=0.9。现在假设α和γ分别为 1.0 和 2.0。如前所述，在这种情况下，FL 中的因子α(1-p)^γ会将这两个样本的相应损耗值减少 100 倍。换句话说，FL 试图把重点放在更难的、错误分类的样本上。</p><p id="9715" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设，为了计算目标，LGB 使用类似于等式 3 的表达式，并且我们设置<code class="fe mo mp mq mr b">is_unbalanced=True, </code>相应的梯度和焦点损失的 Hessian 将仅对于正(少数)样本乘以因子 9。换句话说，设置<code class="fe mo mp mq mr b">is_unbalanced=True</code>将增加少数类样本对目标的贡献，而不管它们是否被很好地分类。</p><p id="9cd6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，在我看来，焦点损失和<code class="fe mo mp mq mr b">is_unbalance=True</code>对于分类良好且属于少数类的样本是竞争效应。尽管如此，我已经使用 FL 和设置<code class="fe mo mp mq mr b">is_unbalance=True</code>对 2 个数据集进行了实验，结果与表 1 中的几乎相同。</p><h1 id="58ba" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">替代方法</h1><p id="eb0f" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">在这篇文章中，我只是想展示如何为 LGB 编写 FL 并演示如何使用它。然而，让我简单地提一下我在这里没有探讨的一些替代技术。</p><p id="94a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用 LGB 时，您可以直接传递与每个观察相关的权重。这也是一个值得你公司探索的好方法。也许你有一些启发，或使用一些无监督的技术，导致你的结论是，一些样本比其他的更重要。在这种情况下，您可以考虑为每个观察传递一个权重来反映这一先验知识。这可以通过简单地使用<code class="fe mo mp mq mr b">lightgbm.Dataset</code>类中的参数<code class="fe mo mp mq mr b">weight </code>来实现。使用 FL 或使用<code class="fe mo mp mq mr b">weight</code>参数都被称为成本敏感学习技术。</p><p id="33f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一种技术是重新采样。正如我提到的，我没有使用任何欠采样/过采样。如果你想探索重采样技术，你可以在著名的 python 包 unbalanced-learn 中找到相当全面的资料。另一种我觉得特别有趣的更近期、更有前途的欠采样技术在 Ehsan Montahaei 等人 2018[5]中讨论过，如果你想更深入地了解重采样技术的话。</p><p id="0d77" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">仅此而已。有什么想法，在这里评论或者发邮件给我:jrzaurin@gmail.com</p><h1 id="5e78" class="lj lk it bd ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg bi translated">参考资料:</h1><p id="16e9" class="pw-post-body-paragraph jq jr it js b jt mh jv jw jx mi jz ka kb mj kd ke kf mk kh ki kj ml kl km kn im bi translated">[1]宗-林逸，普里亚·戈亚尔，罗斯·吉斯克等人，2018: <em class="kp">密集物体探测的焦损失</em>。<a class="ae ko" href="https://arxiv.org/abs/1708.02002v2" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">arXiv:1708.02002 v2</strong></a></p><p id="40a1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[2]郭林·柯，，托马斯·芬利等，2017: LightGBM: <em class="kp">一种高效的梯度推进决策树</em>。</p><p id="6df9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3]纪尧姆·勒迈特，费尔南多·诺盖拉，克里斯特斯·k·阿里达斯 2017:不平衡学习:一个 Python 工具箱，解决机器学习中不平衡数据集的诅咒。</p><p id="4611" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[4]陈天琦，Carlos Guestrin 2016: XGBoost:一个可扩展的树提升系统。<a class="ae ko" href="https://arxiv.org/abs/1603.02754v3" rel="noopener ugc nofollow" target="_blank"><strong class="js iu">arXiv:1603.02754 v3</strong>T3】</a></p><p id="2eb4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5] Ehsan Montahaei，Mahsa Ghorbani，Mahdieh Soleymani Baghshah，Hamid R. Rabiee 2018:不平衡问题的对抗性分类器。<a class="ae ko" href="https://arxiv.org/abs/1811.08812v1" rel="noopener ugc nofollow" target="_blank">T5】arXiv:1811.08812 v1T7】</a></p></div></div>    
</body>
</html>