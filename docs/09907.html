<html>
<head>
<title>Linear regression: the final frontier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:最后的前沿</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-the-final-frontier-5a4dbda23317?source=collection_archive---------31-----------------------#2019-12-27">https://towardsdatascience.com/linear-regression-the-final-frontier-5a4dbda23317?source=collection_archive---------31-----------------------#2019-12-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6c53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">先进的技术，让你的线性回归游戏更上一层楼</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/6fe28f252160483321da57012b690e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*t1KR4NWZdCt2W2SF"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@jeremythomasphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jeremy Thomas</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="b0b0" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">快速介绍</h1><p id="b91e" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">本文旨在介绍一些更高级的线性回归技术，它们可以极大地改善您的模型结果。如果你还没有掌握线性回归的基础知识，我建议另外两篇文章可以帮助你:<a class="ae le" href="https://medium.com/dataseries/linear-regression-the-basics-4daad1aeb845" rel="noopener">第一篇</a>介绍了一些理解线性回归的基本概念，而<a class="ae le" href="https://medium.com/dataseries/linear-regression-digging-deeper-b82672f168ce" rel="noopener">第二篇</a>解释了如何发现并纠正线性回归中出现的一些常见问题。</p><h1 id="7ac2" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">多项式回归</h1><p id="9140" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">顾名思义，线性回归意味着两个变量之间的线性关系，形式为 y = ax + b。但是，通常情况下，这两个变量之间的关系不是线性的，而是遵循某个其他函数，该函数采用 x 的平方(或某个其他幂)值。在这种情况下，一个简单的解决方案是向解释变量添加一个要检查的所有变换的列表(x 的平方，x 的三次方，等等)。这决定了你的多项式的次数(例如，如果你到 x 的五次方，你有一个五次多项式)。一旦你有了解释变量的完整列表，你就可以像往常一样进行线性回归了。这是一个简单的转换，但它可以产生一些非常好的结果。</p><h1 id="2174" class="lf lg it bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">固定过度配合</h1><h2 id="9a0d" class="mi lg it bd lh mj mk dn ll ml mm dp lp kb mn mo lt kf mp mq lx kj mr ms mb mt bi translated">里脊回归</h2><p id="9cfa" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">岭回归是一种特殊类型的线性回归，它试图通过对一些参数应用权重来减少过度拟合，从而减少实际上可能不重要的参数的影响。</p><p id="5ffc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了在 R 中做到这一点，我们使用函数 glmnet()，并指定参数 alpha = 0，使其成为岭回归。我们还必须指定正则化参数 lambda，它将决定我们的回归对其给出的权重有多“严格”。使用交叉验证可以找到最佳值:</p><pre class="kp kq kr ks gt mu mv mw mx aw my bi"><span id="e5ee" class="mi lg it mv b gy mz na l nb nc"><strong class="mv iu">IN:<br/></strong>library(glmnet)<br/>library(tidyverse)</span><span id="41c5" class="mi lg it mv b gy nd na l nb nc">y = data$price<br/>x = data %&gt;% select(sqft_living, floors, yr_built, yr_built_2) %&gt;% data.matrix()</span><span id="ef56" class="mi lg it mv b gy nd na l nb nc">model6 = glmnet(x, y, alpha = 0)</span><span id="f157" class="mi lg it mv b gy nd na l nb nc">lambdas = 10^seq(10, -10, by = -.2)<br/>cv_fit = cv.glmnet(x, y, alpha = 0, lambda = lambdas)<br/>plot(cv_fit)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/51290adaabc54986f682a4f71221208a.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*ROogQJNx_cyDEYOK2jfOIA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">This plot indicates us the lambda that minimises the error, but we can also fetch it directly.</figcaption></figure><pre class="kp kq kr ks gt mu mv mw mx aw my bi"><span id="7d9b" class="mi lg it mv b gy mz na l nb nc"><strong class="mv iu">IN:</strong><br/>opt_lambda = cv_fit$lambda.min<br/>opt_lambda</span><span id="31cd" class="mi lg it mv b gy nd na l nb nc"><strong class="mv iu">OUT:</strong><br/>6.309573e-07</span><span id="a9d0" class="mi lg it mv b gy nd na l nb nc"><strong class="mv iu">IN:<br/></strong>model6 = cv_fit$glmnet.fit</span><span id="a386" class="mi lg it mv b gy nd na l nb nc">plot(model6, xvar = "lambda")<br/>legend("left", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/e9e444ca386db618c9f3e4fe57950324.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*OAHpt1AmXoZumARCVWroPw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">This plot shows how the coefficients weights evolve for each variable, according to the value of lambda.</figcaption></figure><h2 id="89f0" class="mi lg it bd lh mj mk dn ll ml mm dp lp kb mn mo lt kf mp mq lx kj mr ms mb mt bi translated">套索回归</h2><p id="7c54" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">套索回归的工作方式与岭回归类似，只是它不是对不重要的参数赋予小的权重，而是赋予它们 0 的权重，这意味着它将它们从回归中剔除。</p><p id="15d7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 R 中，我们还将使用 glmnet()函数，但这次将 alpha 设置为 1，以使其成为套索回归:</p><pre class="kp kq kr ks gt mu mv mw mx aw my bi"><span id="2484" class="mi lg it mv b gy mz na l nb nc"><strong class="mv iu">IN:</strong><br/>cv_fit = cv.glmnet(x, y, alpha = 1, lambda = lambdas)<br/>opt_lambda = cv_fit$lambda.min</span><span id="0fb5" class="mi lg it mv b gy nd na l nb nc">model7 = cv_fit$glmnet.fit<br/>plot(model7, xvar = "lambda")<br/>legend("left", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/6602a5e9efb121424af325455c1d08ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*P5l7JbT-_rYqhO4N3yOV8A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">This is basically the same as the previous plot, but notice how the convergence of the weights to zero is more abrupt.</figcaption></figure><h2 id="0332" class="mi lg it bd lh mj mk dn ll ml mm dp lp kb mn mo lt kf mp mq lx kj mr ms mb mt bi translated">弹性网</h2><p id="26b4" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">弹性网络正则化本质上是岭回归和套索回归的两个正则化参数的线性组合。在实践中，这意味着将 alpha 值设置在 0 和 1 之间，这将把一些系数设置为 0，就像在 LASSO 回归中一样，并把一些系数设置为权重，就像在 Ridge 回归中一样。</p><pre class="kp kq kr ks gt mu mv mw mx aw my bi"><span id="deaf" class="mi lg it mv b gy mz na l nb nc"><strong class="mv iu">IN:</strong><br/>library(caret)<br/>set.seed(111)<br/>model8 = train(<br/>  price ~., data = data, method = "glmnet",<br/>  trControl = trainControl("cv", number = 10),<br/>  tuneLength = 10<br/>)</span><span id="ad29" class="mi lg it mv b gy nd na l nb nc">model8$bestTune</span><span id="8d9f" class="mi lg it mv b gy nd na l nb nc"><strong class="mv iu">OUT:</strong><br/>   alpha   lambda<br/>94     1 1468.049</span></pre><h2 id="53df" class="mi lg it bd lh mj mk dn ll ml mm dp lp kb mn mo lt kf mp mq lx kj mr ms mb mt bi translated">逐步回归</h2><p id="bd06" class="pw-post-body-paragraph jq jr it js b jt md jv jw jx me jz ka kb mf kd ke kf mg kh ki kj mh kl km kn im bi translated">在前面的方法中，我们为每个参数分配权重以减少过度拟合。在逐步回归中，我们尝试参数的多种组合，并排除那些不重要的参数。一种方法是从所有变量开始，然后一个一个地去掉不太重要的变量。另一种方法是从一个变量开始，然后一个接一个地添加重要的变量。最后，我们可以将两种方法结合起来。这种选择可以通过在 stepAIC 函数中设置参数“方向”来实现，该参数可以取以下值:“<em class="ng">向后“</em>、“<em class="ng">向前”</em>、“<em class="ng">都”</em>。变量的重要性可以用许多不同的方法来衡量，所以这里我们将研究其中的一种方法，它是由 stepAIC()函数计算的，该函数研究 Akaike 信息标准。</p><pre class="kp kq kr ks gt mu mv mw mx aw my bi"><span id="ea34" class="mi lg it mv b gy mz na l nb nc"><strong class="mv iu">IN:</strong><br/>library(MASS)<br/>model8 = lm(price~., data = data)<br/>model8 = stepAIC(model8, direction = "both", <br/>                      trace = FALSE)<br/>summary(model8)</span><span id="dfac" class="mi lg it mv b gy nd na l nb nc"><strong class="mv iu">OUT:</strong><br/>Call:<br/>lm(formula = price ~ id + bedrooms + bathrooms + sqft_living + <br/>    sqft_lot + floors + waterfront + view + condition + grade + <br/>    sqft_above + yr_built + yr_renovated + zipcode + lat + long + <br/>    sqft_living15 + sqft_lot15 + yr_built_2, data = data)</span><span id="f40a" class="mi lg it mv b gy nd na l nb nc">Residuals:<br/>     Min       1Q   Median       3Q      Max <br/>-1317688   -99197    -9464    76111  4340354</span><span id="2f0a" class="mi lg it mv b gy nd na l nb nc">Coefficients:<br/>                Estimate Std. Error t value Pr(&gt;|t|)    <br/>(Intercept)    9.177e+07  7.674e+06  11.958  &lt; 2e-16 ***<br/>id            -1.485e-06  4.812e-07  -3.086 0.002032 ** <br/>bedrooms      -3.298e+04  1.900e+03 -17.358  &lt; 2e-16 ***<br/>bathrooms      3.554e+04  3.276e+03  10.846  &lt; 2e-16 ***<br/>sqft_living    1.510e+02  4.371e+00  34.537  &lt; 2e-16 ***<br/>sqft_lot       1.244e-01  4.783e-02   2.601 0.009307 ** <br/>floors        -1.376e+04  3.974e+03  -3.463 0.000536 ***<br/>waterfront     5.860e+05  1.730e+04  33.868  &lt; 2e-16 ***<br/>view           5.474e+04  2.138e+03  25.599  &lt; 2e-16 ***<br/>condition      3.053e+04  2.371e+03  12.876  &lt; 2e-16 ***<br/>grade          9.579e+04  2.146e+03  44.640  &lt; 2e-16 ***<br/>sqft_above     3.033e+01  4.346e+00   6.979 3.05e-12 ***<br/>yr_built      -8.830e+04  7.167e+03 -12.320  &lt; 2e-16 ***<br/>yr_renovated   2.263e+01  3.652e+00   6.196 5.89e-10 ***<br/>zipcode       -6.053e+02  3.293e+01 -18.382  &lt; 2e-16 ***<br/>lat            6.099e+05  1.072e+04  56.908  &lt; 2e-16 ***<br/>long          -2.203e+05  1.312e+04 -16.785  &lt; 2e-16 ***<br/>sqft_living15  2.175e+01  3.436e+00   6.330 2.51e-10 ***<br/>sqft_lot15    -3.518e-01  7.330e-02  -4.800 1.60e-06 ***<br/>yr_built_2     2.189e+01  1.831e+00  11.955  &lt; 2e-16 ***<br/>---<br/>Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1</span><span id="403d" class="mi lg it mv b gy nd na l nb nc">Residual standard error: 200600 on 21593 degrees of freedom<br/>Multiple R-squared:  0.7018, Adjusted R-squared:  0.7016 <br/>F-statistic:  2675 on 19 and 21593 DF,  p-value: &lt; 2.2e-16</span></pre><p id="2655" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们只剩下一些初始变量，它们都是有意义的。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="c02e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以在这里访问完整的脚本<a class="ae le" href="https://github.com/arthurmello/statistics/tree/master/1.%20Linear%20regression" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>