<html>
<head>
<title>Adding sequential IDs to a Spark Dataframe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">向 Spark 数据帧添加顺序 id</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6?source=collection_archive---------1-----------------------#2019-10-04">https://towardsdatascience.com/adding-sequential-ids-to-a-spark-dataframe-fa0df5566ff6?source=collection_archive---------1-----------------------#2019-10-04</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="7ace" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">怎么做，这是个好主意吗？</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/2f113fb8086e828161b7a56787c4e0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X8A8V7gkYlcNNzD4"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="la"><p id="b1e6" class="lb lc iu bd ld le lf lg lh li lj lk dk translated"><strong class="ak">TL；博士</strong></p><p id="4afb" class="lb lc iu bd ld le ll lm ln lo lp lk dk translated">向 Spark 数据帧添加连续的惟一 id 并不是很直接，尤其是考虑到它的分布式本质。您可以使用 zipWithIndex()或 row_number()来实现这一点(取决于数据的数量和种类)，但是在每种情况下都有一个关于性能的问题。</p></blockquote><h2 id="259d" class="lq lr iu bd ls lt lu dn lv lw lx dp ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated">这背后的想法</h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj mm"><img src="../Images/f3cba7479ce93b8db09e63dbca4ec5cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAWPTt-_eh0Txr35RjaNLg.jpeg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Typical usages for ids — besides the obvious: for identity purposes</figcaption></figure><p id="4bd1" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">来自传统的关系数据库，如<a class="ae kz" href="https://www.mysql.com" rel="noopener ugc nofollow" target="_blank"> MySQL </a>，以及非分布式数据框架，如<a class="ae kz" href="https://pandas.pydata.org" rel="noopener ugc nofollow" target="_blank"> Pandas </a>，人们可能习惯于使用 id(通常自动递增)进行标识，当然，也可以使用它们作为参考来对数据进行排序和约束。例如，按 id(通常是一个索引字段)降序排列数据，将首先给出最近的行，等等。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nf"><img src="../Images/72a2bd7fa0570156725ca2eb6ec6595f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXGB03Uf0IJKcew42e_AEw.jpeg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">A representation of a Spark Dataframe — what the user sees and what it is like physically</figcaption></figure><p id="74dd" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">根据需要，我们可能会发现在 spark 数据帧中拥有一个(独特的)类似 auto-increment-ids '的行为对我们有好处。当数据在一个表或数据帧中时(在一台机器上)，添加 id 是非常直接的。但是，当您将数据分散到可能驻留在不同机器(如 Spark)上的分区中时，会发生什么呢？</p><p id="7704" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">(更多关于分区<a class="ae kz" href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html" rel="noopener ugc nofollow" target="_blank">的信息，请点击</a>)</p><p id="be0b" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">在这篇文章中，我们将探索显而易见和不那么显而易见的选项，它们的作用，以及使用它们背后的陷阱。</p><p id="d0c4" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated"><strong class="mp iv">注释</strong></p><ul class=""><li id="647f" class="ng nh iu mp b mq mr mt mu lz ni md nj mh nk lk nl nm nn no bi translated">请注意，本文假设您对 Spark 有一定的了解，尤其是对<a class="ae kz" href="http://spark.apache.org/docs/latest/quick-start.html" rel="noopener ugc nofollow" target="_blank"> PySpark </a>有一定的了解。如果没有，这里有一个<a class="ae kz" rel="noopener" target="_blank" href="/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9#b88f-81d3a1ffe447">简短介绍</a>以及它是什么，我已经在<em class="np">有用链接和注释</em>部分放了几个有用的资源。我很乐意回答任何我能回答的问题:)。</li><li id="a563" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">再次练习<strong class="mp iv">速写</strong>，是的，整篇文章都有<em class="np">可怕的速写</em>，试图直观地解释我所理解的事物<em class="np"/>。希望它们更有帮助，而不是令人困惑:)。</li></ul></div><div class="ab cl nv nw hy nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="in io ip iq ir"><h2 id="d2d4" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated">RDD 之路—ziptwithindex()</h2><p id="b1d1" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated">一种选择是退回到<a class="ae kz" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> RDDs </a></p><blockquote class="om on oo"><p id="9942" class="mn mo np mp b mq mr jv ms mt mu jy mv op mw mx my oq mz na nb or nc nd ne lk in bi translated"><em class="iu">弹性分布式数据集</em> (RDD)，它是跨集群节点划分的、可以并行操作的元素的集合</p></blockquote><p id="b275" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">并使用<code class="fe os ot ou ov b">df.rdd.zipWithIndex()</code>:</p><blockquote class="om on oo"><p id="aaba" class="mn mo np mp b mq mr jv ms mt mu jy mv op mw mx my oq mz na nb or nc nd ne lk in bi translated">排序首先基于分区索引，然后是每个分区内项目的<br/>排序。所以第一个分区中的第一个项目得到索引 0，最后一个分区中的最后一个项目得到最大的索引。</p><p id="4d87" class="mn mo np mp b mq mr jv ms mt mu jy mv op mw mx my oq mz na nb or nc nd ne lk in bi translated">当这个 RDD 包含<br/>多个分区时，这个方法需要触发一个火花作业。</p></blockquote><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ow ox l"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">An example using <code class="fe os ot ou ov b">zipWithIndex</code></figcaption></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj oy"><img src="../Images/a97895bc369870f453e795a1d406e05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TdfafEJB01ubCSkZR5NFVA.jpeg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">The process of using zipWithIndex()</figcaption></figure><p id="446c" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">这里有四点:</p><ul class=""><li id="9f12" class="ng nh iu mp b mq mr mt mu lz ni md nj mh nk lk nl nm nn no bi translated">索引将从 0 开始<strong class="mp iv">，并且<strong class="mp iv">排序</strong>由分区</strong>完成<strong class="mp iv"/></li><li id="cbf9" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">您需要将所有数据保存在数据框中— <strong class="mp iv">添加*不会添加自动递增 id </strong></li><li id="c4b0" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">退回到 rdds，然后退回到 dataframe <a class="ae kz" href="https://stackoverflow.com/questions/37088484/whats-the-performance-impact-of-converting-between-dataframe-rdd-and-back" rel="noopener ugc nofollow" target="_blank"> <strong class="mp iv">可能会相当昂贵</strong> </a> <strong class="mp iv">。</strong></li><li id="1e70" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">带有 id 的数据帧的更新版本将要求您做一些额外的工作以将数据帧恢复到原始形式。这也增加了<strong class="mp iv">性能损失</strong>。</li></ul><p id="3374" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">你不能真正地更新或添加一个数据帧，因为它们是不可变的，但是你可以把一个数据帧和另一个数据帧连接起来，最终得到一个比原始数据帧有更多行的数据帧。</p></div><div class="ab cl nv nw hy nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="in io ip iq ir"><h2 id="3aac" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated">数据框架方式</h2><p id="6114" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated"><strong class="mp iv">如果您的数据可排序</strong></p><p id="8f23" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">如果您可以按其中一列对数据进行排序，比如我们示例中的<code class="fe os ot ou ov b">column1</code>，那么您可以使用<code class="fe os ot ou ov b"><a class="ae kz" href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.row_number" rel="noopener ugc nofollow" target="_blank">row_number</a>()</code>函数来提供行号:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ow ox l"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Resuming from the previous example — using row_number over sortable data to provide indexes</figcaption></figure><blockquote class="om on oo"><p id="5cc1" class="mn mo np mp b mq mr jv ms mt mu jy mv op mw mx my oq mz na nb or nc nd ne lk in bi translated"><code class="fe os ot ou ov b">row_number()</code>是一个窗口函数，这意味着它在预定义的窗口/数据组上运行。</p></blockquote><p id="a2b3" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">这里的要点是:</p><ul class=""><li id="44e9" class="ng nh iu mp b mq mr mt mu lz ni md nj mh nk lk nl nm nn no bi translated">您的数据必须是<strong class="mp iv">可排序的</strong></li><li id="ecfe" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">你需要使用一个非常大的窗口(和你的数据一样大)</li><li id="7a03" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">您的索引将从 1 开始<strong class="mp iv"/></li><li id="dc3e" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">您需要将所有数据保存在数据框中— <strong class="mp iv">更新不会添加自动递增 id </strong></li><li id="1e67" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated">没有额外的工作来重新格式化你的数据帧</li><li id="55a5" class="ng nh iu mp b mq nq mt nr lz ns md nt mh nu lk nl nm nn no bi translated"><strong class="mp iv">但是</strong>你可能会以一个<strong class="mp iv"> OOM 异常</strong>结束，我稍后会解释。</li></ul><p id="1ccf" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated"><strong class="mp iv">如果您的数据不可排序，或者您不想改变数据的当前顺序</strong></p><p id="27d6" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">另一种选择是将<code class="fe os ot ou ov b">row_number()</code>与<code class="fe os ot ou ov b">monotonically_increasing_id()</code>结合，根据<a class="ae kz" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id" rel="noopener ugc nofollow" target="_blank">文档</a>创建:</p><blockquote class="om on oo"><p id="2917" class="mn mo np mp b mq mr jv ms mt mu jy mv op mw mx my oq mz na nb or nc nd ne lk in bi translated">&gt;生成单调递增的 64 位整数的列。</p><p id="a8ab" class="mn mo np mp b mq mr jv ms mt mu jy mv op mw mx my oq mz na nb or nc nd ne lk in bi translated">&gt;生成的 ID 保证是<strong class="mp iv"/><strong class="mp iv">单调递增且唯一的，而不是连续的</strong>。当前的实现将分区 ID 放在高 31 位，将每个分区内的记录号放在低 33 位。假设数据帧有不到 10 亿个分区，每个分区有不到 80 亿条记录。</p></blockquote><p id="8782" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated"><strong class="mp iv"/><strong class="mp iv">单调递增且唯一，但不连续的</strong>是这里的关键。这意味着你可以按它们排序，但你不能相信它们是连续的。在某些情况下，你只需要排序，<code class="fe os ot ou ov b">monotonically_increasing_id()</code>就非常方便，你根本不需要<code class="fe os ot ou ov b">row_number()</code>。但是在这种情况下，假设我们绝对需要后续 id。</p><p id="9972" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">再次，从我们在代码中留下的地方继续:</p><figure class="kk kl km kn gu ko"><div class="bz fq l di"><div class="ow ox l"/></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Resuming from the previous example — using row_number over initialy non-sortable data to provide indexes</figcaption></figure><p id="1cbd" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">当然有不同的方式(语义上)去做这件事。例如，您可以使用一个临时视图(除了可以使用 pyspark SQL 语法之外，它没有明显的优势):</p><pre class="kk kl km kn gu oz ov pa pb aw pc bi"><span id="8b15" class="lq lr iu ov b gz pd pe l pf pg">&gt;&gt;&gt; df_final.createOrReplaceTempView(‘df_final’)<br/>&gt;&gt;&gt; spark.sql(‘select row_number() over (order by “monotonically_increasing_id”) as row_num, * from df_final’)</span></pre><p id="c334" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">这里的要点是:</p><ul class=""><li id="3687" class="ng nh iu mp b mq mr mt mu lz ni md nj mh nk lk nl nm nn no bi translated">同上，但还有一点要注意，实际上<strong class="mp iv">排序</strong>是由分区完成的<strong class="mp iv"/></li></ul><h2 id="79c5" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated">这整个努力的最大收获是</h2><p id="d283" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated">为了使用<code class="fe os ot ou ov b">row_number()</code>，我们需要将数据移动到一个分区中。两种情况下的<code class="fe os ot ou ov b">Window</code>(可排序和不可排序的数据)基本上都包含了我们当前拥有的所有行，因此<code class="fe os ot ou ov b">row_number()</code>函数可以遍历它们并增加行号。这可能会导致性能和内存问题——我们很容易崩溃，这取决于我们有多少数据和多少内存。所以，我的建议是，你真的要问问自己，你的数据是否需要一种类似自动递增/索引的行为，或者你是否可以用另一种方式来避免这种行为，因为这将是昂贵的。尤其是当您每次处理任意数量的数据时，因此无法仔细考虑内存量(例如，在组或窗口中处理流数据)。</p><p id="abdb" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">每当您使用<code class="fe os ot ou ov b">Window</code>时，Spark 都会给出以下警告，而没有提供对数据进行分区的方法:</p><pre class="kk kl km kn gu oz ov pa pb aw pc bi"><span id="342a" class="lq lr iu ov b gz pd pe l pf pg">WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.</span></pre><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj ph"><img src="../Images/2fef92014ef9aaf6e1171f7d9226e77d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LgzG1UwkwaFNPGeDRt45RQ.jpeg"/></div></div><figcaption class="kv kw gk gi gj kx ky bd b be z dk">Using row_number() over Window and the OOM danger</figcaption></figure></div><div class="ab cl nv nw hy nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="in io ip iq ir"><h1 id="331d" class="pi lr iu bd ls pj pk pl lv pm pn po ly ka pp kb mc kd pq ke mg kg pr kh mk ps bi translated">结论:这到底是不是一个好主意？</h1><p id="0ea9" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated">嗯，<em class="np">大概不是</em>。根据我的经验，如果你发现自己需要这种功能，那么你应该<em class="np">好好审视一下你的需求和你的转变过程</em>，如果可能的话，找出解决方法。即使你使用了<code class="fe os ot ou ov b">zipWithIndex()</code>,你的应用程序的性能可能仍然会受到影响——但是对我来说这似乎是一个更安全的选择。</p><p id="0d7b" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">但如果你无法避免，至少要意识到它背后的机制、风险，并做好相应的计划。</p><p id="f45a" class="pw-post-body-paragraph mn mo iu mp b mq mr jv ms mt mu jy mv lz mw mx my md mz na nb mh nc nd ne lk in bi translated">我希望这有所帮助。任何想法，问题，更正和建议都非常欢迎:)</p></div><div class="ab cl nv nw hy nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="in io ip iq ir"><h1 id="c972" class="pi lr iu bd ls pj pk pl lv pm pn po ly ka pp kb mc kd pq ke mg kg pr kh mk ps bi translated">有用的链接和注释</h1><div class="pt pu gq gs pv pw"><a rel="noopener follow" target="_blank" href="/explaining-technical-stuff-in-a-non-techincal-way-apache-spark-274d6c9f70e9"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">用非技术性的方式解释技术性的东西——Apache Spark</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">什么是 Spark 和 PySpark，我可以用它做什么？</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">towardsdatascience.com</p></div></div><div class="qf l"><div class="qg l qh qi qj qf qk kt pw"/></div></div></a></div><h2 id="fd08" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated"><strong class="ak">从 0 开始调整指标</strong></h2><p id="c889" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated">使用<code class="fe os ot ou ov b">row_number()</code>时的索引从 1 开始。要让它们从 0 开始，我们可以简单地从<code class="fe os ot ou ov b">row_num</code>列中减去 1:</p><pre class="kk kl km kn gu oz ov pa pb aw pc bi"><span id="d1ae" class="lq lr iu ov b gz pd pe l pf pg">df_final = df_final.withColumn(‘row_num’, F.col(‘row_num’)-1)</span></pre><h2 id="c12c" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated"><strong class="ak">关于 rdd 和数据集</strong></h2><div class="pt pu gq gs pv pw"><a href="https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html" rel="noopener  ugc nofollow" target="_blank"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">三个 Apache Spark APIs 的故事:RDDs 与数据帧和数据集</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">总之，选择何时使用 RDD 或数据框架和/或数据集似乎是显而易见的。前者为您提供…</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">databricks.com</p></div></div><div class="qf l"><div class="ql l qh qi qj qf qk kt pw"/></div></div></a></div><div class="pt pu gq gs pv pw"><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html" rel="noopener  ugc nofollow" target="_blank"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">RDD 节目指南</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">默认情况下，Spark 2.4.4 的构建和发布是为了与 Scala 2.12 协同工作。(Spark 可以构建为与其他……</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">spark.apache.org</p></div></div><div class="qf l"><div class="qm l qh qi qj qf qk kt pw"/></div></div></a></div><h2 id="913d" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated"><strong class="ak">关于 createOrReplaceTempView </strong></h2><p id="5800" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated">这将为您的数据创建一个延迟评估的“视图”(如果该视图名称已经存在，则替换它),这意味着如果您不缓存/持久化它，每次您访问该视图时，任何计算都将再次运行。通常，您可以在 Spark SQL 中使用 hive 表。</p><div class="pt pu gq gs pv pw"><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.createOrReplaceTempView" rel="noopener  ugc nofollow" target="_blank"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">pyspark.sql 模块- PySpark 2.4.4 文档</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">schema-py spark . SQL . types . datatype 或数据类型字符串或列名列表，默认值为。数据类型字符串…</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">spark.apache.org</p></div></div></div></a></div><h2 id="a941" class="lq lr iu bd ls lt oc dn lv lw od dp ly lz oe mb mc md of mf mg mh og mj mk ml bi translated">行号和窗口</h2><div class="pt pu gq gs pv pw"><a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.row_number" rel="noopener  ugc nofollow" target="_blank"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">pyspark.sql 模块- PySpark 2.4.4 文档</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">schema-py spark . SQL . types . datatype 或数据类型字符串或列名列表，默认值为。数据类型字符串…</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">spark.apache.org</p></div></div></div></a></div><div class="pt pu gq gs pv pw"><a href="https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html" rel="noopener  ugc nofollow" target="_blank"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">Spark SQL 中的窗口函数介绍</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">在这篇博文中，我们介绍了 Apache Spark 1.4 中添加的新窗口函数特性。窗口功能…</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">databricks.com</p></div></div><div class="qf l"><div class="qn l qh qi qj qf qk kt pw"/></div></div></a></div><h1 id="9135" class="pi lr iu bd ls pj qo pl lv pm qp po ly ka qq kb mc kd qr ke mg kg qs kh mk ps bi translated">接下来去哪里？</h1><p id="df50" class="pw-post-body-paragraph mn mo iu mp b mq oh jv ms mt oi jy mv lz oj mx my md ok na nb mh ol nd ne lk in bi translated">理解你的机器学习模型的预测:</p><div class="pt pu gq gs pv pw"><a href="https://medium.com/mlearning-ai/machine-learning-interpretability-shapley-values-with-pyspark-16ffd87227e3" rel="noopener follow" target="_blank"><div class="px ab fp"><div class="py ab pz cl cj qa"><h2 class="bd iv gz z fq qb fs ft qc fv fx it bi translated">机器学习的可解释性——带有 PySpark 的 Shapley 值</h2><div class="qd l"><h3 class="bd b gz z fq qb fs ft qc fv fx dk translated">解读隔离森林的预测——不仅仅是</h3></div><div class="qe l"><p class="bd b dl z fq qb fs ft qc fv fx dk translated">medium.com</p></div></div><div class="qf l"><div class="qt l qh qi qj qf qk kt pw"/></div></div></a></div></div></div>    
</body>
</html>