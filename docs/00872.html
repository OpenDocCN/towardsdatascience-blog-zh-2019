<html>
<head>
<title>Dropout on convolutional layers is weird</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积层上的丢失很奇怪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=collection_archive---------3-----------------------#2019-02-10">https://towardsdatascience.com/dropout-on-convolutional-layers-is-weird-5c6ab14f19b2?source=collection_archive---------3-----------------------#2019-02-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1cff" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么卷积层上的丢失与全连接层上的丢失有根本的不同。</h2></div><p id="b1ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank"> Dropout </a>常用于正则化深度神经网络；然而，在全连接层上应用丢弃和在卷积层上应用丢弃是根本不同的操作。虽然在深度学习社区中众所周知，当应用于卷积层时，辍学具有<a class="ae lb" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Tompson_Efficient_Object_Localization_2015_CVPR_paper.pdf" rel="noopener ugc nofollow" target="_blank">有限的好处</a> <a class="ae lb" href="https://papers.nips.cc/paper/8271-dropblock-a-regularization-method-for-convolutional-networks.pdf" rel="noopener ugc nofollow" target="_blank">，但我想展示一个简单的数学示例来说明这两者为什么不同。为此，我将定义 dropout 如何在全连接层上操作，定义 dropout 如何在卷积层上操作，并对比这两种操作。</a></p><h2 id="a240" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">全连接层上的脱落</h2><p id="625b" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">一个<em class="ma"> n </em>层全连接神经网络(忽略偏差)可以定义为:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/64b4ea95c11e19eeb9dbe14a86f8c4f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KWo4VIHpTFSbYiLhN6gq4g.png"/></div></div></figure><p id="c5c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="ma"> ϕᵢ </em>为非线性(如 ReLU)，对于<em class="ma"> i </em> ∈{1，…，<em class="ma"> n </em> }为权重矩阵，<strong class="kh ir"> <em class="ma"> x </em> </strong>为输入。</p><p id="a995" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑一个单隐层全连接神经网络<em class="ma">f(</em><strong class="kh ir"><em class="ma">x</em></strong><em class="ma">)</em>:ℝ⁹→没有非线性和偏差的ℝ⁹。我们可以将网络表达为:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mn"><img src="../Images/a9605331fd7c979007ab7a75a0e57330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uP3R_Xr5ejJaDX0BvwgZ9w.png"/></div></div></figure><p id="1005" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是我们的单隐层神经网络的示意图。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mo"><img src="../Images/cd3090039255ae231cace5624e304cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*klRzkBEBiboRWgaLswoFFA.png"/></div></div></figure><p id="aafa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们把辍学加入到这个网络中。设<strong class="kh ir"> <em class="ma"> r </em> </strong> ∈ {0,1}⁹为独立同分布(iid)伯努利随机变量的向量。在我们之前定义的神经网络中，Dropout 可以表示为:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mp"><img src="../Images/aab4cba7dd9b8ef05bd1f4df1dc7966e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBYcjoLalMGc3uzu1j-RVA.png"/></div></div></figure><p id="c48f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="ma">v</em>ᵀ<strong class="kh ir"><em class="ma">x</em></strong>已经折叠到<strong class="kh ir"> <em class="ma"> h </em> </strong>为空格。</p><p id="d7b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要了解这如何等价于辍学，考虑具体的情况，其中<strong class="kh ir"> <em class="ma"> r </em> </strong> = (1，0，0，…，0)ᵀ.最终的网络是:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mo"><img src="../Images/db3900e0fd7b16591572aa8e6219ad04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rDLef9xQk6C8kcaWCv7mjQ.png"/></div></div></figure><p id="11ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是最初的<a class="ae lb" href="http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf" rel="noopener ugc nofollow" target="_blank">辍学论文</a>中讨论的内容。</p><p id="a888" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到，全连接神经网络中的丢失等同于从与全连接层相关联的权重矩阵中清除一列。该操作对应于“丢弃”神经网络中的一个神经元。以这种方式丢弃神经元是合理的，因为定性地说，它促进了权重矩阵中的冗余，即子网络可以稳健地执行所需的操作。</p><h2 id="0b20" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">卷积神经网络中的丢失</h2><p id="86d6" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">一个<em class="ma"> n </em>层卷积神经网络(忽略偏差)可以定义为:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mq"><img src="../Images/50ddd5737c3186fcaf332747bd42c5b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TsWFbiNBGpMv_3jUr9lVPQ.png"/></div></div></figure><p id="ae51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中∫为卷积算子，<em class="ma"> ϕᵢ </em>为非线性，<em class="ma"> Kᵢ </em>为<em class="ma"> i </em> ∈{1，…，<em class="ma"> n </em> }为卷积核，<strong class="kh ir"> <em class="ma"> x </em> </strong>为输入。</p><p id="877b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了弄清我们的方向，让我们先来研究卷积运算。我们可以将离散卷积重写为矩阵乘法。让我们考虑一下将<strong class="kh ir"> <em class="ma"> x </em> </strong> ∈ ℝ ˣ与<em class="ma"> K </em> ∈ ℝ ˣ进行卷积。那么<em class="ma">K</em>∵<em class="ma"/><strong class="kh ir"><em class="ma">x</em></strong>定义为:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mr"><img src="../Images/d19acaea93d7d6a3efd19b9bde8f6570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Aqh-715TbRyj_yCjP96bw.png"/></div></div></figure><p id="70a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这在整形操作之前相当于:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ms"><img src="../Images/398ac42385717cef886bac356c7b8363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cU4k6KHLoo5XQ8tJGP4MTw.png"/></div></div></figure><p id="7b02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们已经确定卷积可以作为矩阵乘法来应用，那么让我们以与我们表述全连接网络大致相同的方式来表述卷积网络。</p><p id="9fd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设<em class="ma">g(</em><strong class="kh ir"><em class="ma">x</em></strong><em class="ma">):</em>ℝˣ→ℝˣ是一个没有非线性和偏差的全卷积神经网络。本着我们之前网络的精神，我们将<em class="ma"> g </em>定义为:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/83f3c90965e159187a1c54d146de7cb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*4zboqQoCJvIPYRywDJRhVA.png"/></div></figure><p id="256e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="ma"> u，v </em> ∈ ℝ ˣ是卷积核，<strong class="kh ir"> <em class="ma"> x </em> </strong> ∈ ℝ ˣ是图像。</p><p id="ddd9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简单起见，也因为不影响我们的分析，让<strong class="kh ir"><em class="ma">【h =</em></strong><em class="ma">v</em><strong class="kh ir"><em class="ma"/></strong>∵<strong class="kh ir"><em class="ma">x</em></strong>。注意，为了使变换<em class="ma"> g </em>有效，我们需要在每个卷积层使用零填充。这可以合并到卷积核的矩阵形式中，因此我们的网络可以写成:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mu"><img src="../Images/282ef46ea2e7a12bdf2ce29cabd8b787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YnJ1K_fYmgZnqyMexH_o9Q.png"/></div></div></figure><p id="62e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其证明留给读者作为练习。</p><p id="a398" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像之前在我们的全连接网络中一样，让我们在卷积网络中加入 dropout。</p><p id="2e2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">设<em class="ma"> U </em>定义为上式中的矩阵(即补零的扩展卷积核)设<strong class="kh ir"> <em class="ma"> r </em> </strong> ∈ {0,1}⁹ iid 伯努利随机变量，如前。然后我们有:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mv"><img src="../Images/45e0a2533626aa5c57de753298a37210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwaBhNTMeNfXIjf_2S1XJg.png"/></div></div></figure><p id="70ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了说明这种压差应用并不等同于全连接情况，只需关注前面所示矩阵左上角的 3 × 3 模块即可:</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/80dde0087eac833e3e807e496fcb71ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*t11R-FF0fPVQ_kF6oHbijg.png"/></div></figure><p id="7440" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，权重<em class="ma"> u </em> ₄ <em class="ma">，u </em> ₅ <em class="ma">，u </em> ₆ <em class="ma"> </em>在这个剪切图的三列中至少出现了两次。因此，如果<em class="ma"> r </em> ₁ = 0 和<em class="ma"> r </em> ₂=1 或<em class="ma"> r </em> ₁ = 1 和<em class="ma"> r </em> ₂=0，我们仍将更新权重<em class="ma"> u </em> ₅和<em class="ma"> u </em> ₄(不考虑<em class="ma"> r </em> ₃的值！).</p><h2 id="15f9" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">结果</h2><p id="b226" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">在我们对全连接网络中的丢失的分析中，我们表明丢失操作可以理解为在神经网络中将权重矩阵的列清零。这个操作相当于不训练或“放弃”一个神经元。</p><p id="c84d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的分析中，我们已经表明卷积层上的丢失不会产生相同的效果。这由以下事实来证明:将对应于卷积核的权重矩阵的列置零仍然允许训练该列中的权重。</p><p id="b0e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于 dropout 通常用于在训练期间不随机训练神经元子集，而卷积层上的 dropout 不进行这种操作，因此该名称具有误导性，并且——因为效果不明显可解释——很奇怪。</p><p id="74a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我并不是说卷积层的丢包没有用。在<a class="ae lb" href="https://arxiv.org/pdf/1511.02680.pdf" rel="noopener ugc nofollow" target="_blank">这些</a> <a class="ae lb" href="https://arxiv.org/abs/1506.02158" rel="noopener ugc nofollow" target="_blank">论文</a>中，作者在卷积层上使用 dropout 时得到了更好的结果。然而，卷积层上的丢失效应似乎相当于将伯努利噪声乘以网络的特征图。我的观点是，在没有更多潜在理论的情况下，选择这种方式将噪声注入系统似乎很奇怪；在全连接层的情况下，哪个<em class="ma">是</em>的理论。</p><p id="fbd8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，如果您尝试在卷积层后添加 dropout 并得到不好的结果，不要沮丧！似乎没有很好的理由让<em class="ma">提供好的结果。</em></p><h2 id="accd" class="lc ld iq bd le lf lg dn lh li lj dp lk ko ll lm ln ks lo lp lq kw lr ls lt lu bi translated">补充说明</h2><p id="c3da" class="pw-post-body-paragraph kf kg iq kh b ki lv jr kk kl lw ju kn ko lx kq kr ks ly ku kv kw lz ky kz la ij bi translated">在全连接单隐层神经网络<a class="ae lb" href="http://proceedings.mlr.press/v80/mianjy18b/mianjy18b.pdf" rel="noopener ugc nofollow" target="_blank">上应用的丢失已经被示出</a>对应于可解释的正则化器，一个均衡权重矩阵的正则化器。然而，这些结果的推导大量利用了矩阵的每个元素的权重彼此独立的事实；这个事实在卷积层中不成立。我没有立即看到如何调和这一事实与卷积层，所以我不明白如何辍学可以提供类似的，可解释的正则化。</p></div></div>    
</body>
</html>