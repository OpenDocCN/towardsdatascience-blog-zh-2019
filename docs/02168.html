<html>
<head>
<title>Sequence Embedding for Clustering and Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于聚类和分类的序列嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sequence-embedding-for-clustering-and-classification-f816a66373fb?source=collection_archive---------6-----------------------#2019-04-10">https://towardsdatascience.com/sequence-embedding-for-clustering-and-classification-f816a66373fb?source=collection_archive---------6-----------------------#2019-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/d3ccb1736023ea3d4c2e46810b9fae4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Er4u8SUeJxzRcTOcq1Lmlw.png"/></div></div></figure><p id="6552" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们将学习一种方法来获得字符串序列的矢量嵌入。这些嵌入可用于聚类和分类。</p><p id="2f5b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">序列建模一直是一个挑战。这是因为序列数据固有的非结构化。就像自然语言处理(NLP)中的文本一样，序列是任意的字符串。对于计算机来说，这些字符串没有任何意义。因此，建立数据挖掘模型非常困难。</p><p id="7159" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于文本，我们已经提出了嵌入，如 word2vec，它将一个单词转换为一个 n 维向量。本质上，把它放在欧几里得空间。在这篇文章中，我们将学习对序列做同样的事情。</p><p id="3397" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们将介绍一种为序列创建嵌入的方法，这种方法将序列带入欧几里得空间。通过这些嵌入，我们可以在序列数据集上执行传统的机器学习和深度学习，例如 kmeans、PCA 和多层感知器。我们提供并处理两个数据集——蛋白质序列和博客。</p><p id="e96b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">序列数据集在我们身边随处可见。例如，科技行业的点击流、音乐收听历史和博客。在生物信息学中，我们有大型的蛋白质序列数据库。蛋白质序列由 20 种氨基酸的某种组合组成。典型的蛋白质序列如下所示，其中每个字母对应一种氨基酸。</p><figure class="kx ky kz la gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi kw"><img src="../Images/d9465e037b759fe380966ec27542045e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEyCRxXoN4dkNqSUyf4iRg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Fig. 1. Example of a protein sequence.</figcaption></figure><p id="33b5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一个蛋白质序列不一定包含所有的 20 个氨基酸，而是它的一个子集。为了清楚起见，我们将定义一些在这篇文章中使用的关键词。</p><blockquote class="lf lg lh"><p id="c09f" class="jy jz li ka b kb kc kd ke kf kg kh ki lj kk kl km lk ko kp kq ll ks kt ku kv ij bi translated">字母表<em class="iq">:组成一个序列的离散元素。例如氨基酸。</em></p><p id="3780" class="jy jz li ka b kb kc kd ke kf kg kh ki lj kk kl km lk ko kp kq ll ks kt ku kv ij bi translated">alphabet-set <em class="iq">:将在语料库中构成序列的所有字母表的集合。例如，语料库中的所有蛋白质序列都由一组 20 个氨基酸组成。</em></p><p id="5290" class="jy jz li ka b kb kc kd ke kf kg kh ki lj kk kl km lk ko kp kq ll ks kt ku kv ij bi translated">序列<em class="iq">:离散</em>字母<em class="iq">的有序序列。语料库中的</em>序列<em class="iq">包含</em>字母表<em class="iq">的子集。</em></p></blockquote><p id="7927" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">序列语料库通常包含数千到数百万个序列。给定我们已标记或未标记的数据，通常需要聚类和分类。然而，由于序列的非结构化——任意长度的任意字符串，这样做并不简单。</p><p id="eb9c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了克服这一点，可以使用序列嵌入。这里我们将使用 SGT 嵌入，将序列中的长期和短期模式嵌入到一个有限维向量中。SGT 嵌入的优点是，我们可以很容易地调整长期/短期模式的数量，而不会增加计算量。</p><p id="5b0f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面的源代码和数据是<a class="ae lm" href="https://github.com/cran2367/sgt/tree/master/python" rel="noopener ugc nofollow" target="_blank">这里是</a>。在继续之前，我们需要安装<code class="fe ln lo lp lq b">sgt</code>包。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="6e6e" class="lv lw iq lq b gy lx ly l lz ma">$ pip install sgt</span></pre><h1 id="e615" class="mb lw iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">使聚集</h1><p id="e44c" class="pw-post-body-paragraph jy jz iq ka b kb my kd ke kf mz kh ki kj na kl km kn nb kp kq kr nc kt ku kv ij bi translated"><strong class="ka ir">蛋白质序列聚类</strong></p><p id="fb09" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里使用的数据取自<a class="ae lm" href="http://www.uniprot.org" rel="noopener ugc nofollow" target="_blank">www.uniprot.org</a>。这是一个蛋白质的公共数据库。该数据包含蛋白质序列及其功能。在这一节中，我们将对蛋白质序列进行聚类，在下一节中，我们将使用它们的功能作为构建分类器的标签。</p><p id="40cd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们首先读取序列数据，并将其转换为一个列表列表。如下所示，每个序列都是一个字母列表。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="e1c1" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; protein_data = pd.DataFrame.from_csv('../data/protein_classification.csv')<br/>&gt;&gt;&gt; X = protein_data['Sequence']<br/>&gt;&gt;&gt; def split(word): <br/>&gt;&gt;&gt;    return [char for char in word]</span><span id="90cb" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; sequences = [split(x) for x in X]<br/>&gt;&gt;&gt; print(sequences[0])<br/>['M', 'E', 'I', 'E', 'K', 'T', 'N', 'R', 'M', 'N', 'A', 'L', 'F', 'E', 'F', 'Y', 'A', 'A', 'L', 'L', 'T', 'D', 'K', 'Q', 'M', 'N', 'Y', 'I', 'E', 'L', 'Y', 'Y', 'A', 'D', 'D', 'Y', 'S', 'L', 'A', 'E', 'I', 'A', 'E', 'E', 'F', 'G', 'V', 'S', 'R', 'Q', 'A', 'V', 'Y', 'D', 'N', 'I', 'K', 'R', 'T', 'E', 'K', 'I', 'L', 'E', 'D', 'Y', 'E', 'M', 'K', 'L', 'H', 'M', 'Y', 'S', 'D', 'Y', 'I', 'V', 'R', 'S', 'Q', 'I', 'F', 'D', 'Q', 'I', 'L', 'E', 'R', 'Y', 'P', 'K', 'D', 'D', 'F', 'L', 'Q', 'E', 'Q', 'I', 'E', 'I', 'L', 'T', 'S', 'I', 'D', 'N', 'R', 'E']</span></pre><p id="ca0f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">接下来，我们生成序列嵌入。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="d92a" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; from sgt import Sgt<br/>&gt;&gt;&gt; sgt = Sgt(kappa = 10, lengthsensitive = False)<br/>&gt;&gt;&gt; embedding = sgt.fit_transform(corpus=sequences)</span></pre><p id="74c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">嵌入是在 400 维空间。我们先对它做 PCA，降维到两个。这也将有助于集群的可视化。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="c702" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; pca = PCA(n_components=2)<br/>&gt;&gt;&gt; pca.fit(embedding)<br/>&gt;&gt;&gt; X = pca.transform(embedding)</span><span id="7deb" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; print(np.sum(pca.explained_variance_ratio_))<br/>0.6019403543806409</span></pre><p id="65bf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">前两个电脑解释了大约 60%的差异。我们将把它们分成 3 组。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="e81e" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; kmeans = KMeans(n_clusters=3, max_iter =300)<br/>&gt;&gt;&gt; kmeans.fit(df)</span><span id="8e49" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; labels = kmeans.predict(df)<br/>&gt;&gt;&gt; centroids = kmeans.cluster_centers_</span><span id="29c1" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; fig = plt.figure(figsize=(5, 5))<br/>&gt;&gt;&gt; colmap = {1: 'r', 2: 'g', 3: 'b'}<br/>&gt;&gt;&gt; colors = list(map(lambda x: colmap[x+1], labels))<br/>&gt;&gt;&gt; plt.scatter(df['x1'], df['x2'], color=colors, alpha=0.5, edgecolor=colors)</span></pre><figure class="kx ky kz la gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/fcb1c5ba3329f0053cc1bc69d7c6a889.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*-rIxFocYObr05kfqSflsjg.png"/></div></figure><p id="75ed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">继续构建分类器。</p><h1 id="368e" class="mb lw iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">分类</h1><p id="ae93" class="pw-post-body-paragraph jy jz iq ka b kb my kd ke kf mz kh ki kj na kl km kn nb kp kq kr nc kt ku kv ij bi translated"><strong class="ka ir">蛋白质序列分类</strong></p><p id="90b0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将从在前面使用的相同蛋白质数据集上构建分类器开始。数据集中的蛋白质有两个功能。因此，我们将构建一个二元分类器。</p><p id="97d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将首先将数据中的<code class="fe ln lo lp lq b">function [CC]</code>列转换成标签，这些标签可以在<code class="fe ln lo lp lq b">keras</code>中内置的 MLP 模型中获取。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="315c" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; y = protein_data['Function [CC]']<br/>&gt;&gt;&gt; encoder = LabelEncoder()<br/>&gt;&gt;&gt; encoder.fit(y)<br/>&gt;&gt;&gt; encoded_y = encoder.transform(y)</span></pre><p id="9543" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在下文中，我们建立了 MLP 分类器，并运行 10 重交叉验证。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="5a53" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; kfold = 10<br/>&gt;&gt;&gt; X = pd.DataFrame(embedding)<br/>&gt;&gt;&gt; y = encoded_y</span><span id="4823" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; random_state = 1</span><span id="eb09" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; test_F1 = np.zeros(kfold)<br/>&gt;&gt;&gt; skf = KFold(n_splits = kfold, shuffle = True, random_state = random_state)<br/>&gt;&gt;&gt; k = 0<br/>&gt;&gt;&gt; epochs = 50<br/>&gt;&gt;&gt; batch_size = 128</span><span id="a46c" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; for train_index, test_index in skf.split(X, y):<br/>&gt;&gt;&gt;     X_train, X_test = X.iloc[train_index], X.iloc[test_index]<br/>&gt;&gt;&gt;     y_train, y_test = y[train_index], y[test_index]<br/>&gt;&gt;&gt;     X_train = X_train.as_matrix(columns = None)<br/>&gt;&gt;&gt;     X_test = X_test.as_matrix(columns = None)<br/>&gt;&gt;&gt;     <br/>&gt;&gt;&gt;     model = Sequential()<br/>&gt;&gt;&gt;     model.add(Dense(64, input_shape = (X_train.shape[1],), init = 'uniform')) <br/>&gt;&gt;&gt;     model.add(Activation('relu'))<br/>&gt;&gt;&gt;     model.add(Dropout(0.5))<br/>&gt;&gt;&gt;     model.add(Dense(32, init='uniform'))<br/>&gt;&gt;&gt;     model.add(Activation('relu'))<br/>&gt;&gt;&gt;     model.add(Dropout(0.5))<br/>&gt;&gt;&gt;     model.add(Dense(1, init='uniform'))<br/>&gt;&gt;&gt;     model.add(Activation('sigmoid'))<br/>&gt;&gt;&gt;     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>&gt;&gt;&gt;     <br/>&gt;&gt;&gt;     model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=0)<br/>&gt;&gt;&gt;     <br/>&gt;&gt;&gt;     y_pred = model.predict_proba(X_test).round().astype(int)<br/>&gt;&gt;&gt;     y_train_pred = model.predict_proba(X_train).round().astype(int)</span><span id="9994" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt;     test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)<br/>&gt;&gt;&gt;     k+=1<br/>    <br/>&gt;&gt;&gt; print ('Average test f1-score', np.mean(test_F1))<br/>Average test f1-score 1.0</span></pre><p id="869b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个数据对分类器来说太好了。我们还有另一个更具挑战性的数据集。让我们过一遍。</p><p id="a5fb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">网络日志数据分类</strong></p><p id="0301" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个数据样本取自这里的<a class="ae lm" href="https://www.ll.mit.edu/r-d/datasets/1998-darpa-intrusion-detection-evaluation-dataset" rel="noopener ugc nofollow" target="_blank"/>。这是一个网络入侵数据，包含审计日志和任何攻击作为正面标签。由于，网络入侵是一个罕见的事件，数据是不平衡的。此外，我们有一个只有 111 条记录的小数据集。这里我们将建立一个序列分类模型来预测网络入侵。</p><p id="4a29" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每个序列中包含的数据是一系列的活动，例如，<code class="fe ln lo lp lq b">{login, password, …}</code>。输入数据序列中的<em class="li">字母</em>已经被编码成整数。原始序列数据文件出现在<a class="ae lm" href="https://github.com/cran2367/sgt/tree/master/data" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="0e7c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与之前类似，我们将首先为分类器准备数据。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="5d93" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; darpa_data = pd.DataFrame.from_csv('../data/darpa_data.csv')<br/>&gt;&gt;&gt; X = darpa_data['seq']<br/>&gt;&gt;&gt; sequences = [x.split('~') for x in X]</span><span id="2b7d" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; y = darpa_data['class']<br/>&gt;&gt;&gt; encoder = LabelEncoder()<br/>&gt;&gt;&gt; encoder.fit(y)<br/>&gt;&gt;&gt; y = encoder.transform(y)</span></pre><p id="82ff" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在该数据中，序列嵌入应该是<strong class="ka ir">长度敏感的</strong>。长度在这里很重要，因为模式相似但长度不同的序列可以有不同的标签。考虑两个会话的简单例子:<code class="fe ln lo lp lq b">{login, pswd, login, pswd,…}</code>和<code class="fe ln lo lp lq b">{login, pswd,…(repeated several times)…, login, pswd}</code>。虽然第一个会话可能是普通用户输错了一次密码，但另一个会话可能是猜测密码的攻击。因此，序列长度和模式一样重要。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="ebef" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; sgt_darpa = Sgt(kappa = 5, lengthsensitive = True)<br/>&gt;&gt;&gt; embedding = sgt_darpa.fit_transform(corpus=sequences)</span></pre><p id="2976" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在这里发现的<code class="fe ln lo lp lq b">embedding</code>很稀少。因此，在训练分类器之前，我们将使用 PCA 进行降维。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="744c" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; from sklearn.decomposition import PCA<br/>&gt;&gt;&gt; pca = PCA(n_components=35)<br/>&gt;&gt;&gt; pca.fit(embedding)<br/>&gt;&gt;&gt; X = pca.transform(embedding)<br/>&gt;&gt;&gt; print(np.sum(pca.explained_variance_ratio_))<br/>0.9862350164327149</span></pre><p id="aa2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所选的前 35 个主成分分析解释了 98%以上的差异。我们现在将继续使用<code class="fe ln lo lp lq b">keras</code>构建一个多层感知器。</p><p id="8a8b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于数据量很小，阳性标记点的数量也很少，我们将进行三重验证。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="a93d" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; kfold = 3<br/>&gt;&gt;&gt; random_state = 11</span><span id="aa80" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; test_F1 = np.zeros(kfold)<br/>&gt;&gt;&gt; time_k = np.zeros(kfold)<br/>&gt;&gt;&gt; skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)<br/>&gt;&gt;&gt; k = 0<br/>&gt;&gt;&gt; epochs = 300<br/>&gt;&gt;&gt; batch_size = 15</span><span id="685e" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; class_weight = {0 : 0.12, 1: 0.88,}  # The weights can be changed and made inversely proportional to the class size to improve the accuracy.</span><span id="fcaf" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; for train_index, test_index in skf.split(X, y):<br/>&gt;&gt;&gt;     X_train, X_test = X[train_index], X[test_index]<br/>&gt;&gt;&gt;     y_train, y_test = y[train_index], y[test_index]<br/>&gt;&gt;&gt;     <br/>&gt;&gt;&gt;     model = Sequential()<br/>&gt;&gt;&gt;     model.add(Dense(128, input_shape=(X_train.shape[1],), init='uniform')) <br/>&gt;&gt;&gt;     model.add(Activation('relu'))<br/>&gt;&gt;&gt;     model.add(Dropout(0.5))<br/>&gt;&gt;&gt;     model.add(Dense(1, init='uniform'))<br/>&gt;&gt;&gt;     model.add(Activation('sigmoid'))<br/>&gt;&gt;&gt;     model.summary()<br/>&gt;&gt;&gt;     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>&gt;&gt;&gt;    <br/>&gt;&gt;&gt;     start_time = time.time()<br/>&gt;&gt;&gt;     model.fit(X_train, y_train ,batch_size=batch_size, epochs=epochs, verbose=1, class_weight=class_weight)<br/>&gt;&gt;&gt;     end_time = time.time()<br/>&gt;&gt;&gt;     time_k[k] = end_time-start_time</span><span id="4acd" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt;     y_pred = model.predict_proba(X_test).round().astype(int)<br/>&gt;&gt;&gt;     y_train_pred = model.predict_proba(X_train).round().astype(int)<br/>&gt;&gt;&gt;     test_F1[k] = sklearn.metrics.f1_score(y_test, y_pred)<br/>&gt;&gt;&gt;     k += 1<br/>&gt;&gt;&gt; print ('Average Test f1-score', np.mean(test_F1))<br/>Average Test f1-score 0.5236467236467236</span><span id="0516" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; print ('Average Run time', np.mean(time_k))<br/>Average Run time 9.076935768127441</span></pre><p id="4f6d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一个很难分类的数据。为了有一个宽松的基准，让我们在相同的数据上建立一个更好的 LSTM 分类器。</p><pre class="kx ky kz la gt lr lq ls lt aw lu bi"><span id="5e18" class="lv lw iq lq b gy lx ly l lz ma">&gt;&gt;&gt; X = darpa_data['seq']<br/>&gt;&gt;&gt; encoded_X = np.ndarray(shape=(len(X),), dtype=list)<br/>&gt;&gt;&gt; for i in range(0,len(X)):<br/>&gt;&gt;&gt;     encoded_X[i]=X.iloc[i].split("~")<br/>&gt;&gt;&gt; max_seq_length = np.max(darpa_data['seqlen'])<br/>&gt;&gt;&gt; encoded_X = sequence.pad_sequences(encoded_X, maxlen=max_seq_length)</span><span id="754c" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; kfold = 3<br/>&gt;&gt;&gt; random_state = 11</span><span id="8548" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; test_F1 = np.zeros(kfold)<br/>&gt;&gt;&gt; time_k = np.zeros(kfold)</span><span id="a06a" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; epochs = 50<br/>&gt;&gt;&gt; batch_size = 15<br/>&gt;&gt;&gt; skf = StratifiedKFold(n_splits=kfold, shuffle=True, random_state=random_state)<br/>&gt;&gt;&gt; k = 0</span><span id="21c7" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; for train_index, test_index in skf.split(encoded_X, y):<br/>&gt;&gt;&gt;     X_train, X_test = encoded_X[train_index], encoded_X[test_index]<br/>&gt;&gt;&gt;     y_train, y_test = y[train_index], y[test_index]<br/>&gt;&gt;&gt; <br/>&gt;&gt;&gt;     embedding_vecor_length = 32<br/>&gt;&gt;&gt;     top_words=50<br/>&gt;&gt;&gt;     model = Sequential()<br/>&gt;&gt;&gt;     model.add(Embedding(top_words, embedding_vecor_length, input_length=max_seq_length))<br/>&gt;&gt;&gt;     model.add(LSTM(32))<br/>&gt;&gt;&gt;     model.add(Dense(1, init='uniform'))<br/>&gt;&gt;&gt;     model.add(Activation('sigmoid'))<br/>&gt;&gt;&gt;     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="86c9" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt;     start_time = time.time()<br/>&gt;&gt;&gt;     model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)<br/>&gt;&gt;&gt;     end_time=time.time()<br/>&gt;&gt;&gt;     time_k[k]=end_time-start_time</span><span id="8147" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt;     y_pred = model.predict_proba(X_test).round().astype(int)<br/>&gt;&gt;&gt;     y_train_pred=model.predict_proba(X_train).round().astype(int)<br/>&gt;&gt;&gt;     test_F1[k]=sklearn.metrics.f1_score(y_test, y_pred)<br/>&gt;&gt;&gt;     k+=1</span><span id="e7c1" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; print ('Average Test f1-score', np.mean(test_F1))<br/>Average Test f1-score 0.0</span><span id="f7b5" class="lv lw iq lq b gy nd ly l lz ma">&gt;&gt;&gt; print ('Average Run time', np.mean(time_k))<br/>Average Run time 425.68603706359863</span></pre><p id="291e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们发现 LSTM 分类器给出的 F1 值为 0。这可以通过改变模型来改善。然而，我们发现 SGT 嵌入可以在不需要复杂分类器的情况下对少量且不平衡的数据起作用。此外，在运行时，SGT 嵌入式网络明显更快。平均耗时 9.1 秒，而 LSTM 模型耗时 425.6 秒。</p><h1 id="9504" class="mb lw iq bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">结束语</h1><ul class=""><li id="04fd" class="nf ng iq ka b kb my kf mz kj nh kn ni kr nj kv nk nl nm nn bi translated">我们学习了使用序列嵌入进行序列聚类和分类。</li><li id="7d4e" class="nf ng iq ka b kb no kf np kj nq kn nr kr ns kv nk nl nm nn bi translated">这个嵌入是这个<a class="ae lm" href="https://arxiv.org/abs/1608.03533" rel="noopener ugc nofollow" target="_blank">论文</a>的一个实现。这篇文章中没有涉及到，但是可以参考这篇文章来看看 SGT 嵌入相对于其他嵌入的准确性比较。</li><li id="3b13" class="nf ng iq ka b kb no kf np kj nq kn nr kr ns kv nk nl nm nn bi translated">由于 SGT 嵌入捕捉长期和短期模式的能力，它比大多数其他序列建模方法工作得更好。</li><li id="e384" class="nf ng iq ka b kb no kf np kj nq kn nr kr ns kv nk nl nm nn bi translated">建议您使用调谐参数<code class="fe ln lo lp lq b">kappa</code>，查看其对精确度的影响。</li></ul><p id="609d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="li">学分:</em></p><ul class=""><li id="d83f" class="nf ng iq ka b kb kc kf kg kj nt kn nu kr nv kv nk nl nm nn bi translated"><a class="ae lm" href="https://www.linkedin.com/in/samaneh-ebrahimi/" rel="noopener ugc nofollow" target="_blank">萨曼内·易卜拉希米博士</a>，他是<a class="ae lm" href="https://arxiv.org/abs/1608.03533" rel="noopener ugc nofollow" target="_blank"> SGT 论文</a>的合著者，对上述代码做出了重大贡献。</li><li id="b6b4" class="nf ng iq ka b kb no kf np kj nq kn nr kr ns kv nk nl nm nn bi translated"><a class="ae lm" href="https://github.com/datashinobi" rel="noopener ugc nofollow" target="_blank"> Yassine Khelifi </a>，为 SGT 编写第一个 Python 版本的数据科学专家。</li></ul></div></div>    
</body>
</html>