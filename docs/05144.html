<html>
<head>
<title>Fine-Tune ERNIE 2.0 for Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">针对文本分类微调 ERNIE 2.0</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-gaganmanku96-fine-tune-ernie-2-0-for-text-classification-6f32bee9bf3c?source=collection_archive---------13-----------------------#2019-08-01">https://towardsdatascience.com/https-medium-com-gaganmanku96-fine-tune-ernie-2-0-for-text-classification-6f32bee9bf3c?source=collection_archive---------13-----------------------#2019-08-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7bde" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何微调 ERNIE 2.0，百度最新的文本分类模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3f2ccbb474b23640e51e249e3723e380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SV7m7LlmyAdRwdestUdd8w.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@casparrubin?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Caspar Camille Rubin</a> on <a class="ae kv" href="https://unsplash.com/search/photos/setup?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="04ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">百度发布了连续自然语言处理框架 ERNIE 2.0。ERNIE 主张通过知识整合增强代表性。百度在其<a class="ae kv" href="https://arxiv.org/abs/1907.12412" rel="noopener ugc nofollow" target="_blank">研究论文</a>中声称，ERNIE 2.0 在中文和英文的 16 个自然语言处理任务中的表现优于 BERT 和最近的<a class="ae kv" href="https://hub.packtpub.com/google-researchers-present-xlnet-a-new-pre-training-method-that-outperforms-bert-on-20-tasks/" rel="noopener ugc nofollow" target="_blank"> XLNet </a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/2b614e359eed6720fa34177520fe3616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bjcbYH3AtKbCLnbp.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae kv" href="https://hub.packtpub.com/baidu-open-sources-ernie-2-0-a-continual-pre-training-nlp-model-that-outperforms-bert-and-xlnet-on-16-nlp-tasks/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="d1c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ERNIE 2.0 是一个持续的预培训框架。持续学习旨在用几个任务按顺序训练模型，以便它在学习新任务时记住以前学习的任务。连续预训练的架构包含一系列共享的文本编码层来编码上下文信息，这些信息可以通过使用递归神经网络或由堆叠的自我关注层组成的深度转换器来定制。编码器的参数可以在所有预训练任务中更新。</p><p id="6632" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基本模型包含 12 层，12 个自关注头和 768 维的隐藏尺寸，而大模型包含 24 层，16 个自关注头和 1024 维的隐藏尺寸。XLNet 的型号设置和 BERT 一样。</p><p id="1ccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于进一步的阅读，我建议查看这些文章——<a class="ae kv" href="https://www.analyticsindiamag.com/baidus-ernie-2-0-gets-nlp-top-honours-eclipses-bert-xlnet/" rel="noopener ugc nofollow" target="_blank">这</a>和<a class="ae kv" href="https://hub.packtpub.com/baidu-open-sources-ernie-2-0-a-continual-pre-training-nlp-model-that-outperforms-bert-and-xlnet-on-16-nlp-tasks/" rel="noopener ugc nofollow" target="_blank">这</a>，以深入了解这种模式是如何工作的。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h2 id="f4ba" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">现在，让我们看看代码</h2><h2 id="d342" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">安装和导入必要的包—</h2><ol class=""><li id="d8af" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated">安装必要的软件包</li></ol><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="d940" class="ma mb iq nf b gy nj nk l nl nm">$ !pip install paddlepaddle-gpu</span></pre><blockquote class="nn no np"><p id="080e" class="kw kx nq ky b kz la jr lb lc ld ju le nr lg lh li ns lk ll lm nt lo lp lq lr ij bi translated">你也可以安装它的 CPU 版本，但它是高度计算的任务，所以我建议在 GPU 上训练它。</p><p id="e5a1" class="kw kx nq ky b kz la jr lb lc ld ju le nr lg lh li ns lk ll lm nt lo lp lq lr ij bi translated">ERNIE 2.0 接受了 48 个英伟达 v100 GPU 卡(用于基本型号)和 64 个英伟达 v100 GPU 卡(用于大型型号)的英语和中文培训。</p></blockquote><p id="6e48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.克隆它的 git 存储库并下载模型。我在这里使用基本模型。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="9baf" class="ma mb iq nf b gy nj nk l nl nm">$ wget <a class="ae kv" href="https://ernie.bj.bcebos.com/ERNIE_Base_en_stable-2.0.0.tar.gz" rel="noopener ugc nofollow" target="_blank">https://ernie.bj.bcebos.com/ERNIE_Base_en_stable-2.0.0.tar.gz</a></span><span id="d98b" class="ma mb iq nf b gy nu nk l nl nm">$ gunzip ERNIE_Base_en_stable-2.0.0.tar.gz</span><span id="5fe2" class="ma mb iq nf b gy nu nk l nl nm">$ tar -xvf ERNIE_Base_en_stable-2.0.0.tar</span><span id="cf26" class="ma mb iq nf b gy nu nk l nl nm">$ git clone <a class="ae kv" href="https://github.com/PaddlePaddle/ERNIE.git" rel="noopener ugc nofollow" target="_blank">https://github.com/PaddlePaddle/ERNIE.git</a></span></pre><h2 id="5a6b" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">准备数据</h2><p id="7af0" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">数据集必须采用特殊的格式化方式。这些列应该是<strong class="ky ir"> <em class="nq"> text_a </em> </strong>和<strong class="ky ir"> <em class="nq">标签。</em> </strong>必须是 tsv 文件。</p><ol class=""><li id="f57f" class="mt mu iq ky b kz la lc ld lf ny lj nz ln oa lr na nb nc nd bi translated">下载数据集</li></ol><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="ef1a" class="ma mb iq nf b gy nj nk l nl nm">!wget !wget <a class="ae kv" href="https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip</a></span><span id="d61d" class="ma mb iq nf b gy nu nk l nl nm">!unzip 'drugsCom_raw.zip'</span></pre><p id="dfda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会得到 2 个文件-训练和测试。</p><p id="84f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.现在，让我们处理数据并更改标签</p><p id="e069" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a)创建培训和开发数据</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">preprocessing data</figcaption></figure><p id="b4a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们以 80:20 的比例将数据分成两部分。</p><p id="9138" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">b)创建测试数据</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h2 id="824c" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">将数据放在正确的位置</h2><ol class=""><li id="2e68" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated">创建 2 个文件夹</li></ol><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="7e8e" class="ma mb iq nf b gy nj nk l nl nm">$ mkdir -p dataset/SST-2<br/>$ mkdir -p parameters/params</span></pre><p id="51e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.将培训、开发和测试文件移入 SST-2 文件夹</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="a54d" class="ma mb iq nf b gy nj nk l nl nm">$ mv train.tsv dataset/SST-2/<br/>$ mv dev.tsv dataset/SST-2/<br/>$ mv test.tsv dataset/SST-2/</span></pre><p id="68c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.将参数内容移动到参数/参数中</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="b577" class="ma mb iq nf b gy nj nk l nl nm">$ mv params/ parameters/params/</span></pre><p id="9cac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.将文件夹数据集和参数都移动到 ERNIE</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="1dda" class="ma mb iq nf b gy nj nk l nl nm">$ mv dataset ERNIE/<br/>$ mv parameters ERNIE/</span></pre><h2 id="f603" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">开始模型训练</h2><ol class=""><li id="0813" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated">将目录更改为 ERNIE</li></ol><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="0b24" class="ma mb iq nf b gy nj nk l nl nm">$ cd ERNIE/</span></pre><p id="43df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.为模型路径和数据集设置环境变量</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="fd17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.开始训练</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="80c8" class="ma mb iq nf b gy nj nk l nl nm">sh script/en_glue/ernie_base/SST-2/task.sh</span></pre><blockquote class="nn no np"><p id="1b16" class="kw kx nq ky b kz la jr lb lc ld ju le nr lg lh li ns lk ll lm nt lo lp lq lr ij bi translated">如果您遇到任何错误，说找不到培训文件或 init_parameter 丢失，请尝试检查您的当前目录。你应该在 ERINE 文件夹里。</p></blockquote><p id="fea8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">全部代码可在<a class="ae kv" href="https://github.com/gaganmanku96/NLP/tree/master/ERNIE%202.0%20Fine%20-Tuning" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="6c0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你也可以试试<a class="ae kv" href="https://colab.research.google.com/drive/1GVXVb8M8kclroIpb3KMmfqQqe_ujWfUd" rel="noopener ugc nofollow" target="_blank"> colab 笔记本</a>上的代码。</p><h2 id="3764" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated">结论</h2><p id="a07a" class="pw-post-body-paragraph kw kx iq ky b kz mv jr lb lc mw ju le lf nv lh li lj nw ll lm ln nx lp lq lr ij bi translated">随着诸如 BERT、ERNIE、XLNET、OPEN-AI 等新方法的出现。我们已经看到每个 NLP 任务都有巨大的改进，但代价是高昂的计算成本。XLNET 的培训成本约为 245，000 美元，这是一笔惊人的金额。由于计算需求很高，初创公司越来越难以适应这些新模式。</p><h2 id="94a9" class="ma mb iq bd mc md me dn mf mg mh dp mi lf mj mk ml lj mm mn mo ln mp mq mr ms bi translated"><strong class="ak">参考文献</strong></h2><ol class=""><li id="2d9f" class="mt mu iq ky b kz mv lc mw lf mx lj my ln mz lr na nb nc nd bi translated"><a class="ae kv" href="https://github.com/PaddlePaddle/ERNIE" rel="noopener ugc nofollow" target="_blank">https://github.com/PaddlePaddle/ERNIE</a></li><li id="86f6" class="mt mu iq ky b kz od lc oe lf of lj og ln oh lr na nb nc nd bi translated"><a class="ae kv" href="https://hub.packtpub.com/baidu-open-sources-ernie-2-0-a-continual-pre-training-nlp-model-that-outperforms-bert-and-xlnet-on-16-nlp-tasks/" rel="noopener ugc nofollow" target="_blank">https://hub . packtpub . com/Baidu-open-sources-Ernie-2-0-a-continuous-pre-training-NLP-model-that-performs-Bert-and-XL net-on-16-NLP-tasks/</a></li></ol></div></div>    
</body>
</html>