<html>
<head>
<title>A Primer in Computer Vision with Julia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">朱莉娅的计算机视觉初级读本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-primer-on-computer-vision-with-julia-2c7068a35b32?source=collection_archive---------19-----------------------#2019-12-05">https://towardsdatascience.com/a-primer-on-computer-vision-with-julia-2c7068a35b32?source=collection_archive---------19-----------------------#2019-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2208" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习如何使用<strong class="ak"> CNN </strong>和<strong class="ak">Julia<strong class="ak">识别数字</strong>。</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/992b5ef19b2d719aac7cfbb178e5ec04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sOaP5WzmI5o3x26bywwMIQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by <a class="ae ky" href="https://pixabay.com/users/cocoparisienne-127419/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1173863" rel="noopener ugc nofollow" target="_blank">cocoparisienne</a> from <a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1173863" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="b292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个帖子是关于<strong class="lb iu">卷积神经网络</strong> (CNN)使用<strong class="lb iu"> Julia </strong>的速成班。CNN 是一种奇特的功能，可以被“训练”来识别图像中的模式。在这篇博文中，我介绍了计算机视觉的<strong class="lb iu">“Hello World”</strong>:来自<a class="ae ky" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST 数据集</a>的手写数字分类。互联网上有数以千计的免费 Python 教程。</p><p id="3214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而是让我们使用<strong class="lb iu"> Julia </strong>和包<strong class="lb iu"> Flux.jl </strong>。为什么？因为 Julia 的速度很快，如果你有数百万张图片要分析，与 Python 相比，速度会大大加快。这篇博文的 Jupyter 笔记本可以在这里找到。</p><h1 id="de7f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据</h1><p id="7496" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">MNIST 数据集包含灰度手写数字(0 到 9)的图像，这些图像居中对齐。每个像素由一个介于 0(黑色)和 255(白色)之间的数字表示。每幅图像都是 28 乘 28 像素。表示图像的一种方式是将其视为 28*28 = 784 像素的一维列向量。然而，这种表示忽略了图像的“结构”:彼此靠近的像素在我们试图识别的数字上是有信息的。CNN 是一个很好的工具，可以保持图像的空间结构，同时避免与<a class="ae ky" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">维度诅咒</a>相关的问题:图像是嘈杂和高维的输入数据。</p><h1 id="778d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">美国有线电视新闻网的速成班</h1><p id="8656" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">CNN 的两个关键组成部分是<strong class="lb iu">卷积层</strong>(因此得名)和<strong class="lb iu">最大池层</strong>。</p><h1 id="c6f5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">卷积层</h1><p id="98b0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">卷积层对每个点应用一个<em class="ms">模板</em>。卷积层的输出是较低维度的“图像”，它提供了输入图像的某些特征(形状、边缘等)的信息。).下图显示了卷积层的工作原理:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b0eb1ebaa207ba31be9e1ff714b2b5a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/0*e-zbg2ADhTF2jLG8.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">How a convolutional layer works. Source: <a class="ae ky" href="https://mitmath.github.io/18337/lecture14/pdes_and_convolutions" rel="noopener ugc nofollow" target="_blank">https://mitmath.github.io/18337/lecture14/pdes_and_convolutions</a></figcaption></figure><h1 id="7a92" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最大池层</h1><p id="3fa9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最大池层是一个<em class="ms">模板</em>，用于选择正方形内的最大值。下图是应用于 4x 4 图像的 maxpool 层:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/303ba7bd1e88b4b63d1d6635dfcae8e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/0*bIECTZ9qOW5AHxt5.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Illustration of a maxpool layer. Source: <a class="ae ky" href="https://mauriciocodesso.com/post/convolution-neural-network/" rel="noopener ugc nofollow" target="_blank">https://mauriciocodesso.com/post/convolution-neural-network/</a></figcaption></figure><h1 id="8ac4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">步幅和衬垫</h1><p id="0224" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">构建 CNN 时，必须指定两个超参数:<strong class="lb iu">步幅和填充</strong></p><ul class=""><li id="102b" class="mv mw it lb b lc ld lf lg li mx lm my lq mz lu na nb nc nd bi translated">当跨度等于 1 时，我们一次移动一个像素的过滤器。当跨距等于 2 时，我们一次移动两个像素的过滤器，等等。</li><li id="42ff" class="mv mw it lb b lc ne lf nf li ng lm nh lq ni lu na nb nc nd bi translated">填充是指在图像的边界“加零”。填充可用于控制输出音量的大小，并有助于保持图像边界的信息</li></ul><p id="7bbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一个应用于 5×5 输入的 3×3 滤波器示例，该输入使用 2×2 步长填充了 1×1 零边界:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c8ea6af4a497f1b3a8252ba59e82bb42.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/0*6j6q9NN4PQ_zdRUV.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">How padding works. Source: <a class="ae ky" href="http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html" rel="noopener ugc nofollow" target="_blank">http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html</a></figcaption></figure><p id="44e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CNN 的典型基础设施是首先对输入图像应用卷积层，然后使用最大池层，最后使用全连接层。在使用全连接(FC)层之前，可以将几个“卷积层—最大池层”单元堆叠在一起。请注意，激活层(通常为<a class="ae ky" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>)通常插在卷积层和最大池层之间。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/6a7fb5fa6882695e56dc58e6d18294a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DGtjPCcEMlWnNreY.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">CNN architecture. Source: <a class="ae ky" rel="noopener" target="_blank" href="/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69">https://towardsdatascience.com/visualizing-the-fundamentals-of-convolutional-neural-networks-6021e5b07f69</a></figcaption></figure><h1 id="f087" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">使用 Flux.jl</h1><p id="780d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Flux.jl 是 Julia 生态系统中领先的机器学习包。接下来，我们加载 MNIST 数据集的训练样本和测试样本。训练样本是一组用于微调 CNN 参数的图像，而测试样本包含用于检查我们没有过度拟合训练样本的图像。过度拟合的确凿证据是训练样本的精度比使用测试样本图像的精度好得多。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Loading Julia packages and data</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/335153a4da2722b0f80f38d06cf581e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/0*i_zomjOnVHQc2Smr.png"/></div></figure><h1 id="f30d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">CNN 架构</h1><p id="f540" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在使用 FC 层之前，我们的 CNN 有通常的 Conv-&gt;ReLU-&gt;MaxPool 组件。我们使用 1 乘 1 的填充，跨距为 1(默认值)。通过使用 2 乘 2 最大池层，输入的大小逐渐减小。Flux.jl 中的默认激活是函数 x-&gt;x。这里，我们使用校正线性单位函数(ReLU)来代替:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Building the model</figcaption></figure><p id="0134" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ReLU 激活函数是分段线性函数。在 Krizhevsky 和合著者的<a class="ae ky" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank">“使用深度卷积神经网络的 ImageNet 分类”</a>论文中，作者写道:</p><blockquote class="no np nq"><p id="d04f" class="kz la ms lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated"><em class="it">我们把具有这种非线性的神经元称为整流线性单元(ReLUs)。具有 ReLUs 的深度卷积神经网络比具有 tanh 单元的等效网络训练速度快几倍。</em></p></blockquote><p id="9ad3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ReLU 激活功能也有助于减少由<a class="ae ky" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>引起的实际问题。也就是用来寻找我们 CNN 的参数的最小化算法的失败。下面是 ReLU 激活功能的曲线图:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="34aa" class="nz lw it nv b gy oa ob l oc od">xgrid = collect(range(-1, 1, length=100)) plot(xgrid, NNlib.relu.(xgrid), label = "relu(x)", title="ReLU activation function", xlabel="x")</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/37014d53a97ced913c2f4edd09c8c1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGdOPXpRv37asAe76nLmYA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">ReLU activation function</figcaption></figure><h1 id="94c4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">培养</h1><h1 id="ac58" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">定量</h1><p id="7ab3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">批量大小是一个参数，它告诉我们网络在“训练”时一次会“看到”多少幅图像。用技术术语来说，当执行梯度下降时，我们不会一次使用所有信息(因为内存限制，也因为不一定高效)。以下函数生成“批量”图像:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Batching</figcaption></figure><h1 id="19a2" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">损失函数和最小化</h1><p id="24dd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">CNN 要“学习”任何东西，它必须有一个“错”或“对”的概念。损失函数正是通过量化模型在识别数字方面的表现来做到这一点的。当输出是一个概率时，<a class="ae ky" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>损失函数是合适的。最后一步是选择一种算法来最小化损失函数。这里，让我们选择<a class="ae ky" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> ADAM </a>算法，我把它理解为某种带有动量和自适应学习率的<a class="ae ky" href="https://julienpascal.github.io/post/ols_ml/" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Loss function</figcaption></figure><p id="5c33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模块“训练”(微调 CNN 参数值)模型，直到达到预定的精度水平:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Training</figcaption></figure><h1 id="d2f0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">预言</h1><p id="6aa9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">一旦模型被训练，预测值很容易获得如下:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="8905" class="nz lw it nv b gy oa ob l oc od"># Get predictions and convert data to Array: <br/>pred = Tracker.data(model(test_set[1])); <br/><br/># Function to get the row index of the max value: <br/>f1(x) = getindex.(argmax(x, dims=1), 1) </span><span id="3e5e" class="nz lw it nv b gy of ob l oc od"># Final predicted value is the one with the maximum probability: <br/>pred = f1(pred) .- 1; #minus 1, because the first digit is 0 (not 1)</span></pre><p id="b2a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看模型在测试集上的表现。CNN 可以使用训练模型时没有使用的图像来识别数字吗？正如您在下面看到的，我们的模型在识别手写数字方面做得非常出色:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="fa56" class="nz lw it nv b gy oa ob l oc od">println("Predicted value = $(pred[1])") a = reshape(test_imgs[1], NROWS, NCOLS)</span><span id="9255" class="nz lw it nv b gy of ob l oc od">Predicted value = 7</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/56387c1eaafa8d23673b522293011115.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/0*-6v4C3CD10SKPYV9.png"/></div></figure><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="ee23" class="nz lw it nv b gy oa ob l oc od">println("Predicted value = $(pred[2])") a = reshape(test_imgs[2], NROWS, NCOLS)</span><span id="f97a" class="nz lw it nv b gy of ob l oc od">Predicted value = 2</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d76b1fcf33d15b924400140b5b3ef00a.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/0*2kp9DFjIzrww1LXc.png"/></div></figure><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="b4ae" class="nz lw it nv b gy oa ob l oc od">println("Predicted value = $(pred[3])") a = reshape(test_imgs[3], NROWS, NCOLS)</span><span id="120a" class="nz lw it nv b gy of ob l oc od">Predicted value = 1</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ef01d0d681bd06b4f66c2eb998188acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/0*BCMDZGJtH6RmZ2xX.png"/></div></figure><h1 id="7aaa" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">准确性检查</h1><p id="1512" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们现在有了一个模型，它在识别数字方面似乎做得很好。但是我们能改善它吗？如果是，如何实现？为了改进我们的模型，我们首先需要确定它失败的时间和原因。</p><h2 id="ec2e" class="nz lw it bd lx og oh dn mb oi oj dp mf li ok ol mh lm om on mj lq oo op ml oq bi translated">混淆矩阵</h2><p id="7209" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为此，一个有用的报告工具是<strong class="lb iu">混淆矩阵</strong>。混淆矩阵的每一行显示真实值的实例，而每一列显示预测值的实例。理想情况下，我们希望我们的模型能够完美地预测结果。对于一个完美的模型，所有的实例都位于混淆矩阵的对角线元素上。</p><p id="7118" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最后一次检查时，<code class="fe or os ot nv b">Flux.jl</code>没有一个内置函数来计算混淆矩阵。幸运的是，包<code class="fe or os ot nv b">MLBase</code>中提供了一个实现。下一个代码块计算混淆矩阵并显示出来。大多数实例位于对角线上，这并不奇怪，因为我们的模型的准确率超过 97.0%</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="2bca" class="nz lw it nv b gy oa ob l oc od">using MLBase </span><span id="0da4" class="nz lw it nv b gy of ob l oc od"># Adding 1 to outcome because the index 0 in arrays does not exist in Julia:<br/>Cm = confusmat(10, test_labels .+ 1, vec(pred) .+ 1)</span><span id="ccde" class="nz lw it nv b gy of ob l oc od"># Normalize output: <br/>Cm = Cm ./ sum(Cm, dims=2) </span><span id="3823" class="nz lw it nv b gy of ob l oc od"># Labels <br/>xs = [string(i) for i = 0:9] <br/>heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/4d68a7fa8c082bf46b343721a5adc198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rT5N802ImyTNhkl8MdWqvA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Heatmap for the confusion matrix</figcaption></figure><p id="a6ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了可视化我们的模型在哪里出错，我们可以使用可选参数<code class="fe or os ot nv b">clim</code>，给底层的颜色图加上一个上限。例如，下一个图显示我们的模型在区分 7 和 2 或者 8 和 2 时有困难。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="a6e5" class="nz lw it nv b gy oa ob l oc od"># Limits to colormap, so we can see where errors are located: <br/>xs = [string(i) for i = 0:9] <br/>heatmap(xs, xs, Cm, aspect_ratio=1, color=:plasma, clim=(0., 0.01))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/caf86126dbb6c86f063e32c856157824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSw8Liw1tb7OzeRvylma-Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Heatmap for the confusion matrix</figcaption></figure><h2 id="e521" class="nz lw it bd lx og oh dn mb oi oj dp mf li ok ol mh lm om on mj lq oo op ml oq bi translated">误差分析</h2><p id="d159" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">下一段代码显示了 CNN 失败的数字:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="bedf" class="nz lw it nv b gy oa ob l oc od">using ImageView, Gtk.ShortNames<br/># indices for errors: <br/>mistakes = test_labels .!= vec(pred) <br/>max_images = 5 grid, frames, canvases = canvasgrid((1,max_images));</span><span id="4e2d" class="nz lw it nv b gy of ob l oc od">k=0#counter <br/>for mistakes for (j, i) in enumerate(mistakes) <br/>    if i == true k+=1 # a false value has been found <br/>       println("Predicted value = $(pred[j])") <br/>       println("True value = $(test_labels[j])") <br/>       imshow(canvases[1,k], test_imgs[j]) <br/>    end <br/>    if k &gt;= max_images <br/>       break <br/>    end <br/>end <br/>win = Window(grid); <br/>Gtk.showall(win);</span><span id="8a8a" class="nz lw it nv b gy of ob l oc od">Predicted value = 5 True value = 9 <br/>Predicted value = 5 True value = 6 <br/>Predicted value = 4 True value = 8 <br/>Predicted value = 3 True value = 2 <br/>Predicted value = 7 True value = 2</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/41d7f667bd39af6f3f09e6836b25de40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zgvLeDtG2E94ciQu.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Five first prediction errors</figcaption></figure><p id="6a3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然从左边开始的两个数字(见上图)显然是 9 和 6，但剩下的 3 个元素并不简单。中间的 8 很容易与其他数字混淆，剩下的两个数字形状怪异。</p><h1 id="bacf" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="226e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在处理图像时，卷积神经网络通常在识别模式方面表现出色。这篇博文是对这个主题的非技术性介绍。而 Python 是机器学习中偏爱的工具(Keras，TensorFlow 等。)，我猜测 Julia 会越来越受欢迎，因为 Julia 既好用又快。</p><h1 id="0d5c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="9dc1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">*这篇博文的代码大量基于这篇 Flux.jl 教程:<a class="ae ky" href="https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl" rel="noopener ugc nofollow" target="_blank">https://github . com/flux ml/model-zoo/blob/master/vision/mnist/conv . JL</a><br/>*关于 CNN 和 PDEs 的链接:<a class="ae ky" href="https://mitmath.github.io/18337/lecture14/pdes_and_convolutions" rel="noopener ugc nofollow" target="_blank">https://MIT math . github . io/18337/lecture 14/PDEs _ and _ convolutions</a><br/>* CNN 的完整课程。大部分内容都可以在网上找到:<a class="ae ky" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">http://cs231n.github.io/convolutional-networks/</a></p></div><div class="ab cl ov ow hx ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="im in io ip iq"><p id="ad97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ms">原载于 2019 年 12 月 5 日</em><a class="ae ky" href="https://julienpascal.github.io/post/cnn/" rel="noopener ugc nofollow" target="_blank"><em class="ms">https://Julien Pascal . github . io</em></a><em class="ms">。</em></p></div></div>    
</body>
</html>