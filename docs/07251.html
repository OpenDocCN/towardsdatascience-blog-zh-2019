<html>
<head>
<title>Speech Recognition Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语音识别分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speech-recognition-analysis-f03ff9ce78e9?source=collection_archive---------11-----------------------#2019-10-12">https://towardsdatascience.com/speech-recognition-analysis-f03ff9ce78e9?source=collection_archive---------11-----------------------#2019-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="99c0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 Keras 建立语音识别模型。</h2></div><p id="f682" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从 Siri 到智能家居设备，语音识别广泛应用于我们的生活中。这个语音识别项目是利用 Kaggle 语音识别挑战数据集在 Tensorflow 上创建 Keras 模型，并对语音文件进行预测。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/6a58ef81c67d9ddc49f735772fb3e7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AX6ZeUixHAKeRrOR"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Photo by <a class="ae lu" href="https://unsplash.com/@solomac?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Adam Solomon</a> on <a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="a6c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面列出了 Kaggle 语音识别挑战数据集的链接:</p><div class="lv lw gp gr lx ly"><a href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">TensorFlow 语音识别挑战</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">www.kaggle.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm lo ly"/></div></div></a></div><h1 id="c3af" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据接收和处理:</h1><p id="eea5" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">与图像识别类似，语音识别最重要的部分是将音频文件转换成 2X2 数组。</p><h2 id="7f95" class="nk mo it bd mp nl nm dn mt nn no dp mx kr np nq mz kv nr ns nb kz nt nu nd nv bi translated">音频文件的采样速率和原始波形:</h2><p id="f021" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">音频文件的采样速率表示每秒传送的音频样本数，以 Hz 为单位。下图显示了音频原始波形和“bed”音频文件的采样速率之间的关系:</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="b9f0" class="nk mo it nx b gy ob oc l od oe">train_audio_path = 'C:<strong class="nx iu">\\</strong>Users<strong class="nx iu">\\</strong>...<strong class="nx iu">\\</strong>train<strong class="nx iu">\\</strong>audio'<br/>filename = '<strong class="nx iu">\\</strong>bed<strong class="nx iu">\\</strong>00f0204f_nohash_0.wav'<br/>sample_rate, samples = wavfile.read(str(train_audio_path) + filename)<br/>ipd.Audio(samples, rate=sample_rate)<br/>print(sample_rate)<br/>print(samples)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/44649c015f5316f884257d3e5697cfab.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*2UYc455ouW5Hc-Vww2bw7Q.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk"><code class="fe og oh oi nx b">Sample rate is the number of samples of audio carried per second, measured in Hz.</code></figcaption></figure><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="177e" class="nk mo it nx b gy ob oc l od oe"><em class="oj"># visualize this audio wave:</em><br/>fig = plt.figure(figsize=(14, 8))<br/>plt.plot(samples)<br/>plt.title('Raw wave of ' + filename)<br/>plt.ylabel('Wave')<br/>plt.show()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ok"><img src="../Images/f5eea5ccf195ad127ca4e6bb28e1983d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0DNbGSrYFZ0bKPy5-H6F7g.png"/></div></div></figure><p id="d2ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我将介绍音频文件的两个重要属性:</p><h2 id="251d" class="nk mo it bd mp nl nm dn mt nn no dp mx kr np nq mz kv nr ns nb kz nt nu nd nv bi translated"><strong class="ak">声谱图:</strong></h2><p id="9c0f" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">声谱图是声音的频谱-时间表示。频谱图的水平方向代表时间，垂直方向代表频率。(1)频谱图可用作一种可视化非平稳信号频率含量随时间变化的方式。</p><p id="683d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谱图(2)的公式如下:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ol"><img src="../Images/63dad49470155a21d0f1d8ab7dd22f44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5gCGBS5-CrSBiTGycMIhNw.png"/></div></div></figure><p id="4c46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我利用以下网站的代码来计算和可视化 log_spectrogram:</p><p id="ec33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">【https://www.tensorflow.org/api_guides/python/contrib.signal<a class="ae lu" href="https://github.com/Tony607/tf_audio_signal/blob/master/tf_audio_signal.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Tony 607/TF _ audio _ signal/blob/master/TF _ audio _ signal . ipynb</a></p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="2b35" class="nk mo it nx b gy ob oc l od oe"><strong class="nx iu">def</strong> log_spectrogram(file, label):<br/>    sample_rate, samples = wavfile.read(str(train_audio_path) + '<strong class="nx iu">\\</strong>'+label+'<strong class="nx iu">\\</strong>' + file)<br/>    signals = tf.cast(tf.reshape(samples, [1,-1 ]),tf.float32) <br/>    spectrogram = signal.stft(signals, frame_length=1024, frame_step= 512)<br/>    magnitude_spectrograms = tf.abs(spectrogram)<br/>    log_offset = 1e-6<br/>    <em class="oj">#When compressing with a logarithm, it's a good idea to use a stabilizing offset </em><br/>    <em class="oj">#to avoid high dynamic ranges caused by the singularity at zero.</em><br/>    log_magnitude_spectrograms = tf.log(magnitude_spectrograms + log_offset)<br/>    <strong class="nx iu">return</strong> log_magnitude_spectrograms</span></pre><p id="5b4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后绘制样本数据的 log _ spectrogram:bed。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="dcd1" class="nk mo it nx b gy ob oc l od oe">log_spe_bed = log_spectrogram(train.file[0],train.label[0]).numpy()<br/>array_bed = log_spe_bed.astype(np.float)[0]<br/>fig = plt.figure(figsize=(14,8))<br/><em class="oj">#plt.ylabel("Freqs in Hz")</em><br/>plt.xlabel("Log_Spectrogram")<br/>plt.imshow(np.swapaxes(array_bed,0,1).T)</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi om"><img src="../Images/fbb0a14eae1e6a3e92a8e4834fb52800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w55TXEKmH5gbXDOUGqKVJg.png"/></div></div></figure><h2 id="d92d" class="nk mo it bd mp nl nm dn mt nn no dp mx kr np nq mz kv nr ns nb kz nt nu nd nv bi translated">梅尔频率倒谱系数(MFCC):</h2><p id="8b48" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">梅尔倒谱系数(MFCC)是自动语音和说话人识别中广泛使用的特征。Mel 标度将纯音的感知频率或音高与其实际测量频率相关联。人类在低频时比高频时更善于辨别音调的微小变化。引入这一尺度使我们的特征与人类听到的更接近。(3)</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="c8bd" class="nk mo it nx b gy ob oc l od oe"><strong class="nx iu">def</strong> mfcc(file=train['file'].tolist(), label=train['label'].tolist()):<br/>    sample_rate, samples = wavfile.read(str(train_audio_path) + '<strong class="nx iu">\\</strong>'+label+'<strong class="nx iu">\\</strong>' + file)<br/>    <strong class="nx iu">if</strong> len(samples) &lt; 16000:<br/>        samples = np.pad(samples, (0,16000-len(samples)), 'linear_ramp')<br/>    <strong class="nx iu">else</strong>:<br/>        samples = samples[:16000]<br/>    signals = tf.cast(tf.reshape(samples, [1,-1 ]),tf.float32) <br/>    spectrogram = signal.stft(signals, frame_length=1024, frame_step= 512)<br/>    magnitude_spectrograms = tf.abs(spectrogram)<br/>    num_spectrogram_bins = magnitude_spectrograms.shape[-1].value<br/>    lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 64<br/>    linear_to_mel_weight_matrix = tf.contrib.signal.linear_to_mel_weight_matrix(num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,upper_edge_hertz)<br/>    mel_spectrograms = tf.tensordot(magnitude_spectrograms, linear_to_mel_weight_matrix, 1)<br/><em class="oj"># Note: Shape inference for &lt;a href="../../api_docs/python/tf/tensordot"&gt;&lt;code&gt;tf.tensordot&lt;/code&gt;&lt;/a&gt; does not currently handle this case.</em></span><span id="f0c8" class="nk mo it nx b gy on oc l od oe">mel_spectrograms.set_shape(magnitude_spectrograms.shape[:-1].concatenate(linear_to_mel_weight_matrix.shape[-1:]))<br/>    log_offset = 1e-6<br/>    log_mel_spectrograms = tf.log(mel_spectrograms + log_offset)<br/>    num_mfccs = 13<br/><em class="oj"># Keep the first `num_mfccs` MFCCs.</em><br/>    mfccs = tf.contrib.signal.mfccs_from_log_mel_spectrograms(log_mel_spectrograms)[..., :num_mfccs]<br/>    <strong class="nx iu">return</strong> mfccs.numpy()[0]</span></pre><p id="8c4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过使用上面定义的“mfcc”函数，很容易计算“bed”的音频文件的 mfcc 并可视化其 MFCC。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="f2f1" class="nk mo it nx b gy ob oc l od oe">mfcc_bed = mfcc(train.file[0],train.label[0])<br/>fig = plt.figure(figsize=(14,8))<br/>plt.ylabel("MFCC (log) coefficient")<br/>plt.imshow(np.swapaxes(mfcc_bed,0,1))</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oo"><img src="../Images/b433b908cb508c04f1082b1712fdae91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEuwYGzE_W-Q6twqQVX32A.png"/></div></div></figure><h1 id="1edd" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据建模:</h1><p id="3a4a" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我建立了一个顺序神经网络模型，这是在 keras 中建立模型的最简单的方法——它告诉 Keras 按顺序堆叠所有层。然后我添加了四个密集层，它们是模型中完全连接的层。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi op"><img src="../Images/1cf3a9478fcb2e1b3a0bda27edfb33e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5we-_JULBm165xaPUIDA0A.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Reference: <a class="ae lu" rel="noopener" target="_blank" href="/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6">https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6</a></figcaption></figure><p id="1d92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在建立模型后，我使用自适应矩估计作为优化器，类别交叉熵作为损失，准确度作为度量来编译模型。</p><pre class="lf lg lh li gt nw nx ny nz aw oa bi"><span id="ccfe" class="nk mo it nx b gy ob oc l od oe"><em class="oj"># Dense(64) is a fully-connected layer with 64 hidden units.</em><br/><em class="oj"># in the first layer, you must specify the expected input data shape:</em><br/><em class="oj"># here, 20-dimensional vectors.</em><br/><strong class="nx iu">with</strong> tf.Session() <strong class="nx iu">as</strong> sess0:<br/>    <strong class="nx iu">assert</strong> <strong class="nx iu">not</strong> tf.executing_eagerly()<br/>    model = Sequential()<br/><br/>    model.add(layers.Dense(32, input_shape=X_train_array.shape[1:], activation='tanh'))<br/>    model.add(Dense(64, activation='tanh'))<br/>    model.add(Dense(128, activation='tanh'))<br/>    <br/>    model.add(Flatten())<br/>    <em class="oj">#model.add(Dense(256, activation='relu'))</em><br/><br/>    model.add(Dense(30))<br/>    model.add(Activation('sigmoid'))<br/>    <br/>    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>    <em class="oj">#Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.</em><br/>    model.summary()<br/>    <br/>  <em class="oj">#  history = model.fit(x=X_train_array, y=y_train_array, epochs=5, verbose=1, validation_split = 0.33, shuffle=True, class_weight=get_class_weights(pd.Series((list(set(labels))),dtype='category').cat.codes.values),batch_size=batch_size) </em><br/>    history = model.fit(x=X_train_array, y=y_train_array, epochs=25, verbose=1, validation_split = 0.1, shuffle=<strong class="nx iu">True</strong>, class_weight=get_class_weights(pd.Series(Y_train,dtype='category').cat.codes.values),batch_size=128) <br/>    <br/>    model_evaluation = model.evaluate(x=X_test_array, y=y_test_array, batch_size=<strong class="nx iu">None</strong>, verbose=1)<br/><br/>    prediction = model.predict(X_test_array, batch_size = 128, verbose = 1)<br/>    <br/>    april_tst = model.predict(mfcc_april_test, batch_size = 128, verbose = 1)<br/><br/>    sess0.close()</span></pre><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi oq"><img src="../Images/543d277b801e7d3ae5f574eaa51548fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaasWKlPFO-Wo-qgfsHMIA.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Sequential Neural Network Model in Keras</figcaption></figure><p id="98f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我选择 25 作为历元数，这是模型在数据中循环的次数。经过约 20 个历元的运行，模型的验证准确率提高到 61% — 62%。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi or"><img src="../Images/24a512b1a60d20436d53856e5bf84889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3R49MS43cQ0D3uLejQd2hA.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Model accuracy regarding the number of epochs</figcaption></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi os"><img src="../Images/6bfc11e373ead601d6e6e79f57fd388c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C62b2WP7UXgHxL6jP2iz4Q.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Model loss regarding the number of epochs</figcaption></figure><p id="d7b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面两张图可以看出，测试和训练精度彼此不够接近，这意味着可以通过克服过拟合问题来改进这个模型。</p><h1 id="451a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结论:</h1><ol class=""><li id="966d" class="ot ou it kk b kl nf ko ng kr ov kv ow kz ox ld oy oz pa pb bi translated">音频文件通常被转换成数组作为 Keras 模型的输入。</li><li id="7918" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">声谱图和 MFCC 是要转换为数组的音频文件的两个特征。</li><li id="62f4" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">我们可以修改 Keras 模型的层次来提高模型的精度。</li><li id="ca3f" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">通过比较训练和测试精度来了解过度拟合问题。</li><li id="6426" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">与 Keras API 模型相比，顺序模型更容易修改。</li></ol><h1 id="521c" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结论:</h1><ol class=""><li id="3757" class="ot ou it kk b kl nf ko ng kr ov kv ow kz ox ld oy oz pa pb bi translated">为了准确预测 Kaggle 数据集中“测试”音频文件中带有噪声的语音，我需要通过添加背景噪声来处理当前的训练数据。</li><li id="cb0a" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">由于 Kaggle“测试”音频文件中存在未知声音，我还需要添加“未知”作为我的标签之一。</li><li id="5806" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">我还可以归一化训练数据的 mfccs，看看是否可以提高模型精度。</li><li id="a9dc" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">我也可以使用 CNN 和 RNN 的组合，看看我是否能提高模型的准确性。</li></ol><h1 id="2fe0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考:</h1><ol class=""><li id="dca6" class="ot ou it kk b kl nf ko ng kr ov kv ow kz ox ld oy oz pa pb bi translated">保罗·波尔斯马和大卫·韦宁克，<a class="ae lu" href="http://www.fon.hum.uva.nl/praat/manual/Intro_3_1__Viewing_a_spectrogram.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">简介 3.1。查看声谱图</strong> </a>，阿姆斯特丹大学语音科学研究所</li><li id="507e" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">维基百科:【https://en.wikipedia.org/wiki/Short-time_Fourier_transform T4】</li><li id="9911" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">密码学:<a class="ae lu" href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener ugc nofollow" target="_blank">http://practical cryptography . com/miscellaneous/machine-learning/guide-Mel-frequency-ceps tral-coefficients-mfccs/</a></li><li id="2dde" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated">Chris Dinant，<a class="ae lu" href="https://github.com/chrisdinant/speech/blob/master/train.ipynb" rel="noopener ugc nofollow" target="_blank"> Kaggle Tensorflow 语音识别挑战</a></li></ol><h1 id="d022" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">致谢:</h1><ol class=""><li id="7bce" class="ot ou it kk b kl nf ko ng kr ov kv ow kz ox ld oy oz pa pb bi translated"><a class="ae lu" href="https://www.kaggle.com/davids1992/speech-representation-and-data-exploration" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Davids 1992/speech-presentation-and-data-exploration</a></li><li id="1a2c" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated"><a class="ae lu" href="https://www.kaggle.com/ollmer/labels-spectrograms-exploration" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ol lmer/labels-spectro grams-exploration</a></li><li id="21a3" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated"><a class="ae lu" href="https://github.com/Tony607/tf_audio_signal/blob/master/tf_audio_signal.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Tony 607/TF _ audio _ signal/blob/master/TF _ audio _ signal . ipynb</a></li><li id="76ad" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/kaggle-tensorflow-speech-recognition-challenge-b46a3bca2501">https://towards data science . com/ka ggle-tensor flow-speech-recognition-challenge-b 46 a3 BCA 2501</a></li><li id="2cbb" class="ot ou it kk b kl pc ko pd kr pe kv pf kz pg ld oy oz pa pb bi translated"><a class="ae lu" href="https://www.kaggle.com/kcs93023/keras-sequential-conv1d-model-classification" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/kcs 93023/keras-sequential-conv1d-model-class ification</a></li></ol></div></div>    
</body>
</html>