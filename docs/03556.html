<html>
<head>
<title>The Kalman Filter and (Maximum) Likelihood</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卡尔曼滤波器和(最大)似然</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-kalman-filter-and-maximum-likelihood-9861666f6742?source=collection_archive---------3-----------------------#2019-06-06">https://towardsdatascience.com/the-kalman-filter-and-maximum-likelihood-9861666f6742?source=collection_archive---------3-----------------------#2019-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7f59" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解估计卡尔曼滤波器矩阵中未知参数的可能性的能力和局限性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/db41a0cecd7be3f5f4f6544ef3de761a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vlW7jdQ2HOPW6q5yJbbh4g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1 from <a class="ae ky" href="https://www.researchgate.net/publication/264392229_Ignoring_Imperfect_Detection_in_Biological_Surveys_Is_Dangerous_A_Response_to_'Fitting_and_Interpreting_Occupancy_Models'" rel="noopener ugc nofollow" target="_blank">Guillera-Arroita et al (2014)</a> depicting a log-likelihood surface. <a class="ae ky" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank">CC BY 4.0</a></figcaption></figure><p id="8b12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，您将:</p><ul class=""><li id="2fcb" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">使用 Python 模块<code class="fe me mf mg mh b"><a class="ae ky" href="https://www.statsmodels.org/stable/index.html" rel="noopener ugc nofollow" target="_blank">statsmodels</a></code>估计卡尔曼滤波器模型矩阵中的未知参数</li><li id="f6d8" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">根据卡尔曼滤波模型计算单个观测值的对数似然</li><li id="6919" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">在<code class="fe me mf mg mh b">statsmodels</code>中探索不同状态初始化选项的影响</li></ul><h1 id="dbb3" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">介绍</h1><p id="7721" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在 Kaggle 的<a class="ae ky" href="https://www.kaggle.com/c/web-traffic-time-series-forecasting" rel="noopener ugc nofollow" target="_blank">网络流量时间序列预测</a>比赛后，我立即意识到了卡尔曼滤波器的威力，这场比赛要求对数千个维基百科页面的未来网络流量进行预测。在这场竞赛中，像“中位数”这样简单的启发式算法很难被击败，我花在 ARIMA 和预言家模型上的精力毫无进展。我想知道传统的预测工具箱中是否有适合这项任务的东西。然后我看了一个帖子，作者是一个只知道叫“<em class="nk"> os、</em>”<a class="ae ky" href="https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43727#latest-492742" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">卡尔曼滤波器</strong> </a>第 8 名的用户。</p><p id="502c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自从这篇文章第一次被写出来，<em class="nk"> os </em>的身份现已被揭露为<a class="ae ky" href="https://medium.com/@oseiskar" rel="noopener"> Otto Seiskari </a>，他对这篇文章留下了一个启发性的回应。自从最初写这篇文章以来，我的观点也发生了变化，我将在最后解释这一点。</p><p id="3ff1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然 Otto 确实使用了中位数作为后备解决方案，但其核心方法是一个 8 态卡尔曼滤波器，对当地水平和每周季节性进行编码。Otto 还专门为这次比赛写了一个定制的 python 包，<a class="ae ky" href="https://github.com/oseiskar/simdkalman" rel="noopener ugc nofollow" target="_blank"> simdkalman </a>。型号规格直接来自<a class="ae ky" href="https://simdkalman.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> simdkalman 文档</a>，粗体字体稍有变化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/bc24cf4405a113c314cf4ab35729099e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/0*3kKePzX2WstpHmSa.png"/></div></figure><p id="bf84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nk"> q_t ~ N </em> (0，<strong class="lb iu"> Q </strong> ) <em class="nk">和 r_t ~ N </em> (0，<strong class="lb iu"> R </strong> ) <em class="nk">。在本文中，度量 y </em> _ <em class="nk"> t 是一个标量；r_ </em> t <em class="nk">的方差由 1x1 矩阵</em>R 表示。<em class="nk"> </em>这里，<strong class="lb iu"> x </strong> _ <em class="nk"> t </em>是未观测的“状态”向量，<em class="nk"> y_t </em>是时间段<em class="nk"> t </em>的观测测量。这些系统共同构成了许多时间序列数据集的有用表示。采用这种框架的一个好处是卡尔曼滤波器机制的可用性，用于计算可能性和创建一步预测等(在 R 的<a class="ae ky" href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html" rel="noopener ugc nofollow" target="_blank"> stats.arima </a>文档中搜索“卡尔曼”以查看其使用示例)。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="bd0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，<strong class="lb iu"> A </strong>和<strong class="lb iu"> H </strong>可以来自每周季节性、本地趋势等的标准“食谱”。、<strong class="lb iu"> Q </strong>和<strong class="lb iu"> R </strong>未知的可能性更大。Otto 没有对它们进行估计，而是使用默认选项来选择 Q 和 R，然后使用称为“平滑参数”的标量来调整一个矩阵与另一个矩阵的比例。</p><p id="8207" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我不能否认这种方法的有效性，但作为一名统计学家，“构建”Q 和<strong class="lb iu"> R </strong>并不适合我。然后我深入研究了 Roger Labbe 的基于 Jupyter 的文本，Python 中的<a class="ae ky" href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python" rel="noopener ugc nofollow" target="_blank">卡尔曼和贝叶斯滤波器</a>，发现它也在<a class="ae ky" href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/blob/9e4ac42f796a65e5d2baffd57a0fb88391a3d956/07-Kalman-Filter-Math.ipynb" rel="noopener ugc nofollow" target="_blank">卡尔曼滤波器数学</a>部分建议了一个类似的过程:“<em class="nk">在实践中</em>，”文本说，“<em class="nk">我们挑选一个数字，对数据进行模拟，并选择一个工作良好的值</em>。”</p><p id="745c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我听到了 15 年前教室里的另一个声音。一位名叫大卫·迪基的教授问道，他的话被反复引用，</p><p id="eb0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你喜欢数字七吗？三个怎么样？”</p><p id="f3e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[暂停以示强调]</p><p id="936b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们达成一致，在不知道数据在估算什么的情况下，不要从数据中计算任何东西。”</p><p id="b74e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最大似然估计，对于它可能具有的任何错误，是估计未知量的原则方法，并且似然是卡尔曼滤波操作的“副产品”。在下一节中，我们将用状态空间形式的 ARMA(2，1)探索 Python 的<code class="fe me mf mg mh b">statsmodels</code>中这些计算的中间过程。最后，我们将使用最大似然法，利用这种似然性得到对<strong class="lb iu"> A、H </strong>和<strong class="lb iu"> Q (R </strong>将是已知的)的估计。</p><h1 id="c7a9" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">卡尔曼滤波器和似然</h1><h2 id="16d8" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">顺序到达数据的可能性</h2><p id="9d96" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">以下联合 pdf 的产品扩展在时序情况下特别有用:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/b583d74620e560057370d8318f368a52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mN3IhW3zbrTQEAte.png"/></div></div></figure><p id="3793" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的情况下，有模型矩阵<strong class="lb iu"> A </strong>、<strong class="lb iu"> H </strong>、<strong class="lb iu"> Q </strong>和<strong class="lb iu"> R </strong>，它们都有潜在的未知成分。设θ包含构造这些矩阵所需的所有未知参数。那么可能性的对数变成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/58c70e719f94a541a586a4f99c61c701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hPU3SOB1Wy1ufRyD.png"/></div></div></figure><p id="64d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，有两个分析任务:</p><ul class=""><li id="5394" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">得到初始点的对数似然，<em class="nk"> y_0，</em></li><li id="c497" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">以过去为条件得到当前点的对数似然，<em class="nk">y _ t | y _</em>(<em class="nk">t</em>-1)<em class="nk">，…，y </em> _0 <em class="nk">。</em></li></ul><p id="a3a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下，卡尔曼滤波器的“滤波”操作给出了未知状态向量<strong class="lb iu">x</strong>_<em class="nk">t</em><strong class="lb iu"/>的条件分布，给出了到当前时间点<em class="nk"> t </em>的所有观察值<em class="nk"> y_t </em>(脚本<em class="nk"> Y_t </em>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/3a5b25e852dbe04f1c63ff47e09f66cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/0*VOZJj8dNXTpi13Xk.png"/></div></figure><p id="54f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" rel="noopener" target="_blank" href="/yet-another-kalman-filter-explanation-article-be0264d99937">的另一篇卡尔曼滤波器说明文章</a>中，该分布被表示为时间<em class="nk"> t </em>时未知状态的贝叶斯后验分布，但在这里，它被表示为潜在随机向量<strong class="lb iu"> x </strong> _t 的采样分布。</p><h2 id="611a" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">对可能性的条件贡献</h2><p id="ef67" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">一次解开测量模型会导致:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/6c9bb60d2d826f82bfe35d7b117e7ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d6sGiyNDJF1bO6jv.png"/></div></div></figure><p id="5922" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由此推出<em class="nk"> y_t | y_{t-1}，…，y_0 </em>一定是正规的，因为<strong class="lb iu"> x </strong> _{t-1} | <em class="nk"> y_{t-1}，…，y_0 </em> <strong class="lb iu"> </strong>是。它也给了我们一个计算这个条件正态的均值和方差的起点。条件意义仅仅是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/aa362c2b5ffb51eadbdd888641a485d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-NvV4MkvmlOVLToI.png"/></div></div></figure><p id="eb4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">条件方差是使用随机向量的标准方差公式得到的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/ac8520511bd7c1811cedfcf14753d3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*InWuIQxxVvzCJQp1.png"/></div></div></figure><p id="551c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下<strong class="lb iu"> Q </strong>、<strong class="lb iu"> R </strong>、<strong class="lb iu"> A </strong>和<strong class="lb iu"> H </strong>都是由θ中的参数完全指定的。这些参数需要数值最大化例程中的起始值，因此需要在似然最大化的每个阶段进行上述计算。</p><h2 id="ff0d" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">初始点似然贡献</h2><p id="d6c6" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">初始测量值的分布需要初始化状态均值向量和方差矩阵。在静态模型的情况下，存在基于系统长期运行行为的状态向量的正确初始(先验)均值和方差(参见华盛顿大学的<a class="ae ky" href="https://faculty.washington.edu/ezivot/econ584/notes/statespacemodels.pdf" rel="noopener ugc nofollow" target="_blank">笔记</a>第 2 页了解更多细节)。</p><p id="151b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于非静态模型，初始化更具挑战性。<code class="fe me mf mg mh b">statsmodels</code>提供的解决方案是“近似扩散”初始化，即零均值和非常大的协方差矩阵。统计学家不喜欢这个(“<em class="nk">这提出了数字问题，并且没有回答扩散结构</em> s 存在的问题”——<a class="ae ky" href="https://projecteuclid.org/euclid.aos/1176348139" rel="noopener ugc nofollow" target="_blank">Piet de Jong，1991 </a>)。虽然我们不会在这里处理它，<a class="ae ky" href="https://cran.r-project.org/web/packages/KFAS/vignettes/KFAS.pdf" rel="noopener ugc nofollow" target="_blank"> R 包 KFAS </a>确实提供了“精确的漫射初始化方法”为了简单明了，我们将在下面的例子中使用一个“已知的”初始化。</p><h1 id="9474" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">示例:状态空间形式的 ARMA 模型</h1><p id="4f01" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">因为本文将使用一个特定的 ARMA(1，2)模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/0b6dcb9efbad34fd8cc80599d4bec77d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QAo1hDHhl_TQ-mcU.png"/></div></div></figure><p id="17af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="nk"> ϵ_t </em> ~ N(0，1.2)为<em class="nk"> t </em> = 1，…，1000。</p><h2 id="339c" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">生成 ARMA(1，2)数据</h2><p id="0737" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">使用<code class="fe me mf mg mh b">statsmodels</code>模拟来自 ARMA(1，2)的数据很简单，但请注意 AR 和 MA 参数系数的输入方式:</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="57c4" class="nt mo it mh b gy oo op l oq or">import numpy as np<br/>from statsmodels.tsa.arima_process import ArmaProcess</span><span id="eb8b" class="nt mo it mh b gy os op l oq or">np.random.seed(20190529)<br/>ar1ma2 = ArmaProcess(ar=np.array([1, -.9]),<br/>                     ma=np.array([1, .2, -.1]))<br/>y = ar1ma2.generate_sample(nsample=1000, scale=1.2)</span></pre><p id="f6c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是一些观察结果:</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="c891" class="nt mo it mh b gy oo op l oq or">In [6]: print(y[0:5])<br/>[1.41505527 1.38650169 1.54085345 2.53833794 3.9467489 ]</span></pre><p id="f558" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">statsmodels</code> <code class="fe me mf mg mh b">ARMA</code>类有一个用于拟合 ARMA 模型的<code class="fe me mf mg mh b">fit()</code>方法:</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="bc39" class="nt mo it mh b gy oo op l oq or">from statsmodels.tsa.arima_model import ARMA</span><span id="00f4" class="nt mo it mh b gy os op l oq or">model = ARMA(y, (1, 2))<br/>model_fit = model.fit(trend='nc')</span></pre><p id="ef89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致输出(略有修改):</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="d305" class="nt mo it mh b gy oo op l oq or">In [13]: model_fit.summary()                                                                                                  Out[13]:                                                                                                                      &lt;class 'statsmodels.iolib.summary.Summary'&gt;                                                                                   """                                                                                                                                                         ARMA Model Results                                                                              ==================================================================<br/>Dep. Variable:   y            No. Observations:             1000<br/>Model:           ARMA(1, 2)   Log Likelihood               <strong class="mh iu">-1629.051</strong><br/>==================================================================<br/>            coef    std err   z       P&gt;|z|  [0.025  0.975]<br/>------------------------------------------------------------------<br/>ar.L1.y    <strong class="mh iu">0.9008</strong>    0.017   51.919   0.000   0.867  0.935<br/>ma.L1.y    <strong class="mh iu">0.1474</strong>    0.037    4.036   0.000   0.076  0.219<br/>ma.L2.y   <strong class="mh iu">-0.1360</strong>    0.037   -3.715   0.000  -0.208 -0.064<br/></span></pre><p id="552a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意样本的对数似然以及系数估计值(粗体)。</p><h2 id="16b6" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">状态空间形式的 ARMA(1，2)模型</h2><p id="27c7" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">为了在状态空间框架中得到这个 ARMA(1，2)模型，我们有许多选择。“哈维”表示法的好处(如沃顿商学院课堂讲稿第 8 页所述)在于它直接结合了 AR 和 MA 系数。对于我们的模型，这种表示是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/17969df7f1724206b43485f7ba6be1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AbfE2FLMKHAogJca.png"/></div></div></figure><p id="e9a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中,<em class="nk"> r </em> _t 已被明确替换为 0，以表明所有误差可变性均在状态项中处理，且</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/36d3afa392dd599a2ec0b4a424ca0274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wX2L8vloJVzhlq0N.png"/></div></div></figure><p id="cda0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管<strong class="lb iu"> R </strong>为零，这个 ARMA(1，2)模型适合卡尔曼滤波框架。下面是这个模型在<code class="fe me mf mg mh b">statsmodels</code>中的精确编码，尽管它比必要的更冗长:</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="89d6" class="nt mo it mh b gy oo op l oq or">from statsmodels.tsa.statespace.mlemodel import MLEModel<br/>from statsmodels.tsa.statespace.tools import (constrain_stationary_univariate, unconstrain_stationary_univariate)</span><span id="6af8" class="nt mo it mh b gy os op l oq or">class AR1MA2_verbose(MLEModel):<br/>    start_params = [.8, 0.24, -.11, 1.3]<br/>    param_names = ['ar1', 'ma1', 'ma2', 'sigma2']</span><span id="af19" class="nt mo it mh b gy os op l oq or">    def __init__(self, endog):<br/>        super().__init__(endog, k_states = 3)</span><span id="0eda" class="nt mo it mh b gy os op l oq or">        # Using the Harvey Representation<br/>        self['transition', 0, 1] = 1.0<br/>        self['transition', 1, 2] = 1.0</span><span id="2c85" class="nt mo it mh b gy os op l oq or">        self['design', 0, 0]   = 1.0</span><span id="7e53" class="nt mo it mh b gy os op l oq or">        self['selection', 0, 0] = 1.0<br/>        self['selection', 1, 1] = 1.0<br/>        self['selection', 2, 2] = 1.0</span><span id="e3db" class="nt mo it mh b gy os op l oq or">        self.initialize_known(np.array([0, 0, 0]), np.eye(3))<br/>        #self.initialize_stationary()<br/>        #self.initialize_approximate_diffuse()</span><span id="4817" class="nt mo it mh b gy os op l oq or">    def transform_params(self, params):<br/>        phi = constrain_stationary_univariate(params[0:1])<br/>        theta = constrain_stationary_univariate(params[1:3])<br/>        sigma2 = params[3] ** 2<br/>        return np.r_[phi, theta, sigma2]</span><span id="be88" class="nt mo it mh b gy os op l oq or">    def untransform_params(self, params):<br/>        phi = unconstrain_stationary_univariate(params[0:1])<br/>        theta = unconstrain_stationary_univariate(params[1:3])<br/>        sigma2 = params[3] ** 0.5<br/>        return np.r_[phi, theta, sigma2]<br/>    <br/>    def update(self, params, **kwargs):<br/>        params = super().update(params, **kwargs)</span><span id="e048" class="nt mo it mh b gy os op l oq or">        self['transition', 0, 0] = params[0]</span><span id="8f42" class="nt mo it mh b gy os op l oq or">        self['state_cov', 0, 0] = params[3] * 1.0<br/>        self['state_cov', 1, 0] = params[3] * params[1]<br/>        self['state_cov', 2, 0] = params[3] * params[2]<br/>        self['state_cov', 0, 1] = params[3] * params[1]<br/>        self['state_cov', 1, 1] = params[3] * params[1] ** 2<br/>        self['state_cov', 2, 1] = params[3] * params[1] * params[2]<br/>        self['state_cov', 0, 2] = params[3] * params[2]<br/>        self['state_cov', 1, 2] = params[3] * params[1] * params[2]<br/>        self['state_cov', 2, 2] = params[3] * params[2] ** 2<br/>        <br/>        pass # adding this to ease some cpaste issues</span></pre><p id="ce30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe me mf mg mh b">statsmodels</code>模型表示的文档在这里是<a class="ae ky" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.representation.Representation.html#statsmodels.tsa.statespace.representation.Representation" rel="noopener ugc nofollow" target="_blank"/>。下面是关于上述实现的一些注意事项。</p><ul class=""><li id="1137" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">注意“选择”矩阵被设置为等于<code class="fe me mf mg mh b">__init__</code>中的恒等式。按照<code class="fe me mf mg mh b">statsmodels</code> <a class="ae ky" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.representation.Representation.html" rel="noopener ugc nofollow" target="_blank">状态空间表示</a>的顺序，这个矩阵被状态空间误差预乘，默认情况下<strong class="lb iu">是零——</strong>如果你想要状态误差，你必须显式编码！。选择矩阵可以设置为(1 <em class="nk"> θ </em> _1 <em class="nk"> θ </em> _2)^T，使上述表示更简单。</li><li id="0a38" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated"><code class="fe me mf mg mh b">start_params</code>变量将初始化最大似然例程，但是我们也将在接下来的演示中使用它。注意数值接近真实。</li><li id="23dc" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">参数转换对于似然性演示来说不是必需的，但是对于似然性最大化例程来说却是必需的。</li><li id="5181" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">如前所述，我们将从潜在状态均值和协方差矩阵(零均值和恒等方差矩阵)的已知初始化开始，以演示可能性计算。</li></ul><h2 id="3b06" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">计算几个点的对数似然</h2><p id="009e" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们将首先创建一个卡尔曼滤波器模型的实例，并用初始值对其进行初始化。由于参数值包含了重建<strong class="lb iu"> A </strong>、<strong class="lb iu"> H </strong>、<strong class="lb iu"> Q </strong>和<strong class="lb iu"> R </strong>的所有必要信息，<code class="fe me mf mg mh b">statsmodels</code>的卡尔曼滤波器可以在任何进一步优化之前立即开始滤波。</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="da1c" class="nt mo it mh b gy oo op l oq or">kf_model_verbose = AR1MA2_verbose(y)<br/>filtered = kf_model_verbose.filter(kf_model_verbose.start_params)</span></pre><p id="21d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 iPython 中交互运行，很容易看到对于所有 1000 个时间点，<code class="fe me mf mg mh b">filtered_state</code>包含一个 3 维状态向量，而<code class="fe me mf mg mh b">filtered_state_cov</code>包含一个 3×3 协方差矩阵。</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="e7fd" class="nt mo it mh b gy oo op l oq or">In [25]: filtered.filtered_state.shape<br/>Out[25]: (3, 1000)</span><span id="2e41" class="nt mo it mh b gy os op l oq or">In [26]: filtered.filtered_state_cov.shape<br/>Out[26]: (3, 3, 1000)</span><span id="541e" class="nt mo it mh b gy os op l oq or">In [27]: kf_model_verbose.loglikeobs ... [0:3]<br/>Out[27]: array([-1.92012925, -1.34946888, -1.37622846])</span></pre><p id="40df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一行显示了根据<code class="fe me mf mg mh b">statsmodels</code>的前三次测量的对数似然贡献。这些都是我们想要搭配的。</p><p id="cc36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在将使用第<em class="nk">节中为顺序到达数据</em>建立的公式。矩阵<strong class="lb iu"> A </strong>、<strong class="lb iu"> H </strong>、<strong class="lb iu"> Q </strong>和<strong class="lb iu"> R </strong>易于访问，矩阵乘积<strong class="lb iu"> HA </strong>被计算并存储在下面的<code class="fe me mf mg mh b">HA</code>中:</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="a52e" class="nt mo it mh b gy oo op l oq or">A = kf_model_verbose['transition', 0:, 0:]<br/>H = kf_model_verbose['design', 0:, 0:]<br/>Q = kf_model_verbose['state_cov', 0:, 0:]<br/>R = kf_model_verbose['obs_cov', 0:, 0:]</span><span id="cbc7" class="nt mo it mh b gy os op l oq or">HA = np.matmul(H, A)</span></pre><p id="7afc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了匹配似然向量中的第一个数字-1.9201，首先注意<strong class="lb iu"> μ </strong>和<strong class="lb iu">σ<em class="nk"/></strong>在零向量和单位矩阵处被初始化。测量向量只取状态向量的第一个元素，没有额外的误差项(<strong class="lb iu"> R </strong>为零)，因此第一个观察值<code class="fe me mf mg mh b">y[0]</code>与一个<em class="nk"> N </em> (0 <em class="nk">，</em> 1)分布进行比较。</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="42be" class="nt mo it mh b gy oo op l oq or">from scipy.stats import norm<br/>np.log(norm.pdf(y[0], 0, 1))</span><span id="a791" class="nt mo it mh b gy os op l oq or"><br/>Out[36]: <strong class="mh iu">-1.9201</strong>292483430195</span></pre><p id="a99c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于 python 从零索引时间开始，<strong class="lb iu"> μ </strong> _0 和<strong class="lb iu">σ</strong>_ 0 实际上是给定初始数据测量的第一个更新的状态均值和方差矩阵。在可能性推导中的计算之后，</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="93bc" class="nt mo it mh b gy oo op l oq or">mu_0 = filtered.filtered_state[0:, 0]<br/>Sigma_0 = filtered.filtered_state_cov[0:, 0:, 0]</span><span id="a67a" class="nt mo it mh b gy os op l oq or">E_alpha_1 = np.matmul(np.matmul(H, A), mu_0)<br/>V_alpha_1 = (np.matmul(np.matmul(HA, Sigma_0), np.transpose(HA)) +<br/>             np.matmul(np.matmul(H, Q), np.transpose(H)) + R)</span><span id="93a2" class="nt mo it mh b gy os op l oq or">np.log(norm.pdf(y[1], E_alpha_1, np.sqrt(V_alpha_1)))</span></pre><p id="0eb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果为<code class="fe me mf mg mh b">array([[-1.34946888]])</code>，匹配第二次测量的对数似然。再来一轮应该能让人相信我们已经掌握了模式。</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="6d22" class="nt mo it mh b gy oo op l oq or">mu_1 = filtered.filtered_state[0:, 1]<br/>Sigma_1 = filtered.filtered_state_cov[0:, 0:, 1]</span><span id="2d41" class="nt mo it mh b gy os op l oq or">E_alpha_2 = np.matmul(np.matmul(H, A), mu_1)<br/>V_alpha_2 = (np.matmul(np.matmul(HA, Sigma_1), np.transpose(HA)) +<br/>             np.matmul(np.matmul(H, Q), np.transpose(H)) + R)</span><span id="17d3" class="nt mo it mh b gy os op l oq or">np.log(norm.pdf(y[2], E_alpha_2, np.sqrt(V_alpha_2)))</span><span id="9af8" class="nt mo it mh b gy os op l oq or">Out[46]: array([[<strong class="mh iu">-1.37622846</strong>]])</span></pre><p id="dbf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还是那句话，匹配。如果我们对 1000 次测量的对数似然性求和，我们得到的结果会比我们在 ARMA 拟合中看到的结果稍小(即更负)，这是意料之中的，因为我们的似然性评估不是最大似然估计。</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="f4e7" class="nt mo it mh b gy oo op l oq or">np.sum(kf_model_verbose.loglikeobs(kf_model_verbose.start_params))<br/>Out[47]: -1655.0364388567427</span></pre><h2 id="445e" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">未知参数的最大似然估计</h2><p id="1a4a" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">上面创建的 Kalman Filter 对象一直在等待最大似然估计(否则它为什么会继承一个名为“MLEModel”的类)。它只需要运行对象的<code class="fe me mf mg mh b">fit()</code>方法:</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="5bbf" class="nt mo it mh b gy oo op l oq or">kf_model_verbose_fit = kf_model_verbose.fit()<br/>kf_model_verbose_fit.summary()</span></pre><p id="a2be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">摘要提供了(略有修改的输出):</p><pre class="kj kk kl km gt ok mh ol om aw on bi"><span id="ba6c" class="nt mo it mh b gy oo op l oq or">Statespace Model Results                                                                                    ==================================================================<br/>Dep. Variable:    y                 No. Observations:     1000<br/>Model:            AR1MA2_verbose    Log Likelihood       <strong class="mh iu">-1629.327</strong><br/><br/>==================================================================<br/>          coef    std err    z       P&gt;|z|  [0.025   0.975]<br/>------------------------------------------------------------------<br/>ar1      <strong class="mh iu">0.9016</strong>    0.018   50.742    0.000   0.867   0.936<br/>ma1      <strong class="mh iu">0.1472</strong>    0.038    3.900    0.000   0.073   0.221<br/>ma2     <strong class="mh iu">-0.1366</strong>    0.037   -3.686    0.000  -0.209  -0.064<br/>sigma2   <strong class="mh iu">1.5219</strong>    0.071   21.542    0.000   1.383   1.660</span></pre><p id="e9dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，参数估计和对数似然接近 ARMA 估计，但不精确。这是由于已知的初始化。作为练习，运行以下两种变体，这将在讨论部分讨论:</p><ul class=""><li id="524f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">注释掉已知的初始化，取消静态初始化的注释，然后重新运行。</li><li id="774d" class="lv lw it lb b lc mi lf mj li mk lm ml lq mm lu ma mb mc md bi translated">注释掉静态初始化，取消注释“近似扩散”初始化，然后重新运行。</li></ul><h1 id="e539" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">讨论</h1><p id="cfd3" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">从上面的练习中，您应该已经观察到，您可以将 ARMA 模型的对数似然和参数估计与平稳初始化精确匹配。这并不奇怪，因为针对<code class="fe me mf mg mh b">ARMA</code>类的<code class="fe me mf mg mh b">fit()</code>方法的<a class="ae ky" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARMA.html" rel="noopener ugc nofollow" target="_blank">文档</a>说，它通过卡尔曼滤波器使用精确的最大似然来“f <em class="nk">它的 ARMA(p，q)模型。</em>”</p><p id="f3ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，近似的漫射初始化导致与其精确的和已知的对应物相似的估计，但是它是一个固定的模型。初始对数似然值要小得多(&lt; -7) so the total log-likelihood of the sample is a bit more negative. A “burn in” option exists to skip these first few observations in computing the likelihood by setting  【T2】  in the class’s  【T3】  function.</p><p id="9384" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Once the machinery is in place to get the filtered state mean and variance, the likelihood of each measurement consists of only a few linear operations. This does mean a double loop will be necessary when maximizing the likelihood numerically, which explains why fitting even a simple ARMA(1, 2) model slows down when the number of observations gets moderately large.</p><h2 id="8166" class="nt mo it bd mp nu nv dn mt nw nx dp mx li ny nz mz lm oa ob nb lq oc od nd oe bi translated">Otto’s response</h2><p id="1aaa" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">My original conclusion to this article concerned the practical methods of constructing a filter that do not involve formal estimation like presented here. I said that I might use one of these methods, if I could ever stop hearing the question: “<em class="nk">你喜欢数字 7 吗？</em>”</p><p id="9d28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那以后，最初的<em class="nk">操作系统</em>做出了回应，解释了为什么一种更基于工程的方法可能优于最大似然法。作为一个固执的人，我不得不在自己的工作中经历最大似然估计的困难(就像这个<a class="ae ky" href="https://nousot.com/blog/how-we-won-gold/" rel="noopener ugc nofollow" target="_blank">最近的竞赛</a>)来充分理解这些论点。奥托·塞斯卡里解释道:</p><blockquote class="ou ov ow"><p id="1229" class="kz la nk lb b lc ld ju le lf lg jx lh ox lj lk ll oy ln lo lp oz lr ls lt lu im bi translated">“MLE 方法找到的解决方案的质量取决于模型描述数据的准确程度。卡尔曼滤波器通常适用于理论上模型假设非常糟糕的问题:噪声可能是非高斯的、非独立的，并且选择的隐藏状态模型与它应该建模的相比可能非常简单。然而，在选择某些参数的情况下，滤波器仍然可以很好地工作，但是实际中的最佳参数可能与最大似然估计有很大不同。”</p></blockquote><p id="35fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我仍然喜欢使用最大似然估计来估计卡尔曼滤波器模型的未知参数，但是我也接受使用基于维持的调优，特别是当主要目标是预测时。在这两种情况下，我希望读者已经了解了更多关于卡尔曼滤波器的潜在概率基础，以及在没有使用最大似然估计的验证集的情况下什么是可能的。</p></div></div>    
</body>
</html>