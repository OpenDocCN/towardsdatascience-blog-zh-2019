# å®šä¹‰åœ¨å›¾ä¸Šçš„å„å‘å¼‚æ€§ã€åŠ¨æ€ã€è°±å’Œå¤šå°ºåº¦æ»¤æ³¢å™¨

> åŸæ–‡ï¼š<https://towardsdatascience.com/tutorial-on-graph-neural-networks-for-computer-vision-and-beyond-part-2-be6d71d70f49?source=collection_archive---------12----------------------->

## ä½œä¸ºâ€œè®¡ç®—æœºè§†è§‰å›¾å½¢ç¥ç»ç½‘ç»œæ•™ç¨‹â€çš„ä¸€éƒ¨åˆ†

æˆ‘å°†é€šè¿‡ä½¿ç”¨ Python å’Œ PyTorch æå–å…³é”®æ€æƒ³å¹¶è§£é‡Šé‡Œç¨‹ç¢‘æ–¹æ³•èƒŒåçš„ç®€å•ç›´è§‰ï¼Œæ¥æ¦‚è¿°é‡è¦çš„å›¾å½¢ç¥ç»ç½‘ç»œå·¥ä½œã€‚æœ¬å¸–ç»§ç»­ [*æˆ‘çš„æ•™ç¨‹ç¬¬ä¸€éƒ¨åˆ†*](https://medium.com/p/3d9fada3b80d) *ã€‚*

![](img/b9072358c1b452466f6b0ab17823e8df.png)

Graph of Graph Neural Network (GNN) and related works. Some other important works and edges are not shown to avoid further clutter. For example, there is a large body of works on dynamic graphs that deserve a separate overview. Best viewed on a very wide screen in color.

# 20 å¤šå¹´çš„å›¾å½¢ç¥ç»ç½‘ç»œ

åœ¨ä¸Šé¢çš„â€œ**å›¾ç¥ç»ç½‘ç»œ(GNN)çš„å›¾åŠç›¸å…³è‘—ä½œ**â€ä¸­ï¼Œæˆ‘è¡¥å……äº†æˆ‘æœ€è¿‘ä¸€å¹´æ¥è§¦åˆ°çš„å…³äºå›¾çš„è®ºæ–‡ã€‚åœ¨è¯¥å›¾ä¸­ï¼Œä¸¤ä¸ªä½œå“ä¹‹é—´çš„æœ‰å‘è¾¹è¡¨ç¤ºä¸€ç¯‡è®ºæ–‡åŸºäºå¦ä¸€ç¯‡è®ºæ–‡(å°½ç®¡æ²¡æœ‰å¿…è¦å¼•ç”¨å®ƒ),ä½œå“çš„é¢œè‰²è¡¨ç¤º:

*   çº¢è‰²â€” **å…‰è°±æ–¹æ³•**(éœ€è¦æ‹‰æ™®æ‹‰æ–¯å›¾çš„ç‰¹å¾åˆ†è§£ï¼Œè¿™å°†åœ¨ä¸‹é¢è§£é‡Š)
*   ç»¿è‰²â€”åœ¨**ç©ºé—´åŸŸ**ä¸­å·¥ä½œçš„æ–¹æ³•(ä¸éœ€è¦æ‹‰æ™®æ‹‰æ–¯å›¾çš„ç‰¹å¾åˆ†è§£)
*   è“è‰²â€”ç›¸å½“äºå…‰è°±æ–¹æ³•ï¼Œä½†ä¸éœ€è¦ç‰¹å¾åˆ†è§£(å› æ­¤ï¼Œå®é™…ä¸Šæ˜¯ç©ºé—´æ–¹æ³•)
*   é»‘è‰²â€”â€”æ˜¯ GNNs çš„è¡¥å……æ–¹æ³•ï¼Œä¸ GNN æœ¬èº«çš„é€‰æ‹©æ— å…³(ä¾‹å¦‚ï¼Œé›†ä¸­æ³¨æ„åŠ›)ã€‚

è¯·æ³¨æ„ï¼Œä¸ºäº†é¿å…æ··ä¹±ï¼Œå…¶ä»–ä¸€äº›é‡è¦çš„ä½œå“å’Œè¾¹ç¼˜æ²¡æœ‰æ˜¾ç¤ºå‡ºæ¥ï¼Œåªæœ‰ä¸€å°éƒ¨åˆ†ä½œå“ï¼Œç”¨ç²—ä½“æ¡†çªå‡ºæ˜¾ç¤ºçš„**å°†åœ¨æœ¬æ–‡ä¸­ä»‹ç»ã€‚å£°æ˜:æˆ‘ä»ç„¶åœ¨é‚£é‡Œæ‰¾åˆ°äº†æŒ¤å‹æˆ‘ä»¬è‡ªå·±æœ€è¿‘ä½œå“çš„ç©ºé—´ğŸ˜Šã€‚**

å¤§å¤šæ•°é‡è¦çš„æ–¹æ³•éƒ½åŒ…å«åœ¨è¿™ä¸ªéè¯¦å°½çš„ä½œå“åˆ—è¡¨ä¸­:

*   Nicket ç­‰äººï¼Œ2015 å¹´ï¼Œ[çŸ¥è¯†å›¾çš„å…³ç³»æœºå™¨å­¦ä¹ ç»¼è¿°](https://arxiv.org/abs/1503.00759)
*   å¸ƒæœ—æ–¯å¦ç­‰ï¼Œ2016ï¼Œ[å‡ ä½•æ·±åº¦å­¦ä¹ :è¶…è¶Šæ¬§å‡ é‡Œå¾·æ•°æ®](https://arxiv.org/abs/1611.08097)
*   æ±‰å¯†å°”é¡¿ç­‰ï¼Œ2017ï¼Œ[å›¾ä¸Šçš„è¡¨å¾å­¦ä¹ :æ–¹æ³•ä¸åº”ç”¨](https://arxiv.org/abs/1709.05584)
*   Kipf ç­‰äººï¼Œ2018ï¼Œ[ç»“æ„åŒ–æ·±åº¦æ¨¡å‹:å›¾å½¢ä¸Šçš„æ·±åº¦å­¦ä¹ åŠè¶…è¶Š](http://tkipf.github.io/misc/SlidesCambridge.pdf)ï¼Œæ¼”ç¤ºå¹»ç¯ç‰‡ã€‚
*   å·´å¡”æ ¼åˆ©äºšç­‰äººï¼Œ2018 å¹´ï¼Œ[å…³ç³»å½’çº³åå·®ã€æ·±åº¦å­¦ä¹ å’Œå›¾ç½‘ç»œ](https://arxiv.org/abs/1806.01261)
*   å¼ ç­‰ï¼Œ2018 [å…³äºå›¾çš„æ·±åº¦å­¦ä¹ :ä¸€ä¸ªè°ƒæŸ¥](https://arxiv.org/abs/1812.04202)
*   å‘¨ç­‰ï¼Œ2018ï¼Œ[å›¾ç¥ç»ç½‘ç»œ:æ–¹æ³•ä¸åº”ç”¨ç»¼è¿°](https://arxiv.org/abs/1812.08434)
*   å´ç­‰ï¼Œ2019ï¼Œã€å›¾ç¥ç»
    ç½‘ç»œç»¼åˆç ”ç©¶ã€‘
*   Petar VeliÄ koviÄ‡ï¼Œ2019ï¼Œ[æ·±åº¦ç¥ç»ç½‘ç»œä¸­ç»“æ„çš„å¤å…´](https://www.repository.cam.ac.uk/handle/1810/292230)ï¼Œåšå£«è®ºæ–‡ã€‚
*   å°¼æ™®æ–¯å’Œ CVPR [è§†é¢‘æ•™ç¨‹](https://sungsoo.github.io/2018/02/01/geometric-deep-learning.html)

ä½¿ç”¨ç¥ç»ç½‘ç»œå¯¹å›¾è¿›è¡Œåˆ†ç±»çš„ç¬¬ä¸€é¡¹å·¥ä½œä¼¼ä¹æ˜¯ç”±[äºšå†æ¡‘å¾·ç½—Â·æ–¯ä½©å°”æœè’‚å’Œå®‰ä¸œå°¼å¨œÂ·æ–¯å¡”ä¸½å¡”åœ¨](https://ieeexplore.ieee.org/document/572108)å‘è¡¨çš„ä¸€ç¯‡ **1997** å…³äºâ€œç”¨äºç»“æ„åˆ†ç±»çš„ç›‘ç£ç¥ç»ç½‘ç»œâ€çš„è®ºæ–‡ã€‚

![](img/39d9bfa336852a10f6a74c4468e4ecaf.png)

A figure from ([Sperduti & Starita, 1997](https://ieeexplore.ieee.org/document/572108)), which is strikingly similar to what we are doing now, after more than 20 years.

> [æ–¯ç€æœè’‚&æ–¯å¡”é‡Œå¡”ï¼Œ1997](https://ieeexplore.ieee.org/document/572108) :â€œåˆ°ç›®å‰ä¸ºæ­¢ï¼Œç¥ç»ç½‘ç»œå·²ç»è¢«ç”¨äºå¯¹éç»“æ„åŒ–æ¨¡å¼å’Œåºåˆ—è¿›è¡Œåˆ†ç±»ã€‚ç„¶è€Œï¼Œæ ‡å‡†çš„ç¥ç»ç½‘ç»œå’Œç»Ÿè®¡æ–¹æ³•é€šå¸¸è¢«è®¤ä¸ºåœ¨å¤„ç†å¤æ‚ç»“æ„æ—¶æ˜¯ä¸å¤Ÿçš„ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŸºäºç‰¹å¾çš„æ–¹æ³•ã€‚â€

è‡ª 1997 å¹´ä»¥æ¥ï¼Œå…³äºä»å›¾ä¸­å­¦ä¹ çš„å¤§é‡å·¥ä½œå·²ç»åœ¨å¦‚æ­¤å¤šçš„ä¸åŒæ–¹å‘ä¸Šå¢é•¿ï¼Œå¦‚æœæ²¡æœ‰ä¸€äº›æ™ºèƒ½çš„è‡ªåŠ¨åŒ–ç³»ç»Ÿï¼Œå¾ˆéš¾ä¿æŒè·Ÿè¸ªã€‚æˆ‘ç›¸ä¿¡æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨åŸºäºç¥ç»ç½‘ç»œçš„æ–¹æ³•(åŸºäºæˆ‘çš„æ•™ç¨‹ç¬¬ä¸€éƒ¨åˆ†*ã€‘*ä¸­çš„ [*ä¸­è§£é‡Šçš„å…¬å¼(2)ã€‘,æˆ–è€…ç¥ç»ç½‘ç»œå’Œå…¶ä»–æ–¹æ³•çš„æŸç§ç»„åˆã€‚*](https://medium.com/p/3d9fada3b80d)

![](img/ff77f544d880031dbcd0027950da2b13.png)

Graph neural layerâ€™s formula (2) from [*the first part of my tutorial*](https://medium.com/p/3d9fada3b80d) *that we will also need in this part. Keep in mind, that if we need to compute a specific loss for the output features or if we need to stack these layers, we apply some activation like ReLU or Softmax.*

å›é¡¾ä¸€ä¸‹æˆ‘ä»¬åœ¨ç¬¬ä¸€éƒ¨åˆ†ä¸­ä½¿ç”¨çš„ç¬¦å·ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›æ— å‘å›¾ *G* å’Œ *N* èŠ‚ç‚¹ã€‚è¯¥å›¾ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰ä¸€ä¸ª *C* ç»´ç‰¹å¾å‘é‡ï¼Œæ‰€æœ‰èŠ‚ç‚¹çš„ç‰¹å¾éƒ½è¡¨ç¤ºä¸ºä¸€ä¸ª *N* Ã— *C* ç»´çŸ©é˜µ *Xâ½Ë¡â¾.*åœ¨ä¸€ä¸ªå…¸å‹çš„å›¾ç½‘ç»œä¸­ï¼Œæ¯”å¦‚ GCN ( [Kipf & Wellingï¼ŒICLRï¼Œ2017](https://arxiv.org/abs/1609.02907) )ï¼Œæˆ‘ä»¬å°†è¿™äº›ç‰¹å¾ *Xâ½Ë¡â¾* é¦ˆé€åˆ°ä¸€ä¸ªå…·æœ‰ *C* Ã— *F* ç»´å¯è®­ç»ƒæƒé‡ *Wâ½Ë¡â¾* çš„å›¾ç¥ç»å±‚ï¼Œè¿™æ ·è¯¥å±‚çš„è¾“å‡ºå°±æ˜¯ä¸€ä¸ª *N* Ã— *F* çŸ©é˜µ*ğ“æ˜¯ä¸€ä¸ª *N* Ã— *N* çŸ©é˜µï¼Œå…¶ä¸­æ¡ç›®ğ“ *áµ¢â±¼* æŒ‡ç¤ºèŠ‚ç‚¹ *i* æ˜¯å¦è¿æ¥åˆ°èŠ‚ç‚¹ *j* çš„ç›¸é‚»èŠ‚ç‚¹*ã€‚è¿™ä¸ªçŸ©é˜µè¢«ç§°ä¸º*é‚»æ¥çŸ©é˜µ*ã€‚æˆ‘ä½¿ç”¨ğ“è€Œä¸æ˜¯æ™®é€šçš„ *A* æ¥å¼ºè°ƒè¿™ä¸ªçŸ©é˜µå¯ä»¥è¢«*è§„èŒƒåŒ–*ä»¥ä¿ƒè¿›æ·±å±‚ç½‘ç»œä¸­çš„ç‰¹å¾ä¼ æ’­ã€‚å‡ºäºæœ¬æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾ğ“= *A* ï¼Œå³çŸ©é˜µä¹˜ç§¯ğ“ *Xâ½Ë¡â¾* çš„ç¬¬ *i* è‡³ç¬¬*t51ã€‘è¡Œå°†åŒ…å«èŠ‚ç‚¹ *i* é‚»å±…çš„ç‰¹å¾çš„æ€»å’Œã€‚***

åœ¨æœ¬éƒ¨åˆ†æ•™ç¨‹çš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘å°†ç®€è¦è§£é‡Šæ¦‚è§ˆå›¾ä¸­ç”¨**ç²—ä½“**æ¡†æ˜¾ç¤ºçš„æˆ‘é€‰æ‹©çš„ä½œå“ã€‚æˆ‘æ¨è[å¸ƒæœ—æ–¯å¦ç­‰äººçš„è¯„è®º](https://arxiv.org/abs/1611.08097)è¿›è¡Œæ›´å…¨é¢å’Œæ­£å¼çš„åˆ†æã€‚

è¯·æ³¨æ„ï¼Œå°½ç®¡æˆ‘åœ¨ä¸‹é¢æ·±å…¥ç ”ç©¶äº†**å…‰è°±å›¾å·ç§¯**çš„ä¸€äº›æŠ€æœ¯ç»†èŠ‚ï¼Œä½†æœ€è¿‘çš„è®¸å¤šä½œå“(å¦‚[å¾ç­‰äººçš„ GINï¼Œ2019](https://arxiv.org/abs/1810.00826) )éƒ½æ˜¯åœ¨æ²¡æœ‰å…‰è°±å·ç§¯çš„æƒ…å†µä¸‹å»ºç«‹çš„ï¼Œå¹¶åœ¨ä¸€äº›ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå¾ˆå¥½çš„ç»“æœã€‚ç„¶è€Œï¼Œäº†è§£è°±å·ç§¯çš„å·¥ä½œåŸç†ä»ç„¶æœ‰åŠ©äºç†è§£å’Œé¿å…å…¶ä»–æ–¹æ³•çš„æ½œåœ¨é—®é¢˜ã€‚

# **1ã€‚å…‰è°±å›¾å·ç§¯**

[å¸ƒé²çº³ç­‰äººï¼Œ2014ï¼ŒICLR 2014](https://arxiv.org/abs/1312.6203)

æˆ‘åœ¨æˆ‘çš„[å¦ä¸€ç¯‡æ–‡ç« ](/spectral-graph-convolution-explained-and-implemented-step-by-step-2e495b57f801)ä¸­è¯¦ç»†è§£é‡Šäº†è°±å›¾å·ç§¯ã€‚

ä¸ºäº†è¿™éƒ¨åˆ†æ•™ç¨‹çš„ç›®çš„ï¼Œæˆ‘åœ¨è¿™é‡Œç®€å•æ€»ç»“ä¸€ä¸‹ã€‚è°±å›¾å·ç§¯çš„æ­£å¼å®šä¹‰ä¸ä¿¡å·/å›¾åƒå¤„ç†ä¸­çš„[å·ç§¯å®šç†](https://en.wikipedia.org/wiki/Convolution_theorem)éå¸¸ç›¸ä¼¼ï¼Œå¯ä»¥å†™æˆ:

![](img/d84facfea1cd1a9c42cd1a0dbf5a468b.png)

Spectral graph convolution, where âŠ™ means element-wise multiplication.

å…¶ä¸­ *V* ä¸ºç‰¹å¾å‘é‡ï¼Œ*Î»*ä¸º [**å›¾æ‹‰æ™®æ‹‰æ–¯**](https://en.wikipedia.org/wiki/Laplacian_matrix) *L* çš„ç‰¹å¾å€¼ï¼Œé€šè¿‡ç‰¹å¾åˆ†è§£å¯ä»¥æ‰¾åˆ°:*l*=*vÎ»váµ€ï¼›w*_ å…‰è°±æ»¤å…‰ç‰‡ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘å°†å‡è®¾â€œ*å¯¹ç§°å½’ä¸€åŒ–æ‹‰æ™®æ‹‰æ–¯ç®—å­*â€ã€‚å®ƒæ˜¯åŸºäºå›¾çš„é‚»æ¥çŸ©é˜µ *A* ä¸Šçš„*å”¯ä¸€*æ¥è®¡ç®—çš„ï¼Œè¿™å¯ä»¥ç”¨å‡ è¡Œ Python ä»£ç æ¥å®Œæˆï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
**# Computing the graph Laplacian
# A is an adjacency matrix** import numpy as npN = A.shape[0] **# number of nodes in a graph**
D = np.sum(A, 0) **# node degrees**
D_hat = np.diag((D + 1e-5)**(-0.5)) **# normalized node degrees**
L = np.identity(N) â€” np.dot(D_hat, A).dot(D_hat) **# Laplacian**
```

è¿™é‡Œï¼Œæˆ‘ä»¬å‡è®¾ *A* æ˜¯å¯¹ç§°çš„ï¼Œå³ *A* = *A* áµ€ï¼Œå¹¶ä¸”æˆ‘ä»¬çš„å›¾æ˜¯æ— å‘å›¾ï¼Œå¦åˆ™èŠ‚ç‚¹åº¦ä¸æ˜¯æ˜ç¡®å®šä¹‰çš„ï¼Œå¹¶ä¸”å¿…é¡»åšå‡ºä¸€äº›å‡è®¾æ¥è®¡ç®—æ‹‰æ™®æ‹‰æ–¯ç®—å­ã€‚åœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ çš„èƒŒæ™¯ä¸‹ï¼Œå›¾å½¢æ‹‰æ™®æ‹‰æ–¯å®šä¹‰äº†å¦‚æœæˆ‘ä»¬ä»¥å…¬å¼(2)çš„å½¢å¼å †å å‡ ä¸ªå›¾å½¢ç¥ç»å±‚ï¼ŒèŠ‚ç‚¹ç‰¹å¾å°†å¦‚ä½•æ›´æ–°ã€‚

æ‰€ä»¥ï¼Œç»™å®šå›¾çš„æ‹‰æ™®æ‹‰æ–¯ *L* ï¼ŒèŠ‚ç‚¹ç‰¹å¾ *X* å’Œè¿‡æ»¤å™¨ *W* _spectralï¼Œåœ¨ Python **å›¾ä¸Šçš„è°±å·ç§¯**çœ‹èµ·æ¥éå¸¸ç®€å•:

```
**# Spectral convolution on graphs
# X is an *NÃ—1 matrix of 1-dimensional node features*** **# L** **is an** ***N******Ã—N* graph Laplacian computed above
# W_spectral are** ***N******Ã—******F weights (filters) that we want to train*** from scipy.sparse.linalg import eigsh **# assumes *L* to be symmetric***Î›**,V* = eigsh(L,k=20,which=â€™SMâ€™) **#** **eigen-decomposition (i.e. find *Î›******,V)***
X_hat = V.T.dot(X) **# *20*****Ã—*****1* node features in the "spectral" domain**
W_hat = V.T.dot(W_spectral)  **# 20Ã—*F* filters in the** **"spectral" domain**
Y = V.dot(X_hat * W_hat)  **# *N******Ã—******F* result of convolution**
```

å…¶ä¸­æˆ‘ä»¬å‡è®¾æˆ‘ä»¬çš„èŠ‚ç‚¹ç‰¹å¾ *Xâ½Ë¡â¾* æ˜¯ä¸€ç»´çš„ï¼Œä¾‹å¦‚ m åƒç´ ï¼Œä½†æ˜¯å®ƒå¯ä»¥æ‰©å±•åˆ° *C* ç»´çš„æƒ…å†µ:æˆ‘ä»¬å°†åªéœ€è¦å¯¹æ¯ä¸ª*é€šé“*é‡å¤è¿™ä¸ªå·ç§¯ï¼Œç„¶ååƒåœ¨ä¿¡å·/å›¾åƒå·ç§¯ä¸­ä¸€æ ·å¯¹ *C* æ±‚å’Œã€‚

å…¬å¼(3)æœ¬è´¨ä¸Šä¸ä½¿ç”¨å‚…ç«‹å¶å˜æ¢çš„è§„åˆ™ç½‘æ ¼ä¸Šçš„ä¿¡å·çš„[é¢‘è°±å·ç§¯](https://en.wikipedia.org/wiki/Convolution_theorem)ç›¸åŒï¼Œå› æ­¤ä¸ºæœºå™¨å­¦ä¹ äº§ç”Ÿäº†ä¸€äº›é—®é¢˜:

1.  å¯è®­ç»ƒæƒé‡(æ»¤æ³¢å™¨)çš„ç»´æ•°å–å†³äºå›¾ä¸­èŠ‚ç‚¹çš„æ•°é‡ï¼›
2.  *W_* å…‰è°±ä¹Ÿå–å†³äºå›¾ç»“æ„ä¸­ç¼–ç çš„ç‰¹å¾å‘é‡ *V.*

è¿™äº›é—®é¢˜é˜»ç¢äº†æ‰©å±•åˆ°å…·æœ‰å¯å˜ç»“æ„çš„å¤§å‹å›¾å½¢çš„æ•°æ®é›†ã€‚

ä¸ºäº†è§£å†³ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œ[å¸ƒé²çº³ç­‰äºº](https://arxiv.org/abs/1312.6203)æå‡ºåœ¨è°±åŸŸå¯¹*æ»¤æ³¢å™¨è¿›è¡Œå¹³æ»‘*ï¼Œæ ¹æ®è°±ç†è®ºä½¿*æ»¤æ³¢å™¨åœ¨ç©ºé—´åŸŸæ›´åŠ å±€éƒ¨åŒ–*ã€‚å…¶æ€æƒ³æ˜¯ï¼Œæ‚¨å¯ä»¥å°†å…¬å¼(3)ä¸­çš„æ»¤æ³¢å™¨ *W_* é¢‘è°±è¡¨ç¤ºä¸ºğ¾é¢„å®šä¹‰å‡½æ•°(å¦‚æ ·æ¡å‡½æ•°)çš„å’Œï¼Œå¹¶ä¸”æˆ‘ä»¬å­¦ä¹ è¿™ä¸ªå’Œçš„ *W* çš„ *N* å€¼ï¼Œè€Œä¸æ˜¯å­¦ä¹ è¿™ä¸ªå’Œçš„ *K* ç³»æ•° *Î±* :

![](img/e4bc0d9cd74eadddfdd33be6cd0db3c0.png)

We can approximate our N dimensional filter*W_*spectral as a finite sum of *K* functions f, such as splines shown below. So, instead of learning N values of *W_*spectral, we can learn K coefficients (alpha) of those functions; it becomes efficient when K << N.

è™½ç„¶ *fk* çš„ç»´æ•°ç¡®å®å–å†³äºèŠ‚ç‚¹ *N* çš„æ•°é‡ï¼Œä½†æ˜¯è¿™äº›å‡½æ•°æ˜¯å›ºå®šçš„ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸å­¦ä¹ å®ƒä»¬ã€‚æˆ‘ä»¬å”¯ä¸€çŸ¥é“çš„æ˜¯ç³»æ•° *Î±* ï¼Œå› æ­¤ *W_* å…‰è°±ä¸å†ä¾èµ–äº *N* ã€‚ä¸ºäº†ä½¿æˆ‘ä»¬åœ¨å…¬å¼(4)ä¸­çš„è¿‘ä¼¼åˆç†ï¼Œæˆ‘ä»¬å¸Œæœ› *K* < < *N* å°†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ä» *N* å‡å°‘åˆ° *K* ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œä½¿å…¶ç‹¬ç«‹äº *N* ï¼Œä»¥ä¾¿æˆ‘ä»¬çš„ GNN å¯ä»¥æ¶ˆåŒ–ä»»ä½•å¤§å°çš„å›¾ã€‚

è™½ç„¶è§£å†³äº†ç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œä½†æ˜¯è¿™ç§å¹³æ»‘æ–¹æ³•æ²¡æœ‰è§£å†³ç¬¬äºŒä¸ªé—®é¢˜ã€‚

# **2ã€‚åˆ‡æ¯”é›ªå¤«**å›¾**å·ç§¯**

[Defferrard ç­‰äººï¼ŒNeurIPSï¼Œ2016 å¹´](https://arxiv.org/abs/1606.09375)

ä¸Šé¢çš„é¢‘è°±å·ç§¯åŠå…¶å¹³æ»‘ç‰ˆæœ¬çš„ä¸»è¦ç¼ºç‚¹æ˜¯ï¼Œå®ƒä»ç„¶éœ€è¦å¯¹ä¸€ä¸ª *N* Ã— *N* ç»´æ‹‰æ™®æ‹‰æ–¯å›¾ *L* è¿›è¡Œæœ¬å¾åˆ†è§£ï¼Œè¿™äº§ç”Ÿäº†ä¸¤ä¸ªä¸»è¦é—®é¢˜:

1.  ğŸ™ç‰¹å¾åˆ†è§£çš„å¤æ‚åº¦æ˜¯å·¨å¤§çš„ï¼ŒO( *N* )ã€‚æ­¤å¤–ï¼Œåœ¨å¤§å›¾çš„æƒ…å†µä¸‹ï¼Œåœ¨ RAM ä¸­ä»¥å¯†é›†æ ¼å¼ä¿æŒå›¾æ‹‰æ™®æ‹‰æ–¯æ˜¯ä¸å¯è¡Œçš„ã€‚ä¸€ç§è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨ç¨€ç–çŸ©é˜µï¼Œå¹¶ç”¨ Python ä¸­çš„`scipy.sparse.linalg.eigs`æ‰¾åˆ°ç‰¹å¾å‘é‡ã€‚æ­¤å¤–ï¼Œæ‚¨å¯ä»¥åœ¨å…·æœ‰å¤§é‡ RAM å’Œ CPU å†…æ ¸çš„ä¸“ç”¨æœåŠ¡å™¨ä¸Šé¢„å¤„ç†æ‰€æœ‰è®­ç»ƒå›¾ã€‚åœ¨å¾ˆå¤šåº”ç”¨ä¸­ï¼Œä½ çš„æµ‹è¯•å›¾ä¹Ÿå¯ä»¥æå‰é¢„å¤„ç†ï¼Œä½†æ˜¯å¦‚æœä½ ä¸æ–­æœ‰æ–°çš„å¤§å›¾æ¶Œå…¥ï¼Œç‰¹å¾åˆ†è§£ä¼šè®©ä½ éš¾è¿‡ã€‚
2.  ğŸ™å¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œä½ è®­ç»ƒçš„æ¨¡å‹æœ€ç»ˆä¸å›¾çš„ç‰¹å¾å‘é‡ *V* å¯†åˆ‡ç›¸å…³ã€‚å¦‚æœæ‚¨çš„è®­ç»ƒå›¾å’Œæµ‹è¯•å›¾å…·æœ‰éå¸¸ä¸åŒçš„ç»“æ„(èŠ‚ç‚¹å’Œè¾¹çš„æ•°é‡)ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªå¤§é—®é¢˜ã€‚å¦åˆ™ï¼Œå¦‚æœæ‰€æœ‰çš„å›¾å½¢éƒ½éå¸¸ç›¸ä¼¼ï¼Œé—®é¢˜å°±ä¸å¤§äº†ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨åœ¨é¢‘åŸŸä¸­ä½¿ç”¨ä¸€äº›å¹³æ»‘æ»¤æ³¢å™¨ï¼Œå¦‚ä¸Šé¢è®¨è®ºçš„æ ·æ¡ï¼Œé‚£ä¹ˆæ‚¨çš„æ»¤æ³¢å™¨å°†å˜å¾—æ›´åŠ å±€éƒ¨åŒ–ï¼Œé€‚åº”æ–°å›¾å½¢çš„é—®é¢˜ä¼¼ä¹æ›´åŠ ä¸æ˜æ˜¾ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹ä»ç„¶éå¸¸æœ‰é™ã€‚

é‚£ä¹ˆï¼Œåˆ‡æ¯”é›ªå¤«å›¾å·ç§¯å’Œè¿™äº›æœ‰ä»€ä¹ˆå…³ç³»å‘¢ï¼Ÿ

åŸæ¥å®ƒåŒæ—¶è§£å†³äº†**ä¸¤ä¸ªé—®é¢˜ï¼**ğŸ˜ƒ

ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒé¿å…äº†è®¡ç®—æ˜‚è´µçš„ç‰¹å¾åˆ†è§£ï¼Œå¹¶ä¸”æ»¤æ³¢å™¨ä¸å†â€œé™„ç€â€äºç‰¹å¾å‘é‡(ç„¶è€Œå®ƒä»¬ä»ç„¶æ˜¯ç‰¹å¾å€¼*Î»)*çš„å‡½æ•°)ã€‚æ­¤å¤–ï¼Œå®ƒæœ‰ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„å‚æ•°ï¼Œé€šå¸¸è¡¨ç¤ºä¸º *K* ï¼Œå…·æœ‰ä¸æˆ‘ä»¬ä¸Šé¢çš„å…¬å¼(4)ä¸­çš„ *K* ç›¸ä¼¼çš„ç›´è§‰ï¼Œç¡®å®šæ»¤æ³¢å™¨çš„å±€éƒ¨æ€§ã€‚éæ­£å¼åœ°:å¯¹äº *K* =1ï¼Œæˆ‘ä»¬åªå°†èŠ‚ç‚¹ç‰¹æ€§ *Xâ½Ë¡â¾* æä¾›ç»™æˆ‘ä»¬çš„ gnn å¯¹äº *K* =2ï¼Œæˆ‘ä»¬é¦ˆ *Xâ½Ë¡â¾* å’Œğ“*xâ½Ë¡â¾*ï¼›å¯¹äº K=3ï¼Œæˆ‘ä»¬é¦ˆ*xâ½Ë¡â¾***ğ“*xâ½Ë¡â¾***å’Œğ“*xâ½Ë¡â¾*ï¼›å¯¹äºæ›´å¤§çš„ *K* ä»¥æ­¤ç±»æ¨(æˆ‘å¸Œæœ›ä½ å·²ç»æ³¨æ„åˆ°è¿™ä¸ªæ¨¡å¼)ã€‚æ›´å‡†ç¡®æ­£å¼çš„å®šä¹‰è§ [Defferrard et al.](https://arxiv.org/abs/1606.09375) å’Œä¸‹é¢æˆ‘çš„ä»£ç ï¼ŒåŠ ä¸Šé¢å¤–çš„åˆ†æåœ¨( [Knyazev et al .ï¼ŒNeurIPS-Wï¼Œ2018](https://arxiv.org/abs/1811.09595) )ä¸­ç»™å‡ºã€‚****

****ç”±äºé‚»æ¥çŸ©é˜µçš„[å¹‚å±æ€§](https://en.wikipedia.org/wiki/Adjacency_matrix#Matrix_powers)ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œğ“ *Xâ½Ë¡â¾* æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¯¹ 2 è·³é‚»å±…è¿›è¡Œå¹³å‡(æˆ–æ±‚å’Œï¼Œå–å†³äºğ“å¦‚ä½•å½’ä¸€åŒ–)ï¼Œå¹¶ä¸”ç±»ä¼¼åœ°ï¼Œå¯¹ğ“ *â¿Xâ½Ë¡â¾* ä¸­çš„ä»»ä½• *n* è¿›è¡Œå¹³å‡ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå…¶ä¸­æˆ‘ä»¬å¯¹ *n* è·³é‚»å±…è¿›è¡Œå¹³å‡ã€‚****

****![](img/65e27b1442b5903c759766e4f3a2a5ab.png)****

****Chebyshev convolution for *K*=3 for node 1 (dark blue). Circled nodes denote the nodes affecting feature representation of node 1\. The [,] operator denotes concatenation over the feature dimension. W*â½Ë¡â¾ are 3C*Ã—F dimensional weights.****

****æ³¨æ„ï¼Œä¸ºäº†æ»¡è¶³åˆ‡æ¯”é›ªå¤«åŸºçš„æ­£äº¤æ€§ï¼Œğ“ å‡è®¾å›¾ä¸­æ²¡æœ‰å›è·¯ï¼Œå› æ­¤åœ¨çŸ©é˜µä¹˜ç§¯ğ“ *Xâ½Ë¡â¾* çš„æ¯ *i* è¡Œä¸­ï¼Œæˆ‘ä»¬å°†å…·æœ‰èŠ‚ç‚¹ *i* çš„é‚»å±…çš„ç‰¹å¾ï¼Œä½†æ˜¯**æ²¡æœ‰**èŠ‚ç‚¹ *i* æœ¬èº«çš„ç‰¹å¾ã€‚èŠ‚ç‚¹ *i* çš„ç‰¹å¾å°†ä½œä¸ºçŸ©é˜µ *Xâ½Ë¡â¾.å•ç‹¬è¾“å…¥*****

****å¦‚æœ *K* ç­‰äºèŠ‚ç‚¹æ•° *N* ï¼Œåˆ™åˆ‡æ¯”é›ªå¤«å·ç§¯éå¸¸æ¥è¿‘äºé¢‘è°±å·ç§¯ï¼Œå› æ­¤æ»¤æ³¢å™¨çš„æ„Ÿå—åŸŸå°†æ˜¯æ•´ä¸ªå›¾å½¢ã€‚ä½†æ˜¯ï¼Œæ­£å¦‚å·ç§¯ç½‘ç»œçš„æƒ…å†µä¸€æ ·ï¼Œç”±äºæˆ‘å·²ç»è®¨è®ºè¿‡çš„ä¸€äº›åŸå› ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æ»¤æ³¢å™¨ä¸è¾“å…¥å›¾åƒä¸€æ ·å¤§ï¼Œå› æ­¤åœ¨å®è·µä¸­ï¼Œ *K* å–åˆç†çš„å°å€¼ã€‚****

> ****æ ¹æ®æˆ‘çš„ç»éªŒï¼Œè¿™æ˜¯æœ€å¼ºå¤§çš„ gnn ä¹‹ä¸€ï¼Œåœ¨éå¸¸å¹¿æ³›çš„å›¾å½¢ä»»åŠ¡ä¸­å–å¾—äº†å¾ˆå¥½çš„ç»“æœã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯åœ¨å‘å‰/å‘åä¼ é€’ä¸­å¿…é¡»å¾ªç¯éå† *K* (å› ä¸ºåˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼æ˜¯é€’å½’çš„ï¼Œæ‰€ä»¥ä¸å¯èƒ½å¹¶è¡ŒåŒ–å®ƒä»¬)ï¼Œè¿™ä¼šé™ä½æ¨¡å‹çš„é€Ÿåº¦ã€‚****

****ä¸ä¸Šé¢è®¨è®ºçš„æ ·æ¡ä¸€æ ·ï¼Œæˆ‘ä»¬ä¸æ˜¯è®­ç»ƒæ»¤æ³¢å™¨ï¼Œè€Œæ˜¯è®­ç»ƒåˆ‡æ¯”é›ªå¤«å¤šé¡¹å¼çš„ç³»æ•°ã€‚****

****![](img/ded74043e7cfcc4998b6db7490c8e1c3.png)****

****Chebyshev basis used to approximate convolution in the spectral domain.****

****è¦ç”Ÿæˆåˆ‡æ¯”é›ªå¤«åŸºï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ Python ä»£ç :****

```
****# Set K to some integer > 0, like 4 or 5 in our plots above
# Set n_points to a number of points on a curve (we set to 100)** import numpy as npx = np.linspace(-1, 1, n_points)
T = np.zeros((K, len(x)))
T[0,:] = 1
T[1,:] = x
for n in range(1, K-1):
    T[n+1, :] = 2*x*T[n, :] - T[n-1, :] **# recursive computation**   
return T**
```

****ç”Ÿæˆæ ·æ¡å’Œåˆ‡æ¯”é›ªå¤«åŸºçš„å®Œæ•´ä»£ç åœ¨[æˆ‘çš„ github repo](https://github.com/bknyaz/examples/blob/master/splines_cheb.py) ä¸­ã€‚****

****ä¸ºäº†è¯´æ˜åˆ‡æ¯”é›ªå¤«æ»¤æ³¢å™¨åœ¨ä¸è§„åˆ™ç½‘æ ¼ä¸Šçš„è¡¨ç°ï¼Œæˆ‘å†æ¬¡éµå¾ª[å¸ƒé²çº³ç­‰äºº](https://arxiv.org/abs/1312.6203)çš„å®éªŒï¼Œä» MNIST ç½‘æ ¼ä¸­éšæœºæŠ½å– 400 ä¸ªç‚¹ï¼Œå…¶æ–¹å¼ä¸æˆ‘å±•ç¤ºæ‹‰æ™®æ‹‰æ–¯å›¾çš„ç‰¹å¾å‘é‡çš„æ–¹å¼ç›¸åŒã€‚æˆ‘åœ¨ä»è¿™ 400 ä¸ªä½ç½®é‡‡æ ·çš„ MNIST å›¾åƒä¸Šè®­ç»ƒäº†ä¸€ä¸ªåˆ‡æ¯”é›ªå¤«å›¾å·ç§¯æ¨¡å‹(ç›¸åŒçš„ä¸è§„åˆ™ç½‘æ ¼ç”¨äºæ‰€æœ‰å›¾åƒ),ä¸‹é¢æ˜¾ç¤ºäº†ä¸€ä¸ªç”¨äº *K* =1 å’Œ *K* =20 çš„è¿‡æ»¤å™¨ã€‚****

****![](img/74e53897cf90f815bdd93486cd829cec.png)****

****A single Chebyshev filter (K=3 on the left and K=20 on the right) trained on MNIST and applied at different locations (shown as a red pixel) on a irregular grid with 400 points. Compared to filters of standard ConvNets, GNN filters have different shapes depending *on the node at which they are applied*, because each node has a different neighborhood structure.****

# ******3ã€‚GCN******

****[Kipf &éŸ¦æ—ï¼ŒICLRï¼Œ2017](https://arxiv.org/abs/1609.02907)****

****æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œå¦‚æœå¢åŠ åˆ‡æ¯”é›ªå¤«å·ç§¯çš„ *K* ï¼Œå¯è®­ç»ƒå‚æ•°çš„æ€»æ•°å°±ä¼šå¢åŠ ã€‚ä¾‹å¦‚ï¼Œå¯¹äº *K* =2ï¼Œæˆ‘ä»¬çš„æƒé‡ *Wâ½Ë¡â¾* å°†æ˜¯ 2 *C* Ã— *F* è€Œä¸æ˜¯ä»…ä»… *C* Ã— *F* ã€‚è¿™æ˜¯å› ä¸ºæˆ‘ä»¬å°†ç‰¹å¾*xâ½Ë¡â¾*t16ã€‘t17ã€‘å’Œğ“ *Xâ½Ë¡â¾* è¿æ¥æˆä¸€ä¸ªå•ä¸€çš„ *N* Ã—2 *C* çŸ©é˜µã€‚æ›´å¤šçš„è®­ç»ƒå‚æ•°æ„å‘³ç€æ¨¡å‹æ¯”æ›´éš¾è®­ç»ƒï¼Œéœ€è¦æ ‡æ³¨æ›´å¤šçš„æ•°æ®è¿›è¡Œè®­ç»ƒã€‚å›¾è¡¨æ•°æ®é›†é€šå¸¸éå¸¸å°ã€‚åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼ŒMNIST è¢«è®¤ä¸ºæ˜¯ä¸€ä¸ªå¾ˆå°çš„æ•°æ®é›†ï¼Œå› ä¸ºå›¾åƒåªæœ‰ 28Ã—28 ç»´ï¼Œåªæœ‰ 60k ä¸ªè®­ç»ƒå›¾åƒï¼Œè€Œåœ¨å›¾ç½‘ç»œä¸­ï¼ŒMNIST æ˜¯ç›¸å½“å¤§çš„ï¼Œå› ä¸ºæ¯ä¸ªå›¾å°†æœ‰ *N* =784 ä¸ªèŠ‚ç‚¹ï¼Œ60k æ˜¯å¤§é‡çš„è®­ç»ƒå›¾ã€‚ä¸è®¡ç®—æœºè§†è§‰ä»»åŠ¡ç›¸æ¯”ï¼Œè®¸å¤šå›¾å½¢æ•°æ®é›†åªæœ‰å¤§çº¦ 20-100 ä¸ªèŠ‚ç‚¹å’Œ 200-1000 ä¸ªè®­ç»ƒæ ·æœ¬ã€‚è¿™äº›å›¾å¯ä»¥è¡¨ç¤ºæŸäº›å°åˆ†å­ï¼Œæ ‡è®°åŒ–å­¦/ç”Ÿç‰©æ•°æ®é€šå¸¸æ¯”æ ‡è®°å›¾åƒæ›´æ˜‚è´µã€‚å› æ­¤ï¼Œè®­ç»ƒåˆ‡æ¯”é›ªå¤«å·ç§¯æ¨¡å‹å¯èƒ½å¯¼è‡´è®­ç»ƒé›†çš„ä¸¥é‡è¿‡æ‹Ÿåˆ(å³ï¼Œæ¨¡å‹å°†å…·æœ‰æ¥è¿‘ 0 çš„è®­ç»ƒæŸå¤±ï¼Œä½†å°†å…·æœ‰è¾ƒå¤§çš„éªŒè¯æˆ–æµ‹è¯•è¯¯å·®)ã€‚æ‰€ä»¥ï¼Œ [Kipf & Welling](https://arxiv.org/abs/1609.02907) çš„ GCN æœ¬è´¨ä¸Šæ˜¯å°†èŠ‚ç‚¹ç‰¹å¾*xâ½Ë¡â¾*t36ã€‘t37ã€‘å’Œğ“ *Xâ½Ë¡â¾* çš„çŸ©é˜µâ€œåˆå¹¶â€æˆä¸€ä¸ªå•ç‹¬çš„ *N* Ã— *C* çŸ©é˜µã€‚ç»“æœï¼Œä¸å…·æœ‰ *K* =2 çš„åˆ‡æ¯”é›ªå¤«å·ç§¯ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹éœ€è¦è®­ç»ƒçš„å‚æ•°å°‘äº†ä¸¤å€ï¼Œä½†å…·æœ‰ 1 è·³çš„ç›¸åŒæ„Ÿå—é‡ã€‚ä¸»è¦çš„æŠ€å·§æ˜¯é€šè¿‡å°†ä¸€ä¸ª[å•ä½çŸ©é˜µ](https://en.wikipedia.org/wiki/Identity_matrix) *I* æ·»åŠ åˆ°ğ“ ä¸­ï¼Œå¹¶ä»¥ç‰¹å®šçš„æ–¹å¼å¯¹å…¶è¿›è¡Œè§„èŒƒåŒ–ï¼Œä»è€Œå°†â€œè‡ªå¾ªç¯â€æ·»åŠ åˆ°æ‚¨çš„å›¾ä¸­ï¼Œå› æ­¤ç°åœ¨åœ¨çŸ©é˜µä¹˜ç§¯çš„æ¯ä¸€è¡Œ*I**xâ½Ë¡â¾*ä¸­ï¼Œæˆ‘ä»¬å°†æ‹¥æœ‰èŠ‚ç‚¹ *iã€* **çš„é‚»å±…çš„ç‰¹å¾ï¼Œä»¥åŠèŠ‚ç‚¹ *i.* çš„**ç‰¹å¾****

> ****è¿™ä¸ªæ¨¡å‹ä¼¼ä¹æ˜¯ä¸€ä¸ªæ ‡å‡†çš„åŸºçº¿é€‰æ‹©ï¼Œéå¸¸é€‚åˆè®¸å¤šåº”ç”¨ç¨‹åºï¼Œå› ä¸ºå®ƒçš„è½»é‡çº§ã€è‰¯å¥½çš„æ€§èƒ½å’Œå¯¹è¾ƒå¤§å›¾å½¢çš„å¯ä¼¸ç¼©æ€§ã€‚****

## ****3.1.GCN vs åˆ‡æ¯”é›ªå¤«å±‚****

****GCN å·ç§¯å’Œåˆ‡æ¯”é›ªå¤«å·ç§¯çš„åŒºåˆ«å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚****

****ä¸Šé¢çš„ä»£ç éµå¾ªä¸æˆ‘çš„æ•™ç¨‹ çš„ç¬¬ä¸€éƒ¨åˆ† [*ä¸­ç›¸åŒçš„ç»“æ„ï¼Œåœ¨é‚£é‡Œæˆ‘æ¯”è¾ƒäº†ç»å…¸çš„ NN å’Œ GNNã€‚GCN å’Œåˆ‡æ¯”é›ªå¤«å·ç§¯ä¸­çš„ä¸€ä¸ªä¸»è¦æ­¥éª¤æ˜¯é‡æ–°æ ‡åº¦å›¾æ‹‰æ™®æ‹‰æ–¯ *L* çš„è®¡ç®—ã€‚è¿›è¡Œè¿™ç§é‡æ–°è°ƒæ•´æ˜¯ä¸ºäº†ä½¿ç‰¹å¾å€¼åœ¨[-1ï¼Œ1]çš„èŒƒå›´å†…ï¼Œä»¥ä¾¿äºè®­ç»ƒ(è¿™åœ¨å®è·µä¸­å¯èƒ½ä¸æ˜¯éå¸¸é‡è¦çš„æ­¥éª¤ï¼Œå› ä¸ºæƒé‡å¯ä»¥åœ¨è®­ç»ƒæœŸé—´é€‚åº”)ã€‚åœ¨ GCNï¼Œå¦‚ä¸Šæ‰€è¿°ï¼Œåœ¨è®¡ç®—æ‹‰æ™®æ‹‰æ–¯ç®—å­ä¹‹å‰ï¼Œé€šè¿‡æ·»åŠ å•ä½çŸ©é˜µå°†è‡ªå¾ªç¯æ·»åŠ åˆ°å›¾ä¸­ã€‚è¿™ä¸¤ç§æ–¹æ³•çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œåœ¨åˆ‡æ¯”é›ªå¤«å·ç§¯ä¸­ï¼Œæˆ‘ä»¬*é€’å½’åœ°*éå† *K* æ¥æ•è· *K* è·³é‚»åŸŸä¸­çš„ç‰¹å¾ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™æ ·çš„ GCN æˆ–åˆ‡æ¯”é›ªå¤«å±‚ä¸éçº¿æ€§äº¤é”™å †å èµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªå›¾å½¢ç¥ç»ç½‘ç»œã€‚*](https://medium.com/p/3d9fada3b80d)****

****ç°åœ¨ï¼Œè®©æˆ‘ç¤¼è²Œåœ°æ‰“æ–­ä¸€ä¸‹ğŸ˜ƒæˆ‘ä»¬çš„é¢‘è°±è®¨è®ºå¹¶ç»™å‡ºäº†å¦å¤–ä¸¤ç§ä»¤äººå…´å¥‹çš„æ–¹æ³•èƒŒåçš„å¤§è‡´æƒ³æ³•:è¾¹ç¼˜æ¡ä»¶æ»¤æ³¢å™¨ï¼Œç”± [Simonovsky & Komodakisï¼ŒCVPRï¼Œ2017](https://arxiv.org/abs/1704.02901) å’Œè«å¥ˆï¼Œç”± [Monti ç­‰äººï¼ŒCVPRï¼Œ2017](https://arxiv.org/abs/1611.08402) ï¼Œå®ƒä»¬å…±äº«ä¸€äº›ç±»ä¼¼çš„æ¦‚å¿µã€‚****

# ******4ã€‚è¾¹ç¼˜è°ƒèŠ‚çš„**æ»¤æ³¢å™¨****

****[è¥¿è’™è¯ºå¤«æ–¯åŸº&CVPR ç§‘è«è¾¾åŸºæ–¯ï¼Œ2017](https://arxiv.org/abs/1704.02901)****

****å¦‚ä½ æ‰€çŸ¥ï¼Œåœ¨ ConvNets ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ä¼˜åŒ–ä¸€äº›æŸå¤±æ¥å­¦ä¹ æƒé‡(è¿‡æ»¤å™¨)ï¼Œå¦‚[äº¤å‰ç†µ](https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss)ã€‚åŒæ ·ï¼Œæˆ‘ä»¬åœ¨ GNNs ä¸­å­¦ä¹  Wâ½Ë¡â¾ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æœ‰å¦ä¸€ä¸ªç½‘ç»œæ¥é¢„æµ‹è¿™äº›æƒé‡ï¼Œè€Œä¸æ˜¯å­¦ä¹ è¿™äº›æƒé‡ã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ è¾…åŠ©ç½‘ç»œçš„æƒé‡ï¼Œå®ƒä»¥ä¸€å¹…å›¾åƒæˆ–ä¸€ä¸ªå›¾å½¢ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›æƒé‡ *Wâ½Ë¡â¾* (ä»–ä»¬å·¥ä½œä¸­çš„Î¸)ä½œä¸ºè¾“å‡ºã€‚è¯¥æƒ³æ³•åŸºäº**åŠ¨æ€æ»¤æ³¢å™¨ç½‘ç»œ** ( [Brabandere ç­‰äººï¼ŒNIPSï¼Œ2016](https://arxiv.org/abs/1605.09673) )ï¼Œå…¶ä¸­â€œåŠ¨æ€â€æ„å‘³ç€æ»¤æ³¢å™¨ *Wâ½Ë¡â¾* å°†æ ¹æ®è¾“å…¥è€Œä¸åŒï¼Œè¿™ä¸æ ‡å‡†æ¨¡å‹ç›¸åï¼Œåœ¨æ ‡å‡†æ¨¡å‹ä¸­ï¼Œæ»¤æ³¢å™¨åœ¨è®­ç»ƒåæ˜¯å›ºå®šçš„(æˆ–é™æ€çš„)ã€‚****

****![](img/d86fbbe0ab053ff325c21ee2760a1de4.png)****

****Using an auxiliary â€œfilter generating networkâ€ FË¡ to predict edge-specific weights Î˜ for the main network. XË¡â»Â¹ are input node features and XË¡ are output features. The figure shows a single iteration of â€œdynamic convolutionâ€ for node 1 (in yellow). Standard GNNs typically would simply average (or sum) features of node 1 neighbors (nodes 2, 3, 4, 5) , which would correspond to having an isotropic filter (Î˜ would be a constant vector). In contrast, this model has anisotropic filters, because it predicts different edge values between node 1 and all itâ€™s neighbors based on edge labels L, so that features XË¡(1) are computed as a weighted average of neighborsâ€™ features. Figure from ([Simonovsky & Komodakis, CVPR, 2017](https://arxiv.org/abs/1704.02901)).****

****è¿™æ˜¯ä¸€ç§éå¸¸æ™®éçš„å·ç§¯å½¢å¼ï¼Œé™¤äº†å›¾åƒä¹‹å¤–ï¼Œè¿˜å¯ä»¥å¾ˆå®¹æ˜“åœ°åº”ç”¨äºå›¾å½¢æˆ–ç‚¹äº‘ï¼Œæ­£å¦‚ä»–ä»¬åœ¨ CVPR çš„è®ºæ–‡ä¸­æ‰€åšçš„é‚£æ ·ï¼Œå¹¶è·å¾—äº†å‡ºè‰²çš„ç»“æœã€‚ç„¶è€Œï¼Œæ²¡æœ‰â€œ[å…è´¹çš„åˆé¤](https://en.wikipedia.org/wiki/No_free_lunch_theorem)â€ï¼Œè®­ç»ƒè¿™æ ·çš„æ¨¡å‹ç›¸å½“å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¸¸è§„çš„ç½‘æ ¼çº¦æŸç°åœ¨å·²ç»æ”¾æ¾ï¼Œè§£å†³æ–¹æ¡ˆçš„èŒƒå›´æ€¥å‰§å¢åŠ ã€‚è¿™å¯¹äºå…·æœ‰è®¸å¤šè¾¹çš„è¾ƒå¤§å›¾å½¢æˆ–è¾ƒæ·±å±‚ä¸­çš„å·ç§¯æ¥è¯´å°¤å…¶å¦‚æ­¤ï¼Œè¿™äº›å›¾å½¢é€šå¸¸å…·æœ‰æ•°ç™¾ä¸ªé€šé“(ç‰¹å¾æ•°é‡ï¼Œ *C)* ï¼Œå› æ­¤æ‚¨å¯èƒ½æœ€ç»ˆä¼šä¸ºæ¯ä¸ªè¾“å…¥æ€»å…±ç”Ÿæˆæ•°åƒä¸ªæ•°å­—ï¼åœ¨è¿™æ–¹é¢ï¼Œæ ‡å‡† ConvNets éå¸¸å¥½ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æµªè´¹æ¨¡å‹çš„èƒ½åŠ›æ¥è®­ç»ƒé¢„æµ‹è¿™äº›æƒé‡ï¼Œè€Œæ˜¯ç›´æ¥å¼ºåˆ¶è¦æ±‚æ»¤æ³¢å™¨å¯¹æ‰€æœ‰è¾“å…¥éƒ½åº”è¯¥ç›¸åŒã€‚ä½†æ˜¯ï¼Œè¿™ç§å…ˆéªŒä½¿ ConvNets å—åˆ°é™åˆ¶ï¼Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥å°†å®ƒä»¬åº”ç”¨äºå›¾å½¢æˆ–ç‚¹äº‘ã€‚å› æ­¤ï¼Œä¸€å¦‚æ—¢å¾€ï¼Œåœ¨ç‰¹å®šä»»åŠ¡ä¸­ï¼Œçµæ´»æ€§å’Œæ€§èƒ½ä¹‹é—´ä¼šæœ‰ä¸€äº›æƒè¡¡ã€‚****

> ****å½“åº”ç”¨äºå›¾åƒæ—¶ï¼Œå¦‚ MNISTï¼Œè¾¹ç¼˜æ¡ä»¶æ¨¡å‹å¯ä»¥å­¦ä¹ é¢„æµ‹*å„å‘å¼‚æ€§*æ»¤æ³¢å™¨â€”â€”å¯¹æ–¹å‘æ•æ„Ÿçš„æ»¤æ³¢å™¨ï¼Œå¦‚è¾¹ç¼˜æ£€æµ‹å™¨ã€‚ä¸æˆ‘çš„æ•™ç¨‹ çš„ç¬¬ä¸€éƒ¨åˆ† [*ä¸­è®¨è®ºçš„é«˜æ–¯æ»¤æ³¢å™¨ç›¸æ¯”ï¼Œè¿™äº›æ»¤æ³¢å™¨èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰å›¾åƒä¸­çš„æŸäº›æ¨¡å¼ï¼Œä¾‹å¦‚æ•°å­—ä¸­çš„ç¬”ç”»ã€‚*](https://medium.com/p/3d9fada3b80d)****

****![](img/f9388b8936b6a2f15a73a9967db5605b.png)****

****Convolutional filters learned on MNIST sampled in low (left) and high (right) resolutions. Figure from ([Simonovsky & Komodakis, CVPR, 2017](https://arxiv.org/abs/1704.02901)).****

****æˆ‘æƒ³å†å¼ºè°ƒä¸€æ¬¡ï¼Œæ¯å½“æˆ‘ä»¬æœ‰ä¸€ä¸ªå¸¦æœ‰è¾…åŠ©ç½‘ç»œçš„å¤æ‚æ¨¡å‹æ—¶ï¼Œåœ¨æŸç§æ„ä¹‰ä¸Šå®ƒå°±å˜æˆäº†ä¸€ä¸ªå…ˆæœ‰é¸¡è¿˜æ˜¯å…ˆæœ‰è›‹çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå…¶ä¸­ä¸€ä¸ªç½‘ç»œ(è¾…åŠ©ç½‘ç»œæˆ–ä¸»ç½‘ç»œ)åº”è¯¥æ¥æ”¶åˆ°éå¸¸å¼ºçš„ä¿¡å·ï¼Œè¿™æ ·å®ƒå°±å¯ä»¥éšå¼åœ°ç›‘æ§å¦ä¸€ä¸ªç½‘ç»œã€‚åœ¨æˆ‘ä»¬çš„ [BMVC è®ºæ–‡](https://arxiv.org/abs/1907.09000)ä¸­ï¼Œç±»ä¼¼äº[Simonovsky&Komodakis](https://arxiv.org/abs/1704.02901)çš„å·¥ä½œï¼Œæˆ‘ä»¬åœ¨ç”Ÿæˆè¾¹çš„ç½‘ç»œä¸Šåº”ç”¨äº†é¢å¤–çš„çº¦æŸæ¥ä¿ƒè¿›è®­ç»ƒã€‚æˆ‘å°†åœ¨åé¢çš„å¸–å­ä¸­è¯¦ç»†æè¿°æˆ‘ä»¬çš„å·¥ä½œã€‚****

# ******5ã€‚è«å¥ˆ******

****[è’™è’‚ç­‰äººï¼ŒCVPRï¼Œ2017 å¹´](https://arxiv.org/abs/1611.08402)****

****MoNet ä¸åŒäºæœ¬æ–‡ä¸­è®¨è®ºçš„å…¶ä»–ä½œå“ï¼Œå› ä¸ºå®ƒå‡å®šå…·æœ‰èŠ‚ç‚¹åæ ‡çš„æ¦‚å¿µï¼Œå› æ­¤æ›´é€‚åˆäºå‡ ä½•ä»»åŠ¡ï¼Œå¦‚ 3D ç½‘æ ¼åˆ†ææˆ–å›¾åƒ/è§†é¢‘æ¨ç†ã€‚å®ƒæœ‰ç‚¹ç±»ä¼¼äº [Simonovsky & Komodakis](https://arxiv.org/abs/1704.02901) çš„è¾¹ç¼˜æ¡ä»¶æ»¤æ³¢å™¨ï¼Œå› ä¸ºå®ƒä»¬ä¹Ÿå¼•å…¥äº†é¢„æµ‹æƒé‡çš„è¾…åŠ©å¯å­¦ä¹ å‡½æ•°ğ·(ğ‘¤ã€ğœƒ *ï¼ŒÏ* ã€‚ä¸åŒçš„æ˜¯ï¼Œè¿™äº›æƒé‡å–å†³äºèŠ‚ç‚¹æåæ ‡(è§’åº¦ğœƒå’ŒåŠå¾„*Ï*)ï¼›å¹¶ä¸”è¯¥å‡½æ•°çš„å¯è®­ç»ƒå‚æ•°ğ‘¤è¢«çº¦æŸä¸ºé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ï¼Œä»è€Œæˆ‘ä»¬ä¸æ˜¯å­¦ä¹  *N* Ã— *N* çŸ©é˜µï¼Œè€Œæ˜¯ä»…å­¦ä¹ ä¸å›¾å¤§å° *N* æ— å…³çš„å›ºå®šå¤§å°çš„å‘é‡(å‡å€¼å’Œæ–¹å·®)ã€‚å°±æ ‡å‡† ConvNets è€Œè¨€ï¼Œå¯¹äºæ¯ä¸ªæ»¤æ³¢å™¨æ¥è¯´ï¼Œåªå­¦ä¹  2 ä¸ªå€¼(é«˜æ–¯åˆ†å¸ƒçš„å¹³å‡å€¼å’Œæ–¹å·®)æ˜¯ç›¸åŒçš„ï¼Œè€Œä¸æ˜¯åˆ†åˆ«å­¦ä¹  3Ã—3ã€5Ã—5 æˆ– 11Ã—11 ç»´æ»¤æ³¢å™¨çš„ 9ã€25 æˆ– 121 ä¸ªå€¼ã€‚è¿™ç§*å‚æ•°åŒ–*å°†æå¤§åœ°å‡å°‘ ConvNet ä¸­çš„å‚æ•°æ•°é‡ï¼Œä½†æ»¤æ³¢å™¨æ•æ‰å›¾åƒç‰¹å¾çš„èƒ½åŠ›éå¸¸æœ‰é™ã€‚****

****[Monti ç­‰äºº](https://arxiv.org/abs/1611.08402)è®­ç»ƒé«˜æ–¯çš„ğ½å‡å€¼å’Œæ–¹å·®ï¼Œè½¬æ¢èŠ‚ç‚¹åæ ‡çš„è¿‡ç¨‹ç±»ä¼¼äºå°†å®ƒä»¬æ‹Ÿåˆåˆ°[é«˜æ–¯æ··åˆæ¨¡å‹](https://scikit-learn.org/stable/modules/mixture.html)ã€‚å¦‚æœæˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„è¿‡æ»¤å™¨è¶³å¤Ÿå…¨å±€ï¼Œé‚£ä¹ˆè¿™ä¸ªæ¨¡å‹çš„è®­ç»ƒè®¡ç®—é‡ç›¸å½“å¤§ï¼Œä½†å®ƒå¯èƒ½æ˜¯è§†è§‰ä»»åŠ¡çš„ä¸€ä¸ªå¾ˆå¥½çš„é€‰æ‹©(å‚è§æˆ‘ä»¬çš„ [BMVC è®ºæ–‡](https://arxiv.org/abs/1907.09000)è¿›è¡Œæ¯”è¾ƒ)ï¼Œä½†åœ¨éè§†è§‰ä»»åŠ¡ä¸Šï¼Œå®ƒå¾€å¾€æ¯”ç®€å•çš„ GCN å·®( [Knyazev ç­‰äººï¼ŒNeurIPS-Wï¼Œ2018](https://arxiv.org/abs/1811.09595) )ã€‚ç”±äºå‡½æ•° *D* ä¾èµ–äºåæ ‡ï¼Œç”Ÿæˆçš„æ»¤æ³¢å™¨ä¹Ÿæ˜¯å„å‘å¼‚æ€§çš„ï¼Œå¹¶ä¸”å…·æœ‰å¦‚ä¸‹å›¾æ‰€ç¤ºçš„å®šå‘å’Œæ‹‰é•¿çš„é«˜æ–¯å½¢çŠ¶ã€‚****

****![](img/ce2745eb015b147d66e6b5fcf7225d40.png)****

****Filters trained with MoNet in polar coordinates ğœƒ and *Ï*. Each ellipse corresponds to a slice of a Gaussian at some fixed level. The idea is that if the coordinates of the i-th node are close to the middle of the j-th Gaussian, then the generated weight at index (i,j) will have a value close to 1.****

```
*****Pseudo-code of the MoNet layer using PyTorch*****# assume X to be input *N***Ã—***C* node features**
**# coord are *N*Ã—*N*Ã—*2* node coordinate differences between all pairs of nodes (node degrees for non-geometric tasks)
# coord can be viewed as angular and radial edges between nodes**1\. Generate *J* Gaussian-shaped filters based on coordinates of nodes    using some trainable function D
   weights = D(coord)  # weights: *J*Ã—*N*Ã—*N*
2\. Multiply node features X by these weights
   X = torch.bmm(weights, X.expand(J, N, C))  # X: *J*Ã—*N*Ã—*C*
3\. Project features by a learnable linear transformation
   X = fc(X.permute(1, 2, 0).view(N, J*C))  # X: *N*Ã—*F* 4\. Feed X to the next layer**
```

# ****ç»“è®º****

****å°½ç®¡è®¨è®ºäº†å¾ˆé•¿æ—¶é—´ï¼Œæˆ‘ä»¬åªæ˜¯è§¦åŠäº†çš®æ¯›ã€‚å›¾å½¢ç¥ç»ç½‘ç»œçš„åº”ç”¨æ­£åœ¨æ‰©å±•ï¼Œè¿œè¿œè¶…å‡ºäº†å…¸å‹çš„å›¾å½¢æ¨ç†ä»»åŠ¡ï¼Œå¦‚åˆ†å­åˆ†ç±»ã€‚ä¸åŒå›¾å½¢ç¥ç»å±‚çš„æ•°é‡å¢é•¿éå¸¸å¿«ï¼Œç±»ä¼¼äºå‡ å¹´å‰å·ç§¯ç½‘ç»œçš„æƒ…å†µï¼Œå› æ­¤å¾ˆéš¾è·Ÿè¸ªå®ƒä»¬ã€‚åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œ[py torch Geometric(PyG)](https://github.com/rusty1s/pytorch_geometric)â€”â€”ä¸€ä¸ªä»å›¾è¡¨ä¸­å­¦ä¹ çš„å¥½å·¥å…·ç®±â€”â€”ç»å¸¸ç”¨æ–°é¢–çš„å›¾å±‚å’ŒæŠ€å·§å¡«å……å®ƒçš„é›†åˆã€‚****

*****é¸£è°¢:æœ¬æ•™ç¨‹çš„å¾ˆå¤§ä¸€éƒ¨åˆ†æ˜¯æˆ‘åœ¨ SRI International å®ä¹ æœŸé—´åœ¨* [*ç©†ç½•é»˜å¾·Â·é˜¿æ¢…å°”*](https://medium.com/u/6cf41cb2c546?source=post_page-----be6d71d70f49--------------------------------) *(* [*ä¸»é¡µ*](https://mohamedramer.com/) *)å’Œæˆ‘çš„åšå£«å¯¼å¸ˆæ ¼æ‹‰æ±‰å§†Â·æ³°å‹’(* [*ä¸»é¡µ*](https://www.gwtaylor.ca/) *)çš„æŒ‡å¯¼ä¸‹ç¼–å†™çš„ã€‚æˆ‘ä¹Ÿæ„Ÿè°¢*[*Carolyn Augusta*](https://www.linkedin.com/in/carolynaugusta/)*çš„æœ‰ç”¨åé¦ˆã€‚*****

****åœ¨ [Github](https://github.com/bknyaz/) ã€ [LinkedIn](https://www.linkedin.com/in/boris-knyazev-39690948/) å’Œ [Twitter](https://twitter.com/BorisAKnyazev) ä¸Šæ‰¾æˆ‘ã€‚[æˆ‘çš„ä¸»é¡µ](https://bknyaz.github.io/)ã€‚****

****å¦‚æœä½ æƒ³åœ¨ä½ çš„è®ºæ–‡ä¸­å¼•ç”¨è¿™ç¯‡åšæ–‡ï¼Œè¯·ä½¿ç”¨:
[*@ misc*](http://twitter.com/misc)*{ Knyazev 2019 Tutorialï¼Œ
title = {ç”¨äºè®¡ç®—æœºè§†è§‰åŠè¶…è¶Šçš„å›¾å½¢ç¥ç»ç½‘ç»œæ•™ç¨‹}ï¼Œ
author={Knyazevï¼ŒBoris and Taylorï¼ŒGraham W and Amerï¼ŒMohamed R}ï¼Œ
year={2019}
}*****