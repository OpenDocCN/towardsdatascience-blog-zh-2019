<html>
<head>
<title>Hyperparameter Tuning Explained — Tuning Phases, Tuning Methods, Bayesian Optimization, and Sample Code!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了超参数调优——调优阶段、调优方法、贝叶斯优化和示例代码！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-explained-d0ebb2ba1d35?source=collection_archive---------14-----------------------#2019-12-13">https://towardsdatascience.com/hyperparameter-tuning-explained-d0ebb2ba1d35?source=collection_archive---------14-----------------------#2019-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="75ab" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">本书"<a class="ae ep" href="https://www.amazon.co.jp/dp/4297108437" rel="noopener ugc nofollow" target="_blank">赢得 KAGGLE </a>的数据分析技巧"</h2><div class=""/><div class=""><h2 id="5516" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">何时以及如何使用手动/网格/随机搜索和贝叶斯优化</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/256148aa5a7a344abccb308832d61792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qv2Su1gKmUJxpfG8lt2Jmw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Simple Mandala of modeling world (Validation Strategy is out of setup space because the same validation approach should be applied to any models compared for fair model selection)</figcaption></figure><p id="242c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">超参数是 ML 模型的重要组成部分，可以使模型成为黄金或垃圾。</p><p id="333b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇文章中，我将讨论:</p><ul class=""><li id="113f" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><strong class="lj jd">随着建模的进行，调谐阶段的演变</strong>，</li><li id="9728" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd">各车型的重要参数(特别是 GBDT 车型)，</strong></li><li id="6f88" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><strong class="lj jd">常见的四种调优方法(手动/网格搜索/随机搜索/贝叶斯优化)。</strong></li></ul></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="8559" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">目录</h1><ol class=""><li id="80b4" class="md me it lj b lk nq ln nr lq ns lu nt ly nu mc nv mj mk ml bi translated"><strong class="lj jd">通用超参数调整策略</strong></li></ol><ul class=""><li id="37e3" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">1.1.特征工程参数调整的三个阶段</li><li id="6d87" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">1.2.什么是超参数基线，哪些参数值得调整？</li></ul><p id="053b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 2。超参数调整的四种基本方法</strong></p><ul class=""><li id="8da4" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">2.1.人工调谐</li><li id="0787" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">2.2.网格搜索</li><li id="187d" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">2.3.随机搜索</li><li id="25df" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">2.4.贝叶斯优化</li></ul><p id="d632" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 3。超参数调整和交叉验证中的 k 折叠</strong></p><p id="f965" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 4。结论</strong></p><p id="6842" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是另一篇帖子接的一本新书中介绍的技巧<em class="nw"/><a class="ae nx" href="https://www.amazon.co.jp/dp/4297108437" rel="noopener ugc nofollow" target="_blank"><em class="nw">数据分析技巧赢 Kaggle</em></a><em class="nw"/>由三位高阶 Kaggle 作者(不包括我自己)由此可见这不是个人推广！:) )</p><p id="9318" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">关于这本书本身的完整目录，请看我的<a class="ae nx" href="https://medium.com/@daydreamersjp/a-new-book-data-analysis-techniques-to-win-kaggle-is-a-current-best-and-complete-for-table-data-4af66a88388" rel="noopener">其他帖子</a>。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="4c19" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">1.通用超参数调整策略</h1><h2 id="8291" class="ny mz it bd na nz oa dn ne ob oc dp ni lq od oe nk lu of og nm ly oh oi no iz bi translated">1.1.特征工程参数调整的三个阶段</h2><p id="6fd4" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">我们如何调整超参数不仅是关于我们使用哪种调整方法的问题，也是关于我们如何发展超参数学习阶段直到我们找到最终的和最好的。</p><p id="ecc8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这应该取决于任务和我们实际上通过超参数变化看到多少分数变化，但我们应该记住以下常见步骤:</p><ul class=""><li id="4118" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><em class="nw">初始阶段</em>:开始<strong class="lj jd">基线参数</strong> &amp;基线特征工程，</li><li id="9657" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><em class="nw">预热阶段</em>:围绕<strong class="lj jd">手动调谐或网格搜索几个重要参数</strong>用几个搜索候选&amp;更多特色工程，</li><li id="9c67" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><em class="nw">激烈调优阶段</em> : <strong class="lj jd">对更多参数进行随机搜索或贝叶斯优化</strong> &amp;最终特征工程</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/cb9c594afe77cb4f4e5e951affd3f01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MUqsGQMnMv-oaOSszMJRMw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Plates are getting hotter.</figcaption></figure><h2 id="c51b" class="ny mz it bd na nz oa dn ne ob oc dp ni lq od oe nk lu of og nm ly oh oi no iz bi translated">1.2.什么是超参数基线，哪些参数值得调整？</h2><p id="9ef5" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">然后，您会有另一个问题:<strong class="lj jd">“什么是超参数基线，哪些参数值得调优？”</strong></p><p id="882e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">每个模型的参数是不同的，因此我不能在这里讨论每个模型的参数。留意参数的选择始终是数据科学家的工作。它应该从对模型算法和模型文档的理解开始。</p><p id="6fe6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这篇文章中，我将只给<strong class="lj jd">留下一些关于 GBDT 模型、xgboost、lightbgm 和 catboost </strong>的资源，这些模型我曾经讨论为<a class="ae nx" rel="noopener" target="_blank" href="/what-is-the-best-starter-model-in-table-data-ml-lessons-from-a-high-rank-kagglers-new-book-f08b821db797">的入门模型</a>。</p><p id="fb30" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae nx" href="https://sites.google.com/view/lauraepp/parameters" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd"><em class="nw">Laurae ++的这个网页</em> </strong> </a> <em class="nw"> </em>对于 xgboost/lightgbm 来说一直是一个很棒的起点。</p><ul class=""><li id="e575" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">你可以找到每个参数的描述，典型值，允许范围，变化的影响等。字面上的大量信息。</li><li id="4f36" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">它们不包括 catboost(截至 2019 年 12 月 7 日)。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/ea57a20bc05d07233830d20c962209e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaMvr60iJmeFJFtNYKmPrw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Laurae++, a descently-designed information-rich web site about hyperparameters of xgboost and lightgbm ( <a class="ae nx" href="https://sites.google.com/view/lauraepp/parameters" rel="noopener ugc nofollow" target="_blank">https://sites.google.com/view/lauraepp/parameters</a>)</figcaption></figure><p id="f398" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> <em class="nw">解析维迪亚</em> </strong>也提供了关于 GBDT 模型超参数的丰富内容:</p><ul class=""><li id="0f49" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><em class="nw"> xgboost </em> : <a class="ae nx" href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank">《使用 Python 代码在 xgboost 中进行参数调整的完整指南》</a></li><li id="beac" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated"><em class="nw">Light GBM</em>::<a class="ae nx" href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/" rel="noopener ugc nofollow" target="_blank">Light GBM vs XGBOOST 哪个算法取冠？</a>”</li></ul><p id="247b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦你开始谷歌搜索，还有许多其他随机的网页。这是我从《数据科学》杂志上找到的一篇文章，它提供了三种主要 GBDT 车型的对比列表。</p><ul class=""><li id="4b9d" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated"><a class="ae nx" rel="noopener" target="_blank" href="/an-example-of-hyperparameter-optimization-on-xgboost-lightgbm-and-catboost-using-hyperopt-12bc41a271e">“使用 Hyperopt 在 XGBoost、LightGBM 和 CatBoost 上进行超参数优化的示例”</a></li></ul><p id="260b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面的图表总结了我对分级重要性中哪些参数是重要的、它们有利的基线选择和调整范围的看法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/11f5d4edb62839d3bcbf793ee84a05b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sfR1X6jG7nYJeOP8ogE75A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">List of important hyperparameters of three GBDT models, their baseline choice and tuning range.</figcaption></figure><p id="d380" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">(对于表格开发，我的朋友兼同事丁玉轩给了我很好的建议。谢谢大家！)</p><p id="50fa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用 Python 包建模 GBDT 的人通常使用原始函数版本(“原始 API”)或 sklearn 包装器版本(“sklearn API”)，这使得函数的使用等同于其他 sklearn ML 模型 API。</p><p id="4818" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">大多数情况下，你可以根据自己的喜好选择任何一个，但请记住，<strong class="lj jd">除了 catboost 包，original API 和 sklearn API 可能会有不同的参数名，即使它们表示相同的参数。</strong>我在上面的总结中包含了两个参数名称。</p></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="5c0b" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">2.超参数调整的四种基本方法</h1><h2 id="b8cf" class="ny mz it bd na nz oa dn ne ob oc dp ni lq od oe nk lu of og nm ly oh oi no iz bi translated">#1 手动调谐</h2><p id="c320" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">通过手动调整，基于当前选择的参数和它们的得分，我们改变它们中的一部分，再次训练模型，并检查得分的差异，而不使用自动选择参数来改变新参数的值。</p><p id="9fe3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">手动调谐</strong>的优点是:</p><ul class=""><li id="9957" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">您可以记住超参数的行为，并在另一个项目中使用您的知识。因此，我建议至少对主要型号进行一次手动调优。</li></ul><p id="1c1c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">缺点</strong>是:</p><ul class=""><li id="ba1f" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">需要手工操作。</li><li id="9205" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">你可能会过度思考乐谱的意外移动，而没有尝试很多次并检查它是否是广义的移动。</li></ul><p id="ed3a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">书中给出了一些手动调谐的例子:</p><ul class=""><li id="7d50" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">当你发现有太多无用的变量输入到模型中时，你就增加正则化参数的权重。</li><li id="fdfa" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">当你认为变量间的相互作用在模型中没有被考虑很多时，你可以增加分裂的次数(GBDT 案例)。</li></ul><p id="171c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦您了解了下面讨论的其他方法，<strong class="lj jd">您可能会说，如果远不是达到全局最佳参数的最佳方法，我们为什么要进行手动调整</strong>，但在实践中，这在早期阶段很好地用于了解对超参数变化的敏感度，或在最后阶段进行调整。</p><p id="79b4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另外，<strong class="lj jd">令人惊讶的是，许多顶尖的 Kagglers 更喜欢使用手动调谐来进行网格搜索或随机搜索。</strong></p><h2 id="ad2c" class="ny mz it bd na nz oa dn ne ob oc dp ni lq od oe nk lu of og nm ly oh oi no iz bi translated">#2 网格搜索</h2><p id="5228" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated"><strong class="lj jd">网格搜索</strong>是一种方法，我们从准备候选超参数集开始，为每个候选超参数集训练模型，并选择性能最佳的超参数集。</p><p id="6748" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通常通过 <code class="fe op oq or os b"><a class="ae nx" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">sklearn.model_selection</a></code>的<code class="fe op oq or os b"><a class="ae nx" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a></code> <a class="ae nx" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">等支持库自动完成参数设置和评估。</a></p><p id="9944" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">这种方法</strong>的优点是:</p><ul class=""><li id="f75f" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">您可以涵盖所有可能的预期参数集。无论你多么坚信其中一套是最可行的，谁知道，邻居可能会更成功。网格搜索不会失去这种可能性。</li></ul><p id="fa91" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">缺点</strong>在于它是:</p><ul class=""><li id="d8db" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">一个超参数集的一次运行需要一些时间。整个参数集的运行时间可能很长，因此要探索的参数数量有实际限制。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h2 id="acb9" class="ny mz it bd na nz oa dn ne ob oc dp ni lq od oe nk lu of og nm ly oh oi no iz bi translated">#3 随机搜索</h2><p id="3f36" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">另一方面，随机搜索是一种方法，其中我们像网格搜索一样准备候选超参数集，但是接下来从准备的超参数搜索空间中随机选择超参数集。重复随机选择、模型训练和评估指定的次数，以搜索超参数。最后，选择性能最佳的超参数集。</p><p id="a1cf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以通过指定参数的密度函数而不是特定值来控制随机性，例如均匀分布或正态分布。</p><p id="dbb9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里再次说明，设置参数和评估通常是通过 <code class="fe op oq or os b"><a class="ae nx" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">sklearn.model_selection</a></code>的<code class="fe op oq or os b"><a class="ae nx" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">RandomizedSearchCV</a></code> <a class="ae nx" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank">等支持库自动完成的。</a></p><p id="27e3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">使用随机搜索</strong>的优点是:</p><ul class=""><li id="a82a" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">您不必担心运行时间，因为您可以控制参数搜索的数量。</li></ul><p id="6859" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">缺点</strong>是:</p><ul class=""><li id="2907" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">应该有一些妥协，最终选择的超参数集可能不是您在搜索中输入的范围之外的真正最佳值。</li><li id="097d" class="md me it lj b lk mm ln mn lq mo lu mp ly mq mc mi mj mk ml bi translated">根据搜索次数和参数空间的大小，有些参数可能没有被充分研究。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ot ou l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/1f308f3cecc8922e13ab8492358ec34f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gaxqCRZa22tunZBJ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">The ‘world famous’ grid search vs. random search illustration by James Bergstra James, Yoshua Bengio on “Random Search for HyperParameter Optimization” ( <a class="ae nx" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</a>)</figcaption></figure><h2 id="b6ee" class="ny mz it bd na nz oa dn ne ob oc dp ni lq od oe nk lu of og nm ly oh oi no iz bi translated">#4 贝叶斯优化</h2><p id="06ba" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated"><strong class="lj jd"/><a class="ae nx" href="https://arxiv.org/abs/1807.02811" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">贝叶斯优化</strong> </a> <strong class="lj jd">的基本概念是“如果我们随机搜索一些点，并且知道其中一些点比其他点更有希望，为什么我们不在它们周围再看一看？”</strong></p><p id="79e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在贝叶斯优化中，它从随机开始，基于贝叶斯方法缩小搜索空间。</p><p id="a8ed" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你知道贝叶斯定理，你可以理解它只是通过开始随机搜索将关于可能超参数的信念的先验分布更新为后验分布。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/52f1364d75112c4806501f11a6eb07b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IiM9Zq5ejQERpD_3sfYTxg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Illustration of Bayesian optimization. Based on the Bayesian update next try will happen on star point in the bottom chart ( <a class="ae nx" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">https://github.com/fmfn/BayesianOptimization</a>)</figcaption></figure><p id="df91" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">贝叶斯优化方法</strong>的优势在于:</p><ul class=""><li id="b962" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">搜索是潜在有效的(<a class="ae nx" rel="noopener" target="_blank" href="/automated-machine-learning-hyperparameter-tuning-in-python-dfda59b72f8a">但不一定</a>)。</li></ul><p id="ec6d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">缺点</strong>是:</p><ul class=""><li id="fc8b" class="md me it lj b lk ll ln lo lq mf lu mg ly mh mc mi mj mk ml bi translated">可能陷入局部最优。</li></ul><p id="ffde" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">做贝叶斯优化常见的有两个 python 库，<code class="fe op oq or os b"><a class="ae nx" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">hyperopt</a></code>和<code class="fe op oq or os b"><a class="ae nx" href="https://github.com/optuna/optuna" rel="noopener ugc nofollow" target="_blank">optuna</a></code>。还有其他名字如<code class="fe op oq or os b"><a class="ae nx" href="https://github.com/SheffieldML/GPyOpt" rel="noopener ugc nofollow" target="_blank">gpyopt</a></code>、<code class="fe op oq or os b"><a class="ae nx" href="https://github.com/JasperSnoek/spearmint" rel="noopener ugc nofollow" target="_blank">spearmint</a></code>、<code class="fe op oq or os b"><a class="ae nx" href="https://scikit-optimize.github.io/" rel="noopener ugc nofollow" target="_blank">scikit-optimize</a></code>。</p><p id="f530" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面是使用<code class="fe op oq or os b">hyperopt</code>的示例代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ot ou l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/759c70bfc3cbeb89766edba6483bf032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*B8ytKyO260NyyCoo.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Bayesian Versus Frequentist ( <a class="ae nx" href="https://irsae.no/blog-report-international-summer-school-on-bayesian-modelling/" rel="noopener ugc nofollow" target="_blank">https://irsae.no/blog-report-international-summer-school-on-bayesian-modelling/</a>)</figcaption></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="33c9" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">3.超参数调整和交叉验证中的折叠</h1><p id="7d35" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">在上面讨论的任何超参数调整方法中，为了避免<strong class="lj jd">过拟合，</strong>首先对数据进行折叠，对训练折叠数据和非折叠数据重复训练和验证，这一点很重要。</p><p id="e217" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此外，如果在交叉验证中继续使用相同的褶皱分割(以便比较模型)，那么<strong class="lj jd">您的具有选定超参数的模型可能过度拟合褶皱，但没有机会识别它</strong>。</p><p id="5b19" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，<strong class="lj jd">通过更改随机数种子</strong>，将折叠分割从超参数调整更改为交叉验证非常重要。</p><p id="5461" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另一种方法可能是做<a class="ae nx" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">嵌套交叉验证</strong> </a>。在嵌套交叉验证中，有两个层次交叉验证循环:外部和内部。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/d6915f0fbda7cb379599baab8b56cb0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enfSht7CZLMdLxVdXnZrUA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Nested cross-validation</figcaption></figure><p id="7651" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">嵌套交叉验证的一个巨大缺点是，它显著增加了运行时间，增加了内循环折叠的次数。我个人觉得做嵌套的交叉验证太多了，对于额外的运行时间来说好处不大，即使在 Kaggle 竞争中也不是主流。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ot ou l"/></div></figure></div><div class="ab cl mr ms hx mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="im in io ip iq"><h1 id="4846" class="my mz it bd na nb nc nd ne nf ng nh ni ki nj kj nk kl nl km nm ko nn kp no np bi translated">4.结论</h1><p id="45c2" class="pw-post-body-paragraph lh li it lj b lk nq kd lm ln nr kg lp lq oj ls lt lu ok lw lx ly ol ma mb mc im bi translated">我们在超参数调整中采用的方法将随着建模阶段的发展而发展，首先通过手动或网格搜索从少量参数开始，随着模型随着有效特征的增加而变得更好，通过随机搜索或贝叶斯优化查看更多参数，但我们如何做没有固定的规则。</p><p id="6cf8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">模型会有很多超参数，因此找到重要的参数及其搜索范围并不是一件容易的事情。然而，像 GBDT 家族这样受欢迎的模型是精心制作的，我们对它们知道得足够多，知道从哪里开始和去哪里。</p><p id="022a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们必须担心数据折叠不会过度拟合模型，那么必须将折叠分割从超参数调整更改为模型选择交叉验证。</p><p id="394a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你发现遗漏了什么，请告诉我。它将帮助我改进这个总结，为读者提供更好的信息！</p></div></div>    
</body>
</html>