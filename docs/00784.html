<html>
<head>
<title>Image forgery detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图像伪造检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-forgery-detection-2ee6f1a65442?source=collection_archive---------1-----------------------#2019-02-06">https://towardsdatascience.com/image-forgery-detection-2ee6f1a65442?source=collection_archive---------1-----------------------#2019-02-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7444" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用 CNN 的力量来检测图像操纵</h2></div><p id="1b86" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着脸书和 Instagram 等社交网络服务的出现，过去十年中生成的图像数据量大幅增加。使用图像(和视频)处理软件，如<a class="ae lb" href="https://www.gimp.org/" rel="noopener ugc nofollow" target="_blank"> GNU Gimp </a>、<a class="ae lb" href="https://www.adobe.com/in/products/photoshop.html" rel="noopener ugc nofollow" target="_blank"> Adobe Photoshop </a>创建篡改过的图像和视频，是脸书等互联网公司的一大担忧。这些图像是假新闻的主要来源，并经常被恶意使用，如煽动暴民。在根据可疑图像采取行动之前，我们必须验证其真实性。IEEE 信息取证与安全技术委员会(IFS-TC)于 2013 年发起了一项检测和定位取证挑战，即<a class="ae lb" href="http://ifc.recod.ic.unicamp.br/fc.website/index.py?sec=0" rel="noopener ugc nofollow" target="_blank">首届图像取证挑战</a>，以解决这一问题。他们提供了一个开放的数字图像数据集，包括在不同光照条件下拍摄的图像和使用以下算法创建的伪造图像:</p><ul class=""><li id="17a7" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">内容感知填充和修补匹配(用于复制/粘贴)</li><li id="f720" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">内容感知修复(用于复制/粘贴和拼接)</li><li id="222d" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">克隆图章(用于复制/粘贴)</li><li id="ca2e" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">线缝雕刻(图像重定向)</li><li id="76ea" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">修复(受损部分的图像重建——复制/粘贴的特殊情况)</li><li id="c64c" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">阿尔法抠图(用于拼接)</li></ul><p id="1288" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">挑战包括两个阶段。</p><ol class=""><li id="0118" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lq li lj lk bi translated">第一阶段要求参与团队将图像分类为伪造的或原始的(从未被篡改)</li><li id="9c47" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">第二阶段要求他们检测/定位伪造图像中的伪造区域</li></ol><p id="7e25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章将讲述一种解决第一阶段挑战的深度学习方法。从数据清洗，预处理，CNN 架构到训练和评估的一切都将被阐述。</p><h1 id="1ee0" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">为什么用 CNN？</h1><p id="588e" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在人工智能的深度学习时代之前，即在 2012 年的<a class="ae lb" href="http://www.image-net.org/challenges/LSVRC/2012/index" rel="noopener ugc nofollow" target="_blank">图像网络挑战赛之前，图像处理领域的研究人员过去常常设计手工制作的功能来解决一般图像处理问题，特别是图像分类问题。一个这样的例子是用于边缘检测的 Sobel 内核。之前使用的图像取证工具可分为 5 类，即</a></p><ol class=""><li id="e1d2" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lq li lj lk bi translated">基于像素的技术，检测像素级引入的统计异常</li><li id="66eb" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">基于格式的技术，利用特定有损压缩方案引入的统计相关性</li><li id="282b" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">基于相机的技术，利用相机镜头、传感器或片内后处理引入的伪像</li><li id="7d65" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">基于物理的技术，明确地模拟和检测物理对象、光和相机之间的三维交互中的异常</li><li id="cae7" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">基于几何的技术，对世界上的对象及其相对于相机的位置进行测量</li></ol><p id="7eeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">礼貌:<a class="ae lb" href="https://ieeexplore.ieee.org/abstract/document/4806202" rel="noopener ugc nofollow" target="_blank">https://ieeexplore.ieee.org/abstract/document/4806202</a></p><p id="6f60" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">几乎所有这些技术都利用了图像的基于内容的特征，即图像中存在的视觉信息。CNN 的<a class="ae lb" href="https://medium.com/@gopalkalpande/biological-inspiration-of-convolutional-neural-network-cnn-9419668898ac" rel="noopener">灵感来自视觉皮层</a>。从技术上讲，这些网络被设计成提取对分类有意义的特征，即最小化损失函数的特征。网络参数——通过梯度下降学习内核权重，以便从输入网络的图像中生成最具鉴别性的特征。这些特征然后被馈送到一个完全连接的层，该层执行分类的最终任务。</p><p id="ca3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在看了一些伪造的图像后，很明显通过人类视觉皮层定位伪造区域是可能的。因此，CNN 是这项工作的完美深度学习模型。如果人类的视觉皮层能够探测到它，那么在一个专门为这项任务设计的网络中肯定会有更多的能量。</p><h1 id="80a7" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">资料组</h1><p id="f886" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在对数据集进行概述之前，先要弄清楚所使用的术语</p><ul class=""><li id="88c7" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">假图像:使用两种最常见的操作，即复制/粘贴和图像拼接，对图像进行了处理/篡改。</li><li id="f50a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">原始图像:除了根据比赛规则调整所有图像的尺寸以达到标准尺寸之外，没有被处理过的图像。</li><li id="6f02" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">图像拼接:拼接操作可以组合人的图像，给建筑物添加门，给停车场添加树和汽车等。拼接图像还可以包含复制/粘贴操作产生的部分。接收拼接部分的图像称为“主”图像。与宿主图像拼接在一起的部分被称为“外星人”。</li></ul><p id="e6fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一阶段和第二阶段的整个数据集可以在<a class="ae lb" href="http://ifc.recod.ic.unicamp.br/fc.website/index.py?sec=5" rel="noopener ugc nofollow" target="_blank">这里</a>找到。对于这个项目，我们将只使用列车组。它包含两个目录——一个包含假图像及其相应的遮罩，另一个包含原始图像。伪图像的遮罩是描述伪图像的拼接区域的黑白(非灰度)图像。蒙版中的黑色像素代表在源图像中执行操作以获得伪造图像的区域，具体来说，它代表拼接区域。</p><div class="mo mp mq mr gt ab cb"><figure class="ms mt mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><img src="../Images/372883f9de16b11100977db3d76a3381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*4ckiWdU6dAYFpMxcdzB_MA.png"/></div></figure><figure class="ms mt mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><img src="../Images/0a498dc550719d3bfb2a5b32382b759a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tnUwrsY7pEK-yjZKv6IrqQ.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk nj di nk nl">Example of a fake image and corresponding mask</figcaption></figure></div><p id="8e29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该数据集由 1050 幅原始图像和 450 幅伪图像组成。彩色图像通常是 3 个通道的图像，每个通道用于红色、绿色和蓝色，然而有时可能存在用于黄色的第四通道。我们数据集中的图像是 1、3 和 4 通道图像的混合。在查看了几幅 1 通道图像(即灰度图像)后，很明显这些图像</p><ol class=""><li id="8d4d" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lq li lj lk bi translated">数量非常少</li><li id="cde5" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">是黑色或蓝色的溪流</li></ol><p id="e57e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">挑战设置者故意添加这些图像，因为他们希望解决方案对这种噪声具有鲁棒性。尽管一些蓝色图像可以是晴朗天空的图像。因此，它们中的一些被包含在内，而另一些则作为噪声被丢弃。来到四通道图像——他们也没有任何有用的信息。它们只是用 0 值填充的像素网格。因此，清理后的原始数据集包含大约 1025 幅 RGB 图像。</p><p id="1905" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">伪图像是 3 和 4 通道图像的混合，然而，它们都没有噪声。相应的遮罩是 1、3 和 4 通道图像的混合。我们将使用的特征提取只需要来自掩模的一个通道的信息。因此，我们的假货图像语料库有 450 个假货。接下来，我们做了一个训练测试分割，保留 1475 个图像中的 20%用于最终测试。</p><h1 id="e932" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">训练集上的特征提取</h1><p id="a015" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">当前状态的数据集不适合训练模型。它必须被转换成非常适合手头任务的状态，即检测由于伪造操作而引入的像素级异常。从<a class="ae lb" href="https://ieeexplore.ieee.org/abstract/document/7823911" rel="noopener ugc nofollow" target="_blank">这里的</a>中汲取灵感，我们设计了以下方法来从给定的数据中创建相关的图像。</p><p id="6a44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于每一个假图像，我们都有一个相应的面具。我们使用该掩模沿着拼接区域边界对伪图像进行采样，以确保图像的伪造部分和非伪造部分至少有 25%的贡献。这些样本将具有只有在假图像中才会出现的有区别的边界。这些界限是我们设计的 CNN 要学习的。由于遮罩的所有 3 个通道包含相同的信息(不同像素的图像的虚假部分)，我们只需要 1 个通道来提取样本。</p><p id="b9da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了使边界更加清晰，在使用高斯滤波器去噪后，使用<a class="ae lb" href="https://en.wikipedia.org/wiki/Otsu%27s_method" rel="noopener ugc nofollow" target="_blank"> Otsu 的阈值</a>(在<a class="ae lb" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"> OpenCV </a>中实现)将灰度图像转换为二进制。在此操作之后，采样仅仅是移动 64×64 的窗口(步长为 8)通过假图像，并计数相应蒙版中的 0 值(黑色)像素，并在值位于某个区间的情况下进行采样。</p><figure class="mo mp mq mr gt mt"><div class="bz fp l di"><div class="nm nn l"/></div></figure><div class="mo mp mq mr gt ab cb"><figure class="ms mt mu mv mw mx my paragraph-image"><img src="../Images/4b72367d7500489d254c715eab8a259a.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*wztLRloEBEyDxbxC6WE_Iw.png"/></figure><figure class="ms mt mu mv mw mx my paragraph-image"><img src="../Images/2856bfc4646d9f229173dc3c2a4a8bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*bpMibvCG2K0zhBI0TWhgvQ.png"/><figcaption class="nf ng gj gh gi nh ni bd b be z dk nj di nk nl">Boundaries in a binary mask are much more distinct than in grayscale</figcaption></figure></div><p id="3e07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">经过采样，我们从假图像中得到了 175，119 个 64×64 的面片。为了生成 0 个标记的(原始的)补丁，我们从真实图像中采样了大致相同的数量。最后，我们有 350，728 个补丁，这些补丁被分成训练集和交叉验证集。</p><p id="4dc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了一个大的高质量输入图像数据集。是时候试验各种 CNN 架构了。</p><h1 id="e2a3" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">定制 CNN 架构</h1><p id="2490" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">我们尝试的第一个架构受到了最初的<a class="ae lb" href="https://ieeexplore.ieee.org/abstract/document/7823911" rel="noopener ugc nofollow" target="_blank">研究论文</a>中给出的架构的启发。他们的输入图像大小为 128×128×3，因此网络很大。由于我们只有一半的空间大小，我们的网络也更小。这是第一个尝试的建筑。</p><figure class="mo mp mq mr gt mt gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi no"><img src="../Images/4fccabb209c42a2362927f7660afe3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_lfzt-hGL25YpoP0o8IxQg.jpeg"/></div></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">First architecture</figcaption></figure><p id="031d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里绿色层是卷积层，蓝色层是最大池。该网络在 150，000 个训练样本(用于测试目的)和 25，000 个验证样本上被训练。该网络有 8，536 个参数，与训练样本相比相对较少，因此避免了对更激进的退出的需要。0.2 的退出率适用于 20 个单位的扁平化产出。我们使用 Adam 优化器，默认值为学习率(0.001)和 beta_1、beta_2。大约 ___ 个时期后，结果如下</p><p id="144a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练准确率:77.13%，训练损失:0.4678</p><p id="e587" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证准确率:75.68%，验证损失:0.5121</p><p id="9787" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些数字并不令人印象深刻，因为 2012 年 CNN 以巨大优势击败了专家长达一年的研究。然而，考虑到我们完全没有使用图像取证知识(像素统计和相关概念)来获得看不见的数据的 ___ 准确性，这些数字也不是很糟糕。</p><h1 id="e7ad" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">迁移学习</h1><p id="187f" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">既然 CNN 在<a class="ae lb" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>分类任务中击败了所有经典的机器学习算法，为什么不利用这些强大机器中的一个来解决手头的问题呢？这就是<a class="ae lb" rel="noopener" target="_blank" href="/transfer-learning-946518f95666">转移学习</a>背后的理念。简而言之，我们使用预训练模型的权重来解决我们的问题，该模型可能是在更大的数据集上训练的，并且在解决问题时给出了更好的结果。换句话说，我们“转移”一个模型的知识来构建我们的模型。在我们的例子中，我们使用在 ImageNet 数据集上训练的<a class="ae lb" href="https://www.kaggle.com/keras/vgg16" rel="noopener ugc nofollow" target="_blank"> VGG16 </a>网络来矢量化数据集中的图像。</p><p id="d6f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从<a class="ae lb" href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" rel="noopener ugc nofollow" target="_blank">这里</a>获取想法，我们尝试了两种方法</p><ol class=""><li id="16a5" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lq li lj lk bi translated">使用 VGG16 输出的瓶颈特性，并在此基础上构建一个浅层网络</li><li id="fe61" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">微调上面(1)中 vgg 16+浅层模型的最后一个卷积层</li></ol><p id="d17f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很明显，2 比 1 给出了更好的结果。在最终实现之前，我们尝试了多种浅层网络架构</p><figure class="mo mp mq mr gt mt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3cf8518fe7efb300e25204bc7993bdb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*WdVfZA586HRKnddyjhqSIg.jpeg"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Top layer architecture</figcaption></figure><p id="03fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">展平层的输入是 VGG16 输出的瓶颈特征。这些是形状为(2×2×512)的张量，因为我们使用了 64×64 的输入图像。</p><p id="56cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以上架构给出了以下结果</p><p id="1d74" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练准确率:83.18%，训练损失:0.3230</p><p id="25c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证准确率:84.26%，验证损失:0.3833</p><p id="0e91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它使用 Adam optimizer 进行训练，具有每 10 个时期后降低的自定义学习率(除了 Adam 在每批后的定期更新之外)。</p><figure class="mo mp mq mr gt mt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/71bfcb4db9a03216a394e3a363cb6dbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*owzc-Jcw8gm91YAw8WJ8AQ.png"/></div><figcaption class="nf ng gj gh gi nh ni bd b be z dk">Custom update rule</figcaption></figure><p id="e36d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二种方法需要最后一层的<strong class="kh ir">微调</strong>。这里需要注意的重要一点是，我们必须使用预训练的顶层模型进行微调。目标是稍微改变已经学习的权重，以便更好地适应数据。如果我们使用一些随机初始化的权重，微小的变化对它们没有任何好处，大的变化会破坏卷积层的学习权重。我们还需要一个非常小的学习率来微调我们的模型(原因和上面提到的一样)。在<a class="ae lb" href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" rel="noopener ugc nofollow" target="_blank">这个</a>帖子里，建议使用 SGD 优化器进行微调。然而，我们观察到亚当在这项任务中表现优于 SGD。</p><p id="feab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">微调模型给出了以下结果</p><p id="2c3f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练准确率:99.16%，训练损失:0.018</p><p id="3aad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证准确率:94.77%，验证损失:0.30</p><p id="5d24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">稍微过度拟合的模型，可以通过使用更小的学习率来补救(我们使用 1e-6)。</p><p id="6e29" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了 VGG16，我们还尝试了在 Image-Net 数据集上预训练的 ResNet50 和 VGG19 模型的瓶颈特征。ResNet50 的特性优于 VGG16。VGG19 没有给出一个很满意的表现。我们使用相同的学习率更新策略，以类似于 VGG16 的方式微调 ResNet50 架构(最后一个卷积层),它给出了更有希望的结果，过拟合问题更少。</p><p id="066f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">训练准确率:98.65%，训练损失:0.048</p><p id="d339" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">验证准确率:95.22%，验证损失:0.18</p><h1 id="0e3d" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">测试数据的最终模型预测</h1><p id="1394" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">为了从先前创建的测试集中采样图像，我们采用了与用于创建训练和交叉验证集类似的策略，即使用它们的遮罩在边界处采样伪图像，并采样相同数量的具有相同尺寸的原始图像。微调的 VGG16 模型用于预测这些补丁的标签，并给出以下结果</p><p id="dab1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试准确率:94.65%，测试损失:0.31</p><p id="e32b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，ResNet50 对测试数据给出了以下结果</p><p id="245a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">测试准确率:95.09%，测试损失:0.19</p><p id="52dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如我们所见，我们的模特表现不错。我们仍有很大的改进空间。如果通过数据扩充(剪切、调整大小、旋转和其他操作)可以生成更多的数据，也许我们可以微调更多层的 SOTA 网络。</p><p id="4771" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们谈到了检测假图像。然而，一旦检测到伪造图像，我们必须确定该图像中的伪造区域。假图像中拼接区域的定位将是下一篇文章的主题。这部分的全部代码可以在<a class="ae lb" href="https://github.com/vishu160196/image-forgery-detection" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="d27f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个帖子到此为止。请在评论区告诉我其他检测假图片的好方法。下次见…再见。</p><h1 id="0086" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">来源</h1><ol class=""><li id="3fcd" class="lc ld iq kh b ki mj kl mk ko nr ks ns kw nt la lq li lj lk bi translated">http://ifc.recod.ic.unicamp.br/fc.website/index.py?sec=5<a class="ae lb" href="http://ifc.recod.ic.unicamp.br/fc.website/index.py?sec=5" rel="noopener ugc nofollow" target="_blank"/></li><li id="def2" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated">【https://ieeexplore.ieee.org/abstract/document/4806202 T4】</li><li id="8720" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated"><a class="ae lb" href="https://www.youtube.com/watch?v=uihBwtPIBxM" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=uihBwtPIBxM</a></li><li id="a817" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated"><a class="ae lb" href="https://medium.com/@gopalkalpande/biological-inspiration-of-convolutional-neural-network-cnn-9419668898ac" rel="noopener">https://medium . com/@ gopalkalpande/biological-inspiration-of-convolutionary-neural-network-CNN-9419668898 AC</a></li><li id="9a6f" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated"><a class="ae lb" href="https://ieeexplore.ieee.org/abstract/document/7823911" rel="noopener ugc nofollow" target="_blank">https://ieeexplore.ieee.org/abstract/document/7823911</a></li><li id="9821" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lq li lj lk bi translated"><a class="ae lb" href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/building-powerful-image-class ification-models-using-very-little-data . html</a></li></ol></div></div>    
</body>
</html>