<html>
<head>
<title>Speaker Diarization with Kaldi</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带 Kaldi 的扬声器二进制化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speaker-diarization-with-kaldi-e30301b05cc8?source=collection_archive---------3-----------------------#2019-02-28">https://towardsdatascience.com/speaker-diarization-with-kaldi-e30301b05cc8?source=collection_archive---------3-----------------------#2019-02-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4799" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">随着语音生物识别和语音识别系统的兴起，处理多个说话者的音频的能力变得至关重要。本文是使用 Kaldi X-Vectors(一种最先进的技术)完成这一过程的基础教程。</h2></div><p id="2f76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在大多数真实世界的场景中，语音不会出现在只有一个说话者的明确定义的音频片段中。在我们的算法需要处理的大多数对话中，人们会相互打断，而切断句子之间的音频将不是一项简单的任务。</p><p id="e641" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除此之外，在许多应用中，我们希望在一次对话中识别多个发言者，例如在编写会议协议时。对于这样的场合，识别不同的说话者并连接同一说话者下的不同句子是一项关键任务。</p><p id="8a07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Speaker_diarisation" rel="noopener ugc nofollow" target="_blank"> <em class="lc">说话人二进制化</em> </a>就是针对这些问题的解决方案。通过这一过程，我们可以根据说话者的身份将输入音频分割成片段。<strong class="kh ir">这个问题可谓是谁在什么时候讲的？</strong>"在一段音频中。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/8766d2922bffa78c17473ff9ba990fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*TYEMaS4dj2CjanKZ"/></div></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk"><strong class="bd lt">Attributing different sentences to different people is a crucial part of understanding a conversation. </strong>Photo by <a class="ae lb" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="3bc1" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">历史</h1><p id="ccc3" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">第一个基于 ML 的扬声器二进制化工作开始于 2006 年左右，但直到 2012 年左右才开始有重大改进(<a class="ae lb" href="http://www.eurecom.fr/en/publication/3152/download/mm-publi-3152.pdf" rel="noopener ugc nofollow" target="_blank"> Xavier，2012 </a>)，当时这被认为是一项极其困难的任务。那时的大多数方法都是基于<a class="ae lb" href="https://scikit-learn.org/stable/modules/mixture.html" rel="noopener ugc nofollow" target="_blank"> GMM </a>或<a class="ae lb" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">嗯</a>的(比如<a class="ae lb" href="https://www.crim.ca/perso/patrick.kenny/FAtheory.pdf" rel="noopener ugc nofollow" target="_blank"> JFA </a>)，不涉及任何神经网络。</p><p id="43d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个真正的重大突破发生在 LIUM 的发布上，这是一个用 Java 编写的致力于说话者二进制化的开源软件。第一次有了一种自由分布的算法，它可以以合理的精度执行这项任务。LIUM 核心中的算法是一种复杂的机制，它将 GMM 和 I-Vectors 结合在一起，这种方法曾经在说话人识别任务中取得了最先进的结果。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/9fb70637a76a4039b39c74b0bb7477aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uCvRbS-IXjGQMsG2Bu8eIg.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">The entire process in the LIUM toolkit. An repetitive multi-part process with a lot of combined models.</figcaption></figure><p id="4d41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天，这种复杂的多部分算法系统正在被许多不同领域的神经网络所取代，如<a class="ae lb" href="https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272" rel="noopener">图像分割</a>甚至<a class="ae lb" href="http://proceedings.mlr.press/v48/amodei16.pdf" rel="noopener ugc nofollow" target="_blank">语音识别</a>。</p><h1 id="79c4" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">x 向量</h1><p id="2ae2" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">最近的一项突破是由<em class="lc"> D. Snyder，D. Garcia-Romero，D. Povey 和 S. Khudanpur </em>在一篇名为“<a class="ae lb" href="http://danielpovey.com/files/2017_interspeech_embeddings.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lc">用于文本无关说话人验证的深度神经网络嵌入</em> </a>”的文章中发表的，该文章提出了一个模型，该模型后来被命名为“X-Vectors”。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/8d300f540d7eb3d5a4b7b268e07989f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*qtHhAZcNiFwb--Yid5t1ZA.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">A diagram of the proposed neural network, The different parts of the networks are highlighted on the right. From <a class="ae lb" href="http://danielpovey.com/files/2017_interspeech_embeddings.pdf" rel="noopener ugc nofollow" target="_blank">The original article</a>.</figcaption></figure><p id="8557" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在该方法中，网络的输入是以<a class="ae lb" href="http://www.practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="noopener ugc nofollow" target="_blank"> MFCC </a>形式的原始音频。这些特征被输入到一个神经网络中，该网络可以分为四个部分:</p><ol class=""><li id="073f" class="mt mu iq kh b ki kj kl km ko mv ks mw kw mx la my mz na nb bi translated"><strong class="kh ir">帧级层</strong>——这些层本质上是一个<a class="ae lb" href="https://en.wikipedia.org/wiki/Time_delay_neural_network" rel="noopener ugc nofollow" target="_blank"> TDNN </a>(时间延迟神经网络)。TDNN 是在神经网络日益普及之前的 90 年代发明的一种架构，然后在 2015 年被<a class="ae lb" href="https://www.danielpovey.com/files/2015_interspeech_multisplice.pdf" rel="noopener ugc nofollow" target="_blank">“重新发现”为语音识别系统的一个关键部分。这个网络本质上是一个全连接的神经网络，它考虑了样本的时间滑动窗口。它被认为比 LSTM 快得多。</a></li><li id="bfb9" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated"><strong class="kh ir">统计池</strong> -因为每一帧给我们一个向量，我们需要以某种方式对这些向量求和。在这个实现中，我们取所有向量的平均值和标准偏差，并将它们连接成一个代表整个片段的<strong class="kh ir">向量。</strong></li><li id="5f4d" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated"><strong class="kh ir">全连接层</strong>——向量被送入两个全连接层(分别有 512 和 300 个神经元)，我们稍后会用到。第二层将具有 ReLU 非线性。</li><li id="454e" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated"><strong class="kh ir"> Softmax 分类器</strong> -一个简单的 Softmax 分类器，它在 ReLU 之后获取输出，并将片段分类到不同的说话者之一。</li></ol><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/011dd378d780fda374ea19c63cfd622c.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*wWZDFp6PD6NE-GZB-ds4HQ.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">A visualization of a TDNN, The first part of the X-Vectors System.</figcaption></figure><p id="4255" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X-Vectors 的真正力量不在于(仅仅)对不同的说话者进行分类，还在于它使用两个完全连接的层作为整个片段的嵌入式表示。在文章中，他们使用这些表示对一个完全不同于他们训练数据集的数据集进行分类。他们首先为每个新的音频样本创建嵌入，然后用<a class="ae lb" href="http://www.odyssey2016.org/papers/pdfs_stamped/12.pdf" rel="noopener ugc nofollow" target="_blank"> PLDA 后端相似性度量</a>对它们进行分类。</p><h2 id="b89f" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">X 向量二分化</h2><p id="33ab" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在我们理解了我们可以使用这些嵌入作为每个音频样本中的说话者的表示之后，我们可以看到该表示可以如何用于分割音频样本的子部分。那种方法在“<a class="ae lb" href="http://www.danielpovey.com/files/2018_interspeech_dihard.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lc">二进制化很难:首届迪哈德挑战赛</em> </a>中的一些经验教训”一文中有所描述。DIHARD 挑战特别困难，因为它包含了从电视节目到电话到儿童对话的 10 个不同的音频域，此外还有 2 个域只出现在验证集中。</p><p id="3572" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在文章中，他们描述了许多实践，这些实践将他们的二进制化算法带到了当前的艺术水平。虽然使用不同的技术(如<a class="ae lb" href="https://speech.fit.vutbr.cz/software/vb-diarization-eigenvoice-and-hmm-priors" rel="noopener ugc nofollow" target="_blank">变分贝叶斯</a>)极大地提高了模型的准确性，但它本质上是基于相同的 X 向量嵌入和 PLDA 后端。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/66effcf09e217236136fcab331922924.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*N8C4NSaEx3rCI24n7HYO-A.png"/></div><figcaption class="lp lq gj gh gi lr ls bd b be z dk">From the DIHARD Callenge article. You can see the major improvements of using X-Vectors. Previous works are in blue and the state of the art results are in red.</figcaption></figure><h1 id="a107" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">如何与卡尔迪合作</h1><p id="ef6d" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">首先，如果你之前没有用过 Kaldi，我强烈推荐你阅读我的第一篇关于使用 Kaldi 的文章<a class="ae lb" rel="noopener" target="_blank" href="/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6"><strong class="kh ir"/></a><strong class="kh ir">。没有语音识别系统的经验，很难开始使用该系统。</strong></p><p id="9dab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，你不需要重新训练 X-Vectors 网络或 PLDA 后端，你可以直接从<a class="ae lb" href="http://kaldi-asr.org/models/m3" rel="noopener ugc nofollow" target="_blank">官方网站</a>下载。如果你仍然想从头开始进行完整的训练，你可以遵循卡尔迪项目中的<a class="ae lb" href="https://github.com/kaldi-asr/kaldi/blob/master/egs/callhome_diarization/v2/run.sh" rel="noopener ugc nofollow" target="_blank"> call_home_v2 recipe </a>。</p><p id="813b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在你有了一个模型，不管你是创建了这个模型还是对它进行了预训练，我都将经历二进制化过程的不同部分。本演练改编自 GitHub 上的不同评论，主要是大卫的评论和文档。</p><h2 id="f0e9" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">准备数据</h2><p id="5a9e" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">你首先需要有一个普通的<em class="lc"> wav.scp </em>和<em class="lc"> segments </em>文件，方式<a class="ae lb" href="http://kaldi-asr.org/doc/data_prep.html" rel="noopener ugc nofollow" target="_blank">与 ASR 项目中的</a>相同。如果您想要一种简单的方法来创建这样的文件，您可以始终使用<a class="ae lb" href="https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/steps/compute_vad_decision.sh" rel="noopener ugc nofollow" target="_blank"><em class="lc">compute _ VAD _ decision . sh</em></a><em class="lc"/>脚本，然后在输出中使用<a class="ae lb" href="https://github.com/kaldi-asr/kaldi/blob/master/egs/callhome_diarization/v1/diarization/vad_to_segments.sh" rel="noopener ugc nofollow" target="_blank"><em class="lc">VAD _ to _ segments . sh</em></a><em class="lc"/>脚本。如果您不想分割音频，只需从头到尾将片段映射到话语。</p><p id="a3d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，您需要创建一个<em class="lc"> utt2spk </em>文件，该文件将片段映射到话语。您可以在 Linux 中通过运行命令<code class="fe nv nw nx ny b">awk ‘{$1, $2}’ segments &gt; utt2spk</code>简单地做到这一点。接下来，要创建其他必要的文件，只需将所有文件放在一个文件夹中，然后运行<a class="ae lb" href="https://github.com/kaldi-asr/kaldi/blob/master/egs/wsj/s5/utils/fix_data_dir.sh" rel="noopener ugc nofollow" target="_blank"> fix_data_dir.sh </a>脚本。</p><h2 id="e544" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">创建特征</h2><p id="1381" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">现在，您需要为音频创建一些特征，这些特征稍后将成为 X 向量提取器的输入。</p><p id="fa28" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将以与 ASR 项目中相同的方式开始创建 MFCC&amp;CMVN。请注意，您需要有一个与您接受的培训相匹配的 mfcc.conf 文件。如果您使用预训练模型，请使用<a class="ae lb" href="https://github.com/kaldi-asr/kaldi/tree/master/egs/callhome_diarization/v2/conf" rel="noopener ugc nofollow" target="_blank">这些文件</a>。</p><p id="86e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于 MFCC 创建，运行以下命令:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="d13f" class="ni lv iq ny b gy od oe l of og">steps/make_mfcc.sh --mfcc-config conf/mfcc.conf --nj 60 \<br/>--cmd "$train_cmd_intel" --write-utt2num-frames true \<br/><strong class="ny ir">$data_dir</strong> exp/make_mfcc <strong class="ny ir">$mfccdir</strong></span></pre><p id="53bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后对<a class="ae lb" href="https://en.wikipedia.org/wiki/Cepstral_mean_and_variance_normalization" rel="noopener ugc nofollow" target="_blank"> CMVN </a>运行这个命令:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="7173" class="ni lv iq ny b gy od oe l of og">local/nnet3/xvector/prepare_feats.sh — nj 60 — cmd \ "$train_cmd_intel" <strong class="ny ir">$data_dir</strong> <strong class="ny ir">$cmn_dir</strong> <strong class="ny ir">$cmn_dir</strong></span></pre><p id="f30d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">完成数据后，使用<code class="fe nv nw nx ny b">utils/fix_data_dir.sh <strong class="kh ir">$data_dir</strong></code> <strong class="kh ir"> </strong>修复数据目录，然后使用<code class="fe nv nw nx ny b">cp <strong class="kh ir">$data_dir</strong>/segments <strong class="kh ir">$cmn_dir</strong>/ </code>将段文件移动到 CMVN 目录，之后使用<code class="fe nv nw nx ny b">utils/fix_data_dir.sh <strong class="kh ir">$cmn_dir </strong></code>再次修复 CMVN 目录。</p><h2 id="3505" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">创建 X 向量</h2><p id="435a" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">下一步是为你的数据创建 X 向量。我这里指的是导出文件夹，其中有 X-Vectors 作为<code class="fe nv nw nx ny b"><strong class="kh ir">$nnet_dir</strong></code>，如果你是从 Kaldi 网站下载的，使用路径"<em class="lc">exp/X Vectors _ sre _ combined</em>"然后运行该命令:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="0b47" class="ni lv iq ny b gy od oe l of og">diarization/nnet3/xvector/extract_xvectors.sh --cmd \ "$train_cmd_intel --mem 5G" \<br/>--nj 60 --window 1.5 --period 0.75 --apply-cmn false \<br/>--min-segment 0.5 <strong class="ny ir">$nnet_dir</strong> \<br/><strong class="ny ir">$cmn_dir</strong> <strong class="ny ir">$nnet_dir</strong>/exp/xvectors</span></pre><p id="c72c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，在这个例子中，我们使用 1.5 秒的窗口，每个窗口有 0.75 秒的偏移。降低偏移可能有助于捕捉更多细节。</p><h2 id="d3f7" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">用 PLDA 评分 X 向量</h2><p id="436c" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">现在你需要对 X 向量和 PLDA 后端之间的成对相似性进行评分。使用以下命令执行此操作:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="6803" class="ni lv iq ny b gy od oe l of og">diarization/nnet3/xvector/score_plda.sh \<br/>--cmd "$train_cmd_intel --mem 4G" \<br/>--target-energy 0.9 --nj 20 <strong class="ny ir">$nnet_dir</strong>/xvectors_sre_combined/ \<br/><strong class="ny ir">$nnet_dir</strong>/xvectors <strong class="ny ir">$nnet_dir</strong>/xvectors/plda_scores</span></pre><h2 id="8918" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">二化</h2><p id="38aa" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">最后一部分是对你创造的 PLDA 分数进行聚类。幸运的是，这也有一个脚本。但是，你可以通过两种方式做到这一点，有监督的方式和无监督的方式。</p><p id="8776" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在监督的方式下，你需要说出每句话中有多少人说话。当你在打一个只有两个发言人的电话时，或者在开一个有已知数量发言人的会议时，这尤其容易。要以监督的方式对分数进行聚类，您首先需要创建一个文件，将来自<em class="lc"> wav.scp </em>文件的话语映射到该话语中的发言者数量。该文件应该被命名为<em class="lc"> reco2num_spk </em>，看起来应该像这样:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="f317" class="ni lv iq ny b gy od oe l of og">rec1 2<br/>rec2 2<br/>rec3 3<br/>rec4 1</span></pre><p id="b1ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个重要的注意事项是，您需要根据说话者的数量来映射每个话语，而不是每个片段。创建了<em class="lc"> reco2num_spk </em>文件后，您可以运行以下命令:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="23e8" class="ni lv iq ny b gy od oe l of og">diarization/cluster.sh --cmd "$train_cmd_intel --mem 4G" --nj 20 \<br/>--reco2num-spk <strong class="ny ir">$data_dir</strong>/reco2num_spk \<br/><strong class="ny ir">$nnet_dir</strong>/xvectors/plda_scores \<br/><strong class="ny ir">$nnet_dir</strong>/xvectors/plda_scores_speakers</span></pre><p id="c10c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你不知道每句话有多少个说话者，你总是可以以一种无人监督的方式运行聚类，并尝试在脚本中调整阈值。一个好的起始值是 0.5。要以无人监督的方式进行聚类，请使用相同的脚本，但使用以下方式:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="8baa" class="ni lv iq ny b gy od oe l of og">diarization/cluster.sh --cmd "$train_cmd_intel --mem 4G" --nj 40 \<br/>--threshold <strong class="ny ir">$threshold</strong> \<br/><strong class="ny ir">$nnet_dir</strong>/xvectors/plda_scores \<br/><strong class="ny ir">$nnet_dir</strong>/xvectors/plda_scores_speakers</span></pre><h2 id="88e8" class="ni lv iq bd lw nj nk dn ma nl nm dp me ko nn no mg ks np nq mi kw nr ns mk nt bi translated">结果</h2><p id="3266" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在集群化之后，您将在<code class="fe nv nw nx ny b"><strong class="kh ir">$nnet_dir</strong>/xvectors/plda_scores_speakers</code>目录中拥有一个名为<em class="lc"> rttm </em>的输出文件。该文件将类似于以下内容:</p><pre class="le lf lg lh gt nz ny oa ob aw oc bi"><span id="6d24" class="ni lv iq ny b gy od oe l of og">SPEAKER rec1 0 86.200 16.400 &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; &lt;NA&gt;`<br/>SPEAKER rec1 0 103.050 5.830 &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; &lt;NA&gt;`<br/>SPEAKER rec1 0 109.230 4.270 &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; &lt;NA&gt;`<br/>SPEAKER rec1 0 113.760 8.625 &lt;NA&gt; &lt;NA&gt; 1 &lt;NA&gt; &lt;NA&gt;`<br/>SPEAKER rec2 0 122.385 4.525 &lt;NA&gt; &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt;`<br/>SPEAKER rec2 0 127.230 6.230 &lt;NA&gt; &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt;`<br/>SPEAKER rec2 0 133.820 0.850 &lt;NA&gt; &lt;NA&gt; 2 &lt;NA&gt; &lt;NA&gt;`</span></pre><p id="0c45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在该文件中，第 2 列是来自<em class="lc"> wav.scp </em>文件的记录 id，第 4 列是当前片段的开始时间，第 5 列是当前片段的大小，第 8 列是该片段中发言者的 ID。</p><p id="773a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至此，<strong class="kh ir">我们完成了二化过程</strong>！我们现在可以尝试使用语音识别技术来确定每个说话者说了什么，或者使用说话者验证技术来验证我们是否知道任何不同的说话者。</p></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="936b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢你读到的内容，你可以随时<strong class="kh ir">关注我的</strong> <a class="ae lb" href="https://twitter.com/YoavR7" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">推特</strong> </a>或者在这里给我留言。我还写了另一篇关于<a class="ae lb" rel="noopener" target="_blank" href="/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6"> Kaldi 的文章</a>并且我还有一个<a class="ae lb" href="https://github.com/YoavRamon/awesome-kaldi" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>充满了关于 Kaldi 的有用链接，请随意投稿！</p></div></div>    
</body>
</html>