<html>
<head>
<title>How to train the word2vec model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练 word2vec 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-the-word2vec-model-24704d842ec3?source=collection_archive---------6-----------------------#2019-09-10">https://towardsdatascience.com/how-to-train-the-word2vec-model-24704d842ec3?source=collection_archive---------6-----------------------#2019-09-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/11bd067a5c23952f5b2eb97be8ebfb69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrrC3cbM7vnOk7CBqElb2Q.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image by Chuk Yong from Pixabay</figcaption></figure><p id="73ca" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们已经讨论过<em class="ld"> word2vec </em> ( <a class="ae le" href="https://medium.com/@andrea.capitanelli/a-mathematical-introduction-to-word2vec-model-4cf0e8ba2b9" rel="noopener">此处</a>)的模型，现在来看看如何实现。为了做到这一点，我们必须建立一个<em class="ld">浅层</em>神经网络并训练它。</p><p id="b40e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld">注:我发现在 Medium 上使用 latex 是相当不满意和讨厌的，所以我选择了使用手绘配方(和方案)。我提前为我的书法道歉。</em></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="d01a" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">训练样本</h2><p id="f24c" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">首先要做的是生成训练数据。回到我们的文本，我们需要:</p><ul class=""><li id="f040" class="mk ml it kh b ki kj km kn kq mm ku mn ky mo lc mp mq mr ms bi translated">跳过课文中的每个单词:这是我们的中心词</li><li id="2f4d" class="mk ml it kh b ki mt km mu kq mv ku mw ky mx lc mp mq mr ms bi translated">在给定的距离内选择它的相邻单词:这些是上下文单词</li></ul><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/4872da1c2a60da22a50de3e874f1ddee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ItxCv0p28JTGFYt8ciOPg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Context with windows size of 3</figcaption></figure><ul class=""><li id="610b" class="mk ml it kh b ki kj km kn kq mm ku mn ky mo lc mp mq mr ms bi translated">生成数据对，每个样本由中心词和上下文词组成</li></ul><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/6aa0d66dbc8cd2f012e054ef3b057e1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtRiZDEvph7XMfGHn6M1XQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">The resulting 6 data pairs</figcaption></figure><p id="0bed" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们还注意从我们的文本中读取的唯一单词的总数，因为这个参数将决定我们嵌入的<strong class="kh iu">词汇</strong> <em class="ld">的大小。</em></p><p id="f4dd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此时，数据生成已完成，该轮到构建和训练神经网络了。</p><h2 id="3852" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">神经网络</h2><p id="02af" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">用于训练模型的神经网络的结构非常简单。它是一个前馈网络，最基本的形式包括:</p><ul class=""><li id="e958" class="mk ml it kh b ki kj km kn kq mm ku mn ky mo lc mp mq mr ms bi translated">一个<strong class="kh iu">输入层</strong>。它接受一个字<em class="ld"> w </em>并返回它的独热编码向量表示，<em class="ld"> x </em>。独热编码向量是除了<em class="ld">1</em>1 之外全为 0 的向量，它们用于指示从集合中挑选的一个成员。对于来自训练数据的每一对，<em class="ld"> w </em>是中心词。输入向量的大小是<em class="ld"> V </em>，即词汇量的大小。这一层没有激活功能。</li><li id="7625" class="mk ml it kh b ki mt km mu kq mv ku mw ky mx lc mp mq mr ms bi translated">一个<strong class="kh iu">隐藏层</strong>。它是一个具有<em class="ld"> V </em>行和<em class="ld"> E </em>列的矩阵，为每个输入单词<em class="ld"> x </em>分配其嵌入表示<em class="ld"> h </em>。就像图片中一样，每个第<em class="ld"> i 个</em>行代表第<em class="ld"> i 个嵌入</em>，输出将是一个大小为<em class="ld"> E. </em>的向量。如果不清楚为什么，可以自己尝试转置一个一键编码的向量，并将其乘以一个小矩阵。这一层也没有激活功能。</li><li id="f0a2" class="mk ml it kh b ki mt km mu kq mv ku mw ky mx lc mp mq mr ms bi translated">一个<strong class="kh iu">输出层</strong>。这一层与上一层相反。它作为一个具有<em class="ld"> E </em>行和<em class="ld"> V </em>列的矩阵，其中<em class="ld"> i-th </em>列包含第<em class="ld"> i-th </em>上下文词的嵌入。因此，给定嵌入<em class="ld"> h，</em>与这个矩阵相乘返回一个 V 大小的向量<em class="ld"> y </em>。作为激活函数，一个<em class="ld"> softmax </em>函数被应用于<em class="ld"> y. </em>生成一个矢量<em class="ld"> z </em>，在<em class="ld"> </em>中，其中<em class="ld">第 k 个</em>元素表示<em class="ld">第 k 个</em>字在<em class="ld"> w </em>  <em class="ld">上下文中的<strong class="kh iu">概率。</strong></em></li></ul><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/1d84442c3948778b39a847ce9aea4c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D5-K4Q8FHP36qyvXxEM26g.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Scheme of neural network</figcaption></figure><p id="e4b7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="ld"> z </em>和<em class="ld">y</em>相对于<em class="ld">W’</em>中第 j 个字的表达式分别为:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/2c3c2a92822e154392c5a95143004cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mPv4LLWSGhSXDcZvvOj6lQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Output of W’</figcaption></figure><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/674c4b83d3cfc9b166139a58b7447970.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlBYrt67TFBb6iNlpNTvqQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Output of softmax</figcaption></figure><p id="ed6a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于每个训练对，大小为 V 的向量<em class="ld"> t </em>是与该对相关联的基本事实。这也是一个热码编码向量，它在上下文单词处等于<em class="ld"> 1 </em>，在其他地方等于<em class="ld"> 0 </em>。换句话说，它是输出标签的指示器功能。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/7078441fe58c530bbfadcbf0820661c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtazTtAiKqjqGEa5GhrLVw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Truth vector</figcaption></figure><h2 id="2fdb" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">训练网络</h2><p id="e8f4" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">为了训练这样的神经网络，我们遵循以下步骤:</p><ul class=""><li id="e897" class="mk ml it kh b ki kj km kn kq mm ku mn ky mo lc mp mq mr ms bi translated">我们获取一个训练样本，并生成网络的输出值</li><li id="61db" class="mk ml it kh b ki mt km mu kq mv ku mw ky mx lc mp mq mr ms bi translated">我们通过比较模型预测和真实输出标签来评估<em class="ld">损失</em></li><li id="a10c" class="mk ml it kh b ki mt km mu kq mv ku mw ky mx lc mp mq mr ms bi translated">我们通过对评估的<em class="ld">损失</em>使用<a class="ae le" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>技术来更新网络的权重</li><li id="90fa" class="mk ml it kh b ki mt km mu kq mv ku mw ky mx lc mp mq mr ms bi translated">然后我们取另一个样本，重新开始</li></ul><p id="6173" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这些步骤在整个训练集上重复，然后整个过程重复多次，称为<em class="ld">时期。</em></p><h2 id="a2b8" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">做数学</h2><p id="418b" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">为了更新神经网络的权重(矩阵<em class="ld"> W </em>和<em class="ld">W’</em>，我们需要评估<strong class="kh iu">损失</strong>对它们的导数。导数的计算并不复杂，因为输出主要是线性函数矩阵运算的结果。当然，除了激活功能和损耗。主要缺陷在于权重指数的正确管理。</p><p id="7ccc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">帮助我们计算的工具也很简单:链式法则。它指出，两个或多个函数的组合的导数可以计算为它们的导数的乘积。</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/a8afe55eb1ab9077c78d440d61c5c65c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BzYMNYhvrEaIEErc4l8k3Q.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Chain rule is applied to F, a composed function, by multiplying derivatives of its composing functions</figcaption></figure><p id="cac5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们从<em class="ld">损失</em>相对于矢量<em class="ld"> z </em>的元素的导数开始，这对于将要进行的两种计算都是有用的。我没有报告所有的步骤，但是通过利用<em class="ld"> softmax </em>的属性可以显示:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/599d6511db686ca9a0f419ffda86a140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ypoZVZkR3ev1YXwXbt0ig.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivative w.r.t. z</figcaption></figure><p id="e6d1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">向前移动，输出矩阵<em class="ld">W’</em>的导数为:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nk"><img src="../Images/00011933c4048c06aa7d768a99c601f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q3K7g9BrrF8ouIxNS9Tpvg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivative for output matrix weights</figcaption></figure><p id="5654" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于隐藏层的矩阵，还需要做一些工作。首先，让我们展开矩阵<em class="ld"> W </em>产生的向量<em class="ld"> h </em>:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/841f9a9dd6b2025d6a4239e5999cf817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yk3xJKgF8qGnCwQKpiYXqA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Element of vector h</figcaption></figure><p id="e350" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，记住之前显示的<em class="ld">损失</em>的导数，并且<em class="ld"> z </em>是向量<em class="ld"> h </em>与列<em class="ld">W’</em>的乘积，我们得到:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nm"><img src="../Images/fb179c6bb890fc56c138789985ac0694.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2zbZSVOKhun3ism3f3L6w.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivative w.r.t. j-th element of h</figcaption></figure><p id="206f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">最后，将最后一个导数与<em class="ld"> h </em> w.r.t. <em class="ld"> W </em>的导数结合，我们得到:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/4945ea09aa487810c944ca821369164e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yqPunRrJllX_3wJcDZniA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivative for hidden layer weights</figcaption></figure><h2 id="5feb" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">更新权重</h2><p id="36f7" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">一旦计算出导数，更新方程就是标准方程:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/e10c5ccdedf4100f8a2753504247de95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXb73uE4KNnn8JzkS0pDmg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Getting the new weights</figcaption></figure><p id="9750" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">更新步骤由<em class="ld"> eta </em>因子控制，该因子通常在训练时期随着权重接近其最佳值而减少。</p><h2 id="5bca" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">一点矢量化</h2><p id="d86c" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">现在，我们已经定义了关于<em class="ld"> W </em>和<em class="ld">W’</em>的元素的点导数，我们可以通过使用张量积将直接给出的定义扩展到矩阵，从而具有更简单和更紧凑的符号:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/a27412a5fedc1ac129dc7fa0984eeb46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gPZVv69Q2ibI-ABetEZjw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivative for output layer weights</figcaption></figure><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/3910912d396b1fd560d0e951a0fbca2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b9fbmWYzdOWmKA2QPOZDZA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Derivative for hidden layer weights</figcaption></figure><p id="6a25" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在后一种情况下，<em class="ld"> x </em>是一个独热编码向量，它只有一个元素等于 1，所有其他元素都等于 0。因此，它在这里的作用是只选择与输入单词的表示相关的行。换句话说，<strong class="kh iu">在<em class="ld">W</em>T35】中只更新输入单词的权重。</strong></p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><h2 id="7309" class="lm ln it bd lo lp lq dn lr ls lt dp lu kq lv lw lx ku ly lz ma ky mb mc md me bi translated">负采样</h2><p id="952c" class="pw-post-body-paragraph kf kg it kh b ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky mj la lb lc im bi translated">正如我们在上一篇文章中看到的，在负采样的情况下，目标函数会发生变化，因此，<em class="ld">损失</em>也会发生变化，权重公式也会更新。</p><p id="7281" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，对于由<em class="ld"> h </em>表示的给定输入字，对于<em class="ld">W’</em>中的第<em class="ld"> j 个</em>字，模型的输出由下式给出:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/3ee9d35da1a4da1bd5b92eb20f119d2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3mvwdb9TdsV0hwucWHdPg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">New output</figcaption></figure><p id="4f42" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">代替以前偏导数的新偏导数是:</p><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/818768e96c2b29d626df4bde34209e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sj44yOxS2V7EkTgSIDxvhg.jpeg"/></div></div></figure><p id="854d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">第一个等式实际上保持不变，但是<em class="ld"> y </em>在这里被替换为使用<em class="ld"> sigmoid </em>函数的新表达式。</p><p id="d052" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">此外，这些等式以及用于权重更新的等式现在必须仅应用于<em class="ld"> K </em>组单词(正样本和负样本)，而不是整个词汇表。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="a401" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于实现的更多细节，你可以看看我在<a class="ae le" href="https://github.com/acapitanelli/word-embedding" rel="noopener ugc nofollow" target="_blank"> github </a>上的代码。我写它主要是为了娱乐和学习，所以它远不是很快，但很容易理解。</p></div></div>    
</body>
</html>