<html>
<head>
<title>Natural Language Processing Classification Using Deep Learning And Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习和 Word2Vec 的自然语言处理分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-classification-using-deep-learning-and-word2vec-50cbadd3bd6a?source=collection_archive---------3-----------------------#2019-06-15">https://towardsdatascience.com/natural-language-processing-classification-using-deep-learning-and-word2vec-50cbadd3bd6a?source=collection_archive---------3-----------------------#2019-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ju"><img src="../Images/bdf6bab6c363f7abd0fa100f17418854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cdZ3_d17M8UoN08h"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Photo by <a class="ae kk" href="https://unsplash.com/@vikubi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Victoria Kubiaki</a> on <a class="ae kk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="8c2a" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">介绍</h1></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><p id="65b4" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi mh translated">我以前体验过机器学习算法，用于不同的问题，如货币汇率预测或图像分类。我最近在做一个文本分类的项目，我读了很多关于这个主题的文献。NLP(自然语言处理)的例子很吸引人。当你开始思考的时候，你意识到没那么简单，在分类之前，还有这个问题:<br/>“一个算法到底是怎么读懂文字的？”。一种解决方案是将单词转换成向量，用数字来表示它们。这种解决方案并不新鲜，几年前，一篇文章提出了 Google Word2Vec 无监督算法:<a class="ae kk" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的高效估计(Mikolov &amp; al。，2013) </a>。可以找到许多关于它的文档，但本文的重点是从头到尾详细介绍如何构建用于文本分类的机器学习算法。我将演示如何将 Word2Vec 与预训练的 Google 新闻数据集一起使用，以及如何用您的数据自己训练它。然后，我将演示两种技术；一个是做你的文档词的意思，另一个是保持你的数据像他们一样，这保留了更多的信息，但它有点复杂，需要更多的时间来训练。所以这取决于你，你认为在你的情况下和你的数据下什么更好。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="14f3" class="kl km iq bd kn ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li bi translated"><strong class="ak"> 1 首先我们需要导入数据</strong></h1><p id="664a" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi mh translated">对于这一步，确保包含您的评论的文件夹与笔记本在同一个文件夹中。</p><p id="4956" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">我用的数据是可以在这里找到的影评:<a class="ae kk" href="https://www.cs.cornell.edu/people/pabo/movie-review-data/" rel="noopener ugc nofollow" target="_blank">影评</a>。我拿的是“句子极性数据集 1.0 版”。我拿的是“句子极性数据集 1.0 版”。我选择这些是因为我可以将我的结果与论文<a class="ae kk" href="https://arxiv.org/pdf/1408.5882.pdf" rel="noopener ugc nofollow" target="_blank">用于句子分类的卷积神经网络(Yoon Kim，2014) </a>进行比较。本文的优势在于为该数据集提供了一个神经网络，但它将其结果与表 2 中的其他算法进行了比较，这非常有趣，因为我们有许多来自不同论文的算法来比较我们的结果。<br/> <br/>提取你下载的带有链接的文件。<br/>好了，现在我们基本上有一个名为<strong class="ll ir">“rt-polarity data”</strong>的文件夹，其中有两个名为<strong class="ll ir">“rt-polarity . neg”</strong>和<strong class="ll ir">“rt-polarity . pos”</strong>的文件。负面评价和正面评价)。我们在这里的工作将是把每一个数据放入熊猫数据框进行分析。开始将它们转换成 CSV 文件。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="69ff" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在，我们正在创建数据的“标签”, 1 表示正面评价，0 表示负面评价。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="2cf4" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在结果应该如下</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f38b9a46b22d4059f0e64dc869b4478d.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*vbrUJXioaTDzm2jYMWtrNQ.png"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 1: Our Dataframe, with the text of the review, and its label</figcaption></figure><p id="be39" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">好的，看起来很棒！现在，我们将每个评论都放入我们的 pandas dataframe，命名为“评论”，并带有特定的标签(1 表示正面评论，0 表示负面评论)。</p><h1 id="76b2" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2 使用 Word2Vec 查看我们单词之间的相似性距离</h1><p id="a101" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi mh translated">ord2Vec 是一个很好的用于单词嵌入的神经网络模型。它主要用于词语的相似上下文。我们将在我们的数据上训练模型，使我们所有的词之间有一个距离，以查看哪些词在语义上彼此接近。还有其他型号，但我选择这款有两个原因:</p><ol class=""><li id="0346" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ni nj nk nl bi translated">这是尹金在他的文章中使用的</li><li id="e72b" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated">是 Google 开发的模型，似乎是完全推荐的，文档也很容易找到，而这篇文章:<a class="ae kk" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的高效估计(Mikolov &amp; al，2013) </a>很好地解释了所有的过程。</li></ol><h1 id="2dc3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2.1 标记化</h1><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="367f" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在，你的数据帧应该是这样的</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nr"><img src="../Images/b2c7e321ebe8ac21a4e75c8d783b9a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQJqDZJ9WPg2LxfGI-ScyA.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 2 : The dataframe, with the tokens</figcaption></figure><p id="4887" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对于培训来说，重要的是将每个评论表示为单词列表，如“令牌”列中所示。</p><h1 id="4cda" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2.2:使用预先训练好的谷歌新闻数据集</h1><p id="2cc1" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">首先你需要在这里下载数据集:<a class="ae kk" href="https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz" rel="noopener ugc nofollow" target="_blank">谷歌新闻数据集</a>。然后，将其提取到您的文件夹中。我将它提取到名为“model”的子文件夹中</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="d105" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">就这么简单！现在你已经有了一个名为<strong class="ll ir">“w2v _ model”</strong>的模型，它经过了训练，包含了数据集中用向量表示的每个单词。</p><h1 id="c53d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2.2.1 根据您的数据训练模型</h1><p id="eec2" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">你也可以用你的个人数据来训练这个模型。但是，我不建议对小文档使用这种技术，因为 Word2Vec 不能正确地捕捉单词的上下文，并且它不会给出令人满意的结果。我在这篇文章的数据上进行了测试，结果明显比预先训练的 Google Word2Vec 要好。在另一个平均每个文档 200 个单词的数据集上，它更可靠，并且在某些情况下显示出比预训练模型更好的结果。</p><p id="5192" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">我们将把工作分成 3 个步骤</p><ol class=""><li id="2212" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ni nj nk nl bi translated"><code class="fe ns nt nu nv b">Word2Vec()</code>。用模型的所有参数初始化模型</li><li id="65f9" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated">从一系列句子中构建词汇</li><li id="1f2e" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated"><code class="fe ns nt nu nv b">.train()</code>我们训练我们的模型</li></ol><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="e0e6" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2.3 结果</h1><p id="1645" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">现在，我们可以用一些单词来测试我们的模型，看看哪些单词与它们最相似。<br/>我们用以下方式进行测试:</p><ol class=""><li id="5f4b" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ni nj nk nl bi translated">电影</li><li id="2ad4" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated">小说</li><li id="0e59" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated">好的</li></ol><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="124a" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">对于“好”这个词，我有这些结果</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/01f5f72e83ada92b3e06fb2d59e168f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*sS_g3iNf9WWwvc3r3P1N1w.png"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 3 : Words that are the most similar to “good”</figcaption></figure><p id="7124" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">这些结果是通过预先训练的谷歌新闻数据集获得的。</p><p id="4a7f" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">然而，我们可以看到，该模型并不完美，没有捕捉到单词的语义，因为我们有[很好、很差、很棒、不错]。这可能是一个问题，因为在这里好在“语义上”接近坏。事实上，它们可以在相同的上下文中使用，但它们的含义是不一样的。</p><h1 id="bfb5" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">2.5 一点点数据可视化</h1><p id="9e47" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">上面是我们数据集的一万字的图。语义相近的，在地图上是挨着的。我用散景让地图变得动态，我们可以和它互动，你可以把鼠标放在一个点上，看看对应的单词。我们现在可以清楚地看到所有单词之间的关系，以及哪些是近的或远的。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi nx"><img src="../Images/f4168de758c9841f1a3c95d2d9c99a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fHz9F8x0t9MupTaQKOcu_Q.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 4 : Bokeh chart of 10000 words of our dataset</figcaption></figure><h1 id="1839" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3 对数据做一点工作</h1><h1 id="acd3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3.1 列车测试分割</h1><p id="3bb2" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">现在我们有了数据框架，我们需要将数据分成一个训练变量和一个测试变量。通过训练，我们的算法将学习它的参数，通过测试，我们将测试它们。<br/>我们将训练和测试分开，以查看是否存在<strong class="ll ir">不过度拟合</strong>的问题，这在深度学习领域是经常出现的。这意味着我们的模型对它所学习的数据有很好的结果，但它有一个概括的问题，并且它在其他数据集上会有不好的结果，这显然不是目标。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="767e" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">3.2 构建向量</h1><p id="6e2c" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">我们在这里做的是使用来自<code class="fe ns nt nu nv b">sklearn</code>的<code class="fe ns nt nu nv b">TfidfVectorizer</code>。该功能反映了文档中单词的强度。<br/>我们使用行<code class="fe ns nt nu nv b">tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))</code>将所有的单词放入一个名为<code class="fe ns nt nu nv b">tfidf</code>的向量中，如果你执行它，你可以在上面看到。这是我在 Ahmed BESBES 的博客上发现的一个提示。这真的很有趣，值得一读。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="e9b4" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在只是为了好玩，为了形象化，我用 WordCloud 来描绘我们字典中最重要的 100 个单词。我们可以看到像<strong class="ll ir">戏剧、电影、场景和故事</strong>这样的词，它们显然对关于电影评论家的数据集很重要。我使用了 Ahmed BESBES 的另一个博客来使用这个库。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi ny"><img src="../Images/16c87722d70848258747eafec23e35f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7cjqr-tBqNmdKW85aqex5A.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 5 : The most “important” words in our corpus</figcaption></figure><p id="7592" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在我们将构建一个函数来计算给定评论家的“平均值”。我们的 w2v_model 给了我们哪些单词彼此接近，所以对于每一个单词，我们都将它们乘以它们在“字典”中的重要性:<code class="fe ns nt nu nv b">w2v_model[word].reshape((1, size)) * tfidf[word]</code>。<br/>注意:我们使用 reshape 函数，因为我们对语料库的每个文本都这样做，所以例如在<code class="fe ns nt nu nv b">X_train</code>中我们有 8529 个文本，如果我们对其应用该函数，我们将得到一个二维形状矩阵(8529，300)。</p><ol class=""><li id="a828" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ni nj nk nl bi translated">8529 代表我们语料库中的文本数量</li><li id="1e10" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated">300 代表 Word2Vec 创建的向量的大小。</li></ol><p id="f191" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">就这样，现在我们把它除以观察次数，我们很好地得到了所有这些的平均值。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="2fbf" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">该计算可以恢复如下:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/82dcf0dd0b36d0c96bb89a5fe74da28e.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*5n7q6OdlUzxoGaB0ura-dQ.png"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 6: Formula of the mean of the words by ponderation with their Tf-idf</figcaption></figure><p id="d6e3" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">其中:</p><ol class=""><li id="d75b" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ni nj nk nl bi translated"><code class="fe ns nt nu nv b">n</code>是文本中的字数</li><li id="5959" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated">对于给定的字<em class="oa"> i </em>，向量<em class="oa"> Word2Vec </em>的大小是 300</li><li id="dfac" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ni nj nk nl bi translated"><code class="fe ns nt nu nv b">Ti</code>是给定字<em class="oa"> i </em>的值<em class="oa"> tfidf </em></li></ol><p id="3552" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在我们将这个函数应用于我们的数据。<br/>所以我说过，<code class="fe ns nt nu nv b">buildWordVector</code>有两个参数，令牌，和大小。尺寸是 300 因为 word2vec 模型我们得到了 300 的形状。对于标记，它将循环增加，以覆盖我们的训练语料库的所有 8529 个文本和我们的测试语料库的 2133 个文本。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="4319" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">4 第一个神经网络</h1><p id="a178" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">第一个神经网络只是一个简单的人工神经网络，只有两个密集层，为了避免过拟合，压差为 0.7。对于这一个，我们把给定评论中每个词的平均向量作为输入。</p><h1 id="0e8b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">4.1 构建神经网络</h1><p id="db51" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">下面是这个简单分类器的特征。</p><ul class=""><li id="dfc7" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ob nj nk nl bi translated"><strong class="ll ir">致密层数:</strong> 2</li><li id="902d" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">激活函数:</strong> relu，和 sigmoid 为最后一个密集层</li><li id="e02f" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">辍学:</strong> 0.7</li><li id="50fe" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">优化器:</strong> Adadelta</li><li id="799f" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">损失:</strong>二元交叉熵</li></ul><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/957cf564ead4f994112038f252b8d509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*Ho380B_JRI6X_TK-fBHRNQ.png"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 7 : summary of the classifier</figcaption></figure><h1 id="16a2" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">4.2 训练神经网络</h1><p id="f1af" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">现在，我们使用批量为 50 的训练数据和 20 个时期来训练我们的神经网络。<br/>做更多的历元似乎不会改变精度。使用不同的 batch_size 和时期数进行网格搜索以查看更好的参数可能是有用的。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="76c3" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">最后，我们绘制训练的历史以观察演变，并比较训练和测试预测</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi od"><img src="../Images/bbc3b8e3a14eaa09a9a295f270509244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*coxMiOOKwaIY8PoB_yms9A.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 8: accuracy and loss for the first classifier</figcaption></figure><p id="c014" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">最终，我们的训练精度为 0.8342，测试精度为 0.7286。这并不坏，重要的是要注意，我们没有太多的过度拟合。</p><h1 id="26b1" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">5 A 卷积神经网络</h1><p id="9f2d" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">CNN 主要用于图像分类，因为它可以通过模式的过滤图来识别模式。但在 2014 年，当 Yoon Kim 发表他的文章时，他表明它们也可以用于文本分类。事实上，这个想法并不完全疯狂，因为句子也有模式。</p><h1 id="d0e7" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">5.1 构建神经网络</h1><p id="fd85" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi translated">首先，我们试图找到所有的参数来构建我们的神经网络。这将是一个 CNN，但不是给他一个句子中所有单词向量的意思，我们会给他一个给定句子中所有的单词向量。<br/>还有，结构有点变化，每层神经元更多。</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2412f0af1a1496835c0eed749e372e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*du98kY35ey2pIg0Pllz-9w.png"/></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 9: structure of our CNN</figcaption></figure><p id="dda4" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">我们的神经网络与我上面描述的 Yoon Kim (2014)构建的神经网络相同。</p><ul class=""><li id="bb15" class="nd ne iq ll b lm ln lq lr lu nf ly ng mc nh mg ob nj nk nl bi translated"><strong class="ll ir">卷积层数:</strong> 3</li><li id="1e5f" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">致密层数:</strong> 2</li><li id="27c8" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">特征图数量:</strong>每卷积 128</li><li id="cc4d" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">激活功能:</strong> relu，和最后一个致密层的 sigmoid</li><li id="29ab" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">滤镜尺寸:</strong> 3、4、5</li><li id="719a" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">辍学:</strong> 0.5</li><li id="4910" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">优化器:</strong> Adadelta</li><li id="6b7f" class="nd ne iq ll b lm nm lq nn lu no ly np mc nq mg ob nj nk nl bi translated"><strong class="ll ir">损失:</strong>二元交叉熵</li></ul><p id="8086" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">这个 CNN 和 Yoon Kim 用的差别不大:<br/> 1。他刚有了 1 密层<br/> 2。他从来没有用过乙状结肠。他在每个卷积中使用了 100 个特征图，而不是 128 个</p><p id="9705" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">然而，我有更好的结果与那些小的变化，所以我保持他们那样。</p><p id="6ae8" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">为了构建它，我们需要一些参数，嵌入维数(word2vec 向量的大小)，vocab 大小的最大值(我们有多少个唯一的词)，以及最大序列长度(每次评论的最大字数)。<br/>下面的代码为您提供了所有这些参数，如果您用另一个数据集测试它，只需用代码的结果更改三个变量:</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="2de3" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">现在，我们创建将在 CNN 中使用的训练和测试输入。对于少于最大字数的每个文档，我们用“0”来完成它们。这并没有改变我们的结果，因为 CNN 识别模式，无论在某一点或另一点，模式仍然是相同的。例如，对于一个图像，这意味着如果一个图像比其他图像小，我们将把它的黑色边框。这不会改变形象。</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><h1 id="fbf8" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">5.2 定义 CNN</h1><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="0bc7" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">汇总的结果应该如下所示:</p><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi of"><img src="../Images/b4e15321bd1f40381a2b54360c8935e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rqETGxLQYkk0lRiAhzWzKg.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 10: Summary of the CNN</figcaption></figure><p id="d91b" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">让我们进行 10 个纪元的训练课程，再来一次批量 50 个！</p><figure class="jv jw jx jy gt jz"><div class="bz fp l di"><div class="na nb l"/></div></figure><figure class="jv jw jx jy gt jz gh gi paragraph-image"><div role="button" tabindex="0" class="ka kb di kc bf kd"><div class="gh gi od"><img src="../Images/3c184c34c025339143f446288511998a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GvmoJpZGxNkv6plDXWPHhg.png"/></div></div><figcaption class="kg kh gj gh gi ki kj bd b be z dk">Figure 11: accuracy and loss for the cnn</figcaption></figure><p id="893e" class="pw-post-body-paragraph lj lk iq ll b lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg ij bi translated">在 10 个时期结束时，训练集的准确度为 0.915，测试集的准确度为 0.7768。我们有一点过度拟合，验证损失相当不稳定，但结果在这里。我用更多的纪元来训练它，但这似乎是我们能做到的最好的了。</p><h1 id="9f5b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">6 个结论</h1><p id="31cc" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi mh translated"><span class="l mi mj mk bm ml mm mn mo mp di">我们</span>可以清楚地看到 CNN 更适合这个任务，用我的其他数据帧，我个人也有同样的结果。<br/>但是，它仍然有一个不方便的地方，它要更深，有更多的参数，并且需要更多的时间来训练。对于这个小数据集，差异并不重要，但我必须根据我的工作数据来训练它，简单的分类器需要 13 分钟训练，而 CNN 需要 5 个小时！所以要用哪个由你自己决定。<br/>这两个分类器仍然显示出一些不错的结果，我注意到它们拥有的数据越多，文档的长度越重要，它们就越好。对于一个包含 70 000 个数据和最大长度为 2387 个文档的数据集，我的测试精度是 0.9829，这非常令人鼓舞！</p><h1 id="345d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">7 个视角</h1><p id="a80c" class="pw-post-body-paragraph lj lk iq ll b lm mv lo lp lq mw ls lt lu mx lw lx ly my ma mb mc mz me mf mg ij bi mh translated">我有两个主要的想法去尝试更好的结果。首先，使用第一个分类器，我们可以使用另一个更复杂的神经网络，如递归神经网络(<a class="ae kk" href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17114/15696" rel="noopener ugc nofollow" target="_blank">CA-:使用上下文对齐的递归神经网络对句子相似性建模(陈，胡&amp; al ., 2018).</a>)或现在开始使用的注意网络(<a class="ae kk" href="https://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank">用于文档分类的分层注意网络(Yang &amp; al .，2016) </a>)。<br/>第二个想法是针对单词嵌入，2018 年谷歌展示了一个新的模型，叫做 BERT ( <a class="ae kk" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练(Devlin，Chang &amp; al。，2018) </a>)谁更有优势使用分割令牌。例如，如果我们的数据中有单词考古学家，它可以记住“考古”，当像“考古”这样的单词出现时，它会知道它与考古学家有关，而 word2Vec 会忽略它不认识的单词。</p></div></div>    
</body>
</html>