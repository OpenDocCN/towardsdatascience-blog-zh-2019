<html>
<head>
<title>Dropout Neural Network Layer In Keras Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 中的神经网络脱层解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab?source=collection_archive---------3-----------------------#2019-06-02">https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab?source=collection_archive---------3-----------------------#2019-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/05d89540213c380bdf87bf495c445626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r6y4O3vS4ubfazNe"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/colleagues-looking-at-survey-sheet-3183153/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/colleagues-looking-at-survey-sheet-3183153/</a></figcaption></figure><div class=""/><p id="4baa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习最终用于在给定一组特征的情况下预测结果。因此，我们可以做的任何概括模型性能的事情都被视为净收益。放弃是一种用于防止模型过度拟合的技术。Dropout 的工作原理是在训练阶段的每次更新时，将隐藏单元(组成隐藏层的神经元)的输出边缘随机设置为 0。如果你看一下 Keras 关于辍学层的文档，你会看到一个由 Geoffrey Hinton 和 friends 撰写的白皮书的链接，该白皮书探讨了辍学背后的理论。</p><figure class="le lf lg lh gt iv"><div class="bz fp l di"><div class="li lj l"/></div></figure><h1 id="01ab" class="lk ll jj bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">密码</h1><p id="426d" class="pw-post-body-paragraph kg kh jj ki b kj mi kl km kn mj kp kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">在前面的例子中，我们将使用 Keras 构建一个神经网络，目标是识别手写数字。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="9440" class="ms ll jj mo b gy mt mu l mv mw">from keras.datasets import mnist<br/>from matplotlib import pyplot as plt<br/>plt.style.use('dark_background')<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Flatten, Activation, Dropout<br/>from keras.utils import normalize, to_categorical</span></pre><p id="17a1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用 Keras 将数据导入我们的程序。数据已经分为训练集和测试集。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="d08f" class="ms ll jj mo b gy mt mu l mv mw">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span></pre><p id="f3ee" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看我们在做什么。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="8b2b" class="ms ll jj mo b gy mt mu l mv mw">plt.imshow(x_train[0], cmap = plt.cm.binary)<br/>plt.show()</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1b1c571dd624912dbd9dc1ba77f3a553.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*VzuNxxeuQ2glSdetb8j9YA.png"/></div></figure><p id="dfc2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们完成模型的训练后，它应该能够识别前面的图像为 5。</p><p id="db4e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们必须事先进行一点预处理。我们将像素(特征)归一化，使它们的范围从 0 到 1。这将使模型更快地收敛到一个解。接下来，我们将给定样本的每个目标标签转换为 1 和 0 的数组，其中数字 1 的索引表示图像代表的数字。我们这样做是因为，否则我们的模型会将数字 9 解释为比数字 3 具有更高的优先级。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="1884" class="ms ll jj mo b gy mt mu l mv mw">X_train = normalize(X_train, axis=1)<br/>X_test = normalize(X_test, axis=1)<br/>y_train = to_categorical(y_train)<br/>y_test = to_categorical(y_test)</span></pre><h2 id="b2ce" class="ms ll jj bd lm my mz dn lq na nb dp lu kr nc nd ly kv ne nf mc kz ng nh mg ni bi translated">没有辍学</h2><p id="dd4c" class="pw-post-body-paragraph kg kh jj ki b kj mi kl km kn mj kp kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">在将二维矩阵输入神经网络之前，我们使用一个展平层，通过将每个后续行附加到其前面的行，将其转换为一维数组。我们将使用两个由 128 个神经元组成的隐藏层和一个由 10 个神经元组成的输出层，每个神经元代表 10 个可能的数字中的一个。softmax 激活函数将返回样本代表给定数字的概率。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="5ceb" class="ms ll jj mo b gy mt mu l mv mw">model = Sequential()<br/>model.add(Flatten(input_shape=(28, 28)))<br/>model.add(Dense(128))<br/>model.add(Activation('relu'))<br/>model.add(Dense(128))<br/>model.add(Activation('relu'))<br/>model.add(Dense(10))<br/>model.add(Activation('softmax'))</span><span id="f661" class="ms ll jj mo b gy nj mu l mv mw">model.summary()</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/a091870376b1c002e7ce0ccd2ebc1481.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*0M9we2gMth1LfDmW7DJyLA.png"/></div></div></figure><p id="923e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们试图预测类别，所以我们使用分类交叉熵作为损失函数。我们将使用准确性来衡量模型的性能。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="182b" class="ms ll jj mo b gy mt mu l mv mw">model.compile(<br/>    loss='categorical_crossentropy',<br/>    optimizer='adam',<br/>    metrics=['accuracy']<br/>)</span></pre><p id="6970" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们留出 10%的数据进行验证。我们将用它来比较一个模型在有和没有辍学的情况下过度适应的趋势。批量大小为 32 意味着在通过神经网络传递 32 个样本后，我们将计算梯度并在梯度方向上迈出一步，其幅度等于学习速率。我们总共这样做 10 次，具体次数由历元数决定。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="f4ab" class="ms ll jj mo b gy mt mu l mv mw">history = model.fit(<br/>    X_train,<br/>    y_train,<br/>    epochs=10,<br/>    batch_size=32,<br/>    validation_split=0.1,<br/>    verbose = 1,<br/>    shuffle=True<br/>)</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/94be82568ad549fd2e4189d9d5df2e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mxa0-mkUodefnZaWuuXspA.png"/></div></div></figure><p id="2e5b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用由<em class="nm"> fit </em>函数返回的历史变量来绘制每个时期的训练和验证精度。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="d551" class="ms ll jj mo b gy mt mu l mv mw">loss = history.history['loss']<br/>val_loss = history.history['val_loss']<br/>epochs = range(1, len(loss) + 1)<br/>plt.plot(epochs, loss, 'y', label='Training loss')<br/>plt.plot(epochs, val_loss, 'r', label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span></pre><p id="646e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你所看到的，在没有丢失的情况下，验证损耗在第三个时期后停止下降。</p><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/3719a9da7718ad2a25e757644faa799f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*uQQcZeG52liCJZv_b_kO5Q.png"/></div></figure><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="9ede" class="ms ll jj mo b gy mt mu l mv mw">acc = history.history['acc']<br/>val_acc = history.history['val_acc']<br/>plt.plot(epochs, acc, 'y', label='Training acc')<br/>plt.plot(epochs, val_acc, 'r', label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><p id="01f3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，在没有丢失的情况下，验证精度往往在第三个历元左右趋于平稳。</p><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c66264a15ca5e7a7b377ac69a05a6b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*UoSisUOkd3rOL9T_koWK7g.png"/></div></figure><p id="43c8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用这个简单的模型，我们仍然能够获得超过 97%的准确率。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="e597" class="ms ll jj mo b gy mt mu l mv mw">test_loss, test_acc = model.evaluate(X_test, y_test)<br/>test_acc</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bebd66402ce8976ccdd9b285146b983c.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*cFsjz3Zbm4G1xzWNQZ-Q6w.png"/></div></figure><h2 id="b2a1" class="ms ll jj bd lm my mz dn lq na nb dp lu kr nc nd ly kv ne nf mc kz ng nh mg ni bi translated">拒绝传统社会的人</h2><p id="bb3e" class="pw-post-body-paragraph kg kh jj ki b kj mi kl km kn mj kp kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">关于 dropout 应该放在 activation 函数之前还是之后还有一些争论。根据经验，对于除<strong class="ki jk"><em class="nm"/></strong>之外的所有激活功能，将 dropout 放在 activate 功能之后。在传递 0.5 时，每个隐藏单元(神经元)以 0.5 的概率被设置为 0。换句话说，有 50%的变化，给定神经元的输出将被强制为 0。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="b888" class="ms ll jj mo b gy mt mu l mv mw">model_dropout = Sequential()<br/>model_dropout.add(Flatten(input_shape=(28, 28)))<br/>model_dropout.add(Dense(128))<br/>model_dropout.add(Dropout(0.5))<br/>model_dropout.add(Activation('relu'))<br/>model_dropout.add(Dense(128))<br/>model_dropout.add(Dropout(0.5))<br/>model_dropout.add(Activation('relu'))<br/>model_dropout.add(Dense(10))<br/>model_dropout.add(Activation('softmax'))</span><span id="8269" class="ms ll jj mo b gy nj mu l mv mw">model_dropout.summary()</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/2c07a0a7eb189ec41d14be4eddbc5424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*1MULS1Zeb15ohYT5zJFBtA.png"/></div></figure><p id="4945" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，由于我们试图预测类别，我们使用分类交叉熵作为损失函数。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="ef45" class="ms ll jj mo b gy mt mu l mv mw">model_dropout.compile(<br/>    loss='categorical_crossentropy',<br/>    optimizer='adam',<br/>    metrics=['accuracy']<br/>)</span></pre><p id="1083" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过提供验证分割参数，模型将分离训练数据的一部分，并将在每个时期结束时评估该数据的损失和任何模型度量。如果 dropout 背后的前提成立，那么我们应该会看到与以前的模型相比，验证准确性有显著的不同。洗牌参数将在每个时期之前洗牌训练数据。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="10f3" class="ms ll jj mo b gy mt mu l mv mw">history_dropout = model_dropout.fit(<br/>    X_train,<br/>    y_train,<br/>    epochs=10,<br/>    batch_size=32,<br/>    validation_split=0.1,<br/>    verbose = 1,<br/>    shuffle=True<br/>)</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nr"><img src="../Images/6ffa1a1293527f28f8f1b23b9fa0db7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zbSXHYadP6sGbUOZQva7nA.png"/></div></div></figure><p id="fb9e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如您所见，验证损失明显低于使用常规模型获得的损失。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="a078" class="ms ll jj mo b gy mt mu l mv mw">loss = history_dropout.history['loss']<br/>val_loss = history_dropout.history['val_loss']<br/>epochs = range(1, len(loss) + 1)<br/>plt.plot(epochs, loss, 'y', label='Training loss')<br/>plt.plot(epochs, val_loss, 'r', label='Validation loss')<br/>plt.title('Training and validation loss')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Loss')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9e518766795d9fc6cdeee86098f0d4a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*oPPm-FBK98RiN-Gzc8h0YQ.png"/></div></figure><p id="b87a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如您所看到的，该模型收敛得更快，并在验证集上获得了接近 98%的准确性，而之前的模型在第三个时期左右达到稳定。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="35ae" class="ms ll jj mo b gy mt mu l mv mw">acc = history_dropout.history['acc']<br/>val_acc = history_dropout.history['val_acc']<br/>plt.plot(epochs, acc, 'y', label='Training acc')<br/>plt.plot(epochs, val_acc, 'r', label='Validation acc')<br/>plt.title('Training and validation accuracy')<br/>plt.xlabel('Epochs')<br/>plt.ylabel('Accuracy')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/81f56da0857c3afe0a81fd3f2bec6110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*fFrb_SvuJCcz1ZEFslp0Kw.png"/></div></figure><p id="074a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在测试集上获得的准确性与从没有丢失的模型中获得的准确性没有太大的不同。这很可能是由于样本数量有限。</p><pre class="le lf lg lh gt mn mo mp mq aw mr bi"><span id="dd45" class="ms ll jj mo b gy mt mu l mv mw">test_loss, test_acc = model_dropout.evaluate(X_test, y_test)<br/>test_acc</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ed0eeba1f7d65e5f16dcd6793b718553.png" data-original-src="https://miro.medium.com/v2/resize:fit:154/format:webp/1*bWm6NY1i1EJmKGON2arRoA.png"/></div></figure><h1 id="f5f8" class="lk ll jj bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">最后的想法</h1><p id="4643" class="pw-post-body-paragraph kg kh jj ki b kj mi kl km kn mj kp kq kr mk kt ku kv ml kx ky kz mm lb lc ld im bi translated">Dropout 可以通过将给定神经元的输出随机设置为 0 来帮助模型泛化。在将输出设置为 0 时，成本函数对改变反向传播过程中权重更新方式的相邻神经元变得更加敏感。</p></div></div>    
</body>
</html>