# ç›´è§‚ç†è§£è£…è¢‹å¯¹æ–¹å·®å’Œåå·®çš„å½±å“

> åŸæ–‡ï¼š<https://towardsdatascience.com/understanding-the-effect-of-bagging-on-variance-and-bias-visually-6131e6ff1385?source=collection_archive---------3----------------------->

## ç»™å‡ºä¸ºä»€ä¹ˆ bagging ç®—æ³•å®é™…å·¥ä½œçš„ç›´è§‰ï¼Œå¹¶ä»¥ç®€å•æ˜“æ‡‚çš„æ–¹å¼å±•ç¤ºå®ƒä»¬çš„æ•ˆæœ

![](img/cde1361c1876f5f513d2442c5a45325e.png)

Â© by my lovely wife [Tinati KÃ¼bler](https://dribbble.com/tinati)

è¿™é‡Œæœ‰å¤§é‡ä¼˜ç§€çš„æ–‡ç« æè¿°äº†åƒéšæœºæ£®æ—è¿™æ ·çš„æ‰“åŒ…æ–¹æ³•åœ¨ç®—æ³•å±‚é¢ä¸Šæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠä¸ºä»€ä¹ˆæ‰“åŒ…æ˜¯ä¸€ä»¶å¥½äº‹ã€‚é€šå¸¸ï¼Œæœ¬è´¨æ˜¯è¿™æ ·çš„:

> â€œä½ åœ¨è®­ç»ƒé›†çš„ä¸åŒéƒ¨åˆ†è®­ç»ƒè®¸å¤šå†³ç­–æ ‘ï¼Œç„¶åå°†å®ƒä»¬çš„é¢„æµ‹å¹³å‡åŒ–ä¸ºæœ€ç»ˆé¢„æµ‹ã€‚é¢„æµ‹å˜å¾—æ›´å¥½ï¼Œå› ä¸ºéšæœºæ£®æ—çš„æ–¹å·®ä¸å•ä¸ªå†³ç­–æ ‘çš„æ–¹å·®ç›¸æ¯”æ›´å°ã€‚(dartboard.png)"
> 
> â€”ä¸€äº›æ–‡ç« 

å½“ç„¶ï¼Œæˆ‘åœ¨è¿™é‡Œæ˜¯è½¬è¿°ã€‚è¿™äº›æ–‡ç« åŒ…æ‹¬å¾ˆæ£’çš„å›¾ç‰‡ã€ä»£ç å’Œæ›´å¤šçš„æƒ³æ³•ã€‚ä½†æˆ‘ç»å¸¸é”™è¿‡çš„æ˜¯å¯¹*ä¸ºä»€ä¹ˆ*è£…è¢‹æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ä»¥åŠ*å¦‚ä½•ä½¿ç”¨çœŸå®æ•°æ®é›†çœ‹åˆ°*æ–¹å·®å‡å°‘çš„è‰¯å¥½ç›´è§‰ã€‚

å› æ­¤ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³è§£å†³è¿™ä¸¤ä¸ªç¼ºç‚¹ï¼Œå¹¶ç»™å‡ºç›´è§‚çš„æ¨ç†ï¼Œä¸ºä»€ä¹ˆéšæœºæ£®æ—ç®—æ³•çš„å·¥ä½œï¼Œä»¥åŠå¦‚ä½•å¯ä»¥çœ‹åˆ°æ–¹å·®çš„å›¾å½¢æ”¹å–„ã€‚ä½ å¯ä»¥æŠŠè¿™ç¯‡æ–‡ç« çœ‹ä½œæ˜¯å¯¹è¿™ä¸¤ä¸ªä¸»é¢˜çš„æ¢ç´¢ï¼Œå®ƒæ¯”ä¸€èˆ¬çš„å…³äºåå·®-æ–¹å·®å›°å¢ƒçš„æ–‡ç« æ›´æ·±å…¥ï¼Œä½†ä¸åƒä¸€ç¯‡æˆç†Ÿçš„ç ”ç©¶è®ºæ–‡é‚£æ ·æ·±å…¥ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘è¿˜æ˜¯ä¼šæä¾›ä¸€äº›æˆ‘è®¤ä¸ºæœ‰ç”¨çš„èµ„æºçš„é“¾æ¥ï¼Œè¿™æ ·ä½ å°±å¯ä»¥åœ¨éœ€è¦çš„æ—¶å€™æ›´æ·±å…¥åœ°äº†è§£ã€‚

æˆ‘è¯•å›¾è®©æ•°å­¦æ°´å¹³éå¸¸å®¹æ˜“ç†è§£ï¼Œè®©æ²¡æœ‰æ•°å­¦ä¸“ä¸šçš„äººä¹Ÿèƒ½ç†è§£ï¼ŒåŒæ—¶ä¹Ÿç»™å‡ºä¸€äº›é«˜æ°´å¹³çš„æƒ³æ³•å’Œæ’å›¾ï¼Œä¹Ÿè®©æ•°å­¦ç›¸å…³çš„äººèƒ½å–œæ¬¢ã€‚

å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘è¿˜æ˜¯ä¸ä¼šè¯¦ç»†è§£é‡Šå†³ç­–æ ‘ã€éšæœºæ£®æ—å’Œæ‰€æœ‰å…¶ä»–æåˆ°çš„æ¨¡å‹æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œå› ä¸ºæ­£å¦‚æ‰€æè¿°çš„ï¼Œè¿™å·²ç»è¢«è®¨è®ºè¿‡æ— æ•°æ¬¡äº†ã€‚æˆ‘å°†åªè§£é‡Šéå¸¸é«˜çº§çš„æƒ³æ³•ï¼Œä»å†³ç­–æ ‘å¼€å§‹ã€‚

# å†³ç­–æ ‘

*å…è´£å£°æ˜:æˆ‘åœ¨è¿™é‡Œåªè°ˆè®ºæ™®é€šçš„å†³ç­–æ ‘ã€‚åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä¸è€ƒè™‘ä¿®å‰ªã€‚è¿™äº›æ ‘å¯ä»¥é•¿å¾—ä»»æ„æ·±ã€‚*

## è¾“å‡ºæè¿°

å¸¦æœ‰ k ä¸ªå¶å­çš„å†³ç­–æ ‘å°±æ˜¯è¿™ç§å½¢å¼çš„æ¨¡å‹

![](img/9886c68e7fc19ccb1ebf3f4debe036ea.png)

è¿™æ„å‘³ç€å†³ç­–æ ‘æ˜¯åœ¨ç‰¹å¾ç©ºé—´çš„åŒºåŸŸ *R* ä¸­å…·æœ‰å®å€¼ *w* çš„åˆ†æ®µå¸¸æ•°å‡½æ•°ã€‚è¿™é‡Œ *x* æ¥è‡ªç‰¹å¾ç©ºé—´ *X* å¹¶ä¸” *y* æ˜¯æ¥è‡ªè¾“å‡ºç©ºé—´ *Y* çš„ç›¸åº”æ ‡ç­¾ã€‚å¯¹ *R* çš„çº¦æŸæ˜¯

![](img/6d5641e19563f60b61c72fcb578a61eb.png)

Property 1 and 2 for a 2-dimensional feature space.

1.  å®ƒä»¬æ˜¯è¾¹ç•Œå¹³è¡Œäºç‰¹å¾ç©ºé—´åæ ‡è½´çš„çŸ©å½¢
2.  æ‰€æœ‰çŸ©å½¢çš„é›†åˆæ˜¯ç‰¹å¾ç©ºé—´çš„ä¸€ä¸ªåˆ’åˆ†ï¼Œå³ï¼Œå¦‚æœä½ å–ä¸¤ä¸ªä¸ç›¸äº¤çš„çŸ©å½¢ï¼Œå¹¶ä¸”æ‰€æœ‰çŸ©å½¢çš„å¹¶é›†æ˜¯å®Œæ•´çš„ç‰¹å¾ç©ºé—´ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬å·²ç»ç¡®å®šäº†è¿™ä¸€ç‚¹ï¼Œè®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹ä¸ºä»€ä¹ˆå†³ç­–æ ‘è¢«ç§°ä¸º*é«˜æ–¹å·®ç®—æ³•*ï¼Œè€Œä¾‹å¦‚*çº¿æ€§å›å½’*è¢«è®¤ä¸ºæ˜¯*ä½æ–¹å·®ç®—æ³•*ã€‚

## ä¸çº¿æ€§å›å½’çš„æ¯”è¾ƒ

ç®€å•å›é¡¾ä¸€ä¸‹ï¼Œçº¿æ€§å›å½’æ¨¡å‹å…·æœ‰ä»¥ä¸‹å½¢å¼:

![](img/fe879d08967feaaae7cca472606e9669.png)

å…¶ä¸­æƒé‡ *w* æ˜¯å®æ•°ï¼Œè€Œ *d* æ˜¯æ ·æœ¬çš„ç»´åº¦ï¼Œå³ç‰¹å¾çš„æ•°é‡ã€‚

ä¸ºäº†æ¯”è¾ƒè¿™äº›æ¨¡å‹çš„æ–¹å·®ï¼Œæˆ‘ä»¬å¿…é¡»åé€€ä¸€æ­¥ï¼Œæ€è€ƒå­¦ä¹ é—®é¢˜å®é™…ä¸Šæ˜¯ä»€ä¹ˆã€‚

é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šå¾—åˆ°å›ºå®šæ•°é‡çš„æ ·æœ¬(*å­¦ä¹ é›†*ï¼Œè®­ç»ƒæ ·æœ¬)ï¼Œè®©æˆ‘ä»¬çš„ç®—æ³•å˜å˜é­”æœ¯ï¼Œæ‹Ÿåˆæ‰€æœ‰å¿…è¦çš„å‚æ•°ï¼Œæœ€ç»ˆï¼Œæˆ‘ä»¬å¯ä»¥é¢„æµ‹æœªçŸ¥æ ·æœ¬çš„å€¼ã€‚ä½†æ˜¯ï¼Œè¿™æ˜¯ä¸€ç§ç›¸å½“åƒµåŒ–çš„çœ‹å¾…äº‹ç‰©çš„è§‚ç‚¹ã€‚

åœ¨å­¦ä¹ ç†è®ºä¸­ï¼Œæˆ‘ä»¬å°†è®­ç»ƒé›†å»ºæ¨¡ä¸ºæ¥è‡ªç©ºé—´ *XÃ—Yï¼Œ*ä¸Šçš„åˆ†å¸ƒ *D* ï¼Œå…¶ä¸­ *X* æ˜¯ç‰¹å¾ç©ºé—´ï¼Œ *Y* æ˜¯è¾“å‡ºç©ºé—´*ã€‚*æˆ‘ä»¬ä»åˆ†å¸ƒä¸­æŠ½å–å¤§å°ä¸º *n* çš„è®­ç»ƒé›† *L* (ä»¥åŠéªŒè¯å’Œæµ‹è¯•é›†):

![](img/600925d3d4a8be1f4913dc3ee81ef02a.png)

n data samples from the distribution D. Here each of the xâ€™s is a vector of some dimension d coming from the feature space X and the yâ€™s are the corresponding labels from the output space Y.

> æƒ³è±¡ä¸€ä¸ªå‘è¡Œç‰ˆæ˜¯ä¸€ä¸ªæœ‰æŒ‰é’®çš„é»‘ç›’ï¼›å¦‚æœä½ ç‚¹å‡»æŒ‰é’®ä¸€æ¬¡ï¼Œä½ ä¼šä»åˆ†å¸ƒä¸­å¾—åˆ°ä¸€ä¸ªéšæœºæ ·æœ¬(xâ‚ï¼Œyâ‚)ã€‚å†æ¬¡ç‚¹å‡»å®ƒï¼Œä½ ä¼šå¾—åˆ°å¦ä¸€ä¸ªæ ·æœ¬(xâ‚‚ï¼Œyâ‚‚)ï¼Œç‹¬ç«‹äºä¹‹å‰çš„æ ·æœ¬ã€‚é‡å¤ï¼Œç›´åˆ°ä½ æœ‰è¶³å¤Ÿçš„æ ·æœ¬ã€‚

ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¥è‡ª *L* çš„ *n* æ•°æ®ç‚¹æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚è¿™ä¸ºæ ·æœ¬ *L ä¸­çš„æ‰€æœ‰ *(xáµ¢ï¼Œyáµ¢)* (å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¥½çš„)è¾“å‡ºä¸€ä¸ªå‡½æ•° *f* å’Œ *f(xáµ¢)â‰ˆyáµ¢* ã€‚*è¿™ç§æ–¹æ³•ç¡®ä¿æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šçš„è¡¨ç°*è‰¯å¥½*ã€‚

ä½†æ˜¯ç°åœ¨æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬ä»åˆ†å¸ƒ *D* ä¸­æŸ¥è¯¢ *n* ä¸ªæ–°æ ·æœ¬ï¼Œå¹¶å°†è¿™äº›æ ·æœ¬ç”¨ä½œè®­ç»ƒé›†*Lâ€™ã€‚*è®©æˆ‘ä»¬æŠŠåœ¨è¿™ä¸ªæ–°é›†åˆä¸Šè®­ç»ƒäº§ç”Ÿçš„æ¨¡å‹ç§°ä¸º *g.* è¿™ä¸ªæ–°æ¨¡å‹ *g* ä¹Ÿå°†æ»¡è¶³*lâ€™ä¸­æ‰€æœ‰*ã€xáµ¢'ã€‘ã€yáµ¢'ã€‘*çš„æ¡ä»¶ *g(xáµ¢')â‰ˆyáµ¢'* ã€‚*

ç°åœ¨ï¼Œç”±äº*lâ€™*ç”±ä¸åŒçš„ç‚¹ *(xáµ¢'ï¼Œyáµ¢')* ç»„æˆï¼Œæ–°å‹å· *g* å°†å…·æœ‰ä¸*f*ä¸åŒçš„è¾“å‡ºå½¢çŠ¶ã€‚å‹å· *f* å’Œ*g**å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„ä¸åŒï¼Œè¿™å–å†³äº *L* å’Œ*çš„ä¸åŒç¨‹åº¦**

> *å¦‚æœå¯¹äºä¸€ä¸ªå›ºå®šçš„ç®—æ³•(ä¾‹å¦‚â€œå†³ç­–æ ‘â€)ï¼Œä¸åŒè®­ç»ƒé›† L å’Œ Lâ€™çš„æ¨¡å‹å¾€å¾€ç›¸å·®å¾ˆå¤§ï¼Œæˆ‘ä»¬ç§°è¿™ä¸ªç®—æ³•ä¸ºé«˜æ–¹å·®ç®—æ³•ã€‚*

*å½“ç„¶ï¼Œè¿™æ²¡æœ‰ç²¾ç¡®çš„å®šä¹‰ï¼Œä½†è¿™å¯¹äºæœ¬æ–‡æ¥è¯´ä¹Ÿæ˜¯ä¸å¿…è¦çš„ã€‚åœ¨ä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å›¾å½¢æ¥ç¡®å®šä¸€ç§ç®—æ³•æ˜¯å¦æ¯”å¦ä¸€ç§ç®—æ³•å…·æœ‰æ›´é«˜çš„æ–¹å·®ã€‚*

*å¦‚æœä½ å¯¹æ•°å­¦æ„Ÿå…´è¶£(å¹²æ¯ï¼)ï¼Œæˆ‘å¯ä»¥æ¨è Gilles Louppe çš„è®ºæ–‡[1]ï¼Œä»¥åŠ Shai Shalev-Shwartz å’Œ T2 Shai Ben-David çš„ä¹¦[2]ï¼Œè¿™æœ¬ä¹¦éå¸¸è¯¦ç»†åœ°è§£é‡Šäº†æœºå™¨å­¦ä¹ çš„ç†è®ºåŸºç¡€ã€‚*

*è®©æˆ‘ä»¬å›åˆ°å†³ç­–æ ‘å’Œçº¿æ€§å›å½’çš„æ¯”è¾ƒã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹è¿è¡Œç¤ºä¾‹: *X=[0ï¼Œ10]* å’Œ *Y=â„ï¼Œ*å³**ç‰¹å¾ç©ºé—´çš„ç»´åº¦ä¸º 1** ï¼Œå¹¶ä¸”è¿™ä¸€ä¸ªç‰¹å¾å¯ä»¥å– 0 åˆ° 10 ä¹‹é—´çš„å®æ•°å€¼ï¼Œè€Œæ ‡ç­¾å¯ä»¥å–ä»»ä½•å®æ•°å€¼ã€‚*

*åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªåˆ†å¸ƒ *D* åšå¦‚ä¸‹äº‹æƒ…:ä» 0 åˆ° 10 å‡åŒ€åœ°é€‰æ‹©ç‰¹å¾ *x* ï¼Œå¹¶ä¸”é€šè¿‡*éšè—å‡½æ•°*æ˜¾å¼åœ°è®¡ç®—æ ‡ç­¾ y*

*![](img/ce53b1983f4c6f5ea8a9376545844d64.png)*

*y is computed deterministically via 3sin(x)+x and then standard normally distributed noise is added.*

*![](img/4fbc119a8cfb9902d28bed2f297bdd85.png)*

*y without the noise.*

*å‡½æ•° *h* æè¿°äº†æ ‡ç­¾çš„åº•å±‚ç»“æ„ï¼Œè¿™æ˜¯æˆ‘ä»¬æƒ³è¦äº†è§£æ ‡ç­¾çš„çœŸç›¸ã€‚æˆ‘ä»¬ç§°ä¹‹ä¸ºéšè—ï¼Œå› ä¸ºæˆ‘ä»¬ä¸ä¼šç»™ç®—æ³•è¿™ä¸ªä¿¡æ¯ã€‚ä»–ä»¬å¿…é¡»è‡ªå·±æƒ³åŠæ³•ã€‚:)*

*æŒ‰ç…§ä¸Šé¢çš„æ¨ç†ï¼Œå¦‚æœæˆ‘ä»¬å¯¹æˆ‘ä»¬çš„åˆ†å¸ƒ *D* æŸ¥è¯¢ä¸‰æ¬¡ï¼Œæ¯æ¬¡ 10 ä¸ªæ ·æœ¬ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šå¾—åˆ°ä»¥ä¸‹ä¸‰ä¸ªè®­ç»ƒé›†:*

*![](img/5bd8678457ec9d59d4756c31c214c404.png)*

*Sampling from the distribution three times yields 3 different results. Each time 10 training samples were generated.*

*è®©æˆ‘ä»¬ä½¿ç”¨æœ€å³è¾¹çš„è®­ç»ƒé›†ï¼Œå¹¶åœ¨åº”ç”¨å†³ç­–æ ‘å’Œçº¿æ€§å›å½’åç»˜åˆ¶ç»“æœã€‚*

*![](img/74555a569f542944cbc92e9c85bb4c71.png)*

*We can see that the decision tree fits the training data perfectly, which is not a reason to celebrate, however. The problem is that the algorithm also captures the noise, which we do not want. We are only interested in capturing the underlying structure of the labels (namely 3sin(x)+x), instead.*

## *å†³ç­–æ ‘çš„åå·®å’Œæ–¹å·®ä¸çº¿æ€§å›å½’*

*è®©æˆ‘ä»¬å¯¹ 3000 ä¸ªç‹¬ç«‹é‡‡æ ·çš„è®­ç»ƒé›†è¿›è¡Œ 3000 æ¬¡ç›¸åŒçš„å®éªŒï¼Œæ¯ä¸ªè®­ç»ƒé›†çš„å¤§å°ä¹Ÿæ˜¯ 10ã€‚åœ¨å·¦ä¾§ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å†³ç­–æ ‘çš„ç»“æœï¼Œåœ¨å³ä¾§ï¼Œçº¿æ€§å›å½’ç»“æœç›¸äº’å åŠ ã€‚*

*![](img/56927d43b9c4e1206d38d5370f952c09.png)*

*Each trial gives one curve in transparent black. The more lines stack, the darker the intersections get. The dashed blue line is 3sin(x)+x again, the underlying truth.*

*åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°å†³ç­–æ ‘(å·¦ä¾§)**å¹³å‡æ¥è¯´éå¸¸é€‚åˆæ•°æ®**ã€‚äººä»¬ä¹Ÿç§°è¿™ç§å±æ€§ä¸º**å†³ç­–æ ‘**å…·æœ‰**ä½åå·®**ã€‚åŒæ—¶ï¼Œå¯¹äºå³ä¾§çš„çº¿æ€§å›å½’ï¼Œè¯¥æ¨¡å‹æ˜¾ç„¶ä¸èƒ½æ•æ‰åº•å±‚æ ‡ç­¾ç»“æ„çš„å¤æ‚æ¨¡å¼ã€‚æˆ‘ä»¬è¯´**çº¿æ€§å›å½’**æœ‰**é«˜åå·®** *ï¼Œ*åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ˜¯ä¸èƒ½å¤Ÿå¾—çŸ¥çœŸç›¸çš„ã€‚*

*ç„¶è€Œï¼Œå¦‚æœä½ è€ƒè™‘è¿™äº›é»‘ç®¡çš„**å‚ç›´å®½åº¦**ï¼Œæºè‡ªå†³ç­–æ ‘çš„é»‘ç®¡æ¯”å³è¾¹çš„çº¿æ€§å›å½’é»‘ç®¡æ›´å®½ã€‚è¿™æ„å‘³ç€ï¼Œå½“å¯¹è®­ç»ƒæ•°æ®é›†è¿›è¡Œé‡æ–°é‡‡æ ·æ—¶ï¼Œå†³ç­–æ ‘é¢„æµ‹*åœ¨*é™„è¿‘æ‘†åŠ¨çš„å¹…åº¦æ¯”çº¿æ€§å›å½’é¢„æµ‹æ›´å¤§ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºå…·æœ‰**é«˜æ–¹å·®**çš„**å†³ç­–æ ‘**å’Œå…·æœ‰**ä½æ–¹å·®**çš„**çº¿æ€§å›å½’**ã€‚*

*![](img/7d840967a194ad72fbfd982f1424bd35.png)*

*Summary of the algorithm properties.*

*æˆ‘ä»¬å®é™…ä¸Šæƒ³è¦çš„æ˜¯å…·æœ‰**ä½åå·®**(å®ƒä»¬å¹³å‡ç¬¦åˆäº‹å®)**å’Œä½æ–¹å·®**(å®ƒä»¬ä¸ä¼šè¿‡å¤šåœ°åç¦»äº‹å®)çš„ç®—æ³•ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰è®¸å¤šæ–¹æ³•å¯ä»¥é™ä½åå·®(ä¾‹å¦‚ï¼Œä½¿ç”¨ä¸€ç§ç§°ä¸º*Boosting*çš„æŠ€æœ¯)ï¼Œè¿˜æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥é™ä½æ–¹å·®*ã€‚*åè€…å¯ä»¥é€šè¿‡æ‰€è°“çš„*è£…è¢‹æ¥å®ç°ã€‚*è£…è¢‹çš„å¥½å¤„æ˜¯ï¼Œå®ƒä¹Ÿä¸ä¼šå†æ¬¡å¢åŠ åå·®ï¼Œè¿™ä¸€ç‚¹æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è®¨è®ºã€‚*

> *è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ Bagging å’Œçº¿æ€§å›å½’ä¸€èµ·ä½¿ç”¨çš„æ•ˆæœå¾ˆä½:ä½ ä¸èƒ½é€šè¿‡ Bagging æ¥å‡å°‘åå·®ï¼Œä½†å¯ä»¥é€šè¿‡ Boosting æ¥å‡å°‘åå·®ã€‚æœ‰è¶£çš„æ˜¯ï¼Œå°†å†³ç­–æ ‘ä¸ Boosting ç»“åˆä½¿ç”¨ä¹Ÿè¢«è¯æ˜æ˜¯æœ‰ç”¨çš„ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤§é‡ä¿®å‰ªçš„å†³ç­–æ ‘ï¼Œå®ƒä¹Ÿå…·æœ‰è¾ƒä½çš„åå·®ã€‚*

# *åˆ¶è¢‹ææ–™*

*åœ¨è¿™ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ° Bagging åšä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆå®ƒå·¥ä½œï¼Œä»¥åŠå¦‚ä½•çœ‹åˆ°æ–¹å·®çš„å‡å°‘ã€‚*

## *ç®€å•çš„åŠ¨æœº*

*å‡è®¾æˆ‘ä»¬æœ‰æ ‡å‡†æ­£æ€åˆ†å¸ƒï¼Œç‰¹åˆ«æ˜¯ï¼Œä¸€ä¸ªè§‚å¯Ÿå€¼çš„å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º 1ã€‚è®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬å–œæ¬¢çœ‹åˆ° 0 é™„è¿‘çš„å€¼(*å°±åƒæˆ‘ä»¬å–œæ¬¢çœ‹åˆ° 3sin(x)+x* é™„è¿‘çš„é¢„æµ‹å‡½æ•°ä¸€æ ·)ã€‚ä½†æ˜¯ 1 çš„æ–¹å·®å¯¹äºæˆ‘ä»¬çš„å£å‘³æ¥è¯´å¤ªå¤§äº†(*å°±åƒé»‘è‰²ç¯ç®¡çš„å®½åº¦*)ï¼Œæˆ‘ä»¬æ­£åœ¨å¯»æ‰¾å‡å°å®ƒçš„æ–¹æ³•ã€‚ä¸€ç§ç®€å•çš„æ–¹æ³•æ˜¯ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­æŠ½å–æ›´å¤šçš„å€¼ï¼Œç„¶åå–å®ƒä»¬çš„å¹³å‡å€¼ã€‚ä»¥ä¸‹ç»“æœæ˜¯ä¼—æ‰€å‘¨çŸ¥ä¸”æ˜“äºéªŒè¯çš„:*

*![](img/b14ca1d74aac39940197e9b0a1040cf1.png)*

*The average of standard normal random variables is [also normally distributed](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables). The new mean is just the sum of the means and the new variance can be computed with the [BienaymÃ© Formula](https://en.wikipedia.org/wiki/Variance#Sum_of_uncorrelated_variables_(Bienaym%C3%A9_formula)). Ï is a term that reflects the dependencies between random variables. If they are all independent, then Ï=0\. If the [covariances](https://en.wikipedia.org/wiki/Covariance) between the random variables are all less than a bound K, then Ï is also less than K.*

*å› æ­¤ï¼Œé€šè¿‡å¹³å‡ï¼Œæˆ‘ä»¬æ¨¡æ‹Ÿä»**å¦ä¸€ä¸ªæ­£æ€åˆ†å¸ƒ**ä¸­æå–ï¼Œå…·æœ‰**ç›¸åŒçš„å¹³å‡å€¼**ï¼Œä½†æ˜¯å…·æœ‰**è¾ƒå°çš„æ–¹å·®**ï¼Œå¦‚æœÏä¸å¤ªå¤§ã€‚è¿™å¾ˆå¥½ï¼Œå› ä¸ºæˆ‘ä»¬å¾—åˆ°çš„**å€¼æ¯”ä»¥å‰æ›´æ¥è¿‘é›¶ï¼Œè€Œ**çš„æ¦‚ç‡æ¯”ä»¥å‰æ›´é«˜ï¼*

*åœ¨ç‹¬ç«‹éšæœºå˜é‡( *Ï=0* )å’Œ *b=100* çš„ç‰¹æ®Šæƒ…å†µä¸‹ï¼Œæ¯”å¦‚æ–¹å·®ä» 1 ä¸‹é™åˆ° 0.01ã€‚ç»“æœå¦‚ä¸‹:*

*![](img/cfb1dba7c984113f05c3a5d5ce2f3f35.png)*

*The normal distribution is with a variance of 0.01 is much narrower than the standard normal distribution with a variance of 1\. In the shaded regions, one can see where 99% of the probability of each distribution lies.*

***æ³¨æ„:**å¦‚æœéšæœºå˜é‡ *X* éƒ½ä¸å€¼ 1 ç›¸å…³ï¼Œè¿™æ„å‘³ç€ *Ï=(b-1)/b* ï¼Œå³å¹³å‡å€¼çš„æ–¹å·®åˆä¼šæ˜¯ 1ã€‚è¿™å¯¹åº”äºæ¯ä¸ªæ ·æœ¬å®é™…ä¸Šæ˜¯ç›¸åŒæ•°é‡çš„æƒ…å†µã€‚å¯¹è®¸å¤šç›¸åŒçš„æ•°å­—è¿›è¡Œå¹³å‡å¹¶ä¸èƒ½ç»™æˆ‘ä»¬ä»»ä½•æ–°çš„ä¿¡æ¯ï¼Œæ‰€ä»¥è¿™ç›¸å½“äºåªç”»ä¸€ä¸ªå€¼ã€‚*

> *åœ¨æœ€å¥½çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å¹³å‡ç‹¬ç«‹æ ·æœ¬ã€‚å®ƒä»¬è¶Šç›¸å…³ï¼Œåœ¨å¹³å‡è¿‡ç¨‹ä¸­å°±è¶Šæ— ç”¨ã€‚*

## *è£…è¢‹çš„æ ¸å¿ƒç†å¿µ*

*ç°åœ¨ï¼Œæœ‰ç”¨çš„è§è§£æ˜¯**æˆ‘ä»¬å¯ä»¥ç”¨é¢„æµ‹æ¨¡å‹**åšåŒæ ·çš„äº‹æƒ…ã€‚åœ¨éšæœºæŠ½å–çš„è®­ç»ƒæ•°æ®é›†ä¸Šè¿è¡Œå†³ç­–æ ‘ç®—æ³•ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¨¡å‹ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ä»åˆ†å¸ƒä¸­å¯¹å‡½æ•°**è¿›è¡Œé‡‡æ ·ã€‚å¯¹è¿™äº›æ¨¡å‹è¿›è¡Œå¹³å‡å¾—åˆ°äº†å¦ä¸€ä¸ªæ¨¡å‹(å¦‚éšæœºæ£®æ—),å…·æœ‰ç›¸åŒçš„åå·®**,ä½†æ–¹å·®**è¾ƒä½ã€‚å¹³å‡è€Œè¨€ï¼Œè¿™ç§é›†æˆæ¨¡å‹æ¯”å•ä¸ªå†³ç­–æ ‘æ›´æ¥è¿‘äº‹å®ã€‚***

*ä½†é—®é¢˜æ˜¯:è¿™äº›åŠŸèƒ½çš„ç›¸å…³æ€§æœ‰å¤šå·®ï¼Ÿè€ƒè™‘ä»¥ä¸‹æƒ…å†µ:å¦‚æœæˆ‘ä»¬é‡åˆ°ä¸€ä¸ªæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å®ƒä¸Šé¢å®‰è£…ä¸€ä¸ªå†³ç­–æ ‘ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä¸€åˆ‡é¡ºåˆ©ã€‚ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬å†åšä¸€æ¬¡ï¼Œåœ¨å†³ç­–æ ‘çš„æƒ…å†µä¸‹ï¼Œç»“æœå°†(å‡ ä¹)ç›¸åŒã€‚è¿™æ„å‘³ç€æˆ‘ä»¬ä»¥è¿™ç§æ–¹å¼é‡‡æ ·çš„å‡½æ•°æ˜¯é«˜åº¦ç›¸å…³çš„ *(Ïâ‰ˆ1)* ï¼Œå¹¶ä¸”ä¸ä¼šæ”¹è¿›å•ä¸ªå†³ç­–æ ‘ã€‚*

> *å®ƒä¸ä¸€å®šæ­£å¥½æ˜¯ 1ï¼Œå› ä¸ºå†³ç­–æ ‘ç®—æ³•å¶å°”å¿…é¡»æ‰“ç ´æŸç¼šï¼Œè¿™å¯ä»¥ä»¥éšæœºæ–¹å¼è¿›è¡Œï¼Œä½†æ˜¯å› ä¸ºè¿™æ˜¯éšæœºæ€§çš„å”¯ä¸€æ¥æºï¼Œæ‰€ä»¥å®ƒä¸ä¼šäº§ç”Ÿå½¼æ­¤æ ¹æœ¬ä¸åŒçš„æ ‘ã€‚*

*ä¸ç®¡æ€æ ·ï¼Œæˆ‘ä»¬å¿…é¡»å»ç›¸å…³è¿™äº›æ ‘ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚çœ‹åˆ°å¦‚ä½•å»åšã€‚*

## *èµ°å‘éšæœºæ£®æ—*

*éšæœºæ£®æ—æ˜¯ Leo Breiman å‘æ˜çš„[3]ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ä»¥ä¸€ç§ç‰¹æ®Šçš„æ–¹å¼åœ¨è®­ç»ƒé›†ä¸Šå®‰è£…è®¸å¤šå†³ç­–æ ‘ï¼Œç»™å‡ºåŒæ ·å¤šçš„æ ‘æ¨¡å‹(=å‡½æ•°)ã€‚ä¹‹åï¼Œè¿™äº›æ ‘è¢«ç»„åˆæˆå•ä¸ªæ¨¡å‹ï¼Œä¾‹å¦‚ï¼Œé€šè¿‡å¯¹ä»»ä½•ç»™å®šè¾“å…¥ *xï¼Œ*çš„è¾“å‡ºè¿›è¡Œå¹³å‡ï¼Œä½¿å…¶æˆä¸ºä¸€ç§ç‰¹æ®Šçš„æ‰“åŒ…æ–¹æ³•ã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªå…·æœ‰è¾ƒä½æ–¹å·®çš„æ¨¡å‹ï¼Œç±»ä¼¼äºæˆ‘ä»¬ä¹‹å‰çœ‹åˆ°çš„æ­£æ€åˆ†å¸ƒéšæœºå˜é‡ã€‚*

*è·å¾—è®¸å¤šéæœ€å¤§ç›¸å…³æ ‘çš„æƒ³æ³•å¦‚ä¸‹:*

1.  *å¯¹æ¯æ£µæ ‘ä½¿ç”¨è®­ç»ƒæ ·æœ¬çš„éšæœºå­é›†ã€‚*
2.  *åœ¨ç”Ÿé•¿æ¯æ£µæ ‘çš„æ¯ä¸ªæ­¥éª¤ä¸­ä½¿ç”¨éšæœºçš„ç‰¹å¾å­é›†ã€‚*

*æ‹¥æœ‰ä¸¤ä¸ªéšæœºåŒ–æºæ¯”åªä½¿ç”¨å…¶ä¸­ä¸€ä¸ªæ›´æœ‰åŠ©äºå‡å°‘ä¸åŒæ ‘ä¹‹é—´çš„ç›¸å…³æ€§ã€‚å¦‚æœä½ ç¢°å·§è®¾è®¡äº†ä¸€ä¸ªæ–°çš„è£…è¢‹ç®—æ³•ï¼Œè¯·éšæ„æ·»åŠ æ›´å¤šï¼è¿˜æœ‰å„ç§å…¶ä»–æ–¹æ³•æ¥ç»„åˆå•ä¸ªå†³ç­–æ ‘ï¼Œä¾‹å¦‚ Geurts ç­‰äºº[4]çš„æåº¦éšæœºåŒ–çš„æ ‘ã€‚*

# *ä¸€ç»´å†³ç­–æ ‘å’Œéšæœºæ£®æ—çš„æ–¹å·®æ¯”è¾ƒ*

*è®©æˆ‘ä»¬å†æ¬¡ä»æˆ‘ä»¬çš„åˆ†å¸ƒä¸­æŠ½å– 10 ä¸ªæ ·æœ¬ï¼Œå¹¶æ‹Ÿåˆä¸€ä¸ªå†³ç­–æ ‘å’Œä¸€ä¸ªåŒ…å« 100 ä¸ªå†³ç­–æ ‘çš„éšæœºæ£®æ—ã€‚æˆ‘ä»¬é‡å¤è¿™ä¸ªè¿‡ç¨‹ 1000 æ¬¡ï¼Œå¾—åˆ°ä¸‹é¢çš„å›¾ç‰‡:*

*![](img/2cb795a1a1ad1d92d45a15eebda36558.png)*

*æˆ‘ä»¬çœ‹åˆ°ç”±éšæœºæ£®æ—å½¢æˆçš„çº¢ç®¡çš„å‚ç›´å®½åº¦å°äºå†³ç­–æ ‘çš„é»‘ç®¡ã€‚å› æ­¤ï¼Œæ­£å¦‚æ‰€æ–™ï¼Œ**éšæœºæ£®æ—æ¯”å†³ç­–æ ‘**å…·æœ‰æ›´ä½çš„æ–¹å·®ã€‚æ­¤å¤–ï¼Œä¼¼ä¹ä¸¤ä¸ªç®¡çš„å¹³å‡å€¼(ä¸­é—´)æ˜¯ç›¸åŒçš„ï¼Œè¿™æ„å‘³ç€å¹³å‡**çš„è¿‡ç¨‹æ²¡æœ‰æ”¹å˜åå·®**ã€‚æˆ‘ä»¬ä»ç„¶å¾ˆå¥½åœ°å®ç°äº†åº•å±‚çš„çœŸå®å‡½æ•° *3sin(x)+x* ã€‚*

*è¯·æ³¨æ„ï¼Œéšæœºæ£®æ—ç®—æ³•åœ¨è¿™é‡Œæ— æ³•æ˜¾ç¤ºå…¶å…¨éƒ¨æ½œåŠ›ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†åªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªå†³ç­–æ ‘éƒ½å¿…é¡»ä½¿ç”¨è¿™ä¸ªç‰¹å¾ã€‚å› æ­¤ï¼Œéšæœºæ£®æ—ä¸­çš„ 100 æ£µå†³ç­–æ ‘åªèƒ½åœ¨è¢«é€‰æ‹©æ¥ç”Ÿé•¿æ¯æ£µæ ‘çš„è®­ç»ƒæ ·æœ¬ä¹‹é—´æœ‰æ‰€ä¸åŒã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œéšæœºæ£®æ—ç®—æ³•å°±å˜æˆäº†æ›´ç®€å•çš„ Bagging ç®—æ³•ï¼Œåªå¯¹æ¯æ£µæ ‘ä½¿ç”¨ä¸åŒçš„è®­ç»ƒæ ·æœ¬ã€‚*

*å¦‚æœæˆ‘ä»¬æƒ³æ‰©å¤§æ–¹å·®çš„å·®è·ï¼ŒåŒæ—¶ä»ç„¶èƒ½å¤Ÿç›´è§‚åœ°è§£é‡Šç»“æœï¼Œæˆ‘ä»¬å¿…é¡»è½¬ç§»åˆ°äºŒç»´ç‰¹å¾ç©ºé—´ã€‚è¿™å…è®¸éšæœºæ£®æ—ç®—æ³•åœ¨ç®—æ³•çš„æ¯ä¸€æ­¥ä»ä¸¤ä¸ªå¯ç”¨ç‰¹å¾ä¸­éšæœºé€‰æ‹©**æ°å¥½ä¸€ä¸ª**ã€‚*

# *äºŒç»´å†³ç­–æ ‘å’Œéšæœºæ£®æ—çš„æ–¹å·®æ¯”è¾ƒ*

*è®©æˆ‘ä»¬ä¸ºè®­ç»ƒæ•°æ®å®šä¹‰ä¸€ä¸ªåˆ†å¸ƒï¼Œå®ƒç±»ä¼¼äºæˆ‘ä»¬åœ¨ä¸€ç»´æƒ…å†µä¸‹ä½¿ç”¨çš„åˆ†å¸ƒã€‚æˆ‘ä»¬é€‰æ‹©*x =ã€0ï¼Œ10ã€‘*å’Œ *Y=â„ï¼Œ*å…¶ä¸­ *D* ä»é¡¶ç‚¹åœ¨ *(0ï¼Œ0)ï¼Œ(0ï¼Œ10)ï¼Œ(10ï¼Œ0)* å’Œ *(10ï¼Œ10)* çš„æ­£æ–¹å½¢ä¸­å‡åŒ€é‡‡æ ·ä¸€ä¸ª *(xï¼Œxâ€™)*å’Œ*

*![](img/3f396b57a036e129e26083e672974b38.png)*

*Similar to what we have seen before, y is computed deterministically via 3sin(x+xâ€™)+x-xâ€™ and then standard normally distributed noise is added.*

*åŒ…å« 50 ä¸ªç‚¹çš„éšæœºæ•°æ®é›†å¯èƒ½å¦‚ä¸‹æ‰€ç¤º:*

*![](img/9e5bcfb9025501859c73a819adc42f73.png)*

*50 random points from the distribution. We can see that there are higher values in the bottom right corner and lower values in the top left corner. The diagonal region separating these two corners is filled with values around zero.*

*ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå†³ç­–æ ‘å’Œéšæœºæ£®æ—çš„æ–¹å·®æ˜¯å¦‚ä½•è¡¨ç°çš„ã€‚äº«å—æˆæœï¼*

*è®©æˆ‘ä»¬é¦–å…ˆä»å†³ç­–æ ‘çš„ä¾‹å­å¼€å§‹ã€‚æˆ‘ä»¬ä½¿ç”¨ 9 ä¸ªä¸åŒçš„è®­ç»ƒæ•°æ®é›†æ¥ç§æ¤ 9 æ£µä¸åŒçš„æ ‘ã€‚*

*![](img/553992734e4a95ef7a402e75fa959c35.png)*

*We see that each of these nine pictures differ quite a lot. The bottom right is always bright (indicating high values) and the top left is dark (indicating low values), but the size and shape of all the rectangles vary significantly.*

*çœ‹ç€çœ¼ç†Ÿï¼Ÿï¼›)*

*ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¯¹éšæœºæ£®æ—åšåŒæ ·çš„äº‹æƒ…ã€‚è¿™é‡Œï¼Œæˆ‘ä»¬åœ¨æ ·æœ¬**çš„ä¸åŒå­é›†ä¸Šå†æ¬¡è®­ç»ƒæ¯ä¸ªéšæœºæ£®æ—çš„ 100 ä¸ªå†³ç­–æ ‘ï¼Œå¹¶ä¸”éšæœºä½¿ç”¨ä¸¤ä¸ªç»™å®šç‰¹å¾** **ä¸­çš„ä»…ä¸€ä¸ª****ï¼æ¯ä¸ªæ¨¡å‹åœ¨ 50 ä¸ªéšæœºæ ·æœ¬ç‚¹ä¸Šè¿›è¡Œè®­ç»ƒã€‚***

***![](img/9df550c930698096d6a2de8b870f3596.png)***

***Not only can we see the high values in the bottom right and the low values in the top left again, but the pictures look very similar. There is a nice and smooth gradient that looks similar in each picture.***

# ***ç»“è®º***

***å¾ˆæ˜æ˜¾ï¼Œå½“è®­ç»ƒé›†æ”¹å˜æ—¶ï¼Œé«˜æ–¹å·®ç®—æ³•ä¼šè¿…é€Ÿæ”¹å˜å®ƒä»¬çš„ç»“æœ(æ¨¡å‹)ã€‚è¿™å¾ˆç³Ÿç³•ï¼Œå› ä¸ºæˆ‘ä»¬æ°¸è¿œä¸çŸ¥é“æˆ‘ä»¬çš„å…·ä½“æ¨¡å‹ç¦»äº‹å®æœ‰å¤šè¿œï¼Œå³ä½¿æˆ‘ä»¬çš„æ¨¡å‹çš„åå·®ä¸ºé›¶ã€‚***

***ä½†æ˜¯æˆ‘ä»¬å­¦ä¼šäº†å¦‚ä½•é€šè¿‡è£…è¢‹æ¥å¢åŠ è·å¾—å¥½æ¨¡å‹çš„æœºä¼šã€‚æˆ‘ä»¬ä¹Ÿæœ‰ä¸€ä¸ªç›´è§‰ï¼Œä¸ºä»€ä¹ˆè£…è¢‹é™ä½äº†æ–¹å·®ï¼Œè€Œä¿æŒåå·®ä¸å˜ï¼Œæˆ‘ä»¬å·²ç»åœ¨å¾ˆå¤šä¾‹å­ä¸­çœ‹åˆ°äº†è¿™äº›ç»“æœã€‚***

# ***å‚è€ƒ***

***[1] G. Louppeï¼Œ[ç†è§£éšæœºæ£®æ—â€”ä»ç†è®ºåˆ°å®è·µ](https://arxiv.org/abs/1407.7502) (2014)ï¼Œå­¦ä½è®ºæ–‡***

***[2] S. Shalev-Shwartz å’Œ S. Ben-Davidï¼Œ[ç†è§£æœºå™¨å­¦ä¹ :ä»ç†è®ºåˆ°ç®—æ³•](https://www.cse.huji.ac.il/~shais/UnderstandingMachineLearning/) (2014)ï¼Œå‰‘æ¡¥å¤§å­¦å‡ºç‰ˆç¤¾***

***[3] L .å¸ƒé›·æ›¼ï¼Œ[éšæœºæ£®æ—](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf) (2001)ï¼Œæœºå™¨å­¦ä¹  45.1(2001):5â€“32***

***[4] P. Geurtsï¼ŒD. Ernst å’Œ L. Wehenkelï¼Œ[æåº¦éšæœºåŒ–çš„æ ‘](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.65.7485&rep=rep1&type=pdf) (2005)ï¼Œæœºå™¨å­¦ä¹  63.1(2006):3â€“42***

***æˆ‘ç”¨[ä¹³èƒ¶](https://www.latex-project.org/)åˆ›é€ äº†æ‰€æœ‰çš„é…æ–¹ã€‚å¯¹äºå…¶ä»–å›¾å½¢ï¼Œæˆ‘ä½¿ç”¨äº† [Python](https://www.python.org/) åº“ [matplotlib](https://matplotlib.org/) å’Œ [numpy](https://numpy.org/) ã€‚å¯¹äºæ¨¡å‹è®­ç»ƒï¼Œæˆ‘ä½¿ç”¨äº† [scikit-learn](https://scikit-learn.org/) ã€‚***

# ***æ‰¿è®¤***

***æˆ‘è¦æ„Ÿè°¢å¸•ç‰¹é‡Œå…‹Â·é²æ›¼åšå£«çš„æ ¡å¯¹å·¥ä½œï¼Œå¹¶ä¸ºæ”¹è¿›æˆ‘çš„æ–‡ç« æä¾›äº†è®¸å¤šæœ‰ç›Šçš„å»ºè®®ã€‚ä¹Ÿæ„Ÿè°¢å®‰å¾·çƒˆÂ·åŸƒå¡çš„å¸®åŠ©ï¼***

# ***å¥–åŠ±:é©¬èµ›å…‹çš„ä»£ç ***

***å½“ä½ å¼€å§‹ç”¨è¿™ç§å†³ç­–æ ‘é©¬èµ›å…‹è‰ºæœ¯èµšé’±æ—¶ï¼Œè¯·æƒ³èµ·æˆ‘ã€‚ğŸ˜€***

```
***import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor
import numpy as np

# Sample from the distribution with a true function f.
def generate_data_2d(f, n_samples):
    x1 = np.random.uniform(0, 10, n_samples)
    x2 = np.random.uniform(0, 10, n_samples)
    y = f(x1, x2) + np.random.randn(n_samples)
    return np.vstack([x1, x2]).transpose(), y

# Parameters to play round with.
f = lambda x1, x2: 3 * np.sin(x1 + x2) + x1 - x2
n_samples = 50
n_rows = 3
n_cols = 3

# Increase numbers to remove white spaces in the pictures.
n_points = 100
size_points = 6

# Prepare the plotting.
fig = plt.figure(constrained_layout=True, figsize=(12, 12))
all_points = np.array([(x1, x2) for x1 in np.linspace(0, 10, n_points) for x2 in np.linspace(0, 10, n_points)])

# Start plotting.
for i in range(1, n_rows * n_cols + 1):
    # Get a random training set.
    x, y = generate_data_2d(f, n_samples)

    # Train a decision tree.
    dt = DecisionTreeRegressor()
    dt.fit(x, y)
    predictions = dt.predict(all_points)

    # Create one mosaic picture.
    ax = fig.add_subplot(n_rows, n_cols, i)
    ax.axis('off')
    ax.scatter(all_points[:, 0], all_points[:, 1], c=predictions, s=size_points)***
```

***æˆ‘å¸Œæœ›ä½ ä»Šå¤©å­¦åˆ°äº†æ–°çš„ã€æœ‰è¶£çš„ã€æœ‰ç”¨çš„ä¸œè¥¿ã€‚æ„Ÿè°¢é˜…è¯»ï¼***

*****ä½œä¸ºæœ€åä¸€ç‚¹ï¼Œå¦‚æœä½ *****

1.  *****æƒ³æ”¯æŒæˆ‘å¤šå†™ç‚¹æœºå™¨å­¦ä¹ å’Œ*****
2.  *****æ— è®ºå¦‚ä½•ï¼Œè®¡åˆ’è·å¾—ä¸€ä¸ªä¸­ç­‰è®¢é˜…ï¼Œ*****

*****ä¸ºä»€ä¹ˆä¸åš** [**é€šè¿‡è¿™ä¸ªç¯èŠ‚**](https://dr-robert-kuebler.medium.com/membership) **ï¼Ÿè¿™å°†å¯¹æˆ‘å¸®åŠ©å¾ˆå¤§ï¼ğŸ˜Š*****

****è¯´ç™½äº†ï¼Œç»™ä½ çš„ä»·æ ¼ä¸å˜ï¼Œä½†æ˜¯å¤§çº¦ä¸€åŠçš„è®¢é˜…è´¹ç›´æ¥å½’æˆ‘ã€‚****

***éå¸¸æ„Ÿè°¢ï¼Œå¦‚æœä½ è€ƒè™‘æ”¯æŒæˆ‘çš„è¯ï¼***

> ***å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·åœ¨ [LinkedIn](https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/) ä¸Šç»™æˆ‘å†™ä¿¡ï¼***