<html>
<head>
<title>The Face Behind the Handle— Using neural networks to distinguish Donald Trump’s tweeting habits</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">手柄后面的脸——使用神经网络区分唐纳德·特朗普的推文习惯</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whos-really-behind-trump-s-tweets-a76b20a7e7?source=collection_archive---------38-----------------------#2019-12-03">https://towardsdatascience.com/whos-really-behind-trump-s-tweets-a76b20a7e7?source=collection_archive---------38-----------------------#2019-12-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/eb7382ac4688308c6f5b7fd7e94ef95a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iuauUIBR1V8cwFHoH9hvoA.png"/></div></div></figure><div class=""/><div class=""><h2 id="5069" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">几乎每一个不朽的宣言都有它的面孔。在我们这个时代，我们有一个推特账号。</h2></div><p id="160d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><a class="lm ln ep" href="https://medium.com/u/f6c36606b9d6?source=post_page-----a76b20a7e7--------------------------------" rel="noopener" target="_blank"> <em class="lo">【德瑞克】</em></a><em class="lo"/><a class="lm ln ep" href="https://medium.com/u/a0b64bf968f5?source=post_page-----a76b20a7e7--------------------------------" rel="noopener" target="_blank"><em class="lo">迦格伦</em></a><em class="lo"/><a class="lm ln ep" href="https://medium.com/u/9373c9c25b0f?source=post_page-----a76b20a7e7--------------------------------" rel="noopener" target="_blank"><em class="lo">吴锡荣</em></a><em class="lo"/><a class="lm ln ep" href="https://medium.com/u/7adf2f371fc5?source=post_page-----a76b20a7e7--------------------------------" rel="noopener" target="_blank"><em class="lo">纳森萧</em> </a></p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="1e9e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">纵观历史，总统们都曾为自己搭建平台，宣布重大变革即将到来。富兰克林·罗斯福利用他的私人炉边谈话宣布对日战争和美国加入第二次世界大战；罗纳德·里根通过他在柏林墙的标志性演讲重新统一了德国；约翰·肯尼迪通过电视广播宣布人类登上月球的梦想将成为现实。这些时刻标志着世界历史，表明来自标志性人物的标志性信息永远不会真正消失。</p><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lx"><img src="../Images/fd39ee0f814a11ffa5f816296d1a1b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UqJ9iJ-8e3a-pNO1YMAXvw.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Three iconic presidential speeches throughout history. Left: FDR fireside chat where he announces war on Japan. Middle: Ronald Reagan’s “Ich Bin Ein Berliner.” Right: JFK giving his moonshot speech. All thumbnails taken from the YouTube videos linked above.</figcaption></figure><p id="4400" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">然而，时代变了:</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="77ed" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">虽然没错，这是全球关系的一个重大事件，但它也是特朗普总统的个人推特——他与世界沟通的主要手段。</p><p id="4a40" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">从<a class="ae lw" href="https://www.independent.co.uk/news/world/middle-east/golan-heights-trump-tweet-netanyahu-israel-syria-a8835991.html" rel="noopener ugc nofollow" target="_blank">承认以色列在戈兰高地的主权</a>到<a class="ae lw" href="https://www.scmp.com/news/china/diplomacy/article/3009018/it-fake-news-china-censors-trumps-us200-billion-tariff-tweets" rel="noopener ugc nofollow" target="_blank">宣布对中国征收 2000 亿美元的新关税</a>，Twitter 上的这些行动一次又一次地证明了它们在世界范围内具有重大影响。<a class="ae lw" href="https://www.bloomberg.com/news/articles/2019-09-09/jpmorgan-creates-volfefe-index-to-track-trump-tweet-impact" rel="noopener ugc nofollow" target="_blank">事实上，摩根大通甚至创造了一个“Volfefe”指数</a>来跟踪由@realDonaldTrump 的推文引起的美国债券市场的波动。</p><p id="204e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在标志性的电视事件中，比如 JFK 出色地宣布，是的，我们<em class="lo">是</em> <strong class="ks jc"> </strong>要去月球，你可以清楚地看到高中 AP Lang 老师的清单:他使用了哪些文学手段？他如何在人群中展示自己？他的<em class="lo">是什么风气？(如果我给了你一个闪回，我道歉)</em></p><p id="3fbf" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">然而，有一个问题几乎从未被提及，但却非常重要:<strong class="ks jc">世卫组织在讲话？</strong></p><p id="7c1d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">仅仅因为它来自@realDonaldTrump 并不意味着它来自(是的，我正在这样做)真正的唐纳德·特朗普。难道你不认为知道谁是世界上最强大的推特账户的幕后操纵者很重要吗？幸运的是，有一个理论。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="2e89" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">一个流行的(阴谋？)关于唐纳德·特朗普的推文来源的理论是，他更具外交色彩的推文是由工作人员撰写的，而他个人则撰写更具争议性的内容。为了识别真正是谁<em class="lo">写了他的推文，我们基于现有的理论训练了几个神经网络。</em></p><p id="f6c5" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在 2017 年 3 月 25 日之前，<a class="ae lw" href="https://twitter.com/realDonaldTrump" rel="noopener ugc nofollow" target="_blank"> @realDonaldTrump </a>的推文在 iPhone 和 Android 设备上发布。一个流行的理论是，特朗普的工作人员用 iPhone 发布推文，而特朗普本人则用他的安卓系统发布推文。虽然特朗普在 2017 年换了一部 iPhone，但我们决定测试一下这个理论，看看我们能否根据文本确定每条推文的来源。</p><h1 id="a34b" class="mi mj jb bd mk ml mm mn mo mp mq mr ms kh mt ki mu kk mv kl mw kn mx ko my mz bi translated">下面是两条不同标签的推文的一个很好的例子:</h1><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Twitter for Android</figcaption></figure><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Twitter for iPhone</figcaption></figure><p id="359a" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">反差很大，不是吗？</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="571b" class="mi mj jb bd mk ml na mn mo mp nb mr ms kh nc ki mu kk nd kl mw kn ne ko my mz bi translated">我们做了什么</h1><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/911e3a2a6141e697c12f7721e7c080d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97BuaZNtFPR7mYr7Dxgtiw.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Screenshot of the Trump Twitter Archive</figcaption></figure><p id="8997" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了将推文分类为工作人员推文或特朗普推文，我们使用了由<a class="ae lw" href="http://www.trumptwitterarchive.com/archive" rel="noopener ugc nofollow" target="_blank">特朗普推特档案馆</a>存档的推文目录，并将它们分为两组，即 2017 年 3 月之前和之后。在 2017 年 3 月之前的集合中，我们过滤掉了从 Twitter for Web 等其他来源发布的推文，并使用 Android 和 iPhone 推文来建立我们的地面真相数据集。</p><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/68bc79ffb6fcb6503177b70d6f0b4c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWUAMmBOlI_cSepo4EbnGw.png"/></div></div></figure><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/359b591362fabebc64e0d3c5b8c5b0bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8DeId3aSCsM0otXua0_jA.png"/></div></div></figure><p id="81cc" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">这里我们有一个基于设备类型的推文频率图，我们发现一般来说，来自 Android 设备(特朗普本人)的推文不如来自 iPhone(他的工作人员)的推文频繁。</p><p id="47e2" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">检查分布图，我们注意到两个设备的<em class="lo">大</em>峰值约为 140 个字符，但来自 Androids 的更多。这可以归因于推特上的<a class="ae lw" href="https://www.theverge.com/2017/9/26/16363912/twitter-character-limit-increase-280-test" rel="noopener ugc nofollow" target="_blank">字符限制——自 2007 年推特成立以来，每条推特就保持在 140 个字符，很大程度上受短信 160 个字符限制的影响。然而，自 2017 年底以来，该限制增加了一倍，达到 280 个字符，这一点从分布图中 iPhone 推文的最右边峰值可以看出。这两个图说明了我们的数据集中明显的偏向，严重偏向 iPhones，数据点几乎是 iPhones 的三倍，分布也超过了旧的 140 个字符的限制。</a></p><h1 id="246e" class="mi mj jb bd mk ml mm mn mo mp mq mr ms kh mt ki mu kk mv kl mw kn mx ko my mz bi translated">准备数据</h1><p id="3622" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">与任何神经网络一样，需要进行一些预处理:</p><h2 id="5d50" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">标记化和格式化</h2><p id="bc71" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">标记化将每个单词映射到字典中的一个索引。为了准备要传递到神经网络的数据，首先使用 Keras 的<code class="fe nz oa ob oc b">Tokenizer</code>模块对推文进行标记化。一旦测试和训练集被标记化，我们就必须决定保持输入大小一致的方法。我们认为按长度裁剪推文会导致我们丢失数据，所以我们选择填充推文。我们选择用 65 个单词的长度来填充输入，因为我们发现最长的推文是 50 个单词，我们希望为未来的推文留有足够的余量。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="od mh l"/></div></figure><h2 id="856a" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">把...嵌入</h2><p id="4d79" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">为了构建我们的单词嵌入，我们在<a class="ae lw" href="https://www.kaggle.com/terenceliu4444/glove6b100dtxt" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到了预处理的全局向量(手套)单词模型。这种类型的模型映射矩阵中两个单词之间的关系，并根据-1 到 1 的值对该关系进行评分。这是生成单词嵌入层的权重的一种非常常见的方法。首先，我们创建了一个出现在数据集中的单词的字典，然后通过从 GloVe 文本文件中解析这些单词及其嵌入值来创建一个嵌入矩阵。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="od mh l"/></div></figure><h2 id="cbd0" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">平衡</h2><p id="f1ee" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">正如前面的<a class="ae lw" href="#8997" rel="noopener ugc nofollow">和</a>所看到的，我们的数据严重失真。与机器人相比，iPhones 的推文数量明显更多。在这种倾斜的数据集上进行训练会有一个有偏见的模型的风险，这意味着模型会过度预测 iPhones，而<em class="lo">在大多数时候仍然</em>是正确的。因此，我们试图通过用平衡批次训练我们的模型来解决这个问题。我们通过使用欠采样技术做到了这一点，该技术从 iPhone 推文中提取较小的样本，这有助于我们的模型做出更可靠的预测，因为它暴露于一组平衡的训练数据。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="od mh l"/></div></figure><h2 id="91e6" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">训练/测试分割</h2><p id="c6ba" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">在所有数据都被收集到 JSON 文件中之后，每条 tweet 的文本和来源都被传递到 sklearn 的<code class="fe nz oa ob oc b">train_test_split</code>中，以便生成一个训练和验证集。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="od mh l"/></div></figure><h1 id="90e2" class="mi mj jb bd mk ml mm mn mo mp mq mr ms kh mt ki mu kk mv kl mw kn mx ko my mz bi translated">建模</h1><p id="b4c2" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">我们选择了五种不同的架构来训练我们的数据集。我们首先看了一个基本的前馈网络作为基线，然后看了 NLP 中流行的其他四个模型。这样做不是为了微调超参数，而是为了探索各种流行的架构，这些架构后来可以扩展为更精细和精确的模型。</p><p id="7e93" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">每个网络的第一层是嵌入层，具有填充字向量(65)的维度。嵌入层的权重是用我们使用的给定 GloVe 文本文件预先实例化的。通过使用预先制作的单词嵌入文件，这是迁移学习的一个应用。这第一层在每个模型中都是标准的，与后面的层无关。</p><p id="7e78" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在训练模型时，我们使用 Keras 中内置的<code class="fe nz oa ob oc b">validation_data</code>参数将测试集设置为验证数据。我们测量每个时期改进的验证准确度，并训练模型，直到验证准确度在 50 个时期内停止改进。</p><p id="4476" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><em class="lo">如果你想跟随我们，请查看我们的</em> <a class="ae lw" href="https://github.com/grenkoca/ANNADL_final" rel="noopener ugc nofollow" target="_blank"> <em class="lo"> GitHub 页面</em> </a> <em class="lo">！</em></p><h2 id="2731" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">正向输送</h2><p id="92ff" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">我们想先尝试前馈模型，因为我们想看看与一些更复杂的模型相比，基本模型的表现如何。</p><pre class="ly lz ma mb gt oe oc of og aw oh bi"><span id="59f0" class="nn mj jb oc b gy oi oj l ok ol">Model: "sequential_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_1 (Embedding)      (None, 65, 100)           2441300   <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 6500)              0         <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 6501      <br/>=================================================================<br/>Total params: 2,447,801<br/>Trainable params: 6,501<br/>Non-trainable params: 2,441,300</span><span id="0d03" class="nn mj jb oc b gy om oj l ok ol">Test Score: 0.4719<br/>Test Accuracy: 0.7909</span></pre><h2 id="072b" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">1D 卷积神经网络(CNN)</h2><p id="d60c" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">1D CNN 将扫描一系列标记化的单词。滤波器长度决定了在单个卷积中要查看多少单词。</p><pre class="ly lz ma mb gt oe oc of og aw oh bi"><span id="52e6" class="nn mj jb oc b gy oi oj l ok ol">Model: "sequential_8"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_8 (Embedding)      (None, 65, 100)           2441300   <br/>_________________________________________________________________<br/>conv1d_4 (Conv1D)            (None, 61, 128)           64128     <br/>_________________________________________________________________<br/>global_max_pooling1d_4 (Glob (None, 128)               0         <br/>_________________________________________________________________<br/>dense_15 (Dense)             (None, 128)               16512     <br/>_________________________________________________________________<br/>activation_6 (Activation)    (None, 128)               0         <br/>_________________________________________________________________<br/>dense_16 (Dense)             (None, 1)                 129       <br/>=================================================================<br/>Total params: 2,522,069<br/>Trainable params: 80,769<br/>Non-trainable params: 2,441,300<br/>_________________________________________________________________<br/>None</span><span id="93c1" class="nn mj jb oc b gy om oj l ok ol">Test Score: 0.8156<br/>Test Accuracy: 0.7650</span></pre><h2 id="d368" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">长短期记忆(LSTM)</h2><p id="970f" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">LSTM 是一个递归神经网络，擅长在较小(短期)和较大(长期)的上下文中查找单词序列的模式。</p><pre class="ly lz ma mb gt oe oc of og aw oh bi"><span id="08b7" class="nn mj jb oc b gy oi oj l ok ol">Model: "sequential_10"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_10 (Embedding)     (None, 65, 100)           2441300   <br/>_________________________________________________________________<br/>lstm_3 (LSTM)                (None, 128)               117248    <br/>_________________________________________________________________<br/>dense_20 (Dense)             (None, 128)               16512     <br/>_________________________________________________________________<br/>activation_8 (Activation)    (None, 128)               0         <br/>_________________________________________________________________<br/>dense_21 (Dense)             (None, 1)                 129       <br/>=================================================================<br/>Total params: 2,575,189<br/>Trainable params: 133,889<br/>Non-trainable params: 2,441,300<br/>_________________________________________________________________<br/>None</span><span id="aac5" class="nn mj jb oc b gy om oj l ok ol">Test Score: 0.5700<br/>Test Accuracy: 0.7748</span></pre><h2 id="1134" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">双向 LSTM</h2><p id="4093" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">这个 LSTM 变体既可以向前也可以向后工作，也可以按逆序计算单词序列。</p><pre class="ly lz ma mb gt oe oc of og aw oh bi"><span id="5501" class="nn mj jb oc b gy oi oj l ok ol">Model: "sequential_13"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_13 (Embedding)     (None, 65, 100)           2441300   <br/>_________________________________________________________________<br/>bidirectional_2 (Bidirection (None, 256)               234496    <br/>_________________________________________________________________<br/>dense_26 (Dense)             (None, 128)               32896     <br/>_________________________________________________________________<br/>activation_11 (Activation)   (None, 128)               0         <br/>_________________________________________________________________<br/>dense_27 (Dense)             (None, 1)                 129       <br/>=================================================================<br/>Total params: 2,708,821<br/>Trainable params: 267,521<br/>Non-trainable params: 2,441,300<br/>_________________________________________________________________<br/>None</span><span id="176e" class="nn mj jb oc b gy om oj l ok ol">Test Score: 2.1170<br/>Test Accuracy: 0.7552</span></pre><h2 id="7cd1" class="nn mj jb bd mk no np dn mo nq nr dp ms kz ns nt mu ld nu nv mw lh nw nx my ny bi translated">门控循环单元(GRU)</h2><p id="320d" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">GRU 是更新的循环神经结构，类似于 LSTM。</p><pre class="ly lz ma mb gt oe oc of og aw oh bi"><span id="c725" class="nn mj jb oc b gy oi oj l ok ol">Model: "sequential_16"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding_16 (Embedding)     (None, 65, 100)           2441300   <br/>_________________________________________________________________<br/>gru_1 (GRU)                  (None, 128)               87936     <br/>_________________________________________________________________<br/>dense_30 (Dense)             (None, 128)               16512     <br/>_________________________________________________________________<br/>activation_12 (Activation)   (None, 128)               0         <br/>_________________________________________________________________<br/>dense_31 (Dense)             (None, 1)                 129       <br/>=================================================================<br/>Total params: 2,545,877<br/>Trainable params: 104,577<br/>Non-trainable params: 2,441,300<br/>_________________________________________________________________</span><span id="e7b8" class="nn mj jb oc b gy om oj l ok ol">Test Score: 0.6943<br/>Test Accuracy: 0.2448</span></pre></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="63f9" class="mi mj jb bd mk ml na mn mo mp nb mr ms kh nc ki mu kk nd kl mw kn ne ko my mz bi translated">我们发现了什么</h1><p id="f022" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">不同模型之间的验证准确性差异非常小。虽然我们的前馈网络具有最好的验证准确性，但 LSTM 模型的准确性仅低 2%。</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div></figure><blockquote class="on oo op"><p id="501b" class="kq kr lo ks b kt ku kc kv kw kx kf ky oq la lb lc or le lf lg os li lj lk ll ij bi translated">LSTM 预测:不是川普</p></blockquote><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div></figure><blockquote class="on oo op"><p id="3341" class="kq kr lo ks b kt ku kc kv kw kx kf ky oq la lb lc or le lf lg os li lj lk ll ij bi translated">LSTM 预测:不是川普</p></blockquote><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div></figure><blockquote class="on oo op"><p id="8124" class="kq kr lo ks b kt ku kc kv kw kx kf ky oq la lb lc or le lf lg os li lj lk ll ij bi translated">LSTM 预测:不是川普</p></blockquote><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div></figure><blockquote class="on oo op"><p id="5b1d" class="kq kr lo ks b kt ku kc kv kw kx kf ky oq la lb lc or le lf lg os li lj lk ll ij bi translated">LSTM 预测:川普</p></blockquote><p id="b26f" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在对一些随机推文进行测试后，我们决定在 2017 年 3 月后的所有推文中尝试我们的 LSTM 模型，以了解特朗普和员工推文的比例是否会随着时间的推移而变化。这是我们的发现:</p><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/ec641245b0afc2e0c993598e1e15c0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FUwtBC_KlpP1e40WfwcKGg.png"/></div></div></figure><p id="6dba" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了便于比较，这是 2017 年 3 月之前的图表:</p><figure class="ly lz ma mb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/68bc79ffb6fcb6503177b70d6f0b4c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWUAMmBOlI_cSepo4EbnGw.png"/></div></div></figure><h1 id="c8fd" class="mi mj jb bd mk ml mm mn mo mp mq mr ms kh mt ki mu kk mv kl mw kn mx ko my mz bi translated">总结反思</h1><p id="58ec" class="pw-post-body-paragraph kq kr jb ks b kt ni kc kv kw nj kf ky kz nk lb lc ld nl lf lg lh nm lj lk ll ij bi translated">基于我们的结果，我们发现前馈网络表现最好，这可能是由于特朗普推文的简单性质。理想情况下，我们的模型应该考虑大写和标点符号，但是由于嵌入文件的限制，我们不能这样做。如果时间允许，我们可以只在特朗普的推文中训练我们自己的嵌入文件，而不是使用预训练的文件。尽管有这些限制，我们发现我们的结果相当有见地。</p><p id="8c14" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">所以下次你看到这样的推文，</p><figure class="ly lz ma mb gt is"><div class="bz fp l di"><div class="mg mh l"/></div></figure><pre class="ly lz ma mb gt oe oc of og aw oh bi"><span id="8d7d" class="nn mj jb oc b gy oi oj l ok ol">[[0.1183652 0.8849327]]<br/># 88.49% Not Trump</span></pre><p id="1176" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">…也许可以半信半疑。也许实际上美国总统并没有就外交政策发表重大声明。</p><p id="202e" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对我们的数据好奇？想自己玩玩吗？查看我们的<a class="ae lw" href="https://github.com/grenkoca/ANNADL_final" rel="noopener ugc nofollow" target="_blank"> GitHub 回购</a>。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="fcdc" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><em class="lo">特别感谢 Ulf Aslak 博士的指导</em> ❤</p><h1 id="9908" class="mi mj jb bd mk ml mm mn mo mp mq mr ms kh mt ki mu kk mv kl mw kn mx ko my mz bi translated">参考</h1><div class="ip iq gp gr ir ot"><a href="http://www.trumptwitterarchive.com/archive" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jc gy z fp oy fr fs oz fu fw ja bi translated">特朗普推特档案</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">特朗普的所有 30，000 多条推文都可以立即搜索到</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">www.trumptwitterarchive.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a href="https://www.kaggle.com/terenceliu4444/glove6b100dtxt" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jc gy z fp oy fr fs oz fu fw ja bi translated">手套. 6B.100d.txt</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">www.kaggle.com</p></div></div><div class="pc l"><div class="pi l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a href="https://keras.io/" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jc gy z fp oy fr fs oz fu fw ja bi translated">Home - Keras 文档</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">Keras 是一个高级神经网络 API，用 Python 编写，能够运行在 TensorFlow、CNTK 或…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">keras.io</p></div></div><div class="pc l"><div class="pj l pe pf pg pc ph ix ot"/></div></div></a></div><div class="ip iq gp gr ir ot"><a href="https://seaborn.pydata.org/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd jc gy z fp oy fr fs oz fu fw ja bi translated">seaborn:统计数据可视化- seaborn 0.9.0 文档</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">Seaborn 是一个基于 matplotlib 的 Python 数据可视化库。它为绘图提供了一个高级接口…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">seaborn.pydata.org</p></div></div><div class="pc l"><div class="pk l pe pf pg pc ph ix ot"/></div></div></a></div></div></div>    
</body>
</html>