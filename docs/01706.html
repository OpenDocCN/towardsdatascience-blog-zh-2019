<html>
<head>
<title>Implementing an Autoencoder in TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 TensorFlow 2.0 中实现自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7?source=collection_archive---------6-----------------------#2019-03-20">https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7?source=collection_archive---------6-----------------------#2019-03-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="458d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">by</em><a class="ae kp" href="https://twitter.com/afagarap" rel="noopener ugc nofollow" target="_blank"><em class="ko">Abien Fred Agarap</em></a></p><blockquote class="kq kr ks"><p id="f3d8" class="jq jr ko js b jt ju jv jw jx jy jz ka kt kc kd ke ku kg kh ki kv kk kl km kn im bi translated">同时发布于<a class="ae kp" href="https://afagarap.works/2019/03/20/implementing-autoencoder-in-tensorflow-2.0.html" rel="noopener ugc nofollow" target="_blank">https://afagarap . works/2019/03/20/implementing-auto encoder-in-tensor flow-2.0 . html</a></p></blockquote><p id="1469" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi kw translated"><span class="l kx ky kz bm la lb lc ld le di"> G </span> oogle 宣布对世界上最受欢迎的开源机器学习库 TensorFlow 进行重大升级，承诺专注于简单易用、热切执行、直观的高级 API 以及在任何平台上灵活的模型构建。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/277bf02a04f9b064fcab51bcb9e2a485.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uG7-gB4sxoRz96Ns7xnNLg.gif"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">A new TensorFlow logo unveiled in this year’s TensorFlow Dev Summit at Sunnyvale, CA. Animated logo from <a class="ae kp" href="https://medium.com/tensorflow/test-drive-tensorflow-2-0-alpha-b6dd1e522b01" rel="noopener">Test Drive TensorFlow 2.0 Alpha by Wolff Dobson and Josh Gordon (2019, March 7)</a> .</figcaption></figure><p id="c6c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章是对 TensorFlow 2.0 例子的一个小小的尝试。具体来说，我们将讨论<strong class="js iu">自动编码器</strong>的<a class="ae kp" href="https://www.tensorflow.org/guide/keras#model_subclassing" rel="noopener ugc nofollow" target="_blank">子类化 API </a>实现。</p><p id="ccd4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要安装 TensorFlow 2.0，使用下面的<code class="fe lv lw lx ly b">pip install</code>命令，</p><p id="202a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe lv lw lx ly b">pip install tensorflow==2.0.0</code></p><p id="5dbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">或者如果你的系统中有一个图形处理器，</p><p id="1611" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe lv lw lx ly b">pip install tensorflow-gpu==2.0.0</code></p><p id="0b07" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过<a class="ae kp" href="https://tensorflow.org" rel="noopener ugc nofollow" target="_blank">tensorflow.org</a>的<a class="ae kp" href="https://www.tensorflow.org/install" rel="noopener ugc nofollow" target="_blank">本指南</a>了解更多安装细节。</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="73c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在深入研究代码之前，让我们先讨论一下什么是<strong class="js iu">自动编码器</strong>。</p><h1 id="0c4c" class="mg mh it bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">自动编码器</h1><p id="d850" class="pw-post-body-paragraph jq jr it js b jt ne jv jw jx nf jz ka kb ng kd ke kf nh kh ki kj ni kl km kn im bi translated">我们在机器学习中处理大量的数据，这自然会导致更多的计算。然而，我们也可以选择对模型学习贡献最大的数据部分，从而减少计算量。选择数据的<em class="ko">重要部分</em>的过程称为<em class="ko">特征选择，</em>是一个<strong class="js iu"> <em class="ko">自动编码器</em> </strong>的用例数之一。</p><p id="a10d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是<strong class="js iu">自动编码器</strong>到底是什么？好吧，让我们首先回忆一下，神经网络是一种计算模型，用于<em class="ko">寻找<em class="ko">描述</em> <em class="ko">的</em> <em class="ko">函数数据<em class="ko">特征</em> <strong class="js iu"> <em class="ko"> x </em> </strong>及其<em class="ko">值</em>(一个<em class="ko">回归</em>任务)或<em class="ko">标签</em>(一个<em class="ko"/></em></em></p><p id="1a9c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，<strong class="js iu">自动编码器</strong>也是一个神经网络。但不是找到函数<em class="ko">映射</em><em class="ko">特征</em> <strong class="js iu"> <em class="ko"> x </em> </strong>到<em class="ko">它们对应的值</em>或<em class="ko">标签</em> <strong class="js iu"> <em class="ko"> y </em> </strong>，而是找到将<em class="ko">特征</em> <strong class="js iu"> <em class="ko"> x </em> </strong> <em class="ko">映射到自身</em> <strong class="js iu"> <em class="ko"> x </em> </strong>的函数。等等，什么？我们为什么要这么做？</p><p id="913e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，有趣的是在<strong class="js iu">自动编码器</strong>内部发生了什么。为了更好的理解，我们来看一个<strong class="js iu">自动编码器</strong>的图解。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nj"><img src="../Images/29ff595b267c0eb17e1686efd75eec5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_LICG__tuCJIXBI64RWOw.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Illustrated using <a class="ae kp" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">NN-SVG</a>. An autoencoder is an artificial neural network that aims to learn how to reconstruct a data.</figcaption></figure><p id="5f23" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上图来看，一个<strong class="js iu">自动编码器</strong>由两部分组成:(1)一个<strong class="js iu">编码器</strong>，它学习数据的表示，即<em class="ko">重要的</em> <em class="ko">特征数据的</em><strong class="js iu"><em class="ko">z</em></strong>；以及(2)一个<strong class="js iu">解码器</strong>，它根据它如何构造的想法<strong class="js iu"> <em class="ko"> z </em> </strong>来重构数据。</p><p id="0bed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">回过头来，我们建立了一个<strong class="js iu">自动编码器</strong>想要找到将<strong class="js iu"> <em class="ko"> x </em> </strong>映射到<strong class="js iu"> <em class="ko"> x </em> </strong>的函数。它通过其组件来实现这一点。数学上，</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1bffdac81b6788ced7729030a1b31f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/0*H-IFn9kyVEz7BRbE"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><strong class="bd nl"><em class="nm">z</em></strong><em class="nm"> is the learned data representation by the </em><strong class="bd nl"><em class="nm">encoder</em></strong><em class="nm"> from input data </em><strong class="bd nl"><em class="nm">x</em></strong><em class="nm">.</em></figcaption></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/577754a80d46d05c4a0e4eb569be7cf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/0*PAR8maZOsQX5lD8W"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk"><strong class="bd nl">x-hat </strong>is the reconstructed data by the <strong class="bd nl">decoder</strong> based on the learned representation <strong class="bd nl">z</strong>.</figcaption></figure><p id="bf7c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">编码器</strong> <em class="ko"> h-sub-e </em>从输入特征<strong class="js iu"><em class="ko"/></strong>中学习数据表示<strong class="js iu"> <em class="ko"> z </em> </strong>，然后该表示作为<strong class="js iu">解码器</strong> <em class="ko"> h-sub-d </em>的输入，以重构原始数据<strong class="js iu"><em class="ko">x</em></strong></p><p id="f10e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将在下面进一步剖析这个模型。</p><h1 id="393d" class="mg mh it bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">编码器</h1><p id="aed2" class="pw-post-body-paragraph jq jr it js b jt ne jv jw jx nf jz ka kb ng kd ke kf nh kh ki kj ni kl km kn im bi translated">第一个组件<strong class="js iu">编码器</strong>，类似于传统的前馈网络。但是，它的任务不是预测值或标签。相反，它的任务是学习数据的结构，即数据表示<strong class="js iu"> <em class="ko"> z </em> </strong>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/606dfb5cee0735219fab2daee3a47f6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*jUmWBepIqiBdzs8fjjU_kA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Illustrated using <a class="ae kp" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">NN-SVG</a>. The <strong class="bd nl">encoder </strong>learns the representation of a given data.</figcaption></figure><p id="dfaf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">编码</em>是通过将数据输入<strong class="js iu"> <em class="ko"> x </em> </strong>传递给<strong class="js iu">编码器</strong>的隐藏层<strong class="js iu"> <em class="ko"> h </em> </strong>来完成的，以便学习数据表示<strong class="js iu"><em class="ko">z = f(h(x))</em></strong>。我们可以如下实现<code class="fe lv lw lx ly b">Encoder</code>层，</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">The <strong class="ak">encoder</strong> layer of the <strong class="ak">autoencoder</strong> written in TensorFlow 2.0 subclassing API.</figcaption></figure><p id="5f03" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们首先定义一个继承了<code class="fe lv lw lx ly b"><a class="ae kp" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="noopener ugc nofollow" target="_blank">tf.keras.layers.Layer</a></code>的<code class="fe lv lw lx ly b">Encoder</code> <strong class="js iu"> </strong>类，将其定义为一个层而不是一个模型。为什么是图层而不是模型？回想一下，编码器是<strong class="js iu">自动编码器</strong>型号的<em class="ko">组件</em>。</p><p id="6283" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">浏览代码，<code class="fe lv lw lx ly b">Encoder</code>层被定义为具有一个单独的神经元隐藏层(<code class="fe lv lw lx ly b">self.hidden_layer</code>)，用于学习输入特性的激活。然后，我们将隐藏层连接到一个层(<code class="fe lv lw lx ly b">self.output_layer</code>)，该层将数据表示编码到一个较低的维度，该维度由它认为重要的特征组成。因此，<code class="fe lv lw lx ly b">Encoder</code>层的“输出”就是输入数据<strong class="js iu"> <em class="ko"> x </em> </strong>的<em class="ko">学习数据表示</em> <strong class="js iu"> <em class="ko"> z </em> </strong>。</p><h1 id="b918" class="mg mh it bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">解码器</h1><p id="a4ec" class="pw-post-body-paragraph jq jr it js b jt ne jv jw jx nf jz ka kb ng kd ke kf nh kh ki kj ni kl km kn im bi translated">第二个组件<strong class="js iu">解码器</strong>，也类似于一个前馈网络。然而，它不是将数据减少到较低的维度，而是将数据从其较低维度表示<strong class="js iu"> <em class="ko"> z </em> </strong>重建到其原始维度<strong class="js iu"> <em class="ko"> x </em> </strong>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/7e103adeeb300c4d88302e395551acca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*xH_5R58lsjPk0Y-PmMhjPw.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Illustrated using <a class="ae kp" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">NN-SVG</a>. The <strong class="bd nl">decoder </strong>learns to reconstruct the data from its lower dimension representation.</figcaption></figure><p id="6b13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">解码</em>是通过将较低的维度表示<strong class="js iu"> <em class="ko"> z </em> </strong>传递给<strong class="js iu">解码器</strong>的隐藏层<strong class="js iu"> <em class="ko"> h </em> </strong>来完成的，以便将数据重建到其原始维度<strong class="js iu"> <em class="ko"> x = f(h(z)) </em> </strong>。我们可以如下实现<strong class="js iu">解码器</strong>层，</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">The <strong class="ak">decoder</strong> layer of the <strong class="ak">autoencoder</strong> written in TensorFlow 2.0 subclassing API.</figcaption></figure><p id="1000" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们定义了一个<code class="fe lv lw lx ly b">Decoder</code> <strong class="js iu"> </strong>类，它也继承了<code class="fe lv lw lx ly b"><a class="ae kp" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="noopener ugc nofollow" target="_blank">tf.keras.layers.Layer</a></code>。</p><p id="3705" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe lv lw lx ly b">Decoder</code>层也被定义为具有单个隐藏的神经元层，以通过编码器从学习到的表示中重建输入特征。然后，我们将它的隐藏层连接到一个层，该层将数据表示从较低的维度解码到其原始维度。因此，解码器层的“输出”是来自数据表示<strong class="js iu"> <em class="ko"> z </em> </strong>的重构数据<strong class="js iu"> <em class="ko"> x </em> </strong>。最终，解码器的输出是自动编码器的输出。</p><p id="92d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们已经定义了我们的<strong class="js iu">自动编码器</strong>的组件，我们终于可以构建模型了。</p><h1 id="16f7" class="mg mh it bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">构建自动编码器模型</h1><p id="d9fa" class="pw-post-body-paragraph jq jr it js b jt ne jv jw jx nf jz ka kb ng kd ke kf nh kh ki kj ni kl km kn im bi translated">我们现在可以通过实例化<code class="fe lv lw lx ly b">Encoder</code>和<code class="fe lv lw lx ly b">Decoder</code>层来构建<strong class="js iu">自动编码器</strong>模型。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">The <strong class="ak">autoencoder</strong> model written in TensorFlow 2.0 subclassing API.</figcaption></figure><p id="be36" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如上所述，我们使用<strong class="js iu">编码器</strong>层的输出作为<strong class="js iu">解码器</strong>层的输入。就这样吗？不，不完全是。</p><p id="5d9b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止，我们只讨论了一个<strong class="js iu">自动编码器</strong>的组件以及如何构建它，但是我们还没有讨论它实际上是如何学习的。至此，我们只知道<em class="ko">数据流</em>；从输入层到学习数据表示的<strong class="js iu">编码器</strong>层，并使用该表示作为重构原始数据的<strong class="js iu">解码器</strong>层的输入。</p><p id="a083" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">像其他神经网络一样，<strong class="js iu">自动编码器</strong>通过<a class="ae kp" href="https://www.youtube.com/watch?v=LOc_y67AzCA" rel="noopener ugc nofollow" target="_blank">反向传播</a>进行学习。但是我们不是比较模型的值或者标签，而是比较<em class="ko">重构数据</em> <strong class="js iu"> <em class="ko"> x-hat </em> </strong>和<em class="ko">原始数据</em> <strong class="js iu"> <em class="ko"> x </em> </strong>。让我们称这个比较为<em class="ko">重建误差</em>函数，它由下面的等式给出:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/11e172bfbef90b8898c6b6035c3b4318.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*fW4iDoAiEi3L3fo3"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">The reconstruction error in this case is the <a class="ae kp" href="https://www.youtube.com/watch?v=6OvhLPS7rj4" rel="noopener ugc nofollow" target="_blank">mean-squared error</a> function that you’re likely to be familiar with.</figcaption></figure><p id="116e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在张量流中，上述等式可以表示如下:</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Reconstruction error written using TensorFlow core operations.</figcaption></figure><p id="ff7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们到了吗？够近了。只是再补充一些东西。现在我们已经定义了误差函数，我们终于可以为模型编写训练函数了。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Training function written in TensorFlow 2.0 following the paradigm of writing the forward pass imperatively through the use of <a class="ae kp" href="https://stackoverflow.com/questions/53953099/what-is-the-purpose-of-the-tensorflow-gradient-tape" rel="noopener ugc nofollow" target="_blank">GradientTape</a>.</figcaption></figure><p id="493d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种实现反向传播的方式通过使我们能够跟踪梯度以及对其应用<a class="ae kp" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">优化算法</a>而为我们提供了更多的自由。</p><p id="db09" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们说完了吗？让我们看看。</p><ul class=""><li id="2671" class="nt nu it js b jt ju jx jy kb nv kf nw kj nx kn ny nz oa ob bi translated">定义一个<strong class="js iu">编码器</strong>层。<em class="ko">已检查</em>。</li><li id="3952" class="nt nu it js b jt oc jx od kb oe kf of kj og kn ny nz oa ob bi translated">定义一个<strong class="js iu">解码器</strong>层。<em class="ko">已检查</em>。</li><li id="cb7a" class="nt nu it js b jt oc jx od kb oe kf of kj og kn ny nz oa ob bi translated">使用<strong class="js iu">编码器</strong>和<strong class="js iu">解码器</strong>层构建<strong class="js iu">自动编码器</strong>。<em class="ko">已检查</em>。</li><li id="6774" class="nt nu it js b jt oc jx od kb oe kf of kj og kn ny nz oa ob bi translated">定义重建误差函数。<em class="ko">已检查</em>。</li><li id="24a2" class="nt nu it js b jt oc jx od kb oe kf of kj og kn ny nz oa ob bi translated">定义培训功能。<em class="ko">已检查</em>。</li></ul><p id="42c0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">是啊！我们完事了。我们终于可以训练我们的模型了！</p><p id="62d1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但在此之前，让我们实例化一个我们之前定义的<code class="fe lv lw lx ly b">Autoencoder</code>类，以及一个要使用的优化算法。然后，让我们加载我们想要重建的数据。对于这篇文章，让我们使用<em class="ko">难忘的</em> <a class="ae kp" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST 手写数字数据集</a>。</p><p id="b546" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以使用<a class="ae kp" href="https://www.tensorflow.org/guide/summaries_and_tensorboard" rel="noopener ugc nofollow" target="_blank"> TensorBoard </a>可视化我们的训练结果，为此，我们需要使用<code class="fe lv lw lx ly b"><a class="ae kp" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/create_file_writer" rel="noopener ugc nofollow" target="_blank">tf.summary.create_file_writer</a></code>为结果定义一个摘要文件编写器。</p><figure class="lg lh li lj gt lk"><div class="bz fp l di"><div class="np nq l"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Instantiating the <strong class="ak">autoencoder</strong> model, and the optimization function. Loading the MNIST dataset. Finally, the training loop for our <strong class="ak">autoencoder</strong> model.</figcaption></figure><p id="cd6c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们使用定义的摘要文件编写器，并使用<code class="fe lv lw lx ly b"><a class="ae kp" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/record_if" rel="noopener ugc nofollow" target="_blank">tf.summary.record_if</a></code>记录培训摘要。</p><p id="8817" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们最终(现在真正地)可以通过向它提供小批量数据来训练我们的模型，并通过我们之前定义的<code class="fe lv lw lx ly b">train</code>函数计算它的每次迭代的损失和梯度，该函数接受定义的<em class="ko">误差函数</em>、<strong class="js iu"> <em class="ko">自动编码器</em> </strong> <em class="ko">模型</em>、<em class="ko">优化算法</em>和<em class="ko">小批量</em>数据。</p><p id="65f3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在训练模型的每一次迭代之后，计算的重构误差应该减少，以查看模型是否真的在学习(就像在其他神经网络中一样)。最后，为了在 TensorBoard 中记录训练摘要，我们使用<code class="fe lv lw lx ly b"><a class="ae kp" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/scalar" rel="noopener ugc nofollow" target="_blank">tf.summary.scalar</a></code>记录重建误差值，使用<code class="fe lv lw lx ly b"><a class="ae kp" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/summary/image" rel="noopener ugc nofollow" target="_blank">tf.summary.image</a></code>记录原始数据和重建数据的小批量。</p><p id="358f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">经过一些时代，我们可以开始看到一个相对良好的 MNIST 图像重建。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oh"><img src="../Images/122a7c6c843ecab1e835c9f11120976a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qw6X0SpSoxq_bAA--ypL3w.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Plotted using <a class="ae kp" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank">matplotlib</a>. Results on MNIST handwritten digit dataset. Images at the top row are the original ones while images at the bottom row are the reconstructed ones.</figcaption></figure><p id="3230" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">重建的图像可能足够好，但它们非常模糊。可以做许多事情来改善这个结果，例如添加更多的层和/或神经元，或者使用卷积神经网络架构作为<strong class="js iu">自动编码器</strong>模型的基础，或者使用<a class="ae kp" href="https://www.deeplearningbook.org/contents/autoencoders.html" rel="noopener ugc nofollow" target="_blank">不同种类的<strong class="js iu">自动编码器</strong> </a>。</p><h1 id="896c" class="mg mh it bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">结束语</h1><p id="ec1f" class="pw-post-body-paragraph jq jr it js b jt ne jv jw jx nf jz ka kb ng kd ke kf nh kh ki kj ni kl km kn im bi translated"><strong class="js iu">自动编码器</strong>对于降维非常有用。但是它也可以用于<a class="ae kp" href="https://www.deeplearningbook.org/contents/autoencoders.html" rel="noopener ugc nofollow" target="_blank">数据去噪</a>，以及用于<a class="ae kp" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">学习数据集的分布</a>。我希望我们在这篇文章中已经涵盖了足够多的内容，让你兴奋地学习更多关于<strong class="js iu">自动编码器</strong>！</p></div><div class="ab cl lz ma hx mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="im in io ip iq"><p id="d10d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">完整的代码可在<a class="ae kp" href="https://gist.github.com/AFAgarap/326af55e36be0529c507f1599f88c06e" rel="noopener ugc nofollow" target="_blank">这里</a>获得。如果您有任何反馈，您可以通过<a class="ae kp" href="https://twitter.com/afagarap" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系我。我们也可以通过<a class="ae kp" href="https://facebook.com/afagarap" rel="noopener ugc nofollow" target="_blank">脸书</a>、<a class="ae kp" href="https://instagram.com/afagarap" rel="noopener ugc nofollow" target="_blank"> Instagram </a>和/或<a class="ae kp" href="https://www.linkedin.com/in/abienfredagarap/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系！</p><h1 id="59d2" class="mg mh it bd mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd bi translated">参考</h1><ol class=""><li id="4254" class="nt nu it js b jt ne jx nf kb oi kf oj kj ok kn ol nz oa ob bi translated">马丁·阿巴迪、阿希什·阿加瓦尔、保罗·巴勒姆、尤金·布莱夫多、陈质枫、克雷格·西特罗、格雷格·科拉多、安迪·戴维斯、杰弗里·迪恩、马蒂厄·德文、桑杰·格玛瓦特、伊恩·古德菲勒、安德鲁·哈普、杰弗里·欧文、迈克尔·伊萨德、拉斐尔·约泽福维茨、杨青·贾、卢卡斯·凯泽、曼朱纳斯·库德鲁尔、乔希·莱文伯格、丹·曼内、迈克·舒斯特、拉杰特·蒙加、雪莉·穆尔、德里克·默里、克里斯·奥拉、黄邦贤·施伦斯、伯努瓦·施泰纳<a class="ae kp" href="https://arxiv.org/abs/1603.04467" rel="noopener ugc nofollow" target="_blank"> <em class="ko"> TensorFlow:异构系统上的大规模机器学习</em> </a> <em class="ko"> </em> (2015)。tensorflow.org<a class="ae kp" href="https://tensorflow.org" rel="noopener ugc nofollow" target="_blank">公司</a>提供的软件。</li><li id="bf57" class="nt nu it js b jt oc jx od kb oe kf of kj og kn ol nz oa ob bi translated">Francois Chollet，<a class="ae kp" href="https://blog.keras.io/building-autoencoders-in-keras.html" rel="noopener ugc nofollow" target="_blank">在 Keras 建立自动编码器</a> (2016 年 5 月 14 日)，<a class="ae kp" href="https://blog.keras.io/" rel="noopener ugc nofollow" target="_blank">Keras 博客</a>。</li><li id="ef99" class="nt nu it js b jt oc jx od kb oe kf of kj og kn ol nz oa ob bi translated">I. Goodfellow，Y. Bengio，&amp; A .库维尔，<a class="ae kp" href="https://deeplearningbook.org" rel="noopener ugc nofollow" target="_blank">深度学习</a> (2016)。麻省理工出版社。</li></ol></div></div>    
</body>
</html>