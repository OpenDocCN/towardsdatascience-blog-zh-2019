<html>
<head>
<title>Deep Learning for Diagnosis of Skin Images with fastai</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 fastai 的深度学习皮肤图像诊断</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-for-diagnosis-of-skin-images-with-fastai-792160ab5495?source=collection_archive---------4-----------------------#2019-07-12">https://towardsdatascience.com/deep-learning-for-diagnosis-of-skin-images-with-fastai-792160ab5495?source=collection_archive---------4-----------------------#2019-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3e9a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学会从皮肤镜图像中识别皮肤癌和其他疾病</h2></div><p id="1428" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们展示了如何使用 fast.ai 解决 2018 年<em class="le">皮肤病变分析对黑色素瘤检测</em>的挑战，并自动识别七种皮肤病变。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/67900e6b19e7f0cc8bb23fcf5026fea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMt06bG75l8bWncUTKxufA.jpeg"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Pixabay/Pexels free images</figcaption></figure><p id="bc16" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由 Aldo von Wangenheim 发布— <a class="ae lv" href="mailto:aldo.vw@ufsc.br" rel="noopener ugc nofollow" target="_blank"> aldo.vw@ufsc.br </a></p><p id="e07f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是基于以下材料:</p><ol class=""><li id="add8" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mb mc md me bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/classifying-skin-lesions-with-convolutional-neural-networks-fc1302c60d54">走向数据科学::用卷积神经网络对皮肤损伤进行分类</a>——医学深度学习指南和介绍</li><li id="0e68" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">Tschandl，Philipp，2018，“HAM10000 数据集，常见色素性皮肤病损的多源皮镜图像大集合”，<a class="ae lv" href="https://doi.org/10.7910/DVN/DBW86T" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.7910/DVN/DBW86T</a>，哈佛 data verse【arXiv 预印本:<a class="ae lv" href="https://arxiv.org/abs/1803.10417" rel="noopener ugc nofollow" target="_blank">arXiv:1803.10417</a>【cs。CV]]</li><li id="7cf0" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><a class="ae lv" href="https://github.com/ptschandl/HAM10000_dataset" rel="noopener ugc nofollow" target="_blank">用于处理 HAM10000 数据集的工具— GitHub </a>。该存储库提供了对为组装训练数据集而创建和使用的工具的访问。</li></ol><p id="11bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">HAM10000 数据集(<em class="le">具有 10000 幅训练图像的人机对抗</em>)用作<a class="ae lv" href="http://arxiv.org/abs/1902.03368" rel="noopener ugc nofollow" target="_blank"> ISIC 2018 挑战赛(任务 3) </a>的训练集。通过挑战网站<a class="ae lv" href="https://challenge2018.isic-archive.com/task3/" rel="noopener ugc nofollow" target="_blank">https://challenge2018.isic-archive.com/</a>可以获得这个挑战的官方验证和测试集，没有地面真相标签。ISIC-Archive 还提供了一个<a class="ae lv" href="https://submission.challenge.isic-archive.com/#challenge/5bedea14c5eaea2c335effca" rel="noopener ugc nofollow" target="_blank">“现场挑战”提交网站</a>，用于在官方验证和测试集上连续评估自动分类器。</p><ul class=""><li id="77a5" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">HAM10000 数据集也在 Kaggle 上。如果你想用 Kaggle 内核比较你的结果，看这里:<a class="ae lv" href="https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000/kernels" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/kmader/skin-cancer-mnist-ham 10000/kernels</a>上次我看的时候(2019 年 6 月)，那里贴了 55 个不同的解决方案。</li><li id="6861" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><a class="ae lv" href="https://drive.google.com/open?id=1-6MlEi2sUW7AYRrBAjar_TsuoxsmUISo" rel="noopener ugc nofollow" target="_blank">本帖子中显示代码的笔记本(以及一些附加单元格)在这里</a>。</li></ul><p id="bf41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些图像看起来像这样:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ml"><img src="../Images/1d83187b195fa9ec3cc13cd6ffe6b6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XbDGv1EBthwcnaCz-yp-9A.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Random images from the HAM10000 dataset with their ground truth labels</figcaption></figure><h1 id="748f" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">这个数据集中有什么？</h1><p id="aded" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">该数据集包含通过标准皮肤镜检查获得的色素性皮肤损伤。这些是组织产生黑色素的病变，黑色素是人类皮肤的天然色素，并且是深色的。并非所有通过皮肤镜初步检查和分类的病变都一定是色素性病变。这意味着，在现实世界中，全科医生或护士通过皮肤镜检查患者(或患者进行自我检查)并打算将这些图像提交给皮肤科医生进行初步分类，可能会遇到除此数据集描述的病变之外的其他病变(参见<em class="le">有 ISIC 2019 吗？</em>下)。</p><p id="e8cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">HAM10000 数据集中的病变类别包括:</p><ol class=""><li id="9f32" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mb mc md me bi translated"><strong class="kk iu"> nv </strong>:黑素细胞痣——黑素细胞良性肿瘤[6705 张图片]</li><li id="ba73" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><strong class="kk iu">梅尔</strong>:黑色素瘤——一种来源于黑色素细胞的恶性肿瘤【1113 张图片】</li><li id="73c9" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><strong class="kk iu"> bkl </strong>:良性角化病——包括脂溢性角化病、日光性雀斑样痣和扁平苔藓样角化病的一般类别【1099 张图片】；</li><li id="d91d" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><strong class="kk iu">基底细胞癌</strong>:基底细胞癌——上皮性皮肤癌的一种常见变体，很少转移，但如果不治疗，会破坏性地生长(基底细胞癌不一定会产生色素性病变)[514 图片]；</li><li id="2a95" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><strong class="kk iu"> akiec </strong>:光化性角化病和上皮内癌——常见的非侵袭性鳞状细胞癌变体，无需手术即可局部治疗【327 张图片】；</li><li id="ce22" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><strong class="kk iu"> vasc </strong>:从樱桃血管瘤到血管角化瘤、化脓性肉芽肿的血管性皮肤病变【142 张图像】；</li><li id="3861" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated"><strong class="kk iu"> df </strong>:皮肤纤维瘤——一种良性皮肤病变，被认为是良性增生或对轻微创伤的炎症反应【115 张图片】。</li></ol><p id="cb13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有关每个类的更详细描述，请查看 ka ggle kernel:<a class="ae lv" href="https://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app" rel="noopener ugc nofollow" target="_blank">Skin Lesion Analyzer+tensor flow . js Web App-Python notebook 使用来自皮肤癌 MNIST: HAM10000 </a>的数据，以及上面 Philipp Tschandl 的论文。</p><h1 id="2730" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">这个数据集的诊断局限性是什么？</h1><p id="0774" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">皮肤镜图像本身并不能为皮肤病学诊断或远程皮肤病学设置中可靠的远程患者分类提供足够的数据。皮肤镜图像缺乏上下文。为了提供背景信息，您需要执行一个图像采集协议，其中包括患者的全景全身图像和每个病灶的近似图像，这些图像是用尺子或图像中可见的其他参照系拍摄的，以便提供病灶大小的背景信息。用尺子拍摄的近似图像对于已经在治疗中的患者也很重要，以便允许随行医生跟踪病变的发展。为了正确获取全景图像和近似图像，需要遵循保证图像聚焦、从正确的距离拍摄并具有正确照明的协议来执行。还有一些细节不能通过目前使用的标准皮肤镜检查技术可靠地检测出来，在一些情况下，需要进行确认性活组织检查。</p><p id="f740" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您有兴趣了解更多关于<strong class="kk iu">远程皮肤病学检查采集协议</strong>的信息，请点击此处:</p><ul class=""><li id="76ee" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">奥尔多·冯·旺根海姆和丹尼尔·霍尔特豪森·努内斯。<em class="le">创建支持临床协议和临床管理的网络基础设施:远程皮肤科的一个例子</em>。远程医疗和电子保健。提前上线:2018 年 11 月 30 日。【http://doi.org/10.1089/tmj.2018.0197】T4。在 ResearchGate 上也有一份<a class="ae lv" href="https://www.researchgate.net/publication/329332581_Creating_a_Web_Infrastructure_for_the_Support_of_Clinical_Protocols_and_Clinical_Management_An_Example_in_Teledermatology" rel="noopener ugc nofollow" target="_blank">预印本。</a></li></ul><h1 id="8f3f" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">皮肤镜图像是如何获得的？</h1><p id="879f" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">现在使用的<em class="le">接触式皮肤镜</em>是在 20 世纪 90 年代上半期由德国慕尼黑大学的一组研究人员领导的这种检查标准化的国际努力的结果。该设备使用放大 10 倍的单透镜，内部由发光二极管照明。使用矿物油进行检查，在将皮肤镜应用于病变部位并拍照之前，将矿物油应用于病变部位的表面。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nj"><img src="../Images/754bd67680ec688698db4f52d553f58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSNoGTOCO9my3bqZrNv-og.jpeg"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Two different 10x contact dermoscopes: (a) analog pocket dermoscope and (b) dermoscopy adapter for Sony digital cameras employed by us at the <a class="ae lv" href="http://site.telemedicina.ufsc.br/" rel="noopener ugc nofollow" target="_blank">Santa Catarina State Telemedicine and Telehealth Network — STT/SC</a></figcaption></figure><p id="e2c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择单眼 10 倍放大镜头作为标准允许开发非常小的设备，这很快变得非常流行。模拟皮肤镜可以放在胸前的口袋里，数字皮肤镜可以很容易地开发成小型 USB 设备或数码相机和智能手机的适配器。</p><p id="befd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该标准的缺点是 10 倍放大不足以可靠地检测某些病理，例如基底细胞癌，它是皮肤癌的最常见形式。这种形式的肿瘤以血管改变为特征，称为树枝状血管形成，不能用放大 10 倍的单目镜可靠地观察到:为了提供明确的诊断，必须进行确认性活检。可靠的检测需要更高的放大倍数和双目光学[1][2]。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nk"><img src="../Images/538bfcc35ccdc147ba3704d86ddb3c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHVez9WiL4hKFl5p-Ya68g.jpeg"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Non-pigmented basal cell carcinomas acquired (a) with a 10x contact planar dermoscope and (b) with a 50x binocular stereoscopic dermoscope, showing arboriform vascularizations (J.Kreusch, Univ. Lübeck, Sur Prise e.K. collection, POB 11 11 07, 23521 Lübeck, Germany) — only one image of the stereo pair is depicted here</figcaption></figure><p id="497b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">直到 20 世纪 90 年代，还在开发其他类型的皮肤镜，它们可以提供更好的成像质量，但更大，不太容易操作，例如 5 <em class="le"> 0x 双目立体接触皮肤镜</em>。这种设备更适合于病理的视觉早期检测，例如基底细胞癌[1][2]。然而，10 倍接触式单眼皮肤镜的实用性、普及性和标准化阻止了这些其他的研究方向。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/d80e1fe5fcffd71db6c59c10d67f9f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*fkGw7ocNU6_m2by4jc29Ig.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">50x sterescopic dermoscope produced by <a class="ae lv" href="https://www.kocher-feinmechanik.de/english/" rel="noopener ugc nofollow" target="_blank">Kocher Feinmechanik</a>, Germany in the late 1990s and still in clinical use (L.F.Kopke, Florianópolis [2])</figcaption></figure><h1 id="a4d0" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">有没有 ISIC 2019？</h1><p id="9e64" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated"><a class="ae lv" href="https://www.isic-archive.com/#!/topWithHeader/wideContentTop/main" rel="noopener ugc nofollow" target="_blank">ISIC——国际皮肤成像协作组织</a>已经纠正了 HAM10000 数据集中一些皮肤病诊断类别的缺乏，在<a class="ae lv" href="https://challenge2019.isic-archive.com/" rel="noopener ugc nofollow" target="_blank"> ISIC 2019 挑战赛中发布了一个新的数据集:面向黑色素瘤检测的皮肤病变分析</a>。2019 年 5 月 3 日发布的 2019 年数据集现在包含 9 个不同的诊断类别和 25，331 幅图像:</p><ol class=""><li id="60b8" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mb mc md me bi translated">黑素瘤</li><li id="dc2d" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">黑素细胞痣</li><li id="4b24" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">基底细胞癌</li><li id="f054" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">光化性角化病</li><li id="6ed4" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">良性角化病(日光性雀斑样痣/脂溢性角化病/扁平苔藓样角化病)</li><li id="3f19" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">皮肤纤维瘤</li><li id="4933" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">血管病变</li><li id="5d90" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">鳞状细胞癌</li><li id="ebd6" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mb mc md me bi translated">其他人都没有</li></ol><ul class=""><li id="c972" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">在撰写本文时，这个新数据集的测试元数据还不可用。宣布将于 2019 年 8 月 9 日上映。</li><li id="25af" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">到目前为止，ISIC 已经赞助了图像分析方面的四项挑战:ISIC 2016 至 ISIC 2019，始终以“皮肤病变分析促进黑色素瘤检测”为主题。<a class="ae lv" href="https://www.isic-archive.com/#!/topWithHeader/tightContentTop/challenges" rel="noopener ugc nofollow" target="_blank">这四项挑战可在国际标准行业分类档案中找到</a>。</li></ul><p id="28fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们开始研究 2018 年挑战的数据集。</p><h1 id="74fe" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">初始化</h1><p id="22f6" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">每次运行此<a class="ae lv" href="https://drive.google.com/open?id=1-6MlEi2sUW7AYRrBAjar_TsuoxsmUISo" rel="noopener ugc nofollow" target="_blank">笔记本</a>时，请执行以下本节中的操作一次…</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="0468" class="nr mn it nn b gy ns nt l nu nv">%reload_ext autoreload<br/>%autoreload 2<br/>%matplotlib inline</span></pre><h1 id="7788" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">在 Google Colab 上测试您的虚拟机…</h1><p id="463f" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">只是为了确定，看看哪一个 CUDA 驱动和哪一个 GPU Colab 已经为你提供了。GPU 通常是:</p><ul class=""><li id="db28" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">一个 11 GB 内存的 K80 或者(如果你真的幸运的话)</li><li id="b885" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">一辆配有 14 GB 内存的特斯拉 T4</li></ul><p id="bf71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果谷歌的服务器很拥挤，你最终只能访问 GPU 的一部分。如果你的 GPU 与另一台<em class="le"> Colab </em>笔记本共享，你会看到可供你使用的内存量减少。</p><p id="dc49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">小贴士:避开美国西海岸的高峰期。我住在 GMT-3，我们比美国东海岸早两个小时，所以我总是试图在早上进行繁重的处理。</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="96aa" class="nr mn it nn b gy ns nt l nu nv">!/opt/bin/nvidia-smi<br/>!nvcc --version</span></pre><p id="6934" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我开始运行这里描述的实验时，我很幸运:我有一个 15079 MB RAM 的完整 T4！我的输出如下所示:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="ed84" class="nr mn it nn b gy ns nt l nu nv">Thu May  2 07:36:26 2019       <br/>+-----------------------------------------------------------------------------+<br/>| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |<br/>|-------------------------------+----------------------+----------------------+<br/>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/>|===============================+======================+======================|<br/>|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |<br/>| N/A   63C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>                                                                               <br/>+-----------------------------------------------------------------------------+<br/>| Processes:                                                       GPU Memory |<br/>|  GPU       PID   Type   Process name                             Usage      |<br/>|=============================================================================|<br/>|  No running processes found                                                 |<br/>+-----------------------------------------------------------------------------+<br/>nvcc: NVIDIA (R) Cuda compiler driver<br/>Copyright (c) 2005-2018 NVIDIA Corporation<br/>Built on Sat_Aug_25_21:08:01_CDT_2018<br/>Cuda compilation tools, release 10.0, V10.0.130</span></pre><h1 id="0214" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">图书馆进口</h1><p id="30fd" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">在这里，我们导入所有必需的包。我们将与<a class="ae lv" href="http://www.fast.ai/2018/10/02/fastai-ai/" rel="noopener ugc nofollow" target="_blank"> fastai V1 库</a>合作，它位于<a class="ae lv" href="https://hackernoon.com/pytorch-1-0-468332ba5163" rel="noopener ugc nofollow" target="_blank"> Pytorch 1.0 </a>之上。fastai 库提供了许多有用的功能，使我们能够快速轻松地构建神经网络并训练我们的模型。</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="3905" class="nr mn it nn b gy ns nt l nu nv">from fastai.vision import *<br/>from fastai.metrics import error_rate<br/>from fastai.callbacks import SaveModelCallback</span><span id="3f3e" class="nr mn it nn b gy nw nt l nu nv"># Imports for diverse utilities<br/>from shutil import copyfile<br/>import matplotlib.pyplot as plt<br/>import operator<br/>from PIL import Image<br/>from sys import intern   # For the symbol definitions</span></pre><h1 id="b439" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">导出和恢复功能</h1><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="1e04" class="nr mn it nn b gy ns nt l nu nv"># Export network for deployment and create a copy</span><span id="856e" class="nr mn it nn b gy nw nt l nu nv">def exportStageTo(learn, path):<br/>    learn.export()<br/>    # Faça backup diferenciado<br/>    copyfile(path/'export.pkl', path/'export-malaria.pkl')<br/>    <br/>#exportStage1(learn, path)</span><span id="1213" class="nr mn it nn b gy nw nt l nu nv"># Restoration of a deployment model, for example in order to conitnue fine-tuning</span><span id="9174" class="nr mn it nn b gy nw nt l nu nv">def restoreStageFrom(path):<br/>  # Restore a backup<br/>  copyfile(path/'export-malaria.pkl', path/'export.pkl')<br/>  return load_learner(path)<br/>  <br/>#learn = restoreStage1From(path)</span></pre><h1 id="2edb" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">下载色素性病变的皮肤镜图像</h1><p id="7495" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">我们将下载这个数据集的 Kaggle 版本，因为 Google Colab 已经预装了 Kaggle API，并且都组织在一个版本中。zip 文件。为了从 Kaggle 下载，您需要:</p><ul class=""><li id="a9e0" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">卡格尔的一个账户</li><li id="1ccd" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">要在 Colab 上安装您的 Kaggle 凭证(一个. json 文件)</li></ul><p id="d447" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解如何做到这一点，首先查看本教程和 Kaggle API 说明，然后生成您的凭证并上传到 Colab:</p><ul class=""><li id="391f" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/setting-up-kaggle-in-google-colab-ebb281b61463"> TowardsDataScience::在 Google Colab 中设置 ka ggle——Anne Bonner 为新手编写的简单教程</a></li><li id="6a0d" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><a class="ae lv" href="https://github.com/Kaggle/kaggle-api#api-credentials" rel="noopener ugc nofollow" target="_blank">https://github.com/Kaggle/kaggle-api#api-credentials</a></li></ul><h1 id="2f27" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">当你创建并复制你的 Kaggle 证书到 Colab</h1><p id="f9f3" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">运行下面的单元格。它将为您的 Kaggle API 凭据创建一个文件夹，并将您的凭据安装在 Colab:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="4808" class="nr mn it nn b gy ns nt l nu nv">!mkdir .kaggle<br/>!mv kaggle.json .kaggle<br/>!chmod 600 /content/.kaggle/kaggle.json<br/>!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json<br/>!chmod 600 ~/.kaggle/kaggle.json<br/>!kaggle config set -n path -v{/content}</span></pre><p id="2b10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">出于某种神秘的原因，这个脚本有时不工作。这似乎与 Colab 命名主文件夹的形式有关。如果您遇到错误消息，只需再次执行它。最终输出应该如下所示:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="2ed5" class="nr mn it nn b gy ns nt l nu nv">- path is now set to: {/content}</span></pre><h1 id="ea6c" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">执行实际的下载和解压缩</h1><p id="8947" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">创建一个“数据”文件夹，并将皮肤镜检查图像下载到其中</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="6b5c" class="nr mn it nn b gy ns nt l nu nv">!mkdir data<br/>!kaggle datasets download kmader/skin-cancer-mnist-ham10000 -p data</span></pre><p id="6b7a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将产生以下输出:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="4090" class="nr mn it nn b gy ns nt l nu nv">Downloading skin-cancer-mnist-ham10000.zip to data<br/>100% 2.61G/2.62G [00:52&lt;00:00, 42.3MB/s]<br/>100% 2.62G/2.62G [00:52&lt;00:00, 53.4MB/s]</span></pre><p id="3f0d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将整个 zip 文件解压到/content/data 中，然后悄悄地(-q)解压图像文件(你不想冗长地解压超过 10k 的图像吧！).我们将使用 override 选项(-o)，以便允许安静地覆盖那些在您之前中断的尝试中创建的文件。</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="7ab4" class="nr mn it nn b gy ns nt l nu nv"># Unzip the whole zipfile into /content/data<br/>!unzip -o data/skin-cancer-mnist-ham10000.zip -d data<br/># Quietly unzip the image files<br/>!unzip -o -q data/HAM10000_images_part_1.zip -d data<br/>!unzip -o -q data/HAM10000_images_part_2.zip -d data<br/># Tell me how many files I unzipped///<br/>!echo files in /content/data: `ls data | wc -l`</span></pre><p id="335a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您有 10，023 个文件，那么您就做对了！</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="5eb4" class="nr mn it nn b gy ns nt l nu nv">Archive:  data/skin-cancer-mnist-ham10000.zip<br/>  inflating: data/hmnist_28_28_RGB.csv  <br/>  inflating: data/HAM10000_metadata.csv  <br/>  inflating: data/HAM10000_images_part_1.zip  <br/>  inflating: data/hmnist_28_28_L.csv  <br/>  inflating: data/hmnist_8_8_L.csv   <br/>  inflating: data/HAM10000_images_part_2.zip  <br/>  inflating: data/hmnist_8_8_RGB.csv  <br/>files in /content/data: 10023</span></pre><h1 id="22b7" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">准备您的数据</h1><p id="049c" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">现在，我们将准备使用 fast.ai 处理我们的数据。</p><p id="ef4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们之前的医学图像分类帖子(<a class="ae lv" rel="noopener" target="_blank" href="/deep-learning-and-medical-image-analysis-for-malaria-detection-with-fastai-c8f08560262f">利用 fastai </a>进行疟疾检测的深度学习和医学图像分析)中，我们已经将图像类别分类到文件夹中，每个类别一个文件夹。</p><p id="ff0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们将所有图像存储在一个文件夹中，并将元数据存储在一个电子表格中，我们将使用 fast.ai 数据块 API 中的 fast . ai<em class="le">imagedata bunch . from _ CSV()</em>方法读取该电子表格。</p><h1 id="5970" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">我们这里有什么不同？</h1><p id="574c" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">HAM10000 数据集不提供根据类别分类到文件夹中的图像。相反，所有图像都在一个文件夹中，并且提供了一个电子表格，其中包含每个图像的若干元数据。在本教程中，我们将从这里读取每个图像的类。csv 电子表格，而不是将图像文件组织到文件夹中，其中文件夹的名称是图像所属的类。<strong class="kk iu"> fast.ai </strong>还提供了解释电子表格和提取图像分类数据的现成方法。在这篇文章中，我们将学习如何利用这些方法。</p><p id="df84" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们将使用 fast.ai 的<a class="ae lv" href="https://docs.fast.ai/data_block.html" rel="noopener ugc nofollow" target="_blank">数据块 API </a>。在下面的帖子中有一个很好的解释:</p><ul class=""><li id="abb8" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">Tom McKenzie 的<a class="ae lv" href="https://medium.com/@tmckenzie.nz/using-the-fastai-data-block-api-b4818e72155b" rel="noopener"> Medium::使用 fastai 数据块 API </a></li></ul><h1 id="66ac" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">创建您的培训和验证数据集</h1><p id="f25a" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">在上面使用 Keras 的原始教程中，有一个从数据中创建训练、验证和测试文件夹的例程。有了 fast.ai 就不需要了:如果你只有一个“train”文件夹，你可以在创建 DataBunch 的时候通过简单的传递几个参数来拆分它…</p><p id="608d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过 fast.ai，我们还可以轻松地使用不同于原始 ImageNet 分辨率的分辨率，这些分辨率用于预先训练我们将使用的网络。在上面列出的教程中，作者将数据集图像分辨率降低到 224x224，以便使用 Keras MobileNet 模型。我们将采用 448x448 的分辨率:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="4d42" class="nr mn it nn b gy ns nt l nu nv">bs = 64        # Batch size, 64 for medium images on a T4 GPU...<br/>size = 448      # Image size, 448x448 is double than the orignal <br/>                # ImageNet<br/>path = Path("./data")   # The path to the 'train' folder you created...</span><span id="ac18" class="nr mn it nn b gy nw nt l nu nv"># Limit your augmentations: it's medical data! You do not want to phantasize data...<br/># Warping, for example, will let your images badly distorted, so don't do it!<br/># This dataset is big, so don't rotate the images either. Lets stick to flipping...<br/>tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_rotate=None, max_warp=None, max_zoom=1.0)<br/># Create the DataBunch!<br/># Remember that you'll have images that are bigger than 128x128 and images that are smaller,   <br/># so squish them all in order to occupy exactly 128x128 pixels...<br/>data = ImageDataBunch.from_csv('data', csv_labels='HAM10000_metadata.csv', suffix='.jpg', fn_col=1, label_col=2, <br/>                               ds_tfms=tfms, valid_pct = 0.2,size=size, bs=bs)<br/>print('Transforms = ', len(tfms))<br/># Save the DataBunch in case the training goes south... so you won't have to regenerate it..<br/># Remember: this DataBunch is tied to the batch size you selected. <br/>data.save('imageDataBunch-bs-'+str(bs)+'-size-'+str(size)+'.pkl')<br/># Show the statistics of the Bunch...<br/>print(data.classes)<br/>data</span></pre><p id="53e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将产生以下输出:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="3213" class="nr mn it nn b gy ns nt l nu nv">Transforms =  2<br/>['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']</span><span id="0767" class="nr mn it nn b gy nw nt l nu nv">ImageDataBunch;</span><span id="7516" class="nr mn it nn b gy nw nt l nu nv">Train: LabelList (8012 items)<br/>x: ImageList<br/>Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448)<br/>y: CategoryList<br/>bkl,bkl,bkl,bkl,bkl<br/>Path: data;</span><span id="1c66" class="nr mn it nn b gy nw nt l nu nv">Valid: LabelList (2003 items)<br/>x: ImageList<br/>Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448)<br/>y: CategoryList<br/>bkl,nv,nv,nv,nv<br/>Path: data;</span><span id="d0a7" class="nr mn it nn b gy nw nt l nu nv">Test: None</span></pre><h1 id="c2d5" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">查看您的数据集群，看看增加是否可以接受…</h1><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="a804" class="nr mn it nn b gy ns nt l nu nv">data.show_batch(rows=5, figsize=(15,15))</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ml"><img src="../Images/1d83187b195fa9ec3cc13cd6ffe6b6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XbDGv1EBthwcnaCz-yp-9A.png"/></div></div></figure><h1 id="e9c7" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">第一次训练实验:ResNet34</h1><p id="b507" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">如果您不知道使用什么模型，从 34 层的剩余网络开始是一个不错的选择。功能强大，但不太小也不太大…</p><p id="ee8f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在上面列出的教程中，作者使用了在 Keras 中实现的 MobileNet 和网络的原始图像分辨率 224x224。在 fast.ai 中，ResNet 很容易适应我们的 DataBunch 的 448x448 分辨率。</p><p id="bde6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们将开始训练我们的模型。我们将使用一个<a class="ae lv" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>主干和一个单隐层全连接头作为分类器。不知道这些东西是什么意思？不要担心，我们将在接下来的课程中深入探讨。目前，您需要知道我们正在构建一个模型，该模型将图像作为输入，并将输出每个类别的预测概率(在这种情况下，它将有 37 个输出)。</p><p id="1378" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用两种不同的指标来衡量我们的培训成功程度:</p><ul class=""><li id="e32d" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><strong class="kk iu">精度</strong>:验证精度</li><li id="c54e" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><strong class="kk iu">错误率</strong>:验证错误率</li></ul><p id="f33d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想了解更多信息，请看 https://docs.fast.ai/metrics.html 的<a class="ae lv" href="https://docs.fast.ai/metrics.html" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="903d" class="nr mn it nn b gy ns nt l nu nv">learn = cnn_learner(data, models.resnet34, metrics=[accuracy, error_rate, dice(iou=True), fbeta])<br/>learn.model</span></pre><p id="b825" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">只需将包含<em class="le"> DataBunch </em>实例的<em class="le"> data </em>变量传递给<em class="le"> cnn_learner() </em>函数，fast.ai 就会自动调整新网络的输入层以适应更高的图像分辨率。该模型将如下所示:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="77a3" class="nr mn it nn b gy ns nt l nu nv">Sequential(<br/>  (0): Sequential(<br/>    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU(inplace)<br/>    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>    (4): Sequential(<br/>      (0): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>      (1): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>      (2): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (5): Sequential(<br/>      (0): BasicBlock(<br/>        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (downsample): Sequential(<br/>          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        )<br/>...<br/>...<br/>... and so on...</span></pre><h1 id="970b" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">培训策略</h1><p id="471b" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">我们将采用 Leslie N. Smith 开发的<em class="le"> fit1cycle </em>方法，详情见下文:</p><ul class=""><li id="3785" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" href="https://docs.fast.ai/callbacks.one_cycle.html" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/callbacks.one_cycle.html</a></li><li id="21cf" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><strong class="kk iu"> <em class="le">一种训练有素的神经网络超参数方法:第一部分——学习速率、批量大小、动量和权重衰减</em>【https://arxiv.org/abs/1803.09820】T21<a class="ae lv" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank"/></strong></li><li id="3473" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><strong class="kk iu"> <em class="le">超收敛:使用大学习率非常快速地训练残差网络</em></strong>—<a class="ae lv" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1708.07120</a></li><li id="3a13" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">还有一篇来自<a class="ae lv" href="https://towardsdatascience.com/@nachiket.tanksale" rel="noopener" target="_blank"> Nachiket Tanksale </a>的非常有趣的文章，名为<a class="ae lv" rel="noopener" target="_blank" href="/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6">寻找好的学习率和单周期政策</a>，其中讨论了周期学习率和动量</li><li id="10fa" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">因为这种方法很快，所以在第一个迁移学习阶段，我们将只使用 10 个时期</li></ul><p id="427d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想了解更多关于 fast.ai 库中新的学习 API 的信息，可以看看 Sylvain Gugger 准备的这个笔记本。</p><p id="9834" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果性能变得更好，我们也将在每个时期保存网络:<a class="ae lv" href="https://docs.fast.ai/callbacks.html#SaveModelCallback" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/callbacks.html#SaveModelCallback</a></p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="b7bd" class="nr mn it nn b gy ns nt l nu nv">learn.fit_one_cycle(10, callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy', name='derma-1')])<br/># Salve a rede (necessita regerar o databunch caso a gente continue)<br/>learn.save('derma-stage-1')<br/># Faça o deploy desta rede para podermos usar offline depois para fazer testes<br/>exportStageTo(learn, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2543f47b273105ccc3bab6d0c2c62167.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*lu68h9M2Lw1PGIAuu4nqbA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">85% accuracy…</figcaption></figure><h1 id="8329" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">ResNet34 的结果</h1><p id="5234" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">让我们看看我们得到了什么结果。</p><p id="9857" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将首先看到模型最容易混淆的类别。我们将尝试看看模型预测的结果是否合理。在这种情况下，错误看起来是合理的(没有一个错误看起来明显幼稚)。这表明我们的分类器工作正常。</p><p id="02f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，当我们绘制混淆矩阵时，我们可以看到分布严重偏斜:该模型一次又一次地犯同样的错误，但它很少混淆其他类别。这表明它只是发现很难区分彼此之间的某些特定类别；这是正常的行为。</p><p id="ca9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们生成一个<em class="le">分类解释</em>，看看一些结果，混淆矩阵和损失曲线。</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="d864" class="nr mn it nn b gy ns nt l nu nv">interp = ClassificationInterpretation.from_learner(learn)</span><span id="1b6c" class="nr mn it nn b gy nw nt l nu nv">losses,idxs = interp.top_losses()</span><span id="f7e1" class="nr mn it nn b gy nw nt l nu nv">len(data.valid_ds)==len(losses)==len(idxs)</span></pre><p id="8b81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看您的最差结果，首先不使用热图:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="3b87" class="nr mn it nn b gy ns nt l nu nv">interp.plot_top_losses(9, figsize=(20,11), <strong class="nn iu">heatmap=False</strong>)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/1bd954979c33055b98a4f0180d736eee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwauRbUjgegVUZAoiMdPVw.png"/></div></div></figure><p id="d966" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，做同样的事情，但是用热图突出显示图像，以便查看每个图像的哪些部分导致了错误的分类:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="362f" class="nr mn it nn b gy ns nt l nu nv">interp.plot_top_losses(9, figsize=(20,11), <strong class="nn iu">heatmap=True</strong>)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/b20a35bac6594847e352481139756fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-B57XV3UCfl-4zlqv5v8nQ.png"/></div></div></figure><h1 id="c0db" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">显示混淆矩阵</h1><p id="0f18" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">这里我们有<strong class="kk iu"> <em class="le">七个职业</em> </strong>看看混乱矩阵很有意义。此外，它能拍出漂亮的照片…</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="024c" class="nr mn it nn b gy ns nt l nu nv">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/e27620344ba06bf76e1f6b39543080b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*wZGeLFCBL5XlN68JZw3HgQ.png"/></div></figure><p id="1997" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在这里可以看到:</p><ul class=""><li id="3cb8" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">痣是最常见的现象。人们可以考虑减少训练集中的痣的实例，以便不扭曲结果；</li><li id="7b38" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">有几种良性角化病(<strong class="kk iu"> bkl </strong>)被错误分类。这可能是因为该数据集中的<strong class="kk iu"> <em class="le"> bkl </em> </strong>是一个通用类别，包括脂溢性角化病、日光性雀斑样痣和扁平苔藓样角化病，这些皮肤病即使相关，看起来也非常不同；</li><li id="3a44" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">还有几个黑色素瘤(<strong class="kk iu"> mel </strong>)被错误分类。这是一个惊喜。我期望网络在这里表现得更好。</li></ul><p id="b1c7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对混淆矩阵感到困惑，请看这里:</p><ul class=""><li id="c1bf" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/taking-the-confusion-out-of-confusion-matrices-c1ce054b3d3e">从混乱矩阵中找出混乱</a>作者<a class="ae lv" href="https://towardsdatascience.com/@allison.n.ragan" rel="noopener" target="_blank">艾莉森·拉冈</a></li><li id="e2ba" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><a class="ae lv" href="https://medium.com/datadriveninvestor/simplifying-the-confusion-matrix-aa1fa0b0fc35" rel="noopener">通过<a class="ae lv" href="https://medium.com/@madhav.mishra" rel="noopener">马达夫·米什拉</a>简化混淆矩阵</a></li></ul><h1 id="3c0a" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">展示你的学习曲线:</h1><p id="805e" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">画出你的损失，以便看到学习曲线:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="eeb2" class="nr mn it nn b gy ns nt l nu nv">learn.recorder.plot_losses()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/edfaa89c8677179a6dd33e7dfc658441.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*P2WETnJ1kLiDZCEpwFevyQ.png"/></div></figure><p id="892a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个成绩确实不错。网络有些波动，但学习稳定。现在让我们试着微调网络。</p><h1 id="a399" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">微调 ResNet34</h1><p id="bff9" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">首先解冻网络，并尝试为这个特定的网络找到一个好的学习速率范围。</p><p id="10bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">方法<a class="ae lv" href="https://docs.fast.ai/callbacks.lr_finder.html" rel="noopener ugc nofollow" target="_blank"> <em class="le"> learn.lr_find() </em> </a>帮你找到一个最优的学习速率。它使用了 2015 年论文<em class="le">中开发的技术，用于训练神经网络</em>(<a class="ae lv" href="http://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1506.01186</a>)的循环学习率，我们只是从一个非常小的值开始增加学习率，直到损失开始减少。</p><p id="d01c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你想知道更多关于寻找最佳学习率的信息，请看这里:</p><ul class=""><li id="5829" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/speeding-up-neural-net-training-with-lr-finder-c3b401a116d0"> <strong class="kk iu">迈向数据科学</strong> :: <em class="le">用 LR-Finder 加速神经网络训练——为你的网络找到良好的初始学习率</em>faiz an Ahemad</a></li></ul><p id="f623" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们开始吧:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="7208" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn.unfreeze()<br/># Find optimum learning rates<br/>learn.lr_find()<br/># Include <strong class="nn iu">suggestion=True</strong> in order to obtain a suggestion on where to look...<br/>learn.recorder.plot(suggestion=True)</span></pre><p id="5483" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们微调一下结果。为了确保万无一失，我们将使用 30 个纪元。学习率搜索器将 1e-5 确定为“安全”学习率。因此，我们将使用<em class="le">经验法则</em>定义一个<em class="le">学习速率范围</em>:以“安全”速率 1e-5 结束，以高一个数量级的速率开始:<strong class="kk iu"> max_lr=slice(1e-4，1e-5) </strong>。</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="b5a3" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn.unfreeze()</span><span id="16ef" class="nr mn it nn b gy nw nt l nu nv">learn.fit_one_cycle(30, <strong class="nn iu">max_lr=slice(1e-4,1e-5)</strong>, <br/>                    callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy', name='derma')])<br/># Agora, salve como estágio 2...<br/>learn.save('derma-stage-2')<br/># Deploy definitivo<br/>exportStageTo(learn, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/df7c8d3dd1676e7c46510b3b863875f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*Ha1j_fxsdGZJz80vaKiIhA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">93% accuracy!</figcaption></figure><p id="3fc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们第一次运行就达到了 93%的准确率。这很好！在上面的教程和内核中达到的准确率分别是 85%和 86%。</p><p id="a350" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们看看我们的统计数据。：</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="006e" class="nr mn it nn b gy ns nt l nu nv">interp = ClassificationInterpretation.from_learner(learn)</span><span id="a6c2" class="nr mn it nn b gy nw nt l nu nv">losses,idxs = interp.top_losses()</span><span id="8166" class="nr mn it nn b gy nw nt l nu nv"># Test to see if there's not anything missing (must return True)<br/>len(data.valid_ds)==len(losses)==len(idxs)</span></pre><p id="1114" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果返回 True，则绘制微调网络的混淆矩阵:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="17a1" class="nr mn it nn b gy ns nt l nu nv">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/262196962eb4e86860e19f35c6f10305.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*GdYuX8t2YOFz1Wx54RKesg.png"/></div></figure><p id="e77d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个矩阵中黑色素瘤的预测看起来更好！让我们看看训练曲线:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="9751" class="nr mn it nn b gy ns nt l nu nv">learn.recorder.plot_losses()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/b3da65a1f2dfef404301798357cb8e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*uluSFbbRGLruNjUnEY8H9w.png"/></div></figure><p id="b078" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，训练和验证曲线都在振荡。训练曲线似乎到达了一个平台，并且与验证曲线分离，验证曲线下降得更慢。这表明我们可能正在走向网络的过度拟合。它会被指示到此为止。</p><p id="5ae1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 Colab 网站上我们提供的<a class="ae lv" href="https://drive.google.com/open?id=1-6MlEi2sUW7AYRrBAjar_TsuoxsmUISo" rel="noopener ugc nofollow" target="_blank">笔记本中，我们又对网络进行了 30 个纪元的训练，以确保万无一失。它实际上变得更糟，可能是由于过度拟合。所以停在这里对 ResNet34 来说是个不错的选择。</a></p><h1 id="5447" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">变大:ResNet50</h1><p id="f1cc" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">ResNet34 的准确率达到了 92.9%。让我们看看更大的网络能否让我们表现得更好。我们将再次创建 DataBunch，这次使用较小的批处理大小，以避免 GPU 内存过载…</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="2371" class="nr mn it nn b gy ns nt l nu nv">bs = 28         # Batch size, 28 for medium images on a T4 GPU and ResNet50...<br/>size = 448      # Image size, 448x448 is double than the orignal <br/>                # ImageNet size of the pre-trained ResNet we'll be using, <br/>                # should be easy to train...<br/>path = Path("./data")   # The path to the 'train' folder you created...</span><span id="45b1" class="nr mn it nn b gy nw nt l nu nv"># Limit your augmentations: it's medical data! You do not want to phantasize data...<br/># Warping, for example, will let your images badly distorted, so don't do it!<br/># This dataset is big, so don't rotate the images either. Lets stick to flipping...<br/>tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_rotate=None, max_warp=None, max_zoom=1.0)<br/># Create the DataBunch!<br/># Remember that you'll have images that are bigger than 128x128 and images that are smaller,   <br/># so squish them all in order to occupy exactly 128x128 pixels...<br/>data = ImageDataBunch.from_csv('data', csv_labels='HAM10000_metadata.csv', suffix='.jpg', fn_col=1, label_col=2, <br/>                               ds_tfms=tfms, valid_pct = 0.2,size=size, bs=bs)<br/>print('Transforms = ', len(tfms))<br/># Save the DataBunch in case the training goes south... so you won't have to regenerate it..<br/># Remember: this DataBunch is tied to the batch size you selected. <br/>data.save('imageDataBunch-bs-'+str(bs)+'-size-'+str(size)+'.pkl')<br/># Show the statistics of the Bunch...<br/>print(data.classes)<br/>data</span></pre><p id="71d7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在输出应该是这样的:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="323c" class="nr mn it nn b gy ns nt l nu nv">Transforms =  2<br/>['akiec', 'bcc', 'bkl', 'df', 'mel', 'nv', 'vasc']</span><span id="9c8d" class="nr mn it nn b gy nw nt l nu nv">ImageDataBunch;</span><span id="94c5" class="nr mn it nn b gy nw nt l nu nv">Train: LabelList (8012 items)<br/>x: ImageList<br/>Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448)<br/>y: CategoryList<br/>bkl,bkl,bkl,bkl,bkl<br/>Path: data;</span><span id="79c9" class="nr mn it nn b gy nw nt l nu nv">Valid: LabelList (2003 items)<br/>x: ImageList<br/>Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448),Image (3, 448, 448)<br/>y: CategoryList<br/>nv,nv,nv,mel,bkl<br/>Path: data;</span><span id="03f4" class="nr mn it nn b gy nw nt l nu nv">Test: None</span></pre><p id="14ee" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在创建一个 ResNet50:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="11e2" class="nr mn it nn b gy ns nt l nu nv">learn50 = cnn_learner(data, models.resnet50, metrics=[accuracy, error_rate])<br/>learn50.model</span></pre><p id="c363" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们在指标中包括了准确性，因此我们不需要根据错误率手动执行计算。模型看起来应该是这样的:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="6485" class="nr mn it nn b gy ns nt l nu nv">Sequential(<br/>  (0): Sequential(<br/>    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU(inplace)<br/>    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>    (4): Sequential(<br/>      (0): Bottleneck(<br/>        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (downsample): Sequential(<br/>          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        )<br/>      )<br/>      (1): Bottleneck(<br/>        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>      )<br/>      (2): Bottleneck(<br/>        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)<br/>        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>      )<br/>    )<br/>...<br/>...<br/>...<br/>(1): Sequential(<br/>    (0): AdaptiveConcatPool2d(<br/>      (ap): AdaptiveAvgPool2d(output_size=1)<br/>      (mp): AdaptiveMaxPool2d(output_size=1)<br/>    )<br/>    (1): Flatten()<br/>    (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (3): Dropout(p=0.25)<br/>    (4): Linear(in_features=4096, out_features=512, bias=True)<br/>    (5): ReLU(inplace)<br/>    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (7): Dropout(p=0.5)<br/>    (8): Linear(in_features=512, out_features=7, bias=True)<br/>  )<br/>)</span></pre><p id="6a50" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">把它学会:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="f1b9" class="nr mn it nn b gy ns nt l nu nv">learn50.fit_one_cycle(15, callbacks=[SaveModelCallback(learn50, every='epoch', monitor='accuracy', name='derma50-1')])<br/># Save weights<br/>learn50.save('derma50-stage-1')<br/># Deploy the whole network (with the databunch)<br/>exportStageTo(learn50, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4600f60dca5bc6dc51f1da4e96c7e0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*UM0bxfRSLJWuIBXaGZfH4w.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">87.6% accuracy….</figcaption></figure><p id="5de3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看结果:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="62e7" class="nr mn it nn b gy ns nt l nu nv">interp = ClassificationInterpretation.from_learner(learn50)</span><span id="cd39" class="nr mn it nn b gy nw nt l nu nv">losses,idxs = interp.top_losses()</span><span id="080b" class="nr mn it nn b gy nw nt l nu nv">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/607e886f7bf14481f83290eb0ec66931.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*nqTWDJnpSirQTR5_xJwazw.png"/></div></figure><p id="9993" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看学习曲线:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="c31a" class="nr mn it nn b gy ns nt l nu nv">learn50.recorder.plot_losses()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/19b30d8fa4ced7b891cb74004b880141.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*PxbEv1-z94vmnYnKdYg3Xg.png"/></div></figure><p id="2863" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">目前，这并没有给人留下深刻印象:在第一个迁移学习阶段，网络比 ResNet34 振荡得更多，结果即使在数字上更好(85% x 87%的准确率)，实际上在混淆矩阵的视觉分析上看起来更差。让我们对 ResNet50 进行微调，看看这是否会产生更好的结果。</p><p id="eab6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在两个实验中完成这项工作。</p><h1 id="6409" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">ResNet50 实验#1:盲接受学习速率建议的微调</h1><p id="51cb" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">当训练一个深度神经网络时，选择一个好的学习速率对于快速收敛和较低的误差都是至关重要的。微调第一步是找到一个合适的学习速率范围:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="96c1" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn50.unfreeze()<br/># Find optimum learning rates<br/>learn50.lr_find()<br/># Include suggestion=True in order to obtain a suggestion on where to look...<br/>learn50.recorder.plot(suggestion=True)</span></pre><p id="baed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将输出:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="f7d2" class="nr mn it nn b gy ns nt l nu nv">LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.<br/>Min numerical gradient: 9.12E-07<br/>Min loss divided by 10: 1.10E-07</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7e94eb56b6c07179eb44bd53364d07ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*NijZHa2YwoQNjojgubBY6Q.png"/></div></figure><p id="d3b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下限，9.12E-07，我们就上舍入到 1.0E-06。现在，您可以使用我们的经验法则进行微调。我们将使用<em class="le">经验法则</em>定义学习率的<em class="le">范围；以“安全”速率 1e-06 结束；以高一个数量级的速率开始；</em></p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="78bd" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn50.unfreeze()<br/>learn50.fit_one_cycle(35, <strong class="nn iu">max_lr=slice(1e-5,1e-6)</strong>, <br/>                    callbacks=[SaveModelCallback(learn50, every='epoch', monitor='accuracy', name='derma50')])</span><span id="2483" class="nr mn it nn b gy nw nt l nu nv"># Save the weights of stage 2 each "better" epoch:<br/>learn50.save('derma50-stage-2')<br/># <strong class="nn iu">Do not</strong> overwrite the stage 1 .pkl with  stage 2 <br/># We will need it for the ResNet50 Experiment #2<br/># exportStageTo(learn50, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/270e55fa5f23a7ade7b29e8d8f519ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*IW0PBXqTk4Yd8QFP0q45ng.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">90% accuracy…</figcaption></figure><p id="8d97" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以，经过 35 个时期和大量的处理，我们达到了 90%的准确率。这看起来没有希望…让我们看看困惑矩阵和学习曲线:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="b78b" class="nr mn it nn b gy ns nt l nu nv">interp = ClassificationInterpretation.from_learner(learn50)</span><span id="3fd5" class="nr mn it nn b gy nw nt l nu nv">losses,idxs = interp.top_losses()</span><span id="3612" class="nr mn it nn b gy nw nt l nu nv">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a1c70d442140d47d8b4fe8bc0135d26a.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*eEknSn8w_JKJu9b48CarNw.png"/></div></figure><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="6801" class="nr mn it nn b gy ns nt l nu nv">learn50.recorder.plot_losses()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/506d6e2c0fda6bf545758b914dd4b359.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*hr4oWQ6KnNr5QU5rLAtkXg.png"/></div></figure><p id="2144" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在这里看到的是一个与 ResNet34 非常不同的结果。对于 ResNet50，网络在微调阶段学习训练集，但它一直在振荡:一些批次使其性能更好，另一些批次使其返回并性能更差。这通常意味着数据质量差，噪音太多。然而，在这种情况下，我们已经看到，我们可以使用 ResNet34 实现 93%的准确性。所以，坏数据不是这里的情况。另一种可能性是，网络有太多的参数，它不是泛化，而是适应训练集的单个实例，从而学习单个实例并去泛化。对于数据集的其他部分，这使得它的性能更差，因此网络就像一个钟摆，在误差空间中来回摆动。验证损失比整个微调过程中的训练损失高得多，证实了对学习曲线的这种解释。</p><p id="f39d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是我们很固执…让我们做另一个实验。</p><h1 id="d6a6" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">ResNet50 实验#2:使用手动学习率设置进行微调</h1><p id="51fe" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">在以前的实验中，我们盲目地接受了分析算法的建议，并采用了非常低的学习率。也许这就是学习不好的原因？</p><p id="daa6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们看一下学习率图，我们可以看到一个<strong class="kk iu"> <em class="le">平台</em> </strong>已经在大约 1.0E-4 处形成。然后学习率陷入两个低谷，一个在 1.0E-5，另一个在 1.0E-6。如果我们采用更稳定的、平坦的区域，从 1.0E-4 到 1.0E-5，并把它作为我们的学习率范围，会怎么样？</p><p id="55c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，将网络恢复到完成初始迁移学习时的状态:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="0ae3" class="nr mn it nn b gy ns nt l nu nv"># Will always load a <em class="le">path/'export.pkl'</em> deployment file<br/>learn50 = restoreStageFrom(path)</span></pre><p id="edc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次微调，现在用<strong class="kk iu"> max_lr=slice(1e-4，1e-5) </strong>:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="0b9a" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn50.unfreeze()<br/>learn50.fit_one_cycle(35, <strong class="nn iu">max_lr=slice(1e-4,1e-5)</strong>, <br/>                    callbacks=[SaveModelCallback(learn50, every='epoch', monitor='accuracy', name='derma50')])<br/>learn50.save('derma50-stage-2')<br/>exportStageTo(learn50, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/46b61931d42a09b3e9a4446189cd9d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*4ilcwRov4b5SBFoZeyySdQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">92.6% accuracy….</figcaption></figure><p id="6b01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看结果图:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c820f5c4403499133838f78f3767a04e.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*iV9UdxrSRL8EFrAdSOqsbw.png"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/09b84b530c9dbfce7efddbb3a65515bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*ZfmVQYiHyhm3SJYdXdBgmg.png"/></div></figure><p id="da26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些结果比以前好，即使网络振荡很大。然而，混淆矩阵向我们显示，黑色素瘤<strong class="kk iu"><em class="le"/></strong>(这里最重要的病理)的结果并不乐观。</p><p id="86c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们反其道而行之，尝试一个比 ResNet34 小的网络，而不是比 ResNet18 大的。</p><h1 id="0950" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">ResNet18</h1><p id="9b9d" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">那么，为什么不尝试一个小得多的网络，看看它的表现呢？让我们用皮肤镜数据来试一试。</p><p id="1a87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们将更改批量大小并重新生成数据分组。ResNet18s 要小得多，我们将有更多的可用内存，因此使用更大的批处理大小是有意义的:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="4ec0" class="nr mn it nn b gy ns nt l nu nv">bs = 48         # Batch size, 64 for medium images on a T4 GPU and ResNet18...<br/>size = 448      # Image size, 448x448 is double than the orignal <br/>                # ImageNet size of the pre-trained ResNet we'll be using, <br/>                # should be easy to train...<br/>path = Path("./data")   # The path to the 'train' folder you created...</span><span id="0d7d" class="nr mn it nn b gy nw nt l nu nv"># Limit your augmentations: it's medical data! You do not want to phantasize data...<br/># Warping, for example, will let your images badly distorted, so don't do it!<br/># This dataset is big, so don't rotate the images either. Lets stick to flipping...<br/>tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_rotate=None, max_warp=None, max_zoom=1.0)<br/># Create the DataBunch!<br/># Remember that you'll have images that are bigger than 128x128 and images that are smaller,   <br/># so squish them all in order to occupy exactly 128x128 pixels...<br/>data = ImageDataBunch.from_csv('data', csv_labels='HAM10000_metadata.csv', suffix='.jpg', fn_col=1, label_col=2, <br/>                               ds_tfms=tfms, valid_pct = 0.2,size=size, bs=bs)<br/>print('Transforms = ', len(tfms))<br/># Save the DataBunch in case the training goes south... so you won't have to regenerate it..<br/># Remember: this DataBunch is tied to the batch size you selected. <br/>data.save('imageDataBunch-bs-'+str(bs)+'-size-'+str(size)+'.pkl')<br/># Show the statistics of the Bunch...<br/>print(data.classes)<br/>data</span></pre><p id="b3f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创建网络:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="0412" class="nr mn it nn b gy ns nt l nu nv">learn = cnn_learner(data, models.resnet18, metrics=[accuracy, error_rate])<br/>learn.model</span></pre><p id="9753" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型将如下所示(注意，这个较小的 ResNet 模型只有 1024 个 out 特性，而不是最后一个块中的 4096 个):</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="64a7" class="nr mn it nn b gy ns nt l nu nv">Sequential(<br/>  (0): Sequential(<br/>    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU(inplace)<br/>    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>    (4): Sequential(<br/>      (0): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>...<br/>...<br/>...<br/>  (1): Sequential(<br/>    (0): AdaptiveConcatPool2d(<br/>      (ap): AdaptiveAvgPool2d(output_size=1)<br/>      (mp): AdaptiveMaxPool2d(output_size=1)<br/>    )<br/>    (1): Flatten()<br/>    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (3): Dropout(p=0.25)<br/>    (4): Linear(in_features=1024, out_features=512, bias=True)<br/>    (5): ReLU(inplace)<br/>    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (7): Dropout(p=0.5)<br/>    (8): Linear(in_features=512, out_features=7, bias=True)<br/>  )<br/>)</span></pre><p id="5109" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们转移-学习 15 个时代:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="3a2f" class="nr mn it nn b gy ns nt l nu nv">learn.fit_one_cycle(15, callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy', name='derma-1')])<br/>learn.save('derma-stage-1')<br/>exportStageTo(learn, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/e2845a99fef77f73157199243b6c8e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*fXYRiUfLnkgujsLp3Ra0bA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">85.5% accuracy…</figcaption></figure><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="99fa" class="nr mn it nn b gy ns nt l nu nv">interp = ClassificationInterpretation.from_learner(learn)</span><span id="f15b" class="nr mn it nn b gy nw nt l nu nv">losses,idxs = interp.top_losses()</span><span id="311d" class="nr mn it nn b gy nw nt l nu nv">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/63df78bc313d16811d57669f8da837e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*qfFouDoEdN_a_VETT3xlMw.png"/></div></figure><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="7069" class="nr mn it nn b gy ns nt l nu nv">learn.recorder.plot_losses()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/388f0ffd5b2a9a53e33ce30218db6703.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*p97p8D3z-d-lKox9FY6xcA.png"/></div></figure><p id="b42b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从数值上看，这比 ResNet34 好 0.5%。让我们看看它在微调后的表现:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="0303" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn.unfreeze()<br/># Find optimum learning rates<br/>learn.lr_find()<br/># Include suggestion=True in order to obtain a suggestion on where to look...<br/>learn.recorder.plot(suggestion=True)</span></pre><p id="537e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们有一条曲线，首先达到一个平台，然后陷入两个洞。只是这里的洞比高地更深。让我们考虑，在这种情况下，空洞是显著的，并接受学习率查找器的建议，使<strong class="kk iu"> max_lr=slice(1e-5，1e-6) </strong>:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="4f92" class="nr mn it nn b gy ns nt l nu nv">LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.<br/>Min numerical gradient: 1.32E-06<br/>Min loss divided by 10: 1.58E-07</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/863b103759225a33b9258a169f51ec7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*wj3lBUluhT7H9M_AW-06Bg.png"/></div></figure><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="d625" class="nr mn it nn b gy ns nt l nu nv"># Unfreeze the network<br/>learn.unfreeze()<br/>learn.fit_one_cycle(35, <strong class="nn iu">max_lr=slice(1e-5,1e-6)</strong>, <br/>                    callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy', name='derma18')])</span><span id="864a" class="nr mn it nn b gy nw nt l nu nv">learn.save('derma18-stage-2')</span><span id="f33e" class="nr mn it nn b gy nw nt l nu nv">exportStageTo(learn, path)</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oh"><img src="../Images/6eae1c1925923a56b8c1aaf35f3186e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*7c3ussw5DEAiXwhC24uMsg.png"/></div></div></figure><p id="1bed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">88%的准确率！那比 ResNet34 差多了。让我们绘制学习曲线:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="41d2" class="nr mn it nn b gy ns nt l nu nv">learn.recorder.plot_losses()</span></pre><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/92c1e2677113a0a84ceda2500fcabf04.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*GDpKN2zjysE2HForcQqNXQ.png"/></div></figure><p id="980b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在这里看到的是，当我们以太小的学习率训练它时，ResNet18 比 ResNet50 振荡得更多。这表明这里的学习率也太小了。</p><p id="05bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我重复了我们之前对 ResNet50 进行的微调实验#2，并将学习率范围设置为<strong class="kk iu"> max_lr=slice(1e-4，1e-5) </strong>，并再次训练它。在这种情况下，ResNet18 实现了 0.904643 的验证精度。结果图形如下所示:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/77582c5b64e0a941ec43d66ad9f908fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*2DTJNrdVja3kfp1VdQSJvQ.png"/></div></figure><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7e3cc91c3a77658b08d33851d7f3df99.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*ioucH3ak6tfbXWvKVG_oYw.png"/></div></figure><p id="4370" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这比以前好，但仍比 ResNet34 和 ResNet50 差。ResNet18 似乎不是解决这个问题的好选择。<a class="ae lv" href="https://drive.google.com/open?id=1-6MlEi2sUW7AYRrBAjar_TsuoxsmUISo" rel="noopener ugc nofollow" target="_blank">代码在我们的笔记本里</a>。</p><h1 id="ebec" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">如果培训中途中断，我该怎么办？</h1><p id="6c36" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">如果你的训练被打断了，你会怎么做？这可能是因为你在 Google Colab 笔记本上达到了连续 12 小时的“免费”操作时间，或者因为你的计算机由于某种原因停止了。我住在巴西，电力短缺是常事…</p><p id="55f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le"> fit_one_cycle </em>方法适用于变化的自适应学习速率，遵循速率先增大后减小的曲线。如果你中断第 10 个纪元的训练，比如说 20 个纪元，然后重新开始 9 个以上的纪元，<strong class="kk iu">你将不会得到与不间断训练 20 个纪元</strong>相同的结果。您必须能够记录您停止的位置，然后从该点重新开始训练周期，并使用该周期部分的正确超参数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ok"><img src="../Images/8d5fa3658ed88e31179157e295612d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SfQdflj95ziJHcMiUfCNbA.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">A fit_one_cycle training session divided into three subsessions. Image by PPW@GitHub</figcaption></figure><p id="47e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你要做的第一件事就是保存你的网络:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="dcb2" class="nr mn it nn b gy ns nt l nu nv">learn.fit_one_cycle(20, max_lr=slice(1e-5,1e-6), <br/>       callbacks=[SaveModelCallback(learn, every='epoch',  <br/>                  monitor='accuracy', name='saved_net')])</span></pre><p id="953c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将使您的网络在每个纪元都被保存，您提供的名称后面跟着<em class="le">_ #纪元</em>。所以在纪元#3，文件<em class="le"> saved_net_3.pth </em>将被写入。您可以在完成以下操作后加载此文件:</p><ul class=""><li id="4a36" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">重新创建了<em class="le">数据束</em>和</li><li id="cb98" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">用它重新实例化了网络。</li></ul><p id="f042" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重装完<em class="le">后。pth </em>文件，你可以重新开始你的训练，只是你要告诉<em class="le"> fit_one_cycle </em>考虑 20 个历元，但是要从历元#4 开始训练。</p><p id="e5e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解这是如何做到的，请看这里:</p><ul class=""><li id="70d6" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" href="https://github.com/PPPW/deep-learning-random-explore/tree/master/divide_1cycle?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">把一个长周期的政策分成几个小周期——PPW 的 GitHub</a></li></ul><p id="a704" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">你是怎么做到的？</strong></p><p id="42cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">fast.ai 中的<em class="le"> fit_one_cycle </em>方法已经开发出来，允许您告诉它从周期的哪个部分恢复中断的训练。恢复培训的代码如下所示:</p><pre class="lg lh li lj gt nm nn no np aw nq bi"><span id="9f1f" class="nr mn it nn b gy ns nt l nu nv"># Create a new net if training was interrupted and you had to <br/># restart your Colab session</span><span id="050b" class="nr mn it nn b gy nw nt l nu nv">learn = cnn_learner(data, models.&lt;your_model_here&gt;, <br/>                    metrics=[accuracy, error_rate])</span><span id="3b39" class="nr mn it nn b gy nw nt l nu nv"># If you're resuming, only indicating the epoch from which to <br/># resume, indicated by <strong class="nn iu"><em class="le">start_epoch=&lt;epoch#&gt;</em></strong> will load the last <br/># saved .pth, it is not necessary to explicitly reload the last <br/># epoch, you only should <strong class="nn iu">NOT</strong> change the name given in <br/># name=&lt;callback_save_file&gt;:<br/># when resuming fast.ai will try to reload <br/># <strong class="nn iu"><em class="le">&lt;callback_save_file&gt;_&lt;previous_epoch&gt;.pth</em></strong><br/># Unfreeze the network<br/>learn50.unfreeze()</span><span id="1b4d" class="nr mn it nn b gy nw nt l nu nv"># Use start_epoch=&lt;some_epoch&gt; to resume training...<br/>learn.fit_one_cycle(20, max_lr=slice(1e-5,1e-6), <br/>                    <strong class="nn iu"><em class="le">start_epoch=&lt;next_epoch#&gt;</em></strong>,<br/>                    callbacks=[SaveModelCallback(learn, <br/>                    every='epoch', monitor='accuracy', <br/>                    <strong class="nn iu"><em class="le">name=&lt;callback_save_file&gt;</em></strong>)])</span></pre><p id="1ba5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">fast.ai 会告诉你“<em class="le">载入&lt;回调 _ 保存 _ 文件&gt; _ &lt;上一个 _ 纪元# &gt; </em>”，恢复训练。</p><p id="9ef1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以在此查看<em class="le"> fit_one_cycle </em>方法支持的所有参数:</p><ul class=""><li id="aa1f" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" href="https://docs.fast.ai/train.html#fit_one_cycle" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/train.html#fit_one_cycle</a></li></ul><h1 id="f47f" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">我们取得了什么成就？</h1><p id="94ec" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">使用 448x448 的图像分辨率和 fast.ai，我们在实验中使用的三个网络模型中的两个(ResNet34 和 ResNet50)获得了大约 93%的验证准确性。这比上面教程的 85%要好得多，上面的教程使用了 MobileNet，图像分辨率为 224x224，Keras。<a class="ae lv" href="https://www.kaggle.com/vbookshelf/skin-lesion-analyzer-tensorflow-js-web-app" rel="noopener ugc nofollow" target="_blank">(目前)ka ggle(employees tensor flow . js)上投票最多的内核获得了 86%的精度</a>。</p><p id="f246" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在任务#3 的 ISIC 2018 挑战赛最终测试中，<strong class="kk iu">排名最高的竞争对手</strong>，来自【MetaOptima Technology Inc .【的 Jordan Yap 取得了 95.8% 的准确率和由 ISIC 选择的竞争对手评估标准的<em class="le">平衡多类准确率 88.5%。Jordan Yap 采用的方法基于:</em></p><ul class=""><li id="c771" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">附加的外部数据[33，644 张图像]；</li><li id="cab3" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">相当于原始 ImageNet 分辨率的低分辨率图像；</li><li id="f1be" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">19 种分类算法的集合，其中一种不是神经网络(直方图分析)；</li><li id="3f22" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">一个 XGBoost 分类器，它在这个集成的结果之上被训练。</li></ul><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/77a810c0bf835e4bf978a41846cd80b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*58L695oly65GB0-h486BrQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Aleksey Nozdryn-Plotnicki, Jordan Yap, and William Yolland, MICCAI, 2018</figcaption></figure><p id="8812" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lv" href="https://s3.amazonaws.com/covalic-prod-assetstore/c9/21/c921175d59914fa79e8098c4ead60cfa?response-content-disposition=inline%3B%20filename%3D%22ISBI_2018.pdf%22&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Expires=3600&amp;X-Amz-Credential=AKIAITHBL3CJMECU3C4A%2F20190705%2Fus-east-1%2Fs3%2Faws4_request&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Date=20190705T183010Z&amp;X-Amz-Signature=7cbc21be5a9e515a83b44b48608b0c76436eff104c2d60a49e379ed4636dcf6e" rel="noopener ugc nofollow" target="_blank">Aleksey Nozdryn-plot nicki、Jordan Yap 和 William Yolland 提交给<em class="le"> ISIC 皮肤图像分析研讨会和 Challenge @ MICCAI 2018 </em>的论文及其结果在此。</a></p><p id="01ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的结果与最初的 ISIC 2018 挑战赛没有直接可比性，因为 ISIC 提供了从 HAM1000 数据集手动提取的 1512 幅图像的测试集，所有竞争对手都必须采用该测试集并将其结果提交给 ISIC。我们使用从 HAM1000 数据集中随机提取的一组 2003 张图像来验证我们的训练，并使用剩余的 8012 张图像来训练我们的网络。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi om"><img src="../Images/f34ecc647453750797eb9e0adbd98401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Oj5oBiU0hTbaykB4CQKGg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Statistics of Jordan Yap from MetaOptima Technology Inc.</figcaption></figure><p id="c849" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可以在此查看 ISIC 2018 结果:</p><ul class=""><li id="e96a" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" href="https://challenge.kitware.com/#phase/5b1c1aa756357d41064da300" rel="noopener ugc nofollow" target="_blank"> ISIC 2018 —任务 3:最终测试</a></li><li id="7cfe" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated"><a class="ae lv" href="https://challenge2018.isic-archive.com/leaderboards/" rel="noopener ugc nofollow" target="_blank"> ISIC Challenge 2018 排行榜</a>(点击“任务 3”窗格)</li></ul><p id="0726" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，有趣的是，我们在 NVIDIA T4 GPU (4.2 x 10 + 4.3 x 35 分钟)上使用 fast.ai 和一个网络 ResNet34 以及总训练时间为 190 分钟的<strong class="kk iu">获得的<strong class="kk iu"> 92.9% </strong>的准确性，仅比 ISIC 2018 挑战赛最高排名的竞争对手使用更复杂的方法获得的准确性差<strong class="kk iu"> 2.9% </strong></strong></p><p id="7853" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这很可能是因为我们采用了两倍的图像分辨率，允许更多的细节，但 fast.ai 中的 HYPOs(超参数优化)可能也发挥了作用。</p><h1 id="13f1" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">我们学到了什么？</h1><p id="2f94" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated"><strong class="kk iu"> ResNet34 入手</strong>是个不错的选择:ResNet 在 fast.ai 中确实管理的很好，有各种好用的 HYPOs(超参数优化)。如果你不知道使用哪种网络，可以使用 ResNet34，它足够小，即使你在家里使用 GPU 也可以相对快速地进行训练，但也足够大，可以表示一大组问题。ResNet34 trains 将如何提供提示，如果你应该与你的网络规模上升或下降。</p><p id="8140" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">盲目接受学习率建议并不总是最佳选择</strong>:resnet 50 的<em class="le"> lr_find() </em>产生了一个很长的平稳期，该方法在图形左端的一个小山谷中建议了一个非常低的学习率值。当我们用这个值训练网络时，它会振荡，没有产生好的结果(只有 90%的准确率)。当我们对图表进行可视化分析，并将一个高一个数量级的值作为学习率范围的下限时，该值位于平台较平坦部分的开始，ResNet50 学习得更好，并达到与 ResNet34 相同的 93%的准确率。所以，使用<em class="le">建议=真</em>模式，但是在你接受它之前给它一个严肃的处理，<em class="le">实际上看着</em>图形。这是学习率范围的第二条经验法则:查看整个图形并找到真正的平稳段——学习率范围的理想下限将位于该平稳段的开始处。</p><p id="f22b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">方法<a class="ae lv" href="https://docs.fast.ai/callbacks.lr_finder.html" rel="noopener ugc nofollow" target="_blank"><em class="le">learn . lr _ find()</em></a>帮助你找到一个最优的学习率。它使用了 2015 年论文<em class="le">中开发的技术，用于训练神经网络</em>(<a class="ae lv" href="http://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1506.01186</a>)的循环学习率，我们只是从一个非常小的值开始增加学习率，直到损失开始减少。如果你想知道更多关于寻找最佳学习率的信息，请看这里:</p><ul class=""><li id="7452" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/speeding-up-neural-net-training-with-lr-finder-c3b401a116d0"> <strong class="kk iu">迈向数据科学</strong>:<em class="le">用 LR-Finder 加速神经网络训练——为你的网络找到良好的初始学习速率</em>作者 Faizan Ahemad </a></li></ul><p id="a75b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">越大并不总是越好</strong>:最终，ResNet50 的表现几乎与 ResNet34 相同，但训练时间要长得多，结果也稍差。用大型网络模型开始您的培训空间探索是一个糟糕的选择。</p><p id="eaac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">图像分辨率发挥作用</strong>:我们采用了 ISIC 2018 挑战赛最高排名竞争对手的两倍分辨率，通过一个相对简单的 ResNet34 获得了可比的结果，而该竞争对手在 18 个不同的网络上采用了机器学习方法，包括一个巨大的 ResNet152。</p><p id="eac3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Fast.ai 很快:</strong>最后，与其他方法相比，使用<em class="le"> fast.ai </em>我们能够使用更少的代码解决相同的分类问题，同时使用高级超参数优化策略，使我们的训练速度更快。同时，一组高级功能也允许我们以表格和图形的形式轻松检查结果。这种简单性使我们可以用三种不同的网络模型进行实验，并比较它们的结果。这表明 fast.ai 是更传统的 CNN 框架的一个非常有前途的替代方案，特别是如果手头的任务是图像分类、对象检测或语义分割等“标准”深度学习任务，可以通过微调现成的预训练网络模型来解决。</p><h1 id="e568" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">承认</h1><p id="4e3a" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">这项工作是除我之外的一组积极参与的研究人员共同努力的结果:</p><ul class=""><li id="ea25" class="lw lx it kk b kl km ko kp kr ly kv lz kz ma ld mk mc md me bi translated">丹尼尔·霍尔特豪森努内斯&lt;<a class="ae lv" href="mailto:daniel@floripa.com.br" rel="noopener ugc nofollow" target="_blank">daniel@floripa.com.br</a>T26】</li><li id="3817" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">路易斯·安东尼奥·布舍托·马卡里尼&lt;<a class="ae lv" href="mailto:luiz.buschetto@posgrad.ufsc.br" rel="noopener ugc nofollow" target="_blank"/></li><li id="c7b5" class="lw lx it kk b kl mf ko mg kr mh kv mi kz mj ld mk mc md me bi translated">马塞洛里卡多斯特梅尔&lt;<a class="ae lv" href="mailto:marcelo.stemmer@ufsc.br" rel="noopener ugc nofollow" target="_blank">马塞洛. stemmer @ ufsc . br</a>T28】</li></ul><p id="5976" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还要感谢于尔根·克鲁施&lt;<a class="ae lv" href="mailto:juergen.kreusch@gmail.com" rel="noopener ugc nofollow" target="_blank">、juergen.kreusch@gmail.com</a>&gt;和路易斯·费尔南多·科普克&lt; <a class="ae lv" href="mailto:luiskopke@uol.com.br" rel="noopener ugc nofollow" target="_blank">、luiskopke@uol.com.br</a>&gt;提供的立体皮肤镜材料。</p><h1 id="f09a" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">参考</h1><p id="8e4d" class="pw-post-body-paragraph ki kj it kk b kl ne ju kn ko nf jx kq kr ng kt ku kv nh kx ky kz ni lb lc ld im bi translated">[1]Kreusch，J. <a class="ae lv" href="https://www.ncbi.nlm.nih.gov/pubmed/1459756#" rel="noopener ugc nofollow" target="_blank"> <em class="le">入射光显微术:对活体皮肤显微术的思考</em> </a>。国际皮肤病学杂志。 1992 年 9 月；31(9):618–20.</p><p id="9f6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]Kopke，L.F. <a class="ae lv" href="http://www.surgicalcosmetic.org.br/exportar-pdf/3/3_n2_126_en/A-dermatoscopia-na-deteccao-precoce--controle-e-planejamento-cirurgico-dos-carcinomas-basocelulares" rel="noopener ugc nofollow" target="_blank"> <em class="le">皮肤镜在基底细胞癌早期发现、控制和手术规划中的作用</em> </a>。2011 年皮肤美容外科手术；3(2):103–8.</p></div></div>    
</body>
</html>