<html>
<head>
<title>Deep Learning Analysis Using Large Model Support</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用大模型支持的深度学习分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-analysis-using-large-model-support-3a67a919255?source=collection_archive---------19-----------------------#2019-07-30">https://towardsdatascience.com/deep-learning-analysis-using-large-model-support-3a67a919255?source=collection_archive---------19-----------------------#2019-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c14b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">借助 IBM 大型模型支持，优化您的深度学习模型内存消耗。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d9b744c2575654e59a6145ebb82e0aee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uh1cFT3sfz3PJe8Mj4qhAw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">(Source: <a class="ae ky" href="https://miro.medium.com/max/3512/1*d-ZbdImPx4zRW0zK4QL49w.jpeg" rel="noopener">https://miro.medium.com/max/3512/1*d-ZbdImPx4zRW0zK4QL49w.jpeg</a>)</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="kz la l"/></div></figure><h1 id="390e" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">介绍</h1><p id="e818" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">内存管理现在是机器学习中一个非常重要的话题。由于内存限制，使用 Kaggle 和谷歌 Colab 等云工具训练深度学习模型变得非常普遍，这要归功于它们免费的 NVIDIA 图形处理单元(GPU)支持。尽管如此，在云中处理大量数据时，内存仍然是一个巨大的限制。</p><p id="9a2c" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在我的<a class="ae ky" rel="noopener" target="_blank" href="/gpu-accelerated-data-analytics-machine-learning-963aebe956ce">上一篇文章</a>中，我解释了如何加速机器学习工作流的执行。相反，本文旨在向您解释如何在实现深度学习模型时有效地减少内存使用。通过这种方式，你可能能够使用相同数量的内存来训练你的深度学习模型(即使之前因为内存错误而无法训练)。</p><p id="67e7" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">模型导致内存不足的主要原因有三个:</p><ul class=""><li id="769d" class="mu mv it lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated"><strong class="lv iu">模型深度/复杂度</strong> =神经网络的层数和节点数。</li><li id="32dc" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><strong class="lv iu">数据大小</strong> =使用的数据集中样本/特征的数量。</li><li id="9f75" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><strong class="lv iu">批量大小</strong> =通过神经网络传播的样本数量。</li></ul><p id="4201" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">这个问题的一个解决方案传统上是通过在预处理阶段试图去除不太相关的特征来减小模型大小。这可以使用<a class="ae ky" rel="noopener" target="_blank" href="/svm-feature-selection-and-kernels-840781cc1a6c">特征重要性</a>或特征提取技术(如 PCA、LDA)来完成。</p><p id="0496" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">使用这种方法可能会降低噪声(减少过度拟合的机会)并缩短训练时间。不过这种方法的一个缺点是准确性会持续下降。</p><p id="23f3" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">如果模型需要很高的复杂度来捕捉数据集的所有重要特征，那么减少数据集的大小实际上不可避免地会导致更差的性能。在这种情况下，大模型支持可以解决这个问题。</p><h1 id="2140" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">大型模型支持</h1><p id="9bc3" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">大型模型支持(LMS)是 IBM 最近推出的一个 Python 库。这个库的构想是为了训练无法容纳在 GPU 内存中的大型深度学习模型。事实上，与中央处理器(CPU)相比，GPU 通常具有更小的内存空间。</p><p id="b181" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">当使用 Tensorflow 和 PyTorch 等库实现神经网络时，会自动生成一组数学运算来构建该模型。这些数学运算可以用计算图来表示。</p><blockquote class="ni nj nk"><p id="62f9" class="lt lu nl lv b lw mp ju ly lz mq jx mb nm mr me mf nn ms mi mj no mt mm mn mo im bi translated">计算图是一个有向图，其中节点对应于<strong class="lv iu">操作</strong>或<strong class="lv iu">变量</strong>。变量可以将它们的值提供给操作，操作可以将它们的输出提供给其他操作。这样，图中的每个节点都定义了变量的函数。</p><p id="a3be" class="lt lu nl lv b lw mp ju ly lz mq jx mb nm mr me mf nn ms mi mj no mt mm mn mo im bi translated">—深刻的想法[1]</p></blockquote><p id="8096" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">计算图中进出节点的值称为张量(多维数组)。</p><p id="5fdf" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">图 1 给出了一个简单的例子，说明如何使用计算图形(z =(x+y)∫(X5))来表示数学运算:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b07eb5cebb354d356c3954faff566708.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*7-gDjZP8484QP52vvKxtPA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1: Computational Graph [2]</figcaption></figure><p id="95f0" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">LMS 能够通过重新设计神经网络计算图来缓解 GPU 内存问题。这是通过在 CPU(而不是 GPU)上存储中间结果来实现张量运算的。</p><p id="2b4d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">IBM 文档概述了使用 Tensorflow 库支持大型模型的三种不同方法:</p><ul class=""><li id="bba0" class="mu mv it lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated"><strong class="lv iu">基于会话的培训。</strong></li><li id="4aa5" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><strong class="lv iu">基于估计器的训练。</strong></li><li id="a10d" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><strong class="lv iu">基于 Keras 的培训。</strong></li></ul><p id="d24d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在本文中，我将提供一个使用基于 Keras 的培训的例子。如果您有兴趣了解其他两种方法的更多信息，IBM 文档是一个很好的起点[3]。</p><p id="dc35" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">使用 LMS 时，我们可以调整两个主要参数来提高模型效率。目标是能够找出我们需要交换的最少数量的张量，而不会导致内存错误。</p><p id="53d6" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">要调整的两个主要参数是:</p><ul class=""><li id="6f10" class="mu mv it lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated"><strong class="lv iu"> n_tensors </strong> =交换张量的数量(例如，刷出比所需更多的张量，会导致通信开销)。</li><li id="4f48" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><strong class="lv iu"> lb </strong> =张量在使用前多久换回(例如，使用较低的 lb 值会使 GPU 训练暂停)。</li></ul><h1 id="94dd" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">示范</h1><p id="7531" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在，我将通过一个简单的例子向您介绍 LMS。这个练习使用的所有代码都可以在这个<a class="ae ky" href="https://drive.google.com/open?id=18PFe_8mVtcZj9PhdZwUQXPZkzbKwMFEG" rel="noopener ugc nofollow" target="_blank"> Google 协作笔记本</a>和我的<a class="ae ky" href="https://github.com/pierpaolo28/Artificial-Intelligence-Projects/blob/master/IBM%20Large%20Model%20Support/LargeModelSupport.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="0e78" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在这个例子中，我将训练一个简单的神经网络，首先使用具有大模型支持的 Keras，然后只使用普通的 Keras。如果是这两种情况，我将记录培训所需的内存使用情况。</p><h2 id="0064" class="nq lc it bd ld nr ns dn lh nt nu dp ll mc nv nw ln mg nx ny lp mk nz oa lr ob bi translated">预处理</h2><p id="d433" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">为了按照这个例子安装所有需要的依赖项，只需在您的笔记本上运行以下单元，并启用您的 GPU 环境(例如 Kaggle、Google Colab)。</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="f84d" class="nq lc it od b gy oh oi l oj ok">! git clone <a class="ae ky" href="https://github.com/IBM/tensorflow-large-model-support.git" rel="noopener ugc nofollow" target="_blank">https://github.com/IBM/tensorflow-large-model-support.git</a><br/>! pip install ./tensorflow-large-model-support<br/>! pip install memory_profiler</span></pre><p id="baff" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">一旦一切就绪，我们就可以导入所有必要的库了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><p id="1620" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">为了记录内存使用情况，我决定使用 Python<a class="ae ky" href="https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html" rel="noopener ugc nofollow" target="_blank"><em class="nl">memory _ profiler</em></a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><p id="256d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">随后，我定义了将在培训中使用的 LMS Keras 回调。根据 Keras 文档，回调的定义是:</p><blockquote class="ni nj nk"><p id="ce75" class="lt lu nl lv b lw mp ju ly lz mq jx mb nm mr me mf nn ms mi mj no mt mm mn mo im bi translated">回调是在训练过程的给定阶段应用的一组函数。在训练期间，您可以使用回调来查看模型的内部状态和统计数据。</p><p id="8cbf" class="lt lu nl lv b lw mp ju ly lz mq jx mb nm mr me mf nn ms mi mj no mt mm mn mo im bi translated">— Keras 文档[4]</p></blockquote><p id="fe74" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">回调通常用于通过在每个训练迭代期间自动化某些任务来控制模型训练过程(在这种情况下，通过添加大型模型支持优化)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><p id="bbc8" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">然后，我决定使用由三个特征和两个标签(0/1)组成的高斯分布来构建一个 200000 行的简单数据集。</p><p id="4f66" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">已经选择了分布的平均值和标准偏差值，以便使这个分类问题相当容易(线性可分数据)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/69ad63382f5f30ddbf2b4faa70a2ccb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*5haLttFvQz5AcKu8Jxxcqg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2: Dataset Head</figcaption></figure><p id="9901" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">创建数据集后，我将其分为要素和标注，然后定义一个函数对其进行预处理。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><p id="91d1" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">现在我们有了训练/测试集，我们终于准备好开始深度学习了。因此，我为二元分类定义了一个简单的序列模型，并选择了 8 个元素的批量大小。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><h2 id="7413" class="nq lc it bd ld nr ns dn lh nt nu dp ll mc nv nw ln mg nx ny lp mk nz oa lr ob bi translated">Keras 和大型模型支持</h2><p id="b50b" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">使用 LMS 时，使用 Keras <em class="nl"> fit_generator </em>函数训练 Keras 模型。这个函数需要的第一个输入是一个生成器。生成器是一种功能，用于在多个内核上实时生成数据集，然后将其结果输入深度学习模型[5]。</p><p id="48e0" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">为了创建本例中使用的生成器函数，我参考了<a class="ae ky" href="https://stackoverflow.com/questions/46493419/use-a-generator-for-keras-model-fit-generator" rel="noopener ugc nofollow" target="_blank">这个实现</a>。</p><p id="6be1" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">如果你对 Keras 发电机的更详细的解释感兴趣，可以在<a class="ae ky" href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><p id="0e9a" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">一旦定义了我们的生成器函数，我就使用之前定义的 LMS Keras 回调来训练我们的模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><p id="091d" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">在上面的代码中，我在第一行额外添加了<em class="nl"> %%memit </em>命令来打印出运行这个单元的内存使用情况。结果如下所示:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="c8a0" class="nq lc it od b gy oh oi l oj ok">Epoch 1/2 200000/200000 [==============================] <br/>- 601s 3ms/step - loss: 0.0222 - acc: 0.9984 </span><span id="bd06" class="nq lc it od b gy on oi l oj ok">Epoch 2/2 200000/200000 [==============================] <br/>- 596s 3ms/step - loss: 0.0203 - acc: 0.9984 </span><span id="2520" class="nq lc it od b gy on oi l oj ok">peak memory: 2834.80 MiB, increment: 2.88 MiB</span></pre><p id="8b59" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">使用 LMS 训练此模型的注册峰值内存等于 2.83GB，增量为 2.8MB</p><p id="cd1b" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">最后，我决定测试我们的训练模型的准确性，以验证我们的训练结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="65ad" class="nq lc it od b gy oh oi l oj ok">Model accuracy using Large Model Support: 99.9995 %</span></pre><h2 id="eac3" class="nq lc it bd ld nr ns dn lh nt nu dp ll mc nv nw ln mg nx ny lp mk nz oa lr ob bi translated">克拉斯</h2><p id="b8df" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">使用普通 Keras 重复相同的程序，获得以下结果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="295d" class="nq lc it od b gy oh oi l oj ok">Epoch 1/2 1600000/1600000 [==============================] <br/>- 537s 336us/step - loss: 0.0449 - acc: 0.9846 </span><span id="7dec" class="nq lc it od b gy on oi l oj ok">Epoch 2/2 1600000/1600000 [==============================] <br/>- 538s 336us/step - loss: 0.0403 - acc: 0.9857 </span><span id="58af" class="nq lc it od b gy on oi l oj ok">peak memory: 2862.26 MiB, increment: 26.15 MiB</span></pre><p id="6eb5" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">使用 Keras 训练该模型的注册峰值内存等于 2.86GB，增量为 26.15MB</p><p id="af18" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">测试我们的 Keras 模型反而导致 98.47%的准确性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol la l"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="1280" class="nq lc it od b gy oh oi l oj ok">Model accuracy using Sklearn: 98.4795 %</span></pre><h1 id="71fd" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">估价</h1><p id="7b54" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">比较使用<em class="nl"> Keras + LMS 与普通 Keras </em>获得的结果，可以注意到，使用 LMS 可以减少内存消耗，并提高模型精度。如果给予更多的 GPU/CPU 资源(可用于优化训练)并使用更大的数据集，LMS 的性能甚至可以得到改善。</p><p id="0c68" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">除此之外，IBM 和 NVIDIA 还决定创建一个由<strong class="lv iu"> 27000 个 NVIDIA TESLA GPU</strong>组成的深度学习计算机集群，以进一步发展这方面的研究人员。如果你有兴趣了解更多，你可以在这里找到更多信息<a class="ae ky" href="https://www.ibm.com/blogs/systems/ibm-and-nvidia-further-collaboration-to-advance-open-source-gpu-acceleration/" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="c80a" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">联系人</h1><p id="4baa" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">如果你想了解我最新的文章和项目<a class="ae ky" href="https://medium.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener">，请通过媒体</a>关注我，并订阅我的<a class="ae ky" href="http://eepurl.com/gwO-Dr?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">邮件列表</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="d8a0" class="mu mv it lv b lw mp lz mq mc mw mg mx mk my mo mz na nb nc bi translated"><a class="ae ky" href="https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="a2f2" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/blog/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人博客</a></li><li id="a003" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人网站</a></li><li id="124f" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><a class="ae ky" href="https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener" target="_blank">中等轮廓</a></li><li id="a9cc" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><a class="ae ky" href="https://github.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="056b" class="mu mv it lv b lw nd lz ne mc nf mg ng mk nh mo mz na nb nc bi translated"><a class="ae ky" href="https://www.kaggle.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><h1 id="b32b" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">文献学</h1><p id="325b" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">[1]从零开始的深度学习 I:计算图——深度思想。访问网址:<a class="ae ky" href="http://www.deepideas.net/deep-learning-from-scratch-i-computational-graphs/" rel="noopener ugc nofollow" target="_blank">http://www . deep ideas . net/deep-learning-from scratch-I-computational-graphs/</a></p><p id="1cc9" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">[2] TFLMS:通过图重写在张量流中支持大模型。董丁乐，今井春树等访问:<a class="ae ky" href="https://arxiv.org/pdf/1807.02037.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1807.02037.pdf</a></p><p id="7927" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">TensorFlow 大型模型支持(TFLMS)入门— IBM 知识中心。访问地址:<a class="ae ky" href="https://www.ibm.com/support/knowledgecenter/en/SS5SF7_1.5.4/navigation/pai_tflms.html" rel="noopener ugc nofollow" target="_blank">https://www . IBM . com/support/knowledge center/en/ss5sf 7 _ 1 . 5 . 4/navigation/pai _ TF LMS . html</a></p><p id="3843" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">[4] Kears 文件。文档-&gt;回调。访问地点:【https://keras.io/callbacks/?source=post_page T2】-</p><p id="215a" class="pw-post-body-paragraph lt lu it lv b lw mp ju ly lz mq jx mb mc mr me mf mg ms mi mj mk mt mm mn mo im bi translated">[5]如何将数据生成器用于 Keras 的详细示例。<br/>谢尔文·阿米迪。访问地址:<a class="ae ky" href="https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly" rel="noopener ugc nofollow" target="_blank">https://Stanford . edu/~ sher vine/blog/keras-how-to-generate-data-on-the-fly</a></p></div></div>    
</body>
</html>