<html>
<head>
<title>Understanding Backpropagation Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解反向传播算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd?source=collection_archive---------0-----------------------#2019-08-08">https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd?source=collection_archive---------0-----------------------#2019-08-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9457" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解神经网络最重要组成部分的具体细节</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/aa3850f9941e42e23c63e1388d85a71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aX5vbutqsmLpuMvY4ltDow.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">“A man is running on a highway” — photo by <a class="ae kv" href="https://unsplash.com/@whatyouhide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Andrea Leopardi</a> on <a class="ae kv" href="https://unsplash.com/search/photos/running?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ec57" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">反向传播算法</strong>可能是神经网络中最基本的构建模块。它最早于 20 世纪 60 年代提出，近 30 年后(1989 年)由 Rumelhart、Hinton 和 Williams 在一篇名为<em class="ls"/><a class="ae kv" href="https://www.nature.com/articles/323533a0" rel="noopener ugc nofollow" target="_blank"><em class="ls">的论文中推广开来。</em></a></p><p id="7b03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">该算法用于通过一种称为链规则的方法有效地训练神经网络。</strong>简单来说，在网络的每次正向传递之后，反向传播在调整模型参数(权重和偏差)的同时执行反向传递。</p><p id="a0a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我想回顾一下训练和优化一个简单的 4 层神经网络的数学过程。我相信这将帮助读者理解反向传播是如何工作的，并意识到它的重要性。</p><h1 id="e539" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">定义神经网络模型</h1><p id="ae2a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">4 层神经网络由 4 个用于<strong class="ky ir">输入层</strong>的神经元、4 个用于<strong class="ky ir">隐含层</strong>的神经元和 1 个用于<strong class="ky ir">输出层</strong>的神经元组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/4c1fe529dda43f2e14948735acb6a492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sSIeU-WhsuHCQlOA00IBXg.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Simple 4-layer neural network illustration</figcaption></figure><h2 id="3fa2" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated">输入层</h2><p id="fec2" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">紫色的神经元代表输入数据。这些可以像标量一样简单，也可以像向量或多维矩阵一样复杂。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/a97e6ea902b6809b2801b663c935f9e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*JVKKlpPpmnes2CQoQ3uAbw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for input x_i</figcaption></figure><p id="069e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一组激活(<em class="ls"> a </em>)等于输入值。<em class="ls">注意:“激活”是应用激活函数后神经元的值。见下文。</em></p><h2 id="0af2" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated">隐藏层</h2><p id="5f66" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">使用<em class="ls">z^l</em>-层<em class="ls"> l </em>中的加权输入，以及<em class="ls">a^l</em>-层<em class="ls"> l </em>中的激活，计算隐藏神经元的最终值，以<strong class="ky ir">绿色</strong>、<strong class="ky ir">t29】表示。对于第 2 层和第 3 层，等式为:</strong></p><ul class=""><li id="4740" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><em class="ls"> l = 2 </em></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5889c49264d55725d3601d1883a79cf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*38G8wzETqpzwQvtExO6k3w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equations for z² and a²</figcaption></figure><ul class=""><li id="d19f" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><em class="ls"> l = 3 </em></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f81fecf20c002588a34178a5c83f27bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*pmnSATBMHgCnTSiL_cCy9Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equations for z³ and a³</figcaption></figure><p id="3b86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> W </em>和<em class="ls"> W </em>是层 2 和层 3 中的权重，而 b 和 b 是这些层中的偏差。</p><p id="3b9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用激活函数<em class="ls"> f </em>计算激活<em class="ls"> a </em>和<em class="ls"> a </em>。典型地，这个<strong class="ky ir">函数<em class="ls"> f </em>是非线性的</strong>(例如<a class="ae kv" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>、<a class="ae kv" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU </a>、<a class="ae kv" href="https://en.wikipedia.org/wiki/Hyperbolic_function" rel="noopener ugc nofollow" target="_blank"> tanh </a>)，并且允许网络学习数据中的复杂模式。我们不会详细讨论激活函数是如何工作的，但是，如果感兴趣，我强烈推荐阅读<a class="ae kv" href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener">这篇伟大的文章</a>。</p><p id="d969" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仔细观察，您会发现所有的<em class="ls"> x、z、a、z、a、W、W、b </em>和<em class="ls"> b </em>都缺少上面 4 层网络图中的下标。<strong class="ky ir">原因是我们已经将矩阵中的所有参数值组合起来，按层分组。这是使用神经网络的标准方式，人们应该对计算感到舒适。不过，我会检查一下方程式，以消除任何混淆。</strong></p><p id="15e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们挑选第 2 层及其参数作为例子。相同的操作可以应用于网络中的任何层。</p><ul class=""><li id="3b7a" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><em class="ls"> W </em>是形状为<em class="ls"> (n，m) </em>的权重矩阵，其中<em class="ls"> n </em>是输出神经元(下一层神经元)的数量，<em class="ls"> m </em>是输入神经元(上一层神经元)的数量。对于我们来说，<em class="ls"> n = 2 </em>和<em class="ls"> m = 4 </em>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c43371a7215a0f4d513e031f8d029957.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*yRlAdSEUTPeteVQ6xFT8xg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for W¹</figcaption></figure><p id="e434" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> NB:任何权重下标中的第一个数字匹配下一层</strong>中神经元的索引(在我们的例子中，这是<em class="ls"> Hidden_2 层</em> ) <strong class="ky ir">，第二个数字匹配上一层</strong>中神经元的索引(在我们的例子中，这是<em class="ls">输入层</em>)。</p><ul class=""><li id="ddd4" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><em class="ls"> x </em>是形状<em class="ls"> (m，1) </em>的输入向量，其中<em class="ls"> m </em>是输入神经元的数量。对于我们来说，<em class="ls"> m = 4 </em>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1870469db2f44628a7f9ef1a681ba19b.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*VQav2v1g6qkRXD48-HlmpQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for x</figcaption></figure><ul class=""><li id="39cb" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><em class="ls"> b </em>是形状<em class="ls"> (n，1) </em>的偏置向量，其中<em class="ls"> n </em>是当前层中神经元的数量。对于我们来说，<em class="ls"> n = 2 </em>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/16fac79ed194160b5284e7ce34d34fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/format:webp/1*S4fXnuMxU0BTPZ_W78hAaQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for b¹</figcaption></figure><p id="d462" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据<em class="ls"> z、</em>的等式，我们可以使用上述<em class="ls"> W、x </em>和<em class="ls"> b </em>的定义来推导出“<em class="ls">z 的等式”</em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/ac43a052bcd50233056d99e725346626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rYAiLFIXevABB60bMZuFOQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><em class="nt">Equation for z²</em></figcaption></figure><p id="6808" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在仔细观察上面的神经网络图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/4d947535e9c8f1dd1445b8193e51099b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*02zF6C6PYzGBbiah4-5fTQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Input and Hidden_1 layers</figcaption></figure><p id="914e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你会看到<em class="ls"> z </em>可以用(<em class="ls"> z_1) </em>和(<em class="ls"> z_2) </em>来表示，其中(<em class="ls"> z_1) </em>和(<em class="ls"> z_2) </em>是每个输入<em class="ls"> x_i </em>与相应权重(<em class="ls"> W_ij)相乘的和。</em></p><p id="be53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这导致 z 的相同的“T14”方程，并证明 z、a、z 和 a 的矩阵表示是正确的。</p><h2 id="6cde" class="mr lu iq bd lv ms mt dn lz mu mv dp md lf mw mx mf lj my mz mh ln na nb mj nc bi translated">输出层</h2><p id="d153" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">神经网络的最后一部分是产生预测值的输出层。在我们的简单示例中，它被表示为单个神经元，以<strong class="ky ir">蓝色</strong>和<strong class="ky ir">T23】着色，评估如下:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/807214c463bf7bf95bc1ae6552471ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*D1vfLBOaZSKCuBaOp8gK7Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for output s</figcaption></figure><p id="4e7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，我们使用矩阵表示来简化方程。人们可以使用上述技术来理解底层逻辑。如果你发现自己迷失在方程式中，请在下面留下你的评论——我很乐意帮助你！</p><h1 id="0850" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">正向传播和评估</h1><p id="458c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">上述方程构成了网络的前向传播。下面是一个简短的概述:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/01d24b98c526b7ec5a5d96467d1ff291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*51X_xj8p-jO8-plMfsyajg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Overview of forward propagation equations colored by layer</figcaption></figure><p id="123a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正向传递的最后一步是相对于<strong class="ky ir">预期输出<em class="ls"> y </em> </strong>评估<strong class="ky ir">预测输出<em class="ls"> s </em> </strong> <em class="ls"> </em>。</p><p id="7a7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出<em class="ls"> y </em>是训练数据集<em class="ls"> (x，y) </em>的一部分，其中<em class="ls"> x </em>是输入(正如我们在上一节中看到的)。</p><p id="29a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> s </em>和<em class="ls"> y </em>之间的评估通过<strong class="ky ir">成本函数</strong>进行。这可以像<a class="ae kv" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> MSE </a>(均方误差)一样简单，也可以像<a class="ae kv" href="http://neuralnetworksanddeeplearning.com/chap3.html" rel="noopener ugc nofollow" target="_blank">交叉熵</a>一样复杂。</p><p id="900c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将这个成本函数命名为<em class="ls"> C </em>，并表示如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1b60d403aa54b8d9ed00eb52aedfafd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*f8CwUGDguGaqCh-qvtW7lA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for cost function C</figcaption></figure><p id="32a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果<em class="ls">成本</em>可以等于 MSE、交叉熵或者<a class="ae kv" href="https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications" rel="noopener ugc nofollow" target="_blank">任何其他成本函数</a>。</p><p id="5c82" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于<em class="ls"> C </em>的值，模型“知道”调整其参数多少，以便更接近预期输出<em class="ls"> y </em>。这是使用反向传播算法实现的。</p><h1 id="59d7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">反向传播和计算梯度</h1><p id="0a3d" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">根据 1989 年的论文，反向传播:</p><blockquote class="ny nz oa"><p id="ad4e" class="kw kx ls ky b kz la jr lb lc ld ju le ob lg lh li oc lk ll lm od lo lp lq lr ij bi translated">重复地调整网络中连接的权重，以便最小化网络的实际输出向量和期望输出向量之间的差的度量。</p></blockquote><p id="32f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和</p><blockquote class="ny nz oa"><p id="f9fb" class="kw kx ls ky b kz la jr lb lc ld ju le ob lg lh li oc lk ll lm od lo lp lq lr ij bi translated">创建有用的新功能的能力将反向传播与早期更简单的方法区分开来…</p></blockquote><p id="cf7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，反向传播旨在通过调整网络的权重和偏差来最小化成本函数。调整水平由成本函数相对于这些参数的梯度决定。</p><p id="e174" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可能会出现一个问题——<strong class="ky ir">为什么要计算梯度</strong>？</p><p id="7722" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要回答这个问题，我们首先需要重温一些微积分术语:</p><ul class=""><li id="29ce" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><em class="ls">函数 C(x1，x2，…，x_m)在 x 点的梯度是 C 在 x 点的</em> <a class="ae kv" href="https://en.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank"> <em class="ls">偏导数</em> </a> <em class="ls">的向量</em></li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/74be2c5141630371d9ced1f87fb89bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*o1TgzZMHMWBUDamAtjKs2w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for derivative of C in x</figcaption></figure><ul class=""><li id="dd32" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Derivative" rel="noopener ugc nofollow" target="_blank"> <em class="ls">函数 C 的导数测量函数值(输出值)相对于其自变量 x(输入值)变化的敏感度</em> </a> <em class="ls">。换句话说，导数告诉我们 C 的方向。</em></li><li id="4b8e" class="ne nf iq ky b kz of lc og lf oh lj oi ln oj lr nj nk nl nm bi translated"><em class="ls">梯度显示了参数 x 需要改变多少(正向或负向)以最小化 c。</em></li></ul><p id="c95f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用一种叫做<a class="ae kv" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式法则</a>的技术来计算这些梯度。</p><p id="4816" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于单个重量<em class="ls"> (w_jk)^l，</em>的梯度为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/d90c2704ff62b9ac57e68d327d48657e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiYDrF5DgmbNubMq9x_czA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equations for derivative of C in a single weight <em class="nt">(w_jk)^l</em></figcaption></figure><p id="a166" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似的方程组可以应用于<em class="ls"> (b_j)^l </em>):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/cf6d4eaec44f466e0187441fe2f946da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xpco_RAhSlM9z5q6VSdy6Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equations for derivative of C in a single bias <em class="nt">(b_j)^l</em></figcaption></figure><p id="418c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">两个方程中的公共部分通常被称为<em class="ls">“局部梯度”</em>，并表示如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/57e58303c17d8cd6fc70599c80d24531.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*WQz6hHI1ymFlVCYwtIBdaQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for local gradient</figcaption></figure><p id="5315" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用链式法则可以很容易地确定<em class="ls">“局部梯度”</em>。我现在不会重复这个过程，但是如果你有任何问题，请在下面评论。</p></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="5e49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度允许我们优化模型的参数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9163949cfbb976450c9e95d9071eba9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*I_qw7AQwKvwfLuOHc7e4fg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Algorithm for optimizing weights and biases (also called “Gradient descent”)</figcaption></figure><ul class=""><li id="6e9e" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">随机选择<em class="ls"> w </em>和<em class="ls"> b </em>的初始值。</li><li id="fe5e" class="ne nf iq ky b kz of lc og lf oh lj oi ln oj lr nj nk nl nm bi translated">ε(<em class="ls">e</em>)是<a class="ae kv" href="https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">学习率</a>。它决定了渐变的影响。</li><li id="4338" class="ne nf iq ky b kz of lc og lf oh lj oi ln oj lr nj nk nl nm bi translated"><em class="ls"> w </em>和<em class="ls"> b </em>是权重和偏差的矩阵表示。<em class="ls"> w </em>或<em class="ls"> b </em>中的<em class="ls"> C </em>的导数可以使用各个权重或偏差中的<em class="ls"> C </em>的偏导数来计算。</li><li id="85e8" class="ne nf iq ky b kz of lc og lf oh lj oi ln oj lr nj nk nl nm bi translated">一旦成本函数最小化，就满足终止条件。</li></ul></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="defe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想把这一节的最后部分献给一个简单的例子，在这个例子中，我们将计算<em class="ls"> C </em>相对于单个重量<em class="ls"> (w_22) </em>的梯度。</p><p id="1991" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们放大上面神经网络的底部:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/0e6dbc7756faafa6f570f238f5cfc26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CxdjKFrE-Vww0KmI-3Z5sA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual representation of backpropagation in a neural network</figcaption></figure><p id="1050" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">权重<em class="ls"> (w_22) </em>连接<em class="ls">(a _ 2)</em><em class="ls">(z _ 2)</em>，因此计算梯度需要通过<em class="ls">(z _ 2)</em><em class="ls">(a _ 2)</em>应用链式法则</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/cb1a1729683a25211592602ce1e58917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*65rk0JpGEi6q0n2t6C688Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation for derivative of C in <em class="nt">(w_22)²</em></figcaption></figure><p id="9605" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">计算<em class="ls"> (a_2) </em>中<em class="ls"> C </em>的导数的最终值需要了解函数<em class="ls"> C </em>。由于<em class="ls"> C </em>依赖于<em class="ls"> (a_2) </em>，计算导数应该相当简单。</p><p id="748f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这个例子能够揭示计算梯度背后的数学原理。为了进一步提高你的技能，我强烈推荐观看斯坦福大学的 NLP 系列，其中理查德·索彻对反向传播给出了 4 个很好的解释。</p><h1 id="3866" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结束语</h1><p id="7f9f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在这篇文章中，我详细解释了反向传播是如何使用计算梯度、链式法则等数学技术进行工作的。了解这种算法的具体细节将巩固你的神经网络知识，并让你对更复杂的模型感到舒适。享受你的深度学习之旅！</p><h1 id="0559" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">谢谢你的阅读。希望你喜欢这篇文章🤩我祝你今天过得愉快！</h1></div></div>    
</body>
</html>