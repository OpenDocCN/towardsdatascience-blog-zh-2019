<html>
<head>
<title>Back-Propagation simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-propagation-simplified-218430e21ad0?source=collection_archive---------10-----------------------#2019-07-14">https://towardsdatascience.com/back-propagation-simplified-218430e21ad0?source=collection_archive---------10-----------------------#2019-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="1750" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章解释了什么可能是反向传播最基本的实现。我假设读者对反向传播(+梯度下降)有坚实的理论理解，只是不知道如何开始实现它。</p><p id="7b53" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Back-prop 是一种比较容易理解但实际编码起来要困难得多的算法。很多时候，你只是理解了背后的数学，感觉很舒服(如果你理解基本的多元微积分和线性代数，这并不难)，然后继续使用预定义版本的 back-prop 训练神经网络。你为什么不呢？基本上所有的深度学习框架(TensorFlow，Chainer，Caffe…等等。)附带已经实现的 back-prop。用自动微分之类的花哨东西(PyTorch！)，你作为用户基本没什么要做的。这也是在训练你自己的神经网络时一半的问题出现的地方。人们简单地认为，互连任何层的组合都是很酷的。使用 back-prop，梯度将神奇地向后流动，并产生下一个国家的艺术预测！</p><p id="6198" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但可悲的是，这种情况不常发生。这就是为什么理解 back-prop 很重要。对我个人来说，自从我自己实现了 back-prop，在最初的几次尝试中，我让我的神经网络做我想让它们做的事情的成功率高得多。安德烈·卡帕西有一篇关于这方面的优秀的<a class="ae kp" href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b" rel="noopener">文章</a>，你绝对应该看看。足够有说服力了，开始吧。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/ab4b2b93d78d6912ebcfcd9a47bd0bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DcLWqOojI1b9jzQaLibUkQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Summary of back-prop at one particular neuron (<a class="ae kp" href="https://www.youtube.com/watch?v=i94OvYb6noo" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="3289" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图从一个神经元的角度解释了算法。这里，<strong class="js iu"> L </strong>是在前一次向前传递中所做预测的成本值。梯度(<strong class="js iu"> δL/δz </strong>)由该神经元从网络中它的前几层接收。我们从这个图像中得到一个想法，即每个神经元的局部梯度(<strong class="js iu"> δz/δx </strong>和<strong class="js iu"> δz/δy </strong>)、输出(<strong class="js iu"> z </strong>)和激活(<strong class="js iu"> x </strong>和<strong class="js iu"> y </strong>)需要存储在存储器中以供以后计算。这些用于计算梯度(<strong class="js iu"> δL/δx </strong>和<strong class="js iu"> δL/δy </strong>)以向后传递，该梯度将被先前层的神经元接收。具体如何操作将在下面的章节中给出。</p><p id="a409" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的完整代码可以在<a class="ae kp" href="https://github.com/ksivaman/simple-net" rel="noopener ugc nofollow" target="_blank">这里</a>找到。本文解释了重要的部分，其余的只是让一切运行的样板代码。我们将创建一个简单的前馈神经网络，只有线性层(+激活)来分类钞票是否是伪造的。我们将使用<a class="ae kp" href="http://archive.ics.uci.edu/ml/datasets/banknote+authentication" rel="noopener ugc nofollow" target="_blank">这个</a>数据集，该数据集具有基于纸币图像的 4 个特征:小波的方差、小波的偏斜度、小波的曲率和熵。根据纸币是否是伪钞，目标为 0 或 1。在这个数据集中总共有<strong class="js iu"> 1372 个观察值</strong>，可以分成训练集和验证集。</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="607c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">变量名是不言自明的，变量前面的前缀'<em class="ko"> d_' </em>表示它存储变量后面的导数'<em class="ko"> d_ </em> ' (' <em class="ko"> d_this' </em>存储'<em class="ko"> this' </em>'的梯度)。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/563aba91e9c87c220fcb9cddaec5dc2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*drNTpW_cO1JZGEzw8NcGyQ.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Fig 1: A simple linear feed-forward network we will use (<a class="ae kp" href="https://pdfs.semanticscholar.org/f7fe/6ef2feff4eb014fedc5a6a5ff14a94892f81.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bd lo">Source</strong></a><strong class="bd lo">)</strong></figcaption></figure><p id="c3ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上图显示了我们将用于分类的网络。4 个输入对应于特征，输出是分类器的预测(0 或 1)。隐藏层将由 relu 激活，输出层将由 sigmoid 激活(以获得类概率)。</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="36ec" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">参数初始化</h1><p id="e82b" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">开始之前的第一件事是初始化我们网络的参数值。不同图层的权重和偏差将作为键值对保存在单独的字典中。这是相当基本的，可以使用下面的代码片段来完成:</p><ul class=""><li id="16da" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated">参数'<em class="ko"> layers' </em>是一个列表，包含每层神经元数量的整数值。因此，根据图 1 中的架构，这将是一个包含 4、5 和 1(默认)的列表。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="52e5" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">成本函数和度量</h1><p id="bf01" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">接下来要做的事情是制定损失函数，并找出一个衡量标准来检查我们的网络在每次转发时的性能。我们将使用简单的二元交叉熵损失作为成本函数，使用准确度作为评估度量。这些很容易计算，可以通过以下方式完成:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="ce68" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">“y_pred”是预测类，“<em class="ko">train _ Y”</em>是基础事实。</p><p id="9b7d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">“get _ class _ from _ probs</em>”是一个效用函数，用于获得预测的类值(0 或 1)，而不是从 sigmoid 函数获得的概率。</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="1fe2" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">激活和衍生</h1><p id="4ffa" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">计算激活及其导数的简单效用函数是重要的。在我们的例子中，我们将有 4 个这样的函数:sigmoid、relu、Sigmoid 的导数和 Relu 的导数。这可以使用下面的代码片段来完成。注意 relu 的导数只是一个单位阶跃函数<em class="ko">。</em></p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="338c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">'<em class="ko"> d_init </em>'参数是初始化梯度。对于最后一个层，该值将为 1，并且该值将是所有先前层直到当前层的梯度的累积乘积。</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="30cd" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">前进传球</h1><p id="3218" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">这是最简单的部分，我们执行矩阵乘法，在每个阶段添加偏差，通过激活函数运行输出，并得到最终结果。代码分为两部分:一层正向传递和整个正向传递。下面给出了一层向前传递的代码。由 n 个神经元组成的层的参数:</p><ul class=""><li id="3316" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated"><em class="ko"> input_activations </em>:表示进入该层的激活值。是一个维数为(n，x)的矩阵，其中 x 是训练数据样本的数量(在我们的例子中是 1097，从总共 1372 个观察值中使用 80%的训练分割)</li><li id="3409" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko">权重</em>:该层的权重矩阵。是一个维数为(m，n)的矩阵，其中 m 是下一层神经元的数目。</li><li id="b157" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko">偏置</em>:该层的偏置矩阵。是维数为(m，1)的矩阵</li><li id="cd38" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko">激活</em>:该层的激活可以是 sigmoid 或 relu(在我们的例子中)。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="67c7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下一步是使用前面的函数创建整个向前传递，其代码如下所示。这里的关键是<strong class="js iu">将该层的输出值</strong>保存在字典中，在通过激活函数之前和之后。当使用链规则向后传递渐变时，这些是必需的。这些参数是:</p><ul class=""><li id="bc17" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated"><em class="ko"> train_X </em>:训练数据。是维数为(n，x)的矩阵，其中 n 是第一层中神经元的数量，而 x 是训练数据样本的数量。</li><li id="3410" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> params_w </em>:存储不同层的权重矩阵及其对应关键字的字典。关键字的形式是“权重 1”、“权重 2”…值是矩阵本身。</li><li id="2827" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">params_b:字典存储不同层的权重矩阵及其对应的关键字。关键字的形式是“bias1”、“bias 2”…值是矩阵本身。</li><li id="ef06" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko">图层</em>:同参数初始化章节<strong class="js iu"> init </strong>。</li><li id="4977" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">激活:不同隐藏层和输出层的激活值。对于我们的例子，默认是我们将要使用的，R 代表 relu(隐藏层)，S 代表 sigmoid(输出层)。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="8ad7" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">偶数道次</h1><p id="bdc4" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">这是最棘手的部分。这也和向前传球一样，可以分为单层传球和整体向后传球。单层向后传递包括几个步骤:</p><ul class=""><li id="899d" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated">找出当前层中使用的激活函数(第 7-12 行)。</li><li id="a479" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">使用传递到该点的累积梯度和该层的输出值计算局部梯度(第 15 行)。(还记得早先把它们保存在字典里吗？)</li><li id="f437" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">计算当前层的权重矩阵的梯度(第 18 行)。</li><li id="a8a6" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">计算当前层的偏差矩阵的梯度(第 21 行)。</li><li id="c229" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">为当前层计算进入该层矩阵的激活的梯度(第 24 行)。</li></ul><p id="0c56" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意使用<em class="ko">链规则</em>计算梯度。下面给出了带有以下参数的代码片段:</p><ul class=""><li id="3905" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated"><em class="ko"> curr_grad </em>:当前累积的梯度从它前面的所有层传递到这一层。是 shape(n，x)的矩阵，其中 n 是当前层的神经元数量，x 是训练样本的数量。</li><li id="f3c3" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> curr_weight </em>:前一层到当前层变换的权重矩阵。是形状(n，p)的矩阵，其中 p 是前一层中神经元的数量。</li><li id="abd4" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> curr_bias </em>:从前一层到当前层转换的偏差矩阵。是一个形状矩阵(n，1)。</li><li id="76a8" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> curr_out </em>:从该层向前传播时的输出矩阵。是一个形状矩阵(n，x)。</li><li id="75ab" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> prev_act </em>:正向传播过程中进入该层的激活。是一个形状矩阵(p，x)。</li><li id="6792" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko">激活</em>:该层使用的激活类型。(' R': relu，' S ':S 形)</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="794e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们有了从一层计算梯度的方法，我们可以通过对我们的前馈神经网络中的所有层重复使用该函数(显然以相反的顺序)来容易地反向支持整个网络。从 one_layer_backward_pass 函数计算的梯度保存在'<em class="ko">梯度</em>字典中。在执行梯度下降时，这些梯度稍后将用于更新参数(权重+偏差)。</p><p id="a214" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">注意:损失函数相对于网络预测的梯度必须在主循环之外计算。这已经在下面代码片段的第 10 行完成了。通过对'<em class="ko"> cross_entropy_loss </em>'函数(由预测和实际标签参数化)相对于预测'<em class="ko"> y_pred' </em>进行求导来得出这个公式是很简单的。</p><p id="765a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些参数是:</p><ul class=""><li id="a437" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated"><em class="ko"> y_pred </em>:网络在正向传递过程中对目标值的预测数组。长度为 n 的数组，其中 n 是训练样本的数量。</li><li id="e91c" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> train_Y </em>:训练数据的标签(0 或 1)。长度为 n 的数组。</li><li id="5fd1" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> activation_dict </em>:一个字典，存储进入所有层的激活。根据“act0”、“act1”、“act2”…形式的关键字对除最后一层之外的所有层进行索引。</li><li id="125d" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> output_dict </em>:存储所有层输出的字典。除第一层外，根据所有层<strong class="js iu">的‘out 1’、‘out 2’、‘out 3’…形式的关键字进行索引。</strong></li><li id="ac04" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> params_w，params_b，layers，activate </em> : <em class="ko"> </em>这些参数与正向传递部分的函数<em class="ko">正向传递</em>中的含义相同。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="070a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里需要注意的重要事项是:</p><ul class=""><li id="4036" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated">反向过程中的层的反向迭代。</li><li id="c7b4" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">激活进入一个层，输出从它出去。一层的输出是下一层的激活。由于这个原因，对于 n 层网络，'<em class="ko"> activation_dict </em>中的激活索引从 0 到 n-2，而'<em class="ko"> output_dict </em>中的输出索引从 1 到 n-1。</li></ul></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><h1 id="aff9" class="lp lq it bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">参数更新和培训</h1><p id="f4e4" class="pw-post-body-paragraph jq jr it js b jt mn jv jw jx mo jz ka kb mp kd ke kf mq kh ki kj mr kl km kn im bi translated">难的部分完成了！现在我们要做的就是更新参数，用梯度下降法训练网络。在这个玩具示例中，我们将使用批量梯度下降，但当训练数据丰富时，您最好使用小批量梯度下降以提高效率。使用以下函数更新权重和偏差。这些参数是:</p><ul class=""><li id="d54a" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated"><em class="ko"> params_w </em>，<em class="ko"> params_b </em>，<em class="ko">层</em>:这些参数与正向传递部分的<em class="ko">正向传递</em>功能中的含义相同。</li><li id="f3f4" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><em class="ko"> gradients </em>:包含偏差和权重梯度的字典，使用该字典更新偏差和权重。这是在'<em class="ko"> backward_pass' </em>函数中计算的。关键字是“d_weight1”、“d _ weight 2”…等等。和“d_bias1”、“d_bias2”..等等。</li></ul><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="651e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，可以使用以下步骤来训练网络:</p><ul class=""><li id="af6c" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated">初始化参数。</li><li id="7400" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">完成一次向前传球。</li><li id="64d5" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">计算损失。</li><li id="d60c" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">完成一次向后传球。</li><li id="0d61" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated">更新参数。</li></ul><p id="5829" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下是训练网络的代码:</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="nb nc l"/></div></figure></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="0b57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">就是这样！真的就这么简单。现在剩下要做的就是用看不见的数据测试我们的网络。为了更彻底地理解，重要的是亲自尝试，在头脑中运行数学和计算，并查看网络不同阶段的梯度、权重和偏差的维度。关键是不要混淆反推和梯度下降。Back-prop 仅用于计算网络各个阶段的梯度，而梯度下降使用这些梯度来更新网络的参数。我在这里省略了测试代码(本质上只是一次向前传递)，但是你可以在这里看到我的完整实现<a class="ae kp" href="https://github.com/ksivaman/simple-net" rel="noopener ugc nofollow" target="_blank">(包括训练验证分割和测试网络)。谢谢你一路走过来！</a></p><h1 id="86eb" class="lp lq it bd lr ls ni lu lv lw nj ly lz ma nk mc md me nl mg mh mi nm mk ml mm bi translated">其他资源</h1><ul class=""><li id="2e6e" class="ms mt it js b jt mn jx mo kb nn kf no kj np kn mx my mz na bi translated"><a class="ae kp" href="https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b" rel="noopener">https://medium . com/@ karpathy/yes-you-should-understand-back prop-e 2f 06 eab 496 b</a></li><li id="8188" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><a class="ae kp" href="https://www.youtube.com/watch?v=i94OvYb6noo" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=i94OvYb6noo</a></li><li id="74a5" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><a class="ae kp" rel="noopener" target="_blank" href="/lets-code-a-neural-network-in-plain-numpy-ae7e74410795">https://towards data science . com/let-code-a-neural-network-in-plain-numpy-ae7e 74410795</a></li><li id="81fb" class="ms mt it js b jt nd jx ne kb nf kf ng kj nh kn mx my mz na bi translated"><a class="ae kp" href="https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . CMU . edu/~ mgormley/courses/10601-s17/slides/lecture 20-back prop . pdf</a></li></ul></div></div>    
</body>
</html>