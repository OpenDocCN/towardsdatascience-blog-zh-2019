<html>
<head>
<title>[CVPR 2019] Efficient Online Multi-Person 2D Pose Tracking with Recurrent Spatio-Temporal Affinity Fields</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[CVPR 2019]利用递归时空亲和场的高效在线多人 2D 姿态跟踪</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cvpr-2019-efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-25c4914e5f6?source=collection_archive---------13-----------------------#2019-07-02">https://towardsdatascience.com/cvpr-2019-efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-25c4914e5f6?source=collection_archive---------13-----------------------#2019-07-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="27aa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在单个 GPU 上以 30 fps 的速度实时检测和跟踪多人的 2D 姿势。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1ad886d4ce476779955038f590321422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gseu8Wok-Y_m0WuL.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Real-time detect and track 2D poses of multiple people at 30 fps on a single GPU. <a class="ae kv" href="https://www.youtube.com/watch?v=1Hg39MVNKBw" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="9b17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本帖中，我们将回顾来自<strong class="ky ir"> CVPR 2019 </strong>的一篇名为<strong class="ky ir">“具有递归时空亲和场的高效在线多人 2D 姿态跟踪”</strong>的新论文。</p><p id="7bb7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者提出了一种在线方法，可以在单个 GPU 上以 30 fps 的速度有效地同时<strong class="ky ir">检测和跟踪多人的 2D 姿势</strong>。</p><p id="56d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据作者的说法，这是<strong class="ky ir">“目前最快和最准确的自下而上方法，它对场景中的人数和摄像机的输入帧率都是运行时不变的和准确性不变的。”</strong></p><p id="2ca0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该文件的一些要点:</p><ul class=""><li id="278c" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">鲁棒且<strong class="ky ir">对输入帧速率</strong>不变，即使在 6Hz 输入时也是如此</li><li id="ae17" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">处理<strong class="ky ir">快速移动目标</strong>和摄像机移动</li><li id="8fc8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">运行时间<strong class="ky ir">对帧中的人数</strong>不变。</li><li id="227d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">提出<strong class="ky ir">时空亲和场(STAF) </strong>编码跨帧的关键点之间的连接。</li><li id="0f06" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">提出一种新颖的<strong class="ky ir">跨肢体交叉链接的时间拓扑</strong>，可以处理移动目标、摄像机运动和运动模糊。</li></ul><p id="7b29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是作者的结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mg mh l"/></div></figure><h1 id="7cec" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">概述</h1><p id="e3b2" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">介绍</p><p id="2192" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">整体管道</p><ul class=""><li id="9ecc" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">VGG 特色</li><li id="df1e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">PAF 和关键点热图</li><li id="e72b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">时间亲和场</li><li id="551a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">推理</li></ul><p id="01d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">交联拓扑</p><p id="1b0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">结果</p><p id="7b69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">限制</p><p id="2d46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">履行</p><p id="edf0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">参考</p><p id="95bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的评论</p><h1 id="b3c5" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">介绍</h1><p id="bf9e" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">在过去的几年中，人体姿态和跟踪受到了极大的关注。最近推出的<strong class="ky ir"> PoseTrack </strong>数据集是一个大规模的视频数据语料库，使得这个问题在计算机视觉社区中得以发展。</p><p id="b8fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于这个问题还有其他几部作品。然而，这些方法都不能像通常那样实时运行:</p><ol class=""><li id="3f17" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nf ly lz ma bi translated">遵循自上而下的检测和跟踪任务方法，随着人数的增加，需要更多的计算。</li><li id="4143" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nf ly lz ma bi translated">需要离线计算或帧堆叠，这降低了速度，并且没有获得比匈牙利算法基线更好的跟踪结果。</li></ol><p id="5562" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，作者的目标是建立一个真正在线和实时的多人 2D 姿态估计器和跟踪器。作者以循环的方式在视频上工作，以使该方法实时。它们通过组合 1)关键点热图 2)零件亲和场和 3)时空亲和场(STAF)来利用来自前一帧的信息。</p><p id="c3d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者在 2017 年 CVPR 的 OpenPose 中扩展了<strong class="ky ir">零件亲和域</strong>。这篇论文的作者也是 OpenPose 的作者。请随意查看<a class="ae kv" rel="noopener" target="_blank" href="/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8">我们之前在 OpenPose </a>上的博文。</p><h1 id="7ea7" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">整体管道</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b011cd4a0cd297f3ccb5ca9841d72e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*RuaG7-gWb6vV2KKL.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The overall pipeline of the STAF algorithm. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="0578" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上图是时空亲和场(STAF) 算法的<strong class="ky ir">整体流水线。视频帧在时间上以<strong class="ky ir">循环方式</strong>进行处理，包括:</strong></p><ul class=""><li id="ec30" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">提取<strong class="ky ir"> VGG 特征</strong>。</li><li id="ba5b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">提取<strong class="ky ir">零件亲缘关系字段(PAF)</strong></li><li id="d20b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">提取<strong class="ky ir">关键点热图</strong></li><li id="7adf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">跨帧提取关键点之间的连接作为<strong class="ky ir">时间相似性场(TAFs) </strong>。</li></ul><h1 id="531d" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">VGG 特色</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4d93aba913ad27ed2a14ca69cf4d7705.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/0*rm-jVUDhtEW4Kli8.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">VGG features. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="e43c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每一帧由 VGG 骨干网处理，以提取 VGG 特征。这部分不需要以前的帧信息。</p><p id="f7c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于视频时间 t 处的帧<strong class="ky ir"> It </strong>,它们计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/16f54164f071a9b67377e34b2ee7d88e.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*e2V_TE_zK8y-ktGNJvmFyg.png"/></div></figure><h1 id="91b9" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">PAF 和关键点热图</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2b2931a630305ae73d90d118beb9702c.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*Nn0lxceJRjXe9FV0.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Part Affinity Fields (PAFs). <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/42afba37a49cb30038d0d0fc1a682a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/0*WEkg4l4CdkxoVr7v.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Keypoints Heatmaps. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="711a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PAFs 和关键点热图<strong class="ky ir">与开放姿态方法</strong>非常相似，除了它们也将<strong class="ky ir">使用来自前一帧</strong>的信息。</p><p id="45e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，有三种计算 PAFs 的方法:</p><ol class=""><li id="3923" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nf ly lz ma bi translated">使用 5 个先前的帧数据。</li><li id="5bbf" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nf ly lz ma bi translated">仅使用 1 个前一帧。</li><li id="1bca" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nf ly lz ma bi translated">同时估计 PAF 和 taf。</li></ol><p id="afa8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在他们的实验中，他们发现:</p><ul class=""><li id="d72a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">方法 1 产生了好的结果，但是由于递归阶段，它是最慢的。</li><li id="0dc9" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">方法 2 <strong class="ky ir">提高速度，而性能没有大的损失</strong>。</li><li id="aa77" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">方法 3 最难训练，但速度最快。</li></ul><p id="5960" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇博客中，我们只讨论方法 2。其他方法，请查看原论文。</p><p id="08fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">方法 2 在单次通过中计算 PAF 和关键点，如下所示:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/46ce7762912e2e6bd608470627fc46e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*pUpdRpps3nOVElQ_hnDiuA.png"/></div></figure><h1 id="f485" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">时间亲和场</h1><p id="03a3" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">时间亲合场(taf)编码跨帧的关键点之间的连接。</p><p id="0bb6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">这些 taf 是向量场，指示每个身体关节将从帧 I t1</strong><strong class="ky ir">移动到帧 It </strong>的方向。Andreas Doering 等人在论文中也介绍了 taf。</p><p id="afeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下图中，taf 表示为蓝色箭头 RtRt。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/28c8805a1ab4e8453f2e8ced16598edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/0*qUMEMqf4W-rlPWa1.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Connections between keypoints across frames. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/562fd4100346921972f8a318afa49157.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/0*wAcA5IHy9xny1M10.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Temporal Affinity Fields (TAFs). <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="f4fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">taf 依赖于来自前一帧和当前帧的 VGG 特征和 PAF 以及来自前一帧的 taf，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8f2f517a55b74343c37d0cd97d84990b.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*mFJ2shvQA2LXMSad6j_XZQ.png"/></div></figure><h1 id="6e3e" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">推理</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/275db784001073c3e7b944baaa49da98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/0*7I8VWHgPQrJjDxYU.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Inference. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c2cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推断出的 PAF 和 taf 在推断出完整的姿势并将它们跨帧与唯一的 id 相关联之前，都按照它们的分数进行排序。</p><p id="bae2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是步骤:遍历排序列表中的每个 PAF:</p><ul class=""><li id="fc0d" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">如果 PAF 的两个关键点都没有指定，初始化一个新姿势</li><li id="5ac5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> 01 关键点</strong>已分配:添加到现有姿势</li><li id="13c0" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">两个关键点都被赋值</strong>:将姿势中 PAF 的分数更新到同一个姿势。</li><li id="6742" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">两个关键点被分配到不同的姿态</strong>:合并两个姿态</li><li id="ede4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用前一帧中最频繁出现的关键点 id<strong class="ky ir">为当前帧中的每个姿势分配 id。</strong></li></ul><p id="5162" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是原始论文中的详细算法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/254d891b36ddfda0b14c088a29793173.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/0*-dpIfXDyxMXX06yL.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Algorithm: Estimation and tracking of keypoints and STAFs. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="440d" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">交联拓扑</h1><p id="2322" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">STAFs 有三种可能的拓扑变体。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/01f215df447a41348127da3e2799834a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/0*Y6u0mhJfpBCjvI3j.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Topology variants for STAFs. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="d411" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">拓扑 A </strong>是关键点 TAF，并且当关键点具有最小运动或者当新人出现时缺乏关联属性。</p><p id="7474" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">拓扑 B </strong>和<strong class="ky ir">拓扑 C </strong>是<strong class="ky ir">交联分支拓扑</strong>。作者证明了<strong class="ky ir">交联肢体拓扑</strong>可以解决来自<strong class="ky ir">拓扑 A </strong>的运动问题。当有最小的运动时，<strong class="ky ir"> TAFs </strong>就变成了那个肢体的<strong class="ky ir"> PAFs </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/83b7b1046cbb3d87a803aa46c181a832.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tN1LK_hOm_vAKUYw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Keypoint TAF vs Cross-Linked Limb TAF. <a class="ae kv" href="https://youtu.be/1DW1FSMs76k?t=1884" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b203" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在论文实验中，<strong class="ky ir">拓扑 C </strong>与<strong class="ky ir">拓扑 B </strong>相比表现不佳。</p><h1 id="19e1" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">结果</h1><ul class=""><li id="ca57" class="ls lt iq ky b kz na lc nb lf nt lj nu ln nv lr lx ly lz ma bi translated">该方法即使在帧率下降到 6Hz 时也能达到高精度。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/add666d8f693e78e54f6e93d8d2c31bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Kv-3W5i5OGP3F4co.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Camera framerate experiments of STAFs. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="69b1" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">该方法具有较高的准确率和较快的推理速度。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="ab gu cl nx"><img src="../Images/103c2b3760c603acac5938adfd1329ef.png" data-original-src="https://miro.medium.com/v2/format:webp/0*e8raSp_CkSDFUt31.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Camera framerate experiments of STAFs. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><ul class=""><li id="040a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">该方法可以在 PoseTrack 数据集上取得有竞争力的结果。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/823f48c763381d0e0bb3e039bce4a7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/0*Qb6DiK0dHFHoVDIm.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Camera framerate experiments of STAFs. <a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="910d" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">限制</h1><p id="06a7" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">这项工作有一些限制:</p><ul class=""><li id="8ed5" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">本文没有嵌入重新识别模块来处理人离开和重新出现在场景中的情况。</li><li id="a475" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">这种方法不能处理场景变化。因此，可能需要检测场景变化，并从该帧重新开始该过程。</li></ul><h1 id="0621" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">履行</h1><p id="61f2" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">本文在作者资源库<a class="ae kv" href="https://github.com/soulslicer/openpose/tree/staf" rel="noopener ugc nofollow" target="_blank"> soulslicer/openpose </a>开源。</p><p id="96ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是来自<a class="ae kv" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose 库</a>的一个分支。在不久的将来，它们将被合并，你可以直接从 OpenPose 使用它。</p><h1 id="646b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">参考</h1><ul class=""><li id="5e92" class="ls lt iq ky b kz na lc nb lf nt lj nu ln nv lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1811.11975" rel="noopener ugc nofollow" target="_blank"> [1] Yaadhav Raaj，Haroon Idrees，Gines Hidalgo，亚塞尔·谢赫，使用递归时空亲和场的高效在线多人 2D 姿态跟踪，CVPR 2019 </a></li><li id="9d82" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">【2】曹哲，托马斯·西蒙，施-韦恩，亚塞尔·谢赫，使用部分亲和场的实时多人 2D 姿态估计(2017)，CVPR 2017 </a></li><li id="d2af" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://arxiv.org/abs/1805.04596" rel="noopener ugc nofollow" target="_blank">【3】Andreas Doering，Umar Iqbal，Juergen Gall，联合流:多人跟踪的时间流场，CoRR，abs/1805.04596，2018 </a></li><li id="9808" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://cmu-perceptual-computing-lab.github.io/spatio-temporal-affinity-fields/" rel="noopener ugc nofollow" target="_blank">项目页面:使用递归时空亲和场的高效在线多人 2D 姿态跟踪</a></li><li id="e15c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://github.com/soulslicer/openpose/tree/staf" rel="noopener ugc nofollow" target="_blank"> Github 代码:soulslicer/openpose </a></li></ul><h1 id="0fc5" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">我的评论</h1><p id="8236" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">图像分类:<a class="ae kv" href="https://medium.com/p/alexnet-review-and-implementation-e37a8e4dab54" rel="noopener">【NIPS 2012】AlexNet</a><br/>图像分割:<a class="ae kv" rel="noopener" target="_blank" href="/cvpr-2019-pose2seg-detection-free-human-instance-segmentation-61e4948ba6db">【CVPR 2019】Pose 2 seg</a><br/>姿态估计:<a class="ae kv" rel="noopener" target="_blank" href="/cvpr-2017-openpose-realtime-multi-person-2d-pose-estimation-using-part-affinity-fields-f2ce18d720e8">【CVPR 2017】open Pose</a><br/>姿态跟踪:<a class="ae kv" rel="noopener" target="_blank" href="/cvpr-2019-efficient-online-multi-person-2d-pose-tracking-with-recurrent-spatio-temporal-affinity-25c4914e5f6">【CVPR 2019】STAF</a></p></div></div>    
</body>
</html>