# ç†ŠçŒ«å°ç»„è¯¦ç»†è§£é‡Šäº†

> åŸæ–‡ï¼š<https://towardsdatascience.com/pandas-groupby-aggregate-transform-filter-c95ba3444bbb?source=collection_archive---------0----------------------->

## ç†ŠçŒ«ç»ˆææŒ‡å—

## äº†è§£å¦‚ä½•æŒæ¡æ‰€æœ‰ Pandas çš„ groupby åŠŸèƒ½ï¼Œå¦‚èšé›†ã€è½¬æ¢å’Œè¿‡æ»¤â€”â€”è¿™æ˜¯ä¸€ä»½é™„æœ‰å¤§é‡ç¤ºä¾‹çš„ä»£ç æŒ‡å—

![](img/7000bf860d83631838a8a7dd8fd1a5c5.png)

[Source](https://www.economist.com/china/2016/09/08/survival-of-the-cutest)

å†™å…³äºç†ŠçŒ«çš„æ–‡ç« æ˜¯æœ€å¥½çš„ã€‚æˆ‘æœ€å–œæ¬¢çš„éƒ¨åˆ†æ˜¯å½“æˆ‘åœ¨ç½‘ä¸Šæœç´¢å¯çˆ±çš„ç†ŠçŒ«å›¾ç‰‡çš„æ—¶å€™ã€‚Cmon ä½ æ€ä¹ˆèƒ½ä¸çˆ±ç†ŠçŒ«å‘¢ï¼Ÿè¿˜æœ‰æˆç¾¤çš„ç†ŠçŒ«ï¼Œæ›´å¥½ï¼åæ­£æˆ‘è·‘é¢˜äº†â€¦

# ä»‹ç»

andas çš„ groupby æ— ç–‘æ˜¯ç†ŠçŒ«å¸¦æ¥çš„æœ€å¼ºå¤§çš„åŠŸèƒ½ä¹‹ä¸€ã€‚ç„¶è€Œï¼Œ**å¤§å¤šæ•°ç”¨æˆ·åªåˆ©ç”¨äº†** `**groupby**`çš„ä¸€å°éƒ¨åˆ†åŠŸèƒ½ã€‚

`Groupby`å…è®¸å¯¹æ•°æ®é›†é‡‡ç”¨æ‹†åˆ†-åº”ç”¨-ç»„åˆçš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•é€šå¸¸ç”¨äºå¯¹æ•°æ®è¿›è¡Œåˆ†å‰²ï¼Œä»¥ä¾¿æ•°æ®åˆ†æå¸ˆå¯ä»¥å›ç­”ç‰¹å®šçš„é—®é¢˜ã€‚

## åœ¨é«˜çº§åˆ†ç»„ä¸Šï¼Œby å…è®¸:

1.  æ ¹æ®åˆ—/æ¡ä»¶å°†æ•°æ®åˆ†æˆç»„ï¼›
2.  å¯¹æ‰€æœ‰ç»„åº”ç”¨å‡½æ•°/å˜æ¢ï¼Œå¹¶å°†ç»“æœç»„åˆæˆè¾“å‡º

## åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºä»¥ä¸‹ä¸»é¢˜:

1.  [åŠ è½½æ•°æ®](#08dc)
2.  [åˆ†ç»„ä¾æ®â€”æ‹†åˆ†æ•°æ®](#3a1f)
3.  [åº”ç”¨å¹¶ç»„åˆâ€”](#d6c9) `[apply](#d6c9)` [ï¼Œ](#d6c9) `[agg(regate)](#d6c9)` [ï¼Œ](#d6c9) `[transform](#d6c9)` [ï¼Œ](#d6c9) `[filter](#d6c9)`

ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°å®Œæ•´çš„ Jupyter ç¬”è®°æœ¬ã€‚ä½†æ˜¯æˆ‘å¼ºçƒˆå»ºè®®æ‚¨äº²è‡ªå®Œæˆè¿™äº›æ­¥éª¤ã€‚æ¯•ç«Ÿï¼Œç†Ÿèƒ½ç”Ÿå·§ã€‚

# â‘ åŠ è½½æ•°æ®

![](img/9d43ec413213993003f79a19ec4bde88.png)

Photo by [NASA](https://unsplash.com/@nasa?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå‡è®¾çš„é”€å”®éƒ¨é—¨çš„æ•°æ®ã€‚æ•°æ®é›†ç”±è™šæ„çš„é”€å”®ä»£è¡¨ã€è®¢å•çº¿ç´¢ã€å¯èƒ½è¾¾æˆäº¤æ˜“çš„å…¬å¸ã€è®¢å•ä»·å€¼å’Œçº¿ç´¢æ—¥æœŸç­‰åˆ—ç»„æˆã€‚

```
order_leads = pd.read_csv(
    '[https://raw.githubusercontent.com/FBosler/Medium-Data-Exploration/master/order_leads.csv'](https://raw.githubusercontent.com/FBosler/Medium-Data-Exploration/master/order_leads.csv'),
    parse_dates = [3]
)
sales_team = pd.read_csv(
    '[https://raw.githubusercontent.com/FBosler/Medium-Data-Exploration/master/sales_team.csv'](https://raw.githubusercontent.com/FBosler/Medium-Data-Exploration/master/sales_team.csv'),
    parse_dates = [3]
)df = pd.merge(
  order_leads,
  sales_team,
  on=['Company Id','Company Name']
)df = df.rename(
  columns={'Order Value':'Val','Converted':'Sale'}
)
```

![](img/5b4b5e8b95d7ca4e44a3789ecc63809d.png)

100k rows of order lead data

# â‘¡åˆ†ç»„ä¾æ®â€”â€”æ‹†åˆ†æ•°æ®

![](img/629e6ee302d9f1441656d4eafab1c246.png)

Photo by [Chris Child](https://unsplash.com/@chris23?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/splitting-wood?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

è°ƒç”¨ groupby çš„é»˜è®¤æ–¹æ³•æ˜¯é€šè¿‡æ˜¾å¼æä¾›ä¸€ä¸ªåˆ—åæ¥æ‹†åˆ†æ•°æ®é›†ã€‚ä½†æ˜¯ï¼Œè¿˜æœ‰ä¸€ç‚¹ä¸å¤ªä¸ºäººæ‰€çŸ¥ï¼Œé‚£å°±æ˜¯æ‚¨ä¹Ÿå¯ä»¥å°†ä¸€ä¸ªç³»åˆ—ä¼ é€’ç»™ groupbyã€‚å”¯ä¸€çš„é™åˆ¶æ˜¯åºåˆ—çš„é•¿åº¦ä¸æ•°æ®å¸§çš„é•¿åº¦ç›¸åŒã€‚
èƒ½å¤Ÿä¼ é€’ä¸€ä¸ªåºåˆ—æ„å‘³ç€æ‚¨å¯ä»¥æ ¹æ®ä¸€ä¸ªåˆ—çš„å¤„ç†ç‰ˆæœ¬è¿›è¡Œåˆ†ç»„ï¼Œè€Œä¸å¿…ä¸ºæ­¤åˆ›å»ºä¸€ä¸ªæ–°çš„å¸®åŠ©åˆ—ã€‚

## æŒ‰é”€å”®ä»£è¡¨åˆ†ç»„

é¦–å…ˆï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªåˆ†ç»„çš„æ•°æ®æ¡†æ¶ï¼Œå³å°†æ•°æ®é›†æ‹†åˆ†ã€‚

```
**IN:**
grouped = df.groupby('Sales Rep')
grouped**OUT:** <pandas.core.groupby.generic.DataFrameGroupBy object at 0x12464a160>**IN:** type(grouped)**OUT:** pandas.core.groupby.generic.DataFrameGroupBy
```

æˆ‘ä»¬ç°åœ¨å·²ç»åˆ›å»ºäº†ä¸€ä¸ª`**DataFrameGroupBy**`å¯¹è±¡ã€‚è®©æˆ‘ä»¬è¿›ä¸€æ­¥è°ƒæŸ¥:

## æ˜¾ç¤ºæ‰€æœ‰ç»„

åœ¨åˆ†ç»„å¯¹è±¡ä¸Šè°ƒç”¨`groups`ä¼šè¿”å›æ¯ä¸ªç»„çš„ç´¢å¼•åˆ—è¡¨(å› ä¸ºæ¯ä¸€è¡Œéƒ½å¯ä»¥é€šè¿‡å…¶ç´¢å¼•å”¯ä¸€åœ°æ ‡è¯†)

```
**IN:** grouped.groups**OUT:** {
**'Aaron Hendrickson'**: Int64Index(
[25612, 25613, 25614, 25615, 25616, 25617, 25618, 25619, 25620, 25621,..., 25894, 25895, 25896, 25897, 25898, 25899, 25900, 25901, 25902, 25903], dtype='int64', length=292
),**'Adam Sawyer'**: Int64Index(
[67140, 67141, 67142, 67143, 67144, 67145, 67146, 67147, 67148, 67149, ..., 67454, 67455, 67456, 67457, 67458, 67459, 67460, 67461, 67462, 67463], dtype='int64', length=324
),**...****'Yvonne Lindsey'**: Int64Index([20384, 20385, 20386, 20387, 20388, 20389, 20390, 20391, 20392, 20393, 20394, 20395, 20396, 20397, 20398, 20399, 20400, 20401, ... , 20447, 20448, 20449, 20450], dtype='int64', length=67)
}
```

## é€‰æ‹©ç‰¹å®šçš„ç»„

é€šè¿‡ç”¨ç»„åè°ƒç”¨`get_group`ï¼Œæˆ‘ä»¬å¯ä»¥è¿”å›å„è‡ªçš„æ•°æ®å­é›†ã€‚

```
grouped.get_group('Aaron Hendrickson')
```

![](img/f1109d380358bccb613aa9e256402704.png)

â€˜Aaron Hendricksonâ€™ group

ä¸ºäº†æ¼”ç¤ºä¸€äº›é«˜çº§åˆ†ç»„åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†é€šè¿‡`size`æ–¹æ³•ä½¿ç”¨ apply æ­¥éª¤çš„æœ€ç®€å•ç‰ˆæœ¬(å¹¶è®¡ç®—æ¯ä¸ªç»„ä¸­çš„è¡Œæ•°)ã€‚æˆ‘ä»¬è¿™æ ·åšæ˜¯ä¸ºäº†å°†é‡ç‚¹æ”¾åœ¨ groupby æ“ä½œä¸Šã€‚

æˆ‘ä»¬å°†åœ¨æ–‡ç« çš„ç¬¬ 2 éƒ¨åˆ†æ›´è¯¦ç»†åœ°è®¨è®ºåº”ç”¨æ–¹æ³•ã€‚

## è®¡ç®—æ¯ç»„ä¸­çš„è¡Œæ•°

```
**IN:** grouped.size()**OUT:** Sales Rep
Aaron Hendrickson    292
Adam Sawyer          324
Adele Kimmel         115
Adrian Daugherty     369
Adrianna Shelton      37
                    ... 
Willie Lin            44
Willie Rau            95
Willie Sanchez       309
Yvonne Jones          74
Yvonne Lindsey        67
Length: 499, dtype: int64
```

## æŒ‰é”€å”®ä»£è¡¨çš„åå­—åˆ†ç»„

ä¸‹é¢æ˜¯ç¬¬ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬æ ¹æ®ä¸€ä¸ªç°æœ‰åˆ—çš„å˜åŒ–è¿›è¡Œåˆ†ç»„ã€‚æˆ‘å‘ç°è¿™æ¯”ä¸€ç›´åˆ›å»ºåŠ©æ‰‹åˆ—æœ‰äº†å¾ˆå¤§çš„æ”¹è¿›ã€‚å®ƒåªæ˜¯è®©æ•°æ®æ›´å¹²å‡€ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªå­—ç¬¦ä¸²è®¿é—®å™¨æ¥æ£€ç´¢åå­—ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»è®¿é—®å™¨ã€‚

```
**IN:**
df.groupby(
  df['Sales Rep'].str.split(' ').str[0]
).size()**OUT:** Sales Rep
Aaron        292
Adam         324
Adele        115
Adrian       369
Adrianna      37
            ... 
Wesley       144
Wilbert      213
William     1393 ***# Plenty of Williams***
Willie       448
Yvonne       141
Length: 318, dtype: int64
```

## æ ¹æ®ä»£è¡¨å§“åä¸­æ˜¯å¦æœ‰â€œWilliamâ€è¿›è¡Œåˆ†ç»„

æˆ‘ä»¬çœ‹åˆ°ä¼¼ä¹æœ‰å¾ˆå¤š Williamsï¼Œè®©æˆ‘ä»¬å°†æ‰€æœ‰åå­—ä¸­æœ‰ William çš„é”€å”®ä»£è¡¨åˆ†ç»„ã€‚

```
**IN:** df.groupby(
  df['Sales Rep'].apply(lambda x: 'William' in x)
).size()**OUT:** Sales Rep
False    97111
True      2889
dtype: int64
```

## æŒ‰éšæœºç³»åˆ—åˆ†ç»„(ä»…ä¾›è¯´æ˜)

ä¸å¯å¦è®¤ï¼Œè¿™ä¸ªä¾‹å­å¾ˆå‚»ï¼Œä½†æ˜¯å®ƒè¯´æ˜äº†ä½ å¯ä»¥å¾ˆå¥½åœ°æŒ‰ç…§ä»»æ„åºåˆ—è¿›è¡Œåˆ†ç»„ã€‚

```
**IN:** df.groupby(
    pd.Series(np.random.choice(list('ABCDG'),len(df)))
).size()**OUT:** A    19895
B    20114
C    19894
D    20108
G    19989
dtype: int64
```

## é€šè¿‡ä¸‰ä¸ªå‡åŒ€åˆ‡å‰²çš„â€œValâ€æ¡¶è¿›è¡Œåˆ†ç»„

åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°†`qcut`åº”ç”¨äºä¸€ä¸ªæ•°å­—åˆ—ã€‚`qcut`å°†æ•°æ®å¹³å‡åˆ†é…åˆ°å›ºå®šæ•°é‡çš„ç®±ä¸­ã€‚

```
**IN:**
df.groupby(
  pd.qcut(
    x=df['Val'],
    q=3,
    labels=['low','mid','high']
  )
).size()**OUT:** Val
low     33339
mid     33336
high    33325
dtype: int64
```

## æŒ‰å®šåˆ¶å¤§å°çš„â€œæœ‰å€¼â€å­˜å‚¨æ¡¶åˆ†ç»„

åƒå‰é¢çš„ä¾‹å­ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†é…åˆ°æ¡¶ä¸­ã€‚ç„¶è€Œï¼Œè¿™ä¸€æ¬¡æˆ‘ä»¬ä¹ŸæŒ‡å®šäº†å®¹å™¨çš„è¾¹ç•Œã€‚

```
**IN:**
df.groupby(
  pd.cut(
    df['Val'],
    [0,3000,5000,7000,10000]
  )
).size()**OUT:** Val
(0, 3000]        29220
(3000, 5000]     19892
(5000, 7000]     20359
(7000, 10000]    30529
dtype: int64
```

# `pd.Grouper`

`**pd.Grouper**`é‡è¦ï¼è¿™ä¸ªæ–¹æ³•èŠ±äº†æˆ‘å¾ˆé•¿æ—¶é—´æ‰å­¦ä¼šï¼Œå› ä¸ºå®ƒåœ¨å¤„ç†æ—¶é—´åºåˆ—æ•°æ®æ—¶éå¸¸æœ‰ç”¨ã€‚

## æŒ‰å¹´ä»½åˆ†ç»„

åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨`pd.Grouper(key=<INPUT COLUMN>, freq=<DESIRED FREQUENCY>)`æ ¹æ®æŒ‡å®šåˆ—çš„æŒ‡å®šé¢‘ç‡å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œé¢‘ç‡æ˜¯`'Y'`ï¼Œç›¸å…³åˆ—æ˜¯`'Date'`ã€‚

```
**IN:**
df.groupby(
  pd.Grouper(
    key='Date',
freq='Y')
).size()**OUT:** Date
2014-12-31    19956
2015-12-31    20054
2016-12-31    20133
2017-12-31    20079
2018-12-31    19778
Freq: A-DEC, dtype: int64
```

## æŒ‰å››åˆ†ä¹‹ä¸€æˆ–å…¶ä»–é¢‘ç‡åˆ†ç»„

ä»£æ›¿`'Y'`ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä¸åŒçš„æ ‡å‡†é¢‘ç‡ï¼Œå¦‚`'D','W','M', or 'Q'`ã€‚æœ‰å…³ä¸å¸¸ç”¨é¢‘ç‡çš„åˆ—è¡¨ï¼Œè¯·æŸ¥é˜…[æ–‡æ¡£](http://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases)ã€‚
æˆ‘å‘ç°`'SM'`çš„åŠæœˆæœ«é¢‘ç‡(15 æ—¥å’Œæœˆæœ«)æ˜¯ä¸€ä¸ªæœ‰è¶£çš„é¢‘ç‡ã€‚

```
**IN:**
df.groupby(pd.Grouper(key='Date',freq='Q')).size()**OUT:** Date
2014-03-31    4949
2014-06-30    4948
2014-09-30    4986
2014-12-31    5073
2015-03-31    4958
2015-06-30    4968
2015-09-30    5109
2015-12-31    5019
2016-03-31    5064
2016-06-30    4892
2016-09-30    5148
2016-12-31    5029
2017-03-31    4959
2017-06-30    5102
2017-09-30    5077
2017-12-31    4941
2018-03-31    4889
2018-06-30    4939
2018-09-30    4975
2018-12-31    4975
Freq: Q-DEC, dtype: int64
```

## æŒ‰å¤šåˆ—åˆ†ç»„

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªæŒ‰ä¸€ä¸ªåˆ—æˆ–è½¬æ¢è¿›è¡Œäº†åˆ†ç»„ã€‚å½“æˆ‘ä»¬æƒ³è¦é€šè¿‡å¤šä¸ªåˆ—æˆ–è½¬æ¢è¿›è¡Œåˆ†ç»„æ—¶ï¼ŒåŒæ ·çš„é€»è¾‘ä¹Ÿé€‚ç”¨ã€‚æˆ‘ä»¬è¦åšçš„å°±æ˜¯ä¼ é€’ä¸€ä¸ªåˆ—è¡¨ç»™`groupby`ã€‚

```
**IN:**
df.groupby(['Sales Rep','Company Name']).size()**OUT:** Sales Rep          Company Name               
Aaron Hendrickson  6-Foot Homosexuals             20
                   63D House'S                    27
                   Angular Liberalism             28
                   Boon Blish'S                   18
                   Business-Like Structures       21
                                                  ..
Yvonne Jones       Entry-Limiting Westinghouse    20
                   Intractable Fairgoers          18
                   Smarter Java                   17
Yvonne Lindsey     Meretricious Fabrication       28
                   Shrill Co-Op                   39
Length: 4619, dtype: int64
```

éšæœºçš„åå­—ï¼Œæˆ‘å‘èª“ï¼

# â‘¢åº”ç”¨å¹¶ç»„åˆâ€” `apply`ã€`agg(regate)`ã€`transform`å’Œ`filter`

![](img/cdbc2734991f15300c0723ace0233ea1.png)

Photo by [andrew welch](https://unsplash.com/@andrewwelch3?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/glue-together?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)

åœ¨å‰é¢çš„ç« èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¦‚ä½•æ ¹æ®å„ç§æ¡ä»¶å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„ã€‚æœ¬èŠ‚è®¨è®ºåœ¨å°†å®ƒä»¬ç»„åˆæˆæœ€ç»ˆç»“æœä¹‹å‰ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨äºè¿™äº›ç»„çš„å¯ç”¨å‡½æ•°ã€‚

## æœ¬èŠ‚å›´ç»•å¦‚ä½•ä½¿ç”¨

ğŸ… `apply`ã€
ğŸ…‘ `agg(regate)`ã€
ğŸ…’ `transform`ã€
ğŸ…“ `filter`

å¦‚æœä½ åœ¨æˆ‘å¼€å§‹ä½¿ç”¨ groupby æ—¶å’Œæˆ‘ä¸€æ ·ï¼Œä½ å¯èƒ½æ­£åœ¨ä½¿ç”¨ğŸ…å’ŒğŸ…‘çš„ç»„åˆï¼Œå¤§è‡´å¦‚ä¸‹:

```
grouped = df.groupby('GROUP') and then:
- group.apply(mean)
- group.agg(mean)
- group['INTERSTING COLUMN'].apply(mean)
- group.agg({'INTERSTING COLUMN':mean})
- group.mean()
```

å…¶ä¸­`mean`ä¹Ÿå¯ä»¥æ˜¯å¦ä¸€ä¸ªå‡½æ•°ã€‚

å¥½æ¶ˆæ¯æ˜¯ï¼Œå®ƒä»¬å…¨éƒ½æœ‰æ•ˆã€‚å¤§å¤šæ•°æ—¶å€™ï¼Œç»“æœä¼šå’Œä½ é¢„æœŸçš„å·®ä¸å¤šã€‚

åæ¶ˆæ¯:apply å’Œ agg éƒ½æœ‰å€¼å¾—æ·±ç©¶çš„ç»†å¾®å·®åˆ«ã€‚

æ­¤å¤–ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œä¸¤ä¸ªé²œä¸ºäººçŸ¥çš„å¼ºå¤§åŠŸèƒ½å¯ä»¥ç”¨åœ¨ä¸€ä¸ªåˆ†ç»„å¯¹è±¡ä¸Šï¼Œ`filter`å’Œ`transform`ã€‚

## ğŸ…Â·é˜¿æ™®ç”³:è®©æˆ‘ä»¬æŠŠé˜¿æ™®ç”³å¼„æ¸…æ¥š

Apply æœ‰ç‚¹ä»¤äººå›°æƒ‘ï¼Œå› ä¸ºæˆ‘ä»¬ç»å¸¸è°ˆè®ºåº”ç”¨å‡½æ•°ï¼Œè€Œå®é™…ä¸Šä¹Ÿæœ‰åº”ç”¨å‡½æ•°ã€‚ä½†æ˜¯è¯·åŸè°…æˆ‘ã€‚`apply`å‡½æ•°æ²¿æ•°æ®å¸§çš„è½´åº”ç”¨ä¸€ä¸ªå‡½æ•°ã€‚åº”ç”¨ç¨‹åºå¯ä»¥æ˜¯æŒ‰åˆ—æˆ–æŒ‰è¡Œçš„ã€‚
`apply`ä¸¥æ ¼æ¥è¯´å¹¶ä¸æ˜¯ä¸€ä¸ªåªèƒ½åœ¨ groupby ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨çš„å‡½æ•°ã€‚æ‚¨è¿˜å¯ä»¥åœ¨å®Œæ•´çš„æ•°æ®å¸§ä¸Šä½¿ç”¨`apply`ï¼Œå¦‚ä¸‹ä¾‹æ‰€ç¤º(æˆ‘ä»¬ä½¿ç”¨`_`ä½œä¸ºæŠ›å¼ƒå˜é‡)ã€‚

```
_ = pd.DataFrame(
    np.random.random((2,6)),
    columns=list('ABCDEF')
)
_
```

![](img/8fe5bf9ff447fb4146a56b667d9edd15.png)

Random DataFrame with six columns

```
**IN:**
_.apply(sum, axis=0) ***# axis=0 is default, so you could drop it*****OUT:** A    0.620289
B    0.818850
C    0.672706
D    1.269064
E    1.156606
F    0.934941
dtype: float64**IN:** _.apply(sum, axis=1)**OUT:** 0    2.868145
1    2.604311
dtype: float64
```

ä½†æ˜¯`apply`ä¹Ÿå¯ä»¥ç”¨åœ¨ groupby ä¸Šä¸‹æ–‡ä¸­ã€‚è¿™å¾ˆæœ‰æ„ä¹‰ï¼Œå› ä¸ºæ¯ä¸ªç»„æœ¬èº«å°±æ˜¯ä¸€ä¸ªæ›´å°çš„æ•°æ®æ¡†æ¶ã€‚è¯·è®°ä½ï¼Œè¯¥å‡½æ•°å°†åº”ç”¨äºæ•´ä¸ªæ•°æ®å¸§ã€‚å°†è¯¥å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®å¸§é€šå¸¸æ„å‘³ç€æ‚¨æƒ³è¦é€‰æ‹©ä½¿ç”¨å‡½æ•°çš„åˆ—ã€‚æˆ‘ä»¬å°†æŠŠå®ƒç•™åœ¨ä¸‹é¢çš„ä¸¤ä¸ªä¾‹å­ä¸­ï¼Œè€Œæ˜¯é›†ä¸­åœ¨`agg(regation)`ä¸Šï¼Œè¿™æ˜¯èšåˆç»„çš„â€œé¢„æœŸâ€æ–¹å¼ã€‚

```
**IN:**
df.groupby(
    pd.Grouper(key='Date',freq='Y')
)['Sale'].apply(sum)**OUT:** Date
2014-12-31    3681
2015-12-31    3800
2016-12-31    3881
2017-12-31    3068
2018-12-31    2478
Freq: A-DEC, Name: Sale, dtype: int64**IN:** df.groupby(
    pd.Grouper(key='Date',freq='Y')
)['Val','Sale'].apply(sum)**OUT:** Date       Val       Sale
2014-12-31 100422394 3681
2015-12-31 101724648 3800
2016-12-31 101789642 3881
2017-12-31 101957784 3068
2018-12-31 100399962 2478
```

## ğŸ…‘Â·é˜¿æ ¼(é›·åŠ ç‰¹)

è¯·æ³¨æ„`agg`å’Œ`aggregate`å¯ä»¥äº’æ¢ä½¿ç”¨ã€‚`agg`æ›´çŸ­ï¼Œæ‰€ä»¥æˆ‘å°†ç»§ç»­ä½¿ç”¨å®ƒã€‚

![](img/0b6b87fbb4e09376004f93a22882abc2.png)

Visualization of a typical split-apply-combine process with multiple aggregations functions that are being applied to each group individually

æ€»çš„æ¥è¯´ï¼Œèšåˆæ˜¯å…¶ä¸­æœ€å¼ºå¤§çš„ã€‚è®©æˆ‘ä»¬ä»”ç»†åˆ†æä¸Šé¢çš„å›¾åƒï¼Œä¸»è¦å…³æ³¨è¿™ä¸ªè¿‡ç¨‹çš„å³è¾¹éƒ¨åˆ†ã€‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µåˆ›å»ºäº†ä¸Šå›¾çš„æ”¾å¤§ç‰ˆæœ¬ã€‚

```
df.groupby('Sales Rep').agg({ 
    'Order Id':'size',
    'Val':['sum','mean'],
    'Sale':['sum','mean']
})
```

æˆ‘ä»¬å°†ä¸€ä¸ªå­—å…¸ä¼ é€’ç»™èšåˆå‡½æ•°ï¼Œå…¶ä¸­é”®(å³`Order Id`ã€`Val`ã€`Sale`)æ˜¯åˆ—ï¼Œå€¼(`'size'`ã€`['sum','mean']`ã€`['sum','mean']`)æ˜¯åº”ç”¨äºå„ä¸ªåˆ—çš„å‡½æ•°ã€‚

è¯·æ³¨æ„ï¼Œå‡½æ•°å¯ä»¥æ˜¯å•ä¸ªå‡½æ•°ï¼Œä¹Ÿå¯ä»¥æ˜¯å‡½æ•°åˆ—è¡¨(æ‰€æœ‰å‡½æ•°éƒ½å°†è¢«åº”ç”¨)ã€‚å¦å¤–ï¼Œè¯·æ³¨æ„`agg`å¯ä»¥ä½¿ç”¨å‡½æ•°å(å³å­—ç¬¦ä¸²)æˆ–å®é™…å‡½æ•°(å³ Python å¯¹è±¡)ã€‚åœ¨è¿™é‡Œå¯ä»¥æ‰¾åˆ°åŠŸèƒ½çš„éè¯¦å°½åˆ—è¡¨[ã€‚æˆ‘æœ€å¸¸ç”¨çš„æ˜¯:](https://pandas.pydata.org/pandas-docs/stable/getting_started/basics.html#descriptive-statistics)

*   `**'size'**` **:** ç»Ÿè®¡è¡Œæ•°
*   `**'sum'**` **:** å‘ä¸Šå¯¹åˆ—æ±‚å’Œ
*   `**'mean'/'median'**` **:** åˆ—çš„å¹³å‡å€¼/ä¸­å€¼
*   `**'max'/'min'**` **:** åˆ—çš„æœ€å¤§å€¼/æœ€å°å€¼
*   `**'idxmax'/'idxmin'**` **:** åˆ—çš„æœ€å¤§å€¼/æœ€å°å€¼ç´¢å¼•ã€‚è·å–æœ€å°å€¼æˆ–æœ€å¤§å€¼çš„ç´¢å¼•æœ‰åŠ©äºæ˜ å°„å…¶ä»–åˆ—ï¼Œä¾‹å¦‚ï¼Œæ¯ä¸ªé”€å”®ä»£è¡¨çš„æœ€å¤§äº¤æ˜“çš„å…¬å¸åç§°æ˜¯ä»€ä¹ˆ
*   `**pd.Series.nunique**` **:** ç»Ÿè®¡å”¯ä¸€å€¼ã€‚æ³¨æ„ï¼Œä¸å‰é¢çš„å‡½æ•°ä¸åŒï¼Œè¿™æ˜¯ä¸€ä¸ªå®é™…çš„å‡½æ•°ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚

## è­¦å¯Ÿã€‚NamedAgg

ç°åœ¨ï¼Œå½“ä»¥è¿™ç§æ–¹å¼å°†å¤šä¸ªèšåˆå‡½æ•°åº”ç”¨äºå¤šä¸ªåˆ—æ—¶ï¼Œä¸€ä¸ªé—®é¢˜æ˜¯ç»“æœä¼šå˜å¾—æœ‰ç‚¹æ··ä¹±ï¼Œå¹¶ä¸”æ— æ³•æ§åˆ¶åˆ—åã€‚åœ¨è¿‡å»ï¼Œæˆ‘ç»å¸¸å‘ç°è‡ªå·±èšé›†äº†ä¸€ä¸ªæ•°æ®å¸§ï¼Œç„¶åç›´æ¥é‡å‘½åç»“æœã€‚æˆ‘æ€»è§‰å¾—é‚£æœ‰ç‚¹ä½æ•ˆã€‚

åƒè¿™æ ·çš„æƒ…å†µæ­£æ˜¯`pd.NamedAgg`æ´¾ä¸Šç”¨åœºçš„æ—¶å€™ã€‚`pd.NamedAgg`æ˜¯åœ¨ Pandas ç‰ˆæœ¬ 0.25 ä¸­å¼•å…¥çš„ï¼Œå…è®¸æŒ‡å®šç›®æ ‡åˆ—çš„åç§°ã€‚

```
def cr(x):
    return round(np.mean(x),2)**# Long Form: Explictly specifying the NamedAgg**
aggregation = {
    'Potential Sales': pd.NamedAgg(column='Val', aggfunc='size'),
    'Sales': pd.NamedAgg(column='Sale', aggfunc='sum'),
    'Conversion Rate': pd.NamedAgg(column='Sale', aggfunc=cr)
}**# Alternative: Since the NamedAgg is just a tuple, we can also pass regular tuples**
aggregation = {
    'Potential Sales': ('Val','size'),
    'Sales': ('Sale','sum'),
    'Conversion Rate': ('Sale',cr)
}df.groupby('Sales Rep').agg(**aggregation)
```

è¿è¡Œä¸Šé¢çš„ä»£ç ç‰‡æ®µä¼šå¯¼è‡´:

![](img/054853fb594b487be7cef1bde39cbe9d.png)

Result of aggregation with built-in renaming of columns

## ğŸ…’ `transform`

![](img/a4877ef7dbf3baf368e88ff0c57e4205.png)

Visualization of a typical split-apply-combine process with transform being applied to the â€˜Valâ€™ column. Transform sums up the column on a group level and assigns the summed value back to every row.

agg è¿”å›è¾“å…¥çš„ç®€åŒ–ç‰ˆæœ¬ï¼Œè€Œ transform è¿”å›å®Œæ•´æ•°æ®çš„ç»„çº§è½¬æ¢ç‰ˆæœ¬çš„ã€‚æ–°è¾“å‡ºæ•°æ®çš„é•¿åº¦ä¸è¾“å…¥æ•°æ®çš„é•¿åº¦ç›¸åŒã€‚å¯¹äºæ¥è‡ª SQL çš„ç”¨æˆ·ï¼Œå¯ä»¥å°† transform çœ‹ä½œä¸€ä¸ªçª—å£å‡½æ•°ã€‚

ä¸€ä¸ªå…¸å‹çš„ä¾‹å­æ˜¯é€šè¿‡é™¤ä»¥ç»„é—´æ€»å’Œæ¥è·å¾—ç»„æ€»æ•°çš„ç™¾åˆ†æ¯”ã€‚

```
**IN:**
df.groupby('Sales Rep')['Val'].transform(lambda x: x/sum(x))**OUT:** 0        0.004991
1        0.005693
2        0.003976
3        0.000799
4        0.003300
           ...   
99995    0.012088
99996    0.000711
99997    0.013741
99998    0.010695
99999    0.001533
Name: Val, Length: 100000, dtype: float64
```

ä¸`agg`ä¸åŒï¼Œ`transform`é€šå¸¸é€šè¿‡å°†ç»“æœåˆ†é…ç»™æ–°åˆ—æ¥ä½¿ç”¨ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥:

```
df['%'] = df.groupby('Sales Rep')['Val'].transform(
  lambda x: x/sum(x)
)
```

æŸ¥çœ‹è¿™ç¯‡æ–‡ç« ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨`transform`æ¥åˆ é™¤ä¸¢å¤±çš„å€¼ã€‚

[](/using-pandas-transform-and-apply-to-deal-with-missing-data-on-a-group-level-cb6ccf060531) [## ä½¿ç”¨ Panda çš„â€œè½¬æ¢â€å’Œâ€œåº”ç”¨â€åœ¨ç»„çº§åˆ«å¤„ç†ç¼ºå¤±æ•°æ®

### äº†è§£å½“æ‚¨ä¸æƒ³ç®€å•åœ°ä¸¢å¼ƒä¸¢å¤±çš„æ•°æ®æ—¶åº”è¯¥æ€ä¹ˆåšã€‚

towardsdatascience.com](/using-pandas-transform-and-apply-to-deal-with-missing-data-on-a-group-level-cb6ccf060531) 

## ğŸ…“ `filter`

![](img/7b845699e119eacb31337a946009ec39.png)

Visualization of a typical split-apply-combine process with the filter being applied to the â€˜Saleâ€™ column. The specified filter is used on a group level and will only leave groups with at least one Sale.

é¡¾åæ€ä¹‰ï¼ŒFilter ä¸ä»¥ä»»ä½•æ–¹å¼æ”¹å˜æ•°æ®ï¼Œè€Œæ˜¯é€‰æ‹©æ•°æ®çš„å­é›†ã€‚å¯¹äºæ¥è‡ª SQL çš„ç”¨æˆ·ï¼Œå¯ä»¥æŠŠ filter çœ‹ä½œ HAVING æ¡ä»¶ã€‚

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ç­›é€‰æ‰€æœ‰è‡³å°‘èµšäº† 20 ä¸‡è‹±é•‘çš„é”€å”®ä»£è¡¨

```
**IN:** df.groupby('Sales Rep').filter(
  lambda x: (x['Val'] * x['Sale']).sum() > 200000
)
```

![](img/7a2690e5a103e6e3906b48e8ed8f6626.png)

Filtered DataFrame â€” Condition: Realized Sales > 200k

æˆ–è€…è½¬åŒ–ç‡> 30%çš„æ‰€æœ‰é”€å”®ä»£è¡¨:

```
**IN:**
**# Let's add this for verification**
df['cr'] = df.groupby('Sales Rep')['Sale'].transform('mean')df.groupby('Sales Rep').filter(lambda x: x['Sale'].mean() > .3)
```

![](img/418e12612a9afa05784da9fbf86f0035.png)

# ç»“æŸè¯­

åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å­¦ä¹ äº†å¦‚ä½•åƒçœŸæ­£çš„ç†ŠçŒ«ä¸“å®¶ä¸€æ ·å¯¹æ•°æ®å¸§è¿›è¡Œåˆ†ç»„ã€‚æ‚¨å­¦ä¹ äº†è®¸å¤šå¯¹æ•°æ®è¿›è¡Œåˆ†ç»„çš„æ–¹æ³•ã€‚ä½ å­¦ä¼šäº†åŒºåˆ†`apply`å’Œ`agg`ã€‚æ‚¨å­¦ä¹ å¹¶åº”ç”¨äº†æœ€å¸¸è§çš„èšåˆå‡½æ•°ã€‚ä½ å·²ç»çœ‹åˆ°äº†ä¸å¸¸ç”¨çš„`transform`å’Œ`filter`æ´¾ä¸Šäº†ç”¨åœºã€‚

æƒ³èŠå¤©è¯·ç™»é™† [LinkedIn](https://www.linkedin.com/in/fbosler/) ï¼æˆ‘å¾ˆä¹æ„å’Œä½ è°ˆè°ˆï¼Œæˆ–è€…å›ç­”ä½ å¯èƒ½æœ‰çš„ä»»ä½•é—®é¢˜ã€‚

å¦å¤–ï¼Œçœ‹çœ‹æˆ‘åœ¨ Medium ä¸Šå†™çš„å…¶ä»–æ–‡ç« 

å¹²å¾—å¥½ï¼Œæ„Ÿè°¢æ‚¨çš„é˜…è¯»ï¼