<html>
<head>
<title>Quora Insincere Questions Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora 虚假问题分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quora-insincere-questions-classification-d5a655370c47?source=collection_archive---------14-----------------------#2019-08-29">https://towardsdatascience.com/quora-insincere-questions-classification-d5a655370c47?source=collection_archive---------14-----------------------#2019-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cfd7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Quora 举办的 Kaggle 竞赛案例研究。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a5ad9c416c007a2945f708829b9a2145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0ti42jifCRRfzEutLK7Yg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Logo (<a class="ae ky" href="https://cdn.vox-cdn.com/thumbor/0tiE44ataBOx8u8Iy_Q3e0fI2L0=/0x0:2182x1455/1200x800/filters:focal(917x554:1265x902)/cdn.vox-cdn.com/uploads/chorus_image/image/62619674/quora.0.jpg" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="88c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Quora 是一个让人们能够互相学习的平台。在 Quora 上，人们可以提出问题，并与贡献独特见解和高质量答案的其他人联系。一个关键的挑战是剔除不真诚的问题——那些建立在错误前提上的问题，或者那些旨在陈述而不是寻找有用答案的问题。</p><p id="81ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Quora 无诚意问题分类是 quora 主办的第二次 kaggle 竞赛，目的是开发更多可扩展的方法来检测他们平台上的有毒和误导内容。</p><h2 id="7be6" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">问题陈述</strong></h2><blockquote class="mo"><p id="61fd" class="mp mq it bd mr ms mt mu mv mw mx lu dk translated">检测有毒内容以改善在线对话。</p></blockquote><h1 id="a6c4" class="my lw it bd lx mz na nb ma nc nd ne md jz nf ka mg kc ng kd mj kf nh kg mm ni bi translated">数据概述</h1><p id="9bc5" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">数据集包含超过 1，300，000 个已标记示例的训练集和超过 300，000 个未标记示例的测试集。训练集中的每个示例都有一个唯一的 id、问题的文本以及一个“0”或“1”的标签来表示“真诚”或“不真诚”。</p><h2 id="d31e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">什么是不真诚的问题？</strong></h2><p id="c6e2" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">一个不真诚的问题被定义为一个旨在陈述而不是寻找有用答案的问题。</p><ul class=""><li id="c442" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">有一种非神经的音调。</li><li id="3f78" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">是贬低或煽动性的。</li><li id="a42b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">并不基于现实。</li><li id="39c1" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用性内容(乱伦，兽交和恋童癖)来寻求震撼，而不是寻求真正的答案。</li></ul><p id="35ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这是一场内核竞赛，外部数据源是不允许的。我们必须提交 kaggle 内核(笔记本或脚本),并按照提交要求中提到的特定格式提交所有代码和输出预测。</p><p id="1491" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了问题的文本数据，quora 还提供了 4 个不同的嵌入文件，这些文件都是在大型数据语料库上训练出来的，可以在模型中使用。给定的嵌入文件如下:</p><ul class=""><li id="421f" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><a class="ae ky" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">谷歌新闻—矢量</strong> </a></li><li id="0917" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">手套</strong> </a></li><li id="a08b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="https://cogcomp.org/page/resource_view/106" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">段落</strong> </a></li><li id="abc0" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="https://fasttext.cc/docs/en/english-vectors.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">维基新闻</strong> </a></li></ul><p id="0b4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这些嵌入文件的每一个中，单词被表示为 300 个 dim 向量。单词的这种表示将允许捕捉单词的语义。具有相同含义的单词将具有相似的矢量表示。</p><h1 id="373a" class="my lw it bd lx mz na nb ma nc nd ne md jz oc ka mg kc od kd mj kf oe kg mm ni bi translated">机器学习问题</h1><blockquote class="of og oh"><p id="5c79" class="kz la oi lb b lc ld ju le lf lg jx lh oj lj lk ll ok ln lo lp ol lr ls lt lu im bi translated">这是一个二元分类问题，对于一个给定的问题，我们需要预测它是否是一个不真诚的问题。</p></blockquote><h2 id="a360" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">公制的</h2><p id="fb5c" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">本次比赛的评估指标是<a class="ae ky" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank">F1-得分</a>，这是精确度和召回率的调和平均值。精确度是实际上不真诚的分类不真诚问题的百分比，而回忆是被正确分类为不真诚的实际不真诚问题的百分比。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3c7889c7b0b76455422dff3420c744ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*xh_3AJTtFuxUkf5qZYb1qg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">F1-Score (<a class="ae ky" href="https://i.stack.imgur.com/U0hjG.png" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="a1ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们需要在精确度和召回率之间寻求平衡，F1 分数是一个更好的指标。当存在不均匀的类别分布时，这也优于准确性。</p><h1 id="58a7" class="my lw it bd lx mz na nb ma nc nd ne md jz oc ka mg kc od kd mj kf oe kg mm ni bi translated"><strong class="ak">探索性数据分析</strong></h1><p id="d051" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在本节中，我们将探索和可视化给定的数据集。</p><p id="59ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们加载训练和测试数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/abb3334219f12448f350c7bc0b4b54dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*jBZKEOBVVz_Z-7STYH73oQ.png"/></div></figure><p id="78d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练数据集包含列:qid、question_text 和 target(二进制值)。所有观察值都是唯一的，不含空值。</p><h2 id="762e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">训练集分布图</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/bc48f9875f9d7aca52432d99f9099885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oohkS6nJujpoIIoxboirtQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Data Distribution</figcaption></figure><ul class=""><li id="dda6" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">数据集高度不平衡，只有 6.2%的不真诚问题。</li><li id="63a6" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">由于数据不平衡，F1-Score 似乎是比准确性更正确的选择。</li></ul><h2 id="e606" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">言云对于真诚和不真诚的问题</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/a058225f9c7b14374a523172eeaf17f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elpVWq7vCFtafM0Jlr7x3Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Word cloud for sincere questions</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/091442ab16878307aac307efcd8fed8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9uEhGAqvFcDcE_dAPObWg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Word cloud for insincere questions</figcaption></figure><ul class=""><li id="970a" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">正如我们所看到的，不真诚的问题包含了许多冒犯性的词语。</li><li id="7932" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">在提供的训练集中，大部分不真诚的问题都与<em class="oi">人</em>、<em class="oi">穆斯林</em>、<em class="oi">女</em>、<em class="oi">川普</em>等有关。</li></ul><h2 id="888e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">特征工程(清洗前)</strong></h2><p id="4571" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">让我们构建一些基本特征，例如:</p><ul class=""><li id="4ec5" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">字数</li><li id="b60c" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">大写字母的数量</li><li id="4faf" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">特殊字符的数量</li><li id="9960" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">唯一单词的数量</li><li id="23f6" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">数字的个数</li><li id="8776" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">字符数</li><li id="2644" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">停用词的数量</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="b238" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">生成特征的方框图</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/d93caba8bf01033b1c436b100b8c5234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHFqoBLQG4YQs6OaV9yTlQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Box plots for generated features</figcaption></figure><ul class=""><li id="5ffe" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">言不由衷的问题似乎更有文字和性格。</li><li id="5209" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">不真诚的问题也比真诚的问题有更多独特的用词。</li></ul><h1 id="e5b3" class="my lw it bd lx mz na nb ma nc nd ne md jz oc ka mg kc od kd mj kf oe kg mm ni bi translated">基于机器学习的方法</h1><h2 id="2587" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">数据预处理和清洗</strong></h2><p id="8b80" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">我遵循了用于解决基于 NLP 的任务的标准数据预处理技术，如拼写纠正、删除停用词、标点符号和其他标签，然后是词条化。</p><blockquote class="of og oh"><p id="c551" class="kz la oi lb b lc ld ju le lf lg jx lh oj lj lk ll ok ln lo lp ol lr ls lt lu im bi translated"><strong class="lb iu">词条化</strong>是使用已知的单词词典将单词替换为其词根的过程。</p></blockquote><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="9458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于数据预处理和其他 NLP 方法的更多细节可以在这个博客中找到。</p><h2 id="d2c9" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">TF-IDF</h2><p id="d16d" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">它代表词频——逆文档频率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/6c415c83c8b546ffaac032eaaa1d536f.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*ngwELQ1rLWM5dlIX.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">TF-IDF (<a class="ae ky" href="https://diggitymarketing.com/wp-content/uploads/2018/10/equation.png" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><blockquote class="of og oh"><p id="e5af" class="kz la oi lb b lc ld ju le lf lg jx lh oj lj lk ll ok ln lo lp ol lr ls lt lu im bi translated">词频:在文档中找到一个词的概率。</p><p id="a40f" class="kz la oi lb b lc ld ju le lf lg jx lh oj lj lk ll ok ln lo lp ol lr ls lt lu im bi translated">逆文档频率:定义单词在整个语料库中的独特性。</p></blockquote><p id="18f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF 是 TF 和 IDF 值的乘积。它给予在文档中出现较多而在语料库中出现较少的单词更大的权重。</p><h2 id="ab89" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">逻辑回归</strong></h2><p id="3a6e" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">逻辑回归通过使用 sigmoid 函数估计概率来测量分类因变量和一个或多个自变量之间的关系。</p><p id="f52c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用逻辑回归作为一个简单的基线模型，问题的文本 TF-IDF 编码加上基本的手工制作的统计特征给出了 0.58 的 F1 值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2258400f94b50e89ed7b851edc6026e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*cy9Ku74w2SsLruucbLyg6Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Confusion matrix for logistic regression</figcaption></figure><h1 id="d66b" class="my lw it bd lx mz na nb ma nc nd ne md jz oc ka mg kc od kd mj kf oe kg mm ni bi translated"><strong class="ak">基于深度学习的方法</strong></h1><p id="2ff9" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">与基于机器学习的方法不同，这里我没有删除停用词，而是允许模型学习它们，并且还在标点符号之间添加了空间，以便找到更多的单词嵌入。</p><p id="999b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词嵌入是使用密集矢量表示来表示单词的一种形式。我已经使用给定的预训练单词嵌入文件来创建嵌入矩阵。</p><p id="08e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我还使用了 spacy tokenizer 来构建单词和词条词典。这两个字典用于创建嵌入矩阵。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="d849" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了增加嵌入覆盖率，我使用了词干、词汇化、大写、小写、大写以及使用拼写检查器嵌入最近的单词的组合来获得 vocab 中所有单词的嵌入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="c5f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用 glove 和 para 嵌入文件创建了两个独立的嵌入矩阵。最后，对它们进行加权平均，赋予手套更高的权重。</p><p id="1aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一步将创建一个大小为(vocab 中的单词数，300)的嵌入矩阵，其中每个单词被表示为一个 300 维向量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="a7de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建嵌入矩阵后，构建三个不同模型架构的集合，以捕获数据集的不同方面，从而提高总体 F1 分数。这些模型如下:</p><h2 id="4489" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模式 1:双向 RNN(LSTM/GRU)</h2><p id="b6fa" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">递归神经网络(RNN)是一种神经网络，其中来自先前步骤的输出作为输入被馈送到当前步骤，从而记住关于序列的一些信息。它有局限性，比如很难记住更长的序列。</p><p id="4182" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTM/GRU 是 RNN 的改进版本，专门使用门控机制长时间记忆信息，而 RNN 没有做到这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/0e0474b48362fdca7bee44a7e3d7b205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q0uzj4QO5OQADSSX.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Bidirectional RNN (<a class="ae ky" href="https://miro.medium.com/max/1200/1*6QnPUSv_t9BY9Fv8_aLb-Q.png" rel="noopener">Source</a>)</figcaption></figure><p id="8688" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单向 RNN 只保存过去的信息，因为它看到的输入来自过去。使用双向将会以两种方式运行输入，一种是从过去到未来，另一种是从未来到过去，从而允许它在任何时间点保存来自过去和未来的上下文信息。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="831d" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型 2:具有注意层的双向 GRU</h2><p id="a4dc" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">模型 2 由双向 GRU、注意力和池层组成。</p><p id="f0a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">神经网络中的注意力(非常)松散地基于在人类中发现的视觉注意力机制。人类的视觉注意力能够以“高分辨率”聚焦在图像的某个区域，而以“低分辨率”感知周围的图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/10872eb8835a4c5273f46005bfd23484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W0wsBo3GdFlvvgdO.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Textual Attention example (<a class="ae ky" href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="b403" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意力用于机器翻译、语音识别、推理、图像字幕、摘要和对物体的视觉识别。</p><p id="4636" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意力加权平均</strong></p><p id="f115" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意层使用加权机制，其中序列中的所有单词都被赋予一个权重(注意值)。这个权重定义了一个单词在序列中的重要性，从而捕获重要的单词并给予序列中的重要部分更多的关注。这些重量是可以训练的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/fd81945ad2f2b171c89539e1e486e84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v8AfZzlnrNQu9Eag"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Bidirectional LSTM model with Attention (Source)</figcaption></figure><p id="733e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意层将让网络顺序地聚焦于输入的子集，处理它，然后改变它的焦点到输入的一些其他部分。</p><p id="1bdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关注层的 keras 代码更深入的解释可以在<a class="ae ky" href="https://docs.google.com/document/d/1UFDS036YBK5-EN-fQ6exn0GRmJc1ibU2gWaokb1jcKM/edit#heading=h.voemotsi64h2" rel="noopener ugc nofollow" target="_blank">这个</a>文档中找到。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="df19" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型 3:具有卷积层的双向 LSTM</h2><p id="f0df" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">模型 3 由双向 LSTM、卷积层和池层组成。</p><p id="8cea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 NLP 中使用 CNN 的想法是利用它们提取特征的能力。CNN 被用于嵌入给定句子的向量，希望它们能够提取有用的特征(如短语和句子中更接近的单词之间的关系)，这些特征可用于文本分类。</p><p id="d0d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与传统 CNN 不同，NLP CNN 通常由 3 层或更多层 1D 卷积层和汇集层组成。这有助于减少文本的维度，并作为排序的总结，然后提供给一系列密集的层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/33a8760e7e184dcc3decd8c89dfe1e39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gnxy8qBMqLuiYtvn.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">CNN for textual data (<a class="ae ky" href="https://mlwhiz.com/blog/2019/03/09/deeplearning_architectures_text_classification/" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="d7ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算机视觉任务中，CNN 中使用的过滤器滑过图像的碎片，而在 NLP 任务中，过滤器滑过句子矩阵，一次几个单词。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><h2 id="6935" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">阿达姆</h2><p id="82a7" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">对于所有这三个模型，使用定制的 Adam 优化器，称为 AdamW(权重衰减),它修复 Adam 中的权重衰减正则化。</p><p id="b2cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank">这篇</a>论文指出，所有流行的深度学习框架(Tensorflow，Pytorch)都实现了权重衰减错误的 Adam。他们提出了以下意见:</p><ul class=""><li id="9767" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><em class="oi"> L2 的正规化</em>和<em class="oi">权重衰减</em>是不一样的。</li><li id="181a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><em class="oi"> L2 正规化</em>在亚当身上并不有效。</li><li id="3064" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><em class="oi">体重下降</em>对亚当和 SGD 同样有效。</li></ul><p id="d3d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们提出了 AdamW，该 AdamW 解耦了权重衰减和 L2 正则化步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/7f122551f45a6db6b217116dc141bd82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ctnPuRu588DoFgDc.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="pa">Comparison between Adam and AdamW (</em><a class="ae ky" href="https://www.fast.ai/2018/07/02/adam-weight-decay/" rel="noopener ugc nofollow" target="_blank"><em class="pa">Source</em></a><em class="pa">)</em></figcaption></figure><p id="d76f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多关于 AdamW 的细节可以在<a class="ae ky" href="https://www.fast.ai/2018/07/02/adam-weight-decay/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="cbb6" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">培训和提交</h2><p id="8608" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">用分层交叉验证训练三个模型，然后取它们预测的平均值作为最终集合预测。</p><p id="d5fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建具有所有折叠(验证)的“折叠外”列表，然后使用该列表来寻找集合模型的最佳阈值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/84bf4b370fdff6a5fcce9399f2f69162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o9fvXLydNTHwlcKnkDrc1w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Validation score with optimal threshold</figcaption></figure><p id="ba8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，使用测试预测的最优阈值，生成提交文件。</p><h1 id="336c" class="my lw it bd lx mz na nb ma nc nd ne md jz oc ka mg kc od kd mj kf oe kg mm ni bi translated">结论</h1><p id="51c1" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">本次 kaggle 比赛重点是利用机器学习和深度学习方法检测 Quora 平台上的有毒内容。</p><p id="883f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我从 kaggle 论坛和公共解决方案中学到了很多。发现更多的词嵌入和模型集成是提高 F1 分数的关键因素。</p><p id="5312" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该解决方案的 Kaggle 私有和公共评分如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/e06169f4e44252490544325bfae1c84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*LFlS-DNmAF64upSUDO7_wQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Kaggle Final Score</figcaption></figure><p id="4c02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过尝试其他预训练模型，如用于单词嵌入的 BERT，可以实现进一步的改进。</p></div><div class="ab cl pd pe hx pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="im in io ip iq"><p id="8777" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。完整的代码可以在<a class="ae ky" href="https://github.com/ronakvijay/Quora_Insincere_Questions" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="dd4a" class="my lw it bd lx mz na nb ma nc nd ne md jz oc ka mg kc od kd mj kf oe kg mm ni bi translated">参考</h1><div class="pk pl gp gr pm pn"><a href="https://www.kaggle.com/wowfattie/3rd-place" rel="noopener  ugc nofollow" target="_blank"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd iu gy z fp ps fr fs pt fu fw is bi translated">第三名</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">www.kaggle.com</p></div></div></div></a></div><div class="pk pl gp gr pm pn"><a href="https://www.kaggle.com/canming/ensemble-mean-iii-64-36" rel="noopener  ugc nofollow" target="_blank"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd iu gy z fp ps fr fs pt fu fw is bi translated">总体，平均值 III(64/36)</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">www.kaggle.com</p></div></div></div></a></div><div class="pk pl gp gr pm pn"><a href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/" rel="noopener  ugc nofollow" target="_blank"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd iu gy z fp ps fr fs pt fu fw is bi translated">NLP 学习系列:第 1 部分——深度学习的文本预处理方法</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">最近，我在 Kaggle 上发起了一个名为 Quora 问题不真诚挑战的 NLP 竞赛。这是一个 NLP…</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">mlwhiz.com</p></div></div><div class="pw l"><div class="px l py pz qa pw qb ks pn"/></div></div></a></div></div></div>    
</body>
</html>