<html>
<head>
<title>How to Perform Explainable Machine Learning Classification — Without Any Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何执行可解释的机器学习分类——没有任何树</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-perform-explainable-machine-learning-classification-without-any-trees-873db4192c68?source=collection_archive---------6-----------------------#2019-03-06">https://towardsdatascience.com/how-to-perform-explainable-machine-learning-classification-without-any-trees-873db4192c68?source=collection_archive---------6-----------------------#2019-03-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/08f3455e2ffc90d0a598f7399aeb88b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDym7uH_ODrxiKF_h0DXpw.jpeg"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Credit: <a class="ae kc" href="https://pixabay.com/en/users/loggawiggler-15/" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><blockquote class="kd ke kf"><p id="0b22" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">严格而清晰的规则……在我们看来是背景中的东西——隐藏在理解的媒介中。<br/>——路德维希·维特斯坦根</p></blockquote><p id="cfdf" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">决策树是一种流行的分类技术。它们直观、易于理解，并且开箱即用。</p><p id="829b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">树模型是人类可以理解的规则的路径。在某些情况下，能够为你的预测提供一个解释——<em class="ki">原告的贷款申请被拒绝，因为他们正面临破产程序，</em>而不是，<em class="ki">某物——某物——某物——点产品</em>——可能是一个优势。</p><p id="a9ba" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">但是还有另一类规则生成算法，您可能不熟悉，可能值得添加到您的工具集中，称为规则集学习器。</p><p id="f503" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在最近一个通过古典音乐作曲家对音频进行分类的项目中，我的规则集实现击败了 sklearn 的 DecisionTreeClassifier，并匹配了网格搜索优化的 SVD 和随机森林。</p><p id="53c4" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们看看什么时候您可能想要考虑规则集模型，它们如何工作，以及如何在您自己的代码中使用它们——包括我最近开发的一个新的 Python 包，您可以使用它进行分类。</p><h1 id="77b3" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated"><strong class="ak">动机</strong></h1><p id="c9e1" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">尽管决策树有很多优点，但它们因过度拟合、脆弱和难以处理不平衡数据集而臭名昭著。</p><p id="6283" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">树通过从完整的训练集开始学习，并贪婪地添加最大化每个子节点的类纯度的条件。随着我们沿着树向下，每个节点添加一个条件，将我们的训练数据分成越来越小的子组:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/3f6dff65db9b7fca4a72e9b5eb90e3b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sMtFUr8NVC-TDnCBWuCjYA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Decision tree model generated by sklearn on the <a class="ae kc" href="https://archive.ics.uci.edu/ml/datasets/mushroom" rel="noopener ugc nofollow" target="_blank">mushroom dataset</a>.</figcaption></figure><p id="6a31" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在短短的几个决策步骤中，我们已经从具有数千个样本的根发展到样本大小低至 29、11、2 甚至 1 的叶。</p><p id="541e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">怎么会？想想二叉树搜索是如何很好地扩展的，因为树的深度只随着节点数量的对数增长。对于训练决策树来说，另一方面是节点的数量随着深度以<em class="ki">指数</em>的速度增长。因此，我们的训练子集收缩得非常快，最终缺乏统计上有效的样本量。</p><p id="eca5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">出于类似的原因，树是“脆弱的”,因为训练集中的微小变化会改变顶级规则，从而在整个模型中产生连锁反应。</p><p id="dc60" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">有几个流行的技术来处理树木过度拟合和脆弱性。我们可以修剪这棵树，要么根据某个阈值提前停止生长，要么在完成后缩小它的大小。或者我们可以建立一个由数千棵树组成的随机森林集合，以训练速度和可解释性为代价。</p><p id="e51f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">规则集类似于决策树，但是因为它们没有层次结构，有有序的子分支决策，所以它们有可能避开这些缺点。</p><p id="b42b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">规则集学习者也倾向于产生更紧凑的模型。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mq"><img src="../Images/041759ef1cf8c8109d31fddaef23feea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jW4DIRu3i-B_Ozh-kQHEwQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Some major differences between trees and rulesets</figcaption></figure><h1 id="0bf7" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">那么什么是规则集呢？</h1><p id="7776" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">规则集就是合取词(and)的析取(or)。</p><p id="1b5d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">例如，描述您的孩子能否说服您出去吃冰淇淋蛋糕的模型可能是这样的:</p><blockquote class="kd ke kf"><p id="723e" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated"><strong class="kj ir">如果:</strong> <br/>我很好<strong class="kj ir">和</strong>我们有足够的时间去买冰淇淋；<strong class="kj ir">或</strong> <br/>你的韧性被磨薄了<strong class="kj ir">和</strong>你不想和我打交道；<strong class="kj ir">或</strong> <br/>你感到大度，<br/> <strong class="kj ir">然后:</strong> <br/>冰淇淋蛋糕。</p></blockquote><p id="0708" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们可以象征性地表达这个规则集:</p><blockquote class="kd ke kf"><p id="0d66" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">(我很好<strong class="kj ir"> ^ </strong>我们有足够的时间)<strong class="kj ir">v</strong>t23】(筋疲力尽<strong class="kj ir"> ^ </strong>你就是不行)<strong class="kj ir"> V </strong> <br/>(坦荡)</p></blockquote><p id="99d1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">用 python 语言来说:</p><blockquote class="kd ke kf"><p id="3437" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">[[(行为=好)，(时间=真)]，<br/> [(韧性=瘦)，(你=连不上)]，<br/> [(感觉=坦荡)]]</p></blockquote><h1 id="7314" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">如何训练规则集模型</h1><p id="62ba" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">规则集不是通过递归将训练数据分解为子集的子集，而是通过对所有尚未检查的训练数据进行训练来迭代增长。</p><p id="4cb7" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">为了获得训练如何工作的基本味道，让我们首先来看看一个简单的规则集学习算法，称为 IREP。</p><p id="d35a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在将我们的数据分成训练集和测试集之后，我们通过以下步骤在训练集上训练模型:</p><p id="5687" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">1.将你的训练集分成“生长集”和“修剪集”</p><p id="8b43" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">2.增长规则(信息增益)</p><p id="4e2a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">3.删减规则(减少错误度量)</p><p id="0729" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">4.从您的训练集中删除新规则涵盖的示例，并重复步骤 1、2 和 3，直到您开始让事情变得更糟(精度度量)。</p><p id="5fbd" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在步骤 1 中，我们将训练数据随机分成 2/3–1/3。我们将使用第一部分增长规则，第二部分修剪规则。这有点像交叉验证要求我们留出评估折叠，这种分割确保我们不会使用刚刚用于增长的相同数据进行修剪！</p><p id="bba1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">接下来，我们通过贪婪地添加最大化 FOIL <a class="ae kc" rel="noopener" target="_blank" href="/decision-trees-decoded-part-1-23b45f69111c">信息增益</a>的条件来增长规则(下面的公式)。随着我们的规则获得越来越多的条件，它变得越来越严格，排除了越来越多的负类例子。(记住，规则是一组“和”的集合，更多的“和”意味着更严格。)当规则不再涵盖负面例子时，我们就停止了。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b38bc863180cb213af82a1c64b0f6f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*AdD-cfqJXXZvhOFhBsW62Q.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">FOIL information gain. p0 (n0) is the number of positive (negative) examples covered by an existing rule, p1 (n1) the number covered by the proposed new rule.</figcaption></figure><p id="d92b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在是时候修改我们刚刚制定的规则了。我们尝试以相反的顺序贪婪地修剪它的每个条件，选择最大化一些修剪度量的规则，比如这个:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/68ea96180f035f6343f334d6dcc8f60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*-9fB6LN2V6ljJ0uUknhQig.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Pruning criterion. p (n) is the number of positive (negative) examples covered by the rule, P (N) the total number. Different versions use different prune metrics.</figcaption></figure><p id="b02a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们刚刚制定并修改了我们的第一条规则！现在我们迭代。从我们的训练集中，删除新规则覆盖的示例。继续增加新规则——使我们的规则集越来越宽松——直到我们增加一个精确度低于 50%的规则。</p><p id="c4a8" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这是整个过程的可视化:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/13f44ec6aefc6bd4e53bbce8a0cb2549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*E0BAbEkz_GpvWSCfDBCZtw.gif"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Growing-pruning a ruleset using iterative coverage.</figcaption></figure><p id="a365" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我的新规则集包，维特根斯坦，实现了 IREP，以及另一个叫做 RIPPER 的规则集算法。除了听起来像重金属乐队，开膛手仍然是这种技术的艺术状态。</p><p id="8994" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">该算法比 IREP 要复杂得多，但主要区别如下:</p><ul class=""><li id="df5b" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated"><strong class="kj ir">一个理论上更严密的停止条件:</strong>RIPPER 没有使用修剪指标来告诉我们何时停止生成新规则，而是借用了昆兰的<a class="ae kc" href="https://pdfs.semanticscholar.org/cb94/e3d981a5e1901793c6bfedd93ce9cc07885d.pdf" rel="noopener ugc nofollow" target="_blank"> C4.5 决策树算法</a>中使用的一种信息论启发式方法，称为描述长度。这个想法是，我们可以在建模过程的任何阶段测量总复杂性(以比特为单位),即我们的试验性模型的复杂性加上它未能捕获的所有示例的复杂性。随着我们的规则集在长度和精度上的增长，模型的复杂性会增加，而它无法捕获的示例数量的复杂性会降低。为了防止过度拟合，一旦总复杂度超过某个阈值，我们就停止增加规则。描述长度指导最小化训练错误和最小化模型复杂性之间的平衡行为。</li><li id="daad" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated">计算模型的描述长度既复杂又昂贵。但是要点是模型的复杂性是基于<em class="ki">的区别性。条件较多的规则比条件较少的规则更复杂，从更大的可能性池中选择条件的规则也是如此。</em></li></ul><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/37fb2a8519392064630fb6d326c0fbfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*Vb4j30fi9kj7SagQPuwNwg.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">k is the number of conditions in the rule, n the number of possible conditions. r is k/n. ||k|| is the number of bits needed to send k (i.e., log2(k)). The 0.5 factor is to account for possible redundancies.</figcaption></figure><ul class=""><li id="bf72" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated">异常描述长度公式更简单。我们从正面和负面预测中选择假阳性和假阴性的组合。(公式使用组合而不是排列，因为顺序并不重要)。Log₂将十进制值转换为比特:</li></ul><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bd77353329c05998b1d21e3284502610.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*aejJ3s21LBu677kWKOLw2A.png"/></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">p is the number of examples classified as positive, n the number classified as negative. fp is the number of false positives, fn false negatives.</figcaption></figure><ul class=""><li id="3e04" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated"><strong class="kj ir">模型优化:</strong>一旦我们生成了初始规则集，我们实际上可以使用我们的模型以更全面的方式重新评估每个规则的贡献。我们考虑用几个备选方案来替换每个规则:一个全新的生长修剪过的替换<em class="ki">和一个生长修剪过的原始<em class="ki">修订</em>。我们的优化模型使用三者中最好的一个——原始、替换或修订。(“最佳”这个词有点复杂，实现起来也有点可怕。这意味着任何规则都会导致规则集的最小描述长度，这是基于如果我们删除所有其他增加描述长度的规则，我们可以得到的最小可能描述长度。)我们可以根据需要多次重复优化阶段，但原始论文建议进行两次迭代。</em></li></ul><blockquote class="kd ke kf"><p id="c9fe" class="kg kh ki kj b kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">不也是这样吗，我们一边玩，一边制定规则？<br/>——路德维希·维特斯坦根</p></blockquote><ul class=""><li id="723a" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated"><strong class="kj ir">总结:</strong>如有必要，增加一些规则，以涵盖我们的优化模型不再涵盖的任何正面训练示例。最后，去掉任何不能提高描述长度的规则。</li></ul><h1 id="8cf7" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">在代码中使用规则集</h1><p id="1f7d" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">想要使用规则集学习者的 Java 用户可以使用<a class="ae kc" href="https://www.cs.waikato.ac.nz/ml/weka/" rel="noopener ugc nofollow" target="_blank"> Weka </a>的 RIPPER 实现<a class="ae kc" href="http://weka.sourceforge.net/doc.dev/weka/classifiers/rules/JRip.html" rel="noopener ugc nofollow" target="_blank"> JRip </a>。还有 Python 和 r 的 Weka 包装器。</p><p id="134a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">Python 用户也可以尝试维特根斯坦。(可能有其他针对这些特定算法的 Python 包，但我找不到任何包。)这里是<a class="ae kc" href="https://github.com/imoscovitz/ruleset" rel="noopener ugc nofollow" target="_blank"> github 回购</a>。</p><p id="a134" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">从命令行安装:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="1ab4" class="np lj iq nl b gy nq nr l ns nt">pip install wittgenstein</span></pre><p id="40ef" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">这里有一个快速的使用示例，使用令人愉快的<a class="ae kc" href="https://archive.ics.uci.edu/ml/datasets/mushroom" rel="noopener ugc nofollow" target="_blank">毒蘑菇数据集</a>。我们的目标是产生一套规则，可以辨别哪些蘑菇有毒。</p><p id="2e42" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">让我们从将数据帧加载到 pandas 开始:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="e8e5" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; import pandas as pd<br/>&gt;&gt;&gt; df = pd.read_csv(mushroom.csv)</span></pre><p id="839c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">训练-测试-分割我们的数据:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="a3b0" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; from sklearn.model_selection import train_test_split<br/>&gt;&gt;&gt; train, test = train_test_split(df, test_size=.33,              ...                                random_state=42)</span></pre><p id="17e9" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">维特根斯坦使用与 scikit-learn 相似的 fit-predict-score 语法。我们将训练一个开膛手分类器，将阳性类别定义为有毒。</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="03a1" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; import wittgenstein as lw<br/>&gt;&gt;&gt; clf = lw.RIPPER()<br/>&gt;&gt;&gt; clf.fit(train, class_feat='<!-- -->Poisonous/Edible', pos_class='p', <br/>...         random_state=42<!-- -->)</span></pre><p id="2a5b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在初始化/拟合期间，我们可以传递几个可选参数:</p><ul class=""><li id="c878" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated"><code class="fe nu nv nw nl b"><strong class="kj ir">prune_size</strong>:</code>更改增长/修剪比例。如果你想跳过修剪阶段(刺激，但不推荐！)使用 IREP 时，您可以将其设置为无。</li><li id="e0b0" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><code class="fe nu nv nw nl b"><strong class="kj ir">k</strong>:</code>优化运行的次数</li><li id="7035" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><code class="fe nu nv nw nl b"><strong class="kj ir">dl_allowance</strong>:</code>复杂性停止阈值</li><li id="4d24" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><code class="fe nu nv nw nl b"><strong class="kj ir">verbosity (1–5)</strong>:</code>如果你想知道规则是如何产生的，就用这个。(每个详细级别在文档字符串中都有解释。)</li><li id="d846" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><code class="fe nu nv nw nl b"><strong class="kj ir">n_discretize_bins</strong>:</code>维特根斯坦会为你自动检测和离散化连续特征——如果你想控制容器的数量，使用这个。</li></ul><p id="6129" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">您可以使用默认的度量标准(准确性)或者通过传递您自己的评分度量标准来测试模型。让我们从 scikit-learn 导入 precision 和 recall。我们还将通过计算条件的数量来检查模型的复杂性。</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="78dd" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; # Split target class feature<br/>&gt;&gt;&gt; X_test = test.drop('Poisonous/edible', axis=1)<br/>&gt;&gt;&gt; y_test = test['Poisonous/edible']</span><span id="a485" class="np lj iq nl b gy nx nr l ns nt">&gt;&gt;&gt; # Collect performance metrics<br/>&gt;&gt;&gt; from sklearn.metrics import precision_score, recall_score<br/>&gt;&gt;&gt; precision = clf.score(X_test, y_test, precision_score)<br/>&gt;&gt;&gt; recall = clf.score(X_test, y_test, recall_score)<br/>&gt;&gt;&gt; cond_count = clf.ruleset_.count_conds()</span><span id="b0ef" class="np lj iq nl b gy nx nr l ns nt">&gt;&gt;&gt; print(f'precision: {precision} recall: {recall}<br/>...         conds: {cond_count}')</span><span id="c9b1" class="np lj iq nl b gy nx nr l ns nt">precision: 0.9938..., recall: 0.9977..., conds: 32</span></pre><p id="539a" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们可以使用 clf.ruleset_ attribute 访问我们的训练模型。经过训练的规则集模型表示“或”的“与”列表:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="d718" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; clf.ruleset_.out_pretty()<br/>[[Stalk-surface-above-ring=k^Gill-spacing=c] V<br/>[Gill-size=n^Stalk-root=?^Stalk-shape=t] V<br/>[Gill-size=n^Population=s] V<br/>[Sport-print-color=h^Cap-surface=s] V<br/>[Gill-size=n^Cap-surface=s^Stalk-shape=e] V<br/>[Gill-size=n^Habitat=g] V<br/>[Population=v^Stalk-shape=e^Bruises?=t] V<br/>[Gill-size=n^Stalk-root=b^Gill-spacing=c] V<br/>[Gill-size=n^Population=c] V<br/>[Gill-size=n^Cap-color=p] V<br/>[Gill-size=n^Gill-color=u^Cap-surface=f] V<br/>[Gill-size=n^Cap-color=g^Gill-spacing=w] V<br/>[Gill-color=g^Stalk-root=b]]</span></pre><p id="5074" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">若要生成新的预测，请使用 predict 方法:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="9961" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; clf.predict(mysterious_unseen_mushrooms)<br/>[True, False, False, True, False...</span></pre><p id="d2d7" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们也可以要求我们的模型告诉我们为什么它做出了每一个肯定的预测:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="06d6" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; clf.predict(mysterious_unseen_mushrooms, give_reasons=True)</span><span id="cca9" class="np lj iq nl b gy nx nr l ns nt">([True, False, False, True, False...,<br/> [[&lt;Rule object: [Gill-size=n^Population=s]&gt;,<br/>   &lt;Rule object: [Gill-size=n^Cap-surface=s^Stalk-shape=e]&gt;],<br/>  [],<br/>  [],<br/>  [&lt;Rule object: [Gill-size=n^Population=s]&gt;],<br/>  []...)</span></pre><p id="8059" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">相当酷！</p><h1 id="b378" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">效果如何？</h1><p id="702b" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">我使用 scikit-learn 的 DecisionTreeClassifier 和网格搜索优化的 RandomForestClassifier 作为基线，在最初的 11 个分类数据集(大部分来自<a class="ae kc" href="https://archive.ics.uci.edu/ml/datasets.html?format=&amp;task=cla&amp;att=cat&amp;area=&amp;numAtt=&amp;numIns=&amp;type=&amp;sort=instDown&amp;view=table" rel="noopener ugc nofollow" target="_blank"> UCI </a>)上重复测试了我的 IREP 和 RIPPER 实现。(我抛出了两个数据集的结果，sklearn 和维特根斯坦分别拒绝做出正面预测。)</p><p id="719b" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">将规则集与随机森林这样的集成技术进行比较有点不公平——网格搜索-调整森林使它更加不公平——但我想看看维特根斯坦能在多大程度上与最好的可比较替代方案竞争。</p><p id="7f5c" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">尽管它是一个决策树分类器，scikit-learn 的树实现实际上并不接受分类数据。但是没关系——我们只需要做一点预处理，将数据转换成 DecisionTreeClassifier 可以接受的格式。</p><p id="60e9" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">首先，让我们使用 scikit 的 LabelEncoder 将我们的分类特征转换成数字特征:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="2c1b" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; from sklearn.preprocessing import LabelEncoder<br/>&gt;&gt;&gt; le = LabelEncoder()<br/>&gt;&gt;&gt; df_le=df.apply(le.fit_transform)<br/>&gt;&gt;&gt; df_le.head()</span></pre><p id="aeb6" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">然后，我们使用一个热编码来创建虚拟变量。否则我们将被序数特征所困，而不是名义特征！</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="705d" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder<br/>&gt;&gt;&gt; encoder = OneHotEncoder(sparse=False)<br/>&gt;&gt;&gt; encoder.fit(df_le)<br/>&gt;&gt;&gt; df_hot = enc.transform(df_le)<br/>&gt;&gt;&gt; df_hot.head()</span></pre><p id="e757" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">预处理完成，我们现在准备分割我们的数据…</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="1042" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; train, test = train_test_split(df_hot, test_size=.33,                             ...                                random_state=random_state)<br/>&gt;&gt;&gt; train_X = train[:,n_classes:]<br/>&gt;&gt;&gt; train_y = train[:,0]<br/>&gt;&gt;&gt; test_X = test[:,n_classes:]<br/>&gt;&gt;&gt; test_y = test[:,0]</span></pre><p id="8f6e" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">…并训练我们的模型:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="df63" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; tree_clf = DecisionTreeClassifier(random_state=random_state)<br/>&gt;&gt;&gt; tree_clf.fit(train_X, train_y)</span></pre><p id="c37f" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">为我们可爱的树打分:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="ca38" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; predictions = tree_clf.predict(test_X)<br/>&gt;&gt;&gt; precision = precision_score(test_y, predictions)<br/>&gt;&gt;&gt; recall = recall_score(test_y, predictions)<br/>&gt;&gt;&gt; print(f'precision: {precision} recall: {recall} node_count:   ...                    {tree_clf.tree_.node_count}')<br/>...</span></pre><p id="ccbf" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">下面是调整和适应随机森林的代码:</p><pre class="mm mn mo mp gt nk nl nm nn aw no bi"><span id="4a44" class="np lj iq nl b gy nq nr l ns nt">&gt;&gt;&gt; forest = RandomForestClassifier(random_state=random_state)<br/>&gt;&gt;&gt; n_features = train_X.shape[1]<br/>&gt;&gt;&gt; grid_params = {<br/>        'n_estimators': [50,100,200],<br/>        'max_depth': [1,3,5,8,10,None],<br/>        'min_samples_leaf': [3,6,10,13,16,20]<br/>    }<br/>&gt;&gt;&gt; clf = GridSearchCV(forest, grid_params, cv=5)<br/>&gt;&gt;&gt; clf.fit(train_X, train_y)<br/>&gt;&gt;&gt; best_params = clf.best_params_<br/>&gt;&gt;&gt; forest = RandomForestClassifier( <br/>         ... n_estimators=best_params['n_estimators'],<br/>         ... max_depth=best_params['max_depth'],<br/>         ... min_samples_leaf=best_params['min_samples_leaf'],<br/>         ... random_state=random_state)<br/>&gt;&gt;&gt; forest.fit(train_X, train_y)</span></pre><h1 id="26c8" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">结果</h1><p id="8d33" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">我的软件包至少在这些数据集上与 sklearn 有竞争力。(详细的 Jupyter 笔记本和测试可以在<a class="ae kc" href="https://github.com/imoscovitz/ruleset/blob/master/Performance%20Tests.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。)</p><p id="9b04" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">以下是每个规则集模型击败每个 sklearn 模型的频率比较，按精度评分:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/b74dac99cfa54116d7dc30ced52747d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUwH7m5_wdUwJs4KgPKUDQ.png"/></div></div></figure><p id="4c18" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">下面是一个回忆对比:</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/bd2e69ca6a071b7d54ab0876777b051d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mbm7L_L9H-geoiGoq2gkYw.png"/></div></div></figure><p id="0e79" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我还比较了它们的紧凑性，通过条件或节点的总数来衡量。</p><figure class="mm mn mo mp gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ny"><img src="../Images/eea5399eb73d897a5a8a01a568932807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6c6GQSA41dWY7xYifinJZA.png"/></div></div></figure><p id="71e1" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">有些领域维特根斯坦做得很好:</p><ul class=""><li id="4fdd" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated"><strong class="kj ir">不平衡类:</strong>两种规则集算法都比基于树的方法更好地处理了不平衡类；IREP 和瑞普在精确度和召回率上击败了特里，而且在严重不平衡的数据集上，他们在召回率上都比特里好得多。</li><li id="12fa" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><strong class="kj ir">过拟合</strong>:在训练样本和每个特征样本都较少的数据集上，两种规则集算法在精度上都胜过 Tree，RIPPER 胜过 Forest。(优势没有延伸到召回。)</li><li id="40ba" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><strong class="kj ir">紧凑性/可解释性:</strong> IREP 和 RIPPER 模型比(现成的)树和(调整的)森林更紧凑。</li></ul><p id="72d5" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">要考虑的潜在缺点:</p><ul class=""><li id="c47f" class="mu mv iq kj b kk kl ko kp lf mw lg mx lh my le mz na nb nc bi translated"><strong class="kj ir">速度:</strong> IREP 和决策树共享相同的时间复杂度——O(an logn)，其中 a 是属性个数，n 是例子。但是 RIPPER 的优化阶段在较大的数据集上可能变得耗时，在 O(模拟 n)上。更具体地说，在我将维特根斯坦的关键部分优化成 C++之前，RIPPER 通常是四个中训练时间最长的。另一方面，IREP 运行速度相当快，即使是在 Python 中。</li><li id="6d90" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><strong class="kj ir">连续特性:</strong> sklearn 的 trees 实现了 CART，它使用了比我目前实现的更复杂的离散化算法。目前，对于具有大量连续要素的数据集，您可能会从 sklearn 获得更好的性能，尽管这种情况可能很快就会改变。</li><li id="c800" class="mu mv iq kj b kk nd ko ne lf nf lg ng lh nh le mz na nb nc bi translated"><strong class="kj ir">性能:</strong>我通常认为 C5.0 树、随机森林和部分决策树(一种混合的树-规则集方法)在大多数(但不是所有)数据集上比规则集模型表现得更好；对 C4.5 和车树，赢家应该更悬而未决。</li></ul><p id="5eb3" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">与任何机器学习模型一样，您的特定数据和您面临的特定问题决定了这项工作的最佳工具。</p><p id="9687" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">规则集学习器是一种机器学习方法，实现起来很有趣，在某些情况下，在您的工具集中包含它会很有用。</p><p id="d26d" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我很想听听你的想法，所以请随时通过<a class="ae kc" href="https://www.linkedin.com/in/ilan-moscovitz/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae kc" href="https://github.com/imoscovitz/wittgenstein" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我！</p><h1 id="47d9" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">参考</h1><p id="4c09" class="pw-post-body-paragraph kg kh iq kj b kk mg km kn ko mh kq kr lf mi ku kv lg mj ky kz lh mk lc ld le ij bi translated">[1]j . funkrantz 和 G. Widmer，<a class="ae kc" href="https://pdfs.semanticscholar.org/f67e/bb7b392f51076899f58c53bf57d5e71e36e9.pdf" rel="noopener ugc nofollow" target="_blank">增量减少错误修剪</a> (1994)，机器学习 1994 年第十一届年会会议录</p><p id="3449" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">[2] J .罗斯·昆兰，<a class="ae kc" href="https://pdfs.semanticscholar.org/cb94/e3d981a5e1901793c6bfedd93ce9cc07885d.pdf" rel="noopener ugc nofollow" target="_blank"> MDL 与范畴理论(续)</a> (1995)机器学习 1995 年第十二届国际会议论文集</p><p id="9c74" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">[3] W. <a class="ae kc" href="https://www.let.rug.nl/nerbonne/teach/learning/cohen95fast.pdf" rel="noopener ugc nofollow" target="_blank">科恩，快速有效规则归纳</a> (1995)机器学习 1995 年第十二届国际会议论文集</p><p id="ca15" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">[4] E. Frank 和 I. H. Witten，<a class="ae kc" href="https://researchcommons.waikato.ac.nz/bitstream/handle/10289/1047/uow-cs-wp-1998-02.pdf?sequence=1&amp;isAllowed=y" rel="noopener ugc nofollow" target="_blank">在没有全局优化的情况下生成精确的规则集</a> (1998)机器学习 1998 年第十二届国际会议论文集</p><p id="3cae" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">[5] T .王等。al，<a class="ae kc" href="https://pdfs.semanticscholar.org/bb51/b3046f6ff607deb218792347cb0e9b0b621a.pdf" rel="noopener ugc nofollow" target="_blank">用于可解释分类的学习规则集的贝叶斯框架</a> (2017)《机器学习研究杂志》</p><p id="0f90" class="pw-post-body-paragraph kg kh iq kj b kk kl km kn ko kp kq kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">[6]路德维希·维特斯坦根，<em class="ki">《哲学研究》</em> (1958)</p></div></div>    
</body>
</html>