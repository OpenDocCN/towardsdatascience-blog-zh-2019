<html>
<head>
<title>Optimization: Ordinary Least Squares Vs. Gradient Descent — from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化:普通最小二乘法与梯度下降法——从头开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-chayankathuria-optimization-ordinary-least-squares-gradient-descent-from-scratch-8b48151ba756?source=collection_archive---------7-----------------------#2019-11-25">https://towardsdatascience.com/https-medium-com-chayankathuria-optimization-ordinary-least-squares-gradient-descent-from-scratch-8b48151ba756?source=collection_archive---------7-----------------------#2019-11-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a047c77b7af56d303e46e710d2c3bd13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yCu3IPisl0qqNHtJ"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@aaaaaaaaaaaaaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Trần Ngọc Vân</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bf40" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">什么是优化？、优化技术—数值方法和迭代方法，以及最终的 Python 实现。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="813b" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">最佳化</h1><blockquote class="mg"><p id="8488" class="mh mi iq bd mj mk ml mm mn mo mp la dk translated">优化是机器学习的核心。</p></blockquote><p id="5fa5" class="pw-post-body-paragraph kd ke iq kf b kg mq ki kj kk mr km kn ko ms kq kr ks mt ku kv kw mu ky kz la ij bi translated">用非常严格的术语来说，优化就是寻找你的<em class="mv">成本函数</em>给出最小值<em class="mv">的过程。</em>对于任何关于机器学习的优化问题，可以有数值方法或分析方法。数值问题是确定性的，这意味着它们有一个不变的封闭解。因此它也被称为时不变问题。这些封闭形式的解是解析可解的。但这些都不是优化问题。</p><p id="f1c4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你有函数 f(x)的“最小”或“最大”这样的词时，优化就来了——目标函数<strong class="kf ir">或成本函数。这个目标函数可以定义与你正在优化的问题相关的任何东西。这可能是一家公司的成本，另一家公司的损失，甚至是收入等等。</strong></p><p id="010a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个函数在特定的点 X*将是最优的。这个 X*就是最优点。因此，你的优化问题可能是——找出 X*,其中 f(x)是最小值/最大值。这也可以写成 argmin(f(x)) —其中函数 f(x)是最小值的自变量(或者反过来是 argmax(f(x))。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="7457" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">普通最小二乘法</h1><p id="8cd3" class="pw-post-body-paragraph kd ke iq kf b kg mw ki kj kk mx km kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">我很确定你知道线性回归的基本知识。如果你没有，也不用担心。看看这个就知道了。基本上，回归意味着找到最适合你的数字数据的<em class="mv">线/曲线——数据的函数近似值。也就是说，您需要一个从输入数据到输出数据(目标)的映射函数。这个映射函数被写成:</em></p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/6d1f088896db1d35a16abfe497ca3714.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*k3nt76Nl2kCVf2CAPy8dEg.png"/></div></figure><p id="e81f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 W0 是截距，W1 是直线的斜率，ŷ是预测输出。需要找到 W0 和 W1 的最佳值。让我们考虑这个非常小的数据集:</p><p id="1330" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X = 1，2，3</p><p id="bf84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Y = 5，12，18</p><p id="5a1b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们需要优化的问题是:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6c9efea3485bdf8a8823df7bdd55842e.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*XOSni3QhMLho1DYEg7Rn3w.png"/></div></figure><p id="9284" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中 L 是损失函数或成本函数或误差函数。现在下一步是为我们的优化问题找到正确的损失函数。因为如果你最小化错误的目标函数，你最终会得到错误的最优点。我们将使用的损失函数 L 是均方误差，计算公式如下:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a71289ef0fcc906bf032a4d14d82d419.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*jTyb4bfsln7RlYBMaMvRkg.png"/></div></figure><p id="8dd4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">求解上述损失函数，我们得到以下用于找到最佳权重的公式:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5fe2d802e27034851dd6367c856fd107.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*aH2n_uW2YRQNyKNXgI1mwA.png"/></div></figure><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/247e32de55450add5c7d6927a78cf3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*6N9bp-5qMvBtJmQ_3_W5nA.png"/></div></figure><p id="35f0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 python 计算上述权重，我们得到以下值:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3e16b8f74c2e98e5c12911c1a2c42b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*cr6LHmlRUOwZmbYRJNOb0Q.png"/></div></figure><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/a4dda785596169f80ef93c30877a29ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*ZWTRviPVXTlAWvb8QtvloA.png"/></div></figure><p id="227e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是普通的最小二乘解，也就是解析解。因为我们找到了误差平方的最小值。</p><p id="75aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是这种解决方案是不可扩展的。将此应用于线性回归相当容易，因为我们有很好的系数和线性方程。将此应用于复杂的非线性算法(如支持向量机)是不可行的。因此，我们将通过迭代法找到这个解的<em class="mv">数值近似值</em>——它将接近(但不完全等于)OLS 解——它给了我们精确的解。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="9657" class="li lj iq bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">梯度下降</h1><p id="470a" class="pw-post-body-paragraph kd ke iq kf b kg mw ki kj kk mx km kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">我们先来了解一下梯度下降优化背后的直觉。假设 20 个人(包括你自己)被随机空投到一个山脉中。你的任务是在 30 天内找到完整区间的最高峰。你们每个人都有对讲机可以交流，还有高度计可以测量高度。每天，你们都要花几个小时寻找可能的最高峰，并向其他人报告他们在指定区域内发现的当天最高海拔——这就是他们的健康值。</p><p id="ca84" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设在第一天你报告 1000 英尺。有人报告高度 1230 英尺。诸如此类。然后有一个人报告 5000 英尺。这是最大值。记住你的任务是集体到达山脉的最高峰。第二天你接下来做什么？第二天，每个人都会聚集到昨天发现最高海拔的地方。他们会认为山脉的最高峰很可能就在这个地区。如果有另一个区域已经有 5000 英尺，为什么昨天报告 500 英尺的人会再次搜索那个区域。于是所有的搜索者<em class="mv">贪婪地</em>向报道的最高点移动。现在，这种贪婪可能会把你带到这个范围的最高峰，但也可能会导致一个彻底的错误。</p><p id="7f50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那个在 500 英尺高的家伙。昨天可能是在一座 10000 英尺高的山峰的底部。！他贪婪地忽略了它，走向另一个最大化 5000 英尺，比如 7000 或 8000 英尺。你实际上陷入了局部最大值/最优值(突变在某种程度上会有所帮助！).没有办法知道你是否停留在局部最优。</p><p id="bcb3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">模拟退火</strong>也是一种可以拯救我们的算法。其中搜索者将彻底搜索整个搜索空间，而不会偏向于最有可能找到全局最大值。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="c870" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在回到我们用 OLS 定义的优化问题。让我们用梯度下降法求解。损失函数也是一样的。但这次我们将一步一步地迭代，以达到最佳点。从任意权重值开始，检查该点的梯度。我们的目标是到达最低点，也就是谷底。所以我们的梯度应该总是负的。</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nl"><img src="../Images/ee9111798be5960033417bf7eb940f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrFZV7pKPcc5dzLaWvngtQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="861d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们需要更新权重，使它们更接近最小值。我们有下面的等式:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/472902d45ea31c2df701359309257c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*9c4tV5YBiJovvxLjIu_pnw.png"/></div></figure><p id="551b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着下一次迭代的权重将是前一次迭代的权重减去更新的权重。现在这个更新有两个组成部分:<em class="mv">方向</em> —斜率或梯度，以及<em class="mv">值</em> —步长。梯度将为:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nn"><img src="../Images/fe102324d54eb80d8392b783bd26833e.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*qAVEf8mV51tm6RRy5kWCPA.png"/></div></div></figure><p id="7db1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以如果在我们的初始重量下，斜率是负的，我们在正确的方向上。我们只需要增加权重的值来使它更接近。这正是上面的等式所做的。如果斜率在特定点为负，则第二项将被添加到上一次迭代的权重值中。相反，如果它是正的，那就意味着我们需要向相反的方向去达到最小值。在这种情况下，该等式从上一个等式的权重值中减去第二项。干净利落。</p><p id="40b0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要考虑的第二个因素是步长α。这是一个<em class="mv">超参数</em>，您需要在算法开始之前决定。如果α太大，那么你的优化器将会跳跃很大，永远找不到最小值。相反，如果你把它设置得太小，优化器将会花很长时间来达到最小值。因此，我们需要预先设定α的最佳值。</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi no"><img src="../Images/660f97cb17da55e67f070b1084ae4827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pPfwbZDoFIvOWc0KbB_SjQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk"><a class="ae kc" href="https://datascience.stackexchange.com/questions/50948/gradient-descent" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="fc56" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你理解正确，你会意识到我们在这里讨论的梯度本质上是误差的总和。我们实际上只是接受了这个错误的一小部分。并且我们传播该误差来更新我们的权重。</p><p id="8296" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们的权重更新等式变为:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi np"><img src="../Images/884fd5049e7febcf1262d2463ca0c362.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*Ze6RuBJaQBRWKeCIIeH12A.png"/></div></figure><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e650ed56e3084503df7bd6e0b439645a.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*0esqhLAd7k7xbb5aFFKf6w.png"/></div></figure><p id="e54f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，让我们记下我们执行的清晰步骤:</p><ol class=""><li id="5f80" class="nr ns iq kf b kg kh kk kl ko nt ks nu kw nv la nw nx ny nz bi translated">初始化权重 W0、W1 的值(可以是任何值)和步长α(需要是一个好值)。</li><li id="7362" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la nw nx ny nz bi translated">求目标 Ŷ的预测值= W0 + W1。所有 x 的 x。</li><li id="3bb7" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la nw nx ny nz bi translated">计算误差值(Ŷ-Y)和 MSE。</li><li id="eb09" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la nw nx ny nz bi translated">根据梯度下降更新规则更新权重。</li><li id="f285" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la nw nx ny nz bi translated">重复 2–4。</li></ol><p id="5723" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这被称为<strong class="kf ir">批次梯度下降</strong>，因为我们在每次迭代中获取整个批次或数据集的误差。我们还有以下梯度下降的变体:</p><ul class=""><li id="5f9e" class="nr ns iq kf b kg kh kk kl ko nt ks nu kw nv la of nx ny nz bi translated"><strong class="kf ir">随机梯度下降</strong>，其中权重的更新在每次迭代中完成</li><li id="85ba" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la of nx ny nz bi translated"><strong class="kf ir">小批量梯度下降</strong>，介于批量和随机之间，将完整数据集分成小批量，然后在每个批量后应用权重更新。</li></ul><p id="1574" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，让我们用 Python 中的几行代码完成所有这些工作！</p><p id="0673" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从初始化我们的小数据集开始:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi og"><img src="../Images/351c7dd5f7aab98fb79650ef1f45b6f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*noumMTzPbpFHCLAwzJj_qA.png"/></div></div></figure><p id="eb24" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在进入第一步，初始化权重和步长，我选择了 0.04。您可以尝试调整该值并亲自查看结果:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oh"><img src="../Images/4eb68acf17791f58ddf19ed3514e8719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*IQrbcdX2RExFpV1SrmL5DQ.png"/></div></div></figure><p id="fe05" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">初始化后，我们对整个数据集进行多次迭代，计算每次迭代的均方误差，并更新权重:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oi"><img src="../Images/d3c3593b46b0c2d8764a5cc111b6d20b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wlK9wJmRMBLdThLv5aCc7w.png"/></div></div></figure><p id="83ce" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们迭代 10 次，希望我们的算法已经足够收敛。让我们看看我们的最终重量，看看它们与我们的 OLS 解决方案有多接近:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oj"><img src="../Images/94c4cedd8415e01f882279e88bcfec84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6HNL8bBobKLxKJWJpEgsw.png"/></div></div></figure><p id="ea1c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相当接近！现在让我们检查预测的目标变量，Ŷ和误差:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ok"><img src="../Images/1b2eaf4994a3b71611063df09b1760a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N15FmnoC6kdn6JxjAkFHWA.png"/></div></div></figure><p id="10b8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，预测变量非常接近实际值。最后，让我们画出每次迭代的均方误差值，看看我们的算法表现如何:</p><figure class="nc nd ne nf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ol"><img src="../Images/4b4adb158590eabca940cc0232aa698f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KDIDjqJo8OZy89zcqfTryQ.png"/></div></div></figure><p id="7c2f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太好了！所以我们讨论了:</p><ul class=""><li id="abf9" class="nr ns iq kf b kg kh kk kl ko nt ks nu kw nv la of nx ny nz bi translated">优化到底是什么，</li><li id="c99b" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la of nx ny nz bi translated">关于机器学习，它有什么样的解决方案，</li><li id="e563" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la of nx ny nz bi translated">它的分析方法和直觉是什么</li><li id="be40" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la of nx ny nz bi translated">线性回归在 python 中的实现</li></ul></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="66ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是本文的全部内容。如需进一步阅读，您可以关注以下精彩内容:</p><ul class=""><li id="08bb" class="nr ns iq kf b kg kh kk kl ko nt ks nu kw nv la of nx ny nz bi translated"><a class="ae kc" href="https://machinelearningmastery.com/gradient-descent-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/gradient-descent-for-machine-learning/</a></li><li id="dab4" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la of nx ny nz bi translated">https://www.youtube.com/watch?v=sDv4f4s2SB8<a class="ae kc" href="https://www.youtube.com/watch?v=sDv4f4s2SB8" rel="noopener ugc nofollow" target="_blank"/></li><li id="d2b3" class="nr ns iq kf b kg oa kk ob ko oc ks od kw oe la of nx ny nz bi translated"><a class="ae kc" href="https://www.amazon.in/Engineering-Optimization-Practice-Singiresu-Rao/dp/0470183527" rel="noopener ugc nofollow" target="_blank">https://www . Amazon . in/Engineering-Optimization-Practice-Singiresu-Rao/DP/0470183527</a></li></ul></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="7be2" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望这篇文章有所帮助。请在下面分享和评论你的想法！</p></div></div>    
</body>
</html>