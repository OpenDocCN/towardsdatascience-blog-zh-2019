<html>
<head>
<title>Time Series Analysis with Deep Learning : Simplified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习时间序列分析:简化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/time-series-analysis-with-deep-learning-simplified-5c444315d773?source=collection_archive---------9-----------------------#2019-10-10">https://towardsdatascience.com/time-series-analysis-with-deep-learning-simplified-5c444315d773?source=collection_archive---------9-----------------------#2019-10-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="a25e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">参加在时间序列分析中使用深度学习的“为什么”和“什么时候”的速成班。</p><h2 id="5727" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">什么是时间序列分析？</h2><p id="1288" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">时间序列是使用时间戳排序的数据点序列。时间序列分析是..<em class="lm">你猜对了</em>..时间序列数据分析:P</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/7b69e8dd79d29ad51975dffff0db3300.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*Kjz7-OYvGG4S_qyYd1ZfEg.png"/></div></figure><p id="e8e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从你喜欢的水果的每日价格到电路提供的电压输出读数，时间序列的范围是巨大的，时间序列分析的领域也是如此。分析时间序列数据通常集中在<strong class="js iu">预测</strong>，但也可以包括分类、聚类、异常检测等。例如，通过研究过去价格变化的模式，你可以试着预测你关注已久的手表的价格，以判断什么是购买它的最佳时机！！</p><h2 id="7f15" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">为什么要深度学习？</h2><p id="2cfe" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">时间序列数据可能非常不稳定和复杂。深度学习方法<em class="lm">对数据中的潜在模式</em>不做任何假设，并且对噪声(这在时间序列数据中很常见)也更加<em class="lm">鲁棒，使它们成为时间序列分析的首选。</em></p><h2 id="4059" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">数据处理</h2><p id="9d1c" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">在我们继续预测之前，首先以数学模型可以理解的形式处理我们的数据是很重要的。通过使用滑动窗口来切割数据点，可以将时间序列数据转化为监督学习问题。每个滑动窗口的期望输出是窗口结束后的时间步长。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/b88d889176aa417095a9e78a779b51f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*sh60bhp1ClyOhWZGzJ7utQ.png"/></div></figure><h2 id="5a04" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">循环网络</h2><p id="9da7" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">递归网络是深度学习网络<em class="lm">，有一个转折……</em>它们可以记住过去，因此是序列处理的首选。RNN 细胞是循环网络的骨干。RNN 单元有 2 个输入连接，输入和<em class="lm">先前的状态。</em>类似地，它们也有 2 个输出连接，即输出和<em class="lm">当前状态。</em>这种<strong class="js iu">状态</strong>帮助他们组合来自过去和当前输入的信息。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/158cbee44eb21e87afdbab420361aa48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*psFYwcDGhtuEB7arpeXPbg.png"/></div></div></figure><p id="6b84" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个简单的 RNN 单元过于简单，不能统一用于不同领域的时间序列分析。因此，多年来已经提出了大量的变体来使递归网络适应各种领域，<em class="lm">但是核心思想仍然是相同的！！</em></p><h2 id="f335" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">RNNs 上的 LSTMs</h2><p id="bb4b" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">LSTM 单元是一种特殊的 RNN 单元，其中有<strong class="js iu"><em class="lm"/>门</strong>，本质上是 0 和 1 之间的值，对应于状态输入。这些门背后的直觉是忘记或保留过去的信息，这让他们记住的不仅仅是眼前的过去。没有人能比 Colah 的博客更好地解释 LSTMs，所以如果你还没有去过，就去看看吧。</p><div class="mb mc gp gr md me"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">了解 LSTM 网络</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">2015 年 8 月 27 日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">colah.github.io</p></div></div></div></a></div><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mn"><img src="../Images/586760be84d223e0ad7ffa11c45395d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFOzE0TEMFERg3G5_5HiPA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Complex internal architecture of an LSTM cell. GRU is another variation of gated RNN cells.</figcaption></figure><h2 id="5bd9" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">美国有线电视新闻网</h2><p id="f5ef" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">我们讨论过，由于<em class="lm">状态信息</em>通过每个时间步传播，rnn 只能记住最近的过去。另一方面，像 LSTMs 和 GRUs 这样的门控网络可以处理相对较长的序列，<strong class="js iu">但即使这些网络也有其局限性！！为了更好地理解这个问题，我们也可以看看<em class="lm">消失和爆炸渐变</em>。</strong></p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi ms"><img src="../Images/f925c54f945188d42a5dbcc1dfeb5d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-mYuxEgsHyrXRd6Oe-vhA.jpeg"/></div></div></figure><p id="649a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">那么如何处理非常长的序列呢？？显而易见的解决方案是<strong class="js iu">缩短它们！！！</strong>但是如何？一种方法是丢弃信号中存在的细粒度时间信息。这可以通过将小组数据点累积在一起并从中创建特征来实现，然后将这些特征传递给 LSTM，<em class="lm">就像一个单独的数据点，</em>。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/b9be6aeb2c8b0a5a4844c7d833019861.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*-OGYz1-7YZ0z133Y2B0ThA.png"/></div></figure><h2 id="68bd" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">多尺度分层 LSTMs</h2><p id="e3e0" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">看着 CNN-LSTM 的架构，我想到了一件事…为什么要用 CNN 来组合那些小团体？？为什么不使用不同的 LSTM 呢！！！多尺度分级 LSTMs 基于相同的理念。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mu"><img src="../Images/c174bdfb846287f57e5a686c24ed2fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRWeqCeRxtO14pL4xvznmQ.png"/></div></div></figure><p id="751d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输入在多个尺度上被处理，每个尺度都致力于做一些独特的事情。对更细粒度的输入起作用的较低等级关注于传递细粒度的(但仅仅是最近的)时间信息。另一方面，上层关注于提供完整的图片(但是没有细粒度的细节)。多种尺度结合在一起可以更好地理解时间序列。</p><h2 id="53f0" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">下一步是什么？</h2><p id="34e3" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">时间序列分析是一个非常古老的领域，包含各种跨学科的问题陈述，每一个都有自己的挑战。然而，尽管每个领域都根据自己的需求调整模型，但在时间序列分析中仍然有一些需要改进的一般研究方向。例如，从非常基本的 RNN 单元到多尺度分级 LSTM 的每个发展都在某种程度上关注于处理更长的序列，但是即使是最新的 LSTM 修改也有其自身的序列长度限制，并且我们仍然没有能够真正处理超长序列的架构。</p></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><p id="e17c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lm">这个博客是努力创建机器学习领域的简化介绍的一部分。点击此处的完整系列</em></p><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/machine-learning-simplified-1fe22fec0fac"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">机器学习:简化</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">在你一头扎进去之前就知道了</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh lt me"/></div></div></a></div><p id="5cea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lm">或者只是阅读本系列的下一篇博客</em></p><div class="mb mc gp gr md me"><a rel="noopener follow" target="_blank" href="/object-detection-simplified-e07aa3830954"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">对象检测:简化</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">让我们来看看计算机视觉中最著名的问题陈述之一</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh lt me"/></div></div></a></div></div><div class="ab cl mv mw hx mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="im in io ip iq"><h2 id="9968" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">参考</h2><p id="0f8b" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated"><em class="lm"> [1]程，闵，等.“MS-LSTM:一种用于 BGP 异常检测的多尺度 LSTM 模型”2016 IEEE 第 24 届国际网络协议大会(ICNP)。IEEE，2016。<br/>[2]阿拉亚、伊格纳西奥·阿、卡洛斯·瓦莱和埃克托尔·阿连德。"基于 LSTM 的多尺度风速预报模式."伊比利亚美洲模式识别大会。施普林格，查姆，2018。<br/> [3] Hochreiter，Sepp，和 Jürgen Schmidhuber。“长短期记忆。”神经计算 9.8(1997):1735–1780。<br/> [4]沃索基、索罗什、普拉尚·维贾亚拉哈万、德布·罗伊。" Tweet2vec:使用字符级 cnn-lstm 编码器-解码器学习 Tweet 嵌入."第 39 届国际 ACM SIGIR 信息检索研究与发展会议录。ACM，2016。<br/> [5]帕斯卡努，拉兹万，托马斯·米科洛夫，约舒阿·本吉奥。"关于训练递归神经网络的难度."机器学习国际会议。2013.</em></p></div></div>    
</body>
</html>