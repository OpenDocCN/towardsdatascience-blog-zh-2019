<html>
<head>
<title>GPT-2: Understanding Language Generation through Visualization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT-2:通过可视化理解语言生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8?source=collection_archive---------4-----------------------#2019-03-05">https://towardsdatascience.com/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8?source=collection_archive---------4-----------------------#2019-03-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="620b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">超大的语言模型是如何完成你的想法的。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0d34610429c4913fd3e001596b8ab4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdQ2iGgp7fEJh8Ul9Hg5uA.png"/></div></div></figure><p id="46e1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在大多数<a class="ae ln" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>研究人员看来，2018 年是技术进步巨大的一年，新的<a class="ae ln" href="http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html" rel="noopener ugc nofollow" target="_blank">预训练</a> NLP 模型打破了从情感分析到问题回答等任务的记录。</p><p id="c855" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但对其他人来说，2018 年是 NLP 永远毁掉芝麻街的一年。</p><p id="b801" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">首先是<a class="ae ln" href="https://allennlp.org/elmo" rel="noopener ugc nofollow" target="_blank">ELMo</a>(<strong class="kt ir">E</strong>embeddings from<strong class="kt ir">L</strong>language<strong class="kt ir">Mo</strong>dels)然后是<a class="ae ln" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">BERT</a><strong class="kt ir"/>(<strong class="kt ir">B</strong>I direction<strong class="kt ir">E</strong>n coder<strong class="kt ir">R</strong>presentations from<strong class="kt ir">T</strong>transformers)，现在 BigBird 坐在<a class="ae ln" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">胶水排行榜的首位</a><a class="ae ln" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank">我自己的思维已经被这种命名惯例所腐蚀，以至于当我听到“<em class="lo">我一直在和伯特玩，”</em>的时候，我脑海中闪现的图像不是我童年时的毛茸茸的单眉锥头鹦鹉，而是这样的:</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lp"><img src="../Images/9fb85b0f4545ea045645c0229551dfb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O4zz9gWFxne5bsxJ7ZKSrg.png"/></div></div></figure><p id="f3a2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我看不到那张，<a class="ae ln" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">插图伯特</a>！</p><p id="0a63" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我问你——如果芝麻街在 NLP 模型品牌化面前都不安全，还有什么是安全的？</p><p id="c006" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但有一个模型让我的童年记忆完好无损，这是一个没有名字和面孔的算法，它的作者 OpenAI 简单地将其称为“语言模型”或“我们的方法”。只有当另一篇论文的作者需要将他们的模型与这个无名的创造物进行比较时，它才被认为配得上一个名字。也不是<em class="lo">厄尼</em>或<em class="lo">格罗弗</em>或<em class="lo">库克怪兽</em>；这个名字准确地描述了算法是什么，仅此而已:<a class="ae ln" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kt ir"><em class="lo">GPT</em></strong></a>，预训练的生成式<a class="ae ln" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">变压器</a>。</p><p id="ece8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是就在它被命名的同时，GPT 被伯特毫不客气地从 GLUE 排行榜上除名了。GPT 失败的一个原因是它使用传统的<a class="ae ln" href="https://machinelearningmastery.com/statistical-language-modeling-and-neural-language-models/" rel="noopener ugc nofollow" target="_blank">语言建模</a>进行了预训练，即预测句子中的下一个单词。相比之下，伯特使用<em class="lo"> </em>掩蔽语言建模<em class="lo">，</em>进行预训练，这更像是一个<a class="ae ln" rel="noopener" target="_blank" href="/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e">填空</a>练习:根据<em class="lo">之前和</em>之后出现的单词猜测缺失(“掩蔽”)的单词。这种双向架构使 BERT 能够学习更丰富的表示，并最终在 NLP 基准测试中表现更好。</p><p id="0d91" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，在 2018 年末，OpenAI 的 GPT 似乎将永远作为伯特的通用名称、奇怪的单向前身而被历史所知。</p><p id="82e6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但 2019 年讲述了一个不同的故事。事实证明，导致 GPT 在 2018 年垮台的单向架构让它有能力做一些伯特永远做不到的事情(或者至少不是为它设计的):<a class="ae ln" href="https://blog.openai.com/better-language-models/" rel="noopener ugc nofollow" target="_blank">写关于会说话的独角兽的故事</a>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lq"><img src="../Images/b07b61882ca52c31a996302dc4a3f3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wUOgqwOJv-eMd0rSjWlTMg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">From <a class="ae ln" href="https://blog.openai.com/better-language-models/" rel="noopener ugc nofollow" target="_blank">https://blog.openai.com/better-language-models/</a> . Edited for length.</figcaption></figure><p id="9617" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你看，从左到右的语言建模不仅仅是一个训练前的练习；它还实现了一个非常实际的任务:语言生成。如果你能预测一个句子中的下一个单词，你就能预测下一个，再下一个，很快你就有了…很多单词。如果你的语言建模足够好，这些单词将形成有意义的句子，这些句子将形成连贯的段落，这些段落将形成，嗯，你想要的任何东西。</p><p id="5932" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">而在 2019 年 2 月 14 日，OpenAI 的语言模型确实变得足够好——好到可以写会说话的独角兽的故事，<a class="ae ln" href="https://www.nytimes.com/interactive/2019/06/07/technology/ai-text-disinformation.html" rel="noopener ugc nofollow" target="_blank">产生假新闻</a>，写<a class="ae ln" href="https://blog.openai.com/better-language-models/#sample8" rel="noopener ugc nofollow" target="_blank">反回收宣言</a>。它甚至被赋予了一个新名字:<em class="lo"> GPT-2 </em>。</p><p id="da0a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那么，GPT 2 号具有类似人类的写作能力的秘密是什么呢？没有根本性的算法突破；这是一个扩大规模的壮举。GPT-2 拥有高达 15 亿个参数(比最初的 GPT 多 10 倍)，并根据来自 800 万个网站的文本进行训练。</p><p id="9a4a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如何理解一个有 15 亿个参数的模型？让我们看看可视化是否有帮助。</p><h1 id="42dc" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">可视化 GPT-2</h1><p id="1073" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">由于担心恶意使用，OpenAI 没有发布完整的 GPT-2 模型，但他们发布了一个较小的版本，大小相当于原始 GPT (117 M 参数)，在新的更大的数据集上进行训练。虽然没有大模型强大，但小版本仍然有一些语言生成能力。让我们看看可视化是否能帮助我们更好地理解这个模型。</p><blockquote class="ms mt mu"><p id="1085" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="iq">注</em> </strong> <em class="iq">:创建这些可视化效果的代码可以在</em> <a class="ae ln" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> <em class="iq"> GitHub </em> </a> <em class="iq">上找到。</em></p></blockquote><p id="18d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">一个说明性的例子</strong></p><p id="20e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们来看看 GPT-2 小模型是如何完成这句话的:</p><blockquote class="ms mt mu"><p id="0f16" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">船上的狗跑了</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/6465f0c8ae435de8289ba35b40f736d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OzrOK-L98SO1jf5nQ4G39w.jpeg"/></div></figure><p id="aec2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是模型生成的结果:</p><blockquote class="ms mt mu"><p id="e176" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">船上的狗跑了<strong class="kt ir">，狗被船员发现。</strong></p></blockquote><p id="6497" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">似乎很合理，对吧？现在让我们稍微调整一下这个例子，把<em class="lo">狗</em>改成<em class="lo">马达</em>，看看这个模型会生成什么<em class="lo"> : </em></p><blockquote class="ms mt mu"><p id="e99c" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">船上的马达运转了</p></blockquote><p id="40de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在是完整的句子:</p><blockquote class="ms mt mu"><p id="3224" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">船上的马达以大约每小时 100 英里的速度运转。</p></blockquote><p id="1ac7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过改变句首的那个单词，我们得到了完全不同的结果。这个模型似乎理解了狗的奔跑类型和马达的完全不同。</p><p id="791f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GPT-2 怎么知道如此关注<em class="lo">做</em> g vs <em class="lo">马达</em>的，尤其是这些词出现在句子的前面？嗯，GPT-2 基于<a class="ae ln" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">变压器</a>，这是一个<a class="ae ln" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank"> <em class="lo">注意力</em> </a>模型——它学会将注意力集中在与手头任务最相关的前面的单词上:预测句子中的下一个单词。</p><p id="97a9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们来看看 GPT 2 号对<em class="lo">船上的狗跑了</em>的关注点在哪里:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/90b6c1da56ecbcd493cc7cad96e62d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*swIkbRPrGUkfTRSLPWHS2g.png"/></div></figure><p id="980d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">从左到右阅读的线条显示了模型在猜测句子中的下一个单词<strong class="kt ir">时注意的位置(颜色强度代表注意强度)。所以，在猜测<em class="lo">跑完之后的下一个单词时，</em>这种情况下模型密切关注<em class="lo">狗</em>。这是有意义的，因为知道谁或什么在跑步对于猜测接下来会发生什么至关重要。</strong></p><p id="2b7a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在语言学术语中，模型关注的是名词短语<em class="lo">的<strong class="kt ir">头</strong>船上的狗</em>。GPT-2 还捕捉到了许多其他语言特性，因为上面的注意力模式只是模型中<strong class="kt ir"> 144 </strong>个注意力模式中的<strong class="kt ir">一个</strong>。GPT-2 有 12 层，每层有 12 个独立的注意机制，称为“头”；结果是 12 x 12 = 144 种不同的注意力模式。在这里，我们将它们全部可视化，突出显示我们刚刚看到的那个:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/1322e06e1c082cbd4a38486c2840073e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OUZ784c9aKroGT2IIm23rg.png"/></div></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Visualization of the attention patterns across the model’s 12 layers (rows) and 12 heads (columns), with Layer 4 / Head 3 selected (zero-indexed).</figcaption></figure><p id="8eb9" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以看到这些模式有许多不同的形式。这里还有一个有趣的例子:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4dbdc1cc47c5e420e4e6b3ca14a277b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*Pawc1E9h13NmVNUedBDizg.png"/></div></figure><p id="b2c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这一层/头把所有注意力集中在句子中的<strong class="kt ir">前一个词</strong>上。这是有意义的，因为相邻的单词通常与预测下一个单词最相关。传统的<a class="ae ln" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> <em class="lo"> n </em> -gram </a>语言模型也是基于同样的直觉。</p><p id="0d50" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是为什么这么多的注意力模式是这样的呢？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/4238a6e527658dbca06151198351adc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*BZMxS-ZmRBYmDV2lKxXjSw.png"/></div></figure><p id="d659" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这种模式下，几乎所有的注意力都集中在句子中的第一个单词上，其他单词都被忽略了。这似乎是零模式，表明注意力头没有发现它正在寻找的任何语言现象。当没有更好的关注点时，该模型似乎已经将第一个单词重新定位为要查找的地方。</p><h2 id="4f05" class="nb lw iq bd lx nc nd dn mb ne nf dp mf la ng nh mh le ni nj mj li nk nl ml nm bi translated">____ 中的猫</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b103003c0d05f5f7352873c74f172b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*xI-4S8gppO5qBcrrIXXYlA.jpeg"/></div></figure><p id="1217" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">好吧，如果我们要让 NLP 玷污我们对芝麻街的记忆，那么我想苏斯博士也是公平的游戏。让我们看看 GPT-2 是如何完成经典作品《戴帽子的猫》中的这些台词的</p><blockquote class="no"><p id="3cb0" class="np nq iq bd nr ns nt nu nv nw nx lm dk translated">在一只风筝线上，我们看到了妈妈的新衣服！她的礼服上有粉色、白色和…</p></blockquote><p id="6a92" class="pw-post-body-paragraph kr ks iq kt b ku ny jr kw kx nz ju kz la oa lc ld le ob lg lh li oc lk ll lm ij bi translated">GPT 2 号是这样完成最后一句话的:</p><blockquote class="ms mt mu"><p id="87b0" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">她的礼服上有粉色、白色和蓝色的圆点。</p></blockquote><p id="d4ca" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不算太差！原文有<em class="lo">红色</em>，至少我们知道不只是背。</p><p id="48e5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">那么 GPT 2 号是如何知道选择颜色的呢？也许是因为下面的注意模式似乎能识别逗号分隔的列表:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/936a48eafefed0d174b56954f29e54c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*skLreRWUCnKK5Xqd26DhJw.png"/></div></figure><p id="7594" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了确定在<em class="lo">和</em>之后的单词，模特将注意力集中在列表中的前面的项目上——粉色<em class="lo">和白色</em>。它知道挑选一个与前面项目类型相匹配的单词，在这个例子中是一种颜色。</p><p id="b102" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">名字有什么意义？</p><p id="f1c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">GPT-2 似乎特别擅长只根据一个名字写短篇传记。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/cfb5f55a41c6945c1883b3a57c6106f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*TRMbrMd39Hs2APtCZVrMkg.png"/></div></figure><p id="7971" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">作为一个实验，尝试让 GPT-2 <a class="ae ln" href="https://gpt2.apps.allenai.org/?text=Who%20is" rel="noopener ugc nofollow" target="_blank">从提示<em class="lo">中生成文本</em></a>“谁是&lt;你的名字&gt;？”这个特殊的提示通常会触发模型写一篇简短的传记，可能是因为这是网络文章中作者传记的常见序言。</p><p id="1138" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面是为提示“谁是杰西·维格？”生成的几个 bios。：</p><blockquote class="ms mt mu"><p id="2952" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">“杰西·维格是社交媒体营销专家，曾任社交媒体营销经理。他是 VigMedia.com 的联合创始人兼首席执行官，最近又是 vig media . com 的创始人兼首席执行官。”</p></blockquote><p id="dc19" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">还不错！有点重复，但以一种意想不到的方式很好地将故事个性化。这里还有一个:</p><blockquote class="ms mt mu"><p id="6039" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">“杰西·维格是一位名叫詹姆斯·维格的福音传教士的儿子。他于 1964 年移居美国，成为密歇根大学的一名传教士，在那里他教了 18 年书，直到 2007 年 10 月去世。</p></blockquote><p id="f9bc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这最后一个例子中，GPT-2 足够聪明，知道我的另一个自我的父亲有相同的姓。让我们看看 GPT 2 号在选择最后一个名字时将注意力集中在哪里:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/63ecc54d028d86417c971f10c107ed44.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*sWvzg4SVZMdZC9Jzwev1BQ.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Layer 11 / Head 10</figcaption></figure><p id="3e72" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">当决定在<em class="lo">詹姆斯</em>之后预测的单词时，这种模式将注意力集中在我的姓的先前提及上。(注意，在模型内部，<em class="lo"> Vig </em>已经被分成单词块“<em class="lo">V”</em>和“<em class="lo">ig”</em>，因为它是一个不常用的单词。)<em class="lo"> </em>似乎这种注意模式专门识别家族名字之间的关系。为了测试这一点，让我们稍微修改一下文本:</p><p id="26b4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="lo">“杰西·维格是一位名叫詹姆斯的福音传道者的</em> <strong class="kt ir"> <em class="lo">同事</em></strong><em class="lo">……</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/c42b32290ee55899928130f491ff7d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*kDSVwDueRx7-Z8M1S8JPlA.png"/></div><figcaption class="lr ls gj gh gi lt lu bd b be z dk">Layer 11 / Head 10</figcaption></figure><p id="b401" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在詹姆斯只是一个<em class="lo">同事</em>，这种注意力模式几乎完全忽略了我的姓。</p><blockquote class="ms mt mu"><p id="9275" class="kr ks lo kt b ku kv jr kw kx ky ju kz mv lb lc ld mw lf lg lh mx lj lk ll lm ij bi translated">注意:《GPT-2》似乎是根据一个名字的种族和性别来生成传记的。需要进一步的研究来了解这个模型可能会编码什么样的偏见；你可以在这里阅读更多关于这个话题<a class="ae ln" href="http://www.fatml.org/" rel="noopener ugc nofollow" target="_blank">的内容。</a></p></blockquote><h1 id="f9bb" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">未来是可生成的</h1><p id="e7bc" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">就在去年，生成各种内容的能力——图像、视频、音频和文本——已经提高到我们不再相信自己的感官和判断的程度。</p><p id="c77a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">而这仅仅是开始；这些技术将继续发展，并变得更加相互融合。很快，当我们看着<a class="ae ln" href="http://thispersondoesnotexist.com" rel="noopener ugc nofollow" target="_blank">thispersondoesnotexist.com</a>上生成的面孔时，他们会迎着我们的目光，和我们聊起他们生成的生活，揭示他们生成的人格的怪癖。</p><p id="8d0d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">最直接的危险也许是真实与生成的混合。我们已经看过了奥巴马作为人工智能傀儡的视频和和与的奇美拉的视频。很快，这些<a class="ae ln" href="https://en.wikipedia.org/wiki/Deepfake" rel="noopener ugc nofollow" target="_blank"> deepfakes </a>就会变成个人的了。所以，当你妈妈打电话说她需要 500 美元电汇到开曼群岛时，问问你自己:这真的是我妈妈吗，还是一个语言生成人工智能从我妈妈 5 年前发布的脸书视频中获得了她的语音皮肤？</p><p id="a522" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但是现在，让我们先欣赏一下关于会说话的独角兽的故事。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="7312" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> <em class="lo">关于这个和相关项目的更新，请随时关注我的</em> </strong> <a class="ae ln" href="https://twitter.com/jesse_vig" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir"> <em class="lo">推特</em> </strong> </a> <strong class="kt ir"> <em class="lo">。</em> </strong></p><p id="2522" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">【<strong class="kt ir">更新】:</strong> <em class="lo">厄尼</em>现正式拍摄(两次！)—参见通过 k<strong class="kt ir">N</strong>owledge<strong class="kt ir">I</strong>nt<strong class="kt ir">E</strong>gration 和<a class="ae ln" href="https://arxiv.org/abs/1905.07129" rel="noopener ugc nofollow" target="_blank">E<strong class="kt ir">E</strong>enhanced Language<strong class="kt ir">R</strong>presentation 通过 I<strong class="kt ir">N</strong>Eentities</a>按原样</p><p id="d8d6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">资源:</strong></p><p id="8082" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>用于 Jupyter 和 Colab 笔记本电脑的可视化工具，使用这些出色的工具/框架构建:</p><ul class=""><li id="914e" class="ol om iq kt b ku kv kx ky la on le oo li op lm oq or os ot bi translated"><a class="ae ln" href="https://github.com/tensorflow/tensor2tensor/tree/master/tensor2tensor/visualization" rel="noopener ugc nofollow" target="_blank"> Tensor2Tensor 可视化工具</a>，由<a class="ou ov ep" href="https://medium.com/u/a3ad3e75263b?source=post_page-----8252f683b2f8--------------------------------" rel="noopener" target="_blank"> Llion Jones </a>打造。</li><li id="8d95" class="ol om iq kt b ku ow kx ox la oy le oz li pa lm oq or os ot bi translated"><a class="ae ln" href="https://medium.com/huggingface" rel="noopener">拥抱脸</a>的<a class="ae ln" href="https://medium.com/syncedreview/hugging-face-releases-pytorch-bert-pretrained-models-and-more-b8a7839e7730" rel="noopener"> Pytorch 实现</a>的 GPT-2</li></ul><p id="8388" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">供进一步阅读:</strong></p><p id="2736" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" rel="noopener" target="_blank" href="/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77">解构 BERT:从 1 亿个参数中提取 6 种模式</a></p><p id="0e16" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" rel="noopener" target="_blank" href="/deconstructing-bert-part-2-visualizing-the-inner-workings-of-attention-60a16d86b5c1">解构伯特，第 2 部分:可视化注意力的内部运作</a></p><p id="5774" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" rel="noopener" target="_blank" href="/a-i-plays-mad-libs-and-the-results-are-terrifying-78fa44e7f04e">人工智能玩疯狂图书馆，结果是可怕的</a></p><p id="3886" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><a class="ae ln" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">图解变压器</a>教程</p></div></div>    
</body>
</html>