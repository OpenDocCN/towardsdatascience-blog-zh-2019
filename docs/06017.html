<html>
<head>
<title>Deep Deterministic Policy Gradients with SONY’s NNabla</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">索尼 NNabla 的深度确定性策略梯度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-deterministic-policy-gradients-with-sonys-nnabla-24357a0a87bb?source=collection_archive---------23-----------------------#2019-09-01">https://towardsdatascience.com/deep-deterministic-policy-gradients-with-sonys-nnabla-24357a0a87bb?source=collection_archive---------23-----------------------#2019-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="4432" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你好，我是一名研究深度强化学习的研究生。我之前写过一篇关于用 NNabla 实现 Deep Q-Network 的博文。</p><div class="kl km gp gr kn ko"><a rel="noopener follow" target="_blank" href="/deep-q-network-implementation-with-sonys-nnabla-490d945deb8e"><div class="kp ab fo"><div class="kq ab kr cl cj ks"><h2 class="bd ir gy z fp kt fr fs ku fu fw ip bi translated">利用索尼的 NNabla 实现深度 Q 网络</h2><div class="kv l"><h3 class="bd b gy z fp kt fr fs ku fu fw dk translated">NNABLA 是什么？</h3></div><div class="kw l"><p class="bd b dl z fp kt fr fs ku fu fw dk translated">towardsdatascience.com</p></div></div><div class="kx l"><div class="ky l kz la lb kx lc ld ko"/></div></div></a></div><p id="969a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，我将通过 NNabla 引入深度确定性策略梯度(DDPG)。完整实现是<a class="ae le" href="https://gist.github.com/takuseno/055e822516f1e12df1f0a0a1811ef2c2" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="ffbe" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">DDPG</h1><p id="d2a6" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">DDPG 是一种用于连续控制任务的策略梯度方法。</p><div class="kl km gp gr kn ko"><a href="https://arxiv.org/abs/1509.02971" rel="noopener  ugc nofollow" target="_blank"><div class="kp ab fo"><div class="kq ab kr cl cj ks"><h2 class="bd ir gy z fp kt fr fs ku fu fw ip bi translated">具有深度强化学习的连续控制</h2><div class="kv l"><h3 class="bd b gy z fp kt fr fs ku fu fw dk translated">我们将深度 Q 学习成功背后的思想应用于持续行动领域。我们提出一个…</h3></div><div class="kw l"><p class="bd b dl z fp kt fr fs ku fu fw dk translated">arxiv.org</p></div></div></div></a></div><p id="c0ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 DDPG，有两个网络:一个政策网络(行动者)和一个行动价值网络(批评家)。这种算法被称为“行动者-评论家”，因为行动者学习政策函数以最大化累积回报，评论家学习(行动)价值函数以正确预测回报。</p><p id="40af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们用 NNabla 制作这两个神经网络。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="3817" class="mr lg iq mn b gy ms mt l mu mv">import nnabla as nn<br/>import nnabla.functions as F<br/>import nnabla.parametric_functions as PF</span><span id="dc39" class="mr lg iq mn b gy mw mt l mu mv">def q_network(obs_t, action_t):<br/>    with nn.parameter_scope('critic'):<br/>        out = PF.affine(obs_t, 64, name='fc1')<br/>        out = F.tanh(out)<br/>        out = F.concatenate(out, action_t, axis=1)<br/>        out = PF.affine(out, 64, name='fc2')<br/>        out = F.tanh(out)<br/>        out = PF.affine(out, 1, name='fc3')<br/>        return out</span><span id="2987" class="mr lg iq mn b gy mw mt l mu mv">def policy_network(obs, action_size):<br/>    with nn.parameter_scope('actor'):<br/>        out = PF.affine(obs, 64, name='fc1')<br/>        out = F.tanh(out)<br/>        out = PF.affine(out, 64, name='fc2')<br/>        out = F.tanh(out)<br/>        out = PF.affine(out, action_size, name='fc3')<br/>        return F.tanh(out)</span></pre><p id="6f0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相当简单！NNabla 基于由<code class="fe mx my mz mn b">nn.parameter_scope()</code>声明的类似 TensorFlow 的命名空间系统来管理权重和偏差。</p><h1 id="8dc9" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">批评家的流失</h1><p id="ef1a" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">用一步时差(TD)误差训练评论家:</p><figure class="mi mj mk ml gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi na"><img src="../Images/e8ccb567c72514e14f9e3efe691f4bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdP63o-ywfL2XRYljThxtQ.png"/></div></div></figure><figure class="mi mj mk ml gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/8260fa3d15671994009f8e5972d48fae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tg6WGAF0Irz7w50upp5h_w.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">from the original paper.</figcaption></figure><p id="2fe5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可以用 NNabla 写成如下形式:</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="8179" class="mr lg iq mn b gy ms mt l mu mv"># N is a batch size<br/># state_size is a size of input vectors<br/># action_size is a size of the policy output<br/>obs_t = nn.Variable((N, state_size))   # observation at t<br/>act_t = nn.Variable((N, action_size))  # take action at t<br/>rew_tp1 = nn.Variable((N, 1))          # reward value at t+1<br/>obs_tp1 = nn.Variable((N, state_size)) # observation at t+1<br/>ter_tp1 = nn.Variable((N, 1))          # 1.0 if terminal state</span><span id="9163" class="mr lg iq mn b gy mw mt l mu mv">with nn.parameter_scope('trainable'):<br/>    q_t = q_function(obs_t, act_t)</span><span id="86dc" class="mr lg iq mn b gy mw mt l mu mv">with nn.parameter_scope('target'):<br/>    act_tp1 = policy_function(obs_tp1, action_size)<br/>    q_tp1 = q_function(obs_tp1, act_tp1)</span><span id="1d8e" class="mr lg iq mn b gy mw mt l mu mv">y = rew_tp1 + gamma * q_tp1 * (1.0 - ter_tp1)<br/>critic_loss = F.mean(F.squared_error(q_t, y))</span></pre><p id="a47e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的代码构造了计算 TD 误差平方的计算图。</p><h1 id="d5ac" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">演员的损失</h1><p id="48b8" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">相比之下，地面实况操作不是直接可用的。因此，演员被训练来最大化评论家的价值估计。策略网络的梯度计算如下。</p><figure class="mi mj mk ml gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nm"><img src="../Images/9173dd83a70f48dee3018804e97f1bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H8Ict0LCJkxsUSkHjEj9Ww.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">from the original paper</figcaption></figure><p id="507c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个梯度计算也可以写成 NNabla。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="6a4e" class="mr lg iq mn b gy ms mt l mu mv">with nn.parameter_scope('trainable'):<br/>    policy_t = policy_function(obs_t, action_size)<br/>    q_t_with_actor = q_function(obs_t, policy_t)</span><span id="1ab0" class="mr lg iq mn b gy mw mt l mu mv">actor_loss = -F.mean(q_t_with_actor) # gradient ascent</span></pre><p id="30a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mx my mz mn b">-</code>有必要更新行动者以最大化价值估计。最终，行动者被引导到在行动-价值函数中获得的高价值空间。</p><h1 id="f28b" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">目标更新</h1><p id="c5c2" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">与 DQN 不同，DDPG 的目标更新是逐步将目标功能与最新参数同步。</p><pre class="mi mj mk ml gt mm mn mo mp aw mq bi"><span id="54ee" class="mr lg iq mn b gy ms mt l mu mv">with nn.parameter_scope('trainable'):<br/>    trainable_params = nn.get_parameters()</span><span id="6c7c" class="mr lg iq mn b gy mw mt l mu mv">with nn.parameter_scope('target'):<br/>    target_params = nn.get_parameters()</span><span id="df08" class="mr lg iq mn b gy mw mt l mu mv">update_ops = []<br/>for key, src in trainable_params.items():<br/>    dst = target_params[key]<br/>    update_ops.append(F.assign(dst, (1.0 - tau) * dst + tau * src)))<br/>target_update = F.sink(*update_ops)</span></pre><p id="2a59" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe mx my mz mn b">F.assign</code>和<code class="fe mx my mz mn b">tf.assign</code>差不多。<code class="fe mx my mz mn b">F.sink</code>是同时运行所有输入的终端节点。</p><h1 id="4905" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">放在一起</h1><figure class="mi mj mk ml gt nb"><div class="bz fp l di"><div class="nn no l"/></div></figure><h1 id="b951" class="lf lg iq bd lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc bi translated">结论</h1><p id="9135" class="pw-post-body-paragraph jn jo iq jp b jq md js jt ju me jw jx jy mf ka kb kc mg ke kf kg mh ki kj kk ij bi translated">我介绍了索尼深度学习框架 NNabla 的 DDPG 实现。如果你用 GPU 尝试这个实现，你会发现它的训练速度很快。</p><p id="9881" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果您需要更多关于 NNabla 的信息，请访问<a class="ae le" href="https://nnabla.org/" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div></div>    
</body>
</html>