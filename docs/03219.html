<html>
<head>
<title>What’s Linear About Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归的线性是什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whats-linear-about-logistic-regression-7c879eb806ad?source=collection_archive---------6-----------------------#2019-05-23">https://towardsdatascience.com/whats-linear-about-logistic-regression-7c879eb806ad?source=collection_archive---------6-----------------------#2019-05-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="eca5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在逻辑回归中，我们如何从决策边界到概率？</h2></div><p id="7832" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">已经有很多关于逻辑回归的令人惊讶的文章和视频，但我很难理解概率和逻辑线性之间的联系，所以我想我应该在这里为自己和那些可能经历同样事情的人记录下来。</p><p id="21e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也将揭示逻辑回归的“逻辑”部分来自哪里！</p><p id="5ba8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇博客的重点是对逻辑模型和线性模型之间的关系建立一个直观的理解，所以我只是做一个什么是逻辑回归的概述，并深入这种关系。为了更完整地解释这个令人敬畏的算法，这里有一些我最喜欢的资源:</p><ul class=""><li id="c85c" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><a class="ae lk" href="https://www.youtube.com/watch?v=-la3q9d7AKQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=-la3q9d7AKQ</a></li><li id="fb53" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/logistic-regression-detailed-overview-46c4da4303bc">https://towards data science . com/logistic-regression-detailed-overview-46 C4 da 4303 BC</a></li><li id="05ae" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><a class="ae lk" href="https://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/logistic _ regression . html</a></li><li id="a252" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated"><a class="ae lk" href="https://christophm.github.io/interpretable-ml-book/logistic.html" rel="noopener ugc nofollow" target="_blank">https://christophm . github . io/interpretable-ml-book/logistic . html</a></li></ul><p id="338c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们来看看逻辑回归的要点。</p><blockquote class="lq lr ls"><p id="0f64" class="kf kg lt kh b ki kj jr kk kl km ju kn lu kp kq kr lv kt ku kv lw kx ky kz la ij bi translated"><strong class="kh ir">什么是逻辑回归？</strong></p></blockquote><p id="7d6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与线性回归一样，逻辑回归用于建模一组自变量和因变量之间的关系。</p><p id="df83" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与线性回归不同，因变量是分类变量，这就是它被视为分类算法的原因。</p><p id="b412" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归可用于预测:</p><ul class=""><li id="261a" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">电子邮件是垃圾邮件还是非垃圾邮件</li><li id="e244" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">肿瘤是不是恶性的</li><li id="8642" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">一个学生将通过或不通过考试</li><li id="b33d" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">我会后悔在凌晨 12 点吃饼干</li></ul><p id="43da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面列出的应用是<strong class="kh ir">二项式/二元逻辑回归</strong>的例子，其中目标是二分的(2 个可能的值)，但是你可以有 2 个以上的类(多项逻辑回归)。</p><p id="4aaf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些分类是基于模型产生的概率和某个阈值(通常为 0.5)进行的。如果一个学生通过的概率大于 0.5，则她被预测为通过。</p><p id="1201" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始探究这些概率是如何计算的。</p><blockquote class="lq lr ls"><p id="2ea7" class="kf kg lt kh b ki kj jr kk kl km ju kn lu kp kq kr lv kt ku kv lw kx ky kz la ij bi translated"><strong class="kh ir">乙状函数</strong></p></blockquote><p id="3b3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们可视化一个带有二进制目标变量的数据集，我们会得到这样的结果:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/fc94eca5ea26e5ed6baa57b36ae5ed92.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*buqlQ5XV8xCdqRIbnoMUEw.png"/></div></figure><p id="5df5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里有几个原因可以解释为什么拟合直线可能不是一个好主意:</p><ol class=""><li id="f586" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la mf lh li lj bi translated">在线性回归中，因变量的范围可以从负 inf 到正 inf，但是我们试图预测应该在 0 和 1 之间的概率。</li><li id="428f" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la mf lh li lj bi translated">即使我们创建了一些规则来将这些越界值映射到标签，分类器也会对离群值非常敏感，这会对其性能产生不利影响。</li></ol><p id="4cf3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们用在 0 和 1 附近变平的 S 形来代替直线:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/0a55ecfc0c46634b06162e2186d54315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*w0rl5LI-GfTf0HHdcNxpRg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk"><a class="ae lk" href="https://cvxopt.org/examples/book/logreg.html" rel="noopener ugc nofollow" target="_blank">source</a></figcaption></figure><p id="1877" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这被称为 sigmoid 函数，其形式如下:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/739d3392bd18fb181e266203dbec598a.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*OzH4dNNrdsS_19rlyqs6CQ.png"/></div></figure><p id="c34f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此函数根据某些因素的组合返回某个观察值属于某个类的概率。</p><p id="508c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们求解线性函数，我们会得到几率的对数或者是<strong class="kh ir"> logit: </strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/fb1bd8d3d319d835bb5a1d0003581924.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*ZI20-FRnRZ7sn1Ren6TXyw.png"/></div></figure><p id="e6af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意当 p(x) ≥0.5，βX ≥ 0 时。</p><p id="a990" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是等一下，这个神奇的函数是从哪里来的，线性模型是怎么进去的？为了回答这个问题，我们来看看逻辑回归是如何形成其决策边界的。</p><blockquote class="lq lr ls"><p id="a9e9" class="kf kg lt kh b ki kj jr kk kl km ju kn lu kp kq kr lv kt ku kv lw kx ky kz la ij bi translated"><strong class="kh ir">决定边界</strong></p></blockquote><p id="6b7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每个伟大的逻辑回归模型背后都有一个不可观察的(潜在的)线性回归模型，因为它真正试图回答的问题是:</p><p id="a050" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">"给定一些特征 x，一个观察值属于第一类的概率是多少？"</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f56a8d3f4ce0940ab6f4d81767015001.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*jLj_xjNBXm8g443Bmj6bUQ.png"/></div></figure><p id="f1b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看一个例子。</p><p id="46be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们想根据一个学生花了多少时间学习和睡觉来预测她是否能通过考试:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/7fc5fe1b347f30ae95f3890bfdb21942.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*FGC94hw7LmI2B7EmWj3Vwg.png"/></div><figcaption class="mh mi gj gh gi mj mk bd b be z dk"><strong class="bd mp">Source: </strong><a class="ae lk" href="https://www.scilab.org/tutorials/machine-learning-%E2%80%93-logistic-regression-tutorial" rel="noopener ugc nofollow" target="_blank">scilab</a></figcaption></figure><p id="15a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们通过绘制针对 Slept 的 Studied 来更好地理解我们的数据，并对我们的类进行颜色编码以可视化这种分离:</p><pre class="ly lz ma mb gt mq mr ms mt aw mu bi"><span id="2ed0" class="mv mw iq mr b gy mx my l mz na">import pandas as pd<br/>import matplotlib<br/>import matplotlib.pyplot as plt</span><span id="0c07" class="mv mw iq mr b gy nb my l mz na">exams = pd.read_csv('data_classification.csv', names=['Studied','Slept','Passed'])</span><span id="4e75" class="mv mw iq mr b gy nb my l mz na">fig = plt.figure()<br/>ax = fig.add_subplot(111)</span><span id="44a1" class="mv mw iq mr b gy nb my l mz na">colors = [‘red’, ’blue’]</span><span id="78eb" class="mv mw iq mr b gy nb my l mz na">ax.scatter(exams.Studied, exams.Slept, s=25, marker=”o”,  c=exams[‘Passed’], cmap=matplotlib.colors.ListedColormap(colors))</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/654ed5b325d55f98057d791cd280b3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*0Ed--rxWr3ab7EdTHZ9xjw.png"/></div></figure><p id="84d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看着这个图，我们可以假设一些关系:</p><ul class=""><li id="a0de" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">花足够的时间学习并且睡眠充足的学生很可能通过考试</li><li id="1ff1" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">睡眠少于 2 小时但花了 8 小时以上学习的学生可能仍然会通过(我肯定在这个组里)</li><li id="a94c" class="lb lc iq kh b ki ll kl lm ko ln ks lo kw lp la lg lh li lj bi translated">偷懒不睡觉的学生可能已经接受了他们通不过的命运</li></ul><p id="37f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的想法是，这两个类别之间有一条清晰的分界线，我们希望逻辑回归能为我们找到这一点。让我们拟合一个逻辑回归模型，并用模型的决策边界覆盖这个图。</p><pre class="ly lz ma mb gt mq mr ms mt aw mu bi"><span id="2172" class="mv mw iq mr b gy mx my l mz na">from sklearn.linear_model import LogisticRegression</span><span id="0c23" class="mv mw iq mr b gy nb my l mz na">features = exams.drop(['Passed'],axis=1)<br/>target = exams['Passed']</span><span id="3467" class="mv mw iq mr b gy nb my l mz na">logmodel = LogisticRegression()<br/>logmodel.fit(features, target)<br/>predictions = logmodel.predict(features)</span></pre><p id="473f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以打印出参数估计值:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/dbec592358ec01972f6ba2c2f0e98dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*g7j9PTDc4Wajtof_46yMwQ.png"/></div></figure><p id="c2c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">利用这些估计，我们可以计算出边界。因为我们的阈值设置为 0.5，所以我将 logit 保持在 0。这也允许我们在 2d 中查看边界:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/8c834ae01418a3e40a3fe270de282f5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*bIdarOh1oNRBbqB9KWyJyw.png"/></div></figure><pre class="ly lz ma mb gt mq mr ms mt aw mu bi"><span id="bd9c" class="mv mw iq mr b gy mx my l mz na">exams['boundary'] = (-logmodel.intercept_[0] - (logmodel.coef_[0][0] * features['Studied'])) / logmodel.coef_[0][1]</span></pre><p id="184e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的散点图上看起来是这样的:</p><pre class="ly lz ma mb gt mq mr ms mt aw mu bi"><span id="8a0a" class="mv mw iq mr b gy mx my l mz na">plt.scatter(exams['Studied'],exams['Slept'], s=25, marker="o", c=exams['Passed'], cmap=matplotlib.colors.ListedColormap(colors))</span><span id="ca2d" class="mv mw iq mr b gy nb my l mz na">plt.plot(exams['Studied'], exams['boundary'])</span><span id="fe58" class="mv mw iq mr b gy nb my l mz na">plt.show()</span></pre><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/bfb939fae4b69c7b03554856317bba91.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*LrAOc32z_PtghLnu6VW7ag.png"/></div></figure><p id="dceb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这看起来很合理！那么 Logistic 回归是如何利用这条线来分配类标签的呢？它着眼于每个单独的观察和线性模型之间的距离。它会将这条线以上的所有点标记为 1，下面的所有点标记为 0。这条线上的任何点都可能属于任何一类(概率为 0.5)，所以为了将一个点分类为 1，我们感兴趣的是这条线和我们的观察之间的距离大于 0 的概率。</p><p id="3d7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事实证明，在逻辑回归中，这个距离被假定为遵循<strong class="kh ir">逻辑</strong>分布。</p><p id="5949" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">换句话说，逻辑回归中潜在线性回归模型的误差项被假定为服从逻辑分布。</p><p id="d0e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着当我们问:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/f56a8d3f4ce0940ab6f4d81767015001.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/format:webp/1*jLj_xjNBXm8g443Bmj6bUQ.png"/></div></figure><p id="d988" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们真的在问:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7cd40d055080d3a8c3c907b686df44e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*udWSivgsT1UrUrt98az_4Q.png"/></div></figure><p id="5627" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了计算这种概率，我们对逻辑分布进行积分，以获得其累积分布函数:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a83991a2c063b3bc281adf0e97c74064.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*QNJZk81VHxlRXSNXhqVvHg.png"/></div></figure><p id="694e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">哦嘿！是乙状结肠函数:)。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="48dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Tada！您现在应该能够更直观地在 sigmoid 函数和线性回归函数之间来回走动了。我希望理解这种联系能让你和我一样对逻辑回归有更高的评价。</p></div></div>    
</body>
</html>