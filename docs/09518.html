<html>
<head>
<title>LSTM to Detect Neanderthal DNA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM 探测尼安德特人的 DNA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstm-to-detect-neanderthal-dna-843df7e85743?source=collection_archive---------6-----------------------#2019-12-15">https://towardsdatascience.com/lstm-to-detect-neanderthal-dna-843df7e85743?source=collection_archive---------6-----------------------#2019-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fdff" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/dl-for-life-sciences" rel="noopener" target="_blank">生命科学的深度学习</a></h2><div class=""/><div class=""><h2 id="6486" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">用于古代基因组学的长记忆神经网络</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ec413a63a8bc98176ac7e51e0d86a9f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SETbjzHf2OvKtrn5uFWReA.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Image licence from <a class="ae lh" href="https://www.istockphoto.com" rel="noopener ugc nofollow" target="_blank">iStock</a></figcaption></figure><p id="048d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是我的专栏<a class="ae lh" href="https://towardsdatascience.com/tagged/dl-for-life-sciences" rel="noopener" target="_blank"> <strong class="lk jd">生命科学深度学习</strong> </a> <strong class="lk jd"> </strong>的第八篇文章，在这里我演示了如何将深度学习用于<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-ancient-dna-df042dc3c73d">古代 DNA </a>、<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-for-single-cell-biology-935d45064438">单细胞生物学</a>、<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-for-data-integration-46d51601f781">组学数据集成</a>、<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-for-clinical-diagnostics-ca7bc254e5ac">临床诊断</a>和<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-microscopy-imaging-865b521ec47c">显微成像</a>。在关于尼安德特人基因<strong class="lk jd"/>的<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7"> <strong class="lk jd">深度学习中，我强调了深度学习和<a class="ae lh" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>对于<a class="ae lh" href="https://en.wikipedia.org/wiki/Ancient_DNA" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">古代基因组</strong> </a> <strong class="lk jd"> </strong>的巨大潜力，并演示了如何实际开始使用它来推断现代人类基因组中<a class="ae lh" href="https://en.wikipedia.org/wiki/Interbreeding_between_archaic_and_modern_humans" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">尼安德特人渗入</strong> </a>的区域。现在，我们将运用长记忆<a class="ae lh" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>的全部力量，更好地预测现代人类基因组中尼安德特人祖先的片段。</strong></a></p><h1 id="0f0c" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">HMM 对 LSTM 的古基因组学</h1><p id="f5e4" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">DNA 是一个具有<strong class="lk jd">长记忆</strong>的序列，这是通过沿着序列的<strong class="lk jd">长程相关性</strong>表现出来的，这被称为<a class="ae lh" href="https://en.wikipedia.org/wiki/Linkage_disequilibrium" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">连锁不平衡</strong> </a>。然而，古代基因组学中的许多分析都是使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Hidden_Markov_model" rel="noopener ugc nofollow" target="_blank">隐马尔可夫模型(HMM) </a>进行的，这是一种<strong class="lk jd">无记忆</strong>模型，只能考虑前面的几个步骤。<a class="ae lh" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">长短期记忆(LSTM) </strong> </a>人工神经网络<a class="ae lh" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">已经显示出</a>到<strong class="lk jd">在许多应用中胜过</strong><strong class="lk jd">HMM</strong>，例如语音识别、文本生成等。假设有足够的数据可用，通过利用他们独特的能力来按顺序<strong class="lk jd"> </strong>记忆<strong class="lk jd">许多先前的步骤</strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nb"><img src="../Images/e9395033d59d89eefd950dbabe43bc29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1UKz2W0HsN0QxXEnItkQQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://link.springer.com/chapter/10.1007/978-3-319-51469-7_8" rel="noopener ugc nofollow" target="_blank">From Panzner and Cimiano, Machine Learning, Optimization, and Big Data, 2016</a></figcaption></figure><p id="163d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基因组学和古代基因组学代表了一个真正的大数据资源，这要归功于<a class="ae lh" href="https://en.wikipedia.org/wiki/DNA_sequencing" rel="noopener ugc nofollow" target="_blank">下一代测序(NGS) </a>，它提供了<strong class="lk jd">数百万和数十亿个</strong> <strong class="lk jd">序列</strong>，可以用作训练 LSTMs 的训练示例/统计观察。因此，古代基因组数据是深度学习的一份礼物！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/83b71fad89ed148ab0dbcb9ef1820766.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/1*RH-rWUBGckjrPfYOe2c_kQ.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://tenor.com/search/boromir-gifs" rel="noopener ugc nofollow" target="_blank">Image source</a></figcaption></figure><p id="4ae7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管基因组学/古代基因组学的培训资源<strong class="lk jd">繁荣</strong>，大多数分析仍然是在<strong class="lk jd">模拟的</strong>基因组数据上进行的，其中<a class="ae lh" href="https://en.wikipedia.org/wiki/Coalescent_theory" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">合并理论</strong> </a>可能是最流行的<a class="ae lh" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2916717/" rel="noopener ugc nofollow" target="_blank">模拟人口统计学和群体遗传学中的选择</a>的框架。</p><h1 id="7cd1" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">尼安德特人 DNA 上的 LSTM +单词嵌入</h1><p id="0e9d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在这里，我将扩展我在之前的帖子<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7"> <strong class="lk jd">中表达的关于使用<a class="ae lh" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"> NLP </a>进行古代基因组学的想法，并展示 LSTMs </strong>在分析测序数据方面的<strong class="lk jd">优势。你可能已经注意到，前一篇文章中的深度学习不是特别“深”，而是相当“广”，此外，随机森林似乎在<a class="ae lh" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋</a>模型中表现得更好。在这里，我将实现一个 LSTM 模型，它可以达到 99%的准确率来检测从尼安德特人那里继承的 DNA 片段。我在<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7">之前的帖子中展示了如何提取尼安德特人渐渗和耗尽的序列，</a>所以在这里我将从读取序列开始，将它们分割成 200 个核苷酸长的子序列，每个子序列代表一个句子，这样我们可以进一步将每个句子分割成<strong class="lk jd"> k-mers </strong> /句子的单词。</strong></a></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="bd7e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们最终得到了 737，340 个句子，它们属于两类:尼安德特人渗入和耗尽。下一步是通过使用 Python 中的<strong class="lk jd">标记器</strong>类将 k-mers /单词转换成<strong class="lk jd">整数</strong>来对句子进行热编码。注意，在我们的例子中并不需要填充，因为所有的句子都是一样长的，<strong class="lk jd"> 191 k-mers 长</strong>，但是我把它包含在这里是为了通用性。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="23a4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在我们的例子中，词汇量为 964 114，小于 4^10 = 1 048 576，这意味着不是所有可能的由 4 个字符组成的 10 聚体都存在于序列中。最后，我们定义了一个<strong class="lk jd">序列模型</strong>，从<strong class="lk jd"> Keras 嵌入层</strong>开始，该嵌入层在对序列进行分类时学习<strong class="lk jd">单词嵌入</strong>，随后是<strong class="lk jd">双向 LSTM </strong>和密集层。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/53e778ac4961c6c76cab0ebc02796554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aV9vOMGpp09O68QlA0bLtA.png"/></div></div></figure><p id="9463" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在第一层使用<strong class="lk jd">字嵌入</strong>的优点是<strong class="lk jd">维数从 964×114 减少到 10 维。这通过强制<strong class="lk jd">相似单词</strong>共享拟合参数/权重来减少过度拟合并提高模型的<strong class="lk jd">可推广性</strong>。现在我们可以开始训练我们的模型了。</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ng"><img src="../Images/01689b65682effbe1957db486cceb037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u-px7j-aYggPNCLvr_h3RA.png"/></div></div></figure><p id="204a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">仅执行 5 个时期的训练就足够了，因为模型在验证数据集上很快达到了令人惊讶的 99% 的<strong class="lk jd">准确度。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nh"><img src="../Images/50420e1f88ec079ebc6d8b318fd762ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FnH2mezT55KYBdm2vX44g.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nh"><img src="../Images/d972e7eb190254bd87be1ebc76e6062b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H6EurPPmLYmM_nwv4VFY9w.png"/></div></div></figure><p id="938c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<strong class="lk jd">测试数据集</strong>上评估模型性能，我们再次获得对尼安德特人渐渗序列与耗尽序列进行分类的准确度的<strong class="lk jd"> 99% </strong> <strong class="lk jd">。</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/3fc3f107c4b992a408ed3100eaa238b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UyxzyJIB-v36qkCyxK_s_g.png"/></div></div></figure><p id="2558" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们有了一个经过训练的 LSTM 模型，我们稍后将使用它来预测从尼安德特人那里继承的基因序列。现在是对模型进行<strong class="lk jd">解释</strong>的时候了，这包括词汇表的可视化和检测驱动分类的最具预测性的 k-mers。</p><h1 id="d3c6" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">可视化单词嵌入</h1><p id="dcde" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">目前，每个 k-mer /单词由一个<strong class="lk jd"> 10 维向量</strong>表示，因为我们在网络前端应用了一个<strong class="lk jd">嵌入层</strong>。为了可视化嵌入层已经学习了什么，我们首先需要保存层的<strong class="lk jd">权重</strong>和词汇表的<strong class="lk jd">单词</strong>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="8cc1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了可视化，我们可以在这里使用<strong class="lk jd"> Tensorflow 嵌入式投影仪</strong><a class="ae lh" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">http://projector.tensorflow.org/</a>并使用<a class="ae lh" href="https://arxiv.org/abs/1802.03426" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> UMAP </strong> </a>非线性降维技术研究 k-mers 的<strong class="lk jd">聚类。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/df2e76d92833707ee8032846e0422655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-Mwl8rQdvfdaVR1EY3diQ.png"/></div></div></figure><p id="5c70" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">嵌入层学习到的 k-mers 之间的关系看起来与<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7">上一篇文章</a>中提出的 Word2Vec 嵌入非常不同，并且没有揭示富含 AT 和富含 GC 的 k-mers 的任何明显聚类。</p><h1 id="ba5e" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">识别最具预测性的 K-mers</h1><p id="9853" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为神经网络构建特征重要性的一种方法是对输入数据施加一些<strong class="lk jd">扰动</strong>，并在网络输出端监控预测准确度的<strong class="lk jd">变化</strong>。这里，我们希望找到<strong class="lk jd">最具</strong> <strong class="lk jd">信息量的 k-mers </strong>，然而，在我们的例子中，输入矩阵 X 具有维度(737340，191)，其中第一维表示神经网络的训练样本的数量，第二维对应于每个句子/序列中的单词数量。这意味着每个单词/ k-mer 的<strong class="lk jd">索引</strong>(或句子中的<strong class="lk jd">位置</strong>)是输入矩阵 x 的<strong class="lk jd">特征</strong>。因此，如果我们在 737340 个样本中一次一个地混洗 191 个特征中的每一个，并检查与未扰动的输入矩阵相比准确度的下降，我们可以根据它们对最终预测的重要性对特征进行排序。在我们的例子中，排列特征相当于确定单词/ k-mers 在所有句子/序列中最重要的位置。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nk"><img src="../Images/22fd297e396a221a274c1ece836e7c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KF9v1hWK3JBOLxVqjBvj7A.png"/></div></div></figure><p id="2e51" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们选择改变准确度高于 0.01 的单词的位置/索引，我们得出单词 0、4、7、18、34、52、75、81、104、130、146 和 182 是最重要的。也许，我们在句子的开头的<strong class="lk jd">处观察到一些重要单词的浓缩。现在，我们简单地检查一下在上述位置的所有句子中最常见的单词/ k-mers。</strong></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/90360ed0bf7ec3408d9b4ab44fbfaa0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MFUHKBLc5j1C4b5UnUtXw.png"/></div></div></figure><p id="0640" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">令人放心的是，我们观察到<strong class="lk jd">富含 at 的 k-mers </strong>在尼安德特人渐渗序列和耗尽序列之间的区分度最高，这也在<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7">之前的帖子</a>中显示过。</p><h1 id="b6e1" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">预测尼安德特人的基因</h1><p id="5482" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">现在我们将考虑人类基因序列和经过训练的 LSTM 模型，以预测每个人类基因从尼安德特人遗传的可能性。在<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7">之前的帖子</a>中，我解释了如何将基因序列提取到一个单独的 fasta 文件中，所以现在我们将读取这些序列，将它们分成 k-mers，用 Tokenizer 将 k-mers 转换成整数，并使用训练好的 LSTM 模型预测尼安德特人的祖先。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/089d1c6b9e7644c4da20653683a3c2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*fL8ek8X7QNMzL8nof5pFcA.png"/></div></figure><p id="3679" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">检查被预测为从尼安德特人遗传而来的<strong class="lk jd">基因的数量，我们确认了来自<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7">先前帖子</a>的结果，即<strong class="lk jd">的绝大多数基因</strong>，即 31 000 个被分析的人类基因中的 22 000 个<strong class="lk jd">与尼安德特人的祖先</strong>无关。因此，我们再次得出结论，由于某种原因，进化将尼安德特人的祖先从人类基因组中最具功能的基因中挤出来了。仔细看看<strong class="lk jd"/>尼安德特人基因列表，并将其与<a class="ae lh" rel="noopener" target="_blank" href="/deep-learning-on-neanderthal-genes-ad1478cf37e7">上一篇</a>帖子中随机森林模型的预测进行比较，我们观察到许多基因与各种人类特征和疾病有关。例如，<strong class="lk jd"> ANK3 </strong>基因被&gt;预测为尼安德特人祖先的概率为 99%。已知这种基因与<strong class="lk jd">双相情感障碍</strong>有关，并且主要在<strong class="lk jd">人脑</strong>中表达。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/0f768cc46a0a42ce04d2c9c79fe53403.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JvbSaaMB0ngA35MPeDm8aA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">ANK3 gene predicted to be of Neanderthal origin is <a class="ae lh" href="https://www.nature.com/articles/ng.209" rel="noopener ugc nofollow" target="_blank">linked to Bipolar Disorder</a></figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/71ec590b54643345dd91f377a992fbfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Bz9z9ESuIyVX6SO2ykOTA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">ANK3 is a human brain-specific gene based on <a class="ae lh" href="https://gtexportal.org/home/gene/ANK3" rel="noopener ugc nofollow" target="_blank">GTEX human tissue expression data</a></figcaption></figure><p id="5aaa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，<a class="ae lh" href="https://www.nature.com/articles/s41598-017-06587-0#additional-information" rel="noopener ugc nofollow" target="_blank">一些研究</a>表明，人类大脑中与<strong class="lk jd">精神疾病</strong>有关的部分可能是由尼安德特人的基因遗传形成的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/b2cdad0d3d8f7d6b21d75271a92f469b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*OHc99tbLRp5oBqi4TZkmdg.jpeg"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Human brain harbors “residual echo” of Neanderthal genes, <a class="ae lh" href="https://www.nimh.nih.gov/news/science-news/2017/our-brains-harbor-residual-echo-of-neanderthal-genes.shtml" rel="noopener ugc nofollow" target="_blank">image source</a></figcaption></figure><p id="13b0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这就是进化科学能够告知生物医学导致现代人类疾病的特征的历史发展和起源的地方。</p><h1 id="eeaa" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">摘要</h1><p id="48e1" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">在这篇文章中，我们了解到将长记忆模型如<strong class="lk jd"> LSTM </strong>应用于已知拥有<strong class="lk jd">长程相关性</strong>的 DNA 测序数据可能是有利的。我们展示了 LSTM 取得了令人印象深刻的准确性，并且在检测现代人类基因组中的尼安德特人基因渗入区域时，<strong class="lk jd">优于所有其他模型。对模型的解释揭示了富含 AT 的 k-mers 是尼安德特人和本土现代人类 DNA 序列的关键区别。我们预测大多数人类基因都没有尼安德特人的祖先，这意味着在进化过程中<strong class="lk jd">对</strong>尼安德特人祖先的强烈<strong class="lk jd">否定选择</strong>。许多被预测会从尼安德特人身上遗传的基因与有趣的人类特征相关联，例如双相情感障碍。</strong></p><p id="bebf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">像往常一样，让我在评论中知道你最喜欢的生命科学领域，你希望在深度学习框架中解决的领域。在媒体<a class="nq nr ep" href="https://medium.com/u/8570b484f56c?source=post_page-----843df7e85743--------------------------------" rel="noopener" target="_blank">关注我，在 Twitter @NikolayOskolkov 关注我，在</a><a class="ae lh" href="http://linkedin.com/in/nikolay-oskolkov-abb321186" rel="noopener ugc nofollow" target="_blank"> Linkedin 关注我。完整的 Jupyter 笔记本可以在我的</a><a class="ae lh" href="https://github.com/NikolayOskolkov/LSTMNeanderthalDNA" rel="noopener ugc nofollow" target="_blank"> github </a>上找到。我计划写下一篇关于<strong class="lk jd">如何用深度学习</strong>及时预测人口规模的帖子，敬请关注。</p></div></div>    
</body>
</html>