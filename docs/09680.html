<html>
<head>
<title>Black Hole Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">黑洞机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/black-hole-machine-learning-d7387a4bd30e?source=collection_archive---------14-----------------------#2019-12-19">https://towardsdatascience.com/black-hole-machine-learning-d7387a4bd30e?source=collection_archive---------14-----------------------#2019-12-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9cf7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/the-singularity-research" rel="noopener" target="_blank">奇点研究</a></h2><div class=""/><div class=""><h2 id="703c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过理论物理的神经网络</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi kr"><img src="../Images/7ce1f822a8ac5ba1d2ee4d072350d527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/0*ccbrw_kvVxoR2tL_"/></div></figure><p id="2409" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主持人:<a class="ae lv" href="https://sketchviz.com/" rel="noopener ugc nofollow" target="_blank"> Sketchviz </a></p><pre class="ks kt ku kv gt lw lx ly lz aw ma bi"><span id="1e60" class="mb mc it lx b gy md me l mf mg"># http://www.graphviz.org/content/cluster<br/>graph G {<br/>    node [style=filled,color=pink];<br/>    q0 -- H0 -- U -- V0 -- W0 -- A;<br/>    q1 -- H1 -- U -- V1 -- W0 -- B;<br/>    q2 -- H2 -- U -- V2 -- W1 -- C;<br/>    q3 -- H3 -- U -- V3 -- W1 -- D;<br/>    q4 -- H4 -- U -- V4 -- W2 -- E;<br/>}</span></pre><p id="488c" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们提供了一些链接，链接到使用张量网络的重要而有趣的代码和软件，张量网络是一种用于引力物理学和黑洞研究的数学工具。我们感兴趣的代码将涉及机器学习和人工智能的应用。我们将回顾张量网络的简史，并稍微解释一下它们是什么，我们将解释它们如何将理论物理与机器学习联系起来，并深入了解为什么谷歌这样的巨头会聘请李奥纳特·苏士侃这样的物理学家来咨询他们的人工智能和机器学习算法。我们还将展示量子计算如何发挥作用，以及如何使用量子计算机创建量子增强的人工智能和量子机器学习算法，迫使我们重新思考我们理解信息处理的方式。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="ba90" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated">用于 TensorFlow 的 Google tensor network 库</h2><p id="5550" class="pw-post-body-paragraph kz la it lb b lc ne kd le lf nf kg lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">今年早些时候(2019 年 6 月)，<a class="ae lv" href="https://ai.googleblog.com/2019/06/introducing-tensornetwork-open-source.html" rel="noopener ugc nofollow" target="_blank">谷歌</a>的人工智能博客发布了一篇关于为其臭名昭著的<a class="ae lv" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>机器学习平台建造的名为“<a class="ae lv" href="https://tensornetwork.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> TensorNetwork </a>的图书馆的帖子。它是为张量网络的操作而建立的(我们将进入张量网络的简史，并在下面解释它们是什么)。谷歌开发这个库的工作是与<a class="ae lv" href="https://x.company/" rel="noopener ugc nofollow" target="_blank"> X </a>、谷歌的<em class="nj"> moonshot factory </em>创新技术以及<a class="ae lv" href="https://www.perimeterinstitute.ca/research/research-initiatives/tensor-networks-initiative" rel="noopener ugc nofollow" target="_blank">Perimeter Institute for theory Physics</a>共同完成的。今年早些时候，周界研究所参与了一个黑洞的成像。除了在 Github 上发布代码，他们还撰写了三篇关于 TensorNetwork 库的应用和性能基准的研究论文:</p><p id="c5ef" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae lv" href="https://arxiv.org/abs/1905.01330" rel="noopener ugc nofollow" target="_blank"> TensorNetwork:物理和机器学习库</a></p><p id="cbaa" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TensorFlow 上的 TensorNetwork:使用树张量网络的自旋链应用</p><p id="69aa" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae lv" href="https://arxiv.org/abs/1906.06329" rel="noopener ugc nofollow" target="_blank">用于机器学习的 TensorNetwork】</a></p><p id="2ff0" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣阅读 TensorNetwork 的文档并深入研究代码，你可以查看<a class="ae lv" href="https://tensornetwork.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">阅读文档页面</a>。那么，为什么这么大惊小怪？张量网络有什么特别的，黑洞和机器学习有什么关系？什么是……张量网络？</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="f0f7" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated">张量网络简史</h2><p id="e25c" class="pw-post-body-paragraph kz la it lb b lc ne kd le lf nf kg lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">张量网络是物理学家使用的张量的图形符号，至少可以追溯到 20 世纪 70 年代，由罗杰·彭罗斯在他的<a class="ae lv" href="https://en.wikipedia.org/wiki/Penrose_graphical_notation" rel="noopener ugc nofollow" target="_blank">图形符号</a>中提出。他还在他的书<a class="ae lv" href="https://www.amazon.com/gp/product/0679776311/ref=as_li_tl?ie=UTF8&amp;tag=singularity07-20&amp;camp=1789&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0679776311&amp;linkId=c71d68a6f793ec73137f11366f9032ad" rel="noopener ugc nofollow" target="_blank">现实之路</a>中多次提到张量网络是如何应用于物理学的。另一个有点晦涩但非常透彻的文本是 Predrag Cvitanovic 所著的《群论:鸟迹、李氏和例外群》一书。这本书实际上在几年前我还是大学生的时候就引起了我的注意，就在它第一次发行后不久，特别是因为我想了解量子引力，以及人们如何使用如此迷人的视觉工具来做一些非常深奥的数学。</p><p id="0f57" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些张量的图形表示不仅仅是漂亮，它们实际上构成了当今理论物理中一些最深刻和最基本的概念的基础。物理学家和数学家使用它们来描述量子信息理论、AdS/CFT 通信、弦理论和全息原理，如<a class="ae lv" href="https://www.amazon.com/gp/product/0465062903/ref=as_li_tl?ie=UTF8&amp;tag=singularity07-20&amp;camp=1789&amp;creative=9325&amp;linkCode=as2&amp;creativeASIN=0465062903&amp;linkId=d9c40008a55f3881c58dbbdb38f57b7f" rel="noopener ugc nofollow" target="_blank"/>、罗杰·彭罗斯、胡安·马尔德塞纳和约翰·普雷斯基尔。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="93fb" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated">其他张量网络机器学习实现</h2><p id="c8ee" class="pw-post-body-paragraph kz la it lb b lc ne kd le lf nf kg lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated"><a class="ae lv" href="https://arxiv.org/abs/1605.05775" rel="noopener ugc nofollow" target="_blank">量子激励张量网络的监督学习</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/f9e4e043bf0048526c040720ab8b8f73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NWiumkXjoYEr-sma"/></div></div></figure><p id="d0c6" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，E. Miles Stoudenmire 和 David J. Schwab 使用张量网络模型对手写数据<a class="ae lv" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>进行机器学习分类，使用“<em class="nj">矩阵乘积状态</em>”。他们能够获得 1%的误差。他们还“讨论了张量网络形式如何给学习模型赋予额外的结构，并提出了一种可能的生成解释”。他们的代码可以在<a class="ae lv" href="https://github.com/emstoudenmire/TNML" rel="noopener ugc nofollow" target="_blank"> Github 库</a>中找到。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="e64e" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated"><a class="ae lv" href="https://quimb.readthedocs.io/en/latest/examples/ex_quantum_circuit.html" rel="noopener ugc nofollow" target="_blank"> Quimb </a></h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi np"><img src="../Images/eb6e968b06a76c8a91c723bb0dff64a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cOEZUiE42Hj_AXpL"/></div></div></figure><p id="1596" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Quimb 是一个 python 库，用于量子信息和多体计算，包括张量网络。它速度快，并针对效率进行了优化。它主要用于物理问题的应用，但也可以适用于其他应用。它可以在 GPU 上运行，并且可以以几何自由的方式计算张量运算和表示。它可以用<code class="fe nq nr ns lx b">tensorflow</code>或<code class="fe nq nr ns lx b">pytorch</code>优化任何张量网络</p><p id="85c2" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它对于量子计算机的模拟是有用的，因为它已经在<a class="ae lv" href="https://qiskit.org/" rel="noopener ugc nofollow" target="_blank"> IBM Qiskit </a>中为计算进行了设置。对于许多问题，模拟是有效的，但是对于感兴趣的解空间不受某种对称性限制的高维计算，当然需要量子计算机来代替 Quimb 提供的经典模拟。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="4980" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated">量子机器学习和一种新的信息论</h2><p id="7d04" class="pw-post-body-paragraph kz la it lb b lc ne kd le lf nf kg lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">在他的文章<a class="ae lv" href="https://arxiv.org/abs/1801.00862" rel="noopener ugc nofollow" target="_blank">NISQ 时代的量子计算和超越</a>中，John Preskill 描述了在“<em class="nj">嘈杂的中间规模量子(NISQ) </em>”硬件上量子计算的当前状态。在他的<a class="ae lv" href="http://www.theory.caltech.edu/people/preskill/talks/APS-March-2016-preskill.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>中，他讨论了一种<em class="nj">量子信息理论</em>，类似于谷歌与数学物理学家约翰·c·贝兹和计算机科学家迈克·斯泰共同撰写的论文<a class="ae lv" href="http://math.ucr.edu/home/baez/rosetta.pdf" rel="noopener ugc nofollow" target="_blank">物理、拓扑、逻辑和计算:一块罗塞塔石碑</a>。论文的一个主题是我们应该重新思考信息论。</p><p id="b3af" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是，谷歌的论文解释了如何通过张量网络从量子物理学的角度思考信息论。毫不奇怪，量子计算中的标准符号<a class="ae lv" href="https://gist.github.com/The-Singularity-Research/cb887503d976188cd796aa34eeff2ee3" rel="noopener ugc nofollow" target="_blank">量子电路图</a>本身就是一个张量网络。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="ddfe" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated">加速！</h2><p id="e2bf" class="pw-post-body-paragraph kz la it lb b lc ne kd le lf nf kg lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">在 Google TensorNetwork 库的一个简短教程中，我展示了如何使用边的同时收缩来提供计算的实质性加速。该代码计算边顺序收缩的运行时间，然后计算边同时收缩的运行时间。</p><pre class="ks kt ku kv gt lw lx ly lz aw ma bi"><span id="e10b" class="mb mc it lx b gy md me l mf mg">def one_edge_at_a_time(a, b):<br/>  node1 = tn.Node(a)<br/>  node2 = tn.Node(b)<br/>  edge1 = node1[0] ^ node2[0]<br/>  edge2 = node1[1] ^ node2[1]<br/>  edge3 = node1[2] ^ node2[2]<br/>  tn.contract(edge1)<br/>  tn.contract(edge2)<br/>  result = tn.contract(edge3)<br/>  return result.tensor</span><span id="044d" class="mb mc it lx b gy nt me l mf mg">def use_contract_between(a, b):<br/>  node1 = tn.Node(a)<br/>  node2 = tn.Node(b)<br/>  node1[0] ^ node2[0]<br/>  node1[1] ^ node2[1]<br/>  node1[2] ^ node2[2]<br/>  # This is the same as<br/>  # tn.contract_between(node1, node2)<br/>  result = node1 @ node2<br/>  return result.tensor</span><span id="3b70" class="mb mc it lx b gy nt me l mf mg">a = np.ones((1000, 1000, 10))<br/>b = np.ones((1000, 1000, 10))</span><span id="3252" class="mb mc it lx b gy nt me l mf mg">%%time<br/>one_edge_at_a_time(a, b)</span><span id="45b6" class="mb mc it lx b gy nt me l mf mg">CPU times: user 15.7 s, sys: 1.88 s, total: 17.6 s<br/>Wall time: 14.3 s</span></pre><p id="5b3d" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">[29]:</p><pre class="ks kt ku kv gt lw lx ly lz aw ma bi"><span id="bb34" class="mb mc it lx b gy md me l mf mg">array(10000000.)</span></pre><p id="5fb2" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将依次收缩三条边，首先是红色，然后是绿色，然后是蓝色，如使用 Graphviz 生成的图像所示:</p><pre class="ks kt ku kv gt lw lx ly lz aw ma bi"><span id="b7a1" class="mb mc it lx b gy md me l mf mg"># http://www.graphviz.org/content/cluster<br/>graph G0 {<br/>    node [style=filled,color=pink];<br/>    edge [color = red];<br/>    a -- b;<br/>    edge [color = green];<br/>    a -- b;<br/>    edge [color = blue];<br/>    a -- b<br/>}</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ccf25b5bc5eda1d6d69f9d3b29ab13a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/0*6B8gWmwpsjcjbyWZ"/></div></figure><p id="1a46" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主持于<a class="ae lv" href="https://sketchviz.com/" rel="noopener ugc nofollow" target="_blank"> Sketchviz </a></p><pre class="ks kt ku kv gt lw lx ly lz aw ma bi"><span id="abb3" class="mb mc it lx b gy md me l mf mg">%%time<br/>use_contract_between(a, b)</span><span id="b749" class="mb mc it lx b gy nt me l mf mg">CPU times: user 495 ms, sys: 145 ms, total: 639 ms<br/>Wall time: 1.75 s</span></pre><p id="1ff8" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">[33]:</p><pre class="ks kt ku kv gt lw lx ly lz aw ma bi"><span id="af0b" class="mb mc it lx b gy md me l mf mg">array(10000000.)</span></pre><p id="6a97" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数学上，这转化为三阶张量的内积:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a71d9294d9b807e848da36584ba32667.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/0*RO48yljV9U0AOJOs.jpg"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk">contracting 3 edges between two nodes “a” and “b”</figcaption></figure><p id="d057" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如本文<a class="ae lv" href="https://venturebeat.com/2019/06/04/googles-tensornetwork-library-speeds-up-computation-by-up-to-100-times/" rel="noopener ugc nofollow" target="_blank">文章</a>中所述，拥有这样的算法可以在高维张量上执行更高效的计算，从而提供显著的加速(高达 100 倍)。加入新的“<a class="ae lv" href="https://cloud.google.com/tpu/" rel="noopener ugc nofollow" target="_blank"> <em class="nj">张量处理单元</em></a><em class="nj">【TPUs】</em>”后，谷歌设计了更高效的张量计算，用于机器学习目的，你就拥有了一个极其强大的机器学习框架，从硬件到云计算软件再到最终用户应用。</p></div><div class="ab cl mh mi hx mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="im in io ip iq"><h2 id="ba4c" class="mb mc it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd iz bi translated">这能用来做什么？</h2><p id="27ae" class="pw-post-body-paragraph kz la it lb b lc ne kd le lf nf kg lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">到目前为止，张量网络方法已经在图像识别、多类分类问题、计算机视觉、物理学、材料科学、生物技术、量子化学和生物物理学中得到应用。由于 TensorNetwork 提供了一种将张量网络集成到标准神经网络层的简单方法，因此它们可以用于任何涉及神经网络的机器学习任务。</p><p id="ea8c" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然这些方法对大多数机器学习社区来说可能是新的，但它们在物理和数学中有很长的应用历史，因此理论和概念理解在数学和理论物理社区中已经很好地建立。这提供了一种理解机器学习的新方法，并在受“<em class="nj">黑盒</em>”方法论困扰的领域提供了新的直觉。努力理解人工智能为什么做出某些决定并得出特定结论是一个问题，这导致人们担心人工智能在某些商业解决方案的应用中会有多大的偏见，例如人工智能辅助的招聘。随着执法部门现在在许多城市应用人工智能，确保人工智能是公平的，而不是做出不人道的决定，也成为许多人关注的问题。除此之外，张量网络方法可以在机器学习任务和计算效率方面提供改进，在更大规模上投资研究和实施这些方法似乎是明智的。</p><p id="64f3" class="pw-post-body-paragraph kz la it lb b lc ld kd le lf lg kg lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有想法要分享，有问题，或者你需要量子机器学习的咨询服务，请通过<a class="ae lv" href="http://linkedin.com/in/amelie-schreiber-694481181" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系作者，访问奇点<a class="ae lv" href="https://thesingularityrese.wixsite.com/singularity/hacking-the-universe" rel="noopener ugc nofollow" target="_blank">网站</a>，或者查看 Github <a class="ae lv" href="https://github.com/The-Singularity-Research" rel="noopener ugc nofollow" target="_blank">教程</a>。</p></div></div>    
</body>
</html>