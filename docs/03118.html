<html>
<head>
<title>Overfitting, underfitting, and the bias-variance tradeoff</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">过度拟合、欠拟合和偏差-方差权衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb?source=collection_archive---------11-----------------------#2019-05-19">https://towardsdatascience.com/overfitting-underfitting-and-the-bias-variance-tradeoff-83b42fb11efb?source=collection_archive---------11-----------------------#2019-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d932" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过 Python、numpy 和 scikit-learn 中的一个工作示例来学习这些基本的机器学习概念</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c99d04ecd1d92fdad8cebd2c85807b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TswD-syYuhwzRUM8.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by <a class="ae kv" href="http://thestrategyguysite.com/strategic-business-planning/does-your-strategic-plan-fit-you/" rel="noopener ugc nofollow" target="_blank">The Strategy Guy</a></figcaption></figure><p id="dda2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过拟合、欠拟合和偏差-方差权衡是机器学习中的基本概念。如果用于拟合模型的训练数据的性能明显好于模型训练过程中测试集的性能，则模型是<strong class="ky ir">过拟合</strong>。例如，训练数据的预测误差可能明显小于测试数据的预测误差。比较这两个数据集之间的模型性能指标是为了训练和测试而拆分数据的主要原因之一。通过这种方式，可以评估模型对新的、未知数据的预测能力。</p><p id="c865" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当一个模型过度拟合训练数据时，它被称为具有<strong class="ky ir">高方差</strong>。考虑这一点的一种方式是，无论训练数据中存在什么样的可变性，模型都已经很好地“学习”了这一点。事实上，太好了。具有高方差的模型很可能已经学习了训练集中的噪声。噪声由数据特征(自变量)和响应(因变量)中的随机波动或偏离真实值组成。噪声会模糊特征和响应变量之间的真实关系。几乎所有真实世界的数据都是有噪声的。</p><p id="b44c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果在训练集中有随机噪声，那么在测试集中也可能有随机噪声。但是随机波动的具体值会和训练集的不同，因为毕竟噪声是随机的。该模型无法预测测试集的新的、看不见的数据的波动。这就是过度拟合模型的测试性能低于训练性能的原因。</p><p id="1efe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下列情况下，过度拟合的可能性更大:</p><ul class=""><li id="1c0a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">相对于样本(观察)的数量，有大量的特征可用。特征越多，发现特征和响应之间虚假关系的机会就越大。</li><li id="3fd2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用复杂的模型，例如深度决策树或神经网络。像这样的模型有效地设计了它们自己的特征，并且有机会开发关于特征和响应之间关系的更复杂的假设，使得过度拟合更有可能。</li></ul><p id="7198" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在光谱的另一端，如果模型没有很好地拟合训练数据，这被称为<strong class="ky ir">欠拟合</strong>，并且该模型被称为具有<strong class="ky ir">高偏差</strong>。在这种情况下，就所使用的模型的特征或类型而言，模型可能不够复杂。</p><p id="fbac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们通过将多项式模型拟合到 Python 中的合成数据，来检查欠拟合、过拟合以及介于两者之间的理想状态的具体示例。下面的代码可以在 Jupyter 笔记本中运行，以生成此处显示的结果和图表。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="c063" class="ml mm iq mh b gy mn mo l mp mq"><em class="mr">#Import packages<br/></em>import numpy <strong class="mh ir">as</strong> np <em class="mr">#numerical computation<br/></em>import matplotlib.pyplot <strong class="mh ir">as</strong> plt <em class="mr">#plotting package<br/>#Next line helps with rendering plots<br/></em><strong class="mh ir">%</strong>matplotlib inline<br/>import matplotlib <strong class="mh ir">as</strong> mpl <em class="mr">#additional plotting functionality</em></span></pre><h1 id="0f0f" class="ms mm iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">多项式模型的欠拟合和过拟合</h1><p id="1bd5" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">首先，我们创建合成数据。我们:</p><ul class=""><li id="aa69" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在区间[0，11]上随机选择 20 个点。这包括 0，但不包括 11。我们还对它们进行排序，使它们按顺序排列，这将有助于您创建一个线图(这里我们只做一个散点图)。</li><li id="2483" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">对它们进行二次变换，并添加一些噪声:<em class="mr">y</em>=(-<em class="mr">x</em>+2)(<em class="mr">x</em>-9)+ϵ，其中ϵ是均值为 0、标准差为 3 的正态分布噪声</li></ul><p id="58f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们做一个数据散点图。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="107b" class="ml mm iq mh b gy mn mo l mp mq">np<strong class="mh ir">.</strong>random<strong class="mh ir">.</strong>seed(seed<strong class="mh ir">=</strong>9)<br/>n_points <strong class="mh ir">=</strong> 20<br/>x <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>random<strong class="mh ir">.</strong>uniform(0, 11, n_points)<br/>x <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>sort(x)<br/>y <strong class="mh ir">=</strong> (<strong class="mh ir">-</strong>x<strong class="mh ir">+</strong>2) <strong class="mh ir">*</strong> (x<strong class="mh ir">-</strong>9) <strong class="mh ir">+</strong> np<strong class="mh ir">.</strong>random<strong class="mh ir">.</strong>normal(0, 3, n_points)</span><span id="d36b" class="ml mm iq mh b gy no mo l mp mq">mpl<strong class="mh ir">.</strong>rcParams['figure.dpi'] <strong class="mh ir">=</strong> 400<br/>plt<strong class="mh ir">.</strong>scatter(x, y)<br/>plt<strong class="mh ir">.</strong>xticks([])<br/>plt<strong class="mh ir">.</strong>yticks([])<br/>plt<strong class="mh ir">.</strong>ylim([<strong class="mh ir">-</strong>20, 20])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/121bbd0f0c39ab6217e0601080e2ce21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jpjtRzVQ1Kgh51nN.png"/></div></div></figure><p id="b814" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这看起来像抛物线的形状，正如对<em class="mr"> x </em>的二次变换所预期的那样。然而，我们可以看到，事实上并不是所有的点都完美地位于抛物线上。</p><p id="a0c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的合成示例中，我们知道<strong class="ky ir">数据生成过程</strong>:响应变量<em class="mr"> y </em>是特征<em class="mr"> x </em>的二次变换。一般来说，在建立机器学习模型时，数据生成过程是未知的。取而代之的是，提出几个候选特征，提出一个模型，并探索这些特征和这个模型如何解释数据。</p><p id="8497" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，我们可能会绘制数据，观察明显的二次关系，并使用二次模型。然而，为了说明<strong class="ky ir">欠拟合模型</strong>，这些数据的线性模型看起来像什么？</p><p id="ab75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用 numpy 的<code class="fe nq nr ns mh b">polyfit</code>拟合一个 1 次多项式模型，换句话说，一个线性模型:</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="8b5e" class="ml mm iq mh b gy mn mo l mp mq">lin_fit <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>polyfit(x, y, 1)<br/>lin_fit</span><span id="4657" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;array([ 0.44464616, -0.61869372])</span></pre><p id="3027" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就产生了这些数据的最佳拟合线的斜率和截距。我们把它画出来，看看它是什么样子。我们可以使用拟合线性模型得到的斜率和截距来计算要素的线性变换，再次使用 numpy，这次是使用<code class="fe nq nr ns mh b">polyval</code>函数。我们称之为“欠拟合模型”。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="26cc" class="ml mm iq mh b gy mn mo l mp mq">cmap <strong class="mh ir">=</strong> mpl<strong class="mh ir">.</strong>cm<strong class="mh ir">.</strong>get_cmap('tab10')</span><span id="8be1" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>scatter(x, y, label<strong class="mh ir">=</strong>'Data', color<strong class="mh ir">=</strong>cmap(0))<br/>plt<strong class="mh ir">.</strong>plot(x, np<strong class="mh ir">.</strong>polyval(lin_fit, x), label<strong class="mh ir">=</strong>'Underfit model', color<strong class="mh ir">=</strong>cmap(1))</span><span id="56d1" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>legend(loc<strong class="mh ir">=</strong>[0.17, 0.1])<br/>plt<strong class="mh ir">.</strong>xticks([])<br/>plt<strong class="mh ir">.</strong>yticks([])<br/>plt<strong class="mh ir">.</strong>ylim([<strong class="mh ir">-</strong>20, 20])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/43997c92e74cb6a1ee5664608dd47943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mmG8aEjkOl0cT8bp.png"/></div></div></figure><p id="2a52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看起来不太合适，不是吗！</p><p id="e813" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们想象我们有许多可用的特性，那就是<em class="mr"> x </em>的多项式变换，具体来说就是<em class="mr"> x </em>、<em class="mr"> x </em>、… <em class="mr"> x </em> ⁵.相对于我们拥有的样本数量(20)，这将是大量的特征。同样，在“现实生活”中，我们可能不会考虑具有所有这些特征的模型，因为通过观察我们可以看到，二次模型或二次多项式可能就足够了。然而，确定理想的特征并不总是那么简单。我们的示例用来展示当我们非常清楚地使用太多特性时会发生什么。</p><p id="3b56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们做一个 15 次多项式拟合:</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="5579" class="ml mm iq mh b gy mn mo l mp mq">high_poly_fit <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>polyfit(x, y, 15)<br/>high_poly_fit</span><span id="0041" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;array([ 1.04191511e-05, -7.58239114e-04,  2.48264043e-02, -4.83550912e-01,<br/>        6.24182399e+00, -5.63097621e+01,  3.64815913e+02, -1.71732868e+03,<br/>        5.87515347e+03, -1.44598953e+04,  2.50562989e+04, -2.94672314e+04,<br/>        2.21483755e+04, -9.60766525e+03,  1.99634019e+03, -1.43201982e+02])</span></pre><p id="ce45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些是这个模型中从 1 到 15 的所有幂的系数，以及截距。请注意系数的标度差异很大:一些接近于零，而另一些则相当大(绝对值)。让我们也画出这个模型。首先，我们在我们的<em class="mr"> x </em>值范围内生成大量均匀分布的点，这样我们不仅可以看到用于模型训练的特定值的模型，还可以看到中间值的模型。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="29ce" class="ml mm iq mh b gy mn mo l mp mq">plt<strong class="mh ir">.</strong>scatter(x, y, label<strong class="mh ir">=</strong>'Data', color<strong class="mh ir">=</strong>cmap(0))<br/>plt<strong class="mh ir">.</strong>plot(x, np<strong class="mh ir">.</strong>polyval(lin_fit, x), label<strong class="mh ir">=</strong>'Underfit model', color<strong class="mh ir">=</strong>cmap(1))</span><span id="f463" class="ml mm iq mh b gy no mo l mp mq">curve_x <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>linspace(0,11,333)plt<strong class="mh ir">.</strong>plot(curve_x, np<strong class="mh ir">.</strong>polyval(high_poly_fit, curve_x),<br/>         label<strong class="mh ir">=</strong>'Overfit model', color<strong class="mh ir">=</strong>cmap(2))</span><span id="3af7" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>legend(loc<strong class="mh ir">=</strong>[0.17, 0.1])<br/>plt<strong class="mh ir">.</strong>xticks([])<br/>plt<strong class="mh ir">.</strong>yticks([])<br/>plt<strong class="mh ir">.</strong>ylim([<strong class="mh ir">-</strong>20, 20])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/485bf427a10509bac6a0633baaadafc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tWpw7V4sXPY9g1Wq.png"/></div></div></figure><p id="6ba9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个经典的过度拟合案例。过度拟合模型几乎完美地通过了所有的训练数据。然而，很容易看出，对于介于两者之间的值，过拟合模型看起来不像是数据生成过程的真实表示。相反，过拟合模型已经适应了训练数据的噪声。这符合上面给出的高方差的定义。</p><p id="9f9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在最后一张图中，您可以看到高方差的另一种定义:输入<em class="mr"> x </em>的小变化会导致输出<em class="mr"> y </em>的大变化。输入和输出变化之间的这种关系是人们谈论机器学习模型差异的另一种方式。</p><p id="7d0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您还可以想象，如果我们使用相同的二次函数<em class="mr">y</em>=(<em class="mr">x</em>+2)(<em class="mr">x</em>-9)生成一个新的、更大的合成数据集，但是根据我们上面使用的相同分布添加新的随机生成的噪声，然后随机采样 20 个点并拟合高次多项式，则最终的模型看起来会有很大不同。它将几乎完美地通过这些新的噪声点，并且 15 次多项式的系数将非常不同，从而允许这种情况发生。对 20 个点的不同样本重复这一过程，将继续导致高度可变的系数估计。换句话说，用于模型训练的数据样本之间的系数会有很大的差异。这是高方差模型的另一种定义。</p><p id="c073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用我们的合成数据，因为在这种情况下我们知道数据生成过程，我们可以看到二次多项式拟合与欠拟合和过拟合模型相比看起来如何。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="7554" class="ml mm iq mh b gy mn mo l mp mq">plt<strong class="mh ir">.</strong>scatter(x, y, label<strong class="mh ir">=</strong>'Data', color<strong class="mh ir">=</strong>cmap(0))<br/>plt<strong class="mh ir">.</strong>plot(x, np<strong class="mh ir">.</strong>polyval(lin_fit, x), label<strong class="mh ir">=</strong>'Underfit model', color<strong class="mh ir">=</strong>cmap(1))<br/>plt<strong class="mh ir">.</strong>plot(curve_x, np<strong class="mh ir">.</strong>polyval(high_poly_fit, curve_x),<br/>         label<strong class="mh ir">=</strong>'Overfit model', color<strong class="mh ir">=</strong>cmap(2))</span><span id="86b9" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>plot(curve_x, np<strong class="mh ir">.</strong>polyval(np<strong class="mh ir">.</strong>polyfit(x, y, 2), curve_x),<br/>         label<strong class="mh ir">=</strong>'Ideal model', color<strong class="mh ir">=</strong>cmap(3))</span><span id="bfc2" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>legend(loc<strong class="mh ir">=</strong>[0.17, 0.1])<br/>plt<strong class="mh ir">.</strong>xticks([])<br/>plt<strong class="mh ir">.</strong>yticks([])<br/>plt<strong class="mh ir">.</strong>ylim([<strong class="mh ir">-</strong>20, 20])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/521959dda98f63c3bcc3c41a78441b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a4rU4YFPAaFVg23y.png"/></div></div></figure><p id="005e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这还差不多。但是，在现实世界中，当我们不使用虚构的数据，并且不知道数据生成过程时，我们该怎么办呢？有许多机器学习技术来处理过度拟合。其中最受欢迎的是正规化。</p><h1 id="2771" class="ms mm iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">岭回归正则化</h1><p id="57e0" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">为了展示正则化如何减少过度拟合，我们将使用 scikit-learn 包。首先，我们需要手动创建多项式要素。虽然上面我们只需告诉 numpy 对数据拟合 15 次多项式，但这里我们需要手动创建特征<em class="mr"> x </em>、<em class="mr"> x </em>、… <em class="mr"> x </em> ⁵，然后拟合线性模型以找到它们的系数。Scikit-learn 使使用<code class="fe nq nr ns mh b">PolynomialFeatures</code>创建多项式特征变得容易。我们只是说，我们想要 15 度的多项式特性，没有偏差特性(截距)，然后通过我们的数组整形为一列。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="8a63" class="ml mm iq mh b gy mn mo l mp mq">from sklearn.preprocessing import PolynomialFeatures<br/>poly <strong class="mh ir">=</strong> PolynomialFeatures(degree<strong class="mh ir">=</strong>15, include_bias<strong class="mh ir">=</strong>False)<br/>poly_features <strong class="mh ir">=</strong> poly<strong class="mh ir">.</strong>fit_transform(x<strong class="mh ir">.</strong>reshape(<strong class="mh ir">-</strong>1, 1))<br/>poly_features<strong class="mh ir">.</strong>shape</span><span id="52bb" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;(20, 15)</span></pre><p id="49a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们得到 15 列，其中第一列是<em class="mr"> x </em>，第二列是<em class="mr"> x </em>等等。现在我们需要确定这些多项式特征的系数。上面，我们通过使用 numpy 来找到为训练数据提供最佳拟合的系数。然而，我们看到这导致了过度拟合模型。这里，我们将把这些数据传递给用于<strong class="ky ir">岭回归</strong>的例程，这是一种<strong class="ky ir">正则化回归</strong>。这里不详细介绍，正则化回归的工作原理是找到最适合数据<em class="mr">的系数，同时限制系数的大小</em>。</p><p id="a96c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这样做的效果是对数据的拟合稍差，换句话说，模型具有更高的偏差。但是，目标是避免拟合随机噪声，从而消除高方差问题。因此，我们希望用一些方差换取一些偏差，以获得信号而非噪声的模型。</p><p id="1dd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用 scikit-learn 中的<code class="fe nq nr ns mh b">Ridge</code>类来进行岭回归。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="502f" class="ml mm iq mh b gy mn mo l mp mq">from sklearn.linear_model import Ridge</span><span id="19f7" class="ml mm iq mh b gy no mo l mp mq">ridge <strong class="mh ir">=</strong> Ridge(alpha<strong class="mh ir">=</strong>0.001, fit_intercept<strong class="mh ir">=</strong>True, normalize<strong class="mh ir">=</strong>True,<br/>                        copy_X<strong class="mh ir">=</strong>True, max_iter<strong class="mh ir">=</strong>None, tol<strong class="mh ir">=</strong>0.001,<br/>                        random_state<strong class="mh ir">=</strong>1)</span></pre><p id="0ed0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在实例化<code class="fe nq nr ns mh b">Ridge</code>类时，有许多选项需要设置。很重要的一个就是<code class="fe nq nr ns mh b">alpha</code>。这控制了应用正则化的程度；换句话说，系数幅度受到多大程度的惩罚，并保持接近于零。我们将使用我已经找到的<code class="fe nq nr ns mh b">alpha</code>的值，只是为了说明正则化的效果。一般来说，选择<code class="fe nq nr ns mh b">alpha</code>的程序是通过检查验证集的模型性能或使用交叉验证程序来系统地评估一系列值，以确定哪一个值有望在未知测试集上提供最佳性能。<code class="fe nq nr ns mh b">alpha</code>是一个模型<strong class="ky ir">超参数</strong>，这将是超参数<strong class="ky ir">调整</strong>的过程。</p><p id="4430" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们为<code class="fe nq nr ns mh b">Ridge</code>指定的其他选项表明我们想要拟合截距(因为我们在生成<code class="fe nq nr ns mh b">PolynomialFeatures</code>时没有包括截距)，在模型拟合之前将特征归一化到相同的比例，这是必要的，因为系数将以相同的方式受到惩罚，以及其他一些选项。我在这里掩饰这些细节，尽管你可以参考 scikit-learn 文档，以及我的<a class="ae kv" href="https://www.amazon.com/Data-Science-Projects-Python-scikit-learn/dp/1838551026" rel="noopener ugc nofollow" target="_blank">书</a>，以获得关于正则化以及超参数调整的更多信息。</p><p id="0e2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们继续使用多项式特征和响应变量来拟合岭回归。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="3087" class="ml mm iq mh b gy mn mo l mp mq">ridge<strong class="mh ir">.</strong>fit(poly_features, y)</span><span id="43fc" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;Ridge(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=None,<br/>   normalize=True, random_state=1, solver='auto', tol=0.001)</span></pre><p id="3b12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与上面通过用 numpy 拟合多项式得到的值相比，拟合系数的值看起来像什么？</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="d0f4" class="ml mm iq mh b gy mn mo l mp mq">ridge<strong class="mh ir">.</strong>coef_</span><span id="23e1" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;array([ 8.98768521e+00, -5.14275445e-01, -3.95480123e-02, -1.10685070e-03,<br/>        4.49790120e-05,  8.58383048e-06,  6.74724995e-07,  3.02757058e-08,<br/>       -3.81325130e-10, -2.54650509e-10, -3.25677313e-11, -2.66208560e-12,<br/>       -1.05898398e-13,  1.22711353e-14,  3.90035611e-15])</span></pre><p id="c16d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，与多项式拟合相比，正则化回归的系数值相对较小。这就是正则化的工作方式:将系数值“缩小”到零。由于这个原因，正则化也可以被称为<strong class="ky ir">收缩</strong>。</p><p id="40d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们在上面使用的大量均匀分布的点<code class="fe nq nr ns mh b">curve_x</code>上获得预测值<code class="fe nq nr ns mh b">y_pred</code>，用于绘图。首先，我们需要为所有这些点生成多项式特征。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="22c5" class="ml mm iq mh b gy mn mo l mp mq">poly_features_curve <strong class="mh ir">=</strong> poly<strong class="mh ir">.</strong>fit_transform(curve_x<strong class="mh ir">.</strong>reshape(<strong class="mh ir">-</strong>1, 1))<br/>y_pred <strong class="mh ir">=</strong> ridge<strong class="mh ir">.</strong>predict(poly_features_curve)</span></pre><p id="9bb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从图中移除欠拟合模型，并添加正则化模型。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="157a" class="ml mm iq mh b gy mn mo l mp mq">plt<strong class="mh ir">.</strong>scatter(x, y, label<strong class="mh ir">=</strong>'Data', color<strong class="mh ir">=</strong>cmap(0))<br/>plt<strong class="mh ir">.</strong>plot(curve_x, np<strong class="mh ir">.</strong>polyval(high_poly_fit, curve_x),<br/>         label<strong class="mh ir">=</strong>'Overfit model', color<strong class="mh ir">=</strong>cmap(2))<br/>plt<strong class="mh ir">.</strong>plot(curve_x, np<strong class="mh ir">.</strong>polyval(np<strong class="mh ir">.</strong>polyfit(x, y, 2), curve_x),<br/>         label<strong class="mh ir">=</strong>'Ideal model', color<strong class="mh ir">=</strong>cmap(3))</span><span id="89b4" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>plot(curve_x, y_pred,<br/>         label<strong class="mh ir">=</strong>'Regularized model',color<strong class="mh ir">=</strong>cmap(4), linestyle<strong class="mh ir">=</strong>'--')</span><span id="1bf9" class="ml mm iq mh b gy no mo l mp mq">plt<strong class="mh ir">.</strong>legend(loc<strong class="mh ir">=</strong>[0.17, 0.1])<br/>plt<strong class="mh ir">.</strong>xticks([])<br/>plt<strong class="mh ir">.</strong>yticks([])<br/>plt<strong class="mh ir">.</strong>ylim([<strong class="mh ir">-</strong>20, 20])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/f6a20c3e5902cc6aad75dfd3634539be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N3htBodFvnXW9vJw.png"/></div></div></figure><p id="572a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正则化模型看起来类似于理想模型。这表明，即使我们不知道数据生成过程，正如我们在现实世界的预测建模工作中通常不知道的那样，当大量候选特征可用时，我们仍然可以使用正则化来减少过度拟合的影响。</p><p id="3208" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，请注意，正则化模型不应用于外推。我们可以看到，正则化模型开始向图的右侧增加。应该怀疑这种增加，因为在训练数据中没有任何东西表明这是可以预期的。这是一个普遍观点的例子，即不推荐在训练数据范围之外外推模型预测。</p><h1 id="1c9f" class="ms mm iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">正则化对模型测试性能的影响</h1><p id="8787" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">用方差换取偏差的目标是提高模型在未知测试数据上的性能。让我们以生成训练数据的相同方式生成一些测试数据，看看我们是否达到了这个目标。我们重复上面用于生成<em class="mr"> x </em>和<em class="mr">y</em>=(<em class="mr">x</em>+2)(<em class="mr">x</em>−9)+ϵ的过程，但是使用不同的随机种子。这导致相同间隔上的不同点<em class="mr"> x </em>和来自相同分布的不同随机噪声ϵ，为响应变量<em class="mr"> y </em>创建新值，但是来自相同的数据生成过程。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="4bbb" class="ml mm iq mh b gy mn mo l mp mq">np<strong class="mh ir">.</strong>random<strong class="mh ir">.</strong>seed(seed<strong class="mh ir">=</strong>4)<br/>n_points <strong class="mh ir">=</strong> 20<br/>x_test <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>random<strong class="mh ir">.</strong>uniform(0, 11, n_points)<br/>x_test <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>sort(x_test)<br/>y_test <strong class="mh ir">=</strong> (<strong class="mh ir">-</strong>x_test<strong class="mh ir">+</strong>2) <strong class="mh ir">*</strong> (x_test<strong class="mh ir">-</strong>9) <strong class="mh ir">+</strong> np<strong class="mh ir">.</strong>random<strong class="mh ir">.</strong>normal(0, 3, n_points)</span></pre><p id="92fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还将定义一个 lambda 函数，以均方根误差(RMSE)来衡量预测误差。</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="4f44" class="ml mm iq mh b gy mn mo l mp mq">RMSE <strong class="mh ir">=</strong> <strong class="mh ir">lambda</strong> y, y_pred: np<strong class="mh ir">.</strong>sqrt(np<strong class="mh ir">.</strong>mean((y<strong class="mh ir">-</strong>y_pred)<strong class="mh ir">**</strong>2))</span></pre><p id="95df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的第一个模型(拟合训练数据的多项式)的 RMSE 是什么？</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="1300" class="ml mm iq mh b gy mn mo l mp mq">y_train_pred <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>polyval(high_poly_fit, x)<br/>RMSE(y, y_train_pred)</span><span id="bb92" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;1.858235982416223</span></pre><p id="dea9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">新生成的测试数据的 RMSE 如何？</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="8b9e" class="ml mm iq mh b gy mn mo l mp mq">y_test_pred <strong class="mh ir">=</strong> np<strong class="mh ir">.</strong>polyval(high_poly_fit, x_test)<br/>RMSE(y_test, y_test_pred)</span><span id="3d59" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;9811.219078261804</span></pre><p id="43a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测试误差远大于该模型的训练误差，这是过度拟合的明显迹象。正则化模型比较怎么样？</p><pre class="kg kh ki kj gt mg mh mi mj aw mk bi"><span id="9e51" class="ml mm iq mh b gy mn mo l mp mq">y_train_pred <strong class="mh ir">=</strong> ridge<strong class="mh ir">.</strong>predict(poly_features)<br/>RMSE(y, y_train_pred)</span><span id="e280" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;3.235497045896461</span><span id="15b1" class="ml mm iq mh b gy no mo l mp mq">poly_features_test <strong class="mh ir">=</strong> poly<strong class="mh ir">.</strong>fit_transform(x_test<strong class="mh ir">.</strong>reshape(<strong class="mh ir">-</strong>1, 1))<br/>y_test_pred <strong class="mh ir">=</strong> ridge<strong class="mh ir">.</strong>predict(poly_features_test)<br/>RMSE(y_test, y_test_pred)</span><span id="6a21" class="ml mm iq mh b gy no mo l mp mq">&gt;&gt;3.5175193708774946</span></pre><p id="937b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然正则化模型比多项式拟合具有稍高的训练误差(较高的偏差)，但是测试误差得到了极大的改善。这显示了如何利用偏差-方差权衡来提高模型预测能力。</p><h1 id="3693" class="ms mm iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">结论</h1><p id="4aa8" class="pw-post-body-paragraph kw kx iq ky b kz nj jr lb lc nk ju le lf nl lh li lj nm ll lm ln nn lp lq lr ij bi translated">这篇文章通过 Python 和 scikit-learn 中的一个示例说明了过拟合、欠拟合和偏差-方差权衡的概念。它扩展了我的书<em class="mr">使用 Python 的数据科学项目:使用 Python、pandas 和 scikit-learn 的成功数据科学项目的案例研究方法</em>中的一个部分。要更深入地解释正则化如何工作，如何使用交叉验证进行超参数选择，以及这些和其他机器学习技术的实践，请查看这本书，您可以在<a class="ae kv" href="https://www.amazon.com/Data-Science-Projects-Python-valuable/dp/1838551026/" rel="noopener ugc nofollow" target="_blank">亚马逊</a>上找到，这里有 Q &amp; A 和勘误表<a class="ae kv" href="http://www.steveklosterman.com/book/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="cd2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里是关于偏差和方差的一些最终想法。</p><p id="ef45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">偏差和方差的统计定义</strong>:这篇文章关注了偏差和方差的直观机器学习定义。还有更正式的统计定义。参见<a class="ae kv" href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec04.pdf" rel="noopener ugc nofollow" target="_blank">本文档</a>中对方差分解偏差的数学表达式的推导，以及<a class="ae kv" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">Hastie 等人的统计学习要素</a>中关于偏差-方差分解和权衡的更多讨论。</p><p id="1ced" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用更多数据应对高方差</strong>:在他的 Coursera 课程<a class="ae kv" href="https://www.coursera.org/learn/machine-learning/lecture/XcNcz/data-for-machine-learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>中，吴恩达指出，根据大数据基本原理，在一个非常大的数据集上进行训练可以是一种有效的方法来对抗过度拟合。这个想法是，有了足够的训练数据，训练和测试误差之间的差异应该很小，这意味着方差减少。这种基本原理有一个潜在的假设，即特征包含足够的信息来对响应变量进行准确的预测。否则，模型将遭受高偏差(高训练误差)，因此低方差将是一个有争议的问题。</p><p id="8db3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">一点点过度拟合可能不是问题</strong>:测试集上的模型性能通常比训练集上的稍低。我们在上面的正则化模型中看到了这一点。从技术上讲，这里至少有一点过度拟合。然而，这并不重要，因为最好的模型通常被认为是测试分数最高的模型。</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="dcd9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mr">原载于 2019 年 5 月 19 日</em><a class="ae kv" href="https://www.steveklosterman.com/over-under/" rel="noopener ugc nofollow" target="_blank"><em class="mr">https://www.steveklosterman.com</em></a><em class="mr">。</em></p></div></div>    
</body>
</html>