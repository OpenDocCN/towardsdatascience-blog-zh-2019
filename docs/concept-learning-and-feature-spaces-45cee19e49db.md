# 概念学习和特征空间

> 原文：<https://towardsdatascience.com/concept-learning-and-feature-spaces-45cee19e49db?source=collection_archive---------14----------------------->

![](img/75c1b610f510dfcbde3eccdcee2a4f30.png)

## 我们如何教会机器理解一个想法？

2004 年夏天，世界各地数百万的电影观众体验了《我，机器人》——《T1》，这部电影描绘了一个美好的未来，在这个未来里，人形机器仆人将成为社会不可或缺的一部分*。*这部电影受到阿西莫夫同名短篇小说集的启发，邀请观众提出与机器智能、人工意识、强人工智能以及最终对人类意味着什么有关的常见问题。

虽然有更好的电影处理类似的哲学主题，童年怀旧是一个强大的东西；所以，在考虑如何最好地介绍**概念学习**的时候，一个来自*我，机器人*的场景浮现在脑海里。在这个场景中，侦探史普纳(威尔·史密斯饰)审问一个被怀疑谋杀了它的创造者的机器人。这两种智能讨论人类的情感，Spooner 试图激怒机器人，以表明它已经学会模拟愤怒:

![](img/a9be2148c67fcb7bcdf1bad05e6ad89e.png)

*“机器人会写交响乐吗？机器人能把画布变成美丽的杰作吗？”—勺子*

为了回答这个问题，我们首先要问自己，机器人是否能理解交响乐是什么？机器能学会从尖叫的婴儿、黑板上的钉子或麦莉·赛勒斯的音乐中分辨出交响乐吗？毕竟，为了让机器创作交响乐，它必须首先能够识别一首。

## 概念学习

教机器区分例子和非例子的想法，如交响乐，愤怒，美丽，狗，猫等。叫做**概念学习**。

> 在**概念学习中，**我们的目标是使用数据来教会机器解决二分类问题。也就是说，将数据点分类为属于或不属于特定的概念或想法。

从形式上来说，概念学习需要试图找到一种学习方法和一组特征，用它们来引导机器学习所选概念的一组特征的*特征函数*。这只是一个布尔值函数，给定某个示例的特征值，如果该示例属于或不属于该概念，则分别为该示例分配值 *1* 或 *0* 。

![](img/028654dba7dc76da7829f434b40a2068.png)

Characteristic function for lookin’ good — Looks_Good(Will Smith) = 1, Looks_Good(You) = 0

在我的[关于解读数据的帖子](/simpsons-paradox-and-interpreting-data-6a0443516765)中，我谈到了采用好的数据观点的重要性和需要注意的地方。特征只是从我们的数据中得到的列/属性，所以为概念学习选择特征，本质上是为学习问题选择一个数据观点。对于概念学习来说，一个好的数据观点是细粒度的和相关的，足以让我们描述我们试图学习的概念，但不会充斥着不必要的信息。概念学习中的一半战斗是选择一组特征:如果特征选择不好，可能根本不可能学习任何类似于特征函数的东西，例如，想象一下试图通过只看动物的高度来判断它是不是狗。另一方面，如果我们选择在我们的学习模型中包含太多的特征，噪音和增加的复杂性会使学习过程变得非常困难。

如果我们觉得我们可以从足够大的数据集中直观地捕捉到一个具有一组特征的概念，我们实际上如何让机器尝试从这些特征样本中学习呢？对我们人类来说幸运的是，有一种相当优雅的方式来数学地描述不同数据点有多“接近”,因此，通过精心选择的特征，属于该概念的示例将分组在一起，并以机器可以理解的数字方式与非示例区分开来。这种“接近度”的描述要求我们在一个**特征空间**中表示我们的数据。

## 特征空间中的概念

给定一个概念学习问题的一组特征，我们可以将该特征集解释为一个**特征空间。**

> 给定一些数据，**特征空间**就是从该数据中选择的一组特征的所有可能值的集合。总是可以仅使用数字来表示特征值，从而表示特征空间，并且进一步以这样的方式来表示，使得特征空间可以被解释为实空间。

我们所说的*真实空间*是什么意思？一位数学家会说，我们的 *n* 维特征空间可以表示为*同构于* *向量空间*ℝ*ⁿ*，但是它们的意思是我们可以在我们的特征空间中移动和跳舞，并且有 *n* 维空间可以让我们进行隐喻性的移动。

![](img/d48312db79931582cd927640ff668b68.png)

More dimensions? More room for activities.

抛开跳舞不谈，真正重要的是我们可以将我们的数据表示为一个特征向量，给出在 *n* 维空间中的坐标，其中 *n* 是(通常)特征的数量。举个例子，假设我们有特征为*高(米)、宽(米)*和*重(公斤)的数据。*我们可以把它写成一个元组(*高，宽，重)*，我们可以把它看作一个三维空间:

![](img/1144fff2457297a6ea344a21832a9f38.png)

3D feature space for (height, width, weight) populated with some examples.

数据的这种空间表示非常有用的原因之一是，它允许我们在数据中引入“距离”的概念，从而引入“接近”的概念。例如，如果我们试图学习“狗的概念”，那么猫应该比房子更接近狗，这似乎是很自然的。可以肯定地说，人们并不经常把房子误认为是狗，但可能有一些相当像狗的猫。

![](img/b24585d0c228ab90836f538ed5573eb3.png)

Left: Atchoum, the dog-cat. Right: The doggiest house I could find.

给定一些关于狗和非狗的标记数据，我们可以选择一些适当的特征，并在我们的特征空间中绘制数据。如果我们在我们的特征空间中看到代表狗的数据点的清晰聚类，那么我们的数据中的特征一定很好地表征了狗。如果我们的特征在描述狗方面做得很好，我们可能会期望看到任何表示猫的数据点比我们期望看到的任何房子更接近狗群。在我们的数据具有良好特征的理想场景中，我们的特征空间中的 ***空间距离类似于我们试图学习的目标概念的概念距离*** 。下面我们可以看到一个概念学习问题的三个不同质量的可能特征空间，有两个特征:

![](img/37fc3e47d4d2bb746618476db6df5155.png)

Examples of different 2 dimensional feature spaces for a concept learning problem.

上图中最左边的特征空间描绘了概念学习的理想特征空间。属于目标概念的红色数据点可与不属于目标概念的蓝色数据点分开。在这里，空间距离对应的是概念距离，这正是我们想要的。这意味着存在基于这两个特征的完美特征函数，并且该目标概念因此是可学习的。中间的图像描绘了用于概念学习的更真实的特征空间。空间距离近似对应于概念距离，并且由于非完美分离，我们不能基于这些特征定义完美的特征函数，但是我们可以学习对具有小误差的特征函数的有用近似。在一个差的特征空间的最右边的例子中，我们甚至不能希望用任何合理的误差量来近似一个连续的特征函数，因为例子和非例子是混合的。在这种情况下，我们选择的数据特征在将空间距离与概念距离相关联方面表现不佳。

如果我们有一个相对较好的特征空间，如上面的前两个例子，我们的分类问题就变成了如何最好地划分我们的特征空间，以便以最小的误差捕获概念的问题，这是一个纯数值问题，是机器的适当领域。这是特征空间和概念学习之间的本质结合。

## **绘制决策边界**

如果我们的直觉告诉我们，在我们的 *n* 维特征空间中存在代表概念的潜在信息，那么让机器使用这些特征来学习概念就像给它一些训练数据并要求它在空间中绘制一条边界一样简单，以最小的误差将示例和非示例分开。然后，为了分类新的和以前没有见过的例子，我们可以简单地计算它们在决策边界的哪一边。

![](img/540bd22becfcdc2b3a94186599b00cfb.png)

One possible decision boundary for the given 2D concept learning problem.

你听说过的每一种分类学习模型:支持向量机、神经网络、朴素贝叶斯分类器、K 近邻、遗传算法等等。所有这些都(明确地或隐含地)依赖于这种特征空间的思想，并且它们都是用于寻找好的决策边界的简单不同的方法，根据基础特征空间的性质具有不同的优点和缺点。

如果你是机器学习的新手，你可能会问:为什么我们需要让机器来解决这些问题，为什么我们不能可视化我们的特征空间并绘制我们自己的决策边界？答案是双重的:首先，我们根本不是在寻找“画”一个决策边界——我们在寻找一个函数，它将一个例子的特征值作为参数，告诉我们这个例子是否属于这个概念。虽然这个函数可以在特征空间中画出来，但是这个理论的视觉元素只是为了帮助你理解人类。这台机器没有将特征空间可视化——它只是在执行纯粹的数字运算。为了找到一个对应于决策边界的函数，你会发现你必须自己做这些相同的计算，最好是留给机器去做。其次，大多数概念和想法都很复杂，需要高维数据和特征空间来捕捉。这些数据中的模式对于具有三维直觉的人类来说很少是显而易见的。机器不会像我们一样看到数据——它们的感官是数字的，它们的决策是二元的。人类的直觉对于让机器尽可能容易地解决概念学习问题是必要的，但寻找模式是机器的工作。

![](img/f68c4d448d5ff79b6ff51ad72a572453.png)

Where does dog end and cat begin? Learning to recognise a concept in an n-by-n image is an n²-dimensional learning problem if we use each pixel intensity as a feature.

## 机器人会创作交响乐吗？

令人惊讶的是，一个机器人已经做到了。AIVA 是世界上第一个被音乐协会(SACEM)认可的人工智能作曲家。AIVA 代表人工智能虚拟艺术家——它的主要学习方法使用在大量无版权的经典分区上训练的深度神经网络。AIVA 的音乐已经被用于商业用途，其背后的团队正在继续进行音乐创作，并探索 AIVA 表达其创造力的其他途径。

![](img/fa10cdaecee04ba0c9e121311510d866.png)

Suck it Spooner!

AIVA 正在解决的不仅仅是一个简单的概念学习问题，但这篇文章中提出的想法确实是所有机器学习的核心，并将允许我们在未来的博客文章中谈论更复杂的想法。AIVA 不会独自完成所有的作品；它需要人为输入更高层次的元素，如配器，所以从某种意义上说，也许斯普纳侦探是对的，但似乎我们离独立的机器作曲家并不太远。

你可以在这里听 AIVA 的音乐[，也可以在这里](https://soundcloud.com/user-95265362)阅读一篇关于 AIVA [的深度文章。](https://futurism.com/expert-self-driving-cars-will-eliminate-traffic-jams-by-2030)

![](img/ce5fe589cd8808a7c3f22b11701391da.png)

这是一个总结——感谢阅读！

这篇博文是我下一篇博文的前奏，我的下一篇博文将更具技术性，并专注于核心技巧:一种优雅的数学方法，允许我们改善分离并获得更多关于数据的信息，而不增加我们的特征空间的复杂性。敬请期待！

*如果你喜欢这个概念学习和功能空间的介绍，请随时联系我(* [*)关于对未来博客文章的任何想法、疑问或建议！*](https://medium.com/u/de201d936fa2?source=post_page-----45cee19e49db--------------------------------)