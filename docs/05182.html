<html>
<head>
<title>Trivial Multi-Node Training With Pytorch-Lightning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Pytorch-Lightning 进行琐碎的多节点训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/trivial-multi-node-training-with-pytorch-lightning-ff75dfb809bd?source=collection_archive---------3-----------------------#2019-08-03">https://towardsdatascience.com/trivial-multi-node-training-with-pytorch-lightning-ff75dfb809bd?source=collection_archive---------3-----------------------#2019-08-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/5394746f420f44bec9e2a300ab0882f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*OOsQRp49fkL0d3sRAfpOxA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">How you feel running on 200 GPUs</figcaption></figure><p id="a7a9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，您拥有这款出色的 HPC 集群，但仍然只在 1 个 GPU 上训练您的模型？</p><p id="e4b8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我知道。斗争是真实的。😭</p><p id="7bc9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Pytorch-lightning ，为人工智能研究人员设计的 Pytorch Keras，让这变得微不足道。在本指南中，我将介绍:</p><ol class=""><li id="b6ab" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">在同一台机器的多个 GPU 上运行单个模型。</li><li id="d5b4" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">在具有多个 GPU 的多台机器上运行单个模型。</li></ol><p id="5f76" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="ll">免责声明:本教程假设您的集群由 SLURM 管理。</em></p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="739f" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated">模型</h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/ca22348f49651a549947270ab9732d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*l-rEO1N3RwycBHN6p8aSGw.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">AI model</figcaption></figure><p id="8e91" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们先来定义一个<a class="ae kw" href="https://github.com/williamFalcon/pytorch-lightning" rel="noopener ugc nofollow" target="_blank"> PyTorch-Lightning </a> (PTL)模型。这将是来自 PTL 文档的<a class="ae kw" href="https://williamfalcon.github.io/pytorch-lightning/LightningModule/RequiredTrainerInterface/#minimal-example" rel="noopener ugc nofollow" target="_blank">简单 MNIST 示例</a>。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="13fd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，这个模型没有关于 GPU 的任何特定内容。cuda 或类似的东西。PTL 的工作流程是定义一个任意复杂的模型，PTL 将在你指定的任何 GPU 上运行它。</p><h1 id="2314" class="lt lu iq bd lv lw my ly lz ma mz mc md me na mg mh mi nb mk ml mm nc mo mp mq bi translated">多 GPU，单机</h1><p id="689a" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">让我们单独在 CPU 上训练我们的 CoolModel，看看是怎么做的。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="7314" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要在单个 GPU 上训练，只需传入 GPU id</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="444c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">而对于多 GPU，只需添加更多的 id！</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="6d24" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，在您的高级集群 GPU 机器上运行的完整脚本是:</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h1 id="74d2" class="lt lu iq bd lv lw my ly lz ma mz mc md me na mg mh mi nb mk ml mm nc mo mp mq bi translated"><strong class="ak">GPU id</strong></h1><p id="849e" class="pw-post-body-paragraph jy jz iq ka b kb nd kd ke kf ne kh ki kj nf kl km kn ng kp kq kr nh kt ku kv ij bi translated">让我们快速迂回到 NVIDIA GPU 旗帜之地。当你在训练器中设置 GPU[0，1]时，你是在说“在这台机器上使用 GPU 0 和 GPU 1。”该顺序对应于 NVIDIA-SMI 命令的输出</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e40c1b30d8885d1065b13f52b9ac52b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*CgDyWJdxIZGjYGfjRx7SGw.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Courtesy of <a class="ae kw" href="https://www.servethehome.com/inspur-systems-nf5468m5-review-4u-8x-gpu-server/nvidia-smi-gmnt-pytorch-8x-tesla-v100-training/" rel="noopener ugc nofollow" target="_blank">servethehome.com</a></figcaption></figure><p id="eca8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在引擎盖下，PTL 做了以下事情来让你的模型在 GPU 上工作</p><ol class=""><li id="7eb9" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">将 CUDA_VISIBLE_DEVICES 设置为这些 ID 和 CUDA_DEVICE_ORDER = "PCI_BUS_ID ",这保证了 ID 将对应于机器上的物理 GPU 布局(即:NVIDIA-SMI 的输出)。</li><li id="0e4a" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">在您的模型上添加适当的 DistributedDataParallel 或 DataParallel 包装器。</li></ol><p id="1680" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">单节点圆滑线</strong></p><p id="6bdf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">假设您提交了一个使用 2 个 GPU 的 SLURM 作业。您仍然应该将 GPU ids，1 传递给 Trainer，即使您可能实际上正在使用一个 8 GPU 节点 上的物理最后 2 个 GPU。这是因为 SLURM 会为您的工作筛选出正确的 GPU，从 0 开始索引。</p><p id="66be" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，例如，如果您在一个交互式作业中，并且该节点有 8 个 GPU，那么您将需要相应地更改索引。</p><p id="5ad7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，通过使用不同的 GPU ids 启动脚本 4 次，或者运行如下所示的 4 个进程，您可以在单个 8 GPU 节点上运行相同的模型 4 次:</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="0135" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当然，您可能只是出于调试的目的才这样做，或者因为您只是直接 ssh 到一台机器上。</p><p id="fba7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">试管使在单个节点上运行网格搜索变得简单。这里有一个网格搜索的例子，当你不通过 SLURM 提交的时候。</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><h1 id="a5fb" class="lt lu iq bd lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq bi translated"><strong class="ak">多机训练</strong></h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e7fbeb9fa0b2be85c0001572f3acfb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/1*LDpH64WlHCoBI79WeuPYlw.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Synced Training</figcaption></figure><p id="1fa3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要跨多个节点训练 PTL 模型，只需在训练器中设置节点数:</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="c0e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您创建适当的 SLURM 提交脚本并运行这个文件，您的模型将在 80 个 GPU 上训练。</p><p id="3b4d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请记住，您编码的原始模型仍然是相同的。底层模型不知道分布式复杂性。</p><p id="0586" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是使用 PTL 的最大优势之一，将工程任务交给框架，这样您就可以专注于研究。</p><p id="591e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一个 SLURM 脚本示例如下:</p><pre class="ms mt mu mv gt nk nl nm nn aw no bi"><span id="7bc5" class="np lu iq nl b gy nq nr l ns nt"># SLURM SUBMIT SCRIPT<br/></span><span id="db28" class="np lu iq nl b gy nu nr l ns nt">#SBATCH --gres=gpu:8<br/>#SBATCH --nodes=10<br/>#SBATCH --ntasks-per-node=8<br/>#SBATCH --mem=0<br/>#SBATCH --time=02:00:00</span><span id="6631" class="np lu iq nl b gy nu nr l ns nt"># activate conda env<br/>conda activate my_env  </span><span id="a5e1" class="np lu iq nl b gy nu nr l ns nt"># run script from above<br/>python my_test_script_above.py</span></pre><p id="5732" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还有其他可能特定于您的集群的标志，例如:</p><ol class=""><li id="5527" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">划分</li><li id="122b" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">限制</li></ol><p id="ba15" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，PTL 目前只支持 NCCL 后端。这是 PyTorch 团队推荐的后端，也是拥有最快库的后端。如果一个新的，更快的出来，PTL 将尽快添加这一选项。</p><h1 id="fd35" class="lt lu iq bd lv lw my ly lz ma mz mc md me na mg mh mi nb mk ml mm nc mo mp mq bi translated">自动圆滑线脚本提交</h1><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/8b36806423fb123e6ccb6a3f27dc5eec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*QLZ3S2YkHginrW4UFnE4mQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">SLURM… ugh</figcaption></figure><p id="ef1f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">写 SLURM 脚本可以是一个皮塔饼。如果您想进行网格搜索，这一点尤为重要。</p><p id="6fb6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，您可以对 PTL 使用 SlurmCluster 对象，它将:</p><ol class=""><li id="dd90" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">为您自动提交脚本。</li><li id="fcd1" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">将网格搜索分割成单独的脚本，然后全部提交。</li><li id="9a02" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">在下班前几分钟重新排队。</li><li id="3ec8" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">PTL 还将保存并加载您的模型，使重新排队无缝(您不需要做任何事情来保存或加载模型，PTL 会自动完成)。</li></ol><p id="106b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们修改上面的脚本来使用 SlurmCluster 示例。</p><p id="c3c7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">首先，让我们使用 SlurmManager 配置 SLURM 作业</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="3b18" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">当<strong class="ka ir">optimizer _ parallel _ cluster _ GPU</strong>被调用时，SlurmManager 将为通过 HyperOptArgumentParser 传入的每组超参数提交一个脚本。</p><p id="0b6f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，更新后的完整脚本现在看起来像这样:</p><figure class="ms mt mu mv gt jr"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="5291" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要在集群上运行网格搜索，只需:</p><ol class=""><li id="f518" class="kx ky iq ka b kb kc kf kg kj kz kn la kr lb kv lc ld le lf bi translated">ssh 进入您的登录节点</li><li id="f395" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">安装 lightning 后激活您的 conda env</li><li id="6efe" class="kx ky iq ka b kb lg kf lh kj li kn lj kr lk kv lc ld le lf bi translated">运行上面的 python 脚本</li></ol><pre class="ms mt mu mv gt nk nl nm nn aw no bi"><span id="c46b" class="np lu iq nl b gy nq nr l ns nt">ssh some_node</span><span id="4804" class="np lu iq nl b gy nu nr l ns nt">conda activate my_env_with_ptl</span><span id="077a" class="np lu iq nl b gy nu nr l ns nt"># run the above script<br/>python above_script.py</span></pre><p id="7953" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这可能不是您习惯的工作流，但是当您运行这个脚本时，它只会提交每个带有一组超参数的 slurm 作业。然后当脚本再次运行时，它会实际运行你传入<strong class="ka ir">optimize _ parallel _ cluster _ GPU</strong>的函数。</p><p id="7244" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">SLURM 会将所有 out、err 和 submit 脚本文件保存到您传递给 SlurmCluster 对象的目录中。</p><p id="eb2a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">实际的训练日志将由实验对象来写。确保将实验版本设置为 cluster.hpc_exp_number，它在集群作业的总体方案中具有正确的实验版本。</p></div><div class="ab cl lm ln hu lo" role="separator"><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr ls"/><span class="lp bw bk lq lr"/></div><div class="ij ik il im in"><p id="799f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不幸的是，由于底层的 SLURM 系统，在多个节点上的分布式训练需要更多的工作。PTL 将尽可能多的内容抽象出来，并给出了一种使用 SlurmCluster 对象来降低集群复杂度的可选方法。</p></div></div>    
</body>
</html>