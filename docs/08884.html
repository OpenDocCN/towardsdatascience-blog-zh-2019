<html>
<head>
<title>Fastai with 🤗Transformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Fastai 与🤗变形金刚(伯特、罗伯塔、XLNet、XLM、迪沃伯特)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=collection_archive---------3-----------------------#2019-11-27">https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=collection_archive---------3-----------------------#2019-11-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="02cc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 Fastai 实现最先进的 NLP 模型进行情感分析的教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/828fbc4610c1442e5668a0d9dd15728a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCE-OsWxx7NYivrxt69QOw.png"/></div></div></figure><p id="b418" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di">在【2018 年初，<a class="lw lx ep" href="https://medium.com/u/34ab754f8c5e?source=post_page-----4f41ee18ecb2--------------------------------" rel="noopener" target="_blank">杰瑞米·霍华德</a>(fast . ai 的联合创始人)和<a class="lw lx ep" href="https://medium.com/u/e3999e445181?source=post_page-----4f41ee18ecb2--------------------------------" rel="noopener" target="_blank">塞巴斯蒂安·鲁德</a>推出了<a class="ae ly" href="https://arxiv.org/pdf/1801.06146.pdf" rel="noopener ugc nofollow" target="_blank">通用语言模型微调文本分类</a> (ULMFiT)方法。ULMFiT 是第一个应用于 NLP 的<strong class="kt ir">迁移学习</strong>方法。因此，除了显著优于许多最先进的任务之外，它还允许在只有 100 个标记示例的情况下，匹配相当于基于 100 倍以上数据训练的模型的性能。</span></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lz"><img src="../Images/9a4386eadbc734936139178edce5ad77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HUhpxwRcyNFEXNNd"/></div></div><figcaption class="ma mb gj gh gi mc md bd b be z dk"><em class="me">ULMFiT requires less data than previous approaches.</em> (<a class="ae ly" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">Howard and Ruder, ACL 2018</a>)</figcaption></figure><p id="6c5a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我第一次听说 ULMFiT 是在杰瑞米·霍华德的一次 fast.ai 课程中。他演示了如何通过几行代码轻松实现完整的 ULMFiT 方法——多亏了<code class="fe mf mg mh mi b">fastai</code>库。在他的演示中，他使用了在 Wikitext-103 上预先训练的 AWD-LSTM 神经网络，并迅速获得了最先进的结果。他还解释了关键技术——也在 ULMFiT 中演示过——来微调模型，如<strong class="kt ir">区分学习率</strong>、<strong class="kt ir">逐步解冻</strong>或<strong class="kt ir">倾斜三角形学习率</strong>。</p><p id="047a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">自从 ULMFiT 的引入，<strong class="kt ir">迁移学习</strong>在 NLP 中变得非常流行，然而谷歌(BERT，Transformer-XL，XLNet)，脸书(RoBERTa，XLM)甚至 OpenAI (GPT，GPT-2)都开始在非常大的语料库上预先训练他们自己的模型。这一次，他们没有使用 AWD-LSTM 神经网络，而是使用了基于变压器的更强大的架构(参见<a class="ae ly" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>)。</p><p id="028b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然这些模型很强大，但是<code class="fe mf mg mh mi b">fastai</code>并没有将它们全部集成。幸好<a class="ae ly" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱紧脸</a>🤗创建了众所周知的<code class="fe mf mg mh mi b"><a class="ae ly" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">transformers</a></code> <a class="ae ly" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">库</a>。这个库以前被称为<code class="fe mf mg mh mi b">pytorch-transformers</code>或<code class="fe mf mg mh mi b">pytorch-pretrained-bert</code>，它汇集了超过 40 个最先进的预训练 NLP 模型(伯特、GPT-2、罗伯塔、CTRL……)。该实现提供了有趣的附加工具，如 tokenizer、optimizer 或 scheduler。</p><div class="mj mk gp gr ml mm"><a href="https://github.com/huggingface/transformers" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">拥抱脸/变形金刚</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">用于 TensorFlow 2.0 和 PyTorch 的最先进的自然语言处理🤗变形金刚(以前称为…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">github.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na kp mm"/></div></div></a></div><p id="d9bc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><code class="fe mf mg mh mi b">transformers</code>库可以是自给自足的，但是将它并入<code class="fe mf mg mh mi b">fastai</code>库提供了与强大的<code class="fe mf mg mh mi b">fastai</code>工具兼容的更简单的实现，如<strong class="kt ir">区分学习速率</strong>、<strong class="kt ir">逐步解冻</strong>或<strong class="kt ir">倾斜三角形学习速率</strong>。这里的要点是允许任何人——专家或非专家——轻松获得最先进的结果，并“让 NLP 再次变得不酷”。</p><p id="d49e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">值得注意的是，在<code class="fe mf mg mh mi b">fastai</code>中集成拥抱脸<code class="fe mf mg mh mi b">transformers</code>库已经在:</p><ul class=""><li id="3474" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated"><em class="nk"> Keita Kurita </em>的文章<a class="ae ly" href="https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/" rel="noopener ugc nofollow" target="_blank">用快速 AI </a>微调 BERT 的教程，使<code class="fe mf mg mh mi b">pytorch_pretrained_bert</code>库兼容<code class="fe mf mg mh mi b">fastai</code>。</li><li id="8b78" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated"><a class="lw lx ep" href="https://medium.com/u/795cbf38e25d?source=post_page-----4f41ee18ecb2--------------------------------" rel="noopener" target="_blank"> Dev Sharma </a>的文章<a class="ae ly" href="https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c" rel="noopener">将 RoBERTa 与 Fastai 一起用于 NLP </a>，这使得<code class="fe mf mg mh mi b">pytorch_transformers</code>库与<code class="fe mf mg mh mi b">fastai</code>兼容。</li></ul><p id="f18a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然这些文章质量很高，但是它们的演示的某些部分不再与<code class="fe mf mg mh mi b">transformers</code>的最新版本兼容。</p><h1 id="e933" class="nq nr iq bd ns nt nu nv nw nx ny nz oa jw ob jx oc jz od ka oe kc of kd og oh bi translated">🛠集成变压器和 fastai 进行多类分类</h1><p id="e87f" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">在开始实现之前，请注意可以通过多种方式将<code class="fe mf mg mh mi b">transformers</code>集成到<code class="fe mf mg mh mi b">fastai</code>中。为此，我带来了——我认为是——最通用、最灵活的解决方案。更准确地说，我试图对两个库进行最小的修改，同时使它们与最大数量的 transformer 架构兼容。但是，如果你找到一个聪明的方法来实现这个，请在评论区告诉我们！</p><p id="e85b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本教程的 Jupiter 笔记本版本可以在这个<a class="ae ly" href="https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta" rel="noopener ugc nofollow" target="_blank"> Kaggle 内核</a>上获得。</p><h2 id="7874" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">库安装</h2><p id="617e" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">首先，您需要安装<code class="fe mf mg mh mi b">fastai</code>和<code class="fe mf mg mh mi b">transformers</code>库。为此，只需遵循此处<a class="ae ly" href="https://github.com/fastai/fastai/blob/master/README.md#installation" rel="noopener ugc nofollow" target="_blank">和此处</a>和<a class="ae ly" href="https://github.com/huggingface/transformers#installation" rel="noopener ugc nofollow" target="_blank">的说明。</a></p><p id="e141" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这个演示中，我使用了已经安装了<code class="fe mf mg mh mi b">fastai</code>库的 Kaggle。所以我只是用命令安装了<code class="fe mf mg mh mi b">transformers</code>:</p><pre class="kg kh ki kj gt oz mi pa pb aw pc bi"><span id="89e0" class="on nr iq mi b gy pd pe l pf pg">pip install transformers</span></pre><p id="2b1f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">用于本演示的库的版本是<code class="fe mf mg mh mi b">fastai 1.0.58</code>和<code class="fe mf mg mh mi b">transformers 2.1.1</code>。</p><h2 id="0e8b" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">🎬示例任务</h2><p id="2a0f" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">选择的任务是对<a class="ae ly" href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview" rel="noopener ugc nofollow" target="_blank">电影评论</a>进行多类文本分类。</p><p id="9a72" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本文的<a class="ae ly" href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data" rel="noopener ugc nofollow" target="_blank">数据集</a>和相应的<a class="ae ly" href="https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta" rel="noopener ugc nofollow" target="_blank">笔记本</a>可以在 Kaggle 上找到。</p><div class="mj mk gp gr ml mm"><a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data" rel="noopener  ugc nofollow" target="_blank"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">电影评论的情感分析</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">www.kaggle.com</p></div></div><div class="mv l"><div class="ph l mx my mz mv na kp mm"/></div></div></a></div><p id="ff4c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于每个文本电影评论，模型必须预测情感的标签。我们根据分类精度<em class="nk">评估模型的输出</em>。情感标签是:</p><p id="edfc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">0→负<br/>1→有点负<br/>2→空档<br/>3→有点正<br/>4→正</p><p id="60e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用<code class="fe mf mg mh mi b">pandas</code>将数据加载到<code class="fe mf mg mh mi b">DataFrame</code>中。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><h2 id="2e96" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">主变压器等级</h2><p id="b8c1" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">在<code class="fe mf mg mh mi b">transformers</code>中，每个模型架构都与 3 种主要类型的类相关联:</p><ul class=""><li id="6b45" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">一个<strong class="kt ir">模型类</strong>，用于加载/存储特定的预训练模型。</li><li id="c3f2" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">一个<strong class="kt ir">记号赋予器类</strong>，用于预处理数据并使其与特定模型兼容。</li><li id="597d" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">一个<strong class="kt ir">配置类</strong>，用于加载/存储特定型号的配置。</li></ul><p id="1857" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">例如，如果您想使用 BERT 架构进行文本分类，您可以使用<code class="fe mf mg mh mi b"><a class="ae ly" href="https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification" rel="noopener ugc nofollow" target="_blank">BertForSequenceClassification</a></code>作为<strong class="kt ir">模型类</strong>，使用<code class="fe mf mg mh mi b"><a class="ae ly" href="https://huggingface.co/transformers/model_doc/bert.html#berttokenizer" rel="noopener ugc nofollow" target="_blank">BertTokenizer</a></code>作为<strong class="kt ir">标记器类</strong>，使用<code class="fe mf mg mh mi b"><a class="ae ly" href="https://huggingface.co/transformers/model_doc/bert.html#bertconfig" rel="noopener ugc nofollow" target="_blank">BertConfig</a></code>作为<strong class="kt ir">配置类</strong>。</p><p id="f844" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">稍后，您将看到那些类共享一个公共的类方法<code class="fe mf mg mh mi b">from_pretrained(pretrained_model_name, ...)</code>。在我们的例子中，参数<code class="fe mf mg mh mi b">pretrained_model_name</code>是一个字符串，带有要加载的预训练模型/标记器/配置的快捷方式名称，例如<code class="fe mf mg mh mi b">bert-base-uncased</code>。我们可以在<code class="fe mf mg mh mi b">transformers</code>文档<a class="ae ly" href="https://huggingface.co/transformers/pretrained_models.html#pretrained-models" rel="noopener ugc nofollow" target="_blank">这里</a>找到所有的快捷方式名称。</p><p id="3d54" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了在类之间轻松切换——每个类都与一个特定的模型类型相关——我创建了一个字典，它允许通过指定正确的模型类型名来加载正确的类。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="888a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">值得注意的是，在这种情况下，我们只对一个<em class="nk">多类文本分类</em>任务使用<code class="fe mf mg mh mi b">transformers</code>库。出于这个原因，本教程只集成了实现了序列分类模型的 transformer 架构。这些模型类型是:</p><ul class=""><li id="e753" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">伯特(来自谷歌)</li><li id="508b" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">XLNet(来自谷歌/CMU)</li><li id="27fd" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">XLM(来自脸书)</li><li id="82d6" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">罗伯塔(来自脸书)</li><li id="b375" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">蒸馏啤酒(来自拥抱脸)</li></ul><p id="7fe8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然而，如果你想更进一步——通过实现另一种类型的模型或 NLP 任务——本教程仍然是一个很好的开端。</p><h2 id="9f8c" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">数据预处理</h2><p id="51f8" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">为了匹配预训练，我们必须以特定的格式格式化模型输入序列。<br/>为此，您必须首先<strong class="kt ir">对</strong>进行标记，然后<strong class="kt ir">对</strong>文本进行正确的数字化。<br/>这里的困难在于，我们将微调的每个预训练模型需要完全相同的特定预处理，即<strong class="kt ir">记号化</strong> &amp; <strong class="kt ir">数值化</strong>，而不是在预训练部分使用的预处理。<br/>幸运的是，来自<strong class="kt ir"> </strong> <code class="fe mf mg mh mi b">transformers </code>的<strong class="kt ir">记号赋予器类</strong>提供了正确的预处理工具，对应于每个预训练的模型。</p><p id="ab02" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在<code class="fe mf mg mh mi b">fastai</code>库中，数据预处理是在<code class="fe mf mg mh mi b">DataBunch</code>创建期间自动完成的。<br/>正如您将在<code class="fe mf mg mh mi b">DataBunch</code>实现部分看到的那样，<strong class="kt ir">记号赋予器</strong>和<strong class="kt ir">数值化器</strong>在处理器参数中以如下格式传递:</p><pre class="kg kh ki kj gt oz mi pa pb aw pc bi"><span id="58b7" class="on nr iq mi b gy pd pe l pf pg">processor = [TokenizeProcessor(tokenizer=tokenizer,…), NumericalizeProcessor(vocab=vocab,…)]</span></pre><p id="5592" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">让我们首先分析如何将<code class="fe mf mg mh mi b">transformers</code>记号赋予器集成到<code class="fe mf mg mh mi b">TokenizeProcessor</code>函数中。</p><p id="a5be" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">自定义标记器</strong></p><p id="3e4a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这一部分可能有点混乱，因为许多类被包装在一起，并且具有相似的名称。<br/>继续，如果我们仔细观察<code class="fe mf mg mh mi b">fastai</code>的实现，我们会注意到:</p><ol class=""><li id="9289" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm pk nh ni nj bi translated"><code class="fe mf mg mh mi b"><a class="ae ly" href="https://docs.fast.ai/text.data.html#TokenizeProcessor" rel="noopener ugc nofollow" target="_blank">TokenizeProcessor</a></code> <a class="ae ly" href="https://docs.fast.ai/text.data.html#TokenizeProcessor" rel="noopener ugc nofollow" target="_blank">对象</a>将<code class="fe mf mg mh mi b">Tokenizer</code>对象作为<code class="fe mf mg mh mi b">tokenizer</code>参数。</li><li id="c9b6" class="nb nc iq kt b ku nl kx nm la nn le no li np lm pk nh ni nj bi translated"><code class="fe mf mg mh mi b"><a class="ae ly" href="https://docs.fast.ai/text.transform.html#Tokenizer" rel="noopener ugc nofollow" target="_blank">Tokenizer</a></code> <a class="ae ly" href="https://docs.fast.ai/text.transform.html#Tokenizer" rel="noopener ugc nofollow" target="_blank">对象</a>将一个<code class="fe mf mg mh mi b">BaseTokenizer</code>对象作为<code class="fe mf mg mh mi b">tok_func</code>参数。</li><li id="bf2d" class="nb nc iq kt b ku nl kx nm la nn le no li np lm pk nh ni nj bi translated"><code class="fe mf mg mh mi b"><a class="ae ly" href="https://docs.fast.ai/text.transform.html#BaseTokenizer" rel="noopener ugc nofollow" target="_blank">BaseTokenizer</a></code> <a class="ae ly" href="https://docs.fast.ai/text.transform.html#BaseTokenizer" rel="noopener ugc nofollow" target="_blank">对象</a>实现函数<code class="fe mf mg mh mi b">tokenizer(t:str) → List[str]</code>,该函数获取文本<code class="fe mf mg mh mi b">t</code>并返回其令牌列表。</li></ol><p id="ef7f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，我们可以简单地创建一个继承自<code class="fe mf mg mh mi b">BaseTokenizer</code>的新类<code class="fe mf mg mh mi b">TransformersBaseTokenizer</code>，并重写一个新的<code class="fe mf mg mh mi b">tokenizer</code>函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="5ea5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这个实现中，要注意三件事:</p><ol class=""><li id="10d7" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm pk nh ni nj bi translated">由于我们没有使用 RNN，我们必须将<em class="nk">序列长度</em>限制为模型输入大小。</li><li id="bb30" class="nb nc iq kt b ku nl kx nm la nn le no li np lm pk nh ni nj bi translated">大多数模型需要在序列的开头和结尾放置特殊的标记。</li><li id="0684" class="nb nc iq kt b ku nl kx nm la nn le no li np lm pk nh ni nj bi translated">像 RoBERTa 这样的一些模型需要一个<em class="nk">空格</em>来开始输入字符串。对于那些模型，应该在<code class="fe mf mg mh mi b">add_prefix_space</code>设置为<code class="fe mf mg mh mi b">True</code>的情况下调用编码方法。</li></ol><p id="fdb4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面，您可以找到本教程中使用的 5 种模型类型的每个预处理要求的简历。您也可以在每个模型部分的<a class="ae ly" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">拥抱脸文档</a>中找到该信息。</p><blockquote class="pl pm pn"><p id="d950" class="kr ks nk kt b ku kv jr kw kx ky ju kz po lb lc ld pp lf lg lh pq lj lk ll lm ij bi translated">伯特:[CLS] +代币+ [SEP] +填充</p><p id="f9f7" class="kr ks nk kt b ku kv jr kw kx ky ju kz po lb lc ld pp lf lg lh pq lj lk ll lm ij bi translated">蒸馏瓶:[CLS] +代币+ [SEP] +填充</p><p id="c468" class="kr ks nk kt b ku kv jr kw kx ky ju kz po lb lc ld pp lf lg lh pq lj lk ll lm ij bi translated">罗伯塔:[CLS] +前缀空格+记号+ [SEP] +填充</p><p id="8df5" class="kr ks nk kt b ku kv jr kw kx ky ju kz po lb lc ld pp lf lg lh pq lj lk ll lm ij bi translated">XLM: [CLS] +代币+ [SEP] +填充</p><p id="86d1" class="kr ks nk kt b ku kv jr kw kx ky ju kz po lb lc ld pp lf lg lh pq lj lk ll lm ij bi translated">XLNet:填充+令牌+ [SEP] + [CLS]</p></blockquote><p id="7fbb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">值得注意的是，在这部分实现中，我们不添加填充。<br/>我们将在后面看到，<code class="fe mf mg mh mi b">fastai</code>会在<code class="fe mf mg mh mi b">DataBunch</code>创建期间自动管理它。</p><p id="9f2a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">自定义数值化器</strong></p><p id="5b06" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在<code class="fe mf mg mh mi b">fastai</code>中，<code class="fe mf mg mh mi b"><a class="ae ly" href="https://docs.fast.ai/text.data.html#NumericalizeProcessor" rel="noopener ugc nofollow" target="_blank">NumericalizeProcessor</a></code> <a class="ae ly" href="https://docs.fast.ai/text.data.html#NumericalizeProcessor" rel="noopener ugc nofollow" target="_blank">对象</a>将一个<code class="fe mf mg mh mi b"><a class="ae ly" href="https://docs.fast.ai/text.transform.html#Vocab" rel="noopener ugc nofollow" target="_blank">Vocab</a></code> <a class="ae ly" href="https://docs.fast.ai/text.transform.html#Vocab" rel="noopener ugc nofollow" target="_blank">对象</a>作为<code class="fe mf mg mh mi b">vocab</code>参数。<br/>通过这种分析，我建议采用两种方法来适应<code class="fe mf mg mh mi b">fastai</code> <strong class="kt ir">数控化器</strong>:</p><ol class=""><li id="c668" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm pk nh ni nj bi translated">你可以像在<a class="ae ly" href="https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c" rel="noopener"> Dev Sharma 的文章</a>中描述的那样(第 1 节。<em class="nk">设置记号赋予器</em>，检索记号列表并创建一个<code class="fe mf mg mh mi b">Vocab</code>对象。</li><li id="9977" class="nb nc iq kt b ku nl kx nm la nn le no li np lm pk nh ni nj bi translated">创建一个继承自<code class="fe mf mg mh mi b">Vocab</code>的新类<code class="fe mf mg mh mi b">TransformersVocab</code>，并覆盖<code class="fe mf mg mh mi b">numericalize</code>和<code class="fe mf mg mh mi b">textify</code>函数。</li></ol><p id="d539" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">即使第一个解决方案看起来更简单，<code class="fe mf mg mh mi b">Transformers</code>也没有为所有模型提供一个直接的方法来检索他的令牌列表。<br/>因此，我实现了第二个解决方案，它针对每个模型类型运行。<br/>包括分别使用<code class="fe mf mg mh mi b">numericalize</code>和<code class="fe mf mg mh mi b">textify</code>中的<code class="fe mf mg mh mi b">convert_tokens_to_ids</code>和<code class="fe mf mg mh mi b">convert_ids_to_tokens</code>功能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="39e7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意:函数<code class="fe mf mg mh mi b">__gestate__ </code>和<code class="fe mf mg mh mi b">__setstate__</code>允许函数<a class="ae ly" href="https://docs.fast.ai/basic_train.html#Learner.export" rel="noopener ugc nofollow" target="_blank">导出</a>和<a class="ae ly" href="https://docs.fast.ai/basic_train.html#load_learner" rel="noopener ugc nofollow" target="_blank"> load_learner </a>与<code class="fe mf mg mh mi b">TranformersVocab</code>一起正确工作。</p><p id="2fff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">定制处理器</strong></p><p id="ae85" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">既然我们已经有了自定义的<strong class="kt ir">记号赋予器</strong>和<strong class="kt ir">数值化器</strong>，我们就可以创建自定义的<strong class="kt ir">处理器</strong>。注意我们正在传递<code class="fe mf mg mh mi b">include_bos = False</code>和<code class="fe mf mg mh mi b">include_eos = False</code>选项。这是因为默认情况下<code class="fe mf mg mh mi b">fastai</code>添加了自己的特殊标记，这会干扰我们的自定义标记器添加的<code class="fe mf mg mh mi b">[CLS]</code>和<code class="fe mf mg mh mi b">[SEP]</code>标记。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="298b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">设置数据发送</strong></p><p id="239a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于 DataBunch 创建，您必须注意将参数<code class="fe mf mg mh mi b">processor</code>设置为我们新的定制处理器<code class="fe mf mg mh mi b">transformer_processor</code>，并正确管理填充。</p><p id="ea55" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如拥抱脸文档中提到的，伯特、罗伯塔、XLM 和迪尔伯特是具有绝对位置嵌入的模型，所以通常建议在右侧填充输入，而不是在左侧。关于 XLNET，它是一个具有相对位置嵌入的模型，因此，您可以在右侧或左侧填充输入。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><h2 id="2eb3" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">定制模型</h2><p id="ac79" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">正如这里提到的<a class="ae ly" href="https://github.com/huggingface/transformers#models-always-output-tuples" rel="noopener ugc nofollow" target="_blank">和</a>一样，每个模型的 forward 方法总是输出一个<code class="fe mf mg mh mi b">tuple</code>，其中包含各种元素，具体取决于模型和配置参数。在我们的例子中，我们只对访问逻辑感兴趣。<br/>访问它们的一种方法是创建自定义模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="ee80" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了使我们的转换器适应多类分类，在加载预训练模型之前，我们需要精确标签的数量。为此，您可以修改 config 实例，或者像 Keita Kurita 的文章中那样修改<code class="fe mf mg mh mi b">num_labels</code>参数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><h2 id="d1f4" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">学员:自定义优化器/自定义指标</h2><p id="f125" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">在<code class="fe mf mg mh mi b">pytorch-transformers</code>中，拥抱脸实现了两个特定的优化器——BertAdam 和 OpenAIAdam——它们已经被一个 AdamW 优化器所取代。<br/>这个优化器匹配 Pytorch Adam 优化器 Api，因此，将它集成到<code class="fe mf mg mh mi b">fastai</code>中变得很简单。<br/>注意，为了重现贝塔姆的特定行为，你必须设置<code class="fe mf mg mh mi b">correct_bias = False</code>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><h2 id="11f1" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">有区别的微调和逐步解冻</h2><p id="7498" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">为了使用<strong class="kt ir">区别学习率</strong>和<strong class="kt ir"> G <em class="nk">随机解冻</em> </strong>，<code class="fe mf mg mh mi b">fastai</code>提供了一个工具，允许将结构模型“分割”成组。fastai 文档<a class="ae ly" href="https://docs.fast.ai/basic_train.html#Discriminative-layer-training" rel="noopener ugc nofollow" target="_blank">中描述了执行“分割”的指令，此处为</a>。</p><p id="793d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">不幸的是，模型架构差异太大，无法创建一个独特的通用函数，以方便的方式“拆分”所有的模型类型。因此，您将不得不为每个不同的模型架构实现一个定制的“分割”。</p><p id="17c5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">例如，如果我们使用 DistilBERT 模型，我们通过制作<code class="fe mf mg mh mi b">print(learner.model)</code>来观察他的架构。我们可以决定将模型分成 8 块:</p><ul class=""><li id="1ec7" class="nb nc iq kt b ku kv kx ky la nd le ne li nf lm ng nh ni nj bi translated">1 嵌入</li><li id="10b1" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">6 变压器</li><li id="5532" class="nb nc iq kt b ku nl kx nm la nn le no li np lm ng nh ni nj bi translated">1 个分类器</li></ul><p id="6eef" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这种情况下，我们可以这样分割我们的模型:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="7e3f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请注意，我没有发现任何文件研究了<strong class="kt ir">区别学习率</strong>和<strong class="kt ir">逐步解冻</strong>甚至<strong class="kt ir">倾斜三角形学习率</strong>对变压器架构的影响。<strong class="kt ir"> </strong>因此，使用这些工具并不能保证更好的结果。如果你发现了任何有趣的文档，请在评论中告诉我们。</p><h2 id="4e16" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">火车</h2><p id="8b5e" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">现在我们终于可以使用所有的<code class="fe mf mg mh mi b">fastai</code>内置特性来训练我们的模型了。像 ULMFiT 方法一样，我们将使用<strong class="kt ir">倾斜三角学习率、区分学习率</strong>和<strong class="kt ir">逐渐解冻</strong>模型。</p><p id="3e1e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，我们首先冻结除分类器之外的所有组，使用:</p><pre class="kg kh ki kj gt oz mi pa pb aw pc bi"><span id="9b64" class="on nr iq mi b gy pd pe l pf pg">learner.freeze_to(-1)</span></pre><p id="cc88" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于<strong class="kt ir">倾斜三角形学习率</strong>，您必须使用函数<code class="fe mf mg mh mi b">fit_one_cycle</code>。更多信息，请查看 fastai 文档<a class="ae ly" href="https://docs.fast.ai/callbacks.one_cycle.html" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="6a82" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了使用我们的<code class="fe mf mg mh mi b">fit_one_cycle</code>,我们需要一个最佳的学习速率。我们可以用一个学习率查找器来找到这个学习率，这个学习率查找器可以用<code class="fe mf mg mh mi b"><a class="ae ly" href="https://docs.fast.ai/callbacks.lr_finder.html#callbacks.lr_finder" rel="noopener ugc nofollow" target="_blank">lr_find</a></code>来调用。我们的图表看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/af727f154ab62c83e127ee65c5c461e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*p0EmMlM1i5AG-OQ3uJoAog.png"/></div></figure><p id="ffb2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将选择一个比最小值稍早的值，此时损失仍会增加。这里 2x 10–3 似乎是一个不错的值。</p><pre class="kg kh ki kj gt oz mi pa pb aw pc bi"><span id="a23d" class="on nr iq mi b gy pd pe l pf pg">learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))</span></pre><p id="7edd" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">损失图看起来像这样:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ps"><img src="../Images/544a08437e64a375c5cdfce7070dbf44.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*dJyzyjsaMaVUhea9beSkCA.png"/></div></div></figure><p id="117b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">然后，我们解冻第二个组，并重复这些操作，直到所有组都解冻。如果您想使用<strong class="kt ir">区别学习率</strong>，您可以如下使用<code class="fe mf mg mh mi b">slice</code>:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="3ea4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要解冻所有组，使用<code class="fe mf mg mh mi b">learner.unfreeze()</code>。</p><h2 id="bc75" class="on nr iq bd ns oo op dn nw oq or dp oa la os ot oc le ou ov oe li ow ox og oy bi translated">创建预测</h2><p id="c566" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">既然我们已经训练了模型，我们想要从测试数据集生成预测。</p><p id="6b16" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如<em class="nk"> Keita Kurita </em>的<a class="ae ly" href="https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/" rel="noopener ugc nofollow" target="_blank">文章</a>中所述，由于函数<code class="fe mf mg mh mi b">get_preds</code>默认情况下不按顺序返回元素，因此您必须将元素按正确的顺序重新排序。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="pi pj l"/></div></figure><p id="8d60" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在<a class="ae ly" href="https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta" rel="noopener ugc nofollow" target="_blank"> Kaggle 示例</a>中，在没有过多使用参数的情况下，我们获得了 0.70059 的公开分数，这使我们在排行榜上排名第 5！</p><h1 id="95d3" class="nq nr iq bd ns nt nu nv nw nx ny nz oa jw ob jx oc jz od ka oe kc of kd og oh bi translated">📋结论</h1><p id="27f0" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">在本文中，我解释了如何将<code class="fe mf mg mh mi b">transformers</code>库与心爱的<code class="fe mf mg mh mi b">fastai</code>库结合起来。它的目的是让你明白在哪里寻找和修改这两个库，使它们一起工作。很可能，它允许你使用<strong class="kt ir">倾斜三角形学习率</strong>、<strong class="kt ir">区别学习率</strong>和<strong class="kt ir"> </strong>甚至<strong class="kt ir">逐步解冻</strong>。因此，甚至无需调整参数，您就可以快速获得最先进的结果。</p><p id="1077" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">今年，变形金刚成了 NLP 的必备工具。正因为如此，我认为预先训练的变形金刚架构将很快被整合到<code class="fe mf mg mh mi b">fastai</code>的未来版本中。同时，本教程是一个很好的开端。</p><p id="5d61" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我希望您喜欢这第一篇文章，并发现它很有用。<br/>感谢您的阅读，不要犹豫，请留下问题或建议。</p><p id="b37a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我会继续在 NLP 上写文章，敬请期待！</p><h1 id="5065" class="nq nr iq bd ns nt nu nv nw nx ny nz oa jw ob jx oc jz od ka oe kc of kd og oh bi translated">📑参考</h1><p id="0a2f" class="pw-post-body-paragraph kr ks iq kt b ku oi jr kw kx oj ju kz la ok lc ld le ol lg lh li om lk ll lm ij bi translated">[1]拥抱脸，变形金刚 GitHub(2019 年 11 月)，<a class="ae ly" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">https://github.com/huggingface/transformers</a></p><p id="1cd7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2] Fast.ai，Fastai 文档(2019 年 11 月)，<a class="ae ly" href="https://docs.fast.ai/text.html" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/text.html</a></p><p id="d6ac" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3]https://arxiv.org/abs/1801.06146<a class="ae ly" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德&amp;塞巴斯蒂安·鲁德，文本分类通用语言模型微调(2018 年 5 月)</a></p><p id="f210" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[4] Keita Kurita，<a class="ae ly" href="https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/" rel="noopener ugc nofollow" target="_blank">用快速人工智能微调 BERT 的教程</a>(2019 年 5 月)</p><p id="331a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[5] <a class="lw lx ep" href="https://medium.com/u/795cbf38e25d?source=post_page-----4f41ee18ecb2--------------------------------" rel="noopener" target="_blank"> Dev Sharma </a>，<a class="ae ly" href="https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c" rel="noopener">使用 RoBERTa 和 Fastai 进行 NLP</a>(2019 年 9 月)</p></div></div>    
</body>
</html>