# 高级 Keras —准确恢复训练过程

> 原文：<https://towardsdatascience.com/resuming-a-training-process-with-keras-3e93152ee11a?source=collection_archive---------4----------------------->

## 处理使用自定义回调的重要情况

![](img/6dfc34bd9904d7a904c28eb4f88e1e69.png)

Photo Credit: Eyal Zakkay

在这篇文章中，我将展示 Keras API 的一个用例，在这个用例中，从一个加载的检查点恢复一个训练过程需要以不同于平常的方式来处理。

**TL；DR —如果您使用的自定义回调函数的内部变量在训练过程中会发生变化，那么您需要在恢复时通过不同的方式初始化这些回调函数来解决这个问题。**

当使用 Keras 训练深度学习模型时，我们通常会保存该模型状态的检查点，以便我们可以恢复中断的训练过程，并从我们停止的地方重新开始。通常这是通过[模型检查点回调](https://keras.io/callbacks/#modelcheckpoint)来完成的。根据 Keras 的[文档，保存的模型(用`model.save(filepath)`保存)包含以下内容:](https://keras.io/getting-started/faq/#savingloading-whole-models-architecture-weights-optimizer-state)

*   模型的架构，允许重新创建模型
*   模型的权重
*   培训配置(损失、优化器)
*   优化器的状态，**允许从您停止的地方继续训练**。

在某些用例中，最后一部分并不完全正确。

**示例**:

假设您正在训练一个具有自定义学习率调度器回调的模型，它会在**每批**之后更新 LR:

当回调被创建时，`counter`变量被初始化为零，并与全局批处理索引保持一致(在`on_batch_end`中的`batch` 参数将批处理索引保存在*当前*时期内)。

假设我们想从一个检查点恢复一个训练过程。通常的方法是:

The wrong way to do it

注意，`LearningRateSchedulerPerBatch`回调用`counter=0`初始化，即使在恢复时也是如此。当训练恢复时**,这将不会重现保存检查点时发生的相同情况。**学习率将从初始值重新开始，这可能不是我们想要的。

# 做这件事的正确方法

我们看到了一个例子，回调的错误初始化如何在恢复时导致不想要的结果。有几种方法可以实现这一点，这里我将描述两种方法:

**解决方案 1:用正确的值更新变量**

在处理简单的情况时，回调只有少量的更新变量，在恢复之前覆盖这些变量的值是相当简单的。在我们的例子中，如果我们想用正确的值`count`来恢复，我们将做以下事情:

**解决方案 2:用 Pickle 保存和加载回调**

当我们的自定义回调有许多更新变量或包含复杂行为时，安全地覆盖每个变量可能会很困难。另一个解决方案是在每次保存检查点时清理回调实例，然后我们可以在恢复时加载这个清理，并用所有正确的值重建原始回调。

***注意:为了清理你的回调函数，它们不能包含任何不可序列化的元素。此外，在 Keras 版本< 2.2.3 中，模型本身是不可序列化的。这防止了任何回调的酸洗，因为每个回调也持有对模型的引用。***

在这种情况下，恢复将如下所示:

您可能已经注意到，我使用了一个名为`ModelCheckpointEnhanced`的修改过的检查点回调。这是因为使用 pickless 方法意味着我们还需要`ModelCheckpoint`回调来保存相关回调的 pickle。这种修改后的回调的示例实现可能如下所示:

上面的例子处理了你只需要处理一个回调的情况，如果你有多个回调需要保存，你将需要执行一些小的修改。

# 摘要

我们看到了在某些情况下，采取天真的方法来恢复训练过程会导致不希望的结果。我们看到了两种处理这种情况的方法，以便在重新开始中断的培训过程时获得一致的结果。

我使用的例子来自我的草图-RNN 算法的 [Keras 实现，这是一个用于生成草图的序列到序列变分自动编码器模型。](https://github.com/eyalzk/sketch_rnn_keras)