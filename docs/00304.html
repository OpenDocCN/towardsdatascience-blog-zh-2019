<html>
<head>
<title>Everything you need to know about Neural Networks and Backpropagation — Machine Learning Easy and Fun</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你需要知道的关于神经网络和反向传播的一切——机器学习简单而有趣</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a?source=collection_archive---------0-----------------------#2019-01-14">https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a?source=collection_archive---------0-----------------------#2019-01-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6c70" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经网络的基础解释，包括理解背后的数学</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/13130b463c4e2da3cbd093e3b6f5ba5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*bhFifratH9DjKqMBTeQG5A.gif"/></div></div></figure><p id="8813" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我发现很难在一个地方得到关于神经网络的一步一步和详细的解释。在课程或视频中，总会遗漏一些解释。所以我试着在一篇博文中收集所有的信息和解释(一步一步)。我会把这个博客分成 8 个部分，因为我觉得它最相关。</p><ol class=""><li id="1cc7" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">模型表示</li><li id="91ec" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">模型表示数学</li><li id="9913" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">激活功能</li><li id="8dad" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">偏置节点</li><li id="7830" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">价值函数</li><li id="ccc5" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">正向传播计算</li><li id="2ad0" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">反向传播算法</li><li id="011d" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">代码实现</li></ol><p id="13f1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="me">那么让我们开始……</em></strong></p><h1 id="baa9" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">模型表示</h1><p id="1e10" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated"><a class="ae nc" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="me">人工神经网络</em> </strong> </a>是受构成动物大脑的生物神经网络启发的计算系统。这种系统通过考虑例子来“学习”执行任务，通常没有用任何特定于任务的规则来编程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/8877d9a0ec989904c6a990a19ca8e595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fA77_mLNiJTSgZFhYnU0Q.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 1: Neural Network Architecture</figcaption></figure><p id="ddbd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">神经网络由 3 种类型的层构成:</p><ol class=""><li id="bcd0" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">输入层-神经网络的初始数据。</li><li id="ef42" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">隐藏层-输入层和输出层之间的中间层，所有计算都在此完成。</li><li id="4a2a" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">输出图层-生成给定输入的结果。</li></ol><p id="a937" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上图中有 3 个黄色圆圈。它们代表输入层，通常记为矢量<strong class="kw iu"><em class="me">x。</em></strong>有 4 个蓝色和 4 个绿色圆圈代表隐藏层。这些圆圈代表“激活”节点，通常记为<strong class="kw iu"> <em class="me"> W </em> </strong>或<strong class="kw iu"> <em class="me"> θ </em> </strong>。红色圆圈是输出图层或预测值(或多个输出类/类型的值)。</p><p id="f7e7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每个节点都与下一层的每个节点相连接，并且每个连接(黑色箭头)都有特定的权重。权重可以被视为该节点对来自下一层节点的影响。因此，如果我们看一下一个节点，它会像这样</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/53ae2b9618d3c427100d720c6e279bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*uz3wd5YeVYlU2JR8rE9VDA.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 2: Node from Neural Network</figcaption></figure><p id="ff7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们来看最上面的蓝色节点(<em class="me">“图片 1】</em>)。前一层(黄色)的所有节点都与之相连。所有这些连接都代表权重(影响)。当来自黄色层的所有节点值乘以它们的权重并且所有这些被汇总时，它给出了顶部蓝色节点的一些值。蓝色节点具有预定义的“激活”功能(<em class="me">“图像 2”</em>上的<em class="me">单位步进功能</em>)，该功能根据汇总值定义该节点是否将被“激活”或其“激活”程度。值为 1 的附加节点称为“偏置”节点。</p><h1 id="f430" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">模型表示数学</h1><p id="7498" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">为了理解数学方程，我将使用一个更简单的神经网络模型。这个模型将有 4 个输入节点(3 + 1“偏差”)<em class="me">。</em>一个隐藏层，有 4 个节点(3 + 1“偏差”)和一个输出节点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/32c60bc09884a53ed268453b54fd42c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pR6bSQXgC1Y_lYTeLDqVqg.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 3: Simple Neural Network</figcaption></figure><p id="9737" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将把“偏差”节点分别标记为 x₀和 a₀。因此，输入节点可以放在一个向量<em class="me"> X </em>中，隐藏层的节点放在向量<em class="me"> A </em>中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/f85869f1f98290ce0f8d642fdb2a997d.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*GkER6dYs0kF42MZRXscDXA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 4: X (input layer) and A (hidden layer) vector</figcaption></figure><p id="adc2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">重量(箭头)通常记为<em class="me"> θ </em>或<em class="me"> W. </em>在这种情况下，我将它们记为<em class="me"> θ。</em>输入层和隐藏层之间的权重将代表<em class="me"> 3x4 </em>矩阵。并且隐藏层和输出层之间的权重将表示为<em class="me"> 1x4 </em>矩阵。</p><blockquote class="nl nm nn"><p id="d84f" class="ku kv me kw b kx ky ju kz la lb jx lc no le lf lg np li lj lk nq lm ln lo lp im bi translated">如果网络在层<em class="it"> j </em>中有<strong class="kw iu"> a </strong>个单元，在层<em class="it"> j </em> +1 中有<strong class="kw iu"> b </strong>个单元，则θⱼ的维数为<strong class="kw iu"> b ×(a+1) </strong>。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0148bb901d9489187d23c849ccafd615.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*-KhURrO42dMXj7aP_VtF9Q.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 5: Layer 1 Weights Matrix (θ)</figcaption></figure><p id="9491" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，我们想要的是计算隐藏层的“激活”节点。为此，我们需要将输入向量<strong class="kw iu"> <em class="me"> X </em> </strong> <em class="me"> </em>和权重矩阵<strong class="kw iu"> <em class="me"> θ </em> </strong> <em class="me"> </em>乘以第一层<em class="me">(</em><strong class="kw iu"><em class="me">X</em></strong><em class="me">*</em><strong class="kw iu"><em class="me">【θ】</em></strong>，然后应用激活函数<strong class="kw iu"> <em class="me"> g </em> </strong>我们得到的是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/542fef89b08bf238036a6f69a2678d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*gq00mlIXFaCZ8h0MZ8D9Sg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 6: Compute activation nodes</figcaption></figure><p id="937c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">并且通过将隐藏层向量与第二层的权重矩阵<strong class="kw iu"><em class="me">θ</em></strong>(<strong class="kw iu"><em class="me">A</em></strong><em class="me">*</em><strong class="kw iu"><em class="me">θ</em></strong>)相乘，我们得到假设函数的输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4dceb4e4061dc1bee4a03a151121e0af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*iQJVglZqVZ6A2er1tNRf0w.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 7: Compute output node value (hypothesis)</figcaption></figure><p id="685c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个例子只有一个隐藏层和 4 个节点。如果我们试图推广具有多个隐藏层和每个层中的多个节点的神经网络，我们将得到下一个公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/bb9a41e70772f3a19e23570e0129f570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8SKwKAo5R2GuAzvUuAjmVw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 8: Generalized Compute node value function</figcaption></figure><p id="6385" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中我们有具有 n 个节点的 L 层和具有 m 个节点的层。</p><h1 id="8193" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">激活功能</h1><p id="135e" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">在神经网络中，激活函数基于加权和来定义给定节点是否应该被“激活”。我们把这个加权和值定义为<strong class="kw iu"> <em class="me"> z </em> </strong>。在本节中，我将解释为什么“阶跃函数”和“线性函数”不起作用，并讨论最流行的激活函数之一的“<em class="me"> Sigmoid 函数</em>”。还有一些其他的功能，我暂时先放在一边。</p><h2 id="1fa3" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">阶跃函数</h2><p id="5303" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">第一个想法是使用所谓的“<em class="me">阶跃函数”</em>(离散输出值)，其中我们定义阈值和:</p><p id="ce6b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="me"> if(z &gt;阈值)——“激活”节点(值 1) <br/> if(z &lt;阈值)——不“激活”节点(值 0) </em></p><p id="212f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这看起来不错，但它有缺点，因为节点只能输出值 1 或 0。如果我们想要映射多个输出类(节点),就会遇到问题。问题是有可能激活多个输出类/节点(值为 1)。所以我们不能正确地分类/决定。</p><h2 id="17dd" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">线性函数</h2><p id="fb01" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">另一种可能是定义“<em class="me">线性函数”</em>，并获得一系列输出值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3b2fe69d2d1ad6e635830f31f9c02c47.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*FbfkI-rN327KMIj2OAnr_w.png"/></div></figure><p id="5eb6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然而，在神经网络中仅使用线性函数会导致输出层是线性函数，因此我们不能映射任何<strong class="kw iu"> <em class="me">非线性</em> </strong>数据。对此的证明如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/8020bd13eb83a33073c2d1c991397d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*HqYoWOQmNQi1B1nmK438Lg.png"/></div></figure><p id="57e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后由<a class="ae nc" href="https://en.wikipedia.org/wiki/Function_composition" rel="noopener ugc nofollow" target="_blank">函数组合</a>我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c8976a9bf309eb9ff2d3813c6f6e0863.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*pYlNJc4VWfJTNIKY_CJiSw.png"/></div></figure><p id="7456" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这也是一个线性函数。</p><h2 id="4d5e" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">Sigmoid 函数</h2><p id="8ff0" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">它是当今使用最广泛的激活功能之一。它的方程式由下面的公式给出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/cdbe302e48dc331024f1d5802f71f820.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*Exx_40zOAvrSV3mP6_aTcw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 9: Sigmoid Equation. source: wikipedia</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/4d8291ee2d2842489d2e91bc35027d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jSCPkJo0ZpBRA5H3JqFhQg.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 10: Sigmoid Function. source: wikipedia</figcaption></figure><p id="be0d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它有多种属性，这使得它如此受欢迎:</p><ul class=""><li id="2ad7" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp om lw lx ly bi translated">这是非线性函数</li><li id="569a" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp om lw lx ly bi translated">范围值在(0，1)之间</li><li id="e7cb" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp om lw lx ly bi translated">在 x 轴上的(-2，2)之间，函数非常陡峭，这导致函数倾向于将值分类为 1 或 0</li></ul><p id="109e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于这个属性，它允许节点取 0 到 1 之间的任何值。最后，在多个输出类的情况下，这将导致每个输出类的<em class="me">【激活】</em>的不同概率。而我们会选择“激活”(概率)值最高的一个。</p><h1 id="3d84" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">偏置节点</h1><p id="2fe3" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">使用“偏差”节点通常是创建成功学习模型的关键。简而言之，<strong class="kw iu"> <em class="me">偏置值允许将激活函数向左或向右</em> </strong>移动，这有助于使<strong class="kw iu">更好地适合数据</strong>(作为输出的更好的预测函数)。</p><p id="9879" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是我画的 3 个 Sigmoid 函数，你可以注意到变量<em class="me"> x </em>与某个值相乘/相加/相减是如何影响函数的。</p><ul class=""><li id="b4b3" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp om lw lx ly bi translated">乘以<strong class="kw iu"> <em class="me"> x </em> </strong> —使函数更加陡峭</li><li id="9c32" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp om lw lx ly bi translated">加减<strong class="kw iu"> <em class="me"> x </em> </strong> —向左/向右移动功能</li></ul><div class="kj kk kl km gt ab cb"><figure class="on kn oo op oq or os paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8052d626223a2841285051032d0217de.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*VomTG4TgrZl65gTZS_lfQg.png"/></div></figure><figure class="on kn ot op oq or os paragraph-image"><img src="../Images/b21f95db088e3d32578a238474cb517a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*uEALCpzwMQCt4hGDGiBrEg.png"/><figcaption class="ne nf gj gh gi ng nh bd b be z dk ou di ov ow">Image 11: Sigmoid Functions. source: <a class="ae nc" href="https://www.desmos.com/calculator" rel="noopener ugc nofollow" target="_blank">desmos.com</a></figcaption></figure></div><h1 id="a938" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">价值函数</h1><p id="e366" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">让我们从定义成本函数的通用方程开始。该函数表示误差的总和，即预测值和真实(标记)值之间的差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5b50a3a95e171e937ce037ac60d958e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*TmcCWdls8i1E9P0SDJuT1w.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 12: General Cost functoin. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="a2f5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于这是一个分类问题类型<strong class="kw iu"> <em class="me"> y </em> </strong>只能取离散值{0，1}。它只能在一种类型的类中。例如，如果我们对狗(类别 1)、猫(类别 2)和鸟(类别 3)的图像进行分类。如果输入图像是狗。对于 dog 类，输出类的值为 1，对于其他类，输出类的值为 0。</p><p id="bfeb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着我们希望我们的假设满足</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6942437b82921a9e4c3bd7e786b8e617.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*roNZJe0uz7cMY1QU6s2eZQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 13: Hypothesis function range values</figcaption></figure><p id="7fd8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是为什么我们将假设定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/dc58be538287ef525bc959bfe8fabef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*c3aiPaF7O6anqf1gfQGChQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 14: Hypothesis function</figcaption></figure><p id="54e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中<em class="me"> g </em>在这种情况下将是 Sigmoid 函数，因为该函数的范围值在(0，1)之间。</p><p id="be20" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的目标是优化成本函数，所以我们需要找到 min<strong class="kw iu"><em class="me">【J(θ)</em></strong>。但是 Sigmoid 函数是一个“非凸”函数(“<em class="me"> Image 15 </em>”)，这意味着存在多个局部极小值。所以不能保证收敛(找到)到全局最小值。我们需要的是“凸”函数，以便梯度下降算法能够找到全局最小值(最小化 J(θ))。为此我们使用<strong class="kw iu"> <em class="me"> log </em> </strong>函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/13ec77d0958d10059d407127c96e2b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pa8FLQcPkVd-jiIwgr0wVw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 15: Convex vs Non-convex function. source: <a class="ae nc" href="https://www.researchgate.net/publication/226717592_Design_for_Optimizability_Traffic_Management_of_a_Future_Internet" rel="noopener ugc nofollow" target="_blank">researchgate.com</a></figcaption></figure><p id="ae28" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这就是为什么我们对神经网络使用以下成本函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7d81235d6a1f314c4d5dd0c90399cc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*a3GRXNHx2P7Biwp0-LGxcw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 16: Neural Network cost function. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="1ce1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在标记值<em class="me"> y </em>等于<strong class="kw iu"> <em class="me"> 1 </em> </strong>的情况下，假设为<strong class="kw iu"><em class="me">-log(h(x))</em></strong><em class="me"/>或<em class="me"/><strong class="kw iu"><em class="me">-log(1-h(x))</em></strong><em class="me"/>否则。</p><p id="cf63" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们看函数图，直觉是非常简单的。先来看一下<strong class="kw iu"> <em class="me"> y=1 </em> </strong>的情况。那么<strong class="kw iu"> <em class="me"> -log(h(x)) </em> </strong>就会像下图这样。我们只对(0，1) x 轴间隔感兴趣，因为假设只能取该范围内的值(<em class="me">“图像 13”</em>)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/b9e2a5936e5bd0d337497e732a6e782f.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*xbuJit2QG6g0Vc_twVUhZg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 17: Cost function -log(h(x)) . source: <a class="ae nc" href="https://www.desmos.com/calculator" rel="noopener ugc nofollow" target="_blank">desmos.com</a></figcaption></figure><p id="bf8f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从图中我们可以看到，如果<strong class="kw iu"><em class="me">y = 1</em></strong><strong class="kw iu"><em class="me">【x】</em></strong>趋近于<strong class="kw iu"> <em class="me"> 1 </em> </strong> ( <em class="me"> x 轴</em>)成本趋近于<strong class="kw iu"><em class="me">0</em></strong><em class="me"/>(<em class="me">h(x)-y</em>就会是<em class="me">否则如果<strong class="kw iu"> <em class="me"> h(x) </em> </strong>趋近于<strong class="kw iu"> <em class="me"> 0 </em> </strong>代价函数趋于无穷大(非常大的代价)。</em></p><p id="f0f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<strong class="kw iu"> <em class="me"> y=0 </em> </strong>的另一种情况下，代价函数为<strong class="kw iu"> <em class="me"> -log(1-h(x)) </em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/66d059ab9dc0b1a9591743bcb67c0fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*hCWTyN7DBOqaH8YsqGXI7g.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 18: -log(1-h) cost function. source: <a class="ae nc" href="https://www.desmos.com/calculator" rel="noopener ugc nofollow" target="_blank">desmos.com</a></figcaption></figure><p id="711d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从图中我们可以看到，如果<strong class="kw iu"> <em class="me"> h(x) </em> </strong>接近值<strong class="kw iu"> <em class="me"> 0 </em> </strong>，成本将接近<strong class="kw iu"> <em class="me"> 0 </em> </strong>，因为这也是正确的预测。</p><p id="8001" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于<strong class="kw iu"> <em class="me"> y </em> </strong> <em class="me"> </em>【标注值】<em class="me"> </em>始终等于<strong class="kw iu"> <em class="me"> 0 </em> </strong>或<strong class="kw iu"> <em class="me"> 1 </em> </strong>我们可以把成本函数写成一个方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9428362b5f7fd739dc86b9f58fea544b.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*g0TmtTR3YfygZXyJmRUDVQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 19: Cost function equation. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="c694" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们用总和来写成本函数，我们将得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/eef2f4e0fa275011ba3aa74fdff47ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*Rit91N03HC8izOJAm4w9RA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 20: Cost function in case of one output node. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="55ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">而且这是针对神经网络输出层只有一个节点的情况。如果我们将此推广到多个输出节点(多类分类),我们会得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/137756cd87cdbd2238d423a5a5ddeb31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1eJWUAn4t3blgxWScfyPZQ.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 21: Generalized Cost function. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="4cf4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">方程的右边部分表示成本函数“正则化”。这种正则化通过降低θ的幅度/值来防止数据“过拟合”。</p><h1 id="896c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">正向传播计算</h1><p id="05c1" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">这个前向传播的过程，其实就是根据给定的输入，得到神经网络输出值。该算法用于计算成本值。它所做的与第 2 节“模型表示数学”中描述的数学过程相同。最终我们得到假设值<em class="me">“图 7”</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4dceb4e4061dc1bee4a03a151121e0af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*iQJVglZqVZ6A2er1tNRf0w.png"/></div></figure><p id="e543" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们得到了<em class="me"> h(x) </em>值(假设)之后，我们使用成本函数方程(<em class="me">【图片 21】</em>)来计算给定输入集的成本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/fb872154eac6a23d055dcb680ae248f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*nUeXvkKLKXt2iLiFg5fqOA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 22: Calculate Forward propagation</figcaption></figure><p id="96d6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这里，我们可以注意到前向传播是如何工作的，以及神经网络<strong class="kw iu">如何生成预测</strong>。</p><h1 id="b950" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">反向传播算法</h1><p id="ed47" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们想要做的是使用<strong class="kw iu"><em class="me"/></strong>(权重)的最优值集合来最小化成本函数<strong class="kw iu"><em class="me">【J(θ)</em></strong>。反向传播是我们为了<strong class="kw iu">计算<em class="me"> J(θ) </em> </strong>的偏导数而使用的一种方法。</p><p id="f3e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该偏导数值然后被用于梯度下降算法(<em class="me">“图像 23”</em>)中，用于计算最小化成本函数<strong class="kw iu"><em class="me">【J(θ)</em></strong>的神经网络的<strong class="kw iu"> <em class="me"> θ </em> </strong>值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/952476d2a63b4fe38dd0f43248f58530.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*alEUFMP3i0vNLN5ubGwDDg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 23: General form of gradient descent. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="350b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">反向传播算法有 5 个步骤:</p><ol class=""><li id="3a21" class="lq lr it kw b kx ky la lb ld ls lh lt ll lu lp lv lw lx ly bi translated">设置<strong class="kw iu"><em class="me">(1)= X</em></strong><em class="me">；</em>训练示例</li><li id="acfb" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">执行前向传播并为其他层<strong class="kw iu"> <em class="me"> (l = 2…L) </em> </strong>计算<strong class="kw iu"> <em class="me"> a(l) </em> </strong></li><li id="b06d" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">使用<strong class="kw iu"> <em class="me"> y </em> </strong>计算最后一层的 delta 值<strong class="kw iu"> <em class="me"> δ(L) = h(x) — y </em> </strong></li><li id="5d0f" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">反向计算每个层的<strong class="kw iu"><em class="me">【l】</em></strong>值(在“反向传播背后的数学”一节中描述)</li><li id="c654" class="lq lr it kw b kx lz la ma ld mb lh mc ll md lp lv lw lx ly bi translated">计算每一层的导数值<strong class="kw iu"><em class="me">δ(l)=(a(l))^t∘δ(l+1)</em></strong>，代表成本<strong class="kw iu"> <em class="me"> J(θ) </em> </strong>相对于<strong class="kw iu"> <em class="me"> θ(l) </em> </strong>的导数</li></ol><blockquote class="pj"><p id="d542" class="pk pl it bd pm pn po pp pq pr ps lp dk translated">反向传播是关于确定改变权重如何影响神经网络中的总成本。</p></blockquote><p id="71ec" class="pw-post-body-paragraph ku kv it kw b kx pt ju kz la pu jx lc ld pv lf lg lh pw lj lk ll px ln lo lp im bi translated">它所做的是在神经网络中向后传播“错误”。在返回的途中，它会发现每个重量在总“误差”中所占的比重。对整体“误差”贡献更大的权重将具有更大的导数值，这意味着它们将改变更多(当计算梯度下降时)。</p><p id="11a9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们已经知道了反向传播算法在做什么，我们可以更深入地研究背后的概念和数学。</p><h2 id="2035" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">为什么是衍生品？</h2><p id="f237" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">一个函数(在我们的例子中是<strong class="kw iu"> <em class="me"> J(θ) </em> </strong>)对每个变量(在我们的例子中是权重<strong class="kw iu"> <em class="me"> θ </em> </strong>)的导数告诉我们该函数对那个变量或<strong class="kw iu">的<strong class="kw iu">敏感度，改变变量如何影响函数值</strong>。</strong></p><p id="7f10" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们看一个简单的神经网络例子</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/d2b258effaa6f2d2442c93725409861d.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*xILhQuB9c7cByW6odMaieQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 24: Simple Neural Network</figcaption></figure><p id="27e9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有两个输入节点<em class="me"> x </em>和<em class="me"> y </em>。输出函数是计算乘积<strong class="kw iu"> <em class="me"> x </em> </strong>和<strong class="kw iu"> <em class="me"> y </em> </strong>。我们现在可以计算两个节点的偏导数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/6969f670b162633e7634045072c06e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*XIQYcgWQYLbIvgu6Rn-JrA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 25: Derivatives to respect to y and x of f(x,y) = xy function</figcaption></figure><p id="2296" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对<strong class="kw iu"> <em class="me"> x </em> </strong>的偏导数是说如果<strong class="kw iu"> <em class="me"> x </em> </strong>的值增加了某个值<strong class="kw iu"><em class="me"/></strong>ϵ那么它会增加<strong class="kw iu"> <em class="me"> 7ϵ </em> </strong>的函数(乘积<strong class="kw iu"> <em class="me"> xy </em> </strong>)而对<strong class="kw iu"> <em class="me"> y </em> </strong>的偏导数是说如果</p><p id="f23c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如我们所定义的，反向传播算法计算成本函数相对于每个<strong class="kw iu"> <em class="me"> θ </em> </strong>权重参数的导数。通过这样做，我们确定成本函数<strong class="kw iu"><em class="me">【J(θ)</em></strong>对这些<strong class="kw iu"> <em class="me"> θ </em> </strong>权重参数中的每一个有多敏感。它还帮助我们确定在计算梯度下降时，我们应该改变每个<strong class="kw iu"> <em class="me"> θ </em> </strong>权重参数多少。所以最后我们得到了最符合我们数据的模型。</p><h2 id="936c" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">反向传播背后的数学</h2><p id="f396" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们将通过使用下面的神经网络模型作为起点来推导方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/51370070235924d38861b5b3ec0a7e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Di4V69e4gC16ooF6PZPt-A.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 26: Neural Network</figcaption></figure><p id="df9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这个模型中，我们得到了 3 个输出节点(<strong class="kw iu"> <em class="me"> K </em> </strong>)和 2 个隐藏层。如前所述，神经网络的成本函数为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/1b8d02ffc6be595fd79beb58c016b685.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wYvrfMFe4W0OVlI0JsFw-A.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 27: Generalized Cost function. source: <a class="ae nc" href="https://www.coursera.org/learn/machine-learning/home" rel="noopener ugc nofollow" target="_blank">coursera.org</a></figcaption></figure><p id="778b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们需要的是计算<strong class="kw iu"> <em class="me"> J(θ) </em> </strong>相对于每个<strong class="kw iu"> <em class="me"> θ </em> </strong>参数的偏导数。我们将省略总结，因为我们使用的是矢量化实现(矩阵乘法)。此外，我们可以省去正则化(上面等式的右边部分)，我们将在最后单独计算它。因为是加法，所以导数可以独立计算。</p><blockquote class="nl nm nn"><p id="e283" class="ku kv me kw b kx ky ju kz la lb jx lc no le lf lg np li lj lk nq lm ln lo lp im bi translated">注意:将使用矢量化实现，因此我们会一次性计算所有训练示例。</p></blockquote><p id="22ba" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们从定义我们将使用的衍生规则开始。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/d16dea7e8b7496e440447a89bcca54e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*TUQ0cHW0EDfnncj3L7w4QQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 28: Derivative Rules</figcaption></figure><p id="985a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们为我们的神经网络模型定义基本方程，其中<strong class="kw iu"> <em class="me"> l </em> </strong>是层符号，而<strong class="kw iu"> <em class="me"> L </em> </strong> <em class="me"> </em>是最后一层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/9d124754be822f00524183b6562552ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*dtFTXfpr5PRfPplWCYjkyg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 29: Initial Neural Network model equations</figcaption></figure><p id="0458" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们的例子中<strong class="kw iu"> <em class="me"> L </em> </strong>的值为 4，因为我们的模型中有 4 层。让我们从计算第三层和第四层之间权重的偏导数开始。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/2126b3d085027f0c1a1dd1475c0369f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*SX5R1SN_Rv-b2Zz3U_Aqcw.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 30: Derivative of θ parameters between 3rd and 4th layer</figcaption></figure><p id="7aa8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="me">步骤(6) — Sigmoid 导数<br/> </em> </strong>为了解释<strong class="kw iu"> <em class="me">步骤(6) </em> </strong>我们需要计算 Sigmoid 函数的偏导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/f45cf03824d7def36be4bb86c24a440d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*_6GsEGtg7jPUGJcoC1s5UA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 31: Derivative of Sigmoid function</figcaption></figure><p id="3b43" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在我们得到的最后一层的情况下，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/8cc854b2030d29aaa6631ccb12fbc1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*lO4TTh-6dMHgW7EZkGO7Cg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 32: Output layer equation</figcaption></figure><p id="633a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/62a23a7e54d79387c73f90da228a56c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*bhszZX-lhmJSs7Ca32NPwA.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 33: Output layer equation</figcaption></figure><p id="52d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="me">步骤(11) —去掉概括(σ)<br/></em></strong>也在最后<strong class="kw iu"> <em class="me">步骤(11) </em> </strong>需要注意的是，我们需要将<strong class="kw iu"><em class="me"/></strong>δ乘以<strong class="kw iu"> <em class="me"> a </em> </strong>转置才能去掉概括(训练示例 1…m)。<br/><strong class="kw iu"><em class="me">【δ】</em></strong><em class="me">——</em><strong class="kw iu"><em class="me"/></strong>带维数的矩阵<br/><strong class="kw iu"><em class="me">【number _ of _ training _ examples，output _ layer _ size】</em></strong>所以这也意味着我们将摆脱第二次汇总(1…K 为输出节点数)。<br/> <strong class="kw iu"> <em class="me"> a </em> </strong> —带维度的矩阵<br/> <strong class="kw iu"> <em class="me">【隐藏 _ 层 _ 大小，数量 _ 训练 _ 示例】</em> </strong></p><p id="0bf3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们继续对第二层和第三层之间的<strong class="kw iu"> <em class="me"> θ </em> </strong>参数进行下一次求导。对于这个推导我们可以从<strong class="kw iu"> <em class="me">【步骤 9】</em></strong>(<em class="me">【图像 30】</em>)开始。由于<strong class="kw iu"> <em class="me"> θ(2) </em> </strong>在<strong class="kw iu"><em class="me"/></strong>函数内，我们需要在计算导数时应用<em class="me">“链式法则”</em>(来自“图 28”上的导数规则的步骤(6))。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/0e6438f39182b87dfbdda2148ff126f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*rcW-rg2uM9zEu8xPDX5bQg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 34: Derivative of θ parameters between 2nd and 3rd layer</figcaption></figure><p id="8673" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在我们得到了第二层和第三层之间的<strong class="kw iu"> <em class="me"> θ </em> </strong>参数的导数。我们剩下要做的是计算输入层和第二层之间的<strong class="kw iu"> <em class="me"> θ </em> </strong>参数的导数。通过这样做，我们将看到相同的过程(方程)将被重复，因此我们可以导出一般的<strong class="kw iu"> <em class="me"> δ </em> </strong>和导数方程。我们再从<strong class="kw iu"> <em class="me">继续第三步</em> </strong> ( <em class="me">【图片 34】</em>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/a3c0ea0a8ca79f19553aedab47a8e8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*0hEM378FPy4jVKpxpKWxfQ.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 35: Derivative of θ parameters between input and 2nd layer</figcaption></figure><p id="6f29" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从上式我们可以推导出<strong class="kw iu"> <em class="me"> δ </em> </strong>参数以及相对于<strong class="kw iu"> <em class="me"> θ </em> </strong>参数的导数的方程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/dc545b5e02afbe9786f0b20e1232ebad.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*0xJnRyTGbjJs29XoGvFKbg.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 36: Recursive δ equation</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/9e159de0765fa37cb3609fa1e6a8b735.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*E2nIxtSWKuf1WeDqTCwt_g.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 37: Derivative of J (cost) with respect to θ in layer l equation</figcaption></figure><p id="cc91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后我们得到三个矩阵(与<strong class="kw iu"> <em class="me"> θ </em> </strong>权重矩阵相同)与<strong class="kw iu"><em class="me"/></strong>权重矩阵具有相同的维数，并计算出每个<strong class="kw iu"><em class="me"/></strong>参数的导数。</p><p id="cde3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> <em class="me">添加正则化</em> </strong> <br/>如前所述，正则化是防止模型过度拟合数据所必需的。我们已经为我们的成本函数定义了正则化，这是在“图像 21”上定义的等式的右边部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/b30004dcce09163cd539d529091c0b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*NktNYziYp8ssf47sRF3J_g.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 38: Regularization equation for Cost function</figcaption></figure><p id="a10c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了增加梯度的正则化(偏导数),我们需要计算上面正则化的偏导数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/43376797c6fd63f7dcf0ce24e7b87e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*jLhUxkLLcOfTONumLxV1Ig.png"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 39: Regularization equation for gradient (partial derivative)</figcaption></figure><p id="92fc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这意味着只需将每一层的所有θ值之和与相对于<strong class="kw iu"> <em class="me"> θ </em> </strong>的偏导数相加。</p><h2 id="0137" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">代码实现</h2><p id="566f" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们现在可以在代码中实现所有方程，我们将在其中计算成本和导数(使用反向传播),以便我们稍后可以在梯度下降算法中使用它们来优化我们模型的<strong class="kw iu"> <em class="me"> θ </em> </strong>参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qn qo l"/></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">Image 38: Code implementation of Neural Network Cost function and Backpropagation algorithm</figcaption></figure><h1 id="7d61" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="5cca" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">希望这是清晰易懂的。如果您认为某些部分需要更好的解释，请随时添加评论或建议。如有任何问题，请随时联系我。</p><p id="0fef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">希望你喜欢它！</p><h2 id="5e15" class="nv mg it bd mh nw nx dn ml ny nz dp mp ld oa ob mr lh oc od mt ll oe of mv og bi translated">有用的链接</h2><div class="qp qq gp gr qr qs"><a href="https://www.mathsisfun.com/calculus/derivatives-introduction.html" rel="noopener  ugc nofollow" target="_blank"><div class="qt ab fo"><div class="qu ab qv cl cj qw"><h2 class="bd iu gy z fp qx fr fs qy fu fw is bi translated">衍生品简介</h2><div class="qz l"><h3 class="bd b gy z fp qx fr fs qy fu fw dk translated">用简单的语言解释数学，加上拼图，游戏，测验，工作表和一个论坛。对于 K-12 的孩子，老师和…</h3></div><div class="ra l"><p class="bd b dl z fp qx fr fs qy fu fw dk translated">www.mathsisfun.com</p></div></div><div class="rb l"><div class="rc l rd re rf rb rg ks qs"/></div></div></a></div></div></div>    
</body>
</html>