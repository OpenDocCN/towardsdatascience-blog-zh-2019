<html>
<head>
<title>Patterns in Self-Supervised Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自我监督学习的模式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-supervised-learning-78bdd989c88b?source=collection_archive---------5-----------------------#2019-05-25">https://towardsdatascience.com/self-supervised-learning-78bdd989c88b?source=collection_archive---------5-----------------------#2019-05-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/2b667ca8f221c42b1e08576b8ba964d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PGQ2-6aWADTUJb0y"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Photo by <a class="ae kc" href="https://unsplash.com/@kevinbgent?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kevin Gent</a> on <a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="4130" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为了乐趣和利益探索自我</h1><p id="fde3" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">自我监督<a class="ae kc" href="https://twitter.com/ylecun/status/1123235709802905600?lang=en" rel="noopener ugc nofollow" target="_blank">在空中</a>(与<a class="ae kc" href="https://youtu.be/VsnQf7exv5I?t=3548" rel="noopener ugc nofollow" target="_blank">会谈</a>)。解释自我学习、无监督学习、弱监督学习、半监督学习、远程学习和完全监督学习(当然还有 RL)之间的区别变得更加困难<a class="ae kc" href="https://twitter.com/ekshakhs/status/1151188702971830272" rel="noopener ugc nofollow" target="_blank">。)尽管如此，我们还是要努力。</a></p><p id="a3d9" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在上下文中，问题是将一个<strong class="ld ir">对象</strong>(一个单词、句子、图像、视频、音频……)编码成一个<em class="me">足够通用的</em>表示(数字块)，这对于解决<em class="me">多任务</em>是<em class="me">有用的</em>(保留足够的对象特征)，例如，找到一个句子的情感，将其翻译成另一种语言，在图像中定位事物，使其分辨率更高，检测正在说的文本，识别扬声器开关，等等。</p><p id="86cd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">鉴于<em class="me">图像、视频或语音的多样性，我们必须经常将<strong class="ld ir">与几个任务(甚至是一个任务)联系起来，如果我们遇到新的例子或新的任务，这些任务就会中断。从新的例子</strong></em>(标有预期输出的输入)中不断地重复学习<em class="me">是我们的首要策略(监督学习)。我们私下里(并且雄心勃勃地)希望这种令人厌倦的重复学习过程最终会消失，我们会学到这些物体的好的<em class="me">通用</em>表示。<strong class="ld ir"> <em class="me">学一次，永远重用</em> </strong>。但是，所谓的<em class="me">无监督学习</em>范式(<em class="me"> only-input-no-labels) </em>并没有带来太多(像 GANs 和 learn-to-cluster 模型这样的轻微例外)。</em></p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="26ad" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">进入<strong class="ld ir">自我监督</strong>:谢天谢地，散布在人工智能研究网络中的一种新的学习模式已经悄然出现，它有望更接近难以实现的目标。<strong class="ld ir">原理</strong>非常简单:为了给一个对象编码，你试着在它的<em class="me">部分</em>或它(自我)的不同<em class="me">视图</em>之间设置学习任务。</p><blockquote class="mm"><p id="e1e2" class="mn mo iq bd mp mq mr ms mt mu mv ly dk translated">给定对象的一部分(输入)，你能预测/生成另一部分(输出)吗？</p></blockquote><p id="c26e" class="pw-post-body-paragraph lb lc iq ld b le mw lg lh li mx lk ll lm my lo lp lq mz ls lt lu na lw lx ly ij bi translated">这个原则有几种不同的味道。</p><ul class=""><li id="0fad" class="nb nc iq ld b le lz li ma lm nd lq ne lu nf ly ng nh ni nj bi translated">例如，给定一个句子<em class="me">上下文</em>围绕一个单词，你能(学会)<strong class="ld ir">预测出<em class="me">遗漏的单词</em> </strong> (skip-grams，BERT)。</li><li id="148b" class="nb nc iq ld b le nk li nl lm nm lq nn lu no ly ng nh ni nj bi translated">或者，<strong class="ld ir">修改输入对象的<em class="me">视图</em> </strong>并预测发生了什么变化(旋转图像并预测旋转角度)。</li><li id="b02a" class="nb nc iq ld b le nk li nl lm nm lq nn lu no ly ng nh ni nj bi translated">或者，修改输入视图，确保<strong class="ld ir">输出不改变</strong>。</li></ul><p id="cbcf" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因为你只是简单地摆弄这个物体，所以这些是<strong class="ld ir"> <em class="me">免费午餐</em> </strong>任务——不需要外部<em class="me">标签</em>。</p><p id="8ebb" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">幸运的是，我们现在有了(大量)自动生成的<em class="me">输入输出示例</em>，我们又回到了游戏中。继续使用你的监督学习工具包中的每一把锤子来学习一个伟大的(通用的？)表示这些例子中的对象。</p><blockquote class="mm"><p id="e738" class="mn mo iq bd mp mq mr ms mt mu mv ly dk translated">通过尝试从自我输入中预测自我输出，您最终了解了对象的<em class="np">内在属性/语义</em>，否则将需要大量的示例来学习。</p></blockquote><figure class="nr ns nt nu nv jr gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/19ef31409ace1c335e8de9d2d186f300.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*lS-eZRo4AEthwzv4Dw3-Vw.png"/></div></figure></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="be46" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">自我监督损失现在已经成为沉默的英雄有一段时间了，跨多个领域的表征学习(如<em class="me">自动编码器</em>、<em class="me">单词嵌入器</em>、<em class="me">辅助损失、许多数据增强、… </em>)。一个非常漂亮的滑梯<a class="ae kc" href="https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf" rel="noopener ugc nofollow" target="_blank">在这里</a>。现在，有了 NLP 的 ImageNet moment(ELMo，BERT 和其他人)，我想他们自己已经成功了。监管光谱中缺失的空白，每个人(包括 AGI；)一直在等。</p><p id="42eb" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">可以理解的是，围绕着<em class="me">更新的自我监督技巧</em>，用<em class="me">更少的例子</em>获得 SoTA，以及<em class="me">混合各种监督</em>(你好<a class="ae kc" href="https://nips.cc/Conferences/2019/CallForPapers" rel="noopener ugc nofollow" target="_blank"> NeurIPS </a>！).迄今为止，自监督方法大多试图将对象的组成部分联系起来，以一部分作为输入，预测另一部分。或者，通过数据增广改变对象的<em class="me">视图</em>，预测相同的标签。</p><p id="d00b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">接下来，让我们看看这个社区在玩新锤子的时候变得多么有创造力。还有许多问题有待解决:例如，你如何<strong class="ld ir">比较多种不同的自我监督技巧</strong>——哪一个比其他人学得更好？你如何<strong class="ld ir">选择输出</strong>？例如，<a class="ae kc" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank"> UDA </a>使用内在输出分布 D 作为标签，而不是使用显式标签作为输出——确保当输入 x 的视图改变时，D 变化最小。</p><p id="1b55" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">另外，我很好奇谁声称他们是第一个做这件事的人:)</p><p id="47d2" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">更新</strong>:一个有趣的<a class="ae kc" href="https://twitter.com/ekshakhs/status/1151188702971830272" rel="noopener ugc nofollow" target="_blank"> twitter 帖子</a>讨论自我监督是否是无监督(或其他)的品牌重塑。</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><blockquote class="nw nx ny"><p id="12e9" class="lb lc me ld b le lz lg lh li ma lk ll nz mb lo lp oa mc ls lt ob md lw lx ly ij bi translated"><strong class="ld ir"> tldr </strong> : <em class="iq">自我监督学习</em>是非监督学习的一个优雅子集，在这里你可以通过暴露对象的部分或对象的不同视图之间的<strong class="ld ir">关系，从数据对象中“内在地”生成输出标签。</strong></p></blockquote></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="c35f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">关于我:</strong>我是一名独立的计算机科学研究员、工程师和演讲者，喜欢提炼复杂的技术并将其转化为可消费的产品。我在学术界、工业界和初创公司都工作过。我帮助公司理解和应对复杂、不断发展的人工智能空间，并构建基于深度学习的解决方案，以最大化投资回报。如果你喜欢这篇文章，请<em class="me">鼓掌</em>并发表你的评论。你可以关注我，在这里阅读我的其他文章<a class="ae kc" href="https://medium.com/@ekshakhs" rel="noopener">，在</a><a class="ae kc" href="https://in.linkedin.com/in/nishant-sinha-a610311" rel="noopener ugc nofollow" target="_blank"> linkedin </a>上找到我，或者直接给我发邮件。</p><p id="adfb" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir"> PS: </strong>如果你想找人‘监督’你(弱监督、完全监督、远程监督甚至共同监督)解决一些非常有趣的文字、视觉和言语问题，请联系 nishant@offnote.co 的我！</p></div></div>    
</body>
</html>