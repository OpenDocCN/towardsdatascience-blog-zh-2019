<html>
<head>
<title>Neural ODEs: breakdown of another deep learning breakthrough</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经 ODEs:另一个深度学习突破的崩溃</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795?source=collection_archive---------0-----------------------#2019-06-11">https://towardsdatascience.com/neural-odes-breakdown-of-another-deep-learning-breakthrough-3e78c7213795?source=collection_archive---------0-----------------------#2019-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/56959ff44d47ac6dda39c1d18f13cdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wGTlHp2-UQvOHO45"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Visualization of the Neural ODE learning the dynamical system</figcaption></figure><p id="3de7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">大家好！如果你正在阅读这篇文章，很可能你正在赶上人工智能世界最近的进展。我们今天要复习的题目来自 NIPS 2018，讲的是来自那里的最佳论文奖:<a class="ae ld" href="https://arxiv.org/abs/1806.07366" rel="noopener ugc nofollow" target="_blank">神经常微分方程</a> (Neural ODEs)。在这篇文章中，我将尝试给出一个简短的介绍和这篇论文的重要性，但我将强调实际应用，以及我们如何和为了什么可以在应用中应用这种需要的神经网络，如果可以的话。和往常一样，如果你想直接进入代码，你可以查看<a class="ae ld" href="https://github.com/Rachnog/Neural-ODE-Experiments" rel="noopener ugc nofollow" target="_blank">这个 GitHub 库</a>，我推荐你在 Google Colab 中启动它。</p><h1 id="a024" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">我们为什么关心颂歌？</h1><p id="d4d4" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">首先，让我们快速回顾一下什么是野兽常微分方程。它描述了依赖于一个变量(这就是为什么普通)的一些过程随时间的演变，这种随时间的变化通过导数来描述:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/fc7cdf15efe814ef8dc3ce90cf39d486.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*IcS3urb0vTUPnHdQfeY68A.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Simple ODE example</figcaption></figure><p id="9450" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">通常，我们可以讨论解这个微分方程，如果我们有一些初始条件(在这一点上过程开始)并且我们想看这个过程如何发展到一些最终状态。解函数也叫积分曲线(因为我们可以对方程积分得到解<em class="mm"> x(t) </em>)。让我们尝试使用 SymPy 软件包来解决上图中的方程:</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="3b87" class="ms lf it mo b gy mt mu l mv mw">from sympy import dsolve, Eq, symbols, Function</span><span id="2a6b" class="ms lf it mo b gy mx mu l mv mw">t = symbols('t')<br/>x = symbols('x', cls=Function)<br/>deqn1 = Eq(x(t).diff(t), 1 - x(t))<br/>sol1 = dsolve(deqn1, x(t))</span></pre><p id="1430" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">它将作为解决方案返回</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="94e6" class="ms lf it mo b gy mt mu l mv mw">Eq(x(t), C1*exp(-t) + 1)</span></pre><p id="3182" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">其中 C1 是一个常数，可以在给定一些初始条件的情况下确定。如果以适当的形式给出，常微分方程可以解析求解，但通常是数值求解。最古老和最简单的算法之一是<strong class="kh iu">欧拉法</strong>，其核心思想是使用切线逐步逼近解函数；</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/df9e2c43f050cc2b402e2913e6002dea.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/0*1Vr-pwjuORoe5pd7.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae ld" href="http://tutorial.math.lamar.edu/Classes/DE/EulersMethod.aspx" rel="noopener ugc nofollow" target="_blank">http://tutorial.math.lamar.edu/Classes/DE/EulersMethod.aspx</a></figcaption></figure><p id="af3a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">访问图片下面的链接可以得到更详细的解释，但是最后，我们得到了一个非常简单的公式，方程式</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c27bf9eb67da79a9183f86b837964cca.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Co38mOfrnEyCBScJpRyq5Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae ld" href="http://tutorial.math.lamar.edu/Classes/DE/EulersMethod.aspx" rel="noopener ugc nofollow" target="_blank">http://tutorial.math.lamar.edu/Classes/DE/EulersMethod.aspx</a></figcaption></figure><p id="9968" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在<em class="mm"> n </em>时间步长的离散网格上的解为</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/66d8d87e5b4b91d81ad0710f3020914f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*i-oCPU52PvUeYroqpxlrZQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae ld" href="http://tutorial.math.lamar.edu/Classes/DE/EulersMethod.aspx" rel="noopener ugc nofollow" target="_blank">http://tutorial.math.lamar.edu/Classes/DE/EulersMethod.aspx</a></figcaption></figure><p id="8175" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">关于 ODEs 的更多细节，尤其是如何用 Python 对它们进行编程以及它们的解决方案，我推荐你去看看<a class="ae ld" href="https://www.springer.com/gp/book/9783319781440" rel="noopener ugc nofollow" target="_blank">这本书</a>，它也有很多化学、物理和工业领域中具有这种时间演化的过程的例子，可以用 ODEs 来描述。此外，与 ML 模型相比，有关微分方程的更多直观信息，请访问<a class="ae ld" href="https://julialang.org/blog/2019/01/fluxdiffeq" rel="noopener ugc nofollow" target="_blank">该资源</a>。与此同时，看看欧拉方程，难道不会让你想起最近的深度学习架构吗？</p><h1 id="2d22" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">ResNets 是 ODEs 解？</h1><p id="6d0b" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">没错。<em class="mm"> y_{n+1} = y_n + f(t_n，y_n) </em>不过是 ResNet 中的一个剩余连接，其中某一层的输出是该层<em class="mm"> f() </em>本身的输出与该层的输入<em class="mm"> y_n </em>之和。这基本上就是神经 ODE 的主要思想:<strong class="kh iu">一个神经网络中的一串残差块，基本上就是用欧拉方法对 ODE 的一个求解！</strong>在这种情况下，系统的初始条件是“时间”<em class="mm"> 0 </em>，这表示神经网络的第一层，并且作为<em class="mm"> x(0) </em>将服务于正常输入，它可以是时间序列、图像，无论你想要什么！在“时间”<em class="mm"> t </em>的最终条件将是神经网络的期望输出:标量值、表示类的向量或任何其他东西。</p><p id="bd47" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果我们记得，这些剩余连接是欧拉方法的离散化时间步骤，这意味着<strong class="kh iu">我们可以调节神经网络</strong>的深度，只需选择离散化方案，因此，使解决方案(又名神经网络)或多或少准确，甚至使其像无限层一样！</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/f9cadd53e342a6bf8339f8cb89ef87be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*gRSMxVqVXB27jXWqOR8TbA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Difference between ResNet with a fixed number of layers and ODENet with a flexible number of layer</figcaption></figure><p id="050e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">欧拉是不是太原始的 ODE 求解方法？的确是这样，所以让我们用一些抽象概念代替 ResNet / EulerSolverNet，如 ODESolveNet ，其中 ODESolveNet 将是一个函数，它提供了一个对 ODE(低调:我们的神经网络本身)的解决方案，比 Euler 的方法具有更好的准确性。网络架构现在可能如下所示:</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="2c00" class="ms lf it mo b gy mt mu l mv mw">nn = Network(<br/>  Dense(...), # making some primary embedding<br/>  ODESolve(...), # "infinite-layer neural network"<br/>  Dense(...) # output layer<br/>)</span></pre><p id="57d9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们忘记了一件事…神经网络是一个可微分的函数，所以我们可以用基于梯度的优化例程来训练它。我们应该如何通过<em class="mm">ode solve()</em><em class="mm"/>函数<strong class="kh iu">反向传播，在我们的例子中它实际上也是一个黑盒？特别地，我们需要输入和动力学参数的损失函数的梯度。数学窍门叫做<strong class="kh iu">伴随灵敏度法</strong>。更多细节我会参考<a class="ae ld" href="https://arxiv.org/pdf/1806.07366.pdf" rel="noopener ugc nofollow" target="_blank">原论文</a>和<a class="ae ld" href="https://nbviewer.jupyter.org/github/urtrial/neural_ode/blob/master/Neural%20ODEs%20(Russian).ipynb" rel="noopener ugc nofollow" target="_blank">本教程</a>，但本质描述在下图(<em class="mm"> L </em>代表我们要优化的主损失函数):</strong></p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/b9d35f9c04981293a2800eaf5a15cd49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnfnLalKwTJDIKEVescgBA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Making “backpropagation” gradients for the ODESolve() method</figcaption></figure><p id="6df2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">简而言之，伴随系统与描述过程的原始动力系统一起，通过链式法则(这就是众所周知的反向传播的根)向后描述过程中每个点的导数状态。正是从它，我们可以通过初始状态获得导数，并且以类似的方式，通过模拟动力学的函数的参数(一个“剩余块”，或“旧”欧拉方法中的离散化步骤)。</p><p id="cb8d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如需了解更多详情，我建议您也观看该论文作者之一的演示:</p><figure class="mi mj mk ml gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><h1 id="727b" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">神经微分方程的可能应用</h1><p id="efc5" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">首先，使用它们而不是“正常结果”的优势和动机:</p><ul class=""><li id="6da6" class="nf ng it kh b ki kj km kn kq nh ku ni ky nj lc nk nl nm nn bi translated">内存效率:在反向传播时，我们不需要存储所有的参数和梯度</li><li id="cda8" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">自适应计算:我们可以用离散化方案来平衡速度和精度，此外，在训练和推理时使其不同</li><li id="bbc8" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">参数效率:相邻“层”的参数自动绑定在一起(参见<a class="ae ld" href="https://arxiv.org/pdf/1806.07366.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></li><li id="aa71" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">规范流新型可逆密度模型</li><li id="a643" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">连续时间序列模型:连续定义的动态可以自然地包含在任意时间到达的数据。</li></ul><p id="dded" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">根据这篇论文，除了用 ODENet 代替 ResNet 用于计算机视觉之外，我认为现在有点不现实的应用程序是下一个:</p><ul class=""><li id="bcf5" class="nf ng it kh b ki kj km kn kq nh ku ni ky nj lc nk nl nm nn bi translated">将复杂的常微分方程压缩到单个动态建模神经网络中</li><li id="8006" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">将它应用于缺少时间步长的时间序列</li><li id="3f16" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">可逆规范化流(超出了本博客的范围)</li></ul><p id="5225" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">对于缺点，查阅原论文，有一些。理论够了，现在来查一些实际例子。提醒一下，<a class="ae ld" href="https://github.com/Rachnog/Neural-ODE-Experiments" rel="noopener ugc nofollow" target="_blank">所有实验的代码都在这里</a>。</p><h1 id="2a1f" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">学习动力系统</h1><p id="992a" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">正如我们之前看到的，微分方程被广泛用于描述复杂的连续过程。当然，在现实生活中，我们将它们视为离散的过程，最重要的是，时间步长 t_i 上的许多观察值可能会完全丢失。让我们假设你想用神经网络来模拟这样一个系统。在经典的序列建模范例中，你如何处理这种情况？以某种方式把它扔进循环神经网络，它甚至不是为它设计的。在这一部分，我们将检查神经 ode 如何处理它们。</p><p id="cf90" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们的设置如下:</p><ol class=""><li id="a0c6" class="nf ng it kh b ki kj km kn kq nh ku ni ky nj lc nt nl nm nn bi translated">定义<strong class="kh iu">颂歌本身</strong>我们将建模为 PyTorch nn。模块()</li><li id="ba0f" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nt nl nm nn bi translated">定义一个简单的(或者不是真正的)<strong class="kh iu">神经网络，该网络将对从 h_t 到 h_{t+1}的两个后续动态步骤之间的动态</strong>进行建模，或者在动态系统的情况下，对 x_t 和 x_{t+1}进行建模。</li><li id="c0e0" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nt nl nm nn bi translated">运行<strong class="kh iu">优化过程</strong>，该过程通过 ODE 解算器反向传播，并最小化实际动态和模拟动态之间的差异。</li></ol><p id="d5c5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在所有下面的实验中，神经网络将只是下面的(它被认为足以模拟具有两个变量的简单函数):</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="44f1" class="ms lf it mo b gy mt mu l mv mw">self.net = nn.Sequential(<br/>            nn.Linear(2, 50),<br/>            nn.Tanh(),<br/>            nn.Linear(50, 2),<br/>        )</span></pre><p id="516d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">所有进一步的例子都受到了<a class="ae ld" href="https://nbviewer.jupyter.org/github/urtrial/neural_ode/" rel="noopener ugc nofollow" target="_blank">这个知识库</a>惊人解释的高度启发。在接下来的小节中，我将展示我们所模拟的动态系统在代码中是如何表现它们自己的，以及系统如何随着时间的推移而演化，并且相位图是如何被 ODENet 拟合的。</p><h2 id="e528" class="ms lf it bd lg nu nv dn lk nw nx dp lo kq ny nz ls ku oa ob lw ky oc od ma oe bi translated">简单螺旋函数</h2><p id="d4c9" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">在这个和所有未来的可视化中，虚线代表试衣模型。</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="c13d" class="ms lf it mo b gy mt mu l mv mw">true_A = torch.tensor([[-0.1, 2.0], [-2.0, -0.1]])</span><span id="961f" class="ms lf it mo b gy mx mu l mv mw">class Lambda(nn.Module):<br/>    def forward(self, t, y):<br/>        return torch.mm(y, true_A)</span></pre><div class="mi mj mk ml gt ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/70462f27fb8d40329b9d52a197b0c9b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*HSaZiAJOZEIPhWUjxvYlig.gif"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/f2a9b16eeb827121181106808362fade.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*F3ZMrbP0LO-Sf2kxn-awXQ.gif"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Phase space on the left, time-space on the right. The straight line stands for the real trajectory and dotted one — for the evolution of the learned by the Neural ODE system</figcaption></figure></div><h2 id="23db" class="ms lf it bd lg nu nv dn lk nw nx dp lo kq ny nz ls ku oa ob lw ky oc od ma oe bi translated">随机矩阵函数</h2><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="0cfb" class="ms lf it mo b gy mt mu l mv mw">true_A = torch.randn(2, 2)/2.</span></pre><div class="mi mj mk ml gt ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/32fadf61e743bcf8d51832abb08a2954.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*dKEzk3mGPPNXgaACGRNpYw.gif"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/d577fed3ba0e2421796ac7d36bc48783.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*RrsuUSUyKg2PprYBdhveVQ.gif"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Phase space on the left, time-space on the right. The straight line stands for the real trajectory and dotted one — for the evolution of the learned by the Neural ODE system</figcaption></figure></div><h2 id="e01d" class="ms lf it bd lg nu nv dn lk nw nx dp lo kq ny nz ls ku oa ob lw ky oc od ma oe bi translated">沃尔泰拉-洛特卡系统</h2><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="e994" class="ms lf it mo b gy mt mu l mv mw">a, b, c, d = 1.5, 1.0, 3.0, 1.0<br/>true_A = torch.tensor([[0., -b*c/d], [d*a/b, 0.]])</span></pre><div class="mi mj mk ml gt ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/e8ec9cadf9cc075f48274f5a72ffa092.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*E6xaxLWRMHkQZMbwcpZ_hA.gif"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/fe2764e9437deb83d316d4c6f4b9b0a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*tF54KDc1nR-FsdaNhEWUuA.gif"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Phase space on the left, time-space on the right. The straight line stands for the real trajectory and dotted one — for the evolution of the learned by the Neural ODE system</figcaption></figure></div><h2 id="ed0e" class="ms lf it bd lg nu nv dn lk nw nx dp lo kq ny nz ls ku oa ob lw ky oc od ma oe bi translated">非线性函数</h2><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="61b1" class="ms lf it mo b gy mt mu l mv mw">true_A2 = torch.tensor([[-0.1, -0.5], [0.5, -0.1]])<br/>true_B2 = torch.tensor([[0.2, 1.], [-1, 0.2]])</span><span id="c467" class="ms lf it mo b gy mx mu l mv mw">class Lambda2(nn.Module):<br/>    <br/>    def __init__(self, A, B):<br/>        super(Lambda2, self).__init__()<br/>        self.A = nn.Linear(2, 2, bias=False)<br/>        self.A.weight = nn.Parameter(A)<br/>        self.B = nn.Linear(2, 2, bias=False)<br/>        self.B.weight = nn.Parameter(B)<br/>    <br/>    def forward(self, t, y):<br/>        xTx0 = torch.sum(y * true_y0, dim=1)<br/>        dxdt = torch.sigmoid(xTx0) * self.A(y - true_y0) + torch.sigmoid(-xTx0) * self.B(y + true_y0)<br/>        return dxdt</span></pre><div class="mi mj mk ml gt ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/06e8b445cf4079cf97a85d8253f055c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*Z-kHTAeMwUg58kCYRHBaHA.gif"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/50fdc0eede2c21251ad05b2811c4eb15.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*rN0YhYscBY0yUIDyFv7AVQ.gif"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Phase space on the left, time-space on the right. The straight line stands for the real trajectory and dotted one — for the evolution of the learned by the Neural ODE system</figcaption></figure></div><p id="befb" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">正如我们所看到的，我们的单个“剩余块”不能很好地学习这个过程，所以我们可能会使它在接下来的函数中变得更复杂。</p><h2 id="7231" class="ms lf it bd lg nu nv dn lk nw nx dp lo kq ny nz ls ku oa ob lw ky oc od ma oe bi translated">神经网络功能</h2><p id="2b36" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">让我们通过具有随机初始化权重的多层感知器来制作完全参数化的函数:</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="b381" class="ms lf it mo b gy mt mu l mv mw">true_y0 = torch.tensor([[1., 1.]])<br/>t = torch.linspace(-15., 15., data_size)</span><span id="07c1" class="ms lf it mo b gy mx mu l mv mw">class Lambda3(nn.Module):<br/>  <br/>    def __init__(self):<br/>        super(Lambda3, self).__init__()<br/>        self.fc1 = nn.Linear(2, 25, bias = False)<br/>        self.fc2 = nn.Linear(25, 50, bias = False)<br/>        self.fc3 = nn.Linear(50, 10, bias = False)<br/>        self.fc4 = nn.Linear(10, 2, bias = False)<br/>        self.relu = nn.ELU(inplace=True)<br/>        <br/>    def forward(self, t, y):<br/>        x = self.relu(self.fc1(y * t))<br/>        x = self.relu(self.fc2(x))<br/>        x = self.relu(self.fc3(x))<br/>        x = self.relu(self.fc4(x))<br/>        return x</span></pre><div class="mi mj mk ml gt ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/d32011bd818fced554831d3d7f278f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*6G0kZEEhNaVh3lla7vhmCw.gif"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/6dd8ab9cbe131448298c484b1966b8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*7UnECJ02feCwRtGv58icFw.gif"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Phase space on the left, time-space on the right. The straight line stands for the real trajectory and dotted one — for the evolution of the learned by the Neural ODE system</figcaption></figure></div><p id="81ed" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里 2–50–2 网络<strong class="kh iu">因为太简单</strong>而可怕地失败，让我们增加它的深度:</p><pre class="mi mj mk ml gt mn mo mp mq aw mr bi"><span id="aca7" class="ms lf it mo b gy mt mu l mv mw">self.net = nn.Sequential(<br/>            nn.Linear(2, 150),<br/>            nn.Tanh(),<br/>            nn.Linear(150, 50),<br/>            nn.Tanh(),<br/>            nn.Linear(50, 50),<br/>            nn.Tanh(),<br/>            nn.Linear(50, 2),<br/>        )</span></pre><div class="mi mj mk ml gt ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/10561d4182fcfcb61b7ed29a14a39405.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*eYWCn44y3UbIGWtpHTZqlg.gif"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/a6cfb5b9372ddb6ddf038c0f5b6e1db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*N7lfYgNbCJYX3GpjCgp0Ww.gif"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Phase space on the left, time-space on the right. The straight line stands for the real trajectory and dotted one — for the evolution of the learned by the Neural ODE system</figcaption></figure></div><p id="11de" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在它或多或少像预期的那样工作了，不要忘记检查代码:)</p><h1 id="d71a" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">作为生成模型的神经微分方程</h1><p id="e02a" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">作者还声称，他们可以通过 VAE 框架建立一个生成时间序列模型，使用神经微分方程作为其中的一部分。它是如何工作的？</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oo"><img src="../Images/58107dc790b8ee9ed11e97fbcfc68d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIJLrB5FGFOBX294HSl1IA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Illustration from <a class="ae ld" href="https://arxiv.org/pdf/1806.07366.pdf" rel="noopener ugc nofollow" target="_blank">the original paper</a></figcaption></figure><ul class=""><li id="3e27" class="nf ng it kh b ki kj km kn kq nh ku ni ky nj lc nk nl nm nn bi translated">首先，我们用一些“标准”时间序列算法对输入序列进行编码，比如说 RNN 算法，以获得过程的主要嵌入</li><li id="1fa6" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">通过神经 ODE 运行嵌入以获得“连续”嵌入</li><li id="736c" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">以 VAE 方式从“连续”嵌入中恢复初始序列</li></ul><p id="f53c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">作为概念验证，我刚刚重新运行了来自<a class="ae ld" href="https://nbviewer.jupyter.org/github/urtrial/neural_ode/blob/master/Neural%20ODEs%20(Russian).ipynb" rel="noopener ugc nofollow" target="_blank">这个库</a>的代码，它看起来在学习螺旋轨迹方面工作得很好:</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi op"><img src="../Images/8f1a919ea2939e78127705d03861384e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*R0STWOt0McfKqZQT.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Dots are sampled noisy trajectories, Blue line is true trajectory, the orange line stands for recovered and interpolated trajectory</figcaption></figure><p id="8316" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，我决定将心跳从心电图(ECG)转换为相位图，将<em class="mm"> x(t) </em>作为时间-空间，将<em class="mm"> x`(t) </em>作为导数-空间(如本作品所示)，并尝试适应不同的 VAE 设置。这个用例可能对像<a class="ae ld" href="https://mawi.band" rel="noopener ugc nofollow" target="_blank"> Mawi Band </a>这样的可穿戴设备非常有用，在这些设备中，由于噪声或中断的信号，我们必须恢复它(实际上，我们是在深度学习的帮助下<a class="ae ld" href="https://medium.com/mawi-band/towards-ai-based-only-biosignal-analysis-pipeline-39e6e31244a6" rel="noopener">完成的，但 ECG 是一个连续信号，不是吗？).不幸的是，它并没有很好地收敛，显示出过度适应单一形式节拍的所有迹象:</a></p><div class="mi mj mk ml gt ab cb"><figure class="of ju oq oh oi oj ok paragraph-image"><img src="../Images/f8a865df69598dff237681036de15743.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*NBv2CoRPrPVNm6-ZNneYAg.png"/></figure><figure class="of ju or oh oi oj ok paragraph-image"><img src="../Images/00f09dfa86a8b30da74434449fdfda4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*fkCH0cTMw6_9v8vAH-a30Q.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk os di ot on">Phase spaces. Blue line — real trajectory, orange line — sampled and noisy trajectory, green line — auto-encoded trajectory</figcaption></figure></div><div class="ab cb"><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/72d1c4812bb7c98aae9f5730b7b431b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*JWBbBs2MKM_NSUDFPCP0Kw.png"/></figure><figure class="of ju og oh oi oj ok paragraph-image"><img src="../Images/5e0578ff88852750fffb6909754549cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*jN--OvHD4Ggi1MPBHZPemA.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk ol di om on">Time spaces. Blue line — real signal, orange line — sampled and noisy signal, green line — auto-encoded signal</figcaption></figure></div><p id="4d91" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我还尝试了另一个实验:只在每次心跳的部分学习这个自动编码器，并从中恢复整个波形(也就是说，让我们外推一个信号)。不幸的是，无论我对超参数和数据预处理做了什么，<strong class="kh iu">将这段信号向左或向右外推</strong>—<strong class="kh iu">都没有得到任何有意义的结果。或许，读者中的某个人可以帮助理解哪里出错了:(</strong></p><h1 id="d53d" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">下一步是什么？</h1><p id="cc56" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">很明显，神经 ode 被设计用来学习相对简单的过程(这就是为什么我们甚至在标题中有<em class="mm">普通的</em>，所以我们需要一个能够建模更丰富的函数族的模型。已经有两种有趣的方法了:</p><ul class=""><li id="9dbb" class="nf ng it kh b ki kj km kn kq nh ku ni ky nj lc nk nl nm nn bi translated">增强神经颂:<a class="ae ld" href="https://github.com/EmilienDupont/augmented-neural-odes" rel="noopener ugc nofollow" target="_blank">https://github.com/EmilienDupont/augmented-neural-odes</a></li><li id="e24a" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">神经跳跃随机 DEs:<a class="ae ld" href="https://www.groundai.com/project/neural-jump-stochastic-differential-equations/1" rel="noopener ugc nofollow" target="_blank">https://www . ground ai . com/project/neural-Jump-random-differential-equations/1</a></li></ul><p id="c55f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">也需要一些时间来探索它们:)</p><h1 id="6792" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="9a7b" class="pw-post-body-paragraph kf kg it kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">依我看，神经微分方程还没有准备好用于实践。这个想法本身很棒，就创新水平而言，它让我想起了杰弗里·辛顿的《胶囊网络》,但它们现在在哪里？和神经 ode 一样，它们在玩具任务上表现出了良好的结果，但在任何接近真实应用或大规模数据集的任务上都失败了。</p><p id="02b3" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我目前只能看到两个实际应用:</p><ul class=""><li id="3a21" class="nf ng it kh b ki kj km kn kq nh ku ni ky nj lc nk nl nm nn bi translated">使用<em class="mm"> ODESolve() </em>层来平衡经典神经网络中的速度/精度权衡</li><li id="0130" class="nf ng it kh b ki no km np kq nq ku nr ky ns lc nk nl nm nn bi translated">将常规 ode“压缩”到神经架构中，以将其嵌入标准数据科学管道中</li></ul><p id="3689" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">就我个人而言，我希望这个方向能有进一步的发展(我在上面展示了一些链接),让这些神经(O)DEs 能代表更丰富的函数类，我会密切关注它。</p><p id="707f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> P.S. </strong> <br/>如果你觉得这个内容有用和有观点，你可以<a class="ae ld" href="https://bitclout.com/u/alexrachnog" rel="noopener ugc nofollow" target="_blank">在 Bitclout </a>上支持我。也请在<a class="ae ld" href="https://www.facebook.com/rachnogstyle.blog" rel="noopener ugc nofollow" target="_blank">脸书</a>博客上关注我，在那里我会定期发布简短的人工智能文章或新闻，对于媒体来说太短了，在<a class="ae ld" href="http://instagram.com/rachnogstyle" rel="noopener ugc nofollow" target="_blank"> Instagram </a>上发布个人信息，在<a class="ae ld" href="https://www.linkedin.com/in/alexandr-honchar-4423b962/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上发布！如果你想在可解释的人工智能应用或其他人工智能项目上合作，请联系我。</p></div></div>    
</body>
</html>