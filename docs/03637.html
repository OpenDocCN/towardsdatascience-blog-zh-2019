<html>
<head>
<title>Sentence Embeddings. Fast, please!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">句子嵌入。请快点！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fse-2b1ffa791cf9?source=collection_archive---------3-----------------------#2019-06-10">https://towardsdatascience.com/fse-2b1ffa791cf9?source=collection_archive---------3-----------------------#2019-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="71f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">句子嵌入是现代自然语言处理应用中的一个关键因素。使用 Gensim、Cython 和 BLAS 将句子嵌入计算速度提高 38 倍。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/38f15a89a61ad1400647ccc3597f911f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JE3x6or9jqcsGNg_"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@bernardhermant?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bernard Hermant</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="432d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">本文中的 fse 代码已弃用。请确保使用</em></strong><a class="ae ky" href="https://github.com/oborchers/Fast_Sentence_Embeddings" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="lv">Github</em></strong></a><strong class="lb iu"><em class="lv">上概述的更新代码。</em>T13】</strong></p><h2 id="3c44" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">介绍</h2><p id="9f25" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">当在机器学习管道中处理文本数据时，您可能会遇到计算句子嵌入的需要。类似于常规的单词嵌入(如 Word2Vec、GloVE、Elmo、Bert 或 Fasttext)，句子嵌入将一个完整的句子嵌入到一个向量空间。实际上，一个句子嵌入可能看起来像这样:</p><p id="bc54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">《我枪杀了警长》→[0.2；0.1 ;-0.3 ;0.9 ;…]</p><p id="0713" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些句子嵌入保留了一些好的特性，因为它们继承了潜在单词嵌入的特征[1]。因此，我们可以出于不同的目的使用句子嵌入:</p><ul class=""><li id="954f" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">基于句子的嵌入计算句子的相似度矩阵。</li><li id="566b" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">用一种普通的绘图技术来画句子，比如 t-SNE。</li><li id="fc8b" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">预测句子的某些值，即情感。</li></ul><p id="d9b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在“深度平均网络”[2]中可以看到一个非常简单的句子嵌入的监督应用，其中作者在情感分析和问题回答中使用句子嵌入。事实上，当你处理文本数据时，句子嵌入是一个看似简单的基线。幸运的是，如果(预训练的)单词嵌入已经可用，它们不需要任何形式的基于梯度的优化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/8d74cf8c56171e9e87152dff0a18e40b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*10sQimplTw56FlQi"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@alvaroserrano?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Álvaro Serrano</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="33bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的目标读者是<em class="lv">技术数据科学受众</em>。我们将探索平滑逆频率(SIF)句子嵌入[1]。具体来说，我们正在通过手工制作一个函数来优化 SIF 嵌入的计算，这个函数是专门为<strong class="lb iu">尽快计算 SIF 嵌入而定制的</strong>。为此，我们首先使用 Python，然后迁移到 Cython，最后迁移到基本线性代数子程序(BLAS)。Cython 和 BLAS 是 Gensims Word2Vec 实现中使用的核心组件。我必须感谢拉迪姆·řehůřek，因为 Gensims 的实现是这篇文章的基础。对于类似的帖子，请随意阅读原始 Word2Vec 优化<a class="ae ky" href="https://rare-technologies.com/word2vec-in-python-part-two-optimizing/" rel="noopener ugc nofollow" target="_blank">博客</a>。下面的优化允许我们将 SIF 嵌入的计算速度提高 38 倍(！)。</p><h2 id="9958" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">SIF 嵌入</h2><p id="082d" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">平滑逆频率嵌入最初由[1]构思，相应的论文已在 2017 年 ICLR 上发表。原始论文的代码可从<a class="ae ky" href="https://github.com/PrincetonML/SIF" rel="noopener ugc nofollow" target="_blank"> Github </a>获得。作者为逆频率加权连续词袋模型提出了一个很好的概率动机。我们不讨论数学的技术细节，而是讨论计算 SIF 嵌入的算法的优化。如果您必须计算数百万个句子的 SIF 嵌入，您需要一个例程在一生中完成这项任务。在本帖中，我们只是在寻找<strong class="lb iu">速度</strong>。Gensim 提供了相当多的信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/28ceedebaa7ffcb9a075a7a0ae3c20d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BIyaKj1RzixzKMNN6mRhXA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Algorithm to compute the SIF embeddings. Source: [1].</figcaption></figure><p id="b3a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">辨别算法，我们可以推断大部分工作将在第 1 &amp; 2 行完成。虽然看起来很简单，但是在使用 Python 时，有足够的空间来优化第 1 行和第 2 行。任务如下:尽可能快地为 brown 语料库中的所有句子计算 SIF 嵌入。我们将依靠 Gensims Word2Vec 实现来获得单词向量，并且只使用一点预处理。</p><h2 id="9dcb" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><strong class="ak">实现</strong></h2><p id="9f09" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这个项目的全部代码可以在<a class="ae ky" href="https://github.com/oborchers/Fast_Sentence_Embeddings" rel="noopener ugc nofollow" target="_blank"> Github </a>获得。可以安装我用 pip 写的<strong class="lb iu"> <em class="lv">快速句子嵌入库</em> </strong>:</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="dd51" class="lw lx it nl b gy np nq l nr ns">pip install fse</span></pre><p id="d03a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您将需要常规的 Python 包，特别是 Numpy、Scipy、Cython 和 Gensim。<strong class="lb iu">TL；DR:如果你需要快速的句子嵌入，就用:</strong></p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="0787" class="lw lx it nl b gy np nq l nr ns">from gensim.models import Word2Vec<br/>sentences = [[“cat”, “say”, “meow”], [“dog”, “say”, “woof”]]<br/>model = Word2Vec(sentences, min_count=1)</span><span id="63d1" class="lw lx it nl b gy nt nq l nr ns">from fse.models import Sentence2Vec<br/>se = Sentence2Vec(model)<br/>sentences_emb = se.train(sentences)</span></pre><h2 id="0b64" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><strong class="ak">优化</strong></h2><p id="25f5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了优化 SIF 功能，我们首先需要一个能够正常工作的原型。让我们来看看原型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="f848" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代码应该是不言自明的。我们首先定义变量<em class="lv"> vlookup </em>，它将我们指向 gensims vocab 类。在此我们可以访问一个<em class="lv">单词索引</em>和<em class="lv">计数</em>。<em class="lv"> Vectors </em>将我们指向 gensim <em class="lv"> wv </em>类，它包含单词 Vectors 本身。然后我们迭代所有的句子。对于每个句子，我们将句子中所有单词的向量相加，同时乘以它们的 SIF 权重。最后，我们用向量除以句子的长度。不要太花哨。</p><p id="77e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要一个嵌入来度量这个函数的执行时间。让我们使用一个 100 维的 Word2Vec 嵌入，它是在通常可用的 Brown 语料库上训练的。然后我们使用语料库的前 400 个句子，并对函数计时。<strong class="lb iu"> 5.70 秒</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nv l"/></div></figure><p id="d007" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那太慢了。敏锐的眼睛已经注意到了这个缺陷:我在向量加法中使用了 for 循环。这正是在这种情况下<strong class="lb iu">不使用 python for-loops </strong>的原因。他们太慢了。为了避免这一点，让我们重写第 37–40 行来使用 numpy 例程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="7b55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是惊人的:<strong class="lb iu"> 0.041434 秒。</strong>比基准版本快<strong class="lb iu">137 倍</strong>。然而，我们仍然可以做得更好。好多了。您可能已经注意到，vectors[w]充当单词向量的查找表。如果我们<em class="lv">预先计算</em>句子的单词索引，我们可以直接使用 numpy 索引一次访问所有向量，这样要快得多。在同一步骤中，我们必须预先计算向量的 SIF 权重，以便通过索引访问它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="4eb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行时间为<strong class="lb iu"> 0.011987 秒</strong>，这些小的增加导致该函数比我们建立的基线快<strong class="lb iu">476 倍</strong>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="ef59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了进一步加速实现，我们可能实际上预先计算所有的加权向量，因为权重和单词向量本身不会改变。因此，我们将运行时间减少到<strong class="lb iu"> 0.008674 秒</strong>，比基线增加了<strong class="lb iu">658 倍</strong><strong class="lb iu"/>。<em class="lv">但是等等。有一个陷阱:</em></p><p id="6476" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过预先计算 SIF 向量，我们可能不得不计算一个相当大的矩阵，如果我们只有几个句子和大量的词汇，这不是时间有效的。它也不节省空间，因为我们必须在 RAM 中存储一个单独的加权嵌入矩阵。因此，我们正在内存和时间效率之间进行权衡。只有当句子中用于训练的有效单词数大于词汇量时，预计算加权向量才有意义。</p><p id="b148" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我会稍微破坏一下，声明这是不必要的，因为我们稍后将使用 BLAS 函数，但是现在记住它。</p><p id="c288" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经获得了相当大的加速。我们要优化的下一件事是向量的求和，我们仍然可以获得显著的加速。然而，为了更好地对结果进行基准测试，我们将语句到索引的转换从等式中去掉。因此，我们预计算了以下基准的所有句子索引:</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="c0c6" class="lw lx it nl b gy np nq l nr ns">sentences_idx = [np.asarray([int(model.wv.vocab[w].index) for w in s if w in model.wv.vocab], dtype=np.intc) for s in sentences]</span></pre><p id="cd06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你会立即意识到这在实践中是一个非常糟糕的想法，因为我们现在必须存储第二个数据集。然而，目前它很好地服务于我们的基准测试目的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="18ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<strong class="lb iu"> 0.004472 秒</strong>的情况下，新函数——使用预先计算的加权向量和句子索引——以基准实现速度的<strong class="lb iu">1276 倍</strong>计时。<strong class="lb iu">然而，由于基线没有使用预先计算的指数</strong>，这种比较有些偏离。因此，这是我们目前的新基准。</p><p id="d7b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一步是迁移到<a class="ae ky" href="https://cython.org" rel="noopener ugc nofollow" target="_blank"> Cython </a>。请注意，我们仍然在使用预先计算的指数和加权向量。我们将所有东西包装到一个漂亮的 Cython 文件中，然后编译它。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="a6f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">cdef float[:，:]命令允许我们访问预先计算的向量的内存视图。关于更详细的解释，Cython 网站提供了一个关于内存视图的简洁的教程。记得用</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="ce2f" class="lw lx it nl b gy np nq l nr ns"># cython: boundscheck=False<br/># cython: wraparound=False</span></pre><p id="d8f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在文件的开头。这两个命令移除了安全网，这可能会降低我们的速度。前者检查我们是否超出了数组的边界，而后者允许我们执行负索引(我们不打算使用)。我们现在运行的<strong class="lb iu">比之前的<em class="lv">纯 numpy 实现</em>快 2.42 倍</strong>，后者也依赖于预先计算的句子索引。因此，这种比较又是公平的。</p><p id="86a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，我们已经将所有必要的结构<em class="lv">预定义为 C 变量</em>，我们可以在纯 C 循环中使用它们，而没有任何 Pythonic 干扰。因此，我们也可以释放全局解释器锁(<a class="ae ky" href="https://realpython.com/python-gil/" rel="noopener ugc nofollow" target="_blank"> GIL </a>)，当我们考虑<strong class="lb iu"> fse </strong>的多线程实现时，这可能会在未来的某个时间点派上用场。我们基本上重写了用于对句子中所有单词求和的代码部分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="01a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们将运行时间减少到<strong class="lb iu"> 0.000805 秒</strong>，这标志着比第一次 Cython 实现又增加了<strong class="lb iu"> 2.28 倍</strong>。</p><p id="9793" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将进行<a class="ae ky" href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" rel="noopener ugc nofollow" target="_blank">爆破</a>。基本线性代数子程序描述低级线性代数例程。它们分为三级:第一级:向量运算，第二级:向量-矩阵运算，第三级:矩阵-矩阵运算。出于我们的目的，我们只需要访问 1 级。确切地说，我们需要:</p><ul class=""><li id="87c9" class="mu mv it lb b lc ld lf lg li mw lm mx lq my lu mz na nb nc bi translated">SAXPY  : (Single)常量乘以一个向量加上一个向量</li><li id="784f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu mz na nb nc bi translated">SSCAL  : (Single)用常数缩放一个向量</li></ul><p id="6993" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Gensims 核心类大量使用这些高度优化的例程来加速 Word2Vec、Doc2Vec 和 Fasttext。</p><p id="5f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在回想一下我们之前写的关于预计算 SIF 加权向量的内容。SAXPY 函数接受参数<em class="lv"> a </em>，它用在 y = a*x + y 中。因此，我们无论如何都要做权重*向量乘法。因此，预先计算 SIF 向量实际上毕竟是一个低效的想法。</p><p id="e9e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们正在放弃 Cython 内存视图，以便通过指针和内存地址直接不安全地访问 numpy 变量。</p><p id="25cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们必须在其他地方定义 sscal 和 saxpy 指针，这在代码片段中是多余的(通常在*。pxd 文件)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="8cbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从这些额外的麻烦中得到了什么？运行时间<strong class="lb iu"> 0.000267 秒</strong>，这再次标志着<strong class="lb iu">3.01 倍的增长(！)</strong>超过以前的实现。如果我们只看使用预先计算的向量和索引的函数，我们将运行时间从<strong class="lb iu"> 0.004472 </strong>减少到<strong class="lb iu"> 0.000267 秒</strong>，速度增加了<strong class="lb iu">16.76 倍</strong>。随着我们增加向量和数据的大小，这些值可能会发生显著变化。总的来说，我们将代码的速度提高了 21，400 倍，尽管由于预先计算的不同，这有点像苹果和橘子的比较。</p><h2 id="3185" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated"><strong class="ak">验证&amp;申请</strong></h2><p id="d052" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">为了测试一切是否正常，我们现在看两个任务:<br/>STS 句子相似性基准[3]和在句子水平上预测情绪[4]。你可以在相应的<a class="ae ky" href="https://github.com/oborchers/Fast_Sentence_Embeddings/blob/master/fse/Sentence%20Prediction.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>中找到脚本。让我们使用预先训练的 GoogleNews-vectors-negative300 个向量来估计句子嵌入。</p><p id="06ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预测情绪的 reddit 数据集包含四个类别:令人毛骨悚然、血腥、快乐和愤怒。经过一些预处理后，我们得到了 2460 个句子和所有四种情感类别的平衡。</p><p id="a4c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">至于时间:使用我们之前开发的第二个变体计算句子嵌入，每个循环需要大约 991 ms 19.2 ms(平均标准时间。戴夫。7 次运行，每次 1 个循环)。fse.models.Sentence2Vec 中包含的最终实现在每个循环 25.9 ms 615 s 内完成任务(平均标准时间。戴夫。7 次运行，每次 10 个循环)，一个<strong class="lb iu">38 倍的速度提升</strong>和一个公平的比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/45a9545a317a54ee4de9929c0e8702c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mezfq_jV0rV8PAzga_km7A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://github.com/oborchers/Fast_Sentence_Embeddings/blob/master/fse/Sentence%20Prediction.ipynb" rel="noopener ugc nofollow" target="_blank">Author</a>.</figcaption></figure><p id="ae50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在预测情绪方面，我们也能够很好地完成任务。注:多项逻辑回归，训练/测试分离 50%。</p><p id="3d29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们接近句子相似度基准。数据在<a class="ae ky" href="http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark" rel="noopener ugc nofollow" target="_blank">这里</a>可用。我们只看开发集。最初的论文报道了 71.7 的 spearman 相关性，但是他们使用了手套向量。根据其他基准测试，实现工作正常。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/8e70f7b3e391114c8df6344818f542d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PmJJT4YTPQh-198Esva2LQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: <a class="ae ky" href="https://github.com/oborchers/Fast_Sentence_Embeddings/blob/master/fse/Sentence%20Prediction.ipynb" rel="noopener ugc nofollow" target="_blank">Author</a>.</figcaption></figure><h2 id="32fc" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">结论</h2><p id="1108" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">句子嵌入是自然语言处理流水线的重要组成部分。这篇博文展示了如何实现平均和 SIF 加权 CBOW 嵌入，这可以作为后续任务的良好基线。由于它们的简单性和模块化，我们预计会有各种各样的应用。相应的<strong class="lb iu"> fse </strong>包可以在 pip / Github 上获得，它为数据科学家提供了一种快速计算句子嵌入的方法。</p><p id="0350" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如有疑问，欢迎联系<a class="ae ky" href="mailto:borchers@bwl.uni-mannheim.de" rel="noopener ugc nofollow" target="_blank"> me </a>。</p><h2 id="8c75" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">附加说明</h2><p id="778b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这篇文章的代码可以在<a class="ae ky" href="https://github.com/oborchers/Fast_Sentence_Embeddings" rel="noopener ugc nofollow" target="_blank"> Github </a>和 pip 上找到:</p><pre class="kj kk kl km gt nk nl nm nn aw no bi"><span id="030d" class="lw lx it nl b gy np nq l nr ns">pip install fse</span></pre><h2 id="14a1" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">文学</h2><ol class=""><li id="5ca4" class="mu mv it lb b lc mp lf mq li nz lm oa lq ob lu oc na nb nc bi translated">Arora S，Liang Y，Ma T (2017)一个简单但难以击败的句子嵌入基线。里面的糖膏剂学习。代表。(法国土伦)，1-16 岁。</li><li id="5172" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu oc na nb nc bi translated">Iyyer M，Manjunatha V，Boyd-Graber J，Daumé III H (2015)深度无序合成与文本分类的句法方法相匹敌。继续。第 53 届 Annu。见面。协会计算机。语言学家。第七国际。Jt。糖膏剂纳特。郎。过程。, 1681–1691.</li><li id="3e6f" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu oc na nb nc bi translated">埃内科·阿吉雷、丹尼尔·塞尔、莫娜·迪亚卜、伊戈·洛佩兹·加兹皮奥、露西娅·斯佩亚。Semeval-2017 任务 1:语义文本相似度多语种和跨语种聚焦评估。SemEval 2017 会议录。</li><li id="9d62" class="mu mv it lb b lc nd lf ne li nf lm ng lq nh lu oc na nb nc bi translated">杨志成、雷米·勒布雷特和卡尔·阿伯勒。"用于分析社交媒体的多模态分类."2017 年第 27 届欧洲机器学习和数据库知识发现原理与实践会议(ECML-PKDD)</li></ol><h2 id="7e44" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">放弃</h2><p id="8e40" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated"><em class="lv">所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。作者对本网站内容的任何错误或遗漏不承担任何责任或义务。本网站包含的信息按“原样”提供，不保证完整性、准确性、有用性或及时性。</em></p></div></div>    
</body>
</html>