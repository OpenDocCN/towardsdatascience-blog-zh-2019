<html>
<head>
<title>Classification: A Linear Approach (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类:线性方法(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-a-linear-approach-part-1-b080c13992dd?source=collection_archive---------10-----------------------#2019-04-18">https://towardsdatascience.com/classification-a-linear-approach-part-1-b080c13992dd?source=collection_archive---------10-----------------------#2019-04-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a036d2bd7da343a388e3e58968722202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mb6APwj7wL2G5fIHMVQ_BQ.jpeg"/></div></div></figure><p id="d32e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">通过线性方法分类</em> —这到底是什么意思？</p><p id="3579" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">分类</em> —使用决策边界将输入空间划分为一组标记区域的行为。或者，剥离术语，将事物分成相同类型的类的行为。</p><p id="d87c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="kz">线性方法— </em>使用直线对数据集进行分区。从直观上来说，这对于描述分区(“如果它落在线类的一侧，则为 A，如果它落在边类上，则为 B”)和执行切割来说都是最容易的。</p><p id="0106" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">给你一把刀，你会怎么把蛋糕切成两半？很明显，有很多方法可以进行切割，但是你上一次(故意)不从中间直接切是什么时候？这种想法是一系列模型的基础，这些模型使用这种原理来分割你的蛋糕(数据集)，并帮助你理解每个切片的特征，并帮助你将新的蛋糕分配给现有的切片，嗯，我认为这个比喻太过了。</p><p id="5029" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">体验了监督学习技术之后(参见<a class="ae la" rel="noopener" target="_blank" href="/building-blocks-of-supervised-learning-cebab32c15ac">监督学习的构建模块</a>，本文的目标是带你沿着<strong class="kd iu">线性模型</strong>在<strong class="kd iu">分类</strong>中发展的激励因素的道路走下去。你可以称之为时间线。</p><p id="01f0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当然，数学理论是至关重要的，但走出数学严谨压倒一切的学术泡沫，为什么<em class="kz">总是获胜——特别是对于那些希望在现实世界问题上取得进展的人。</em></p><p id="728b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我为什么关心这个技术？为什么我需要高质量的数据？为什么我不能用这个模型？为什么这种模式会失败？你明白了，我们开始吧。</p><p id="4467" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu"> #0 —问题定义</strong></p><p id="984a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正式介绍，一个典型的分类问题需要一组输入变量，<em class="kz"> X=(X1，X2，…) </em>和一个分类输出变量，<em class="kz"> Y </em>。同样，这个问题属于<strong class="kd iu">监督学习</strong>问题的子集。因此，单个观察值可以由一组值<em class="kz"> (x1，x2，y) </em>组成，看起来像是<em class="kz"> (1.25，-0.5，绿色)。</em></p><p id="9045" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">*打响指*</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/88f315085d2965f2a5b2b2e178479d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*vtCRlbsauGapqsNy5z4BQQ.png"/></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Figure 1 — Our First Bivariate Training Set!</figcaption></figure><p id="7fc5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">图 1 显示了我们的第一个蛋糕！生成的数据集由两个连续变量<em class="kz"> X1 </em>和<em class="kz"> X2 </em> ( <em class="kz"> p=2)、</em>和一个输出变量<em class="kz"> Y </em>组成，该变量取集合{蓝、橙、绿} <em class="kz"> (K=3)中的值。</em>由于输出可能有三种颜色，我们检查哪种线性模型可以为我们的数据集提供最佳的三向切割。</p><p id="28a0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这些数据是为了说明模型问题而模拟的，这意味着它们不是真实数据。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><p id="b6cd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">尝试#1 —线性回归</strong></p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lr"><img src="../Images/1ad1cdf7726d828c5f2836df22a4b5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwqQAQooPLBuWqKgrTfoIw.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Figure 2 — Real dataset (left), Linear Regression fitted dataset (right)</figcaption></figure><p id="1a1f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">图 2 中右边的图显示了从我们的数据集中给定<em class="kz"> (X1，X2) </em>的线性回归模型(通过最小二乘法拟合)预测的类。第一想法？垃圾。</p><p id="4d14" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">回归将我们知道是橙色的极少数观察结果分类到橙色类别中。为什么？</p><p id="6aee" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们诊断之前，我们推迟这种方法的基本原理<a class="ae la" rel="noopener" target="_blank" href="/building-blocks-of-supervised-learning-cebab32c15ac">这里</a>并且强调线性回归的一个非常友好的例子<a class="ae la" rel="noopener" target="_blank" href="/linear-regression-in-real-life-4a78d7159f16">这里</a>。</p><p id="a89a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">分类环境中的线性建模包括回归，然后进行转换以返回分类输出，从而产生<strong class="kd iu">决策边界</strong>。实际上，这个模型并没有多少让诊断变得简单的东西。唯一移动的部分是我们对回归系数的选择和我们的决策边界(我们从连续回归输出到颜色的转换)。</p><p id="d20b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">回想一下上面的第一个资源，线性回归旨在对<strong class="kd iu">回归函数</strong>建模，给出所有可用信息的<em class="kz"> Y </em>的期望值。我们在这里不证明这一点，但是对于分类问题，这在数学上等价于给定所有可用信息的情况下，特定输出<em class="kz"> Y </em>在某个类别<em class="kz"> k </em>中的概率，<em class="kz"> X </em>。这就是所谓的<strong class="kd iu">后验概率</strong>，并在第一次提到的资源中的贝叶斯上下文中进行了讨论。因此，决策边界由两个类别具有相同后验概率的区域来定义。直觉上，观察结果被分类到概率最高的区域——这似乎是合理的。</p><p id="90fd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这就让我们来看看回归系数的选择。因为这些都是经过优化的，所以必须仔细检查优化技术。我们知道，优化系数相当于最小化我们的损失函数，即残差平方和。让我们画出这个损失函数的输出，我们称之为<strong class="kd iu">误差</strong>。</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lr"><img src="../Images/aa9fe4022aa000a57474e66fa97d83d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kobWmglhKFzWWa4pjYEgrQ.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Figure 3 — Error function (left), Linear Regression fitted dataset (right)</figcaption></figure><p id="f657" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">图 3(左)显示了相对于<em class="kz"> X1 </em>绘制的每个观察值(<em class="kz"> X1、</em>T26】X2)的评估误差函数。每个观察值都有一个绿色、橙色和蓝色的十字，误差最小的十字的颜色就是为回归选择的类别。</p><p id="e7f3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于大多数观察，我们可以看到，将其分类到橙色类别是次优的。也就是说，沿着 x 轴从左到右，很少最接近 x 轴的点是橙色的。事实上，这种情况只会发生一次。</p><p id="f293" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正确的情节是为了透视；当您从左至右遍历两个图中的 x 轴时，误差函数明显由蓝色类别的<em class="kz"> X1 &lt; - </em> 0.25 最小化，由绿色类别的<em class="kz"> X1 &gt; 0.5 </em>最小化。在两者之间，蓝色和绿色阶级之间的区别并不明显。因此，该区域中 m，nkimiobservations 的输出是两者的集合。<em class="kz">等等！</em>通过查看数据集图，我们知道这正是大多数 Orange 类所在的区域。橙色类别中的观察结果几乎总是被错误地分类为蓝色或绿色类别。哎呀！</p><p id="adde" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">原来，对于 2 个以上的类(<em class="kz"> K &gt; 2) </em>线性回归挣扎到<em class="kz">见</em>所有类。这被称为<strong class="kd iu">屏蔽</strong>并导致严重的错误分类。我们显然需要一个更好的模型。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><p id="68cb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">尝试#2 —线性判别分析(LDA) </strong></p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lr"><img src="../Images/d9faf97622d47ad354a81a075f643b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4kWCrOP5wohcktyd8eKzg.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Figure 4 — Real dataset (left), LDA fitted dataset (right)</figcaption></figure><p id="7643" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">线性判别分析(LDA)是我们第一次尝试的直接改进。图 4 显示了我们的训练集上 LDA 模型的输出。我们不再展示掩蔽，错误分类的数量已经大大减少。太好了…但是实际上什么是 LDA 呢？</p><blockquote class="ls lt lu"><p id="7724" class="kb kc kz kd b ke kf kg kh ki kj kk kl lv kn ko kp lw kr ks kt lx kv kw kx ky im bi translated">区别性的特征或特性。</p></blockquote><p id="a78f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一般来说，判别分析是一种多类分类技术，它假设每一类的数据都来自一个表现出非常特殊的空间行为的家族(称为<strong class="kd iu">分布</strong>)。其统计特性(如均值和方差)是分布的显著特征，然后用于评估哪个类对于任何观察值来说是有条件地最可能的。</p><p id="0782" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">LDA 是这种技术的一个特例，它假设来自每个类的观测值来自具有跨类的<em class="kz">公共协方差矩阵</em>的个体<em class="kz">高斯分布</em>。</p><p id="d5ee" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">解释这个问题的一种方法是在一个封闭的盒子里考虑三个不同颜色的物体。虽然你不能窥视盒子，但盒子上有许多小孔，可以显示三种颜色中的一种。你的工作是识别盒子内每种颜色的边界，从而了解每个物体的大小和形状。翻译上一段，LDA 假设盒子里的每一个物体都是一个球体(或者椭球体— <em class="kz">高斯分布</em>)，大小相同(<em class="kz">公共协方差矩阵</em>)。</p><p id="7e09" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这些统计特性通常是假设的高斯分布的参数，然后将其插入下面的<strong class="kd iu">线性判别函数</strong>。</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ly"><img src="../Images/81a9491cf1deb5da09cc28a077fad742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cy55dShPatzFSalc8TbXXA.jpeg"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Equation 1 — Linear Discriminant Functions</figcaption></figure><p id="58d5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是这项技术所需的唯一数学函数。它的推导来自两个类别的<strong class="kd iu">后验概率</strong>的比较，以及随后基于最高评估的分类(也可见于<strong class="kd iu">朴素贝叶斯</strong>分类器)。</p><p id="2460" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">拟合模型的过程包括使用给定的数据集估计参数。为了评估每个类别的线性判别函数，需要计算以下估计值，</p><ol class=""><li id="607b" class="lz ma it kd b ke kf ki kj km mb kq mc ku md ky me mf mg mh bi translated">类别<strong class="kd iu">样本平均值</strong> —每个类别的平均值<em class="kz"> (X1，X2) </em>，直观地，这给出了每个类别的中心位置的指示(称为<strong class="kd iu">质心</strong>)。</li><li id="2516" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky me mf mg mh bi translated">类别<strong class="kd iu">先验概率</strong> —给定类别<em class="kz"> k </em>中的观察数量除以观察总数，即数据集中每个类别的简单比例。这是一个天真的猜测，在没有数据知识的情况下，我们有多大可能得到一个观察结果。</li><li id="2599" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky me mf mg mh bi translated">样本<strong class="kd iu">协方差矩阵</strong> —对整个样本展开程度的估计度量。这实际上是每个类别的平均分布。</li></ol><p id="a52c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">生成 LDA 预测剩下的工作就是将上述估计值插入线性判别函数，并为给定的一组输入选择最大化函数的类。</p><p id="c489" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">扩展—二次判别分析(QDA) </strong></p><p id="32ea" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们的对象隐喻中，如果我们放弃每个对象都是一个大小相等的椭球体的假设，会怎么样呢？也就是说，如果我们放松对共同协方差和高斯数据的假设会怎么样。很明显，我们的判别函数会不同，但如何不同呢？</p><p id="37ca" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">保留椭圆体物体假设，但允许尺寸差异，我们被引导到 LDA 的扩展，称为<strong class="kd iu">二次判别分析(QDA) </strong>。所得的 QDA 判别函数在<em class="kz"> X、</em>中是二次的</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mn"><img src="../Images/2df681f0278e1baadfc5eb28a5d1b24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TYcYJ8zpJW0MtpBbJsmxjA.jpeg"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Equation 2 — Quadratic Discriminant Functions</figcaption></figure><p id="9bed" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该过程与 LDA 一样，我们需要以下估计值，</p><ol class=""><li id="6464" class="lz ma it kd b ke kf ki kj km mb kq mc ku md ky me mf mg mh bi translated">类别<strong class="kd iu">样本平均值</strong> —每个类别的平均值<em class="kz"> (X1，X2) </em>，直观地，这给出了每个类别的中心位置的指示(称为<strong class="kd iu">质心</strong>)。</li><li id="c347" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky me mf mg mh bi translated">类别<strong class="kd iu">先验概率</strong> —给定类别<em class="kz"> k </em>中的观察数量除以观察总数，即数据集中每个类别的简单比例。这是一个天真的猜测，在没有数据知识的情况下，我们有多大可能得到一个观察结果。</li><li id="160a" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky me mf mg mh bi translated">类别样本<strong class="kd iu">协方差矩阵</strong> —对每个类别如何展开<em class="kz">的估计度量。</em></li></ol><p id="f7b7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，LDA 场景中只有 3 处发生了变化。事实上，LDA 可以被认为是 QDA 的一个特例，其中每个类别的协方差矩阵都是相同的。</p><p id="7471" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">图 5 显示了 LDA 和 QDA 在更复杂的数据集上的性能。</p><figure class="lc ld le lf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mo"><img src="../Images/4a5312fef0efad3cce51077797e554b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBGZq9jw9FFBY-AGbMtdkA.png"/></div></div><figcaption class="lg lh gj gh gi li lj bd b be z dk">Figure 5 — Real dataset (left), LDA fitted model (centre), QDA fitted model (right)</figcaption></figure><p id="ef81" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">两个模型之间的边界差异类似于绘制直线边界线(LDA)和曲线边界线(QDA)的差异。显然，训练集 2 中点的分布变化比我们以前看到的更大。QDA 的灵活性足以比 LDA 更有效地捕捉这种传播，并且可以通过 Orange 类上的表现来观察。</p><p id="8eb6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以我们应该一直用 QDA，对吗？嗯，也许吧。虽然 QDA 是首选，但当类别数量增加时，我们需要估计的参数数量也会增加。边际模型改进真的值得增加计算复杂度吗？对于像我们在这里看到的“小数据”问题，答案是肯定的，但是对于更大的一组类，答案就不清楚了。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><p id="2bd5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">总而言之，在第 1 部分中，我们有:</p><ul class=""><li id="5c5c" class="lz ma it kd b ke kf ki kj km mb kq mc ku md ky mp mf mg mh bi translated">发现了能够对数据集进行分组/切片的问题。</li><li id="8753" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky mp mf mg mh bi translated">为一个 3 类示例开发了一个<strong class="kd iu">线性回归</strong>分类器，该分类器会受到屏蔽。</li><li id="8ce9" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky mp mf mg mh bi translated">发现<strong class="kd iu"> LDA </strong>对于表现良好的高斯数据集是一个非常强大的工具。</li><li id="fcf1" class="lz ma it kd b ke mi ki mj km mk kq ml ku mm ky mp mf mg mh bi translated">扩展到<strong class="kd iu"> QDA </strong>中，对于表现不太好的数据集，这是一个稍微灵活但更昂贵的方法。</li></ul><p id="f847" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">感谢评论和反馈！</p></div></div>    
</body>
</html>