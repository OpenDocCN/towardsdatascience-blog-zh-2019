<html>
<head>
<title>Debating the AI Safety Debate</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能安全辩论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d?source=collection_archive---------22-----------------------#2019-07-24">https://towardsdatascience.com/debating-the-ai-safety-debate-d93e6641649d?source=collection_archive---------22-----------------------#2019-07-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/6a0468c2e8ded615baade9f833b60d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OL77-ALAjhmCsz0x91WcNg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@drewbutler" rel="noopener ugc nofollow" target="_blank">@drewbutler</a> Unsplash</figcaption></figure><div class=""/><div class=""><h2 id="f030" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">杰弗里·欧文、保罗·克里斯蒂安诺和达里奥·阿莫代伊谈人工智能安全</h2></div><p id="10b3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我进入人工智能(AI)领域的 AI 安全领域时，我发现自己既困惑又不知所措。你从哪里开始？我昨天在报道了 OpenAI 的金融发展，他们是 AI 安全方面的最权威之一。因此，我认为看一看他们的论文会很有趣。我将关注的论文名为<a class="ae jg" href="https://arxiv.org/pdf/1805.00899.pdf" rel="noopener ugc nofollow" target="_blank">人工智能安全辩论</a>，于 2018 年 10 月发表。你当然可以在 arXiv 里自己看文章，反过来批判我的文章；那将是最理想的情况。这场关于 AI 辩论的辩论当然还在继续。</p><h2 id="2ef0" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">这篇论文的作者是谁？</h2><p id="253c" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated"><strong class="la jk">杰弗里·欧文</strong>是<a class="ae jg" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>人工智能安全团队的成员。他之前一直在<a class="ae jg" href="http://research.google.com/teams/brain" rel="noopener ugc nofollow" target="_blank">谷歌大脑</a>工作。他收集了大量论文，从模拟、匹配、张量流。</p><p id="0834" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> Paul Christiano </strong>从事人工智能校准工作。他是 OpenAI 安全团队的成员。他也是人类未来研究所的副研究员。他被引用最多的论文之一是关于人工智能安全中的具体问题。</p><p id="032c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">达里奥·阿莫德伊是 OpenAI 的研究主管。此前，他是谷歌的高级研究科学家，是斯坦福大学医学院的博士后学者，在那里他致力于将质谱应用于细胞蛋白质组的网络模型以及寻找癌症生物标志物。</p><h2 id="88e0" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">零和辩论游戏</h2><p id="0b67" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">该论文建议通过在零和<em class="ms">辩论</em>游戏中的自我游戏来训练代理人。</p><blockquote class="mt mu mv"><p id="485e" class="ky kz ms la b lb lc kk ld le lf kn lg mw li lj lk mx lm ln lo my lq lr ls lt im bi translated">给定一个问题或提议的行动，两个代理人轮流在一定限度内做简短的陈述，然后一个人判断哪个代理人给出了最真实、最有用的信息</p></blockquote><p id="599b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是因为需要学习复杂的人类目标和偏好。在让人工智能保持安全方面，我们有哪些挑战？他们列举了几个例子:</p><ul class=""><li id="ebdb" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated">学会让一个智能体的行为与人类的价值观和偏好保持一致，这可能会导致不安全的行为，而且随着系统变得越来越强大，情况可能会变得更糟。</li><li id="1968" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">结盟是一个训练时的问题:很难追溯性地修正受过训练的非结盟代理的行为和激励。</li><li id="5bd3" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">对于人类来说太难完成的任务，但是人类仍然可以判断行为或答案的质量。因此，这就是<strong class="la jk">基于人类偏好的强化学习(HumRe) </strong>。</li></ul><h2 id="c747" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">将机器行为与人类行为联系起来很难</h2><p id="e83f" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">作者认为，人类有时可能无法判断答案。行为可能太难理解，或者问题本身可能有缺陷。他们要求读者想象一个既能给出答案又能指出缺陷的系统。当系统指出缺陷时，对缺陷的过程或判断可能是错误的。</p><p id="8c4e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">他们的最终目标是自然语言辩论，由人类来评判代理之间的对话。</strong></p><p id="9958" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们的论文结构如下:</p><ol class=""><li id="ef9c" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt nn nf ng nh bi translated">介绍对齐的辩论模型及其有用性</li><li id="c8a8" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">提出使用图像任务的初步实验</li><li id="c650" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">讨论辩论模式乐观和悲观的原因</li><li id="d0ad" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">展示辩论如何减少不对称或合并多个代理</li><li id="fc94" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">采用辩论和放大方法的混合模型</li><li id="56ab" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt nn nf ng nh bi translated">结束语是对未来工作的要求。</li></ol><h2 id="f0cf" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">辩论模型(TDM)</h2><p id="b471" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在展示给两个代理的 TDM 问题中，两个代理都给出答案，然后由人类决定谁赢。作者声称，在辩论游戏中，撒谎比反驳谎言更难。他们宣称<em class="ms">简短的辩论</em>是强大的。然而，还有许多更复杂的任务。他们列出了一系列可以改进模型的方向:</p><ul class=""><li id="04f5" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated"><strong class="la jk">查询可能太大:</strong>问题可能太大而无法向人类展示，或者无法期望人类理解。</li><li id="c850" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated"><strong class="la jk">答案可能太大:</strong>同样，一个问题的最佳答案可能大得惊人。例如，答案可能是一个很长的文档。</li><li id="0adf" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated"><strong class="la jk">人类的时间是昂贵的:</strong>我们可能缺乏足够的人类时间来评判每一场辩论。</li><li id="10a2" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated"><strong class="la jk">环境交互:</strong>如果我们想要一个系统采取影响环境的行动，比如操作一个机器人，期望的输出是一系列的行动。</li><li id="5d60" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated"><strong class="la jk">长期状态:</strong>每个辩论都是一个独立的游戏，但是代理可以使用过去辩论的信息来做出更好的决策</li></ul><h2 id="646d" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">TDM 实验</h2><p id="8c20" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在实验中，他们预先指定一个诚实的代理人和一个说谎的代理人。他们还训练了另一个诚实优点的例子。为这家报纸建立了一个名为 https://debate-game.openai.com 的网站。在这个页面上可以进行实验。</p><h2 id="1546" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">乐观的理由</h2><p id="4ace" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我在论文中观察到的一个有趣的句子是关于<strong class="la jk">诚实</strong> : <em class="ms">“为了保持诚实的行为，人类法官应该被指示奖励那些承认无知并成功证明无知是合理的代理人。”</em>然而，正如这篇论文所指出的，承认无知存在一个平衡问题:如果最佳策略是争论我们永远无法确定地知道任何事情，那么争论是无用的。</p><p id="7ace" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="ms">“</em><strong class="la jk"><em class="ms">没有对手会因为愚弄人类而获得奖励</em> </strong> <em class="ms">，这很好，因为(1)我们不知道这意味着什么，以及(2)我们没有明确地训练代理人进行欺骗。”</em></p><p id="127d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者认为，人类之间现存的对抗性辩论是一个有用的类比。<strong class="la jk">法律论证</strong>特别包括领域专家向没有领域知识的人类法官或陪审团解释论证的细节。</p><h2 id="b33c" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">担忧的理由</h2><p id="dd50" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">辩论作为一种人工智能对齐的方法可能会失败，他们列出了一些原因。一是:也许人类想听什么就相信什么？如果一个演员以一个人类已经相信的虚假陈述开场，那么另一个演员的任何论点会说服他们吗？</p><p id="b4a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一个问题是，人类是否会理解这些争论。他们提出了一个问题:一个真实的人是否能玩好这个游戏，以至于最佳的游戏是诚实的行为。其中一个代理能够偷偷插入一个看似合理但包含隐藏缺陷的语句，而另一个代理却没有指出这个缺陷吗？</p><p id="ea4e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人类作为足够的法官是一个经验性的问题。如果答案是否定的，我们可以问这个模型是否在诚实承认无知时失败了。作者质疑诚实是否是最好的视角:</p><ul class=""><li id="328d" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated">许多判断需要综合不同的证据，而辩论只探索一条证据。</li><li id="892d" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">复杂的辩论将依赖于法官无法理解的概念。</li><li id="d535" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated">复杂的推理可能涉及人类尚不了解的过程。</li></ul><p id="5587" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接受辩论训练的特工会失去表现吗？直接培训可能更难；广告系列是一种很好的思考方式；我们可能不想要无法解释的答案。辩论可能无法与其他方法竞争，因为辩论需要人的参与。他们建议:“… <em class="ms">训练人类法官的模型，并将其用于采样，但与纯模拟环境竞争可能仍然具有挑战性。”</em></p><p id="f3be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">安全漏洞</strong>:一个足够强大的错位人工智能可能能够通过文本通道说服人类做危险的事情</p><p id="124c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人工智能安全二分法:如果代理在整个训练过程中保持诚实，我们会更安全，但如果辩论进行得足够好，突然的大规模叛逃得到纠正，我们也会更安全。</p><p id="0024" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个足够强大的错位爱丽丝可以从内部侵入辩论环境，让游戏不再是零和游戏。</p><h2 id="da36" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">训练超人 AI 令人信服听起来很危险！</h2><p id="c174" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这是我最喜欢的标题，它有一个感叹号，所以我决定在这个标题中逐字引用它。最大化说服的训练安全吗？</p><h2 id="2b10" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">有变化的 TDM 辩论</h2><p id="956b" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">辩论模型依赖于在力量上近似匹配的代理。对称性被提出:(1)如果一个玩家移动另一个玩家；(2)比较两个游戏；(3)科米(<em class="ms"> from Go </em>)第二个玩家获得额外的积分来对抗第一个玩家的优势。</p><p id="9e6f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不完全信息博弈是一个挑战。此外，许多智能体的一个困难是:<em class="ms">“…人类必须能够判断辩论中的陈述是好是坏，这可能比判断整个辩论要困难得多。”</em></p><p id="7981" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们将辩论和扩展总结为:</p><ul class=""><li id="fc1e" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated">辩论:两个代理人轮流在对抗的环境中说服一个人类法官。</li><li id="ca8b" class="mz na jj la b lb ni le nj lh nk ll nl lp nm lt ne nf ng nh bi translated"><strong class="la jk">扩增:</strong>一个智能体在一个人身上训练，结合对智能体的递归调用。</li></ul><p id="215a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们建议，通过对提问者进行对抗性训练，让放大更接近辩论。作者声明:<em class="ms">“我们可以通过训练辩论者根据人类提供的陈述进行辩论，这相当于将演示注入 RL 中。”</em></p><h2 id="0dba" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">他们的结论</h2><p id="6eeb" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">未来的工作领域将包括:更丰富的理论模型；测试价值判断的人体实验；模拟辩论中人的方面的 ML 实验；自然语言辩论；以及辩论和其他安全方法之间的相互作用。</p><h2 id="6a1e" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">旁注机器行为</h2><p id="3ff5" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">作为最后的补充说明，我想说这些想法可能已经导致或影响了该领域的其他讨论。在麻省理工学院媒体实验室 2019 年 4 月 24 日发表的一篇名为《机器行为》的论文中，一群科学家呼吁研究:<br/><em class="ms">“…一个广泛的研究机器行为的科学研究议程，该议程纳入并扩展了计算机科学学科，并包括了来自各个科学领域的见解。”</em></p><figure class="np nq nr ns gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5c4c7da35161cb21b83e93d52ab22f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/0*5OMtXfRdymWWDPo1.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Machine behaviour lies at the intersection of the fields that design and engineer AI systems and the fields that traditionally use scientific methods to study the behaviour of biological agents.</figcaption></figure><p id="ba91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我今天浏览的论文的作者之一 Dario Amodei 在机器行为文章中提到了他的工作:<a class="ae jg" href="https://arxiv.org/abs/1606.06565" rel="noopener ugc nofollow" target="_blank">人工智能安全中的具体问题</a>。安全这个词经常在机器行为的上下文中被提及，看到诚实或说谎的行为者产生的困境是完全有意义的。</p><h2 id="c5de" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">辩论辩论</h2><p id="431e" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">老实说，我觉得很难对这场辩论进行辩论。这似乎是一个显而易见的结论，然而其中的复杂性远远超出了我的能力和理解。通过阅读这篇文章，我可以肯定的一件事是，有一些优秀的人正在研究这个主题，但他们正在以非常工程化的重点来处理这个问题。然而，他们确实说得很清楚，需要更多的人参与到这个过程中来，与此同时，他们还认为“人的时间是昂贵的”。</p><p id="eda3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">考虑到声明的上下文，这是可以理解的。特别是通过 OpenAI 的首席技术官 Greg Brockman 最近的另一个声明“OpenAI 的 DOTA 2 机器人已经训练了相当于 45000 人类年的时间。”OpenAI Five 在一场著名的视频游戏中击败了世界冠军。尽管这是很好的营销，但同样多的时间和抽象成扭曲的时间观的实践可能会给人以人工智能的错误印象。</p><p id="228b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是人类和/或机器行为之间具有挑战性的区别。从处理或计算转移到与人类的进一步比较，进一步模糊界限——有用吗？</p><p id="5768" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这场辩论当然仍未结束。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><h2 id="a2d1" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">我今天文章中的关键概念</h2><p id="f0b3" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated"><strong class="la jk">基于人类偏好的强化学习</strong>:对于人类来说太难执行的任务，但是人类仍然可以判断行为或答案的质量</p><p id="aa84" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">机器行为</strong>:位于设计和工程人工智能系统的领域和传统上使用科学方法研究生物制剂行为的领域的交叉点。</p><p id="9b63" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">辩论模型安全漏洞</strong>:一个足够强大的错位 AI 或许能够说服人类去做危险的事情。</p><p id="857e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人工智能安全二分法:如果代理在整个训练过程中保持诚实，我们会更安全，但如果辩论进行得足够好，突然的大规模叛逃得到纠正，我们也会更安全。</p></div><div class="ab cl nt nu hx nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="im in io ip iq"><p id="849c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">这是第 500 天的第 52 天</strong></p><p id="7b48" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经坚持每天写一篇文章 50 天了。这些文章在探索人工智能领域中社会科学和计算机科学的交叉方面表现得很一般。</p></div></div>    
</body>
</html>