<html>
<head>
<title>Multicollinearity in Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学中的多重共线性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf?source=collection_archive---------7-----------------------#2019-06-14">https://towardsdatascience.com/multicollinearity-in-data-science-c5f6c0fe6edf?source=collection_archive---------7-----------------------#2019-06-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b45d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">它是什么，为什么它是一个问题？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/a9d7db7a37a6c16e9b65fa3590272e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*oD_MhD7zqh3Kga0EYA5Nng.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><a class="ae kr" href="http://qlytic.com/images/large/26.png" rel="noopener ugc nofollow" target="_blank">Multicollinearity</a></figcaption></figure><p id="2967" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最近在一次关于人工智能的会议上，讨论中出现了统计学的话题。统计方法是一个很好的工具，可以量化您的测试，并检查自变量(您可以控制和改变的变量——想想图表中的 X 轴项)之间的显著影响，以及它如何影响因变量(因自变量的改变而改变的变量——图表中的 y 轴项)。</p><p id="b756" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">例如，y=2x 将意味着 X(我们的自变量)的每一个变化，y 将改变 2(因变量的变化)。当一个自变量对应一个因变量时，这就很简单了。然而，当我们的数据集中有多个变量时，问题就变得有点复杂了——这些变量被称为会影响我们的目标变量(我们试图使用线性回归来预测的变量)的特征。</p><p id="f230" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">当每个功能变得相互“依赖”时，情况就变得更加复杂了。另一种说法是，当我们改变一个自变量，并期望因变量发生变化时，我们看到另一个自变量也发生了变化。这两个独立变量现在是相互依赖的，或者说是相互共线的。添加更多彼此共线的要素，我们得到多重共线性。</p><blockquote class="lo lp lq"><p id="6fac" class="ks kt lr ku b kv kw jr kx ky kz ju la ls lc ld le lt lg lh li lu lk ll lm ln ij bi translated">当<a class="ae kr" href="https://statisticsbyjim.com/glossary/regression-analysis/" rel="noopener ugc nofollow" target="_blank">回归</a>模型中的<a class="ae kr" href="https://statisticsbyjim.com/glossary/predictor-variables/" rel="noopener ugc nofollow" target="_blank">自变量</a>相关时，多重共线性出现。</p></blockquote><p id="e5d0" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">多重共线性主要有两种类型。第一个是结构性的(自变量 x 是平方的)，这只是一个副产品，因为更多的时候你会使用现有的自变量来创建它，你将能够跟踪它。想象一下，当我们有了一个数据集，我们决定使用 log 来缩放所有要素或对它们进行归一化。这就是结构性多重共线性的一个例子。<strong class="ku ir">第二个</strong>，在我看来更危险的是，<strong class="ku ir">是数据多重共线性</strong>。这已经嵌入在你的数据框架(熊猫数据框架)的特征集中，并且很难观察到。</p><p id="333d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这会导致什么问题？它会降低我们的总体系数和 p 值(称为显著性值),并导致不可预测的方差。这将导致过度拟合，其中模型可能在已知的训练集上做得很好，但在未知的测试集上将失败。由于这会导致较高的标准误差和较低的统计显著性值，多重共线性使得难以确定某个特征对目标变量的重要性。如果显著性值较低，我们将无法拒绝空值，这将导致假设检验的第二类错误。</p><p id="67ca" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在讨论中，我被问到的下一个问题与你的问题相同。我们如何注意到它们？嗯，我发现最好的方法是互相测试每个独立的特性。如果你有有限的功能可以使用，这是很棒的，不需要太多的工作，这就是我为我的一个涉及线性基本模型的<a class="ae kr" href="https://github.com/imamun93/Maternal_Mortality_Rate/blob/master/Final_Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">项目</a>所做的。但是当你有数百个或者更多的特征时，就困难多了。在这一点上，它也成为一个维度问题(称为维度的诅咒)。此时最好是使用 PCA(主成分分析)来减少你的特征。</p><p id="3816" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">另一种检测方法是看系数得分。</p><blockquote class="lo lp lq"><p id="cb91" class="ks kt lr ku b kv kw jr kx ky kz ju la ls lc ld le lt lg lh li lu lk ll lm ln ij bi translated">如果您可以确定哪些变量受到多重共线性和相关性强度的影响，那么您就可以确定是否需要修复它了。幸运的是，有一个非常简单的测试来评估您的回归模型中的多重共线性。方差膨胀<a class="ae kr" href="https://statisticsbyjim.com/glossary/factors/" rel="noopener ugc nofollow" target="_blank">因子</a> (VIF)确定独立变量之间的相关性以及相关性的强度。</p></blockquote><p id="5daa" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">使用方差膨胀因子 VIF，我们可以确定两个独立变量是否共线。测量时，如果两个特征的 VIF 为 1，则它们彼此不共线(即这两个特征之间没有相关性)。然而，随着数字的增加，它们之间的相关性就越高。如果 VIF 返回一个大于 5 的数，那么这两个特征应该使用 PCA 减少到一个。</p><p id="f4c2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">还有另一个最终的统计工具，它对于分析要素之间的差异非常有用。ANOVA 代表方差分析。一般来说，变量之间的方差越大，它们相关的可能性就越小。但也许这将是未来讨论的主题。我将在下面链接我的讨论来源，其中也包含一个很好的例子，你可以尝试一下。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><div class="kg kh ki kj gt mc"><a href="https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/" rel="noopener  ugc nofollow" target="_blank"><div class="md ab fo"><div class="me ab mf cl cj mg"><h2 class="bd ir gy z fp mh fr fs mi fu fw ip bi translated">回归分析中的多重共线性:问题、检测和解决方案</h2><div class="mj l"><h3 class="bd b gy z fp mh fr fs mi fu fw dk translated">当回归模型中的独立变量相关时，会出现多重共线性。这种相关性是一个问题…</h3></div><div class="mk l"><p class="bd b dl z fp mh fr fs mi fu fw dk translated">statisticsbyjim.com</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq kl mc"/></div></div></a></div></div></div>    
</body>
</html>