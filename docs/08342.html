<html>
<head>
<title>The essence of eigenvalues and eigenvectors in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中特征值和特征向量的本质</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-essence-of-eigenvalues-and-eigenvectors-in-machine-learning-f28c4727f56f?source=collection_archive---------7-----------------------#2019-11-13">https://towardsdatascience.com/the-essence-of-eigenvalues-and-eigenvectors-in-machine-learning-f28c4727f56f?source=collection_archive---------7-----------------------#2019-11-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c611" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解实际应用</h2></div><p id="38a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词<strong class="kk iu"> Eigen </strong>可能最有用的翻译是德语，意思是<strong class="kk iu">特性。</strong>所以当我们谈论矩阵的特征值和特征向量时，我们谈论的是寻找矩阵的特征。</p><p id="55f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在深入研究特征向量之前，让我们先了解什么是矩阵除了是一个数字的矩形阵列，它还代表什么？</p><p id="35e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以矩阵只是应用于向量的线性变换。可以有不同类型的变换应用于向量，例如-</p><p id="ff7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看一些矩阵变换</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="0e20" class="ln lo it lj b gy lp lq l lr ls">img = cv2.imread(path_to_image,flags=cv2.IMREAD_UNCHANGED)<br/>img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)<br/>rows,cols,_ = img.shape<br/>M = np.float32([[1, -np.sin(.1), 0],[0,  np.cos(.1), 0]])<br/>out = cv2.warpAffine(img,M,(cols,rows))</span></pre><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lt"><img src="../Images/28082773fb369ae48e70ead1454d2ba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOsMw1-SqHWdOpc_4z7Zfw.png"/></div></div></figure><p id="5ecc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个矩阵 M 和图像有什么关系？它给图像中的每个矢量引入了水平剪切。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="b393" class="ln lo it lj b gy lp lq l lr ls">#Let's try this one<br/>M = cv2.getRotationMatrix2D((cols/2,rows/2),45,1)<br/>out = cv2.warpAffine(img,M,(cols,rows))<br/>print(M)<br/>=&gt; [[   0.70710678    0.70710678 -124.36235483]<br/> [  -0.70710678    0.70710678  501.76271632]]</span></pre><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lt"><img src="../Images/5fb5648f4f5d3a27b9f174159df419b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HUQHOADwNZbYyfQqNmtQDw.png"/></div></div></figure><p id="355d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么矩阵 M 对图像做了什么？所以这个线性变换 M 将图像中的每个向量旋转了 45 度。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="b791" class="ln lo it lj b gy lp lq l lr ls">M = [[  1.   0. 100.]<br/> [  0.   1. 100.]]<br/>out = cv2.warpAffine(img,M,(cols,rows))</span></pre><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lt"><img src="../Images/8cb96d544a9a622cfed9919b48217c8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-oGyqZkrD0_aHJdv38fZA.png"/></div></div></figure><p id="edac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它在水平和垂直方向上平移图像。</p><p id="b370" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">旋转没有本征矢[180 度旋转的情况除外]。对于纯剪切，水平向量是一个特征向量。向量长度改变的因子叫做特征值。</p><h1 id="0c7d" class="mb lo it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">应用程序</h1><p id="4a1a" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated"><strong class="kk iu">特征值</strong>和<strong class="kk iu">特征向量</strong>的概念在很多实际应用中使用。我将只讨论其中的几个。</p><h2 id="de62" class="ln lo it bd mc mx my dn mg mz na dp mk kr nb nc mm kv nd ne mo kz nf ng mq nh bi translated">主成分分析</h2><p id="137d" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">PCA 是一种非常流行的经典降维技术，它使用这一概念通过降低数据的维数来压缩数据，因为维数灾难一直是经典计算机视觉处理图像甚至机器学习中非常关键的问题，具有高维数的特征增加了模型容量，这反过来需要大量的数据来训练。</p><p id="6c0f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是一种使用简单的矩阵运算和统计来计算原始数据在相同或更少维度上的投影的方法。</p><p id="67a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">设数据矩阵<strong class="kk iu"> 𝐗 </strong>为𝑛×𝑝大小，其中 n 为样本数，p 为每个样本的维数。在 PCA 中，本质上我们通过<strong class="kk iu">特征值</strong>分解对角化 X 的协方差矩阵，因为协方差矩阵是对称的</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="a937" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj iu"><em class="ni">C = VLVᵀ</em></strong></span><span id="9a3f" class="ln lo it lj b gy nj lq l lr ls"><strong class="lj iu"><em class="ni">In Python-</em></strong></span><span id="f660" class="ln lo it lj b gy nj lq l lr ls">from numpy import cov<br/>from numpy.linalg import eig</span><span id="32ed" class="ln lo it lj b gy nj lq l lr ls">X = array([[-2, -2], [0, 0], [2, 2]])<br/>C = cov(X)<br/>#To do eigendecomposition of C<br/>values, vectors = eig(C)<br/>#Project data into pricipal directions<br/>P = vectors.T.dot(X.T)<br/>print(P.T)</span><span id="d57b" class="ln lo it lj b gy nj lq l lr ls">Output-<br/>[[-2.82842712  0.        ]<br/> [ 0.          0.        ]<br/> [ 2.82842712  0.        ]]</span></pre><p id="b65a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> 𝐕 </strong>是一个由<strong class="kk iu">个特征向量</strong>组成的矩阵(每一列都是一个特征向量)，而<strong class="kk iu"> 𝐋 </strong>是一个对角矩阵，其中<strong class="kk iu">个特征值</strong> 𝜆𝑖在对角线上按降序排列。</p><blockquote class="nk nl nm"><p id="f3a3" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">对称矩阵的特征向量，这里是协方差矩阵，是实的和正交的。</p></blockquote><p id="7079" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据的<strong class="kk iu">特征向量</strong>称为<em class="ni">主轴</em>或<em class="ni">主方向</em>。数据在主轴上的投影称为<em class="ni">主分量。</em></p><p id="5aad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过在比原始维度更少的主方向上投影来降低数据的维度。</p><h2 id="58fb" class="ln lo it bd mc mx my dn mg mz na dp mk kr nb nc mm kv nd ne mo kz nf ng mq nh bi translated">谱聚类</h2><p id="72ca" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">K-Means 是最流行的聚类算法，但它有几个相关的问题，如依赖于聚类初始化和特征的维数。同样，如果你的星团不是球形的，它也会面临问题，如下图所示</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="ab45" class="ln lo it lj b gy lp lq l lr ls">from sklearn.datasets import make_moons<br/>X_mn, y_mn = make_moons(150, noise=.07, random_state=21)<br/>fig, ax = plt.subplots(figsize=(9,7))<br/>ax.set_title('Data with ground truth labels ', fontsize=18, fontweight='demi')<br/>ax.scatter(X_mn[:, 0], X_mn[:, 1],c=y_mn,s=50, cmap='viridis')</span></pre><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nq"><img src="../Images/1e6b3541ef593f8531be61e9cc81578f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iGGipPaNtFZjhISoWmLxrA.png"/></div></div></figure><p id="7745" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谱聚类是使用矩阵的<strong class="kk iu">特征向量</strong>来寻找 K 个聚类的一系列方法。它可以处理这些问题，并轻松胜过其他聚类算法。</p><p id="894d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里数据以图表的形式表示。现在，聚类可以被认为是生成图割，其中两个聚类 A 和 B 之间的割(A，B)被定义为两个聚类之间的权重连接的总和。比如说-</p><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nr"><img src="../Images/e40657c4be03909225c12aeb37361002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5Z8jtMA0EWuR7HnptDhMQ.png"/></div></div></figure><p id="af3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了找到最佳聚类，我们需要最小割，最小割方法的目标是找到具有最小权重和连接的两个聚类 A 和 B。</p><p id="816d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在谱聚类中，使用从图的邻接矩阵和度矩阵计算的图拉普拉斯矩阵来近似这个最小割目标。证据见<a class="ae ns" href="https://sites.cs.ucsb.edu/~veronika/SpectralClustering.pdf" rel="noopener ugc nofollow" target="_blank">本</a></p><h2 id="3840" class="ln lo it bd mc mx my dn mg mz na dp mk kr nb nc mm kv nd ne mo kz nf ng mq nh bi translated">算法</h2><p id="4add" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">给定:一个有𝑛顶点和边权重𝑊𝑖𝑗的图，期望聚类数𝑘</p><ul class=""><li id="29fc" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">构建(标准化)图拉普拉斯𝐿 𝐺 𝑉，𝐸=𝐷𝑊</li><li id="98ee" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">求𝐿的𝑘最小特征值对应的𝑘特征向量</li><li id="0977" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">设 u 是特征向量的 n × 𝑘矩阵</li><li id="d57a" class="nt nu it kk b kl oc ko od kr oe kv of kz og ld ny nz oa ob bi translated">用𝑘-means 找出𝑘簇𝐶′让𝑥𝑖’是 U 5 的行。如果𝑥𝑖’被分配到聚类 j，则将数据点𝑥𝑖分配到𝑗'th 聚类</li></ul><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="21df" class="ln lo it lj b gy lp lq l lr ls">from sklearn.neighbors import radius_neighbors_graph<br/>from scipy.sparse import csgraph<br/>from sklearn.cluster import KMeans</span><span id="16bc" class="ln lo it lj b gy nj lq l lr ls">#Create adjacency matrix from the dataset<br/>A = radius_neighbors_graph(X_mn,0.4,mode='distance', metric='minkowski', p=2, metric_params=<strong class="lj iu">None</strong>, include_self=<strong class="lj iu">False</strong>)<br/>A = A.toarray()</span><span id="ef01" class="ln lo it lj b gy nj lq l lr ls">'''Next find out graph Laplacian matrix, which is defined as the L=D-A where A is our adjecency matrix we just saw and D is a diagonal degree matrix, every cell in the diagonal is the sum of the weights for that point'''</span><span id="8acc" class="ln lo it lj b gy nj lq l lr ls">L = csgraph.laplacian(A, normed=<strong class="lj iu">False</strong>)<br/>eigval, eigvec = np.linalg.eig(L)</span></pre><blockquote class="nk nl nm"><p id="fe12" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">现在，使用𝑘-means 找到𝑘群集𝐶让𝑥𝑖是 eigvec 的行。最后，为了将数据点分配到聚类中，如果𝑥𝑖被分配到聚类 j，则将𝑥𝑖分配到𝑗'th 聚类</p><p id="3d04" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">第二小特征向量，也称为 Fiedler 向量，用于通过寻找最佳分裂点来递归地二分图。</p></blockquote><p id="a59f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ni">参见完整示例代码</em> <a class="ae ns" href="https://github.com/ranjeetthakur/ml_codebase/blob/master/Spectral_clustering.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ni">此处</em> </a></p><p id="4d6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae ns" href="https://repository.upenn.edu/cgi/viewcontent.cgi?article=1101&amp;context=cis_papers" rel="noopener ugc nofollow" target="_blank">谱聚类的变体</a>用于计算机视觉中基于区域提议的对象检测和语义分割。</p><h2 id="2637" class="ln lo it bd mc mx my dn mg mz na dp mk kr nb nc mm kv nd ne mo kz nf ng mq nh bi translated">计算机视觉中的兴趣点检测</h2><p id="7a51" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">最后我来讨论一下 AI 下我最喜欢的领域，就是计算机视觉。在计算机视觉中，图像中的兴趣点是在其邻域中唯一的点。这些点在经典的计算机视觉中起着重要的作用，它们被用作特征。</p><p id="98aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">角点是有用的兴趣点，以及其他更复杂的图像特征，如 SIFT、SURF 和 HOG 等。我将讨论一种这样的角点检测方法。</p><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oh"><img src="../Images/14a56b68ac68b9e76c68b6e26975d84b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItXqhaXtQgBYuQ2WloTkTA.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk"><a class="ae ns" href="http://www.cs.cmu.edu/~16385/s17/Slides/6.2_Harris_Corner_Detector.pdf" rel="noopener ugc nofollow" target="_blank">http://www.cs.cmu.edu/~16385/s17/Slides/6.2_Harris_Corner_Detector.pdf</a></figcaption></figure><p id="a08f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">透过一个小窗口很容易认出角落。</p><figure class="le lf lg lh gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi om"><img src="../Images/442c8197226fe7915bad50ea69bafbf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3I5349nqhbbrM1zTr5KQmw.png"/></div></div></figure><p id="1ec9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果窗口内部有一个角，移动窗口应该会在强度 E 上产生很大的变化。</p><h2 id="d7a0" class="ln lo it bd mc mx my dn mg mz na dp mk kr nb nc mm kv nd ne mo kz nf ng mq nh bi translated">哈里斯角探测器</h2><p id="0fbd" class="pw-post-body-paragraph ki kj it kk b kl ms ju kn ko mt jx kq kr mu kt ku kv mv kx ky kz mw lb lc ld im bi translated">这个算法-</p><ul class=""><li id="9931" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">计算小区域上的图像梯度</li></ul><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="87b5" class="ln lo it lj b gy lp lq l lr ls">imggray = cv2.imread('checkerboard.png',0)<br/>i_x = cv2.Sobel(imggray,cv2.CV_64F,1,0,ksize=5)<br/>i_y = cv2.Sobel(imggray,cv2.CV_64F,0,1,ksize=5)</span></pre><ul class=""><li id="1871" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">计算协方差矩阵</li></ul><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="1bee" class="ln lo it lj b gy lp lq l lr ls"># Calculate the product of derivates in each direction<br/>i_xx, i_xy, i_yy = multiply(i_x, i_x), multiply(i_x, i_y), multiply(i_y, i_y)</span><span id="d2ad" class="ln lo it lj b gy nj lq l lr ls"># Calculate the sum of product of derivates<br/>s_xx, s_xy, s_yy = cv2.GaussianBlur(i_xx, (5,5), 0), cv2.GaussianBlur(i_xy, (5,5), 0), cv2.GaussianBlur(i_yy, (5,5), 0)</span></pre><ul class=""><li id="c7d8" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">计算<strong class="kk iu">特征向量</strong>和<strong class="kk iu">特征值。</strong></li></ul><p id="25a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Harris 描述了一种更快的近似方法<strong class="kk iu"/>——避免计算特征值，只需计算迹和行列式。结合这两个属性，我们计算出一个度量值<em class="ni">角度</em> -R</p><blockquote class="nk nl nm"><p id="5856" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">矩阵的行列式=特征值的乘积</p><p id="d934" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">矩阵的迹=特征值之和</p></blockquote><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="ff20" class="ln lo it lj b gy lp lq l lr ls"># Compute the response of the detector at each point<br/>k = .04 # Recommended value between .04 and .06<br/>det_h = multiply(s_xx, s_yy) - multiply(s_xy, s_xy)<br/>trace_h = s_xx + s_yy<br/>R = det_h - k*multiply(trace_h, trace_h)</span></pre><ul class=""><li id="3ce0" class="nt nu it kk b kl km ko kp kr nv kv nw kz nx ld ny nz oa ob bi translated">使用<strong class="kk iu">特征值</strong>上的阈值来检测角点</li></ul><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="5ba3" class="ln lo it lj b gy lp lq l lr ls">ratio = .2 # Number to tweak<br/>thresh = abs(R) &gt; ratio * abs(R).max()</span></pre><blockquote class="nk nl nm"><p id="ff65" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">如果任一个<strong class="kk iu">特征值</strong>接近 0，那么这不是一个角点，所以寻找两者都大的位置。e 在所有方向上几乎都是常数。</p><p id="94d1" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">λ2 &gt;&gt; λ1 或λ1 &gt;&gt; λ2 描绘了一条边</p><p id="19cb" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">λ1 和λ2 较大，λ1 ~ λ2 E 在各个方向都增大</p></blockquote><p id="8e3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="ni">参见完整示例代码</em> <a class="ae ns" href="https://github.com/ranjeetthakur/ml_codebase/blob/master/Harris%20%26%20Stephens%20Corner%20Detector.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ni">此处</em> </a></p><h1 id="55e1" class="mb lo it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">参考</h1><blockquote class="nk nl nm"><p id="0f88" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">归一化切割和图像分割。史和马利克，2000 年</p><p id="abde" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">一个组合的组合和边缘检测器，克里斯哈里斯和迈克斯蒂芬斯，1988 年</p><p id="134e" class="ki kj ni kk b kl km ju kn ko kp jx kq nn ks kt ku no kw kx ky np la lb lc ld im bi translated">图的代数连通性 M. Fiedler，1973</p></blockquote></div></div>    
</body>
</html>