<html>
<head>
<title>How to deploy ONNX models on NVIDIA Jetson Nano using DeepStream</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用 DeepStream 在 NVIDIA Jetson Nano 上部署 ONNX 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-deploy-onnx-models-on-nvidia-jetson-nano-using-deepstream-b2872b99a031?source=collection_archive---------4-----------------------#2019-12-05">https://towardsdatascience.com/how-to-deploy-onnx-models-on-nvidia-jetson-nano-using-deepstream-b2872b99a031?source=collection_archive---------4-----------------------#2019-12-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5444" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在 Jetson Nano 上测试 DeepStream 的多流神经网络推理性能的实验。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0637ba7ff60ae00e75e524b49a59a5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u8j1T5880DsonQJRSK_jLA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Jetson Nano. (<a class="ae kv" href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="dc63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将复杂的深度学习模型部署到小型嵌入式设备上是一项挑战。即使有针对深度学习优化的硬件，如<a class="ae kv" href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">杰特森纳米</strong> </a>和推理优化工具，如<a class="ae kv" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> TensorRT </strong> </a>，瓶颈仍然可以在 I/O 管道中出现。如果模型必须处理具有多个输入和输出流的复杂 I/O 管道，这些瓶颈可能会加剧。拥有一个能够以端到端的方式处理所有瓶颈的工具不是很好吗？</p><h1 id="af0c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">向深溪问好</h1><p id="d9a0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">原来有一个 SDK 试图缓解这个问题。<a class="ae kv" href="https://developer.nvidia.com/deepstream-sdk" rel="noopener ugc nofollow" target="_blank"> DeepStream </a>是一款 SDK，针对 NVIDIA Jetson 和 T4 平台进行了优化，以提供无缝的端到端服务，将原始流数据转换为可操作的见解。它构建在<a class="ae kv" href="https://gstreamer.freedesktop.org/" rel="noopener ugc nofollow" target="_blank"> GStreamer </a>框架之上。在这里，“原始流数据”通常是连续的(和多个)视频流，“可操作的见解”是深度学习或其他分析算法的最终输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/bdd9a5c81148122bc8a68ff3035689b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SL0-s417iMMXcgMOxvRXCQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The DeepStream pipeline. (<a class="ae kv" href="https://developer.nvidia.com/deepstream-sdk" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="4a3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DeepStream SDK 使用其定制的<a class="ae kv" href="https://developer.nvidia.com/deepstream-sdk#plugins" rel="noopener ugc nofollow" target="_blank"> GStreamer 插件</a>来提供各种功能。值得注意的是，它有基于推理和物体跟踪的插件。下图列出了他们插件的功能。关于他们插件的详尽技术指南，你可以参考他们的<a class="ae kv" href="https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html" rel="noopener ugc nofollow" target="_blank">插件手册</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/a75c29442555447fd1e184d29d6378f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9uLhdeZVagy0U5QSKG6gQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Plugins available in DeepStream. (<a class="ae kv" href="https://developer.nvidia.com/deepstream-sdk#plugins" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="5a16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我特别喜欢 DeepStream 的一个特性是，它以流水线方式优化了整个 I/O 处理。我们还可以堆叠多个深度学习算法，异步处理信息。这允许您增加吞吐量，而没有手动创建和管理多处理系统设计的麻烦。</p><p id="5691" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最好的部分是，对于一些支持的应用程序，如对象检测，跟踪，分类或语义分割，DeepStream 易于使用！对于这样的应用，只要你有一个兼容格式的深度学习模型，只需要在一些文本文件中设置几个参数，就可以轻松启动 DeepStream。</p><p id="7a92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇博客中，我们将在 DeepStream 上设计并运行一个实验来测试它的功能，并看看它是否易于在 Jetson Nano 上使用。</p><h1 id="19c1" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">实验</h1><p id="f8c5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了测试 DeepStream 的功能，让我们在<a class="ae kv" href="https://developer.nvidia.com/embedded/jetson-nano-developer-kit" rel="noopener ugc nofollow" target="_blank"> Jetson Nano </a>上部署一个预训练的对象检测算法。出于几个原因，这是一个理想的实验:</p><ul class=""><li id="76e3" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">DeepStream 针对英伟达 T4 和杰特森平台上的推理进行了优化。</li><li id="0405" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">DeepStream 有一个使用 TensorRT 进行推理的插件，支持对象检测。此外，它会自动将 ONNX 格式的模型转换为优化的 TensorRT 引擎。</li><li id="b44d" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">它有支持多种流输入的插件。它也有插件来保存多种格式的输出。</li></ul><p id="520a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">ONNX <a class="ae kv" href="https://github.com/onnx/models" rel="noopener ugc nofollow" target="_blank">模型动物园</a>有一堆预先训练好的物体探测模型。我从动物园选择了<strong class="ky ir">微型 YOLO v2 </strong> <a class="ae kv" href="https://github.com/onnx/models/tree/master/vision/object_detection_segmentation/tiny_yolov2" rel="noopener ugc nofollow" target="_blank">型号</a>，因为它很容易与 DeepStream 兼容，而且足够轻，可以在杰特森纳米上快速运行。</p><blockquote class="nf ng nh"><p id="582d" class="kw kx ni ky b kz la jr lb lc ld ju le nj lg lh li nk lk ll lm nl lo lp lq lr ij bi translated"><strong class="ky ir">注:</strong>我确实试过使用来自<a class="ae kv" href="https://github.com/onnx/models" rel="noopener ugc nofollow" target="_blank">动物园</a>的 SSD 和 YOLO v3 型号。但是有一些兼容性问题。我的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX/blob/master/FAQ.md" rel="noopener ugc nofollow" target="_blank"> GitHub 知识库</a>中讨论了这些问题，以及验证和处理此类情况的技巧。我最终使用微型 YOLO v2，因为它很容易兼容，没有任何额外的努力。</p></blockquote><p id="6bae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们要研究的特性如下:</p><ol class=""><li id="c9b8" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr nm mx my mz bi translated"><strong class="ky ir">多个输入流:</strong>运行 DeepStream，同时对多个视频流进行推理。具体来说，我们将尝试使用多达 4 个视频流。</li><li id="ffaf" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr nm mx my mz bi translated"><strong class="ky ir">多输出接收器:</strong>在屏幕上显示结果，并使用 RTSP 进行流式传输。该流将被连接到网络的另一设备访问。</li></ol><p id="fa04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实验将评估性能(每秒帧数，FPS)和易用性。接下来的几节将指导您如何在 Jetson Nano 上设置 DeepStream 来运行这个实验。这个实验使用的所有代码都可以在我的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>上获得。如果你只是好奇结果如何，请随意跳到<strong class="ky ir">结果</strong>部分。</p><h1 id="73da" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">入门指南</h1><p id="5922" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在本节中，我们将浏览一些说明，为我们的实验做好准备。</p><h2 id="a9df" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 1 部分:设置您的 Jetson Nano</h2><p id="e5ed" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">按照<a class="ae kv" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html" rel="noopener ugc nofollow" target="_blank">Jetson Nano 开发工具包</a>上的说明设置并启动您的 Jetson Nano。如果你在设置上遇到一些问题，我强烈推荐以下<a class="ae kv" href="https://www.hackster.io/news/getting-started-with-the-nvidia-jetson-nano-developer-kit-43aa7c298797" rel="noopener ugc nofollow" target="_blank">这些</a> <a class="ae kv" href="https://medium.com/@ageitgey/build-a-hardware-based-face-recognition-system-for-150-with-the-nvidia-jetson-nano-and-python-a25cb8c891fd" rel="noopener">资源</a>。</p><p id="e07a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想强调一些可能会帮你省去一些麻烦的要点:</p><ul class=""><li id="bad0" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">建议至少用 32GB 的 MicroSD 卡(我用的是 64GB)。</li><li id="a5ab" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">您需要有线以太网连接。如果您需要将 Jetson Nano 连接到 WiFi，您需要使用 Edimax EW-7811Un 等加密狗。</li><li id="7fd4" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">你需要一个直接接受 HDMI 输入的显示器。我无法使用带有 VGA-HDMI 适配器的 VGA 显示器。</li></ul><h2 id="b11b" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 2 部分:安装 DeepStream SDK</h2><p id="1cf3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在你已经有了你的 Jetson Nano，我们可以安装 DeepStream 了。Nvidia 已经整理了<a class="ae kv" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html" rel="noopener ugc nofollow" target="_blank"> DeepStream 快速入门指南</a>，你可以按照<strong class="ky ir"> Jetson 设置</strong>部分下的说明进行操作。</p><p id="b9c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在您使用上面的链接安装 DeepStream 之前，我想强调一下我的安装经验:</p><ul class=""><li id="56f4" class="mr ms iq ky b kz la lc ld lf mt lj mu ln mv lr mw mx my mz bi translated">安装程序会建议您使用 Nvidia SDK 管理器安装 Jetpack。我跳过了这一步，因为我意识到在第 1 部分(上面)中使用操作系统映像在默认情况下具有大多数必需的依赖性。</li><li id="de54" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">在快速入门指南的“安装 DeepStream SDK”小节中，我使用了方法 2。</li></ul><p id="f2c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在安装了 DeepStream 并提升了时钟之后(如指南中所述)，我们可以运行他们的一个示例来验证安装是否正确完成。将(<code class="fe nz oa ob oc b"><strong class="ky ir">cd</strong></code>)移动到您的 DeepStream 安装文件夹中，并运行以下命令:</p><pre class="kg kh ki kj gt od oc oe of aw og bi"><span id="f187" class="nn lt iq oc b gy oh oi l oj ok">deepstream-app -c ./samples/configs/deepstream-app/source8_1080p_dec_infer-resnet_tracker_tiled_display_fp16_nano.txt</span></pre><p id="0da5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在执行时，您应该会看到类似这样的内容:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/5c4f33c6b039bcf2f4b0ef5d4745a33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6_2ILhsdpOEgubZ1SbBAzw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Output on executing DeepStream using the sample configuration file.</figcaption></figure><p id="15f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你看到类似的东西，恭喜你！如果你愿意，你可以尝试更多的样品。该指南有一个名为“参考应用程序源详细信息”的部分，提供了示例的描述。</p><h2 id="5306" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 3 部分:设置实验</h2><p id="4f0f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在你已经安装并测试了 DeepSteam，我们可以继续我们的实验了。我已经将实验所需的所有文件打包在我的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX" rel="noopener ugc nofollow" target="_blank"> GitHub 存储库</a>中。您可以按照存储库的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX/blob/master/README.md" rel="noopener ugc nofollow" target="_blank"> readme </a>文件中关于设置说明的逐步说明进行操作。</p><p id="d777" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在继续实验之前，如果您以前没有使用过 GStreamer，那么浏览一下他们的<a class="ae kv" href="https://gstreamer.freedesktop.org/documentation/application-development/introduction/basics.html?gi-language=c" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir">基础</strong> </a>页面是值得的。这有助于更好地理解 DeepStream 文档中使用的一些术语。</p><h1 id="338c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">将您的自定义 ONNX 模型与 DeepStream 接口</h1><p id="f227" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在这一节中，我们将探讨如何将 ONNX 模型的输出与 DeepStream 进行接口。更具体地说，我们将逐步完成在<strong class="ky ir"> C++ </strong>中创建自定义处理函数的过程，以从 ONNX 模型的输出中提取边界框信息，并将其提供给 DeepStream。</p><h2 id="7408" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 1 部分:理解 Tiny YOLOv2 的输出</h2><p id="0506" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">ONNX 模型以<strong class="ky ir">通道优先</strong>格式输出形状为<code class="fe nz oa ob oc b"><strong class="ky ir">(125, 13, 13)</strong></code>的张量。然而，当与 DeepStream 一起使用时，我们获得了形状为<code class="fe nz oa ob oc b"><strong class="ky ir">(21125)</strong></code>的张量的<strong class="ky ir">展平</strong>版本。我们的目标是从这个展平张量中手动提取包围盒信息。</p><p id="1b4a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先尝试直观地理解 ONNX 模型输出的输出张量。考虑输出张量是一个维度为<code class="fe nz oa ob oc b"><strong class="ky ir">(B, H, W)</strong></code>的长方体，在我们的例子中是<code class="fe nz oa ob oc b">B=<strong class="ky ir">125</strong>,H=<strong class="ky ir">13</strong>,W=<strong class="ky ir">13</strong></code>。我们可以分别沿着宽度(W)、高度(H)和深度(B)考虑 X、Y 和 B 轴。现在，XY 平面中的每个位置代表一个网格单元。</p><p id="2683" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们想象一个网格单元<code class="fe nz oa ob oc b"><strong class="ky ir">(X=0, Y=0)</strong></code>。对于这个给定的(X，Y)位置，沿着深度轴(B)有 125 个值。让我们以 25 个为一组重新排列 125 个值，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/27fd1cee2fecdd5fe9e51d8f9ce090a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfE5tog6D7Xs319zRUJFWA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure A: Interpreting the meaning of the 125 b-values along the B-axis for the grid cell (X = 0, Y = 0).</figcaption></figure><p id="21e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们在这里看到的，每个连续的 25 个值属于一个单独的边界框。在每组 25 个值中，前 5 个值是边界框参数，后 20 个值是类别概率。利用这一点，我们可以提取 5 个边界框中每一个的坐标和置信度得分，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a84f467c8a3694a31e09f6051a5b97d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*7_z6sinutiwfddCFtp2xtg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Formulae for extracting the bounding box parameters. (<a class="ae kv" href="https://pjreddie.com/media/files/papers/YOLO9000.pdf" rel="noopener ugc nofollow" target="_blank">Source</a>)</figcaption></figure><p id="32b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我们只在一个网格单元(X=0，Y=0)上执行了此操作。我们必须迭代 X 和 Y 的所有组合，以在每个网格单元找到 5 个边界框预测。</p><p id="3fca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们对信息是如何存储的有了一个直观的概念，让我们尝试使用索引来提取它。在<strong class="ky ir">展平</strong>输出张量后，我们得到一个存储信息的数组，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/3c8a2423b05d61ed49645fca0feb28c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6maY2VB9rHD5oaWj4Qr9Lw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure B: Flattened representation of the output tensor.</figcaption></figure><p id="656d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">展平的数组有<code class="fe nz oa ob oc b">125 * 13 * 13 = <strong class="ky ir">21125</strong></code>个元素。如上所示，数组中的每个位置对应于索引<code class="fe nz oa ob oc b"><strong class="ky ir">(b, y, x)</strong></code>。我们可以观察到，对于给定的<code class="fe nz oa ob oc b"><strong class="ky ir">(y,x)</strong></code> <strong class="ky ir"> </strong>值，对应的<code class="fe nz oa ob oc b"><strong class="ky ir">b</strong></code> <strong class="ky ir"> </strong>值由<code class="fe nz oa ob oc b">13 * 13 = <strong class="ky ir">169</strong></code>隔开。</p><p id="f86c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的<strong class="ky ir"> Python </strong>中的代码片段展示了我们如何获得对应于给定<code class="fe nz oa ob oc b"><strong class="ky ir">(y, x)</strong></code>位置中 5 个边界框中每一个的<code class="fe nz oa ob oc b"><strong class="ky ir">b</strong></code>值的位置。请注意，如<strong class="ky ir">图 A </strong>所示，对于给定的<code class="fe nz oa ob oc b"><strong class="ky ir">(y, x)</strong></code>位置，每个边界框有 25 个<code class="fe nz oa ob oc b"><strong class="ky ir">b</strong></code>值。</p><pre class="kg kh ki kj gt od oc oe of aw og bi"><span id="16c1" class="nn lt iq oc b gy oh oi l oj ok">## Let <strong class="oc ir">arr </strong>be the flattened array.<br/>## The array <strong class="oc ir">values </strong>contains the value of <strong class="oc ir">arr</strong> at the 25 b_values per ## bbox,x,y combination.<strong class="oc ir"><br/>num_anchors</strong> = 5<br/><strong class="oc ir">num_classes</strong> = 20<br/><strong class="oc ir">xy_offset</strong> = y * 13 + x<br/><strong class="oc ir">b_offset</strong> = 13 * 13<br/><strong class="oc ir">bbox_offset</strong> = 5 + num_classes<br/>for <strong class="oc ir">bbox</strong> in range(<strong class="oc ir">num_anchors</strong>):<br/>  <strong class="oc ir">values</strong> = []<br/>  for <strong class="oc ir">b</strong> in range(<strong class="oc ir">bbox_offset</strong>):<br/>     value = <strong class="oc ir">arr</strong>[xy_offset + b_offset * (b + bbox * bbox_offset)]<br/>     values.append(value)</span></pre><p id="af34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剩下要做的就是编写与此相同的<strong class="ky ir"> C++ </strong>代码。</p><h2 id="93e7" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 2 部分:编写边界框解析函数</h2><p id="a529" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在我们已经了解了输出是如何存储和提取的，我们需要用 C++编写一个函数来做同样的事情。DeepStream 需要一个具有如下所示参数的函数:</p><pre class="kg kh ki kj gt od oc oe of aw og bi"><span id="9e4c" class="nn lt iq oc b gy oh oi l oj ok">extern "C" bool <strong class="oc ir">NvDsInferParseCustomYoloV2Tiny</strong>(<br/>    std::vector&lt;NvDsInferLayerInfo&gt; const&amp; <strong class="oc ir">outputLayersInfo</strong>,<br/>    NvDsInferNetworkInfo const&amp; <strong class="oc ir">networkInfo</strong>,<br/>    NvDsInferParseDetectionParams const&amp; <strong class="oc ir">detectionParams</strong>,<br/>    std::vector&lt;NvDsInferParseObjectInfo&gt;&amp; <strong class="oc ir">objectList</strong><br/>);</span></pre><p id="87d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的函数原型中，<code class="fe nz oa ob oc b"><strong class="ky ir">outputLayersInfo</strong></code> <strong class="ky ir"> </strong>是一个<code class="fe nz oa ob oc b"><strong class="ky ir">std::vector</strong></code>，包含了我们 ONNX 模型的各个输出层的信息和数据。在我们的例子中，因为我们只有一个输出层，我们可以使用<code class="fe nz oa ob oc b"><strong class="ky ir">outputLayersInfo[0].buffer</strong></code>来访问数据。变量<code class="fe nz oa ob oc b"><strong class="ky ir">networkInfo</strong></code> <strong class="ky ir"> </strong>具有模型期望的高度和宽度信息，变量<strong class="ky ir"> </strong> <code class="fe nz oa ob oc b"><strong class="ky ir">detectionParams</strong></code> <strong class="ky ir"> </strong>具有<strong class="ky ir"> </strong>关于某些配置的信息，例如<strong class="ky ir"> </strong> <code class="fe nz oa ob oc b"><strong class="ky ir">numClassesConfigured</strong></code>。</p><p id="6917" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变量<code class="fe nz oa ob oc b"><strong class="ky ir">objectList</strong></code> <strong class="ky ir"> </strong>应在每次调用该函数时用作为<code class="fe nz oa ob oc b"><strong class="ky ir">NvDsInferParseObjectInfo</strong></code> <strong class="ky ir"> </strong>类型对象存储的边界框信息<code class="fe nz oa ob oc b"><strong class="ky ir">std::vector</strong></code>进行更新。因为变量是通过引用传递的，所以我们不需要返回它，因为更改会在源中反映出来。但是，函数在执行结束时必须返回<code class="fe nz oa ob oc b"><strong class="ky ir">true</strong></code> <strong class="ky ir"> </strong>。</p><p id="d1d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的用例，我们创建<code class="fe nz oa ob oc b"><strong class="ky ir">NvDsInferParseCustomYoloV2Tiny</strong></code> <strong class="ky ir"> </strong>，这样它将首先解码 ONNX 模型的输出，如本节第 1 部分所述。对于每个边界框，我们创建一个类型为<code class="fe nz oa ob oc b"><strong class="ky ir">NvDsInferParseObjectInfo</strong></code> <strong class="ky ir"> </strong>的对象来存储它的信息。然后我们应用<a class="ae kv" href="https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH" rel="noopener ugc nofollow" target="_blank">非最大抑制</a>来移除相同对象的重复边界框检测。然后，我们将生成的边界框添加到<code class="fe nz oa ob oc b"><strong class="ky ir">objectList</strong></code> <strong class="ky ir"> </strong>矢量中。</p><p id="ae75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>的<code class="fe nz oa ob oc b"><strong class="ky ir">custom_bbox_parser</strong></code> <strong class="ky ir"> </strong>目录下有<code class="fe nz oa ob oc b"><strong class="ky ir">nvdsparsebbox_tiny_yolo.cpp</strong></code>，已经为你写好了函数。下面的<strong class="ky ir">流程图</strong>解释了<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX/blob/master/custom_bbox_parser/nvdsparsebbox_tiny_yolo.cpp" rel="noopener ugc nofollow" target="_blank">文件</a>中的逻辑流程。代码可能看起来很大，但那仅仅是因为为了便于理解，它被大量地记录和注释了！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/c0b9bba50a17c369c7ff7b5f89f7045e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9UV41Cx0Ilr_gQ4LnG9VA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Flowchart approximately describing the flow of logic in the code file.</figcaption></figure><h2 id="c8d4" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 3 部分:编译函数</h2><p id="e18a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在剩下的就是把函数编译成一个<code class="fe nz oa ob oc b"><strong class="ky ir">.so</strong></code>文件，这样 DeepStream 就可以加载和使用它了。在编译它之前，您可能需要在 Makefile 中设置一些变量。你可以参考我的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>中自述文件的第 4 步获取说明。一旦完成，进入 GitHub 库并运行下面的命令:</p><pre class="kg kh ki kj gt od oc oe of aw og bi"><span id="21c6" class="nn lt iq oc b gy oh oi l oj ok">make -C custom_bbox_parser</span></pre><h1 id="4c3b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">设置配置文件</h1><p id="ae6a" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">好消息是大部分繁重的工作已经完成。剩下的就是设置一些配置文件，告诉 DeepStream 如何运行实验。一个配置文件有一组“组”，每个“组”有一组以<a class="ae kv" href="https://specifications.freedesktop.org/desktop-entry-spec/latest/" rel="noopener ugc nofollow" target="_blank">密钥文件格式</a>编写的“属性”。</p><p id="ec17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的实验，我们需要设置两个配置文件。在本节中，我们将探究这些配置文件中的一些重要属性。</p><h2 id="746d" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 1 部分:Tiny YOLOv2 的配置文件</h2><p id="8c9e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们的 ONNX 模型由 DeepStream 的<strong class="ky ir"> Gst-Nvinfer </strong>插件使用。我们需要设置一些属性来告诉插件信息，比如 ONNX 模型的位置，编译后的边界框解析器的位置等等。</p><p id="82c0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 GitHub 存储库中，已经为我们的实验设置了名为<code class="fe nz oa ob oc b"><strong class="ky ir">config_infer_custom_yolo.txt</strong></code>的配置文件。文件中给出了每个属性设置的注释和理由。有关所有受支持属性的详细列表，请查看此<a class="ae kv" href="https://docs.nvidia.com/metropolis/deepstream/plugin-manual/index.html#page/DeepStream_Plugin_Manual%2Fdeepstream_plugin_details.02.01.html%23wwpID0E0IZ0HA" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="7697" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们没有使用的一些有趣的属性是“净比例因子”和“偏移”属性。他们基本上使用公式<code class="fe nz oa ob oc b">net_scale_factor * (x — mean)</code>缩放输入<code class="fe nz oa ob oc b">(x)</code>。我们没有使用这些属性，因为我们的网络直接将未缩放的图像作为输入。</p><h2 id="2b21" class="nn lt iq bd lu no np dn ly nq nr dp mc lf ns nt me lj nu nv mg ln nw nx mi ny bi translated">第 2 部分:DeepStream 的配置文件</h2><p id="c57e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们还需要为 DeepStream 设置一个配置文件，以启用和配置它将使用的各种插件。如前所述，GitHub 存储库包含配置文件<code class="fe nz oa ob oc b"><strong class="ky ir">deepstream_app_custom_yolo.txt</strong></code>，它已经为我们的实验设置好了。</p><p id="5501" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与上一部分不同的是，这个配置有“osd”(屏幕显示)、“primary-gie”(初级 GPU 推理机)等一大堆组。此<a class="ae kv" href="https://docs.nvidia.com/metropolis/deepstream/dev-guide/index.html#page/DeepStream_Development_Guide%2Fdeepstream_app_config.3.2.html%23" rel="noopener ugc nofollow" target="_blank">链接</a>包含所有可配置的可能组的信息以及每个组支持的属性。</p><p id="1d61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的实验中，我们定义了一个源组(<code class="fe nz oa ob oc b">source0</code>)和三个宿组(<code class="fe nz oa ob oc b">sink0</code>、<code class="fe nz oa ob oc b">sink1</code>和<code class="fe nz oa ob oc b">sink2</code>)。单源组负责并行读取<strong class="ky ir">四个</strong>输入视频流。三个接收器组分别用于在屏幕上显示输出、使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Real_Time_Streaming_Protocol" rel="noopener ugc nofollow" target="_blank"> RTSP </a>传输输出以及将输出保存到磁盘。我们在<code class="fe nz oa ob oc b">primary-gie</code>组中提供了 Tiny YOLOv2 的配置文件路径。此外，我们还设置了<code class="fe nz oa ob oc b">titled-display</code>和<code class="fe nz oa ob oc b">osd</code>组来控制输出在屏幕上的显示方式。</p><h1 id="a6cf" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">运行深流</h1><p id="730c" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">这是最简单的部分。您只需运行以下命令:</p><pre class="kg kh ki kj gt od oc oe of aw og bi"><span id="0568" class="nn lt iq oc b gy oh oi l oj ok">deepstream-app -c ./config/<!-- -->deepstream_app_custom_yolo.txt</span></pre><p id="f6b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首次启动 DeepStream 需要一段时间，因为 ONNX 模型需要转换为 TensorRT 引擎。建议在此过程中关闭 Chromium 等内存密集型应用。一旦创建了引擎文件，如果在微小的 YOLOv2 配置文件中定义了引擎文件的路径，后续的启动将会很快。</p><h1 id="e7a3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结果</h1><p id="eba9" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在运行 DeepStream 时，一旦创建了引擎文件，我们就会看到一个<code class="fe nz oa ob oc b">2x2</code>平铺显示，如下图所示。拼接显示器中的每个单元对应于不同的流输入。正如所料，所有的<strong class="ky ir">四个不同的输入</strong>被同时处理。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oq or l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Output displayed by DeepStream.</figcaption></figure><p id="65ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们还启用了 RTSP，所以我们可以在<code class="fe nz oa ob oc b">rtsp://localhost:8554/ds-test</code>访问流。我使用 VLC 和 RTSP 的地址(在用我的 Jetson Nano 的 IP 地址替换了<code class="fe nz oa ob oc b">localhost</code>之后)来访问连接到同一网络的笔记本电脑上的流。请注意，另一个接收器也用于将输出流保存到磁盘。令人印象深刻的是，控制台定期记录每视频流接近<strong class="ky ir"> 6.7 </strong>的 FPS！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c621412923bb67eb91d8c10937e32c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*J224jhQU4TVhbD_5TASn1w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">FPS per video stream while simultaneously using four video streams.</figcaption></figure><p id="d7a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们有一个单一的输入流，那么我们的 FPS 应该是四个视频的四倍。我通过更改配置文件中的值并再次启动 DeepStream 来测试这一点。正如所料，我们得到了一个巨大的接近<strong class="ky ir"> 27 FPS </strong>的单一视频流！考虑到它仍然向三个不同的接收器发送输出，性能令人印象深刻。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/6198cd9bf72f7f188a0dff39747bc2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*8yMyc8kBqtm96QHFWurAng.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">FPS while using a single video stream.</figcaption></figure><p id="b7f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我们注意到微小的 YOLOv2 的检测精度没有 FPS 那么惊人。这尤其是因为该模型是以牺牲一些准确性为代价来优化速度的。此外，视频中的人面部模糊，模型在训练时可能不会遇到这种模糊。因此，该模型可能会面临额外的困难。</p><h1 id="cec5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">判决和想法</h1><p id="45d3" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">深溪速度惊人。尽管 Tiny YOLOv2 针对速度而非准确性进行了优化，但稳定的高 FPS 性能，同时提供无缝多流处理和 RTSP 流等惊人的功能，是值得赞赏的。</p><p id="3fc6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，使用 DeepStream 可能并不简单，尤其是如果您的模型与 TensorRT 不完全兼容。在这种情况下，手动编写自己的 TensorRT 层可能是一个更可行的(尽管很乏味)选择。此外，可能会出现这样的情况，即现成的 ONNX 模型的 opset 版本可能高于 DeepStream 当前接受的版本。</p><p id="4e0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管如此，我确实觉得 DeepStream 提供的功能是值得努力的。我建议你通过复制我的<a class="ae kv" href="https://github.com/thatbrguy/Deep-Stream-ONNX" rel="noopener ugc nofollow" target="_blank">实验</a>来尝试一下！</p></div></div>    
</body>
</html>