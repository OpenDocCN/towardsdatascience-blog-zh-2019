<html>
<head>
<title>Reinforcement Learning — TD(λ) Introduction(1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习— TD(λ)简介(1)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60?source=collection_archive---------6-----------------------#2019-08-24">https://towardsdatascience.com/reinforcement-learning-td-%CE%BB-introduction-686a5e4f4e60?source=collection_archive---------6-----------------------#2019-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4038" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对随机漫步应用 offline-λ</h2></div><p id="09a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将讨论 TD(λ)，这是一种通用的强化学习方法，它结合了蒙特卡罗模拟和一步 TD 方法。我们一直在详尽地讨论 TD 方法，如果您还记得，在<a class="ae lb" href="https://medium.com/zero-equals-false/n-step-td-method-157d3875b9cb" rel="noopener"> TD(n) </a>方法中，我说过它也是 MC 仿真和 1 步 TD 的统一，但在 TD(n)中，我们必须跟踪沿途的所有轨迹，并根据 n 步前的值更新当前估计，在 TD(λ)中，我们将看到更优雅的统一。</p><p id="dd5e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将:</p><ol class=""><li id="60c4" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">学习 TD(λ)的思想</li><li id="bbe0" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">介绍前向视图更新方法—离线—λ返回</li><li id="7c1e" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">将该方法应用于随机游走实例</li></ol><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi lq"><img src="../Images/89b7d270df78bd400f7134d1f1d1d384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_zCI_fBfBJq-f_-Y-IVYSg.jpeg"/></div></div></figure><h1 id="314c" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">TD(λ)的概念</h1><p id="184e" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">TD(λ)实际上是 TD(n)方法的扩展，记住在 TD(n)中，我们有如下形式的累积报酬:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi mz"><img src="../Images/9235818a8b3e2c29b1e94f418d58e0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyBlh2RZz1SIHf7_Hzncug.png"/></div></div></figure><p id="3357" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">直到步骤<code class="fe na nb nc nd b">t+n</code>的该值估计用于更新步骤<code class="fe na nb nc nd b">t</code>的值，TD(λ)所做的是平均该值，例如，使用</p><pre class="lr ls lt lu gt ne nd nf ng aw nh bi"><span id="ed0b" class="ni md iq nd b gy nj nk l nl nm">0.5*Gt:t+2 + 0.5*Gt:t+4</span></pre><p id="f2ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为目标值。但是它没有使用直接权重，而是使用λ作为参数来控制权重，使其总和为 1:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nn"><img src="../Images/1482f2c3886c7bd468b1bd3c7a1322a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-LUOyW5W-0gBxx80GHdHQ.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">TD(λ)</figcaption></figure><p id="d630" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我打赌这个图像看起来很熟悉，一个代理从一个状态开始，通过采取一个动作，它到达下一个状态，然后它选择另一个动作，SARSA 过程继续下去。所以第一列实际上是 TD(1)方法，它被赋予了<code class="fe na nb nc nd b">1-λ</code>的权重，第二列是 TD(2)，它的权重是<code class="fe na nb nc nd b">(1-λ)λ</code>，…，直到最后一个 TD(n)被赋予了<code class="fe na nb nc nd b">λ^(T-t-1)</code>的权重(T 是一集的长度)。注意，权重随着 n 的增加而衰减，总和为 1。TD(λ)更一般的形式是:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ns"><img src="../Images/9e21117f3d14427318c4dedca2dc0350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cb8k99aQ0z32hK7amGVbKA.png"/></div></div></figure><p id="2fde" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从公式中可以看出，当<code class="fe na nb nc nd b">λ = 1</code>时，只保留最后一项，这本质上是蒙特卡罗方法，作为状态，动作过程一直进行到最后，当<code class="fe na nb nc nd b">λ = 0</code>时，该项减少到<code class="fe na nb nc nd b">G_t:t+1</code>，这是一步 TD 方法，对于<code class="fe na nb nc nd b">0 &lt; λ &lt; 1</code>，该方法变成了加权 TD(n)混合方法。</p><h1 id="80d3" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">离线λ-返回(前视图)</h1><p id="dc0f" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">随着目标<code class="fe na nb nc nd b">G_t</code>的定义，我们现在开始我们的第一个算法定义。概括的更新公式可以定义为:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nt"><img src="../Images/e91c18bbcd6f966b874ec29c22e3a89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wTsrf_hPSS5mjVBbK4_zA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">offline λ-return</figcaption></figure><p id="c2fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更新规则与一般的<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa">半梯度法</a>相同，唯一的区别在于上述目标值 I。</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi nu"><img src="../Images/169fa1ec1721f7e0c0bdc597d2172137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bw2RWGlOHPq3ym7IX8vuBg.png"/></div></div></figure><p id="9150" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考 Sutton 书中的一幅图像，这种方法也被称为<em class="nv">前视</em>学习算法，因为在每个状态下，更新过程都会前视<code class="fe na nb nc nd b">G_t:t+1</code>、<code class="fe na nb nc nd b">G_t:t+2</code>、……的值，并基于该值的加权值来更新当前状态。</p><h1 id="9dbf" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">随机漫步的正向更新</h1><p id="367b" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">现在让我们来看看随机漫步例子中算法的实现。我们已经在这里学习了随机漫步示例<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-generalisation-in-continuous-state-space-df943b04ebfa">，但是仅供参考，在随机漫步中，一个代理从中间位置开始，在每一步，它都有相等的概率向左或向右移动一步(行动策略是固定的)，并且通过仅在左侧或最左侧结束，它可以停止一集。</a></p><p id="2667" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将<a class="ae lb" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/RandomWalk(Lambda)/TD_Lambda.py" rel="noopener ugc nofollow" target="_blank">实现</a>一个 19 态随机游走，尽管状态空间实际上是离散的，我们仍然可以对其应用一般化算法。</p><h2 id="5efc" class="ni md iq bd me nw nx dn mi ny nz dp mm ko oa ob mo ks oc od mq kw oe of ms og bi translated">价值函数</h2><figure class="lr ls lt lu gt lv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="20fc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">价值函数简单明了。我们有 19 个状态和 2 个结束状态，所以总共有 21 个状态，每个状态都有一个权重，本质上就是它的值估计。<code class="fe na nb nc nd b">value</code>函数返回特定状态的值，而<code class="fe na nb nc nd b">learn</code>函数基于差值<code class="fe na nb nc nd b">delta</code>更新电流估计，在本例中差值为<code class="fe na nb nc nd b">Gt — v(St, wt)</code> ( <code class="fe na nb nc nd b">alpha</code>为学习速率)。</p><h2 id="144e" class="ni md iq bd me nw nx dn mi ny nz dp mm ko oa ob mo ks oc od mq kw oe of ms og bi translated">一些常规功能</h2><p id="2cc3" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">由于这不是我们第一次实现随机漫步，我将列出一些常见的共享函数:</p><figure class="lr ls lt lu gt lv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="1e84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每个状态下，一个代理人<code class="fe na nb nc nd b">chooseAction</code> → <code class="fe na nb nc nd b">takeAction</code> → <code class="fe na nb nc nd b">giveReward</code>并重复直到游戏结束。</p><h2 id="5b81" class="ni md iq bd me nw nx dn mi ny nz dp mm ko oa ob mo ks oc od mq kw oe of ms og bi translated">播放和更新</h2><p id="14fd" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">同样，主要区别在于播放过程和计算每个状态的增量。</p><figure class="lr ls lt lu gt lv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="6858" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每一集，我们将需要首先跟踪所有的状态，直到游戏结束，以便在更新每个状态的值时获得一个前瞻性的视图。<code class="fe na nb nc nd b">self.states</code>记录沿途所有状态，<code class="fe na nb nc nd b">self.reward</code>只保存最新的奖励，因为沿途所有奖励都是 0，除了最终状态。</p><p id="fed5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二部分是在游戏结束后更新状态值估计。回想一下公式:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi ns"><img src="../Images/9e21117f3d14427318c4dedca2dc0350.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cb8k99aQ0z32hK7amGVbKA.png"/></div></div></figure><p id="5000" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于在时间<code class="fe na nb nc nd b">t</code>和步骤<code class="fe na nb nc nd b">n</code>的每个状态，我们需要计算<code class="fe na nb nc nd b">G_t:t+n</code>的值，并将它们与衰减的权重相结合，以便得到<code class="fe na nb nc nd b">St</code>的目标值。因此函数<code class="fe na nb nc nd b">gt2tn</code>计算给定<code class="fe na nb nc nd b">t</code>和<code class="fe na nb nc nd b">n</code>的值，定义为:</p><figure class="lr ls lt lu gt lv"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="b95c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而前面代码片段中的<code class="fe na nb nc nd b">gtlambda</code>就是目标值。此外，我们还设置了一个截尾值，当<code class="fe na nb nc nd b">lambda_power</code>太小时，我们干脆忽略该值。利用目标值<code class="fe na nb nc nd b">gtlambda</code>和来自<code class="fe na nb nc nd b">valueFunc</code>的当前值，我们能够计算差值<code class="fe na nb nc nd b">delta</code>并使用我们上面定义的函数<code class="fe na nb nc nd b">learn</code>更新估计值。</p><h1 id="f4e7" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">离线λ-返回和 TD(n)</h1><p id="9a9c" class="pw-post-body-paragraph kf kg iq kh b ki mu jr kk kl mv ju kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">记得在 TD(n)会议中，我们用完全相同的设置对随机漫步应用了 n 步 TD 方法。现在随着<code class="fe na nb nc nd b">off-line λ-Return</code>的介绍，让我们比较一下两者的学习结果:</p><figure class="lr ls lt lu gt lv gh gi paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="gh gi oj"><img src="../Images/49366ac19dc6aafdd9e267081c23900e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7LyoGHfWiB_uSn5T5IZmw.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">Image from Reinforcement Learning an Introduction</figcaption></figure><p id="89d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到，两条均方根误差曲线相似，结果具有可比性。一般来说，最好的学习结果通常出现在中等学习速率和λ值的情况下(我从 Sutton 的书中复制了图像，因为这张比我绘制的要清晰得多)。</p><p id="e5b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是前视更新，请在这里查看完整的实现<a class="ae lb" href="https://github.com/MJeremy2017/Reinforcement-Learning-Implementation/blob/master/RandomWalk(Lambda)/TD_Lambda.py" rel="noopener ugc nofollow" target="_blank"/>。在下一篇文章中，我们将学习向后更新，这是一种通过使用<em class="nv">资格跟踪</em>的更优雅的 TD(λ)方法。</p><p id="b5df" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考</strong>:</p><ul class=""><li id="f4f9" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la ok li lj lk bi translated"><a class="ae lb" href="http://incompleteideas.net/book/the-book-2nd.html?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="08e4" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la ok li lj lk bi translated"><a class="ae lb" href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/Shang tong Zhang/reinforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>