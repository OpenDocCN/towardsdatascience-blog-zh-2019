<html>
<head>
<title>What’s growing there?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">那里在生长什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whats-growing-there-a5618a2e6933?source=collection_archive---------8-----------------------#2019-09-20">https://towardsdatascience.com/whats-growing-there-a5618a2e6933?source=collection_archive---------8-----------------------#2019-09-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6f83" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用 eo-learn 和 fastai 从多光谱遥感数据中识别作物</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5988279dffb9b000f7ef7c7a7c792a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AdZoclPklV5tFuKJsudEQw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A section of the Orange River, South Africa: colour imagery and NDVI from Sentinel 2 and target masks from Zindi’s Farm Pin Crop Detection Challenge</figcaption></figure><h1 id="643f" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="faa2" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这篇文章描述了我如何使用 eo-learn 和 fastai 库来创建机器学习数据管道，该管道可以从卫星图像中对作物类型进行分类。我用这个管道进入了 Zindi 的<a class="ae mj" href="https://zindi.africa/competitions/farm-pin-crop-detection-challenge" rel="noopener ugc nofollow" target="_blank">农场 Pin 作物检测挑战赛</a>。我可能没有赢得比赛，但我学到了一些处理遥感数据的伟大技术，我在这篇文章中详细介绍了这些技术。</p><p id="e61c" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">以下是我遵循的预处理步骤:</p><ol class=""><li id="497e" class="mp mq iq lp b lq mk lt ml lw mr ma ms me mt mi mu mv mw mx bi translated">将感兴趣的区域划分成“小块”网格，</li><li id="f552" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">从磁盘加载图像，</li><li id="cfdc" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">遮住了云层，</li><li id="0ccb" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">增加了 NDVI 和欧几里德范数特性，</li><li id="d5a1" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">按照固定的时间间隔对图像进行重新采样，</li><li id="3e27" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">添加了具有目标和标识符的栅格图层。</li></ol><p id="0de2" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我将作物类型分类的问题重新定义为一项语义分割任务，并使用图像增强和混合对多时相多光谱数据训练了一个带有 ResNet50 编码器的 U-Net，以防止过度拟合。</p><p id="6aa2" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我的解决方案在很大程度上借鉴了<a class="nd ne ep" href="https://medium.com/u/fe489d29ae67?source=post_page-----a5618a2e6933--------------------------------" rel="noopener" target="_blank"> Matic Lubej </a>在他的<a class="ae mj" href="https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-1-2471e8098195" rel="noopener">三篇</a> <a class="ae mj" href="https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-2-bd9aa86f8500" rel="noopener">优秀</a> <a class="ae mj" href="https://medium.com/sentinel-hub/land-cover-classification-with-eo-learn-part-3-c62ed9ecd405" rel="noopener">帖子</a>中概述的关于土地覆盖分类与<a class="ae mj" href="https://github.com/sentinel-hub/eo-learn" rel="noopener ugc nofollow" target="_blank"> eo-learn </a>的方法。</p><p id="c648" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我创建的 python 笔记本可以在这个 github 资源库中找到:<a class="ae mj" href="https://github.com/simongrest/farm-pin-crop-detection-challenge" rel="noopener ugc nofollow" target="_blank">https://github . com/Simon grest/farm-pin-crop-detection-challenge</a></p><h1 id="786b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">挑战</h1><p id="4afb" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">Zindi 是一个非洲竞争性数据科学平台，致力于利用数据科学造福社会。在 Zindi 的 2019 年<a class="ae mj" href="https://zindi.africa/competitions/farm-pin-crop-detection-challenge" rel="noopener ugc nofollow" target="_blank">农场 Pin 作物检测挑战赛</a>中，<strong class="lp ir">的参与者使用</strong><a class="ae mj" href="https://sentinel.esa.int/web/sentinel/missions/sentinel-2" rel="noopener ugc nofollow" target="_blank"><strong class="lp ir">sentinel 2</strong></a><strong class="lp ir">图像训练机器学习模型，以便对南非奥兰治河沿岸</strong>的农田中种植的作物进行分类。</p><p id="229e" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">提供给参赛者的数据由两个形状文件组成，其中包含训练集和测试集的场边界，以及 2017 年 1 月至 8 月期间 11 个不同时间点的感兴趣区域的 Sentinel2 影像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/bb44aa5777539a6349a35dbbf0ca1ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncEqYnxT6Yq9MdWp7sdKcA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The section of the Orange River — grey test set fields interspersed amongst green training set fields</figcaption></figure><p id="7241" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">训练集和测试集分别由 2497 个字段和 1074 个字段组成。训练集中的每个田地都标有九个标签中的一个，指示该田地在 2017 年期间种植的作物。</p><p id="fe62" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">作物类型有:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="17c2" class="nl kw iq nh b gy nm nn l no np">Cotton<br/>Dates<br/>Grass<br/>Lucern<br/>Maize<br/>Pecan<br/>Vacant<br/>Vineyard<br/>Vineyard &amp; Pecan (“Intercrop”)</span></pre><p id="b477" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">参赛者只能使用提供的数据，并且(由于在比赛期间发现数据泄露)禁止使用 Field_Id 作为训练功能。</p><h1 id="898d" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">使用 eo-learn ( <a class="ae mj" href="https://github.com/simongrest/farm-pin-crop-detection-challenge/blob/master/notebooks/Process%20Satellite%20Imagery.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)进行数据预处理</h1><p id="faa8" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">eo-learn 库允许用户将感兴趣的区域划分为补丁，定义工作流，然后在补丁上并行执行工作流。</p><h2 id="337c" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">1.分割感兴趣的区域</h2><p id="38eb" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">使用<code class="fe ob oc od nh b">sentinelhub</code>库中的<code class="fe ob oc od nh b">BBoxSplitter</code>,我将河流分成了 12 块:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/07fe50619e13e958517b47c7efd86390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKy6WDVn4uPbO7OpQNJRcg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The area of interest partitioned into a grid of ‘patches’</figcaption></figure><h2 id="dd81" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">2.从磁盘加载图像数据</h2><p id="ef41" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">比赛图像数据以 JPEG2000 格式在标准<a class="ae mj" href="https://sentinel.esa.int/web/sentinel/user-guides/sentinel-2-msi/data-formats" rel="noopener ugc nofollow" target="_blank"> Sentinel2 文件夹结构</a>中提供，如下图所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/58e5fcad2593878e53c9bb6b16a6fe93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CbyCQhd1R031kOXOMPkI7Q.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Sentinel2 folder structure</figcaption></figure><p id="d4ce" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">eo-learn 库提供了许多有用的预定义任务，用于从 Sentinel Hub 加载影像、处理影像和生成要素。在撰写本文时，它还没有以上述格式从磁盘加载图像的任务。然而，定义我自己的<code class="fe ob oc od nh b">EOTask</code>类来做这件事被证明足够简单。<code class="fe ob oc od nh b">EOTask</code>类需要一个<code class="fe ob oc od nh b">execute()</code>方法，该方法可以选择接受一个<code class="fe ob oc od nh b">EOPatch</code>对象作为参数。</p><p id="4d64" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><code class="fe ob oc od nh b">EOPatch</code>对象本质上只是<code class="fe ob oc od nh b">numpy</code>数组和元数据的集合。我自己定制的<code class="fe ob oc od nh b">EOTask</code>加载的<code class="fe ob oc od nh b">EOPatch</code>对象看起来是这样的:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="6a84" class="nl kw iq nh b gy nm nn l no np">data: {<br/>    BANDS: numpy.ndarray(shape=(11, 1345, 1329, 13), dtype=float64)<br/>  }<br/>  mask: {}<br/>  mask_timeless: {}<br/>  scalar_timeless: {}<br/>  label_timeless: {}<br/>  vector_timeless: {}<br/>  meta_info: {<br/>    service_type: 'wcs'<br/>    size_x: '10m'<br/>    size_y: '10m'<br/>  }<br/>  bbox: BBox(((535329.7703788084, 6846758.109461494), (548617.0052632861, 6860214.913734847)), crs=EPSG:32734)<br/>  timestamp: [datetime.datetime(2017, 1, 1, 8, 23, 32), ..., datetime.datetime(2017, 8, 19, 8, 20, 11)], length=11<br/>)</span></pre><p id="ca53" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我们可以通过使用波段 4、3 和 2(红色、绿色和蓝色)为每个斑块生成彩色图像来可视化斑块:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/fba2f485d46d84b083e84a8711d16635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zaoHmpWKwIyOb339LK-_hg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Colour images of the 12 patches made with the red, green and blue bands</figcaption></figure><h2 id="0b4d" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">3.遮蔽云层</h2><p id="7eb0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在上面图像的右下角有一些云层。eo-learn 库提供了一个预训练的像素级云探测器模型。此功能可通过<code class="fe ob oc od nh b">S2PixelCloudDetector</code> <em class="oh"> </em>和<em class="oh"/><code class="fe ob oc od nh b">AddCloudMaskTask</code>类获得。</p><p id="c00b" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><code class="fe ob oc od nh b">S2PixelCloudDetector</code>来自一个单独的库<a class="ae mj" href="https://github.com/sentinel-hub/sentinel2-cloud-detector" rel="noopener ugc nofollow" target="_blank">sentinel2-云探测器</a>，使用 sentinel 2 图像的所有 13 个波段进行预测。通过设置概率阈值，可以将云概率预测转化为云掩膜。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/1fe6a3ded2c4530aac73fe90f5b5dcdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ui_wN1SrPy5jV1Y6KY_zeQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Colour image with clouds, cloud probabilities and resulting cloud mask</figcaption></figure><p id="0dcb" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我使用这个云检测功能向我的数据添加了一个云遮罩。</p><h2 id="b1d0" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">4.随时间重新采样</h2><p id="6845" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">删除云会在每个时间片上有云覆盖的区域的数据中留下间隙。填充这些间隙的一种可能的方法是在前面和后面的时间片之间进行插值。</p><p id="88ae" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">已经为此定义了一个<code class="fe ob oc od nh b">LinearInterpolation</code> <code class="fe ob oc od nh b">EOTask</code>。类要求您指定要插值的波段和重新采样的间隔。我决定将我的数据平均到大约每月一个时间片，这将我的时间维度从 11 个时间点减少到 8 个。</p><p id="dd16" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">此外，为了处理时间段开始或结束时的任何间隙，我使用了一个<code class="fe ob oc od nh b">ValueFilloutTask</code>进行简单的外推，即根据需要从之前或之后的时间点复制值。</p><h2 id="89fc" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">5.添加 NDVI</h2><p id="9420" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="oh">归一化差异植被指数</em> (NDVI)是卫星图像中植物生命存在的简单指标。该指数是使用红色和近红外(NIR)波段计算的。</p><p id="98ff" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">NDVI =(近红外光谱-红色)/(近红外光谱+红色)</p><p id="2573" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">维基百科上关于 NDVI 的文章对这个指标背后的基本原理有一个很好的解释。基本的想法是，植物物质吸收大部分可见的红色光谱光，而它反射近红外光，这不能用于光合作用，NDVI 在比率中捕捉这种反射率的差异。</p><p id="bb90" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">方便的是，eo-learn 提供了一个<code class="fe ob oc od nh b">NormalizedDifferenceIndex</code>任务，允许我轻松地计算和添加每个补丁的 NDVI。</p><p id="8da7" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">对于不同的作物，NDVI 随着时间的推移会有不同的演变。不同的作物在不同的时间种植和收获，生长速度也不同。下面的动画展示了 NDVI 在邻近油田的不同发展。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/c31d12ca431787ed585f734f0cfedd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*xBky4ockxbzajTUyh8Evug.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">NDVI through time (in March you can see artefacts that result from the cloud masking and interpolation)</figcaption></figure><h2 id="d012" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">6.添加目标遮罩</h2><p id="cb84" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">为了将作物识别挑战视为语义分割任务，我需要为我们的图像创建目标遮罩。eo-learn 中的<code class="fe ob oc od nh b">VectorToRaster</code>任务获取矢量几何图形并创建光栅化图层。我使用此任务来添加指示作物类型的栅格图层。我还添加了一个带有字段标识符的层，用于推理。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/c69cf39f6e67f282c5ebde78218e08e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEBvN5ikY-mKN31OvyINdg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Crop type raster layer for patch number 6</figcaption></figure><h2 id="f55f" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">创建工作流并执行它</h2><p id="661c" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">为了运行上述每个预处理步骤，我将所有任务放入一个工作流中。一般来说，eo-learn 工作流可以是任何在每个节点都有<code class="fe ob oc od nh b">EOTask</code>对象的无环有向图。我只是使用了一个线性工作流程，看起来像这样:</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="ed62" class="nl kw iq nh b gy nm nn l no np">LinearWorkflow(<br/> add_data,           # load the data<br/> add_clm,            # create cloud mask<br/> ndvi,               # compute ndvi<br/> norm,               # compute the euclidean norm of the bands<br/> concatenate         # add the ndvi and norm to the bands<br/> linear_interp,      # linear interpolation<br/> fill_extrapolate,   # extrapolation<br/> target_raster,      # add target masks<br/> field_id_raster,    # add field identifiers<br/> save                # save the data back to disk<br/>)</span></pre><p id="36e5" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">为了执行这个工作流，我为每个补丁创建了执行参数，然后使用一个<code class="fe ob oc od nh b">EOExecutor</code>以分布式方式在所有补丁上运行整个工作流。</p><pre class="kg kh ki kj gt ng nh ni nj aw nk bi"><span id="e1ee" class="nl kw iq nh b gy nm nn l no np">execution_args = []<br/>for patch_idx in range(12):<br/>    execution_args.append({<br/>        load: {'eopatch_folder': f'eopatch_{patch_idx}'},<br/>        save: {'eopatch_folder': f'eopatch_{patch_idx}'}<br/>    })<br/>    <br/>executor = EOExecutor(workflow, execution_args, save_logs=True)<br/>executor.run(workers=6, multiprocess=False)</span></pre><h1 id="7e6a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">构建预测模型</h1><h2 id="9ca4" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">语义分割</h2><p id="fa83" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">语义分割是给图像的每个像素分配类别标签的过程。通过将这次挑战中的作物识别问题重新构建为语义分割任务，我可以利用每个领域的局部空间上下文中的信息，如下所示，这还允许我通过重复采样生成更多的训练数据。</p><h2 id="3441" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">1.生成训练集(<a class="ae mj" href="https://github.com/simongrest/farm-pin-crop-detection-challenge/blob/master/notebooks/Create%20Unet%20Features.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</h2><p id="a77d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">从我的 12 个小块中，我随机抽取了 64 x 64 像素的小块来训练我的模型。我保持小块的大小，因为这些区域本身相对较小，并且提供的 Sentinel2 影像的最大空间分辨率为 10 米。这意味着一个 1 公顷大小(10，000 平方米)的正方形区域在影像中显示为 32 x 32 像素的面积。</p><p id="c708" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我以确保每个小块至少包含一部分训练字段的方式对小块进行了采样。对于每个 patchlet，我保存了两个 pickle 文件，一个包含输入影像，另一个包含作物类型的栅格图层。</p><p id="e344" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">对于输入图像，我选择包括六个通道，三个可见波段(红色、绿色和蓝色)，近红外和计算的 NDVI 和欧几里德范数。当我通过时间插值对图像进行重新采样时，我得到了八个不同的时间点。为了得到一个秩为 3 的张量，我简单地在八个时间点的每一个点上叠加六个通道，得到一个 48 通道的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/9e76a227e571894663d11def0e584551.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LN8c786HRp40pyXxRvCp6w.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">NDVI and visible images at a single time point along with the corresponding target crop types for nine randomly sampled 64x64 training ‘patchlets’</figcaption></figure><h2 id="a559" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">2.数据扩充</h2><p id="5ffc" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">竞争中可用的相对较小的数据集和我选择的网络架构中的大量参数意味着我需要特别小心过度拟合。为了避免这一点，我使用了图像放大和混合。</p><p id="06e0" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">fastai 库提供了一系列图像增强技术。我使用了:</p><ul class=""><li id="3be1" class="mp mq iq lp b lq mk lt ml lw mr ma ms me mt mi om mv mw mx bi translated">垂直翻转</li><li id="c85a" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi om mv mw mx bi translated">水平翻转</li><li id="dd7e" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi om mv mw mx bi translated">旋转</li><li id="8140" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi om mv mw mx bi translated">嗡嗡声</li><li id="9734" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi om mv mw mx bi translated">翘曲</li><li id="a06b" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi om mv mw mx bi translated">和剪切</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/e19d86e452b6eb5387a64e9b91bc81c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqqf_wXMHCthUdjIIyGElQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">A batch of training images with image augmentations applied</figcaption></figure><h2 id="0772" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">3.创建 fastai U-Net 模型(<a class="ae mj" href="https://github.com/simongrest/farm-pin-crop-detection-challenge/blob/master/notebooks/Train%20Unet%20Model.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>)</h2><p id="0cc1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><a class="ae mj" href="https://docs.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fastai </a>库通过允许用户从现有卷积网络编码器动态构建 U-Net 来提供语义分段。我选择了一个在<a class="ae mj" rel="noopener ugc nofollow" target="_blank" href="/www.image-net.org"> ImageNet </a>上预先训练的 ResNet50 作为我的编码器网络。为了处理我的输入张量的形状，我将 ResNet50 网络的第一个卷积层(采用 3 个通道)替换为采用 48 个通道的卷积层。</p><p id="7ab3" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我不会试图在这里解释 U-网或残差神经网络，因为已经有很多很好的解释了。例如，这里有一个<a class="ae mj" rel="noopener" target="_blank" href="/understanding-semantic-segmentation-with-unet-6be4f42d4b47">帖子</a>解释 U-net，这里有<a class="ae mj" rel="noopener" target="_blank" href="/introduction-to-resnets-c0a830a288a4">另一个</a>解释 ResNets。</p><p id="b7aa" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我创建了<code class="fe ob oc od nh b">SegmentationPklList</code>和类<code class="fe ob oc od nh b">SegmentationPklLabelList</code>来实现加载 pickle 文件‘图像’的功能，这样我的数据就可以与 fastai 的数据块 API 一起工作。</p><p id="4ee1" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">fastai <code class="fe ob oc od nh b">MixUpCallback</code>和<code class="fe ob oc od nh b">MixUpLoss</code>也需要一些小的调整来处理语义分割。</p><h2 id="a1d7" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">4.损失函数</h2><p id="239c" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我用一个修改过的<code class="fe ob oc od nh b">CrossEntropyFlat</code>损失函数来给我的模型打分。我将其实例化为:</p><p id="0fd9" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><code class="fe ob oc od nh b">CrossEntropyFlat(axis=1, weight=inv_prop, ignore_index=0)</code></p><p id="7ed7" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">不同作物类型在训练集中的出现是不平衡的，某些作物类型只出现少数几次。我通过使用损失构造函数的<code class="fe ob oc od nh b">weight</code>参数，将我的损失函数与每种作物类型的逆频率成比例加权。</p><p id="ed82" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">训练图像的大部分区域没有作物类型，或者该区域没有田地，或者如果有田地，它也不是训练集的一部分。通过使用 loss 构造函数的<code class="fe ob oc od nh b">ignore_index</code>参数，我忽略了没有作物类型标签的预测。</p><h2 id="33aa" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">5.培养</h2><p id="80e9" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">fastai 库提供的最大优势之一是灵活的训练循环，以及通过<a class="ae mj" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">单周期训练策略</a>等技术控制训练参数的强大开箱即用支持。我使用<code class="fe ob oc od nh b">fit_one_cycle</code>函数训练我的 U-Net 五个时期，保持预训练的编码器参数不变，然后再训练十个时期，允许更新编码器权重。</p><p id="8f40" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">在训练过程中，验证集的损失持续下降，而我的自定义像素精度度量稳步上升。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/0ae8bdb0734d7778a985ca9aac0cc5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZ8dqPjE-ZVAx-zOxqCHDg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Training results from 5 frozen and 10 unfrozen epochs</figcaption></figure><p id="6cf2" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">对于验证集中的示例，将预测的像素掩膜与目标掩膜进行比较似乎表明网络工作正常，但是存在少数类和具有非标准形状的字段性能较差的示例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/c1fd9f3a8a5487750b2424bda0c4c18e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kb2jtZTMDkLmQdB4ygvOYQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/4ba19fbe2c3b8baa1620625248464ef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7e3odw0qLMbPWTsnj2XGvw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">More predictions on examples from the validation set</figcaption></figure><h1 id="05b7" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">成果和有待改进的领域</h1><p id="964b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">为了对测试集进行推断，我将每个补丁分成一个 64x64 的“小补丁”网格，并保存每个小补丁的 pickle 文件。我对整个测试集进行了预测，并通过<code class="fe ob oc od nh b">Field_Id</code>对结果进行了分组。对每个像素的预测由来自 U-Net 的十个最终激活组成。我取了每个类的中值激活值，然后应用一个 softmax 函数来得到测试集中每个<code class="fe ob oc od nh b">Field_Id</code>的单个概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/18f7de7c584c3961517420c78d847e68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*mcKa5KTQAN_tGEfI-opnEw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">sample grid of ‘patchlets’ for inference — colouring by Field_Id clearly shows the data leak</figcaption></figure><h2 id="7cce" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">利用时间模式</h2><p id="259a" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">反思我的方法，我认为最可以改进的地方是对时间维度的处理。我在 48 个通道中堆叠所有时间点的天真方法不允许我的模型通过时间从图像中正确地学习模式。我很想探索使用循环网络来学习这些时间模式。</p><p id="a74d" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">eo-learn 背后的团队自己提出了使用一个<em class="oh">时间全卷积网络</em> (TFCN)来实现这个目的:<a class="ae mj" href="https://sentinel-hub.com/sites/default/lps_2019_eolearn_TFCN.pdf" rel="noopener ugc nofollow" target="_blank">https://sentinel-hub . com/sites/default/LPS _ 2019 _ eol learn _ tfcn . pdf</a>。TFCNs 以秩为 4 的张量作为输入，并使用 3D 卷积来同时捕获空间和时间中的模式。</p><h2 id="41b5" class="nl kw iq bd kx nq nr dn lb ns nt dp lf lw nu nv lh ma nw nx lj me ny nz ll oa bi translated">使用无监督学习构建潜在表示</h2><p id="a89c" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果竞赛允许使用外部数据，那么探索本文【https://arxiv.org/abs/1805.02855<a class="ae mj" href="https://arxiv.org/abs/1805.02855" rel="noopener ugc nofollow" target="_blank">中描述的<em class="oh"> Tile2Vec </em>技术将会很有趣。这里的想法是通过使用三重损失设置无监督学习任务，从卫星图像生成区域的潜在向量表示。</a></p><p id="6c7f" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我很想知道其他竞争对手采用了什么方法。</p><h1 id="f327" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><em class="os">感谢</em></h1><p id="7cd6" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated"><em class="oh">我要感谢</em> <a class="ae mj" href="http://zindi.africa" rel="noopener ugc nofollow" target="_blank"> <em class="oh"> Zindi </em> </a> <em class="oh">的团队组织了如此有趣的挑战。我还要感谢 eo-learn 团队提供了如此有用的库，以及关于如何使用它的如此引人入胜的帖子。也感谢 fastai 社区为让深度学习变得更加平易近人和广泛可用所做的所有工作。最后，我要感谢</em><a class="ae mj" href="https://forums.fast.ai/u/ste/summary" rel="noopener ugc nofollow" target="_blank"><em class="oh">Stefano Giomo</em></a><em class="oh">对这个项目的所有投入。</em></p></div></div>    
</body>
</html>