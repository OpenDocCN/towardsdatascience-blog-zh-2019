<html>
<head>
<title>Reinforcement Learning: Bellman Equation and Optimality (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:贝尔曼方程和最优性(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3?source=collection_archive---------1-----------------------#2019-08-30">https://towardsdatascience.com/reinforcement-learning-markov-decision-process-part-2-96837c936ec3?source=collection_archive---------1-----------------------#2019-08-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="36cd" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">#优化 RL</h2><div class=""/><blockquote class="jw jx jy"><p id="2a84" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated">这个故事是上一个故事的延续，<a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da?source=friends_link&amp;sk=9ecb7b106959a3ae34396f7924bec50d"> <em class="iq">强化学习:马尔可夫决策过程(第一部分)</em> </a>故事中，我们谈到了如何为给定的环境定义 MDP。我们还谈到了贝尔曼方程，以及如何找到一个国家的价值函数和政策函数。<strong class="kc ja">在这个故事</strong>中，我们将更深入<strong class="kc ja">一步</strong>并了解<strong class="kc ja">贝尔曼期望方程</strong>，我们如何找到给定状态的<strong class="kc ja">最优值</strong>和<strong class="kc ja">最优策略函数</strong>，然后我们将定义<strong class="kc ja">贝尔曼最优方程</strong>。</p></blockquote><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/9ccf8ee1134596b5680101feb8ba48b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/1*5PGCR0jwd15kLhRCA09R1w.gif"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Google’s Parkour using Reinforcement Learning</figcaption></figure><p id="9dd7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们快速浏览一下这个故事:</p><ul class=""><li id="2ca8" class="lo lp iq kc b kd ke kh ki ll lq lm lr ln ls kx lt lu lv lw bi translated">贝尔曼期望方程</li><li id="69ea" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated">最优策略</li><li id="5008" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated">状态值函数的贝尔曼最优性方程</li><li id="97ad" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated">状态-行为价值函数的贝尔曼最优性方程</li></ul><p id="382f" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">所以，一如既往地拿起你的咖啡，直到你感到自豪才停下来。🤓</p><p id="72e0" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">先说，<strong class="kc ja">什么是贝尔曼期望方程？</strong></p><h2 id="432a" class="mc md iq bd me mf mg dn mh mi mj dp mk ll ml mm mn lm mo mp mq ln mr ms mt iw bi translated">贝尔曼期望方程</h2><p id="4067" class="pw-post-body-paragraph jz ka iq kc b kd mu kf kg kh mv kj kk ll mw kn ko lm mx kr ks ln my kv kw kx ij bi translated">快速回顾一下我们在前面的故事中谈到的贝尔曼方程:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/57a74ee6d2c311e54f3299e844680bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/0*p2EZAHDeVSZL23H4.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Equation for Value Function (State-Value Function)</figcaption></figure><p id="3e83" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">从上面的<strong class="kc ja">等式</strong>可以看出，一个状态的值可以<strong class="kc ja">分解</strong>为即时奖励(<strong class="kc ja">r【t+1】</strong>)<strong class="kc ja">加上</strong>后继状态的值(<strong class="kc ja">v【s(t+1)】</strong>)加上一个贴现因子(<strong class="kc ja"> 𝛾 </strong>)。这仍然代表贝尔曼期望方程。但是现在我们正在做的是找到一个特定状态<strong class="kc ja">服从</strong>某种策略(<strong class="kc ja"> π </strong>)的值。这就是贝尔曼方程和贝尔曼期望方程的区别。</p><p id="a7f5" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，我们可以将贝尔曼期望方程定义为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b0f0c626ebeb9ae4f628c340588f9119.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*PfqQf6Sp9oXuuAJ90eb94Q.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Expectation Equation for Value Function (State-Value Function)</figcaption></figure><p id="d86b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们称之为等式 1。上面的等式告诉我们，当我们在遵循某个政策(<strong class="kc ja"> π </strong> ) <strong class="kc ja">时，特定状态的值是由眼前的回报加上后继状态的值决定的。</strong></p><p id="f694" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">类似地，我们可以将我们的状态-动作值函数(Q-函数)表达如下:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/aa9030eaeb13b3e22a5a76ae72e6cf83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Fdbohxs6LedqarJjw65Qg.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Expectation Equation for State-Action Value Function (Q-Function)</figcaption></figure><p id="7892" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们称这个为等式 2。从上面的等式中，我们可以看到，一个状态的状态-动作值可以分解为我们在状态(<strong class="kc ja"> s </strong>执行某个动作并移动到另一个状态(<strong class="kc ja">s’</strong>)时获得的<strong class="kc ja">即时奖励</strong>，加上状态(<strong class="kc ja">s’</strong>)<strong class="kc ja">的状态-动作值相对于</strong>的贴现值，我们的代理人将从该状态向前采取某个动作(<strong class="kc ja"> a </strong>)。</p><p id="8e01" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated"><strong class="kc ja"> <em class="kb">深入贝尔曼期望方程:</em> </strong></p><p id="cbc6" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">首先，让我们借助备用图来理解状态-值函数的贝尔曼期望方程:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/a6f30553ff4af0d1245e2a4e4e3f4340.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*rvBTXXINBk4K7Y-ZGzb7zg.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-Value Function</figcaption></figure><p id="4940" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这个备份图描述了处于特定状态的价值。从状态 s 来看，我们有可能采取这两种行动。每个动作都有一个 Q 值(状态-动作值函数)。我们平均 Q 值，它告诉我们在一个特定的状态下有多好。基本上它定义了 V <strong class="kc ja"> π(s)。【看等式 1】</strong></p><p id="7b16" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，我们可以定义如下:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/faa62aceae687596a9db1754d7d0f29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*BHPIPkH4ecNGL-mqFdvOKQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Value of Being in a state</figcaption></figure><p id="d8c8" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这个等式也告诉我们状态-值函数和状态-动作值函数之间的联系。</p><p id="bc12" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，让我们看看状态-动作值函数的备份图:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/2ed8aa40d272842e36a69eefcbe15cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*vncwpTCOtrEN3ncpApPntg.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-action Value Function</figcaption></figure><p id="6934" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这张备份图表明，假设我们从采取一些行动(a)开始。因此，由于动作(a ),代理可能被环境吹到这些状态中的任何一个。因此，我们在问这个问题，<strong class="kc ja"> <em class="kb">采取行动有多好(a)？</em> </strong></p><p id="e523" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">我们再次对两种状态的状态值进行平均，加上即时奖励，这告诉我们采取特定的行动(a)有多好。这就定义了我们的 q <strong class="kc ja"> π(s，a)。</strong></p><p id="d39b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，我们可以这样定义:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7efcf770b715ccc8c2c587bb00f9b5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*VaTUGZR3_WHlVQrgcxzG1A.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Equation defining how good it is to take a particular action a in state s</figcaption></figure><p id="e2a9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">其中 P 是转移概率。</p><p id="739f" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，让我们将这些备份图拼接在一起，以定义状态值函数 V <strong class="kc ja"> π(s) </strong>:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/5752be28f65a376802032f885d818d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*hY1sGpzmpKqM7o1b4_nygw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-Value Function</figcaption></figure><p id="4b9b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">根据上图，如果我们的代理处于某个<strong class="kc ja">状态</strong>，并且从该状态假设我们的代理可以采取两个动作，由于哪个环境可能将我们的代理带到任何<strong class="kc ja">状态</strong>。请注意，我们的代理可能从状态<strong class="kc ja"> s </strong>采取行动的概率由我们的策略加权，并且在采取该行动后，我们到达任何状态(<strong class="kc ja">s’</strong>)的概率由环境加权。</p><p id="e868" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在我们的问题是，在采取一些行动并降落在另一个州(s ')之后，在另一个州(s)并遵循我们的政策(<strong class="kc ja"> π </strong>)有多好？</p><p id="c3ef" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这与我们之前所做的类似，我们将对后续状态的值(<strong class="kc ja">s’</strong>)进行平均，并使用我们的策略对一些转移概率(P)进行加权。</p><p id="915c" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，我们可以定义如下:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0a1bc0695beb3fd78af6291cf537e285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*9Xh-HfapxEPc9jeuMn4gFg.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">State-Value function for being in state S in Backup Diagram</figcaption></figure><p id="3d1a" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，让我们对状态-动作值函数 q <strong class="kc ja"> π(s，a) </strong>做同样的处理:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/77389cb812a378b080c61126bfb14f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*IGxGBCJcGUa9cZtEH-VEqw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-Action Value Function</figcaption></figure><p id="b98c" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这与我们在<strong class="kc ja">状态值函数</strong>中所做的非常相似，只是它是相反的，所以这个图基本上说我们的代理采取一些行动(<strong class="kc ja"> a </strong>)，因为环境可能会将我们带到任何状态(<strong class="kc ja"> s </strong>)，然后从那个状态我们可以选择采取任何行动(<strong class="kc ja">a’</strong>)，用我们策略的概率(<strong class="kc ja"> π </strong>)加权。同样，我们将它们平均在一起，这让我们知道始终遵循特定策略(<strong class="kc ja"> π </strong>)采取特定行动有多好。</p><p id="98c2" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，这可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nn"><img src="../Images/42d9330db76aba208fca73765f66da1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*unJD42XiA1VqoqPxLGJcKg.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">State-Action Value Function from the Backup Diagram</figcaption></figure><p id="b91c" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">所以，这就是我们如何为一个给定的 MDP 制定贝尔曼期望方程，找到它的状态-价值函数和状态-行为价值函数。<strong class="kc ja"> <em class="kb">但是，它并没有告诉我们 MDP 的最佳行为方式</em> </strong>。为此，让我们来谈谈什么是最优值和最优政策函数 T21。</p><h2 id="99cb" class="mc md iq bd me mf mg dn mh mi mj dp mk ll ml mm mn lm mo mp mq ln mr ms mt iw bi translated">最优值函数</h2><blockquote class="jw jx jy"><p id="c4fe" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">定义最佳状态值函数</strong></p></blockquote><p id="eb80" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">在 MDP 环境中，根据不同的政策，有许多不同的价值函数。<strong class="kc ja"> <em class="kb">最佳价值函数是与所有其他价值函数</em> </strong>相比产生最大值的函数。当我们说我们正在求解一个 MDP 时，实际上意味着我们正在寻找最优值函数。</p><p id="1b63" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">因此，数学上最优的状态值函数可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a5de9bb7aa05961552b011750834fcff.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*JgBQkp-t-vzn5cqjBw10hQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Optimal State-Value Function</figcaption></figure><p id="efd2" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">上式中，v∫(s)告诉我们，我们能从系统中得到的最大回报是什么。</p><blockquote class="jw jx jy"><p id="6043" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">定义最佳状态-动作值函数(Q-Function) </strong></p></blockquote><p id="285b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">类似地，<strong class="kc ja">最优状态-行动价值函数</strong>告诉我们，如果我们处于状态 s 并从那里开始采取行动 a，我们将获得的最大回报。</p><p id="7e54" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，它可以定义为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi np"><img src="../Images/137f7cd6eba0745c832b19e839e21a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*HDT7YYHewp2mDc5jFAWw8w.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Optimal State-Action Value Function</figcaption></figure><blockquote class="jw jx jy"><p id="833b" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">最优状态值函数</strong>:所有策略的最大值函数。</p><p id="492e" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">最优状态-动作值函数</strong>:是所有策略的最大动作值函数。</p></blockquote><p id="3e3e" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，让我们看看，什么是最优政策？</p><h2 id="1455" class="mc md iq bd me mf mg dn mh mi mj dp mk ll ml mm mn lm mo mp mq ln mr ms mt iw bi translated">最优策略</h2><p id="e0a8" class="pw-post-body-paragraph jz ka iq kc b kd mu kf kg kh mv kj kk ll mw kn ko lm mx kr ks ln my kv kw kx ij bi translated">在我们定义最优策略之前，让我们先了解一下，<strong class="kc ja"> <em class="kb">一种策略比另一种策略更好是什么意思？</em> </strong></p><p id="8736" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">我们知道，对于任何一个 MDP 来说，都有一个政策(<strong class="kc ja">【π】)</strong>比其他任何政策(<strong class="kc ja">【π】)都要好。但是怎么做呢？</strong></p><p id="8c30" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">如果针对所有状态的策略<strong class="kc ja"> π </strong>的价值函数大于针对所有状态的策略<strong class="kc ja">π’</strong>的价值函数，则我们说一个策略(<strong class="kc ja"> π) </strong>优于其他策略(<strong class="kc ja">π’</strong>)。直观上，它可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/ae2662d44febbfaf5ebc7de96c698346.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*dpJKumwuULjYKLaHjTxr7g.png"/></div></figure><p id="1b32" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，让我们定义<strong class="kc ja"> <em class="kb">最优策略:</em> </strong></p><blockquote class="jw jx jy"><p id="6462" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">最优策略</strong>是产生最优价值函数的策略。</p></blockquote><p id="4ada" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">请注意，在一个 MDP 中可以有多个最优策略。但是，<strong class="kc ja"> <em class="kb">所有的最优政策都实现相同的最优价值函数和最优状态-行动价值函数(Q-function) </em> </strong>。</p><p id="fd82" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，问题来了，我们如何找到最佳政策。</p><blockquote class="jw jx jy"><p id="b544" class="jz ka kb kc b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ij bi translated"><strong class="kc ja">寻找最佳策略:</strong></p></blockquote><p id="4fc2" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">我们通过最大化超过<strong class="kc ja"> <em class="kb"> q </em> * </strong> (s，a)即我们的最优状态-行为价值函数来找到最优策略。我们求解 q*(s，a)，然后选择给出最佳状态-动作值函数(q*(s，a))的动作。</p><p id="9ad4" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">上面的陈述可以表达为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/dd2cf9c5fde693483623d2351707e10c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*nw_fIgyOoFm0BMIS4Wb-Aw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Finding Optimal Policy</figcaption></figure><p id="a3d1" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这里说的是，对于状态 s，我们选择概率为 1 的动作 a，如果它给我们最大的 q*(s，a)。所以，如果我们知道 q*(s，a)，我们可以从中得到一个最优策略。</p><p id="efb9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">我们用一个例子来理解一下:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ns"><img src="../Images/3bb950903d46832ed4bf28f4e0203630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cqudfnL1_iZPXeZTIPLGZw.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Example for Optimal Policy</figcaption></figure><p id="042e" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">在这个例子中，红色弧线是最优策略，这意味着如果我们的代理遵循这条路径，它将从这个 MDP 中获得最大的回报。同样，通过查看每个状态的 q* 值，我们可以说我们的代理将采取的行动产生了最大的回报。因此，最优策略总是采取 q*值较高的行动(状态-行动值函数)。例如，在值为 8 的状态中，有值为 0 和 8 的 q*。我们的代理选择具有更大的<strong class="kc ja"> q* </strong>值的那个，即 8。</p><p id="ccd7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在问题来了，<strong class="kc ja"> <em class="kb">我们怎么求这些 q*(s，a)值？</em> </strong></p><p id="6041" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">这就是贝尔曼最优方程发挥作用的地方。</p><h2 id="0086" class="mc md iq bd me mf mg dn mh mi mj dp mk ll ml mm mn lm mo mp mq ln mr ms mt iw bi translated">贝尔曼最优方程</h2><blockquote class="nt"><p id="1c18" class="nu nv iq bd nw nx ny nz oa ob oc kx dk translated">最优值函数与贝尔曼最优方程递归相关。</p></blockquote><p id="be3b" class="pw-post-body-paragraph jz ka iq kc b kd od kf kg kh oe kj kk ll of kn ko lm og kr ks ln oh kv kw kx ij bi translated">贝尔曼最优方程和贝尔曼期望方程是一样的，但是唯一的区别是，我们不是取我们的代理可以采取的行动的平均值，而是取最大值的行动。</p><p id="83d2" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们借助备份图来理解这一点:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/352d2b0da8abae6473a51f09d7dc0b78.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*7p-nZZiF6v0u7shQleEVEQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup diagram for State-Value Function</figcaption></figure><p id="bdce" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">假设我们的代理处于状态 S，从该状态它可以采取两个动作(a)。因此，我们查看每个动作的动作值，并且<strong class="kc ja">不同于</strong>、贝尔曼期望方程、<strong class="kc ja">而不是</strong>取<strong class="kc ja">平均值</strong>，我们的代理采取具有<strong class="kc ja">更大 q*值</strong>的动作。这给了我们处于状态 s 的价值。</p><p id="574a" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，这可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2591a36daab633d74cdd30977149fb54.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*ixfRgSvNluDW5DaeAbTckA.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Optimality Equation for State-value Function</figcaption></figure><p id="cd42" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">同样，我们来定义<strong class="kc ja">状态-作用值函数(Q-Function) </strong>的贝尔曼最优性方程。</p><p id="a308" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们看看状态-动作值函数(Q 函数)的备份图:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c440a9eeefe01a6c7d4f4750c65a78c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*qGtg89UHLIy_SSIPFDbPYw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-Action Value Function</figcaption></figure><p id="8af9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">假设，我们的代理人在一些州 s 采取了行动 a。现在，它取决于环境，它可能会把我们吹到这些州中的任何一个(<strong class="kc ja">s’</strong>)。我们仍然取两个状态的平均值，但是唯一的<strong class="kc ja">差异</strong>是在贝尔曼最优方程中，我们知道每个状态的最优值<strong class="kc ja"/>。不像在贝尔曼期望方程中，我们只知道状态的值。</p><p id="d3b9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，这可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/334cf6e4a5d28db6ef1094d9b1a74f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*hEIbIonh4cbWu04MmMY5cw.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Optimality Equation for State-Action Value Function</figcaption></figure><p id="bbd2" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们再次缝合这些状态值函数的备份图:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi om"><img src="../Images/65efdd9552dfaf774a7c42c3b9a672cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*6YVaMjuJV2IuZb6y_Zpo2w.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-Value Function</figcaption></figure><p id="e6e7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">假设我们的代理处于状态 s，并从该状态采取了一些行动(a ),其中采取该行动的概率由策略<strong class="kc ja">加权</strong>。并且由于动作(a)，代理可能被吹到概率由环境加权的任何状态(<strong class="kc ja">s’</strong>)。<strong class="kc ja">为了找到状态 S 的值，我们简单地对状态(S’)的最优值进行平均</strong>。这给了我们处于状态 s 的价值。</p><p id="125f" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，这可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi on"><img src="../Images/30cdd67c52ac381dd3bae262b500007d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*Jntq2W9k565P-hLleMzlZA.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Optimality Equation for State-Value Function from the Backup Diagram</figcaption></figure><p id="0d12" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">等式中的最大值是因为我们最大化了代理在上弧中可以采取的行动。这个方程也显示了我们如何把 V*函数和它本身联系起来。</p><p id="58f0" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，让我们看看状态-动作值函数 q*(s，a)的贝尔曼最优方程:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/00f32e236e83041472e86ce97bb72255.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*2bMhBw2bc4pcvETmIQ-NFQ.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Backup Diagram for State-Action Value Function</figcaption></figure><p id="ff20" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">假设，我们的代理处于状态<strong class="kc ja"> s </strong>，它采取了一些动作(<strong class="kc ja"> a </strong>)。由于该动作，环境可能会将我们的代理带到任何状态(<strong class="kc ja">s’</strong>)，并且从这些状态我们可以<strong class="kc ja">最大化</strong>我们的代理将采取的动作，即选择具有<strong class="kc ja">最大 q*值</strong>的动作。我们回到顶点，这告诉我们动作 a 的值。</p><p id="2b85" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">数学上，这可以表示为:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div class="gh gi op"><img src="../Images/a055d8288490aa8766de85459a64c8cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*ps4FtXzAHS8fqMkQ-hVogA.png"/></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Bellman Optimality Equation for State-Action Value Function from the Backup Diagram</figcaption></figure><p id="7dd8" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">让我们看一个例子来更好地理解它:</p><figure class="la lb lc ld gt le gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oq"><img src="../Images/0020305b21b5c1597e1e580669742018.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lIIA5CczkzwXPWUmhK6DFA.png"/></div></div><figcaption class="lh li gj gh gi lj lk bd b be z dk">Example for Bellman Optimality Equation</figcaption></figure><p id="dd39" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">请看红色箭头，假设我们希望找到值为 6 的 state 的<strong class="kc ja">值</strong>(红色的<strong class="kc ja"/>)，我们可以看到，如果我们的代理选择脸书，我们将获得-1 的奖励，如果我们的代理选择学习，我们将获得-2 的奖励。为了找到红色状态的值，我们将使用状态值函数的<strong class="kc ja">贝尔曼最优方程，即<em class="kb">，考虑到其他两个状态具有最优值，我们将对两个动作取平均值并最大化(选择给出最大值的一个)</em>。因此，从图中我们可以看到，对于我们的红色州来说，去脸书得到的值是 5，去学习得到的值是 6，然后我们将这两个值最大化，得到的答案是 6。</strong></p><p id="35a7" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">现在，我们如何求解大型 MDP 的贝尔曼最优方程。为了做到这一点，我们使用<strong class="kc ja">动态编程算法</strong>，如<strong class="kc ja">策略迭代</strong>和值迭代，我们将在下一个故事中介绍，以及其他方法，如<strong class="kc ja"> Q-Learning </strong>和<strong class="kc ja"> SARSA </strong>，它们用于时间差异学习，我们将在未来的故事中介绍。</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="1a7b" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">太棒了！</p><p id="c3ca" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">恭喜你走到这一步！👍</p><p id="1284" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">希望这个故事能增加你对 MDP 的了解。很乐意在 insta gram<a class="ae ky" href="https://www.instagram.com/ayushsingh.__/" rel="noopener ugc nofollow" target="_blank">上与您联系。</a></p><p id="e837" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">谢谢你与我分享你的时间！</p><p id="4af6" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">如果你喜欢这个，请点击让我知道👏。它帮助我写更多！</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><h2 id="d1ec" class="mc md iq bd me mf mg dn mh mi mj dp mk ll ml mm mn lm mo mp mq ln mr ms mt iw bi translated">关于马尔可夫决策过程的第 1 部分、第 2 部分和第 3 部分:</h2><ul class=""><li id="803b" class="lo lp iq kc b kd mu kh mv ll oy lm oz ln pa kx lt lu lv lw bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-reinforcement-learning-markov-decision-process-44c533ebf8da">强化学习:马尔可夫决策过程(第一部分)</a></li><li id="2a48" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated">强化学习:贝尔曼方程和最优性(下)</li><li id="517a" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/reinforcement-learning-solving-mdps-using-dynamic-programming-part-3-b53d32341540">强化学习:使用动态规划解决马尔可夫决策过程</a>(第三部分)</li><li id="5b35" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated"><a class="ae ky" href="https://pub.towardsai.net/reinforcement-learning-monte-carlo-learning-dc9b49aa16bd" rel="noopener ugc nofollow" target="_blank">强化学习:蒙特卡罗学习</a></li></ul><p id="c0d9" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated">参考资料:</p><ul class=""><li id="d56a" class="lo lp iq kc b kd ke kh ki ll lq lm lr ln ls kx lt lu lv lw bi translated"><a class="ae ky" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/psych 209/Readings/suttonbartoiprlbook 2 nded . pdf</a></li><li id="817f" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated"><a class="ae ky" href="https://www.oreilly.com/library/view/hands-on-reinforcement-learning/9781788836524/" rel="noopener ugc nofollow" target="_blank">使用 Python 进行强化学习</a></li><li id="42f0" class="lo lp iq kc b kd lx kh ly ll lz lm ma ln mb kx lt lu lv lw bi translated">大卫·西尔弗的 DeepMind 强化学习课程</li></ul><p id="f281" class="pw-post-body-paragraph jz ka iq kc b kd ke kf kg kh ki kj kk ll km kn ko lm kq kr ks ln ku kv kw kx ij bi translated"><strong class="kc ja"> <em class="kb">待得深</em> </strong></p></div></div>    
</body>
</html>