<html>
<head>
<title>Reinforcement Learning: let’s teach a taxi-cab how to drive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:让我们教出租车司机如何驾驶</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-lets-teach-a-taxi-cab-how-to-drive-4fd1a0d00529?source=collection_archive---------13-----------------------#2019-07-28">https://towardsdatascience.com/reinforcement-learning-lets-teach-a-taxi-cab-how-to-drive-4fd1a0d00529?source=collection_archive---------13-----------------------#2019-07-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/153acd83683c993b557c84efb7e53d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2o9u-xzU8j74Q1DNN4tCug.png"/></div></div></figure><div class=""/><p id="997b" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">强化学习是机器学习的一个子领域，其任务不同于“标准”的学习方式。事实上，你希望你的强化算法从周围环境中从头开始学习，而不是获得历史数据并根据这些数据做出预测或推断。基本上，你希望它像你在类似情况下所做的那样(如果你想了解更多关于 RL 的结构，点击<a class="ae kz" rel="noopener" target="_blank" href="/reinforcement-learning-beyond-the-supervised-and-unsupervised-ways-b3cae32eef65?source=post_page---------------------------">这里</a>阅读我以前的文章)。</p><p id="2088" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在本文中，我将向您展示如何使用 Python 及其库 gym-OpenAI 实现 RL 解决方案，您可以通过在 Jupyter 控制台<em class="la">上运行 pip install gym </em>来轻松安装它。我要向你提出的问题如下:</p><figure class="lc ld le lf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lb"><img src="../Images/8f46fffbba6bebbe2a2163df0538f075.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zX8uQgbis5-BXJSmoP66kw.png"/></div></div></figure><p id="45b7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您的环境由一个 5x5 矩阵组成，其中每个单元都是您的出租车可以停留的位置。然后，你有 4 个坐标，分别代表上下车地点，分别是(0，0)，(0，4)，(4，0)，(4，3)(为了和 Python 语言的连贯性，第一个索引是 0 而不是 1)。我们将它们称为 R，G，Y，B，并分别用 0，1，2，3 来表示它们的位置。最后，有一名乘客既可以上车，也可以下车，还可以被运送(因此要花时间在出租车上)。具体来说，这个乘客想要到达 b 点。</p><p id="ceb2" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，如果我们导入我们的<em class="la">健身房</em>模块并初始化出租车环境，我们可以看到它复制了我们到目前为止所说的内容:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="7469" class="ll lm je lh b gy ln lo l lp lq">import gym<br/>env = gym.make("Taxi-v2").env<br/>env.render()</span></pre><figure class="lc ld le lf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lr"><img src="../Images/d5c6ff1850641c3a5a9acd24efc0b377.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*SzwlbwcQME6NFoVFbDCrhA.png"/></div></div></figure><p id="d088" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如您所见，我们的 5x5 空间有 4 个位置，其中蓝色字母代表当前乘客的位置，紫色字母代表下车位置。我们在那个空间中也有我们的出租车/代理，它是黄色的矩形，还有一些墙，用符号“|”表示。</p><p id="c4ad" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在有两个因素需要我们注意:国家和行动。</p><p id="1139" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们首先检查我们的行动。根据导入的模块，代理可以采取 6 种方式:</p><ul class=""><li id="1c3b" class="ls lt je kd b ke kf ki kj km lu kq lv ku lw ky lx ly lz ma bi translated">0:向下(南)</li><li id="3999" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">1:向上(北)</li><li id="4aab" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">2:向右(东)走</li><li id="e20d" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">3:向左(西)走</li><li id="89f4" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">4:拿起</li><li id="4954" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">5:下车</li></ul><p id="1eb4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">第二，我们有多少个州？嗯，至少 25 个:事实上，有一个 5x5 的空间，我们知道驾驶室可以简单地占用这些单元。此外，出租车也可以处于搭载或放下乘客的状态(不管它实际上是否在那里:记住出租车将通过尝试前进)，因此我们还有 4 种状态。最后，我们必须计算乘客实际上车、下车的状态(+ 4 个状态，因为乘客可能在的位置是 4 个)或只是被运送的状态(+ 1 个状态)。因此，总共有 5x5x4x5=500 个状态。</p><p id="6933" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们的代理的每个状态由一个向量值<em class="la">【出租车的行，出租车的列，乘客索引，目的地索引】</em>表示，因此用 0 到 499 之间的值进行编码。也就是说，我们可以像这样复制上一张图片的位置:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="4ff2" class="ll lm je lh b gy ln lo l lp lq">state = env.encode(4, 2, 3, 2) <br/>print("State:", state)env.s = state<br/>env.render()</span></pre><figure class="lc ld le lf gt iv gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/10fdf6925fded734a8206d8ffd085637.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*_5b4AK2V98RaYioR8IOXKA.png"/></div></figure><p id="8368" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如您所见，已知我们的出租车位于位置(4，2)，乘客索引=3，下车位置=2，我们可以推导出编码状态为 454。在接下来的实验中，我们将使用这个起点，但在深入之前，我们需要介绍最后一个要素:奖励系统。</p><p id="b0c4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">奖励系统是强化学习背后的主要思想:代理人在任何表现良好的时候都会得到奖励，否则就会受到负面奖励的“惩罚”。在这个特定的例子中，一旦创建了 env，就会创建一个嵌入式奖励表<em class="la"> P </em>。逻辑如下:</p><ul class=""><li id="5008" class="ls lt je kd b ke kf ki kj km lu kq lv ku lw ky lx ly lz ma bi translated">如果出租车正确接送乘客，奖励+20 分</li><li id="a491" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">如果出租车非法上落客，将被扣-10 分</li><li id="4a3e" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">不包括上述情况的每一步，减 1 分</li></ul><p id="4470" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们看看我们的状态 454 是什么样子的:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="936f" class="ll lm je lh b gy ln lo l lp lq">env.P[454]</span></pre><figure class="lc ld le lf gt iv gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/6649e24e8685e49fb36a07faabc82246.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*cmw5JmUIMWX4Gsk1n5ig8A.png"/></div></figure><p id="a1dd" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先要注意的是，我们的<em class="la"> P </em>表的每个条目都是一个字典，其结构为<code class="fe mi mj mk lh b">{action: [(probability, nextstate, reward, done)]}</code></p><ul class=""><li id="8274" class="ls lt je kd b ke kf ki kj km lu kq lv ku lw ky lx ly lz ma bi translated">动作:范围从 0 到 5</li><li id="04d0" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">概率:在这种情况下，总是为 1</li><li id="9169" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">Nextstate:这是动作完成时出现的状态</li><li id="a52e" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">奖励:与该行为相关的奖励/惩罚</li><li id="9f2f" class="ls lt je kd b ke mb ki mc km md kq me ku mf ky lx ly lz ma bi translated">done:如果为真，说明这一集结束了，否则不是。</li></ul><p id="e7f4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们试着读一下我们的结果:第一行告诉我们，如果我们往下走(动作 0 =南)，我们将保持在相同的位置，因为我们有一个边界，因此奖励是-1，情节没有结束；第二行，对应于 action=north，会把我们的滑行带向位置 354，但是奖励永远是-1，剧集没有结束。所有动作的推理都是一样的。注意，如果动作是上车或下车，由于出租车不在正确的位置(R，Y，G，B ),如在最后两行(对应于动作 4 和 5 ),它收到-10 的惩罚。</p><p id="8f0a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在是时候训练我们的算法了。我们要用的算法叫做 Q-learning。我已经在这篇文章中解释了<a class="ae kz" href="https://medium.com/dataseries/understanding-the-idea-behind-q-learning-63c666c8a8a2" rel="noopener">背后的思想，因此在这里我不再深入探讨。</a></p><p id="3826" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下代码解释了该过程:</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="62c1" class="ll lm je lh b gy ln lo l lp lq">import random</span><span id="690d" class="ll lm je lh b gy ml lo l lp lq"># setting yperparameters<br/>lr = 0.1  #learning rate<br/>gamma = 0.6 #discount factor<br/>epsilon = 0.1 #trade-off between exploration and exploitation</span><span id="a130" class="ll lm je lh b gy ml lo l lp lq">for i in range(1, 1000):  #we will see 1000 episodes<br/>    state = env.reset()  #let's reset our env</span><span id="5b5a" class="ll lm je lh b gy ml lo l lp lq">epochs, penalties, reward, = 0, 0, 0<br/>    done = False<br/>    <br/>    while not done:<br/>        if random.uniform(0, 1) &lt; epsilon:<br/>            action = env.action_space.sample() # explore action space<br/>        else:<br/>            action = np.argmax(q_table[state]) # exploit learned values</span><span id="c32c" class="ll lm je lh b gy ml lo l lp lq">next_state, reward, done, info = env.step(action) <br/>        <br/>        old_value = q_table[state, action]<br/>        next_max = np.max(q_table[next_state])<br/>        <br/>        new_value = (1 - alpha) * old_value + lr * (reward + gamma * next_max)<br/>        q_table[state, action] = new_value</span><span id="2746" class="ll lm je lh b gy ml lo l lp lq">if reward == -10:<br/>            penalties += 1</span><span id="92ee" class="ll lm je lh b gy ml lo l lp lq">state = next_state<br/>        epochs += 1<br/>        <br/></span></pre><p id="4c45" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，想象一下，你必须决定哪一个动作能最大化你的效用(翻译过来，这导致对位置 3 的乘客的最佳可能方式)。你的答案可能是北，因此行动 1:事实上，这将是到达你的乘客所在位置(4，3)的最快方式。我们的算法会怎么说？</p><pre class="lc ld le lf gt lg lh li lj aw lk bi"><span id="c5ca" class="ll lm je lh b gy ln lo l lp lq">np.argmax(q_table[454]) #argmax function return the position of the <br/>#maximum value among those in the vector examined</span></pre><p id="17e3" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi">1</p><p id="8d2c" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如您所见，<em class="la"> argmax </em>函数返回位置 1，这对应于动作“北”。因此，对于每一个位置，我们的 q 表会告诉我们哪一个动作可以最大化当前和未来的回报。</p></div></div>    
</body>
</html>