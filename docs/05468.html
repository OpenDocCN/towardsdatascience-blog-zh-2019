<html>
<head>
<title>Locating and Identifying Honeycomb Cells Using Object Detection in Custom Vision AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用自定义视觉人工智能中的对象检测来定位和识别蜂窝单元</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/locating-and-identifying-honeycomb-cells-using-object-detection-in-custom-vision-ai-9b314532791b?source=collection_archive---------33-----------------------#2019-08-12">https://towardsdatascience.com/locating-and-identifying-honeycomb-cells-using-object-detection-in-custom-vision-ai-9b314532791b?source=collection_archive---------33-----------------------#2019-08-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="dc0e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由<a class="ae kl" href="http://iqbioreu.uprrp.edu/index.php/program/" rel="noopener ugc nofollow" target="_blank">智商生物项目</a>在<a class="ae kl" href="http://www.uprrp.edu/" rel="noopener ugc nofollow" target="_blank"> UPR-RP </a>举办的<a class="ae kl" href="https://sites.google.com/a/upr.edu/iq-hackathon/" rel="noopener ugc nofollow" target="_blank">智商黑客马拉松</a>参赛</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/f949702a7453189b08c0fcea0cc50bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Y_ofKF-eAey0m80d"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Photo by <a class="ae kl" href="https://unsplash.com/@nwfandi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Scott Hogan</a> on <a class="ae kl" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b6b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从 7 月 31 日到 8 月 1 日，波多黎各有一场黑客马拉松，由<a class="ae kl" href="http://iqbioreu.uprrp.edu/index.php/program/" rel="noopener ugc nofollow" target="_blank">跨学科定量生物学项目</a>在波多黎各<a class="ae kl" href="http://www.uprrp.edu/" rel="noopener ugc nofollow" target="_blank">大学里约皮得拉斯校区</a> (UPRRP)主办。黑客马拉松在波多黎各 Bayamon 的一个合作空间<a class="ae kl" href="https://engine-4.com/" rel="noopener ugc nofollow" target="_blank"> Engine 4 </a>举行。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi lc"><img src="../Images/d1404f430a0d638a9d621db431967a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZ_cO1VjhjIypsX3R-IzLg.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Hackathon Flyer</figcaption></figure><p id="a734" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">黑客马拉松的重点是寻找跨学科和定量(IQ)生物学问题的创造性解决方案。</p><p id="abf3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我和<a class="ae kl" href="https://www.linkedin.com/in/armasalejandro/" rel="noopener ugc nofollow" target="_blank">Alejandro Armas</a>(<a class="ld le ep" href="https://medium.com/u/9adc9b6f0534?source=post_page-----9b314532791b--------------------------------" rel="noopener" target="_blank">Alejandro Armas</a>)<a class="ae kl" href="https://www.linkedin.com/in/carlos-navea/" rel="noopener ugc nofollow" target="_blank">Carlos Navea</a>和<a class="ae kl" href="https://www.linkedin.com/in/giovanni-cortes-339983189/" rel="noopener ugc nofollow" target="_blank"> Giovanni Cortes </a>一起参加了这次黑客马拉松，他们都是 IQ 生物学暑期实习的聪明参与者。我们的团队名称是“蜂巢”。我们得到了 UPRRP 大学计算机科学系副教授雷米·梅里特教授的指导。</p><p id="7691" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们解决了两个问题:</p><ul class=""><li id="acac" class="lf lg iq jp b jq jr ju jv jy lh kc li kg lj kk lk ll lm ln bi translated">通过 webapp 简化研究人员的数据输入。</li><li id="d330" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">使用对象检测自动蜂窝单元识别和计数。</li></ul><p id="6cde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文主要讨论后者。</p><p id="a0d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们为黑客马拉松实现的代码可以在 https://github.com/alejandroarmas/PRhacks 的 GitHub 上找到。用于准备物体检测数据的代码在<code class="fe lt lu lv lw b">mlTool</code>文件夹中。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="e4d3" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">问题陈述</h1><p id="c610" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">波多黎各大学蜜蜂研究人员的一些职责是拍摄蜂窝并记录与蜂窝相关的各种变量，例如:蜂窝中有多少花蜜，有多少蜜蜂，有多少花粉，有多少幼虫，有多少密封的巢(容纳蜂蜜)，有多少蛹。由于计算这些的任务过于繁重，研究人员记录了这些近似值(例如，5%的蜂巢有蜂蜜)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/de1eb2d346b9bd858caa85da874c96a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*r6bzlTPQWOSRDDOOrCL7UQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Types of honeycomb cells. Glossy cells contain nectar. Sealed cells contain honey.</figcaption></figure><p id="2b8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于黑客马拉松，我们选择使用对象检测来解决计数问题。通过对象检测，我们可以自动识别不同类型的细胞并进行计数，以便研究人员可以记录这些细胞的准确数据。</p><p id="7ac7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对象检测是深度学习和计算机视觉的一个分支，涉及自动识别图像中感兴趣的项目，并确定围绕它们的矩形边界框(bbox)的坐标。然后可以使用坐标画出边界框。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0203d63799019b2334775077d41f9d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Qk4bRD9AP5hQmQ3Z.jpeg"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Object detection example. (<a class="ae kl" rel="noopener" target="_blank" href="/beginners-guide-to-object-detection-algorithms-6620fb31c375">source</a>)</figcaption></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="a3a2" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">数据注释</h1><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nj"><img src="../Images/dd7138813c8179db7f2a0ad8fc6c82c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eeWxWZSpXAGo_j9530M_pQ.jpeg"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">One of the images taken by the researchers. We did not annotate it completely due to time constraints.</figcaption></figure><p id="a645" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在机器学习(ML)和深度学习(DL)中，对数据进行注释或标记意味着将其标记为属于某一类。当使用对象检测时，这意味着确定围绕图像中感兴趣区域的矩形的坐标(或者在使用注释工具时围绕感兴趣区域绘制矩形)，并保存矩形的坐标和该区域所属的类。我们希望 ML 或 DL 模型从带注释/标签的数据中学习，并用这些注释/标签预测新数据。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3fe080356be55c5129f61d10cc3d82b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*e-1BfKq8CSbHmVjZDPQJCw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">One labeled cell.</figcaption></figure><p id="83d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们在研究人员使用名为<a class="ae kl" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> labelImg </a>的开源标记工具拍摄的蜂窝图像中标记了花蜜和空细胞，该工具将注释保存到 XML 文件中。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nl"><img src="../Images/b119dd91d96c6eab08ec10bb37910569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6cilLy1HERDl6w3NoJkTnw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">LabelImg’s GUI.</figcaption></figure><p id="dbc6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们只关注 nectar 和 empty 类，以便在实现解决方案之前不要花太多时间来注释数据，还因为它们是我们注释的蜂巢中最丰富的单元。最理想的情况是，我们应该标记所有的种类:密封的、空的、花蜜的、蜜蜂的、蛹的、幼虫的和花粉的。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Preview of one of the label XML files. Bounding box coordinates are within the &lt;object&gt; tags.</figcaption></figure><p id="36c5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注</strong>:在黑客马拉松期间，我们将花蜜细胞标记为蜂蜜细胞。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="no nn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Oops.</figcaption></figure><p id="28c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们认为光泽细胞有蜂蜜，而事实上它们有花蜜(从观看<a class="ae kl" href="https://www.youtube.com/watch?v=6LOhkHHpnHU" rel="noopener ugc nofollow" target="_blank">这个</a>视频中发现)。文章的其余部分将保持原来的约定，因为它反映了黑客马拉松结束之日的现实。通过在注释(XML)文件中找到单词“honey”的所有实例并用单词“nectar”替换它们，可以很容易地修复注释。我们将在未来的工作中实现这一修复。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="a6e4" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">数据预处理</h1><h2 id="06e3" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">阅读标签</h2><p id="e23c" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">使用 labelImg 创建的注释以 XML 格式保存在名为<code class="fe lt lu lv lw b">labeled_data</code>的文件夹中。我们开发代码是为了以一种更容易使用的格式存储这些标签。</p><p id="f1ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的代码遍历了<code class="fe lt lu lv lw b">labeled_data</code>中的所有 XML 文件，其中保存了图像及其 XML 注释。然后，它使用库<code class="fe lt lu lv lw b">BeautifulSoup</code>访问 XML 中每个边界框的相关信息。然后，它将信息存储到 dataframe 中，其中每行对应一个边界框，列的标题为:image(图像的相对路径)、label(边界框的标记)、xmin(左 x 点)、xmax(右 x 点)、ymin(顶部 y 点)和 ymax(底部 y 点)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/b8e2b24755bd96c3d265cdf9576aefda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*2GtNoz2x3MUYMax7mRRSBA.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Diagram example of an image with a bbox. xmin, xmax, ymin and ymax define all edges of the bbox.</figcaption></figure><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Code for reading in annotations from all XML files in labeled_data folder and storing them into a dataframe.</figcaption></figure><p id="05fd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在黑客马拉松中，我们标注了 562 个“蜂蜜”单元格和 619 个“空”单元格。</p><p id="cf03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用于读取使用 labelImg 创建的标签的代码存在于 GitHub repo 的<code class="fe lt lu lv lw b">mlTool</code>文件夹中，作为一个名为<code class="fe lt lu lv lw b">read_labels.ipynb</code>的 Jupyter 笔记本。</p><h2 id="5ddd" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">从大图像创建小图像</h2><p id="065b" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">研究人员拍摄的图像非常大，分辨率很高。例如，之前显示的照片的宽度为 4608 像素，高度为 3456 像素。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/53db2b13f29c5ce983ca55c63e61d349.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*YQSXJrnsETynOP004dJlFg.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Original image properties.</figcaption></figure><p id="33ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对象检测模型最适合大量较小的图像，因此我们决定从大图像中创建较小的 300 x 300 像素图像。让我们称这些更小的图像为“块”。我们只保留包含至少一个边界框的块。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Separate each image into many 300 x 300 px images, and adjust bounding boxes to the new dimensions.</figcaption></figure><p id="4fff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可以从原始图像中提取 192 个大小为 300×300 px 的块。90 个组块包含边界框。</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="4706" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">目标检测模型</h1><p id="339c" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">由于时间限制，我们使用了 Azure 的自定义视觉 AI 来快速训练一个对象检测模型。</p><p id="e5b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们无法确定在在线 GUI 中导入已经标记的数据来训练对象检测模型的方法，所以我们选择使用 API。</p><h2 id="0ff4" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">以自定义视觉可接受的格式保存标签</h2><p id="9d51" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">自定义视觉人工智能期望标记的数据是一种特殊的格式。左上点和右下点在 0 和 1 之间标准化。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="od nn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">This is the one thing that made us not start training the model before the hackathon finish time.</figcaption></figure><p id="0c0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些是文档中的规范:</p><blockquote class="oe of og"><p id="77dd" class="jn jo oh jp b jq jr js jt ju jv jw jx oi jz ka kb oj kd ke kf ok kh ki kj kk ij bi translated">这些区域在标准化坐标中指定边界框，并且坐标以左、上、宽、高的顺序给出。(<a class="ae kl" href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></blockquote><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="nm nn l"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Code for extracting chunks from images present in the labeled_data folder.</figcaption></figure><p id="5a17" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">左相当于 xmin，右相当于 ymin。我们通过减去 xmax 和 xmin 得到一个边界框的宽度，通过减去 ymax 和 ymin 得到它的高度。</p><p id="e69a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将标签保存为 custom vision AI 接受的格式的代码存在于 GitHub repo 的<code class="fe lt lu lv lw b">mlTool</code>文件夹中，作为标题为<code class="fe lt lu lv lw b">image_chunks.ipynb</code>的 Jupyter 笔记本。</p><h2 id="fc54" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">创建对象检测模型</h2><p id="f40c" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">我们使用定制的 vision AI Python 库<code class="fe lt lu lv lw b">azure-cognitiveservices-vision-customvision</code>连接到 Python 中的 API，创建一个项目，并将图像和边界框导入到项目中。我们通过访问在线界面验证了它的工作原理。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ol"><img src="../Images/55f4d9185b51dfcdd73e454cac040ae1.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*b0IirpAgvC1cBChDGm0JMA.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">We added 619 bounding boxes belonging to the ‘empty’ class, and 562 belonging to the ‘honey’ class.</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi om"><img src="../Images/c2afb24418106d859cfd692f67c7d4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40OV9AiAX7Hke-of4M43oQ.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Custom Vision projects home screen.</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi on"><img src="../Images/6a4d8f8f2b8bf4ab075a96b19589866f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0l2ed_rKbJQ3maHYKcyjug.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">View when Honeycomb project is selected.</figcaption></figure><p id="a1ef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">模型在 90 幅大小为 300×300 像素的图像上进行训练。58 幅图像包含“空”类的边界框，53 幅图像包含“蜜”类的边界框(一幅图像可能包含不止一个类)。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ba8e2a418c9888e8a7da3a0a7c262933.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*CbICzLywR4r7wxVpNn1chQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Image count per tag in Custom Vision AI interface.</figcaption></figure><p id="fa1a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在点击和查看图像时，我们能够在视觉上确认我们正确地将标签转换为 Custom Vision AI 接受的格式。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7c4be6391a157d84db30217873422c3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*gy7eLX1lDOrvr3-GIcp_CQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">View when an image is clicked. Bounding boxes and their labels are present.</figcaption></figure><p id="a03e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这方面的代码存在于一个名为<code class="fe lt lu lv lw b">Azure_Model.ipynb</code>的 Jupyter 笔记本的<code class="fe lt lu lv lw b">mlTool</code>文件夹中的存储库中。</p><h2 id="8dad" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">火车</h2><p id="1092" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">为了训练我们在 Custom Vision AI 中设置的模型，我们访问了<a class="ae kl" href="https://www.customvision.ai/" rel="noopener ugc nofollow" target="_blank"> customvision.ai </a>，登录到我们的帐户，进入我们使用 API 创建的项目，并选择右上角名为“训练”的绿色按钮。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/275126ca5a0d69213481e82fe0ad83c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*1l5gRu0A1wXC573CeGhcYQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Click the green button titled “Train” in order to train the model.</figcaption></figure><p id="9039" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">出现一个模式窗口，要求在两种培训类型之间进行选择:快速培训和高级培训。选择高级培训可以选择培训的小时数，但会产生费用。我们选择了快速培训，以避免收费并快速获得结果。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f5d34848f47cf8ac64e0344079806c35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*tCyF6U0rK_uwjWk6QP93_A.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">We chose fast training due to time constraints. For better performance, choose advanced training.</figcaption></figure><p id="361f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">快速训练花了几分钟。</p><h2 id="4d54" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">表演</h2><p id="e5d2" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">默认情况下，Custom Vision AI 将基于 50%的概率阈值和 30%的重叠阈值显示模型的性能。这些值可以用可用的刻度盘来修改。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d182506c9a03f04fe120ec1e5facefd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*EtMA65K3Xj8ANgfcVwIKGQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">By default, probability threshold is 50% and overlap threshold is 30%. Changing these changes the scores.</figcaption></figure><p id="6018" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用默认的概率和重叠阈值，该模型的准确率为 48%，召回率为 31.9%，mAP 为 43.0%。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi os"><img src="../Images/92c79ce872038c06453178945194f393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3J07CKl-eKk6fg3Kf5nePw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Visual representation of scores. Precision is 48%, recall is 31.9%, and mAP is 43%.</figcaption></figure><p id="bc19" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自定义视觉人工智能还提供了每类得分的性能。在“蜂蜜”类上，该模型的准确率为 55.9%，召回率为 24.8%，平均准确率为 50.6%。在“空”类上，它的准确率为 41.2%，召回率为 48.3%，A.P .为 35.5%。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b742d21a6a7dd3380536c3678d9539ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*NHofU1pmikhYtfpUIfVTRw.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Performance per tag. Performed better in identifying the ‘honey’ class.</figcaption></figure><p id="206e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“蜂蜜”类比“空”类得分更高。“空的”班级比“有蜂蜜的”班级的回忆率高。</p><p id="fb20" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些分数可以通过更多的数据(图像和注释)和更多的迭代来提高。重要的是下次不要让感兴趣的区域未被标记，因为模型可能会因此而混淆(它会认为这是背景噪声)。</p><h2 id="cabb" class="np mf iq bd mg nq nr dn mk ns nt dp mo jy nu nv ms kc nw nx mw kg ny nz na oa bi translated">预测</h2><p id="6a5d" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">我们对用于训练的蜂巢图像的一个微小片段进行了截图，并通过点击自定义视觉 AI 界面右上角的按钮“快速测试”并上传截图来对其进行预测。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9564688acac02c028082e324735883aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*etxA9cF0qfxzNwjbcJ2ObQ.png"/></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">We clicked on the button titled ‘Quick Test’ in order to get the predictions for an image.</figcaption></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ov"><img src="../Images/2cd6f8aa5032c8a1c8cf1c6701dfea74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*961fY0S8Aew7Ad8ncWYFJw.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Predictions with default probability threshold of 15%.</figcaption></figure><p id="db77" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型仅预测了图像中总共约 27 个蜂房中的 4 个蜂房，其概率超过 15%。</p><p id="ea24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将概率阈值降低到 2%,出现了更多正确的边界框。正确边界框的数量从 4 个增加到 19 个。这意味着大多数预测的蜂蜜细胞的概率低于 15%。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ow"><img src="../Images/d9730d68b2d950857f6820a4f082ad7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MdyCaYjnVxjaJVbfchD03g.png"/></div></div><figcaption class="ky kz gj gh gi la lb bd b be z dk">Predictions at 2% probability threshold. Most predicted honey cells have probabilities below 15%.</figcaption></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="cc30" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">结果和结论</h1><p id="4d00" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">对于黑客马拉松，我们注释了一小部分数据，处理了注释和图像，以自定义视觉 AI 接受的格式保存了注释，并使用自定义视觉 AI 训练了一个对象检测模型</p><p id="ef56" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通过 90 幅大小为 300 x 300 像素的图像，619 个用于“空”类的边界框，562 个用于“蜜”类的边界框，并且在仅一次迭代的训练之后，Custom Vision AI 实现了 48%的精度、31.9%的召回率和 43%的 mAP。对于“蜂蜜”类，定制视觉人工智能的准确率为 55.9%，召回率为 24.8%，平均准确率(A.P .)为 50.6%。对于“空”类，Custom Vision AI 的准确率为 41.2%，召回率为 48.3%，A.P .为 35.5%。</p><p id="f2a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于训练数据包含未标记的感兴趣对象，模型的性能受到阻碍。这些分数将随着完全标记的数据、附加数据和更多的训练迭代而提高。</p><p id="acf6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于蜂窝的性质，大多数边界框在某种程度上重叠。重叠的边界框可能会导致性能问题。我们应该考虑语义分割。</p><h1 id="b62b" class="me mf iq bd mg mh ox mj mk ml oy mn mo mp oz mr ms mt pa mv mw mx pb mz na nb bi translated">经验教训</h1><p id="4011" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">如果你希望为黑客马拉松实现深度学习解决方案，我建议你在电脑上安装/设置一个对象检测架构，并在黑客马拉松之前学习如何使用它，这样一旦你标记了数据，你就可以快速使用它。SSD、<a class="ae kl" href="https://github.com/matterport/Mask_RCNN/releases" rel="noopener ugc nofollow" target="_blank"> MaskRCNN </a>和 YOLO 是很好的开源对象检测架构。</p><p id="f506" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至少一名团队成员应该专注于编写图像处理的代码，而其他成员则专注于为概念证明添加足够的数据。应该为数据注释定义一个时间限制和最小数量的注释，以免将注释时间延长太久。</p><p id="60b0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">蜂窝图像应该被裁剪成较小的片段，以确保所有单元都被标记，从而防止混淆模型。</p><h1 id="d12c" class="me mf iq bd mg mh ox mj mk ml oy mn mo mp oz mr ms mt pa mv mw mx pb mz na nb bi translated">后续步骤</h1><p id="4842" class="pw-post-body-paragraph jn jo iq jp b jq nc js jt ju nd jw jx jy ne ka kb kc nf ke kf kg ng ki kj kk ij bi translated">为了继续并改进该项目，我们需要:</p><ul class=""><li id="622c" class="lf lg iq jp b jq jr ju jv jy lh kc li kg lj kk lk ll lm ln bi translated">标记更多图像</li><li id="55fe" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">可能选择不同的标注工具</li><li id="21ff" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">考虑语义分割</li><li id="6933" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">选择不同的对象检测架构，如 SSD 或 MaskRCNN</li><li id="c42b" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">更多时代的训练模型</li><li id="6ca6" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">评估模型性能并加以改进</li><li id="4e88" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">创建一个 API 来服务模型</li><li id="73ad" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">将模型整合到研究人员可以轻松使用的应用程序中</li><li id="3bc3" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk lk ll lm ln bi translated">合并数据库</li></ul></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><p id="8eda" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢您的阅读！</p></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h1 id="04de" class="me mf iq bd mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz na nb bi translated">参考</h1><ol class=""><li id="7e03" class="lf lg iq jp b jq nc ju nd jy pc kc pd kg pe kk pf ll lm ln bi translated"><a class="ae kl" href="https://en.wikipedia.org/wiki/Honeycomb" rel="noopener ugc nofollow" target="_blank">维基百科:蜂巢</a></li><li id="f0fa" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk pf ll lm ln bi translated">Youtube:福克斯总统参观蜜蜂研究实验室</li><li id="130b" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk pf ll lm ln bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/beginners-guide-to-object-detection-algorithms-6620fb31c375">中级:目标检测算法入门指南</a></li><li id="7b0f" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk pf ll lm ln bi translated"><a class="ae kl" href="https://www.customvision.ai/" rel="noopener ugc nofollow" target="_blank">定制视觉 AI </a></li><li id="5b4e" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk pf ll lm ln bi translated"><a class="ae kl" href="https://www.henkboelman.com/object-detection-with-microsoft-custom-vision/" rel="noopener ugc nofollow" target="_blank"> Henk Boelman:使用微软定制视觉进行物体检测</a></li><li id="dbf2" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk pf ll lm ln bi translated"><a class="ae kl" href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/python-tutorial-od" rel="noopener ugc nofollow" target="_blank">快速入门:使用自定义视觉 Python SDK 创建对象检测项目</a></li><li id="7aad" class="lf lg iq jp b jq lo ju lp jy lq kc lr kg ls kk pf ll lm ln bi translated"><a class="ae kl" href="https://heartbeat.fritz.ai/a-2019-guide-to-semantic-segmentation-ca8242f5a7fc" rel="noopener ugc nofollow" target="_blank">2019 年语义分割指南</a></li></ol></div></div>    
</body>
</html>