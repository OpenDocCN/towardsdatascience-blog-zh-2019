<html>
<head>
<title>Classify A Rare Event Using 5 Machine Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 5 种机器学习算法对罕见事件进行分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classifying-rare-events-using-five-machine-learning-techniques-fab464573233?source=collection_archive---------5-----------------------#2019-10-19">https://towardsdatascience.com/classifying-rare-events-using-five-machine-learning-techniques-fab464573233?source=collection_archive---------5-----------------------#2019-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e3a5" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习:监督学习</h2><div class=""/><div class=""><h2 id="b27d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">哪一种最适合不平衡数据？有什么权衡吗？</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/334c7e5d83565b003c4134570d7dc7a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oneAF1ru_QWnIb775r0wmw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@franckinjapan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Franck V.</a> on <a class="ae lh" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d2dc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">* * * * * *最近更新于 2019 年 12 月 28 日***** </strong></p><blockquote class="me"><p id="1c5f" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">机器学习是数据科学的皇冠；</p><p id="f0a1" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">监督学习是机器学习皇冠上的宝石。</p></blockquote><h1 id="748a" class="mo mp it bd mq mr ms mt mu mv mw mx my ki mz kj na kl nb km nc ko nd kp ne nf bi translated">背景</h1><p id="cfa8" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">几年前，《哈佛商业评论》发表了一篇文章，标题是“<a class="ae lh" href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century" rel="noopener ugc nofollow" target="_blank">数据科学家:21 世纪最性感的工作</a>自从它发布以来，数据科学或统计部门受到大学生的广泛追捧，数据科学家(书呆子)第一次被称为性感。</p><p id="e976" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于一些行业，数据科学家已经重塑了公司结构，并将许多决策重新分配给“一线”员工。能够从数据中产生有用的商业见解从未如此容易。</p><p id="8d48" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">据吴恩达(<a class="ae lh" href="https://www.deeplearning.ai/machine-learning-yearning/" rel="noopener ugc nofollow" target="_blank">机器学习向往</a>，第 9 页)，</p><blockquote class="me"><p id="aff9" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">监督学习算法为行业贡献了大部分价值。</p></blockquote><p id="9a9c" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">SL 为什么会产生这么大的商业价值，这一点毋庸置疑。银行用它来检测信用卡欺诈，交易员根据模型告诉他们的内容做出购买决定，工厂通过生产线过滤有缺陷的产品(根据吴恩达的说法，这是人工智能和人工智能可以帮助传统公司的一个领域)。</p><p id="d4d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这些业务场景有两个共同的特征:</p><ol class=""><li id="dcae" class="nq nr it lk b ll lm lo lp lr ns lv nt lz nu md nv nw nx ny bi translated"><strong class="lk jd">二元结果</strong>:欺诈 VS 不欺诈，买 VS 不买，有缺陷 VS 无缺陷。</li><li id="a7c2" class="nq nr it lk b ll nz lo oa lr ob lv oc lz od md nv nw nx ny bi translated"><strong class="lk jd">不平衡的数据分布</strong>:一个多数群体对一个少数群体。</li></ol><p id="7867" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">正如吴恩达最近指出的那样，<a class="ae lh" href="https://info.deeplearning.ai/the-batch-self-driving-cars-that-cant-see-pedestrians-evolutionary-algorithms-fish-recognition-fighting-fraud-?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=79686634&amp;_hsenc=p2ANqtz-8JYm57kQehRZzewKP7GRcg1KzCEiTzMaPaYmA1fKuzs_IU9AoooG7ABIqqRLuOubgAU8r8pBVED-l1D6mOoCjVrF6lYw&amp;_hsmi=79686634" rel="noopener ugc nofollow" target="_blank">小数据</a>、<a class="ae lh" href="https://info.deeplearning.ai/the-batch-deepmind-masters-starcraft-2-ai-attacks-on-amazon-a-career-in-robot-management-banks-embrace-bots-1?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=79686634&amp;_hsenc=p2ANqtz-8JYm57kQehRZzewKP7GRcg1KzCEiTzMaPaYmA1fKuzs_IU9AoooG7ABIqqRLuOubgAU8r8pBVED-l1D6mOoCjVrF6lYw&amp;_hsmi=79686634" rel="noopener ugc nofollow" target="_blank">健壮性</a>和<a class="ae lh" href="https://blog.deeplearning.ai/blog/the-batch-google-achieves-quantum-supremacy-amazon-aims-to-sway-lawmakers-ai-predicts-basketball-plays-face-detector-preserves-privacy-problems-beyond-bounding-box?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;utm_content=80323254&amp;_hsenc=p2ANqtz-_88W5PvaNASb06SH_AW1uzL2ETjfCivTbmXc7m87jMcF4rrMG42U9qp7EATDPRM-rxHm0biLE3yMyHebUyR-pMaLZm2A&amp;_hsmi=80323254" rel="noopener ugc nofollow" target="_blank">人为因素</a>是人工智能项目成功的三大障碍。在一定程度上，我们的具有一个少数群体的罕见事件问题也是一个小数据问题:<strong class="lk jd">ML 算法从多数群体学习更多，可能容易对小数据群体进行错误分类。</strong></p><p id="defe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是价值百万的问题:</p><blockquote class="me"><p id="5ec6" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">对于这些罕见事件，哪种 ML 方法表现更好？</p><p id="a507" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">什么指标？</p><p id="1cd5" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">权衡？</p></blockquote><p id="6ee1" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">在这篇文章中，我们试图通过将 5 ML 方法应用于一个真实的数据集来回答这些问题。</p><p id="bae4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="oe">完整描述及原始数据集，请查看原始</em> <a class="ae lh" href="https://archive.ics.uci.edu/ml/datasets/bank+marketing" rel="noopener ugc nofollow" target="_blank"> <em class="oe">数据集</em></a><em class="oe">；完整的 R 代码请查看我的</em><a class="ae lh" href="https://github.com/LeihuaYe/Machine-Learning-Classification-for-Imbalanced-Data" rel="noopener ugc nofollow" target="_blank"><em class="oe">Github</em></a><em class="oe">。</em></p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="3126" class="mo mp it bd mq mr om mt mu mv on mx my ki oo kj na kl op km nc ko oq kp ne nf bi translated">商业问题</h1><p id="8c16" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">葡萄牙的一家银行实施了一项新银行服务(定期存款)的营销策略，并想知道哪些类型的客户订购了该服务。因此，银行可以调整其营销策略，并在未来瞄准特定人群。数据科学家已经与销售和营销团队合作，提出统计解决方案来识别未来的用户。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="8b3e" class="mo mp it bd mq mr om mt mu mv on mx my ki oo kj na kl op km nc ko oq kp ne nf bi translated">r 实现</h1><p id="aea7" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">下面是模型选择和 R 实现的管道。</p><h2 id="ebde" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> 1。导入、数据清理和探索性数据分析</strong></h2><p id="9f12" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">让我们加载并清理原始数据集。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="a93a" class="or mp it pd b gy ph pi l pj pk">####load the dataset<br/>banking=read.csv(“bank-additional-full.csv”,sep =”;”,header=T)</span><span id="9460" class="or mp it pd b gy pl pi l pj pk">##check for missing data and make sure no missing data<br/>banking[!complete.cases(banking),]</span><span id="a877" class="or mp it pd b gy pl pi l pj pk">#re-code qualitative (factor) variables into numeric<br/>banking$job= recode(banking$job, “‘admin.’=1;’blue-collar’=2;’entrepreneur’=3;’housemaid’=4;’management’=5;’retired’=6;’self-employed’=7;’services’=8;’student’=9;’technician’=10;’unemployed’=11;’unknown’=12”)</span><span id="9890" class="or mp it pd b gy pl pi l pj pk">#recode variable again<br/>banking$marital = recode(banking$marital, “‘divorced’=1;’married’=2;’single’=3;’unknown’=4”)</span><span id="878a" class="or mp it pd b gy pl pi l pj pk">banking$education = recode(banking$education, “‘basic.4y’=1;’basic.6y’=2;’basic.9y’=3;’high.school’=4;’illiterate’=5;’professional.course’=6;’university.degree’=7;’unknown’=8”)</span><span id="dab3" class="or mp it pd b gy pl pi l pj pk">banking$default = recode(banking$default, “‘no’=1;’yes’=2;’unknown’=3”)</span><span id="d8a9" class="or mp it pd b gy pl pi l pj pk">banking$housing = recode(banking$housing, “‘no’=1;’yes’=2;’unknown’=3”)</span><span id="c40a" class="or mp it pd b gy pl pi l pj pk">banking$loan = recode(banking$loan, “‘no’=1;’yes’=2;’unknown’=3”)<br/>banking$contact = recode(banking$loan, “‘cellular’=1;’telephone’=2;”)</span><span id="48e0" class="or mp it pd b gy pl pi l pj pk">banking$month = recode(banking$month, “‘mar’=1;’apr’=2;’may’=3;’jun’=4;’jul’=5;’aug’=6;’sep’=7;’oct’=8;’nov’=9;’dec’=10”)</span><span id="3bc0" class="or mp it pd b gy pl pi l pj pk">banking$day_of_week = recode(banking$day_of_week, “‘mon’=1;’tue’=2;’wed’=3;’thu’=4;’fri’=5;”)</span><span id="07b1" class="or mp it pd b gy pl pi l pj pk">banking$poutcome = recode(banking$poutcome, “‘failure’=1;’nonexistent’=2;’success’=3;”)</span><span id="1b91" class="or mp it pd b gy pl pi l pj pk">#remove variable “pdays”, b/c it has no variation<br/>banking$pdays=NULL </span><span id="3ece" class="or mp it pd b gy pl pi l pj pk">#remove variable “pdays”, b/c itis collinear with the DV<br/>banking$duration=NULL</span></pre><p id="6682" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">清理原始数据似乎很繁琐，因为我们必须重新编码缺失的变量，并将定性变量转换为定量变量。在现实世界中，清理数据需要更多的时间。<strong class="lk jd">有句话说“数据科学家 80%的时间用来清理数据，20%的时间用来建立模型。”</strong></p><p id="37e4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">接下来，让我们探索我们的结果变量的分布。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="6e41" class="or mp it pd b gy ph pi l pj pk">#EDA of the DV<br/>plot(banking$y,main="Plot 1: Distribution of Dependent Variable")</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/d12a64b3a4f20a633c3960496e3e4fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkJ9tgTz2FMhsUOROmPoZg.png"/></div></div></figure><p id="4f9c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可以看出，因变量(服务订购)的分布并不均匀，其中“否”多于“是”。<strong class="lk jd">这种不平衡的分布应该发出一些警告信号，因为数据分布会影响最终的统计模型</strong>。使用从多数案例发展而来的模型，很容易将少数案例错误分类。</p><h2 id="599b" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> 2。数据分割</strong></h2><p id="b85c" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">接下来，让我们将数据集分成两部分:训练集和测试集。根据经验，我们坚持 80-20 的划分:80%作为训练集，20%作为测试集。对于时间序列数据，我们基于 90%的数据训练模型，剩下的 10%作为测试数据集。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="e375" class="or mp it pd b gy ph pi l pj pk">#split the dataset into training and test sets randomly <br/>set.seed(1)#set seed so as to generate the same value each time we run the code</span><span id="6069" class="or mp it pd b gy pl pi l pj pk">#create an index to split the data: 80% training and 20% test<br/>index = round(nrow(banking)*0.2,digits=0)</span><span id="53ea" class="or mp it pd b gy pl pi l pj pk">#sample randomly throughout the dataset and keep the total number equal to the value of index<br/>test.indices = sample(1:nrow(banking), index)</span><span id="574f" class="or mp it pd b gy pl pi l pj pk">#80% training set<br/>banking.train=banking[-test.indices,] </span><span id="25a2" class="or mp it pd b gy pl pi l pj pk">#20% test set<br/>banking.test=banking[test.indices,] </span><span id="2c30" class="or mp it pd b gy pl pi l pj pk">#Select the training set except the DV<br/>YTrain = banking.train$y<br/>XTrain = banking.train %&gt;% select(-y)</span><span id="79b9" class="or mp it pd b gy pl pi l pj pk"># Select the test set except the DV<br/>YTest = banking.test$y<br/>XTest = banking.test %&gt;% select(-y)</span></pre><p id="8364" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，让我们创建一个空的跟踪记录。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="ff13" class="or mp it pd b gy ph pi l pj pk">records = matrix(NA, nrow=5, ncol=2) <br/>colnames(records) &lt;- c(“train.error”,”test.error”)<br/>rownames(records) &lt;- c(“Logistic”,”Tree”,”KNN”,”Random Forests”,”SVM”)</span></pre><h2 id="4f77" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> 3。火车模型</strong></h2><p id="cdaa" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">在本节中，我们定义了一个新函数(<strong class="lk jd"> calc_error_rate </strong>)，并将其应用于计算每个 ML 模型的训练和测试误差。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="0734" class="or mp it pd b gy ph pi l pj pk">calc_error_rate &lt;- function(predicted.value, true.value)<br/>                    {return(mean(true.value!=predicted.value))}</span></pre><p id="2b10" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该函数计算预测标签不等于真实值时的比率。</p><h2 id="9a67" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> #1 逻辑回归模型</strong></h2><p id="147c" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">关于 logistic 模型的简介，请查看我的其他帖子:<a class="ae lh" rel="noopener" target="_blank" href="/machine-learning-101-predicting-drug-use-using-logistic-regression-in-r-769be90eb03d"> <strong class="lk jd">机器学习 101 </strong> </a>和<a class="ae lh" rel="noopener" target="_blank" href="/machine-learning-102-logistic-regression-with-polynomial-features-98a208688c17"> <strong class="lk jd">机器学习 102 </strong> </a>。</p><p id="e94e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们拟合一个逻辑模型，其中包括除结果变量之外的所有其他变量。由于结果是二进制的，我们将模型设置为二项分布(“家庭=二项”)。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="c922" class="or mp it pd b gy ph pi l pj pk">glm.fit = glm(y ~ age+factor(job)+factor(marital)+factor(education)+factor(default)+factor(housing)+factor(loan)+factor(contact)+factor(month)+factor(day_of_week)+campaign+previous+factor(poutcome)+emp.var.rate+cons.price.idx+cons.conf.idx+euribor3m+nr.employed, data=banking.train, <strong class="pd jd">family=binomial</strong>)</span></pre><p id="5888" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">下一步是获得列车误差。我们将类型设置为 response，因为我们预测结果的类型并采用多数规则:如果先验概率超过或等于 0.5，我们预测结果为 yes 否则，答案是否定的</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="1a5d" class="or mp it pd b gy ph pi l pj pk">prob.training = predict(glm.fit,<strong class="pd jd">type=”response”</strong>)</span><span id="bfe6" class="or mp it pd b gy pl pi l pj pk">banking.train_glm = banking.train %&gt;% #select all rows of the train<br/> mutate(predicted.value=as.factor(ifelse(prob.training&lt;=0.5, “no”, “yes”))) <strong class="pd jd">#create a new variable using mutate and set a majority rule using ifelse</strong></span><span id="44ed" class="or mp it pd b gy pl pi l pj pk"># get the training error<br/>logit_traing_error &lt;-  calc_error_rate(predicted.value=banking.train_glm$predicted.value,  true.value=YTrain)</span><span id="cd33" class="or mp it pd b gy pl pi l pj pk"># get the test error of the logistic model<br/>prob.test = predict(glm.fit,banking.test,type=”response”)</span><span id="d691" class="or mp it pd b gy pl pi l pj pk">banking.test_glm = banking.test %&gt;% # select rows<br/> mutate(predicted.value2=as.factor(ifelse(prob.test&lt;=0.5, “no”, “yes”))) # set rules</span><span id="702a" class="or mp it pd b gy pl pi l pj pk">logit_test_error &lt;- calc_error_rate(predicted.value=banking.test_glm$predicted.value2, true.value=YTest)</span><span id="a976" class="or mp it pd b gy pl pi l pj pk"># write down the training and test errors of the logistic model <br/>records[1,] &lt;- c(logit_traing_error,logit_test_error)#write into the first row</span></pre><h2 id="aab6" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> #2 决策树</strong></h2><p id="9301" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">对于 DT，我们遵循交叉验证，并确定拆分的最佳节点。关于 DT 的快速介绍，请参考<a class="pn po ep" href="https://medium.com/u/a84d0e60277a?source=post_page-----fab464573233--------------------------------" rel="noopener" target="_blank"> Prashant Gupta </a>的帖子(<a class="ae lh" rel="noopener" target="_blank" href="/decision-trees-in-machine-learning-641b9c4e8052">链接</a>)。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="3fef" class="or mp it pd b gy ph pi l pj pk"># finding the best nodes<br/># the total number of rows<br/>nobs = nrow(banking.train)</span><span id="a7b8" class="or mp it pd b gy pl pi l pj pk">#build a DT model; <br/>#please refer to this document (<a class="ae lh" href="https://www.datacamp.com/community/tutorials/decision-trees-R" rel="noopener ugc nofollow" target="_blank">here</a>) for constructing a DT model<br/>bank_tree = tree(y~., data= banking.train,na.action = na.pass,<br/> control = tree.control(nobs , mincut =2, minsize = 10, mindev = 1e-3))</span><span id="37e8" class="or mp it pd b gy pl pi l pj pk">#cross validation to prune the tree<br/>set.seed(3)<br/>cv = cv.tree(bank_tree,FUN=prune.misclass, K=10)<br/>cv</span><span id="0d6b" class="or mp it pd b gy pl pi l pj pk">#identify the best cv<br/>best.size.cv = cv$size[which.min(cv$dev)]<br/>best.size.cv#best = 3</span><span id="fcde" class="or mp it pd b gy pl pi l pj pk">bank_tree.pruned&lt;-prune.misclass(bank_tree, best=3)<br/>summary(bank_tree.pruned)</span></pre><p id="b02f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">交叉验证的最佳规模是 3。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="00e5" class="or mp it pd b gy ph pi l pj pk"># Training and test errors of bank_tree.pruned<br/>pred_train = predict(bank_tree.pruned, banking.train, type=”class”)<br/>pred_test = predict(bank_tree.pruned, banking.test, type=”class”)</span><span id="841d" class="or mp it pd b gy pl pi l pj pk"># training error<br/>DT_training_error &lt;- calc_error_rate(predicted.value=pred_train, true.value=YTrain)</span><span id="1d19" class="or mp it pd b gy pl pi l pj pk"># test error<br/>DT_test_error &lt;- calc_error_rate(predicted.value=pred_test, true.value=YTest)</span><span id="57aa" class="or mp it pd b gy pl pi l pj pk"># write down the errors<br/>records[2,] &lt;- c(DT_training_error,DT_test_error)</span></pre><h2 id="1c38" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"># 3k-最近邻居</strong></h2><p id="bb28" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">作为一种非参数方法，KNN 不需要任何分布的先验知识。简而言之，KNN 为感兴趣的单元分配 k 个最近邻。</p><p id="35b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要快速开始，请查看我在 KNN 的帖子:<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb"><strong class="lk jd">R 中 K 近邻初学者指南:从零到英雄。</strong> </a> <strong class="lk jd"> </strong>关于交叉验证和 do.chunk 函数的详细解释，请重定向到我的<a class="ae lh" rel="noopener" target="_blank" href="/beginners-guide-to-k-nearest-neighbors-in-r-from-zero-to-hero-d92cd4074bdb">帖子</a>。</p><p id="11ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用交叉验证，我们发现当 k=20 时交叉验证误差最小。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="afbe" class="or mp it pd b gy ph pi l pj pk">nfold = 10<br/>set.seed(1)</span><span id="e2a0" class="or mp it pd b gy pl pi l pj pk"># cut() divides the range into several intervals<br/>folds = seq.int(nrow(banking.train)) %&gt;%<br/>     cut(breaks = nfold, labels=FALSE) %&gt;%  <br/>     sample</span><span id="39b5" class="or mp it pd b gy pl pi l pj pk">do.chunk &lt;- function(chunkid, folddef, Xdat, Ydat, k){ <br/>     train = (folddef!=chunkid)# training index</span><span id="f3a9" class="or mp it pd b gy pl pi l pj pk">Xtr = Xdat[train,] # training set by the index</span><span id="e7f8" class="or mp it pd b gy pl pi l pj pk">Ytr = Ydat[train] # true label in training set</span><span id="57dd" class="or mp it pd b gy pl pi l pj pk">Xvl = Xdat[!train,] # test set</span><span id="4fb3" class="or mp it pd b gy pl pi l pj pk">Yvl = Ydat[!train] # true label in test set</span><span id="da80" class="or mp it pd b gy pl pi l pj pk">predYtr = knn(train = Xtr, test = Xtr, cl = Ytr, k = k) # predict training labels</span><span id="2765" class="or mp it pd b gy pl pi l pj pk">predYvl = knn(train = Xtr, test = Xvl, cl = Ytr, k = k) # predict test labels</span><span id="82c6" class="or mp it pd b gy pl pi l pj pk">data.frame(fold =chunkid, # k folds <br/>train.error = calc_error_rate(predYtr, Ytr),#training error per fold <br/> val.error = calc_error_rate(predYvl, Yvl)) # test error per fold<br/> }</span><span id="3306" class="or mp it pd b gy pl pi l pj pk"># set error.folds to save validation errors<br/>error.folds=NULL</span><span id="80b5" class="or mp it pd b gy pl pi l pj pk"># create a sequence of data with an interval of 10<br/>kvec = c(1, seq(10, 50, length.out=5))</span><span id="d042" class="or mp it pd b gy pl pi l pj pk">set.seed(1)</span><span id="4fec" class="or mp it pd b gy pl pi l pj pk">for (j in kvec){<br/> tmp = ldply(1:nfold, do.chunk, # apply do.function to each fold<br/> folddef=folds, Xdat=XTrain, Ydat=YTrain, k=j) # required arguments<br/> tmp$neighbors = j # track each value of neighbors<br/> error.folds = rbind(error.folds, tmp) # combine the results <br/> }</span><span id="7700" class="or mp it pd b gy pl pi l pj pk"><strong class="pd jd">#melt() in the package reshape2 melts wide-format data into long-format data<br/></strong>errors = melt(error.folds, id.vars=c(“fold”,”neighbors”), value.name= “error”)</span></pre><p id="248b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，让我们找到使验证误差最小化的最佳 k 数。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="b032" class="or mp it pd b gy ph pi l pj pk">val.error.means = errors %&gt;%<br/> filter(variable== “val.error” ) %&gt;%<br/> group_by(neighbors, variable) %&gt;%<br/> summarise_each(funs(mean), error) %&gt;%<br/> ungroup() %&gt;%<br/> filter(error==min(error))</span><span id="6b63" class="or mp it pd b gy pl pi l pj pk">#the best number of neighbors =20<br/>numneighbor = max(val.error.means$neighbors)<br/>numneighbor</span><span id="83db" class="or mp it pd b gy pl pi l pj pk">## [20]</span></pre><p id="03ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">按照同样的步骤，我们会发现训练和测试错误。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="6eec" class="or mp it pd b gy ph pi l pj pk">#training error<br/>set.seed(20)<br/>pred.YTtrain = knn(train=XTrain, test=XTrain, cl=YTrain, k=20)<br/>knn_traing_error &lt;- calc_error_rate(predicted.value=pred.YTtrain, true.value=YTrain)</span><span id="5a6c" class="or mp it pd b gy pl pi l pj pk">#test error =0.095</span><span id="6398" class="or mp it pd b gy pl pi l pj pk">set.seed(20)<br/>pred.YTest = knn(train=XTrain, test=XTest, cl=YTrain, k=20)<br/>knn_test_error &lt;- calc_error_rate(predicted.value=pred.YTest, true.value=YTest)</span><span id="f0fe" class="or mp it pd b gy pl pi l pj pk">records[3,] &lt;- c(knn_traing_error,knn_test_error)</span></pre><h2 id="957f" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> #4 随机森林</strong></h2><p id="12bf" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们遵循构建随机森林模型的标准步骤。由<a class="pn po ep" href="https://medium.com/u/840a3210fbe7?source=post_page-----fab464573233--------------------------------" rel="noopener" target="_blank">饶彤彤</a>撰写的 RF ( <a class="ae lh" rel="noopener" target="_blank" href="/understanding-random-forest-58381e0602d2">链接</a>)快速介绍。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="443b" class="or mp it pd b gy ph pi l pj pk"># build a RF model with default settings <br/>set.seed(1)<br/>RF_banking_train = randomForest(y ~ ., data=banking.train, importance=TRUE)</span><span id="24fd" class="or mp it pd b gy pl pi l pj pk"># predicting outcome classes using training and test sets<br/><br/>pred_train_RF = predict(RF_banking_train, banking.train, type=”class”)</span><span id="fa8b" class="or mp it pd b gy pl pi l pj pk">pred_test_RF = predict(RF_banking_train, banking.test, type=”class”)</span><span id="7634" class="or mp it pd b gy pl pi l pj pk"># training error<br/>RF_training_error &lt;- calc_error_rate(predicted.value=pred_train_RF, true.value=YTrain)</span><span id="daf2" class="or mp it pd b gy pl pi l pj pk"># test error<br/>RF_test_error &lt;- calc_error_rate(predicted.value=pred_test_RF, true.value=YTest)</span><span id="3e92" class="or mp it pd b gy pl pi l pj pk">records[4,] &lt;- c(RF_training_error,RF_test_error)</span></pre><h2 id="0b09" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> #5 支持向量机</strong></h2><p id="fc3b" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">同样，我们遵循构建 SVM 的标准步骤。一个好的方法介绍，请参考<a class="pn po ep" href="https://medium.com/u/8f4e7f7a57e3?source=post_page-----fab464573233--------------------------------" rel="noopener" target="_blank">罗希斯甘地</a>的帖子(<a class="ae lh" rel="noopener" target="_blank" href="/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47">链接</a>)。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="1554" class="or mp it pd b gy ph pi l pj pk">set.seed(1)<br/>tune.out=tune(svm, y ~., data=banking.train,<br/>kernel=”radial”,ranges=list(cost=c(0.1,1,10)))</span><span id="16b5" class="or mp it pd b gy pl pi l pj pk"># find the best parameters<br/>summary(tune.out)$best.parameters</span><span id="c89a" class="or mp it pd b gy pl pi l pj pk"># the best model<br/>best_model = tune.out$best.model</span><span id="509e" class="or mp it pd b gy pl pi l pj pk">svm_fit=svm(y~., data=banking.train,kernel=”radial”,gamma=0.05555556,cost=1,probability=TRUE)</span><span id="91d7" class="or mp it pd b gy pl pi l pj pk"># using training/test sets to predict outcome classes<br/>svm_best_train = predict(svm_fit,banking.train,type=”class”)<br/>svm_best_test = predict(svm_fit,banking.test,type=”class”)</span><span id="8eba" class="or mp it pd b gy pl pi l pj pk"># training error<br/>svm_training_error &lt;- calc_error_rate(predicted.value=svm_best_train, true.value=YTrain)</span><span id="151b" class="or mp it pd b gy pl pi l pj pk"># test error<br/>svm_test_error &lt;- calc_error_rate(predicted.value=svm_best_test, true.value=YTest)</span><span id="085a" class="or mp it pd b gy pl pi l pj pk">records[5,] &lt;- c(svm_training_error,svm_test_error)</span></pre><h2 id="4b44" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">4.模型度量</h2><p id="90ba" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">我们按照模型选择程序构建了所有的 ML 模型，并获得了它们的训练和测试误差。在本节中，我们将使用一些模型指标来选择最佳模型。</p><h2 id="9853" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> 4.1 训练/测试错误</strong></h2><p id="5336" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">有可能利用训练/测试误差找到最佳模型吗？</p><p id="975a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，让我们检查结果。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="7206" class="or mp it pd b gy ph pi l pj pk">records</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/7f6376375f77268f621974612eb40bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFDAK3f-wXJ5aN1e7VoNEA.png"/></div></div></figure><p id="5719" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，随机森林具有最小的训练误差，尽管与其他方法具有相似的测试误差。您可能已经注意到，训练和测试误差非常接近，很难判断哪一个明显胜出。</p><p id="21e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，分类精度，无论是训练误差还是测试误差，都不应该成为高度不平衡数据集的衡量标准。这是因为数据集是由大多数案例支配的，即使是随机的猜测也有 50%的机会猜对(50%的准确率)。更糟糕的是，一个高度精确的模型可能会严重地惩罚少数情况。出于这个原因，让我们检查另一个指标 ROC 曲线。</p><h2 id="1e9c" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated"><strong class="ak"> 4.2 </strong>接收机工作特性(ROC)曲线</h2><p id="dd1d" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">ROC 是一种图形表示，显示分类模型在所有分类阈值下的表现。我们更喜欢比其他分类器更快接近 1 的分类器。</p><p id="7d76" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">ROC 曲线在同一图表中不同阈值处绘制了两个参数—真阳性率和假阳性率:</p><blockquote class="me"><p id="5be2" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">TPR(召回)= TP/(TP+FN)</p><p id="b22e" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">FPR = FP/(TN+FP)</p></blockquote><figure class="pr ps pt pu pv kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pq"><img src="../Images/d20451c2536b7b99d6dff2f71975bd23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V8ig9JIqlrKfHqBh3GBp7A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk"><a class="ae lh" href="https://commons.wikimedia.org/wiki/File:ROC_space-2.png" rel="noopener ugc nofollow" target="_blank">Indon</a></figcaption></figure><p id="aa6a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在很大程度上，ROC 曲线不仅衡量分类精度的水平，而且在 TPR 和 FPR 之间达到了良好的平衡。这对于罕见事件来说是非常可取的，因为我们也希望在多数和少数情况之间达到平衡。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="1dd3" class="or mp it pd b gy ph pi l pj pk"># load the library<br/>library(ROCR)</span><span id="636d" class="or mp it pd b gy pl pi l pj pk">#creating a tracking record<br/>Area_Under_the_Curve = matrix(NA, nrow=5, ncol=1)<br/>colnames(Area_Under_the_Curve) &lt;- c(“AUC”) <br/>rownames(Area_Under_the_Curve) &lt;- c(“Logistic”,”Tree”,”KNN”,”Random Forests”,”SVM”)</span><span id="815f" class="or mp it pd b gy pl pi l pj pk">########### logistic regression ###########<br/># ROC<br/>prob_test &lt;- predict(glm.fit,banking.test,type=”response”)<br/>pred_logit&lt;- prediction(prob_test,banking.test$y)<br/>performance_logit &lt;- performance(pred_logit,measure = “tpr”, x.measure=”fpr”)</span><span id="a958" class="or mp it pd b gy pl pi l pj pk">########### Decision Tree ###########<br/># ROC<br/>pred_DT&lt;-predict(bank_tree.pruned, banking.test,type=”vector”)<br/>pred_DT &lt;- prediction(pred_DT[,2],banking.test$y)<br/>performance_DT &lt;- performance(pred_DT,measure = “tpr”,x.measure= “fpr”)</span><span id="abde" class="or mp it pd b gy pl pi l pj pk">########### KNN ########### <br/># ROC<br/>knn_model = knn(train=XTrain, test=XTrain, cl=YTrain, k=20,prob=TRUE)prob &lt;- attr(knn_model, “prob”)<br/>prob &lt;- 2*ifelse(knn_model == “-1”, prob,1-prob) — 1<br/>pred_knn &lt;- prediction(prob, YTrain)<br/>performance_knn &lt;- performance(pred_knn, “tpr”, “fpr”)</span><span id="c5e2" class="or mp it pd b gy pl pi l pj pk">########### Random Forests ###########<br/># ROC<br/>pred_RF&lt;-predict(RF_banking_train, banking.test,type=”prob”)<br/>pred_class_RF &lt;- prediction(pred_RF[,2],banking.test$y)<br/>performance_RF &lt;- performance(pred_class_RF,measure = “tpr”,x.measure= “fpr”)</span><span id="8ae5" class="or mp it pd b gy pl pi l pj pk">########### SVM ########### <br/># ROC<br/>svm_fit_prob = predict(svm_fit,type=”prob”,newdata=banking.test,probability=TRUE)<br/>svm_fit_prob_ROCR = prediction(attr(svm_fit_prob,”probabilities”)[,2],banking.test$y==”yes”)<br/>performance_svm &lt;- performance(svm_fit_prob_ROCR, “tpr”,”fpr”)</span></pre><p id="f8c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们画出 ROC 曲线。</p><p id="f254" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们画一条线来表示随机分配的机会。我们的分类器应该比随机猜测表现得更好，对吗？</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="3df5" class="or mp it pd b gy ph pi l pj pk">#logit<br/>plot(performance_logit,col=2,lwd=2,main=”ROC Curves for These Five Classification Methods”)</span><span id="9b2c" class="or mp it pd b gy pl pi l pj pk">legend(0.6, 0.6, c(‘logistic’, ‘Decision Tree’, ‘KNN’,’Random Forests’,’SVM’), 2:6)</span><span id="19d0" class="or mp it pd b gy pl pi l pj pk">#decision tree<br/>plot(performance_DT,col=3,lwd=2,add=TRUE)</span><span id="a864" class="or mp it pd b gy pl pi l pj pk">#knn<br/>plot(performance_knn,col=4,lwd=2,add=TRUE)</span><span id="4c1d" class="or mp it pd b gy pl pi l pj pk">#RF<br/>plot(performance_RF,col=5,lwd=2,add=TRUE)</span><span id="242f" class="or mp it pd b gy pl pi l pj pk"># SVM<br/>plot(performance_svm,col=6,lwd=2,add=TRUE)</span><span id="3cac" class="or mp it pd b gy pl pi l pj pk">abline(0,1)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/18b3c63197ab16585a4909081bf4a645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sTRtEpgD41HM73VDRgt7Rg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">ROC</figcaption></figure><p id="222f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们这里有一个赢家。</p><p id="4980" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据 ROC 曲线，KNN(蓝色的)高于所有其他方法。</p><h2 id="f99f" class="or mp it bd mq os ot dn mu ou ov dp my lr ow ox na lv oy oz nc lz pa pb ne iz bi translated">4.3 曲线下面积(AUC)</h2><p id="01e7" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">顾名思义，AUC 就是 ROC 曲线下的面积。它是直观 AUC 曲线的算术表示。AUC 提供分类器如何跨越可能的分类阈值的综合结果。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="e390" class="or mp it pd b gy ph pi l pj pk">########### Logit ########### <br/>auc_logit = performance(pred_logit, “auc”)<a class="ae lh" href="http://twitter.com/y" rel="noopener ugc nofollow" target="_blank">@y</a>.values<br/>Area_Under_the_Curve[1,] &lt;-c(as.numeric(auc_logit))</span><span id="8560" class="or mp it pd b gy pl pi l pj pk">########### Decision Tree ###########<br/>auc_dt = performance(pred_DT,”auc”)<a class="ae lh" href="http://twitter.com/y" rel="noopener ugc nofollow" target="_blank">@y</a>.values<br/>Area_Under_the_Curve[2,] &lt;- c(as.numeric(auc_dt))</span><span id="28d8" class="or mp it pd b gy pl pi l pj pk">########### KNN ###########<br/>auc_knn &lt;- performance(pred_knn,”auc”)<a class="ae lh" href="http://twitter.com/y" rel="noopener ugc nofollow" target="_blank">@y</a>.values<br/>Area_Under_the_Curve[3,] &lt;- c(as.numeric(auc_knn))</span><span id="3485" class="or mp it pd b gy pl pi l pj pk">########### Random Forests ###########<br/>auc_RF = performance(pred_class_RF,”auc”)<a class="ae lh" href="http://twitter.com/y" rel="noopener ugc nofollow" target="_blank">@y</a>.values<br/>Area_Under_the_Curve[4,] &lt;- c(as.numeric(auc_RF))</span><span id="bc2e" class="or mp it pd b gy pl pi l pj pk">########### SVM ########### <br/>auc_svm&lt;-performance(svm_fit_prob_ROCR,”auc”)<a class="ae lh" href="http://twitter.com/y" rel="noopener ugc nofollow" target="_blank">@y</a>.values[[1]]<br/>Area_Under_the_Curve[5,] &lt;- c(as.numeric(auc_svm))</span></pre><p id="7329" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们检查一下 AUC 值。</p><pre class="ks kt ku kv gt pc pd pe pf aw pg bi"><span id="44b1" class="or mp it pd b gy ph pi l pj pk">Area_Under_the_Curve</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi px"><img src="../Images/c923e6668fd809add243d25e999f05b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*rzNxjfKqNk5vAIqMRQLUpg.png"/></div></figure><p id="7526" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">此外，KNN 的 AUC 值最大(0.847)。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="8c1a" class="mo mp it bd mq mr om mt mu mv on mx my ki oo kj na kl op km nc ko oq kp ne nf bi translated">结论</h1><p id="67cf" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">在这篇文章中，我们发现 KNN，一个非参数分类器，比它的参数分类器表现更好。就度量标准而言，对于罕见事件，选择 ROC 曲线比选择分类准确度更合理。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="d6fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="oe"> Medium 最近进化出了自己的</em> <a class="ae lh" href="https://blog.medium.com/evolving-the-partner-program-2613708f9f3c" rel="noopener"> <em class="oe">作家伙伴计划</em> </a> <em class="oe">，支持像我这样的普通作家。如果你还不是订户，通过下面的链接注册，我会收到一部分会员费。</em></p><div class="py pz gp gr qa qb"><a href="https://leihua-ye.medium.com/membership" rel="noopener follow" target="_blank"><div class="qc ab fo"><div class="qd ab qe cl cj qf"><h2 class="bd jd gy z fp qg fr fs qh fu fw jc bi translated">阅读叶雷华博士研究员(以及其他成千上万的媒体作家)的每一个故事</h2><div class="qi l"><h3 class="bd b gy z fp qg fr fs qh fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="qj l"><p class="bd b dl z fp qg fr fs qh fu fw dk translated">leihua-ye.medium.com</p></div></div><div class="qk l"><div class="ql l qm qn qo qk qp lb qb"/></div></div></a></div></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><h1 id="e951" class="mo mp it bd mq mr om mt mu mv on mx my ki oo kj na kl op km nc ko oq kp ne nf bi translated">喜欢读这本书吗？</h1><blockquote class="qq qr qs"><p id="99b8" class="li lj oe lk b ll lm kd ln lo lp kg lq qt ls lt lu qu lw lx ly qv ma mb mc md im bi translated">请在<a class="ae lh" href="https://www.linkedin.com/in/leihuaye/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://www.youtube.com/channel/UCBBu2nqs6iZPyNSgMjXUGPg" rel="noopener ugc nofollow" target="_blank"> Youtube </a>上找到我。</p><p id="0cb1" class="li lj oe lk b ll lm kd ln lo lp kg lq qt ls lt lu qu lw lx ly qv ma mb mc md im bi translated">还有，看看我其他关于人工智能和机器学习的帖子。</p></blockquote></div></div>    
</body>
</html>