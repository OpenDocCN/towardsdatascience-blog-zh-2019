<html>
<head>
<title>Faster Training for Efficient CNNs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高效 CNN 的快速培训</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/faster-training-of-efficient-cnns-657953aa080?source=collection_archive---------18-----------------------#2019-05-30">https://towardsdatascience.com/faster-training-of-efficient-cnns-657953aa080?source=collection_archive---------18-----------------------#2019-05-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="b008" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">最近，已经证明<a class="ae ks" href="https://arxiv.org/abs/1610.02357" rel="noopener ugc nofollow" target="_blank">深度卷积</a>在设计高效网络方面非常有效，例如<a class="ae ks" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> MobileNet </a>和<a class="ae ks" href="https://arxiv.org/abs/1807.11164" rel="noopener ugc nofollow" target="_blank"> ShuffleNet </a>。然而，这种高效的网络需要花费<strong class="jw iu">更长的时间来训练，通常需要 300-400 个历元</strong>，才能在 ImageNet 数据集上达到最先进的精度。</p></blockquote><p id="4368" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在本文中，我们描述了<strong class="jw iu"> <em class="jv">在<a class="ae ks" href="https://arxiv.org/pdf/1811.11431.pdf" rel="noopener ugc nofollow" target="_blank"> ESPNetv2 </a>论文(CVPR'19)中介绍的一种有效的学习率调度器</em> </strong>，它允许在大约 100 个时期内训练有效的网络，而不会对准确性产生任何重大影响。为了展示 ESPNetv2 中学习率调度程序的卓越性能，我们在本文中使用了<a class="ae ks" href="https://arxiv.org/abs/1807.11164" rel="noopener ugc nofollow" target="_blank"> ShuffleNetv2 </a>的架构。我们注意到我们的发现与其他高效网络一致，包括<a class="ae ks" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> MobileNetv2 </a>。</p></div><div class="ab cl kw kx hx ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="im in io ip iq"><h1 id="f974" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">学习率调度程序</h1><p id="8f2e" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">ESPNetv2 引入了<a class="ae ks" href="https://arxiv.org/abs/1608.03983" rel="noopener ugc nofollow" target="_blank">余弦学习率</a>的变体，其中学习率线性衰减直到周期长度，然后重新开始。在每个时期<strong class="jw iu"> <em class="jv"> t </em> </strong>，学习率ηₜ被计算为:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi mg"><img src="../Images/57c5e3b50afd9270f5f256311e2e726d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-nifc4VbeUSA-6sKBx8XA.png"/></div></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ms"><img src="../Images/4f6fd3b2760d91bbb7b7065d32dcfe9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*el17aMECacRt_C1e6KRGhA.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Figure 1:</strong> Cyclic learning rate policy with linear learning rate decay and warm restarts</figcaption></figure><h2 id="2faf" class="my le it bd lf mz na dn lj nb nc dp ln kt nd ne lr ku nf ng lv kv nh ni lz nj bi translated"><span class="l nk nl nm bm nn no np nq nr di"> F </span>紫苑培训</h2><p id="2a48" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">使用上述学习速率调度程序，我们在四个 TitanX GPUs 上使用 SGD 在两种不同的 FLOP 设置下为<em class="jv">训练 ShuffleNetv2 总共 120 个历元</em>，批量大小为 512:(1)41m flops 和(2) 146 MFLOPs。对于前 60 个时期，我们将<strong class="jw iu"> ηₘᵢₙ </strong>、<strong class="jw iu"> ηₘₐₓ </strong>和<strong class="jw iu"> T </strong>分别设置为 0.1、0.5 和 5。对于剩余的 60 个纪元，我们设置<strong class="jw iu"> ηₘₐₓ=0.1 </strong>、<strong class="jw iu"> T=60 </strong>、<strong class="jw iu"> ηₘᵢₙ </strong> = <strong class="jw iu"> ηₘₐₓ/T </strong>。下图显示了 120 个时期的学习策略。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ns"><img src="../Images/5cc47189cfb63c05233daa156eab45c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gl6QA-CaqAAqJTjOb-LBug.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Figure 2:</strong> Cyclic learning rate scheduler with warm restarts for faster training proposed in the ESPNetv2 paper.</figcaption></figure><h2 id="e0df" class="my le it bd lf mz na dn lj nb nc dp ln kt nd ne lr ku nf ng lv kv nh ni lz nj bi translated">ImageNet 数据集上的结果</h2><p id="c993" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi translated">结果见图 3 和表 1。我们可以清楚地看到，学习率调度器(如上所述)能够实现更快的训练，同时提供与 ShuffleNet 论文中的线性学习率调度器相似的性能。</p><div class="mh mi mj mk gt ab cb"><figure class="nt ml nu nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/27baab8333ddb71a4ac16af4d52d073d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Ju88iQPTaNLpMtxCrxdWfQ.png"/></div></figure><figure class="nt ml nz nv nw nx ny paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><img src="../Images/9a0df85f27a98a3fe3d055f4372f8e34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*28HNyDt46lkR9DK6cfH4hw.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk oa di ob oc"><strong class="bd mx">Figure 3: </strong>Epoch-wise training (<strong class="bd mx"><em class="od">left</em></strong>) and validation (<strong class="bd mx"><em class="od">right</em></strong>) accuracy of ShuffleNetv2 (41 MFLOPs) using linear (see ShuffleNetv1/v2 papers) and cyclic (discussed above) learning rate schedule.</figcaption></figure></div><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi oe"><img src="../Images/76951c58636749b4e60b6e098267ed16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTd34igLBG9aeekL7yJCnw.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk"><strong class="bd mx">Table 1: </strong>Top-1 accuracy of the ShuffleNetv2 model with two different learning rate schedules: (1)<strong class="bd mx"><em class="od"> Linear,</em></strong> <strong class="bd mx">as in the ShuffleNet paper</strong> and (2) <strong class="bd mx">Cyclic, as in the ESPNetv2</strong> <strong class="bd mx">paper</strong> (described above). The slight difference in reported and our implementation is likely due to the batch size. We used a batch size of 512 while in ShuffleNetv2 paper a batch size of 1024 is used.</figcaption></figure><h1 id="215a" class="ld le it bd lf lg of li lj lk og lm ln lo oh lq lr ls oi lu lv lw oj ly lz ma bi translated">我们可以在不同的工作上使用 ESPNetv2 排程器吗？</h1><p id="55f6" class="pw-post-body-paragraph jt ju it jw b jx mb jz ka kb mc kd ke kt md kh ki ku me kl km kv mf kp kq kr im bi ok translated">是的，调度程序是通用的，可以跨不同的任务使用。在 ESPNetv2 论文中，我们将它用于所有三个标准视觉任务:(1)图像分类，(2)对象检测，以及(3)语义分割。详见<a class="ae ks" href="https://arxiv.org/pdf/1811.11431.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p></div></div>    
</body>
</html>