<html>
<head>
<title>Review: SENet — Squeeze-and-Excitation Network, Winner of ILSVRC 2017 (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:SENet —挤压和激励网络，ILSVRC 2017(图像分类)获奖者</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=collection_archive---------6-----------------------#2019-05-08">https://towardsdatascience.com/review-senet-squeeze-and-excitation-network-winner-of-ilsvrc-2017-image-classification-a887b98b2883?source=collection_archive---------6-----------------------#2019-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b318" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">凭借 SE 积木，超越<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc">盗梦空间-v4 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>、<a class="ae kf" href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78" rel="noopener"> PyramidNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-dpn-dual-path-networks-image-classification-d0135dce8817"> DPN </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet V1 </a></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/4d63a5ef0fbcb12b2790ebba418ed3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUn4ojyEVxqPdM-vDV63IA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">SENet got the first place in ILSVRC 2017 Classification Challenge</strong></figcaption></figure><p id="7ecd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di">在</span>这个故事里，回顾了<strong class="kz ir">牛津大学</strong>的<strong class="kz ir">压缩-激发网络(SENet) </strong>。利用“挤压-激励”(SE)模块<strong class="kz ir">，通过显式模拟通道之间的相互依赖性</strong>，自适应地重新校准通道式特征响应，构建 SENet。并且<strong class="kz ir">在 ILSVRC 2017 分类挑战赛</strong>中以 2.251%的前 5 名误差获得第一名，相对于 2016 年的获奖参赛作品有大约<strong class="kz ir"> 25%的提升</strong>。而这是一篇在<strong class="kz ir"> 2018 CVPR </strong>超过<strong class="kz ir"> 600 次引用</strong>的论文。最近还发表在<strong class="kz ir"> 2019 TPAMI </strong>上。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----a887b98b2883--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="51ca" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="d0f6" class="nd ne iq kz b la nf ld ng lg nh lk ni lo nj ls nk nl nm nn bi translated"><strong class="kz ir">挤压和激励(SE)块</strong></li><li id="be45" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">SE-Inception&amp;SE-ResNet</strong></li><li id="031e" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">与最先进方法的比较</strong></li><li id="5c2c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls nk nl nm nn bi translated"><strong class="kz ir">分析解释</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="7a82" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 1。挤压和激励(SE)模块</strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/747ba8c0a85579f38f05475f85f3deed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CHDHQ2hNuwIwNEdW0Z-PA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Squeeze-and-Excitation (SE) Block</strong></figcaption></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8e908838cf662a5898c48d2f6219afac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*hKQQXZhZhM0xZB_2R_223A.png"/></div></figure><ul class=""><li id="cfae" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">其中<em class="nz"> Ftr </em>是将<em class="nz"> X </em>转换为<em class="nz"> U </em>的卷积算子。</li><li id="97be" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">这个<em class="nz"> Ftr </em>可以是残差块或者 Inception 块，后面会更详细的提到。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0676095af80bfdccf29e94acfa6e89ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*WDNndkDYbFsfTvA52fRSUQ.png"/></div></figure><ul class=""><li id="edf8" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">其中<em class="nz"> V </em> =[ <em class="nz"> v </em> 1，<em class="nz"> v </em> 2，…，<em class="nz"> v </em> c]为学习后的滤波器核集合。</li></ul><h2 id="db8f" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">1.1.挤压:全球信息嵌入</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi on"><img src="../Images/8476fd27013e6ca28520a87472600f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*C7vDgQ2ce3k1_gO7345WuA.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">SE Path, Same as the Upper Path at the Figure Above</strong></figcaption></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/6b10eb4edde3c71e76f019731f882c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*4TXMI5KeUAk1OMqEaoTYmQ.png"/></div></figure><ul class=""><li id="ced3" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">变换输出<em class="nz"> U </em>可以被解释为<strong class="kz ir">局部描述符的集合，其统计表示整个图像</strong>。</li><li id="780d" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">建议<strong class="kz ir">将全局空间信息压缩到通道描述符</strong>中。</li><li id="26fa" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">这是通过使用<strong class="kz ir">全局平均池</strong>生成渠道统计数据来实现的。</li></ul><h2 id="7927" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">1.2.激发:自适应重新校准</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi op"><img src="../Images/455cc6eaf3418b765dfabe4e6b9e9c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*mtEKNSG4VHcvlbVBpeaSbw.png"/></div></figure><ul class=""><li id="8fe0" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">其中，δ是 ReLU 函数。</li><li id="f2d5" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">使用了一个使用 sigmoid 激活<em class="nz">σ</em>T21<strong class="kz ir">的简单浇口机制。</strong></li><li id="a773" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">提出了一种<strong class="kz ir">激励</strong>操作，以便<strong class="kz ir">完全捕获通道相关</strong>，并且<strong class="kz ir">学习通道之间的非线性和非互斥关系</strong>。</li><li id="1cf3" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">我们可以看到有<em class="nz"> W </em> 1 和<em class="nz"> W </em> 2，输入 z 是全局平均池化后的通道描述符，有<strong class="kz ir">两个全连接(FC)层</strong>。</li><li id="3e17" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">使用缩减比率<em class="nz">r</em>T37】通过<strong class="kz ir">维度缩减形成具有两个 FC 层的瓶颈。</strong></li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oq"><img src="../Images/587f5957034415205ff655e7b1759a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*8l6y_FIC3cs90bQuiSDS2g.png"/></div></div></figure><ul class=""><li id="9d23" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir">引入的附加参数的数量取决于如上所述的<em class="nz"> r </em> </strong>其中<strong class="kz ir"> <em class="nz"> S </em> </strong>指的是<strong class="kz ir">级的数量</strong>(其中每一级指的是在公共空间维度的特征图上操作的块的集合)<strong class="kz ir"> <em class="nz"> Cs </em> </strong>表示<strong class="kz ir">输出通道的维度</strong>和<strong class="kz ir"> <em class="nz"> Ns </em> </strong>表示<strong class="kz ir"/></li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e67acbd985493f05887fbc71513af3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*8uRHu8nK9Ni4PjgcmgUgmA.png"/></div></figure><ul class=""><li id="319d" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir">块的最终输出是通过用激活</strong>重新调整变换输出<em class="nz"> U </em>获得的，如上所示。</li><li id="4a61" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">激活充当适应于输入特定描述符 z 的通道权重。在这方面，<strong class="kz ir"> SE 模块本质上引入了以输入</strong>为条件的动态，有助于提高特征的可辨性。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="6528" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 2。SE-Inception &amp; SE-ResNet </strong></h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi os"><img src="../Images/9c3e8ada3b3bb829a86fe418ca3a031c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-UztHZDZEYb35TZBtfvqeA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Left: SE-Inception, Right: SE-ResNet</strong></figcaption></figure><ul class=""><li id="2f14" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">如上图所示，SE 块可以作为<strong class="kz ir"> SE-Inception </strong>和<strong class="kz ir"> SE-ResNet </strong>轻松添加到 Inception 和<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>块中。</li><li id="0c81" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">特别是在 SE-ResNet 中，压缩和激发都在同分支求和之前起作用。</li><li id="1bb2" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir">更多与</strong> <a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> <strong class="kz ir"> ResNeXt </strong> </a> <strong class="kz ir">，</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kz ir">Inception-ResNet</strong></a><strong class="kz ir">，</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"><strong class="kz ir">mobilenetv 1</strong></a><strong class="kz ir">和</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"><strong class="kz ir">shuffle net V1</strong></a><strong class="kz ir">可以按照类似的方案构造</strong>。</li><li id="7a9c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir"> SE-ResNet-50 </strong>和<strong class="kz ir"> SE-ResNeXt-50 (32×4d) </strong>更详细的架构如下:</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ot"><img src="../Images/f6ad6430c708264f0d6944a807d98636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x3qCQ7Ep_eKSJC6TSmhebA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd kw">ResNet-50</strong></a><strong class="bd kw"> (Left), SE-ResNet-50 (Middle), SE-ResNeXt-50 (32×4d) (Right)</strong></figcaption></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="6f3a" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="96ed" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">3.1. ImageNet 验证集的单作物<strong class="ak">错误率</strong></h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ou"><img src="../Images/02856a267a21f53ef5c990bed9a8a039.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQB3X6LFhpX5MkeAhADhjw.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Single-Crop Error Rates (%) on ImageNet Validation Set</strong></figcaption></figure><ul class=""><li id="8921" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">SE 块被添加到<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>、<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>、<a class="ae kf" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener"> BN-Inception </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>。对于<a class="ae kf" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGGNet </a>来说，为了更容易训练，在每次卷积后增加了批量归一化层。</li><li id="162f" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在训练期间，使用<strong class="kz ir">256 个图像的小批量</strong>，通过<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kz ir"> ResNet-50 </strong> </a> <strong class="kz ir">的单次前后传递需要 190 毫秒</strong>，相比之下，SE-ResNet-50 的<strong class="kz ir">需要 209 毫秒(这两个计时都是在具有<strong class="kz ir"> 8 个 NVIDIA Titan X GPU</strong>的服务器上执行的)。</strong></li><li id="e0a0" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在测试过程中，对于 224 × 224 像素的输入图像，每个型号的<strong class="kz ir"> CPU 推断时间</strong>:<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kz ir">ResNet-50</strong></a><strong class="kz ir">需要 164 ms </strong>，相比之下，SE-ResNet-50 的<strong class="kz ir">需要 167 ms。</strong></li><li id="8cff" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">值得注意的是，<strong class="kz ir"> SE-ResNet-50 </strong>实现了 6.62%<strong class="kz ir">的<strong class="kz ir">单作物 top-5 验证误差，超过</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kz ir">ResNet-50</strong></a><strong class="kz ir">(7.48%)0.86%</strong><strong class="kz ir">接近</strong>更深层次的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kz ir"> ResNet-101 </strong> </a>网络实现的性能</strong></li><li id="78a5" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">而<strong class="kz ir"> SE-ResNet-101 (6.07% top-5 误差)</strong>不仅匹配，而且跑赢更深层次的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kz ir">ResNet-152</strong></a><strong class="kz ir">网络(6.34% top-5 误差)0.27% </strong>。</li><li id="315c" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">同样，<strong class="kz ir"> SE-ResNeXt-50 </strong>的 top-5 误差为<strong class="kz ir"> 5.49% </strong>，不仅优于其直接对应的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kz ir">ResNeXt-50</strong></a>(<strong class="kz ir">5.90%</strong>top-5 误差)，也优于更深层次的<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kz ir">ResNeXt-101</strong></a>(<strong class="kz ir">5.57%【T6)</strong></li><li id="faa6" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir">SE-Inception-ResNet-v2</strong>(<strong class="kz ir">4.79%</strong>top-5 error)优于重新实现的<strong class="kz ir"/><a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"><strong class="kz ir">Inception-ResNet-v2</strong></a><strong class="kz ir"/>(<strong class="kz ir">5.21%</strong>top-5 error)0.42%(相对提高 8.1%)</li><li id="e5d9" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir">在一系列不同深度的训练中，性能改进是一致的</strong>，这表明 SE 模块带来的改进可以与增加基础架构的深度结合使用。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ov"><img src="../Images/d50e0e4a6e24aded24fc84189de58d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jIk_xMegMeQiVf8mzs4ceA.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Single-Crop Error Rates (%) on ImageNet Validation Set</strong></figcaption></figure><ul class=""><li id="5059" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">对于轻量级模型，<a class="ae kf" rel="noopener" target="_blank" href="/review-mobilenetv1-depthwise-separable-convolution-light-weight-model-a382df364b69"> MobileNetV1 </a>和<a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet V1 </a>，<strong class="kz ir"> SE 模块可以在最小的计算成本增加下持续大幅度提高精度</strong>。</li></ul><h2 id="9f8f" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">3.2.ILSVRC 2017 分类竞赛</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ow"><img src="../Images/144131f27428f80b1cac6cae690b92c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0GNoVbkIrMn70YMN7jHVag.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Single-Crop Error Rates (%) on ImageNet Validation Set</strong></figcaption></figure><ul class=""><li id="324b" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">使用了多尺度、多作物和集合。</li><li id="6aae" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在测试集上得到 2.251%的 top-5 误差。</li><li id="272f" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在验证集<strong class="kz ir"> SENet-154 上，使用 224 × 224 中心作物评估，具有修改的</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kz ir">ResNeXt</strong></a><strong class="kz ir">，</strong>的 SE 块实现了 18.68%的前 1 误差和 4.47%的前 5 误差<strong class="kz ir"/>。</li><li id="9120" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">比<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae kf" href="https://medium.com/@sh.tsang/review-inception-v3-1st-runner-up-image-classification-in-ilsvrc-2015-17915421f77c" rel="noopener"> Inception-v3 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-v4 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-inception-v4-evolved-from-googlenet-merged-with-resnet-idea-image-classification-5e8c339d18bc"> Inception-ResNet-v2 </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8">剩余注意力网络</a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>、<a class="ae kf" href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78" rel="noopener"> PyramidNet </a>、<a class="ae kf" rel="noopener" target="_blank" href="/review-dpn-dual-path-networks-image-classification-d0135dce8817"> DPN </a>。</li></ul><h2 id="0499" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">3.3.场景分类</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0aacd66c62f86f648a4fa96d6fdf1b96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*lp6BvOXDmqF9QAVcvUjVLw.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Single-crop error rates (%) on Places365 validation set</strong></figcaption></figure><ul class=""><li id="b2ba" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir">SE-ResNet-152</strong>(<strong class="kz ir">11.01%</strong>top-5 误差)实现了比<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kz ir">ResNet-152</strong></a><strong class="kz ir"/>(<strong class="kz ir">11.61%</strong>top-5 误差)更低的验证误差，提供了 SE 块可以在不同数据集上表现良好的证据。</li><li id="b8c7" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">并且 SENet 超过了之前最先进的模型 place 365-CNN，其前 5 名误差为 11.48%。</li></ul><h2 id="ac8a" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">3.4.COCO 上的对象检测</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/b19905a4df473ecfd38acdd79ef931a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*GyKqpXt4bbwA517uI1TQAg.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Object detection results on the COCO 40k validation set by using the basic </strong><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"><strong class="bd kw">Faster R-CNN</strong></a></figcaption></figure><ul class=""><li id="c0aa" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>用作检测网络。</li><li id="3518" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">在 COCO 的标准指标<strong class="kz ir"> AP </strong>和 AP@IoU=0.5 上，<strong class="kz ir"> SE-ResNet-50 比</strong><a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kz ir">ResNet-50</strong></a><strong class="kz ir">高出 1.3% </strong>(相对提高了 5.2%)。</li><li id="1361" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">重要的是，在 AP 指标上，SE 模块能够使更深层的架构<a class="ae kf" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kz ir"> ResNet-101 </strong> </a>受益 0.7%(相对提高 2.6%)。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="98d2" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 4。分析和解释</strong></h1><h2 id="179d" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">4.1.减速比 r</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/83532e51c31135f8dbbb9c15ee4b0824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*KSZcXr7ict8DUhu2oYN_7w.png"/></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Single-Crop Error Rates (%) on ImageNet Validation Set</strong></figcaption></figure><ul class=""><li id="a94f" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated"><strong class="kz ir"> <em class="nz"> r </em> = 16 在准确性和复杂性</strong>之间取得了良好的平衡，因此，该值用于所有实验。</li></ul><h2 id="913e" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">4.2.激励的作用</h2><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi pa"><img src="../Images/2974aa390cad878a9ecab04a01709b54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HO6y2_Ag6bAvA1mrRBT0Xg.png"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk"><strong class="bd kw">Activations induced by Excitation in the different modules of SE-ResNet-50 on ImageNet.</strong></figcaption></figure><ul class=""><li id="74e1" class="nd ne iq kz b la lb ld le lg nv lk nw lo nx ls ny nl nm nn bi translated">对于上述 5 个类别，从验证集中为每个类别抽取 50 个样本，并计算每个阶段中最后 SE 块中 50 个均匀采样的通道的平均激活。</li><li id="3c09" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">首先，在<strong class="kz ir">较低层</strong>，例如 SE_2_3，<strong class="kz ir">特征信道的重要性可能由网络早期阶段的不同类别</strong>共享。</li><li id="e360" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">第二，在<strong class="kz ir">更大的深度</strong>，例如 SE_4_6 和 SE_5_1，每个通道的值变得更加特定于类别，因为不同的类别对特征的区别值表现出不同的偏好。</li><li id="18c8" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">结果，表示学习受益于由 SE 块引起的重新校准，这在需要的程度上自适应地促进了特征提取和专门化。</li><li id="e2ed" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">最后，<strong class="kz ir">最后一级</strong>，即 SE_5_2，呈现出一种有趣的趋势<strong class="kz ir">趋向饱和状态</strong>，其中大部分激活接近 1，其余接近 0。在 SE_5_3 中也发现了类似的模式，只是尺度略有变化。</li><li id="bec7" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated">这表明在向网络提供重新校准时，SE 5 2 和 SE 5 3 没有以前的模块重要。</li><li id="1b52" class="nd ne iq kz b la no ld np lg nq lk nr lo ns ls ny nl nm nn bi translated"><strong class="kz ir">通过移除最后一级</strong>的 SE 模块，可以显著减少总参数数量，而性能损失很小。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="22bd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">SE 模块通过使网络能够执行动态的逐通道特征再校准来提高网络的代表性容量。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="5fe3" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">参考</h2><p id="221c" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg pb li lj lk pc lm ln lo pd lq lr ls ij bi translated">【2018 CVPR】【塞内】<br/> <a class="ae kf" href="https://arxiv.org/abs/1709.01507" rel="noopener ugc nofollow" target="_blank">压缩-激发网络</a></p><p id="f07d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">【2019 TPAMI】【SENet】<br/><a class="ae kf" href="https://ieeexplore.ieee.org/document/8701503" rel="noopener ugc nofollow" target="_blank">压缩和激励网络</a></p><h2 id="17f5" class="ob mm iq bd mn oc od dn mr oe of dp mv lg og oh mx lk oi oj mz lo ok ol nb om bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kx ky iq kz b la nf jr lc ld ng ju lf lg pb li lj lk pc lm ln lo pd lq lr ls ij bi translated">)(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(我)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(?)(我)(们)(都)(不)(在)(这)(些)(情)(况)(上)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(情)(况)(,)(我)(们)(还)(没)(有)(什)(么)(好)(好)(的)(情)(感)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(。 <a class="ae kf" href="https://medium.com/@sh.tsang/review-pyramidnet-deep-pyramidal-residual-networks-image-classification-85a87b60ae78" rel="noopener"> PyramidNet </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5"> DRN </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-dpn-dual-path-networks-image-classification-d0135dce8817"> DPN </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-residual-attention-network-attention-aware-features-image-classification-7ae44c4f4b8"> 残留注意网络 </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-msdnet-multi-scale-dense-networks-image-classification-4d949955f6d5"> MSDNet </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-shufflenet-v1-light-weight-model-image-classification-5b253dfe982f"> ShuffleNet V1 </a></p><p id="8b77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">物体检测<br/></strong><a class="ae kf" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kf" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kf" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae kf" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [</a><a class="ae kf" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4">G-RMI</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener">TDM</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11">SSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5">DSSD</a>][<a class="ae kf" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae kf" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a>[<a class="ae kf" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a></p><p id="6582" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">语义切分<br/></strong><a class="ae kf" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplabv 1&amp;deeplabv 2</a><a class="ae kf" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae kf" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae kf" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kf" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1">RefineNet</a><a class="ae kf" rel="noopener" target="_blank" href="/review-refinenet-multi-path-refinement-network-semantic-segmentation-5763d9da47c1"/></p><p id="fc65" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">生物医学图像分割<br/></strong>[<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kf" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-3d-u-net-volumetric-segmentation-medical-image-segmentation-8b592560fac1">3D U-Net</a>][<a class="ae kf" rel="noopener" target="_blank" href="/review-m²fcn-multi-stage-multi-recursive-input-fully-convolutional-networks-biomedical-image-4f8d5e3f07f1">M FCN</a></p><p id="3134" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir"> 实例分割 <br/> </strong> <a class="ae kf" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener"> SDS </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979"> Hypercolumn </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae kf" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></p><p id="58de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><strong class="kz ir">人体姿态估计</strong><br/><a class="ae kf" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊·尼普斯 14</a><a class="ae kf" rel="noopener" target="_blank" href="/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c">汤普逊·CVPR 15</a></p></div></div>    
</body>
</html>