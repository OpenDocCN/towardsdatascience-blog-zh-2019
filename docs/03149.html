<html>
<head>
<title>Detection Free Human Instance Segmentation using Pose2Seg and PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Pose2Seg 和 PyTorch 的免检测人体实例分割</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch-72f48dc4d23e?source=collection_archive---------10-----------------------#2019-05-20">https://towardsdatascience.com/detection-free-human-instance-segmentation-using-pose2seg-and-pytorch-72f48dc4d23e?source=collection_archive---------10-----------------------#2019-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9359" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">考虑到人类的独特性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6c32e7a2cc6ecddf0e000bb042439445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z89HWdi93dYxiXHV"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@jezael?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jezael Melgoza</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="66d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">近年来，由于现实生活应用的高需求，计算机视觉领域中与“人”相关的研究变得越来越活跃，其中之一就是实例分割。</p><p id="b5c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图像实例分割的标准方法是首先执行对象检测，然后从检测包围盒中分割对象。最近，像<a class="ae ky" href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46" rel="noopener ugc nofollow" target="_blank"> Mask R-CNN </a>这样的深度学习方法联合执行它们。然而，随着与人类相关的任务变得越来越普遍，如人类识别、跟踪等。有人可能想知道为什么“人类”这一类别的独特性没有被考虑进去。</p><p id="1273" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“人类”类别的独特性可以由姿势骨架很好地定义。此外，与使用包围盒相比，人类姿态骨架可以用于更好地区分具有严重遮挡的情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/5ab96a731d9f1878d9aec17cb7f4f6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6hGGCyC8DIJNH0Ik9blaJg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1: Heavily occluded people are better separated using human pose than using bounding-box.</figcaption></figure><p id="18cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将回顾<a class="ae ky" href="https://arxiv.org/pdf/1803.10683.pdf" rel="noopener ugc nofollow" target="_blank">“pose 2 seg:无检测人体实例分割”</a>，它提出了一种新的基于姿势的人体实例分割框架，该框架基于人体姿势来分离实例。</p><p id="9e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我将介绍两件事:第一，实例分割任务的概述。二、“Pose2Seg”概述。</p><blockquote class="lw lx ly"><p id="3f65" class="kz la lz lb b lc ld ju le lf lg jx lh ma lj lk ll mb ln lo lp mc lr ls lt lu im bi translated"><strong class="lb iu">代码提示:</strong> <br/>我们在这里分享代码<a class="ae ky" href="https://github.com/erezposner/Pose2Seg" rel="noopener ugc nofollow" target="_blank">。包括数据集和训练模型。跟着走！</a></p></blockquote></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="64d8" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">1.什么是实例分段？</h1><p id="1599" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">实例分割是一项任务，我们希望在像素级别识别每个对象。这意味着标签是类感知和实例感知的。例如，图 2(d)显示了绵羊 1、绵羊 2 等的单独标记。</p><p id="e88f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实例分段被认为是常见用例中最具挑战性的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/f4a18a302ea19dfc43e7e5e9388d344d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAM_dbxILTsoVT5GZmFjpQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2. Common computer vision use cases</figcaption></figure><ul class=""><li id="6078" class="ni nj it lb b lc ld lf lg li nk lm nl lq nm lu nn no np nq bi translated"><strong class="lb iu">分类:</strong>此图中有一个人。图 2(a)</li><li id="f26a" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">目标检测:</strong>在该图像中的这些位置有 5 只羊。图 2(b)</li><li id="cec8" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">语义分割:</strong>有羊、人、狗像素。图 2(c)</li><li id="a421" class="ni nj it lb b lc nr lf ns li nt lm nu lq nv lu nn no np nq bi translated"><strong class="lb iu">实例分割</strong>:在这些位置有 5 只不同的羊、1 个人和 1 只狗。图 2(d)</li></ul></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="5349" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">2.Pose2Seg:无检测人体实例分割</h1><h2 id="5ff2" class="nw ml it bd mm nx ny dn mq nz oa dp mu li ob oc mw lm od oe my lq of og na oh bi translated">2.1 直觉</h2><p id="e41f" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Pose2Seg 背后的主要思想是，虽然<em class="lz">通用对象实例分割</em>方法工作良好，但大多数都基于强大的对象检测基线。即首先生成大量的建议区域，然后使用非最大抑制(NMS)来移除冗余区域，如图 3 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/55589a9fa5e0956746712e6df5a33690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-5C4AU0OlRf0J81H_d-zNw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 3. (Left) Before non-max suppression, (Right) After Before non-max suppression</figcaption></figure><p id="ef1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，当同一类别的两个对象有很大的重叠时，NMS 会将其中一个对象视为多余的建议区域，并将其删除。这意味着几乎所有的对象检测方法都不能处理大重叠的情况。</p><p id="f5b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，当处理大多数“人类”类别时，它可以由姿势骨架很好地定义。如图 1 所示，人体姿态骨架更适合于区分两个纠缠在一起的人，因为它们可以提供比边界框更清晰的个人信息，如不同身体部位的位置和可见性。</p><p id="ff00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自底向上方法的主要思想是首先检测所有人的每个身体部位的关键点，然后将这些部位分组或连接以形成人体姿态的几个实例，这使得可以分离具有大重叠的两个缠绕的人体实例</p><h2 id="aef9" class="nw ml it bd mm nx ny dn mq nz oa dp mu li ob oc mw lm od oe my lq of og na oh bi translated">2.2 网络结构</h2><p id="22ac" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">总体网络结构如下图 4 所示。网络的输入是存在的所有人体实例的 RGB 图像和人体姿态。首先，使用主干网络提取图像的特征。然后，名为<em class="lz">仿射对齐</em>的模块用于根据人体姿态将感兴趣区域对齐到统一的大小(为了一致性)。此外，为每个人体实例生成<em class="lz">骨架</em>特征。</p><p id="11c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，RoI 和<em class="lz">骨架</em>特征被融合并被传递到称为 S <em class="lz"> egModule </em>的分割模块，以产生每个 RoI 的实例分割。最后，对估计矩阵进行仿射对齐操作，对每个实例进行反向对齐，得到最终的分割结果。</p><p id="a464" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络子模块将在下面的小节中详细描述。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/81c19e6c64c7deecc21e005c02539651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gHNr6o-PGpFTIyuUCDZlTw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 4. Overview of our network structure. (a) Affine-Align operation. (b) Skeleton features. (c) Structure of <em class="ok">SegModule</em></figcaption></figure><h2 id="9a73" class="nw ml it bd mm nx ny dn mq nz oa dp mu li ob oc mw lm od oe my lq of og na oh bi translated">2.3 仿射对齐操作</h2><p id="7204" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">仿射对齐操作主要受快速 R-CNN 中呈现的 RoI 汇集和掩模 R-CNN 中的 RoI 对齐的启发。但是，虽然这些根据它们的包围盒来对齐人类，但是仿射对齐用于基于人类姿态来对齐。</p><p id="0f2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，最频繁的人体姿态被离线存储，稍后在训练/推断时与每个输入姿态进行比较(参见下面的图 5)。这个想法是为每个估计的姿势选择最佳模板。这是通过估计输入姿态和模板之间的仿射变换矩阵<em class="lz"> H </em>并选择产生最佳分数的一个来实现的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/59f7e18c2fbdfe835fe0739f8c32600f.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*_1ZHl5ctMmtH92G9mHofnA.png"/></div></figure><p id="5c4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<strong class="lb iu"> <em class="lz"> P_u </em> </strong>表示姿势模板，而<strong class="lb iu"> <em class="lz"> P </em> </strong>表示单人姿势估计。矩阵 H*是为最适合的每姿态模板选择的仿射变换。最后，将产生最佳分数的变换<em class="lz"> H* </em>应用于图像或特征，并将其变换到期望的分辨率。关于仿射对齐操作还有更多细节，请参考<a class="ae ky" href="https://arxiv.org/pdf/1803.10683.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>了解更多细节。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/a12b565a725c1b9d0c27e8bd18fe60fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C3dEsahAD4R1iLCtEBV1cA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 5. Affine-Align operation</figcaption></figure><h2 id="f10b" class="nw ml it bd mm nx ny dn mq nz oa dp mu li ob oc mw lm od oe my lq of og na oh bi translated">2.4 骨骼特征</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/59ca359394dc05d73f020c9e2a4b5b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*iXuk-0cxc16sDf2O7JkHiw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 6. Skeleton feature module</figcaption></figure><p id="55c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">图 6 显示了 s <em class="lz"> keleton </em>的特征。对于该任务<a class="ae ky" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">，采用零件亲缘关系字段(PAF)</a>。PAF 的输出是每个骨架的 2 通道矢量场图。PAF 用于表示人类姿势的骨架结构以及身体部位的部位置信度图，以强调身体部位关键点周围的那些区域的重要性。</p><h2 id="faff" class="nw ml it bd mm nx ny dn mq nz oa dp mu li ob oc mw lm od oe my lq of og na oh bi translated">2.5 秒模块</h2><p id="5934" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">SegModule 是一个简单的编码器-解码器架构。一个主要的考虑是它的感受野。由于<em class="lz">骨架</em>特征是在对齐后引入的，因此 SegModule 需要有足够的感受野，以便不仅完全理解这些人工特征，而且学习它们与基础网络提取的图像特征之间的联系。因此，它是基于对准的 ROI 的分辨率来设计的。</p><p id="a5f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络从一个 7 × 7、步长为 2 的卷积层开始，其后是几个标准残差单元，以便为感兴趣区域提供足够大的感受野。然后，双线性上采样层用于恢复分辨率，另一个残差单元与 1 × 1 卷积层一起用于预测最终结果。这样一个具有 10 个剩余单元的结构可以实现大约 50 个像素的感受野，对应于 64 × 64 的对准尺寸。单元越少，网络的学习能力越差，单元越多，学习能力的提高越小。</p><h1 id="7bcf" class="mk ml it bd mm mn oo mp mq mr op mt mu jz oq ka mw kc or kd my kf os kg na nb bi translated">3.实验和结果</h1><p id="a378" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">Pose2Seg 在两个数据集上进行了评估:(1) OCHuman，这是本文提出的最大的验证数据集，主要针对严重遮挡的人；以及(2) COCOPersons(可可的人称类别)，包含日常生活中最常见的场景。</p><p id="803b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该算法主要与常用的基于检测的实例分割框架 Mask-RCNN 进行比较。</p><p id="d387" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在使用 OCHuman 数据集对遮挡数据的测试中，如表 1 所示，Pose2Seg 框架实现了比 Mask R-CNN 高近 50%的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/a296132d67f4d48fd369c9b3dee8274b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fOSFssaxXOk-WxAfsj7nuQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Table 1. Performance on occlusion. All methods are trained on COCOPersons train split, and tested on OCHuman</figcaption></figure><p id="5d6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一般情况下的测试中，在 COCOPerson 验证集 Pose2Seg 上的实例分割任务获得了 0.582 AP(平均精度)，而 Mask R-CNN 获得了 0.532。见表 2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/5c1a7506a2064c22db3f723828f91f4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2E-UkzjuEWyXh32qntJEaA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Table 2. Performance on general cases.</figcaption></figure><p id="1b59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要从基于边框的框架中更好地理解 Pose2Seg 的优势，请参见下面的图 7。看看“开箱即用”的器官在 Mask R-CNN 中是如何不分段的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/d4c74adc9b2a1d5fe576c354c2289621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eta4Sry2-NZaKdKx1B4CBA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 7. Pose2Seg method’s results vs. Mask R-CNN on occlusion cases. Bounding-boxes are generated using predicted masks for better visualization and comparison.</figcaption></figure><h1 id="0dad" class="mk ml it bd mm mn oo mp mq mr op mt mu jz oq ka mw kc or kd my kf os kg na nb bi translated">4.结论</h1><p id="64fc" class="pw-post-body-paragraph kz la it lb b lc nc ju le lf nd jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">如果你对源代码感兴趣，可以在我的<a class="ae ky" href="https://github.com/erezposner/Pose2Seg" rel="noopener ugc nofollow" target="_blank"> Pose2Seg GitHub 库</a>中找到。</p><p id="bad9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一如既往，如果您有任何问题或意见，请随时在下面留下您的反馈，或者您可以随时通过<a class="ae ky" href="http://www.linkedin.com/in/erezposner" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p><p id="530c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在那之前，下一篇文章再见！😄</p></div></div>    
</body>
</html>