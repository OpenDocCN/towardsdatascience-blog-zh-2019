<html>
<head>
<title>Understand the architecture of CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解 CNN 的架构</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-the-architecture-of-cnn-90a25e244c7?source=collection_archive---------0-----------------------#2019-10-31">https://towardsdatascience.com/understand-the-architecture-of-cnn-90a25e244c7?source=collection_archive---------0-----------------------#2019-10-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/077cda2e8e9bab373f159f903613ab53.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*81Y95NKxLzXLEut7nepmZA.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Architecture of VGG16 (a CNN model)</figcaption></figure><p id="5e02" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2012 年，一场革命发生了:在一年一度的 ILSVRC 计算机视觉比赛期间，一种新的深度学习算法打破了记录！这是一个名为<strong class="ka ir"> Alexnet </strong>的卷积神经网络。</p><p id="8179" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">卷积神经网络的方法论类似于传统的监督学习方法:它们接收输入图像，检测每个图像的特征，然后在其上拖动一个分类器。</p><p id="33af" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">但是，功能是自动学习的！CNN 自己执行提取和描述特征的所有繁琐工作:在训练阶段，分类误差被最小化以优化分类器的参数<strong class="ka ir">和</strong>特征！</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="5c59" class="ld le iq bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">什么是 CNN？</h1><p id="d3bf" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">卷积神经网络是指神经网络的一个子类:因此，它们具有神经网络的所有特征。然而，CNN 是专门为处理输入图像而设计的。他们的架构更加具体:它由两个主要模块组成。</p><p id="cb0e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">第一个模块</strong>使这种类型的神经网络具有特殊性，因为它起着特征提取器的作用。为此，它通过应用卷积过滤操作来执行模板匹配。第一层使用几个卷积核对图像进行滤波，并返回“<strong class="ka ir">特征图</strong>，然后对其进行归一化(使用激活函数)和/或调整大小。</p><p id="033d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这个过程可以重复几次:我们过滤用新内核获得的特征图，这给了我们新的特征图来归一化和调整大小，我们可以再次过滤，等等。最后，最后的特征映射的值被连接成一个向量。这个向量定义了第一个模块的输出和第二个模块的输入。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/715aefd051aa5df6ff79858e506f8254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/1*L7SsTcJO4EY4CbDVHEFkfQ.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The first block is encircled in black</figcaption></figure><p id="e48c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">第二块</strong>不是 CNN 的特征:事实上，它位于所有用于分类的神经网络的末端。输入向量值被转换(使用几个线性组合和激活函数)以向输出返回一个新的向量。这最后一个向量包含与类别一样多的元素:元素 I 表示图像属于类别 I 的概率。因此，每个元素在 0 和 1 之间，并且所有元素的总和等于 1。这些概率由该块的最后一层(因此也是网络的最后一层)计算，该层使用一个<strong class="ka ir">逻辑函数</strong>(二元分类)或一个<strong class="ka ir"> softmax 函数</strong>(多类分类)作为激活函数。</p><p id="6f12" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与普通神经网络一样，层的参数由梯度反向传播确定:在训练阶段，<strong class="ka ir">交叉熵</strong>被最小化。但是在 CNN 的情况下，这些参数特别指的是图像特征。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/78e3081fb43aa3f567173b194362f230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/1*cF4OKyH8l-nUNnzMUkGfiA.gif"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">The second block is encircled in black</figcaption></figure><h1 id="4a74" class="ld le iq bd lf lg ml li lj lk mm lm ln lo mn lq lr ls mo lu lv lw mp ly lz ma bi translated">有线电视新闻网的不同层次</h1><p id="5403" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">卷积神经网络有四种类型的层:卷积层、<strong class="ka ir">汇集</strong>层、<strong class="ka ir"> ReLU 校正</strong>层和<strong class="ka ir">全连接</strong>层。</p><h2 id="3615" class="mq le iq bd lf mr ms dn lj mt mu dp ln kj mv mw lr kn mx my lv kr mz na lz nb bi translated">卷积层</h2><p id="a525" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">卷积层是卷积神经网络的关键组成部分，并且总是至少是它们的第一层。</p><p id="4ab9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">其目的是检测作为输入接收的图像中一组特征的存在。这是通过卷积滤波来实现的:原理是在图像上“拖动”一个表示特征的窗口，并计算该特征与扫描图像各部分之间的卷积积。<strong class="ka ir">一个特征被视为一个过滤器</strong>:这两个术语在上下文中是等价的。</p><p id="5dc4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，卷积层接收几个图像作为输入，并用每个滤波器计算每个图像的卷积。过滤器与我们想要在图像中找到的特征完全对应。</p><p id="ade2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们为每一对(图像，过滤器)获得一个<strong class="ka ir">特征图</strong>，它告诉我们特征在图像中的位置:值越高，图像中相应的位置就越像特征。</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nc"><img src="../Images/221851203a0712a295dd5028aa75d6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4urhxejLy6xVdAV6VxniZg.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk"><strong class="bd nh">Convolutional layer</strong> (source: <a class="ae ni" href="https://www.embedded-vision.com/industry-analysis/blog/what%E2%80%99s-difference-between-cnn-and-rnn" rel="noopener ugc nofollow" target="_blank">https://www.embedded-vision.com</a>)</figcaption></figure><p id="61e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与传统方法不同，特征不是根据特定的形式(例如 SIFT)预先定义的，而是由网络在训练阶段学习的！滤波器核指的是卷积层权重。<strong class="ka ir">它们被初始化，然后使用梯度下降通过反向传播进行更新</strong>。</p><h2 id="78eb" class="mq le iq bd lf mr ms dn lj mt mu dp ln kj mv mw lr kn mx my lv kr mz na lz nb bi translated">汇集层</h2><p id="41e3" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">这种类型的图层通常位于两个卷积图层之间:它接收多个要素地图，并对每个要素地图应用汇集操作。</p><p id="4e20" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">汇集操作包括<strong class="ka ir">减小图像的尺寸</strong>，同时<strong class="ka ir">保留它们的重要特征</strong>。</p><p id="ec4b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为此，我们将图像切割成规则的单元格，然后在每个单元格内保留最大值。在实践中，经常使用小正方形单元格来避免丢失太多信息。最常见的选择是<strong class="ka ir">不与</strong>重叠的<strong class="ka ir"> 2x2 相邻单元格</strong>，或<strong class="ka ir"> 3x3 单元格</strong>，彼此相隔 2 个像素的步长(因此<strong class="ka ir">与</strong>重叠)。</p><p id="9ef3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们在输出中得到与输入相同数量的特征地图，但是这些要小得多。</p><p id="08d8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">池层<strong class="ka ir">减少了网络</strong>中的参数和计算的数量。这提高了网络的效率，避免了过度学习。</p><p id="7249" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">与输入中接收到的最大值相比，合并后获得的最大值在要素地图中的定位不太准确，这是一个很大的优势！例如，当你想识别一只狗时，它的耳朵不需要尽可能精确地定位:知道它们几乎位于头部旁边就足够了！</p><h2 id="497e" class="mq le iq bd lf mr ms dn lj mt mu dp ln kj mv mw lr kn mx my lv kr mz na lz nb bi translated">ReLU 校正层</h2><p id="0613" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">ReLU(整流线性单位)是指由<em class="nj"> ReLU(x)=max(0，x) </em>定义的实非线性函数。从视觉上看，它如下所示:</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d88310149db004d1a87bdda678c3f201.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*6e6PP99KBS8RJoY6LPbUIA.png"/></div></figure><p id="bb95" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">ReLU 校正层用零替换作为输入接收的所有负值。它作为一个<strong class="ka ir">激活功能</strong>。</p><h2 id="bf14" class="mq le iq bd lf mr ms dn lj mt mu dp ln kj mv mw lr kn mx my lv kr mz na lz nb bi translated">全连接层</h2><p id="b3bc" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">无论是否卷积，全连接层总是神经网络的最后一层，因此它不是 CNN 的特征。</p><p id="29d6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这种类型的层接收输入向量并产生新的输出向量。为此，它对接收到的输入值应用一个<strong class="ka ir">线性组合</strong>和<strong class="ka ir">，然后可能应用一个激活函数</strong>。</p><p id="4ca7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后一个全连接层将图像分类为网络的输入:它返回大小为 N 的向量，其中 N 是我们的图像分类问题中的类的数量。向量的每个元素指示输入图像属于一个类别的概率。</p><p id="d671" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">因此，为了计算概率，全连接层将每个输入元素乘以权重，进行求和，然后应用激活函数(如果 N=2，则为逻辑，如果 N&gt;2，则为 softmax)。这相当于将输入向量乘以包含权重的矩阵。每个输入值都与所有输出值相连接，这一事实解释了术语“完全连接”的含义。</p><p id="eee1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">卷积神经网络学习权重值的方式与其学习卷积层滤波器的方式相同:在训练阶段，通过<strong class="ka ir">梯度</strong>的反向传播。</p><p id="32ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">完全连接的图层决定了影像中要素的位置与类之间的关系。实际上，输入表是前一层的结果，它对应于给定特征的特征图:<strong class="ka ir">高值指示图像</strong>中该特征的位置(根据汇集的不同，更精确或更不精确)<strong class="ka ir">。如果图像中某一点的特征位置是某一类别的特征，则表中的相应值被赋予显著的权重。</strong></p><h1 id="9288" class="ld le iq bd lf lg ml li lj lk mm lm ln lo mn lq lr ls mo lu lv lw mp ly lz ma bi translated">各层的参数化</h1><p id="8551" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">卷积神经网络与其他网络的不同之处在于层的堆叠方式，以及参数化。</p><p id="911c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">卷积层和池层确实有超参数，也就是说您必须首先定义其值的参数。</p><p id="a373" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">卷积图层和池图层的输出要素地图的大小取决于超参数。</p><p id="340c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">每个图像(或特征图)都是 W×H×D，其中 W 是其宽度(以像素为单位), H 是其高度(以像素为单位), D 是通道数(黑白图像为 1，彩色图像为 3)。</p><h2 id="16fd" class="mq le iq bd lf mr ms dn lj mt mu dp ln kj mv mw lr kn mx my lv kr mz na lz nb bi translated">卷积层有四个超参数:</h2><p id="8cfe" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">1.过滤器的数量 K</p><p id="7da6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.过滤器的尺寸:每个过滤器的尺寸为 F×F×D 像素。</p><p id="2432" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">3.拖动与图像上的滤镜相对应的窗口的步骤。例如，步长为 1 意味着一次移动窗口一个像素。</p><p id="1633" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">4.零填充 P:给图层的输入图像添加一个 P 像素厚的黑色轮廓。没有该轮廓，出口尺寸更小。因此，P=0 的卷积层叠得越多，网络的输入图像就越小。我们很快丢失了大量信息，这使得提取特征的任务变得困难。</p><p id="bcde" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于每个尺寸为 W×H×D 的输入图像，池层返回一个尺寸为 Wc×Hc×Dc 的矩阵，其中:</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b4e93b0941822551dc99a55a7d6874b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/1*c5y84EOlYMllK9B4bfRHqg.gif"/></div></figure><p id="576f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">选择<strong class="ka ir"> P=F-1/2 和 S=1 </strong>给出与输入中接收到的相同宽度和高度的特征地图。</p><h2 id="f8b1" class="mq le iq bd lf mr ms dn lj mt mu dp ln kj mv mw lr kn mx my lv kr mz na lz nb bi translated">池层有两个超参数:</h2><p id="0181" class="pw-post-body-paragraph jy jz iq ka b kb mb kd ke kf mc kh ki kj md kl km kn me kp kq kr mf kt ku kv ij bi translated">1.单元格的大小 F:图像被分成大小为 F×F 像素的正方形单元格。</p><p id="e1a3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">2.步骤 S:单元格之间相隔 S 个像素。</p><p id="c2f8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于每个尺寸为 W×H×D 的输入图像，池层返回一个尺寸为 Wp×Hp×Dp 的矩阵，其中:</p><figure class="mh mi mj mk gt jr gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/143d51ef7a15422c61efde7c53db4be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/1*P7aAya1BWr_X3_2Dd0176g.gif"/></div></figure><p id="5547" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">就像叠加一样，超参数的选择是根据一个经典方案进行的:</p><ul class=""><li id="19ee" class="nm nn iq ka b kb kc kf kg kj no kn np kr nq kv nr ns nt nu bi translated">对于卷积层，滤镜很小，每次在图像上拖动一个像素。选择零填充值，以便输入音量的宽度和高度在输出时不会改变。一般来说，我们然后选择 F=3，P=1，S=1 或者 F=5，P=2，S=1</li><li id="6762" class="nm nn iq ka b kb nv kf nw kj nx kn ny kr nz kv nr ns nt nu bi translated">对于池层，<strong class="ka ir"> F=2，S=2 </strong>是明智的选择。这个<strong class="ka ir">消除了 75%的输入像素</strong>。我们也可以选择 F=3，S=2:在这种情况下，单元格重叠。选择较大的小区会导致太多的信息丢失，并且在实践中导致不太好的结果</li></ul></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="9df6" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，你有建立自己的 CNN 的基础！但是不要厌倦这样做:已经有许多适合大多数应用程序的体系结构。</p><p id="540d" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在实践中，我强烈建议你不要从零开始创建一个卷积神经网络来解决你的问题:最有效的策略是采用一个现有的对大量图像进行良好分类的网络(如 ImageNet)并应用<strong class="ka ir">迁移学习</strong>。</p></div></div>    
</body>
</html>