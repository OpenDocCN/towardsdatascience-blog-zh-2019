<html>
<head>
<title>Automated movie Tagging- A Multiclass classification problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动电影标记——一个多类分类问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automated-movie-tagging-a-multiclass-classification-problem-721eb7fb70c2?source=collection_archive---------17-----------------------#2019-05-15">https://towardsdatascience.com/automated-movie-tagging-a-multiclass-classification-problem-721eb7fb70c2?source=collection_archive---------17-----------------------#2019-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8751" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从电影概要中自动提取标签的多类分类问题。</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/caba343e253be1812862b757d90c4bb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5pa1xSVpboCDvIHM7lEFnQ.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by: Kyle Hinkson on Unsplash</figcaption></figure><p id="90f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">电影标签揭示了关于电影的各种各样的异质信息，如类型、情节结构、配乐、元数据、视觉和情感体验。这些信息对于构建自动系统来为电影创建标签很有价值。</p><p id="de81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自动标记系统也有助于推荐引擎改进相似电影的检索，以及帮助观众预先知道对电影的期望。</p><p id="360a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将使用广泛的机器学习算法和 NLTK 功能从电影概要中挖掘出电影标签。所以让我们开始吧！</p><h1 id="22f4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">数据来源</strong></h1><p id="53ec" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们在本案例研究中使用的数据是从<a class="ae ms" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags#mpst_full_data.csv" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>中的数据集收集的，该数据集包含超过 14k 部电影和大约 142 个独特的标签。</p><p id="c48c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看看数据:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mv"><img src="../Images/32cca7f8cc5d3a41ed8f83032d89adfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPjbpVcY9pEfFyZ_CxqFTw.png"/></div></div></figure><p id="5511" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上表中，我们可以看到每部电影都有概要和与之对应的几个异构标签。此外，数据是从 IMDB 和维基百科收集的。</p><h1 id="88c7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">探索性数据分析</h1><p id="c093" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">首先，我们尝试从数据集中删除重复项并进行分析:</p><ol class=""><li id="6297" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">每个问题的标签数。</li><li id="bd86" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">最常见的标签。</li><li id="4a98" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">数据集中标签的频率。</li></ol><p id="b6aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">每个问题的标签数量:- </strong></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk"><a class="ae ms" href="https://github.com/shamim-io/MPST-Movie-Plot-Synopses-with-Tags/blob/master/theshamimahmed%40gmail.com_CS1_V3.ipynb" rel="noopener ugc nofollow" target="_blank">Code</a></figcaption></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nk"><img src="../Images/df9c2150c37020818e5c94f643f089d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2NPuxhPd1JLVZIqb93otwg.png"/></div></div></figure><p id="85e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在进行了一些计算后，我们发现大多数电影都有一个标签。每部电影的平均标签数为 2.99。</p><p id="974e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最常用的标签:- </strong></p><p id="130e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为此，我们将绘制一个单词云并将其可视化。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nl"><img src="../Images/6bf20d01248aba0262b7df9ca4ae9f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1WeYhZSZfflRNNyZd8rTeg.png"/></div></div></figure><p id="0eab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">标签的频率:- </strong></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nm"><img src="../Images/da2bfef7b49c9c6f5e308e2b3b25aab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wmov3vNhKwdvmddGPhAfSg.png"/></div></div></figure><p id="c04b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在分析了上述 EDA 之后，我们得出结论，一些电影有大量的标签，但大多数电影只标记了一两个标签。谋杀、暴力、倒叙和浪漫是语料库中出现频率最高的四个标签。最不常用的标签是愚蠢，聪明，感觉良好，幽闭恐惧症。</p><h1 id="e6b0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">情节提要的清理和预处理</h1><p id="1fe9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，我们已经完成了重复数据删除，我们的数据需要一些预处理，然后才能继续分析和建立预测模型。</p><p id="83e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在预处理阶段，我们按以下顺序执行以下操作</p><ol class=""><li id="b24a" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">从移除 HTML 标签开始</li><li id="25da" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">删除任何标点符号或有限的一组特殊字符，如、或。或者#，等等。</li><li id="b6c5" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">检查单词是否由英文字母组成，并且不是字母数字</li><li id="b6fb" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">检查单词的长度是否大于 2(据调查，没有两个字母的形容词)</li><li id="0a62" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">将单词转换成小写</li><li id="7616" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">移除<a class="ae ms" href="https://en.wikipedia.org/wiki/Stop_words" rel="noopener ugc nofollow" target="_blank">停用词</a></li><li id="7c58" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">单词被词汇化<a class="ae ms" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank"/>——第三人称的单词被改成第一人称，过去时态和将来时态的动词被改成现在时态。</li><li id="5d1d" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">最后，雪球<a class="ae ms" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">用词干</a>这个词(据观察，它比波特<a class="ae ms" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">用词干</a>要好)</li></ol><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="ccc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经对标签分析有了简单的了解，并在更大程度上清理了我们的情节概要，让我们开始真正有趣的部分。</p><h1 id="48a6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">利用情节概要预测标签的机器学习方法</h1><p id="7da6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最重要的工作是将数据分成训练集和测试集。集合的训练部分将用于训练模型。我们使用测试部分<a class="ae ms" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank">交叉验证</a>并测试训练好的模型。为此，我们根据文本语料库中给定的列<code class="fe nn no np nq b">split</code>拆分数据。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="840b" class="nv lw it nq b gy nw nx l ny nz">x_train=new_data.loc[(new_data['split'] == 'train') |(new_data['split'] == 'val')]<br/>x_test=new_data.loc[(new_data['split'] == 'test')]</span></pre><p id="dad0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们定义随机基线模型来比较我们提出的模型在预测电影标签的任务中的性能。基线模型将所有标签分配给所有电影。</p><p id="9e30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但在继续之前，让我们先了解几个术语:</p><p id="6a34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Pipeline </strong> </a>类允许将多个进程粘贴到单个 Scikit-learn 估计器中。我们将在开始部分使用相同的。</p><p id="afe7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> OneVSRest 分类器</strong> </a>，对于每个分类器，该类与所有其他类匹配。</p><p id="77a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">计数矢量器</strong> </a>提供了一种简单的方法，既可以标记一组文本文档，构建一个已知单词的词汇表，还可以使用这个词汇表对新文档进行编码。</p><p id="54e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">tfidfttransformer</strong></a><strong class="lb iu"/>将计数矩阵转换为归一化的 tf 或 tf-idf 表示。CountVectorizer 和 TfidfTransformer(use _ IDF = False)都产生词频，TfidfTransformer 正在对计数进行规范化。</p><p id="f595" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">A <a class="ae ms" href="https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">宏观平均</strong> </a>计算<a class="ae ms" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">精度</strong> </a>，<a class="ae ms" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">召回</strong> </a>和<a class="ae ms" href="https://en.wikipedia.org/wiki/F1_score" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">f1-独立地为每个类评分</strong> </a>，然后取平均值(因此平等地对待所有类)，而<a class="ae ms" href="https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">微观平均</strong> </a>将聚集所有类的贡献来计算平均度量。在多类别分类设置中，如果您怀疑可能存在类别不平衡(例如，一个类别的示例可能比其他类别的多得多)，微平均值是更好的选择。</p><p id="3a6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有上述术语需要更广泛的解释，但那是另一天的故事。</p><p id="363f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这是一个多类分类问题，我们将上述模块用于我们的基线和特征工程(将在下面讨论)模型。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="6e95" class="nv lw it nq b gy nw nx l ny nz">Micro-average quality numbers<br/>Precision: 0.1586, Recall: 0.3639, F1-measure: 0.2209<br/>Macro-average quality numbers<br/>Precision: 0.0733, Recall: 0.1752, F1-measure: 0.0969</span><span id="1a17" class="nv lw it nq b gy oa nx l ny nz">               precision    recall  f1-score   support<br/>avg / total       0.20      0.36      0.25      9020</span></pre><p id="dace" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到该模型在只有 25%的微观 f1 分数的情况下表现不佳。让我们尝试使用<a class="ae ms" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">网格搜索</strong> </a>超参数调整建立另一个模型。我们将使用逻辑回归和 oneVSRest 分类器作为我们的分类算法。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="7f6a" class="nv lw it nq b gy nw nx l ny nz">vectorizer = CountVectorizer(min_df=0.00009, max_features=50000, tokenizer = <strong class="nq iu">lambda</strong> x: x.split())<br/>vectorizer.fit(x_train['preprocessed_data'])<br/>x_train_multilabel = vectorizer.transform(x_train['preprocessed_data'])<br/>x_test_multilabel = vectorizer.transform(x_test['preprocessed_data'])</span></pre><p id="03d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">网格搜索交叉验证:</strong></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="1906" class="nv lw it nq b gy nw nx l ny nz">0.10172648121582308<br/>{'estimator__alpha': 1e-05}</span></pre><p id="4c73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在使用网格搜索得到的最佳超参数<code class="fe nn no np nq b">1e-05</code>建立模型。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="a576" class="nv lw it nq b gy nw nx l ny nz">Micro-average quality numbers<br/>Precision: 0.0678, Recall: 0.3704, F1-measure: 0.1146<br/>Macro-average quality numbers<br/>Precision: 0.0471, Recall: 0.3029, F1-measure: 0.0717<br/>              precision    recall  f1-score   support<br/>avg / total       0.16      0.37      0.21      9020</span></pre><p id="b41d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到我们的基线模型仍然没有改进。我们使用不同的模型进行了一些其他的实验，我们可以得到以下结果。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6db13c54079952593877458aefb8ac13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*_ZsdgxiC8x35QBFnklbt3g.png"/></div></figure><h1 id="8ad9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">特征工程</h1><p id="cf14" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在，为了提高模型的性能，非常需要特征工程。在进行特征工程之前，我们考虑的参数是:-</p><ol class=""><li id="e3e7" class="mw mx it lb b lc ld lf lg li my lm mz lq na lu nb nc nd ne bi translated">提取单词 n-grams (n=1，2，3)</li><li id="c78d" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">字符 n 元语法(n=3，4)</li><li id="04b7" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">跳过 n 元语法(n=2，3)</li></ol><p id="4f81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在情节概要上执行所有上述操作，因为它们是强词汇表示。我们使用术语频率-逆文档频率(TF-IDF)作为加权方案。</p><p id="4ef2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在上面已经看到，每个大纲的标签分布并不均匀。有些大纲有 20 个标签，而有些只有 1 个标签。但是平均来说，每个大纲的标签数量是 2.9 个(见 EDA)。所以我们会考虑每个情节大纲 4 个标签，并进行我们的操作。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="927e" class="nv lw it nq b gy nw nx l ny nz">vectorizer = CountVectorizer(tokenizer = <strong class="nq iu">lambda</strong> x: x.split(','), binary='true', max_features = 4)<br/>multilabel_y_train = vectorizer.fit_transform(y_train)<br/>multilabel_y_test = vectorizer.transform(y_test)</span></pre><p id="4f95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">什么是 n-gram，uni-gram，bi-gram，tri-gram？</p><p id="28ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算语言学和概率领域，n 元语法是来自给定文本或语音样本的 n 个项目的连续序列。使用拉丁数字前缀，大小为 1 的 n-gram 被称为“<strong class="lb iu">unigram</strong>”；尺寸 2 是一个“<strong class="lb iu">二元组</strong>”(或者，不太常见的是，“二元组”)；大小 3 是一个“<strong class="lb iu">三元组</strong>”。<em class="oc"> ( </em> <a class="ae ms" href="https://en.m.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> <em class="oc">维基</em> </a> <em class="oc"> ) </em></p><h2 id="bb13" class="nv lw it bd lx od oe dn mb of og dp mf li oh oi mh lm oj ok mj lq ol om ml on bi translated">Unigram</h2><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="e835" class="nv lw it nq b gy nw nx l ny nz">0.4297968028569633<br/>{'estimator__alpha': 0.001}</span></pre><p id="5627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注:- <code class="fe nn no np nq b">SGDClassifier</code>与<code class="fe nn no np nq b">log</code>的损失是<a class="ae ms" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">的 Logistic 回归</a>。现在让我们来构建 unigram 模型。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="7a01" class="nv lw it nq b gy nw nx l ny nz">Micro-average quality numbers<br/>Precision: 0.3311, Recall: 0.6079, F1-measure: 0.4287<br/>Macro-average quality numbers<br/>Precision: 0.3223, Recall: 0.5954, F1-measure: 0.4118<br/>              precision    recall  f1-score   support<br/><br/>           0       0.20      0.54      0.29       308<br/>           1       0.25      0.53      0.34       507<br/>           2       0.48      0.64      0.55       844<br/>           3       0.36      0.68      0.47       552<br/><br/>   micro avg       0.33      0.61      0.43      2211<br/>   macro avg       0.32      0.60      0.41      2211<br/>weighted avg       0.36      0.61      0.44      2211</span></pre><p id="2f37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们观察到模型的 f1 值、精确度和召回率都有显著提高。类似地，我们通过改变<code class="fe nn no np nq b">ngram_range=(2,2)</code>、三元组<code class="fe nn no np nq b">ngram_range=(3,3)</code>和<code class="fe nn no np nq b">ngram_range=(1,3)</code>对二元组执行相同的操作。我们观察到所有的模型都比我们的基线模型表现得更好。一旦我们建立了所有的 n 元模型。我们组合这些特征，并在其上运行我们的分类器。</p><h2 id="e843" class="nv lw it bd lx od oe dn mb of og dp mf li oh oi mh lm oj ok mj lq ol om ml on bi translated"><strong class="ak">单字+双字+单字</strong></h2><p id="d99e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">有一个非常好的方法来组合 Scipy 提供的稀疏矩阵，称为<code class="fe nn no np nq b">hstack</code>或水平堆叠。我们将使用相同的来组合我们的特征。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="926d" class="nv lw it nq b gy nw nx l ny nz"><strong class="nq iu">from</strong> <strong class="nq iu">scipy.sparse</strong> <strong class="nq iu">import</strong> coo_matrix, hstack<br/>train_1 = hstack((x_train_multilabe_uni, x_train_multilabe_bi),format="csr",dtype='float64')</span><span id="7806" class="nv lw it nq b gy oa nx l ny nz">test_1 = hstack((x_test_multilabel_uni, x_test_multilabel_bi),format="csr",dtype='float64')</span><span id="c6f3" class="nv lw it nq b gy oa nx l ny nz">train_2 = hstack((train_1, x_train_1),format="csr",dtype='float64')<br/>test_2 = hstack((test_1, x_test_1),format="csr",dtype='float64')</span></pre><p id="a78b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">型号:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="78fc" class="nv lw it nq b gy nw nx l ny nz">Micro-average quality numbers<br/>Precision: 0.3509, Recall: 0.6065, F1-measure: 0.4446<br/>Macro-average quality numbers<br/>Precision: 0.3358, Recall: 0.5894, F1-measure: 0.4232<br/>              precision    recall  f1-score   support<br/><br/>           0       0.21      0.55      0.30       308<br/>           1       0.28      0.47      0.35       507<br/>           2       0.48      0.67      0.56       844<br/>           3       0.38      0.68      0.48       552<br/><br/>   micro avg       0.35      0.61      0.44      2211<br/>   macro avg       0.34      0.59      0.42      2211<br/>weighted avg       0.37      0.61      0.46      2211</span></pre><p id="b916" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到性能再次提高。但是，让我们再深入一点，看看我们能否将 f1 分数提高到 50%以上。</p><h2 id="08fd" class="nv lw it bd lx od oe dn mb of og dp mf li oh oi mh lm oj ok mj lq ol om ml on bi translated">字符 3 克</h2><p id="f936" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">char n-gram 是来自给定文本或语音样本的 n 个字符的连续序列。在 Char 3-gram 特征化过程中，我们所做的唯一改变是将<code class="fe nn no np nq b">analyzer = 'char'</code>作为矢量函数的一个参数。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="db62" class="nv lw it nq b gy nw nx l ny nz">vectorizer = TfidfVectorizer(sublinear_tf=<strong class="nq iu">True</strong>, strip_accents='unicode', analyzer='char', ngram_range=(3, 3),  max_features=20000)<br/>x_train_3char = vectorizer.fit_transform(X_train)<br/>x_test_3char = vectorizer.transform(X_test)</span></pre><p id="832e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我们继续进行网格搜索和建模。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="mt mu l"/></div></figure><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="08ee" class="nv lw it nq b gy nw nx l ny nz">Micro-average quality numbers<br/>Precision: 0.3567, Recall: 0.6680, F1-measure: 0.4651<br/>Macro-average quality numbers<br/>Precision: 0.3408, Recall: 0.6407, F1-measure: 0.4418<br/>              precision    recall  f1-score   support<br/><br/>           0       0.22      0.52      0.31       308<br/>           1       0.28      0.62      0.38       507<br/>           2       0.49      0.74      0.59       844<br/>           3       0.38      0.68      0.49       552<br/><br/>   micro avg       0.36      0.67      0.47      2211<br/>   macro avg       0.34      0.64      0.44      2211<br/>weighted avg       0.37      0.67      0.48      2211</span></pre><p id="6745" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也对 Char 4-gram 执行相同的操作，但是放置<code class="fe nn no np nq b">analyzer =char</code> <code class="fe nn no np nq b">ngram_range = (4,4)</code>，我们得到下面的结果。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="dbc9" class="nv lw it nq b gy nw nx l ny nz">               precision    recall  f1-score<br/>weighted avg       0.39      0.65      0.49</span></pre><p id="ae31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们接近 50%的 f1 分数。我们还试图通过结合 char 3 和 char 4-gram 特性来构建一个模型，但是这个模型并没有显示出任何显著的改进。</p><p id="8bdc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在上面做 EDA 时已经看到，每个大纲的平均标签数是 2.9。所以现在我们会考虑每个剧情梗概 3 个标签。让我们看看我们能否让性能超过 50%大关。</p><h1 id="cda7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">带 3 个标签的型号</h1><p id="6641" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">像往常一样，我们从类标签的二进制矢量化开始，这次有 3 个类。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="ef5b" class="nv lw it nq b gy nw nx l ny nz">vectorizer = CountVectorizer(tokenizer = <strong class="nq iu">lambda</strong> x: x.split(','), binary='true', max_features = 3)<br/>multilabel_y_train = vectorizer.fit_transform(y_train)<br/>multilabel_y_test = vectorizer.transform(y_test)</span></pre><p id="571b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如上面解释的模型，我们建立了一元，二元，三元模型，但它们的行为与我们的 4 类标签模型相同。然后我们尝试了角色级别的模型，嘣！！该车型的性能超过了 f1 得分的 50%。</p><p id="9dd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于使用 3 个类级别的 char 3-gram，我们得到了以下结果。</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="be7f" class="nv lw it nq b gy nw nx l ny nz">               precision    recall  f1-score   support<br/><br/>           0       0.28      0.58      0.38       507<br/>           1       0.50      0.73      0.59       844<br/>           2       0.39      0.63      0.48       552<br/><br/>   micro avg       0.39      0.66      0.49      1903<br/>   macro avg       0.39      0.65      0.48      1903<br/>weighted avg       0.41      0.66      0.50      1903</span></pre><p id="1059" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似地，对于 char 4-gram:</p><pre class="kk kl km kn gt nr nq ns nt aw nu bi"><span id="4988" class="nv lw it nq b gy nw nx l ny nz">               precision    recall  f1-score   support<br/><br/>           0       0.29      0.61      0.40       507<br/>           1       0.49      0.74      0.59       844<br/>           2       0.41      0.68      0.51       552<br/><br/>   micro avg       0.41      0.69      0.51      1903<br/>   macro avg       0.40      0.68      0.50      1903<br/>weighted avg       0.42      0.69      0.52      1903</span></pre><h1 id="b19f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">总结和观察</h1><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oo"><img src="../Images/537ed0bbc95991dbc37bf24ee4b93dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RJqYQDf4q0TxAB4jTtmSw.png"/></div></div></figure><h1 id="3686" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论和后续步骤:</h1><ol class=""><li id="dba8" class="mw mx it lb b lc mn lf mo li op lm oq lq or lu nb nc nd ne bi translated">我们最初建立了基线模型。基线模型的主要问题是地块概要不平衡，每个概要的标签数量变化很大。</li><li id="b8c5" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">在 EDA 中，我们观察到平均每部电影有 2.9 个标签。所以我们确实选择了 4 个标签和 3 个标签建立了 2 套模型，并对它们进行了很大程度的分析。</li><li id="0a8b" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">在 4 标签模型(3char + 4char gram)的情况下，模型表现最佳，微 f1 分数为 49%，这与基线模型相比有显著提高。</li><li id="f5db" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">在 3 标签模型的情况下，再次(3 察尔+4 察尔克)模型表现最佳，微 f1 分数为 51%，这也是从基线模型的显著增加。</li><li id="251f" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">有一点我注意到，我的模型比他们在研究论文中使用的模型表现得更好，这是一个令人满意的问题。</li><li id="820e" class="mw mx it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">最后，我也使用了语义向量，如 word2vec、手套向量和潜在狄利克雷分配(LDA)来进一步改进我的模型，但它们没有多大帮助。</li></ol><p id="ad5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">源代码可以在我的<a class="ae ms" href="https://github.com/shamim-io/MPST-Movie-Plot-Synopses-with-Tags" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Github repo </strong> </a>上找到。我期待听到任何反馈或问题。</p><p id="7678" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">研究论文:</strong><a class="ae ms" href="https://arxiv.org/pdf/1802.07858.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.07858.pdf</a></p><p id="022b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据来源:</strong><a class="ae ms" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags#mpst_full_data.csv" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/cryptexcode/mpst-movie-plot-synopses-with-tags # mpst _ full _ Data . CSV</a></p></div></div>    
</body>
</html>