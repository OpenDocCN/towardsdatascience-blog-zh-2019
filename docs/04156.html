<html>
<head>
<title>DBSCAN Python Example: The Optimal Value For Epsilon (EPS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DBSCAN Python 示例:Epsilon (EPS)的最佳值</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc?source=collection_archive---------1-----------------------#2019-06-30">https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc?source=collection_archive---------1-----------------------#2019-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/eda1585dbcd4204f12a388b9d5815f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i6QAEa1aIcCAmveS"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/a-boy-holding-a-chalk-5212334/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/a-boy-holding-a-chalk-5212334/</a></figcaption></figure><div class=""/><p id="4aa7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">DBSCAN，即基于密度的带噪声应用空间聚类，是一种无监督的机器学习算法。无监督的机器学习算法用于分类未标记的数据。换句话说，用于训练我们的模型的样本没有预定义的类别。与其他聚类算法相比，DBSCAN 特别适用于需要以下条件的问题:</p><ol class=""><li id="70be" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">确定输入参数的最少领域知识(即 k-means 中的<strong class="ki jk"> K </strong>和层次聚类中的<strong class="ki jk"> Dmin </strong></li><li id="7638" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">发现任意形状的团簇</li><li id="540c" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">大型数据库的良好效率</li></ol><p id="097a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对阅读 DBSCAN 感兴趣，可以在<a class="ae jg" href="http://www2.cs.uh.edu/~ceick/7363/Papers/dbscan.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到原文。</p><h1 id="ee8c" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">算法</h1><p id="bb54" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">与大多数机器学习算法一样，模型的行为由几个参数决定。在接下来的文章中，我们将触及三个问题。</p><ul class=""><li id="2cb9" class="le lf jj ki b kj kk kn ko kr lg kv lh kz li ld mv lk ll lm bi translated"><strong class="ki jk"> eps </strong>:如果两点之间的距离低于阈值ε，则认为这两点是相邻的。</li><li id="f69b" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld mv lk ll lm bi translated"><strong class="ki jk"> min_samples </strong>:一个给定点为了被分类为核心点而应该拥有的最小邻居数量。<strong class="ki jk">需要注意的是，点本身包含在最小样本数中。</strong></li><li id="8819" class="le lf jj ki b kj ln kn lo kr lp kv lq kz lr ld mv lk ll lm bi translated"><strong class="ki jk">度量</strong>:计算特征数组中实例间距离时使用的度量(即欧几里德距离)。</li></ul><p id="94f2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该算法通过计算每个点和所有其他点之间的距离来工作。然后，我们将这些点分为三类。</p><p id="530d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">核心点:至少有<strong class="ki jk"> <em class="mw">个 min_samples </em> </strong>个点的点，这些点相对于该点的距离低于由ε定义的阈值。</p><p id="e593" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">边界点:至少与<strong class="ki jk"> <em class="mw"> min_samples </em> </strong>点不接近，但与一个或多个核心点足够接近的点。边界点包含在最近核心点的群集中。</p><p id="66b3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">噪声点:离核心点不够近的点被认为是边界点。噪声点被忽略。也就是说，它们不属于任何集群。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/9d9238b75459921e41015a591724e2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utiusBXcwuEau7Q7W54Jlw.png"/></div></div></figure><h1 id="4918" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">密码</h1><p id="6e89" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">让我们看看如何用 python 实现 DBSCAN。若要开始，请导入以下库。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="a911" class="nh lt jj nd b gy ni nj l nk nl">import numpy as np<br/>from sklearn.datasets.samples_generator import make_blobs<br/>from sklearn.neighbors import NearestNeighbors<br/>from sklearn.cluster import DBSCAN<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="1851" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与导入数据相反，我们可以使用<code class="fe nm nn no nd b">scikit-learn</code>来生成定义良好的集群。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="dc57" class="nh lt jj nd b gy ni nj l nk nl">X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)<br/>plt.scatter(X[:,0], X[:,1])</span></pre><figure class="my mz na nb gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/406fb6d2beca898218dfbee7c3c2259d.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*Whdtk19vCAoaOP22FGT0uQ.png"/></div></figure><p id="46f1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如前所述，我们必须为ε提供一个值，它定义了两点之间的最大距离。下面的文章描述了一种自动确定 Eps 最佳值的方法。</p><p id="86d7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae jg" href="https://iopscience.iop.org/article/10.1088/1755-1315/31/1/012012/pdf" rel="noopener ugc nofollow" target="_blank">https://IOP science . IOP . org/article/10.1088/1755-1315/31/1/012012/pdf</a></p><p id="883b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通俗地说，我们通过计算每个点到最近的<strong class="ki jk"> <em class="mw"> n </em> </strong>点的距离，对结果进行排序和作图，从而为ε找到一个合适的值。然后我们观察变化最明显的地方(想想你的手臂和前臂之间的角度),并选择它作为ε。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/444945e13d4f4b4afab9fb9dde7d85ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KuRf8j7JUZSwA_-NPXog5g.png"/></div></div></figure><p id="bf77" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用<code class="fe nm nn no nd b">NearestNeighbors</code>来计算每个点到最近邻点的距离。点本身包含在<code class="fe nm nn no nd b">n_neighbors</code>中。<code class="fe nm nn no nd b">kneighbors</code>方法返回两个数组，一个包含到最近的<code class="fe nm nn no nd b">n_neighbors</code>点的距离，另一个包含这些点的索引。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="8cd2" class="nh lt jj nd b gy ni nj l nk nl">neigh = NearestNeighbors(n_neighbors=2)</span><span id="376c" class="nh lt jj nd b gy nr nj l nk nl">nbrs = neigh.fit(X)</span><span id="dc7a" class="nh lt jj nd b gy nr nj l nk nl">distances, indices = nbrs.kneighbors(X)</span></pre><figure class="my mz na nb gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/b58012878e007395cb10eadd27f0849c.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*NMU3Au3oBzvUbyjLSI02uw.png"/></div></figure><p id="1954" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们对结果进行排序和绘图。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="199c" class="nh lt jj nd b gy ni nj l nk nl">distances = np.sort(distances, axis=0)</span><span id="6446" class="nh lt jj nd b gy nr nj l nk nl">distances = distances[:,1]</span><span id="3840" class="nh lt jj nd b gy nr nj l nk nl">plt.plot(distances)</span></pre><p id="8b97" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ε的最佳值将在最大曲率点找到。</p><figure class="my mz na nb gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/c2f3bc3cb7509e1c8be5dd230e0b8905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*KUYsoRqDm5vVYX9qHB-xeQ.png"/></div></figure><p id="952b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们训练我们的模型，选择<code class="fe nm nn no nd b">0.3</code>为<code class="fe nm nn no nd b">eps</code>，设置<code class="fe nm nn no nd b">min_samples</code>为<code class="fe nm nn no nd b">5.</code></p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="75da" class="nh lt jj nd b gy ni nj l nk nl">m = DBSCAN(eps=0.3, min_samples=5)</span><span id="5d4f" class="nh lt jj nd b gy nr nj l nk nl">m.fit(X)</span></pre><p id="f777" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nm nn no nd b">labels_</code>属性包含集群列表和它们各自的点。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="07c6" class="nh lt jj nd b gy ni nj l nk nl">clusters = m.labels_</span></pre><p id="53eb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们将每个单独的聚类映射到一种颜色。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="64da" class="nh lt jj nd b gy ni nj l nk nl">colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']</span><span id="5ce1" class="nh lt jj nd b gy nr nj l nk nl">vectorizer = np.vectorize(lambda x: colors[x % len(colors)])</span></pre><p id="1c09" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该模型对人口密集区进行了分类。正如我们所见，所有深蓝色的点都被归类为噪声。</p><pre class="my mz na nb gt nc nd ne nf aw ng bi"><span id="1e4a" class="nh lt jj nd b gy ni nj l nk nl">plt.scatter(X[:,0], X[:,1], c=vectorizer(clusters))</span></pre><figure class="my mz na nb gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/7c8189dd870fc01ca0cda109c02c74b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*aZJOhCIkq4YOdD-nG6DMhw.png"/></div></figure><h1 id="67a4" class="ls lt jj bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">最后的想法</h1><p id="dbb1" class="pw-post-body-paragraph kg kh jj ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">与 k-means 不同，DBSCAN 会计算出聚类数。DBSCAN 的工作原理是确定最小数量的点是否彼此足够接近以被视为单个聚类的一部分。DBSCAN 对比例非常敏感，因为ε是两点之间最大距离的固定值。</p></div></div>    
</body>
</html>