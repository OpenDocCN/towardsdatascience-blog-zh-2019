<html>
<head>
<title>The theory you need to know before you start an NLP project</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在你开始一个 NLP 项目之前你需要知道的理论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-theory-you-need-to-know-before-you-start-an-nlp-project-1890f5bbb793?source=collection_archive---------4-----------------------#2019-07-27">https://towardsdatascience.com/the-theory-you-need-to-know-before-you-start-an-nlp-project-1890f5bbb793?source=collection_archive---------4-----------------------#2019-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/340682f3a2dea31cfe800767f704eae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJQD5UN3C6845p-w3Wtbtw.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="e275" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">概述开始处理任何涉及文本的项目所需的最常见的自然语言处理和机器学习技术。</h2></div><h1 id="f083" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">介绍</h1><p id="6dd5" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">从一月份开始，我一直在做一个关于从非结构化文本中提取信息的项目。在开始这个项目之前，我对自然语言处理(NLP)领域一无所知。一旦我开始研究这个领域，我很快就遇到了“Python 的自然语言处理”，这里有<a class="ae me" href="https://www.nltk.org/book/" rel="noopener ugc nofollow" target="_blank"/>。这本书对我来说太理论化了，尽管它的基本原理是正确的，所以它仍然是一个无价的资源。我的下一个重大发现是 Dipanjan Sarkar 的“<a class="ae me" href="https://www.apress.com/gp/book/9781484243534" rel="noopener ugc nofollow" target="_blank">文本分析与 Python </a>”，我已经从头到尾读了一遍。这是一本非常棒的书，它教会了我启动 NLP 项目所需的所有技术技能。第二版<a class="ae me" href="https://www.apress.com/gp/book/9781484243534" rel="noopener ugc nofollow" target="_blank">版本</a>【6】也在最近发布。</p><p id="32f1" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">在这篇文章中，我想概述一下我在获得 NLP 技能时了解的一些话题。我知道有很多很棒的帖子在讨论同样的事情，比如 Sarkar 的这个<a class="ae me" rel="noopener" target="_blank" href="/a-practitioners-guide-to-natural-language-processing-part-i-processing-understanding-text-9f4abfd13e72">牛逼系列，但是写下来真的帮助我组织我所知道的一切。</a></p><h1 id="03f7" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">这篇文章涵盖的内容</h1><p id="531f" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">为了简洁起见，这篇文章主要是理论性的。以后我会多写一些实用的文章。现在，我将讨论几个主题，例如:</p><ol class=""><li id="cfe1" class="mk ml jb lk b ll mf lo mg lr mm lv mn lz mo md mp mq mr ms bi translated">使用自然语言处理文本</li><li id="fcf1" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated">从文本中提取特征</li><li id="d659" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated">文本的监督学习</li><li id="58f8" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated">文本的无监督学习</li></ol><h1 id="f30a" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">预处理文本</h1><p id="00c5" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">预处理文本的典型管道由以下步骤组成:</p><ol class=""><li id="5a43" class="mk ml jb lk b ll mf lo mg lr mm lv mn lz mo md mp mq mr ms bi translated">句子分割。</li><li id="70d9" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated">规范化和符号化。</li><li id="a503" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated">词性标注。</li><li id="362b" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md mp mq mr ms bi translated">命名实体识别。</li></ol><p id="6f24" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">在大多数应用中，并不是所有的步骤都必须执行。对命名实体识别的需求取决于应用程序的业务需求，而词性标注通常由现代工具自动完成，以改进部分规范化和标记化步骤。</p><h2 id="e61a" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">句子分割</h2><p id="08b0" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在预处理流水线的第一步，文本被分割成句子。在许多语言中，如英语，标点符号，尤其是句号/句号字符，感叹号和问号可以用来标识句子的结尾。但是，句点字符也可以用在缩写中，如 Ms 或 U.K .,在这种情况下，句号字符并不表示句子的结束。在这些情况下，使用缩写表来避免句子边界的错误分类。当文本包含特定领域的术语时，必须创建一个额外的缩写字典来避免不自然的标记。</p><h2 id="ce37" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">标记化和规范化</h2><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/34868296ff16194b4b436201411000b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*bwegs5vEdx5-2tgM3Nq2OQ.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Tokenization corner case</figcaption></figure><p id="685f" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">标记化是将文本分成单词和标点符号，即标记。与句子分割一样，标点符号也很有挑战性。例如，U.K .应被视为一个标记，而“don 't”应被拆分为两个标记:“do”和“n't”。</p><p id="13f0" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">词干化和词汇化是规范化过程的重要部分。规范化包括词干化和词汇化。在词干提取过程中，通过删除后缀(如–ed 和–ing)来识别单词的词干。由此产生的词干不一定是单词。类似地，词汇化包括删除前缀和后缀，重要的区别是结果属于该语言。这个结果叫做引理。在下表中可以看到词干化和词汇化的例子。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/949dde004970f138435e2abff4c10a54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*BG-DR0BB9yxsW-AJ2_m_Tw.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Examples of the differences between stemming and lemmatization</figcaption></figure><p id="b2a8" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">这两种技术都通过将单词转换成它们的基本形式来减少文本中的噪声。对于大多数应用程序，如文本分类或文档聚类，保持单词的含义是很重要的，最好使用词汇化而不是词干化。例如，meeting ( <em class="nu">名词</em>)和 meeting ( <em class="nu">动词</em>)都将被词干化为 meet，从而失去其原始含义，而各自的词条将是 meeting 和 meet。</p><p id="9471" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">其他标准化技术包括:扩展缩写，删除数字和标点符号，纠正典型的语法错误等。这些操作中的大部分可以通过使用正则表达式来完成。</p><h2 id="8c38" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">词性标注</h2><p id="a0f6" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">这一步是根据单词的上下文和定义，将标记分类为词性(POS)类，也称为单词类或词汇类。词类有名词、动词、介词、副词等。下表列出了英语词汇类别及示例。词性标注改善了词汇化，对于命名实体识别是必要的。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/eacc482ce28bc38dd89a4d4b7e116df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*W1oBvtHFNRolZWuPF-uQsw.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Examples of the common POS classes</figcaption></figure><p id="d47f" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">有三种类型的标记器:基于规则的、基于统计的和基于深度学习的。基于规则的标记依赖于明确的规则，比如一个冠词必须跟一个名词，来标记记号。统计标记器使用概率模型来标记单个单词或单词序列。基于规则的标记非常精确，但也依赖于语言。扩展标记器以支持其他语言需要大量的工作。统计标签更容易创建，并且独立于语言，尽管它们牺牲了精确性。如今，使用基于规则和统计模型的混合方法，尽管大多数行业开始慢慢转向深度学习解决方案，即在预先标记的句子集上训练模型。基于混合和深度学习的方法改进了上下文敏感标记。</p><h2 id="342f" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">命名实体识别</h2><p id="0ade" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">在可以识别命名实体之前，必须对标记进行分块。分块意味着分割和标记记号集。最常用的组块之一是由限定词、形容词和名词组成的名词短语组块，例如，“一只快乐的独角兽”。一句“他找到了一只快乐的独角兽”由“他”和“一只快乐的独角兽”两个组块组成。</p><p id="bde2" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">命名实体是指特定对象的名词短语，如个人、组织、地点、日期和地缘政治实体。命名实体识别(NER)步骤的目标是从文本中识别命名实体的提及。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/91810d10b3610c386a8b8ef10a141875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*lnXmX625pnS8Jy8PdZ5_8g.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Sentence with NER tags</figcaption></figure><h1 id="6789" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">机器学习</h1><p id="bb0e" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">正如 Brink 等人所定义的，机器学习(ML)是利用历史数据中的模式来对新数据做出决策[1]，或者正如谷歌首席决策科学家 Cassie Kozyrkov 如此雄辩地所说:“<a class="ae me" href="https://hackernoon.com/the-simplest-explanation-of-machine-learning-youll-ever-read-bebc0700047c" rel="noopener ugc nofollow" target="_blank">机器学习只是一个事物标签器，接受你对某事物的描述，并告诉你它应该被贴上什么标签。</a>“当手头的问题过于复杂而无法通过编程解决时，应用 ML 技术是有用的，例如在图像上区分不同的猫品种，或者解决方案需要随着时间的推移而适应，例如识别手写文本[2]。</p><p id="3979" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">通常，机器学习分为有监督的和无监督的机器学习[2]。当我们的历史训练数据包含标签时(例如，下图中的“鸭子”和“不是鸭子”)，我们可以使用监督学习。另一方面，当数据中没有标签时，应用无监督学习。无监督的机器学习方法旨在总结或压缩数据。这种差异的一个例子是检测垃圾邮件与异常检测的问题。在第一种情况下，我们将拥有带有<em class="nu">垃圾邮件/非垃圾邮件</em>标签的训练数据，在后一种情况下，我们将不得不根据电子邮件训练集来检测异常电子邮件。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/0f13453a879a8156956e5830b72dc47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*40FD63DY9nzCiXBCbJSvZA.jpeg"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">The difference between supervised and unsupervised learning. <a class="ae me" href="https://i0.wp.com/blog.westerndigital.com/wp-content/uploads/2018/05/supervised-learning-diagram.jpg?w=600" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="d795" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">特征抽出</h1><p id="5e27" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">所有机器学习算法都需要数字数据作为输入。这意味着文本数据必须转换成数字。这是 NLP 世界中特征提取步骤的本质。</p><h2 id="3d71" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">基于计数的策略</h2><p id="c06d" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">将文本转换成数字向量的最简单的方法是使用单词包(BoW)方法。BoW 的原理是从文本中取出所有独特的词，并创建一个名为词汇的文本语料库。使用词汇表，每个句子可以表示为一个由 1 和 0 组成的向量，这取决于词汇表中的某个单词是否出现在句子中。下图显示了在五个规范化句子上使用 BoW 方法创建的矩阵示例。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/22c510217623b0b63709fa0406ea23bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*JmWEywZbz0AmGaa8TRSGMA.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Example sentences</figcaption></figure><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/1bf3c27177d0134c079ee788ab4ac3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URN8CEOKy77TLTN_5StJoQ.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">BoW feature matrix created from the sentences above</figcaption></figure><p id="cc41" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">为了向词汇表添加更多的上下文，可以将标记分组在一起。这种方法被称为 N 元语法方法。一个 N 元文法是 N 个记号的序列。例如，二元模型是两个单词的序列，而三元模型是三个单词的序列。</p><p id="86cd" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">一旦选择了词汇，不管是 1 个、2 个还是 3 个单词，都必须计算单词出现的次数。我们可以用弓的方法。这种方法的缺点是流行词变得太重要了。因此，最流行的方法是术语频率-逆文档频率(TF-IDF)。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/08482d2b93f60deacea18858949fecc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*uBtVGBCbbw494K9QUHa_Ew.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">High-level explanation of TF-IDF</figcaption></figure><p id="06ce" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">TF-IDF 由词频(TF)和逆文档频率(IDF)组成，词频(TF)捕捉单词相对于句子长度的重要性，逆文档频率(IDF)捕捉单词相对于文档中的总行数出现在多少个文档行中，从而突出单词的稀有性。直观上，如果一个单词在一个文档中频繁出现，但在所有文档的集合中不频繁出现，则该单词具有较高的 TF-IDF 得分。下图显示了使用 TF-IDF 方法在前面看到的例句上创建的矩阵示例。请注意单词 fox 的分数与更频繁出现的兔子的分数是如何不同的。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/a0f6ba3c6679042a5f1a7eb3801e8e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qcszBDL3dVqILDW9zMempA.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">TF-IDF feature matrix created from the example sentences</figcaption></figure><h2 id="0d8f" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">高级策略</h2><p id="59dc" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">虽然基于计数的方法可以用来捕获单词序列(n 元语法)，但它们不能捕获单词的语义上下文，而语义上下文是许多 NLP 应用程序的核心。单词嵌入技术被用来克服这个问题。使用单词嵌入，词汇被转换成向量，使得具有相似上下文的单词在附近。</p><p id="8dd1" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated"><a class="ae me" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>是来自 Google 的一个框架，使用浅层神经网络来训练单词嵌入模型[3]。有两种类型的 Word2Vec 算法:Skip-Gram，用于预测给定单词周围的上下文，而连续单词包(CBOW)模型用于预测给定上下文中的下一个单词。</p><p id="ca3e" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">全局向量方法，<a class="ae me" href="https://www.aclweb.org/anthology/D14-1162" rel="noopener ugc nofollow" target="_blank"> GloVe </a>，使用同现统计来创建向量空间【4】。这个方法是 Word2Vec 的扩展，可以提供更好的单词嵌入。下图显示了例句中手套词嵌入的例子以及嵌入的图示。正如人们所料，类似的概念就在附近。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/c9b3a720ca8b0ee1c28f4e7ed2b2c3b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OikNAPr_cQTpgx5fqWw0bg.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Feature matrix created using GloVe embeddings</figcaption></figure><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/8d36093778a7ccbc5450a2503ec8b861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*glzWNEeolBmOkfoS0YHihg.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Word vectors projected to a 2D space</figcaption></figure><p id="58dc" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">Word2Vec 的另一个临时版本由脸书开发，名为<a class="ae me" href="https://aclweb.org/anthology/Q17-1010" rel="noopener ugc nofollow" target="_blank"> fastText </a>。fastText 是一个深度学习框架，在构建向量空间时会考虑单个字符[5]。</p><h1 id="e474" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">监督学习</h1><p id="9469" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">基于标签(也称为目标)的格式，监督机器学习任务分为两个。如果目标是一个分类值(<em class="nu">猫/狗</em>)，那么这是一个分类问题，另一方面，如果目标是数值型的(<em class="nu">房子的价格</em>)，那么这是一个回归问题。在处理文本时，我们大多会遇到分类问题。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/61c4a31bc0f0be3e22d86071ac8a2399.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MpEQ2Ms88ECaBD4YlqPYmw.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Typical supervised learning pipeline</figcaption></figure><p id="3318" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">上图显示了一个典型的文本分类系统的工作流程。我们首先将数据分为训练集和测试集。必须对训练和测试数据进行预处理和归一化，然后才能提取特征。前面几节介绍了最流行的文本数据特征提取技术。一旦文本数据被转换成数字形式，就可以对其应用机器学习算法。这个过程称为训练模型-模型从特征中学习模式来预测标签。通过称为超参数调整的过程，可以使用模型参数优化模型以获得更好的性能。然后，根据以前看不到的测试数据对结果模型进行评估。使用各种度量来测量模型的性能，例如准确度、精确度、召回率、F1 分数等等。本质上，这些分数是为了比较数据的真实标签和预测标签而构建的。</p><p id="a7d2" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">用于文本分类的典型算法有:</p><ul class=""><li id="c32f" class="mk ml jb lk b ll mf lo mg lr mm lv mn lz mo md of mq mr ms bi translated">多项式朴素贝叶斯-属于朴素贝叶斯算法家族，建立在贝叶斯定理的基础上，假设每个要素相互独立。多项式朴素贝叶斯是一种扩展，用于具有两个以上不同标签的分类任务(多类分类)。</li><li id="6d3b" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">逻辑回归-一种使用 Sigmoid 函数预测分类值的算法。流行的<em class="nu"> sklearn </em>包允许调整模型参数，使得算法也可用于多标签分类。</li><li id="0335" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated"><a class="ae me" href="https://medium.com/machine-learning-101/chapter-2-svm-support-vector-machine-theory-f0812effc72" rel="noopener">支持向量机</a>(SVM)——一种使用直线或超平面(如果有两个以上的特征，从而创建一个多维空间)来区分类别的算法。</li><li id="5fd4" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">随机森林——一种集成方法，它并行地在各种数据子集上训练多个决策树。</li><li id="e2c4" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">梯度推进机器(GBM)-一系列集成方法，用于训练一系列弱学习器，如决策树，以获得准确的结果。XGBoost 是这个算法家族中最流行的实现之一。</li></ul><p id="8c2e" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">分类算法列表中的最后两项是集成方法，它们使用许多预测算法来实现更好的泛化。集成方法的结果通常比单个模型更平均，集成往往在较大的数据集上工作得更好。然而，正如 Sarkar 在[6]中证明的，集成方法不一定在文本数据上工作得更好。</p><h2 id="4b91" class="my kr jb bd ks mz na dn kw nb nc dp la lr nd ne lc lv nf ng le lz nh ni lg nj bi translated">评估指标</h2><figure class="nl nm nn no gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/88d062b353433b37e457f5b2d6ea3843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*Gm6CslClTFF-D4k7y4gHMw.png"/></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Confusion matrix and the metrics derived from it</figcaption></figure><p id="7439" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">混淆矩阵是评估机器学习模型的最简单和最直观的工具之一。它显示了实际值和预测值之间的关系。尽管混淆矩阵本身是一个有价值的工具，但与之相关的术语被用作其他度量的基础。关于混淆矩阵的重要术语:</p><ul class=""><li id="cd66" class="mk ml jb lk b ll mf lo mg lr mm lv mn lz mo md of mq mr ms bi translated">真阳性——我们预测为阳性，而实际输出也为阳性的情况。</li><li id="df6a" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">真阴性——我们预测为阴性而实际产出为阴性的情况。</li><li id="2ecb" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">假阳性——我们预测为阳性而实际输出为阴性的情况。</li><li id="7621" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">假阴性——我们预测为阴性而实际结果为阳性的情况。</li></ul><p id="405b" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">从混淆矩阵中得出的指标有:</p><ul class=""><li id="b767" class="mk ml jb lk b ll mf lo mg lr mm lv mn lz mo md of mq mr ms bi translated">准确性-模型在所有预测中做出的正确预测数。</li><li id="2502" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">精度-所有阳性预测中正确阳性案例的数量，换句话说，有多少所选项目是相关的。</li><li id="d046" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">回忆—所有实际阳性事件中正确阳性案例的数量，换句话说，选择了多少个相关项目。</li><li id="2f6d" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">F1 分数—结合精确度和召回率的单一分数，使用调和平均值获得。调和平均值是 x 和 y 相等时的平均值。但是当 x 和 y 不同时，它更接近于较小的数，而不是较大的数。</li></ul><p id="e517" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">只有当标注包含大致相等数量的数据点时，准确性才是有用的度量标准。所有四个指标的范围从 0 到 1，1 是最好的，0 是最差的分数。</p><h1 id="acf2" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">无监督学习</h1><p id="2454" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">当要分析的数据集没有标签时，可以使用无监督的机器学习技术，例如聚类。聚类是无监督学习的一个分支，其目的是将相似的对象组合在一起。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oh"><img src="../Images/99afec52370afb7fae3618b6f1883df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OR3B9tAM5v7wZs0SIkQOeg.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Examples of clustering. <a class="ae me" href="https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="b33e" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">有几类可用的聚类算法:</p><ul class=""><li id="cf53" class="mk ml jb lk b ll mf lo mg lr mm lv mn lz mo md of mq mr ms bi translated">基于连通性的聚类(也称为分层聚类)根据数据点之间的距离来连接它们。有两种类型的策略用于连接这些点:凝聚，一种“自下而上”的方法，其中每个数据点成为自己的聚类，成对的聚类迭代合并，或“自上而下”的分裂方法，其中整个数据空间是一个被递归分裂的聚类。对于凝聚层次聚类，两个额外的度量是必要的:显示两个数据点有多相似的距离度量，欧几里德、汉明或余弦距离是典型的例子，以及显示数据点组有多相似的链接标准。</li><li id="e677" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">基于质心的聚类-根据点与聚类质心的接近程度将数据划分到聚类中。K-means 是该算法最流行的实现。基本算法如下:(1)。)选择 k 作为集群的数量，(2。)将数据点分配到聚类中，(3。)计算群集质心，(4)。)将数据点重新分配到最近的质心，(5。)重复前面两步，直到质心不变。</li><li id="b564" class="mk ml jb lk b ll mt lo mu lr mv lv mw lz mx md of mq mr ms bi translated">基于密度的聚类—数据空间被划分并聚类成密度区域。DBSCAN 和 OPTICS 是两种流行的算法，它们提取数据空间的密集区域，将“有噪声的”数据留在稀疏区域。光学试图克服 DBSCAN 在边界和不同密度的数据集上表现不佳的弱点。</li></ul><h1 id="2a6b" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">文本摘要</h1><p id="d70a" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">文本摘要可以分为两部分:主题建模和自动文本摘要。自动文本摘要是使用 ML 算法来创建文档或一组文档的摘要的过程。这些算法在处理大量文档和/或长文档时表现最佳。</p><p id="8792" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">另一方面，主题建模侧重于从文档集合中提取主题。主题模型通常被称为概率统计模型，因为它们使用统计技术，如奇异值分解(SVD)，来揭示文本中潜在的语义结构。SVD 依赖于矩阵分解，这是一种来自线性代数的技术，它将特征矩阵分成更小的分量。潜在语义索引(LSI)、潜在狄利克雷分配(LDA)和非负矩阵分解(NNMF)等方法利用线性代数中的技术将文档划分为主题，这些主题本质上是单词簇，如下所示。当文本多样化时，主题建模算法倾向于产生更好的结果。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oi"><img src="../Images/dd55f0d4b248de9d0eedf96e6ef41a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nhHn0cX8msP5pHXiyN4NGA.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">The essence of topic modeling. <a class="ae me" href="http://chdoig.github.io/pytexas2015-topic-modeling/#/3/4" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="ca77" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">结论</h1><p id="fa71" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">我已经简要概述了一些重要的主题，一旦你开始一个涉及自然语言处理和机器学习的项目，你将会遇到这些主题。我几乎没有触及这个领域的表面。我甚至没有提到使用迁移学习的语言建模的令人兴奋的发展，你可以在 Sebastian Ruder 的这篇有见地的<a class="ae me" href="http://ruder.io/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank">帖子</a>中读到。</p><p id="5eb1" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">在我看来，这是一个非常令人兴奋的时间，开始在行业中实践应用的自然语言处理。正如 Yoav Goldberg 在我最近参加的 T2 会议上所说，大多数行业仍然使用正则表达式来解决问题。通过理解我在这篇文章中提出的理论，并将其应用于现实生活中的问题，你可以让一些人真正快乐起来。</p><figure class="nl nm nn no gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/d5f548ec5edd958d717a2e13f2e58d3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_nICmFGQT_CVPcq-WZmCdA.jpeg"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Yoav Goldberg presenting the current state of the applied NLP at spaCy IRL</figcaption></figure><h1 id="4a00" class="kq kr jb bd ks kt ku kv kw kx ky kz la kh lb ki lc kk ld kl le kn lf ko lg lh bi translated">参考</h1><p id="b9fb" class="pw-post-body-paragraph li lj jb lk b ll lm kc ln lo lp kf lq lr ls lt lu lv lw lx ly lz ma mb mc md ij bi translated">[1] H. Brink、J. W. Richards 和 M. Fetherolf，真实世界机器学习(2017 年)，曼宁出版社</p><p id="4582" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">[2] S. Shalev-Shwartz，S. Ben-David，<a class="ae me" href="https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/" rel="noopener ugc nofollow" target="_blank">理解机器学习:从理论到算法</a> (2014)，剑桥大学出版社</p><p id="8656" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">[3] T. Mikolov、I. Sutskever、K. Chen、G. S Corrado 和 J. Dean。<a class="ae me" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">单词和短语的分布式表示及其组成</a> (2013)，神经信息处理系统进展 26</p><p id="aae9" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">[4] J. Pennington、R. Socher 和 C. D. Manning，<a class="ae me" href="https://www.aclweb.org/anthology/D14-1162" rel="noopener ugc nofollow" target="_blank"> GloVe:单词表示的全局向量</a> (2014)，载于 EMNLP。</p><p id="8f01" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">[5] P. Bojanowski、E. Grave、A. Joulin 和 T. Mikolov。<a class="ae me" href="https://aclweb.org/anthology/Q17-1010" rel="noopener ugc nofollow" target="_blank">用子词信息丰富词向量</a> (2016)，arXiv 预印本</p><p id="d4d1" class="pw-post-body-paragraph li lj jb lk b ll mf kc ln lo mg kf lq lr mh lt lu lv mi lx ly lz mj mb mc md ij bi translated">[6] D .萨卡尔。使用 Python 的文本分析:自然语言处理从业者指南(2019)，Apress</p></div></div>    
</body>
</html>