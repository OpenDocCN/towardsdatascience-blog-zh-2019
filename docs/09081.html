<html>
<head>
<title>Pseudo-Labeling to deal with small datasets — What, Why &amp; How?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">处理小数据集的伪标记——什么、为什么和如何？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af?source=collection_archive---------0-----------------------#2019-12-03">https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af?source=collection_archive---------0-----------------------#2019-12-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d67a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用模型输出提高模型输出的指南！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/65b1161758ff98a4e3a2f106d3609d3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5R3WVXfm_hjV9ozcavKodg.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">TSNE Plot of MNIST Pseudo-Labeling</figcaption></figure><p id="0d48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">几天前，我看到了 Yoshua Bengio 对 Quora 问题的回复——“为什么无监督学习很重要？”  <em class="lv">。</em>以下是他的回复摘录:</p><blockquote class="lw lx ly"><p id="f2e8" class="ky kz lv la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">通过监督学习攀登人工智能阶梯可能需要通过显示大量这些概念出现的例子来“教”计算机所有对我们重要的概念。这不是人类学习的方式:是的，由于语言，我们得到了一些例子来说明给我们的新的命名概念，但我们观察到的大部分东西并没有被标记，至少最初是这样。</p></blockquote><p id="b36a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从神经科学的角度和实践的角度来看，他的回答都很有意义。<strong class="la iu"> </strong>标注数据在时间和金钱方面都很昂贵<strong class="la iu">。这个问题的显而易见的解决方案是找出一种方法:</strong></p><p id="45dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(a)使 ML 算法在没有标记数据的情况下工作(即无监督学习)</p><p id="4f29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(b)自动标记数据或使用大量未标记数据和少量标记数据(即半监督学习)</p><p id="b08e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如 Yann LeCun 在这篇<a class="ae lu" href="https://www.wired.com/2014/08/deep-learning-yann-lecun/" rel="noopener ugc nofollow" target="_blank">文章</a>中提到的，无监督学习是一个很难解决的问题:</p><blockquote class="lw lx ly"><p id="def1" class="ky kz lv la b lb lc ju ld le lf jx lg lz li lj lk ma lm ln lo mb lq lr ls lt im bi translated">“我们知道最终的答案是无监督学习，但我们还没有答案。”</p></blockquote><p id="f4d5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，最近人们对半监督学习有了新的兴趣，这在学术和工业研究中都有所反映。这里有一张图表，显示了谷歌学术每年与半监督学习相关的研究论文数量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/99c42f719a3c784378f41880d5a97b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*5uXsroZyepTu0cv3SNtutw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Semi-Supervised Learning Research Papers by Year</figcaption></figure><p id="22a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇博客中，我们将深入研究伪标签——一种简单的半监督学习(SSL)算法。尽管伪标记是一种幼稚的方法，但它给我们提供了一个极好的机会来理解 SSL 面临的挑战，并提供了一个基础来学习一些现代的改进，如 MixMatch、虚拟对抗训练等。</p><h2 id="1561" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">大纲:</h2><ol class=""><li id="e23b" class="mw mx it la b lb my le mz lh na ll nb lp nc lt nd ne nf ng bi translated">什么是伪标签？</li><li id="d217" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">理解伪标记方法</li><li id="0759" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">实现伪标签</li><li id="2c57" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">伪标签为什么会起作用？</li><li id="4c99" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">伪标签什么时候效果不好？</li><li id="3fb8" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">用传统的 ML 算法进行伪标记</li><li id="6018" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">半监督学习的挑战</li></ol><h1 id="36c8" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">1.什么是伪标签？</h1><p id="c0ab" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">伪标记方法由<a class="ae lu" href="http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf" rel="noopener ugc nofollow" target="_blank"> Lee 于 2013 年【1】</a>首次提出，使用一小组已标记数据和大量未标记数据来提高模型的性能。这项技术本身非常简单，只需遵循 4 个基本步骤:</p><ol class=""><li id="7d3c" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">基于一批标记数据的训练模型</li><li id="3037" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">使用训练好的模型来预测一批未标记数据上的标签</li><li id="e9d5" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">使用预测的标签来计算未标记数据的损失</li><li id="649f" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">将标记的损失与未标记的损失和反向传播结合起来</li></ol><p id="b7ad" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">…然后重复。</p><p id="ec03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种技术可能看起来很奇怪——几乎类似于 youtube 上数百个“免费能源设备”视频。然而，伪标记已经成功地用于几个问题。事实上，<a class="ae lu" href="https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003" rel="noopener ugc nofollow" target="_blank">在一场 Kaggle 竞赛</a>中，一个团队使用伪标签来提高他们模型的性能，以确保第一名并赢得 25，000 美元。</p><p id="d281" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们一会儿就来看看为什么会这样，现在，让我们来看看一些细节。</p><h1 id="8387" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">2.理解伪标记方法</h1><p id="074c" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">伪标记在每批中同时用标记和未标记的数据训练网络。这意味着对于每批标记和未标记的数据，训练循环会:</p><ol class=""><li id="920a" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">对贴有标签的批次进行一次正向传递以计算损耗→这是贴有标签的损耗</li><li id="7d84" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">对未标记的批次进行一次正向传递，以预测未标记的批次的“伪标签”</li><li id="77dc" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">使用这个“伪标签”来计算未标记的损失。</li></ol><p id="0fd2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，Lee 建议使用权重，而不是简单地将未标记的损失与标记的损失相加。总损失函数如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/f29eaefcc28f36eb9a31c60d258ff72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h6UJE8uAgaVvIx2XJiDBdg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation [15] Lee (2013) [1]</figcaption></figure><p id="1721" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者用更简单的话说:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/5733397402f6a44323afc14d050070f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x_dFxoZxklMMdRLc8Gw3Gw.png"/></div></div></figure><p id="5c04" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在等式中，权重(α)用于控制未标记数据对总损失的贡献。此外，重量是时间(历元)的函数，并且在训练期间缓慢增加。当分类器的性能可能很差时，这允许模型最初更多地关注标记的数据。随着模型的性能随着时间(时期)的增加，权重增加，未标记的损失更加强调整体损失。</p><p id="eb3e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Lee 建议对 alpha (t)使用以下等式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4c20d32fe224df72cd5b9279220caf36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*LAxeb_l9k1yfAOUiRLYxAA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equation [16] Lee (2013) [1]</figcaption></figure><p id="4d9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中α_ f = 3，T1 = 100，T2 = 600。所有这些都是基于模型和数据集而变化的超参数。</p><p id="db1a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看 Alpha 如何随着时代的变化而变化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7f5ef0f2903c9c1418c837374fa90b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*_QXnK4RdT4oCPsIUtxhmpA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Variation of Alpha with Epochs</figcaption></figure><p id="eeef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第一个 T1 时段(本例中为 100)中，权重为 0-有效地强制模型仅根据标记的数据进行训练。在 T1 时段之后，权重线性增加到 alpha_f(在这种情况下为 3 ),直到 T2 时段(在这种情况下为 600 ),这允许模型缓慢地合并未标记的数据。T2 和 alpha_f 分别控制权重增加的速率和饱和后的值。</p><p id="d71c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">如果你熟悉最优化理论，你可能会从</em> <a class="ae lu" href="https://en.wikipedia.org/wiki/Simulated_annealing" rel="noopener ugc nofollow" target="_blank"> <em class="lv">模拟退火</em> </a> <em class="lv">中认出这个方程。</em></p><p id="4ec8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是从实现的角度理解伪标签的全部内容。该白皮书使用 MNIST 来报告性能，因此我们将坚持使用相同的数据集，这将有助于我们检查我们的实现是否正常工作。</p><h1 id="2765" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">3.实现伪标签</h1><p id="7a89" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">我们将使用 PyTorch 1.3 和 CUDA 来实现，尽管使用 Tensorflow/Keras 应该也没有问题。</p><h2 id="6c36" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">模型架构:</h2><p id="c2bd" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">虽然本文使用了一个简单的 3 层全连接网络，但在测试过程中，我发现 Conv 网络的性能要好得多。我们将使用一个简单的 2 Conv 层+ 2 全连接层网络，并带有 dropout(如本<a class="ae lu" href="https://github.com/peimengsui/semi_supervised_mnist" rel="noopener ugc nofollow" target="_blank">报告</a>中所述)</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Model Architecture for MNIST</figcaption></figure><h2 id="4d97" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">基准性能:</h2><p id="3075" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">我们将使用 1000 个标记图像(类别平衡)和 59，000 个未标记图像用于训练集，10，000 个图像用于测试集。</p><p id="b82a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们在不使用任何未标记图像(即简单的监督训练)的情况下检查 1000 个标记图像的性能</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="ba4e" class="md me it ok b gy oo op l oq or">Epoch: 290 : Train Loss : 0.00004 | Test Acc : 95.57000 | Test Loss : 0.233</span></pre><p id="5e84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于 1000 幅标记图像，最佳测试准确率为 95.57%。现在我们有了基线，让我们继续伪标签实现。</p><h2 id="3e69" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">伪标签实现:</h2><p id="2a64" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">对于实现，我们将做两个小的更改，使代码更简单，性能更好:</p><ol class=""><li id="cd74" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">在前 100 个历元中，我们将像往常一样在标记数据上训练模型(没有未标记的数据)。正如我们之前看到的，这与伪标记没有区别，因为在此期间 alpha = 0。</li><li id="c8a4" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">在接下来的 100+个时期中，我们将对未标记的数据进行训练(使用 alpha 权重)。这里，对于每 50 个未标记的批次，我们将在已标记的数据上训练一个时期—这充当校正因子。</li></ol><p id="b625" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果这听起来令人困惑，不要担心，这在代码中要容易得多。这次修改是基于这个 Github <a class="ae lu" href="https://github.com/peimengsui/semi_supervised_mnist" rel="noopener ugc nofollow" target="_blank">回购</a>，它在两个方面有帮助:</p><ol class=""><li id="6632" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">它减少了对标记的训练数据的过度拟合</li><li id="e613" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">提高了速度，因为我们只需要每批向前传递 1 次(在未标记的数据上)，而不是论文中提到的 2 次(未标记和已标记)。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/413df210fd130f7b28cdd0242e3232ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*Yk_mGPVIJgkIhf0gWo7PTA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Flowchart</figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pseudo-Labeling Loop for MNIST</figcaption></figure><p id="55d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">注意:我这里不包括监督训练的代码(前 100 个时期),因为它非常简单。你可以在我的</em> <a class="ae lu" href="https://github.com/anirudhshenoy/pseudo_labeling_small_datasets" rel="noopener ugc nofollow" target="_blank"> <em class="lv">回购这里</em> </a>找到所有代码</p><p id="2728" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是对标记数据进行 100 个时期的训练，然后进行 170 个时期的半监督训练后的结果:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="aa14" class="md me it ok b gy oo op l oq or"># Best Accuracy is at 168 epochs</span><span id="e7af" class="md me it ok b gy ot op l oq or">Epoch: 168 : Alpha Weight : 3.00000 | Test Acc : 98.46000 | Test Loss : 0.075</span></pre><p id="1ac5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在使用未标记数据后，我们达到了 98.46%的准确率，比监督训练高出了 3%。事实上，我们的结果比论文的结果更好——1000 个标记样本的准确率为 95.7%。</p><p id="cfc3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们做一些可视化来理解伪标签是如何工作的。</p><p id="a22a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">阿尔法权重与准确度</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/d92a512a3296b528d1a68a0045fa5a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*ryiPnYITG9BdrEMhtC98Uw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Alpha vs Epochs</figcaption></figure><p id="ee1e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很明显，随着 alpha 的增加，测试精度也慢慢增加，然后达到饱和。</p><p id="c36f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> TSNE 可视化</strong></p><p id="ebf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们看看伪标签是如何在每个时期被分配的。在下面的图中，有 3 点需要注意:</p><ol class=""><li id="ed62" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">每一簇背景中淡淡的颜色才是真正的标签。这是使用所有 60k 训练图像(使用标签)的 TSNE 创建的</li><li id="116d" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">每个聚类中的小圆圈来自监督训练阶段使用的 1000 幅训练图像。</li><li id="9dc9" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">不断移动的小星星是模型为每个历元的未标记图像分配的伪标记。(对于每个时期，我使用了大约 750 张随机采样的未标记图像来创建绘图)</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/08711e4e25201d0803fb34b242e57d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nr3FiUs8uV6jHCvUnWlhQQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/84b1ceaade84edcaa5a6524d4093fc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*2_fCYW4di1G_cQ9KWkJFqw.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">TSNE Animation</figcaption></figure><p id="d0e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是一些需要注意的事项:</p><ol class=""><li id="83e3" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">伪标签大部分都是对的。(恒星在具有相同颜色的星团中)这可以归因于较高的初始测试准确度。</li><li id="d800" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">随着训练的继续，正确伪标签的百分比增加。这反映在模型的整体测试精度的提高上。</li></ol><p id="16dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有一个图显示了在第 0 时段(左)和第 140 时段(右)的<strong class="la iu">相同的</strong> 750 点。我已经用红圈标出了改进的点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/683f9c81ec3b6cf6b1a2350896b4bc80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2D-T3XR_RqYLwQtdc9xDHQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="ox">(Note: This TSNE plot only show 750 unlabeled samples out of 59000 pts)</em></figcaption></figure><p id="b76d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们来看看为什么伪标签实际上是有效的。</p><h1 id="daca" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">4.伪标签为什么会起作用？</h1><p id="4730" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">任何半监督学习算法的目标都是使用未标记和标记样本来学习数据的底层结构。伪标签能够通过做出两个重要的假设来做到这一点:</p><ol class=""><li id="4813" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated"><strong class="la iu">连续性假设(平滑度)</strong> : <em class="lv">彼此靠近的点更有可能共享一个标签。(</em> <a class="ae lu" href="https://en.wikipedia.org/wiki/Semi-supervised_learning#Continuity_assumption" rel="noopener ugc nofollow" target="_blank"> <em class="lv">维基</em> </a> <em class="lv"> ) </em>换句话说，输入的小变化<strong class="la iu">不会引起输出的大变化</strong>。这种假设允许伪标记推断图像中的小变化如旋转、剪切等不会改变标记。</li><li id="a787" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated"><strong class="la iu">聚类假设</strong> : <em class="lv">数据往往会形成离散的聚类，而分在</em> <strong class="la iu"> <em class="lv">中的同一个聚类</em> </strong> <em class="lv">更有可能与</em> <strong class="la iu"> <em class="lv">共用一个标签</em> </strong>。<em class="lv">这是连续性假设</em> ( <a class="ae lu" href="https://en.wikipedia.org/wiki/Semi-supervised_learning#Cluster_assumption" rel="noopener ugc nofollow" target="_blank">维基百科</a>)的一个特例，另一种看待这个问题的方式是——类之间的决策边界位于低密度区域<em class="lv">(这样做有助于泛化——类似于 SVM 这样的最大间隔分类器)。</em></li></ol><p id="3f18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是初始标记数据非常重要的原因，它有助于模型了解底层的聚类结构。当我们在代码中分配一个伪标签时，我们使用模型已经学习的聚类结构来推断未标记数据的标签。随着训练的进行，使用未标记的数据来改进所学习的聚类结构。</p><p id="76c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果初始标记的数据太小或包含离群值，伪标记可能会将不正确的标记分配给未标记的点。相反的情况也成立，即伪标记可以受益于仅用标记数据就已经表现良好的分类器。</p><p id="39f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在下一节中，当我们查看伪标签失败的场景时，这应该更有意义。</p><h1 id="2153" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">5.伪标签什么时候效果不好？</h1><h2 id="f598" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">初始标记数据不足以确定聚类</h2><p id="2596" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">为了更好地理解这种情况，让我们进行一个小实验:不使用 1000 个初始点，让我们采取极端情况，只使用 10 个标记点，看看伪标记如何执行:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/a1cb952d847b8ecedc60fe691ab77baa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*q_rapXo8Xd_np4mvcMwSHQ.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">10 Initial Labeled Points</figcaption></figure><p id="c697" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不出所料，伪标几乎没有区别。模型本身和随机模型一样好，精度 10%。由于每个类实际上只有 1 个点，该模型无法学习任何类的底层结构。</p><p id="d71e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们将标记点的数量增加到 20(每节课 2 点) :</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/8ce9f7f2a6275a2a309b7c1be92e687f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Wv7DAKioR4sAUqRPCwg1gw.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">20 Initial Labeled Points</figcaption></figure><p id="7f9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，该模型的性能略有提高，因为它学习了一些类的结构。这里有一些有趣的东西-请注意，伪标注为这些点(下图中用红色标记)分配了正确的标注，这很可能是因为附近有两个标注点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/bbedc892f3532e3c5192a6d002d98911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GgV8wJX6sF-gpci2CxZZhQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Small Cluster near labeled points</figcaption></figure><p id="ccff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，让我们试试 50 分:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/bdb1c15c9cab272665df1da40399d13b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OoyNFHZA_RQDf4vi78QOxA.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">50 Labeled Points</figcaption></figure><p id="2d24" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">性能好多了！再一次，注意图像正中央的一小组棕色标记的点。同一个棕色聚类中距离标记点较远的点总是被错误地预测为水绿色(“4”)或橙色(“7”)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/194cb80fb7253daf6efba1513fd5d54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*z7vCeKytV4SujnLNZSy1FQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pseudo Labels near labeled points</figcaption></figure><p id="e04b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">需要注意一些事情:</p><ol class=""><li id="c742" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">对于上述所有实验(10 分、20 分和 50 分)，标记点的选择方式产生了巨大的差异。任何异常值都会完全改变模型的性能和对伪标签的预测。这是小数据集的常见问题。(<em class="lv">你可以阅读</em> <a class="ae lu" rel="noopener" target="_blank" href="/text-classification-with-extremely-small-datasets-333d322caee2"> <em class="lv">我之前的博客</em> </a> <em class="lv">我已经详细讨论过这个问题了)</em></li><li id="c6c8" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">虽然 TSNE 是一个很好的可视化工具，但我们需要记住，它是概率性的，仅仅给了我们一个关于星团如何在高维空间中分布的想法。</li></ol><p id="3d9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总之，当涉及伪标记时，初始标记点的数量和质量都会产生影响。此外，模型可能需要不同类的不同数量的数据来理解特定类的结构。</p><h2 id="b9ab" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">初始标记数据不包括某些类别</h2><p id="430d" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">让我们看看如果标记的数据集不包含一个类会发生什么(例如:“7”不包含在标记的集中，但未标记的数据仍然保留所有类)</p><p id="3505" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在标记数据上训练 100 个时期后:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1c3e" class="md me it ok b gy oo op l oq or">Test Acc : 85.63000 | Test Loss : 1.555</span></pre><p id="bd98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在半监督训练后:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="9809" class="md me it ok b gy oo op l oq or">Epoch: 99 : Alpha Weight : 2.50000 | Test Acc : 87.98000 | Test Loss : 2.987</span></pre><p id="c588" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总体准确度确实从 85.6%增加到 87.98%，但是之后没有显示出任何改进。这显然是因为模型无法学习类标签“7”的聚类结构。</p><p id="a61b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的动画应该清楚地说明了这些:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/ae880e395e5eed608b6cec0730e2c427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpc3Xu7KmJE0kgL1qo4OOg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/81d984b05287fe1e247785cc2e3d157f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*DVgaEZNLtzqeJTvnjZvaWQ.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Animation for pseudo-labeling with a missing class</figcaption></figure><p id="2a9a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">毫不奇怪，伪标签在这里很困难，因为我们的模型没有能力学习它以前从未见过的类。然而，在过去几年中，人们对零触发学习技术表现出了极大的兴趣，这种技术使模型能够识别标签，即使它们不存在于训练数据中。</p><h2 id="b424" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">数据增加没有任何好处</h2><p id="80f9" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">在某些情况下，模型可能不够复杂，无法利用额外的数据。这通常发生在将伪标记与传统的 ML 算法(如逻辑回归或 SVM)一起使用时。当谈到深度学习模型时，正如吴恩达在他的 Coursera 课程中提到的那样——大型 DL 模型几乎总是受益于更多的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/0258797334685cacdcf31d18324f9a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1b57MV6akYI-O-7dmJlFbQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Andrew Ng — Coursera Deep Learning Specialization</figcaption></figure><h1 id="b3c0" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">6.用传统的 ML 算法进行伪标记</h1><p id="c10d" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">在这一节中，我们将把伪标签概念应用于逻辑回归。我们将使用相同的 MNIST 数据集，包括 1000 张标记图像、59000 张未标记图像和 10000 张测试图像。</p><h2 id="4997" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">特征工程</h2><p id="04b1" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">我们首先将所有的图像归一化，然后进行 PCA 分解，从 784 维分解到 50 维。接下来，我们将使用 sklearn 的 degree = 2 的<code class="fe pf pg ph ok b">PolynomialFeatures()</code>来添加交互和二次特性。这使得我们每个数据点有 1326 个特征。</p><h2 id="37d1" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">数据集规模增加的影响</h2><p id="70ba" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">在我们开始伪标记之前，让我们检查一下当训练数据集大小缓慢增加时，逻辑回归是如何执行的。这将有助于我们了解模型是否可以从伪标签中受益。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/d78ff1d0a419c51c00ee6ecb0522bee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*vmH8igMI09sQi_rZkA8G_g.png"/></div></figure><p id="9170" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随着训练数据集中的样本数量从 100 增加到 1000，我们看到准确性慢慢增加。此外，看起来准确性并没有停滞不前，而是呈上升趋势。由此，我们可以得出这样的结论:伪标签应该可以提高我们的性能。</p><h2 id="f3ca" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">基准性能:</h2><p id="f837" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">让我们检查当逻辑回归只使用 1000 个标记图像时的测试准确性。我们将进行 10 次训练，以考虑测试分数的任何变化。</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="46b6" class="md me it ok b gy oo op l oq or">from sklearn.linear_model import SGDClassifier</span><span id="dfc6" class="md me it ok b gy ot op l oq or">test_acc = []<br/>for _ in range(10):<br/>    log_reg  = SGDClassifier(loss = 'log', n_jobs = -1, alpha = 1e-5)<br/>    log_reg.fit(x_train_poly, y_train)<br/>    y_test_pred = log_reg.predict(x_test_poly)<br/>    test_acc.append(accuracy_score(y_test_pred, y_test))<br/>    <br/>    <br/>print('Test Accuracy: {:.2f}%'.format(np.array(test_acc).mean()*100))</span><span id="c82d" class="md me it ok b gy ot op l oq or"><strong class="ok iu">Output:<br/></strong>Test Accuracy: 90.86%</span></pre><p id="32a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用逻辑回归的基线测试准确率为 90.86%</p><h2 id="351c" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">伪标签实现:</h2><p id="7a48" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">当使用逻辑回归和其他传统的 ML 算法时，我们需要以稍微不同的方式使用伪标记，尽管概念是相同的。</p><p id="a362" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是步骤:</p><ol class=""><li id="6de4" class="mw mx it la b lb lc le lf lh oa ll ob lp oc lt nd ne nf ng bi translated">我们首先在我们的标记训练集上训练一个分类器。</li><li id="d168" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">接下来，我们使用这个分类器从无标签数据集中预测随机采样集上的标签。</li><li id="7bf7" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">我们组合原始训练集和预测集，并在这个新数据集上重新训练分类器。</li><li id="3b40" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">重复步骤 2 和 3，直到使用完所有未标记的数据。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/33276c80d0c01510c50eedc4e5aa588d.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*XJYhSZKG1tSB2Lf0iSG4xA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Flowchart for Pseudo-Labeling with conventional ML Algos</figcaption></figure><p id="01de" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种技术与这篇<a class="ae lu" href="https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/" rel="noopener ugc nofollow" target="_blank">博客</a>中提到的略有相似。然而，这里我们递归地生成伪标签，直到所有未标记的数据都被使用。阈值也可以用于确保只为模型非常确信的点生成伪标签(尽管这不是必需的)。</p><p id="4cee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是围绕 sklearn 估算器的包装实现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pseudo-Labeling Wrapper for Logistic Regression</figcaption></figure><p id="472e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">(完整代码可在</em> <a class="ae lu" href="https://github.com/anirudhshenoy/pseudo_labeling_small_datasets" rel="noopener ugc nofollow" target="_blank"> <em class="lv">回购</em> </a> <em class="lv"> ) </em></p><p id="72c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们可以把它和逻辑回归一起使用:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1add" class="md me it ok b gy oo op l oq or">from sklearn.linear_model import SGDClassifier</span><span id="6594" class="md me it ok b gy ot op l oq or">log_reg = SGDClassifier(loss = 'log', n_jobs = -1, alpha = 1e-5)</span><span id="ca61" class="md me it ok b gy ot op l oq or">pseudo_labeller = pseudo_labeling(<br/>        log_reg,<br/>        x_unlabeled_poly,<br/>        sample_rate = 0.04,<br/>        verbose = True<br/>    )</span><span id="081d" class="md me it ok b gy ot op l oq or">pseudo_labeller.fit(x_train_poly, y_train)</span><span id="caf3" class="md me it ok b gy ot op l oq or">y_test_pred = pseudo_labeller.predict(x_test_poly)<br/>print('Test Accuracy: {:.2f}%'.format(accuracy_score(y_test_pred, y_test)*100))</span><span id="fb28" class="md me it ok b gy ot op l oq or"><strong class="ok iu">Output:</strong><br/>Test Accuracy: 92.42%</span></pre><p id="0fcc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">伪标记将准确率从 90.86%提高到 92.42%。<em class="lv">(具有更高复杂性的非线性模型，如 XGBoost，可能执行得更好)</em></p><p id="afc9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，<code class="fe pf pg ph ok b">sample_rate</code>类似于深度学习模型例子中的<code class="fe pf pg ph ok b">alpha(t)</code>。以前，alpha 用于控制使用的未标记丢失的数量，而在这种情况下,<code class="fe pf pg ph ok b">sample_rate</code>控制在每次迭代中使用多少未标记的点。</p><p id="cd3f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe pf pg ph ok b">sample_rate</code>值本身是一个超参数，需要根据数据集和模型<em class="lv">进行调整(类似于 T1、T2 和 alpha_f)。</em>值 0.04 最适合 MNIST +逻辑回归示例。</p><p id="f90f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个有趣的修改是安排<code class="fe pf pg ph ok b">sample_rate</code>随着训练的进行而加速，就像<code class="fe pf pg ph ok b">alpha(t).</code>一样</p><p id="c651" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们结束之前，让我们看看半监督学习中的一些挑战。</p><h1 id="67c3" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">7.半监督学习的挑战</h1><h2 id="c224" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">将未标记数据与标记数据相结合</h2><p id="c18c" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">半监督学习的主要目标是使用未标记数据和标记数据来理解数据集的底层结构。这里显而易见的问题是——如何利用未标记的数据来达到这个目的？</p><p id="c6ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在伪标记技术中，我们看到使用预定的权重函数(<code class="fe pf pg ph ok b">alpha</code>)来缓慢地将未标记的数据与标记的数据相结合。然而，<code class="fe pf pg ph ok b">alpha(t)</code>函数假设模型置信度随时间增加，因此线性增加未标记的损失。不一定是这种情况，因为模型预测有时可能不正确。事实上，如果模型做出了几个错误的未标记预测，伪标记可能会像一个糟糕的反馈循环一样，进一步恶化性能。<em class="lv">(参考:第 3.1 节</em> <a class="ae lu" href="https://arxiv.org/abs/1908.02983" rel="noopener ugc nofollow" target="_blank"> <em class="lv">阿拉佐等人 2019</em></a><em class="lv">【2】)</em></p><p id="dbed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述问题的一个解决方案是使用概率阈值——类似于我们对逻辑回归所做的。</p><p id="5772" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其他半监督学习算法使用不同的方式来组合数据，例如，MixMatch 使用两步过程来猜测标签(对于未标记的数据),然后使用 MixUp 数据扩充来组合未标记的数据和标记的数据。(<a class="ae lu" href="https://arxiv.org/abs/1905.02249" rel="noopener ugc nofollow" target="_blank">贝特洛等(2019)[3] </a>)</p><h2 id="af2b" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">数据效率</h2><p id="f077" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">半监督学习的另一个挑战是设计可以处理非常少量的标记数据的算法。正如我们在伪标记中看到的，该模型对 1000 个初始标记样本的效果最好。然而，当标注数据集进一步减少时(例如:50 个点)，伪标注的性能开始下降。</p><p id="5078" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://arxiv.org/abs/1804.09170" rel="noopener ugc nofollow" target="_blank">Oliver et al .(2018)</a>【4】对几种半监督学习算法进行了比较，发现伪标记在“two-moons”数据集上失败，而 VAT 和 pi-model 等其他模型工作得更好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/17487168e4303ff69b9f5bc6b7037e28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*itl5Zy73wN5Z6aENBUCu0A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: Oliver et al (2018) [4]</figcaption></figure><p id="fbab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如图所示，VAT 和 Pi-Model 学习了一个决策边界，仅用 6 个带标签的数据点(显示在白色和黑色的大圆圈中)就取得了惊人的效果。另一方面，伪标记完全失败，取而代之的是学习线性决策边界。</p><p id="3486" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用 Oliver 等人使用的相同模型重复了实验，发现伪标记需要 30-50 个标记点(取决于标记点的位置)来学习底层数据结构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/5ea9493be7b85466c3c117231676c75a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*_OlEaffoxWKq7SqzIV9FqA.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Pseudo-Labeling on Two Moons Dataset. Triangles are labeled points.</figcaption></figure><p id="3170" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了使半监督学习更加实用，我们需要数据效率高的算法，即可以在非常少量的标记数据上工作的算法。</p><h1 id="b30d" class="nm me it bd mf nn no np mi nq nr ns ml jz nt ka mo kc nu kd mr kf nv kg mu nw bi translated">8.结论</h1><p id="3bed" class="pw-post-body-paragraph ky kz it la b lb my ju ld le mz jx lg lh nx lj lk ll ny ln lo lp nz lr ls lt im bi translated">Oliver 等人[4]提到:“<em class="lv">伪标记是一种简单的启发式方法，在实践中被广泛使用，可能是因为它的简单性和通用性</em>”，正如我们所见，它提供了一种学习半监督学习的好方法。</p><p id="2285" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在过去的 2-3 年中，用于图像分类的半监督学习已经取得了一些令人难以置信的进步。无监督数据增强(<a class="ae lu" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank">谢等(2019) [5] </a>)在 CIFAR- 10 上仅用 4000 个标签就取得了 97.3%的效果。为了客观地看待这一点，DenseNet ( <a class="ae lu" href="https://arxiv.org/pdf/1608.06993v5.pdf" rel="noopener ugc nofollow" target="_blank">黄等人(2016 </a> )[6])在 2016 年在完整的 CIFAR-10 数据集上实现了 96.54%。</p><p id="7bd9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看到机器学习和数据科学社区如何转向使用更少标记数据(如半监督学习、零/少量学习)或更小数据集(如迁移学习)的算法，真的很有趣。就个人而言，我认为如果我们真的想为所有人普及人工智能，这些发展是至关重要的。</p><p id="6af6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有任何问题，请随时与我联系。我希望你喜欢！</p></div><div class="ab cl pm pn hx po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="im in io ip iq"><p id="8533" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">Github Repo:</strong><a class="ae lu" href="https://github.com/anirudhshenoy/pseudo_labeling_small_datasets" rel="noopener ugc nofollow" target="_blank">https://Github . com/anirudhshenoy/pseudo _ labeling _ small _ datasets</a></p><p id="f361" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">数据集:</strong><a class="ae lu" href="https://www.kaggle.com/oddrationale/mnist-in-csv" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/oddrationale/mnist-in-csv</a></p><h2 id="76c9" class="md me it bd mf mg mh dn mi mj mk dp ml lh mm mn mo ll mp mq mr lp ms mt mu mv bi translated">参考资料:</h2><ol class=""><li id="44ee" class="mw mx it la b lb my le mz lh na ll nb lp nc lt nd ne nf ng bi translated">李东炫。“伪标签:深度神经网络的简单有效的半监督学习方法”ICML 2013 研讨会:表征学习的挑战(WREPL)，美国佐治亚州亚特兰大，2013 年(<a class="ae lu" href="http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf" rel="noopener ugc nofollow" target="_blank">http://Deep Learning . net/WP-content/uploads/2013/03/Pseudo _ Label _ final . pdf</a>)</li><li id="a7c5" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">埃里克阿拉索，迭戈奥特戈，保罗阿尔伯特，诺埃尔奥康纳，凯文麦克吉尼斯。“深度半监督学习中的伪标记和确认偏差”(<a class="ae lu" href="https://arxiv.org/abs/1908.02983" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.02983</a>)</li><li id="3557" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">大卫·贝特洛，尼古拉斯·卡里尼，伊恩·古德菲勒，尼古拉斯·帕伯诺，阿维塔尔·奥利弗，科林·拉斐尔。“混合匹配:半监督学习的整体方法”(<a class="ae lu" href="https://arxiv.org/abs/1905.02249" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.02249</a>)</li><li id="28ba" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">"阿维塔尔·奥利弗，奥古斯都·奥登纳，科林·拉斐尔，艾金·d·库布克，伊恩·j·古德菲勒. "深度半监督学习算法的现实评估”(<a class="ae lu" href="https://arxiv.org/abs/1804.09170" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1804.09170</a>)</li><li id="9fca" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">谢启哲，戴子航，Eduard Hovy，Minh-Thang Luong，郭诉乐。“用于一致性训练的无监督数据扩充”(<a class="ae lu" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1904.12848</a>)</li><li id="040f" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">黄高，刘庄，劳伦斯·范·德·马滕，基利安·q·温伯格。“密集连接的卷积网络”(【https://arxiv.org/abs/1608.06993】T2)</li><li id="bc9d" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated">【https://github.com/peimengsui/semi_supervised_mnist T4】</li><li id="d56a" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated"><a class="ae lu" href="https://www.analyticsvidhya.com/blog/2017/09/pseudo-labelling-semi-supervised-learning-technique/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/09/pseudo-labeling-semi-supervised-learning-technique/</a></li><li id="475a" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated"><a class="ae lu" href="https://www.quora.com/Why-is-unsupervised-learning-important" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Why-is-unsupervised-learning-important</a></li><li id="23f9" class="mw mx it la b lb nh le ni lh nj ll nk lp nl lt nd ne nf ng bi translated"><a class="ae lu" href="https://www.wired.com/2014/08/deep-learning-yann-lecun/" rel="noopener ugc nofollow" target="_blank">https://www.wired.com/2014/08/deep-learning-yann-lecun/</a></li></ol></div></div>    
</body>
</html>