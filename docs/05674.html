<html>
<head>
<title>Data Preprocessing in Data Mining &amp; Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据挖掘和机器学习中的数据预处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-preprocessing-in-data-mining-machine-learning-79a9662e2eb?source=collection_archive---------3-----------------------#2019-08-20">https://towardsdatascience.com/data-preprocessing-in-data-mining-machine-learning-79a9662e2eb?source=collection_archive---------3-----------------------#2019-08-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="192b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有了详细的概念…</h2></div><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk">Video version of the story, if you are into that sort of thing | Part 1</figcaption></figure><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk">Video version of the story, if you are into that sort of thing | Part 2</figcaption></figure><p id="8287" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">在</span> <a class="ae ly" rel="noopener" target="_blank" href="/measures-of-proximity-in-data-mining-machine-learning-e9baaed1aafb"> <em class="lz">我之前的一个帖子</em> </a>中，我讲过数据挖掘中的<strong class="kv iu"> <em class="lz">接近度的度量&amp;机器学习</em> </strong>。这个就继续那个，如果还没看的话，这里看一下<a class="ae ly" rel="noopener" target="_blank" href="/measures-of-proximity-in-data-mining-machine-learning-e9baaed1aafb"><strong class="kv iu"><em class="lz"/></strong></a><strong class="kv iu"><em class="lz"/></strong>以便对我在文章中要讲的话题和概念有一个恰当的把握。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="7da0" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi lp translated">数据预处理是指使数据更适合数据挖掘的步骤。用于数据预处理的步骤通常分为两类:</p><ol class=""><li id="e20f" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo mm mn mo mp bi translated">为分析选择数据对象和属性。</li><li id="6eb5" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo mm mn mo mp bi translated">创建/更改属性。</li></ol><blockquote class="mv"><p id="2ba1" class="mw mx it bd my mz na nb nc nd ne lo dk translated">请容忍我的概念部分，我知道这可能有点无聊，但如果你有强大的基础，那么没有什么可以阻止你成为一名伟大的数据科学家或机器学习工程师。</p></blockquote><p id="9741" class="pw-post-body-paragraph kt ku it kv b kw nf ju ky kz ng jx lb lc nh le lf lg ni li lj lk nj lm ln lo im bi translated">在本次讨论中，我们将讨论以下数据预处理方法:</p><ul class=""><li id="80b4" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated">聚合</li><li id="438a" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">抽样</li><li id="0808" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">降维</li><li id="e0ba" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">特征子集选择</li><li id="d339" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">特征创建</li><li id="699f" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">离散化和二值化</li><li id="9b3d" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">变量变换</li></ul></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="d8cd" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是聚合？</strong></p></blockquote><p id="128c" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→简单来说，它是指将两个或多个属性(或对象)组合成一个属性(或对象)。</p><p id="e709" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv iu"> <em class="lz">聚合</em> </strong>的用途如下:</p><p id="7bc2" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <strong class="kv iu">数据缩减:</strong>减少对象或属性的数量。这导致更小的数据集，因此需要更少的内存和处理时间，因此，聚合可以允许使用更昂贵的数据挖掘算法。</p><p id="f36a" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <strong class="kv iu">规模的变化:</strong>通过提供数据的高级视图而不是低级视图，聚合可以充当范围或规模的变化。举个例子，</p><ul class=""><li id="b498" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated">聚集成地区、州、国家等的城市。</li><li id="377c" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated">天数累计成周、月和年。</li></ul><p id="dd5d" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <strong class="kv iu">更“稳定”的数据:</strong>聚合数据往往可变性更小。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="b3a4" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是采样？</strong></p></blockquote><p id="79f6" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→抽样是选择要分析的数据对象子集的常用方法。</p><p id="fc87" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→取样的关键是使用具有代表性的样品<strong class="kv iu">。</strong>如果样本具有与原始数据集大致相同的(感兴趣的)属性，则该样本具有代表性。如果数据对象的均值(平均值)是感兴趣的属性，那么如果样本的均值接近原始数据的均值，则样本是有代表性的。</p><h2 id="c650" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">取样类型</h2><ul class=""><li id="53a5" class="mh mi it kv b kw ok kz ol lc om lg on lk oo lo nk mn mo mp bi translated"><strong class="kv iu">简单随机抽样</strong>:</li></ul><p id="9950" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→选择任何特定项目的概率相等</p><p id="e1c4" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <strong class="kv iu">无替换抽样:</strong>当每一项被选中时，就从总体中删除。</p><p id="b661" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <strong class="kv iu">替换抽样:</strong>当对象被选作样本时，它们不会从总体中删除。在替换取样中，同一物体可以被多次拾取。</p><ul class=""><li id="221a" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">分层抽样:</strong>把数据分成几个分区，然后从每个分区抽取随机样本。</li><li id="2349" class="mh mi it kv b kw mq kz mr lc ms lg mt lk mu lo nk mn mo mp bi translated"><strong class="kv iu">渐进采样:</strong>恰当的样本量可能难以确定，因此有时会使用<em class="lz">自适应或渐进采样</em>方案。这些方法从小样本开始，然后增加样本量，直到获得足够大的样本。</li></ul></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="3e5f" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是降维？</strong></p></blockquote><p id="07a9" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→降维一词通常是指那些通过创建旧属性组合而成的新属性来降低数据集维度的技术。</p><ul class=""><li id="8582" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">目的:</strong></li></ul><p id="52eb" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→避免<a class="ae ly" rel="noopener" target="_blank" href="/types-of-data-sets-in-data-science-data-mining-machine-learning-eb47c80af7a">维数灾难</a>。要了解这方面的更多信息，请访问我之前的一篇详细解释它的文章<a class="ae ly" rel="noopener" target="_blank" href="/types-of-data-sets-in-data-science-data-mining-machine-learning-eb47c80af7a">。</a></p><p id="14e9" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→减少数据挖掘算法所需的时间和内存。</p><p id="f80e" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→使数据更容易可视化。</p><p id="6edf" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→可能有助于消除不相关的特征或减少噪音。</p><ul class=""><li id="0ecb" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">技法:</strong></li></ul><p id="7987" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <a class="ae ly" href="https://www.youtube.com/watch?v=kw9R0nD69OU" rel="noopener ugc nofollow" target="_blank">主成分分析</a></p><p id="e038" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→ <a class="ae ly" href="https://www.youtube.com/watch?v=mBcLRGuAFUk" rel="noopener ugc nofollow" target="_blank">奇异值分解</a></p><p id="0cda" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里提到的技术非常广泛，可以在本文中讨论。你可以在网上了解更多。我在这两个网站上都添加了 YouTube 链接，如果你想看这些视频并学习的话。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="e745" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是特征子集选择？</strong></p></blockquote><p id="8ef0" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→这是通过仅使用可用特征的子集来降低数据维数的另一种方法。虽然这种方法似乎会丢失信息，但如果存在冗余和不相关的特征，情况就不是这样了。</p><ul class=""><li id="dae4" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">冗余功能:</strong></li></ul><p id="38b2" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→复制一个或多个其他属性中包含的大部分或全部信息。示例:产品的购买价格和支付的销售税金额。</p><ul class=""><li id="15a5" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">无关特性:</strong></li></ul><p id="baea" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→不包含对手头的数据挖掘任务有用的信息。例子:学生的 ID 通常与预测学生 GPA 的任务无关。</p><p id="d988" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">虽然通过使用常识或领域知识可以立即消除一些不相关和冗余的属性，但是选择最佳的特征子集通常需要系统的方法。选择功能的理想方法是尝试将所有可能的功能子集作为感兴趣的数据挖掘算法的输入，然后选择产生最佳结果的子集。</p><p id="14c1" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">有三种标准的特征选择方法:<strong class="kv iu"> <em class="lz">嵌入、过滤和包装。</em>T3】</strong></p><h2 id="da56" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">嵌入式方法</h2><p id="63ef" class="pw-post-body-paragraph kt ku it kv b kw ok ju ky kz ol jx lb lc op le lf lg oq li lj lk or lm ln lo im bi translated">特征选择作为数据挖掘算法的一部分自然发生。具体来说，在数据挖掘算法的操作过程中，算法本身决定使用哪些属性，忽略哪些属性。</p><h2 id="934c" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">过滤方法</h2><p id="9705" class="pw-post-body-paragraph kt ku it kv b kw ok ju ky kz ol jx lb lc op le lf lg oq li lj lk or lm ln lo im bi translated">使用一些独立于数据挖掘任务的方法，在运行数据挖掘算法之前选择特征。例如，我们可能选择那些成对<a class="ae ly" rel="noopener" target="_blank" href="/measures-of-proximity-in-data-mining-machine-learning-e9baaed1aafb">相关性</a>尽可能低的属性集。</p><h2 id="4e6c" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">包装方法</h2><p id="9dd6" class="pw-post-body-paragraph kt ku it kv b kw ok ju ky kz ol jx lb lc op le lf lg oq li lj lk or lm ln lo im bi translated">这些方法使用目标数据挖掘算法作为黑盒，以类似于上述理想算法的方式找到属性的最佳子集，但是通常不枚举所有可能的子集。</p><h2 id="e748" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">特征子集选择过程的流程图</h2><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="ot ou di ov bf ow"><div class="gh gi os"><img src="../Images/c8d14a462ff2c1be039e596cce2eee6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dmNypXLLHNKV5hZ87XCFsw.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk">Introduction to Data Mining — Pang-Ning Tan, Michael Steinbach, Vipin Kumar</figcaption></figure></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="8671" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是特征创建？</strong></p></blockquote><p id="9f3c" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→它包括创建新属性，这些属性可以比原始属性更有效地捕获数据集中的重要信息。</p><h2 id="f1bd" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">三种通用方法:</h2><ul class=""><li id="91a3" class="mh mi it kv b kw ok kz ol lc om lg on lk oo lo nk mn mo mp bi translated"><strong class="kv iu">特征提取</strong></li></ul><p id="8378" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→从原始数据中创建一组新的特征称为特征提取。考虑一组照片，其中每张照片根据是否包含人脸来分类。原始数据是一组像素，因此不适合许多类型的分类算法。然而，如果数据被处理以提供更高级的特征，例如与人脸的存在高度相关的某些类型的边缘和区域的存在或不存在，那么一组更广泛的分类技术可以被应用于这个问题。</p><p id="b783" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→这种方法具有高度的领域特异性。</p><ul class=""><li id="32a8" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">特征构造</strong></li></ul><p id="ac67" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→有时原始数据集中的特征具有必要的信息，但其形式不适合数据挖掘算法。在这种情况下，由原始特征构建的一个或多个新特征可能比原始特征更有用。</p><p id="6766" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→示例:用质量除以体积得到密度</p><ul class=""><li id="8036" class="mh mi it kv b kw kx kz la lc mj lg mk lk ml lo nk mn mo mp bi translated"><strong class="kv iu">将数据映射到新空间</strong></li></ul><p id="bc10" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→完全不同的数据视图可以揭示重要而有趣的特征。例如，考虑通常包含周期性模式的时间序列数据。如果只有一个周期模式并且没有太多噪声，那么该模式很容易被检测到。另一方面，如果存在许多周期性模式，并且存在大量噪声，则这些模式很难被检测到。然而，这种模式通常可以通过对时间序列应用傅立叶变换来检测，以便改变到频率信息明确的表示。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="1ba9" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是离散化和二值化？</strong></p></blockquote><h2 id="6519" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">[数]离散化</h2><p id="1e82" class="pw-post-body-paragraph kt ku it kv b kw ok ju ky kz ol jx lb lc op le lf lg oq li lj lk or lm ln lo im bi translated">→离散化是将连续属性转化为有序属性的过程。</p><p id="e8c9" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→潜在的无限数量的值被映射到少数类别中。</p><p id="99a0" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→分类中常用离散化。</p><p id="c28d" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→如果自变量和因变量都只有几个值，许多分类算法效果最好。</p><h2 id="02ca" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated"><strong class="ak">二值化</strong></h2><p id="8ef3" class="pw-post-body-paragraph kt ku it kv b kw ok ju ky kz ol jx lb lc op le lf lg oq li lj lk or lm ln lo im bi translated">→二进制化将连续或分类属性映射成一个或多个二进制变量</p><p id="f598" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→通常用于关联分析</p><p id="f41c" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→通常将连续属性转换为分类属性，然后将分类属性转换为一组二元属性</p><p id="01d0" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→关联分析需要不对称的二元属性</p><p id="6d17" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→示例:眼睛颜色和高度测量为{低、中、高}</p><h2 id="962f" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">一个分类属性到三个二元属性的转换</h2><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="ot ou di ov bf ow"><div class="gh gi oz"><img src="../Images/97aa819fe2b3a38fd4be2389e0a71513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJkvtR8S4pcNImUp7deRbQ.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk">Introduction to Data Mining — Pang-Ning Tan, Michael Steinbach, Vipin Kumar</figcaption></figure><h2 id="bb5b" class="nr ns it bd nt nu nv dn nw nx ny dp nz lc oa ob oc lg od oe of lk og oh oi oj bi translated">一个分类属性到五个非对称二进制属性的转换</h2><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="ot ou di ov bf ow"><div class="gh gi pa"><img src="../Images/b43c76434737adbc97655959aab559e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GCn3L9zAXenntybsotLTGg.png"/></div></div><figcaption class="kp kq gj gh gi kr ks bd b be z dk">Introduction to Data Mining — Pang-Ning Tan, Michael Steinbach, Vipin Kumar</figcaption></figure></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="ba6a" class="kt ku lz kv b kw kx ju ky kz la jx lb no ld le lf np lh li lj nq ll lm ln lo im bi translated"><strong class="kv iu">什么是变量变换？</strong></p></blockquote><p id="b837" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">→属性转换是一种将给定属性的整组值映射到一组新的替换值的功能，这样每个旧值都可以用一个新值来标识</p><p id="7994" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv iu"> →简单函数:</strong>幂(x，k)，对数(x)，幂(e，x)，|x|</p><p id="b671" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><strong class="kv iu"> →标准化:</strong>指的是根据出现频率、平均值、方差、范围调整属性间差异的各种技术<strong class="kv iu"> →标准化:</strong>在统计学中，指的是减去平均值并除以标准差。</p><p id="4621" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们对数据预处理的讨论到此结束。</p><p id="b7e4" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个帖子的后续是<a class="ae ly" rel="noopener" target="_blank" href="/data-preprocessing-in-python-b52b652e37d5">这里</a>。</p><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/data-preprocessing-in-python-b52b652e37d5"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">Python 中的数据预处理</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">对于机器学习与工作代码的例子…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ox pe"/></div></div></a></div></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="e133" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我将免费赠送一本关于一致性的电子书。在这里获得你的免费电子书。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="4530" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你喜欢阅读这样的故事，那么你应该<a class="ae ly" href="https://tarun-gupta.medium.com/subscribe" rel="noopener"> <strong class="kv iu">在你的收件箱</strong> </a>中收到我的帖子，如果你想支持我成为一名作家，可以考虑<a class="ae ly" href="https://tarun-gupta.medium.com/membership" rel="noopener">注册成为一名媒体会员</a>。每月 5 美元，你可以无限制地阅读媒体上的故事。如果你注册使用我的链接，我会赚一小笔佣金，不需要你额外付费。</p><div class="pb pc gp gr pd pe"><a href="https://tarun-gupta.medium.com/membership" rel="noopener follow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">加入我的推荐链接-塔伦古普塔</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">tarun-gupta.medium.com</p></div></div><div class="pn l"><div class="pt l pp pq pr pn ps ox pe"/></div></div></a></div><p id="6f87" class="pw-post-body-paragraph kt ku it kv b kw kx ju ky kz la jx lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">感谢阅读。如果你喜欢这篇文章，可以去看看我关于数据挖掘和机器学习的其他文章。</p><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/data-preprocessing-in-python-b52b652e37d5"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">Python 中的数据预处理</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">对于机器学习与工作代码的例子…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ox pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/measures-of-proximity-in-data-mining-machine-learning-e9baaed1aafb"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">数据挖掘和机器学习中的相似性度量</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">在分析过程中执行数据转换</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="pu l pp pq pr pn ps ox pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/assessing-the-quality-of-data-e5e996a1681b"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">评估数据质量</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">对于数据挖掘和机器学习算法…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="pv l pp pq pr pn ps ox pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/types-of-data-sets-in-data-science-data-mining-machine-learning-eb47c80af7a"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">数据科学、数据挖掘和机器学习中的数据集类型</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">以及它们的一般特征…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="pw l pp pq pr pn ps ox pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a rel="noopener follow" target="_blank" href="/journey-into-data-mining-3b5ccfa5343"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">数据挖掘之旅</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">数据导论</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">towardsdatascience.com</p></div></div><div class="pn l"><div class="px l pp pq pr pn ps ox pe"/></div></div></a></div></div></div>    
</body>
</html>