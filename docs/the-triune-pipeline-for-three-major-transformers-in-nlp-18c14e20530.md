# åŸºäº Scikit-Learn æµæ°´çº¿çš„æ–‡æœ¬ç‰¹å¾æå–

> åŸæ–‡ï¼š<https://towardsdatascience.com/the-triune-pipeline-for-three-major-transformers-in-nlp-18c14e20530?source=collection_archive---------15----------------------->

## ä½¿ç”¨ 2020 å¹´åˆé€‰è¾©è®ºè®°å½•

![](img/3aabb13249559774b4e08a1ecb6a88e9.png)

Image [Source](https://www.looper.com/122484/best-worst-movies-blockbuster-franchises/)

# è¿™ç¯‡æ–‡ç« çš„ç›®çš„æ˜¯åŒé‡çš„ã€‚

**é¦–å…ˆ**ï¼Œå¦‚æˆ‘æ‰€æ‰¿è¯ºçš„ï¼Œæˆ‘å°†è·Ÿè¿›ä¹‹å‰çš„[å¸–å­](/which-presidential-candidate-talks-like-that-b2b16060ff8b?source=friends_link&sk=521c1d6609bdfb96e41fa2439d5b18d1)ï¼Œåœ¨é‚£ç¯‡å¸–å­ä¸­ï¼Œæˆ‘æ¯”è¾ƒäº† 2020 å¹´ 21 ä½æ°‘ä¸»å…šåˆé€‰æ€»ç»Ÿå€™é€‰äººçš„æ¼”è®²ç‰¹æ€§ã€‚æˆ‘ç¡®å®šäº†ä¸€ç³»åˆ—è¯­è¨€ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å°†åœ¨æè¿°å±‚é¢ä¸ŠåŒºåˆ†æˆ‘ä»¬çš„æ€»ç»Ÿå€™é€‰äººã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æƒ³ä½¿ç”¨è¿™äº›ç‰¹å¾æ¥å»ºç«‹ä¸€ä¸ªåˆ†ç±»æ¨¡å‹ï¼Œå¯ä»¥é¢„æµ‹è°å°†æœ‰èµ„æ ¼å‚åŠ  12 æœˆ 19 æ—¥çš„è¾©è®ºã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ç°åœ¨çŸ¥é“è°æœ‰èµ„æ ¼å‚åŠ è¾©è®ºï¼Œä½†æˆ‘æƒ³è¿™é¡¹ä»»åŠ¡èƒŒåçš„çœŸæ­£åŠ¨æœºæ˜¯æ›´å…¨é¢åœ°äº†è§£ä¸€ä¸ªäººä»¥æŸç§æ–¹å¼è¯´è¯(åœ¨è¾©è®ºèˆå°ä¸Š)å¯¹äºä½œä¸ºæ€»ç»Ÿå€™é€‰äººè¢«è®¤çœŸå¯¹å¾…(å¹¶è¢«å…è®¸è¿›å…¥ä¸‹ä¸€è½®)æœ‰å¤šé‡è¦ã€‚

**ç¬¬äºŒä¸ª**(ä¹Ÿæ˜¯æ›´é‡è¦çš„ä¸€ä¸ª)åœ¨æ„å»ºè¿™ä¸ªæ¨¡å‹çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘æƒ³åˆ†äº«ä¸€äº›æˆ‘æœ€è¿‘åœ¨ scikit-learn çš„ä¸–ç•Œä¸­éå¸¸é«˜å…´åœ°å‘ç°çš„äº‹æƒ…:`Pipeline`(æˆ‘çŸ¥é“æˆ‘å·²ç»è¿Ÿåˆ°äº†ã€‚:)**è¿™ä¸ªéå¸¸å¼ºå¤§çš„ç±»ä½¿æˆ‘çš„æ•´ä¸ª ML å·¥ä½œæµç¨‹â€”â€”ä»é¢„å¤„ç†åˆ°è¯„ä¼°â€”â€”å˜å¾—æ›´åŠ å®¹æ˜“å¤„ç†ã€æ›´åŠ å¥å£®ï¼Œå¹¶ä¸”æ›´å°‘å—åˆ°çŒœæµ‹çš„å½±å“**ï¼Œå°¤å…¶æ˜¯åœ¨è¶…å‚æ•°è°ƒä¼˜é˜¶æ®µã€‚æ­£å¦‚æˆ‘çš„ä¸€ä½åŒäº‹æ‰€è¯´ï¼Œå®ƒçœŸçš„åº”è¯¥æˆä¸ºæ¯ä¸€ä¸ªåŸºäº sklearn çš„ ML é¡¹ç›®çš„ä¸€éƒ¨åˆ†ï¼ä¸‹é¢æ˜¯å®ƒçš„åŠŸèƒ½æè¿°:

> é¡ºåºåº”ç”¨ä¸€åˆ—**å˜æ¢**å’Œä¸€ä¸ª**æœ€ç»ˆ** **ä¼°è®¡å™¨**ã€‚ç®¡é“çš„ä¸­é—´æ­¥éª¤å¿…é¡»æ˜¯â€œè½¬æ¢â€ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä»¬å¿…é¡»å®ç° fit å’Œ transform æ–¹æ³•ã€‚æœ€ç»ˆçš„ä¼°è®¡å™¨åªéœ€è¦å®ç° fitã€‚*[*1*](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)*

**å¦‚æœä½ æƒ³è¦æ›´å¤šçš„èƒŒæ™¯ä¿¡æ¯å’Œæ·±å…¥æè¿°æ€§åˆ†æçš„ç»“æœï¼Œè¯·å‚è€ƒæˆ‘ä¹‹å‰çš„æ–‡ç« ï¼Œä½†æ˜¯åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ç›´æ¥åˆ©ç”¨æ–‡ç« æ¥æ„å»ºæ‰€è°“çš„ä¸‰ä½ä¸€ä½“ç®¡é“ğŸ”±ï¼Œå…¶ç»„ä»¶ç®¡é“å¯ä»¥å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸º NLP ä»»åŠ¡çš„ä¸‰ä¸ªä¸»è¦ç‰¹å¾æ„å»ºå—:**

**![](img/364ec0920ae3f9f55f6172dc386026cc.png)**

**Three types of feature representation in NLP**

**æ ¹æ®æ‚¨æ­£åœ¨å¤„ç†çš„ç‰¹å®šä»»åŠ¡æˆ–æ¨¡å‹ï¼Œç»´æ©å›¾ä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªç‰¹å¾ç±»å‹å¯èƒ½å¯¹æ‚¨çš„æ¨¡å‹çš„æ€§èƒ½ç‰¹åˆ«é‡è¦ï¼Œä¾‹å¦‚ï¼Œå•è¯åµŒå…¥ä¼´éšç€ä¸€äº›è‡ªå®šä¹‰è¯­è¨€ç‰¹å¾çš„å­é›†ã€‚è¦äº†è§£å“ªäº›é‡å æˆ–ä¸é‡å çš„è¦ç´ é›†æ˜¯æœ€é‡è¦çš„ï¼Œæˆ‘ä»¬éœ€è¦è®¾ç½®ä¸€ä¸ªè®­ç»ƒæµæ¥æµ‹è¯•æ¯ç§è¦ç´ ç±»å‹çš„å½±å“ã€‚è¿™å°±æ˜¯`Pipeline`æ¥æ‹¯æ•‘æˆ‘ä»¬çš„åœ°æ–¹ã€‚**

# **ä½†é¦–å…ˆè¦åšçš„æ˜¯ã€‚**

**ä½¿ç”¨å‰ä¸€ç¯‡[æ–‡ç« ](/which-presidential-candidate-talks-like-that-b2b16060ff8b)ä¸­å‡†å¤‡å¥½çš„æ•°æ®å¸§ï¼Œæˆ‘ä»¬æƒ³ä¸ºç›®æ ‡å˜é‡`qualified`æ·»åŠ ä¸€åˆ—(1 æˆ– 0 è¡¨ç¤ºæ˜¯æˆ–å¦)ï¼Œå®ƒè¡¨ç¤ºè°æœ‰èµ„æ ¼å‚åŠ  12 æœˆçš„è¾©è®ºã€‚å…«åå€™é€‰äººæœ‰èµ„æ ¼å‚åŠ è¾©è®º(åŒ…æ‹¬é€€å‡ºçš„å“ˆé‡Œæ–¯):**

```
**import numpy as np
import pandas as pd**QUALIFIED_CANDIDATES** = ["BIDEN", "WARREN", "SANDERS", "BUTTIGIEG", "HARRIS", "KLOBUCHAR", "STEYER", "YANG"]CANDIDATE_TO_TARGET = {}
[CANDIDATE_TO_TARGET.update({c: 1}) if c in QUALIFIED_CANDIDATES else CANDIDATE_TO_TARGET.update({c: 0}) for c in ALL_CANDIDATE_NAMES]# add the target variable column 
qualified = df['speaker'].apply(lambda x: CANDIDATE_TO_TARGET[x] if x in CANDIDATE_TO_TARGET else np.NaN)
df['qualified'] = qualified**
```

**![](img/7f3bd2a7d03d4e0245d9544a46801444.png)**

**A dataframe containing the features and target variable (not all features shown)**

**ç„¶åï¼Œæˆ‘ä»¬å°†æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†:**

```
**from sklearn.model_selection import train_test_splittrain_df, test_df = train_test_split(df, test_size=0.1)train_data = train_df["segment"]
train_target = train_df["qualified"]test_data = test_df["segment"]
test_target = test_df["qualified"]**
```

**ç°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½äº†æœ‰è¶£çš„éƒ¨åˆ†ï¼**

# **è®¾ç½®ä¸‰ä½ä¸€ä½“ç®¡é“ã€‚**

**`**Pipeline**` **æä¾›äº†ä¸€ç§æ–¹ä¾¿ç›´è§‚çš„æ–¹å¼æ¥æ„å»ºæˆ‘ä»¬çš„ ML æµç¨‹ï¼Œå…¶ç‰¹å¾åœ¨äºä¸€ç³»åˆ—å¯é¢„æµ‹çš„æ ¸å¿ƒä»»åŠ¡**ï¼ŒåŒ…æ‹¬é¢„å¤„ç†ã€ç‰¹å¾é€‰æ‹©ã€æ ‡å‡†åŒ–/è§„èŒƒåŒ–å’Œåˆ†ç±»ã€‚`Pipeline`é€šè¿‡è¿ç»­è°ƒç”¨æ¯ä¸ªä¼°ç®—å™¨ä¸Šçš„`fit`ï¼Œå°†`transform`åº”ç”¨åˆ°è¾“å…¥ï¼Œå¹¶å°†è½¬æ¢åçš„è¾“å…¥ä¼ é€’åˆ°ä¸‹ä¸€æ­¥ï¼Œæ¥è‡ªåŠ¨åŒ–æ‹Ÿåˆ/è½¬æ¢è¿‡ç¨‹çš„å¤šä¸ªå®ä¾‹ã€‚è¿™æ„å‘³ç€`Pipeline`ä¸­çš„æ¯ä¸ªä¼°è®¡å™¨(é™¤äº†æœ€åä¸€ä¸ª)éƒ½å¿…é¡»æœ‰ä¸€ä¸ª`transform`æ–¹æ³•[ [2](https://scikit-learn.org/stable/modules/compose.html#notes) ]ï¼Œå°±åƒè¿™ä¸ªå‹å¥½çš„ sklearn transformer `TfidfVectorizer`:**

```
**class TfidfVectorizer(CountVectorizer):
    ... def fit(self, raw_documents, y=None):
        *# Learn vocabulary and idf from training set.*self._check_params()
        X = super().fit_transform(raw_documents)
        self._tfidf.fit(X)
        return self        def **transform**(self, raw_documents, copy=True):
        *# Transform documents to document-term matrix.*check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')

        X = super().transform(raw_documents)
        return self._tfidf.transform(X, copy=False)**
```

**`Pipeline`ä¸­çš„æœ€åä¸€ä¸ªä¼°è®¡å™¨ä¸ä¸€å®šæ˜¯è½¬æ¢å™¨(å³æœ‰ä¸€ä¸ª`transform`æ–¹æ³•)ï¼Œä½†å¯ä»¥æ˜¯(é€šå¸¸æ˜¯)ä¸€ä¸ªåˆ†ç±»å™¨ï¼Œåƒ`SVC`ã€‚è¿™ç§ç»“æ„å°†å…è®¸æ‚¨åœ¨æ•´ä¸ªå®‰è£…å¥½çš„ç®¡é“ä¸Šè°ƒç”¨`predict`ã€‚**

## **ç®¡é“ I:ä½¿ç”¨ tfidf çŸ¢é‡å™¨çš„å•è¯åŒ…**

**![](img/6b5e27980c2ebeff4a51aab2de9fc9b2.png)**

**ä»¥æˆ‘ä»¬çš„è¾©è®ºè®°å½•æ–‡æœ¬ä¸ºä¾‹ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„`Pipeline`å¯¹è±¡ï¼Œè¯¥å¯¹è±¡(1)å°†è¾“å…¥æ•°æ®è½¬æ¢ä¸º TF-IDF ç‰¹å¾çŸ©é˜µï¼Œä»¥åŠ(2)ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œåˆ†ç±»:**

```
****bow_pipeline** = Pipeline(
    steps=[
        ("tfidf", TfidfVectorizer()),
        ("classifier", RandomForestClassifier()),
    ]
)bow_pipeline.fit(train_data, train_target)
y_pred = bow_pipeline.predict(test_data)
cr = classification_report(test_target, y_pred)**
```

**ç„¶åæˆ‘ä»¬å¯ä»¥åœ¨æ•´ä¸ªç®¡é“ä¸Šè°ƒç”¨`fit`,åœ¨æµ‹è¯•æ•°æ®ä¸Šè°ƒç”¨`predict`ã€‚æ­£å¦‚æ‚¨æ‰€çœ‹åˆ°çš„ï¼Œé™¤äº†æä¾›æœ‰åºçš„ä¼°è®¡é‡ä¹‹å¤–ï¼Œæ²¡æœ‰ä»€ä¹ˆåˆ«çš„äº†ã€‚å½“ç„¶ï¼Œæ‚¨å¯ä»¥é€šè¿‡åœ¨`â€œclassifierâ€`æ­¥éª¤ä¹‹å‰æ·»åŠ ä¸€ä¸ªè§„èŒƒåŒ–æˆ–æ ‡å‡†åŒ–æ­¥éª¤æ¥æ‰©å±•ç®¡é“ã€‚(æ‚¨å°†åœ¨ä¸‹ä¸€ä¸ªç®¡é“ä¸­çœ‹åˆ°ä¸€ä¸ªé™ç»´æ­¥éª¤çš„ç¤ºä¾‹ã€‚)**

**åœ¨ä¸‹é¢çš„åˆ†ç±»æŠ¥å‘Šä¸­ï¼Œ`0`ä»£è¡¨â€œæ²¡æœ‰èµ„æ ¼å‚åŠ  12 æœˆçš„è¾©è®ºâ€ï¼Œ`1`ä»£è¡¨â€œæœ‰èµ„æ ¼â€ã€‚æˆ‘ä»¬çœ‹åˆ°ï¼Œä»…ä½¿ç”¨è¾©è®ºè®°å½•æ•°æ®â€” **è€Œä¸äº†è§£å€™é€‰äººçš„å…¶ä»–ä¿¡æ¯**(ä¾‹å¦‚ä»–ä»¬çš„æŠ•ç¥¨çŠ¶å†µã€æ€§åˆ«ã€å—æ¬¢è¿ç¨‹åº¦å’Œæ”¿æ²»ç»éªŒæ°´å¹³)â€”æˆ‘ä»¬èƒ½å¤Ÿä»¥ 70%çš„å‡†ç¡®ç‡é¢„æµ‹è°å°†å‚åŠ æˆ–ä¸å‚åŠ ä¸‹ä¸€åœºè¾©è®ºã€‚ä»–ä»¬åœ¨è¾©è®ºèˆå°ä¸Šè¯´çš„è¯çœŸçš„å¾ˆé‡è¦ï¼Œä¸ä»–ä»¬å¦‚ä½•è¯´æ— å…³(ç¬¬ä¸‰æ¡ç®¡é“å°†åœ¨ä¸‹é¢è®¨è®º)ï¼Œä¹Ÿä¸ä»–ä»¬æ˜¯è°æ— å…³ã€‚**

```
**# tf-idf features only precision    recall  f1-score   support

           0       0.66      0.54      0.59       165
           1       0.72      0.81      0.76       242

    accuracy                           **0.70 **      407
   macro avg       0.69      0.67      0.68       407
weighted avg       0.70      0.70      0.69       407**
```

## **ç®¡é“ II:ä½¿ç”¨å®šåˆ¶è½¬æ¢å™¨çš„å•è¯åµŒå…¥**

**![](img/69250de520e798c5821319b5ace4fda6.png)**

**ç¬¬äºŒä¸ªç®¡é“éœ€è¦åˆ›å»ºä¸€ä¸ª**è‡ªå®šä¹‰è½¬æ¢å™¨**ï¼Œå®ƒåŸºäºå•è¯åµŒå…¥å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—å‘é‡ã€‚æœ‰å‡ ç§æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼ŒåŒ…æ‹¬ä»[è¿™é‡Œ](https://nlp.stanford.edu/projects/glove/)ä¸‹è½½é¢„å…ˆè®­ç»ƒå¥½çš„æ‰‹å¥—å•è¯å‘é‡ï¼Œå¹¶ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥åŠ è½½æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤º:**

```
**path_to_word_vectors = "/Users/jlee/glove.6B.50d.txt"def load_glove(path_to_word_vectors):
    f = open(path_to_word_vectors, "r")
    word2vec = {}
    for line in f:
        split_line = line.split()
        word = split_line[0]
        embedding = np.array([float(val) for val in split_line[1:]])
        model[word] = embedding
    return word2vecword2vec = load_glove(path_to_word_vectors)**
```

**æˆ–è€…ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ spaCy çš„å†…ç½®å•è¯å‘é‡æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„`.vector`å±æ€§è®¿é—®ã€‚ä¸‹é¢æˆ‘åŠ è½½äº†`"en_core_web_md"`æ¨¡å‹ï¼Œå®ƒæä¾›äº† 685k ä¸ªé”®å’Œ 20k ä¸ªå”¯ä¸€å‘é‡(300 ç»´)ã€‚è¿™ä¸ªæ¨¡å‹è¢«å­˜å‚¨ä¸º`SpacyVectorTransformer`çš„ä¸€ä¸ªå±æ€§ï¼Œè¿™æ˜¯ä¸€ä¸ªè¿”å›å‘é‡å¹³å‡å€¼çš„å®šåˆ¶è½¬æ¢å™¨ã€‚**

```
**import spacy 
from sklearn.base import BaseEstimator, TransformerMixinnlp = spacy.load("en_core_web_md")  # this model will give you 300Dclass **SpacyVectorTransformer**(BaseEstimator, TransformerMixin):
    def __init__(self, nlp):
        self.nlp = nlp
        self.dim = 300

    def fit(self, X, y):
        return self

    def transform(self, X):
        # Doc.vector defaults to an **average** of the token vectors.
        # https://spacy.io/api/doc#vector
        return [self.nlp(text).vector for text in X]**
```

**ä¸€æ—¦æˆ‘ä»¬å¾—åˆ° 300 ç»´çš„å¹³å‡åµŒå…¥ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©ä½¿ç”¨`TrancatedSVD`æ¥é™ä½è¿™äº›ç‰¹å¾çš„ç»´æ•°ã€‚**

```
****embeddings_pipeline** = Pipeline(
    steps=[
        ("mean_embeddings", SpacyVectorTransformer(nlp)),
        ("reduce_dim", TruncatedSVD(50)),
        ("classifier", RandomForestClassifier()),
    ]
)embeddings_pipeline.fit(train_data, train_target)
y_pred = embeddings_pipeline.predict(test_data)
cr = classification_report(test_target, y_pred)**
```

**ç»“æœæ˜¯:**

```
**# embeddings only               precision    recall  f1-score   support

           0       0.56      0.61      0.58       165
           1       0.72      0.67      0.69       242

    accuracy                           **0.65 **      407
   macro avg       0.64      0.64      0.64       407
weighted avg       0.65      0.65      0.65       407**
```

**æˆ‘ä»¬çœ‹åˆ°ç»“æœä¸å¦‚æ›´ç®€å•çš„ TF-IDF æ¨¡å‹çš„ç»“æœå¥½ã€‚**

## **ç®¡é“ä¸‰:å®šåˆ¶è¯­è¨€åŠŸèƒ½ç®¡é“**

**![](img/3449350b24d2747a08414d7693790d5c.png)**

**æ‰€è°“â€œè‡ªå®šä¹‰è¯­è¨€ç‰¹å¾â€ï¼Œæˆ‘æŒ‡çš„æ˜¯é‚£ç§ä½ å¯ä»¥é€šè¿‡å¯¹ä½ çš„æ•°æ®åº”ç”¨ä¸€äº›è§„åˆ™æ¥æ‰‹åŠ¨æå–çš„ç‰¹å¾ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä½¿ç”¨ç©ºé—´ä¾èµ–è§£æå™¨æå–ç‰¹æ€§`number of words before the main verb`:**

**An example of a custom rule-extracted linguistic feature**

**æ›´å¤šåŸºäºè§„åˆ™çš„å®šåˆ¶è¯­è¨€ç‰¹æ€§çš„ä¾‹å­å¯ä»¥åœ¨ä»¥ä¸‹ä¸¤ç¯‡æ–‡ç« ä¸­æ‰¾åˆ°:**

**[](/linguistic-rule-writing-for-nlp-ml-64d9af824ee8) [## ä¸ºè‡ªç„¶è¯­è¨€å¤„ç†ç¼–å†™è¯­è¨€è§„åˆ™

### ç”¨ spaCy æå–é—®é¢˜ç±»å‹çš„æŒ‡å—

towardsdatascience.com](/linguistic-rule-writing-for-nlp-ml-64d9af824ee8) [](/which-presidential-candidate-talks-like-that-b2b16060ff8b) [## è¶…è¶Šæ¼”è®²æ—¶é—´:æ°‘ä¸»å…šæ€»ç»Ÿè¾©è®ºåˆ†æ

### ä½¿ç”¨çœŸå®ä¸–ç•Œæ•°æ®è¿›è¡Œé¢„æµ‹å»ºæ¨¡çš„æ•°æ®å‡†å¤‡å’Œç‰¹å¾å·¥ç¨‹

towardsdatascience.com](/which-presidential-candidate-talks-like-that-b2b16060ff8b) 

ç¬¬ä¸‰ä¸ªç®¡é“éœ€è¦ä¸€ä¸ªå®šåˆ¶çš„è½¬æ¢å™¨ï¼Œå°±åƒä¸Šä¸€ä¸ªä¸€æ ·ï¼›`CustomLinguisticFeatureTransformer`é‡‡ç”¨ä¸€ä¸ª`fit`æ–¹æ³•(å®ƒè¿”å›è‡ªèº«)å’Œä¸€ä¸ª`transform`æ–¹æ³•ã€‚åè€…è¿”å›`featurize`çš„è¾“å‡ºï¼Œè¿™æ˜¯æˆ‘åˆ›å»ºçš„å¦ä¸€ä¸ªåä¸º`SegmentFeaturizer`çš„ç±»çš„æ–¹æ³•ã€‚

```
segment_featurizer = SegmentFeaturizer()  # more on this belowclass CustomLinguisticFeatureTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        pass def fit(self, x, y=None):
        return self def transform(self, data):
        return **segment_featurizer.featurize**(data)
```

`SegmentFeaturizer`å®šä¹‰ç”¨äºæå–ä¸€ç»„è¯­è¨€ç‰¹å¾çš„æ–¹æ³•ã€‚ä¸‹é¢æ˜¯å®ƒçš„åŸºæœ¬ç»“æ„ï¼Œå…¶ä¸­ä¸€äº›ç‰¹å¾æå–å‡½æ•°åé¢æ˜¯è¿”å›ç‰¹å¾å­—å…¸åˆ—è¡¨çš„ä¸»`featurize`å‡½æ•°:

(è¦äº†è§£æ›´å¤šå…³äº`featurize`çš„ä¿¡æ¯ï¼Œè¯·çœ‹è¿™ç¯‡[å¸–å­](/how-to-build-a-reusable-nlp-code-pipeline-with-scikit-learn-with-an-emphasis-on-feature-504f8aa14699?source=friends_link&sk=135abfa127e7094b1a17963b254ab678)ã€‚)ç„¶åï¼Œ`transform`çš„è¾“å‡ºä½œä¸ºæµæ°´çº¿çš„ä¸‹ä¸€ä¸ªéƒ¨ä»¶ï¼Œå³`DictVectorizer`çš„è¾“å…¥ã€‚å®Œæ•´çš„ç®¡é“å¦‚ä¸‹:

```
manual_pipeline = Pipeline(
    steps=[
        ("stats", CustomLinguisticFeatureTransformer()),
        ("dict_vect", DictVectorizer()),
        ("classifier", RandomForestClassifier()),
    ]
)manual_pipeline.fit(train_data, train_target)
y_pred = manual_pipeline.predict(test_data)
cr = classification_report(test_target, y_pred)
```

ç»“æœ:

```
# manual linguistic features only precision    recall  f1-score   support

           0       0.62      0.56      0.59       165
           1       0.72      0.77      0.74       242

    accuracy                           **0.68**       407
   macro avg       0.67      0.66      0.67       407
weighted avg       0.68      0.68      0.68       407
```

ä¸å¦‚è¯è¢‹æ¨¡å‹å¥½ï¼Œä½†æ¯”å¹³å‡åµŒå…¥è¦å¥½ã€‚ä¸‹é¢æ˜¯ä¸‰ä¸ªç®¡é“æŒ‡æ ‡çš„å¯¹æ¯”æ±‡æ€»è¡¨:

![](img/f4a937a5cbfb6acb40733e8fd8e29909.png)

(æ˜¾ç¤ºäº†[å®](https://scikit-learn.org/stable/modules/model_evaluation.html#from-binary-to-multiclass-and-multilabel)çš„ç²¾ç¡®åº¦ã€å¬å›ç‡å’Œ f åˆ†æ•°çš„å¹³å‡å€¼ã€‚)

# åœ¨ä¸€èµ·æ›´å¥½ã€‚

å·²ç»çœ‹åˆ°å•è¯è¢‹ç®¡é“å’Œè‡ªå®šä¹‰è¯­è¨€ç‰¹å¾ç®¡é“ç‹¬ç«‹åœ°äº§ç”Ÿæœ€ä½³ç»“æœï¼Œæˆ‘æƒ³çŸ¥é“å¦‚æœæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç»“åˆä¸¤è€…ç‰¹å¾çš„æ–°ç®¡é“ä¼šæœ‰ä»€ä¹ˆæ•ˆæœã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªç®€å•çš„æ–¹æ³•å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ï¼

## ç®¡é“å››:ç»„åˆåŠŸèƒ½

![](img/2dd27f55ded849d6cdb8e56fbe4b36da.png)

ç»„åˆç‰¹å¾é›†ç”±`**FeatureUnion**`å®Œæˆ:

> [FeatureUnion](https://scikit-learn.org/0.16/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) å°†å‡ ä¸ª transformer å¯¹è±¡ç»„åˆæˆä¸€ä¸ªæ–°çš„ transformerï¼Œè¯¥ transformer å°†å®ƒä»¬çš„è¾“å‡ºè¿›è¡Œç»„åˆã€‚FeatureUnion æ¥å—ä¸€ä¸ª transformer å¯¹è±¡åˆ—è¡¨ã€‚åœ¨æ‹ŸåˆæœŸé—´ï¼Œ**æ¯ä¸€ä¸ªéƒ½ç‹¬ç«‹åœ°æ‹Ÿåˆåˆ°æ•°æ®**ã€‚ä¸ºäº†è½¬æ¢æ•°æ®ï¼Œè½¬æ¢å™¨è¢«å¹¶è¡Œåº”ç”¨**ï¼Œå¹¶ä¸”å®ƒä»¬è¾“å‡ºçš„æ ·æœ¬å‘é‡è¢«**é¦–å°¾ç›¸è¿**æˆæ›´å¤§çš„å‘é‡ã€‚**

**ä¸‹é¢ï¼Œ`"classifier"`æ­¥éª¤å·²ç»ä»æ¯ä¸ªç®¡é“ä¸­åˆ é™¤ï¼Œå› ä¸ºè¯¥æ­¥éª¤éœ€è¦åœ¨æˆ‘ä»¬åº”ç”¨äº†`FeatureUnion`ä¹‹åå‡ºç°ã€‚**

```
# individual pipelines minus the estimator step: **bow_pipeline** = Pipeline(
    steps=[
        ("tfidf", TfidfVectorizer()),
    ]
)**manual_pipeline** = Pipeline(
    steps=[
        ("stats", ManualTransformer()),
        ("dict_vect", DictVectorizer()),
    ]
)
```

**æœ€ç»ˆç®¡é“ç”±é€šè¿‡`FeatureUnion`å’Œæœ€ç»ˆåˆ†ç±»å™¨è¿æ¥çš„ç»„åˆç‰¹å¾ç»„æˆ:**

```
combined_features = FeatureUnion(
    transformer_list=[
        ("bow", bow_pipeline),
        ("manual", manual_pipeline),
    ]
)**final_pipeline** = Pipeline(
    steps=[
        ("combined_features", combined_features),
        ("classifier", RandomForestClassifier()),
    ]
)
```

**æ­¤æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥æ‹Ÿåˆ`final_pipeine`å¹¶è®¡ç®—æŒ‡æ ‡ï¼Œä½†æ˜¯ä¸ºäº†æ›´å¥½åœ°æµ‹é‡ï¼Œæˆ‘å°†å¯¹æ‹Ÿåˆçš„éšæœºæ£®æ—åˆ†ç±»å™¨çš„å‚æ•°æ‰§è¡Œéšæœºæœç´¢ã€‚(æ‚¨ä¹Ÿå¯ä»¥è¿è¡Œç½‘æ ¼æœç´¢ï¼Œä½†è¿™éœ€è¦æ›´é•¿æ—¶é—´ã€‚)**

```
from sklearn.model_selection import RandomizedSearchCV# the keys can be accessed with **final_pipeline.get_params().keys()**
params = {
    "combined_features__bow__tfidf__use_idf": [True, False],
    "combined_features__bow__tfidf__ngram_range": [(1, 1), (1, 2)],
    "classifier__bootstrap": [True, False],
    "classifier__class_weight": ["balanced", None],
    "classifier__n_estimators": [100, 300, 500, 800, 1200],
    "classifier__max_depth": [5, 8, 15, 25, 30],
    "classifier__min_samples_split": [2, 5, 10, 15, 100],
    "classifier__min_samples_leaf": [1, 2, 5, 10]
}search = RandomizedSearchCV(final_pipeline, params)
search.fit(train_data, train_target)
y_pred = search.predict(test_data)
classification_report(test_target, y_pred)
```

**é…·ï¼ç»“æœ*æ¯”*æ›´å¥½ï¼Œå…·æœ‰ç‰¹å¾ç»Ÿä¸€å’Œç½‘æ ¼æœç´¢æ”¯æŒçš„å‚æ•°ä¼˜åŒ–ï¼**

```
# combined features + randomized search               precision    recall  f1-score   support

           0       0.70      0.55      0.61       165
           1       0.73      0.84      0.78       242

    accuracy                           **0.72**       407
   macro avg       0.72      0.69      0.70       407
weighted avg       0.72      0.72      0.71       407
```

**åœ¨æ¯é¡¹æŒ‡æ ‡ä¸Šï¼Œæ··åˆæ¸ é“çš„å¾—åˆ†éƒ½é«˜äºè¡¨ç°æœ€ä½³çš„å•ä¸ªæ¸ é“:**

**![](img/025beb1ef3dc97c7d6778108b09dafc3.png)**

**ä½œä¸ºæ­£ç¡®çš„ä¸‹ä¸€æ­¥ï¼Œæ‚¨å¯ä»¥å°è¯•ç»„åˆæ‰€æœ‰ä¸‰ä¸ªç‰¹æ€§é›†ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œä»è€Œåˆ›å»ºä¸€ä¸ªçœŸæ­£çš„â€œtriuneâ€(ä¸‰åˆä¸€)ç®¡é“ã€‚ğŸ”±**

# **ç»“æŸäº†ã€‚**

**åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘æ¼”ç¤ºäº†å¦‚ä½•å»ºç«‹ä¸‰ä½ä¸€ä½“ç®¡é“æ¥é¢„æµ‹æ€»ç»Ÿå€™é€‰äººæœ‰èµ„æ ¼å‚åŠ ä¸‹ä¸€åœºè¾©è®ºçš„å¯èƒ½æ€§ã€‚ä»–ä»¬æ˜¯:**

1.  **ä½¿ç”¨æ ‡å‡†**å­—è¢‹**æ¨¡å‹çš„ç®¡é“**
2.  **ä½¿ç”¨**å­—åµŒå…¥çš„ç®¡é“****
3.  **ä¸€æ¡ç®¡é“ä½¿ç”¨**æ‰‹åŠ¨(è‡ªå®šä¹‰)åŠŸèƒ½****

**æ¯ä¸ªç»„ä»¶ç®¡é“åŒ…æ‹¬ä¸€ä¸ªè½¬æ¢å™¨ï¼Œå®ƒè¾“å‡º NLP ä¸­çš„ä¸»è¦ç‰¹å¾ç±»å‹/è¡¨ç¤ºã€‚æˆ‘è¿˜å±•ç¤ºäº†æˆ‘ä»¬å¯ä»¥ç»„åˆæ¥è‡ªä¸åŒç®¡é“çš„ä¸åŒç‰¹æ€§é›†æ¥è·å¾—æ›´å¥½çš„ç»“æœï¼Œå°¤å…¶æ˜¯ä¸`RandomizedSearchCV`ç»“åˆä½¿ç”¨æ—¶ã€‚æˆ‘è®¤ä¸ºå¦ä¸€ä¸ªå¾ˆå¥½çš„æ”¶è·æ˜¯ç»“åˆ ML é©±åŠ¨å’ŒåŸºäºè§„åˆ™çš„æ–¹æ³•æ¥æå‡æ¨¡å‹æ€§èƒ½çš„ä»·å€¼ã€‚**

**æˆ‘ç¡®å®å‘ç°ï¼Œå€™é€‰äººçš„æ¼”è®²ç‰¹æ€§*æœ¬èº«â€”â€”ä¸å—ä»»ä½•å…¶ä»–äººå£ç»Ÿè®¡ç‰¹å¾æˆ–å€™é€‰äººæ”¿æ²»æˆå°±ä¿¡æ¯çš„å½±å“*â€”â€”*èƒ½å¤Ÿç›¸å½“å‡†ç¡®åœ°é¢„æµ‹æŸäººæ˜¯å¦æœ‰èµ„æ ¼å‚åŠ ä¸‹ä¸€åœºè¾©è®ºï¼Œè¿™æ˜¯é¢„æµ‹æˆ‘ä»¬ä¸‹ä¸€ä»»æ€»ç»Ÿçš„ä¼—å¤šé¢„æµ‹å› ç´ ä¹‹ä¸€ã€‚***

***å¦‚æœä½ æƒ³è¦ä¸é‚£ä¹ˆç½—å—¦çš„ä¸œè¥¿:***

***[](https://medium.com/swlh/randomized-or-grid-search-with-pipeline-cheatsheet-719c72eda68) [## ç®¡é“éšæœº(æˆ–ç½‘æ ¼)æœç´¢è¶…çº§å¿«é€ŸæŒ‡å—

### ä½¿ç”¨ scikit çš„äº”ä¸ªæ­¥éª¤-å­¦ä¹ 

medium.com](https://medium.com/swlh/randomized-or-grid-search-with-pipeline-cheatsheet-719c72eda68)*****