<html>
<head>
<title>A gentle introduction to Apache Arrow with Apache Spark and Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿帕奇箭与阿帕奇火花和熊猫的温柔介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-apache-arrow-with-apache-spark-and-pandas-bb19ffe0ddae?source=collection_archive---------1-----------------------#2019-01-29">https://towardsdatascience.com/a-gentle-introduction-to-apache-arrow-with-apache-spark-and-pandas-bb19ffe0ddae?source=collection_archive---------1-----------------------#2019-01-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="fdd9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这一次我将尝试解释如何将 Apache Arrow 与 Apache Spark 和 Python 结合使用。首先，让我分享一些关于这个开源项目的基本概念。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/ce904110cad1b6b435ce16052cd7c794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9nTSlFd1Ox6XfYcz"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">[<a class="ae ko" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank">Apache Arrow</a>]</figcaption></figure><blockquote class="lf lg lh"><p id="5151" class="jq jr li js b jt ju jv jw jx jy jz ka lj kc kd ke lk kg kh ki ll kk kl km kn im bi translated">Apache Arrow 是内存数据的跨语言开发平台。它为平面和层次数据指定了一种标准化的独立于语言的列内存格式，为现代硬件上的高效分析操作而组织。[ <a class="ae ko" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank">阿帕奇箭头页</a></p></blockquote><p id="6ef0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之，它促进了许多组件之间的通信，例如，用 Python (pandas)读取 parquet 文件并转换为 Spark 数据帧、<a class="ae ko" href="https://github.com/uwdata/falcon" rel="noopener ugc nofollow" target="_blank"> Falcon </a>数据可视化或<a class="ae ko" href="http://cassandra.apache.org/" rel="noopener ugc nofollow" target="_blank"> Cassandra </a>而不用担心转换。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lm"><img src="../Images/458508e8ba97cd2db3112bb66a514605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-5NKsHXBZ5glJmi_ifRpg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Overview Apache Arrow [<a class="ae ko" href="https://databricks.com/speaker/julien-le-dem" rel="noopener ugc nofollow" target="_blank">Julien Le Dem</a>, Spark Summit 2017]</figcaption></figure><p id="d0e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一个好问题是问数据在内存中是什么样子的？Apache Arrow 利用列缓冲区来减少 IO 并提高分析处理性能。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ln"><img src="../Images/2a097f6bf918c4fbb497ee3fd7014f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b3jEIVWk_m0_J9jH.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Columnar In-memory [<a class="ae ko" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank">Apache Arrow page</a>]</figcaption></figure><p id="07a2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我们的例子中，我们将使用<a class="ae ko" href="https://arrow.apache.org/docs/python/" rel="noopener ugc nofollow" target="_blank"> pyarrow </a>库来执行一些基本代码并检查一些特性。为了安装，我们有两个选项使用 conda 或 pip 命令*。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="3584" class="lt lu it lp b gy lv lw l lx ly">conda install -c conda-forge pyarrow</span><span id="259a" class="lt lu it lp b gy lz lw l lx ly">pip install pyarrow</span></pre><p id="3246" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">*建议在 Python 3 环境中使用 conda。</p></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="b74a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Apache Arrow with Pandas(本地文件系统)</strong></p><p id="586c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将 Pandas 数据帧转换为 Apache 箭头表</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="b968" class="lt lu it lp b gy lv lw l lx ly">import numpy as np<br/>import pandas as pd<br/>import pyarrow as pa<br/>df = pd.DataFrame({'one': [20, np.nan, 2.5],'two': ['january', 'february', 'march'],'three': [True, False, True]},index=list('abc'))<br/>table = pa.Table.from_pandas(df)</span></pre><p id="30f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> Pyarrow 表到熊猫数据框</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="d27d" class="lt lu it lp b gy lv lw l lx ly">df_new = table.to_pandas()</span></pre><p id="9dd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">读取 CSV </strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="48b4" class="lt lu it lp b gy lv lw l lx ly">from pyarrow import csv<br/>fn = ‘data/demo.csv’<br/>table = csv.read_csv(fn)<br/>df = table.to_pandas()</span></pre><p id="9716" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">从 Apache 编写拼花文件箭头</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="b7e6" class="lt lu it lp b gy lv lw l lx ly">import pyarrow.parquet as pq<br/>pq.write_table(table, 'example.parquet')</span></pre><p id="2827" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">阅读拼花文件</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="915c" class="lt lu it lp b gy lv lw l lx ly">table2 = pq.read_table(‘example.parquet’)<br/>table2</span></pre><p id="6a1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">从拼花文件中读取一些列</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="c5ae" class="lt lu it lp b gy lv lw l lx ly">table2 = pq.read_table('example.parquet', columns=['one', 'three'])</span></pre><p id="a368" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">从分区数据集读取</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="a564" class="lt lu it lp b gy lv lw l lx ly">dataset = pq.ParquetDataset(‘dataset_name_directory/’)<br/>table = dataset.read()<br/>table</span></pre><p id="22a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将拼花文件转换成熊猫数据帧</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="ae92" class="lt lu it lp b gy lv lw l lx ly">pdf = pq.read_pandas('example.parquet', columns=['two']).to_pandas()<br/>pdf</span></pre><p id="c0c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">避开熊猫指数</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="25b2" class="lt lu it lp b gy lv lw l lx ly">table = pa.Table.from_pandas(df, preserve_index=False)<br/>pq.write_table(table, 'example_noindex.parquet')<br/>t = pq.read_table('example_noindex.parquet')<br/>t.to_pandas()</span></pre><p id="236e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">检查元数据</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="7185" class="lt lu it lp b gy lv lw l lx ly">parquet_file = pq.ParquetFile(‘example.parquet’)<br/>parquet_file.metadata</span></pre><p id="ecd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">参见数据模式</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="639f" class="lt lu it lp b gy lv lw l lx ly">parquet_file.schema</span></pre><p id="9b69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">时间戳</strong></p><p id="0857" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">记住熊猫使用纳秒，所以为了兼容你可以用毫秒截断。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="92cb" class="lt lu it lp b gy lv lw l lx ly">pq.write_table(table, where, coerce_timestamps='ms')<br/>pq.write_table(table, where, coerce_timestamps='ms', allow_truncated_timestamps=True)</span></pre><p id="77bf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">压缩</strong></p><p id="a98d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">默认情况下，Apache arrow 使用 snappy 压缩(不那么压缩，但更容易访问)，尽管也允许使用其他编解码器。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="5e38" class="lt lu it lp b gy lv lw l lx ly">pq.write_table(table, where, compression='snappy')<br/>pq.write_table(table, where, compression='gzip')<br/>pq.write_table(table, where, compression='brotli')<br/>pq.write_table(table, where, compression='none')</span></pre><p id="dfa2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，可以在一个表中使用多种压缩</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="a415" class="lt lu it lp b gy lv lw l lx ly">pq.write_table(table, ‘example_diffcompr.parquet’, compression={b’one’: ‘snappy’, b’two’: ‘gzip’})</span></pre><p id="5484" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">写一个分区拼花表</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="a874" class="lt lu it lp b gy lv lw l lx ly">df = pd.DataFrame({‘one’: [1, 2.5, 3],<br/>                   ‘two’: [‘Peru’, ‘Brasil’, ‘Canada’],<br/>                   ‘three’: [True, False, True]},<br/>                   index=list(‘abc’))<br/>table = pa.Table.from_pandas(df)<br/>pq.write_to_dataset(table, root_path=’dataset_name’,partition_cols=[‘one’, ‘two’])</span></pre><ul class=""><li id="3df9" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">兼容性说明:如果您使用 pq.write_to_dataset 创建一个将由 HIVE 使用的表，则分区列值必须与您正在运行的 HIVE 版本的允许字符集兼容。</li></ul></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="9b3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">带 HDFS(远程文件系统)的 Apache Arrow】</strong></p><p id="6f54" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Apache Arrow 附带了与 Hadoop 文件系统的基于 c++(T3)的接口的<a class="ae ko" href="https://arrow.apache.org/docs/python/filesystems.html#hadoop-file-system-hdfs" rel="noopener ugc nofollow" target="_blank">绑定。这意味着我们可以从 HDFS 读取或下载所有文件，并直接用 Python 解释。</a></p><p id="a300" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">连接</strong></p><p id="b18d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">主机是 namenode，端口通常是 RPC 或 WEBHDFS 更多的<a class="ae ko" href="https://arrow.apache.org/docs/python/generated/pyarrow.hdfs.connect.html#pyarrow.hdfs.connect" rel="noopener ugc nofollow" target="_blank">参数</a>像用户，kerberos 票证是允许的。强烈建议阅读所需的<a class="ae ko" href="https://arrow.apache.org/docs/python/filesystems.html#hadoop-file-system-hdfs" rel="noopener ugc nofollow" target="_blank">环境变量</a>。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="23f8" class="lt lu it lp b gy lv lw l lx ly">import pyarrow as pa<br/>host = '1970.x.x.x'<br/>port = 8022<br/>fs = pa.hdfs.connect(host, port)</span></pre><ul class=""><li id="5857" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">可选，如果您的连接是在数据或边缘节点可能使用的前端</li></ul><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="50cb" class="lt lu it lp b gy lv lw l lx ly">fs = pa.hdfs.connect()</span></pre><p id="958a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">将拼花文件写入 HDFS </strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="f997" class="lt lu it lp b gy lv lw l lx ly">pq.write_to_dataset(table, root_path=’dataset_name’, partition_cols=[‘one’, ‘two’], filesystem=fs)</span></pre><p id="7835" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">从 HDFS 读取 CSV</strong></p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="f492" class="lt lu it lp b gy lv lw l lx ly">import pandas as pd<br/>from pyarrow import csv<br/>import pyarrow as pa<br/>fs = pa.hdfs.connect()<br/>with fs.open(‘iris.csv’, ‘rb’) as f:<br/> df = pd.read_csv(f, nrows = 10)<br/>df.head()</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/d8a25b3e724e281af4a88badb0ad2c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*xuLOsfL1GkIrJGjawuHvUw.png"/></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Reading CSV from HDFS</figcaption></figure><p id="bc2d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">从 HDFS 读取拼花文件</strong></p><p id="da35" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从 HDFS 读取拼花文件有两种形式</p><p id="65b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用熊猫和 Pyarrow 引擎</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="d77f" class="lt lu it lp b gy lv lw l lx ly">import pandas as pd<br/>pdIris = pd.read_parquet(‘hdfs:///iris/part-00000–27c8e2d3-fcc9–47ff-8fd1–6ef0b079f30e-c000.snappy.parquet’, engine=’pyarrow’)<br/>pdTrain.head()</span></pre><p id="d1ee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Pyarrow .拼花地板</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="7a37" class="lt lu it lp b gy lv lw l lx ly">import pyarrow.parquet as pq<br/>path = ‘hdfs:///iris/part-00000–71c8h2d3-fcc9–47ff-8fd1–6ef0b079f30e-c000.snappy.parquet’<br/>table = pq.read_table(path)<br/>table.schema<br/>df = table.to_pandas()<br/>df.head()</span></pre><p id="783a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">其他文件扩展名</strong></p><p id="c40f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因为我们可以存储任何类型的文件(SAS、STATA、Excel、JSON 或 objects)，所以大多数文件都很容易被 Python 解释。为了实现这一点，我们将使用 open 函数返回一个缓冲区对象，许多 pandas 函数如<a class="ae ko" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sas.html#pandas.read_sas" rel="noopener ugc nofollow" target="_blank"> read_sas </a>、<a class="ae ko" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_json.html" rel="noopener ugc nofollow" target="_blank"> read_json </a>可以接收这个对象作为输入，而不是一个字符串 URL。</p><p id="b7d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">斯堪的纳维亚航空公司</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="c2e1" class="lt lu it lp b gy lv lw l lx ly">import pandas as pd<br/>import pyarrow as pa<br/>fs = pa.hdfs.connect()<br/>with fs.open(‘/datalake/airplane.sas7bdat’, ‘rb’) as f:<br/> sas_df = pd.read_sas(f, format='sas7bdat')<br/>sas_df.head()</span></pre><p id="7366" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">擅长</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="0f65" class="lt lu it lp b gy lv lw l lx ly">import pandas as pd<br/>import pyarrow as pa<br/>fs = pa.hdfs.connect()<br/>with fs.open(‘/datalake/airplane.xlsx’, ‘rb’) as f:<br/> g.download('airplane.xlsx')<br/>ex_df = pd.read_excel('airplane.xlsx')</span></pre><p id="b174" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">JSON</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="304e" class="lt lu it lp b gy lv lw l lx ly">import pandas as pd<br/>import pyarrow as pa<br/>fs = pa.hdfs.connect()<br/>with fs.open(‘/datalake/airplane.json’, ‘rb’) as f:<br/> g.download('airplane.json')<br/>js_df = pd.read_json('airplane.json')</span></pre><p id="b83b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">从 HDFS 下载文件</strong></p><p id="d3bd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们只是需要下载文件，Pyarrow 为我们提供了下载功能，将文件保存在本地。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="24c8" class="lt lu it lp b gy lv lw l lx ly">import pandas as pd<br/>import pyarrow as pa<br/>fs = pa.hdfs.connect()<br/>with fs.open(‘/datalake/airplane.cs’, ‘rb’) as f:<br/> g.download('airplane.cs')</span></pre><p id="4203" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">上传文件到 HDFS </strong></p><p id="882a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们只是需要下载文件，Pyarrow 为我们提供了下载功能，将文件保存在本地。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="d8d2" class="lt lu it lp b gy lv lw l lx ly">import pyarrow as pa<br/>fs = pa.hdfs.connect()<br/>with open(‘settings.xml’) as f:<br/> pa.hdfs.HadoopFileSystem.upload(fs, ‘/datalake/settings.xml’, f)</span></pre></div><div class="ab cl ma mb hx mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="im in io ip iq"><p id="aa0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">阿帕奇箭带阿帕奇火花</strong></p><p id="435d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Apache Arrow 从版本 2.3 开始与 Spark 集成，存在关于优化时间、避免序列化和反序列化过程以及与其他库集成的良好演示，如来自<a class="ae ko" href="https://databricks.com/speaker/holden-karau" rel="noopener ugc nofollow" target="_blank"> Holden Karau </a>的关于在 Spark 上加速 Tensorflow Apache Arrow 的演示。</p><p id="a53c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">还有其他有用的文章，比如由<a class="ae ko" href="https://arrow.apache.org/blog/2017/07/26/spark-arrow/" rel="noopener ugc nofollow" target="_blank"> Brian Cutler </a>发表的文章，以及 Spark 的<a class="ae ko" href="https://spark.apache.org/docs/2.4.0/sql-pyspark-pandas-with-arrow.html#apache-arrow-in-spark" rel="noopener ugc nofollow" target="_blank">官方文档</a>中非常好的例子</p><p id="750b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Apache Arrow 的一些有趣用法是:</p><ul class=""><li id="da3f" class="mh mi it js b jt ju jx jy kb mj kf mk kj ml kn mm mn mo mp bi translated">加速从 Pandas 数据帧到 Spark 数据帧的上转换</li><li id="b3a3" class="mh mi it js b jt mr jx ms kb mt kf mu kj mv kn mm mn mo mp bi translated">加速从 Spark 数据帧到 Pandas 数据帧的上转换</li><li id="2243" class="mh mi it js b jt mr jx ms kb mt kf mu kj mv kn mm mn mo mp bi translated"><strong class="js iu">使用熊猫 UDF(又名矢量化 UDF)</strong></li><li id="1e31" class="mh mi it js b jt mr jx ms kb mt kf mu kj mv kn mm mn mo mp bi translated"><a class="ae ko" href="https://arrow.apache.org/blog/2019/01/25/r-spark-improvements/" rel="noopener ugc nofollow" target="_blank">使用 Apache Spark 优化 R</a></li></ul><p id="054e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第三项将是下一篇文章的一部分，因为这是一个非常有趣的话题，以便在不损失性能的情况下扩展 Pandas 和 Spark 之间的集成，对于第四项，我建议您阅读<a class="ae ko" href="https://arrow.apache.org/blog/2019/01/25/r-spark-improvements/" rel="noopener ugc nofollow" target="_blank">文章</a>(发表于 2019 年！)去了解更多。</p><p id="90e0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们先测试熊猫和 Spark 之间的转换，不做任何修改，然后允许 Arrow。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="f733" class="lt lu it lp b gy lv lw l lx ly">from pyspark.sql import SparkSession<br/>warehouseLocation = “/antonio”<br/>spark = SparkSession\<br/>.builder.appName(“demoMedium”)\<br/>.config(“spark.sql.warehouse.dir”, warehouseLocation)\<br/>.enableHiveSupport()\<br/>.getOrCreate()</span><span id="1928" class="lt lu it lp b gy lz lw l lx ly">#Create test Spark DataFrame<br/>from pyspark.sql.functions import rand<br/>df = spark.range(1 &lt;&lt; 22).toDF(“id”).withColumn(“x”, rand())<br/>df.printSchema()</span><span id="233b" class="lt lu it lp b gy lz lw l lx ly">#Benchmark time<br/>%time pdf = df.toPandas()<br/>spark.conf.set(“spark.sql.execution.arrow.enabled”, “true”)<br/>%time pdf = df.toPandas()<br/>pdf.describe()</span></pre><p id="851f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在结果中使用箭头来减少时间转换显然更方便。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mw"><img src="../Images/d058e1c482334bf70592f6b5352073c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BiXMhCYQbWxjquKj2154Bg.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk">Optimizing transformation from Spark Data Frame to Pandas</figcaption></figure><p id="b98a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果我们需要测试相反的情况(熊猫引发 df ),我们也能及时看到优化。</p><pre class="kq kr ks kt gt lo lp lq lr aw ls bi"><span id="7538" class="lt lu it lp b gy lv lw l lx ly">%time df = spark.createDataFrame(pdf)<br/>spark.conf.set("spark.sql.execution.arrow.enabled", "false")<br/>%time df = spark.createDataFrame(pdf)<br/>df.describe().show()</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi mx"><img src="../Images/aa395562d56f8ca5021396781e2323d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LKxCegYXRzWyB0vbE0SX4g.png"/></div></div></figure><p id="5109" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">总之</strong></p><p id="afee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的目的是发现和理解 Apache Arrow，以及它如何与 Apache Spark 和 Pandas 一起工作，我还建议你查看它的官方页面，以了解更多关于其他可能的集成，如<a class="ae ko" href="https://arrow.apache.org/docs/python/cuda.html" rel="noopener ugc nofollow" target="_blank"> CUDA </a>或 C++，如果你想更深入地了解 Apache Spark，我认为<a class="ae ko" href="https://amzn.to/2NQxTmZ" rel="noopener ugc nofollow" target="_blank"> Spark:权威指南</a>是一本很好的书。</p><p id="38e1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PS 如果你有任何问题，或者想要澄清一些事情，你可以在<a class="ae ko" href="https://twitter.com/thony_ac77" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae ko" href="https://www.linkedin.com/in/antoniocachuan/" rel="noopener ugc nofollow" target="_blank"> LinkedIn 找到我。</a>我最近发表了<a class="ae ko" rel="noopener" target="_blank" href="/a-gentle-introduction-to-apache-druid-in-google-cloud-platform-c1e087c87bf1"> <strong class="js iu">一篇关于 Apache Druid </strong> </a>的温和介绍，这是一个新的 Apache 项目，非常适合分析数十亿行。</p><div class="my mz gp gr na nb"><a rel="noopener follow" target="_blank" href="/a-gentle-introduction-to-apache-druid-in-google-cloud-platform-c1e087c87bf1"><div class="nc ab fo"><div class="nd ab ne cl cj nf"><h2 class="bd iu gy z fp ng fr fs nh fu fw is bi translated">Google 云平台中 Apache Druid 的温和介绍</h2><div class="ni l"><h3 class="bd b gy z fp ng fr fs nh fu fw dk translated">使得分析数十亿行变得容易</h3></div><div class="nj l"><p class="bd b dl z fp ng fr fs nh fu fw dk translated">towardsdatascience.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np kz nb"/></div></div></a></div></div></div>    
</body>
</html>