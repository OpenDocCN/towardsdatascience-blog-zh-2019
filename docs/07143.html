<html>
<head>
<title>Real-time Sound event classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时声音事件分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/real-time-sound-event-classification-83e892cf187e?source=collection_archive---------8-----------------------#2019-10-09">https://towardsdatascience.com/real-time-sound-event-classification-83e892cf187e?source=collection_archive---------8-----------------------#2019-10-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/a5619ca43454ceb7d4dc3d0e4ae82ac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LMeN7c8posQaB9PuPh5kCA.png"/></div></div></figure><p id="9158" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我在之前的一篇文章中介绍了一种使用机器学习对声音事件进行分类的方法。在之前的帖子中，声音事件是作为小音频片段单独捕获的。因此，不需要分割过程。这些独立的音频片段被用来训练神经网络并对其进行测试。在这篇文章中，我将介绍一种对声音事件进行分类的方法，这些声音事件在一个音频片段(或一个音频流)中按顺序连接在一起。我们必须对事件进行分类，并给它们一个标签，以及相关音频片段或音频流的时间戳。让我们称之为时间标签。注意，这里将重复使用用于分类孤立声音事件的相同程序。所以我强烈推荐你先看完<a class="ae kz" href="https://medium.com/@chathuranga.15/sound-event-classification-using-machine-learning-8768092beafc" rel="noopener">上一篇</a>。</p><p id="3da7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们来看看一个样本音频片段的波形，我们将对其进行时间标记。</p><figure class="lb lc ld le gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi la"><img src="../Images/2989db0262a8459202932749278e8f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQPdqoiEVM3VgjxLLqeIlw.png"/></div></div><figcaption class="lf lg gj gh gi lh li bd b be z dk">A sequence of sound events</figcaption></figure><p id="cf3e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">时间标记的建议方法如下。</p><p id="87a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">1.使用普通噪声样本降低噪声</p><p id="9075" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2.将音频剪辑拆分成包含音频剪辑的单个声音事件</p><p id="2e24" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">3.修剪单个声音事件音频片段的开头和结尾静音。</p><p id="3f18" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">4.使用先前训练的神经网络对单个声音事件片段进行分类</p><p id="9b14" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">5.输出时间标签。</p><p id="389c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该过程适用于没有两个声音事件同时发生的情况。这是因为我们在这里使用的预测模型只针对孤立的声音事件进行训练。让我们假设噪声在整个声音事件系列中保持不变。让我们来看看用单个声音事件连接片段执行上述步骤。然后，在文章的最后，我将介绍一种使用麦克风的音频流实时分类声音事件的方法。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><p id="ca72" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">您可以通过连接一些单个音频片段来准备样本，如下所示。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="85cd" class="lv lw it lr b gy lx ly l lz ma">raw_audio = numpy.concatenate((raw_audio,data))</span></pre><p id="495d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">按如下方式降低均匀分布的噪声。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="3864" class="lv lw it lr b gy lx ly l lz ma">noisy_part = raw_audio[0:50000]  # Empherically selected noisy_part position for every sample<br/>nr_audio = nr.reduce_noise(audio_clip=raw_audio, noise_clip=noisy_part, verbose=False)</span></pre></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h1 id="1b4b" class="mb lw it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">分割音频剪辑</h1><p id="6964" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">现在，我们已经到了连续声音事件识别的核心思想。对声音事件序列进行分类的挑战在于确定这些声音事件的起点和终点。几乎总是在两个声音事件之间有一个无声部分。请注意，在一些声音事件中，可能会有静默。我们可以用这些无声部分来分割一系列声音事件。看看下面用来完成任务的代码。注意到参数<code class="fe nd ne nf lr b">tolerence</code>用于调整分割灵敏度。从而一个声音事件内的小无声部分不会被用来进一步分割相应的声音事件。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="e232" class="lv lw it lr b gy lx ly l lz ma"># Split a given long audio file on silent parts.<br/># Accepts audio numpy array audio_data, window length w and hop length h, threshold_level, tolerence<br/># threshold_level: Silence threshold<br/># Higher tolence to prevent small silence parts from splitting the audio.<br/># Returns array containing arrays of [start, end] points of resulting audio clips<br/>def split_audio(audio_data, w, h, threshold_level, tolerence=10):<br/>    split_map = []<br/>    start = 0<br/>    data = np.abs(audio_data)<br/>    threshold = threshold_level*np.mean(data[:25000])<br/>    inside_sound = False<br/>    near = 0<br/>    for i in range(0,len(data)-w, h):<br/>        win_mean = np.mean(data[i:i+w])<br/>        if(win_mean&gt;threshold and not(inside_sound)):<br/>            inside_sound = True<br/>            start = i<br/>        if(win_mean&lt;=threshold and inside_sound and near&gt;tolerence):<br/>            inside_sound = False<br/>            near = 0<br/>            split_map.append([start, i])<br/>        if(inside_sound and win_mean&lt;=threshold):<br/>            near += 1<br/>    return split_map</span></pre><p id="e0af" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">该算法使用具有跳跃长度<code class="fe nd ne nf lr b">h</code>的固定大小的窗口<code class="fe nd ne nf lr b">w</code>。窗口在给定音频上滑动，检查窗口的平均振幅。如果振幅低于给定的<code class="fe nd ne nf lr b">threshold_level</code>，算法会增加一个名为<code class="fe nd ne nf lr b">near</code>的内部参数。当参数<code class="fe nd ne nf lr b">near</code>获得大于参数<code class="fe nd ne nf lr b">tolerence</code>的值时，确定音频剪辑结束分割点。同样，音频剪辑起始分割点也是使用窗口平均振幅来确定的。注意，内部布尔参数<code class="fe nd ne nf lr b">inside_sound</code>被维护以区分开始和结束分割点。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h1 id="d4e9" class="mb lw it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">修剪单个声音事件音频片段</h1><p id="89cb" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">现在，我们已经将我们的音频剪辑分成单个声音事件。小音频片段需要修剪它们的前导和拖尾无声部分。让我们使用<code class="fe nd ne nf lr b">librosa</code>来完成任务。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="05d1" class="lv lw it lr b gy lx ly l lz ma">sound_clips = split_audio(nr_audio, 10000, 2500, 15, 10)<br/>for intvl in sound_clips:<br/>    clip, index = librosa.effects.trim(nr_audio[intvl[0]:intvl[1]],       top_db=20, frame_length=512, hop_length=64)</span></pre><p id="5000" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意<code class="fe nd ne nf lr b">split_audio</code>仅提供时间标记间隔。我们需要在<code class="fe nd ne nf lr b">nr_audio[intvl[0]:intvl[1]]</code>之前获得实际的音频剪辑。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h1 id="1923" class="mb lw it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">对声音事件进行分类</h1><p id="7bc4" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">为了对孤立的声音片段进行分类，我们可以使用来自<a class="ae kz" href="https://medium.com/@chathuranga.15/sound-event-classification-using-machine-learning-8768092beafc" rel="noopener">先前帖子</a>的经过训练的神经网络模型。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="6715" class="lv lw it lr b gy lx ly l lz ma">#Load segment audio classification model<br/>model_path = r"best_model/"<br/>model_name = "audio_NN3_grouping2019_10_01_11_40_45_acc_91.28"</span><span id="ef2e" class="lv lw it lr b gy ng ly l lz ma"># Model reconstruction from JSON file<br/>with open(model_path + model_name + '.json', 'r') as f:<br/>    model = model_from_json(f.read())</span><span id="b754" class="lv lw it lr b gy ng ly l lz ma"># Load weights into the new model<br/>model.load_weights(model_path + model_name + '.h5')</span></pre><p id="f8af" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">创建该模型是为了使用标签编码器预测标签。我们还需要在这里复制标签编码器。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="c139" class="lv lw it lr b gy lx ly l lz ma"># Replicate label encoder<br/>lb = LabelEncoder()<br/>lb.fit_transform(['Calling', 'Clapping', 'Falling', 'Sweeping', 'WashingHand', 'WatchingTV','enteringExiting','other'])</span></pre><p id="c174" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了使用加载的模型对音频剪辑进行分类，我们需要绝对平均 STFT 特征。看看下面完成这项任务的代码。注意，该函数接受标签编码器<code class="fe nd ne nf lr b">lb </code>作为输入，以产生一个有意义的标签作为声音事件。</p><pre class="lb lc ld le gt lq lr ls lt aw lu bi"><span id="f148" class="lv lw it lr b gy lx ly l lz ma">def predictSound(X, lb):<br/>    stfts = np.abs(librosa.stft(X, n_fft=512, hop_length=256, win_length=512))<br/>    stfts = np.mean(stfts,axis=1)<br/>    stfts = minMaxNormalize(stfts)<br/>    result = model.predict(np.array([stfts]))<br/>    predictions = [np.argmax(y) for y in result]<br/>    return lb.inverse_transform([predictions[0]])[0]</span></pre><p id="ce60" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，我们可以在上述修剪操作后立即对隔离的剪辑使用<code class="fe nd ne nf lr b">predictSound</code>功能。访问 GitHub <a class="ae kz" href="https://github.com/chathuranga95/SoundEventClassification" rel="noopener ugc nofollow" target="_blank">库</a>获取完整代码和分类示例。</p></div><div class="ab cl lj lk hx ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="im in io ip iq"><h1 id="c040" class="mb lw it bd mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx bi translated">实时声音事件识别</h1><p id="9acb" class="pw-post-body-paragraph kb kc it kd b ke my kg kh ki mz kk kl km na ko kp kq nb ks kt ku nc kw kx ky im bi translated">到目前为止，我们一直在研究声音事件的记录序列。如果我们想做同样的事情，但不需要记录，而是实时的。为此，我们可以使用麦克风输入流缓冲到一个临时缓冲区，并在缓冲区上工作。IO 可以用 python 库轻松处理，比如<a class="ae kz" href="https://pypi.org/project/PyAudio/" rel="noopener ugc nofollow" target="_blank"> PyAudio </a>(文档<a class="ae kz" href="https://people.csail.mit.edu/hubert/pyaudio/docs/" rel="noopener ugc nofollow" target="_blank">这里</a>)。查看 GitHub <a class="ae kz" href="https://github.com/chathuranga95/SoundEventClassification" rel="noopener ugc nofollow" target="_blank">库</a>的实现。</p><p id="5cb3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">希望这篇文章对你有用。</p></div></div>    
</body>
</html>