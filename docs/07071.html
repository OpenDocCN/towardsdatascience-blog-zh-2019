<html>
<head>
<title>JupyterLab for complex Python and Scala Spark projects</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复杂 Python 和 Scala Spark 项目的 JupyterLab</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/jupyterlab-for-complex-python-and-scala-spark-projects-da1aa64fcc12?source=collection_archive---------15-----------------------#2019-10-06">https://towardsdatascience.com/jupyterlab-for-complex-python-and-scala-spark-projects-da1aa64fcc12?source=collection_archive---------15-----------------------#2019-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/83d884f032b0c2b3ad59a5ab7f498888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6rLXLfcVsfbmF71XuNDDQ.png"/></div></div></figure><div class=""/><p id="167f" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">JupyterLab 是一项用于原型和自我记录研究的了不起的技术。但是你能把它用于有一个大代码库的项目吗？</p><h1 id="d8ca" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">外部库的情况</h1><p id="a0c0" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">笔记本工作流对全球所有数据科学家来说都是一大进步。能够直接看到每个步骤的结果，而不是一遍又一遍地运行同一个程序，这是一个巨大的生产力提升。此外，自我记录的能力使它很容易与同事分享。</p><p id="364e" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">也就是说，你在笔记本上所能实现的是有限的。它最适合交互式计算，但是当每个单元超过 100 行代码时，它就不再是交互式的了。在这一点上，你需要的是一个真正的 IDE，比如 T2 VS 代码，或者 T4 py charm，也许还有一些单元测试。</p><p id="c5cc" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个好的库是在你当前项目之外开发的，它应该足够通用，能够在广泛的项目中帮助你和你的同事。把它看作是一项未来会有很多倍回报的投资。</p><p id="85b7" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，你如何把这个图书馆推回朱庇特？</p><h1 id="2bd9" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">Python 内核</h1><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi md"><img src="../Images/c505def2662e7262e5ff79a62c11a376.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/0*VyMwS_EI0aUhZeMt.png"/></div></figure><p id="d33d" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">假设您想要向 Spark 对象添加新的功能，例如 Spark 上的 doSomething()方法和 DataFrame 上的 predict()方法。使用以下代码创建一个模块(Python 中的一个文件，例如 mylib.py ):</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mi"><img src="../Images/3d828a570cc0828f44e8f4920bab5ca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/0*wNJqYEtVVkA-AEhQ.png"/></div></div></figure><p id="6611" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">mylib_init()使用 setattr()函数来扩展 SparkSession 和 DataFrame 的类定义，并添加新方法。你要做的就是用下面的两行代码创建一个代码块(代码块 4)。首先导入函数“mylib_init()”定义，然后调用它。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/011c5021d27e9b5ca089b17cba120cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*jCXqTpvroqmTxt21.png"/></div></figure><p id="eee4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">小问题是，如果您更改 mylib.py 文件，即使您再次调用 mylib_init()，它也不会更改工作簿中的任何内容。我在<a class="ae mc" href="https://florianwilhelm.info/2018/11/working_efficiently_with_jupyter_lab/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>中发现了关于自动卸载插件的内容。这将允许 Jupyter 每 2 秒检查一次. py 的新版本，您只需要调用 mylib_init()来使用新的模块。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/e1deefaa9cb9283f6aca27e863e00435.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/0*FIg9ljmWUrA82Yz9.png"/></div></figure><p id="214a" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">因此，当您使用 pyspark 库在 Jupyter 实例中创建 Spark 驱动程序时，这是一个简单的例子(例如通过使用下面的代码)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/cbd0e5434668756ed3d6117b05bb3135.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/0*rACxKmHFFivDg5HN.png"/></div></figure><p id="19f4" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这是最简单的方法，因为它们都在同一台电脑上。当事情变得遥远时，这就有点复杂了(但没那么难)，例如当使用<a class="ae mc" href="https://github.com/jupyter-incubator/sparkmagic" rel="noopener ugc nofollow" target="_blank"> Sparkmagic </a>时。</p><h1 id="c7a7" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">Sparkmagic 内核(Python 和 Scala)</h1><p id="9546" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated"><a class="ae mc" href="https://github.com/jupyter-incubator/sparkmagic" rel="noopener ugc nofollow" target="_blank"> Sparkmagic </a>内核允许你的 Jupyter 实例通过<a class="ae mc" href="https://livy.apache.org/" rel="noopener ugc nofollow" target="_blank"> Livy </a>与 Spark 实例通信，Livy 是 Spark 的 REST 服务器。Sparkmagic 会将你的代码块作为 web 请求发送到 Livy 服务器。然后，Livy 会把它翻译成 Spark 驱动程序并返回结果。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mm"><img src="../Images/ac52a6d01eb46f1f7647417b252cf4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZqEsXCl_V5PkROUO.png"/></div></div></figure><p id="0150" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如您在下图中看到的，Spark 驱动程序在集群上是远程的，根本不能访问 Jupyter 实例下的文件。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mn"><img src="../Images/a308aa2575ec6130dc23c0f661d6a6ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-fMceHpFECZJUdr_.png"/></div></div></figure><p id="8439" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你如何解决这个问题并使用你的图书馆？</p><p id="de70" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果您使用 Python，您可以将代码存储在底层 HDFS 上并使用 sc。<a class="ae mc" href="https://spark.apache.org/docs/0.7.0/api/pyspark/pyspark.context.SparkContext-class.html#addFile" rel="noopener ugc nofollow" target="_blank"> addPath </a> (sc 是 Sparkmagic 自动创建的 SparkContext)。由于驱动程序和执行程序都可以访问 HDFS，所以它会起作用。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/bdc478ebade2dd3b9858b22e055d1a87.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/0*D7t8R3-WNulE11jk.png"/></div></figure><p id="0433" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于 Scala，我们希望能够在类路径中添加一个或多个 JAR 文件。JAR 是 Java 生态系统中的库。遗憾的是，虽然 SparkContext 上有 addJar，但这种方式行不通。随着 Scala 的编译和类型化，你不能只是在一大块代码中添加 JAR，然后立刻使用这个 JAR 中的类型。addJar 将使这对于执行者成为可能，但是对于驱动程序来说，当调用 addJar 时，不再可能添加类定义。</p><p id="2aa9" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">幸运的是，有一种方法可以在 Livy 创建 SparkContext 时通知它将哪个 JAR 添加到类路径中(在发送任何代码进行编译之前)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/4f5731c6a4645f2bf7a75db1e563b7e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/0*0ukA0rc_kHwZk6ja.png"/></div></figure><p id="1d34" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意，如果已经有一个上下文，这个块将重新启动一个上下文，所以它应该是您笔记本的第一个块。</p><h1 id="3669" class="kz la je bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">两全其美</h1><p id="169f" class="pw-post-body-paragraph kb kc je kd b ke lx kg kh ki ly kk kl km lz ko kp kq ma ks kt ku mb kw kx ky im bi translated">正如我们所看到的，无论您的情况如何，都有一种方法可以利用现有的代码库，只在 Jupyter 笔记本中保留高级的有意义的代码。</p><p id="ac24" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我强烈感觉到，高性能数据科学团队的关键是始终在提高生产力的工具上投入时间。共享的高级原语库对生产力至关重要。Jupyter 对此很满意，这是个好消息。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="5ff9" class="pw-post-body-paragraph kb kc je kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><em class="mx">原载于 2019 年 10 月 6 日</em><a class="ae mc" href="https://dataintoresults.com/post/jupyterlab-for-complex-python-and-scala-spark-projects/" rel="noopener ugc nofollow" target="_blank"><em class="mx">https://dataintoresults.com</em></a><em class="mx">。</em></p></div></div>    
</body>
</html>