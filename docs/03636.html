<html>
<head>
<title>Feature Selection Techniques in Regression Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回归模型中的特征选择技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-techniques-in-regression-model-26878fe0e24e?source=collection_archive---------2-----------------------#2019-06-10">https://towardsdatascience.com/feature-selection-techniques-in-regression-model-26878fe0e24e?source=collection_archive---------2-----------------------#2019-06-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="ea44" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">特征选择是一种减少特征数量的方法，从而降低模型的计算复杂度。很多时候，特征选择对于克服过度拟合问题变得非常有用。它帮助我们确定高精度预测响应变量所需的最小特征集。如果我们问模型，添加新特性是否一定会显著提高模型性能？如果不是，那么为什么要添加那些只会增加模型复杂性的新特性。</p><p id="5891" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们了解如何从给定数据集中的所有可用特征中选择重要的特征集。</p><p id="238b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用例子来理解总是比较好的。因此，让我们看看下面 R 中的“mtcars”数据集:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/a080e4f942955d020ffedfcf50caee30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/0*JH_lULq0ns5DlIgX"/></div></figure><p id="2242" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将删除 x 列，因为它只包含汽车型号，不会给预测增加太多价值。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi kt"><img src="../Images/03b4791a73084e73ba52a128e374d8c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6VP_wfJPbc9ESu0A"/></div></div></figure><p id="471f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上述数据中，有 12 个特征(x、mpg、cyl、disp、hp、drat、wt、qsec、vs、am、gear、carb ),我们希望预测 mpg(英里/加仑),因此它成为我们的目标/响应变量。</p><p id="26b8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们随机选择任何预测变量，并尝试拟合预测 mpg 的模型。让我们从“wt”开始:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/2485296f64f604933b40eb6020efb0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/0*OBnbaiNKCoHHpMHq"/></div></figure><p id="3428" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">三颗星(或星号)代表高度显著的 p 值。因此，截距和斜率的小 p 值表明我们可以拒绝零假设，这使得我们可以得出 mpg 和体重之间有很大关系的结论。通常，p 值为 5% (.05)或更低是一个很好的分界点。在我们的模型示例中，p 值非常接近于零。此外，R 平方值 0.74 告诉我们，模型可以解释目标变量中约 74%的方差，因此模型也很重要。</p><p id="cfff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们用两个变量 wt 和 hp(马力)来拟合模型，如下所示:(注意，我们可以使用任意两个随机选取的预测值，因为我们只是试图了解如果使用试凑法会发生什么)</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/21ec83920b80f209492b8c6cbb56d1fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/0*ovL-K1ochpBKB9wI"/></div></figure><p id="8858" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，R 平方值从 0.74 增加到 0.81。这意味着这个模型变得更加重要。此外，查看 wt 和 hp 的星级数，我们可以说两者都与目标变量密切相关，因此两者都很重要。</p><p id="3941" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">可能会有这样的情况，通过添加新变量，已经添加的变量的影响会降低，在这种情况下，如果任何旧变量的 p 值超过 0.05 的上限，则意味着该变量现在变得不重要，然后我们删除该变量。</p><p id="2de7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在再添加一个变量“qsec ”,并分析模型摘要，如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi la"><img src="../Images/4f7622e6bc1254df38127d7fe8db7ca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/0*wLUt6Mr3AO0rnXws"/></div></figure><p id="8b52" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从逻辑上来说，通过添加一个新的变量，它不应该减少已经添加的变量的影响，但是在这种情况下，正如我们在上面的图像中看到的，变量 hp 和“qsec”都变得不重要(p 值&gt; . 05 也没有星号)。</p><p id="e626" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们添加所有变量，看看会发生什么:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/1cff11c44c27dd82ba8d51d7d228bc85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/0*5aYzAjeYU-mlNQ7-"/></div></figure><p id="9632" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从上面的总结中，我们看到没有一个变量是重要的，因为所有的 p 值都大于阈值限制 0.05，而且总结也没有产生任何星号作为重要代码。这有点令人惊讶。如果没有显著的变量，那么如何拟合模型？</p><p id="95a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，如果我们对变量的所有组合使用试凑法，那么将会有总 2^k-1 线性模型，我们必须尝试并查看哪些是重要的特征。这不是很费时间的工作吗，当然可以。那么现在该怎么办呢？接下来是特征选择技术，它帮助我们找到产生显著模型拟合的最小特征集。因此，在回归中，非常常用的特征选择技术如下:</p><ul class=""><li id="869e" class="lc ld iq jp b jq jr ju jv jy le kc lf kg lg kk lh li lj lk bi translated">逐步回归</li><li id="c031" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">预选</li><li id="57c8" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated">反向消除</li></ul><h1 id="8afe" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">1.逐步回归</h1><p id="74dc" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在逐步回归技术中，我们开始用每个单独的预测因子来拟合模型，并查看哪一个具有最低的 p 值。然后选择变量，然后使用两个变量拟合模型，一个是我们在上一步中已经选择的变量，另一个是所有剩余的变量。我们再次选择具有最低 p 值的一个。还要记住，通过添加新变量，上一步中已经选择的变量的影响应该仍然很大。我们保持这个迭代，直到我们得到一个 p 值小于阈值 0.05 的组合。</p><p id="1d95" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们用一个例子来理解整个过程:</p><h1 id="e9c1" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第一步:</h1><p id="b686" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">我们用一个预测器和目标来拟合模型。我们一个接一个地尝试了每一个预测值，每一行下面分别代表模型拟合的 t 值、p 值和 R 平方值。正如我们所见，mpg ~ wt fit 具有最低的 p 值(也应小于 0.05)，因此将选择 wt 并转到步骤 2。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1b17a07d13d7193381179060b7518794.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/0*dSqzlV7NgUFyLDWA"/></div></figure><h1 id="8a30" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第二步:</h1><p id="928d" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">现在，我们将使用两个预测值来拟合模型。我们已经在步骤 1 中选择了一个作为 wt，对于第二个预测值，我们将逐一尝试所有剩余的预测值。并且将再次选择具有最低 p 值的那些。在这个例子中，我们得到了 wt 和 cyl。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/1baf06d5d8dec503876a1dd9d5be24c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zLQkBs32jlMX7G4D"/></div></div></figure><h1 id="275a" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第三步:</h1><p id="261d" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">现在，我们将尝试拟合 3 个预测值，其中两个已经在步骤 2 中选择，第三个将尝试剩余的预测值。但是在这里我们看到没有一个 p 值小于 0.05，因此都不显著。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/7af5fa61dcea87368b18253661658e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*POiow3l3_TJ1bBwr"/></div></div></figure><p id="1b1b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于所有 p 值都大于 0.05，因此三个组合特征都不显著。因此我们就此打住。</p><p id="c888" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，使用逐步回归，我们得到了对最终模型拟合有重大影响的最小特征集{wt，cyl}。这并不意味着其他特征没有影响，但如果我们得到一个只有两个变量的重要模型，它们的影响可以忽略不计。</p><p id="92c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，在这里，我们已经观察到，与我们必须比较 2 个⁰-1 = 1023 个模型的试凑法相比，我们的搜索空间已经急剧减小。</p><h1 id="c384" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">2.预选</h1><p id="03fc" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">向前选择几乎类似于逐步回归，然而唯一的区别是在向前选择中，我们仅不断添加特征。我们不删除已经添加的特征。在每次迭代中，我们只添加那些增加整体模型拟合度的特性。</p><h1 id="c683" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">3.反向消除</h1><p id="2790" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在第一步的反向消除中，我们包括所有预测值，在随后的步骤中，继续移除具有最高 p 值(&gt; 0.05 阈值极限)的预测值。经过几次迭代后，它将产生最终的特征集，这些特征集对于以期望的精度预测结果来说是足够重要的。</p><p id="fa6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将以 mtcars 数据集为例，按如下步骤进行:</p><h1 id="d632" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第一步:</h1><p id="1ee4" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">在步骤 1 中，我们用数据集中可用的所有特征构建模型。然后观察一些事情:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mu"><img src="../Images/a99a9ae3ac1c874c851cd96bd888d452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NUkny13nus0OkzN0"/></div></div></figure><h1 id="2f14" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第二步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mv"><img src="../Images/d40ccea5dab578eb5c635b47f1a06588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uUUqjne9480VhHLR"/></div></div></figure><h1 id="ee2b" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第三步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mw"><img src="../Images/98f59cc11a56f2dccab1f8c3d07efb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6yrWOUW5zF-73yes"/></div></div></figure><h1 id="1c9d" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第四步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/4f0be4af0e2fbb210719330e36130b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hQlIoqCE8Vd51yGO"/></div></div></figure><h1 id="db5e" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第五步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi my"><img src="../Images/6ac7d60c47518b462c15cb4f52426fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OuGEit2fTVAIcaQX"/></div></div></figure><h1 id="e6d1" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第六步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mz"><img src="../Images/5d87494c89cf91de1ee5e22e3a2db189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gyXosr5uRy7sCmTq"/></div></div></figure><h1 id="4f96" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第七步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi na"><img src="../Images/2091d58568fea3afbb34886c31efbe19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hdeepI-Q3rhEG3A3"/></div></div></figure><h1 id="785a" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第八步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nb"><img src="../Images/5049254a8fe91e0279c17e4a09482187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2ZTJucsBE-biAf7o"/></div></div></figure><h1 id="c367" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">第九步</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nc"><img src="../Images/d447624d97add394d4e30333b08789a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FqZQ23l0ilFH5Fb4"/></div></div></figure><p id="7f97" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最终，我们得到了{wt，qsec}作为最小的特征集。现在让我们来看看有趣的事情，这里是否反向消除产生了我们用逐步回归得到的相同的特征集。使用逐步回归，我们得到了{wt，cyl}作为最佳可能的最小特征集。</p><p id="ffd3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">还有一点我们可以得出结论，那就是我们用所有的特征选择技术得到的特征集并不总是相同的。我们必须根据业务问题和我们的理解，明智地选择不同的技术。</p><p id="3848" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">以上就是这三种特征选择技术。还有其他同样重要的技巧需要理解，我将在接下来的文章中写下这些技巧。</p><p id="6d3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文首次出现在“<a class="ae nd" href="https://ashutoshtripathi.com/" rel="noopener ugc nofollow" target="_blank">科技隧道</a>”博客上，网址为<a class="ae nd" href="https://ashutoshtripathi.com/2019/06/07/feature-selection-techniques-in-regression-model/" rel="noopener ugc nofollow" target="_blank">https://ashutoshtripathi . com/2019/06/07/feature-selection-techniques-in-regression-model/</a></p><p id="1f01" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢您的阅读。</p><p id="04bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">非常欢迎你对这篇文章的想法。请在评论区分享。</p><h1 id="8fb2" class="lq lr iq bd ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn bi translated">相关文章:</h1><ul class=""><li id="c7ff" class="lc ld iq jp b jq mo ju mp jy ne kc nf kg ng kk lh li lj lk bi translated"><a class="ae nd" href="https://ashutoshtripathi.com/2019/01/16/what-is-linear-regression-part1/" rel="noopener ugc nofollow" target="_blank">什么是线性回归？零件:1 </a></li><li id="dfef" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><a class="ae nd" href="https://ashutoshtripathi.com/2019/01/06/what-is-linear-regression-part2/" rel="noopener ugc nofollow" target="_blank">什么是线性回归？零件:2 </a></li><li id="8f04" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><a class="ae nd" href="https://ashutoshtripathi.com/2019/01/15/covariance-and-correlation/" rel="noopener ugc nofollow" target="_blank">协方差和相关性</a></li><li id="1438" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><a class="ae nd" href="https://ashutoshtripathi.com/2019/01/22/what-is-the-coefficient-of-determination-r-square/" rel="noopener ugc nofollow" target="_blank">什么是决定系数| R 平方</a></li><li id="4953" class="lc ld iq jp b jq ll ju lm jy ln kc lo kg lp kk lh li lj lk bi translated"><a class="ae nd" href="https://ashutoshtripathi.com/2019/05/30/examples-nlp-natural-language-processing/" rel="noopener ugc nofollow" target="_blank">自然语言处理的重要使用案例</a></li></ul></div></div>    
</body>
</html>