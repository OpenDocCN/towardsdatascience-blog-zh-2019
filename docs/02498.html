<html>
<head>
<title>Build it Yourself — Chatbot API with Keras/TensorFlow Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自己构建—使用 Keras/TensorFlow 模型的聊天机器人 API</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-it-yourself-chatbot-api-with-keras-tensorflow-model-f6d75ce957a5?source=collection_archive---------0-----------------------#2019-04-24">https://towardsdatascience.com/build-it-yourself-chatbot-api-with-keras-tensorflow-model-f6d75ce957a5?source=collection_archive---------0-----------------------#2019-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="440e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在 Keras/TensorFlow 模型上构建简单聊天机器人的分步解决方案及源代码</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/aef3b589a2a68fe96aa21afbf8ec9d51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cv8XSfMtOXxyOfEmQy7nSw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Source: Pixabay</figcaption></figure><p id="83a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">构建自己的聊天机器人(或助手，这个词是聊天机器人的一个新的流行术语)并不像你想象的那么复杂。各种聊天机器人平台正在使用分类模型来识别用户意图。虽然很明显，在现有平台的基础上构建聊天机器人时，你会得到一个很好的提示，但研究背景概念并尝试自己构建它不会有什么坏处。为什么不自己用一个类似的模型。聊天机器人实施的主要挑战是:</p><ol class=""><li id="504a" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">对用户输入进行分类以识别意图(这可以通过机器学习来解决，我正在使用带有 TensorFlow 后端的 Keras)</li><li id="3cb5" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">保持上下文。这部分是编程，这里没有什么 ML 相关的。我使用 Node.js 后端逻辑来跟踪对话上下文(在上下文中，通常我们不需要对用户意图进行分类——用户输入被视为聊天机器人问题的答案)</li></ol><p id="fbfd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的<a class="ae mi" href="https://github.com/katanaml/katana-assistant" rel="noopener ugc nofollow" target="_blank"> GitHub </a> repo(开源)上提供了本文的完整源代码和自述说明。</p><p id="b19d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是实现中使用的 Python 库的列表。<a class="ae mi" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>深度学习库用于建立分类模型。Keras 在 TensorFlow 后端上运行培训。兰开斯特词干库用于折叠不同的单词形式:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="22f0" class="mo mp it mk b gy mq mr l ms mt">import nltk<br/>from nltk.stem.lancaster import LancasterStemmer<br/>stemmer = LancasterStemmer()</span><span id="4662" class="mo mp it mk b gy mu mr l ms mt"># things we need for Tensorflow<br/>import numpy as np<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Dropout<br/>from keras.optimizers import SGD<br/>import pandas as pd<br/>import pickle<br/>import random</span></pre><p id="5419" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">聊天机器人的意图和学习模式在普通的 JSON <a class="ae mi" href="https://github.com/katanaml/katana-assistant/blob/master/mlmodels/intents.json" rel="noopener ugc nofollow" target="_blank">文件</a>中定义。不需要有庞大的词汇量。我们的目标是建立一个特定领域的聊天机器人。也可以为小词汇量创建分类模型，它将能够识别为训练提供的一组模式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/8ea2c5c3cae2a8f61b2c8a8d079c045a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djvACUmdFgbhG9e5PFi6PQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">chatbot training data</figcaption></figure><p id="3d6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们开始分类模型训练之前，我们需要首先建立词汇。模式被处理以构建词汇表。每个单词被词干化以产生通用词根，这将有助于覆盖用户输入的更多组合:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="f309" class="mo mp it mk b gy mq mr l ms mt">words = []<br/>classes = []<br/>documents = []<br/>ignore_words = ['?']<br/># loop through each sentence in our intents patterns<br/>for intent in intents['intents']:<br/>    for pattern in intent['patterns']:<br/>        # tokenize each word in the sentence<br/>        w = nltk.word_tokenize(pattern)<br/>        # add to our words list<br/>        words.extend(w)<br/>        # add to documents in our corpus<br/>        documents.append((w, intent['tag']))<br/>        # add to our classes list<br/>        if intent['tag'] not in classes:<br/>            classes.append(intent['tag'])</span><span id="5db5" class="mo mp it mk b gy mu mr l ms mt"># stem and lower each word and remove duplicates<br/>words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]<br/>words = sorted(list(set(words)))</span><span id="94cb" class="mo mp it mk b gy mu mr l ms mt"># sort classes<br/>classes = sorted(list(set(classes)))</span><span id="599b" class="mo mp it mk b gy mu mr l ms mt"># documents = combination between patterns and intents<br/>print (len(documents), "documents")<br/># classes = intents<br/>print (len(classes), "classes", classes)<br/># words = all words, vocabulary<br/>print (len(words), "unique stemmed words", words)</span></pre><p id="11a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是词汇创造的输出。共有 9 个意图(类别)和 82 个词汇:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="8c5b" class="mo mp it mk b gy mq mr l ms mt">45 documents<br/>9 classes ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']<br/>82 unique stemmed words ["'s", ',', 'a', 'advers', 'al', 'anyon', 'ar', 'awesom', 'be', 'behavy', 'blood', 'by', 'bye', 'can', 'caus', 'chat', 'check', 'could', 'dat', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'giv', 'good', 'goodby', 'hav', 'hello', 'help', 'hi', 'hist', 'hospit', 'how', 'i', 'id', 'is', 'lat', 'list', 'load', 'loc', 'log', 'look', 'lookup', 'man', 'me', 'mod', 'nearby', 'next', 'nic', 'of', 'off', 'op', 'paty', 'pharm', 'press', 'provid', 'react', 'rel', 'result', 'search', 'see', 'show', 'suit', 'support', 'task', 'thank', 'that', 'ther', 'til', 'tim', 'to', 'transf', 'up', 'want', 'what', 'which', 'with', 'you']</span></pre><p id="4451" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练不会基于单词的词汇来运行，单词对于机器来说是没有意义的。我们需要将单词翻译成包含 0/1 数组的单词包。数组长度将等于词汇大小，当当前模式中的一个单词位于给定位置时，将设置 1:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="2787" class="mo mp it mk b gy mq mr l ms mt"># create our training data<br/>training = []<br/># create an empty array for our output<br/>output_empty = [0] * len(classes)</span><span id="fd6e" class="mo mp it mk b gy mu mr l ms mt"># training set, bag of words for each sentence<br/>for doc in documents:<br/>    # initialize our bag of words<br/>    bag = []<br/>    # list of tokenized words for the pattern<br/>    pattern_words = doc[0]<br/>    # stem each word - create base word, in attempt to represent related words<br/>    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]<br/>    # create our bag of words array with 1, if word match found in current pattern<br/>    for w in words:<br/>        bag.append(1) if w in pattern_words else bag.append(0)<br/>    <br/>    # output is a '0' for each tag and '1' for current tag (for each pattern)<br/>    output_row = list(output_empty)<br/>    output_row[classes.index(doc[1])] = 1<br/>    <br/>    training.append([bag, output_row])</span><span id="fb70" class="mo mp it mk b gy mu mr l ms mt"># shuffle our features and turn into np.array<br/>random.shuffle(training)<br/>training = np.array(training)</span><span id="cdb8" class="mo mp it mk b gy mu mr l ms mt"># create train and test lists. X - patterns, Y - intents<br/>train_x = list(training[:,0])<br/>train_y = list(training[:,1])</span></pre><p id="b734" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练数据— X(转换为数组[0，1，0，1…，0]的模式)，Y(转换为数组[1，0，0，0，…，0]的 intents，intents 数组将只有一个 1)。模型是用 Keras 构建的，基于三层。根据我的实验，三层提供了良好的结果(但这都取决于训练数据)。分类输出将是多类数组，这将有助于识别编码意图。使用 softmax 激活生成多类分类输出(结果返回 0/1 的数组:[1，0，0，…，0] —该集合标识编码意图):</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="4bfd" class="mo mp it mk b gy mq mr l ms mt"># Create model - 3 layers. First layer 128 neurons, second layer 64 neurons and 3rd output layer contains number of neurons<br/># equal to number of intents to predict output intent with softmax</span><span id="254a" class="mo mp it mk b gy mu mr l ms mt">model = Sequential()<br/>model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(64, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(len(train_y[0]), activation='softmax'))</span></pre><p id="d2c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用 SGD <a class="ae mi" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank">优化器</a>编译 Keras 模型:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="128b" class="mo mp it mk b gy mq mr l ms mt"># Compile model. Stochastic gradient descent with Nesterov accelerated gradient gives good results for this model</span><span id="43d2" class="mo mp it mk b gy mu mr l ms mt">sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)<br/>model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])</span></pre><p id="4dcb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拟合模型—执行训练并构建分类模型。我在 200 次迭代中执行训练，批量大小= 5:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="c506" class="mo mp it mk b gy mq mr l ms mt"># Fit the model</span><span id="cede" class="mo mp it mk b gy mu mr l ms mt">model.fit(np.array(train_x), np.array(train_y), epochs=200, batch_size=5, verbose=1)</span></pre><p id="bd87" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型已建立。现在我们可以定义两个辅助函数。函数<em class="mw"> bow </em>帮助将用户句子翻译成数组为 0/1 的单词包:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="846b" class="mo mp it mk b gy mq mr l ms mt">def clean_up_sentence(sentence):<br/>    # tokenize the pattern - split words into array<br/>    sentence_words = nltk.word_tokenize(sentence)<br/>    # stem each word - create short form for word<br/>    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]<br/>    return sentence_words</span><span id="5480" class="mo mp it mk b gy mu mr l ms mt"># return bag of words array: 0 or 1 for each word in the bag that exists in the sentence<br/>def bow(sentence, words, show_details=True):<br/>    # tokenize the pattern<br/>    sentence_words = clean_up_sentence(sentence)<br/>    # bag of words - matrix of N words, vocabulary matrix<br/>    bag = [0]*len(words)  <br/>    for s in sentence_words:<br/>        for i,w in enumerate(words):<br/>            if w == s: <br/>                # assign 1 if current word is in the vocabulary position<br/>                bag[i] = 1<br/>                if show_details:<br/>                    print ("found in bag: %s" % w)</span><span id="45ad" class="mo mp it mk b gy mu mr l ms mt">return(np.array(bag))</span></pre><p id="55d7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看这个例子——把这个句子翻译成一个单词包:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="6669" class="mo mp it mk b gy mq mr l ms mt">p = bow("Load blood pessure for patient", words)<br/>print (p)<br/>print (classes)</span></pre><p id="baa1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当该函数在 chatbot 词汇表的句子中找到一个单词时，它会在数组中的相应位置设置 1。该数组将被发送到模型进行分类，以识别其所属的意图:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="09b2" class="mo mp it mk b gy mq mr l ms mt">found in bag: load<br/>found in bag: blood<br/>found in bag: for<br/>found in bag: paty<br/>[0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</span></pre><p id="80ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将训练好的模型保存到 pickle 文件中是一个很好的做法，以便能够重用它来通过 Flask REST API 发布:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="5dd5" class="mo mp it mk b gy mq mr l ms mt"># Use pickle to load in the pre-trained model<br/>global graph<br/>graph = tf.get_default_graph()</span><span id="54a0" class="mo mp it mk b gy mu mr l ms mt">with open(f'katana-assistant-model.pkl', 'rb') as f:<br/>    model = pickle.load(f)</span></pre><p id="7370" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在通过 Flask REST API 发布模型之前，运行一个额外的测试总是好的。使用<em class="mw"> model.predict </em>函数对用户输入进行分类，并根据计算出的概率返回意图(可以返回多个意图):</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="bb21" class="mo mp it mk b gy mq mr l ms mt">def classify_local(sentence):<br/>    ERROR_THRESHOLD = 0.25<br/>    <br/>    # generate probabilities from the model<br/>    input_data = pd.DataFrame([bow(sentence, words)], dtype=float, index=['input'])<br/>    results = model.predict([input_data])[0]<br/>    # filter out predictions below a threshold, and provide intent index<br/>    results = [[i,r] for i,r in enumerate(results) if r&gt;ERROR_THRESHOLD]<br/>    # sort by strength of probability<br/>    results.sort(key=lambda x: x[1], reverse=True)<br/>    return_list = []<br/>    for r in results:<br/>        return_list.append((classes[r[0]], str(r[1])))<br/>    # return tuple of intent and probability<br/>    <br/>    return return_list</span></pre><p id="9f1a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">句子分类示例:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="a196" class="mo mp it mk b gy mq mr l ms mt">classify_local('Fetch blood result for patient')</span></pre><p id="d38b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">意图计算正确:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="1632" class="mo mp it mk b gy mq mr l ms mt">found in bag: blood<br/>found in bag: result<br/>found in bag: for<br/>found in bag: paty</span><span id="110a" class="mo mp it mk b gy mu mr l ms mt">[('blood_pressure_search', '1.0')]</span></pre><p id="c9fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了通过 REST 端点发布相同的函数，我们可以<a class="ae mi" rel="noopener" target="_blank" href="/publishing-machine-learning-api-with-python-flask-98be46fb2440">将</a>包装到 Flask API 中:</p><pre class="kj kk kl km gt mj mk ml mm aw mn bi"><span id="8bf9" class="mo mp it mk b gy mq mr l ms mt">app = Flask(__name__)<br/>CORS(app)</span><span id="144d" class="mo mp it mk b gy mu mr l ms mt"><a class="ae mi" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/katana-ml/api/v1.0/assistant", methods=['POST'])<br/>def classify():<br/>    ERROR_THRESHOLD = 0.25<br/>    <br/>    sentence = request.json['sentence']<br/>    <br/>    # generate probabilities from the model<br/>    input_data = pd.DataFrame([bow(sentence, words)], dtype=float, index=['input'])<br/>    results = model.predict([input_data])[0]<br/>    # filter out predictions below a threshold<br/>    results = [[i,r] for i,r in enumerate(results) if r&gt;ERROR_THRESHOLD]<br/>    # sort by strength of probability<br/>    results.sort(key=lambda x: x[1], reverse=True)<br/>    return_list = []<br/>    for r in results:<br/>        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})<br/>    # return tuple of intent and probability<br/>    <br/>    response = jsonify(return_list)<br/>    return response</span><span id="0417" class="mo mp it mk b gy mu mr l ms mt"># running REST interface, port=5000 for direct test, port=5001 for deployment from PM2<br/>if __name__ == "__main__":<br/>    app.run(debug=False, host='0.0.0.0', port=5001)</span></pre><p id="7040" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经解释了如何实现分类部分。在帖子开头引用的 GitHub repo 中，你会找到如何维护上下文的完整例子。上下文由用 JavaScript 编写并运行在 Node.js 后端的逻辑维护。一旦意图被分类并且后端逻辑找到了上下文的起点，就必须在意图列表中定义上下文流——我们进入循环并询问相关的问题。上下文处理有多高级完全取决于后端实现(这超出了现阶段机器学习的范围)。</p><p id="bba5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">聊天机器人用户界面:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/9cc99e6a146bf5dcbd162d200a1c3bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aCVr2vDEe0re4pYIdFsohw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Chatbot UI implemented with Oracle JET</figcaption></figure></div></div>    
</body>
</html>