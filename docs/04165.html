<html>
<head>
<title>Bayesian Neural Networks (LSTM): implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯神经网络(LSTM):实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-neural-networks-lstm-3616327e8b7c?source=collection_archive---------10-----------------------#2019-06-30">https://towardsdatascience.com/bayesian-neural-networks-lstm-3616327e8b7c?source=collection_archive---------10-----------------------#2019-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="26a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">贝叶斯推理允许量化不确定性，因此，能够开发<a class="ae ko" href="https://arxiv.org/pdf/1804.11313.pdf" rel="noopener ugc nofollow" target="_blank">健壮的机器学习模型</a>。在实践中，需要采用抽样方法来近似贝叶斯设置中遇到的后验分布/积分。对于实践中典型使用的大型神经网络，采样方法是计算密集型的。变分推理方法已经被发展来克服这个限制。在早先的<a class="ae ko" href="https://medium.com/p/788fd83f0e38/" rel="noopener">帖子</a>中，我们讨论了实践的理论方面，变分推理算法，Bayes (BBB)的 Back Prop。BBB 通过设置权重先验，提供了一种减少认知不确定性的有效方法。认知不确定性通常与缺乏训练数据有关。在这篇文章中，我们考虑了 BBB 的实际应用，特别是对 LSMT 的应用。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/ef56ef1793e6dcbb9304034114b88f0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VNBxJ4pVWhEDZ136Fc33gA.png"/></div></div><figcaption class="lb lc gj gh gi ld le bd b be z dk"><strong class="bd lf">From </strong><a class="ae ko" href="https://arxiv.org/pdf/1704.02798.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bd lf">Fortunato et al</strong></a><strong class="bd lf">, 2017</strong></figcaption></figure><p id="ba5a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">Tensorflow probability 已经为标准层提供了一个实现，比如<a class="ae ko" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/DenseVariational" rel="noopener ugc nofollow" target="_blank">密集</a>、<a class="ae ko" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution1DReparameterization" rel="noopener ugc nofollow" target="_blank">卷积</a>。下面显示了一个网络模型示例，请注意，API 与 Tensorflow API 非常相似。因此，如果您需要使用这些标准图层之一，请查阅官方<a class="ae ko" href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lg"><img src="../Images/9d9a670071cc0d430e548c16364a08b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHbtDMfxI2_jVeOAOp4OaQ.png"/></div></div></figure><p id="b7e7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> LSMT: </strong>需要更多工作的一个例子是 LSTM。LSTM 是一类递归神经网络。Colah 的博客对此做了很好的解释。LSTM 的一步一步的 Tensorflow 实现也可以在<a class="ae ko" href="https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767" rel="noopener">这里</a>找到。如果你对 LSTM 的基本知识不确定，我强烈建议你在继续之前阅读它们。<a class="ae ko" href="https://arxiv.org/pdf/1704.02798.pdf" rel="noopener ugc nofollow" target="_blank"> Fortunato 等人</a>，2017 年验证了贝叶斯 LSTM。原始源代码是可用的，不幸的是，我发现它很难跟上。</p><p id="2a97" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">实现包含 LSTM 的这四个方程</strong></p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi lh"><img src="../Images/f65523aac9cdb6e238d3d810f3793ae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/0*xOBUVHALaAXd9q9v"/></div></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi li"><img src="../Images/6b8e60b8580755245d089f44f4465900.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*bRw08iQbQRVGCqURNBhonA.png"/></div></figure><p id="f525" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们需要权重的先验和变分后验。</p><p id="e252" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们通过为每个权重指定其(均值=0，标准差=1)来使用正态/高斯先验。</p><p id="1a91" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过效用函数获得每个权重“w”的变分后验概率</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lj"><img src="../Images/cf6f8a03b5c7e0738f2e6f97d877d49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AotLJtAJGq8ra7QUsr5Mog.png"/></div></div></figure><p id="b346" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该函数还计算这些权重的 KL，并将其添加到张量流集合中。该功能在<a class="ae ko" href="https://github.com/JP-MRPhys/bayesianLSTM" rel="noopener ugc nofollow" target="_blank"> github </a>上可用。</p><p id="e0cc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了实现贝叶斯 LSTM，我们从 tensorflow 的基本 LSMT 类开始，并通过向权重添加变分后验来覆盖调用函数，之后我们照常计算门 f、I、o、c 和 h。这里的<a class="ae ko" href="https://github.com/JP-MRPhys/bayesianLSTM/blob/master/model/BayesianLSTM.py" rel="noopener ugc nofollow" target="_blank">非常简单明了。</a></p><p id="b887" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这确保了我们拥有一致的 tensorflow API，然后能够在此基础上构建我们的模型，例如，使用多层 LSTM 的情感分析任务。请注意，结果测量的不确定性相当简单(二元)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi lk"><img src="../Images/82f596019b1d5d67da85fa61f13bb298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wovXyMKSUnetoUJVIz37-Q.png"/></div></div></figure><p id="21c5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里我们定义贝叶斯 LSTM 层，并通过单元格函数执行<a class="ae ko" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">展开</a>。这个实现的一个缺点是不能使用 tf.nn.dynamic_rnn，因为它会产生与梯度检查相关的循环错误。</p><p id="4499" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们可以获得逻辑和损失，注意，我们对<a class="ae ko" href="https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax" rel="noopener ugc nofollow" target="_blank"> softmax 层</a>采用变分后验概率</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ll"><img src="../Images/cae7ef16f79ae25530bcf722e7a06272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6J--12Fu00MrcCSmpZWHCQ.png"/></div></div></figure><p id="06c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在训练中，我们获得了我们创建的每个变分后验概率的 KL 集合。这使我们能够计算总 KL 损失，除以批量大小以获得基于证据的下限，如<a class="ae ko" href="https://arxiv.org/pdf/1704.02798.pdf" rel="noopener ugc nofollow" target="_blank"> Fortunato 等人</a>的论文中所述。</p><p id="3451" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">仅此而已。其余代码只是标准数据输入和训练循环，模型保存等可以在这里找到<a class="ae ko" href="https://github.com/JP-MRPhys/bayesianLSTM" rel="noopener ugc nofollow" target="_blank">。</a>然后我们可以补充文中提到的其他方面。</p></div></div>    
</body>
</html>