<html>
<head>
<title>Build an end-to-end Machine Learning Model with MLlib in pySpark.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 pySpark 中用 MLlib 搭建一个端到端的机器学习模型。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-an-end-to-end-machine-learning-model-with-mllib-in-pyspark-4917bdf289c5?source=collection_archive---------3-----------------------#2019-02-13">https://towardsdatascience.com/build-an-end-to-end-machine-learning-model-with-mllib-in-pyspark-4917bdf289c5?source=collection_archive---------3-----------------------#2019-02-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="42d5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对于具有不平衡类别的二元分类问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f8346fb87a7931b1eec4348bce60868f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEFKupkh_9XwmBj7jdxDFw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">photo credit: pexels</figcaption></figure><h1 id="b006" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="a8a6" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">内存计算和并行处理是<a class="ae mm" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> Apache Spark </strong> </a>在大数据行业非常受欢迎的一些主要原因，以处理大规模的数据产品并执行更快的分析。<strong class="ls iu"> MLlib </strong>建立在 Spark 之上，是一个可扩展的机器学习库，提供高质量的算法和超快的速度。它拥有优秀的 Java、<strong class="ls iu"> Python </strong>和 Scala API，是数据分析师、数据工程师和数据科学家的首选。MLlib 由常见的学习算法和实用程序组成，包括分类、回归、聚类、协同过滤(矩阵分解)、降维等。</p><h1 id="4df3" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">履行</h1><p id="9cbd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本文中，我们将在<strong class="ls iu"> pySpark </strong>中使用 MLlib 构建一个端到端的机器学习模型。我们将使用 kaggle 上的<a class="ae mm" href="https://www.kaggle.com/c/home-credit-default-risk" rel="noopener ugc nofollow" target="_blank">家庭信用违约风险</a>竞赛的真实数据集。这项竞赛的目的是根据从每个申请人那里收集的数据，确定贷款申请人是否有能力偿还贷款。目标变量为 0(有能力偿还贷款的申请人)或 1(没有能力偿还贷款的申请人)。这是一个目标标签高度不平衡的二元分类问题。分配比率接近 0.91 比 0.09，0.91 是能够偿还贷款的申请人的比率，0.09 是不能偿还贷款的申请人的比率。</p><p id="1a79" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们先来看看数据集的结构:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="642d" class="mx kz it mt b gy my mz l na nb">#we use the findspark library to locate spark on our local machine</span><span id="85e1" class="mx kz it mt b gy nc mz l na nb">import findspark<br/>findspark.init('home Diredtory of Spark')</span><span id="65fe" class="mx kz it mt b gy nc mz l na nb">from pyspark.sql import SparkSession</span><span id="c617" class="mx kz it mt b gy nc mz l na nb"># initiate our session and read the main CSV file, then we print the #dataframe schema</span><span id="0d05" class="mx kz it mt b gy nc mz l na nb">spark = SparkSession.builder.appName('imbalanced_binary_classification').getOrCreate()<br/>new_df = spark.read.csv('application_train.csv', header=True, inferSchema=True)<br/>new_df.printSchema()</span><span id="9df9" class="mx kz it mt b gy nc mz l na nb">root<br/> |-- SK_ID_CURR: integer (nullable = true)<br/> |-- TARGET: integer (nullable = true)<br/> |-- NAME_CONTRACT_TYPE: string (nullable = true)<br/> |-- CODE_GENDER: string (nullable = true)<br/> |-- FLAG_OWN_CAR: string (nullable = true)<br/> |-- FLAG_OWN_REALTY: string (nullable = true)<br/> |-- CNT_CHILDREN: integer (nullable = true)<br/> |-- AMT_INCOME_TOTAL: double (nullable = true)<br/> |-- AMT_CREDIT: double (nullable = true)<br/> |-- AMT_ANNUITY: double (nullable = true)<br/> |-- AMT_GOODS_PRICE: double (nullable = true)<br/> |-- NAME_TYPE_SUITE: string (nullable = true)<br/> |-- NAME_INCOME_TYPE: string (nullable = true)<br/> |-- NAME_EDUCATION_TYPE: string (nullable = true)<br/> |-- NAME_FAMILY_STATUS: string (nullable = true)<br/> |-- NAME_HOUSING_TYPE: string (nullable = true)<br/> |-- REGION_POPULATION_RELATIVE: double (nullable = true)<br/>...</span></pre><p id="1dfb" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">printSchema()只显示了列名及其数据类型。我们将删除 SK_ID_CURR 列，将“TARGET”列重命名为“label ”,并查看我们的目标变量的分布:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9a1c" class="mx kz it mt b gy my mz l na nb"># Sk_ID_Curr is the id column which we dont need it in the process #so we get rid of it. and we rename the name of our <br/># target variable to "label"<br/>drop_col = ['SK_ID_CURR']<br/>new_df = new_df.select([column for column in new_df.columns if column not in drop_col])<br/>new_df = new_df.withColumnRenamed('TARGET', 'label')<br/>new_df.groupby('label').count().toPandas()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5a6ed4b52d5b90a83c453da570e14bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*5RKU0GnjC5jHHyYQA8PWIg.jpeg"/></div></figure><p id="a7d9" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以用<strong class="ls iu"> matplotlib </strong>可视化标签的分布:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="564e" class="mx kz it mt b gy my mz l na nb"># let's have a look at the distribution of our target variable:<br/># to make it look better, we first convert our spark df to a Pandas</span><span id="9fdb" class="mx kz it mt b gy nc mz l na nb">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>%matplotlib inline<br/>df_pd = new_df.toPandas()<br/>print(len(df_pd))<br/>plt.figure(figsize=(12,10))<br/>sns.countplot(x='label', data=df_pd, order=df_pd['label'].value_counts().index)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/b916830124cbca6e3a9827d799ceafc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mt5rujbqywZtfDYbQxIWpw.jpeg"/></div></div></figure><p id="ec3f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下是熊猫数据帧格式的数据:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="6f68" class="mx kz it mt b gy my mz l na nb"># let's see how everything look in Pandas</span><span id="6e91" class="mx kz it mt b gy nc mz l na nb">import pandas as pd<br/>pd.DataFrame(new_df.take(10), columns= new_df.columns)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/9331440d61609868bc9d5be83094d720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3rnWDTRvdrBP1q6GubI4A.jpeg"/></div></div></figure><h2 id="f5e3" class="mx kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">数据争论</h2><p id="1ac6" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在我们对数据集的一般结构有了一些想法，让我们继续一些<strong class="ls iu">数据争论</strong>。首先我们检查我们有多少<strong class="ls iu">分类</strong>和<strong class="ls iu">数字</strong>特征。接下来，我们构建一个函数，输出数据集中缺失值的基本信息:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="14e3" class="mx kz it mt b gy my mz l na nb"># now let's see how many categorical and numerical features we have:</span><span id="70c8" class="mx kz it mt b gy nc mz l na nb">cat_cols = [item[0] for item in new_df.dtypes if item[1].startswith('string')] <br/>print(str(len(cat_cols)) + '  categorical features')</span><span id="c92c" class="mx kz it mt b gy nc mz l na nb">num_cols = [item[0] for item in new_df.dtypes if item[1].startswith('int') | item[1].startswith('double')][1:]</span><span id="06b4" class="mx kz it mt b gy nc mz l na nb">print(str(len(num_cols)) + '  numerical features')</span><span id="afba" class="mx kz it mt b gy nc mz l na nb"><strong class="mt iu">16  categorical features<br/>104  numerical features</strong></span></pre><p id="9a7c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">下面是我们如何得到<strong class="ls iu">缺失信息</strong>的表格:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="15d6" class="mx kz it mt b gy my mz l na nb"># we use the below function to find more information about the #missing values</span><span id="192e" class="mx kz it mt b gy nc mz l na nb">def info_missing_table(df_pd):<br/>    """Input pandas dataframe and Return columns with missing value and percentage"""<br/>    mis_val = df_pd.isnull().sum() #count total of null in each columns in dataframe</span><span id="5f24" class="mx kz it mt b gy nc mz l na nb">#count percentage of null in each columns<br/>    mis_val_percent = 100 * df_pd.isnull().sum() / len(df_pd) <br/>    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1) </span><span id="aee3" class="mx kz it mt b gy nc mz l na nb"> #join to left (as column) between mis_val and mis_val_percent<br/>    mis_val_table_ren_columns = mis_val_table.rename(<br/>    columns = {0 : 'Missing Values', 1 : '% of Total Values'}) </span><span id="86ba" class="mx kz it mt b gy nc mz l na nb">#rename columns in table<br/>    mis_val_table_ren_columns = mis_val_table_ren_columns[<br/>    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1) <br/>        <br/>    print ("Your selected dataframe has " + str(df_pd.shape[1]) + " columns.\n"    #.shape[1] : just view total columns in dataframe  <br/>    "There are " + str(mis_val_table_ren_columns.shape[0]) +              <br/>    " columns that have missing values.") #.shape[0] : just view total rows in dataframe</span><span id="0520" class="mx kz it mt b gy nc mz l na nb">    return mis_val_table_ren_columns<br/></span><span id="1cb5" class="mx kz it mt b gy nc mz l na nb">missings = info_missing_table(df_pd)<br/>missings</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e94441c4d5a862c442b86d8ddbec02af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*y7unFnMydD0vhAOp6SyaLA.jpeg"/></div></figure><p id="ec07" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">121 列中有 67 列缺少值。它没有在图像中显示所有这些列，但总体而言，这 67 列中的大多数都有超过 50%的缺失值。所以我们正在处理大量缺失的值。<strong class="ls iu">我们将用每列的平均值填充数值缺失值，用每列最常见的类别填充分类缺失值。</strong>但首先，让我们统计每一列中缺失的值:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="cb2c" class="mx kz it mt b gy my mz l na nb">miss_counts = count_missings(new_df)<br/>miss_counts</span><span id="29cc" class="mx kz it mt b gy nc mz l na nb">[('AMT_ANNUITY', 12),<br/> ('AMT_GOODS_PRICE', 278),<br/> ('NAME_TYPE_SUITE', 1292),<br/> ('OWN_CAR_AGE', 202929),<br/> ('OCCUPATION_TYPE', 96391),<br/> ('CNT_FAM_MEMBERS', 2),<br/> ('EXT_SOURCE_1', 173378),<br/> ('EXT_SOURCE_2', 660),<br/> ('EXT_SOURCE_3', 60965),<br/> ('APARTMENTS_AVG', 156061),<br/> ('BASEMENTAREA_AVG', 179943),<br/> ('YEARS_BEGINEXPLUATATION_AVG', 150007),<br/> ('YEARS_BUILD_AVG', 204488),<br/>...</span></pre><p id="5f4e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们将缺失值的分类列和数字列分开:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="5f4f" class="mx kz it mt b gy my mz l na nb"># here we seperate missing columns in our new_df based on #categorical and numerical types</span><span id="8506" class="mx kz it mt b gy nc mz l na nb">list_cols_miss=[x[0] for x in miss_counts]<br/>df_miss= new_df.select(*list_cols_miss)<br/>#categorical columns<br/>catcolums_miss=[item[0] for item in df_miss.dtypes if item[1].startswith('string')]  #will select name of column with string data type<br/>print("cateogrical columns_miss:", catcolums_miss)</span><span id="9e9b" class="mx kz it mt b gy nc mz l na nb">### numerical columns<br/>numcolumns_miss = [item[0] for item in df_miss.dtypes if item[1].startswith('int') | item[1].startswith('double')] #will select name of column with integer or double data type<br/>print("numerical columns_miss:", numcolumns_miss)</span></pre><p id="6b0b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">接下来，我们填充缺失的值:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="7ec8" class="mx kz it mt b gy my mz l na nb"># now that we have seperated the columns based on categorical and #numerical types, we will fill the missing categiracl <br/># values with the most frequent category</span><span id="86b7" class="mx kz it mt b gy nc mz l na nb">from pyspark.sql.functions import rank,sum,col<br/>df_Nomiss=new_df.na.drop()<br/>for x in catcolums_miss:                  mode=df_Nomiss.groupBy(x).count().sort(col("count").desc()).collect()[0][0] <br/>    print(x, mode) #print name of columns and it's most categories <br/>    new_df = new_df.na.fill({x:mode})</span><span id="ca52" class="mx kz it mt b gy nc mz l na nb"># and we fill the missing numerical values with the average of each #column</span><span id="557b" class="mx kz it mt b gy nc mz l na nb">from pyspark.sql.functions import mean, round</span><span id="8080" class="mx kz it mt b gy nc mz l na nb">for i in numcolumns_miss:<br/>    meanvalue = new_df.select(round(mean(i))).collect()[0][0] <br/>    print(i, meanvalue) <br/>    new_df=new_df.na.fill({i:meanvalue})</span></pre><p id="13da" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">既然我们的数据集中不再有缺失值，让我们来研究如何处理<strong class="ls iu">不平衡类</strong>。有不同的方法来缓解这个问题。一种方法是<strong class="ls iu">下采样</strong>多数阶级或者<strong class="ls iu">上采样</strong>少数阶级以取得更平衡的结果。另一种方法是为每个类分配<strong class="ls iu">权重</strong>，通过分配较小的权重来惩罚多数类，通过分配较大的权重来促进少数类。我们将在数据集中创建一个名为“weights”的新列，并将每个类的<strong class="ls iu">倒数</strong> <strong class="ls iu">比率</strong>指定为权重。这是如何做到的:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="614a" class="mx kz it mt b gy my mz l na nb"># adding the new column weights and fill it with ratios</span><span id="f533" class="mx kz it mt b gy nc mz l na nb">from pyspark.sql.functions import when</span><span id="3c2b" class="mx kz it mt b gy nc mz l na nb">ratio = 0.91<br/>def weight_balance(labels):<br/>    return when(labels == 1, ratio).otherwise(1*(1-ratio))</span><span id="59cb" class="mx kz it mt b gy nc mz l na nb">new_df = new_df.withColumn('weights', weight_balance(col('label')))</span></pre><p id="b6b6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">下面是添加重量列后的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/de2173f6b85426898e8ee54b54438068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j92FSal1VW7jgusC8d8cdA.jpeg"/></div></div></figure><h2 id="6490" class="mx kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">特征工程</h2><p id="5e84" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">下一步是<strong class="ls iu">特征工程</strong>。pySpark 使得提取特征变得如此简单，我们不需要做太多工作。以下是步骤:</p><ol class=""><li id="c9a9" class="nt nu it ls b lt mn lw mo lz nv md nw mh nx ml ny nz oa ob bi translated">我们应用 StringIndexer()为分类列中的每个类别分配索引。</li><li id="8315" class="nt nu it ls b lt oc lw od lz oe md of mh og ml ny nz oa ob bi translated">我们应用 OneHotEncoderEstimator()将分类列转换为 onehot 编码的向量。</li><li id="28d6" class="nt nu it ls b lt oc lw od lz oe md of mh og ml ny nz oa ob bi translated">我们应用 VectorAssembler()从所有分类和数字特征中创建一个特征向量，我们将最终向量称为“特征”。</li></ol><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="89a4" class="mx kz it mt b gy my mz l na nb"># we use the OneHotEncoderEstimator from MLlib in spark to convert #aech v=categorical feature into one-hot vectors<br/># next, we use VectorAssembler to combine the resulted one-hot ector #and the rest of numerical features into a <br/># single vector column. we append every step of the process in a #stages array</span><span id="ad4e" class="mx kz it mt b gy nc mz l na nb">from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler</span><span id="5c5f" class="mx kz it mt b gy nc mz l na nb">stages = []<br/>for categoricalCol in cat_cols:<br/>    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')<br/>    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + "classVec"])</span><span id="f554" class="mx kz it mt b gy nc mz l na nb">stages += [stringIndexer, encoder]</span><span id="abf4" class="mx kz it mt b gy nc mz l na nb">assemblerInputs = [c + "classVec" for c in cat_cols] + num_cols<br/>assembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")<br/>stages += [assembler]</span></pre><p id="a763" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在让我们把所有东西都放到一个管道里。这里我们执行一系列转换，因此我们使用管道一次完成所有转换:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="fd30" class="mx kz it mt b gy my mz l na nb"># we use a pipeline to apply all the stages of transformation</span><span id="8c44" class="mx kz it mt b gy nc mz l na nb">from pyspark.ml import Pipeline</span><span id="d007" class="mx kz it mt b gy nc mz l na nb">cols = new_df.columns<br/>pipeline = Pipeline(stages = stages)<br/>pipelineModel = pipeline.fit(new_df)<br/>new_df = pipelineModel.transform(new_df)</span><span id="c203" class="mx kz it mt b gy nc mz l na nb">selectedCols = ['features']+cols<br/>new_df = new_df.select(selectedCols)<br/>pd.DataFrame(new_df.take(5), columns=new_df.columns)</span></pre><p id="a3b0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以下是我们的新数据集在特征工程后的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/79acde11f0b7dec923c51190ac962e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0P-5jQTkqbd5kTbP7RFLA.jpeg"/></div></div></figure><h2 id="a4ea" class="mx kz it bd la ng nh dn le ni nj dp li lz nk nl lk md nm nn lm mh no np lo nq bi translated">训练和超参数调整</h2><p id="894b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于训练，我们首先将数据集分成训练集和测试集。然后，我们开始使用<strong class="ls iu">逻辑回归</strong>进行训练，因为它对二元分类问题表现良好。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="3928" class="mx kz it mt b gy my mz l na nb"># split the data into trainign and testin sets</span><span id="02b8" class="mx kz it mt b gy nc mz l na nb">train, test = new_df.randomSplit([0.80, 0.20], seed = 42)<br/>print(train.count())<br/>print(test.count())</span><span id="e616" class="mx kz it mt b gy nc mz l na nb"># first we check how LogisticRegression perform <br/>from pyspark.ml.classification import LogisticRegression</span><span id="cbe3" class="mx kz it mt b gy nc mz l na nb">LR = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=15)<br/>LR_model = LR.fit(train)</span></pre><p id="3c9c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们将为训练数据绘制 ROC 曲线，以了解如何执行逻辑回归，然后我们将使用 ROC 曲线下面积(一种用于评估二元分类的标准度量)作为评估模型的度量:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="9f44" class="mx kz it mt b gy my mz l na nb">#plotting the ROC Curve</span><span id="578c" class="mx kz it mt b gy nc mz l na nb">trainingSummary = LR_model.summary</span><span id="aa0a" class="mx kz it mt b gy nc mz l na nb">roc = trainingSummary.roc.toPandas()<br/>plt.plot(roc['FPR'],roc['TPR'])<br/>plt.ylabel('False Positive Rate')<br/>plt.xlabel('True Positive Rate')<br/>plt.title('ROC Curve')<br/>plt.show()</span><span id="fa7d" class="mx kz it mt b gy nc mz l na nb">print('Training set ROC: ' + str(trainingSummary.areaUnderROC))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/72d8e265e274bd59c26bb5e62ed5e815.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*YYKulA8J7MmGA_LfykD4-g.jpeg"/></div></figure><p id="b868" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在测试集上检查模型的性能:</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="57e4" class="mx kz it mt b gy my mz l na nb">from pyspark.ml.evaluation import BinaryClassificationEvaluator</span><span id="7c44" class="mx kz it mt b gy nc mz l na nb">predictions_LR = LR_model.transform(test)<br/>evaluator = BinaryClassificationEvaluator()<br/>print("Test_SET (Area Under ROC): " + str(evaluator.evaluate(predictions_LR, {evaluator.metricName: "areaUnderROC"})))</span><span id="8509" class="mx kz it mt b gy nc mz l na nb"><strong class="mt iu">Test_SET (Area Under ROC): 0.7111434396856681</strong></span></pre><p id="5f65" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">0.711 对于逻辑回归来说并不是一个很差的结果。接下来，我们尝试另一个模型，<strong class="ls iu">梯度推进树(GBT)。</strong>这是一种非常流行的分类和回归方法，使用决策树的集合。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="b625" class="mx kz it mt b gy my mz l na nb"># next we checkout gradient boosting trees</span><span id="247a" class="mx kz it mt b gy nc mz l na nb">from pyspark.ml.classification import GBTClassifier</span><span id="aec2" class="mx kz it mt b gy nc mz l na nb">gbt = GBTClassifier(maxIter=15)<br/>GBT_Model = gbt.fit(train)<br/>gbt_predictions = GBT_Model.transform(test)</span><span id="4503" class="mx kz it mt b gy nc mz l na nb">evaluator = BinaryClassificationEvaluator()<br/>print("Test_SET (Area Under ROC): " + str(evaluator.evaluate(gbt_predictions, {evaluator.metricName: "areaUnderROC"})))</span><span id="cf42" class="mx kz it mt b gy nc mz l na nb"><strong class="mt iu">Test_SET (Area Under ROC): 0.7322019340889893</strong></span></pre><p id="d2de" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们使用 GBT 获得了更好的结果，0.732。作为这里的最后一个策略，我们将使用网格搜索实现超参数调整，然后我们运行交叉验证来更好地提高 GBT 的性能。</p><pre class="kj kk kl km gt ms mt mu mv aw mw bi"><span id="a063" class="mx kz it mt b gy my mz l na nb">from pyspark.ml.tuning import ParamGridBuilder, CrossValidator</span><span id="d583" class="mx kz it mt b gy nc mz l na nb">paramGrid = (ParamGridBuilder()<br/>             .addGrid(gbt.maxDepth, [2, 4, 6])<br/>             .addGrid(gbt.maxBins, [20, 30])<br/>             .addGrid(gbt.maxIter, [10, 15])<br/>             .build())</span><span id="87ef" class="mx kz it mt b gy nc mz l na nb">cv = CrossValidator(estimator=gbt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)</span><span id="515d" class="mx kz it mt b gy nc mz l na nb"># Run cross validations.</span><span id="9b05" class="mx kz it mt b gy nc mz l na nb">cvModel = cv.fit(train)<br/>gbt_cv_predictions = cvModel.transform(test)<br/>evaluator.evaluate(gbt_cv_predictions)</span><span id="0ac4" class="mx kz it mt b gy nc mz l na nb"><strong class="mt iu">CV_GBT (Area Under ROC) = 0.7368288195372332</strong></span></pre><p id="5674" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">结果有了一点改善，这意味着我们仍然可以通过超参数调整来看看我们是否可以进一步改善结果。</p><p id="8a03" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这个项目中，我们建立了一个<strong class="ls iu">端到端的机器学习模型(具有不平衡类的二元分类)。</strong>我们展示了 Apache Spark 的 MLlib 的强大功能，以及它如何应用于端到端 ML 项目。</p><p id="7220" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">像往常一样，代码和 jupyter 笔记本在我的<a class="ae mm" href="https://github.com/nxs5899/end-to-end-Machine-Learning-model-with-MLlib-in-pySpark" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> Github </strong> </a>上可用。</p><p id="bf1b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">非常感谢您的提问和评论。</p><p id="03b8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">参考资料:</p><ol class=""><li id="d1c9" class="nt nu it ls b lt mn lw mo lz nv md nw mh nx ml ny nz oa ob bi translated"><a class="ae mm" href="https://github.com/elsyifa/Classification-Pyspark" rel="noopener ugc nofollow" target="_blank">https://github.com/elsyifa/Classification-Pyspark</a></li><li id="b7f1" class="nt nu it ls b lt oc lw od lz oe md of mh og ml ny nz oa ob bi translated"><a class="ae mm" href="https://spark.apache.org/docs/2.3.0/ml-classification-regression.html" rel="noopener ugc nofollow" target="_blank">https://spark . Apache . org/docs/2 . 3 . 0/ml-classification-regression . html</a></li></ol></div></div>    
</body>
</html>