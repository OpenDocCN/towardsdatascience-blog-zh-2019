<html>
<head>
<title>Is your Machine Learning Model Biased?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你的机器学习模型有偏差吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-your-machine-learning-model-biased-94f9ee176b67?source=collection_archive---------6-----------------------#2019-02-07">https://towardsdatascience.com/is-your-machine-learning-model-biased-94f9ee176b67?source=collection_archive---------6-----------------------#2019-02-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c3e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何衡量模型的公平性，并决定最佳的公平性指标。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/732a21ca2c50b858ca5930e44866406c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZZgdwWGvPQT_r3w0"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="kz"><p id="349a" class="la lb it bd lc ld le lf lg lh li lj dk translated">机器学习模型正越来越多地用于做出影响人们生活的决策。有了这种权力，就有责任确保模型预测是公平的，没有歧视。</p></blockquote></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><p id="82d0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><a class="ae ky" href="https://www.propublica.org/about/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="mm">ProPublica</em></strong></a><em class="mm">，一个独立的调查性新闻机构，在 2016 年 5 月 23 日出了一个令人不安的故事，标题是</em> <a class="ae ky" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="mm">机器偏见</em> </strong> </a> <strong class="lt iu"> <em class="mm">。</em> </strong> <em class="mm">它突出了美国司法系统的一个重大偏见，举了一个例子，一个名叫布里莎·博登的 18 岁女孩</em> <strong class="lt iu"> <em class="mm"> </em> </strong> <em class="mm">。2014 年，布里沙因盗窃自行车被捕。她被指控盗窃 80 美元。就在一年前，41 岁的弗农·普拉特因在附近的家得宝商店偷了价值 86.35 美元的工具而被捕。普拉特是一名惯犯，过去曾因多次盗窃和持械抢劫被定罪。另一方面，Brisha 在少年时犯过一些小过错，尽管并不严重。</em></p><p id="0579" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><a class="ae ky" href="http://www.northpointeinc.com/files/technical_documents/FieldGuide2_081412.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="mm">COMPAS</em></strong><strong class="lt iu"><em class="mm">，</em> </strong> <em class="mm">代表</em> <strong class="lt iu"> <em class="mm">惩教罪犯管理概况替代制裁</em> </strong> <em class="mm">是美国各州用来评估刑事被告成为</em> <strong class="lt iu"> <em class="mm">累犯</em> </strong> <em class="mm">的可能性的算法——这个术语用来描述重新犯罪的罪犯。根据 COMPAS 的评分，黑人博登被认为有更高的风险。白人 Prater 被评为低风险。</em></a></p><p id="78d2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">两年后，博登没有被指控任何新的罪行，而普拉特因另一项罪行正在服刑 8 年。 <strong class="lt iu"> <em class="mm">是算法全错了吗？</em>T55】</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/4301803240eb31857a330447af6be0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*fqvZE7CTFCMmRMSycgqRVg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">COMPAS scores comparison</figcaption></figure></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="b40a" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">ProPublica 与 COMPAS 的争议</h1><p id="eec8" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated"><a class="ae ky" href="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> ProPublica 对<strong class="lt iu"> COMPAS </strong>工具的</strong> </a>的分析发现，黑人被告远比白人被告更有可能被错误地判断为处于更高的再犯风险，而白人被告比黑人被告更有可能被错误地标记为处于低风险。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b99b1103b3e7261e7517ee34d194733a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*NuB8m8JmiV_LenoF2m2kGA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="nm">(</em><a class="ae ky" href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing" rel="noopener ugc nofollow" target="_blank"><em class="nm">Source</em></a><em class="nm">: ProPublica analysis of data from Broward County, Fla.)</em></figcaption></figure><p id="4bad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">Northpointe，该工具背后的公司回应说，该模型不是不公平的，因为它对白人和黑人都有几个类似的整体表现。为了证明这一点，他们查看了 AUC 分数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/2fe4bf8bbba417fb4d61f5e740d5a340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jsHMmG0QFsn4sYWGYVyZCg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf" rel="noopener ugc nofollow" target="_blank">http://www.northpointeinc.com/files/publications/Criminal-Justice-Behavior-COMPAS.pdf</a></figcaption></figure><p id="a6f8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">上表显示了非裔美国人和白人男性的任何逮捕结果的每个模型的结果。非裔美国男性的 AUC 从 0.64 到 0.73 不等，而白人的 AUC 从 0.69 到 0.75 不等。Northpoint 因此得出结论，由于白人男性的 AUC 结果与非裔美国男性的结果非常相似，他们的算法是完全公平的。</p><blockquote class="no np nq"><p id="f5f4" class="lr ls mm lt b lu lv ju lw lx ly jx lz nr mb mc md ns mf mg mh nt mj mk ml lj im bi translated">这给我们留下了一个非常重要的问题:<strong class="lt iu">我们如何决定哪种公平的衡量标准是合适的？在回答这个问题之前，让我们先了解一下模型是如何产生偏差的。</strong></p></blockquote></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="78fc" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">机器学习模型中的偏差</h1><p id="e837" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">维基百科称，“……偏差是学习算法中错误假设的错误。高偏差会导致算法错过特征和目标输出之间的相关关系”。</p><p id="d10e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">今天，几乎每个领域都在使用机器学习模型。他们有能力决定一个人是否有资格获得住房贷款，一个人对什么样的电影或新闻感兴趣，一个人对广告有多大反应等等。机器学习模型也进入了刑事司法领域，并被用于决定罪犯在监狱等待假释的时间。但是我们如何确保这些模型没有偏见，没有不公平地歧视人们呢？</p><p id="baad" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">让我们考虑三种不同的情况:</p><h2 id="5725" class="nu mp it bd mq nv nw dn mu nx ny dp my ma nz oa na me ob oc nc mi od oe ne of bi translated">案例 1:不同的组具有不同的基本事实阳性率</h2><p id="fd64" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">考虑以下美国乳腺癌统计数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/16d48cc0077668111899def30cfee70a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*AEjk3nNG06mGSYemvRMEJQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://www.breastcancer.org/symptoms/understand_bc/statistics" rel="noopener ugc nofollow" target="_blank">https://www.breastcancer.org/symptoms/understand_bc/statistics</a></figcaption></figure><p id="9665" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">现在，如果我们要建立一个分类器来预测乳腺癌的发生，根据这些统计数据，女性患乳腺癌的频率将比男性高两个数量级，概率比大约为 12%比 0.1 %。因此，在这种情况下，不同性别之间的基本事实阳性率存在非常合理的<strong class="lt iu">差异，这将影响分类器的结果。</strong></p><h2 id="7e4e" class="nu mp it bd mq nv nw dn mu nx ny dp my ma nz oa na me ob oc nc mi od oe ne of bi translated">案例 2:数据是对基本事实的有偏见的表示</h2><p id="f207" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">当一个受保护的属性(类似于<strong class="lt iu">性别、种族、年龄</strong>)在数据集内被赋值时，数据集可以包含<strong class="lt iu">标签偏差</strong>。让我们看看如何？</p><p id="084b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">2011 年，发布了一篇论文，该论文回顾了美国 364 所中小学办公室纪律推荐的记录模式。作者发现提到:</p><blockquote class="no np nq"><p id="a2b1" class="lr ls mm lt b lu lv ju lw lx ly jx lz nr mb mc md ns mf mg mh nt mj mk ml lj im bi translated">描述性和逻辑回归分析表明，来自非裔美国家庭的学生因问题行为而被提交到办公室的可能性是白人同龄人的 2.19 倍(初级)到 3.78 倍(中级)。此外，结果表明，来自<strong class="lt iu">非裔美国人</strong>和<strong class="lt iu">拉丁裔家庭</strong>的学生比他们的<strong class="lt iu">白人</strong>同龄人更有可能因为相同或类似的问题行为而被开除或停学。</p></blockquote><p id="66b1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">因此，如果我们有一个这样的数据集，并着手建立一个分类模型，预测一个学生将来是否会有<strong class="lt iu">行为问题</strong>，并使用“<strong class="lt iu">已被暂停</strong>”作为其标签，那么我们就存在标签偏差，因为在该模型中，不同的组被赋予不同的标签，这并不能准确反映实际的学生问题行为。</p><h2 id="5eea" class="nu mp it bd mq nv nw dn mu nx ny dp my ma nz oa na me ob oc nc mi od oe ne of bi translated">案例 3:当模型预测的结果很重要时。</h2><p id="9d09" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">如果我们正在制作一个惩罚性的模型<strong class="lt iu">，</strong>模型的<br/>决策的结果将是负面的，这可能就像惩罚某人一样。在这种情况下，人们可能会更关心<strong class="lt iu">假</strong> <br/> <strong class="lt iu">正</strong>以避免根据模型的输出惩罚无辜者。</p><p id="cab6" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">另一方面，当一个模型是<strong class="lt iu">辅助性的，</strong>模型决策的结果将是正面的，我们会更关心<strong class="lt iu">假阴性</strong>，这样我们就不会浪费我们的资源或资源在那些<br/>不需要我们提供的好处的人身上。</p><blockquote class="kz"><p id="76bf" class="la lb it bd lc ld oh oi oj ok ol lj dk translated">我们需要仔细思考我们的模型的后果，它可以告诉我们什么样的错误对我们很重要。偏见很容易被引入到我们的数据集中，我们需要选择合适的公平指标来处理不同的情况。</p></blockquote></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="ecd7" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">选择合适的“公平”指标</h1><p id="6a32" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">看起来有特定的公平性度量，但是，我们如何决定在特定情况下哪种公平性度量是必要的？在我们正在构建的模型或正在解决的问题的背景下，我们应该如何定义公平？嗯，今天有很多公平的度量标准被用来确保最终的模型没有偏见，但不幸的是，它们有其局限性。</p><h2 id="df8d" class="nu mp it bd mq nv nw dn mu nx ny dp my ma nz oa na me ob oc nc mi od oe ne of bi translated"><a class="ae ky" href="https://arxiv.org/abs/1412.3756)" rel="noopener ugc nofollow" target="_blank"> 1。不同的影响</a></h2><p id="bedb" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">这种度量有时也被称为“<strong class="lt iu">统计奇偶校验</strong>”。在美国法律中，无意偏见是通过完全不同的影响来编码的，当选择过程对不同的群体产生非常不同的结果时，即使它看起来是中性的，也会出现这种情况。这一法律决定取决于受保护阶层的定义(种族、性别、宗教习俗)和对这一过程的明确描述。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/5f38dc70220b28457f86dabc115b4bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*FAbhmO8kDcJP9HZ57uiCAQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Certifying and removing disparate impact, Feldman et al. (https://arxiv.org/abs/1412.3756)</figcaption></figure><p id="e28b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">换句话说，我们取两组的肯定分类的概率，然后取它们的比率。让我们借助一个例子来理解这个指标，在这个例子中，我们需要确定在一家公司招聘期间是否有不同的影响。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/364dc5a7b12f527c23dcd191e7c233e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*9soWjNrA4qK0qj1jvkObnw.png"/></div></figure><p id="4246" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><strong class="lt iu">由于 60%小于 80%，因此存在完全不同的影响</strong></p><p id="0ebc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><strong class="lt iu">权衡:</strong>正如我们所看到的，这个指标意味着基本事实的阳性率必须相同。然而，在上面讨论的<a class="ae ky" href="https://medium.com/p/94f9ee176b67/#5725" rel="noopener">案例 1 </a>中，这种公平的衡量标准将会落空，因为不同性别之间的真实阳性率存在两个数量级的差异。</p><h2 id="136f" class="nu mp it bd mq nv nw dn mu nx ny dp my ma nz oa na me ob oc nc mi od oe ne of bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1610.02413.pdf" rel="noopener ugc nofollow" target="_blank"> 2。机会均等</a></h2><p id="db67" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">这是另一个比较保护组之间真实阳性率的流行指标。在监督学习中，它是对特定敏感属性进行区分的标准，目标是基于可用特征预测某个目标。定义平等机会的论文节选:</p><p id="bfa7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><em class="mm">在二元的情况下，我们常常把结果 Y = 1 认为是“有利”的结果，比如“没有拖欠贷款”、“被大学录取”或者“获得晋升”。一种可能的放宽均等优势的方法是只在“优势”结果组中要求不歧视。也就是说，要求偿还贷款的人首先有获得贷款的平等机会(没有对那些最终将违约的人提出任何要求)。这导致我们放松了我们称之为“平等机会”的观念。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/754ff06a18e75f3b893b36d32bd8f361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*aL9vbtxxaY_LhOacJGXmhg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Equality of Opportunity in Supervised Learning, Hardt et al. (<a class="ae ky" href="https://arxiv.org/pdf/1610.02413.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1610.02413.pdf</a>)</figcaption></figure><p id="bcd7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><strong class="lt iu">权衡:</strong>同样，该指标无助于解决<a class="ae ky" href="https://medium.com/p/94f9ee176b67/#7e4e" rel="noopener">案例 2 </a>中的情况。其中数据集包含标签偏差的可能性很高。“平等机会”衡量标准是基于与标签的一致，如果标签本身有偏见，那么我们就没有解决公平问题。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="4d6a" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">模型不仅仅是“数学”。</h1><p id="9b46" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">我们从上面的讨论中收集到的是，模型的“偏见”和“公平”具有潜在的伦理含义，因此我们仍然需要一些人类的判断来做出关于我们如何在我们的环境中定义和衡量公平的适当决定。</p><p id="44e1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">当机器学习模型首次被用于决策时，人们认为既然它们依赖于数学和计算，它们就应该是公平的。但是通过上面的例子，我们知道这个神话是如何被揭穿的。现在人们说，“<strong class="lt iu">加上这个约束会让我的模型变得公平</strong>”这还是不对的。我们不能仅仅依赖数学；我们仍然需要一个人来运用人类的判断。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="a7e7" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">解决方法是什么？</h1><p id="60b7" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">在真实世界的数据中检测偏差并不容易，也没有放之四海而皆准的解决方案。<a class="ae ky" href="https://www.civisanalytics.com/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu">Civis Analytics</strong></a><strong class="lt iu">，</strong>一家数据科学和分析公司进行了一项<a class="ae ky" href="https://arxiv.org/pdf/1809.09245.pdf" rel="noopener ugc nofollow" target="_blank">案例研究</a>，该研究检验了六种不同的公平指标在预测中检测不公平偏见的能力，这些预测是由包含已知人工偏见的数据集训练的模型生成的。他们的一个发现是，“在评估机器学习设置的公平性时，从业者必须仔细考虑他们希望建模的基本事实中可能存在的不平衡，以及他们将用来创建这些模型的数据集的偏差来源”。</p><p id="c385" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated">构建机器学习模型时可以采用的一些最佳实践是:</p><ul class=""><li id="55c2" class="op oq it lt b lu lv lx ly ma or me os mi ot lj ou ov ow ox bi translated">思考模型的输入和输出。</li><li id="246a" class="op oq it lt b lu oy lx oz ma pa me pb mi pc lj ou ov ow ox bi translated">可用的公平性度量可以在一定程度上有所帮助。我们应该明智地使用它们。</li><li id="5e43" class="op oq it lt b lu oy lx oz ma pa me pb mi pc lj ou ov ow ox bi translated">使用多元化的团队来创建模型</li><li id="b72a" class="op oq it lt b lu oy lx oz ma pa me pb mi pc lj ou ov ow ox bi translated">总是研究数据，它的来源，也检查预测。</li><li id="5aec" class="op oq it lt b lu oy lx oz ma pa me pb mi pc lj ou ov ow ox bi translated">在训练模型时，可以采用一些方法来使其公平，但是应该清楚术语“公平”的含义和上下文。这些方法包括:</li></ul><div class="kj kk kl km gt ab cb"><figure class="pd kn pe pf pg ph pi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7f1372190a46c1d2b08417acb7923860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*1RpGCY2qv1ZictUjSF556Q.png"/></div></figure><figure class="pd kn pj pf pg ph pi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/f57d0c227d15e4c424b0c072fb822b7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*f1sdWuD8xtUUHUPWBS79LA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk pk di pl pm"><a class="ae ky" href="https://arxiv.org/pdf/1809.09245.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1809.09245.pdf</a> || <a class="ae ky" href="https://arxiv.org/pdf/1412.3756.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1412.3756.pdf</a></figcaption></figure></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/7cde7d831af90878be226484e8a50d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*vCCol6R1Fm_sewdmmdpzxw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://arxiv.org/pdf/1806.08010.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1806.08010.pdf</a></figcaption></figure></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><h1 id="6521" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">结论</h1><p id="739d" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml lj im bi translated">机器学习已经在许多应用和领域证明了它的实力。然而，机器学习模型的工业应用的关键障碍之一是确定用于训练模型的原始输入数据是否包含歧视性偏见。这是一个重要的问题，可能有伦理和道德的含义。然而，对此并没有单一的解决方案。对于模型的输出影响人们的情况，将公平置于利润之前是明智的。</p></div><div class="ab cl lk ll hx lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="im in io ip iq"><p id="3d30" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml lj im bi translated"><strong class="lt iu"> <em class="mm">参考</em> </strong> <em class="mm">:本文灵感来源于</em><a class="po pp ep" href="https://medium.com/u/654ed03ca126?source=post_page-----94f9ee176b67--------------------------------" rel="noopener" target="_blank"><em class="mm">Civis Analytics</em></a><em class="mm">的 J. Henry Hinnefeld 题为“</em> <a class="ae ky" href="https://www.youtube.com/watch?v=V3tmxMf2UH8" rel="noopener ugc nofollow" target="_blank"> <em class="mm">测量模型公平性</em> </a> <em class="mm">”的精彩演讲。非常感谢他强调了人工智能和机器学习如此重要的一个方面。</em></p></div></div>    
</body>
</html>