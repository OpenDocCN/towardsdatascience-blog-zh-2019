<html>
<head>
<title>Review: YOLOv3 — You Only Look Once (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">复习:YOLOv3 —你只看一次(物体检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=collection_archive---------1-----------------------#2019-02-07">https://towardsdatascience.com/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6?source=collection_archive---------1-----------------------#2019-02-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="84dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">改进的 YOLOv2，性能与 RetinaNet 相当，速度快 3.8 倍！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/9c382131b90b9ba0c2f0c197ad0affe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/1*tQ9PotwEr93jwFte56U8aA.gif"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">YOLOv3</strong></figcaption></figure><p id="e5f5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi lo translated"><span class="l lp lq lr bm ls lt lu lv lw di">在</span>这个故事里，<strong class="ku ir">华盛顿大学</strong>的<strong class="ku ir"> YOLOv3(你只看一次 v3) </strong>进行了回顾。YOLO 是一个非常著名的物体探测器。我想每个人都应该知道。以下是作者的演示:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lx ly l"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="ak">YOLOv3</strong></figcaption></figure><p id="dc32" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">由于作者忙于 Twitter 和 GAN，也帮助其他人的研究，YOLOv3 在 YOLOv2 上几乎没有增量改进。例如，具有快捷连接的更好的特征提取器<strong class="ku ir"> DarkNet-53 </strong>，以及具有<strong class="ku ir">特征图上采样和连接</strong>的更好的对象检测器。并作为<strong class="ku ir"> 2018 arXiv </strong>技术报告发表，引用<strong class="ku ir"> 200 余篇</strong>。(<a class="ma mb ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----eab75d7a1ba6--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="14dc" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">概述</h1><ol class=""><li id="568d" class="nb nc iq ku b kv nd ky ne lb nf lf ng lj nh ln ni nj nk nl bi translated"><strong class="ku ir">包围盒预测</strong></li><li id="64d3" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln ni nj nk nl bi translated"><strong class="ku ir">班级预测</strong></li><li id="f7aa" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln ni nj nk nl bi translated"><strong class="ku ir">跨尺度预测</strong></li><li id="8c15" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln ni nj nk nl bi translated"><strong class="ku ir">特征提取器:Darknet-53 </strong></li><li id="0529" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln ni nj nk nl bi translated"><strong class="ku ir">结果</strong></li></ol></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="3772" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> 1。包围盒预测</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9580497d5763e133347bcc9bd9eba2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*bsg1o-WBa-F_gYg3_lYufQ.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Bounding Box Prediction, Predicted Box (Blue), Prior Box (Black Dotted)</strong></figcaption></figure><ul class=""><li id="f9ca" class="nb nc iq ku b kv kw ky kz lb ns lf nt lj nu ln nv nj nk nl bi translated">与<a class="ae lz" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65"> YOLOv2 </a>相同。</li><li id="0219" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated"><strong class="ku ir">预测 tx，ty，tw，th。</strong></li><li id="f856" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">在训练期间，使用误差损失平方和。</li><li id="fd04" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">并且使用逻辑回归来预测客观性分数。如果边界框先验比任何其他边界框先验与地面真实对象重叠更多，则为 1。仅为每个地面真实对象分配一个边界框先验。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="2684" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">2.类别预测</h1><ul class=""><li id="1ea2" class="nb nc iq ku b kv nd ky ne lb nf lf ng lj nh ln nv nj nk nl bi translated">不使用 Softmax。</li><li id="90cf" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">相反，使用<strong class="ku ir">独立逻辑分类器</strong>和<strong class="ku ir">二元交叉熵损失</strong>。因为对于多标签分类可能存在重叠标签，例如如果 YOLOv3 被移动到其他更复杂的域，例如开放图像数据集。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="762d" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">3.跨尺度预测</h1><ul class=""><li id="a4dd" class="nb nc iq ku b kv nd ky ne lb nf lf ng lj nh ln nv nj nk nl bi translated"><strong class="ku ir">使用了 3 种不同的刻度</strong>。</li><li id="5fd6" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">特征是从这些尺度中提取出来的，像<a class="ae lz" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>。</li><li id="2590" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated"><strong class="ku ir">基本特征提取器 Darknet-53 </strong>增加了几个卷积层(下一节会提到)。</li><li id="c186" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated"><strong class="ku ir">这些层中的最后一层预测边界框、对象和类别预测。</strong></li><li id="152e" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">在 COCO 数据集上，<strong class="ku ir">每个尺度 3 个框。</strong>因此，输出张量为<strong class="ku ir"><em class="nw">N</em>×<em class="nw">N</em>×【3×(4+1+80)】</strong>，即<strong class="ku ir"> 4 个包围盒偏移，1 个对象预测，80 个类预测</strong>。</li><li id="ab32" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">接下来，<strong class="ku ir">特征图</strong>取自前 2 层，并通过 2× 对<strong class="ku ir">进行上采样。还从网络的早期获取特征图，并使用<strong class="ku ir">连接</strong>将其与我们的上采样特征合并。这其实就是典型的<strong class="ku ir">编解码架构</strong>，就像<a class="ae lz" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>进化到<a class="ae lz" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a>一样。</strong></li><li id="aab9" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">这种方法允许我们从上采样的特征中获得更有意义的语义信息，从早期的特征图中获得更细粒度的信息。</li><li id="1aa0" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">然后，<strong class="ku ir">增加几个卷积层来处理这个组合的特征图</strong>，并最终预测一个类似的张量，尽管现在大小是两倍。</li><li id="52f9" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated"><strong class="ku ir"> k-means 聚类</strong>也用于在之前找到<strong class="ku ir">更好的边界框。最后，在 COCO 数据集上，使用了<strong class="ku ir"> (10×13)、(16×30)、(33×23)、(30×61)、(62×45)、(59×119)、(116×90)、(156×198)、(373×326) </strong>。</strong></li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="19a5" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated"><strong class="ak"> 4。特征提取器:Darknet-53 </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/fd5ee7b4fcd4a3eff28949cf8567fdd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*tF1fK8-D5PVDb4khxvIH_g.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Darknet-53</strong></figcaption></figure><ul class=""><li id="67eb" class="nb nc iq ku b kv kw ky kz lb ns lf nt lj nu ln nv nj nk nl bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65"> YOLOv2 </a>使用 Darknet-19 分类网络进行特征提取。</li><li id="b082" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">现在，在 YOLOv3 中，<strong class="ku ir">使用了更深的网络 Darknet-53 </strong>，即 53 个卷积层。</li><li id="c0cf" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65"> YOLOv2 </a>和 YOLOv3 也使用批量标准化。</li><li id="4656" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated"><strong class="ku ir">快捷连接</strong>也如上图所示。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/10951f9ae5fbd317bb7640678617ad49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*U8n4uNBiRzhK8ZYlHbHZpg.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">1000-Class ImageNet Comparison (Bn Ops</strong>: Billions of Operations, <strong class="bd kr">BFLOP/s</strong>: Billion Floating Point Operation Per Second, <strong class="bd kr">FPS</strong>: Frame Per Second<strong class="bd kr">)</strong></figcaption></figure><ul class=""><li id="c6b2" class="nb nc iq ku b kv kw ky kz lb ns lf nt lj nu ln nv nj nk nl bi translated">1000 级 ImageNet Top-1 和 Top5 错误率的测量方法如上。</li><li id="f08f" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">在 Titan X GPU 上使用单作物 256×256 图像测试。</li><li id="06de" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">与<a class="ae lz" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>相比，Darknet-53 具有更好的性能(作者在论文中提到了这一点)，速度快 1.5 倍。</li><li id="4ff9" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">与<a class="ae lz" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-152 </a>相比，Darknet-53 具有相似的性能(作者在论文中提到了这一点),并且速度快 2 倍。</li></ul></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="082e" class="mj mk iq bd ml mm mn mo mp mq mr ms mt jw mu jx mv jz mw ka mx kc my kd mz na bi translated">5.结果</h1><h2 id="8079" class="nz mk iq bd ml oa ob dn mp oc od dp mt lb oe of mv lf og oh mx lj oi oj mz ok bi translated">5.1.可可地图@0.5</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi ol"><img src="../Images/d8759375ec52679d7f10d6d55281af39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lz2pcwoMBn22z6-Fhrv-qQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">mAP@0.5</strong></figcaption></figure><ul class=""><li id="5eb5" class="nb nc iq ku b kv kw ky kz lb ns lf nt lj nu ln nv nj nk nl bi translated">如上图，与<a class="ae lz" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4"> RetinaNet </a>相比，YOLOv3 以快得多的推理时间得到了不相上下的 mAP@0.5。</li><li id="f526" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">例如，yolov 3–608 在 51 毫秒内获得 57.9%的地图，而<a class="ae lz" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net-101–800</a>在 198 毫秒内仅获得 57.5%的地图，快了 3.8 倍。</li></ul><h2 id="906b" class="nz mk iq bd ml oa ob dn mp oc od dp mt lb oe of mv lf og oh mx lj oi oj mz ok bi translated">5.2.COCO 整体图</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi oq"><img src="../Images/0e4e9c6491ef32e7907a424996646f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LvKy1Cb0SQ1PLE26QgBgQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Overall mAP</strong></figcaption></figure><ul class=""><li id="9808" class="nb nc iq ku b kv kw ky kz lb ns lf nt lj nu ln nv nj nk nl bi translated">对于整体地图，YOLOv3 的性能明显下降。</li><li id="800a" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">然而，yolov 3–608 在 51 毫秒的推理时间内获得了 33.0%的 mAP，而<a class="ae lz" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net-101–50–500</a>在 73 毫秒的推理时间内仅获得了 32.5%的 mAP。</li><li id="143a" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">而 YOLOv3 与速度快 3 倍的<a class="ae lz" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>变种并驾齐驱。</li></ul><h2 id="2b48" class="nz mk iq bd ml oa ob dn mp oc od dp mt lb oe of mv lf og oh mx lj oi oj mz ok bi translated">5.3.细节</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="om on di oo bf op"><div class="gh gi or"><img src="../Images/14540d00809df3ffad24addf48f4bc41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABn1lSA2Y9akd0oSf9SL7Q.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">More Details</strong></figcaption></figure><ul class=""><li id="74c1" class="nb nc iq ku b kv kw ky kz lb ns lf nt lj nu ln nv nj nk nl bi translated">YOLOv3 比<a class="ae lz" rel="noopener" target="_blank" href="/review-ssd-single-shot-detector-object-detection-851a94607d11"> SSD </a>好很多，性能和<a class="ae lz" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a>差不多。</li><li id="4fec" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">并且发现 YOLOv3 在 AP_S 上的性能相对较好，而在 AP_M 和 AP_L 上的性能相对较差。</li><li id="53dc" class="nb nc iq ku b kv nm ky nn lb no lf np lj nq ln nv nj nk nl bi translated">YOLOv3 比使用<a class="ae lz" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae lz" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a>、<a class="ae lz" rel="noopener" target="_blank" href="/review-g-rmi-winner-in-2016-coco-detection-object-detection-af3f2eaf87e4"> G-RMI </a>和<a class="ae lz" href="https://medium.com/datadriveninvestor/review-tdm-top-down-modulation-object-detection-3f0efe9e0151" rel="noopener"> TDM </a>的两级更快 R-CNN 变体具有更好的 AP_S。</li></ul><h2 id="2ab0" class="nz mk iq bd ml oa ob dn mp oc od dp mt lb oe of mv lf og oh mx lj oi oj mz ok bi translated">5.4.定性结果</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/81b6976ff6457ef71dafacd7e33f6ac3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*DMXQU43dJEisBTuwJvkcXA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Nearly Exactly The Same Between Predicted Boxes and Ground-Truth Boxes</strong></figcaption></figure></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="cab8" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">其实技术报告中关于 YOLOv3 的细节并不多。因此，我只能简单回顾一下。读 YOLOv3 时建议在<a class="ae lz" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65"> YOLOv2 </a>和 YOLOv3 之间来回。(而且还有段落讲的是整体地图的测量。“是否真的反映了实际的检测精度？”如果有兴趣，请访问该文件。)</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h2 id="1099" class="nz mk iq bd ml oa ob dn mp oc od dp mt lb oe of mv lf og oh mx lj oi oj mz ok bi translated">参考</h2><p id="0efc" class="pw-post-body-paragraph ks kt iq ku b kv nd jr kx ky ne ju la lb ot ld le lf ou lh li lj ov ll lm ln ij bi translated">【2018 arXiv】【yolo v3】<br/><a class="ae lz" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank">yolo v3:增量改进</a></p><h2 id="8aa9" class="nz mk iq bd ml oa ob dn mp oc od dp mt lb oe of mv lf og oh mx lj oi oj mz ok bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph ks kt iq ku b kv nd jr kx ky ne ju la lb ot ld le lf ou lh li lj ov ll lm ln ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(们)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">物体检测<br/></strong><a class="ae lz" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lz" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lz" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lz" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lz" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lz" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lz" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lz" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lz" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">语义切分<br/></strong><a class="ae lz" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lz" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lz" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lz" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae lz" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lz" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae lz" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">生物医学图像分割<br/></strong>[<a class="ae lz" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lz" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lz" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lz" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lz" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a></p><p id="3134" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">实例分割<br/>T32】[<a class="ae lz" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lz" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lz" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae lz" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae lz" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae lz" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">)( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )( )(</p></div></div>    
</body>
</html>