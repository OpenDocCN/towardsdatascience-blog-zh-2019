<html>
<head>
<title>Introduction to Artificial Neural Networks — Explanation, Formulation &amp; Derivation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络导论——解释、公式和推导</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-artificial-neural-networks-5036081137bb?source=collection_archive---------19-----------------------#2019-12-04">https://towardsdatascience.com/introduction-to-artificial-neural-networks-5036081137bb?source=collection_archive---------19-----------------------#2019-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/387ce5bcfda699f11552a0c658c908ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U0mpyWD9I_v4Hk2DYRJRjQ.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@billy_huy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Billy Huynh</a> on <a class="ae kf" href="https://unsplash.com/s/photos/abstract?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e545" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络处于机器学习的前沿。他们正在接受训练，以完成各种可以想象的任务。通过这篇文章，我们试图看看神经网络的公式，推导。我们也着眼于神经网络学习的实际方面。</p><h1 id="8d3e" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">来自生物学的动机</h1><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mc"><img src="../Images/d4d952dc7cf765f1931441b901fe4407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AkMbEEDcoW73fDkpuWGxcA.png"/></div></div></figure><p id="58c3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大脑是我们身体中支持学习的重要组成部分。它有大约 100 亿个相互连接的神经元。一个神经元从它的突触接收来自其他神经元的输入。输入发生总和，当总和超过特定阈值时，神经元通过轴突向另一个神经元发送电尖峰。</p><h1 id="774c" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">感知器</h1><p id="7e46" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> P </span> erceptron 是机器学习中用于二元分类器监督学习的算法，即确定输入向量所属类别的函数。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/47d4ba1b5aedd15be93f1786dc1fbfcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qW0bk9bOh6xuO2eB6wCr2Q.png"/></div></div></figure><p id="10ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以写成:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/e77a2dd78ac7ac12de8152abd436dfc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9u0tuQS21de0Gu5owcZcdw.png"/></div></div></figure><h1 id="50fe" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">走向神经网络</h1><p id="e5b7" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">基本的人工神经网络是感知器的自然扩展。我们可以说，一个基本的神经网络是一个多层感知器，称为前馈神经网络。它将包含:</p><ul class=""><li id="e04c" class="mx my it ki b kj kk kn ko kr mz kv na kz nb ld nc nd ne nf bi translated">隐藏层</li><li id="2966" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated">偏差单位</li><li id="b349" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated">神经元(输入、输出和感知器)</li><li id="fc12" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated">突触权重</li><li id="aab7" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated">激活功能</li></ul><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/73f325ae5e00d6119cfb78e132a3ee71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKwp_vJsyBimo12rMxYZ0g.png"/></div></div></figure><h2 id="712e" class="nm lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">前馈神经网络</h2><p id="a683" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">任何神经网络的目标都是估计一个函数 f，该函数对给定的一组输入给出一组输出的估计。<br/>上面提到的神经网络被称为前馈，因为没有输出反馈给输入(不同于递归神经网络)。</p><blockquote class="ny"><p id="15ba" class="nz oa it bd ob oc od oe of og oh ld dk translated"><strong class="ak">输入时间权重，添加偏差并激活</strong></p></blockquote><p id="8b84" class="pw-post-body-paragraph kg kh it ki b kj oi kl km kn oj kp kq kr ok kt ku kv ol kx ky kz om lb lc ld im bi translated">我们可以说:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/1ccc0a205287974ced59649dc5f345c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*Pqfvf6YcRVIwfv6buzA1Cg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">where f is the activation function</figcaption></figure><p id="7c28" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以进一步说:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/63da4748eac6d34bd906bdaa4804c414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*mMtQ_zeFmhhxIIRpFhx5Yg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">where nH represents the number of perceptrons in the hidden layer &amp; w0 are the bias units.</figcaption></figure><p id="7e64" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，输出神经元 z 可以导出为:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi op"><img src="../Images/6836ed7a071f178823907ea26de58e49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*Q8-V3jbNGcpaCDaZ48Twbw.png"/></div></figure><h1 id="00a7" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">但是人工神经网络是怎么学习的呢？</h1><p id="50d8" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">对于每个神经网络，我们都有一个与其预测相关的成本函数。我们称这个成本函数为损失函数。该损失函数将根据输入和隐藏权重进行优化。现在，我们可以计算关于每个权重的成本函数偏导数，并更新最优成本函数的权重。</p><blockquote class="oq or os"><p id="977c" class="kg kh ot ki b kj kk kl km kn ko kp kq ou ks kt ku ov kw kx ky ow la lb lc ld im bi translated">注意:在这种方法中，我们将单个输入样本多次(等于权重的数量)传递给神经网络，用于计算和优化单个输入<strong class="ki iu">的所有权重。这在计算上非常昂贵。</strong></p></blockquote><h2 id="9b66" class="nm lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">反向传播是来拯救我们的</h2><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/91b2b0f7015e7ce5f98221c035c799fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*YnQ8JS7wAZ8-tWVuI4jz2g.png"/></div></figure><p id="0ff5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Geoff Hinton 于 1986 年在其<a class="ae kf" href="http://www.cs.toronto.edu/~hinton/absps/naturebp.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ot">论文</em> </a> <em class="ot">中提出了反向传播。</em>当时它被高度低估，但后来逐渐成为前馈神经网络的主干。</p><blockquote class="ny"><p id="9504" class="nz oa it bd ob oc od oe of og oh ld dk translated">本质上，反向传播只是链式法则的一个巧妙应用。</p></blockquote><p id="97ae" class="pw-post-body-paragraph kg kh it ki b kj oi kl km kn oj kp kq kr ok kt ku kv ol kx ky kz om lb lc ld im bi translated">反向传播使我们能够在一次反向传递中计算每个重量的偏导数。我们可以说:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/7d0cd89c2e74e9926143194ba74faeae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*STgIKPTX-bH42ZmIOFnHVw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">where J(w) represents the loss function, c = total number of output &amp; t is the given output and z is the predicted output.</figcaption></figure><p id="31b4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以说:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oz"><img src="../Images/ced1f453f121031bd968ce370e3296fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7oPeEc3pqJ2FRMLNXt2sA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">where eta is the learning rate.</figcaption></figure><p id="ec23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，对于其他重量，</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pa"><img src="../Images/09d36459eb4f5b159385ed38b20a6619.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uJhi4laBpeUl9etFGuqJ1Q.png"/></div></div></figure><p id="d2a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们获得由于后向传递中的误差函数(误差函数中每个权重的贡献)引起的权重变化之后，我们可以如下更新它们:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pb"><img src="../Images/b2c15ef223f10ddff28229ab2c4451f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*9B-ow69IZzaI1iBpy7R7ng.png"/></div></div></figure><p id="ea67" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过<a class="ae kf" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> <em class="ot">随机梯度下降</em> </a>或其变体反向传播来计算权重。</p></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><h1 id="da67" class="le lf it bd lg lh pj lj lk ll pk ln lo lp pl lr ls lt pm lv lw lx pn lz ma mb bi translated">反向传播的实际问题</h1><h2 id="9d12" class="nm lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">激活功能</h2><p id="c04d" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">激活函数的主要目的是将输入信号转换为神经网络中某一节点的输出信号。没有激活函数的神经网络只是一个线性回归模型。因此，为了学习复杂的非线性曲线，我们需要激活函数。</p><p id="4d15" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">激活函数应遵循的属性:</p><ul class=""><li id="3d9a" class="mx my it ki b kj kk kn ko kr mz kv na kz nb ld nc nd ne nf bi translated"><strong class="ki iu">非线性</strong>为了生成非线性输入映射，我们需要一个非线性激活函数。</li><li id="1444" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><strong class="ki iu">饱和</strong>一个饱和的激活函数挤压输入，把输出放在一个有限的范围内；因此，没有一个重量会对最终输出产生显著影响。</li><li id="b912" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><strong class="ki iu">连续和平滑</strong>对于基于梯度的优化，更平滑的函数通常显示出更好的结果。因为输入取连续范围，所以输出也应该取节点的连续范围。</li><li id="0890" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><strong class="ki iu">可微</strong>正如我们在推导<em class="ot"> f </em>的反向传播导数时所看到的，应该定义。</li><li id="f219" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><strong class="ki iu">单调</strong>如果激活函数不是单调递增的，则神经元的权重可能会使其影响较小，反之亦然；这与我们想要的正好相反。</li><li id="d29d" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><strong class="ki iu">对于小值是线性的</strong>如果对于小值是非线性的，我们需要在神经网络的权重初始化时考虑约束，因为我们可能会面临消失梯度或爆炸梯度的问题。</li></ul><h2 id="274e" class="nm lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">消失和爆炸渐变</h2><p id="2df8" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> V </span></p><p id="b2fe" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们之前讨论的，饱和激活函数将输入压缩到一个小值，因此输入的显著变化将导致输出的小变化，因此导数更小。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi po"><img src="../Images/30072c74b0240285919527283f322175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yjENqkY8qmzvwrNbsROI-w.png"/></div></div></figure><p id="4b56" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae kf" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> ReLU </strong> </a>是一个没有消失梯度问题的激活函数。大多数深度学习模型都以此为激活函数。</p><p id="4ede" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是如果你仍然坚持使用 tanh 或 sigmoid，你可以选择批量标准化。它只是把输入保持在导数不小的绿色区域。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi pp"><img src="../Images/f3a5f50fbe28a45584a79cbc39ced0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dj633mISQVR16n41nHfE3w.png"/></div></div></figure><p id="0ac9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi mm translated"><span class="l mn mo mp bm mq mr ms mt mu di"> E </span>在极端情况下，权重值可能会溢出，从而导致 NaN 权重。因此，这些 NaN 权重不能被进一步更新，导致学习过程停止。</p><p id="255c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有许多方法来处理爆炸式渐变，如<a class="ae kf" href="https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/" rel="noopener ugc nofollow" target="_blank">渐变剪辑(</a>如果范数超过特定阈值则剪辑渐变)和<a class="ae kf" href="https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/" rel="noopener ugc nofollow" target="_blank">权重正则化</a>(对大权重值的损失函数进行惩罚)。</p><h2 id="6184" class="nm lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">损失函数</h2><p id="c440" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">成本函数或损失函数本质上计算神经网络输出和目标变量之间的<strong class="ki iu"> <em class="ot">差</em> </strong>。它们可以分为三种分类:</p><p id="e900" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">回归损失函数:</strong>当目标变量为连续回归损失函数时使用。最常用的是均方差。其他名称包括绝对误差和平滑绝对误差。</p><p id="2742" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">分类损失函数:</strong>当输出变量是一个类的概率时，我们使用分类损失函数。大多数分类损失函数倾向于最大限度地增加利润。著名的名字包括分类交叉熵，负对数似然，边缘分类器。</p><p id="8255" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">嵌入损失函数:</strong>当我们必须测量两个或多个输入之间的相似性时，我们使用嵌入损失函数。一些广泛使用的嵌入损失函数是 L1 铰链误差和余弦误差。</p><h2 id="20e1" class="nm lf it bd lg nn no dn lk np nq dp lo kr nr ns ls kv nt nu lw kz nv nw ma nx bi translated">优化算法</h2><p id="3829" class="pw-post-body-paragraph kg kh it ki b kj mh kl km kn mi kp kq kr mj kt ku kv mk kx ky kz ml lb lc ld im bi translated">优化算法负责更新神经网络的权重和偏差，以减少损失。它们可以主要分为两类:</p><p id="55e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">恒定学习率算法:</strong>其中学习率<strong class="ki iu"> η </strong>对于所有参数和权重都是固定的。最常见的例子是随机梯度下降。</p><p id="0090" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">自适应学习率算法:</strong>Adam 等自适应优化器具有每参数学习率方法，该方法提供了更直接的启发式方法，而不是手动处理超参数。</p></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><p id="b40f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以上让你开始学习神经网络。保持文章简短，我将在后续文章中讨论 CNN 和 RNN。请关注此空间了解更多信息。</p><p id="dee0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">上一篇:</strong> <a class="ae kf" rel="noopener" target="_blank" href="/introduction-to-statistical-methods-in-ai-23f89df72cdb"> <strong class="ki iu">人工智能中的统计方法介绍</strong> </a></p><h1 id="a8e1" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><ul class=""><li id="35b5" class="mx my it ki b kj mh kn mi kr pq kv pr kz ps ld nc nd ne nf bi translated"><a class="ae kf" href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/expanding-gradients-in-neural-networks/</a></li><li id="4792" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><a class="ae kf" href="https://medium.com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb92daff331c" rel="noopener">https://medium . com/data-science-group-iitr/loss-functions-and-optimization-algorithms-demystified-bb 92 daff 331 c</a></li><li id="32dd" class="mx my it ki b kj ng kn nh kr ni kv nj kz nk ld nc nd ne nf bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/the-vanishing-gradient-problem-69bf08b15484">https://towards data science . com/the-vanishing-gradient-problem-69 BF 08 b 15484</a></li></ul></div></div>    
</body>
</html>