<html>
<head>
<title>Introduction to Statistical Methods in AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能中的统计方法导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-statistical-methods-in-ai-23f89df72cdb?source=collection_archive---------8-----------------------#2019-11-14">https://towardsdatascience.com/introduction-to-statistical-methods-in-ai-23f89df72cdb?source=collection_archive---------8-----------------------#2019-11-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7c1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">文章力求提供关于人工智能中不同统计方法的简明信息</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/818574fd6e2b57b23e17cec891536e21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tQ7zewI4-6WMgbb-jpcAnA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@yogidan2012?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Daniele Levis Pelusi</a> on <a class="ae ky" href="https://unsplash.com/s/photos/infinity?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="8e76" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">统计学习是一套理解数据的工具。这些工具大致分为两类:监督学习和非监督学习。一般来说，监督学习指的是基于一个或多个输入来预测或估计输出。另一方面，无监督学习在没有监督输出的情况下，在给定数据中提供关系或找到模式。</p><h2 id="5d68" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">什么是统计学习？</h2><p id="f554" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让，假设我们观察到一个反应 y 和 p 不同的预测因子 X = (X₁，X₂,….，Xp)。一般来说，我们可以说:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b357" class="lv lw it mu b gy my mz l na nb">Y =f(X) + ε</span></pre><p id="7420" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里<strong class="lb iu"> f </strong>为未知函数，<strong class="lb iu"> ε </strong>为<em class="nc">随机误差项</em>。</p><blockquote class="nd ne nf"><p id="c744" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">本质上，统计学习指的是一套估计 f 的方法。</p></blockquote><p id="2ed2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，我们有现成的 X 组，但输出 Y，没有这么多，误差平均为零，我们可以说:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a3e6" class="lv lw it mu b gy my mz l na nb">¥ = ƒ(X)</span></pre><p id="5c09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu">表示我们对<strong class="lb iu"> f </strong>的估计，<strong class="lb iu"> </strong>表示结果预测。</strong></p><p id="9462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，对于一组预测值 X，我们可以说:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="ea5c" class="lv lw it mu b gy my mz l na nb">E(Y — ¥)² = E[f(X) + ε — ƒ(X)]²</span><span id="fb7d" class="lv lw it mu b gy nj mz l na nb">=&gt; E(Y — ¥)² = [f(X) - ƒ(X)]² + Var(ε)<br/></span></pre><p id="7c2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在哪里，</p><ul class=""><li id="7ae8" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated"><em class="nc"> E(Y — ) </em>表示<em class="nc">实际结果与预期结果的平方差的期望值</em>。</li><li id="4504" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">【f(X)—ф(X)】</em>代表<strong class="lb iu"> <em class="nc">可约误差</em> </strong> <em class="nc">。</em>它是可简化的，因为我们可以通过更好的建模来潜在地提高的精度。</li><li id="ce07" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc"> Var(ε) </em>代表<strong class="lb iu">不可约误差</strong>。它是不可约的，因为无论我们估计得多好，我们都无法减少ε中由<em class="nc">方差</em>引入的误差。</li></ul><h2 id="163f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">回归 Vs 分类问题</h2><p id="9706" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">变量 Y 可以广义地描述为<em class="nc">定量</em>或<em class="nc">定性</em>(也称为<em class="nc">分类</em>)。量化变量采用数字值，例如年龄、身高、收入、价格等等。估计定性响应通常被称为<em class="nc"/><strong class="lb iu"><em class="nc">回归问题</em> </strong>。定性变量采用分类值，例如性别、品牌、词性等等。估计定性反应通常被称为<em class="nc"/><strong class="lb iu"><em class="nc">分类问题</em> </strong>。</p><blockquote class="ny"><p id="f676" class="nz oa it bd ob oc od oe of og oh lu dk translated">统计学中没有免费的午餐:在所有可能的数据集上，没有一种方法能支配所有其他方法。</p></blockquote><h2 id="46cb" class="lv lw it bd lx ly oi dn ma mb oj dp md li ok mf mg lm ol mi mj lq om ml mm mn bi translated">方差和偏差</h2><p id="c6ad" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated"><em class="nc">方差</em>是指如果我们用不同的训练数据集进行估计时，会发生变化的量。一般来说，当我们在给定的训练数据集上过度拟合模型时(训练集中的可约误差非常低，但测试集中的可约误差非常高)，我们得到的模型具有较高的方差，因为数据点的任何变化都会导致显著不同的模型。</p><p id="aa92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nc">偏差</em>指的是通过逼近现实生活中的问题而引入的误差，这可能会因为一个简单得多的模型而变得极其复杂——例如，用线性模型来建模非线性问题。一般来说，当我们过度拟合给定数据集上的模型时，它会导致非常小的偏差。</p><p id="4a35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这导致方差偏差的权衡。</p><blockquote class="ny"><p id="f61d" class="nz oa it bd ob oc od oe of og oh lu dk translated">当我们在给定的数据集上拟合模型时，偏差下降的速度往往比方差最初增加的速度快。因此，预期测试误差(可减少的)下降。然而，在某些时候，当过度拟合开始时，对偏差有一点影响，但是方差开始快速增加，当这种情况发生时，测试误差增加。</p></blockquote><h2 id="321f" class="lv lw it bd lx ly oi dn ma mb oj dp md li ok mf mg lm ol mi mj lq om ml mm mn bi translated">线性回归</h2><p id="c564" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">线性回归是一种属于监督学习的统计方法，用于预测定量反应。</p><p id="8b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nc">简单线性回归</em> </strong>方法基于假设线性关系的单个变量 X 预测定量反应。我们可以说:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="3c4e" class="lv lw it mu b gy my mz l na nb">¥ ≈ β₀ + β₁X</span></pre><p id="3a29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在的工作是<em class="nc">基于训练数据集估计</em> β₀和β₁，我们的模型的参数/系数，使得超平面(在这种情况下是一条线)与训练数据集<em class="nc">接近</em>。许多标准可以估计接近程度，最常见的是<em class="nc">最小二乘法。</em></p><p id="05e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有观察响应和预测响应之间的差的平方和表示为<em class="nc">残差平方和(RSS)。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on oo l"/></div></figure><p id="9599" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">线性回归中的问题</strong></p><ul class=""><li id="6f38" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated"><em class="nc">反应-预测值关系的非线性。</em></li><li id="86bc" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">误差项的相关性。</em></li><li id="423e" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">误差项的非恒定方差</em>。</li><li id="4c12" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">异常值:</em>当实际预测值与估计值相差很大<em class="nc"/>时，可能会由于数据记录不准确而出现异常值。</li><li id="8e3a" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">高杠杆点:</em>预测值的异常值会影响称为高杠杆点的回归线。</li><li id="2279" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">共线性:</em>当两个或两个以上的预测变量彼此密切相关时，剔除单个预测变量的个体效应可能具有挑战性<strong class="lb iu">。</strong></li></ul><p id="bd8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> KNN 回归</strong></p><p id="769b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">KNN 回归是一种估计或预测值的非参数方法，它不采用(X)的形式。它通过平均最接近 x₀.的所有 N₀响应来估计/预测ƒ(x₀),其中 x₀是预测点我们可以说:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/de3614bffa7f5798a75d40c4908e79b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*EZaQhNITK5peXrlhnrE_Ag.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><blockquote class="nd ne nf"><p id="081c" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">注意:如果 K 很小，拟合将是灵活的，数据的任何变化将导致不同的拟合，因此对于小 K，方差将很高，偏差很低；相反，如果 K 很大，它可能会掩盖数据中的一些结构，因此偏差会很大。</p></blockquote><h1 id="ac37" class="oq lw it bd lx or os ot ma ou ov ow md jz ox ka mg kc oy kd mj kf oz kg mm pa bi translated">分类问题</h1><p id="7354" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们到目前为止所讨论的反应，可能不总是<em class="nc">定量的</em>，也可能是<em class="nc">定性的，</em>预测这些定性的反应被称为分类。</p><p id="a37f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将讨论分类的各种统计方法，包括:</p><ul class=""><li id="239d" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">SVM</li><li id="4648" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">逻辑回归</em></li><li id="6df9" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc"> KNN 分类器</em></li><li id="d515" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc"> GAM </em></li><li id="9444" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">树木</em></li><li id="57f1" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">随机森林</em></li><li id="c3a2" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated"><em class="nc">增压</em></li></ul><h2 id="4e1e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">支持向量机(SVM)</h2><p id="be3f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">SVM 或支持向量机是最大限度地提高利润率的分类器。在下面的例子中，分类器的目标是找到一条线或(n-1)维超平面，它将 n 维空间中的两个类分开。我已经写了一篇详细的<a class="ae ky" rel="noopener" target="_blank" href="/support-vector-machine-formulation-and-derivation-b146ce89f28">文章</a>解释 SVM 的推导和提法。在我看来，它是我们人工智能统计方法工具箱中最强大的技术之一。</p><h2 id="997e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">逻辑回归</strong></h2><p id="9cbf" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">逻辑模型模拟属于特定类别的输出响应的概率。</p><p id="bf52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以说:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7c894944d4f500c7c1a90ac41275faf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*fxTK88SFrL-qrzIWrMTS6A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="d320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">应用<a class="ae ky" href="https://brilliant.org/wiki/componendo-and-dividendo/" rel="noopener ugc nofollow" target="_blank">componendo dividend to</a>我们得到:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/9753061c5e993d15cde4fd144d46a2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*N9--X1ZfT0jGDMwbyOIvPg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="0a36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这无非是<a class="ae ky" href="https://en.wikipedia.org/wiki/Odds" rel="noopener ugc nofollow" target="_blank">赔率</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/bdeeccca896cf5f304a669d3f777706c.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*RJQShGXYlVEBniI9wu4LBw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="835e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了估计β系数，我们可以使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然</a>。基本思想是估计β，使得结果的估计值和观察值尽可能接近。在二元分类中，观察到的类为<em class="nc"> 1 </em>和<em class="nc"> 0 </em>，我们可以说似然函数看起来像:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/50db4d058dc5f044a9461554171a0140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*iffBLyBY4vy_ZKvzkl0rGg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><h2 id="79be" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">KNN 分类器</h2><p id="9bfb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">KNN(K 最近邻)分类器是一种惰性学习技术，其中训练数据集表示在欧几里德超平面上，测试数据基于 K 欧几里德距离度量被分配标签。</p><p id="09e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实用方面</strong></p><ul class=""><li id="e2ad" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">k 应该根据经验选择，最好是奇数，以避免平局。</li><li id="49f0" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">KNN 应该有离散的和连续的目标函数。</li><li id="a52c" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">来自不同邻居的加权贡献(例如，基于距离的)可以用于计算最终标签。</li></ul><blockquote class="nd ne nf"><p id="e28d" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">注意:当数据是高维时，KNN 的性能会降低。这可以通过向特征本身提供权重来避免。</p></blockquote><p id="9c08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">K 对决策边界的影响</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/a0ff7c462ee33fd8d8f98c4e9699fd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0pGRa7XYU4nZt7dahsIgA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Chapter 8, <a class="ae ky" href="https://www.wiley.com/en-us/Pattern+Classification%2C+2nd+Edition-p-9780471056690" rel="noopener ugc nofollow" target="_blank">Pattern Classification</a> by <a class="ae ky" href="https://www.wiley.com/en-us/search?pq=%7Crelevance%7Cauthor%3ARichard+O.+Duda" rel="noopener ugc nofollow" target="_blank">Richard O. Duda</a>, <a class="ae ky" href="https://www.wiley.com/en-us/search?pq=%7Crelevance%7Cauthor%3APeter+E.+Hart" rel="noopener ugc nofollow" target="_blank">Peter E. Hart</a>, <a class="ae ky" href="https://www.wiley.com/en-us/search?pq=%7Crelevance%7Cauthor%3ADavid+G.+Stork" rel="noopener ugc nofollow" target="_blank">David G. Stork</a></figcaption></figure><p id="f47a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">KNN 的优势</strong></p><ul class=""><li id="8c8c" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">我们可以学习复杂的目标函数。</li><li id="2035" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">任何信息都不会丢失。</li></ul><p id="5421" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">KNN 的劣势</strong></p><ul class=""><li id="b8f7" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated">新实例的分类成本非常高。</li><li id="0883" class="nk nl it lb b lc nt lf nu li nv lm nw lq nx lu np nq nr ns bi translated">大量的计算发生在分类的时候。</li></ul><h2 id="c4d1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">广义可加模型</strong></h2><p id="639e" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">GAM 提供了一个广义的框架，用每个变量的非线性函数扩展了标准的多变量线性回归，同时保持了它的可加性。因此，所有非线性函数都可以独立计算并在以后相加。</p><blockquote class="nd ne nf"><p id="0d52" class="kz la nc lb b lc ld ju le lf lg jx lh ng lj lk ll nh ln lo lp ni lr ls lt lu im bi translated">注:GAM like 线性回归可用于定量和定性响应。</p></blockquote><h2 id="1c13" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">树木、随机森林、助推和装袋</h2><p id="d24a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">对于涉及将预测空间分割成简单区域的回归和分类，树或决策树都是有用和直接的方法。</p><p id="84b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">典型的决策树是上下颠倒的<em class="nc">表示树叶在树的底部。预测器空间被分割的点被称为<em class="nc">内部节点</em>，并且<em class="nc">叶节点</em>或<em class="nc">终端节点</em>是给出预测的节点。连接节点的线段称为<em class="nc">分支</em>。</em></p><p id="cf82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于预测，我们采取一种<em class="nc">自上而下的</em>(在第一个点上所有的观察值都属于一个区域)<em class="nc">、贪婪的</em>(在特定步骤中进行最佳分割)<em class="nc"> </em>的方法，称为递归二元拟合。</p><p id="3e81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有像<a class="ae ky" href="https://en.wikipedia.org/wiki/Decision_tree_pruning" rel="noopener ugc nofollow" target="_blank">树修剪</a>这样的策略，通过砍掉一些树枝得到一个小的子树来解决树的过度拟合问题。</p><p id="9fc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于分类问题，我们或者使用基尼指数，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/61b2e3b57a15c032227c81db14d6413b.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*E_eCMjU16rc2XPBirpr-Gw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="465d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者熵</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/dce27362e9b1e089dedaeba7ffee1caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*DBjSH75j1qgcKK2DWXV8CA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="649e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来表示节点的纯度，其中 Pmk 是来自第 k 类的第 m 个区域中的样本的比例。</p><p id="a0c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树仍然受到高方差的影响，与其他监督方法相比没有竞争力。因此，我们引入随机森林助推和装袋。</p><p id="009b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">装袋</strong></p><p id="381e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Bagging 是统计学习方法中减少方差的通用方法。核心思想是对一组观察值进行平均会减少方差。因此，我们对数据进行了多次随机采样，对于每个样本，我们构建了一个树，并对所有预测进行平均，以给出一个低方差的结果。</p><p id="839b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机森林</strong></p><p id="1153" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当在袋装树的集合中，从具有总共 m 个预测因子(k &lt; m)的每棵树中随机选择固定的 k 个预测因子，则袋装成为随机森林。</p><p id="60a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样做是因为大多数装袋的树看起来差不多一样。因此，单个包树的预测将是高度相关的。因此，我们推论的方差不会有太大的减少。随机森林可以被认为是去相关袋装树的过程。</p><p id="e983" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">增压</strong></p><p id="8afd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Boosting 方法是一种缓慢学习的统计方法，其中分类器是在修改的数据集<em class="nc">上顺序学习的</em>。在决策树的上下文中，每棵树都是使用来自先前树的信息来生长的。这样，我们就不需要一棵大树了。</p><h2 id="97d7" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">无监督学习</h2><p id="8872" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">以上所有方法都有某种形式的带注释的数据集。但是，当我们想要在没有任何注释的情况下学习数据中的模式时，无监督学习就出现了。</p><p id="c411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无监督学习最广泛使用的统计方法是<em class="nc"> K-Means 聚类。</em>我们在数据集中随机选取 K 个点，并根据它们与 K 个随机点的接近程度，将所有其他点映射到 K 个区域中的一个。然后，我们将 K 个随机点改变为这样形成的簇的质心。我们这样做，直到我们观察到每次迭代后形成的集群中的变化可以忽略不计。</p><p id="b502" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有其他像无监督学习中的 PCA 这样的技术被大量使用，但是现在，我们就到此为止。</p><p id="e97e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">接下来:</strong> <a class="ae ky" rel="noopener" target="_blank" href="/introduction-to-artificial-neural-networks-5036081137bb"> <strong class="lb iu">人工神经网络简介</strong> </a></p></div></div>    
</body>
</html>