<html>
<head>
<title>Traffic Sign Detection using Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于卷积神经网络的交通标志检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/traffic-sign-detection-using-convolutional-neural-network-660fb32fe90e?source=collection_archive---------5-----------------------#2019-09-01">https://towardsdatascience.com/traffic-sign-detection-using-convolutional-neural-network-660fb32fe90e?source=collection_archive---------5-----------------------#2019-09-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="95b4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们将建立一个 CNN 模型，以检测交通标志。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/25cf1453fee3177aaee07f048622a16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*ePgozv3WJymCYHUQdKHELw.jpeg"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">CNN Model</figcaption></figure><p id="90df" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你想从事计算机视觉领域的职业，学习卷积神经网络或 ConvNets 或 CNN 是非常重要的。CNN 有助于直接在图像上运行神经网络，并且比许多深度神经网络更有效和准确。与其他模型相比，ConvNet 模型在图像上的训练更容易、更快。</p><p id="057f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你不熟悉 ConvNet 的基础知识，你可以从<a class="ae ln" href="https://medium.com/@sdoshi579/convolutional-neural-network-learn-and-apply-3dac9acfe2b6" rel="noopener">这里</a>学习。</p><p id="92cb" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用<code class="fe lo lp lq lr b">keras</code>包来建立 CNN 模型。</p><h1 id="350e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">获取数据集</h1><p id="5818" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">德国交通标志检测数据集在此提供<a class="ae ln" href="http://benchmark.ini.rub.de/" rel="noopener ugc nofollow" target="_blank">。该数据集由 43 个不同类别的 39209 幅图像组成。这些图像在这些类别之间分布不均匀，因此该模型可以比其他类别更准确地预测一些类别。</a></p><p id="9a87" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以用各种图像修改技术填充数据集，如旋转、颜色失真或模糊图像。我们将在原始数据集上训练模型，并将查看模型的准确性。然后，我们将添加更多的数据，使每个类均匀，并检查模型的准确性。</p><h1 id="71ca" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据预处理</h1><p id="cfac" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">CNN 模型的局限性之一是它们不能在不同维度的图像上进行训练。因此，数据集中必须有相同维度的图像。</p><p id="a3b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将检查数据集的所有图像的尺寸，以便我们可以将图像处理成具有相似的尺寸。在该数据集中，图像具有从 16*16*3 到 128*128*3 的非常动态的尺寸范围，因此不能直接传递给 ConvNet 模型。</p><p id="cda6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们需要将图像压缩或插值成一维图像。不，为了压缩大部分数据，不要过度拉伸图像，我们需要确定两者之间的维度，并保持图像数据的最大准确性。我决定用 64*64*3 的尺寸。</p><p id="211a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们将使用<code class="fe lo lp lq lr b"><a class="ae ln" href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_setup/py_intro/py_intro.html" rel="noopener ugc nofollow" target="_blank">opencv</a></code>包将图像转换成给定的尺寸。</p><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="a692" class="mt lt iq lr b gy mu mv l mw mx">import cv2</span><span id="0073" class="mt lt iq lr b gy my mv l mw mx">def resize_cv(img):<br/>    return cv2.resize(img, (64, 64), interpolation = cv2.INTER_AREA)</span></pre><p id="0039" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><code class="fe lo lp lq lr b">cv2</code>是<code class="fe lo lp lq lr b">opencv</code>的一个包。<code class="fe lo lp lq lr b"><a class="ae ln" href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_geometric_transformations/py_geometric_transformations.html?highlight=resize#scaling" rel="noopener ugc nofollow" target="_blank">resize</a></code>方法将图像变换到给定的维度。在这里，我们将图像转换为 64*64 的尺寸。插值将定义您想要使用哪种类型的技术来拉伸或压缩图像。Opencv 提供了 5 种类型的插值技术，基于它们用来评估结果图像的像素值的方法。技法有<a class="ae ln" href="https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#resize" rel="noopener ugc nofollow" target="_blank"> </a> <code class="fe lo lp lq lr b"><a class="ae ln" href="https://docs.opencv.org/2.4/modules/imgproc/doc/geometric_transformations.html#resize" rel="noopener ugc nofollow" target="_blank">INTER_AREA, INTER_NEAREST, INTER_LINEAR, INTER_CUBIC, INTER_LANCZOS4</a></code>。我们将使用<code class="fe lo lp lq lr b">INTER_AREA</code>插值技术，它更适合图像抽取，但对于外推技术，它类似于<code class="fe lo lp lq lr b">INTER_NEAREST</code>。我们可以使用<code class="fe lo lp lq lr b">INTER_CUBIC</code>,但是它需要很高的计算能力，所以我们不会使用它。</p><h1 id="4cfd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据加载</h1><p id="4017" class="pw-post-body-paragraph kr ks iq kt b ku mk jr kw kx ml ju kz la mm lc ld le mn lg lh li mo lk ll lm ij bi translated">上面我们学习了如何预处理图像。现在，我们将加载数据集，并在决定的维度中转换它们。</p><p id="3425" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该数据集总共包括 43 个类。换句话说，该数据集中有 43 种不同类型的交通标志，每个标志都有自己的文件夹，由不同大小和清晰度的图像组成。数据集中总共有 39209 幅图像。</p><p id="74e2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以绘制不同交通标志的图像数量直方图。</p><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="3d1e" class="mt lt iq lr b gy mu mv l mw mx">import seaborn as sns<br/>fig = sns.distplot(output, kde=False, bins = 43, hist = True, hist_kws=dict(edgecolor="black", linewidth=2))<br/>fig.set(title = "Traffic signs frequency graph",<br/>        xlabel = "ClassId",<br/>        ylabel = "Frequency")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ebcf21a02fd6d4a35165932019f74435.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*dAA0f-vo3_Ifdlp6bVsmBw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk">Traffic signs frequency graph</figcaption></figure><p id="cf7c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ClassId 是为每个唯一的交通标志指定的唯一 Id。</p><p id="ee13" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">正如我们从图表中看到的，数据集并不包含每个类别的等量图像，因此，该模型在检测某些交通标志时可能会比其他交通标志更准确。</p><p id="d8c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以通过使用旋转或扭曲技术改变图像来使数据集一致，但我们将在其他时间这样做。</p><p id="c7e1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于数据集被划分到多个文件夹中，并且图像的命名不一致，我们将通过将(64*64*3)维中的所有图像转换到一个列表<code class="fe lo lp lq lr b">list_image </code>中，并将它相似的交通标志转换到另一个列表<code class="fe lo lp lq lr b">output</code>中来加载所有图像。我们将使用<code class="fe lo lp lq lr b">imread</code>读取图像。</p><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="8596" class="mt lt iq lr b gy mu mv l mw mx">list_images = []<br/>output = []<br/>for dir in os.listdir(data_dir):<br/>    if dir == '.DS_Store' :<br/>        continue<br/>    <br/>    inner_dir = os.path.join(data_dir, dir)<br/>    csv_file = pd.read_csv(os.path.join(inner_dir,"GT-" + dir + '.csv'), sep=';')<br/>    for row in csv_file.iterrows() :<br/>        img_path = os.path.join(inner_dir, row[1].Filename)<br/>        img = imread(img_path)<br/>        img = img[row[1]['Roi.X1']:row[1]['Roi.X2'],row[1]['Roi.Y1']:row[1]['Roi.Y2'],:]<br/>        img = resize_cv(img)<br/>        list_images.append(img)<br/>        output.append(row[1].ClassId)</span></pre><p id="9a09" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><code class="fe lo lp lq lr b">data_dir</code>是数据集所在目录的路径。</p><p id="979f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">数据集已加载，现在我们需要将其分为训练集和测试集。也在验证集中。但是如果我们直接分割，那么模型将不会得到所有交通标志的训练，因为数据集不是随机的。所以，首先我们将随机化数据集。</p><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="2381" class="mt lt iq lr b gy mu mv l mw mx">input_array = np.stack(list_images)</span><span id="0a2f" class="mt lt iq lr b gy my mv l mw mx">import keras<br/>train_y = keras.utils.np_utils.to_categorical(output)</span><span id="8d57" class="mt lt iq lr b gy my mv l mw mx">randomize = np.arange(len(input_array))<br/>np.random.shuffle(randomize)<br/>x = input_array[randomize]<br/>y = train_y[randomize]</span></pre><p id="b740" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们可以看到，我已经将输出数组转换为分类输出，因为模型将以这种方式返回。</p><p id="a3d0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，分割数据集。我们将以 60:20:20 的比例将数据集分别拆分为训练数据集、验证数据集和测试数据集。</p><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="2ec0" class="mt lt iq lr b gy mu mv l mw mx">split_size = int(x.shape[0]*0.6)<br/>train_x, val_x = x[:split_size], x[split_size:]<br/>train1_y, val_y = y[:split_size], y[split_size:]</span><span id="2532" class="mt lt iq lr b gy my mv l mw mx">split_size = int(val_x.shape[0]*0.5)<br/>val_x, test_x = val_x[:split_size], val_x[split_size:]<br/>val_y, test_y = val_y[:split_size], val_y[split_size:]</span></pre><h1 id="dfc2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">训练模型</h1><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="67c2" class="mt lt iq lr b gy mu mv l mw mx">from keras.layers import Dense, Dropout, Flatten, Input<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras.layers import BatchNormalization<br/>from keras.optimizers import Adam<br/>from keras.models import Sequential</span><span id="7f88" class="mt lt iq lr b gy my mv l mw mx">hidden_num_units = 2048<br/>hidden_num_units1 = 1024<br/>hidden_num_units2 = 128<br/>output_num_units = 43</span><span id="c947" class="mt lt iq lr b gy my mv l mw mx">epochs = 10<br/>batch_size = 16<br/>pool_size = (2, 2)<br/>#list_images /= 255.0<br/>input_shape = Input(shape=(32, 32,3))</span><span id="df7d" class="mt lt iq lr b gy my mv l mw mx">model = Sequential([</span><span id="8693" class="mt lt iq lr b gy my mv l mw mx">Conv2D(16, (3, 3), activation='relu', input_shape=(64,64,3), padding='same'),<br/> BatchNormalization(),</span><span id="a624" class="mt lt iq lr b gy my mv l mw mx">Conv2D(16, (3, 3), activation='relu', padding='same'),<br/> BatchNormalization(),<br/> MaxPooling2D(pool_size=pool_size),<br/> Dropout(0.2),<br/>    <br/> Conv2D(32, (3, 3), activation='relu', padding='same'),<br/> BatchNormalization(),<br/>    <br/> Conv2D(32, (3, 3), activation='relu', padding='same'),<br/> BatchNormalization(),<br/> MaxPooling2D(pool_size=pool_size),<br/> Dropout(0.2),<br/>    <br/> Conv2D(64, (3, 3), activation='relu', padding='same'),<br/> BatchNormalization(),<br/>    <br/> Conv2D(64, (3, 3), activation='relu', padding='same'),<br/> BatchNormalization(),<br/> MaxPooling2D(pool_size=pool_size),<br/> Dropout(0.2),</span><span id="30e9" class="mt lt iq lr b gy my mv l mw mx">Flatten(),</span><span id="800b" class="mt lt iq lr b gy my mv l mw mx">Dense(units=hidden_num_units, activation='relu'),<br/> Dropout(0.3),<br/> Dense(units=hidden_num_units1, activation='relu'),<br/> Dropout(0.3),<br/> Dense(units=hidden_num_units2, activation='relu'),<br/> Dropout(0.3),<br/> Dense(units=output_num_units, input_dim=hidden_num_units, activation='softmax'),<br/>])</span><span id="4c5c" class="mt lt iq lr b gy my mv l mw mx">model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])</span><span id="3f98" class="mt lt iq lr b gy my mv l mw mx">trained_model_conv = model.fit(train_x.reshape(-1,64,64,3), train1_y, epochs=epochs, batch_size=batch_size, validation_data=(val_x, val_y))</span></pre><p id="7fda" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们已经使用了<code class="fe lo lp lq lr b">keras</code>包。</p><p id="fc25" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于了解每一层的意义你<strong class="kt ir">可以阅读</strong> <a class="ae ln" href="https://medium.com/@sdoshi579/convolutional-neural-network-learn-and-apply-3dac9acfe2b6" rel="noopener"> <strong class="kt ir">这篇博客</strong> </a> <strong class="kt ir">。</strong></p><h1 id="43bc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">评估模型</h1><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="8233" class="mt lt iq lr b gy mu mv l mw mx">model.evaluate(test_x, test_y)</span></pre><p id="e2b4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该模型得到评估，你可以找到 99%的准确性。</p><h1 id="4f06" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预测结果</h1><pre class="kg kh ki kj gt mp lr mq mr aw ms bi"><span id="41a1" class="mt lt iq lr b gy mu mv l mw mx">pred = model.predict_classes(test_x)</span></pre><p id="39f3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您可以预测每个图像的类别，并验证模型的工作方式。</p><p id="839e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你可以在这里 找到整个<strong class="kt ir">工作</strong> <a class="ae ln" href="https://gist.github.com/sdoshi579/748fa76919ffe35a78fff24ce22b43a3" rel="noopener ugc nofollow" target="_blank"> <strong class="kt ir">代码。</strong></a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="na nb l"/></div></figure></div></div>    
</body>
</html>