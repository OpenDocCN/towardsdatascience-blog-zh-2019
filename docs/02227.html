<html>
<head>
<title>A couple tricks for using spaCy at scale</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大规模使用空间的几个技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-couple-tricks-for-using-spacy-at-scale-54affd8326cf?source=collection_archive---------2-----------------------#2019-04-13">https://towardsdatascience.com/a-couple-tricks-for-using-spacy-at-scale-54affd8326cf?source=collection_archive---------2-----------------------#2019-04-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f0bc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python 包 spaCy 是自然语言处理的一个很好的工具。下面是我在大型数据集上使用它所做的一些事情。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b7ba2bea75217f988c1f58c402d9d2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PVjBsuOi2gAMGS8q4K8AsQ.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Me processing text on a Spark cluster (artist’s rendition).</figcaption></figure><p id="9e87" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> <em class="lr">编辑:这个帖子现在已经过时了(看看一些评论)。自从我写这篇文章以来，SpaCy 引入了一个</em> </strong> <code class="fe ls lt lu lv b"><strong class="kx ir"><em class="lr">pipe</em></strong></code> <strong class="kx ir"> <em class="lr">方法，它做了我在这里所做的事情，只不过它不需要黑客，而且更快。建议大家都用那个方法。</em> </strong></p><p id="1a79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当我正在做的项目需要自然语言处理时，我倾向于首先求助于 SpaCy。Python 还有其他几个 NLP 包，<a class="ae lw" href="https://medium.com/activewizards-machine-learning-company/comparison-of-top-6-python-nlp-libraries-c4ce160237eb" rel="noopener">每个包都有自己的优缺点</a>。我通常发现 spaCy 很快，它的约定很容易学习。</p><p id="03a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我在 Spacy 遇到的一个困难是需要处理大量的小文本。例如，我最近有几十万个不同的标签，每个标签有 1 到 15 个单词，我决定用 word2vec 来比较它们。我对 spaCy 的单词矢量化很满意，但我还需要处理每个字符串，首先删除不相关的单词和词性，对一些标记进行词汇化，等等。花了很长时间。以下是我加快速度的两种方法。</p><h2 id="3c6e" class="lx ly iq bd lz ma mb dn mc md me dp mf le mg mh mi li mj mk ml lm mm mn mo mp bi translated">将所有内容作为一个文档处理，然后拆分成多个部分</h2><p id="5317" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">处理一堆独立记录的直观方法是分别处理它们。我开始遍历每条记录，并对每条记录调用 spaCy 的<code class="fe ls lt lu lv b">nlp</code>。我不知道处理每件事要花多长时间，因为我已经没有耐心了，只是在进程结束前就终止了它。所以我想了想，又读了一些文档，得出了这个结论:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="6557" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这将所有文本连接成一个字符串，每个原始文本由一些字符序列分隔，这些字符序列不太可能自然出现。我选择了三个管道，每边都有一个空间。然后我给<code class="fe ls lt lu lv b">nlp</code>打了一次电话。处理大约 25，000 条记录只需几秒钟。之后，我可以遍历标记列表，识别特殊的字符序列，并使用它将文档分割成多个部分。这些跨度包含单独的标记，并且具有文档的属性，例如单词向量。</p><h2 id="78bd" class="lx ly iq bd lz ma mb dn mc md me dp mf le mg mh mi li mj mk ml lm mm mn mo mp bi translated">使用火花</h2><p id="ccf8" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">这并不像听起来那么简单。根据我的经验，做 NLP 最昂贵的部分是加载语料库。一旦你把所有的东西都装上了，就只需要高效地查找你需要的信息了。如果你正在使用 spaCy 提供的最大的英语语料库，就像我一样，你将面临一些困难来把这些语料库放到你的 Spark 执行器上。</p><p id="cc81" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，spaCy 使用了与 PySpark 不同的 pickling 库，这导致在试图将语料库包含在用户定义的字段中时出错——无论是作为广播变量还是其他。无论如何，序列化庞大的语料库，然后将它转移到执行器，然后再反序列化，这不一定有意义，因为您可以将它加载到执行器本身。</p><p id="bd46" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这提出了一个新问题。加载语料库需要很长时间，所以你想尽量减少需要加载的次数。我发现，在将 Scikit-learn 模型部署到 Pyspark 时，遵循我过去使用的一个实践<a class="ae lw" rel="noopener" target="_blank" href="/deploy-a-python-model-more-efficiently-over-spark-497fc03e0a8d">非常有用:将记录分组到一个相当长的记录列表中，然后调用列表中的一个 UDF。这允许你做一次昂贵的事情，代价是必须连续做一小组相对便宜的事情。</a></p><p id="7ce2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的意思是:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mv mw l"/></div></figure><p id="a92b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我将我的 spark 数据框随机分成 20 个部分，将我所有的文本收集到一个列表中。我的 UDF 从 spaCy 加载语料库，然后遍历文本进行处理。</p><p id="45ef" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">需要注意两件事:</p><ul class=""><li id="234a" class="mx my iq kx b ky kz lb lc le mz li na lm nb lq nc nd ne nf bi translated">我在一个循环中分别处理每个文档。我没有理由要那么做。我可以使用第一个技巧，将所有文档作为一个整体处理，然后拆分成多个部分。这两个技巧并不相互排斥。</li><li id="ea24" class="mx my iq kx b ky ng lb nh le ni li nj lm nk lq nc nd ne nf bi translated">出于我的目的，我只需要单词向量，所以这就是我返回的全部内容。我不知道 PySpark 是否很难将整个 spaCy 文档或 span 对象从执行者那里移回来。我从来没试过。无论如何，我发现使用 Spark 只获取自己需要的东西是一个好习惯，因为移动东西是很昂贵的。所以如果你只需要单词向量，写一个只返回这些向量的 UDF。如果您只需要命名实体或词类之类的东西，编写一个 UDF 来返回它们。如果您需要所有这些内容，请在 UDF 中将其分离出来，并作为单独的字段返回。</li></ul><p id="1f70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用这两种方法，处理大量文本对我来说变得更有效率。</p></div></div>    
</body>
</html>