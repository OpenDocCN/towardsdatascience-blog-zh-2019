# 神经网络:概括抽象

> 原文：<https://towardsdatascience.com/neural-networks-generalizing-abstractions-7f4574f1619a?source=collection_archive---------23----------------------->

![](img/eba0396c0e8fa54abc6900536044eb7b.png)

[https://unsplash.com/photos/XookgCxFw6Q](https://unsplash.com/photos/XookgCxFw6Q)

神经网络，只要给出足够多的例子，就能区分猫和狗。然而，当进行区分时，对网络参与的详细研究显示，视觉网络对 T2 纹理非常敏感，而人类看到的是 T4 的轮廓。如果把猫的图片换成猫的**形状**的绿叶图案，人类还是会识别出它是猫。神经网络见榕树。

总的来说，神经网络似乎从这些微妙的结构差异中获得了大部分性能，而从未理解复杂的关系或构想“整体”。人工智能看不到森林或树木——它们都只是“树叶”。*为了处理复杂的任务，就像寻找新的数学证明一样，神经网络将需要形成* ***抽象*** *，并且还需要一种方法在这些抽象*之间 ***概括*** *。那些是思想的轮廓。*

**想法气球**

考虑一个正在观看数百万个 YouTube 视频的神经网络。它看到一个人拿着一个棒球，让它落入另一只手里。在目睹了棒球的类似事件后，我们希望神经网络能够说“啊，所有这些棒球*在释放时都以抛物线方式*下落……”抛物线行为是一种*抽象*，当其条件满足时，可以预测未来。

然后，观看沙滩球的视频，我们希望网络得出结论:“哦，这些沙滩球与棒球类似地下落，尽管它们以大致可预测的方式不同……”神经网络将这两种行为联系起来，在它们之间进行归纳——“沙滩球和棒球都服从重力**。”**

最后，假设网络观看气球的视频。“哇！这些与沙滩球和棒球不同，但它们做的是一种类似的事情，*倒立*！”这种网络可以查看所有物理学家的数据，并找到新的理论。那么，我们怎么去呢？

**两步**

神经网络体系结构的主要范例是创建一个单一的整体网络，一端是输入数据，另一端是输出数据。然后，通过训练，网络被修剪下来。我们看到许多迹象表明，这是*而不是*构建智能的最佳方式。相比之下，彩票假说是一个奇妙而令人困惑的例子。

彩票的过程很简单，尽管*完全违背了*的直觉。像平常一样，从一个随机加权的网络开始。像平常一样训练网络。然后，看看每个神经元连接:它要么*开始*变弱，而*保持*变弱，要么*变强，要么它开始变强，并保持强或变弱。**只保留*开始弱*变强**的。没错。*去掉神经连接，使* ***开始*******保持*** *强*。你不会想要这些的。很奇怪。**

*然后，**重置**所有剩余神经元的权重*为你从*开始的随机值。是啊，把你刚刚学的东西都扔掉。而且，你**不能**使用一些*新的*随机权重——只有*原来的*随机权重才可以。现在，训练那个微小的网络，它将实现一个**更高的最终性能**，用**更少的训练**，只使用**十分之一的处理能力**。WTF？*

*显然，我们不知道自己在做什么，甚至盲目地磕磕绊绊也足以发现不可思议的进步。艾是一个*果园*低垂的果实。*

*神经网络架构的另一种模式可能会产生我们所寻求的抽象和概括:**生成**新的迷你网络，然后**合并**它们。怎么会？*

***假定翻译，假定共享状态***

*GANs 擅长获取一组数据，比如一个城市的照片，并将其转换为一组相关的数据——这个城市的一幅画，或者这个城市在秋天，而不是春天。如果我们创建一个*迷你网络*，其任务是**将**的‘棒球坠落’视频转换成‘沙滩球坠落’视频，我们就有了功能类比的存在证据。*

*如果在两种行为之间有一个有效的类比，那么这意味着两种情况共享一个隐藏的状态。抽象概念。在我们的例子中重力。要搞清楚这一点，另一个小型网络是必要的:找到一个分类器，将棒球事件和沙滩球事件归类为相同的事件，尽管棒球和沙滩球在 T2 的其他情况下被归类为不同的事件。起初，分类器不能准确地识别重力效应的存在。它只看到“棒球”、“沙滩球”和“落垒/沙滩球”。在排除除重力之外的所有因素之前，你需要在配对之间进行大量的归纳。(并且，隐藏状态*本身*可以被概括和抽象——例如，将流体动力学与电磁学联系起来……它变得元。)*

*因为每一个事件群，棒球-沙滩球-气球，都由一个迷你 GAN 联系起来，并且它的区别特征由一个迷你分类器识别，所有这些事件都变成了一个相关现象的 **GLOB** 。当遇到*新的*系列事件时，您只需要为*glob*的一个成员训练一个迷你 GAN，以评估新事件是否属于该 glob 的类型。(“如果游泳的鱼不会像棒球一样落下，它们可能也不会像沙滩球一样落下。”)*

*Globs 可以训练一个单一的、统一的网络，用于任意两个成员之间的翻译，尽管这可能只值得定期进行。我们需要一段时间来消化和整合新的思想，所以这种局限性并不奇怪。并且，整个 glob 共享相同的隐藏状态，该隐藏状态先前由每个小型分类器识别，因此超级分类器可以从所有这些分类器一起提取相同的结论。不过，随着每一个球的成长，它也需要重新训练。*

***结婚戒指***

*如果我们的小型网络可以缝合在一起，而不是针对整个特定实例组进行重新训练，我们将接近一个可行的抽象概括机器。这就是模糊的地方。*

*彩票假设的运作原理是，如果你从一个单一的整体网络和随机权重开始，那么该网络会有一些**子集**(…并且*你想要哪个*子集是特定于随机权重的——因此，你必须将修整后的网络返回到*原始的*随机权重，**而不是**新随机权重)，这将比任何其他子集训练得更快，结果更好。奇怪的是，精简到那个神奇网络的方法是只挑选“改进最多的”神经连接。*

*我认为，在我们所有公认的智慧之外，可能有一个相对简单的缝合迷你网络的过程。*

*以猫的照片为例-在网络的每一层，由于较小要素的存在，较大要素被识别出来。小花絮分层组合，反映了现实的组成性质。然而，图像的某些方面在神经元之间以模糊不清的形式传输，只有在网络的某个更高层才会变得清晰。也有这个*完形*在起作用。*

*如果我们想要结合许多小型网络，我们必须提取每个小型网络使用的组合特征——如果这些特征中的一些被多个小型网络共享，我们实现一些压缩。而且，我们必须保留那些*模糊的东西*，它们只有在更高的层次才会变得清晰。如果我们把这些模糊的东西缝合在一起，找到一种方法来重叠它们，我们就有了一个超级网络**而不需要重新训练**，它可以形成抽象，并在这些抽象之间进行概括。我想再买一张乐透彩票…*