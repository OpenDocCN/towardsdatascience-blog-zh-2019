<html>
<head>
<title>Review: DRRN — Deep Recursive Residual Network (Super Resolution)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:DRRN——深度递归残差网络(超分辨率)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994?source=collection_archive---------17-----------------------#2019-02-03">https://towardsdatascience.com/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994?source=collection_archive---------17-----------------------#2019-02-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5947" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">多达 52 个卷积层，具有全局和局部剩余学习，性能优于 SRCNN、FSRCNN、ESPCN、VDSR、DRCN 和 RED-Net。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/82fd7429b97e290f9b1cec59f6bcad65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xWk76SuPdcF2Spa3"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Digital Image Enlargement, The Need of Super Resolution</strong></figcaption></figure><p id="af8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这个故事里，<strong class="ky ir"> DRRN(深度递归剩余网络)</strong>是回顾。用<strong class="ky ir">全局残差学习(GRL) </strong>和<strong class="ky ir">多路径模式局部残差学习(LRL) </strong>，加上<strong class="ky ir">递归学习</strong>在增加深度的同时控制模型参数，最多可以达到 52 层。DRRN 明显优于最先进的方法，如<a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4"> FSRCNN </a>、<a class="ae mb" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener"> ESPCN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>、<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>。并且发表在<strong class="ky ir"> 2017 CVPR </strong>上<strong class="ky ir"> 100 多篇引用</strong>。(<a class="mc md ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----dca4a35ce994--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="b637" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">概述</h1><ol class=""><li id="847f" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nk nl nm nn bi translated"><strong class="ky ir"> DRRN 架构</strong></li><li id="a30c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir">残差单元的数量(U) &amp;递归块的数量(B) </strong></li><li id="1e9a" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir">消融研究</strong></li><li id="0930" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nk nl nm nn bi translated"><strong class="ky ir">结果</strong></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="2da1" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">1.DRRN 架构</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/c3eb06852f53e2581bc525765317d629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kaIN4zRChyhJBwqCqwlTaQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Comparison of Different Architectures</strong></figcaption></figure><ul class=""><li id="fd33" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="ky ir"> ResNet </strong> </a> : <strong class="ky ir">残差单位</strong>用于预测。对于每个残差单元，有使用两个卷积的<strong class="ky ir">身份映射</strong>(即跳过连接)和<strong class="ky ir">残差映射</strong>。</li><li id="5c13" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> <strong class="ky ir"> VDSR </strong> </a>:残差学习用于输入低分辨率(LR)图像和输出高分辨率(HR)图像之间。这是一个<strong class="ky ir">全局剩余学习(GRL) </strong>。剩余分支中叠加了 20 层 3×3 卷积。可获得 41×41 的大感受野。而<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>实际上是 DRRN 的一个特例。(后面会提到)</li><li id="d584" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> <strong class="ky ir"> DRCN </strong> </a>:还有一种可以当作 GRL 的跳接。在剩余分支，使用递归卷积，即所有卷积共享参数(绿色)。每个递归卷积的输出负责生成 HR 图像。并且所有的 HR 图像被轻量级地组合以获得最终的输出。</li><li id="7a74" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir"> DRRN </strong> : <strong class="ky ir"> GRL 由于在<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>中的有效性也被使用</strong>。在残差分支处，不使用<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>、<strong class="ky ir">中的递归卷积，而是通过使用残差块</strong>内的<strong class="ky ir">递归卷积来使用一堆残差块</strong>，如上所示。因此，<strong class="ky ir">这里介绍</strong>多路径局部残差学习(LRL)。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="0336" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated"><strong class="ak"> 2。残差单元的数量(U) &amp;递归块的数量(B) </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ca"><img src="../Images/e839d0d5be64bf975ef1d5871a0e321d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JgXY3POkQ_1ej0uzdDJJzQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Number of Residual Units (U)</strong></figcaption></figure><h2 id="154c" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">2.1.剩余单元数(<em class="ok"> U </em>)</h2><ul class=""><li id="94e8" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nx nl nm nn bi translated"><strong class="ky ir"> <em class="ol"> U </em> </strong> : <strong class="ky ir">递归块中剩余单元的数量</strong>。<em class="ol"> U </em> ={1，2，3}的例子如上图所示。</li><li id="8521" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">注意，当 U 增加时，参数<strong class="ky ir">没有增加。因为<strong class="ky ir">参数是共享的</strong>。</strong></li><li id="8c75" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">并且使用<strong class="ky ir">预激活剩余单元</strong>，即<strong class="ky ir"> BN-ReLU-Conv </strong>，而不是 Conv-BN-ReLU。这是建议在<a class="ae mb" rel="noopener" target="_blank" href="/resnet-with-identity-mapping-over-1000-layers-reached-image-classification-bb50a42af03e">预激活 ResNet </a>中对原<a class="ae mb" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>进行改进。</li></ul><h2 id="d125" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">2.2.递归块数(<em class="ok"> B </em>)</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/492529c21a08def1cbd4efc5fd7d0e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*aquj3F3q7iZXBoOLGvqbsg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">B=6, 6 Recursive Blocks (Left), U=3, 3 Residual Units in a Recursive Block (right)</strong></figcaption></figure><ul class=""><li id="19ef" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">上面显示了<em class="ol"> B </em>的含义，即网络中使用了多少个递归块。</li><li id="ff4d" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">因此，可以根据<em class="ol"> B </em>和<em class="ol"> U </em>计算 DRRN <em class="ol"> d </em>的深度(卷积层数):</li></ul><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="750b" class="ny mm iq oo b gy os ot l ou ov"><em class="ol">d</em>=(1+2×<em class="ol">U</em>)×<em class="ol">B</em>+1</span></pre><ul class=""><li id="eaef" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">如果<em class="ol"> U </em> =0，DRRN 变为<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>。</li><li id="5cc2" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">损失函数是标准 MSE:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/ea26bb8ce02bbdabc48a8fef05642604.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*daDX6XpP3FzkP4DwfDzslQ.png"/></div></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="8b6d" class="ml mm iq bd mn mo mp mq mr ms mt mu mv jw mw jx mx jz my ka mz kc na kd nb nc bi translated">3.<strong class="ak">消融研究</strong></h1><h2 id="820f" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">3.1.一些细节</h2><ul class=""><li id="26df" class="nd ne iq ky b kz nf lc ng lf nh lj ni ln nj lr nx nl nm nn bi translated"><strong class="ky ir">训练集</strong>:来自杨的 91 幅图像，来自伯克利分割数据集的 200 幅图像，共 291 幅图像。</li><li id="f9b4" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir">测试装置</strong>:装置 5、装置 14、BSD100 和 Urban100。</li><li id="9c85" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir">数据增加</strong> : <strong class="ky ir">翻转</strong>和<strong class="ky ir">旋转</strong>版本，即 7 个附加增加版本。<strong class="ky ir">比例</strong>放大也用于不同的比例(×2、×3 和×4)。</li><li id="c734" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">使用跨距为 21 的 31×31 面片。小批量是 128 个。</li><li id="f4cb" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">每个卷积层有 128 个滤波器，大小为 3×3。</li><li id="0c66" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">在<em class="ol"> d </em> =20 的情况下，用 2 个 Titan X GPUs 训练 4 天。</li><li id="2718" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">双三次插值在进入网络之前首先被应用。</li><li id="d515" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">仅亮度分量。</li><li id="9efa" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">图像边界附近的像素在评估前被裁剪。</li></ul><h2 id="e131" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">3.2.硼和铀的研究</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/7d7beb865aa7109a1eb422e7492d631b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*Gf8N99gmg4Xjwbje3_Ryyw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Various combinations of <em class="ok">B</em> and <em class="ok">U (scaling factor </em>×3 on Set5)</strong></figcaption></figure><ul class=""><li id="d7d9" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated"><strong class="ky ir">通过将一个参数固定为 3，并将另一个参数从 1 改为 4 </strong>，上图显示<strong class="ky ir">增加<em class="ol"> B </em>或<em class="ol"> U </em> </strong> <strong class="ky ir">会导致更深的型号</strong> <strong class="ky ir">并获得更好的性能</strong>，这表明更深还是更好。</li><li id="560c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir">只要深度相近</strong>，型号就有可比性，如 B2U3 ( <em class="ol"> d </em> = 15，k = 784K)和 B3U2 ( <em class="ol"> d </em> = 16，k = 1,182K)分别达到 33.76 和 33.77 dB。(<em class="ol">k</em>= #参数)</li><li id="a566" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir">通过固定一个参数为 1，改变另一个参数来构造</strong>与<em class="ol"> d </em> = 52 的网络，我们可以得到 B1U25 ( <em class="ol"> k </em> = 297K)和 b17u 1(<em class="ol">k</em>= 7375k)。对于 B1U25，只有一个具有 25 个剩余单元的递归块被递归学习。对于 B17U1，堆叠 17 个递归块，无需任何递归学习。</li><li id="c93e" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">还构造了 B3U8 ( <em class="ol"> d </em> = 52，<em class="ol">k</em>= 1182k)。</li><li id="2a7c" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">所有的 B17U1、B3U8、B1U25，具有<em class="ol"> d </em> =52，具有相似的性能。</li><li id="6d03" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir"> B1U25 使用的参数要少得多。因此，它被视为最佳模型。</strong></li></ul><h2 id="2716" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">3.3.DRRN 变体</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/91508c3f1f91ea6d3393d4a622060f18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*a69__itV3TWDY0KTcyeGDA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">DRRN Variants (NS: No Sharing of Weights, C: Chained, Not Multi-path)</strong></figcaption></figure><ul class=""><li id="494f" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated"><strong class="ky ir"> DRRN_NS_C </strong>:有 LRL 但没有多路径递归学习，33.92dB</li><li id="6151" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir"> DRRN_NS </strong> : DRRN 但无重量分担，33.97dB。</li><li id="5fa1" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir"> DRRN_C: </strong> DRRN 但不使用多径，仅在剩余分支处使用链式卷积，33.95dB。</li><li id="c8d4" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir"> DRRN </strong> : 33.99dB，这说明所有组件对改善结果都很重要。</li></ul><h1 id="41fd" class="ml mm iq bd mn mo oz mq mr ms pa mu mv jw pb jx mx jz pc ka mz kc pd kd nb nc bi translated">4.结果</h1><h2 id="70ba" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">4.1.与最先进模型的比较</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/5758fd6b27c6c6fe2d80ede694c2f97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i91TxxetithXjtOKsPQlSA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Benchmark Results Using PSNR and SSIM</strong></figcaption></figure><ul class=""><li id="8cca" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">DRRN_B1U9 ( <em class="ol"> d </em> = 20，<em class="ol"> k </em> = 297K):与<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>深度相同，但参数更少。</li><li id="b18b" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">在所有数据集和缩放因子中，DRRN_B1U9 和 DRRN_B1U25 都优于所有方法，包括<a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>。</li><li id="ce40" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">特别是在 Urban100 数据集上，DRRN 大幅领先所有方法。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/c3269039348bb70eed627d23e633cb85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*So8fiLtCr_gXs9_QQ94LbQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Benchmark Results Using Information Fidelity Criterion (IFC) metric</strong></figcaption></figure><ul class=""><li id="4d4a" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">还评估与感知分数相关的度量<strong class="ky ir">信息保真度标准(IFC) </strong>。</li><li id="60dc" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">这里，<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>由作者重新实现，使用 BN。(原<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>不使用 BN。)</li><li id="cfb4" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated">DRRN 仍然优于所有方法。</li><li id="8dcf" class="nd ne iq ky b kz no lc np lf nq lj nr ln ns lr nx nl nm nn bi translated"><strong class="ky ir"> 20 层 B1U9 DRRN 在 Titan X GPU 上处理 288×288 图像需要 0.25 秒</strong>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/e659c698583664752f1f4ffe40f69846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*PBXktnRpGnYsQa02kVuf2w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PSNR for scale factor ×3 on Set5 and Set14</strong></figcaption></figure><ul class=""><li id="4967" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">结果表明，更深层次的模型是关键，DRRN 是最近三年所有模型中最好的，包括<a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4"> FSRCNN </a>、<a class="ae mb" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener"> ESPCN </a>、<a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>、<a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/39facdf0579c553c782a773c76184983.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*Gc2Hl4Pbl1-wc60aT_lADw.png"/></div></figure><ul class=""><li id="e3a5" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">就参数数量而言，DRRN 具有最高的 PSNR，同时由于权重的共享而具有相对较少的参数。</li></ul><h2 id="4fc3" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">4.2.定性结果</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/4814e60aca390b165598897955b7de6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tz5IUwB8UJQkqvc3be97Rw.png"/></div></div></figure><ul class=""><li id="4692" class="nd ne iq ky b kz la lc ld lf nu lj nv ln nw lr nx nl nm nn bi translated">DRRN 可以获得更清晰的边缘，而其他边缘则很模糊。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="75b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GRL 和 LRL 让我想起了<a class="ae mb" rel="noopener" target="_blank" href="/review-ror-resnet-of-resnet-multilevel-resnet-image-classification-cd3b0fcc19bb"> RoR </a>和<a class="ae mb" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener"> U-Net+ResNet </a>，在这些地方使用了长短跳线连接来提高精度。但是当然，也有不同之处，例如，对于 LRL，跳过分支在这里总是接受相同的输入，并且在剩余分支的卷积中共享权重。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="507b" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">参考</h2><p id="5d52" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf pj lh li lj pk ll lm ln pl lp lq lr ij bi translated">【2017 CVPR】【DRRN】<br/><a class="ae mb" href="http://cvlab.cse.msu.edu/pdfs/Tai_Yang_Liu_CVPR2017.pdf" rel="noopener ugc nofollow" target="_blank">通过深度递归残差网络的图像超分辨率</a></p><h2 id="50c0" class="ny mm iq bd mn nz oa dn mr ob oc dp mv lf od oe mx lj of og mz ln oh oi nb oj bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf pj lh li lj pk ll lm ln pl lp lq lr ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(们)(,)(我)(们)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(们)(还)(没)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(里)(去)(了)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测<br/></strong><a class="ae mb" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae mb" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae mb" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae mb" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae mb" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae mb" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a></p><p id="6582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">语义切分<br/></strong><a class="ae mb" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a>】<a class="ae mb" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae mb" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae mb" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物医学图像分割<br/></strong>[<a class="ae mb" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae mb" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae mb" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae mb" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae mb" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a></p><p id="3134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实例分割<br/>T32】[<a class="ae mb" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">实例中心</a> ] [ <a class="ae mb" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a></strong></p><p id="58de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">超分辨率<br/></strong><a class="ae mb" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a><a class="ae mb" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a><a class="ae mb" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a></p></div></div>    
</body>
</html>