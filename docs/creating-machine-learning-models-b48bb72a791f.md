# 创建机器学习模型

> 原文：<https://towardsdatascience.com/creating-machine-learning-models-b48bb72a791f?source=collection_archive---------22----------------------->

## 使用、测试和比较多种机器学习模型

![](img/f8eb3d60ade31bb6992e3f37d866d9d5.png)

Source: Pixabay

现在，我们已经成功地完成了每个数据科学项目中最困难的部分——数据清理和争论，我们将进入有趣的部分，建模！

虽然本文可以自成体系，但最好先经过 [**第 1 部分**](/exploratory-data-analysis-feature-engineering-and-modelling-using-supermarket-sales-data-part-1-228140f89298) 和 [**第 2 部分**](/feature-engineering-and-data-preparation-using-supermarket-sales-data-part-2-171b7a7a7eb7) 。

> 机器学习中的建模是一个迭代阶段，在这个阶段，数据科学家不断地训练和测试机器学习模型，以发现最适合给定任务的模型。

存在不同的机器学习模型，选择使用哪一个通常取决于手头的问题。没有机器学习模型对所有类型的问题都是最好的。因此，在这个阶段，您的工作是测试多个模型并微调参数，以挤出每一点准确性。

**注**:虽然我们通常追求性能更高的模型，但通常更明智、更好的选择是性能几乎和复杂模型一样好的更简单的模型。

为您的机器学习竞赛节省 *.0001* 增加的性能，因为在现实世界的大部分时间里，这些微小的差异并不重要，因为我们需要更简单的可解释模型。

有鉴于此，我们开始工作吧。从一个简单的基础模型开始总是好的，这样你就可以得到一个基线来衡量性能。

由于我们的数据集很小，我们将使用交叉验证，对于性能指标，我们将使用平均绝对误差。交叉验证有助于衡量我们模型的真实性能。

我们创建了一个交叉验证函数，如下所示。

# 对于回归任务，从简单的线性回归开始

我们从简单的线性回归开始建模，交叉验证并打印分数。

## 我们能调谐什么

线性回归很简单，因此没有太多要调整的参数。所以我们只需使用默认参数。

```
MAE Sccore:  0.42615411746170206
```

# 使用决策树

接下来，我们使用另一种简单但有效的算法，称为决策树

## 我们能调谐什么

决策树包含许多超参数，但我们可以快速调整的最重要的超参数是:

1.  **最大深度**:最大深度参数表示树的深度。树越深，模型就越复杂，它就能获取更多关于数据的信息。如果您的模型太简单，请尝试增加 max_depth，反之亦然。
2.  **min_samples_leaf** :这是一个叶节点所需的最小样本数。较高的叶片数导致更简单的模型，并可能有助于过拟合，而较小的叶片数可能导致欠拟合。
3.  **min_samples_split:** 这是分割内部节点所需的最小样本数。该参数增加导致更简单/受约束的模型。
4.  **max_features** :这是模型在分割一个节点之前考虑的特征数量。

```
MAE Sccore:  0.4280371717119684
```

# 使用 K-最近邻

接下来，我们尝试一种流行的基于距离的算法，称为 k-最近邻算法

## 我们能调什么

KNN 包含几个超参数。最重要的一项调整是:

1.  n_neighbors :这是投票时使用的邻居数量。数字越大，它就越精确(有时是这样)，但代价是速度。

```
**MAE Sccore:  0.431624658307474**
```

查看简单模型的 MAE(平均绝对误差)分数，我们看到决策树是迄今为止最好的。这将是我们的基本模型。任何表现更好的模型都将成为下一个要超越的底线。

现在，我们将从简单模型转向高级模型。许多高性能的机器学习模型通常基于集成。集成是一种结合多种机器学习算法的方法，以获得比任何单一算法都更好的预测性能。

下面我们将使用一些流行的合奏方法并比较它们的性能。

# 打包算法

# 随机森林

随机森林算法是 bagging 的一种流行而有效的实现，它从一个引导样本构建多棵树。它有很高的预测能力，开箱即用。

## 我们能调什么

随机森林包含许多超参数，但最重要的优化参数是:

1.  n_estimators :这是要创建的决策树的数量。正如我们可能猜测的那样，更多的树会产生更好的模型，但代价是更长的训练时间。
2.  **最大深度**:这是单棵树的最大深度。数字越大，树越简单，因此随机森林集合也越简单。
3.  **max_features:** 定义了允许分割的最大特征数。
4.  **min_samples_split:** 这是分割前叶节点所需的最小样本数。

```
MAE Sccore:  0.41512597643232896
```

# 额外的树

额外树算法是 bagging 的另一个流行的实现，它类似于随机森林，但构建其基础树的方式不同，并且通常更快。它还具有很高的预测能力，有时甚至超过随机森林。

## 我们能调什么

额外的树包含与随机森林相似的超参数。要调整的重要参数有:

1.  **n_estimators** :这是要创建的决策树的数量。正如我们可能猜测的那样，更多的树会产生更好的模型，但代价是更长的训练时间。
2.  **最大深度**:这是单棵树的最大深度。数字越大，树越简单，因此随机森林集合也越简单。
3.  **max_features:** 定义了允许分割的最大特征数。
4.  **min_samples_split:** 这是分割前叶节点所需的最小样本数。

```
MAE Sccore:  0.4131678014407999
```

# BAGGING 元估计量

sklearn 中的 bagging 估计器允许您从任何选择的模型中创建 Bagging 系综。也就是说，你可以从线性回归这样的单一模型中创建 bagging 系综，甚至是像随机森林或额外树这样的系综。

## 我们能调什么

这里要调整的重要超参数是:

1.  **base_estimator:** 这是在执行装袋时使用的估计值。bagging 元估计器从指定的基本估计器构建多个模型。
2.  **n_estimators** :这是要创建的基本估计数。较高数量的估计器将导致更好的模型，但是也以更长的训练时间为代价。
3.  **max_samples** :指定训练每个基本估计器的最大样本数。
4.  **max_features** :这是训练每个基本估计器时从数据集中提取的最大特征数

```
MAE Sccore:  0.4045400489482442
```

从误差指标来看，我们的下一个最佳模型是 bagging 回归器，它基于额外的树回归器。这成为我们新的基线模型。

接下来，我们来尝试一些 boosting 算法。

# 助推算法

Boosting 是另一种流行且有效的集合技术。在 Boosting 中，多个模型被顺序训练。目标是训练比他们的前辈做得更好的模型。这意味着我们必须考虑以前的模型表现不佳的领域，并在这些领域进行改进。

## adaboost 算法

我们从称为 AdaBoost 的升压的普通实现开始。这是一种古老但广泛使用的 boosting 技术，在回归任务中表现良好，尽管众所周知它会过度拟合。

## 我们能调什么

这里要调整的重要超参数是:

1.  **n_estimators** :这是要使用的基本估计数。通常，值越高，性能越好。
2.  **learning_rate** :该参数控制训练权重的更新。它通常与 n 估计器一起调整。一个流行的经验法则是“当你将 n_estimator 增加 10 倍时，你也将学习速率降低 10 倍。
3.  **base_estimator** :指定要提升的基本模型。
4.  **max_depth** :指定基本估计值的深度。

```
MAE Sccore:  0.4243339946387083
```

## 梯度推进

另一种流行的提升算法是梯度提升。

## 我们能调什么

这里要调整的重要超参数是:

1.  **n_estimators** :这是要使用的基本估值器的数量。通常，值越高，性能越好。
2.  **max_depth** :指定基本估算器的深度。
3.  **min_samples_split** :指定分割一个内部节点所需的最小样本数。
4.  **max_features** :训练每个基本估计器时从数据集中提取的最大特征数

```
MAE Sccore:  0.4063934439500354
```

## XGBOOST

“当有疑问时，使用 XGBoost”是近年来 kaggle 平台上最流行的名言之一。

XGBoost 高效、快速，开箱即用。这是梯度增强的高级实现，确实非常有效。

## 我们能调什么

这里要调整的重要超参数是:

1.  **eta** :只是学习率的不同叫法。它有助于模型权重更新。
2.  **min_child_weight:这个**指定一个子节点中所需的所有特性的最小权重和。它主要用于控制过度拟合。
3.  **max_depth:** 该用于定义每个树节点的最大深度。深度越大，模型越复杂，反之亦然。
4.  **max_leaf_nodes:** 指定一棵树的最大叶子数。
5.  Gamma:Gamma 值指定进行分割所需的最小损失减少量。只有当产生的分裂给出损失函数的正减少时，节点才被分裂。这些值可能因损失函数而异，应该进行调整。
6.  **子样本**:指定为每棵树随机选择的特征部分。较低的值使算法更简单，并防止过度拟合。

```
MAE Sccore:  0.4099352064907469
```

## LIGHTGBM

LightGBM 是微软的 boosting 算法。这是最快的助推算法之一；高效的开箱即用，目前是大型结构化数据的首选。

## 我们能调什么

这里要调整的重要超参数是:

1.  **num_iterations** :指定要执行的增强迭代次数。
2.  **叶子数量**:指定一棵树要形成的叶子数量。**注意** : *在轻型 GBM 中，由于分裂发生在叶方向而不是深度方向，因此 num_leaves 必须小于 2^(max_depth，否则可能导致过拟合。*
3.  **min_data_in_leaf** :它是处理过拟合最重要的参数之一。
4.  **max_depth** :指定一棵树可以生长的最大深度。此参数的值非常高会导致过度拟合。
5.  **bagging_fraction** :用于指定每次迭代使用的数据的分数。该参数通常用于加速训练。

```
MAE Sccore:  0.4084352064907469
```

## CATBOOST

Catboost 是最近出现的 boosting 算法。它快速、高效且容易处理分类特征(它最好的特征之一)。

CatBoost 可以自动处理分类变量，不需要像其他机器学习算法那样进行大量的数据预处理

## 我们能调什么

这里要调整的重要超参数是:

1.  **loss_function:** 指定用于训练的度量。
2.  **迭代次数:**可以构建的最大树数。
3.  **learning_rate:** 指定用于减少梯度步长的学习率。
4.  **深度:**指定要创建的树的深度。

```
MAE Score: 0.4123613834195344
```

## **最终想法**

在这篇文章中，我们学习了如何获取预处理过的数据集，并尝试不同的机器学习模型。虽然本文基于回归任务，但是同样的步骤也可以应用于分类任务。这篇文章的一些要点是:

1.  选择一个好的验证策略。(小型数据集的交叉验证，大型数据集的拆分)
2.  从简单开始，得到一个基线模型。
3.  通过调整和测量性能来不断改进模型。

手动调整最佳模型后，更高级的技术是使用自动超参数搜索算法，如网格或随机搜索。

> 记住，尽可能让你的模型简单易懂。

如果你从这一系列的帖子中学到了什么，别忘了鼓掌和分享。

[在 GitHub 上链接到带有代码和解释的完整笔记本。](https://github.com/risenW/medium_tutorial_notebooks/blob/master/supermarket_regression.ipynb)

![](img/0864bceae82b36631903d1763ce643e2.png)

在 [**Twitter**](https://twitter.com/risingodegua) 上和我联系。

在 [**LinkedIn**](https://www.linkedin.com/in/risingdeveloper) 上与我联系。