<html>
<head>
<title>Line Follower Robot using CNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 CNN 的直线跟随机器人</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/line-follower-robot-using-cnn-4bb4f297c672?source=collection_archive---------8-----------------------#2019-04-03">https://towardsdatascience.com/line-follower-robot-using-cnn-4bb4f297c672?source=collection_archive---------8-----------------------#2019-04-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7272e0977583b89ff77cd06fdce8d1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*opeFd5syg5Rd3M6MrHjV1g.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image by author</figcaption></figure><p id="423c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本教程中，我们将学习如何制作一个线跟随机器人。虽然，有大量的线追随者教程，因为这个概念本身是相当古老的。然而，这里我们将学习如何使用卷积神经网络(CNN)来检测直线。基本上，我们将使用我们的 Raspberry Pi 相机以预定的间隔捕捉一系列图像，然后我们将使用预先训练的 CNN 来预测我们的机器人应该移动的方向，即向前、向右或向左。本教程需要以下内容:-</p><ol class=""><li id="45f6" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">树莓 Pi 板，</li><li id="569d" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">Pi 相机，</li><li id="2bfd" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">跳线</li><li id="9dcd" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">底盘、电机、轮胎</li><li id="cd02" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">电机控制 IC (L293d)</li></ol><p id="5d03" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我假设你已经知道如何使用你的 Raspberry Pi 的 GPIO 引脚来控制电机。如果你想提高你的技能，请浏览我之前的教程。此外，在继续下一步之前，请确保您的 Pi 上安装了 tensorflow 1.1 和 open CV。</p><p id="2ee5" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我将把本教程分为三个部分</p><ol class=""><li id="2bc2" class="ld le it kh b ki kj km kn kq lf ku lg ky lh lc li lj lk ll bi translated">为 CNN 捕捉图像。</li><li id="15d9" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">训练 CNN</li><li id="b860" class="ld le it kh b ki lm km ln kq lo ku lp ky lq lc li lj lk ll bi translated">在树莓派上部署 CNN</li></ol><h1 id="4c40" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">为 CNN 捕捉图像</h1><p id="f9ad" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">我们需要三组图像用于三种条件中的每一种，即向前、向左和向右，来训练我们的 CNN。一旦我们训练了我们的 CNN，它将能够预测机器人在哪个方向移动，然后我们可以相应地采取纠正措施。例如，如果生产线左转，CNN 将预测机器人相对于生产线向右移动，因此，我们应该向左移动机器人。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/a9ad24f7b6d6a201e4fa571ce26d1661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPpGae_GEaRCrmUU6qbn9Q.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Fig. depicting robot moving in various directions and the corresponding images captured by the camera</figcaption></figure><p id="6fb2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">可以有各种方法来创建用于训练 CNN 的数据集。在我的例子中，我首先建造了一个轨道。然后，我使用下面提到的代码以 0.5 秒的间隔捕捉一系列图像。然后你把机器人放在轨道上的不同位置，捕捉一系列图像。例如，在第一个图像中，我放置了机器人，这样我希望 CNN 检测到机器人应该向左移动。同样，你在赛道的不同位置重复这个过程。一旦完成，你可以重复这个过程向前和向右。一旦你捕捉到三个方向的图像，将它们分别放入分别命名为向前、向左和向右的文件夹中。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="be1e" class="nf lt it nb b gy ng nh l ni nj"># import the necessary packages<br/>from picamera.array import PiRGBArray<br/>from picamera import PiCamera<br/>import time<br/>import cv2<br/> <br/># initialize the camera and grab a reference to the raw camera capture<br/>camera = PiCamera()<br/>camera.resolution = (640, 480) # set the resolution<br/>camera.framerate = 32 # set the frame rate<br/>rawCapture = PiRGBArray(camera, size=(640, 480))<br/> <br/># allow the camera to warm up<br/>time.sleep(0.1)<br/> <br/># capture frames from the camera<br/>start = 1</span><span id="00c0" class="nf lt it nb b gy nk nh l ni nj">for frame in camera.capture_continuous(rawCapture, format="bgr", use_video_port=True):<br/>  # grab the raw NumPy array representing the image, then initialize the timestamp and occupied/unoccupied text<br/>  image = frame.array<br/>  # show the frame<br/>  cv2.imshow("Frame", image)<br/>  key = cv2.waitKey(1) &amp; 0xFF<br/>  cv2.imwrite(str(start) + ".jpg", image)<br/>  start = start + 1<br/> <br/>  # clear the stream in preparation for the next frame<br/>  rawCapture.truncate(0)<br/> <br/>  # if the `q` key was pressed, break from the loop<br/>  if key == ord("q"):<br/>    break<br/>  time.sleep(.5)</span></pre><h1 id="dbef" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">训练 CNN</h1><p id="1df8" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">您可以在 raspberry PI 本身或不同的更强大的系统上训练 CNN，然后保存训练好的模型，PI 可以读取该模型。下面是训练 CNN 捕捉图像的代码。</p><p id="ccf7" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">首先，我们导入必要的库。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="87c4" class="nf lt it nb b gy ng nh l ni nj"># import the necessary packages<br/>from keras.models import Sequential<br/>from keras.layers.convolutional import Conv2D, MaxPooling2D <br/>from keras.layers.core import Activation, Flatten, Dense<br/>from keras import backend as K<br/>from keras.preprocessing.image import ImageDataGenerator<br/>from keras.preprocessing.image import img_to_array<br/>from keras.optimizers import Adam<br/>from sklearn.model_selection import train_test_split<br/>from keras.utils import to_categorical<br/>from imutils import paths<br/>import numpy as np<br/>import argparse<br/>import random<br/>import cv2<br/>import os<br/>import matplotlib</span></pre><p id="60dc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后我们定义一个可以用来构建 CNN 的类<code class="fe nl nm nn nb b">LeNet</code>。这里你可以看到这个类有一个函数<code class="fe nl nm nn nb b">build </code>，它接受参数<code class="fe nl nm nn nb b">width</code>、<code class="fe nl nm nn nb b">height</code>、<code class="fe nl nm nn nb b">depth</code>和<code class="fe nl nm nn nb b">classes</code>。宽度和高度应该等于您将用作 CNN 输入的图像的宽度和高度。在我的例子中，我使用的是 28x28 的图像，因此宽度和高度分别是 28。深度定义输入图像中的通道数。单色的深度为 1，RGB 的深度为 3。类别定义了您希望 CNN 检测的不同类型图像的数量。在我们的例子中，我们想要区分 3 种类型的图像——前向、左向和右向。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="e0c3" class="nf lt it nb b gy ng nh l ni nj">class LeNet:<br/>  <a class="ae lr" href="http://twitter.com/staticmethod" rel="noopener ugc nofollow" target="_blank">@staticmethod</a><br/>  def build(width, height, depth, classes):<br/>    # initialize the model<br/>    model = Sequential()<br/>    inputShape = (height, width, depth)</span><span id="d280" class="nf lt it nb b gy nk nh l ni nj"># first set of CONV =&gt; RELU =&gt; POOL layers<br/>    model.add(Conv2D(20, (5, 5), padding="same",<br/>      input_shape=inputShape))<br/>    model.add(Activation("relu"))<br/>    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))</span><span id="caac" class="nf lt it nb b gy nk nh l ni nj"># second set of CONV =&gt; RELU =&gt; POOL layers<br/>    model.add(Conv2D(50, (5, 5), padding="same"))<br/>    model.add(Activation("relu"))<br/>    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))</span><span id="84c3" class="nf lt it nb b gy nk nh l ni nj"># first (and only) set of FC =&gt; RELU layers<br/>    model.add(Flatten())<br/>    model.add(Dense(500))<br/>    model.add(Activation("relu"))</span><span id="2a95" class="nf lt it nb b gy nk nh l ni nj"># softmax classifier<br/>    model.add(Dense(classes))<br/>    model.add(Activation("softmax"))</span><span id="132d" class="nf lt it nb b gy nk nh l ni nj"># return the constructed network architecture<br/>    return model</span></pre><p id="982d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">当您调用<code class="fe nl nm nn nb b">build </code>函数时，它将定义一个具有两个卷积层和一个两个密集层的神经网络。人们可以试验这些层的参数，或者甚至添加额外的层来提高模型的准确性。接下来，提供培训图像文件夹所在的路径。您必须已经创建了三个文件夹向前、向左和向右，并将上面捕获的相应图像放在各自的文件夹中。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="a73b" class="nf lt it nb b gy ng nh l ni nj">dataset = '/home/pi/Desktop/tutorials/raspberry/trainImages/' # please change this path<br/># initialize the data and labels<br/>print("[INFO] loading images...")<br/>data = []<br/>labels = []<br/> <br/># grab the image paths and randomly shuffle them<br/>imagePaths = sorted(list(paths.list_images(dataset)))<br/>random.seed(42)<br/>random.shuffle(imagePaths)</span><span id="7c2c" class="nf lt it nb b gy nk nh l ni nj"># loop over the input images<br/>for imagePath in imagePaths:<br/>    # load the image, pre-process it, and store it in the data list<br/>    image = cv2.imread(imagePath)<br/>    image = cv2.resize(image, (28, 28))<br/>    image = img_to_array(image)<br/>    data.append(image)</span><span id="9670" class="nf lt it nb b gy nk nh l ni nj"># extract the class label from the image path and update the<br/>    # labels list<br/>    label = imagePath.split(os.path.sep)[-2]<br/>    print(label)<br/>    if label == 'forward':<br/>        label = 0<br/>    elif label == 'right':<br/>        label = 1<br/>    else:<br/>        label =2<br/>    labels.append(label)</span></pre><p id="6960" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后，上面的代码获取子文件夹中每个图像的路径，存储在列表<code class="fe nl nm nn nb b">imagePaths</code>中，并对它们进行重排。在训练模型时，随机提供来自每个类的数据是必要的。此外，每个图像然后被读取、调整大小、转换成 numpy 数组并存储在<code class="fe nl nm nn nb b">data</code>中。现在，我们还必须给每个图像分配一个标签。为此，我们从每个图像的图像路径中提取文件夹名称。然后，我们比较文件夹名称是否为“转发”,我们指定标签为 0。类似地，标签 1 和 2 被分配给文件夹“右”和“左”中的图像。这就是为什么需要创建子文件夹，并将相似的图像放在不同的文件夹中。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="245c" class="nf lt it nb b gy ng nh l ni nj"># scale the raw pixel intensities to the range [0, 1]<br/>data = np.array(data, dtype="float") / 255.0<br/>labels = np.array(labels)<br/> <br/># partition the data into training and testing splits using 75% of<br/># the data for training and the remaining 25% for testing<br/>(trainX, testX, trainY, testY) = train_test_split(data,<br/>    labels, test_size=0.25, random_state=42)</span><span id="7992" class="nf lt it nb b gy nk nh l ni nj"># convert the labels from integers to vectors<br/>trainY = to_categorical(trainY, num_classes=3)<br/>testY = to_categorical(testY, num_classes=3)</span></pre><p id="b92c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，图像被归一化，使得每个像素的值在 0 和 1 之间。然后，将图像划分为训练集和测试集，以检查模型性能。在接下来的步骤中，我们使用 LeNet 类构建模型，根据训练数据训练模型，然后保存训练数据。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="bf27" class="nf lt it nb b gy ng nh l ni nj"># initialize the number of epochs to train for, initial learning rate,<br/># and batch size<br/>EPOCHS = 15<br/>INIT_LR = 1e-3<br/>BS = 32</span><span id="d0dc" class="nf lt it nb b gy nk nh l ni nj"># initialize the model<br/>print("[INFO] compiling model...")<br/>model = LeNet.build(width=28, height=28, depth=3, classes=3)<br/>opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)<br/>model.compile(loss="binary_crossentropy", optimizer=opt,<br/>    metrics=["accuracy"])<br/> <br/># train the network<br/>print("[INFO] training network...")<br/>H = model.fit(trainX, trainY, batch_size=BS,<br/>    validation_data=(testX, testY),# steps_per_epoch=len(trainX) // BS,<br/>    epochs=EPOCHS, verbose=1)<br/> <br/># save the model to disk<br/>print("[INFO] serializing network...")<br/>model.save("model")</span></pre><h1 id="b92d" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">部署 CNN</h1><p id="466d" class="pw-post-body-paragraph kf kg it kh b ki mq kk kl km mr ko kp kq ms ks kt ku mt kw kx ky mu la lb lc im bi translated">一旦我们训练了我们的模型，我们就可以在我们的 Pi 上部署它。你可以使用下面提到的代码根据 CNN 的预测来控制机器人的方向。首先，我们从导入必要的库开始。这里我们也使用了<code class="fe nl nm nn nb b">motor_control.py</code>文件，你可以在我之前的教程中找到。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="42dd" class="nf lt it nb b gy ng nh l ni nj"># import the necessary packages<br/>from keras.preprocessing.image import img_to_array<br/>from keras.models import load_model<br/>import numpy as np<br/>import cv2, time, sys, imutils, argparse<br/>import RPi.GPIO as GPIO<br/>from picamera.array import PiRGBArray<br/>from picamera import PiCamera<br/>import motor_control as mc</span></pre><p id="3abe" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，定义您将用来控制电机的 raspberryPi 的引脚。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="cdcc" class="nf lt it nb b gy ng nh l ni nj">GPIO.setmode(GPIO.BCM)</span><span id="9f82" class="nf lt it nb b gy nk nh l ni nj">#choose the GPIO pins for mptor control<br/>fwd1 = 23  # pin 16<br/>bwd1 = 24 #pin 18<br/>fwd2 = 16 # pin 36<br/>bwd2 = 20 # pin 38</span><span id="d4d9" class="nf lt it nb b gy nk nh l ni nj"># declare selected pin as output pin<br/>GPIO.setup(fwd1, GPIO.OUT)<br/>GPIO.setup(bwd1, GPIO.OUT)<br/>GPIO.setup(fwd2, GPIO.OUT)<br/>GPIO.setup(bwd2, GPIO.OUT)</span></pre><p id="4bdd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们定义一个函数<code class="fe nl nm nn nb b">control_robot</code>，它将根据模型的预测控制机器人的方向。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="aae2" class="nf lt it nb b gy ng nh l ni nj">#function to control direction of robot based on prediction from CNN<br/>def control_robot(image):<br/>  prediction = np.argmax(model.predict(image))<br/>  if prediction == 0:<br/>    print("forward")<br/>    mc.forward()<br/>  elif prediction == 2:<br/>    print("left")<br/>    mc.left()<br/>  else:<br/>    print("right")<br/>    mc.right()</span></pre><p id="9a85" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们初始化相机，并开始在预定的时间间隔抓取图像。这些图像中的每一个都经过预处理，类似于我们对训练图像所做的那样。然后，该图像被传递给控制机器人方向的功能<code class="fe nl nm nn nb b">control_robot</code>。</p><pre class="mw mx my mz gt na nb nc nd aw ne bi"><span id="dc2b" class="nf lt it nb b gy ng nh l ni nj">model = load_model("model")</span><span id="a397" class="nf lt it nb b gy nk nh l ni nj">if __name__ == "__main__":<br/>  try:<br/>    mc.stop()<br/>    # initialize the camera and grab a reference to the raw camera capture<br/>    camera = PiCamera()<br/>    camera.resolution = (640, 480)<br/>    camera.framerate = 32<br/>    rawCapture = PiRGBArray(camera, size=(640, 480))<br/>    # allow the camera to warmup<br/>    time.sleep(0.1)<br/>    # capture frames from the camera</span><span id="f004" class="nf lt it nb b gy nk nh l ni nj">for frame in camera.capture_continuous(rawCapture, format="bgr", use_video_port=True):<br/>     # grab the raw NumPy array representing the image, then initialize the timestamp<br/>     # and occupied/unoccupied text<br/>     image = frame.array<br/>     # show the frame<br/>     key = cv2.waitKey(1) &amp; 0xFF</span><span id="731d" class="nf lt it nb b gy nk nh l ni nj">     image = cv2.resize(image, (28, 28))<br/>     image = img_to_array(image)<br/>     image = np.array(image, dtype="float") / 255.0<br/>     image = image.reshape(-1, 28, 28, 3)<br/>     #cv2.imshow("Frame", image[0])</span><span id="1587" class="nf lt it nb b gy nk nh l ni nj">     control_robot(image)</span><span id="e5fe" class="nf lt it nb b gy nk nh l ni nj">     # clear the stream in preparation for the next frame<br/>     rawCapture.truncate(0)</span><span id="fed8" class="nf lt it nb b gy nk nh l ni nj">  except KeyboardInterrupt:<br/>    mc.stop()<br/>    GPIO.cleanup()<br/>    sys.exit()</span></pre><p id="a14f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果一切顺利，当您在 Raspberry Pi 上运行这段代码时，您将能够看到类似的结果。</p><figure class="mw mx my mz gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b4c64de877322c8bc4ed95651025cba0.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/1*t_RfAonDq-z790X9Wpdrdg.gif"/></div></figure><p id="0534" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我希望你喜欢这个教程。请随时评论和询问或提出一些建议。非常感谢 Adrian Rosebrock 关于 RaspberryPi 的精彩教程，我在这里以此为基础。做检查他的<a class="ae lr" href="https://www.pyimagesearch.com/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p></div></div>    
</body>
</html>