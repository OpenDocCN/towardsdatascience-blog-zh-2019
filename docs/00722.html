<html>
<head>
<title>Contextual Embeddings for NLP Sequence Labeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理序列标注的上下文嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/contextual-embeddings-for-nlp-sequence-labeling-9a92ba5a6cf0?source=collection_archive---------12-----------------------#2019-02-03">https://towardsdatascience.com/contextual-embeddings-for-nlp-sequence-labeling-9a92ba5a6cf0?source=collection_archive---------12-----------------------#2019-02-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cb24" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用于序列标记的上下文字符串嵌入</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/37d03cb827279a4327cdd3ff32f071b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yrR4NXBXQ3EWkZt8"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="6704" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">文本表示(即文本嵌入)是解决自然语言处理任务的一个突破口。起初，单个单词向量代表一个单词，即使在上下文中携带不同的含义。例如，“华盛顿”可以是一个地点、名称或州。“华盛顿大学”</p><p id="6c51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Zalando 发布了一个惊人的 NLP 库，flair，让我们的生活更轻松。它已经实现了他们上下文字符串嵌入算法和其他经典的和最新的文本表示算法。</p><p id="a9f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个故事中，您将通过一些示例代码理解用于序列标记的上下文字符串嵌入的架构和设计。</p><h1 id="1a3c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">建筑和设计</h1><p id="3dc0" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">总体设计是将一个句子传递给<strong class="lb iu">字符语言模型</strong>来检索<strong class="lb iu">上下文嵌入</strong>，这样<strong class="lb iu">序列标注模型</strong>就可以对实体进行分类</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e335eb4d78e8197b2e497a6f25a90aac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*remR_Hb5gtMTOeBOqbjdOw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Architecture and Design (Akbik et al., 2018)</figcaption></figure><h2 id="f6b1" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">语境嵌入</h2><p id="5624" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与经典的<a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">单词嵌入</a>不同，Akbik 等人将其称之为语境化的单词嵌入。换句话说，单词嵌入捕获了上下文中的单词语义，因此即使是同一个单词，在不同的上下文中也可以有不同的表示。你可以从语言模型(ELMo) 中找到<a class="ae ky" rel="noopener" target="_blank" href="/replacing-your-word-embeddings-by-contextualized-word-vectors-9508877ad65d">语境化的词向量(CoVe) </a>和<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">嵌入，以了解更多细节。Albik 等人将他们的嵌入命名为<strong class="lb iu">上下文字符串嵌入</strong>。</a></p><h2 id="8d69" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">字符语言模型</h2><p id="6122" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与其他模型不同，它基于字符级标记化，而不是单词级标记化。换句话说，它将把句子转换成字符序列，并通过语言模型来学习单词表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/97478dc9f8c4d14e1ac748dd66d6f47a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmYMAW0u69OkTAUrk25iyQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Contextual Embeddings of “Washington” (Akbik et al., 2018)</figcaption></figure><p id="a4ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以“Washington”为例，双向 LSTM 模型允许“Washington”从前一个单词(即 George)和后一个单词(即 birth)中检索信息，从而可以计算句子上下文中的向量。</p><p id="7df6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">向量由前向神经网络和后向神经网络级联而成。对于前向神经网络，将提取单词中最后一个字符(即“n”)之后的隐藏状态。单词中第一个字符(即“W”)前的隐藏状态将从反向神经网络中提取。</p><h2 id="d147" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">堆叠嵌入</h2><p id="597e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">与其他研究一样，Akbik 等人利用堆叠嵌入获得了更好的结果。堆叠嵌入意味着组合多个嵌入来表示一个单词。</p><p id="ae12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，Akbik 等人连接上下文嵌入和手套嵌入来表示用于序列标记的单词。</p><h2 id="5c56" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">序列标签</h2><p id="2c8b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">来自字符语言模型的上下文嵌入和手套嵌入被传递到双向 LSTM-CRF 架构，以解决<a class="ae ky" href="https://medium.com/@makcedward/named-entity-recognition-3fad3f53c91e" rel="noopener">命名实体识别(NER) </a>问题。</p><h1 id="1c0e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实验</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/fb87c9cff8959f9771c9b5b390cc33ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GWxWkuBZkOHFCNw4XdBwEQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Experiment Result between previous best result (Akbik et al., 2018)</figcaption></figure><h1 id="282b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">履行</h1><h2 id="5874" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">命名实体识别(NER)</h2><p id="527d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">只需要执行以下命令来加载预先训练的 NER 标记。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="50ac" class="mt lw it ni b gy nm nn l no np">from flair.data import Sentence<br/>from flair.models import SequenceTagger</span><span id="663d" class="mt lw it ni b gy nq nn l no np">tagger = SequenceTagger.load('ner')</span></pre><p id="e504" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，您可以简单地将句子传递给句子对象，然后执行预测。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="e3c4" class="mt lw it ni b gy nm nn l no np">sample_texts = [<br/>    "I studied in University of Washington.",<br/>]</span><span id="9f53" class="mt lw it ni b gy nq nn l no np">for text in sample_texts:<br/>  print('-' * 50)<br/>  print('Original Text')<br/>  print(text)<br/>  <br/>  print('NER Result')<br/>  sentence = Sentence(text)<br/>  tagger.predict(sentence)<br/>  for entity in sentence.get_spans('ner'):<br/>    print(entity)</span></pre><p id="9a1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c763" class="mt lw it ni b gy nm nn l no np">Original Text:<br/><strong class="ni iu">I studied in University of Washington.</strong><br/>NER Result:<br/><strong class="ni iu">ORG-span [4,5,6]: "University of Washington."</strong></span></pre><h2 id="1f82" class="mt lw it bd lx mu mv dn mb mw mx dp mf li my mz mh lm na nb mj lq nc nd ml ne bi translated">情感分类</h2><p id="d257" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">它和 NER 一样容易。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="453b" class="mt lw it ni b gy nm nn l no np">from flair.data import Sentence<br/>from flair.models import TextClassifier</span><span id="afdc" class="mt lw it ni b gy nq nn l no np">classifier = TextClassifier.load('en-sentiment')</span></pre><p id="f765" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将句子传递给预先训练的分类器</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="32b7" class="mt lw it ni b gy nm nn l no np">sample_texts = [<br/>    "Medium is a good platform for sharing idea",<br/>]</span><span id="e41f" class="mt lw it ni b gy nq nn l no np">for text in sample_texts:<br/>  print('-' * 50)<br/>  print('Original Text')<br/>  print(text)<br/>  <br/>  print('Classification Result')<br/>  sentence = Sentence(text)<br/>  classifier.predict(sentence)<br/>  print(sentence.labels)</span></pre><p id="5682" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="9f81" class="mt lw it ni b gy nm nn l no np">Original Text:<br/>Medium is a good platform for sharing idea <br/>Classification Result:<br/> [POSITIVE (0.7012046575546265)]</span></pre><h1 id="8e29" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">拿走</h1><p id="f97e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">要访问所有代码，你可以访问这个<a class="ae ky" href="https://colab.research.google.com/drive/1H6F5ZCO728_CT37_EEfAFhfbDrEF7nQc" rel="noopener ugc nofollow" target="_blank"> CoLab 笔记本</a>。</p><ul class=""><li id="4a68" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated">除了预训练的 flair 上下文嵌入，我们不仅可以应用经典的嵌入方法，如 GloVe、word2vec，还可以应用最新的嵌入方法，如 ELMo 和 BERT。你可以去看看这个<a class="ae ky" href="https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_4_ELMO_BERT_FLAIR_EMBEDDING.md" rel="noopener ugc nofollow" target="_blank">指南</a>做参考。</li><li id="08eb" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">我们还可以非常容易地实现堆叠嵌入。只需要很少的代码。可以访问这个<a class="ae ky" href="https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md" rel="noopener ugc nofollow" target="_blank">指南</a>进行参考。</li><li id="bc11" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">如果你想在你的数据上训练定制模型，这个<a class="ae ky" href="https://github.com/zalandoresearch/flair/blob/master/resources/docs/TUTORIAL_9_TRAINING_LM_EMBEDDINGS.md" rel="noopener ugc nofollow" target="_blank">指南</a>会对你有用。</li></ul><h1 id="9478" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="623b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Akbik A .，Blythe D. Vollgraf R. 2018。<a class="ae ky" href="http://alanakbik.github.io/papers/coling2018.pdf" rel="noopener ugc nofollow" target="_blank">用于序列标记的上下文字符串嵌入</a>。</p><p id="9e88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/zalandoresearch/flair" rel="noopener ugc nofollow" target="_blank">py torch 的天赋</a></p></div></div>    
</body>
</html>