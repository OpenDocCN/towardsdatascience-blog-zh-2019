<html>
<head>
<title>Predicting Workplace Productivity Using Employees’ Happiness Index</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用员工的幸福指数预测工作场所的生产力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-regression-models-bfd779a73a60?source=collection_archive---------17-----------------------#2019-02-11">https://towardsdatascience.com/training-regression-models-bfd779a73a60?source=collection_archive---------17-----------------------#2019-02-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c5dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">训练回归模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/de61f20d45eb63400e5b3f86ff89ba2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SmgY1m4HClo3w4bHJjPBCw.jpeg"/></div></div></figure><p id="dcf7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di"> Y </span>你已经观察到，在过去的几年里，快乐的员工是你公司的主要利润来源，在这些年里，你记下了所有员工的快乐指数和他们的工作效率。现在你有大量的员工数据躺在 excel 文件中，你最近听说<strong class="kt ir"> <em class="lw">“数据是新的石油。将会胜出的公司正在使用数学。”</em></strong>——<em class="lw">凯文·普兰克，安德玛创始人兼 CEO，2016。</em></p><p id="7a42" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">您想知道是否也可以通过某种方式匹配这些数据来预测新员工的生产力，这些数据基于他们的幸福指数。这样你就能更容易地找出效率最低的员工(然后假设解雇他们——只是假设)。如果是这样的话，干杯！🎉—因为这篇博文将向你介绍一种机器学习算法(回归)来构建这个 AI 应用！</p><h1 id="45c9" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">这篇博文的目的是…📚</h1><p id="2e44" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">提供训练回归模型的完整见解</p><h1 id="eee6" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">目标观众是… 🗣 🗣 🗣</h1><p id="7fd8" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">任何人都在寻找回归背后的数学的深入而易懂的解释</p><h1 id="fdd7" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">本教程结束时…🔚</h1><p id="0993" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">你将会对回归的本质有一个完整的了解😃你对 ML 的理解将会提高👍是的，你将能够识别出你效率最低的员工！</p><h1 id="8ee8" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">现在是路线图…..🚵</h1><p id="235e" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated"><strong class="kt ir">里程碑# 1 : </strong> <a class="ae mu" href="#fe1c" rel="noopener ugc nofollow"> <strong class="kt ir">什么是回归，它与分类有何不同？</strong>T13】</a></p><p id="303b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">里程碑# 2.1 : </strong> <a class="ae mu" href="#f95d" rel="noopener ugc nofollow"> <strong class="kt ir">训练回归模型——决定损失函数作为回归模型的评价指标</strong> </a></p><p id="0a8e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">里程碑# 2.2 : </strong> <a class="ae mu" href="#d1ed" rel="noopener ugc nofollow"> <strong class="kt ir">训练回归模型——梯度下降</strong> </a></p><p id="e200" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">里程碑# 2.3 : </strong> <a class="ae mu" href="#cd55" rel="noopener ugc nofollow"> <strong class="kt ir">训练回归模型——使用梯度下降</strong> </a>完成简单回归模型的遍历</p><p id="afa7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">里程碑# 3 : </strong> <a class="ae mu" href="#858f" rel="noopener ugc nofollow"> <strong class="kt ir">利用训练好的回归模型进行智能预测！</strong> </a></p><p id="c992" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">里程碑# 4: </strong> <a class="ae mu" href="#a711" rel="noopener ugc nofollow"> <strong class="kt ir">最后是结束语……</strong> </a></p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="59d9" class="lx ly iq bd lz ma nc mc md me nd mg mh jw ne jx mj jz nf ka ml kc ng kd mn mo bi translated">让我们开始吧！</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/4b0f6c71b6747d5cad5bba271b983a78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FcSrTgXwHvqxHdrHhrI_hw.jpeg"/></div></div></figure><h1 id="fe1c" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated"><strong class="ak">什么是回归，它与分类有何不同？</strong></h1><p id="c1c2" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">在定义回归之前，让我们先定义连续变量和离散变量。</p><h2 id="13ad" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">连续变量</h2><p id="3443" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">连续变量是可以从它的无限可能值集中取任何值的变量——可能值的数量简直是数不清的。</p><h2 id="a6ab" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated"><strong class="ak">离散变量</strong></h2><p id="db39" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">离散变量是可以从其有限的可能值集中取任何值的变量，可能值的数量是可数的。</p><h2 id="c12f" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">区分分类和回归</h2><p id="4cca" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">在上述讨论的背景下，我们现在可以清楚地区分分类和回归。分类是从一组有限的值(显然是我们在训练中得到的值)中预测一个值(即一个类)，而回归是从一组无限的可能值中预测一个值。例如，预测个人体重的模型是回归模型，因为其预测体重值有无限种可能性。一个人的体重可能是 12.0 千克或 12.01 千克或 12.21 千克，这种可能性是无限的，因此是一个回归模型。</p><p id="9109" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">举几个例子来进一步阐明分类的区别&amp;回归</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/e1b99c37ab48033671dde07501c1ee54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jHHsqGfSLzLrpWkfHSGtQg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Classification &amp; Regression Examples</figcaption></figure><blockquote class="nz oa ob"><p id="726c" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">里程碑 1 达成！<em class="iq">👍</em> </strong></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="ee4f" class="lx ly iq bd lz ma nc mc md me nd mg mh jw ne jx mj jz nf ka ml kc ng kd mn mo bi translated"><strong class="ak">训练回归模型——决定损失函数作为回归模型的评估指标</strong></h1><p id="0310" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">假设我们有一个员工幸福指数和员工生产力的训练数据集，将它们绘制成下图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/52326bba3cca10c99518f7e8beee84d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rH1FtqOohdfyNuJiKBmEYg.png"/></div></div></figure><p id="dbf5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">该图显示了训练数据的基本模式是两个变量之间的线性关系。因此，训练一个<em class="lw">广义</em>回归模型将对这个线性关系/函数建模，即<em class="lw">在许多可能的线</em>中找到一条最佳拟合线。在此之后，一个看不见的数据点(测试时间示例)将根据在训练阶段学习的线性函数/趋势预测其值。</p><h2 id="bb7e" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">训练回归模型的摘要概述</h2><p id="c567" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">假设我们已经知道一条<em class="lw">线</em>将很好地适合给定的数据集。为了对给定数据集的直线建模，我们现在需要找到梯度(m)和 y 轴截距(c)的最佳值。因此，在培训期间，我们试图学习 m 和 c 的最佳可能值。一旦我们有了这些值，我们就可以简单地将给定幸福指数(即 x)的值代入学习的等式，并预测其对应的员工生产率，即 y。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/41710370b40bf1e082c28a9c0dad47e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ieIfmBjN5H46h1VXaxz2aQ.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Weights/Thetas — Learning Parameters</figcaption></figure><h2 id="a9d0" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">如果我们简单地使用准确性作为评估标准来评估我们模型的性能会怎么样？</h2><p id="f00b" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">假设我们训练了一个回归模型，现在我们需要评估它在测试集(理想情况下是验证集)上有多好。因此，我们将所有验证集数据点(x 值)插入到线性函数中，并预测它们对应的 y 值。</p><p id="4019" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">使用准确性作为回归模型的评估标准将总是导致训练、验证和测试集的准确性为 0！为什么？因为实际上来说，预测值可以非常接近真实的连续值，但很难<em class="lw">与真实标签的</em>完全相等。因此，由于预测标签和真实标签不匹配，准确性将始终为零。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/a695eeead4d3d74b610fb3602a650624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01ckTJuG-5LoHTSW9u1xpg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Using Accuracy as Evaluation Metric for Regression</figcaption></figure><p id="dc90" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面的例子表明，虽然预测值接近实际值，但并不完全相等，精度完全为 0。因此，使用准确性作为评估回归模型的评估标准是不明智的。我们需要定义另一个评估指标来评估我们模型的性能。</p><h2 id="40fd" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">决定回归模型的评估标准</h2><p id="ef92" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">我们需要一个评估指标，以反映我们的训练模型的有效性，即如何推广。它不应该仅仅因为预测值和实际值不完全匹配就输出零值。相反，它应该能够以某种方式证明我们的最佳可能模型与理想模型(一个具有所有理想θ/权重的函数，但这在现实生活场景中几乎不可能)相比有多好。</p><blockquote class="oi"><p id="ff94" class="oj ok iq bd ol om on oo op oq or lm dk translated">定义 L1- <em class="os">损失函数</em>为评估度量</p></blockquote><figure class="ou ov ow ox oy kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/596b89288775b5fe7f8335b322859cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYhFTXPptVNhdqwFnvufog.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Notations for Loss Function</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/c32c87519ea15d2d4fe574ec5d440df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kTB4ce-4R0cNTICYFJzOcA.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">L1 — Loss Function</figcaption></figure><p id="e17b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">更小的损失值</strong> <br/>如果预测值与实际值之间的总差值相对较小，则总误差/损失将是更小的值，因此表示模型良好。</p><p id="e8d5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">更大的损失值</strong> <br/>如果实际值和预测值之间的差异很大，损失函数的总误差/值也会相对较大，这也意味着模型没有被很好地训练。</p><p id="8be1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">作为均方误差的损失函数</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/fee5d2f626e63520f0c32216b57d0667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCGSFjZYF9zWSlZ3HWgU_Q.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">L2 Loss Function</figcaption></figure><blockquote class="nz oa ob"><p id="32fc" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated">L2 损失/均方误差/二次损失—都是一样的</p></blockquote><p id="f337" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">训练回归模型的目标<br/> </strong>训练回归模型的目标是找到那些损失函数最小的权重值，即预测值和真实标签之间的差异尽可能最小。</p><p id="761d" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">为什么预测标签和真实标签永远不会完全匹配？<br/></strong></p><blockquote class="nz oa ob"><p id="1290" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">达到里程碑 2.1！<em class="iq">👍 👍1️⃣ </em> </strong></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="a7df" class="lx ly iq bd lz ma nc mc md me nd mg mh jw ne jx mj jz nf ka ml kc ng kd mn mo bi translated">很多吗？休息一下再回来！</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/6ae8e8bd8a74465691337b7741296217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ZwG8sUVTTwjKcIPshVsaA.jpeg"/></div></div></figure><h1 id="7abb" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">梯度下降算法</h1><p id="5ba9" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">到目前为止，我们假设我们已经有了回归模型的权重。但是，实际上是找到这些权重的最佳可能值的过程说明了损失值的最小化。这一发现过程就是众所周知的算法“梯度下降”发挥作用的地方，如下所述。</p><h2 id="faeb" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated"><strong class="ak">梯度下降——直觉</strong></h2><p id="12d8" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">假设你只是随机降落在一座山上，并且你是盲折叠。你的目标是尽快到达山脚。你会怎么做？一个可能的解决办法是向最陡的方向下降/移动，并在没有到达山脚时继续这样做。</p><p id="32d1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">下降时，你特别注意一些事情，以确保你确实到达了基点</strong></p><ol class=""><li id="5bf2" class="pc pd iq kt b ku kv kx ky la pe le pf li pg lm ph pi pj pk bi translated">因为你的目标是尽可能快地到达基点，所以你决定在开始的几个下降步骤中迈出更大的步伐。</li><li id="422e" class="pc pd iq kt b ku pl kx pm la pn le po li pp lm ph pi pj pk bi translated">因为你还想确保一旦你到达了更接近基地的地方，跳到任何上升点的机会就被最小化了。因此，你决定在后面的下降步骤中采取较小的跳跃，以确保在接近基点时不会偏离基点。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/bcddba50eeb1c692e2ec81abd298543c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TnkhYu4YIzteD1ltyg59Q.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Gradient Descent — Intuition</figcaption></figure><h2 id="cd03" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">梯度下降——数学观点</h2><p id="b6ad" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">训练回归模型的目的是通过以与我们下降到山的底部时相同的方式收敛函数，找到/学习将最小化 L2 损失的权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pr"><img src="../Images/5e71978f0bc13ea75feefe4f33625552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0gxWA7rXLAUO57ddVvxYg.png"/></div></div></figure><h2 id="153f" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated"><strong class="ak">定义梯度下降</strong>伪代码</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ps"><img src="../Images/c3a0bd6c10c37fdbda2c9b6c7a702524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JRi18oNxmBWYcPR_5WHKxg.png"/></div></div></figure><h2 id="0622" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated"><strong class="ak">深入探究 L2 损失的偏导数— 2.1.1 </strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pt"><img src="../Images/71e10fd5ca458f9b8ab2c1b63ad28cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thiHjr8URIBXWh5dpCAXnw.png"/></div></div></figure><h2 id="eed9" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated"><strong class="ak">更新权重值— 2.1.2 </strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pu"><img src="../Images/5a813b5afe05a03a8dc1cd0d3227fbf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eqTNI1yzyUbKtF2igDla4g.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Updating Weights to Minimize Loss</figcaption></figure><blockquote class="nz oa ob"><p id="de91" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">里程碑 2.2 达成！<em class="iq">👍 👍2️⃣ </em> </strong></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="cd55" class="lx ly iq bd lz ma nc mc md me nd mg mh jw ne jx mj jz nf ka ml kc ng kd mn mo bi translated"><strong class="ak">训练回归模型——对</strong>梯度下降的完整遍历</h1><p id="8a95" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">现在我们已经熟悉了梯度下降的工作原理，让我们借助几个例子来进一步阐明它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pv"><img src="../Images/7ddd06b7678d28dfd3e39e191342de45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AcpOePJ3TO8lZuRaboivvQ.jpeg"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Gradient Descent Walk-through</figcaption></figure><h2 id="2172" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">上面例子中的一些假设</h2><p id="925a" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">为了上面的例子，我们确实假设了一些事情。下图将这些假设与现实世界的情景进行了比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/7e048add40bde06be2d91ac26992c7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*91z_mKCzC8v3ty_dH57Cbg.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Example Assumptions</figcaption></figure><blockquote class="nz oa ob"><p id="fe15" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">达到里程碑 2.3！<em class="iq">👍 👍3️⃣ </em> </strong></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><blockquote class="nz oa ob"><p id="b8cc" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">这标志着您的第二个里程碑已经完成！<em class="iq">👍 ️👍</em> </strong></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="1185" class="lx ly iq bd lz ma nc mc md me nd mg mh jw ne jx mj jz nf ka ml kc ng kd mn mo bi translated">只要再多一点点，你就完成了！</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi px"><img src="../Images/ae8d44c9d19e5aa1db3702f0945bf70d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jlFJ0eB-pxAIO5wtK7FHqw.jpeg"/></div></div></figure><h1 id="b039" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated"><strong class="ak">使用训练好的回归模型进行智能预测！</strong></h1><p id="75d8" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">一旦我们在训练阶段训练/调整了权重，对给定测试示例的预测就非常简单了——只需计算训练权重和给定测试示例特征的点积。</p><p id="fd25" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下图进一步阐明了这一点…</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi py"><img src="../Images/ba5f53474baaf726312cb0535f8344f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aJ7oQpTDhEn0vHoR9K3qaw.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Generating Predictions at Run Time</figcaption></figure><blockquote class="nz oa ob"><p id="1b92" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">里程碑 3 实现！<em class="iq">👍 ️👍👍️ </em> </strong></p></blockquote></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h1 id="43f1" class="lx ly iq bd lz ma nc mc md me nd mg mh jw ne jx mj jz nf ka ml kc ng kd mn mo bi translated">转到结论部分…..</h1><h2 id="3405" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">1.调整学习率</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/00eb6c6c22391496c4af427e7af6a79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4v-RJ7xBO9G6eV8Ar2FLuA.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Learning Rate — Hyperparameter</figcaption></figure><h2 id="3074" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">2 <strong class="ak">。为什么在培训中使用 1 的附加功能</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qa"><img src="../Images/cc2445e0fde30753623d8fc90380c397.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TZ70pD70OjwOOQtNlooww.png"/></div></div><figcaption class="nv nw gj gh gi nx ny bd b be z dk">Additional Training Feature of 1's</figcaption></figure><h2 id="9fa4" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">3.<strong class="ak">梯度下降需要特征归一化</strong></h2><p id="b275" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">应在数据预处理步骤中对要素进行范围归一化。否则，训练预测很可能会受到具有极端正值/负值的特征的影响。由于特征值确实会影响其对应的导数，极值可能会使学习过程变得更长，或者函数可能不会很好地收敛到基点——学习变得困难。因此，将你的特征标准化通常是个好主意。</p><h2 id="c76e" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">4.用 L2 损失代替 L1 损失</h2><p id="9e9a" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">你可能会奇怪，为什么我们不利用 L1 损失，而不是 L2 损失。其原因是 L1 损失的导数在 X=0 处未定义，梯度下降假设损失函数在任何地方都可以微分。因此，我们通常不喜欢使用 L1 损失。</p><h2 id="407a" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">5.关于梯度下降收敛的一点注记</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/8ba8f1b908efa97dc5f53b3f27a6c6f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BbbPCPakgBJtu2hCKcXqdA.png"/></div></div></figure><blockquote class="nz oa ob"><p id="f15a" class="kr ks lw kt b ku kv jr kw kx ky ju kz oc lb lc ld od lf lg lh oe lj lk ll lm ij bi translated"><strong class="kt ir">达到里程碑 4！<em class="iq">👍 👍 👍 👍</em>T3】</strong></p></blockquote><h1 id="d40f" class="lx ly iq bd lz ma mb mc md me mf mg mh jw mi jx mj jz mk ka ml kc mm kd mn mo bi translated">不要再做什么聪明的事情了！</h1><p id="8995" class="pw-post-body-paragraph kr ks iq kt b ku mp jr kw kx mq ju kz la mr lc ld le ms lg lh li mt lk ll lm ij bi translated">你现在可以着手建立你自己的人工智能模型来预测你的员工的生产力！</p></div><div class="ab cl mv mw hu mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="ij ik il im in"><h2 id="4efb" class="ni ly iq bd lz nj nk dn md nl nm dp mh la nn no mj le np nq ml li nr ns mn nt bi translated">这篇博文到此为止！</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/673692a0527284c50a7bbfbd242d844f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*smBMNYJpILmpIu2cOW2Y8Q.jpeg"/></div></div></figure><p id="8662" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果您有任何想法、意见或问题，欢迎在下面评论或联系📞跟我上<a class="ae mu" href="https://www.linkedin.com/in/aisha-javed/" rel="noopener ugc nofollow" target="_blank">T5】LinkedInT7】</a></p></div></div>    
</body>
</html>