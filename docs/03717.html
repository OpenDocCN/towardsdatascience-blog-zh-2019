<html>
<head>
<title>A Summary of Machine Learning and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/table-of-contents-689c8af0c731?source=collection_archive---------19-----------------------#2019-06-12">https://towardsdatascience.com/table-of-contents-689c8af0c731?source=collection_archive---------19-----------------------#2019-06-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="abaf" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习和深度学习之旅</h2><div class=""/><div class=""><h2 id="8ee8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">机器学习和深度学习模型和算法的映射和总结</h2></div></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><p id="5291" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这一系列博客将涵盖机器学习和深度学习从理论到实践的主题。目前市场上有许多优秀的教科书，包括模式识别和机器学习，统计学习的元素，深度学习等。然而，似乎那些作者喜欢直接跳到一个算法的结论，而跳过一步一步的解释和说明。因此，对于大一新生来说，那些书不容易消化。还有，那些书没有涉及如何在 R 或 Python 等统计软件中实现算法。我认为要很好的掌握统计模型和算法，理论和数学很重要，实现也很重要。因此，我想在理论和实践之间架起一座桥梁。</p><p id="0cc6" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">经过多年的学习，我认为所有这些方法都是相互联系的，而不是孤立的。当一个传统的方法显示出一些局限性时，那么一个新的方法就被引入来弥补这个局限性，从而进行改进。比如传统的线性回归很难处理多重共线性，于是发明了 LASSO 和岭回归来解决这个问题。另一个例子是引入装袋和增压来提高单棵树模型稳定性和准确性。如今，你也可以看到越来越多的方法涉及到不同领域的知识。将时间序列中的移动平均的思想应用于深度学习中的梯度下降中，以获得更快的收敛并减少振荡，这被称为带动量的梯度下降。因此，首先我想在这个博客中展示方法论之间的关系，然后我想写另外的博客来分别解释每个主题。你可以点击链接阅读你感兴趣的话题。</p><p id="74f1" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个博客会不时更新，将来会增加更多的内容。</p><p id="a511" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 1。深度学习:</strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lr"><img src="../Images/c6d9ad663eb446f1404cd9cd6c28d10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cEHA2H2-5sx7MsfjtanNEQ.png"/></div></div></figure><p id="6aac" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图显示了传统统计模型和深度学习模型之间的关系。绿色的术语是用于生成相应模型的算法。</p><p id="5726" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在深入研究神经网络(如 ANN、CNN 或 RNN)之前，让我们从单层神经网络开始我们的旅程。单层神经网络的思想是首先对输入变量进行加权线性组合，然后应用激活函数进行非线性变换。经典的单层神经网络包括逻辑回归和感知器。逻辑回归和感知器的区别在于，逻辑回归使用 sigmoid 函数作为激活函数，而感知器使用符号函数。</p><p id="f514" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于逻辑回归，请阅读我的博客:</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/an-introduction-to-logistic-regression-8136ad65da2e"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">逻辑回归导论</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">逻辑回归从理论到实践的深度探讨</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu mb mg"/></div></div></a></div><p id="d4b6" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于感知器算法，请阅读我的博客:</p><div class="md me gp gr mf mg"><a href="https://medium.com/@songyangdetang_41589/an-introduction-to-perceptron-algorithm-40f2ab4e2099" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">感知器算法简介</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">本博客将涵盖以下问题和主题</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="mv l mr ms mt mp mu mb mg"/></div></div></a></div><p id="0374" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">逻辑回归和感知器都使用梯度下降来获得最终模型。对于梯度下降，请阅读我的博客:</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/an-introduction-to-gradient-descent-c9cca5739307"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">梯度下降导论</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">本博客将涵盖以下问题和主题:</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mp l"><div class="mw l mr ms mt mp mu mb mg"/></div></div></a></div><p id="56ef" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">逻辑回归和感知器的限制是这两个模型的决策边界只能是线性的。当我们有一个更复杂的分类问题时，如下图所示，我们可能会期望一个更先进的方法，所以神经网络被引入。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mx"><img src="../Images/b0421a743beabb12ed9094bb4e063e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqeCkhYcceZ5uQ_gVRzobw.png"/></div></div></figure><p id="b2ff" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想用简单的数字建立一个神经网络，并更深入地理解这种奇特的算法，请阅读我的博客:</p><div class="md me gp gr mf mg"><a href="https://medium.com/@songyangdetang_41589/build-up-a-neural-network-with-python-7faea4561b31" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">用 python 构建神经网络</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">使用 Numpy 实现正向传播和反向传播</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="my l mr ms mt mp mu mb mg"/></div></div></a></div><p id="0441" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 2。线性回归</strong></p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mz"><img src="../Images/b870b99a09e2658e3ce90d5584f6408e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f3hrEJJ7wBXdhCKm6MMEmw.png"/></div></div></figure><p id="e2be" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">经典线性回归提供了另一个引入机器学习的完美角度。普通最小二乘法(OLS)用于估计线性模型的系数。然而，线性模型需要遵循几个假设，这些假设对模型的成功和预测的准确性至关重要。上表显示了假设、测试方法和每个假设的解决方案。</p><p id="a08e" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 3。贝叶斯统计</strong></p><p id="c5de" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">贝叶斯统计在机器学习中也起着重要的作用。贝叶斯统计的一个范例是朴素贝叶斯分类器，它可以简单快速地训练。贝叶斯统计也用于无监督学习，如高斯混合模型(GMM)。贝叶斯公式如下所示:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f1af98f1788572c7a87cae1ee22910b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/1*CkrCY3wXC71C30pFxXP5oQ.gif"/></div></figure><p id="8c2d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">公式中，<em class="nb"> p(y|x) </em>为后验概率；<em class="nb"> p(x|y) </em>是似然性；<em class="nb"> p(y) </em>是先验概率<em class="nb"> p(x) </em>是证据。</p><p id="990a" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于朴素贝叶斯分类器，请阅读我的博客:</p><div class="md me gp gr mf mg"><a href="https://medium.com/@songyangdetang_41589/introduction-to-na%C3%AFve-bayes-classifier-fa59e3e24aaf" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">朴素贝叶斯分类器简介</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">从理论到实践，学习感知机的基本原理</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="nc l mr ms mt mp mu mb mg"/></div></div></a></div><p id="4b5c" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在频数统计中，分布的参数是一个确定的数，最大似然估计(MLE)用于进行估计；在贝叶斯统计中，分布的参数也是一个随机变量，而是使用最大后验概率(MAP)。MLE 和 MAP 的对比见博客:</p><div class="md me gp gr mf mg"><a href="https://medium.com/@songyangdetang_41589/mle-vs-map-a989f423ae5c" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">最大似然估计与最大后验概率</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">最大似然估计(MLE)和最大后验概率(MAP)都被用来估计模型中的一些变量</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="nd l mr ms mt mp mu mb mg"/></div></div></a></div><p id="3b86" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 4。基于树的模型</strong></p><p id="fd3d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们从线性模型转移到非线性模型。非线性模型的一个范例是决策树。决策树是一种基于树的算法，用于解决回归和分类问题。回归树用于具有连续值的因变量，分类树用于具有分类值的因变量。树与更经典的方法(如线性回归或逻辑回归模型)有着非常不同的味道。</p><p id="e1f7" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">特别是，线性回归假设模型的形式为:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/d69c0a68ca83b275d2010003772e7ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/1*o9k3TzFxB8TqnGNKZEf6Zw.gif"/></div></figure><p id="b8a0" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">而回归树假设模型的形式为:</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/0a6d27f900e710d0cedc274f436d0529.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/1*BUNuvqTXUT6Dj66TT5lF8g.gif"/></div></figure><p id="8eda" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">其中<em class="nb"> R1，…，Rm </em>表示特征空间的一个划分和<em class="nb"> Cm: </em>的表达式</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b923448a65d67a657dce208f9ac21557.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/1*KzJQOMIO_J0_m01N0e-kyw.gif"/></div></figure><p id="025f" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">哪款比较好？根据没有免费的午餐定理，没有一个模型能对所有可能的情况都适用。因此，我们选择的方法取决于手头的问题。如果真正的决策边界是线性的，如顶部的两个图(下图)所示。线性模型将优于树模型。同时，如果真正的决策边界是矩形的，如下面的两个图所示。树模型比线性模型更能拟合数据。除了拟合度之外，为了可解释性和可视化，树模型是优选的。</p><figure class="ls lt lu lv gt lw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0e775b044210a9b753bfc28c260cc168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*07xDiSkkh2JVBuMqtX2mng.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Figures Source: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, 2017, An Introduction to Statistical Learning</figcaption></figure><p id="094c" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可以在博客中看到决策树的详细信息:</p><div class="md me gp gr mf mg"><a href="https://medium.com/@songyangdetang_41589/how-decision-tree-model-works-ce681cae10a6" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">机器学习中的决策树</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">决策树背后的数学原理及其 Python 实现</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="nm l mr ms mt mp mu mb mg"/></div></div></a></div><p id="40d5" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的 Github 链接:</p><div class="md me gp gr mf mg"><a href="https://github.com/sytangtang1991" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd ja gy z fp ml fr fs mm fu fw iz bi translated">sytangtang1991 -概述</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">在 GitHub 上注册您自己的个人资料，这是托管代码、管理项目和构建软件的最佳地方</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">github.com</p></div></div><div class="mp l"><div class="nn l mr ms mt mp mu mb mg"/></div></div></a></div><p id="471c" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja">参考</strong></p><p id="68f4" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1]伊恩·古德费勒，约舒阿·本吉奥，亚伦·库维尔，(2017) <em class="nb">深度学习</em></p><p id="f8cd" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] Gareth James，Daniela Witten，Trevor Hastie，Robert Tibshirani，(2017) <em class="nb">统计学习介绍</em></p><p id="ad18" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] Christopher M. Bishop，(2009)，<em class="nb">模式识别和机器学习</em></p><p id="43e2" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]特雷弗·哈斯蒂，罗伯特·蒂布拉尼，杰罗姆·弗里德曼，(2008)，<em class="nb">统计学习的要素</em></p></div></div>    
</body>
</html>