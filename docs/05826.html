<html>
<head>
<title>Road detection using segmentation models and albumentations libraries on Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 Keras 的分割模型和白蛋白库的道路检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/road-detection-using-segmentation-models-and-albumentations-libraries-on-keras-d5434eaf73a8?source=collection_archive---------5-----------------------#2019-08-25">https://towardsdatascience.com/road-detection-using-segmentation-models-and-albumentations-libraries-on-keras-d5434eaf73a8?source=collection_archive---------5-----------------------#2019-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="3275" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">在这篇文章中，我将展示如何编写自己的数据生成器以及如何使用 albumentations 作为增强库。以及 segmentation_models 库，它为 unet 和其他类似 Unet 的体系结构提供了几十个预训练的头。完整代码请访问 Github 。链接到<a class="ae ks" href="https://www.kaggle.com/insaff/massachusetts-roads-dataset" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p></blockquote><p id="bbbc" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">查看我的机器和深度学习博客【https://diyago.github.io/ T4】</p><h1 id="918f" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">理论</h1><p id="4d3f" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">语义图像分割的任务是将图像的每个<strong class="jw iu">像素</strong>标记为所表示内容的相应<strong class="jw iu">类</strong>。对于这样一项任务，经过各种改进的 Unet 架构表现出了最佳效果。它背后的核心思想只是几个卷积块，提取深度和不同类型的图像特征，随后是所谓的去卷积或上采样块，恢复输入图像的初始形状。此外，在每个卷积层之后，我们有一些跳跃连接，这有助于网络<em class="jv">记住关于初始图像的</em>，并有助于防止渐变。有关更多详细信息，您可以阅读 arxiv 文章或另一篇文章<a class="ae ks" rel="noopener" target="_blank" href="/understanding-semantic-segmentation-with-unet-6be4f42d4b47">或</a>。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/342f2c88bc2f14bea027a44026df8a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvXoKMHoPJMKpKK7keZMEA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Vanilla U-Net <a class="ae ks" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1505.04597</a></figcaption></figure><p id="8bcb" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">我们是来练习的，让我们开始吧。</p><h1 id="db99" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">数据集-卫星图像</h1><p id="ebe3" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">对于分割，我们不需要太多的数据来获得一个像样的结果，甚至 100 张带注释的照片就足够了。目前，我们将使用来自<a class="ae ks" href="https://www.cs.toronto.edu/~vmnih/data/" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~vmnih/data/</a>的马萨诸塞州道路数据集，大约有 1100 多张带注释的列车图像，它们甚至提供验证和测试数据集。不幸的是，没有下载按钮，所以我们必须使用脚本。这个<a class="ae ks" href="https://gist.github.com/Diyago/83919fcaa9ca46e4fcaa632009ec2dbd" rel="noopener ugc nofollow" target="_blank">脚本</a>将完成这项工作(可能需要一些时间来完成)。</p><p id="67d2" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">让我们来看看图片示例:</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mp"><img src="../Images/f9ad0c51d7e73fce3d7cf79fc25f706b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*suITn_6WHcKOyTJ1Rhf8iA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Massachusetts Roads Dataset image and ground truth mask ex.</figcaption></figure><p id="8243" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">注释和图像质量似乎不错，网络应该可以检测道路。</p><h1 id="1a9e" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">库安装</h1><p id="cf86" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">首先你需要安装带有 TensorFlow 的 Keras。对于 Unet 构建，我们将使用<a class="mq mr ep" href="https://medium.com/u/e34a5c21265a?source=post_page-----d5434eaf73a8--------------------------------" rel="noopener" target="_blank"> Pavel Yakubovskiy </a>的名为<a class="ae ks" href="https://github.com/qubvel/segmentation_models" rel="noopener ugc nofollow" target="_blank"> segmentation_models </a>的库，用于数据扩充<a class="ae ks" href="https://github.com/albu/albumentations" rel="noopener ugc nofollow" target="_blank">albumination</a>库。稍后我会写更多关于他们的细节。这两个库的更新都非常频繁，所以我更喜欢直接从 git 更新它们。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="c5a7" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">定义数据生成器</h1><p id="67bd" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">作为数据生成器，我们将使用我们的自定义生成器。它应该继承<em class="jv"> keras.utils.Sequence </em>并且应该定义这样的方法:</p><ul class=""><li id="8194" class="mu mv it jw b jx jy kb kc kt mw ku mx kv my kr mz na nb nc bi translated"><em class="jv"> __init__ </em>(类初始化)</li><li id="0500" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated"><em class="jv"> __len__ </em>(数据集的返回长度)</li><li id="7abf" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated"><em class="jv"> on_epoch_end </em>(时段结束时的行为)</li><li id="8510" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated"><em class="jv"> __getitem__ </em>(生成的批次馈入网络)</li></ul><p id="a9d0" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">使用自定义生成器的一个主要优点是，您可以处理您拥有的每种格式数据，并且可以做任何您想做的事情——只是不要忘记为 keras 生成所需的输出(批处理)。</p><p id="1dbe" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">这里我们定义<strong class="jw iu"> <em class="jv"> __init__ </em> </strong>方法。它的主要部分是为图像(<em class="jv"> self.image_filenames </em>)和遮罩名称(<em class="jv"> self.mask_names </em>)设置路径。不要忘记对它们进行排序，因为对于<em class="jv">self . image _ filenames[I]</em>对应的掩码应该是 self.mask_names[i]。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="d4c7" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">接下来重要的事情<strong class="jw iu"><em class="jv">_ _ getitem _ _</em></strong><em class="jv">。</em>通常情况下，我们无法将所有图像存储在 RAM 中，因此每次生成新的一批数据时，我们都需要读取相应的图像。下面我们定义训练的方法。为此，我们创建一个空的 numpy 数组(np.empty ),它将存储图像和遮罩。然后我们通过<em class="jv"> read_image_mask </em>方法读取图像，对每一对图像和掩膜进行增强。最后，我们返回 batch (X，y ),它已经准备好被放入网络中。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="659b" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">接下来，我们定义发电机，它将被馈入网络:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h2 id="8c95" class="ni kx it bd ky nj nk dn lc nl nm dp lg kt nn no lk ku np nq lo kv nr ns ls nt bi translated">数据扩充— <strong class="ak">相册</strong></h2><p id="db06" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">数据扩充是一种策略，能够显著增加可用于训练模型的数据的多样性，而无需实际收集新数据。这有助于防止过拟合，并使模型更加健壮。</p><p id="726c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">对于这样的任务，有大量的库:imaging、augmentor、solt、keras/pytorch 的内置方法，或者您可以使用 OpenCV 库编写您的自定义增强。但是我强烈推荐<a class="ae ks" href="https://github.com/albu/albumentations" rel="noopener ugc nofollow" target="_blank"> <strong class="jw iu">相册</strong> </a>图书馆。用起来超级快捷方便。有关用法示例，请访问官方<a class="ae ks" href="https://github.com/albu/albumentations" rel="noopener ugc nofollow" target="_blank">知识库</a>或查看<a class="ae ks" href="https://github.com/albu/albumentations/tree/master/notebooks" rel="noopener ugc nofollow" target="_blank">示例笔记本</a>。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f8a9050f586f0d6fcf91618f039f3523.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*L-qO5Tvw4oLaCBrhlDSS1g.png"/></div></figure><p id="9c5a" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在我们的任务中，我们将使用基本的增强，如翻转和非平凡的弹性变换的对比。你可以在上图中找到它们的例子。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="fa47" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在定义了所需的增强后，您可以很容易地得到如下输出:</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="d55c" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">复试</h1><p id="1526" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">我们将使用常见的<a class="ae ks" href="https://keras.io/callbacks/" rel="noopener ugc nofollow" target="_blank">回调</a>:</p><ul class=""><li id="9230" class="mu mv it jw b jx jy kb kc kt mw ku mx kv my kr mz na nb nc bi translated">模型检查点-允许您在训练时保存模型的权重</li><li id="ab83" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated">ReduceLROnPlateau —如果验证指标停止增加，则减少训练</li><li id="a7c8" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated">EarlyStopping —一旦验证指标停止，就停止训练，以增加几个时期</li><li id="7561" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated">tensor board——监控训练进度的绝佳方式。链接到官方<a class="ae ks" href="https://www.tensorflow.org/tensorboard/r2/scalars_and_keras" rel="noopener ugc nofollow" target="_blank">文档</a></li></ul><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="339b" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">培养</h1><p id="e46e" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">作为模型，我们将使用 Unet。使用它最简单的方法就是从<a class="ae ks" href="https://github.com/qubvel/segmentation_models" rel="noopener ugc nofollow" target="_blank"> segmentation_models </a>库中获取。</p><ul class=""><li id="5937" class="mu mv it jw b jx jy kb kc kt mw ku mx kv my kr mz na nb nc bi translated">backbone_name:用作编码器的分类模型的名称。EfficientNet 目前在分类模型中是最先进的，所以让我们来尝试一下。虽然它应该给出更快的推理并且具有更少的训练参数，但是它比众所周知的 resnet 模型消耗更多的 GPU 内存。还有许多其他选择可以尝试</li><li id="7e5a" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated">encoder_weights —使用 imagenet weights 加速训练</li><li id="ffdd" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated">encoder_freeze:如果<em class="jv">为真</em>则将编码器(骨干模型)的所有层设置为不可训练。首先冻结和训练模型，然后解冻可能是有用的</li><li id="2da1" class="mu mv it jw b jx nd kb ne kt nf ku ng kv nh kr mz na nb nc bi translated">decoder_filters —您可以指定解码器块的数量。在某些情况下，带有简化解码器的较重编码器可能会有用。</li></ul><p id="9e75" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">在初始化 Unet 模型之后，你应该编译它。此外，我们将<a class="ae ks" href="https://medium.com/koderunners/intersection-over-union-516a3950269c" rel="noopener">IOU</a>(union 上的交集)设置为我们将监控的度量，并将 bce_jaccard_loss ( <a class="ae ks" rel="noopener" target="_blank" href="/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a">二进制交叉熵</a>加上<a class="ae ks" href="https://arxiv.org/pdf/1705.08790.pdf" rel="noopener ugc nofollow" target="_blank"> jaccard loss </a>)设置为我们将优化的损失。我给了链接，所以不会去这里为他们进一步的细节。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="af70" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">开始训练后，你可以观看 tensorboard 日志。正如我们可以很好地看到模型训练，即使在 50 个纪元后，我们也没有达到全局/局部最优。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/fee9d540014c485b2556cac4b404a893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*zfWdBHeXwOWjrGH-Dcrriw.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Tensorboard logs</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nw"><img src="../Images/7871bac9e7fa7e808d701014828994fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5rlf8odv4HwAVC6NwjLNA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Loss and IOU metric history</figcaption></figure><h1 id="decc" class="kw kx it bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">推理</h1><p id="6814" class="pw-post-body-paragraph jt ju it jw b jx lu jz ka kb lv kd ke kt lw kh ki ku lx kl km kv ly kp kq kr im bi translated">因此，我们有 0.558 IOU 进行验证，但每个高于 0 的像素预测都被视为屏蔽。通过选择合适的阈值，我们可以将结果进一步提高 0.039 (7%)。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/629f0d8a9653931220c0e330ac8bd7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*oASXmiNsMFBZfslj8ja2MA.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Validation threshold adjusting</figcaption></figure><p id="6415" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke kt kg kh ki ku kk kl km kv ko kp kq kr im bi translated">度量当然很有趣，但是更有洞察力的模型预测。从下面的图片中，我们看到我们的网络很好地完成了任务，这很好。对于推理代码和计算指标，您可以阅读完整的代码。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ny"><img src="../Images/cf63017618c3eeab24c3b8a165114bac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vItXSopl3vhwmn-_Q9GvOQ.png"/></div></div></figure><h2 id="0032" class="ni kx it bd ky nj nk dn lc nl nm dp lg kt nn no lk ku np nq lo kv nr ns ls nt bi translated">参考</h2><pre class="ma mb mc md gt nz oa ob oc aw od bi"><span id="b4b2" class="ni kx it oa b gy oe of l og oh">@phdthesis{MnihThesis,<br/>    author = {Volodymyr Mnih},<br/>    title = {Machine Learning for Aerial Image Labeling},<br/>    school = {University of Toronto},<br/>    year = {2013}<br/>}</span></pre></div></div>    
</body>
</html>