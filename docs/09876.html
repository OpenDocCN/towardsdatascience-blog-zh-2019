<html>
<head>
<title>Logistic Regression in Machine Learning using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 实现机器学习中的逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-explained-and-implemented-in-python-880955306060?source=collection_archive---------0-----------------------#2019-12-27">https://towardsdatascience.com/logistic-regression-explained-and-implemented-in-python-880955306060?source=collection_archive---------0-----------------------#2019-12-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2f15" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解逻辑回归是如何工作的，以及如何使用 python 和 sklearn 轻松实现它。</h2></div><p id="d8e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在统计学中，逻辑回归用于模拟某一类或某一事件的概率。在这篇文章中，我将更多地关注模型的基础和实现，而不是深入数学部分。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/b9396e91f05990a39c74623ed669544c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*CYAn9ACXrWX3IneHSoMVOQ.gif"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">The value of weights b0 and b1 are updated at each iteration</figcaption></figure><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="ln lo l"/></div></figure><p id="f912" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是这段视频的书面版本。如果你喜欢看视频。</p><p id="9fe6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑回归与<a class="ae lq" rel="noopener" target="_blank" href="/linear-regression-using-gradient-descent-97a6c8700931">线性回归</a>相似，因为两者都涉及根据给定的训练数据估计预测方程中使用的参数值。线性回归预测一些连续的因变量的值。而逻辑回归预测依赖于其他因素的事件或类别的概率。因此，逻辑回归的输出总是在 0 和 1 之间。由于这一特性，它通常用于分类目的。</p><h1 id="9f2a" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">逻辑模型</h1><p id="9387" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">考虑一款具有<em class="lp"> x1、x2、x3 … xn </em>功能的车型。让二进制输出由<em class="lp"> Y </em>表示，它可以取值 0 或 1。<br/>设<em class="lp"> p </em>为<em class="lp"> Y = 1 </em>的概率，我们可以表示为<em class="lp"> p = P(Y=1) </em>。<br/>这些变量之间的数学关系可以表示为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8e2c79ecb4bec3d35a9ee129a865e5af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*ymom0dF4as8bYdxUHrwuOQ.png"/></div></figure><p id="3845" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里的术语<em class="lp">p/(1p)</em>被称为<em class="lp">几率</em>，表示事件发生的可能性。因此，<em class="lp">ln(p/(1p))</em>被称为<em class="lp">对数概率</em>，它只是用来将介于 0 和 1 之间的概率映射到一个介于(∞，+∞)之间的范围。术语<em class="lp"> b0，b1，b2… </em>是我们将在训练期间估计的参数(或权重)。</p><p id="43f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这只是我们要做的事情背后的基本数学。我们对这个等式中的概率 p 感兴趣。因此，我们简化等式以获得 p 的值:</p><ol class=""><li id="21d3" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">LHS 上的对数项<em class="lp"> ln </em>可以通过升高 RHS 作为<em class="lp"> e </em>的幂来移除:</li></ol><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi my"><img src="../Images/468eaf7f5430d4c33919be01bb9bbdf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*AyUToOnaSsLsblAAONHAPg.png"/></div></figure><p id="1947" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.现在我们可以很容易地简化来获得<em class="lp"> p </em>的值:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/d311592ff37bd96c2eb4da482ac19b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*j0s9tFm1AA4jqrAclHtWLg.png"/></div></figure><p id="2489" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这实际上是在其他机器学习应用中广泛使用的<em class="lp"> Sigmoid 函数</em>的方程。<em class="lp">s 形函数</em>由下式给出:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi na"><img src="../Images/f8c915caf5901c816503932d0a212371.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*_c6wT6T98K2LPOuh7NRjCQ.png"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e1d9c383bb0c14c104fa316ddf1f0e23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PaEjawWyMpwVttNjwswDnA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">The sigmoid curve (Wikipedia)</figcaption></figure><p id="7e76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们将使用上面推导出的方程来进行预测。在此之前，我们将训练我们的模型，以获得导致最小误差的参数值<em class="lp"> b0，b1，b2… </em>。这就是误差或损失函数的由来。</p><h1 id="60f8" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">损失函数</h1><p id="38b1" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">损失基本上就是我们预测值的误差。换句话说，这是我们的预测值和实际值之间的差异。我们将使用<a class="ae lq" href="https://afteracademy.com/blog/what-are-l1-and-l2-loss-functions" rel="noopener ugc nofollow" target="_blank"> L2 损失函数</a>来计算误差。理论上你可以用任何函数来计算误差。该功能可以分解为:</p><ol class=""><li id="9fc2" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">假设实际值为 yᵢ.让使用我们的模型预测的值被表示为ȳᵢ.找出实际值和预测值之间的差异。</li><li id="e041" class="mp mq iq kh b ki nc kl nd ko ne ks nf kw ng la mu mv mw mx bi translated">平方这个差值。</li><li id="c563" class="mp mq iq kh b ki nc kl nd ko ne ks nf kw ng la mu mv mw mx bi translated">计算训练数据中所有值的总和。</li></ol><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/1decef963efb32a5443489c0510fd7f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*HJ9043WWZhIQ6p4qfoFQXw.png"/></div></figure><p id="9449" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了误差，我们需要更新我们的参数值来最小化这个误差。这是“学习”实际发生的地方，因为我们的模型正在根据它以前的输出更新自己，以便在下一步中获得更准确的输出。因此，随着每次迭代，我们的模型变得越来越精确。我们将使用<em class="lp">梯度下降算法</em>来估计我们的参数。另一种常用的算法是<a class="ae lq" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/7348f7cf2c0ee6603dd57878d85b7afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jwpAGOptlfkHlYtbV3Kpg.jpeg"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">The loss or error on the y axis and number of iterations on the x axis.</figcaption></figure><h1 id="097e" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">梯度下降算法</h1><p id="e750" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">你可能知道函数的偏导数在最小值时等于 0。所以梯度下降基本上就是用这个概念，通过最小化损失函数来估计我们模型的参数或者权重。<a class="ae lq" href="https://www.youtube.com/watch?v=4PHI11lX11I" rel="noopener ugc nofollow" target="_blank">点击此处</a>获得关于梯度下降如何工作的更详细的解释。<br/>为了简单起见，在本教程的剩余部分，让我们假设我们的输出只依赖于单个特征<em class="lp"> x </em>。因此，我们可以将等式改写为:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ea98c2b4738313c9c055684eadc7086f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*7tH6JmoP2EOfmnJa_5VjiQ.png"/></div></figure><p id="b1c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们需要使用给定的训练数据来估计权重 b0 和 b1 的值。</p><ol class=""><li id="7039" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">最初让 b0=0，b1=0。设 L 为学习率。学习速率通过学习过程中每一步 b0 和 b1 值的更新量来控制。这里设 L=0.001。</li><li id="651f" class="mp mq iq kh b ki nc kl nd ko ne ks nf kw ng la mu mv mw mx bi translated">计算关于 b0 和 b1 的偏导数。偏导数的值会告诉我们损失函数离它的最小值有多远。这是衡量我们的权重需要更新多少才能达到最小或理想的 0 误差。如果您有多个特征，您需要计算每个权重 b0，b1 … bn 的偏导数，其中 n 是特征的数量。关于计算偏导数背后的数学的详细解释，请查看<a class="ae lq" href="https://youtu.be/l8VEth6leXA" rel="noopener ugc nofollow" target="_blank">我的视频。</a></li></ol><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi no"><img src="../Images/61f551a746705a7a83918aa4deb9c40e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*boKvDOlGd0VvBFWmRME-eg.png"/></div></figure><p id="acb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.接下来，我们更新 b0 和 b1 的值:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi np"><img src="../Images/342b69573a54d69729b06065cfbd2444.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*Rby1P0sST5axa9cXi107vw.png"/></div></figure><p id="4fa7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.我们重复这个过程，直到我们的损失函数是一个非常小的值或者理想地达到 0(意味着没有错误和 100%的准确性)。我们重复这个学习过程的次数被称为迭代或时期。</p><h1 id="b420" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">实施模型</h1><p id="8701" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">导入必要的库并在此下载数据集<a class="ae lq" href="https://drive.google.com/uc?id=15WAD9_4CpUK6EWmgWVXU8YMnyYLKQvW8&amp;export=download" rel="noopener ugc nofollow" target="_blank">。这些数据来自</a><a class="ae lq" href="https://www.kaggle.com/rakeshrau/social-network-ads" rel="noopener ugc nofollow" target="_blank"> kaggle </a>，描述了通过社交媒体上的广告购买的产品信息。我们将预测购买的<em class="lp">的价值</em>，并考虑一个单一特征<em class="lp">年龄</em>来预测购买的<em class="lp">的价值</em>。您也可以拥有多个特性。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nq lo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f1a35f4ae0bc6e70562203a4f8a15d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*hSyYKQE10Vaz4PokiC77fQ.png"/></div></figure><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nq lo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/ca6d84147abf1b8abea073c061ed510e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UxZjqa_MjSb-5OmqTgD_g.jpeg"/></div></div></figure><p id="cc52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要将我们的训练数据标准化，并将平均值移至原点。由于逻辑方程式的性质，这对于获得准确的结果是很重要的。这是通过<em class="lp">规格化</em>方法完成的。<em class="lp">预测</em>方法只需将权重值插入逻辑模型方程并返回结果。这个返回值就是所需的概率。</p><p id="5eb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型被训练 300 个时期或迭代。在每次迭代中计算偏导数，并更新权重。你甚至可以计算每一步的损失，看看每一步它是如何趋近于零的。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nq lo l"/></div></figure><p id="1b12" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于预测方程返回一个概率，我们需要将其转换为二进制值，以便能够进行分类。为此，我们选择一个阈值，比如 0.5，所有高于 0.5 的预测值将被视为 1，其他值将为 0。您可以根据正在解决的问题选择合适的阈值。</p><p id="d354" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，对于测试数据中的每个年龄值，我们预测产品是否被购买，并绘制图表。通过检查我们做出了多少正确的预测，并用它除以测试用例的总数，可以计算出准确性。我们的准确率似乎是 85%。</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nq lo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/7f58846b747ffd390b615d8496869a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6t1V6Xpw-3gIJhGd4DiN2A.jpeg"/></div></div></figure><pre class="lc ld le lf gt ns nt nu nv aw nw bi"><span id="ad4f" class="nx ls iq nt b gy ny nz l oa ob">Accuracy = 0.85</span></pre><h1 id="a5c5" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">使用 Sklearn 实现</h1><p id="89b9" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">库 sklearn 可用于使用<em class="lp"> LogisticRegression </em>类在几行代码中执行逻辑回归。它还支持多种功能。它要求输入值为特定格式，因此在使用<em class="lp"> fit </em>方法进行训练之前，已经对其进行了整形。</p><p id="5f7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用它的准确度是 86.25%，非常接近我们从头实现的模型的准确度！</p><figure class="lc ld le lf gt lg"><div class="bz fp l di"><div class="nq lo l"/></div></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ni"><img src="../Images/7f58846b747ffd390b615d8496869a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6t1V6Xpw-3gIJhGd4DiN2A.jpeg"/></div></div></figure><pre class="lc ld le lf gt ns nt nu nv aw nw bi"><span id="099a" class="nx ls iq nt b gy ny nz l oa ob">Accuracy = 0.8625</span></pre></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="5b1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们使用 python 从头开始轻松实现了一个看似复杂的算法，并将其与 sklearn 中做同样工作的标准模型进行了比较。我认为这里最关键的部分是梯度下降算法，以及学习如何在每一步更新权重。一旦你学会了这个基本概念，你就能够估计任何函数的参数。</p><p id="a9f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lq" href="https://colab.research.google.com/drive/1qmdfU8tzZ08D3O84qaD11Ffl9YuNUvlD" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">点击这里</strong> </a> <strong class="kh ir">查看谷歌合作实验室的完整代码和解释。您可以使用它来轻松地探索和研究代码。</strong></p><blockquote class="oj ok ol"><p id="385a" class="kf kg lp kh b ki kj jr kk kl km ju kn om kp kq kr on kt ku kv oo kx ky kz la ij bi translated"><em class="iq">有问题吗？需要帮助吗？联系我！</em></p></blockquote><p id="d576" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lp">电子邮件:adarsh1021@gmail.com</em></p><p id="45dc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lp">领英:</em><a class="ae lq" href="https://www.linkedin.com/in/adarsh-menon-739573146/" rel="noopener ugc nofollow" target="_blank"><em class="lp">https://www.linkedin.com/in/adarsh-menon-739573146/</em></a></p><p id="09d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lp">推特:</em><a class="ae lq" href="https://twitter.com/adarsh_menon_" rel="noopener ugc nofollow" target="_blank"><em class="lp">https://twitter.com/adarsh_menon_</em></a></p><p id="aa1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p><ul class=""><li id="c2b0" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la op mv mw mx bi translated">人工智能，现代方法—第 726、727 页</li><li id="83c5" class="mp mq iq kh b ki nc kl nd ko ne ks nf kw ng la op mv mw mx bi translated"><a class="ae lq" href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/logistic-regression-for-machine-learning/</a></li><li id="8d56" class="mp mq iq kh b ki nc kl nd ko ne ks nf kw ng la op mv mw mx bi translated"><a class="ae lq" rel="noopener" target="_blank" href="/logit-of-logistic-regression-understanding-the-fundamentals-f384152a33d1">https://towards data science . com/logit-of-logistic-regression-understanding-the-fundamentals-f 384152 a 33d 1</a></li><li id="bbb1" class="mp mq iq kh b ki nc kl nd ko ne ks nf kw ng la op mv mw mx bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Logistic_regression</a></li></ul></div></div>    
</body>
</html>