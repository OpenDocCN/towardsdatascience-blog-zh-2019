<html>
<head>
<title>BottleNet: Learnable Feature Compression for Accelerated Edge Intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">瓶颈:加速边缘智能的可学习特征压缩</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bottlenet-1d93b9393dd8?source=collection_archive---------38-----------------------#2019-09-17">https://towardsdatascience.com/bottlenet-1d93b9393dd8?source=collection_archive---------38-----------------------#2019-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9703" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">仅将 316 字节的数据上传到云中用于 ImageNet 分类</h2></div><p id="7fda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络要么部署在本地设备上，要么部署在云服务器上。如果它非常大，那么唯一的选择就是云服务器。使用云服务的缺点是你需要上传一个比较大的输入的通信成本。将有价值的原始数据交给云所有者也会损害隐私。所以问题是为什么不卸载特性而不是原始输入呢？特征通常是稀疏的，我们可以在特征空间中实现更高的压缩比。BottleNet 是一篇介绍特征压缩方法的论文，该方法只需要将 316 字节的数据卸载到云服务器，以便在 ImageNet 数据集上进行推理。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/21ced18f0c68c201d326a83660098cfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VixeHFeZxaXplxGoJT4vaQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">BottleNet adds two extra units into an existing neural network. 1. A convolutional layer for reducing the channel/spatial dimensions 2. A lossy compressor (e.g. JPEG). Photo by author.</figcaption></figure><p id="1969" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一个深度神经网络(DNN)，我们插入一个减少空间和信道维度的卷积层。然后，我们将卷积的简化特征传递给 JPEG 压缩。这导致平均大小为 316 字节的数据需要上传到云中，以便进行其余的推理！比 JPEG 压缩的 224x224 图像少很多(平均 26667 字节)！在云上使用卷积层来恢复原始特征尺寸，然后进行 JPEG 解压缩。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/c19349a15926838be2f446e22b2865e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*R2PF5B4Ubmr4xcKqExfq8g.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Learnable dimension reduction and restoration units along the (a) channel and (b) spatial dimension of features. Photo by author.</figcaption></figure><p id="387c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是如何训练一个中间有不可微层(JPEG)的神经网络呢？近似！因为一对压缩器和解压缩器可以用一个恒等函数来近似，所以我们简单地把它导数设为 1。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ls"><img src="../Images/47b85463f5e72cc4eaea6c2c83c9a9ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WkV_q7Ok4UpwHIKUlSCVzQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Embedding non-differentiable compression (e.g., JPEG) in DNN architecture. We approximate the pair of JPEG compressor and decompressor units by identity function to make the model differentiable in backpropagation. Photo by author.</figcaption></figure><p id="d5b4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，总之，我们在神经网络中添加了一组层(瓶颈单元),以减少将原始输入图像传输到云服务器的通信成本。如果我们有一个边缘设备，最好在初始层中插入瓶颈单元，以避免边缘设备的高计算成本:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/97defca1b8e7c1e75e61f878e2701c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*Bm6x67Uzll2lh8bNnMo8yQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Bottleneck Unit — all the reduction, compression, decompression, restorations units altogether. Photo by author.</figcaption></figure><p id="4d7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们简单地在中间特性上应用 JPEG 压缩，并将其卸载到云上用于下游层的其余计算，会发生什么？巨大的精度损失！但是如果在训练神经网络的同时意识到中间存在 JPEG 压缩单元，那么精度损失会变得更小。下图显示了这两种方法之间的精确度差距:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/2a1f2336a81f377cf55311d1b65f7a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*bZMSq7ZIrkjWqIEkbAu9DQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Accuracy loss will be much lower if the network is aware of the presence of JPEG compression on its features. RB1 in this figure refers to the first residual block of the ResNet-50 model. Photo by author.</figcaption></figure><p id="b4d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在 https://arxiv.org/abs/1902.01000 找到更多关于 BottleNet 的信息。</p><p id="9ea8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这项工作已发表在 2019 年低功耗电子与设计国际研讨会(ISLPED)上。https://ieeexplore.ieee.org/document/8824955/<a class="ae lv" href="https://ieeexplore.ieee.org/document/8824955/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>