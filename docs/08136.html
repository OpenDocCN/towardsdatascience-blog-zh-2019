<html>
<head>
<title>Biomedical Image Segmentation: U-Net</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生物医学图像分割:U-Net</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/biomedical-image-segmentation-u-net-a787741837fa?source=collection_archive---------13-----------------------#2019-11-07">https://towardsdatascience.com/biomedical-image-segmentation-u-net-a787741837fa?source=collection_archive---------13-----------------------#2019-11-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b8ee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用非常少的训练图像工作，并产生更精确的分割</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/74e18c0f53aaa6f4b73e7f3836a9d2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmKZh5IwvECVQuY36bNngQ.jpeg"/></div></div></figure><h1 id="88cf" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">图象分割法</h1><p id="406a" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">假设我们想知道一个物体在图像中的位置以及这个物体的形状。我们必须给图像中的每个像素分配一个标签，这样具有相同标签的像素就属于那个对象。与对象检测模型不同，图像分割模型可以提供图像中对象的精确轮廓。</p><h2 id="1500" class="mi kv it bd kw mj mk dn la ml mm dp le lv mn mo lg lz mp mq li md mr ms lk mt bi translated">图像分类、目标检测和图像分割的区别</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/b3b78fd830d2acf4174cee61c437119f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*40Un8kNxY1ctb8z7cXn7gg.jpeg"/></div></div></figure><p id="ea75" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">图像分类</strong>帮助我们对图像中包含的内容进行分类。目标是回答“这个图像里有猫吗？”，通过预测是或否。</p><p id="2822" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">物体检测</strong>指定物体在图像中的位置。目标是识别“这张图片中的猫在哪里？”通过围绕感兴趣的对象绘制边界框。</p><p id="4ac5" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">图像分割</strong>为图像中的每个对象创建一个像素式遮罩。目标是通过对所需标签中的每个像素进行分类来识别图像中不同对象的位置和形状。</p><h1 id="2b83" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">优信网</h1><p id="b0d7" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在本文中，我们将探索由奥拉夫·龙内伯格、菲利普·费舍尔和托马斯·布罗克斯撰写的<a class="ae na" href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/" rel="noopener ugc nofollow" target="_blank"> U-Net </a>。这篇<a class="ae na" href="https://arxiv.org/pdf/1505.04597.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>发表在<a class="ae na" href="https://www.miccai2019.org/" rel="noopener ugc nofollow" target="_blank"> 2015 MICCAI </a>上，在 2019 年 11 月有超过 9000 次引用。</p><h2 id="66d2" class="mi kv it bd kw mj mk dn la ml mm dp le lv mn mo lg lz mp mq li md mr ms lk mt bi translated">关于优信网</h2><p id="0c6e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">U-Net 用于生物医学图像的许多图像分割任务，尽管它也<a class="ae na" href="https://www.tensorflow.org/tutorials/images/segmentation" rel="noopener ugc nofollow" target="_blank">用于自然图像的分割</a>。U-Net 已经超越了先前 Ciresan 等人的最佳方法，赢得了 ISBI 2012 EM(电子显微镜图像)分割挑战赛。</p><p id="7a08" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">需要更少的训练样本<br/> </strong>深度学习模型的成功训练需要数千个带注释的训练样本，但是获取带注释的医学图像是昂贵的。U-Net 可以用较少的训练样本进行端到端的训练。</p><p id="c527" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">精确分割<br/> </strong>精确分割掩模在自然图像中可能并不重要，但是医学图像中的边缘分割误差导致临床设置中的结果不可靠。尽管训练样本较少，但 U-Net 可以产生更精确的分段。</p><h2 id="131d" class="mi kv it bd kw mj mk dn la ml mm dp le lv mn mo lg lz mp mq li md mr ms lk mt bi translated">U-Net 之前的相关工作</h2><p id="26f1" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如上所述，<a class="ae na" href="http://papers.nips.cc/paper/4741-deep-neural-networks" rel="noopener ugc nofollow" target="_blank"> Ciresan 等人</a>致力于神经网络分割神经元膜，用于电子显微镜图像的分割。网络使用滑动窗口通过提供像素周围的局部区域(小块)作为输入来预测每个像素的类别标签。</p><p id="bda8" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">相关工作的限制:</p><ul class=""><li id="6b08" class="nb nc it lo b lp mv ls mw lv nd lz ne md nf mh ng nh ni nj bi translated">由于滑动窗口、扫描每个补丁以及重叠造成的大量冗余，它非常慢</li><li id="8418" class="nb nc it lo b lp nk ls nl lv nm lz nn md no mh ng nh ni nj bi translated">无法确定滑动窗口的大小，这会影响定位精度和上下文使用之间的权衡</li></ul><h2 id="0586" class="mi kv it bd kw mj mk dn la ml mm dp le lv mn mo lg lz mp mq li md mr ms lk mt bi translated">体系结构</h2><p id="c968" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">U-Net 具有优雅的架构，扩展路径或多或少与收缩路径对称，并产生 u 形架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/26afa9266bbfdc9ff018724a5d78d5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAH3t6HeG5Xoq5sJm0rLGg.png"/></div></div></figure><p id="b890" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">收缩路径(下采样)<br/> </strong>看起来像一个典型的 CNN 架构，通过连续堆叠两个 3×3 卷积(蓝色箭头)，然后是一个 2×2 最大池(红色箭头)进行下采样。在每一个下采样步骤中，通道的数量都会翻倍。</p><p id="be74" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">扩展路径(上卷积)<br/> </strong>一个用于上采样的 2x2 上卷积(绿色箭头)和两个 3x3 卷积(蓝色箭头)。在每个上采样步骤中，通道数量减半。</p><p id="8cd3" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">在每个 2×2 上卷积之后，由于每个卷积中边界像素的丢失，特征图与来自收缩路径(灰色箭头)的相应层的连接提供了从收缩路径到扩展路径的定位信息。</p><p id="447e" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">最终层<br/> </strong>一个 1x1 卷积，将特征图映射到所需数量的类。</p><h1 id="9aed" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">我在优信网上的实验</h1><p id="b293" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我将使用<a class="ae na" href="https://cvit.iiit.ac.in/projects/mip/drishti-gs/mip-dataset2/Home.php" rel="noopener ugc nofollow" target="_blank"> Drishti-GS 数据集</a>，这与 Ronneberger 等人在论文中使用的数据集不同。该数据集包含 101 个视网膜图像，以及光盘和视杯的注释掩模，用于检测青光眼，青光眼是世界上失明的主要原因之一。50 幅图像将用于训练，51 幅用于验证。</p><h2 id="12fc" class="mi kv it bd kw mj mk dn la ml mm dp le lv mn mo lg lz mp mq li md mr ms lk mt bi translated">韵律学</h2><p id="fe38" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们需要一组指标来比较不同的模型，这里我们有二元交叉熵、Dice 系数和交集。</p><p id="aba1" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">二元交叉熵<br/> </strong>二元分类的常用度量和损失函数，用于衡量误分类的概率。</p><p id="37e4" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">我们将使用 PyTorch 的 binary _ cross _ entropy _ with _ logits<a class="ae na" href="https://pytorch.org/docs/stable/nn.functional.html#binary-cross-entropy-with-logits" rel="noopener ugc nofollow" target="_blank">。与 Dice 系数一起用作训练模型的损失函数。</a></p><p id="0099" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">骰子系数</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/d812b74b97d20082ad1a454659e44d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNoa0X7NYtKyFD3-Tw8w3A.jpeg"/></div></div></figure><p id="0c4f" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">预测值和实际值之间重叠的常用度量标准。计算方法是 2 *预测值和实际值之间的重叠面积(<em class="nr">)除以预测值和实际值组合的总面积(<em class="nr"/>)。</em></p><p id="05d4" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">该度量的范围在 0 和 1 之间，其中 1 表示完美和完全的重叠。</p><p id="ca4e" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">我将使用这个度量和二进制交叉熵作为训练模型的损失函数。</p><p id="f7ac" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated"><strong class="lo iu">交集超过并集</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/0010cdcc908c8d1686f71068cb548593.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vsx1tBly1KnY7e8IKJvvQQ.jpeg"/></div></div></figure><p id="2996" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">一个简单(然而有效！)用于计算预测遮罩与地面真实遮罩的准确度的度量。计算预测值与实际值之间的重叠面积<em class="nr"> </em> ( <em class="nr">)并除以并集面积<em class="nr"> </em> ( <em class="nr">预测值与实际值</em>)的计算。</em></p><p id="f9b7" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">类似于 Dice 系数，该度量的范围从 0 到 1，其中 0 表示没有重叠，而 1 表示预测值和实际值之间完全重叠。</p><h2 id="ad70" class="mi kv it bd kw mj mk dn la ml mm dp le lv mn mo lg lz mp mq li md mr ms lk mt bi translated">培训和结果</h2><p id="b440" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">为了优化该模型以及随后的 U-Net 实现以进行比较，使用具有 1e-4 学习率的<a class="ae na" href="https://pytorch.org/docs/stable/optim.html#torch.optim.Adam" rel="noopener ugc nofollow" target="_blank"> Adam 优化器</a>和每 10 个历元具有 0.1 衰减(伽马)的<a class="ae na" href="https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR" rel="noopener ugc nofollow" target="_blank"> Step LR </a>来训练超过 50 个历元。损失函数是二进制交叉熵和 Dice 系数的组合。</p><p id="cabf" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">该模型在 11 分 33 秒内完成训练，每个历元用时约 14 秒。总共有 34，527，106 个可训练参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/130d9c909368335ee3742b41348caed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edQcSC35S-iQ4tppwBRhEA.png"/></div></div></figure><p id="de68" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">具有最佳性能的时段是时段# 36(50 个中的一个)。</p><ul class=""><li id="49f9" class="nb nc it lo b lp mv ls mw lv nd lz ne md nf mh ng nh ni nj bi translated">二元交叉熵:0.3319</li><li id="6424" class="nb nc it lo b lp nk ls nl lv nm lz nn md no mh ng nh ni nj bi translated">骰子系数:0.8367</li><li id="2d4c" class="nb nc it lo b lp nk ls nl lv nm lz nn md no mh ng nh ni nj bi translated">并集上的交集:0.8421</li></ul><p id="a120" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">用几个看不见的样本测试模型，预测光盘(红色)和光学杯(黄色)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/fe0f339359f726835c47c3db586da064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vp6hqIa7_ZUFcVQ7DWOUTw.jpeg"/></div></div></figure><p id="ff02" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">从这些测试样本来看，结果相当不错。我选择了第一张图片，因为它的左上角有一个有趣的边缘，这里有一个错误的分类。第二个图像有点暗，但没有问题得到部分。</p><h1 id="2edd" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结论</h1><p id="ab54" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">U-Net 架构非常适合生物医学图像分割，尽管仅使用 50 幅图像进行训练，但仍取得了非常好的性能，并且具有非常合理的训练时间。</p><p id="a6f7" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">阅读周等人关于 UNet++的文章:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/biomedical-image-segmentation-unet-991d075a3a4b"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">生物医学图像分割:UNet++</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">通过一系列嵌套、密集的跳过路径提高分段准确性</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om ks ny"/></div></div></a></div><p id="343a" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">关注 U-Net:</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/biomedical-image-segmentation-attention-u-net-29b6f0827405"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">生物医学图像分割:注意力 U 网</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">通过在标准 U-Net 上附加注意门来提高模型的灵敏度和准确性</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="on l oj ok ol oh om ks ny"/></div></div></a></div><p id="d2e1" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">嗨！，我是叮当。我喜欢构建机器学习项目/产品，我在<a class="ae na" href="https://towardsdatascience.com/@jinglesnote" rel="noopener" target="_blank">向数据科学</a>写关于它们的文章。在<a class="ae na" href="https://medium.com/@jinglesnote" rel="noopener">媒体</a>上关注我或者在<a class="ae na" href="https://www.linkedin.com/in/jingles/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p><div class="kj kk kl km gt ab cb"><figure class="oo kn op oq or os ot paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/37b3c6bbeb134b28c91c9382ab95f170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*i2NzU4j49rZ36Mxz4gp4Sg.png"/></a></figure><figure class="oo kn op oq or os ot paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/2e369202b596d5b518dca78b96ab5f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*oENDSDMTwXi2CJdO1gryug.png"/></a></figure></div><p id="96f2" class="pw-post-body-paragraph lm ln it lo b lp mv ju lr ls mw jx lu lv mx lx ly lz my mb mc md mz mf mg mh im bi translated">以下是优信网的 PyTorch 代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ou ov l"/></div></figure></div></div>    
</body>
</html>