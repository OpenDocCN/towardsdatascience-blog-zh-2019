<html>
<head>
<title>Optimize NVIDIA GPU performance for efficient model inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化 NVIDIA GPU 性能，实现高效的模型推断</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimize-nvidia-gpu-performance-for-efficient-model-inference-f3e9874e9fdc?source=collection_archive---------5-----------------------#2019-07-20">https://towardsdatascience.com/optimize-nvidia-gpu-performance-for-efficient-model-inference-f3e9874e9fdc?source=collection_archive---------5-----------------------#2019-07-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/ecaa485b871a3e356108be2e86ce355c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0djIIc5P3umk8pBOUBNcuA.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Image from <a class="ae kf" href="https://www.cgdirector.com/best-hardware-for-gpu-rendering-in-octane-redshift-vray/" rel="noopener ugc nofollow" target="_blank">https://www.cgdirector.com/best-hardware-for-gpu-rendering-in-octane-redshift-vray/</a></figcaption></figure><p id="8df5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">GPU 已被证明是加速计算机视觉和自然语言处理(NLP)等深度学习和人工智能工作负载的有效解决方案。现在，许多基于深度学习的应用程序在其生产环境中使用 GPU 设备，如 NVIDIA Tesla 用于数据中心，Jetson 用于嵌入式平台。这就带来了一个问题:如何从你的 NVIDIA GPU 设备中获得最佳的推理性能？</p><p id="f2e4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将一步一步地展示我们如何优化预训练的 TensorFlow 模型，以改善支持 CUDA 的 GPU 上的推理延迟。在我们的实验中，我们使用 SSD MobileNet V2 进行目标检测。我们在 Colab 上进行实验。所有重现结果的源代码和指令都可以在<a class="ae kf" href="https://colab.research.google.com/drive/10ah6t0I2-MV_3uPqw6J_WhMHlfLflrr8" rel="noopener ugc nofollow" target="_blank">本笔记本</a>中找到。本文组织如下:</p><ol class=""><li id="1bb6" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">在 TensorFlow 中下载并运行原始模型</li><li id="34d6" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">通过与 CPU 协作优化模型</li><li id="c7a4" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">使用 TensorRT 优化模型</li><li id="a001" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">比较和结论</li></ol><p id="de51" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TL；速度三角形定位法(dead reckoning)</p><p id="9f76" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将 Colab GPU 实例上的推理时间改进为:</p><ol class=""><li id="b513" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">通过将控制流操作放在 CPU 上，提高了 1.3 倍</li><li id="45a3" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">通过转换预训练的 TensorFlow 模型并在 TensorRT 中运行，可提高 4.0 倍</li></ol><p id="5e9e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第 0 步:下载并运行 TensorFlow 中的 origin 模型</strong></p><p id="63be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先从<a class="ae kf" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" rel="noopener ugc nofollow" target="_blank">tensor flow Detection Model Zoo</a>下载 SSD MobileNet V2 预训练模型，该模型提供了一组在 COCO 数据集上训练的预训练模型。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="fa19" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个解压缩的文件夹中，我们可以找到以下文件:</p><ul class=""><li id="3e40" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld ly lk ll lm bi translated"><code class="fe lz ma mb mc b">frozen_inference_graph.pb</code>是针对任意图像和批次大小的冻结推理图</li><li id="c4ef" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld ly lk ll lm bi translated"><code class="fe lz ma mb mc b">pipeline.config</code>包含用于生成模型的配置</li><li id="8968" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld ly lk ll lm bi translated"><code class="fe lz ma mb mc b">model.ckpt.*</code>包含预先训练好的模型变量</li><li id="a5af" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld ly lk ll lm bi translated"><code class="fe lz ma mb mc b">saved_model</code>文件夹包含 TensorFlow <em class="md"> SavedModel </em>文件</li></ul><p id="212e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们使用<a class="ae kf" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> TensorFlow 对象检测 API </a>导出模型。这允许我们固定批量大小和图像大小。对于这个实验，我们使用 300x300 的图像作为输入，批量大小为 1。因此，我们的输入形状是<code class="fe lz ma mb mc b">[1, 300, 300, 3]</code>。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="d141" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经准备好运行模型了。我们首先从互联网上下载输入图像，并预处理成所需的形状。然后，我们使用 TensorFlow 加载模型并执行推理。请注意，我们添加了<code class="fe lz ma mb mc b">options</code>和<code class="fe lz ma mb mc b">run_metadata</code>来记录分析数据，以便进一步分析。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="43f4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们执行健全性检查以确保模型做出有意义的推断。注意，SSD MobileNet V2 模型将图像阵列作为每个检测到的对象的输入和输出绑定框<code class="fe lz ma mb mc b">[xmin, ymin, xmax, ymax]</code>。我们使用输出来绘制结合框，并得到以下结果。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/9f7b3b9d5e9b11318743e9b9fb089ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ptBrpCMb51rEPFr4tXctuA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Detection result we get from previous run</figcaption></figure><p id="5b47" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个结果听起来很合理。因此，我们可以相信我们的模型工作正常。现在，我们准备分析性能。我们将使用 Chrome 的跟踪工具来分析模型。打开 Chrome 浏览器，输入网址<code class="fe lz ma mb mc b">chrome://tracing</code>。拖动我们从上一个脚本中得到的时间轴 JSON 文件，然后我们可以看到下面的界面。</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/154cc757ca119241c1f2f2d283d846c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upQ_iCGXvHA6NND6ESXAaA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Inference timeline trace for origin SSD MobileNert V2</figcaption></figure><p id="e8c0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的跟踪中，您可能会注意到一些操作是在 CPU 上运行的，即使我们告诉 TensorFlow 在 GPU 上运行所有这些操作。这是因为 TensorFlow 没有为这些操作注册 GPU 内核(例如<code class="fe lz ma mb mc b">NonMaxSuppressionV3</code>)。由于这些操作无法在 GPU 上处理，TensorFlow 必须将中间输出从 GPU 内存传输到 CPU 内存，在 CPU 上处理，并将结果传输回 GPU，然后继续进行。从图中可以看出，这种情况发生了很多次。结果，我们的程序在数据传输上花费了太多时间，变得更慢。</p><p id="1f07" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，从图表的底部，我们可以看到每种类型的操作的时间成本。花费时间最多的前 3 项操作是<code class="fe lz ma mb mc b">GatherV2</code>、<code class="fe lz ma mb mc b">NonMaxSuppressionV3</code>和<code class="fe lz ma mb mc b">Conv2D</code>。当它对<code class="fe lz ma mb mc b">Conv2D</code>有意义时，因为 MobileNet V2 严重依赖它，并且计算量很大，它对其他人没有意义。我们将在下一节中解决这些问题并优化我们模型的推理性能。</p><p id="2ee6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第一步:与 CPU 合作优化模型</strong></p><p id="6d65" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">许多人认为 GPU 比 CPU 快——这就是为什么我们使用 GPU 来加速我们的程序。然而，这只是部分正确。为了解释这一点，我们需要了解一点 GPU 是如何工作的。</p><blockquote class="mf mg mh"><p id="b2cb" class="kg kh md ki b kj kk kl km kn ko kp kq mi ks kt ku mj kw kx ky mk la lb lc ld im bi translated">CPU 和 GPU 之间浮点能力的差异背后的原因是 GPU 专门用于计算密集型、高度并行的计算，这正是图形渲染的目的，因此设计了更多的晶体管用于数据处理，而不是数据缓存和流控制，如下图所示:</p></blockquote><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ml"><img src="../Images/774abb5cb445650ff4541c08b1c7d842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nO4oUNGNWJ1e0HDKddLW3Q.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">CPU vs. GPU structure, reference from <a class="ae kf" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#from-graphics-processing-to-general-purpose-parallel-computing" rel="noopener ugc nofollow" target="_blank">CUDA Toolkit documentation</a></figcaption></figure><p id="07a7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，对于可以并行处理的操作，如矩阵乘法，GPU 比 CPU 快得多。然而，由于 GPU 具有较少的用于流控制和高速缓存的晶体管，这可能不是流控制操作的情况(例如，<code class="fe lz ma mb mc b">if</code>、<code class="fe lz ma mb mc b">where</code>、<code class="fe lz ma mb mc b">while</code>等)。).</p><p id="9373" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在 CPU 和 GPU 上运行时间开销最高的 5 个操作(除了<code class="fe lz ma mb mc b">NonMaxSuppressionV3</code>，因为它只能在 CPU 上处理),并比较它们的性能，我们得到以下结果:</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="a564" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，对输入数据进行矩阵乘法和加法运算的<code class="fe lz ma mb mc b">Conv2D</code>，在 GPU 上的运行速度比预期快了~ 10 倍。但是对于<code class="fe lz ma mb mc b">GatherV2</code>、<code class="fe lz ma mb mc b">ConcatV2</code>和<code class="fe lz ma mb mc b">Select</code>这些访问内存给定索引的，CPU 的表现都优于 GPU。因此，我们可以通过简单地将这些操作放在 CPU 上来提高我们的推理性能:</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="fbf5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述代码将所有操作放在 CPU 的<code class="fe lz ma mb mc b">NonMaxSuppression</code>块中，因为大多数流控制操作都发生在这个块中。然后，我们使用相同的代码测试修改后的模型，并记录时间轴跟踪。我们得到以下结果:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/b5be84f5148402cdc956a683639e5ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jjEGKCZZylmzNxpRmh7aRw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Inference timeline trace for our optimized model</figcaption></figure><p id="51c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，总推断时间从~<strong class="ki iu">50 毫秒</strong>减少到~<strong class="ki iu">30 毫秒</strong>。<code class="fe lz ma mb mc b">GatherV2</code>的时间成本现在是<strong class="ki iu"> 2.140 </strong>毫秒，相比之下原来是<strong class="ki iu"> 5.458 </strong>毫秒。<code class="fe lz ma mb mc b">ConcatV2</code>的时间成本从<strong class="ki iu"> 3.588 </strong>毫秒减少到<strong class="ki iu"> 1.422 </strong>毫秒。此外，在修改后的模型中，GPU 和 CPU 之间的数据传输更少。所以像<code class="fe lz ma mb mc b">NonMaxSuppressionV3</code>这种原本在 CPU 上运行的操作也受益于此。</p><p id="6f70" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">第二步:使用 TensorRT 优化模型</strong></p><p id="09be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本节中，我们将展示如何通过使用 NVIDIA TensorRT 来进一步加速推理。</p><blockquote class="mf mg mh"><p id="5694" class="kg kh md ki b kj kk kl km kn ko kp kq mi ks kt ku mj kw kx ky mk la lb lc ld im bi translated"><em class="it">什么是 tensort</em></p><p id="4019" class="kg kh md ki b kj kk kl km kn ko kp kq mi ks kt ku mj kw kx ky mk la lb lc ld im bi translated"><em class="it"> NVIDIA TensorRT 是一个高性能深度学习推理平台。它包括一个深度学习推理优化器和运行时，为深度学习推理应用程序提供低延迟和高吞吐量。</em></p></blockquote><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/f90fb45e5c04aeaef5966700a09a48a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGSWoHDEJrFDYWbehAVtXw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">TensorRT overview from <a class="ae kf" href="https://developer.nvidia.com/tensorrt" rel="noopener ugc nofollow" target="_blank">NVIDIA TensorRT</a></figcaption></figure><p id="2d01" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">为什么使用 tensort</strong></p><p id="a2db" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TensorRT 提供了一系列用于深度学习模型优化的工具，如精度校准和层融合。您可以在不了解底层算法细节的情况下使用这些方便的工具。此外，TensorRT 专门为您的 GPU 设备选择内核，进一步优化性能。我们总结了使用 TensorRT 的利弊:</p><p id="b80b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">优点</strong>:</p><ul class=""><li id="d3db" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld ly lk ll lm bi translated">方便的优化工具使用户能够轻松有效地优化生产模型</li><li id="8d80" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld ly lk ll lm bi translated">特定于平台的内核选择，最大限度地提高您设备的性能</li><li id="2dce" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld ly lk ll lm bi translated">支持 TensorFlow 和 Caffe 等主要框架</li></ul><p id="1939" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">缺点</strong>:</p><ul class=""><li id="104d" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld ly lk ll lm bi translated">TensorRT 中仅支持部分操作。因此，在构建模型时，您必须仔细选择图层，以使其与 TensorRT 兼容</li></ul><p id="b02f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要在 TensorRT 中运行预训练的 TensorFlow 模型，我们需要执行以下步骤:</p><ol class=""><li id="58ba" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">将张量流模型转换为 UFF 格式</li><li id="3e48" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">构建 TensorRT 推理引擎</li></ol><p id="09bb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">将张量流模型转换为 UFF 格式</strong></p><p id="17c1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们使用图形外科医生和 UFF 转换器将 SSD MobileNet V2 TensorFlow 冻结模型转换为 TensorRT 可以解析的 UFF 格式。对于一些简单的模型(例如 Mobilenet V2，Inception v4 用于图像分类)，我们可以直接使用 UFF 转换器进行转换。但是，对于包含 TensorRT 不支持的操作的模型(如 SSD MobileNet V2 中的<code class="fe lz ma mb mc b">NonMaxSuppression </code>)，我们必须做一些预处理。诀窍是使用 Graph Surgeon 用支持的操作替换不支持的操作。</p><p id="3872" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面的脚本提供了一个预处理函数并修改了原始图形。关键的操作是用<code class="fe lz ma mb mc b">NMS_TRT</code>操作代替了原图中的<code class="fe lz ma mb mc b">NonMaxSuppression </code>操作，这是一个用于非最大值抑制的 TensorRT 核。然后，它将修改后的图形传递给 UFF 转换器，并输出最终的 UFF 模型。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="ac69" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">构建 TensorRT 推理机</strong></p><p id="bb23" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了 UFF 模型文件。我们准备制造 TensorRT 发动机。您可以构建一次引擎，并将其部署到不同的设备上。但是，由于引擎针对构建它的设备进行了优化，因此建议针对不同的设备重新构建引擎，以最大限度地提高设备性能。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="6592" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了 tensort 引擎，我们准备在 tensort 上运行我们的模型。请注意，TensorRT 需要 NCHW 格式的输入图像。因此，我们的输入格式应该是<code class="fe lz ma mb mc b">[1, 3, 300, 300]</code>，而不是 TensorFlow 中的<code class="fe lz ma mb mc b">[1, 300, 300, 3]</code>。</p><figure class="ls lt lu lv gt ju"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="3421" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的实验中，这次运行的平均推断时间是 4.9 毫秒。</p><p id="bd08" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">比较和结论</strong></p><p id="4fc4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们比较我们实验的推理时间，得到如下的情节:</p><figure class="ls lt lu lv gt ju gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/8084f035504317d9d7e8894083a5bf83.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*yRriMEMZ7xgUAgOlXuxW0Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Inference time comparison</figcaption></figure><p id="a20e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到，通过简单地将控制流操作放在 CPU 上，与原始模型相比，我们获得了 1.3 倍的性能提升。通过使用 TensorRT，与原始模型相比，我们可以获得 4 倍的改进。</p><p id="f302" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">总之，使用各种技术可以进一步提高 GPU 性能。在我们的实验中，我们通过以下方式优化预训练的 SSD Mobilenet V2 张量流模型:</p><ol class=""><li id="d7dd" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">将控制流操作放在 CPU 上并获得 1.3 倍的改进</li><li id="6b20" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">跑合 TensorRT，增益<strong class="ki iu"> 4x </strong>提升</li></ol><p id="bbb6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当 TensorRT 达到最佳性能时，它支持有限的操作。考虑在您生产环境中使用这些技术来最大化您的 GPU 性能。最后，我们强烈建议在<a class="ae kf" href="https://colab.research.google.com/drive/10ah6t0I2-MV_3uPqw6J_WhMHlfLflrr8" rel="noopener ugc nofollow" target="_blank"> Colab </a>上运行这个实验，看看如何实现性能。</p></div></div>    
</body>
</html>