<html>
<head>
<title>An intuitive guide to Gaussian processes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯过程直观指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d?source=collection_archive---------2-----------------------#2019-01-15">https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d?source=collection_archive---------2-----------------------#2019-01-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b234" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个不受重视的算法的数学解释</h2></div><p id="5c5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">高斯过程是回归和分类的强大算法。他们最大的实际优势是可以对自己的不确定性给出一个可靠的估计。在这篇没有数学的高水平文章结束时，我的目标是让你对高斯过程有一个直观的认识，以及是什么让它们在其他算法中独一无二。</p><p id="deb4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">内容:</p><ul class=""><li id="09c3" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">回顾机器学习</li><li id="4001" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">如何应对不确定性</li><li id="26e9" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">简而言之贝叶斯推理</li><li id="5841" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">高斯过程</li></ul></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><h1 id="899f" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">什么是机器学习？</h1><blockquote class="mo"><p id="775e" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">机器学习是类固醇上的线性回归。</p></blockquote><p id="8294" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">机器学习是使用我们拥有的数据(称为训练数据)来学习一个函数，我们可以用它来预测我们还没有的数据。最简单的例子是线性回归，我们学习直线的斜率和截距，这样我们就可以从点的水平位置预测点的垂直位置。如下所示，训练数据为蓝色点，学习的功能为红色线。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nd"><img src="../Images/1eb88901cc01fc917afadb7aec023d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wdoRkVBtcBRFdAL-d-TWsg.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Left: linear regression. Right: steroids. Together: machine learning.</figcaption></figure><p id="6ce4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">机器学习在几个方面是线性回归的扩展。首先，现代 ML 处理更复杂的数据，而不是像线性回归那样学习一个函数来计算另一个数，我们可能会处理不同的输入和输出，例如:</p><ul class=""><li id="8589" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">房屋的<a class="ae nt" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques" rel="noopener ugc nofollow" target="_blank">价格</a>(输出)取决于其位置、房间数量等……(输入)</li><li id="dd3c" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">基于图像(输入)中像素的强度和颜色的图像(输出)的<a class="ae nt" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank">内容</a></li><li id="d97a" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">根据围棋(输入)棋盘的状态，<a class="ae nt" href="https://deepmind.com/blog/alphazero-shedding-new-light-grand-games-chess-shogi-and-go/" rel="noopener ugc nofollow" target="_blank">最好移动</a>(输出)</li><li id="7c25" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">基于低分辨率图像(输入)的<a class="ae nt" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank">高分辨率图像</a>(输出)</li></ul><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi nu"><img src="../Images/074f041160c10e40e91626508e5cc51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ir3XRIQ3RgsiM3cFR3SOnw.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Clockwise from top left: Figure 1 from the <a class="ae nt" href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" rel="noopener ugc nofollow" target="_blank">AlphaGo paper,</a> figure 12 from <a class="ae nt" href="https://arxiv.org/pdf/1609.04802.pdf" rel="noopener ugc nofollow" target="_blank">Ledig et al</a>, samples from <a class="ae nt" href="https://en.wikipedia.org/wiki/ImageNet" rel="noopener ugc nofollow" target="_blank">ImageNet</a>, image from the <a class="ae nt" href="https://www.kaggle.com/c/zillow-prize-1" rel="noopener ugc nofollow" target="_blank">Zillow house price prediction competition</a>.</figcaption></figure><p id="471b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，现代 ML 使用更强大的方法来提取模式，深度学习只是其中的一种。高斯过程是这些方法中的另一种，它们的主要区别是它们与不确定性的关系。</p><h1 id="1301" class="lw lx iq bd ly lz nv mb mc md nw mf mg jw nx jx mi jz ny ka mk kc nz kd mm mn bi translated">思考不确定性</h1><blockquote class="mo"><p id="9adb" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">不确定性可以表示为一组可能的结果及其各自的可能性，称为概率分布</p></blockquote><p id="6861" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">我们周围的世界充满了不确定性——我们不知道我们的通勤需要多长时间，也不知道明天中午的天气如何。一些不确定性是由于我们缺乏知识，不管我们有多少知识，这是这个世界固有的。既然我们无法完全消除宇宙中的不确定性，我们最好有一个好的方法来处理它。<em class="oa">概率分布</em>正是如此，事实证明这是理解高斯过程的关键。</p><p id="fc5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">概率分布最明显的例子是掷出一个公平的六面骰子的结果，即任何特定面的概率为六分之一。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/9b62af41a506188f9c3f7f8b7ce2b1f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*I2ucUDK81-4hTxhHhFTMhw.png"/></div></figure><p id="9414" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个<em class="oa">离散</em>概率分布的例子，因为有有限数量的可能结果。在离散的情况下，概率分布仅仅是一系列可能的结果和它们发生的几率。在许多真实世界的场景中，连续的<em class="oa">概率分布更合适，因为结果可以是任何实数，下一节将探讨一个例子。</em></p><p id="fd9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一个稍后会有用的关键概念是从概率分布中<em class="oa">采样</em>。这意味着从一组可能的结果到一个真实的结果——在这个例子中是掷骰子。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oc"><img src="../Images/1f8f4bccf2ca79d873b346db7d396045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KmQddrdIF0LOIaD7"/></div></div></figure><h1 id="ce22" class="lw lx iq bd ly lz nv mb mc md nw mf mg jw nx jx mi jz ny ka mk kc nz kd mm mn bi translated">贝叶斯推理</h1><p id="7ebe" class="pw-post-body-paragraph kf kg iq kh b ki od jr kk kl oe ju kn ko of kq kr ks og ku kv kw oh ky kz la ij bi translated">贝叶斯推理可能是一个令人生畏的短语，但它可以归结为一种基于我们观察到的证据来更新我们对世界的信念的方法。在贝叶斯推理中，我们对世界的信念通常表示为概率分布，而<a class="ae nt" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯法则</a>告诉我们如何更新这些概率分布。</p><blockquote class="mo"><p id="11f9" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">贝叶斯统计为我们提供了基于新数据更新信念(用概率分布表示)的工具</p></blockquote><p id="d1fb" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">让我们来看一个贝叶斯推理的说明性例子——我们将根据一些证据来调整我们对巴拉克·奥巴马身高的看法。</p><p id="08a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们考虑一下，我们从来没有听说过巴拉克·奥巴马(请原谅)，或者至少我们不知道他的身高。然而，我们知道他是一个居住在美国的男性。因此，在看到任何证据之前，我们对奥巴马身高的信念(用贝叶斯术语来说，这是我们的<strong class="kh ir">先验信念</strong>)应该只是美国男性的身高分布。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ce4197b629890002198fcf88eec90f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*5ojcx91C-Fd-UBVqJ1yaHA.png"/></div></figure><p id="f15d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们假装维基百科不存在，这样我们就不能只查奥巴马的身高，而是观察一些照片形式的证据。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oj"><img src="../Images/668d8e5e6a4682ddfe7ccfc7bd3422f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*irBXQ2qS1s4wdMdZICu9jg.jpeg"/></div></div></figure><p id="40f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们更新的信念(贝叶斯术语中的<strong class="kh ir">后验</strong>)看起来像这样。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ok"><img src="../Images/c862633411ff7bb6c2b60ddf92a775a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*MUjHIaR_-cX95j5w8otirg.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Blue: updated belief. Red: old belief</figcaption></figure><p id="cb41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到奥巴马肯定高于平均水平，略高于其他几位世界领导人，但是我们不能确定具体有多高。显示的概率分布仍然反映了奥巴马是平均身高，而照片中的其他人都非常矮的小概率。</p><h1 id="6acf" class="lw lx iq bd ly lz nv mb mc md nw mf mg jw nx jx mi jz ny ka mk kc nz kd mm mn bi translated">什么是高斯过程？</h1><p id="d129" class="pw-post-body-paragraph kf kg iq kh b ki od jr kk kl oe ju kn ko of kq kr ks og ku kv kw oh ky kz la ij bi translated">现在我们知道了如何用数字值来表示不确定性，比如高度或掷骰子的结果，我们准备学习什么是高斯过程。</p><blockquote class="mo"><p id="30bc" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">高斯过程是可能函数的概率分布。</p></blockquote><p id="1d3b" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">由于高斯过程让我们描述函数的概率分布，我们可以使用<a class="ae nt" href="https://en.wikipedia.org/wiki/Bayes'_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯规则</a>通过观察训练数据来更新我们的函数分布。</p><p id="c229" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了加强这种直觉，我将使用高斯过程运行一个贝叶斯推理的例子，这个例子与上一节中的例子完全相似。我们不是根据照片来更新我们对奥巴马身高的信念，而是根据一个未知函数的样本来更新我们对该函数的信念。</p><p id="8364" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们关于未知函数的<strong class="kh ir">先验信念</strong>如下图所示。右边是我们的高斯过程的平均值和标准差——我们对函数没有任何了解，所以我们的平均值的最佳猜测是在实数的中间，即 0。</p><p id="edb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">左边的每一行都是函数分布的样本，我们知识的缺乏反映在展示的各种可能的函数和不同的函数形状上。从高斯过程中采样就像掷骰子，但每次你得到的是不同的函数，并且有无限多种可能的函数。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ol"><img src="../Images/9fcb7b7eb6cb9a84a857db58fab5d6fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YAPmNXea5gKoH3uyRrtITQ.png"/></div></div></figure><p id="698e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将观察未知函数在不同点的输出，而不是观察奥巴马的一些照片。对于高斯过程，我们的<strong class="kh ir">证据</strong> <strong class="kh ir">是训练数据</strong>。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div class="gh gi om"><img src="../Images/fa219622663bc10b26bbbdab637843b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*9xMQMnSPnAFkWqIY2jvIpQ.png"/></div></figure><p id="2d53" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们已经看到了一些证据，让我们使用贝叶斯规则来更新我们对函数的信念，以获得<strong class="kh ir">后验</strong>高斯过程，也就是我们对我们试图拟合的函数的更新信念。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi ol"><img src="../Images/2309ae267988aca75a9b1fab643ff189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNQg-o-C2JELQFQjEEDrLw.png"/></div></div></figure><p id="7e37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">类似于奥巴马可能高度的狭窄分布，你可以看到一个更狭窄的函数分布。更新的高斯过程被限制为适合我们的训练数据的可能函数-我们的函数的平均值截取所有训练点，每个采样函数也是如此。我们还可以看到，标准差远离我们的训练数据，这反映了我们对这些领域知识的缺乏。</p><h2 id="5dbc" class="on lx iq bd ly oo op dn mc oq or dp mg ko os ot mi ks ou ov mk kw ow ox mm oy bi translated">GPs 的优点和缺点</h2><blockquote class="mo"><p id="eb49" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">高斯过程知道他们不知道的。</p></blockquote><p id="e0d9" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">这听起来很简单，但是许多(如果不是大多数的话)ML 方法并不具备这一点。一个关键的好处是，拟合 GP 的不确定性随着训练数据的增加而增加，这是 GPs 植根于概率和贝叶斯推理的直接结果。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi oz"><img src="../Images/697594b9d3086e28c7cd34256a1e1ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-OB2Myvf4h5xZP39qUn5w.png"/></div></div><figcaption class="np nq gj gh gi nr ns bd b be z dk">Subset of the images from the <a class="ae nt" href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html" rel="noopener ugc nofollow" target="_blank">Classifier comparison</a> page on the scikit-learn docs.</figcaption></figure><p id="2996" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面我们可以看到在一个简单的分离蓝点和红点的任务中通过不同的方法学习到的分类函数。请注意，两种常用且强大的方法在远离训练数据的情况下保持了其预测的高度确定性——这可能与<a class="ae nt" rel="noopener" target="_blank" href="/know-your-enemy-7f7c5038bdf3">对立</a> <a class="ae nt" rel="noopener" target="_blank" href="/know-your-enemy-the-fascinating-implications-of-adversarial-examples-5936bccb24af">示例</a>现象有关，其中强大的分类器出于奇怪的原因给出了非常错误的预测。高斯过程的这一特性与身份验证和安全关键用途特别相关，因为您希望完全确定您的模型输出是有充分理由的。</p><blockquote class="mo"><p id="0aa9" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">高斯过程让你结合专家知识。</p></blockquote><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">当你使用 GP 来建模你的问题时，你可以通过选择<a class="ae nt" href="https://www.cs.toronto.edu/~duvenaud/cookbook/" rel="noopener ugc nofollow" target="_blank">内核</a>来塑造你的先验信念(对这些的完整解释超出了本文的范围)。</p><figure class="ne nf ng nh gt ni gh gi paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="gh gi pa"><img src="../Images/058c4a9273ff22fb8882ed3e029d8484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0q4vMQZ8BeZnDvnL1wx_IQ.png"/></div></div></figure><p id="00ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可以让你以多种不同的方式塑造你的拟合函数。你们中善于观察的人可能一直在想，在给定上述不确定性属性的情况下，高斯过程如何能够超越其训练数据进行推广。答案是，GPs 的泛化特性几乎完全取决于核的选择。</p><blockquote class="mo"><p id="b3e1" class="mp mq iq bd mr ms mt mu mv mw mx la dk translated">高斯过程计算量很大。</p></blockquote><p id="c898" class="pw-post-body-paragraph kf kg iq kh b ki my jr kk kl mz ju kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">高斯过程是一种非参数方法。参数化方法将有关训练数据的知识提炼为一组数字。对于线性回归，这只是两个数字，斜率和截距，而其他方法，如神经网络，可能有几十万个。这意味着在他们被训练之后，进行预测的成本<em class="oa">只取决于参数</em>的数量。</p><p id="d808" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，由于高斯过程是非参数的(尽管核超参数模糊了图像)，它们需要在每次进行预测时考虑整个训练数据。这不仅意味着训练数据必须在推断时保存，还意味着预测的计算成本成比例(立方！)与训练样本数。</p><h1 id="1ea0" class="lw lx iq bd ly lz nv mb mc md nw mf mg jw nx jx mi jz ny ka mk kc nz kd mm mn bi translated">高斯过程的未来</h1><p id="4852" class="pw-post-body-paragraph kf kg iq kh b ki od jr kk kl oe ju kn ko of kq kr ks og ku kv kw oh ky kz la ij bi translated">高斯过程的世界在可预见的时间内仍将令人兴奋，因为正在进行的研究将它们的概率优势引入到目前由深度学习主导的问题中——<a class="ae nt" href="http://proceedings.mlr.press/v5/titsias09a/titsias09a.pdf" rel="noopener ugc nofollow" target="_blank">稀疏</a>和<a class="ae nt" href="http://www.auai.org/uai2013/prints/papers/244.pdf" rel="noopener ugc nofollow" target="_blank">迷你批处理</a>高斯过程增加了它们对大型数据集的可扩展性，而<a class="ae nt" href="http://proceedings.mlr.press/v31/damianou13a.pdf" rel="noopener ugc nofollow" target="_blank">深度</a>和<a class="ae nt" href="https://arxiv.org/abs/1709.01894" rel="noopener ugc nofollow" target="_blank">卷积</a>高斯过程使高维和图像数据触手可及。注意这个空间。</p></div></div>    
</body>
</html>