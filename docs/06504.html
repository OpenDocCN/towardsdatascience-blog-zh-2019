<html>
<head>
<title>Reimagining Plutarch with Tensorflow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Tensorflow 2.0 重新想象 Plutarch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reimagining-plutarch-with-tensorflow-2-0-45998bc16feb?source=collection_archive---------34-----------------------#2019-09-17">https://towardsdatascience.com/reimagining-plutarch-with-tensorflow-2-0-45998bc16feb?source=collection_archive---------34-----------------------#2019-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/697b269d58460b54b33fd1410c65d9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ccsP4wMg98KzRZQDyB9k1w.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Original photo by Sandra Povilaitis; Vincent van Gogh style transferred using VGG19</figcaption></figure><div class=""/><div class=""><h2 id="578a" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">通过 TensorFlow 2.0 中的单词嵌入看普鲁塔克的希腊和罗马贵族生活</h2></div><h1 id="6463" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">序文</h1><p id="6f0b" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/Parallel_Lives" rel="noopener ugc nofollow" target="_blank">普鲁塔克的《高贵的希腊人和罗马人的生活<em class="mj"/></a>，也被称为<em class="mj">平行生活</em>或者仅仅是<em class="mj">普鲁塔克的生活</em>，是一系列著名的古希腊人和罗马人的传记，从<a class="ae mi" href="https://en.wikipedia.org/wiki/Theseus" rel="noopener ugc nofollow" target="_blank">忒修斯</a>和<a class="ae mi" href="https://en.wikipedia.org/wiki/Lycurgus_of_Sparta" rel="noopener ugc nofollow" target="_blank">吕库古</a>到<a class="ae mi" href="https://en.wikipedia.org/wiki/Mark_Antony" rel="noopener ugc nofollow" target="_blank">阿非利加努斯·戈狄亚努斯二世</a>。</p><p id="d4c6" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/reimagining-plutarch-with-nlp-part-2-dc4e360baa68">在最近发表的文章</a>中，我们研究了使用 gensim 库训练我们自己的单词嵌入。这里，我们将主要关注利用 TensorFlow 2.0 平台的单词嵌入层；目的是更好地理解该层是如何工作的，以及它如何有助于大型 NLP 模型的成功。</p><div class="ip iq gp gr ir mp"><a rel="noopener follow" target="_blank" href="/reimagining-plutarch-with-nlp-part-2-dc4e360baa68"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd jg gy z fp mu fr fs mv fu fw je bi translated">用 NLP 重构 Plutarch:第 2 部分</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">普鲁塔克通过自然语言处理研究希腊和罗马贵族的生活:这部分包括 word2vec…</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">towardsdatascience.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd ix mp"/></div></div></a></div><div class="ip iq gp gr ir mp"><a rel="noopener follow" target="_blank" href="/reimagining-plutarch-with-nlp-part-1-24e82fc6556"><div class="mq ab fo"><div class="mr ab ms cl cj mt"><h2 class="bd jg gy z fp mu fr fs mv fu fw je bi translated">用 NLP 重构 Plutarch:第 1 部分</h2><div class="mw l"><h3 class="bd b gy z fp mu fr fs mv fu fw dk translated">普鲁塔克通过自然语言处理研究希腊和罗马贵族的生活:这部分包括 NLTK 和 word…</h3></div><div class="mx l"><p class="bd b dl z fp mu fr fs mv fu fw dk translated">towardsdatascience.com</p></div></div><div class="my l"><div class="ne l na nb nc my nd ix mp"/></div></div></a></div><p id="633a" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">为了便于复制，我将代码改编成了适用于<a class="ae mi" href="https://colab.research.google.com/notebooks/welcome.ipynb" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>的代码，并强调了该平台的独特之处——否则，整个代码可以使用 Python 3.6+和相关包在您的本地机器上运行。整篇文章中都有代码，但我会跳过一些补充或次要的代码——完整的代码可以在我的<a class="ae mi" href="https://github.com/mlai-demo/TextExplore" rel="noopener ugc nofollow" target="_blank"> Github 库中找到。</a></p><p id="8ce8" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">古登堡计划已经提供了本分析中使用的文本<a class="ae mi" href="https://www.gutenberg.org/ebooks/674" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="e9b6" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">设置事物</h1><p id="6a05" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">在 Colab 上，让我们将运行时类型更改为 GPU，然后导入最新的 TensorFlow 版本——下面的代码片段仅适用于 Colab，否则只需使用 pip 或 conda install 命令<a class="ae mi" href="https://www.tensorflow.org/install/" rel="noopener ugc nofollow" target="_blank">在您的机器上上传最新的 TensorFlow </a>。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="00d5" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">我们还需要 OS 和<a class="ae mi" href="https://en.wikipedia.org/wiki/Regular_expression" rel="noopener ugc nofollow" target="_blank">正则表达式</a>库，然后保存&amp;打印文件路径以备将来参考:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="ac86" class="nq kv jf nm b gy nr ns l nt nu">import os<br/>import re<br/>fpath = os.getcwd(); fpath</span></pre><p id="39db" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">让我们将文本(Plutarch.txt)导入到 Google Colab 驱动器中——我们需要记住，我们在那里的文件是短暂的，我们需要在每次使用该平台较长时间后上传它们:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="ee03" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">上面的代码也可以在 Colab 的 Code Snippets 选项卡下找到——还有许多其他非常有用的代码。当执行这段代码时，我们将看到 Colab 上传文件，然后我们可以单击左边的 Colab Files 选项卡，以确保该文件与 Google 的默认示例数据目录在一起。</p><p id="d1e3" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">让我们阅读文本并做一些基本的正则表达式操作:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="5334" class="nq kv jf nm b gy nr ns l nt nu">import re</span><span id="6e89" class="nq kv jf nm b gy nv ns l nt nu">corpus = open(fpath + '/Plutarch.txt',  'rb').read().lower().decode(encoding='utf-8')</span><span id="f235" class="nq kv jf nm b gy nv ns l nt nu">corpus = re.sub('\n', ' ', corpus) #remove new line<br/>corpus = re.sub('\r', ' ', corpus) #remove "return"</span></pre><p id="f66b" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">因为我们将把文本分成句子，所以新行对我们的分析没有意义。此外，在使用文本标记器时，我注意到“\r”(表示回车)会产生错误的唯一单词，比如“we”和“we \ r”——同样，这在我们的例子中并不重要。因此“\n”和“\r”都需要删除。</p><h1 id="ec85" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">建立字典</h1><p id="171e" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">随着我们逐渐接近实际的单词嵌入，让我们将文本标记成句子:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="e1f8" class="nq kv jf nm b gy nr ns l nt nu">import nltk<br/>from nltk.tokenize import sent_tokenize<br/>nltk.download('punkt') #need in Colab upon resetting the runtime<br/> <br/># tokenize at sentence level<br/>sentences = nltk.sent_tokenize(corpus)<br/>print("The number of sentences is {}".format(len(sentences)))</span></pre><p id="862d" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">我们会看到这篇课文总共有 16989 个句子。接下来，我们需要计算最长句子中的单词数——原因将在本教程的后面变得显而易见:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="24db" class="nq kv jf nm b gy nr ns l nt nu">from nltk.tokenize import word_tokenize</span><span id="53ca" class="nq kv jf nm b gy nv ns l nt nu">word_count = lambda sentence: len(word_tokenize(sentence))<br/>longest_sentence = max(sentences, key=word_count)<br/>length_longest_sentence = len(word_tokenize(longest_sentence))</span><span id="4fa8" class="nq kv jf nm b gy nv ns l nt nu">print("The longest sentence has {} words".format(length_longest_sentence))</span></pre><p id="3077" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">原来最长的句子有 370 个单词长。接下来，让我们将整个文本转换为正数，以便我们可以开始使用 TensorFlow 说一种通用语言:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="051e" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">从上面我们还发现，该文本有 20241 个唯一的单词，因为记号赋予器对每个相同的单词只赋予一个数字。为了标准化所有句子的长度(即，将输入数据转换为单个相同形状的张量，以使其可处理/更容易用于模型——我们在这里是为了满足机器的需求)，我们需要将表示单词的数字列表(sent_numeric)转换为实际的字典(word_index)，并添加填充。我们也可以将截断很长的句子和填充短句子结合起来，但是在这种情况下，我们只填充最长句子的长度。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="ebc4" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">词汇大小(也称为独特单词的数量)将增加 1，达到 20，242，这是添加 0 进行填充的结果。键入“data[0]”(即第一句话)，看看第一句话在填充后会是什么样子。</p><p id="48e8" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">为了能够在单词和它们的数字表示之间来回转换，我们需要为查找添加反向单词索引:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="fa02" class="nq kv jf nm b gy nr ns l nt nu">reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])</span><span id="3c88" class="nq kv jf nm b gy nv ns l nt nu">def decode_data(text):<br/>    return ' '.join([reverse_word_index.get(i, '?') for i in text])</span></pre><p id="bcea" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">仔细检查单词 indexing 和 conversion 是有意义的——一个错误就可能使整个数据集混乱不堪，让人无法理解。在<a class="ae mi" href="https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_TFembPub.ipynb" rel="noopener ugc nofollow" target="_blank">我的 Github 库</a>中可以找到转换前后的交叉检查的例子。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/c406da4fc41874ed69deec282b16a4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CgWvRM9rC1VsQil3AS282A.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by Sandra Povilaitis</figcaption></figure><h1 id="20e2" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">模型</h1><p id="eb39" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">最后，让我们构建并运行模型。TensorFlow 提供了一个<a class="ae mi" href="https://www.tensorflow.org/beta/tutorials/text/word_embeddings" rel="noopener ugc nofollow" target="_blank">不错的教程，我们正在根据自己的需要进行修改。</a></p><p id="17e8" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">但是首先，我们可以简单地只运行嵌入层，这将产生一个嵌入的数组。我读到过这样一个数组可以被保存并在另一个模型中使用——是的，它可以，但是除了跳过新模型中的嵌入步骤之外，我不太确定它的实用性，因为为每个单词生成的向量对于所解决的问题是不可知的:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="5199" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">我们不会在上面花太多时间，而是将重点放在嵌入只是第一部分的模型上。</p><p id="57a7" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">在导入相关库之后，让我们继续构建新的、非常基本的模型架构:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="eb52" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">嵌入层(通常可用作模型中的第一层)会将数字编码的唯一单词序列(提醒一下，其中 20，241 个单词加上填充编码为零)转换为向量序列，后者在模型训练时被学习。每个向量将有 100 个维度(embedding_dim=100)，因此我们将得到一个 20242 x 100 的矩阵..输入长度将固定为最长句子的长度，即 370 个单词，因为每个单词由于填充而被模型感知为具有相同的大小。Mask_zero 通知模型输入值 0 是否为应被屏蔽的特殊填充值，这在模型可以处理可变输入长度的重复图层中特别有用。</p><p id="6db3" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">在对足够多的有意义的数据进行训练之后，具有相似含义的单词将可能具有相似的向量。</p><p id="e7d3" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">下面是模型总结(带有额外密集层的模型在 github 库中):</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/e3feee6e60bb6297fb2beede88d9bcd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhqSJvYMeQfSqcbUjNM7kA.png"/></div></div></figure><p id="bbd5" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">在模型摘要中，我们将看到嵌入层的参数数量是 2，024，200，这是 20，242 个单词乘以嵌入维度 100。</p><p id="32ca" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">前面提到的 TensorFlow 教程使用了一个评论数据集，每个评论根据积极或消极的情绪标记为 1 或 0。我们没有奢侈的标签，但仍然希望测试这个模型，所以将简单地创建一个 0 的数组，并附加到每个句子；模型需要这样的结构。这不会是机器智能第一次也不会是最后一次遇到无法解决的任务，但仍然会迫使我们找到解决方案。让我们来训练这个模型:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="a1fd" class="nq kv jf nm b gy nr ns l nt nu">import numpy as np</span><span id="c02a" class="nq kv jf nm b gy nv ns l nt nu">adam = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) <br/>model.compile(optimizer='adam',<br/>              loss='binary_crossentropy',<br/>              metrics=['accuracy'])</span><span id="fd92" class="nq kv jf nm b gy nv ns l nt nu">batch_size = 16989  #number of sentences<br/>data_labels = np.zeros([batch_size, 1])</span><span id="5471" class="nq kv jf nm b gy nv ns l nt nu">history = model.fit(<br/>    data,<br/>    data_labels,<br/>    epochs=200,<br/>    batch_size=batch_size,<br/>    verbose = 0)</span></pre><p id="af00" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">嵌入被训练。在我们转向可视化之前，让我们快速检查一下 gensim 上的单词相似性。首先，我们需要创建 vectors 文件——临时保存在 Colab 中或下载到本地机器:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="23a4" class="nq kv jf nm b gy nr ns l nt nu">f = open('vectors.tsv' ,'w')<br/>f.write('{} {}\n'.format(vocab_size-1, embedding_dim))<br/>vectors = model.get_weights()[0]<br/>for words, i in tokenizer.word_index.items():<br/>    str_vec = ' '.join(map(str, list(vectors[i, :])))<br/>    f.write('{} {}\n'.format(words, str_vec))<br/>f.close()</span><span id="e982" class="nq kv jf nm b gy nv ns l nt nu"># download the file to the local machine by double-clicking the Colab file or using this:<br/>try:<br/>  from google.colab import files<br/>except ImportError:<br/>   pass<br/>else:<br/>  files.download('vectors.tsv')</span></pre><p id="6642" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">第二，让我们</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="89f0" class="nq kv jf nm b gy nr ns l nt nu">import gensim</span><span id="109e" class="nq kv jf nm b gy nv ns l nt nu">w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.tsv', binary=False)</span><span id="aa52" class="nq kv jf nm b gy nv ns l nt nu">w2v.most_similar('rome')</span></pre><p id="0805" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">最后，让我们检查一下庞贝和凯撒之间的相似性，这在我们之前培训的<a class="ae mi" rel="noopener" target="_blank" href="/reimagining-plutarch-with-nlp-part-2-dc4e360baa68"> CBOW 模型中显示得很高</a>:</p><pre class="nf ng nh ni gt nl nm nn no aw np bi"><span id="c389" class="nq kv jf nm b gy nr ns l nt nu">round(w2v.similarity('pompey', 'caesar'),4)</span></pre><p id="b460" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">词与词之间的关系很高。同样，正如人们所料，凯撒表现出与罗马高度相似。</p><p id="4943" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">对于那些对更复杂的模型感兴趣的人来说，额外的变体，包括递归神经网络(<a class="ae mi" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a>)可以在<a class="ae mi" href="https://github.com/mlai-demo/TextExplore/blob/master/RePlutarch_TFembPub.ipynb" rel="noopener ugc nofollow" target="_blank">我的 Github 文件</a>中找到，但请记住，它们的训练速度将比上面的简单模型慢得多。</p><h1 id="8384" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">形象化</h1><p id="d62c" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">对于嵌入的可视化来说，TensorFlow Projector 无与伦比，因此让我们创建矢量和元(即对应于这些矢量的单词)文件供其使用:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="97b4" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">在本地导入文件，然后我们可以前往<a class="ae mi" href="https://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow 的投影仪</a>，上传文件以替换默认数据，并尝试网站上可用的各种选项。这里是文本的整个向量空间的主成分分析视图:</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c6ff84d5c1cbf2843e6ca215881e21b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/1*TvuJYkZJDfPo2CTuH7e7kQ.gif"/></div></figure><p id="ce02" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">这是 100 个单词的向量空间，这些单词和“罗马”最相似。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/ad69b4c7cf3235f77bb6aff860f242c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*JlXwa5sUU5wyIlEm94xidg.gif"/></div></div></figure><h1 id="cb90" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">结论</h1><p id="1f4d" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">在本文中，我们简要地看了一下单词嵌入层在深度学习模型中的作用。在这种模型的上下文中，该层支持解决特定的 NLP 任务，例如文本分类，并通过迭代训练单词向量，使其最有助于最小化模型损失。一旦模型被训练，我们可以通过相似性计算和可视化来检查嵌入层输出。</p><p id="f852" class="pw-post-body-paragraph lm ln jf lo b lp mk kg lr ls ml kj lu lv mm lx ly lz mn mb mc md mo mf mg mh ij bi translated">嵌入层还可以用于加载预训练的单词嵌入(例如 GloVe、BERT、FastText、ELMo)，我认为这通常是利用需要这种嵌入的模型的更有效的方式，部分原因是生成它们所需的“工业级”工作和数据量。然而，在专门文本的情况下，尤其是如果可以训练单词嵌入的语料库相当大，训练自己的嵌入仍然会更有效。</p></div></div>    
</body>
</html>