<html>
<head>
<title>A gentle introduction to algorithmic fairness</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">算法公平性的温和介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-algorithmic-fairness-46c825bdaf5d?source=collection_archive---------30-----------------------#2019-04-23">https://towardsdatascience.com/a-gentle-introduction-to-algorithmic-fairness-46c825bdaf5d?source=collection_archive---------30-----------------------#2019-04-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="30f5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">算法公平问题的温和介绍:一些美国历史，法律动机，和四个带有反证的定义。</h2></div><figure class="ki kj kk kl gt km gh gi paragraph-image"><div class="ab gu cl kn"><img src="../Images/4388551b8d6e3af5d0338c9f250458fc.png" data-original-src="https://miro.medium.com/v2/0*A3MIm5vz1cLohQwj"/></div></figure><h1 id="48d2" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">历史</h1><p id="d612" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在美国，借贷中的公平问题由来已久。</p><p id="9b05" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">例如，<a class="ae mj" href="https://en.wikipedia.org/wiki/Redlining" rel="noopener ugc nofollow" target="_blank">划红线</a>:</p><p id="19d4" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="mk">1935 年，联邦住房贷款银行委员会要求住房所有者贷款公司调查 239 个城市，并绘制“住宅安全地图”，以显示每个被调查城市的房地产投资安全水平。在地图上，“D 型”社区用红色标出，被认为是抵押贷款支持风险最高的地区..</em></p><p id="a4b8" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="mk">’20 世纪 60 年代，社会学家约翰·麦克奈特创造了“红线”一词，用来描述银行基于社区人口统计数据而避开投资领域的歧视性做法。在红线的全盛时期，最常受到歧视的地区是市中心的黑人社区……'</em></p><p id="b6a2" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">划红线显然是不公平的，因为投资的决定不是基于个人房主偿还贷款的能力，而是基于位置；这个基础系统地拒绝贷款给一个种族群体，黑人。事实上，1988 年在《亚特兰大宪法日报》上发表的普利策奖系列文章第一部分表明，位置比收入更重要:“T10 在收入相同的稳定社区中(在亚特兰大市区)，每 1000 户家庭中，白人社区总是获得最多的银行贷款。融合的社区得到的总是较少。黑人社区——包括市长的社区——收到的总是最少。</p><p id="3bf6" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">1968 年《公平住房法案》和 1977 年《社区再投资法案》就是为了打击住房和贷款中的这类不公平行为而通过的。</p><p id="c6e3" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">最近，在 2018 年，WUNC <a class="ae mj" href="https://www.wunc.org/post/blacks-and-latinos-denied-mortgages-rates-double-whites" rel="noopener ugc nofollow" target="_blank">报告称</a>北卡罗来纳州一些城市的黑人和拉丁美洲人被拒绝的抵押贷款利率高于白人:</p><p id="00a9" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">贷方和他们的贸易组织不否认他们拒绝有色人种的比率远远高于白人。但他们坚持认为，这种差异可以用该行业竭力隐瞒的两个因素来解释:潜在借款人的信用记录和整体债务收入比。他们特别指出三位数的信用评分——银行用来确定借款人是否有可能偿还贷款——在贷款决策中尤为重要。"</p><p id="18dc" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">WUNC 的例子提出了一个有趣的观点:通过一个指标(按人口统计的贷款利率)看起来不公平是可能的，但通过另一个指标(根据信用历史和债务收入比判断的支付能力)看起来不公平是不可能的。衡量公平是复杂的。在这种情况下，我们无法判断贷款行为是否公平，因为我们无法获得这些特定群体的信用历史和债务收入比数据，来评估贷款人对差异的解释。</p><p id="8c16" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">2007 年，美国联邦储备委员会(FRB) <a class="ae mj" href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/" rel="noopener ugc nofollow" target="_blank">报告了</a>关于信用评分及其对信贷可用性和可负担性的影响。他们<a class="ae mj" href="https://www.federalreserve.gov/boarddocs/rptcongress/creditscore/executivesummary.htm" rel="noopener ugc nofollow" target="_blank">得出结论</a>信用历史评分模型中包含的信用特征并不能代表种族，尽管不同的人口统计群体平均而言有着显著不同的信用评分，并且<em class="mk">“对于给定的信用评分，不同的人口统计群体的信用结果——包括贷款绩效、可用性和可负担性的衡量指标——也是不同的。”</em>该 FRB 研究支持贷方的主张，即信用评分可能解释抵押拒绝率的差异(因为人口统计群体具有不同的信用评分)，同时也指出不同群体的信用结果不同。</p><p id="b3c2" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">这公平不公平？</p><h1 id="46ae" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">定义公平</h1><p id="086f" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着机器学习(ML)的普及，人们对 ML 中的公平、问责和透明越来越感兴趣(例如<a class="ae mj" href="https://fatconference.org/" rel="noopener ugc nofollow" target="_blank"> fat* </a>会议和<a class="ae mj" href="http://www.fatml.org/" rel="noopener ugc nofollow" target="_blank"> fatml </a>研讨会)。</p><p id="189d" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">一些研究人员说，公平不是一个统计学概念，没有任何统计数据能够完全捕捉到它。有许多统计定义，人们试图联系(如果不是定义)公平。</p><p id="077d" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">首先，这里有两个在许多关于公平的讨论中出现的法律概念:</p><ol class=""><li id="58da" class="ml mm it lk b ll me lo mf lr mn lv mo lz mp md mq mr ms mt bi translated"><a class="ae mj" href="https://en.wikipedia.org/wiki/Disparate_treatment" rel="noopener ugc nofollow" target="_blank"> <strong class="lk iu">不同的待遇</strong> </a>:“根据《美国民权法案》第七章，由于受保护的特征(如种族或性别)而对某人的不平等行为。”如果目的是拒绝黑人贷款，红线就是完全不同的待遇。</li><li id="e353" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md mq mr ms mt bi translated"><a class="ae mj" href="https://en.wikipedia.org/wiki/Disparate_impact" rel="noopener ugc nofollow" target="_blank"> <strong class="lk iu">三教九流冲击</strong> </a>:“惯例..这对具有受保护特征的一组人的不利影响比对另一组人的不利影响更大，即使规则适用..在形式上是中立的。”(<em class="mk">“完全不同的影响原则在具有里程碑意义的美国最高法院格里戈斯诉杜克电力公司案(1971)中正式确立。1955 年，杜克电力公司制定了一项政策，规定雇员必须有高中文凭才能被考虑晋升，这大大限制了黑人雇员的资格。法院发现这一要求与工作表现几乎没有关系，因此认为它具有不合理的——也是非法的——完全不同的影响。”</em>【corb 2018】)</li></ol><p id="af3e" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">[Lipt2017]指出这些是差异的法律概念，并为应用于机器学习分类器的奇偶技术概念创建了相应的术语:</p><ol class=""><li id="c23f" class="ml mm it lk b ll me lo mf lr mn lv mo lz mp md mq mr ms mt bi translated"><strong class="lk iu">处理奇偶校验</strong>:分类器应该对给定的受保护特征不敏感。在[Corb2018]中也被称为反分类，或“通过无意识的公平”</li><li id="d966" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md mq mr ms mt bi translated"><strong class="lk iu">影响均等</strong>:在不同的群体中，做出肯定决定的人的比例应该是均等的。这也叫人口统计奇偶性，统计奇偶性，或者被保护阶层与分数的独立性[Fair2018]。</li></ol><p id="1194" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">有大量关于算法公平性的文献。从[Corb2018]开始，又多了两个定义:</p><ol class=""><li id="89e9" class="ml mm it lk b ll me lo mf lr mn lv mo lz mp md mq mr ms mt bi translated"><strong class="lk iu">分类奇偶校验</strong>:某个给定的分类错误度量在由受保护属性定义的组之间是相等的。[Hard2016]称此为<em class="mk">均等机会</em>如果测量值为真阳性率，以及<em class="mk">均等几率</em>如果有两个均等测量值，即真阳性率和假阳性率。</li><li id="04b2" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md mq mr ms mt bi translated"><strong class="lk iu">校准</strong>:以风险分值为条件，结果独立于受保护属性。也就是现实符合风险评分。例如，预计有 20%违约几率的所有贷款中，约有 20%实际上违约了。</li></ol><p id="c874" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">研究界对公平的理想统计定义缺乏共识。事实上，同时实现多个公平概念是不可能的结果([Klei2016] [Chou2017])。正如我们之前提到的，一些研究人员认为公平不是一个统计概念。</p><h1 id="f3a5" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">没有完美的定义</h1><p id="ac41" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面描述的每个统计定义都有反例。</p><p id="edec" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">待遇平等不公平地忽略了真正的差异。[Corb2018]描述了 COMPAS 评分用于预测累犯(某人如果出狱是否会犯罪)的案例。在控制了 COMPAS 评分和其他因素后，女性复发的可能性降低了。因此，在这个预测中忽略性可能会不公平地惩罚女性。请注意,<a class="ae mj" href="https://www.consumer.ftc.gov/articles/0347-your-equal-credit-opportunity-rights" rel="noopener ugc nofollow" target="_blank">平等信贷机会法案</a>从法律上规定了待遇平等:“<em class="mk">债权人在某些情况下可能会向你索要【受保护的阶级信息，如种族】，但在决定是否给你信贷或设定信贷条款时，他们可能不会使用这些信息。</em>“因此，[Corb2018]暗示这种不公平是法律规定的。</p><p id="a706" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">影响均等并不能确保公平(人们反对配额)，而且会削弱模型的准确性，损害模型对社会的效用。[Hard2016]在其导言中讨论了这个问题(使用术语“人口均等”)。</p><p id="b535" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">Corbett 等人[Corb2018]详细论述了分类奇偶性自然被违反:“<em class="mk">当暴力累犯的基本比率在不同群体之间不同时，真实的风险分布也必然不同——无论在预测中使用了哪些特征，这种差异都将持续存在。</em></p><p id="3445" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">他们还认为，校准不足以防止不公平。他们假设的例子是，一家银行只根据邮政编码内的违约率发放贷款，而忽略了收入等其他属性。假设(1)在邮政编码范围内，白人和黑人申请者有相似的违约率；(2)黑人申请者居住在违约率相对较高的邮政编码区。那么，该银行的计划将会不公平地惩罚信誉良好的黑人申请者，但仍然是经过校准的。</p><h1 id="306a" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">结论</h1><p id="dce5" class="pw-post-body-paragraph li lj it lk b ll lm ju ln lo lp jx lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">总之，可能的公平没有单一的衡量标准。我们对四个统计定义进行了旋风式的考察，其中两个由历史驱动，两个最近由机器学习驱动，并总结了每个定义的反驳意见。</p><p id="c837" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">这也意味着自动决定一个算法是否公平具有挑战性。开源公平测量包通过提供许多不同的测量方法来反映这一点。</p><p id="a571" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">然而，这并不意味着我们应该忽略统计数据。他们可以给我们一个主意，我们是否应该更仔细地看。精神食粮。我们应该好好喂养我们的大脑，因为它最有可能做出最后的决定。</p><p id="1d39" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated">(注意:这个问题是有争议的。我们的目的是以富有成效、尊重的方式加入对话。我们欢迎任何形式的反馈。)</p><p id="fcf7" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="mk">感谢</em> <a class="ae mj" href="https://twitter.com/kkenthapadi" rel="noopener ugc nofollow" target="_blank"> <em class="mk">克里希纳拉姆</em></a><em class="mk"/><a class="ae mj" href="https://twitter.com/zacharylipton" rel="noopener ugc nofollow" target="_blank"><em class="mk">扎克·利普顿</em></a><em class="mk"/><a class="ae mj" href="https://twitter.com/lukemerrick_" rel="noopener ugc nofollow" target="_blank"><em class="mk">卢克·梅里克</em></a><em class="mk"/><a class="ae mj" href="https://twitter.com/amitpaka" rel="noopener ugc nofollow" target="_blank"><em class="mk">阿米特·卡帕</em> </a> <em class="mk">，以及</em> <a class="ae mj" href="https://twitter.com/krishnagade" rel="noopener ugc nofollow" target="_blank"> <em class="mk">克里希纳·加德</em> </a> <em class="mk">的反馈</em></p><h1 id="9e34" class="kq kr it bd ks kt ku kv kw kx ky kz la jz lb ka lc kc ld kd le kf lf kg lg lh bi translated">参考</h1><ul class=""><li id="e6dd" class="ml mm it lk b ll lm lo lp lr mz lv na lz nb md nc mr ms mt bi translated">亚历山德拉·乔尔德乔娃。"具有不同影响的公平预测:对累犯预测工具偏差的研究."大数据 5，第 2 期(2017 年 6 月 1 日):153–63。<a class="ae mj" href="https://doi.org/10.1089/big.2016.0047" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1089/big.2016.0047</a>。</li><li id="0a20" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md nc mr ms mt bi translated">科比特-戴维斯、萨姆和沙拉德-戈埃尔。"公平的测量和错误测量:公平机器学习的评论."ArXiv:1808.00023 [Cs]，2018 年 7 月 31 日。<a class="ae mj" href="http://arxiv.org/abs/1808.00023" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1808.00023</a>。</li><li id="1fa8" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md nc mr ms mt bi translated">[Fair2018]“公平与机器学习。”2019 年 4 月 9 日接入。<a class="ae mj" href="https://fairmlbook.org/" rel="noopener ugc nofollow" target="_blank">https://fairmlbook.org/</a>。</li><li id="157c" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md nc mr ms mt bi translated">[哈德 2016]哈特、莫里茨、埃里克·普莱斯和内森·斯雷布罗。"监督学习中的机会均等."ArXiv:1610.02413 [Cs]，2016 年 10 月 7 日。<a class="ae mj" href="http://arxiv.org/abs/1610.02413" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1610.02413</a>。</li><li id="79d4" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md nc mr ms mt bi translated">[Klei2016]克莱恩伯格、乔恩、森迪尔·穆莱纳坦和马尼什·拉格哈万。"公平确定风险分值的内在权衡."ArXiv:1609.05807 [Cs，Stat]，2016 年 9 月 19 日。<a class="ae mj" href="http://arxiv.org/abs/1609.05807" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1609.05807</a>。</li><li id="3c12" class="ml mm it lk b ll mu lo mv lr mw lv mx lz my md nc mr ms mt bi translated">利普顿、扎卡里·c、亚历山德拉·乔尔德乔娃和朱利安·麦考利。"减轻 ML 的影响差异需要治疗差异吗？"ArXiv:1711.07076 [Cs，Stat]，2017 年 11 月 19 日。【http://arxiv.org/abs/1711.07076】T42。</li></ul></div><div class="ab cl nd ne hx nf" role="separator"><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni nj"/><span class="ng bw bk nh ni"/></div><div class="im in io ip iq"><p id="9637" class="pw-post-body-paragraph li lj it lk b ll me ju ln lo mf jx lq lr mg lt lu lv mh lx ly lz mi mb mc md im bi translated"><em class="mk">原载于 2019 年 4 月 23 日</em><a class="ae mj" href="https://blog.fiddler.ai/2019/04/a-gentle-introduction-to-algorithmic-fairness/" rel="noopener ugc nofollow" target="_blank"><em class="mk">https://blog . fiddler . ai</em></a><em class="mk">。</em></p></div></div>    
</body>
</html>