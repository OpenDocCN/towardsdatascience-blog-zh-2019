<html>
<head>
<title>Explainable Machine Learning for Healthcare</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向医疗保健的可解释机器学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-machine-learning-for-healthcare-7e408f8e5130?source=collection_archive---------19-----------------------#2019-12-26">https://towardsdatascience.com/explainable-machine-learning-for-healthcare-7e408f8e5130?source=collection_archive---------19-----------------------#2019-12-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6e3d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解医疗保健中的机器学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/24151db470369be37b443232a3e47917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4HobEkYU9_ZRSMaKUchiwQ.jpeg"/></div></div></figure><p id="9ed1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">机器学习在医疗诊断等医疗保健领域有着广泛的应用[1]。癌症研究中经常使用的数据集之一是威斯康星州乳腺癌诊断(WBCD)数据集[2]。与其他领域一样，医疗保健中使用的机器学习模型在很大程度上仍然是黑箱。如[3]中所述，如果医生计划根据诊断预测采取癌症治疗措施，了解机器学习模型预测背后的原因对于确定信任非常重要。这种理解也可以帮助具有领域专业知识的医生发现机器学习模型预测的错误。这可以通过不同的方法实现。其中一种方法叫做“可解释的机器学习”[4]。</p><p id="cd5c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我使用 WBCD 数据集[2]来演示如何实现可解释的机器学习，以使不可信的预测变得可信。</p><h1 id="8b18" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">1.可解释的机器学习</h1><p id="e4d9" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如[4]中所述，本文中可解释的机器学习指的是用于理解预训练模型预测的<a class="ae mn" href="https://en.wikipedia.org/wiki/Post_hoc_analysis" rel="noopener ugc nofollow" target="_blank">事后</a>分析和方法。有不同的事后方法，如原因代码生成、模型预测的局部和全局可视化等。[4].在本文中，我使用原因代码生成作为事后方法。特别地，局部可解释模型不可知解释(LIME) [3]用于解释机器学习模型使用哪些特征来做出预测决策。</p><h1 id="e54e" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">2.数据集准备</h1><p id="0018" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如前所述，本文中使用的数据集是公共 WBCD 数据集[2]。在该数据集中，为每个样本提供了以下 9 个特征，并将用作机器学习模型的输入:</p><ul class=""><li id="35d0" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated">团块厚度</li><li id="e606" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">细胞大小的均匀性</li><li id="92dd" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">细胞形状的均匀性</li><li id="1f0b" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">边缘粘连</li><li id="22be" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">单一上皮细胞大小</li><li id="fe55" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">裸核</li><li id="3a06" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">平淡的染色质</li><li id="bdc6" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">正常核仁</li><li id="ef3c" class="mo mp it kw b kx mx la my ld mz lh na ll nb lp mt mu mv mw bi translated">有丝分裂</li></ul><h2 id="e87c" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">2.1 加载数据</h2><p id="57b4" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">假设 WBCD 数据集已经作为<em class="no"> csv </em>文件<em class="no">乳腺癌-威斯康星. csv </em>下载到本地计算机上，那么数据集文件可以加载到熊猫数据帧中，如下所示:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="d4a2" class="nc lr it nq b gy nu nv l nw nx">feature_names = ["CT","UCSize","UCShape","MA","SECSize","BN","BC","NN","Mitoses"]<br/>columns = ["ID"]<br/>columns.extend(feature_names)<br/>columns.extend(["Diagnosis"])<br/>raw_data = pd.read_csv('breast-cancer-wisconsin.csv', na_values='?', header=None, index_col=['ID'], names = columns)<br/>raw_data = raw_data.reset_index(drop=True)<br/>raw_data.head(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/3074571d8f059fcbe81d2912b52d8789.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ce5FknPTQpnZxGTpmSQjLw.png"/></div></div></figure><h2 id="32a3" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">2.2 数据预处理</h2><p id="bf90" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">WBCD 数据集有许多共同的问题，因此需要预处理来解决这些问题，然后才能被机器学习模型使用。</p><ul class=""><li id="1a76" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><strong class="kw iu">缺失数据</strong></li></ul><p id="08ea" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下命令用于检查哪些要素缺少数据:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="41cf" class="nc lr it nq b gy nu nv l nw nx">data = raw_data.copy()<br/>data.isnull().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0b8a8e7a48488e012343846e88083870.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*KWkJ7YPoIRFQCATfugG6Gw.png"/></div></figure><p id="7da0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如上表所示，有 16 个裸核缺失数据条目。在本文中，这些缺失的数据条目简单地用 0 替换，因为缺失条目的总数相对较小。</p><ul class=""><li id="39a0" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><strong class="kw iu">小规模数据集</strong></li></ul><p id="a6ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">WBCD 数据集[2]只包括 699 个样本，这对机器学习来说太小了。这种小数据量问题可以通过数据扩充来缓解(有关详细信息，请参阅不平衡数据集)。</p><ul class=""><li id="8f50" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><strong class="kw iu">不同尺度的特征值</strong></li></ul><p id="ab2e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不同特性的取值范围不同。下面的类<em class="no">尺度</em>是将特征值变换到相同的范围【0，1】内。这个类被设计成可以和其他类一起使用，在以后形成一个管道。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="f94d" class="nc lr it nq b gy nu nv l nw nx">from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.base import BaseEstimator, TransformerMixin</span><span id="18ef" class="nc lr it nq b gy oa nv l nw nx">minMaxScaler = MinMaxScaler()<br/>minMaxScaler.fit(features[feature_names])</span><span id="634f" class="nc lr it nq b gy oa nv l nw nx">class Scale(BaseEstimator, TransformerMixin):<br/>    def __init__(self):<br/>        pass</span><span id="73aa" class="nc lr it nq b gy oa nv l nw nx">    def fit(self, X, y=None):<br/>        return self<br/>    <br/>    def transform(self, X, y=None):  <br/>        if type(X) == pd.Series or type(X) == pd.DataFrame:<br/>            X1 = X.values <br/>            X1 = X1.reshape(1, -1)<br/>        else:<br/>            X1 = X.copy()<br/>         <br/>        X1 = minMaxScaler.transform(X1) <br/>            <br/>        return X1</span></pre><ul class=""><li id="cefc" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><strong class="kw iu">不平衡数据集</strong></li></ul><p id="8e33" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下代码用于检查数据集在标记的诊断值方面是否平衡:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="8c65" class="nc lr it nq b gy nu nv l nw nx">data['Diagnosis'].value_counts()</span></pre><p id="6545" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">不同标签的数据样本数量不平衡。有 458 个数据样本被标记为 2(即 B(良性))，但是只有 241 个数据样本被标记为 4(即 M(恶性))。</p><p id="b0d7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了平衡数据集，为简单起见，将标记为 M 的 241 个数据样本的副本添加到原始数据集中。</p><ul class=""><li id="ef90" class="mo mp it kw b kx ky la lb ld mq lh mr ll ms lp mt mu mv mw bi translated"><strong class="kw iu">倾斜数据集</strong></li></ul><p id="37a1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下代码可用于可视化特征值的分布:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="63e2" class="nc lr it nq b gy nu nv l nw nx">data[numerical].hist(bins=20, figsize=(15, 10))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/c1b1c931afbae245d1d512afeccd19d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPOCiNY7yNzUPtnjKHPktg.png"/></div></div></figure><p id="f2ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">可以看出，大多数特征的值明显向右倾斜。下面的类<em class="no"> Sqrt </em>通过对特征值求平方根来缓解数据偏斜问题。像<em class="no"> Scale </em>类一样，这个类被设计成一个管道组件。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="86c0" class="nc lr it nq b gy nu nv l nw nx">class Sqrt(BaseEstimator, TransformerMixin):<br/>    def __init__(self):<br/>        pass</span><span id="09a5" class="nc lr it nq b gy oa nv l nw nx">    def fit(self, X, y=None):<br/>        return self<br/>    <br/>    def transform(self, X, y=None):  <br/>        if type(X) == pd.Series or type(X) == pd.DataFrame:<br/>            X1 = X.copy()<br/>            <br/>            if X1.isnull().values.any():<br/>                X1 = X1.fillna(0)<br/>                <br/>            X1 = X1.abs() # avoid negative values<br/>        else:<br/>            X1 = X.copy()<br/>            X1 = np.absolute(X1) # avoid negative values<br/>        <br/>        X1 = np.sqrt(X1) <br/>            <br/>        return X1</span></pre><h2 id="cd46" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">2.3 将标签与特征分离</h2><p id="8448" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">标签(即诊断)与特征分离，因为标签和特征是模型训练和测试所需要的。</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="a485" class="nc lr it nq b gy nu nv l nw nx">labels   = data['Diagnosis']<br/>features = data.drop(['Diagnosis'], axis = 1)</span></pre><h2 id="8ebb" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">2.4 将数据集分为训练数据集和测试数据集</h2><p id="8128" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">特征和标签被随机分成两部分，75%用于模型训练，25%用于模型测试:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="3bcb" class="nc lr it nq b gy nu nv l nw nx">from sklearn.model_selection import train_test_split</span><span id="f984" class="nc lr it nq b gy oa nv l nw nx">X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.25, random_state=42)</span></pre><h1 id="5b35" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">3.机器学习模型</h1><p id="39d4" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">具有默认设置的随机森林分类器用于演示目的。模型训练和测试可以如下执行:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="ec37" class="nc lr it nq b gy nu nv l nw nx">from sklearn.ensemble import RandomForestClassifier</span><span id="0ff5" class="nc lr it nq b gy oa nv l nw nx">rfc = RandomForestClassifier(n_estimators=100)<br/>rfc.fit(X_train, y_train)<br/>score = rfc.score(X_test, y_test)</span></pre><p id="44a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">训练后的模型准确率达到 98.23%。</p><h1 id="2a92" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">4.解释预测的石灰法</h1><p id="7b7f" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">如[3]中所述，在通过机器学习模型进行决策时，确定个体预测的可信度非常重要。例如，当机器学习用于医疗诊断时，如威斯康星州乳腺癌诊断[1]，预测不能简单地用于行动，因为后果可能是灾难性的。本节展示了如何使用 LIME [3]为个人预测提供解释，作为确定信任的解决方案。</p><h2 id="698c" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">4.1 创建管道</h2><p id="86b5" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">LIME 中原因代码生成背后的关键点是将模型预测结果与原始的未转换特征值相关联。为此，LIME 方法在解释模型预测结果时，将机器学习管道(包括位于管道开头的数据预处理管道和位于管道末尾的机器学习模型)作为输入。以下管道将用于此目的:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="921f" class="nc lr it nq b gy nu nv l nw nx">from sklearn.pipeline import make_pipeline</span><span id="c0d1" class="nc lr it nq b gy oa nv l nw nx">scale = Scale()<br/>sqrt  = Sqrt()<br/># rfc is a pre-trained Random Forest model</span><span id="9e69" class="nc lr it nq b gy oa nv l nw nx">machine_learning_pipeline = make_pipeline(scale, sqrt, rfc)</span></pre><h2 id="af64" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">4.2 选择石灰解释器</h2><p id="ec0d" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">LIME 方法支持不同类型的机器学习模型解释器，用于不同类型的数据集，如图像、文本、表格数据等。本文选择表格解释器，因为 WBCD 数据集的格式是表格:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="80c3" class="nc lr it nq b gy nu nv l nw nx">from lime.lime_tabular import LimeTabularExplainer</span><span id="16d7" class="nc lr it nq b gy oa nv l nw nx">class_names = ['Benign', 'Malignant']<br/>explainer = LimeTabularExplainer(feature_names=feature_names, <br/>                                 class_names=class_names, <br/>                                 training_data=X_train.values)</span></pre><h2 id="86f4" class="nc lr it bd ls nd ne dn lw nf ng dp ma ld nh ni mc lh nj nk me ll nl nm mg nn bi translated">4.3 解释模型预测</h2><p id="4520" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">一旦机器学习模型已经被训练，给定原始特征值的任何向量(即，特征向量)，下面的<em class="no">解释</em>函数可以用于解释模型预测:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="4d12" class="nc lr it nq b gy nu nv l nw nx">def explain(feature_vector, label=1): # feature_vector - a Pandas Series of features<br/>    exp = explainer.explain_instance(feature_vector, machine_learning_pipeline.predict_proba, num_features=9)<br/>    <br/>    fig = plot(exp, label)<br/>    exp.show_in_notebook(show_table=True, show_all=False)</span></pre><p id="3b25" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">LIME explainer 的<em class="no"> show_in_notebook </em>()方法以笔记本的形式展示了预测和对应的原始特征值之间的关联。</p><p id="1d3b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面的<em class="no">图</em>功能是创建一个条形图来显示石灰模型解释结果:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="57d8" class="nc lr it nq b gy nu nv l nw nx">def plot(exp, label=1):<br/>        exp_list = exp.as_list()<br/>        fig = plt.figure()<br/>        vals = [x[1] for x in exp_list]<br/>        names = [x[0] for x in exp_list]<br/>        vals.reverse()<br/>        names.reverse()<br/>        colors = ['green' if x &lt;= 0 else 'red' for x in vals]<br/>        pos = np.arange(len(exp_list)) + .5<br/>        plt.barh(pos, vals, align='center', color=colors)<br/>        plt.yticks(pos, names)<br/>        if exp.mode == "classification":<br/>            title = 'Local explanation for class {}'.format(exp.class_names[label])<br/>        else:<br/>            title = 'Local explanation'<br/>        plt.title(title)<br/>        <br/>        return fig</span></pre><p id="1d1d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">例 1:解释恶性肿瘤的预测</strong></p><p id="8255" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是 M(恶性)的一个样本(特征向量):</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="a603" class="nc lr it nq b gy nu nv l nw nx">sample_M = raw_data.loc[5, feature_names]</span></pre><p id="bb7f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该特征向量的预测结果可以使用 LIME 解释如下:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="5276" class="nc lr it nq b gy nu nv l nw nx">explain(m_feature_vector, machine_learning_pipeline, label=1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/7d95ff678b682c79fbc23af5c43e07b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YVNpBAh3P6zAr-C-FVbvSA.png"/></div></div></figure><p id="fd60" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">恶性的预测结果如上图左上角所示。右上角和底部显示哪些特征与恶性的预测正相关，哪些特征与恶性的预测负相关。特别地，橙色/红色的特征表示正相关，而蓝色/绿色的特征表示负相关。</p><p id="fbc3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">例 2:解释良性预测</strong></p><p id="bdd0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">以下是 B(良性)特征向量的示例:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="dee4" class="nc lr it nq b gy nu nv l nw nx">sample_B = raw_data.loc[1, feature_names]</span></pre><p id="1d11" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">该特征向量的预测结果可以由 LIME 解释如下:</p><pre class="kj kk kl km gt np nq nr ns aw nt bi"><span id="7ff2" class="nc lr it nq b gy nu nv l nw nx">explain(sample_B, machine_learning_pipeline, label=0)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/ef61202316e00f81cdc46fbfcb747748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kgKFThwtpFEP8f9Pze87QA.png"/></div></div></figure><p id="0f9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">良性的预测结果如上图左上角所示。右上角和底部显示哪些特征与恶性的预测正相关，哪些特征与恶性的预测负相关。特别地，橙色/红色的特征表示正相关，而蓝色/绿色的特征表示负相关。</p><h1 id="b2eb" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">5.结论</h1><p id="3a73" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">在本文中，我使用了 LIME 方法[3]和 WBCD 数据集[2]来演示如何解释机器学习模型在乳腺癌诊断中的预测结果。</p><p id="0af7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如本文开头所述，如果医生计划根据诊断预测采取癌症治疗措施，那么理解机器学习模型预测背后的原因对于评估信任非常重要。这种理解还可以帮助具有领域专业知识的医生检测机器学习模型预测的错误，以便可以进一步改进正在使用的机器学习模型。</p><p id="badb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Github [6]中有一个 Jupyter 笔记本，上面有本文中的所有源代码。</p><h1 id="3afa" class="lq lr it bd ls lt lu lv lw lx ly lz ma jz mb ka mc kc md kd me kf mf kg mg mh bi translated">参考</h1><p id="facc" class="pw-post-body-paragraph ku kv it kw b kx mi ju kz la mj jx lc ld mk lf lg lh ml lj lk ll mm ln lo lp im bi translated">[1] <a class="ae mn" href="https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)" rel="noopener ugc nofollow" target="_blank">威斯康星州乳腺癌诊断数据集</a></p><p id="9e59" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2] <a class="ae mn" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/" rel="noopener ugc nofollow" target="_blank"> WBCD 数据集和描述</a></p><p id="a22a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3] M. T. Ribeiro，S. Singh，C. Guestrin，<a class="ae mn" href="https://arxiv.org/pdf/1602.04938.pdf" rel="noopener ugc nofollow" target="_blank">“我为什么要相信你？”<br/>解释任何分类器的预测</a></p><p id="e503" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4] P. Hall 和 N. Gill,《机器学习可解释性导论》,《从应用角度看公平、问责、透明和可解释的人工智能》,第二版，奥赖利媒体公司，2019 年</p><p id="f84e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5] M. T. Ribeiro，S. Singh 和 C. Guestrin，<a class="ae mn" href="https://www.oreilly.com/learning/introduction-to-local-interpretable-model-agnostic-explanations-lime" rel="noopener ugc nofollow" target="_blank">局部可解释模型不可知解释(LIME):介绍</a></p><p id="6963" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6]黄宇，Jupyter 笔记本在<a class="ae mn" href="https://github.com/yuhuang3/machine-learning/tree/master/lime/tabular_explainer" rel="noopener ugc nofollow" target="_blank"> Github </a></p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="30ce" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">披露声明:2020 年。本文中表达的观点仅代表作者的观点，不代表阿贡国家实验室的观点。</p></div></div>    
</body>
</html>