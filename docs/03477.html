<html>
<head>
<title>ktrain: A Lightweight Wrapper for Keras to Help Train Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">kt rain:Keras 的一个轻量级包装器，用于帮助训练神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c?source=collection_archive---------9-----------------------#2019-06-03">https://towardsdatascience.com/ktrain-a-lightweight-wrapper-for-keras-to-help-train-neural-networks-82851ba889c?source=collection_archive---------9-----------------------#2019-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e6c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae ko" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank"> <em class="kp"> ktrain </em> </a>是一个在深度学习软件框架<a class="ae ko" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>中帮助构建、训练、调试和部署神经网络的库。(从<code class="fe kq kr ks kt b">v0.7</code>开始，ktrain 在 TensorFlow 中使用<code class="fe kq kr ks kt b">tf.keras</code>而不是独立的 Keras。)受<em class="kp"> fastai </em>库的启发，<em class="kp"> ktrain </em>只需几行代码，就能让您轻松:</p><ul class=""><li id="2f4a" class="ku kv it js b jt ju jx jy kb kw kf kx kj ky kn kz la lb lc bi translated">在给定数据的情况下，使用学习率查找器估计模型的最佳学习率</li><li id="e765" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">采用学习率计划，如<a class="ae ko" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">三角学习率政策</a>、<a class="ae ko" href="https://sgugger.github.io/the-1cycle-policy.html" rel="noopener ugc nofollow" target="_blank">1 周期政策</a>和<a class="ae ko" href="https://arxiv.org/abs/1608.03983" rel="noopener ugc nofollow" target="_blank"> SGDR </a>来更有效地训练您的模型</li><li id="22e2" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">为文本分类(例如，NBSVM、fastText、带有预训练单词嵌入的 GRU)和图像分类(例如，ResNet、Wide Residual Networks、Inception)采用快速且易于使用的预设模型</li><li id="91db" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">加载和预处理各种格式的文本和图像数据</li><li id="dc4c" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">检查错误分类的数据点，以帮助改进模型</li><li id="339e" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">利用简单的预测 API 保存和部署模型和数据预处理步骤，对新的原始数据进行预测</li></ul><p id="2dbd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="kp"> ktrain </em>是开源的，可以在 GitHub <a class="ae ko" href="https://github.com/amaiya/ktrain" rel="noopener ugc nofollow" target="_blank">这里</a>获得。它需要 Python 3，可以用 pip 安装如下:<code class="fe kq kr ks kt b">pip3 install ktrain</code></p><p id="5032" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将通过例子展示几个关于<em class="kp"> ktrain </em>的用例。</p><h1 id="13fa" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">将模型和数据包装在学习者对象中</h1><p id="0b6a" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">ktrain 设计用于与 Keras 无缝协作。在这里，我们加载数据并定义模型，就像您在 Keras 中通常所做的那样。以下代码直接复制自<a class="ae ko" href="https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py" rel="noopener ugc nofollow" target="_blank"> Keras fastText 文本分类示例</a>。它加载 IMDb 电影评论数据集，并定义一个简单的文本分类模型来推断电影评论的情感。</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="b275" class="mt lj it kt b gy mu mv l mw mx"><em class="kp"># load  and prepare data as you normally would in Keras</em><br/><strong class="kt iu">from</strong> <strong class="kt iu">tensorflow</strong>.<strong class="kt iu">keras.preprocessing</strong> <strong class="kt iu">import</strong> sequence<br/><strong class="kt iu">from</strong> <strong class="kt iu">tensorflow.keras.datasets</strong> <strong class="kt iu">import</strong> imdb<br/>NUM_WORDS = 20000<br/>MAXLEN = 400<br/><strong class="kt iu">def</strong> load_data():<br/>    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)<br/>    x_train = sequence.pad_sequences(x_train, maxlen=MAXLEN)<br/>    x_test = sequence.pad_sequences(x_test, maxlen=MAXLEN)<br/>    <strong class="kt iu">return</strong> (x_train, y_train), (x_test, y_test)<br/>(x_train, y_train), (x_test, y_test) = load_data()</span><span id="6853" class="mt lj it kt b gy my mv l mw mx"><em class="kp"># build a fastText-like model as you normally would in Keras</em><br/><strong class="kt iu">from</strong> <strong class="kt iu">tensorflow.keras.models</strong> <strong class="kt iu">import</strong> Sequential<br/><strong class="kt iu">from</strong> <strong class="kt iu">tensorflow.keras.layers</strong> <strong class="kt iu">import</strong> Dense, Embedding, GlobalAveragePooling1D<br/><strong class="kt iu">def</strong> get_model():<br/>    model = Sequential()<br/>    model.add(Embedding(NUM_WORDS, 50, input_length=MAXLEN))<br/>    model.add(GlobalAveragePooling1D())<br/>    model.add(Dense(1, activation='sigmoid'))<br/>    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])<br/>    <strong class="kt iu">return</strong> model<br/>model = get_model()</span></pre><p id="53a7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要使用<em class="kp"> ktrain </em>，我们只需使用<code class="fe kq kr ks kt b">get_learner</code>函数将模型和数据包装在一个<code class="fe kq kr ks kt b">ktrain.Learner</code>对象中:</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="c1f8" class="mt lj it kt b gy mu mv l mw mx">import <strong class="kt iu">ktrain</strong><br/>learner = <strong class="kt iu">ktrain.get_learner</strong>(model, <br/>                             train_data=(x_train, y_train),<br/>                             val_data = (x_test, y_test))</span></pre><p id="2e99" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">默认的批处理大小是 32，但是可以通过向<code class="fe kq kr ks kt b">get_learner</code>提供一个<code class="fe kq kr ks kt b">batch_size</code>参数来改变。学习者对象有助于以各种方式训练您的神经网络。例如，调用学习者对象的<code class="fe kq kr ks kt b">fit</code>方法允许你以不同的学习速度进行交互式训练:</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="5bcb" class="mt lj it kt b gy mu mv l mw mx"># train for three epochs at 0.005<br/><strong class="kt iu">learner.fit</strong>(5e-3, 3)</span><span id="6d06" class="mt lj it kt b gy my mv l mw mx"># train for additional three epochs at 0.0005<br/><strong class="kt iu">learner.fit</strong>(5e-4, 3)</span></pre><p id="c0fc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由学习者对象包装的底层 Keras 模型总是可直接访问的，如下:<code class="fe kq kr ks kt b">learner.model</code></p><p id="a3b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">接下来，我们展示了学习者对象也可以用于找到一个好的初始学习率，并且容易地使用各种不同的学习率计划，这些计划在训练期间自动改变学习率。</p><h1 id="25a4" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">调整学习率</h1><p id="2a80" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">学习率是在神经网络中设置的最重要的超参数之一。各种优化器(如 Adam 和 SGD)的默认学习速率可能并不总是适合给定的问题。神经网络中的训练包括最小化损失函数。如果学习率太低，训练将会很慢或者停滞不前。如果学习率太高，损失将无法最小化。这两种情况都会对模型的性能产生负面影响。要为您的模型找到一个最佳的学习率，可以通过以低学习率开始并逐渐增加学习率来模拟训练。<a class="ae ko" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank"> Leslie Smith 展示了</a>当绘制学习率与损失的关系图时，与持续下降的损失相关的最大学习率是一个很好的培训选择。他将此称为 LR 范围测试(也称为 LR Finder)。遵循与<em class="kp"> fastai </em>库类似的语法，这可以在<em class="kp"> ktrain </em>中完成，如下所示:</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="6308" class="mt lj it kt b gy mu mv l mw mx"><strong class="kt iu">learner.lr_find()<br/>learner.lr_plot()</strong></span></pre><p id="9623" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的代码将为上面加载的模型和数据显示以下图形:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5d448b65804a150cf9129559b8ca132f.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*EpHpSHwTtS4_IsTSsujMWg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">We must select the maximal learning rate where the loss is still falling prior to divergence.</figcaption></figure><p id="a1d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据该图，0.005 的学习率似乎是一个合理的选择，因为损失在更高的学习率下开始发散。</p><h1 id="feb5" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">学习费率表</h1><p id="dc84" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">许多研究表明，在训练过程中以各种方式改变学习率可以提高神经模型的性能，使损失最小化，并提高验证的准确性。例如，除了最初的<a class="ae ko" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">1 周期论文</a>之外，Sylvain Gugger 的<a class="ae ko" href="https://sgugger.github.io/the-1cycle-policy.html" rel="noopener ugc nofollow" target="_blank">本实验</a>中还演示了具有循环动量的<a class="ae ko" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">1 周期学习率计划</a>的好处。<em class="kp"> ktrain </em>允许您轻松采用几种不同的学习率策略。这里，我们展示一些例子:</p><p id="b737" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<em class="kp"> ktrain </em>中训练模特的不同方法:</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="518d" class="mt lj it kt b gy mu mv l mw mx"># employs a static learning rate of 0.005 for 3 epochs<br/><strong class="kt iu">learner.fit</strong>(0.005, 3)</span><span id="fb86" class="mt lj it kt b gy my mv l mw mx"># employs an SGDR schedule with a cycle length of one epoch.<br/># learning rate is varied between 0.005 and near-zero value.<br/><strong class="kt iu">learner.fit</strong>(0.005, 3, cycle_len=1)</span><span id="1e36" class="mt lj it kt b gy my mv l mw mx"># employs an SGDR schedule with a cycle length<br/># that increases by a factor of 2 each cycle<br/><strong class="kt iu">learner.fit</strong>(0.005, 3, cycle_len=1, cycle_mult=2)</span><span id="13ff" class="mt lj it kt b gy my mv l mw mx"># employs the 1cycle learning rate policy<br/><strong class="kt iu">learner.fit_onecycle</strong>(0.005, 3)</span><span id="26e2" class="mt lj it kt b gy my mv l mw mx"># employs a triangular learning rate policy with automatic stopping<br/><strong class="kt iu">learner.autofit</strong>(0.005)</span><span id="2aaa" class="mt lj it kt b gy my mv l mw mx"># employs a triangular learning rate policy with both maximum<br/># and base learning rates reduced when validation loss stalls<br/><strong class="kt iu">learner.autofit</strong>(0.005, 20, reduce_on_plateau=3)</span></pre><p id="ea41" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将在下面详细介绍每一种方法，并从 SGDR 学习率政策开始。</p><h2 id="cd8e" class="mt lj it bd lk nh ni dn lo nj nk dp ls kb nl nm lw kf nn no ma kj np nq me nr bi translated">SGDR 学习率政策</h2><p id="2788" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated"><a class="ae ko" href="https://arxiv.org/abs/1608.03983" rel="noopener ugc nofollow" target="_blank">带重启的随机梯度下降(或 SGDR) </a>在用前述学习率探测器识别的初始学习率和接近零的学习率之间循环学习率。使用余弦退火来衰减学习速率。<code class="fe kq kr ks kt b">fit</code>方法允许你以类似于<em class="kp"> fastai </em>库的语法轻松使用 SGDR 学习率策略。当提供<strong class="js iu"> cycle_len </strong>参数时，余弦退火用于衰减周期持续时间内的学习速率。这里，我们显示了两个周期，每个周期具有一个时期的长度:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e9f516d9343b8cdab1034d1912c38d07.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*Sly7PWJ8gv32BhfT7guumg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nt">SGDR:</strong> learner.fit(0.005, 2, cycle_len=1)</figcaption></figure><p id="7bbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> cycle_mult </strong>参数以指定的因子增加循环的长度。这里，周期长度随着每个周期而加倍(<strong class="js iu"> cycle_mult=2 </strong>):</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ebbc0c2c1f0ab6f2d364541cad57c2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*UTkI1ymm8YeUImZE13cfBg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nt">SGDR</strong>: learner.fit(0.005, 3, cycle_len=1, cycle_mult=2)</figcaption></figure><h2 id="ac61" class="mt lj it bd lk nh ni dn lo nj nk dp ls kb nl nm lw kf nn no ma kj np nq me nr bi translated">1 周期和三角学习率政策</h2><p id="6caf" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">除了<code class="fe kq kr ks kt b">fit</code>，还有<code class="fe kq kr ks kt b">autofit</code>方法(采用<a class="ae ko" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">三角学习率策略</a>)和<code class="fe kq kr ks kt b">fit_onecycle</code>方法(采用<a class="ae ko" href="https://arxiv.org/pdf/1803.09820.pdf" rel="noopener ugc nofollow" target="_blank">1 周期策略</a>)。两者都是由 NRL 海军研究实验室的 Leslie Smith 提出的。<code class="fe kq kr ks kt b">fit_onecycle</code>方法在前半段训练中将学习速率从基本速率提高到最大速率，在后半段训练中将学习速率衰减到接近零值。最大学习率是使用上述学习率查找器设置的。</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/57c26e1f920ccb4fce0c2b5d9403c888.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*hg_R6bWOZYZI2uHNONipeg.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nt">1cycle policy</strong>: learner.fit_onecycle(0.005, 3)</figcaption></figure><p id="39fa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，如果将 Adam、Nadam 或 Adamax 优化器与<code class="fe kq kr ks kt b">fit_onecycle</code>一起使用，<a class="ae ko" href="https://www.quora.com/What-is-the-difference-between-momentum-and-learning-rate" rel="noopener ugc nofollow" target="_blank">动量</a>在 0.95 和 0.85 之间循环，使得动量在低学习率时高，动量在高学习率时低。以这种方式改变动量在本文中被提出，并被证明可以加速收敛。</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4981dc5d20397af4046848f9ddf91615.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*nv5V16PGUhjE2qTIbDpaxA.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">cyclical momentum in the <strong class="bd nt">1cycle policy</strong></figcaption></figure><p id="ef40" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe kq kr ks kt b">autofit</code>方法简单地在每个时期执行一个周期策略(可以认为是<a class="ae ko" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank">三角策略</a>的变体):</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/cc7029c3dd454fe32a0da89eb61b1e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*_xsgcNzIRDFBO1uywQryMw.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nt">Triangular Policy</strong>: learner.autofit(0.005, 2)</figcaption></figure><p id="94b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">像这样每个时期执行一个周期，更适合与明显有效的内置 Keras 训练回调一起使用。这样的 Keras 回调可以很容易地通过<code class="fe kq kr ks kt b">autofit</code>的方法参数来启用，例如<strong class="js iu">early _ stopping</strong>(early stopping 回调)、<strong class="js iu">reduce _ on _ plateau</strong>(ReduceLROnPlataeu)和<strong class="js iu">check point _ folder</strong>(model check point)。例如，当<strong class="js iu"> reduce_on_plateau </strong>启用时，如果验证损失没有改善，峰值和基本学习率都会定期降低(或退火),这有助于提高性能:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/bbcc753379a76743715966871fafb9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*FQwWTUmDkgmIHKWMXMIXow.png"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk"><strong class="bd nt">Triangular Policy with ReduceLROnPlateau:</strong> learner.autofit(0.005, 8, reduce_on_plateau=2)</figcaption></figure><p id="6e4b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果未向<code class="fe kq kr ks kt b">autofit</code>提供时期数，则自动启用提前停止回调，训练将继续，直到验证损失不再增加。也有更多的理由让<code class="fe kq kr ks kt b">autofit</code>进一步微调训练过程。在 Jupyter 笔记本中键入<code class="fe kq kr ks kt b">help(learner.autofit)</code>了解更多详情。最后，虽然这里没有显示，但是<code class="fe kq kr ks kt b">autofit</code>方法(像 1 周期策略)在 0.95 和 0.85 之间循环动量。</p><p id="f8b7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在前面的章节中，我们手动定义了一个模型，并在<em class="kp"> ktrain </em>之外加载了数据。<em class="kp"> ktrain </em>公开了许多方便的函数，可以轻松地从各种来源加载数据，并毫不费力地使用一些非常强大的基线模型。我们将展示一个图像分类和文本分类的例子——每一个都只需要几行代码。</p><h1 id="5195" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">图像分类:给狗和猫分类</h1><p id="c8ed" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">图像分类和深度学习介绍中使用的一个标准数据集是<a class="ae ko" href="https://www.kaggle.com/c/dogs-vs-cats" rel="noopener ugc nofollow" target="_blank">狗对猫数据集</a>。我们将在<em class="kp"> ktrain </em>中使用该数据集作为图像分类的示例。在下面的代码块中，<code class="fe kq kr ks kt b">images_from_folder</code>函数用于加载训练和验证图像作为 Keras <a class="ae ko" href="https://keras.io/preprocessing/image/#flow_from_directory" rel="noopener ugc nofollow" target="_blank">目录迭代器</a>对象，并为训练图像增加数据。然后使用<code class="fe kq kr ks kt b">image_classifier</code>函数在<a class="ae ko" href="https://en.wikipedia.org/wiki/ImageNet" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>上建立一个<a class="ae ko" href="https://keras.io/applications/#resnet" rel="noopener ugc nofollow" target="_blank"> ResNet50 </a>模型。我们在目测了<code class="fe kq kr ks kt b">lr_plot</code>生成的剧情后，选择 7e-5 作为学习率。由于我们在本例中调用<code class="fe kq kr ks kt b">autofit</code>时没有指定历元数，当验证损失没有改善时，训练将自动停止。默认情况下，<a class="ae ko" href="https://keras.io/callbacks/#earlystopping" rel="noopener ugc nofollow" target="_blank">提前停止</a>耐心值为 5，而<a class="ae ko" href="https://keras.io/callbacks/#reducelronplateau" rel="noopener ugc nofollow" target="_blank">减少延迟</a>耐心值仅为 2。这些可以使用<code class="fe kq kr ks kt b">autofit</code>的 early_stopping 和 reduce_on_plateau 参数来更改。该代码块的精度通常在<strong class="js iu"> 99.35% </strong>和<strong class="js iu"> 99.55% </strong>之间，如<a class="ae ko" href="https://github.com/amaiya/ktrain/blob/master/tutorial-03-image-classification.ipynb" rel="noopener ugc nofollow" target="_blank">本笔记本</a>所示。</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="9b3c" class="mt lj it kt b gy mu mv l mw mx"># import ktrain modules<br/>import <strong class="kt iu">ktrain</strong><br/>from <strong class="kt iu">ktrain import vision as vis</strong></span><span id="92cb" class="mt lj it kt b gy my mv l mw mx"># get default data augmentation with <br/># horizontal_flipping as only modification<br/>data_aug = <strong class="kt iu">vis.get_data_aug</strong>(horizontal_flip=True)</span><span id="28b6" class="mt lj it kt b gy my mv l mw mx"># load the data as Keras DirectoryIterator generators<br/>(trn, val, preproc) = <strong class="kt iu">vis.images_from_folder</strong>(<br/>                         datadir='data/dogscats',<br/>                         data_aug=data_aug,<br/>                         train_test_names=['train', 'valid'], <br/>                         target_size=(224,224), color_mode='rgb')</span><span id="0a1b" class="mt lj it kt b gy my mv l mw mx"># build a pre-trained ResNet50 model and freeze first 15 layers<br/>model = <strong class="kt iu">vis.image_classifier</strong>('pretrained_resnet50', <br/>                             trn, val, freeze_layers=15)</span><span id="0d89" class="mt lj it kt b gy my mv l mw mx"># wrap model and data in a Learner object<br/>learner = <strong class="kt iu">ktrain.get_learner</strong>(model=model, <br/>                             train_data=trn, val_data=val, <br/>                             workers=8, use_multiprocessing=False,<br/>                             batch_size=64)<br/><strong class="kt iu">learner.lr_find()</strong> # simulate training to find good learning rate<br/><strong class="kt iu">learner.lr_plot()</strong> # visually identify best learning rate</span><span id="7a1d" class="mt lj it kt b gy my mv l mw mx"># train with triangular learning rate policy<br/># ReduceLROnPlateau and EarlyStopping automatically enabled.<br/># ModelCheckpoint callback explicitly enabled.<br/><strong class="kt iu">learner.autofit</strong>(7e-5, checkpoint_folder='/tmp')</span></pre><p id="0bd1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过在训练后调用<code class="fe kq kr ks kt b">learner.view_top_losses(preproc, n=3)</code>，我们可以查看验证集中分类错误最严重的前<strong class="js iu"> n </strong>个例子。这可以揭示如何改进您的模型或数据处理管道，以及是否要删除“垃圾”数据的数据集。例如，在狗和猫的数据集中，下图是验证集中分类错误最多的示例之一:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3fe9fd939106060bbd71fbbc09567ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/1*KZf4j-VFsxtH-V0AndJVow.gif"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">A misclassified example in the validation set</figcaption></figure><p id="4d0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你所看到的，这张图片被贴上了“猫”的标签，尽管图片上同时出现了一只狗和一只猫，但狗的特征更加突出。这可能会有问题，因为该数据集将类视为互斥的。类别不<strong class="js iu">互斥的数据集称为多标签分类问题，将在本文稍后讨论。</strong></p><h2 id="585f" class="mt lj it bd lk nh ni dn lo nj nk dp ls kb nl nm lw kf nn no ma kj np nq me nr bi translated">对新数据的预测</h2><p id="8fb3" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">有了训练好的模型，我们可以将我们的模型和由<code class="fe kq kr ks kt b">images_from_folder</code>返回的<strong class="js iu">预预测</strong>对象包装在一个<strong class="js iu">预测器</strong>对象中，从而轻松地对新的原始图像进行分类:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/1a63accc3dffffee44854fe8a775465d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/1*A0nqK3cLoIxDYg3iEwYY0Q.gif"/></div><figcaption class="nd ne gj gh gi nf ng bd b be z dk">The Predictor object automatically preprocesses raw data before making predictions.</figcaption></figure><p id="74ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> preproc </strong>对象自动预处理并适当转换原始数据，以便准确地做出预测。<strong class="js iu">预测器</strong>对象可以保存到磁盘，并在以后作为已部署应用程序的一部分重新加载:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/f323e2033ba532ba7490d04bfa6e02f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7uoJyz2CHlrTq8fZmoucJA.gif"/></div></div></figure><p id="3ef0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">有关详细的解释和结果，请参见我们的<a class="ae ko" href="https://github.com/amaiya/ktrain/blob/master/tutorial-03-imageclassification.ipynb" rel="noopener ugc nofollow" target="_blank">图像分类教程笔记本</a>。</p><h1 id="0047" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">文本分类:识别有毒在线评论</h1><p id="24b8" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">Kaggle 上的<a class="ae ko" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank">有毒评论分类挑战</a>涉及将维基百科评论分类为一个或多个所谓的<em class="kp">有毒评论的类别</em>。有毒在线行为的类别包括<em class="kp">有毒</em>、<em class="kp">重度 _ 有毒</em>、<em class="kp">淫秽</em>、<em class="kp">威胁</em>、<em class="kp">侮辱</em>、<em class="kp">身份 _ 仇恨</em>。与前面的例子不同，这是一个<strong class="js iu">多标签分类问题</strong>，因为类别不是互斥的。例如，一条评论可能属于多个类别的有害在线行为。<em class="kp"> ktrain </em>从数据中自动检测多标签分类问题，并适当配置内置模型。</p><p id="4f39" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据集可以从<a class="ae ko" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge" rel="noopener ugc nofollow" target="_blank">比赛现场</a>以 CSV 文件的形式下载(即下载文件<a class="ae ko" href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> train.csv </strong> </a>)。我们将使用<code class="fe kq kr ks kt b">texts_from_csv</code>方法加载数据，该方法假设<strong class="js iu"> label_columns </strong>字段已经在电子表格中进行了一次热编码(Kaggle 的<strong class="js iu"> train.csv </strong>文件就是这种情况)。然后，我们将使用<code class="fe kq kr ks kt b">text_classifier</code>方法加载一个类似<a class="ae ko" href="https://arxiv.org/abs/1607.01759" rel="noopener ugc nofollow" target="_blank"> fastText </a>的模型。最后，我们使用<code class="fe kq kr ks kt b">autofit</code>方法来训练我们的模型。在第二个示例中，我们明确地将历元数指定为 8。使用三角形学习率策略，因此执行 8 个三角形循环。</p><pre class="ml mm mn mo gt mp kt mq mr aw ms bi"><span id="29ec" class="mt lj it kt b gy mu mv l mw mx">import <strong class="kt iu">ktrain<br/>from ktrain import text as txt</strong></span><span id="6eb8" class="mt lj it kt b gy my mv l mw mx">DATA_PATH = 'data/toxic-comments/train.csv'<br/>NUM_WORDS = 50000<br/>MAXLEN = 150<br/>label_columns = ["toxic", "severe_toxic", "obscene", <br/>                 "threat", "insult", "identity_hate"]</span><span id="27fd" class="mt lj it kt b gy my mv l mw mx">(x_train, y_train), (x_test, y_test), preproc =    <br/>               <strong class="kt iu">txt.texts_from_csv</strong>(DATA_PATH,<br/>                                 'comment_text',<br/>                                 label_columns=label_columns,<br/>                                 val_filepath=None, <br/>                                 max_features=NUM_WORDS, <br/>                                 maxlen=MAXLEN,<br/>                                 ngram_range=1)</span><span id="a1b9" class="mt lj it kt b gy my mv l mw mx"># define model a fastText-like architecture using ktrain<br/>model = <strong class="kt iu">txt.text_classifier</strong>('fasttext', (x_train, y_train), <br/>                             preproc=preproc)</span><span id="0021" class="mt lj it kt b gy my mv l mw mx"># wrap model and data in Learner object<br/>learner = <strong class="kt iu">ktrain.get_learner</strong>(model, train_data=(x_train, y_train),<br/>                                   val_data=(x_test, y_test))</span><span id="9b61" class="mt lj it kt b gy my mv l mw mx"># find a good learning rate<br/><strong class="kt iu">learner.lr_find()</strong><br/><strong class="kt iu">learner.lr_plot()</strong></span><span id="72a0" class="mt lj it kt b gy my mv l mw mx"># train using triangular learning rate policy<br/><strong class="kt iu">learner.autofit</strong>(0.0007, 8)</span></pre><p id="e33f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的代码块在 Titan V GPU 上仅用 6 分钟的训练就实现了大约为<strong class="js iu"> 0.98 </strong>的 ROC-AUC。如我们 GitHub 项目中的<a class="ae ko" href="https://github.com/amaiya/ktrain/blob/master/examples/text/toxic_comments-bigru.ipynb" rel="noopener ugc nofollow" target="_blank">示例笔记本</a>所示，使用带有预训练单词向量的双向 GRU(在<em class="kp"> ktrain </em>中称为“bigru ”)可以获得更好的结果。</p><p id="620e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和前面的例子一样，我们可以实例化一个<strong class="js iu">预测器</strong>对象来轻松地对新的原始数据进行预测:</p><figure class="ml mm mn mo gt na gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi of"><img src="../Images/2cbb36152afd3a4fd7c629d32c27c0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*IlquAoJ-10osaFdFzX9lEA.gif"/></div></div></figure><h1 id="e469" class="li lj it bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">更多信息</h1><p id="f52b" class="pw-post-body-paragraph jq jr it js b jt mg jv jw jx mh jz ka kb mi kd ke kf mj kh ki kj mk kl km kn im bi translated">有关<em class="kp"> ktrain </em>的更多信息和细节，请参见 GitHub 上的<a class="ae ko" href="https://github.com/amaiya/ktrain/" rel="noopener ugc nofollow" target="_blank">教程笔记本:</a></p><ul class=""><li id="4c7f" class="ku kv it js b jt ju jx jy kb kw kf kx kj ky kn kz la lb lc bi translated">教程笔记本 1: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-01-introduction.ipynb" rel="noopener ugc nofollow" target="_blank">简介<em class="kp">k train</em>T30】</a></li><li id="c198" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 2: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-02-tuning-learning-rates.ipynb" rel="noopener ugc nofollow" target="_blank">调整学习率</a></li><li id="4c2c" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 3: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-03-image-classification.ipynb" rel="noopener ugc nofollow" target="_blank">图像分类</a></li><li id="5501" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 4: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-04-text-classification.ipynb" rel="noopener ugc nofollow" target="_blank">文字分类</a></li><li id="57f4" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 5: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-05-learning_from_unlabeled_text_data.ipynb" rel="noopener ugc nofollow" target="_blank">从无标签文本数据中学习</a></li><li id="156d" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 6: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-06-sequence-tagging.ipynb" rel="noopener ugc nofollow" target="_blank">文本序列标注</a></li><li id="d21f" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 7: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-07-graph-node_classification.ipynb" rel="noopener ugc nofollow" target="_blank">图形神经网络</a></li><li id="04b3" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 A1: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-A1-additional-tricks.ipynb" rel="noopener ugc nofollow" target="_blank">关于其他主题的附加技巧</a>，例如为模型设置全局权重衰减、调试 Keras 模型、预览数据扩充方案，以及在<em class="kp"> ktrain </em>中使用内置回调</li><li id="5670" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 A2: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/master/tutorials/tutorial-A2-explaining-predictions.ipynb" rel="noopener ugc nofollow" target="_blank">讲解文字和图像分类</a></li><li id="de3b" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated">教程笔记本 A3: <a class="ae ko" href="https://nbviewer.jupyter.org/github/amaiya/ktrain/blob/develop/tutorials/tutorial-A3-hugging_face_transformers.ipynb" rel="noopener ugc nofollow" target="_blank">文字分类与抱脸变形金刚</a></li><li id="391a" class="ku kv it js b jt ld jx le kb lf kf lg kj lh kn kz la lb lc bi translated"><a class="ae ko" href="https://github.com/amaiya/ktrain/tree/master/examples" rel="noopener ugc nofollow" target="_blank">附加示例</a></li></ul><p id="47ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">2019–08–16:</strong>在<strong class="js iu"> Google Colab </strong>上使用<em class="kp"> ktrain </em>？请参见 BERT 的演示。</p></div></div>    
</body>
</html>