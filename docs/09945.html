<html>
<head>
<title>An Intro to Kernels</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">内核介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-intro-to-kernels-9ff6c6a6a8dc?source=collection_archive---------9-----------------------#2019-12-29">https://towardsdatascience.com/an-intro-to-kernels-9ff6c6a6a8dc?source=collection_archive---------9-----------------------#2019-12-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/447e086ce80fb25a1b93fc5f7e1616be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gQXRhVT8vrS6_rP8uEt4yw.jpeg"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Mathematical tricks</figcaption></figure><p id="b4cc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">内核是神奇的。</p><p id="be1e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不完全是，但是他们看起来很喜欢。它们是一种数学“技巧”，允许我们不需要对数据进行技术转换就可以更快地进行某些计算。在机器学习中，它们在分类模型中用于将我们的数据分成组。对相似数据进行分组的最简单的方法是用直线，但这并不总是可行的。有时，我们的数据是以这样一种方式组织的，用一条直线将它们分开是不可能的。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi kz"><img src="../Images/f5e890bcb7bd8f1a6e464c5cd3b44f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YCB7aRSGpVklAwTyWP33Vw.png"/></div></div></figure><p id="e91f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">那么当我们有不可线性分离的数据时，我们能做什么呢？一个解决方案是投影我们的数据。这可以通过为我们的数据创建一个新的维度(或特征)来实现，这样我们的数据可以变得更加独立。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="gh gi li"><img src="../Images/6fb8c8238d86b92020752c7d12562961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aNSIzDGvpJaoBMV-4xvSKw.png"/></div></div></figure><p id="4d3d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将我们的数据映射到一个更高维的空间是通过一个我们称之为 phi (𝜙).)的函数来完成的</p><p id="c3e2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将数据映射到更高维度空间的问题在于，它在计算上可能是昂贵的。映射函数𝜙必须应用于每个数据点，然后我们仍然必须使用包含的新功能对数据进行计算。当处理大量数据和添加许多新功能时，计算成本会呈指数级增长。</p><p id="c9f8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对我们来说幸运的是，内核的出现扭转了局面。由于我们只需要数据点的内积来计算支持向量机的决策障碍，这是一种常见的分类模型，因此内核允许我们跳过将数据映射到更高维空间的过程，直接计算内积。数学上，核函数的定义是:</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/8676d441021848eed765e6f0eb35ea98.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*qTZDgxudR-IkHJloVktbGA.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Definition of a kernel</figcaption></figure><p id="8fd1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中<em class="lk"> x </em>和<em class="lk"> y </em>是独立的数据点，𝜙是将我们的数据映射到更高维空间的东西，两端的尖括号意味着你取整个语句的内积。</p><p id="f2ad" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了被认为是内核，函数必须满足一些要求。</p><ul class=""><li id="5008" class="ll lm it kd b ke kf ki kj km ln kq lo ku lp ky lq lr ls lt bi translated">函数需要是连续的，这意味着在其定义域中不能有任何缺失点</li><li id="f804" class="ll lm it kd b ke lu ki lv km lw kq lx ku ly ky lq lr ls lt bi translated">它必须是对称的，这意味着<em class="lk"> K(x，y) = K(y，x) </em></li><li id="f193" class="ll lm it kd b ke lu ki lv km lw kq lx ku ly ky lq lr ls lt bi translated">它具有正半定性。这意味着核是具有非负特征值的对称矩阵。</li></ul><p id="15b7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有几十种核用于各种不同的问题，所以让我们来看看机器学习中最常见的三种核，线性核、多项式核和径向基函数核。</p><h1 id="698e" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">线性核</h1><p id="1c75" class="pw-post-body-paragraph kb kc it kd b ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku nb kw kx ky im bi translated">也称为“非内核”，线性内核是所有内核中最简单的。从技术上讲，当使用这个内核时，数据不会被投影到更高的维度上，所以它只是带有可选常数项<em class="lk"> c </em>的<em class="lk"> x </em>和<em class="lk"> y </em>的内积。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f83b55e46bd1b78e4f0e603f055363f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*tldcETd28fv_0uF5PK3NOQ.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Linear kernel equation</figcaption></figure><p id="315f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">线性核的好处是它非常简单，只有常数项<em class="lk"> c </em>作为参数。线性核通常用于具有大量特征的数据集，因为增加这些数据集的维度并不一定提高可分性。文本分类就是这类数据集的典型例子。</p><h1 id="bfef" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">多项式核</h1><p id="2fae" class="pw-post-body-paragraph kb kc it kd b ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku nb kw kx ky im bi translated">与线性核不同，多项式核确实涉及到从更高维度空间取内积。多项式核可以表示为</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/5b272a1a379d93a04948e39f0e500783.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*Oc0sAmIPv2bJ1fR4Aj9ECg.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Polynomial kernel equation</figcaption></figure><p id="fce3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中三个参数是𝛼、<em class="lk"> c </em>和<em class="lk"> d </em>。最常用的度数(<em class="lk"> d </em>)是 2，因为更大的度数会导致过度拟合。多项式核通常用于自然语言处理问题。</p><p id="c771" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们看一个例子。让我们设定𝛼=1，<em class="lk"> c </em> =1/2，<em class="lk"> d=2，</em>使这个例子成为二次型。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/6bdd2fdebe73629ccfc02354148e25a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*QJ3-rP1IjjF1CMLiMHz7qg.png"/></div></figure><p id="c9fd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如我们所见，这个内核相当于𝜙<em class="lk">(x)</em><em class="lk"/>【𝜙】<em class="lk">(y)</em>的内积，其中𝜙函数的每个元素都表示变量的不同幂。如果我们使用𝜙函数，我们将需要评估六个特征(<em class="lk"> x，x，1/2，y，y，1/2 </em>)。内核省去了我们创建四个新特性的麻烦，只留给我们评估<em class="lk"> x </em>和<em class="lk"> y. </em></p><h1 id="75e1" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">径向基函数核</h1><p id="bf88" class="pw-post-body-paragraph kb kc it kd b ke mx kg kh ki my kk kl km mz ko kp kq na ks kt ku nb kw kx ky im bi translated">径向基函数(RBF)核是支持向量机中最常用的核。它被定义为</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/ec37e202bdf86b965937a8432c21daea.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*6tyxkQIYanOvpBCWOesJnQ.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Radial basis function kernel equation</figcaption></figure><p id="9978" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">其中，𝛾是一个自由参数，用于衡量两点之间的相互影响程度。与着眼于额外维度的多项式核不同，RBF 扩展到无限多个维度。这是由于指数的膨胀。为了使人信服，让我们设定𝛾 = 1/2 并扩展指数。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/4fc562dfff85e2d091332f2016685714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*S_aOeCPRELLP2KVXwJ0HzA.png"/></div></figure><p id="60fe" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">从这里，<em class="lk"> exp(x，y) </em>可以使用泰勒级数近似展开成无限维。这看起来像</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3fda7ee5c28333b99cb478cdd4bcf445.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*wPAFeWTDblGtSAQfwEK-Dg.png"/></div></figure><p id="ed26" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这意味着两个向量的点积可以表示为</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ed277e231a0b3a748625086aaa333100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*Od6V-y2meyhXduV5BWBVSQ.png"/></div></figure><p id="1971" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以我们可以看到，一个 RBF 核等价于两个数据点的内积，这两个数据点有无限多个维度。我们现在可以明白为什么 RBF 如此受欢迎了。</p><p id="439b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我希望这篇博客能帮助你更好地理解内核。如果你还有问题，我推荐<a class="ae nj" href="https://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf" rel="noopener ugc nofollow" target="_blank">这篇论文</a>和以下来源:</p><div class="nk nl gp gr nm nn"><a href="https://en.wikipedia.org/wiki/Kernel_method" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">核方法</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">在机器学习中，核方法是一类用于模式分析的算法，其最著名的成员是核方法</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">en.wikipedia.org</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jv nn"/></div></div></a></div></div></div>    
</body>
</html>