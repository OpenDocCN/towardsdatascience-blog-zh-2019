<html>
<head>
<title>Using a Generalised Translation Vector for Handling Misspellings and Out-of-Vocabulary (OOV) words like Abbreviations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用一般化的翻译向量来处理拼写错误和词汇外(OOV)单词，如缩写</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-a-generalised-translation-vector-for-handling-misspellings-and-out-of-vocabulary-oov-words-494cd142cd31?source=collection_archive---------27-----------------------#2019-11-04">https://towardsdatascience.com/using-a-generalised-translation-vector-for-handling-misspellings-and-out-of-vocabulary-oov-words-494cd142cd31?source=collection_archive---------27-----------------------#2019-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8bf4be9d8e1c231761fe3828e5d086ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d-zxYh0jJqCKbUCH"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@andreasonda?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrea Sonda</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h1 id="f514" class="kn ko jj bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">介绍</h1><p id="7b52" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">在这篇文章中，我将分享一种管理拼写错误和缩写的新方法。这个想法不是我自己的，但我认为它真的很有趣，并决定在这里测试和展示我自己的成果。</p><blockquote class="mj mk ml"><p id="f24c" class="ll lm mm ln b lo mn lq lr ls mo lu lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated">这个想法来自艾德·拉什顿，他来自一个<a class="ae jg" href="https://forums.fast.ai/t/nlp-any-libraries-dictionaries-out-there-for-fixing-common-spelling-errors/16411/7" rel="noopener ugc nofollow" target="_blank">快速人工智能论坛</a>的帖子，是我在做自己的研究时偶然发现的。我强烈建议你阅读他的初步测试。</p></blockquote><p id="8193" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我采取了额外的步骤来测量将 Ed 的方法应用于下游自然语言处理(NLP)任务的效果，以查看它是否会提高分类准确性。</p><p id="1aff" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">像使用外部数据源来创建一个广义的转换向量或测试缩写/OOV 词的方法是我自己的。</p><p id="ad0a" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">但是在我开始之前，了解什么是单词嵌入以及它们为什么重要是很重要的。如果您需要快速复习，请花 3-5 分钟通读关于词汇外(OOV)单词嵌入系列的<a class="ae jg" rel="noopener" target="_blank" href="/creating-word-embeddings-for-out-of-vocabulary-oov-words-such-as-singlish-3fe33083d466"> <strong class="ln jk">第一部分</strong>。</a></p><p id="9232" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">文章链接还谈到了为 OOV 单词生成单词嵌入，比如新加坡英语。这篇文章实际上是我关于如何最好地使用新加坡式英语的研究的延伸。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><p id="43b2" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">回忆一下，我们开始吧。</p><p id="7634" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">这种方法的基本原理来自于单词嵌入<strong class="ln jk">如何与另一个</strong>交互。这种“互动”的经典词语类比例子如下:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/feb437bd2458d27823de4768e0e538d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*FTWAWC7D9tPAehIiHoy4RA.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 1 — Word Analogy “Equation”</figcaption></figure><p id="569b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">直觉上，这个英语中的“等式”应该对你有意义。在向量空间中，数学将是这样的:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/39679454e91009b62db7c0b4790b698d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*wmU1T2jrJ8WGP0UNJJSK1w.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 2 — Word Analogy Example in Vector Space — <a class="ae jg" href="https://twitter.com/toshi2fly/status/911306344376012800" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="c644" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">为了向您展示这是真实的情况，我已经使用来自斯坦福的预训练单词嵌入运行了一些代码。这些单词嵌入在维基百科 2014 + Gigaword 上进行训练，以获得 60 亿个标记和 40 万个独特的词汇单词。</p><p id="f5da" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">由于每个单词都表示在一个 100 维的空间中(即 100 个数字)，为了可视化的目的，我应用了 T-分布式随机邻居嵌入(T-SNE)将维度减少到只有 2。</p><p id="d8a8" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">下面的图 3 是感兴趣的单词和数学方程的可视化，帮助你更好地理解单词之间的关系。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/05a4786c925786a4fbf6c418e98d815e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtaRnBFSCYrzmu6kL3QLLg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 3 — Word Analogy Equation</figcaption></figure><p id="80fc" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">正如您所看到的，经过适当训练的单词嵌入可以真实地表示单词之间的关系。模型生成的数字向量不仅仅是随机的。这些数字确实有一定的意义。</p><p id="cb72" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">你可能不知道，我上面提到的是<strong class="ln jk">平移向量</strong>的实现。</p><blockquote class="mj mk ml"><p id="2f64" class="ll lm mm ln b lo mn lq lr ls mo lu lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated"><strong class="ln jk">平移向量</strong>是一种将坐标平面中的点/图形从一个位置移动到另一个位置的变换。</p></blockquote><blockquote class="nc"><p id="5204" class="nd ne jj bd nf ng nh ni nj nk nl mi dk translated">这是一个翻译向量，我将使用它来将拼写错误/缩写的单词移向最能解释这些单词意思的单词。</p></blockquote><p id="1b27" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated"><em class="mm">“等一下...你是说有一个翻译向量可以将拼写错误的单词移动到向量空间中相应的正确拼写上吗？”</em></p><p id="aa78" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">答案是肯定的。</p><p id="6eee" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">最重要的是，这个翻译工具甚至可以翻译缩写或 OOV 单词。这将使这些单词更接近于最能代表缩写或 OOV 单词真正含义的单词。</p><h1 id="8f17" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">那么广义翻译向量是如何创建的呢？</h1><blockquote class="mj mk ml"><p id="a913" class="ll lm mm ln b lo mn lq lr ls mo lu lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated">这一系列的测试是由斯坦福大学使用手套单词嵌入法进行的。</p></blockquote><p id="31a3" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">在这一部分，我将首先分享处理拼写错误单词的实验。然后，我将转向我所做的处理缩写或 OOV 单词的实验。在此之后，我将试图解释为什么<strong class="ln jk">我认为</strong>这个翻译向量是有效的。最后测试这种方法对下游 NLP 任务的影响。</p><h2 id="c142" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">处理拼写错误的单词</h2><p id="5c12" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">为了处理拼写错误的单词，我最初认为拼写错误的单词的正确拼写应该出现在拼写错误的单词附近。</p><p id="aee3" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">令我惊讶的是，我错了。事实完全不是这样。</p><p id="ff6e" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">以下是一些拼写错误的单词及其在向量空间中最接近的邻居的输出。</p><pre class="mw mx my mz gt oi oj ok ol aw om bi"><span id="a597" class="nw ko jj oj b gy on oo l op oq">Input word: becuase<br/>Closest 10:<br/>becuase 1.0<br/>becasue 0.768174409866333<br/>beause 0.740034282207489<br/>beacuse 0.7367663979530334<br/>becaue 0.7192652225494385<br/>athough 0.6518071889877319<br/>althought 0.6410444378852844<br/>becuse 0.6402466893196106<br/>inspite 0.631598711013794<br/>beleive 0.6224651336669922</span><span id="71d3" class="nw ko jj oj b gy or oo l op oq">Input word: athough<br/>Closest 10:<br/>athough 0.9999999403953552<br/>altough 0.7992535829544067<br/>althought 0.7640241980552673<br/>eventhough 0.7555050849914551<br/>&amp;#8220; 0.7399924993515015<br/>addding 0.7239811420440674<br/>lthough 0.7079077363014221<br/>surmising 0.6986074447631836<br/>howver 0.6851125359535217<br/>aknowledged 0.6843773126602173</span><span id="a70c" class="nw ko jj oj b gy or oo l op oq">Input word: aknowledged<br/>Closest 10:<br/>aknowledged 1.0<br/>ackowledged 0.8641712665557861<br/>inisted 0.8378102779388428<br/>admited 0.8242745399475098<br/>annonced 0.81769859790802<br/>avowing 0.7911248803138733<br/>testifed 0.7896023392677307<br/>addding 0.7784746885299683<br/>sasid 0.7712235450744629<br/>acknowleges 0.7595445513725281</span></pre><p id="8934" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">注意在上面所有的例子中，前 10 个邻居中没有一个是正确拼写的单词？</p><p id="1d97" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我开始在越来越多拼错的单词上运行最近邻，直到一个模式出现，我得出结论…</p><blockquote class="nc"><p id="3e56" class="nd ne jj bd nf ng os ot ou ov ow mi dk translated">所有拼写错误似乎都在向量空间中被分组在一起。</p></blockquote><blockquote class="mj mk ml"><p id="a97c" class="ll lm mm ln b lo nm lq lr ls nn lu lv mp no ly lz mr np mc md mt nq mg mh mi im bi translated"><strong class="ln jk">注意:这里着重强调了“似乎”这个词。因为我真的不确定。</strong></p></blockquote><p id="8d35" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">如果我粗略地把它画出来，它<strong class="ln jk">可能</strong>看起来像这样:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ox"><img src="../Images/3fec90ad806a74ce82041daf966a6a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwF54j0dEGM6Rls7x2R43g.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 4 — Misspelled words location in vector space</figcaption></figure><p id="9b9b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">下一步是创建一个翻译向量，使拼写错误的单词嵌入更接近正确的拼写，如:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/bb975bad6f97bfdebae770efc4462eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KNvRgySjK0U7TjyDtc9uLw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 5 — Translation of misspelled words to correctly spelled words</figcaption></figure><p id="08dc" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">过程是这样的:</p><ol class=""><li id="e7f7" class="oz pa jj ln b lo mn ls mo lw pb ma pc me pd mi pe pf pg ph bi translated">运行余弦距离得到最近的 10 个单词拼写错误的选择。</li><li id="5280" class="oz pa jj ln b lo pi ls pj lw pk ma pl me pm mi pe pf pg ph bi translated">对于 10 个最接近的单词中的每一个，取每个单词的单词向量，并将其从正确拼写的单词的单词向量中减去。</li><li id="50d4" class="oz pa jj ln b lo pi ls pj lw pk ma pl me pm mi pe pf pg ph bi translated">在步骤 2 中计算所有这些单词向量的平均值。这成为步骤 1 中所选单词的翻译向量。</li><li id="62f2" class="oz pa jj ln b lo pi ls pj lw pk ma pl me pm mi pe pf pg ph bi translated">测试平移向量。</li></ol><pre class="mw mx my mz gt oi oj ok ol aw om bi"><span id="e352" class="nw ko jj oj b gy on oo l op oq">STEP 1:<br/>Input word: becuase<br/>Closest 10:<br/>becuase 1.0<br/>becasue 0.768174409866333<br/>beause 0.740034282207489<br/>beacuse 0.7367663979530334<br/>becaue 0.7192652225494385<br/>athough 0.6518071889877319<br/>althought 0.6410444378852844<br/>becuse 0.6402466893196106<br/>inspite 0.631598711013794<br/>beleive 0.6224651336669922</span><span id="dc15" class="nw ko jj oj b gy or oo l op oq">STEP 2:<br/>sum_vec = ((glove["because"]-glove["becuase"]) + (glove["because"]-glove["becasue"]) + <br/>       (glove["because"]-glove["belive"]) + (glove["because"]-glove["beause"]) + <br/>       (glove["because"]-glove["becaue"]) + (glove["because"]-glove["beleive"]) + <br/>       (glove["because"]-glove["becuse"]) + (glove["because"]-glove["wont"]) +<br/>       (glove["because"]-glove["inspite"]) + (glove["because"]-glove["beleive"])<br/>      )</span><span id="fe0d" class="nw ko jj oj b gy or oo l op oq">STEP 3:<br/>translation_vec = sum_vec / 10</span><span id="87b8" class="nw ko jj oj b gy or oo l op oq">STEP 4:<br/>real_word = glove["becuase"] + translation_vec</span><span id="d188" class="nw ko jj oj b gy or oo l op oq">Closest 10:<br/>because 0.9533640742301941<br/>but 0.8868525624275208<br/>though 0.8666126728057861<br/>even 0.8508625030517578<br/>when 0.8380306363105774<br/>if 0.8379863500595093<br/>so 0.8338960409164429<br/>although 0.8258169293403625<br/>that 0.8221235871315002<br/>. 0.8172339797019958</span></pre><p id="6606" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">看看上面第 4 步的结果。注意平移向量是如何工作的？单词“becuase”的拼写错误被推到了正确的拼写“因为”。</p><p id="d7d0" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">既然我们知道处理拼写错误的翻译向量是可能的…</p><p id="c45c" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">现在的问题是:</p><blockquote class="nc"><p id="6e4b" class="nd ne jj bd nf ng os ot ou ov ow mi dk translated">我们能为所有拼错的单词创建一个通用的翻译向量吗？</p></blockquote><p id="6037" class="pw-post-body-paragraph ll lm jj ln b lo nm lq lr ls nn lu lv lw no ly lz ma np mc md me nq mg mh mi im bi translated">是的，你可以。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><p id="d4be" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated"><strong class="ln jk">创建广义翻译向量</strong></p><p id="be4a" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我先从维基百科上拿了一个经常拼错的英文单词列表——<a class="ae jg" href="https://en.wikipedia.org/wiki/Commonly_misspelled_English_words" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Commonly _ misselled _ English _ words</a>。</p><p id="4c61" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">总共有 257 个拼错的单词。</p><p id="bd7e" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">接下来，我按照上面提到的步骤(步骤 1 到 4)运行每个单词。在 257 个单词中，只有 101 个出现在 GloVe 中。因此，计算了 101 个平移向量。</p><p id="0d25" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">下一步很简单，要得到广义的翻译向量，只需得到所有 101 个翻译向量的平均值。</p><p id="911a" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">现在，在看不见的拼写错误的单词上测试广义翻译向量。即不在“经常拼写错误的英语单词”列表中的单词。</p><p id="5e52" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">以下是一些结果:</p><pre class="mw mx my mz gt oi oj ok ol aw om bi"><span id="6820" class="nw ko jj oj b gy on oo l op oq">TEST 1:<br/>real_wd = word2vec["altough"] + generalised_translation_vector<br/>Closest 10:<br/>although 0.6854726149933246<br/>though 0.6850398829597103<br/>fact 0.6601790765388513<br/>however 0.6519905808301287<br/>moreover 0.6505953960788824<br/>unfortunately 0.6351817153852294<br/>why 0.6347064566357319<br/>neither 0.6276211965351607<br/>because 0.626889292147095<br/>there 0.6217807488951306</span><span id="b412" class="nw ko jj oj b gy or oo l op oq">TEST 2:<br/>real_wd = word2vec["belive"] + generalised_translation_vector<br/>Closest 10:<br/>belive 0.7508078651443635<br/>believe 0.6821138617649115<br/>we 0.6761371769103022<br/>think 0.6593009947423304<br/>i 0.6531635268562578<br/>suppose 0.6463823815449183<br/>know 0.643466951434791<br/>why 0.6394517724397308<br/>n't 0.6282966846577815<br/>there 0.6273096464496348</span><span id="a307" class="nw ko jj oj b gy or oo l op oq">TEST 3:<br/>real_wd = word2vec["howver"] + generalised_translation_vector<br/>Closest 10:<br/>moreover 0.6818620293021886<br/>nevertheless 0.6726130620400201<br/>neither 0.6667918863182073<br/>unfortunately 0.6568077159269134<br/>though 0.6563466579691621<br/>indeed 0.6555591633099542<br/>nor 0.6473033198348439<br/>noting 0.6457127023810979<br/>fact 0.6423381172434424<br/>however 0.639381512943384</span></pre><p id="7e1b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">如你所见，仅仅在 101 个平移向量上，广义平移向量的表现并不太差。</p><p id="9436" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">然而，由于它只在 101 个翻译向量上建模，它在单词“howver”上表现不佳。如果我们对更多的平移向量进行平均，这应该很容易纠正。</p><h2 id="5c7d" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">这个一般化的翻译向量在缩写的 OOV 单词上表现如何？</h2><p id="9bc0" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">为了测试这一点，我键入了一个缩写“ftw”来寻找它的最近邻居<strong class="ln jk">，而没有</strong>应用广义翻译向量。</p><p id="da91" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">结果就是这样。</p><pre class="mw mx my mz gt oi oj ok ol aw om bi"><span id="b2ef" class="nw ko jj oj b gy on oo l op oq">Closest 10:<br/>ftw 1.0<br/>ftg 0.6485665440559387<br/>ccts 0.6331672072410583<br/>srw 0.6109601855278015<br/>efts 0.6108307242393494<br/>cmtc 0.6050553321838379<br/>tfts 0.6022316217422485<br/>okl 0.6015480756759644<br/>jrtc 0.597512423992157<br/>cacs 0.5950496196746826</span></pre><p id="94be" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">如您所见，所有缩写似乎都被分组到了同一个向量空间中。这还没有真正的价值。</p><p id="4bd2" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">让我们应用广义平移向量，看看会发生什么。</p><pre class="mw mx my mz gt oi oj ok ol aw om bi"><span id="c4a2" class="nw ko jj oj b gy on oo l op oq">real_wd = word2vec["ftw"] + generalised_translation_vector</span><span id="02ef" class="nw ko jj oj b gy or oo l op oq">Closest 10:<br/>ftw 0.6879674996316458<br/>training 0.5672708687073991<br/>wing 0.5443954733951136<br/>) 0.5178717182506045<br/>fighter 0.4847256601149461<br/>flying 0.4701913180805779<br/>tactical 0.4680302804591241<br/>combat 0.4674059145693783<br/>squadrons 0.4593665765485848<br/>foot 0.459181622011263</span></pre><p id="02c7" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">注意到这里有什么有趣的吗？如果你用谷歌搜索“ftw 战斗”，你会发现“ftw”代表“飞行训练联队”。</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pn"><img src="../Images/b081727aeea5e996c43c2d45d149fc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wgqw98wRDbjSRo4QTnpZkg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 6 — Google search results for “ftw combat”</figcaption></figure><p id="d8d3" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">这表明广义翻译向量甚至对缩写也起作用，而不仅仅局限于拼写错误。</p><p id="12d6" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">它设法将“ftw”的缩写翻译成它在文本中真正代表的意思。即军字。</p><p id="2c65" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">现在你已经看到了结果，如果你像我一样，你会问自己这怎么可能。下一部分是我对我认为正在发生的事情的<strong class="ln jk">假设</strong>。</p></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><h2 id="6918" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">为什么我认为广义翻译向量有效</h2><blockquote class="mj mk ml"><p id="0644" class="ll lm mm ln b lo mn lq lr ls mo lu lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated">这一段纯属假设。我不认为这些是真的。我在这里所说的一切都仅仅是基于对结果的回顾和对我认为正在发生的事情的推断。</p></blockquote><p id="665d" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">回想一下上面的图 4 和图 5。</p><p id="670a" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我认为正在发生的事情大概是这样的:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi po"><img src="../Images/1c6bc5b338e5f96193904ab6975ff285.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bv-cB6ijH-2Azzt06On7Vg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 7 — Why Generalised Translation Vector works</figcaption></figure><p id="a7e3" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">首先，根据我的测试，本地化的翻译向量似乎工作得最好，即更好的准确性。</p><blockquote class="mj mk ml"><p id="9c67" class="ll lm mm ln b lo mn lq lr ls mo lu lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated">本地化的翻译向量指的是具有缩写或拼写错误的单词的聚类的向量。</p></blockquote><p id="f85d" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">但我认为广义平移向量是可行的，因为它只是所有局部平移向量的平均值。它只是将蓝色圆圈中的所有东西推回到橙色圆圈中。</p><p id="25bc" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">这是基于这样的假设:所有拼错的<strong class="ln jk">、缩写或 OOV 单词都聚集在蓝色圆圈中。</strong></p><p id="7eee" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">因为每个拼写错误的单词、缩写词或 OOV 单词都在一起，所以拥有一个通用的翻译向量只会将这些单词推向真正代表它们实际意思的单词。</p><p id="f09f" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated"><strong class="ln jk">重要提示:</strong></p><ol class=""><li id="4db9" class="oz pa jj ln b lo mn ls mo lw pb ma pc me pd mi pe pf pg ph bi translated">翻译向量只对它被训练的单词起作用。我曾试图从 GloVe 中提取一个通用的翻译向量，并尝试将其应用于一个完全不同的数据集，但没有成功。</li><li id="fb66" class="oz pa jj ln b lo pi ls pj lw pk ma pl me pm mi pe pf pg ph bi translated">为了纠正拼写错误，正确拼写的单词必须首先存在于语料库中。</li></ol></div><div class="ab cl kg kh hx ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="im in io ip iq"><blockquote class="mj mk ml"><p id="4c1e" class="ll lm mm ln b lo mn lq lr ls mo lu lv mp mq ly lz mr ms mc md mt mu mg mh mi im bi translated">看到目前为止的结果，你认为模型学到了什么？我的假设有意义吗？你的假设是什么？</p></blockquote><h1 id="93e0" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">广义平移向量的应用</h1><p id="5f21" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">说了这么多，做了这么多，你可能想知道的下一个问题是:</p><p id="7840" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated"><strong class="ln jk"> <em class="mm">“那又怎样？我能用这些知识做什么？”</em> </strong></p><p id="0693" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">在这一部分，我将分享两组代码在一个简单的多分类评论毒性任务中运行的结果。即有毒、剧毒、淫秽、威胁、侮辱或人身攻击。</p><p id="9a9b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">以下是训练集的大致情况:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pp"><img src="../Images/71c52b99743029bd2ecbd29a8c70f0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DYu19giNWxof2kxkbjunKA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 8 — Toxic comments data set</figcaption></figure><p id="9f4b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">训练集被分成 127，656 个训练样本和 31，915 个验证样本。</p><p id="a45d" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我用二元交叉熵损失函数和带有 Keras 的 Adam 优化器训练了一个简单的双向 LSTM 模型。</p><p id="c733" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">在第一种情况下，我使用了 GloVe <strong class="ln jk">中预先训练的单词嵌入，而没有使用</strong>中的广义翻译向量。</p><p id="cf5b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">结果如下:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pq"><img src="../Images/e9eadef95e5f179f228bba8c4e8decf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gUwqB9Cw1d4uExzc7XEBcw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 9 — Validation Loss and Accuracy</figcaption></figure><p id="8635" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">在由 153164 个样本组成的<strong class="ln jk">测试集</strong>上，模型精度为:<strong class="ln jk"> 93.7687% </strong></p><p id="b9cc" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">使用上面相同的配置，我将 GloVe 单词嵌入改为我应用了广义翻译向量的嵌入。</p><p id="5c6b" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">为了确保我只翻译需要翻译的向量，我从<a class="ae jg" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank">20 新闻组数据集</a>的现有词汇表中运行了 GloVe 嵌入。</p><p id="c4c4" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我只翻译了<strong class="ln jk">没有</strong>出现在 61118 个正确拼写单词的词汇列表中的向量。</p><p id="c3fe" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">结果如下:</p><figure class="mw mx my mz gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pr"><img src="../Images/9a9ecc0eed2f9f2af09e17182de0d057.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIm3wAtg5oGKD7Dtzs9rlA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Figure 10 — Validation loss and accuracy after applying Generalised Translation Vector</figcaption></figure><p id="1635" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">在上面的同一个<strong class="ln jk">测试集</strong>上，模型精度为:<strong class="ln jk"> 93.8267% </strong></p><p id="2287" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">精确度的差异只有 0.058%。</p><p id="49c9" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">从这个实验来看，对拼写错误或 OOV 单词应用广义翻译向量是否有助于提高任何下游 NLP 任务的准确性，这是不确定的。</p><p id="317d" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">然而，凭直觉，我相信它确实有帮助。也许不是这个任务本身，而是其他 NLP 任务？</p><h2 id="20ac" class="nw ko jj bd kp nx ny dn kt nz oa dp kx lw ob oc lb ma od oe lf me of og lj oh bi translated">快速小结</h2><ol class=""><li id="4247" class="oz pa jj ln b lo lp ls lt lw ps ma pt me pu mi pe pf pg ph bi translated">我发现 fast.ai 上的一个帖子很有趣，于是决定测试一下某人(Ed Rushton)处理拼写错误的新颖方法。</li><li id="e189" class="oz pa jj ln b lo pi ls pj lw pk ma pl me pm mi pe pf pg ph bi translated">我进一步测试了这种方法在缩写/OOV 单词上的应用，并得出结论:它对这些单词也有效。</li><li id="abfa" class="oz pa jj ln b lo pi ls pj lw pk ma pl me pm mi pe pf pg ph bi translated">我进一步研究了它是否有助于提高下游 NLP 任务(如分类)的准确性，发现我的实验没有结论。</li></ol><h1 id="0342" class="kn ko jj bd kp kq nr ks kt ku ns kw kx ky nt la lb lc nu le lf lg nv li lj lk bi translated">结尾注释</h1><p id="094c" class="pw-post-body-paragraph ll lm jj ln b lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi im bi translated">我确实发现这个练习非常有趣，并且很可能在将来我的其他 NLP 项目中使用它。</p><p id="882f" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">所有的赞扬和感谢都归于埃德·拉什顿，因为他提出了这样一种新颖的方法。</p><p id="9c36" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我将来想尝试的是:</p><p id="6d67" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">与其创建一个通用的翻译向量，不如根据一个单词属于哪个簇来运行本地化的翻译向量。</p><p id="4072" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">例如，如果单词是拼写错误，运行本地(拼写错误)翻译向量。如果单词是缩写，运行本地(缩写)翻译向量。</p><p id="2bc2" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我坚信这种本地化的方法是使这种方法更强大的下一步。</p><p id="9df1" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">我希望我在这方面的实验能帮助你更深入地了解单词嵌入是如何工作的，也许还能激发你的兴趣，让你自己做测试。</p><p id="8d86" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">下次再见，朋友们！</p><p id="0765" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated"><strong class="ln jk">如果你正在寻找 NLP 数据集，点击这里</strong><a class="ae jg" href="https://medium.com/@timothyguang/12-free-nlp-datasets-to-work-on-to-tide-you-through-this-pandemic-de58996e182a" rel="noopener"><strong class="ln jk"/></a><strong class="ln jk">查看我创建的精选列表！:)</strong></p><p id="4ce0" class="pw-post-body-paragraph ll lm jj ln b lo mn lq lr ls mo lu lv lw mq ly lz ma ms mc md me mu mg mh mi im bi translated">LinkedIn 简介:<a class="ae jg" href="https://www.linkedin.com/in/timothy-tan-97587190/" rel="noopener ugc nofollow" target="_blank">蒂莫西·谭</a></p></div></div>    
</body>
</html>