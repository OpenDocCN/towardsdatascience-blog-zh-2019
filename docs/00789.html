<html>
<head>
<title>Algorithms for Text Classification — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类算法—第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/algorithms-for-text-classification-part-1-naive-bayes-3ff1d116fdd8?source=collection_archive---------6-----------------------#2019-02-06">https://towardsdatascience.com/algorithms-for-text-classification-part-1-naive-bayes-3ff1d116fdd8?source=collection_archive---------6-----------------------#2019-02-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="28ee" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">朴素贝叶斯算法讲解</h2></div><p id="e87f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当你这几天查看关于自然语言处理(NLP)的新闻时，你会看到很多围绕语言模型、迁移学习、OpenAI、ULMFit 等的炒作。赶上 NLP 的当前技术水平是很好的，尽管我仍然相信人们应该很好地理解经典算法，如朴素贝叶斯和逻辑回归。为什么？因为你可能合作的公司可能并不专门开发聊天机器人或文本生成机器！大多数时候，从简单的模型开始可能会给你带来很好的结果，而不需要你拼命向你的商业伙伴解释你的方法。所以这篇博文是一系列文本分类方法的开始，从基本的开始。我将尝试解释理论以及如何在实践中使用算法。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="651e" class="li lj iq bd lk ll lm ln lo lp lq lr ls jw lt jx lu jz lv ka lw kc lx kd ly lz bi translated"><strong class="ak">朴素贝叶斯</strong></h1><p id="42ae" class="pw-post-body-paragraph kf kg iq kh b ki ma jr kk kl mb ju kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">为什么取这个名字？“Bayes”得名于概率论中著名的 Bayes '定理，“Naive”是因为这个算法的假设非常简单，而且大部分时间都不成立。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mf"><img src="../Images/0b65e73b43401365187f67df0fc14fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DzKgmuxNrvX2RapGbzMv5Q.png"/></div></div></figure><p id="26c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">朴素贝叶斯的一般思想:</strong></p><ol class=""><li id="9b57" class="mr ms iq kh b ki kj kl km ko mt ks mu kw mv la mw mx my mz bi translated">将文档 X 表示为一组(<em class="na"> w </em>，频率为<em class="na"> w </em>)对。</li><li id="3981" class="mr ms iq kh b ki nb kl nc ko nd ks ne kw nf la mw mx my mz bi translated">对于每个标签<em class="na"> y </em>，建立类别<em class="na"> y </em>中文档的概率模型 P(X| Y = y)。</li><li id="27ad" class="mr ms iq kh b ki nb kl nc ko nd ks ne kw nf la mw mx my mz bi translated">为了分类，选择最有可能产生 X 的标签<em class="na"> y </em>:</li></ol><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ng"><img src="../Images/103e55abe6011e1978b80e4e9d485452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSSaNndMCMIqasemR_Q9NQ.png"/></div></div></figure><p id="fb1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">假设:</strong></p><ol class=""><li id="8424" class="mr ms iq kh b ki kj kl km ko mt ks mu kw mv la mw mx my mz bi translated">文档 X 中单词的顺序没有关系，但是单词的重复有关系。</li><li id="a366" class="mr ms iq kh b ki nb kl nc ko nd ks ne kw nf la mw mx my mz bi translated">给定文档类，单词彼此独立出现。</li></ol><p id="ea0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于这些假设，我们有以下等式来估计 P(X|y):</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nh"><img src="../Images/888a8110c6f0a972192e4c771c386de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D2hZQSgoZbvLHQhkAEldIw.png"/></div></div></figure><p id="53e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">这些方程有问题:</strong></p><p id="6cfb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于等式(1)，如果我们的文档具有超过 100 个单词，则 P(w₁,…，w_n|Y = y)将是非常小的单词概率的乘积(&lt; 0.1), leading to the UNDERFLOW problem =&gt;用对数工作是维持数值稳定性所希望的)。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ni"><img src="../Images/e7eb15526d20c80e205a1f94718a73d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n84WudBV31P1NvVaplZkXA.png"/></div></div></figure><p id="a713" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于等式(2)，如果我们在新文本中有一个新词<em class="na"> w </em>需要分类，P(W = w | Y = y) = 0 作为<em class="na"> w </em>在我们的训练数据中从未出现过。一个解决方案是平滑概率。假设我们有 P(w|y) = <em class="na"> p </em>的<em class="na"> m </em>个例子。此处使用的<em class="na"> m </em>和<em class="na"> p </em>是多项式分布的<em class="na">狄利克雷先验</em>。请注意，平滑方法有很多种。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ni"><img src="../Images/30df83f062806b274d4893973d146fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sOIkpKzpvyuNqzUS1Qfdlw.png"/></div></div></figure><p id="50f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">把所有这些放在一起，我们有下面的算法:</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nj"><img src="../Images/74ecea7d330ebaefc131ba49b102062f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXYsSpLAuGgII7nlfn1Xsg.png"/></div></div></figure></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="477d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在，让我们通过一个假设的例子来理解这个算法:</strong></p><p id="b357" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们有 3 个文档:</p><p id="dd99" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X₁ =“政府关闭”,标签为 y₁ =新闻</p><p id="8584" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X₂ =“联邦雇员抗议关闭”,标签为 y₂ =新闻</p><p id="5320" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X₃ =“将忧郁转向葬礼”，标签是 y₃ =诗歌</p><p id="45fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">和一个要分类的新文档:</p><p id="94da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">X_new = "关闭影响联邦雇员利益"</p><p id="457a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们可以从我们的训练数据中得到这个计数表</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nk"><img src="../Images/820e50f30722442b76b17779ee5b1687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cH4NHsNu_uShUiruRYTKfw.png"/></div></div></figure><p id="19bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了简单起见，我没有排除停用词，但在实践中，您肯定应该这样做。另外，为了防止下溢问题，我定义了平滑参数:<em class="na"> p </em> = 0.5，<em class="na"> m </em> = 1。然后，我们可以如下计算新文档 X_new 的每个标签的分数。</p><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nl"><img src="../Images/cb07b92dd00420d0b43bca12dd192d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qE2Q4kx_ZndLWt305lPWhg.png"/></div></div></figure><p id="6413" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到标签“新闻”的得分高于标签“诗歌”的得分，所以我们将 X_new 归类为“新闻”。</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><p id="fbe0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">接下来，让我们看看如何使用 Python 和真实数据运行这个算法:</strong></p><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="7630" class="nr lj iq nn b gy ns nt l nu nv">import pandas as pd<br/>import numpy as np</span><span id="ebd3" class="nr lj iq nn b gy nw nt l nu nv">spam_data = pd.read_csv('spam.csv')</span><span id="23d9" class="nr lj iq nn b gy nw nt l nu nv">spam_data['target'] = np.where(spam_data['target']=='spam',1,0)<br/>print(spam_data.shape)<br/>spam_data.head(10)</span></pre><figure class="mg mh mi mj gt mk gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi nx"><img src="../Images/22a9925c8450963fd9af7f69f49fd5aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1w4fmlxe3LWM0sntpG8LQ.png"/></div></div></figure><pre class="mg mh mi mj gt nm nn no np aw nq bi"><span id="d88b" class="nr lj iq nn b gy ns nt l nu nv">from sklearn.model_selection import train_test_split<br/><strong class="nn ir">#Split data into train and test sets</strong></span><span id="b167" class="nr lj iq nn b gy nw nt l nu nv">X_train, X_test, y_train, y_test = train_test_split(spam_data['text'],spam_data['target'],random_state=0)</span><span id="ffe7" class="nr lj iq nn b gy nw nt l nu nv">from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn.metrics import roc_auc_score</span><span id="00e0" class="nr lj iq nn b gy nw nt l nu nv"><strong class="nn ir">#Train and evaluate the model</strong></span><span id="0495" class="nr lj iq nn b gy nw nt l nu nv">vect = CountVectorizer().fit(X_train)<br/>X_train_vectorized = vect.transform(X_train)<br/>clfrNB = MultinomialNB(alpha = 0.1)<br/>clfrNB.fit(X_train_vectorized, y_train)<br/>preds = clfrNB.predict(vect.transform(X_test))<br/>score = roc_auc_score(y_test, preds)</span><span id="e310" class="nr lj iq nn b gy nw nt l nu nv">print(score)</span><span id="49b2" class="nr lj iq nn b gy nw nt l nu nv">#0.9720812182741116</span></pre><p id="37f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的 AUC 评分是 0.97，对于这样一个简单的模型来说已经不错了。我们将在以后的博客文章中使用相同的数据集将这种性能与其他算法的性能进行比较。</p></div></div>    
</body>
</html>