<html>
<head>
<title>Least Squares Linear Regression In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 中的最小二乘线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77?source=collection_archive---------1-----------------------#2019-08-16">https://towardsdatascience.com/least-squares-linear-regression-in-python-54b87fc49e77?source=collection_archive---------1-----------------------#2019-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9b188b3e7c329d99414f1078a2b821c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LNEn7mqaJmhWQ9K3"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@jeswinthomas?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jeswin Thomas</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="af75" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">顾名思义，<strong class="ki jk"/><strong class="ki jk">最小二乘法</strong>的方法是将数据集中观察到的目标与线性近似预测的目标之间的残差的<strong class="ki jk">平方</strong>之和最小化。在这篇后续文章中，我们将看到如何使用线性代数找到最佳拟合线，而不是像梯度下降这样的东西。</p><h1 id="177c" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">算法</h1><p id="4c80" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">与我最初的想法相反，线性回归的<code class="fe mh mi mj mk b">scikit-learn</code>实现最小化了以下形式的成本函数:</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/46652b1509d31762a70fa24991666836.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*w4pissUaRbANQWt9XQE6jA.png"/></div></figure><p id="ce24" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">利用 x 的奇异值分解。</p><p id="8282" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您已经熟悉线性回归，您可能会发现它与前面的方程和均方差(MSE)有一些相似之处。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/a3d818a01473389d27aed816b7711654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OAdhpUn7ovxcqubq.png"/></div></figure><p id="a646" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为快速复习，假设我们有下面的散点图和回归线。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/73451d0dc12dce50d7d0e8a2dbae5d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/0*sIdJ49yoT3U164h-.png"/></div></figure><p id="4c32" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过从一个数据点减去另一个数据点来计算从直线到给定数据点的距离。我们取差值的平方，因为我们不希望低于实际值的预测值被高于实际值的预测值抵消。用数学术语来说，后者可以表示如下:</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/67f42bfce3ca3e6bffc0a3ceac7b4c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*YKch5UaKcgS1_HSBOmPH-Q.png"/></div></figure><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1ba9f9a5216aade557d982972fc12fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*shga3-yt9jVwbj8YrJk3Yg.png"/></div></figure><p id="494c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe mh mi mj mk b">scikit-learn</code>库中使用的成本函数是相似的，只是我们同时使用矩阵运算来计算它。</p><p id="1f2a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于那些上过微积分课程的人来说，你们可能以前遇到过这种符号。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/242443207a2b4a9a457c6570271fcc7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*UfFFdLeNQgM2rslACiLOUQ.png"/></div></figure><p id="4c71" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，<strong class="ki jk"> x </strong>是一个矢量，我们正在计算它的大小。</p><p id="77a5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在同样的意义上，当我们用竖线包围矩阵的变量(即 A)时，我们是说我们想从一个由行和列组成的矩阵变成一个标量。从矩阵中导出标量有多种方法。取决于使用哪一个，你会在变量的右边看到一个不同的符号(等式中多出来的 2 不是偶然放在那里的)。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/501a4a44160be670079c518953f2ad92.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*O2QGdSk9EwAy1CPyNBf8ww.png"/></div></figure><p id="dd07" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">额外的 2 意味着我们取矩阵的<a class="ae jg" href="https://en.wikipedia.org/wiki/Euclidean_norm" rel="noopener ugc nofollow" target="_blank">欧几里德范数</a>。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/72ac6b343be52367c912e80fcb59dc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*qmynvUUSrYSG9D4FdaUlxQ.png"/></div></figure><p id="139b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有一个矩阵 A，A 的欧氏范数等于 A 点转置的最大<a class="ae jg" href="https://en.wikipedia.org/wiki/Eigenvalue" rel="noopener ugc nofollow" target="_blank">特征值</a>的平方根，为了清楚起见，我们来走一个例子。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/545be9cb9457b795f7bbe73f6d72114a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rxqBxvyeQ75yV3xY6gC57g.png"/></div></div></figure><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/a78c4a31dc95bb006ea6b1cde6d49b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ObXJ9cKBzd70iTtgy1L_iw.png"/></div></div></figure><p id="4ef0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是我们量化误差的方法。然而，这引起了一个新的问题。具体来说，我们实际上如何着手最小化它呢？事实证明，最小范数最小二乘解(系数)可以通过计算输入矩阵 X 的伪逆并乘以输出向量 y 来找到。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/106c56acd3e91b287ad26bfb37d488a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*kkr4AdNe36egdlAhOWiOXg.png"/></div></figure><p id="b598" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 X 的伪逆定义为:</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/22eb2458bfb59c064bf3d1d05bf032a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*K4yXDeZfHutq5C2VCFqemA.png"/></div></figure><p id="12af" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上述矩阵均可从 X 的奇异值分解(SVD)中获得。回想一下，X 的 SVD 可描述如下:</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/3d99d9ff42273354d7df21f78710ac1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*S8CqU4G8D2WcUom5nqV4lg.png"/></div></figure><p id="29b1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对如何确定 U，sigma 和 V 的转置很好奇，看看我不久前写的这篇文章<a class="ae jg" rel="noopener" target="_blank" href="/singular-value-decomposition-example-in-python-dab2507d85a0"><strong class="ki jk"/></a>，这篇文章讲述了如何使用 SVD 进行降维。</p><p id="5fa5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过对<code class="fe mh mi mj mk b">sigma</code>矩阵中的值求逆来构造对角矩阵 D^+。<code class="fe mh mi mj mk b">+</code>指的是所有元素必须大于 0，因为我们不能被 0 整除。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/611d3a4d4f29829aa4d8acdc4012eeb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*llAdOsMYOPWmolQbTgVweQ.png"/></div></figure><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/3960ada37a35cd625afc85c226728d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*0ye7Wss20-xtClHgsSkUUg.png"/></div></figure><h1 id="eee0" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Python 代码</h1><p id="db25" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们看看如何使用基本的<code class="fe mh mi mj mk b">numpy</code>函数从头开始实现线性回归。首先，我们导入以下库。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="7fc3" class="ni lf jj mk b gy nj nk l nl nm">from sklearn.datasets import make_regression<br/>from matplotlib import pyplot as plt<br/>import numpy as np<br/>from sklearn.linear_model import LinearRegression</span></pre><p id="192e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们使用<code class="fe mh mi mj mk b">scikit-learn</code>库生成数据。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="c3e5" class="ni lf jj mk b gy nj nk l nl nm">X, y, coefficients = make_regression(<br/>    n_samples=50,<br/>    n_features=1,<br/>    n_informative=1,<br/>    n_targets=1,<br/>    noise=5,<br/>    coef=True,<br/>    random_state=1<br/>)</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9855673fd03a2531f95e8bca600e21f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*KuEBMSuUcdK8MOKS7Z5bvw.png"/></div></figure><p id="a877" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将矩阵的秩和列数存储为变量。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="a0fb" class="ni lf jj mk b gy nj nk l nl nm">n = X.shape[1]<br/>r = np.linalg.matrix_rank(X)</span></pre><p id="4cc3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用奇异值分解找到我们的特征矩阵的等价物。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="4770" class="ni lf jj mk b gy nj nk l nl nm">U, sigma, VT = np.linalg.svd(X, full_matrices=False)</span></pre><p id="5b93" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，D^+可以从西格玛得到。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="bcfd" class="ni lf jj mk b gy nj nk l nl nm">D_plus = np.diag(np.hstack([1/sigma[:r], np.zeros(n-r)]))</span></pre><p id="056e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="no"> V </em>当然等于其转置的转置，如以下恒等式所述。</p><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/04f896e493d0308fa9385f98b4f4b380.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*OcCHewFlwd4ZzsqBJ338ug.png"/></div></figure><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="15e7" class="ni lf jj mk b gy nj nk l nl nm">V = VT.T</span></pre><p id="debf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们确定 x 的<strong class="ki jk"> Moore-Penrose 伪逆</strong>。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="c3f6" class="ni lf jj mk b gy nj nk l nl nm">X_plus = V.dot(D_plus).dot(U.T)</span></pre><p id="00c7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们在上一节中看到的，系数向量可以通过矩阵 X 的伪逆乘以 y 来计算。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="fcb8" class="ni lf jj mk b gy nj nk l nl nm">w = X_plus.dot(y)</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3bc4357eafe94a25803a8e25445d46f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*9rQXcaQbHbcedE7M1aJDKg.png"/></div></figure><p id="f787" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了获得实际误差，我们使用我们看到的第一个等式来计算残差平方和。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="7b77" class="ni lf jj mk b gy nj nk l nl nm">error = np.linalg.norm(X.dot(w) - y, ord=2) ** 2</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e1e73f4181779f43e8162a1cf5781053.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*xEHVjzl5xt_JPSjmLYezOQ.png"/></div></figure><p id="384a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了验证我们得到了正确的答案，我们可以使用一个<code class="fe mh mi mj mk b">numpy</code>函数来计算并返回线性矩阵方程的最小二乘解。具体来说，该函数返回 4 个值。</p><ol class=""><li id="c6b1" class="ns nt jj ki b kj kk kn ko kr nu kv nv kz nw ld nx ny nz oa bi translated">最小二乘解</li><li id="0e3a" class="ns nt jj ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated">残差和(误差)</li><li id="a3bb" class="ns nt jj ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated">矩阵的秩(X)</li><li id="7b02" class="ns nt jj ki b kj ob kn oc kr od kv oe kz of ld nx ny nz oa bi translated">矩阵的奇异值(X)</li></ol><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="b395" class="ni lf jj mk b gy nj nk l nl nm">np.linalg.lstsq(X, y)</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2fef82d4c8a313303c16c5bed237b493.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*VWXGZYpXpe9ddAHmALUvug.png"/></div></figure><p id="2f9b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过绘制回归线，我们可以直观地确定系数实际上是否导致最佳拟合。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="f8af" class="ni lf jj mk b gy nj nk l nl nm">plt.scatter(X, y)<br/>plt.plot(X, w*X, c='red')</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1f0143941edf2225a6b437df36d81222.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*dqSJL8LI2oxYoT6UwFdhsw.png"/></div></figure><p id="30f6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用线性回归的<code class="fe mh mi mj mk b">scikit-learn</code>实现做同样的事情。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="5e46" class="ni lf jj mk b gy nj nk l nl nm">lr = LinearRegression()</span><span id="f0eb" class="ni lf jj mk b gy oi nk l nl nm">lr.fit(X, y)</span><span id="d390" class="ni lf jj mk b gy oi nk l nl nm">w = lr.coef_[0]</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2132da538a69c5bb0182765fce4053dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*jSlx3xGuS5IB5QprpVHOzQ.png"/></div></figure><p id="cdca" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们使用新发现的系数绘制回归线。</p><pre class="mm mn mo mp gt ne mk nf ng aw nh bi"><span id="84b8" class="ni lf jj mk b gy nj nk l nl nm">plt.scatter(X, y)<br/>plt.plot(X, w*X, c='red')</span></pre><figure class="mm mn mo mp gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/1f0143941edf2225a6b437df36d81222.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*dqSJL8LI2oxYoT6UwFdhsw.png"/></div></figure></div></div>    
</body>
</html>