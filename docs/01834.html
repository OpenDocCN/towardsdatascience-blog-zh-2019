<html>
<head>
<title>Illustrated: Efficient Neural Architecture Search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图解:有效的神经结构搜索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/illustrated-efficient-neural-architecture-search-5f7387f9fb6?source=collection_archive---------4-----------------------#2019-03-26">https://towardsdatascience.com/illustrated-efficient-neural-architecture-search-5f7387f9fb6?source=collection_archive---------4-----------------------#2019-03-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/97f741b661fdb3c81c90c865a182d9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*zruyqPYCZd70rCkB4WcMow.gif"/></div></div></figure><h2 id="6ed8" class="jc jd je bd b dl jf jg jh ji jj jk dk jl translated" aria-label="kicker paragraph"><a class="ae ep" rel="noopener" target="_blank" href="/inside-ai/">内部 AI </a></h2><div class=""/><div class=""><h2 id="5e19" class="pw-subtitle-paragraph kk jn je bd b kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb dk translated">ENAS 宏观和微观搜索策略指南</h2></div><p id="1bce" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">(TL；DR 你只需要知道的两个动画是</em> <a class="ae lz" href="#d217" rel="noopener ugc nofollow"> <em class="ly">这里</em> </a> <em class="ly"> ) </em></p><p id="2972" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">更新:<br/>2020 年 3 月 23 日:勘误表—之前提到过，micro search 中的所有卷积单元互不相同。这是错误的；卷积单元是</em> <strong class="le jo"> <em class="ly">在最终的子模型中多次重复</em> </strong> <em class="ly">。作者核实了这一信息。感谢</em> <a class="ma mb ep" href="https://medium.com/u/a83bfe733f25?source=post_page-----5f7387f9fb6--------------------------------" rel="noopener" target="_blank"> <em class="ly">马丁·费利安</em> </a> <em class="ly">的指正！</em></p><p id="bf50" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi mc translated">为图像分类和自然语言理解等各种任务设计神经网络通常需要大量的架构工程和专业知识。进入<strong class="le jo">神经架构搜索</strong> (NAS)，这是一个自动化人工设计神经网络过程的任务。NAS 将其日益增长的研究兴趣归功于最近深度学习模型的日益突出。</p><p id="ec9d" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">有许多方法可以搜索或发现神经结构。在过去的几年中，社区已经提出了不同的搜索方法，包括:</p><ul class=""><li id="82f1" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated"><strong class="le jo">强化学习<br/> </strong> <a class="ae lz" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank">带强化学习的神经架构搜索</a> (Zoph and Le，2016)<br/><a class="ae lz" href="https://arxiv.org/abs/1707.07012" rel="noopener ugc nofollow" target="_blank">NASNet</a>(Zoph<em class="ly">et al .</em>，2017)<br/><a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank"/>(Pham<em class="ly">et al .</em>，2018)</li><li id="6b2b" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><strong class="le jo">进化算法<br/> </strong> <a class="ae lz" href="https://arxiv.org/abs/1711.00436" rel="noopener ugc nofollow" target="_blank">分层 Evo </a>(刘<em class="ly">等人</em>，2017) <br/> <a class="ae lz" href="https://arxiv.org/abs/1802.01548" rel="noopener ugc nofollow" target="_blank">阿米巴内</a>(实<em class="ly">等人</em>，2018)</li><li id="5da2" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><strong class="le jo"/><br/><a class="ae lz" href="https://arxiv.org/abs/1712.00559" rel="noopener ugc nofollow" target="_blank"/>【刘<em class="ly">等</em>，2017</li><li id="3503" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><strong class="le jo">贝叶斯优化<br/> </strong> <a class="ae lz" href="https://arxiv.org/abs/1806.10282" rel="noopener ugc nofollow" target="_blank"> Auto-Keras </a>(金<em class="ly">等人</em>，2018)<br/><a class="ae lz" href="https://arxiv.org/abs/1802.07191" rel="noopener ugc nofollow" target="_blank">NASBOT</a>(kanda Samy<em class="ly">等人</em>。2018)</li><li id="5818" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><strong class="le jo">基于梯度的优化<br/></strong><a class="ae lz" href="https://arxiv.org/abs/1812.09926" rel="noopener ugc nofollow" target="_blank"/>(谢<em class="ly">等</em>，2018) <br/> <a class="ae lz" href="https://arxiv.org/abs/1806.09055" rel="noopener ugc nofollow" target="_blank">飞镖</a>(刘<em class="ly">等</em>。, 2018)</li></ul><p id="f99d" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在本帖中，我们将关注<strong class="le jo">高效神经架构搜索</strong> (ENAS)，它采用强化学习来构建卷积神经网络(CNN)和递归神经网络(RNNs)。作者 Hieu Pham、Melody Guan、Barret Zoph、Quoc V. Le 和 Jeff Dean 提出了一个预定义的神经网络，以使用<em class="ly">宏搜索</em>和<em class="ly">微搜索</em>在增强学习框架的指导下生成新的神经网络(参见论文<a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">此处</a>)。没错——一个神经网络构建另一个神经网络。</p><p id="d5d0" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这篇文章是一篇关于宏观和微观搜索策略如何产生神经网络的教程。虽然插图和动画用于指导读者，但动画的顺序并不一定反映操作流程(由于矢量化等原因)。).</p><p id="cc0f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们将把本教程的范围缩小到<strong class="le jo">在图像分类任务</strong>中搜索 CNN 的神经结构。本文假设读者熟悉 RNNs、CNN 和强化学习的基础知识。熟悉像迁移学习和跳过/剩余连接这样的深度学习概念将非常有帮助，因为它们在架构搜索中被大量使用。这并不要求你阅读这篇论文，但它会加快你的理解。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h2 id="3a44" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">内容</h2><p id="4e1e" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">0.<a class="ae lz" href="#066bb" rel="noopener ugc nofollow">概述</a>T36】1。<a class="ae lz" href="#9137" rel="noopener ugc nofollow">搜索策略</a> <br/> 1.1。<a class="ae lz" href="#6ae7" rel="noopener ugc nofollow">宏搜索</a> <br/> 1.2。<a class="ae lz" href="#5b7b" rel="noopener ugc nofollow">微搜</a> <br/> 2。<a class="ae lz" href="#d217" rel="noopener ugc nofollow">注释</a>注释<br/> 3。<a class="ae lz" href="#0788" rel="noopener ugc nofollow">概要</a>T51】4。<a class="ae lz" href="#df67" rel="noopener ugc nofollow">实现</a> <br/> 5。<a class="ae lz" href="#0e01" rel="noopener ugc nofollow">参考文献</a></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h2 id="066b" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">0.概观</h2><p id="1de6" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">在 ENAS，有两种类型的神经网络:</p><ul class=""><li id="b4c5" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated"><strong class="le jo">控制器</strong>–预定义的 RNN，是一种长短期记忆(LSTM)单元</li><li id="d21c" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><strong class="le jo">子模型</strong>–用于图像分类的理想 CNN</li></ul><p id="d976" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">像大多数其他 NAS 算法一样，ENAS 涉及 3 个概念:</p><ol class=""><li id="6675" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx od mr ms mt bi translated"><strong class="le jo">搜索空间</strong>——所有可能产生的不同架构或子模型</li><li id="8a81" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated"><strong class="le jo">搜索策略</strong> —生成这些架构或子模型的方法</li><li id="7426" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated"><strong class="le jo">性能评估</strong> —衡量生成的子模型有效性的方法</li></ol><p id="5fed" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">让我们看看这五个想法是如何形成 ENAS 故事的。</p><p id="3649" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">控制器</strong>通过使用某种<strong class="le jo">搜索策略</strong>“生成一组指令”(或者更严格地说，<em class="ly">做出决策</em>或<em class="ly">采样</em> <em class="ly">决策</em>)来控制或指导子模型架构的构建。这些决定是像什么类型的操作(卷积，池等。)在子模型的特定层执行。使用这些决定，建立一个子模型。生成的子模型是可以在<strong class="le jo">搜索空间</strong>中构建的许多可能的子模型之一。</p><p id="9dd8" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然后，使用随机梯度下降将该特定子模型训练至收敛(约 95%的训练精度)，以最小化预测类和地面真实类之间的预期损失函数(对于图像分类任务)。这是在指定数量的时期内完成的，我称之为<em class="ly">子时期</em>，比如 100。然后，从该训练的模型获得验证准确度。</p><p id="9b2e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">然后，我们使用基于策略的强化学习算法来更新控制器的参数，以最大化期望奖励函数，即验证准确性。这个参数更新希望改进控制器，以产生更好的决定，给出更高的验证精度。</p><p id="13e3" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这整个过程(从之前的 3 个段落开始)只是一个时期——姑且称之为<em class="ly">控制器时期。然后，我们对指定数量的控制器时期重复这一过程，比如 2000 个。</em></p><p id="9797" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在所有生成的 2000 个子模型中，具有最高验证准确性的模型将获得<em class="ly">的荣誉，成为执行图像分类任务的</em>神经网络。然而，这个子模型在用于部署之前，必须再经过一轮训练(同样由子时期的数量指定)。</p><p id="7200" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">整个训练的伪算法写在下面:</p><pre class="oe of og oh gt oi oj ok ol aw om bi"><span id="17a5" class="ng nh je oj b gy on oo l op oq">CONTROLLER_EPOCHS = 2000<br/>CHILD_EPOCHS = 100</span><span id="c62e" class="ng nh je oj b gy or oo l op oq">Build controller network</span><span id="385e" class="ng nh je oj b gy or oo l op oq">for i in CONTROLLER_EPOCHS:</span><span id="ea03" class="ng nh je oj b gy or oo l op oq">     1. Generate a child model<br/>     2. Train this child model for CHILD_EPOCHS<br/>     3. Obtain val_acc<br/>     4. Update controller parameters</span><span id="7b45" class="ng nh je oj b gy or oo l op oq">Get child model with the highest val_acc<br/>Train this child model for CHILD_EPOCHS</span></pre><p id="8a6f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这整个问题本质上是一个具有原型元素的强化学习框架:</p><ul class=""><li id="cfa9" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated">代理—控制器</li><li id="c04c" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">行动——建立子网络的决策</li><li id="0efc" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">奖励—来自子网络的验证准确性</li></ul><p id="0b14" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这个强化学习任务的目的是最大化代理(控制器)所采取的行动(建立子模型架构的决策)的回报(验证准确性)。</p><p id="f754" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#3a44" rel="noopener ugc nofollow">返回页首</a></p><h2 id="9137" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">1.搜索策略</h2><p id="eda8" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">回想一下上一节，控制器使用某种搜索策略生成子模型的架构。在这个陈述中，你应该问两个问题——(1)管制员如何做决定，以及(2)什么样的搜索策略？</p><p id="52d5" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">管制员如何决策？</strong></p><p id="2384" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这就把我们带到了控制器的模型，这是一个 LSTM。该 LSTM 通过 softmax 分类器以自动回归的方式对决策进行采样:前一步中的决策作为输入嵌入到下一步中。</p><p id="febe" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">有哪些搜索策略？</strong></p><p id="7458" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">ENAS 的作者提出了搜索或生成架构的两个策略。</p><ol class=""><li id="9f26" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx od mr ms mt bi translated">宏搜索</li><li id="3613" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated">微搜索</li></ol><p id="4708" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">宏搜索是一种控制器<strong class="le jo">设计整个网络</strong>的方法。使用这种方法的出版物包括 Zoph 和 Le 的 NAS、FractalNet 和 SMASH。另一方面，微搜索是一种方法，其中控制器<strong class="le jo">设计模块或构建块</strong>，它们被组合以构建最终网络。实现这种方法的一些论文是分层 NAS、渐进式 NAS 和 NASNet。</p><p id="c0bd" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在以下两个小节中，我们将了解 ENAS 是如何实施这两项战略的。</p><p id="e3f4" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#6ae7" rel="noopener ugc nofollow">宏搜索</a> ][ <a class="ae lz" href="#5b7b" rel="noopener ugc nofollow">微搜索</a> ]</p><h2 id="6ae7" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">1.1 宏搜索</h2><p id="233d" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">在宏搜索中，控制器为子模型中的每一层做出 2 个决定:</p><ul class=""><li id="e8ee" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated">对上一层执行的操作(操作列表见<a class="ae lz" href="#d217" rel="noopener ugc nofollow">注释</a>)</li><li id="3e10" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">跳过连接时要连接到的上一层</li></ul><p id="8147" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">在这个宏搜索示例中，我们将看到控制器如何生成 4 层子模型。这个子模型中的每一层分别用红色、绿色、蓝色和紫色进行颜色编码。</p><p id="4787" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">卷积层 1(红色)</strong></p><p id="db6e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们将从运行控制器的第一个时间步开始。这个时间步骤的输出被软最大化以获得一个向量，该向量被转换成一个<code class="fe os ot ou oj b">conv3×3</code>操作。</p><p id="a0c6" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这对子模型意味着，我们在输入图像上用 3×3 滤波器执行卷积。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/29574d54151bbd1695278fee51fac07e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Gu3ZP7zgh7YvcWgyEk-80w.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">The output from the first time step (<strong class="bd oz">conv3×3</strong>) of the controller corresponds to building the first layer (red) in the child model. This means the child model will first perform 3×3 convolution on the input image.</figcaption></figure><p id="6f49" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我知道我提到过管制员需要做出两个决定，但这里只有一个。由于这是第一层，我们只能对要执行的操作中的一个决策进行采样，因为除了输入图像本身之外，没有其他要连接的内容。</p><p id="bbab" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">卷积层 2(绿色)</strong></p><p id="3b47" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">为了构建随后的卷积层，控制器做出 2 个决定(没有更多谎言):(I)操作和(ii)要连接的层。在这里，我们看到它生成了<code class="fe os ot ou oj b">1</code>和<code class="fe os ot ou oj b">sep5×5</code>。</p><p id="19fb" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这对于子模型意味着我们首先对前一层的输出执行<code class="fe os ot ou oj b">sep5×5</code> <strong class="le jo"> </strong>操作<strong class="le jo">。然后，该输出沿着深度与层<code class="fe os ot ou oj b">1</code>的输出连接在一起，即红色层的输出。</strong></p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/c287ce6531036ff4147f84497063aae9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3E5Yxrl6aG4aGsLHfHR8Zw.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">The outputs from the 2nd and 3rd time step (<strong class="bd oz">1</strong> and <strong class="bd oz">sep5×5</strong>) in the controller correspond to building Convolutional Layer 2 (green) in the child model.</figcaption></figure><p id="a3c4" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">卷积层 3(蓝色)</strong></p><p id="b1d5" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们再次重复上一步以生成第三卷积层。同样，我们在这里看到控制器生成 2 个东西:(I)操作和(ii)要连接的层。下面，控制器产生<code class="fe os ot ou oj b">1</code>和<code class="fe os ot ou oj b">2</code>，以及操作<code class="fe os ot ou oj b">max3×3</code>。</p><p id="f760" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">因此，子模型对前一层(第 2 层，绿色)的输出执行操作<code class="fe os ot ou oj b">max3×3</code> <strong class="le jo">。然后，该操作的结果沿着深度维度与层<code class="fe os ot ou oj b">1</code>和<code class="fe os ot ou oj b">2</code>连接。</strong></p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/56dc221ed308851706751ee10ca50344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*P9HYNNy6r0nBK3ovl6OENQ.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">The outputs from the 4th and 5th time step (<strong class="bd oz">1,2</strong> and <strong class="bd oz">max3×3</strong>) in the controller correspond to building Convolutional Layer 3 (blue) in the child model.</figcaption></figure><p id="8f0a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">卷积层 4(紫色)</strong></p><p id="363f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">我们再次重复上一步以生成第四卷积层。这次控制器产生了<code class="fe os ot ou oj b">1</code>和<code class="fe os ot ou oj b">3</code>，以及操作<code class="fe os ot ou oj b">conv5×5</code>。</p><p id="853e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">子模型对前一层(第三层，蓝色)的输出执行操作<code class="fe os ot ou oj b">conv5×5</code> <strong class="le jo">。然后，该操作的结果沿着深度维度与层<code class="fe os ot ou oj b">1</code>和<code class="fe os ot ou oj b">3</code>连接。</strong></p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/1743d115c29ae2aa4c903ba00b692c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*AHQJSNMwxMSTLuaHa2cxUw.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">The outputs from the 6th and 7th time step (<strong class="bd oz">1,3</strong> and <strong class="bd oz">conv5×5</strong>) in the controller correspond to building Convolutional Layer 4 (purple) in the child model.</figcaption></figure><p id="b270" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">结束</strong></p><p id="faad" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在，您有了它——一个使用宏搜索生成的子模型！现在转到微搜索。</p><p id="c4be" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#3a44" rel="noopener ugc nofollow">返回顶部</a></p><h2 id="5b7b" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">1.2 微搜索</h2><p id="6884" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">微搜索仅设计一个构建块，其架构在整个最终架构中重复。ENAS 称这个构建模块为<strong class="le jo">卷积单元</strong>和<strong class="le jo">归约单元</strong>。两者是相似的——约简单元的唯一不同之处是运算的步长为 2，从而减少了空间维度。</p><p id="e7c0" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">微搜索的想法是<strong class="le jo">为卷积单元构建一个单一的架构，并在整个最终模型中重复这个相同的架构。</strong>在下面的例子中，最终模型由一个卷积单元组成，其架构在 3 个模块中重复<em class="ly"> N </em>次。如前所述，归约单元类似于卷积单元的架构。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/26c26780eed2e8a397db4db53c6b8ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6L57quG_W1l84HEo6kMJg.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.1: Overview of the final neural network generated. <a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">Image source</a>.</figcaption></figure><p id="f30c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">让我们过一会儿再回到这个话题。</p><p id="883e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">为微搜索导出的网络构建单元</strong></p><p id="100f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">从微搜索衍生的子网络的“构建单元”中有一种等级制度。从最大到最小:</p><ul class=""><li id="85ac" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated">街区</li><li id="0da4" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">卷积单元/归约单元</li><li id="3019" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">结节</li></ul><p id="2a3e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">子模型由几个<strong class="le jo">模块</strong>组成。每个块由<em class="ly"> N </em>个卷积<strong class="le jo">单元</strong>和 1 个归约单元组成。每个卷积/归约单元包括<em class="ly">B</em>T15】节点。并且每个节点都由标准卷积运算组成(我们稍后会看到这一点)。(<em class="ly"> N </em>和<em class="ly"> B </em>是超参数，可以由你这个架构师来调优。)</p><p id="9dc2" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">下面是一个有 3 个积木的子模型。每个块由<em class="ly"> N </em> =3 个卷积单元和 1 个归约单元组成。这里没有显示每个单元内的操作。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/26c26780eed2e8a397db4db53c6b8ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6L57quG_W1l84HEo6kMJg.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.2: Overview of the final neural network generated. It has 3 blocks and each block consists of 3 convolutional cells and 1 reduction cell. <a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">Image source</a>.</figcaption></figure><p id="818c" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">那么你可能会问，如何从微搜索生成这个子模型呢？继续读！</p><p id="1672" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">通过微搜索生成子模型</strong></p><p id="4437" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">对于这个微搜索教程，为了简单起见，让我们构建一个有 1 个块的子模型。该块包括<em class="ly"> N </em> =3 个卷积单元和 1 个归约单元，每个单元包括<em class="ly"> B </em> =4 个节点。</p><p id="d5ff" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">回想一下，这意味着只需为一个卷积单元设计一个架构，然后重复 3 次以上(2 个以上的卷积单元和 1 个归约单元)。这意味着我们生成的子模型应该如下所示:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pb"><img src="../Images/1a7f4e01ae843a3d51fd08d3c4f9c592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*512vRWTjQ7JcEiF8Q9WR_w.jpeg"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.3: A neural network generated from micro search which has 1 block, consisting of 3 convolutional cells and 1 reduction cell. The individual operations are not shown here.</figcaption></figure><p id="f35a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在让我们构建一个卷积单元！</p><p id="38d7" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">为了解释如何建立一个卷积单元，让我们假设我们已经有 2 个卷积单元。请注意，这两个单元的最后一个操作是<code class="fe os ot ou oj b">add</code>操作。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/15b25dcae04548fe0ad57a725fa7c6a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y--glExv0Y4O5USYenv9xw.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.5: ‘Preparing’ the third convolutional cell in micro search.</figcaption></figure><p id="7e20" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">回想一下，一个卷积单元由 4 个节点组成。那么这些节点在哪里呢？</p><p id="41de" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">节点 1 和节点 2 </strong></p><p id="ef37" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">前两个单元格</em> <strong class="le jo"> </strong>(红色和蓝色)将分别被视为节点 1 和节点 2。其他两个节点就在我们现在正在构建的这个卷积单元中。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/9a21fee3bebe7c05c5461e0b62bfba6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aePFc1G5KGozpKULOpsNkA.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.6: Identifying the 4 nodes while building Convolutional Cell #3.</figcaption></figure><p id="3cb2" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">从本节开始，您可以放心地忽略上图中的“卷积单元”标签，而专注于“节点”标签:</p><ul class=""><li id="69a5" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated">节点 1 —红色</li><li id="915e" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">节点 2 —蓝色</li><li id="a2f4" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">节点 3 —绿色</li><li id="37c5" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">节点 4 —紫色</li></ul><blockquote class="pe pf pg"><p id="a459" class="lc ld ly le b lf lg ko lh li lj kr lk ph lm ln lo pi lq lr ls pj lu lv lw lx im bi translated"><strong class="le jo"> <em class="je">勘误</em></strong><em class="je"><br/>i̶f̶̶y̶o̶u̶'̶r̶e̶̶w̶o̶n̶d̶e̶r̶i̶n̶g̶̶i̶f̶̶t̶h̶e̶s̶e̶̶n̶o̶d̶e̶s̶̶w̶i̶l̶l̶̶c̶h̶a̶n̶g̶e̶̶f̶o̶r̶̶e̶v̶e̶r̶y̶̶c̶o̶n̶v̶o̶l̶u̶t̶i̶o̶n̶a̶l̶̶c̶e̶l̶l̶̶w̶e̶'̶r̶e̶̶b̶u̶i̶l̶d̶i̶n̶g̶</em>,̶̶t̶h̶e̶̶a̶n̶s̶w̶e̶r̶̶i̶s̶̶y̶e̶s̶！̶̶e̶v̶e̶r̶y̶̶c̶e̶l̶l̶̶w̶i̶l̶l̶̶'̶a̶s̶s̶i̶g̶n̶'̶̶t̶h̶e̶̶n̶o̶d̶e̶s̶̶i̶n̶̶t̶h̶i̶s̶̶m̶a̶n̶n̶e̶r̶.̶</p></blockquote><p id="4171" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">节点 3 </strong></p><p id="089b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">节点 3 是建筑的起点。控制器采样 4 个决策(或者说 2 组决策):</p><ul class=""><li id="e377" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated">要连接的 2 个节点</li><li id="027c" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">要在要连接的节点上执行的两个操作</li></ul><p id="b167" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">有 4 个决策要做，控制器运行 4 个时间步。请看下面:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/e8b854cbcc5fd03758377fef03cd0590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*wSQnip36t6SSyQKcVz9QbA.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.7: The outputs of the first four controller time steps (<strong class="bd oz">2</strong>, <strong class="bd oz">1</strong>, <code class="fe os ot ou oj b"><strong class="bd oz">avg5×5</strong>, <strong class="bd oz">sep5×5</strong>), which will be used to build Node 3.</code></figcaption></figure><p id="0c92" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">从上面我们看到，控制器从四个时间步长的每一个采样节点<code class="fe os ot ou oj b">2</code>、节点<code class="fe os ot ou oj b">1</code>、<code class="fe os ot ou oj b">avg5×5</code>和<code class="fe os ot ou oj b">sep5×5</code>。这如何转化为子模型的架构？让我们看看:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/460f3d56115da85af6ddb0cd4a5f0054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3-CjrAOkM_vzKyL86RVbJw.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.8: How the outputs of the first four controller time steps (<strong class="bd oz">2</strong>, <strong class="bd oz">1</strong>, <code class="fe os ot ou oj b"><strong class="bd oz">avg5×5</strong>, <strong class="bd oz">sep5×5</strong>) are translated to build Node 3.</code></figcaption></figure><p id="fe2f" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">从上面，我们观察到三件事:</p><ol class=""><li id="a033" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx od mr ms mt bi translated">节点<code class="fe os ot ou oj b">2</code>(蓝色)的输出经过<code class="fe os ot ou oj b">avg5×5</code>操作。</li><li id="ee10" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated">节点<code class="fe os ot ou oj b">1</code>(红色)的输出经过<code class="fe os ot ou oj b">sep5×5</code>运算。</li><li id="00d6" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated">这两个操作的结果都经过一个<code class="fe os ot ou oj b">add</code>操作。</li></ol><p id="b3bf" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">来自该节点的<strong class="le jo">输出</strong>是经过<code class="fe os ot ou oj b">add</code>运算的张量。这解释了为什么节点 1 和 2 以<code class="fe os ot ou oj b">add</code>操作结束。</p><p id="1478" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">节点 4 </strong></p><p id="62b6" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">现在是节点 4。我们重复相同的步骤，但是现在控制器有三个节点可供选择(节点 1、2 和 3)。下面，控制器生成<code class="fe os ot ou oj b">3</code>、<code class="fe os ot ou oj b">1</code>、<code class="fe os ot ou oj b">id</code>和<code class="fe os ot ou oj b">avg3×3</code>。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/cfbea05885b9819261d5694c476a67e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*-P-uc0KzH2qq8y8Lb3GpNw.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.9: The outputs of the first four controller time steps (<strong class="bd oz">3</strong>, <strong class="bd oz">1</strong>, <code class="fe os ot ou oj b"><strong class="bd oz">id</strong>, <strong class="bd oz">avg3×3</strong>), which will be used to build Node 4.</code></figcaption></figure><p id="6bde" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">这转化为构建以下内容:</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/0f6f3ab1093ccf8cc46cb0ca1b1a6e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nZBZuIXmh8OkRUKSmqGFEQ.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.10: How the outputs of the first four controller time steps (<strong class="bd oz">3</strong>, <strong class="bd oz">1</strong>, <code class="fe os ot ou oj b"><strong class="bd oz">id</strong>, <strong class="bd oz">avg3×3</strong>) are translated to build Node 3.</code></figcaption></figure><p id="81da" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">刚刚发生了什么？</p><ol class=""><li id="a434" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx od mr ms mt bi translated">节点<code class="fe os ot ou oj b">3</code>的输出(绿色)经过<code class="fe os ot ou oj b">id</code>运算。</li><li id="b747" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated">节点<code class="fe os ot ou oj b">1</code>的输出(红色)经过<code class="fe os ot ou oj b">avg3×3</code>运算。</li><li id="bf4a" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx od mr ms mt bi translated">这两个操作的结果都经过一个<code class="fe os ot ou oj b">add</code>操作。</li></ol><p id="9d48" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">就是这样！最终子模型中的所有卷积单元将共享相同的架构。类似地，最终子模型中的所有归约单元(其架构与卷积单元的架构不同，具有步长 2 运算)将共享相同的架构。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/2d30e489dd336ec17d99feb4e120029e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rh6MBzFi4PVtTjS2huP6fQ@2x.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.11: An example of a convolution cell (with 7 nodes) discovered in the micro search space (<a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">credits</a>).</figcaption></figure><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pl"><img src="../Images/9a6addd17573296d20dd629d30cac22a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eztgd7XQB1QT3kAXL4fgxA@2x.png"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 1.2.12: An example of a reduction cell (with 7 nodes) discovered in the micro search space (<a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">credits</a>).</figcaption></figure><p id="bd09" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#3a44" rel="noopener ugc nofollow">返回顶部</a></p><h2 id="d217" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">2.笔记</h2><p id="cf06" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated">因为这篇文章主要展示了宏观和微观的搜索策略，所以我忽略了许多小细节(尤其是关于迁移学习的概念)。让我简单介绍一下:</p><ul class=""><li id="697b" class="ml mm je le b lf lg li lj ll mn lp mo lt mp lx mq mr ms mt bi translated">ENAS 有什么这么“高效”？答案:迁移学习。如果两个节点之间的计算之前已经完成(训练),则来自卷积滤波器和 1×1 卷积的权重(以保持信道输出的数量；在前面的章节中没有提到)将被重用。这就是 ENAS 比它的前辈更快的原因！</li><li id="5f7f" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">控制器有可能对不需要跳过连接的决策进行采样。</li><li id="4f75" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">控制器有 6 种操作:滤波器大小为 3×3 和 5×5 的卷积，滤波器大小为 3×3 和 5×5 的深度可分卷积，内核大小为 3×3 的最大池和平均池。</li><li id="a986" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">请仔细阅读每个单元格末尾的 concatenate 操作，它将任何节点的“松散端”连接起来。</li><li id="c0cb" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated">请简要阅读政策梯度算法(强化)强化学习。</li></ul><p id="aa71" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#3a44" rel="noopener ugc nofollow">返回页首</a></p><h2 id="0788" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">3.摘要</h2><p id="6ff1" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated"><strong class="le jo">宏搜索(针对整个网络)</strong></p><p id="280b" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">最终的子模型如下所示。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/97f741b661fdb3c81c90c865a182d9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*zruyqPYCZd70rCkB4WcMow.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 3.1: Generating a convolutional neural network with macro search.</figcaption></figure><p id="6257" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">微搜索(针对卷积小区)</strong></p><p id="037e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">注意，这里只显示了最终子模型的一部分。</p><figure class="oe of og oh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pk"><img src="../Images/b752e0d8b6995e324e27550a9a84c7fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*FPu5Yi8mAsh_-NHxp1CpzA.gif"/></div></div><figcaption class="ov ow gj gh gi ox oy bd b be z dk">Fig. 3.2: Generating a convolutional neural network with micro search. Only part of the full architecture is shown.</figcaption></figure><p id="e19e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#3a44" rel="noopener ugc nofollow">返回页首</a></p><h2 id="df67" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">4.履行</h2><ul class=""><li id="e400" class="ml mm je le b lf ny li nz ll pm lp pn lt po lx mq mr ms mt bi translated"><a class="ae lz" href="https://github.com/melodyguan/enas" rel="noopener ugc nofollow" target="_blank"> TensorFlow 由作者实现</a></li><li id="948e" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><a class="ae lz" href="https://github.com/shibuiwilliam/ENAS-Keras" rel="noopener ugc nofollow" target="_blank"> Keras 实施</a></li><li id="0e7b" class="ml mm je le b lf mu li mv ll mw lp mx lt my lx mq mr ms mt bi translated"><a class="ae lz" href="https://github.com/carpedm20/ENAS-pytorch" rel="noopener ugc nofollow" target="_blank"> PyTorch 实施</a></li></ul><p id="91e1" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">[ <a class="ae lz" href="#3a44" rel="noopener ugc nofollow">返回页首</a></p><h2 id="0e01" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">5.参考</h2><p id="8ae1" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated"><a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">通过参数共享进行有效的神经结构搜索</a></p><p id="5650" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" href="https://arxiv.org/abs/1611.01578" rel="noopener ugc nofollow" target="_blank">具有强化学习的神经架构搜索</a></p><p id="d6a0" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" href="https://arxiv.org/abs/1707.07012" rel="noopener ugc nofollow" target="_blank">学习可扩展图像识别的可转移架构</a></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="4c3e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated">就是这样！记得阅读 ENAS 的论文<a class="ae lz" href="https://arxiv.org/abs/1802.03268" rel="noopener ugc nofollow" target="_blank">通过参数共享进行有效的神经架构搜索</a>。如果您有任何问题，请突出显示并留下评论。</p><h2 id="208d" class="ng nh je bd ni nj nk dn nl nm nn dp no ll np nq nr lp ns nt nu lt nv nw nx jk bi translated">其他关于深度学习的文章</h2><p id="334c" class="pw-post-body-paragraph lc ld je le b lf ny ko lh li nz kr lk ll oa ln lo lp ob lr ls lt oc lv lw lx im bi translated"><strong class="le jo">一般</strong></p><p id="4b5a" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">统计深度学习模型中的参数数量</a></p><p id="f984" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">与 NLP 相关</strong></p><p id="226e" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/animated-rnn-lstm-and-gru-ef124d06cf45">动画版的 RNN、LSTM 和 GRU </a></p><p id="0af6" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/attn-illustrated-attention-5ec4ad276ee3">经办人:图文并茂</a></p><p id="b07d" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a">画报:自我关注</a></p><p id="5307" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/an-implementation-guide-to-word2vec-using-numpy-and-google-sheets-13445eebd281">逐行 Word2Vec 实现</a></p><p id="a227" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">与计算机视觉相关</strong></p><p id="8846" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/breaking-down-mean-average-precision-map-ae462f623a52">分解平均平均精度(mAP) </a></p><p id="3c32" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><strong class="le jo">优化</strong></p><p id="7124" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/step-by-step-tutorial-on-linear-regression-with-stochastic-gradient-descent-1d35b088a843">带随机梯度下降的线性回归分步指南</a></p><p id="7232" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/10-gradient-descent-optimisation-algorithms-86989510b5e9"> 10 种梯度下降优化算法+备忘单</a></p><p id="4d00" class="pw-post-body-paragraph lc ld je le b lf lg ko lh li lj kr lk ll lm ln lo lp lq lr ls lt lu lv lw lx im bi translated"><em class="ly">关注我上</em> <a class="ae lz" href="https://www.twitter.com/remykarem" rel="noopener ugc nofollow" target="_blank"> <em class="ly">推特</em> </a> <em class="ly"> @remykarem 或者</em><a class="ae lz" href="http://www.linkedin.com/in/raimibkarim" rel="noopener ugc nofollow" target="_blank"><em class="ly">LinkedIn</em></a><em class="ly">。你也可以通过 raimi.bkarim@gmail.com 联系我。欢迎访问我的网站</em><a class="ae lz" href="https://remykarem.github.io/" rel="noopener ugc nofollow" target="_blank"><em class="ly">remykarem . github . io</em></a><em class="ly">。</em></p></div></div>    
</body>
</html>