# å¹¶è¡Œå¤„ç†å¤§åž‹ AWS S3 æ–‡ä»¶

> åŽŸæ–‡ï¼š<https://towardsdatascience.com/parallelize-processing-a-large-aws-s3-file-d43a580cea3?source=collection_archive---------19----------------------->

## è¿™ç¯‡æ–‡ç« å±•ç¤ºäº†ä½¿ç”¨ AWS S3 é€‰æ‹©å°†ä¸€ä¸ªå¤§çš„ AWS S3 æ–‡ä»¶(å¯èƒ½æœ‰æ•°ç™¾ä¸‡æ¡è®°å½•)å¤„ç†æˆå¯ç®¡ç†çš„å¹¶è¡Œå—çš„æ–¹æ³•

![](img/501c1e9666e0960690c2d12d195eb454.png)

**Parallel Processing S3 File Workflow |** Image created by Author

åœ¨æˆ‘çš„[ä¸Šä¸€ç¯‡æ–‡ç« ](/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46)ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†é€šè¿‡ S3 é€‰æ‹©æ¥æé«˜å¤„ç†å¤§åž‹ AWS S3 æ–‡ä»¶çš„æ•ˆçŽ‡ã€‚å¤„ç†è¿‡ç¨‹æœ‰ç‚¹é¡ºåºï¼Œå¯¹äºŽä¸€ä¸ªå¤§æ–‡ä»¶æ¥è¯´å¯èƒ½éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬å¦‚ä½•åœ¨å¤šä¸ªå•å…ƒä¹‹é—´å¹¶è¡Œå¤„ç†å‘¢ï¼ŸðŸ¤”å—¯ï¼Œåœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å®žçŽ°å®ƒï¼Œå¹¶çœ‹åˆ°å®ƒçš„å·¥ä½œï¼

ðŸ“*æˆ‘å¼ºçƒˆæŽ¨èé€šè¿‡ S3 æŸ¥çœ‹æˆ‘åœ¨* [*ä¸Šçš„ä¸Šä¸€ç¯‡æ–‡ç« â€”â€”é€‰æ‹©*](/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46) *æ¥è®¾ç½®è¿™ç¯‡æ–‡ç« çš„èƒŒæ™¯ã€‚*

æˆ‘æ€»æ˜¯å–œæ¬¢æŠŠä¸€ä¸ªé—®é¢˜åˆ†è§£æˆè§£å†³å®ƒæ‰€å¿…éœ€çš„å°éƒ¨åˆ†(åˆ†æžæ–¹æ³•)ã€‚è®©æˆ‘ä»¬è¯•ç€ç”¨ä¸‰ä¸ªç®€å•çš„æ­¥éª¤æ¥è§£å†³è¿™ä¸ªé—®é¢˜:

# 1.æ‰¾å‡º S3 æ–‡ä»¶çš„æ€»å­—èŠ‚æ•°

ä¸Žæˆ‘ä»¬ä¸Šä¸€ç¯‡æ–‡ç« çš„ç¬¬ä¸€æ­¥éžå¸¸ç›¸ä¼¼ï¼Œè¿™é‡Œæˆ‘ä»¬ä¹Ÿå°è¯•å…ˆæ‰¾åˆ°æ–‡ä»¶å¤§å°ã€‚ä¸‹é¢çš„ä»£ç ç‰‡æ®µå±•ç¤ºäº†å°†å¯¹æˆ‘ä»¬çš„ S3 æ–‡ä»¶æ‰§è¡Œ HEAD è¯·æ±‚å¹¶ç¡®å®šæ–‡ä»¶å¤§å°(ä»¥å­—èŠ‚ä¸ºå•ä½)çš„å‡½æ•°ã€‚

```
# core/utils.py

def get_s3_file_size(bucket: str, key: str) -> int:
    """Gets the file size of S3 object by a HEAD request

    Args:
        bucket (str): S3 bucket
        key (str): S3 object path

    Returns:
        int: File size in bytes. Defaults to 0 if any error.
    """
    aws_profile = current_app.config.get('AWS_PROFILE_NAME')
    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')
    file_size = 0
    try:
        response = s3_client.head_object(Bucket=bucket, Key=key)
        if response:
            file_size = int(response.get('ResponseMetadata').get('HTTPHeaders').get('content-length'))
    except ClientError:
        logger.exception(f'Client error reading S3 file {bucket} : {key}')
    return file_size
```

# 2.åˆ›å»ºä¸€ä¸ªèŠ¹èœä»»åŠ¡æ¥å¤„ç†ä¸€ä¸ªå—

è¿™é‡Œï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª celery ä»»åŠ¡æ¥å¤„ç†ä¸€ä¸ªæ–‡ä»¶å—(ç¨åŽå°†å¹¶è¡Œæ‰§è¡Œ)ã€‚è¿™é‡Œçš„æ•´ä¸ªå¤„ç†è¿‡ç¨‹å¦‚ä¸‹æ‰€ç¤º:

*   æŽ¥æ”¶è¿™ä¸ªå—çš„`start`å’Œ`end bytes`ä½œä¸ºå‚æ•°
*   é€šè¿‡ S3 èŽ·å– S3 æ–‡ä»¶çš„è¿™ä¸€éƒ¨åˆ†â€”â€”é€‰æ‹©å¹¶å°†å…¶å­˜å‚¨åœ¨æœ¬åœ°çš„ä¸€ä¸ªä¸´æ—¶æ–‡ä»¶ä¸­(åœ¨æœ¬ä¾‹ä¸­ä¸º CSV)
*   è¯»å–è¿™ä¸ªä¸´æ—¶æ–‡ä»¶å¹¶æ‰§è¡Œä»»ä½•éœ€è¦çš„å¤„ç†
*   åˆ é™¤è¿™ä¸ªä¸´æ—¶æ–‡ä»¶

ðŸ“æˆ‘å°†è¿™ä¸ªä»»åŠ¡ç§°ä¸ºæ–‡ä»¶å—å¤„ç†å™¨ã€‚å®ƒå¤„ç†æ–‡ä»¶ä¸­çš„ä¸€ä¸ªå—ã€‚è¿è¡Œå¤šä¸ªè¿™æ ·çš„ä»»åŠ¡å¯ä»¥å®Œæˆæ•´ä¸ªæ–‡ä»¶çš„å¤„ç†ã€‚

```
# core/tasks.py

@celery.task(name='core.tasks.chunk_file_processor', bind=True)
def chunk_file_processor(self, **kwargs):
    """ Creates and process a single file chunk based on S3 Select ScanRange start and end bytes
    """
    bucket = kwargs.get('bucket')
    key = kwargs.get('key')
    filename = kwargs.get('filename')
    start_byte_range = kwargs.get('start_byte_range')
    end_byte_range = kwargs.get('end_byte_range')
    header_row_str = kwargs.get('header_row_str')
    local_file = filename.replace('.csv', f'.{start_byte_range}.csv')
    file_path = path.join(current_app.config.get('BASE_DIR'), 'temp', local_file)

    logger.info(f'Processing {filename} chunk range {start_byte_range} -> {end_byte_range}')
    try:
        # 1\. fetch data from S3 and store it in a file
        store_scrm_file_s3_content_in_local_file(
            bucket=bucket, key=key, file_path=file_path, start_range=start_byte_range,
            end_range=end_byte_range, delimiter=S3_FILE_DELIMITER, header_row=header_row_str)

        # 2\. Process the chunk file in temp folder
        id_set = set()
        with open(file_path) as csv_file:
            csv_reader = csv.DictReader(csv_file, delimiter=S3_FILE_DELIMITER)
            for row in csv_reader:
                # perform any other processing here
                id_set.add(int(row.get('id')))
        logger.info(f'{min(id_set)} --> {max(id_set)}')

        # 3\. delete local file
        if path.exists(file_path):
            unlink(file_path)
    except Exception:
        logger.exception(f'Error in file processor: {filename}')
```

# 3.å¹¶è¡Œæ‰§è¡Œå¤šä¸ª celery ä»»åŠ¡

è¿™æ˜¯è¿™ä¸ªæµç¨‹ä¸­æœ€æœ‰è¶£çš„ä¸€æ­¥ã€‚æˆ‘ä»¬å°†é€šè¿‡ [celery Group](https://docs.celeryproject.org/en/stable/userguide/canvas.html#groups) åˆ›å»ºå¤šä¸ªå¹¶è¡Œè¿è¡Œçš„ Celery ä»»åŠ¡ã€‚
ä¸€æ—¦æˆ‘ä»¬çŸ¥é“äº† S3 ä¸­ä¸€ä¸ªæ–‡ä»¶çš„æ€»å­—èŠ‚æ•°(æ¥è‡ªæ­¥éª¤ 1)ï¼Œæˆ‘ä»¬å°±ä¸ºè¿™ä¸ªå—è®¡ç®—`start`å’Œ`end bytes`ï¼Œå¹¶é€šè¿‡ celery ç»„è°ƒç”¨æˆ‘ä»¬åœ¨æ­¥éª¤ 2 ä¸­åˆ›å»ºçš„ä»»åŠ¡ã€‚`start`å’Œ`end bytes`èŒƒå›´æ˜¯æ–‡ä»¶å¤§å°çš„è¿žç»­èŒƒå›´ã€‚å¯é€‰åœ°ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨æ‰€æœ‰å¤„ç†ä»»åŠ¡å®ŒæˆåŽè°ƒç”¨å›žè°ƒ(ç»“æžœ)ä»»åŠ¡ã€‚

```
# core/tasks.py

@celery.task(name='core.tasks.s3_parallel_file_processing', bind=True)
def s3_parallel_file_processing_task(self, **kwargs):
    """ Creates celery tasks to process chunks of file in parallel
    """
    bucket = kwargs.get('bucket')
    key = kwargs.get('key')
    try:
        filename = key
        # 1\. Check file headers for validity -> if failed, stop processing
        desired_row_headers = (
            'id',
            'name',
            'age',
            'latitude',
            'longitude',
            'monthly_income',
            'experienced'
        )
        is_headers_valid, header_row_str = validate_scrm_file_headers_via_s3_select(
            bucket=bucket,
            key=key,
            delimiter=S3_FILE_DELIMITER,
            desired_headers=desired_row_headers)
        if not is_headers_valid:
            logger.error(f'{filename} file headers validation failed')
            return False
        logger.info(f'{filename} file headers validation successful')

        # 2\. fetch file size via S3 HEAD
        file_size = get_s3_file_size(bucket=bucket, key=key)
        if not file_size:
            logger.error(f'{filename} file size invalid {file_size}')
            return False
        logger.info(f'We are processing {filename} file about {file_size} bytes :-o')

        # 2\. Create celery group tasks for chunk of this file size for parallel processing
        start_range = 0
        end_range = min(S3_FILE_PROCESSING_CHUNK_SIZE, file_size)
        tasks = []
        while start_range < file_size:
            tasks.append(
                chunk_file_processor.signature(
                    kwargs={
                        'bucket': bucket,
                        'key': key,
                        'filename': filename,
                        'start_byte_range': start_range,
                        'end_byte_range': end_range,
                        'header_row_str': header_row_str
                    }
                )
            )
            start_range = end_range
            end_range = end_range + min(S3_FILE_PROCESSING_CHUNK_SIZE, file_size - end_range)
        job = (group(tasks) | chunk_file_processor_callback.s(data={'filename': filename}))
        _ = job.apply_async()
    except Exception:
        logger.exception(f'Error processing file: {filename}')

@celery.task(name='core.tasks.chunk_file_processor_callback', bind=True, ignore_result=False)
def chunk_file_processor_callback(self, *args, **kwargs):
    """ Callback task called post chunk_file_processor()
    """
    logger.info('Callback called') # core/utils.py

def store_scrm_file_s3_content_in_local_file(bucket: str, key: str, file_path: str, start_range: int, end_range: int,
                                             delimiter: str, header_row: str):
    """Retrieves S3 file content via S3 Select ScanRange and store it in a local file.
       Make sure the header validation is done before calling this.

    Args:
        bucket (str): S3 bucket
        key (str): S3 key
        file_path (str): Local file path to store the contents
        start_range (int): Start range of ScanRange parameter of S3 Select
        end_range (int): End range of ScanRange parameter of S3 Select
        delimiter (str): S3 file delimiter
        header_row (str): Header row of the local file. This will be inserted as first line in local file.
    """
    aws_profile = current_app.config.get('AWS_PROFILE_NAME')
    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')
    expression = 'SELECT * FROM S3Object'
    try:
        response = s3_client.select_object_content(
            Bucket=bucket,
            Key=key,
            ExpressionType='SQL',
            Expression=expression,
            InputSerialization={
                'CSV': {
                    'FileHeaderInfo': 'USE',
                    'FieldDelimiter': delimiter,
                    'RecordDelimiter': '\n'
                }
            },
            OutputSerialization={
                'CSV': {
                    'FieldDelimiter': delimiter,
                    'RecordDelimiter': '\n',
                },
            },
            ScanRange={
                'Start': start_range,
                'End': end_range
            },
        )

        """
        select_object_content() response is an event stream that can be looped to concatenate the overall result set
        """
        f = open(file_path, 'wb')  # we receive data in bytes and hence opening file in bytes
        f.write(header_row.encode())
        f.write('\n'.encode())
        for event in response['Payload']:
            if records := event.get('Records'):
                f.write(records['Payload'])
        f.close()
    except ClientError:
        logger.exception(f'Client error reading S3 file {bucket} : {key}')
    except Exception:
        logger.exception(f'Error reading S3 file {bucket} : {key}')
```

å°±æ˜¯è¿™æ ·ï¼ðŸ˜ŽçŽ°åœ¨ï¼Œæˆ‘ä»¬ä¸æ˜¯ä¸€ä¸ªå­—èŠ‚ä¸€ä¸ªå­—èŠ‚åœ°ä¼ è¾“ S3 æ–‡ä»¶ï¼Œè€Œæ˜¯é€šè¿‡å¹¶å‘å¤„ç†æ•°æ®å—æ¥å®žçŽ°å¹¶è¡Œå¤„ç†ã€‚æ²¡é‚£ä¹ˆéš¾ï¼Œä¸æ˜¯å—ï¼ŸðŸ˜…

ðŸ“Œæ‚¨å¯ä»¥[æŸ¥çœ‹æˆ‘çš„ GitHub åº“](https://github.com/idris-rampurawala/s3-select-demo)ä»¥èŽ·å¾—è¿™ç§æ–¹æ³•çš„å®Œæ•´å·¥ä½œç¤ºä¾‹ã€‚

# ðŸ”æ¯”è¾ƒå¤„ç†æ—¶é—´

å¦‚æžœæˆ‘ä»¬ç”¨è¿™ç§æ–¹æ³•æ¯”è¾ƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­å¤„ç†çš„åŒä¸€ä¸ªæ–‡ä»¶çš„å¤„ç†æ—¶é—´ï¼Œå¤„ç†é€Ÿåº¦å¤§çº¦æ¯”**å¿« 68%**(ä½¿ç”¨ç›¸åŒçš„ç¡¬ä»¶å’Œé…ç½®)ã€‚ðŸ˜†

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                 â•‘ **Streaming S3 File** â•‘ **Parallel Processing S3 File**â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¬â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ **File size** â•‘ 4.8MB             â•‘ 4.8MB                      â•‘
â•‘ **Processing time** â•‘ ~37 seconds       â•‘ ~12 seconds                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

![](img/0193f332707396b6048acd2a2001764f.png)

**Streaming S3 File** **Logs** | Image by the Author

![](img/60aa39cc9e6bad315a14b04265b71fd8.png)

**Parallel Processing S3 File Logs |** Image by the Author

# âœ”ï¸è¿™ç§æ–¹æ³•çš„å¥½å¤„

*   åŒ…å«æ•°ç™¾ä¸‡æ¡è®°å½•çš„éžå¸¸å¤§çš„æ–‡ä»¶å¯ä»¥åœ¨å‡ åˆ†é’Ÿå†…å¾—åˆ°å¤„ç†ã€‚æˆ‘åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ä½¿ç”¨è¿™ç§æ–¹æ³•å·²ç»æœ‰ä¸€æ®µæ—¶é—´äº†ï¼Œå®ƒéžå¸¸ä»¤äººæ„‰å¿«
*   è®¡ç®—å’Œå¤„ç†åˆ†å¸ƒåœ¨åˆ†å¸ƒçš„å·¥ä½œäººå‘˜ä¸­
*   å·¥ä½œæ± çš„å¯ç”¨æ€§å¯ä»¥è°ƒæ•´å¤„ç†é€Ÿåº¦
*   ä¸å†æœ‰å†…å­˜é—®é¢˜

# ðŸ“‘èµ„æº

*   [æˆ‘çš„ GitHub åº“å±•ç¤ºäº†ä¸Šè¿°æ–¹æ³•](https://github.com/idris-rampurawala/s3-select-demo)
*   [AWS S3 é€‰æ‹© boto3 å‚è€ƒå€¼](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content)
*   [AWS S3 é€‰æ‹©ç”¨æˆ·æŒ‡å—](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)

*åŽŸå‘å¸ƒäºŽ 2019 å¹´ 1 æœˆ 22 æ—¥*[*https://dev . to*](https://dev.to/idrisrampurawala/parallelize-processing-a-large-aws-s3-file-8eh)*ã€‚*