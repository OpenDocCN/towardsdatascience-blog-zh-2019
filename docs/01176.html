<html>
<head>
<title>Spam Filtering System With Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有深度学习的垃圾邮件过滤系统</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spam-filtering-system-with-deep-learning-b8070b28f9e0?source=collection_archive---------6-----------------------#2019-02-23">https://towardsdatascience.com/spam-filtering-system-with-deep-learning-b8070b28f9e0?source=collection_archive---------6-----------------------#2019-02-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8944" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">并探讨了单词嵌入的幂特征提取</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/55b6307936b4940e75f1f9cb52955951.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WAB5v7IvB4vtvivR"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@rozetsky?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ant Rozetsky</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5b1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习在许多行业变得非常流行，许多有趣的问题都可以通过深度学习技术来解决。在本文中，我将向您展示如何利用深度学习模型来设计一个超级有效的垃圾邮件过滤系统。</p><p id="d0e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不久前，我写了一篇关于用传统的机器学习算法过滤垃圾邮件的文章。</p><p id="619b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在那篇文章中，我介绍了从数据探索、数据预处理、特征提取到为算法选择正确的评分标准。有兴趣的可以看这里的文章<a class="ae kv" href="https://medium.com/@edricgan.44/email-spam-detection-1-2-b0e06a5c0472" rel="noopener">！</a></p><p id="01e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">今天，在构建垃圾邮件过滤系统时，我将更多地关注以下两个部分:</p><ol class=""><li id="a497" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">单词嵌入</li><li id="9985" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">GRU +双向深度学习模型</li></ol><h1 id="b90a" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">什么是文字嵌入？</strong></h1><p id="f163" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">单词嵌入是以矢量化格式表示的文本数据。因此，对于像“dog”这样的单词，单词 embedding 会将其转换为一个形状为(1，x)的向量，x 是一个可以配置的值。</p><p id="2256" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直觉上，你可以把这个向量看作是描述这个词的一种方式。如果这个词是一个形状向量(1300)，这意味着有 300 个不同的特征描述这个词。</p><p id="6ebd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那些特征是什么？老实说，我们不知道。深度学习将在训练过程中识别这些特征。</p><p id="b3a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练过程的最终结果是从单词到有意义的单词向量的映射。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/0bc27757105a29d37a025cd362234554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W_IiI8LjuPmPmLGR.jpg"/></div></div></figure><p id="68a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">听起来像个很酷的魔术，是吧？更酷的是，两个单词向量之间的余弦距离实际上意味着一些重要的东西。</p><p id="7108" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具有更接近语义的单词在向量空间中将具有更短的余弦距离。</p><p id="c1e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，向量“男人”和向量“女人”之间的余弦距离非常接近。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="a9f5" class="nj mh iq nf b gy nk nl l nm nn">w2v.most_similar('man')</span><span id="3944" class="nj mh iq nf b gy no nl l nm nn"># Output<br/># [('woman', 0.699),('person', 0.644),('him', 0.567)]</span><span id="ae1e" class="nj mh iq nf b gy no nl l nm nn"># 'woman' has the highest similarity score with 'man'</span></pre><p id="a992" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预训练权重的单词嵌入</strong></p><p id="698e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有一些开源的单词嵌入已经在大量的文本数据上进行训练，它们的权重开放给公众下载。</p><p id="0ec3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用开源单词嵌入，您可以节省收集文本数据的时间。而且，还可以节省生成单词嵌入的时间和计算资源。</p><p id="8720" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用预先训练的单词嵌入的缺点是单词嵌入可能在来自不同域的数据源上被训练。这可能并不总是适合您正在应用的用例。</p><p id="827c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，在科学期刊的文本数据上训练的预训练单词嵌入可能不太适合检测恶意推文之类的问题。这个单词嵌入的好处不会那么显著。</p><p id="a52e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预训练单词嵌入的不同变体</strong></p><p id="cb4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大多数开源单词嵌入都列出了它们被训练的来源，所以你需要根据你要解决的问题仔细选择。</p><p id="5170" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下是一些例子:</p><ol class=""><li id="54a8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">手套</a></li></ol><p id="3eb1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<a class="ae kv" href="https://fasttext.cc/docs/en/english-vectors.html" rel="noopener ugc nofollow" target="_blank">维基新闻</a></p><p id="2b60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<a class="ae kv" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">谷歌新闻矢量</a></p><p id="4630" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">试验单词嵌入</strong></p><p id="fef0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们使用手套字嵌入。手套嵌入的格式与 python 库<strong class="ky ir"> gensim，</strong>所期望的有点不同。</p><p id="c780" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以你从手套官网下载嵌入这个词之后，需要做一些简单的转换。</p><p id="7d55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过运行下面的 python 脚本，可以很容易地将其转换成与 word2vec 嵌入兼容的格式。</p><p id="3517" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">确保您安装了 python gensim 模块。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="1971" class="nj mh iq nf b gy nk nl l nm nn">python -m gensim.scripts.glove2word2vec -i glove.6B.300d.txt -o glove.6B.300d.word2vec.txt</span></pre><p id="f46d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之后，您可以使用 gensim 库轻松加载它，</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="f2b3" class="nj mh iq nf b gy nk nl l nm nn">w2v = KeyedVectors.load_word2vec_format(<br/>      'glove.6B.300d.word2vec.txt’,binary=False)</span></pre><p id="ba69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以对这个向量执行一些操作，并得到一些有趣的结果。</p><p id="ab97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如:</p><blockquote class="np"><p id="18b0" class="nq nr iq bd ns nt nu nv nw nx ny lr dk translated">例 1:国王-男人+女人=王后</p><p id="2d1a" class="nq nr iq bd ns nt nu nv nw nx ny lr dk translated">示例 2:马德里-西班牙+法国=巴黎</p></blockquote><p id="fadc" class="pw-post-body-paragraph kw kx iq ky b kz nz jr lb lc oa ju le lf ob lh li lj oc ll lm ln od lp lq lr ij bi translated">让我们看看如何在代码中实现这一点，以及我们可能会得到什么有趣的结果。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="f07d" class="nj mh iq nf b gy nk nl l nm nn">w2v.most_similar(['king','woman'],negative=['man'],topn=1)<br/># Output: [('queen', 0.6713277101516724)]</span><span id="90a8" class="nj mh iq nf b gy no nl l nm nn">w2v.most_similar(['madrid','france'],negative=['spain'],topn=1)<br/># Output: [('paris', 0.758114755153656)]</span></pre><p id="490a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以让单词嵌入模型计算出给定单词列表中哪个是离群值</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="0de3" class="nj mh iq nf b gy nk nl l nm nn">w2v.doesnt_match("england china vietnam laos".split())<br/>#Output: england</span><span id="9283" class="nj mh iq nf b gy no nl l nm nn">w2v.doesnt_match("pig dog cat tree".split())<br/>#Output : tree</span><span id="1433" class="nj mh iq nf b gy no nl l nm nn">w2v.doesnt_match("fish shark cat whale".split())<br/>#Output : cat</span></pre><p id="67c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我已经在笔记本中包含了相关代码，您可以尝试不同的输入，看看它是否符合您的期望。</p><p id="959b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我想你已经见证了单词嵌入强大的特征提取能力。</p><p id="4386" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下一节中，我们将了解如何将这种预训练的单词嵌入与 Keras 中的<strong class="ky ir">嵌入层</strong>相结合，以便我们可以在训练中使用它。</p><p id="8dcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">嵌入层</strong></p><p id="d3a8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Keras </strong>是一个非常棒的高级深度学习库，可以帮助你轻松构建深度学习模型。它抽象了许多低级的数学细节，让你以一种非常直观的方式建立你的模型。</p><p id="467b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入层是 Keras 提供的包装层之一，方便我们训练单词嵌入。</p><p id="8f7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们需要利用 tokenizer 来帮助我们将所有的单词转换成某种标记/索引。</p><p id="0618" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在标记化层，他们维护一个将单词映射到索引的字典。比如狗-&gt; 0，猫-&gt;1 等等。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="b741" class="nj mh iq nf b gy nk nl l nm nn">max_feature = 50000</span><span id="a084" class="nj mh iq nf b gy no nl l nm nn">tokenizer = Tokenizer(num_words=max_feature)<br/>tokenizer.fit_on_texts(x_train)</span><span id="d419" class="nj mh iq nf b gy no nl l nm nn"># Converting x_train to integer token, token is just an index number that can uniquely identify a particular word</span><span id="f58a" class="nj mh iq nf b gy no nl l nm nn">x_train_features = np.array(tokenizer.texts_to_sequences(x_train))<br/>x_test_features = np.array(tokenizer.texts_to_sequences(x_test))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ddb0921386657a122fc20bb2fac4a6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*ZG46k8e2NvwRtYE-TrMBdg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Tokenization Layer to transfer text to simple token</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/c1654b02530d640043e5417d80877613.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*pNrEOhjdBxnFHFecR0b0KQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Internal working of the tokenization layer</figcaption></figure><p id="8e3a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">嵌入层将在内部维护一个查找表，该查找表将索引/标记映射到一个向量，该向量在高维空间中表示单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/47dd32f646cc7e4105849ab7c63350d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cv3Nd9VygY4DNbPt4zFXZA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Full overview of the whole transformation</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c3f058014f1bc9a7fc14126059ad59de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*MWzECItaf2jLyU5DFbzG6w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Brief summary of the whole process</figcaption></figure><h1 id="fdb3" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">嵌入+双向+门控递归单元(GRU) </strong></h1><p id="9727" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir"> GRU </strong></p><p id="5085" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GRU 是 LSTM 建筑的变体，许多文章在解释背后的理论方面做了大量工作。</p><p id="3337" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了简洁起见，我不会对所有这些模型背后的理论做太多解释。</p><p id="5a1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我推荐克里斯·奥拉写的关于这个话题的<a class="ae kv" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a>。他的博客展示了一些精美的图片来解释 GRU 的内部运作。</p><p id="859e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">双向</strong></p><p id="300b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">双向的想法简单而强大。它的作用是拥有两个 LSTM 网络，而不是一个。</p><p id="e61d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于第一个 LSTM 网络，它将按照正常方式输入序列。对于第二个 LSTM 网络，它将反转输入序列并馈入 LSTM 网络。这两个网络的输出将被合并，然后传递到下一层。</p><p id="20cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">双向背后的直觉是，对于某些句子，上下文信息在句子的末尾。没有上下文信息，可能会产生歧义。例如:</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="15ca" class="nj mh iq nf b gy nk nl l nm nn">1. Find me at the bank in that forest. (River bank)<br/></span><span id="e891" class="nj mh iq nf b gy no nl l nm nn">2. Find me at the bank in the city center. (Financial Bank)</span></pre><p id="3f6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，双向阅读句子有助于模型确定单词的确切含义。</p><p id="61fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你对理论知识感兴趣，我会推荐阅读原文<a class="ae kv" href="https://maxwell.ict.griffith.edu.au/spl/publications/papers/ieeesp97_schuster.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="4312" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">构建网络</strong></p><p id="cda1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们开始在 Keras 中构建我们的网络，我们将涉及嵌入层、双向层和门控循环单元(GRU)等组件。</p><p id="18fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于嵌入，我们有两种选择:</p><ol class=""><li id="738b" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">从头开始训练嵌入层</li><li id="04b7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用一些预先训练好的开源权重嵌入。</li></ol><p id="33db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文的最后，我们将比较使用<strong class="ky ir">普通</strong>单词嵌入和<strong class="ky ir">手套</strong>单词嵌入产生的结果。</p><p id="29f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">香草字嵌入</strong></p><p id="7c2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的代码片段显示了如何在 Keras 中轻松构建嵌入层、双向和 GRU。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="cb35" class="nj mh iq nf b gy nk nl l nm nn">inp = Input(shape=(max_len,))</span><span id="8abb" class="nj mh iq nf b gy no nl l nm nn">x = EmbeddingLayer(max_features,embed_size)(inp)</span><span id="fc85" class="nj mh iq nf b gy no nl l nm nn">x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)</span><span id="ef6b" class="nj mh iq nf b gy no nl l nm nn">x = GlobalMaxPool1D()(x)</span><span id="8d0e" class="nj mh iq nf b gy no nl l nm nn">x = Dense(16, activation="relu")(x)i</span><span id="17f2" class="nj mh iq nf b gy no nl l nm nn">x = Dropout(0.1)(x)</span><span id="02b3" class="nj mh iq nf b gy no nl l nm nn">x = Dense(1, activation="sigmoid")(x)</span><span id="af1a" class="nj mh iq nf b gy no nl l nm nn">model = Model(inputs=inp, outputs=x)</span><span id="1782" class="nj mh iq nf b gy no nl l nm nn">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="9bce" class="nj mh iq nf b gy no nl l nm nn">print(model.summary())</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/209b17ef0507bc00da6e195d97335292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*vcSXVz8iDpyxN4if78vegQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Summary of the model</figcaption></figure><p id="5460" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预训练手套字嵌入</strong></p><p id="141f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手套单词嵌入是由斯坦福大学 NLP 小组开源的。我们需要先下载嵌入这个词，你可以在官方<a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">网站</a>找到出处信息。</p><p id="63a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们已经发布了不同版本的 word embedding，并在不同的数据源上接受了培训。请随意使用这里列出的单词嵌入的其他变体进行更多的实验。</p><p id="35cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将重量从手套转移到 Keras 嵌入层的代码相当长，这里我只展示一小段代码，让您了解转移是如何进行的。我会在本文末尾分享笔记本代码。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="98cb" class="nj mh iq nf b gy nk nl l nm nn">embeddings_index = convert_glove_to_index('glove.6B.300d.txt')</span><span id="d9be" class="nj mh iq nf b gy no nl l nm nn"># Randomly initialize the embedding matrix<br/>embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))</span><span id="9369" class="nj mh iq nf b gy no nl l nm nn"># Transferring weight<br/>for word, i in word_index.items():<br/>    embedding_vector = embeddings_index.get(word)<br/>    if embedding_vector is not None: <br/>       embedding_matrix[i] = embedding_vector</span></pre><p id="f9e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，嵌入矩阵包含了来自手套嵌入的所有权重，将它们转移到 Keras 层只是多了一行代码。</p><p id="8a4f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">构建模型时，需要指定嵌入层的权重。其余的代码与构建普通嵌入层的方式相同。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="311b" class="nj mh iq nf b gy nk nl l nm nn">x = Embedding(max_features, embed_size, weights = [embedding_matrix])(inp)</span></pre><p id="05ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">模型的性能</strong></p><p id="1cf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练集上训练几次迭代后，让我们从<strong class="ky ir">准确度</strong>、<strong class="ky ir">精度</strong>和<strong class="ky ir">召回</strong>方面比较模型的性能。</p><p id="f8c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你对为什么选择这些指标感兴趣，你可以阅读我之前写的<a class="ae kv" href="https://medium.com/@edricgan.44/email-spam-detection-1-2-b0e06a5c0472" rel="noopener">文章</a>。我已经包含了对这些不同的性能指标选择的详细解释。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="bfb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上表中，你可以清楚地看出 LSTM 模型比朴素贝叶斯算法做得更好。原因可能是:</p><ol class=""><li id="cee5" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">Tfidf 矢量器没有考虑句子中单词的顺序，因此丢失了大量信息。</li><li id="2959" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">LSTM 是近年来序列数据(文本、语音、时间序列数据)中最伟大的算法之一</li></ol></div><div class="ab cl ol om hu on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/6323783b38e13555cef37af1470da5b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gbOF-SrM_9sZJJFA"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@rawpixel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">rawpixel</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="7e64" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">两种嵌入模型的性能比较</h1><p id="357c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir">精度&amp;召回</strong></p><p id="403c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我只运行了 20 个时期的模型，没有做任何进一步的微调。</p><p id="eaa5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这两个模型的精确度和召回率相差不大。一些原因可能是由于:</p><ol class=""><li id="ffef" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">手套嵌入是在与我们在这个问题中拥有的数据非常不同的源上训练的，因此我们从预训练嵌入中获得的好处没有显著的帮助</li><li id="73c3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">这个垃圾邮件过滤问题中的文本数据并不太复杂。普通的单词嵌入是捕捉模式的一个足够好的模型。</li></ol><p id="2317" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">比较这两个模型的精度图，你会发现一些有趣的东西:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a764513f8454defb56bc46634f2c4bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*2Ew_fbuJjfIwc343BPX-0Q.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Accuracy of Glove Word Embedding</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b98910cb72744b98729d00366c16a4d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*NtTjJTwgdJLhYx1L5cfZ3A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Accuracy of Vanilla Word Embedding</figcaption></figure><p id="4164" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开始有更好的性能之前，香草词嵌入的准确性在最初的几个时期保持平稳。</p><p id="a934" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，手套词嵌入的准确度具有平滑的准确度曲线，并且逐渐提高。</p><p id="904e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些观察可以粗略地给我们一些启示，即在训练的早期，普通单词嵌入模型仍然在学习和调整它的权重。</p><p id="7f67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，预先训练的单词嵌入仍然有助于加快模型的学习，您可能不会注意到这个问题中的速度优势，但是当您在足够大的文本数据上训练时，这些速度差异将非常显著。</p><h1 id="9b7c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated"><strong class="ak">结论</strong></h1><p id="73e4" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在本文中，我展示了如何通过 Keras 使用单词嵌入和 LSTM 模型构建一个垃圾邮件过滤系统。</p><p id="262d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于文本数据，单词嵌入无疑是一个很好的特征提取工具，利用 LSTM 模型，我们可以构建一个性能非常好的垃圾邮件过滤系统。</p><p id="5550" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在这个 Github <a class="ae kv" href="https://github.com/huai99/Email-Spam-Detection-Python/blob/master/Email%20Spam%20(%20Medium%20Part%202).ipynb" rel="noopener ugc nofollow" target="_blank">库</a>中找到这个笔记本的代码，或者你可以直接从<a class="ae kv" href="https://colab.research.google.com/drive/1mvY1vFMvMnUIbC8hvHvlm-UeSeSAlil9" rel="noopener ugc nofollow" target="_blank"> Colab </a>中运行它。</p></div></div>    
</body>
</html>