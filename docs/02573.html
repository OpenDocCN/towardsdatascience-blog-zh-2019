<html>
<head>
<title>Automatically Generate Hotel Descriptions with LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 LSTM 自动生成酒店描述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/automatically-generate-hotel-descriptions-with-lstm-afa37002d4fc?source=collection_archive---------6-----------------------#2019-04-27">https://towardsdatascience.com/automatically-generate-hotel-descriptions-with-lstm-afa37002d4fc?source=collection_archive---------6-----------------------#2019-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/1824b1bf620296c69641d6b1183e58d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BI0UF6bKHA6RLozIOF0wJw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: <a class="ae jd" href="https://www.marriott.com/hotels/travel/seawh-w-seattle/" rel="noopener ugc nofollow" target="_blank">W Seattle</a></figcaption></figure><div class=""/><div class=""><h2 id="a6dd" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如何使用 Python 中的 LSTM 递归神经网络和 Keras 创建文本生成模型</h2></div><p id="675a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了建立一个<a class="ae jd" rel="noopener" target="_blank" href="/building-a-content-based-recommender-system-for-hotels-in-seattle-d724f0a32070">基于内容的推荐系统</a>，我收集了西雅图 152 家酒店的描述。我在想一些其他的方法来折磨这个高质量的干净数据集。</p><p id="2e49" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">嘿！为什么不训练我自己的酒店描述文本生成神经网络？即，通过实现和训练基于单词的递归神经网络来创建用于生成自然语言文本(即，酒店描述)的语言模型。</p><p id="460d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个项目的目标是生成新的酒店描述，给定一些输入文本。我不期望结果是准确的，只要预测的文本是连贯的，我就高兴了。</p><p id="37e6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢<a class="ae jd" href="https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275" rel="noopener">Shivam ban sal</a>的这个教程帮助我完成了这个练习。</p><h1 id="fcf2" class="lt lu jg bd lv lw lx ly lz ma mb mc md km me kn mf kp mg kq mh ks mi kt mj mk bi translated">数据</h1><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mp mq l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">desc_preprocessing.py</figcaption></figure><figure class="ml mm mn mo gt is gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/4997ae5c1b7062f94f930b26437a40d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*z7d3LHoyl1ByfkRNkMpmLw.png"/></div></figure><p id="487e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的数据集中总共有 152 个描述(即酒店)。</p><p id="2c4a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">先看一下第一个描述:</p><pre class="ml mm mn mo gt ms mt mu mv aw mw bi"><span id="0c4d" class="mx lu jg mt b gy my mz l na nb">corpus = [x for x in all_descriptions]<br/>corpus[:1]</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/0a71a1a553efe85e31c6a4730c585fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrP4WyuhCt4nIewAedGAvQ.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 1</figcaption></figure><p id="2d57" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在标记化之后，我们可以:</p><ul class=""><li id="8818" class="nd ne jg kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">探索单词及其计数的字典。</li><li id="76ff" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">浏览单词词典，了解每个单词出现在多少个文档中。</li><li id="3b6e" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">探索用于适应记号赋予器的文档总数的整数计数(即文档总数)。</li><li id="739d" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">探索单词及其唯一指定整数的字典。</li></ul><pre class="ml mm mn mo gt ms mt mu mv aw mw bi"><span id="99f1" class="mx lu jg mt b gy my mz l na nb">t = Tokenizer(num_words=None, filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n', lower=True, split=' ', char_level=False, oov_token=None, document_count=0)<br/>t.fit_on_texts(corpus)</span><span id="2833" class="mx lu jg mt b gy nr mz l na nb">print(t.word_counts)<br/>print(t.word_docs)<br/>print(t.document_count)<br/>print(t.word_index)<br/>print('Found %s unique tokens.' % len(t.word_index))</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6f1b643874fb246c559d4ac74475b000.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*q95qEGWYHhVXoC__naZKSQ.png"/></div></figure><h1 id="e57a" class="lt lu jg bd lv lw lx ly lz ma mb mc md km me kn mf kp mg kq mh ks mi kt mj mk bi translated">文本预处理</h1><h2 id="4e6d" class="mx lu jg bd lv nt nu dn lz nv nw dp md le nx ny mf li nz oa mh lm ob oc mj od bi translated">标记化</h2><p id="c996" class="pw-post-body-paragraph kv kw jg kx b ky oe kh la lb of kk ld le og lg lh li oh lk ll lm oi lo lp lq ij bi translated">我们使用 Keras 的标记器来矢量化文本描述，</p><ul class=""><li id="bdf7" class="nd ne jg kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">我们删除所有标点符号。</li><li id="60d0" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们将文本转换成小写的空格分隔的单词序列。</li><li id="255b" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">然后，这些序列被分割成记号列表。</li><li id="343a" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们设置了<code class="fe oj ok ol mt b">char_level=False</code>，所以每一个单词都会被当作一个记号而不是字符。</li><li id="e055" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">然后，记号列表将被索引或/和矢量化。</li><li id="687e" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们将语料库转换成符号序列。</li></ul><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mp mq l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">sequence.py</figcaption></figure><figure class="ml mm mn mo gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/0f5299f09cebb10eb7b0b8ca336854bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*1XDX5rUNNXW_HlHS0lrTzA.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 2</figcaption></figure><p id="a314" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面的整数列表表示从语料库中生成的 ngram 短语。例如，假设一个句子“<strong class="kx jh"> <em class="on">位于尤宁</em> </strong>湖的南端”由这样的词的索引来表示:</p><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/9e6d4749769df365c38b4bf7d99929e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pB_lF8A8rh_KQGuxcO9cJw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Table 1</figcaption></figure><h2 id="3999" class="mx lu jg bd lv nt nu dn lz nv nw dp md le nx ny mf li nz oa mh lm ob oc mj od bi translated">填充序列并创建预测值和标签</h2><ul class=""><li id="a35e" class="nd ne jg kx b ky oe lb of le op li oq lm or lq ni nj nk nl bi translated">将序列填充到相同的长度</li><li id="5f68" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">Pad sequences 将整数列表转换成形状为<code class="fe oj ok ol mt b">(num_samples, maxlen)</code>的 2D Numpy 数组。</li><li id="f6a8" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">预测值和标签看起来像这样:</li></ul><figure class="ml mm mn mo gt is gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d605325cce7b89efba5aa897b26fcdc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*1WGeAMhHJltnAEuMNv2EUQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Table 2</figcaption></figure><p id="670a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如你所见，如果我们想要精确，这将会非常非常困难。</p><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mp mq l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">pad_sequence.py</figcaption></figure><h1 id="bfea" class="lt lu jg bd lv lw lx ly lz ma mb mc md km me kn mf kp mg kq mh ks mi kt mj mk bi translated">建模</h1><p id="79e7" class="pw-post-body-paragraph kv kw jg kx b ky oe kh la lb of kk ld le og lg lh li oh lk ll lm oi lo lp lq ij bi translated">我们现在可以定义我们的单一 LSTM 模型。</p><ul class=""><li id="0420" class="nd ne jg kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">具有 100 个存储单元的单个隐藏 LSTM 层。</li><li id="478d" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">网络使用概率为 10 的辍学。</li><li id="c369" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">输出层是使用 softmax 激活函数输出 0 到 1 之间的 3420 个单词中每个单词的概率预测的密集层。</li><li id="f459" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们的问题是具有 3420 个类别的单个单词分类问题，因此被定义为优化对数损失(交叉熵)，并使用 ADAM 优化算法来提高速度。</li><li id="d620" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">没有测试数据集。我们正在对整个训练数据进行建模，以学习序列中每个单词的概率。</li><li id="6c37" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">根据<a class="ae jd" href="https://keras.io/examples/lstm_text_generation/" rel="noopener ugc nofollow" target="_blank"> Keras 文档</a>，在生成的文本开始听起来连贯之前，至少需要 20 个历元。所以，我们要训练 100 个纪元。</li></ul><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mp mq l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">text_generator.py</figcaption></figure><h1 id="8d8a" class="lt lu jg bd lv lw lx ly lz ma mb mc md km me kn mf kp mg kq mh ks mi kt mj mk bi translated">使用训练好的 LSTM 网络生成文本</h1><ul class=""><li id="2230" class="nd ne jg kx b ky oe lb of le op li oq lm or lq ni nj nk nl bi translated">关于这一点，我们可以编写一个函数，将种子文本作为输入，并预测下一个单词。</li><li id="74c5" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">我们对种子文本进行标记，填充序列，并将其传递给训练好的模型。</li></ul><figure class="ml mm mn mo gt is"><div class="bz fp l di"><div class="mp mq l"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">generate_text.py</figcaption></figure><p id="d6b7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">试试吧！</p><ul class=""><li id="107b" class="nd ne jg kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">我随机选择“<strong class="kx jh"><em class="on"/></strong>”作为种子文本，我希望 model 返回我接下来的 100 个单词。</li></ul><pre class="ml mm mn mo gt ms mt mu mv aw mw bi"><span id="8175" class="mx lu jg mt b gy my mz l na nb">print(generate_text("hilton seattle downtown", 100, model, max_sequence_len))</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ot"><img src="../Images/6486707f32c9fd551ccf89a47d816a75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KxMA5DU2fOvI5KdMT7FD_Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 3</figcaption></figure><ul class=""><li id="53fb" class="nd ne jg kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">我选择“<strong class="kx jh"> <em class="on">贝斯特韦斯特西雅图机场酒店</em> </strong> l”作为种子文本，并且我希望模型预测接下来的 200 个单词。</li></ul><pre class="ml mm mn mo gt ms mt mu mv aw mw bi"><span id="8d9b" class="mx lu jg mt b gy my mz l na nb">print(generate_text("best western seattle airport hotel", 200, model, max_sequence_len))</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ou"><img src="../Images/a58b301ab1b3c745826390414f1c762e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y4PtzTJLym_8nx8beeijig.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 4</figcaption></figure><ul class=""><li id="948d" class="nd ne jg kx b ky kz lb lc le nf li ng lm nh lq ni nj nk nl bi translated">我选择“<strong class="kx jh"> <em class="on">位于西雅图市中心</em> </strong>”作为种子文本，并且我希望模型预测接下来的 300 个单词。</li></ul><pre class="ml mm mn mo gt ms mt mu mv aw mw bi"><span id="1c49" class="mx lu jg mt b gy my mz l na nb">print(generate_text('located in the heart of downtown seattle', 300, model, max_sequence_len))</span></pre><figure class="ml mm mn mo gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ov"><img src="../Images/39b77437d0652ee2901640f60652d834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*csYwJtVuooBnBfbEXXjmyg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure 5</figcaption></figure><h1 id="a81c" class="lt lu jg bd lv lw lx ly lz ma mb mc md km me kn mf kp mg kq mh ks mi kt mj mk bi translated">结论</h1><ul class=""><li id="2101" class="nd ne jg kx b ky oe lb of le op li oq lm or lq ni nj nk nl bi translated">没有拼写错误。</li><li id="be8c" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">这些句子看起来很真实。</li><li id="4f60" class="nd ne jg kx b ky nm lb nn le no li np lm nq lq ni nj nk nl bi translated">一些短语被一遍又一遍地重复，特别是预测给定种子的大量单词输出。</li></ul><p id="da61" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">关于改进的一些想法:更多的训练数据、更多的训练时期、更多的层、更多的层存储单元、对于给定的种子，预测更少数量的单词作为输出。</p><p id="affb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Hotel%20Description%20Generation%20LSTM.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae jd" href="https://github.com/susanli2016/NLP-with-Python/blob/master/Hotel%20Description%20Generation%20LSTM.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受余下的周末吧！</p><p id="b467" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">参考资料:</p><div class="ip iq gp gr ir ow"><a href="https://keras.io/preprocessing/text/" rel="noopener  ugc nofollow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jh gy z fp pb fr fs pc fu fw jf bi translated">文本预处理- Keras 文档</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">这个类允许向量化文本语料库，通过将每个文本转换成整数序列(每个整数…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">keras.io</p></div></div><div class="pf l"><div class="pg l ph pi pj pf pk ix ow"/></div></div></a></div><div class="ip iq gp gr ir ow"><a href="https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275" rel="noopener follow" target="_blank"><div class="ox ab fo"><div class="oy ab oz cl cj pa"><h2 class="bd jh gy z fp pb fr fs pc fu fw jf bi translated">使用 LSTMs 的语言建模和文本生成 NLP 的深度学习</h2><div class="pd l"><h3 class="bd b gy z fp pb fr fs pc fu fw dk translated">随着深度学习和人工智能领域的最新发展和改进，许多严格的…</h3></div><div class="pe l"><p class="bd b dl z fp pb fr fs pc fu fw dk translated">medium.com</p></div></div><div class="pf l"><div class="pl l ph pi pj pf pk ix ow"/></div></div></a></div></div></div>    
</body>
</html>