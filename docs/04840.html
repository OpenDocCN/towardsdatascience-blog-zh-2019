<html>
<head>
<title>Build up a Neural Network with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 构建神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/build-up-a-neural-network-with-python-7faea4561b31?source=collection_archive---------15-----------------------#2019-07-22">https://towardsdatascience.com/build-up-a-neural-network-with-python-7faea4561b31?source=collection_archive---------15-----------------------#2019-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4ff4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习和深度学习之旅</h2><div class=""/><div class=""><h2 id="cc51" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用 NumPy 实现正向传播、反向传播</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8877d9a0ec989904c6a990a19ca8e595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fA77_mLNiJTSgZFhYnU0Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 1: Neural Network</figcaption></figure><p id="95e7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个博客的目的是使用 python 中的包 NumPy 来建立一个神经网络。虽然像 Keras 和 Tensorflow 这样的成熟包使建立模型变得容易，但是值得自己编写向前传播、向后传播和梯度下降的代码，这有助于更好地理解该算法。</p><h1 id="67cf" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">概述</strong></h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mv"><img src="../Images/d56fa0644dfd46f433bcee35c17fbcb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmIryR5PGYoJP8cnkwZQZQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 2: Overview of forward propagation and backward propagation</figcaption></figure><p id="9d6d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上图显示了训练神经网络模型时，信息是如何流动的。输入<em class="mw"> Xn </em>后，权重<em class="mw"> W1 </em>和偏差<em class="mw"> B1 </em>的线性组合被应用于<em class="mw"> Xn </em>。接下来，应用激活函数进行非线性变换以得到<em class="mw"> A1 </em>。然后进入<em class="mw"> A1 </em>作为下一个隐藏层的输入。相同的逻辑应用于生成<em class="mw"> A2 </em>和<em class="mw"> A3 </em>。产生<em class="mw"> A1 </em>、<em class="mw"> A2 </em>和<em class="mw"> A3 </em>的过程称为正向传播。<em class="mw"> A3 </em>也作为神经网络的输出，与自变量<em class="mw"> y </em>比较计算成本。然后计算成本函数的导数，得到<em class="mw"> dA3 </em>。对<em class="mw"> W3 </em>和<em class="mw"> B3 </em>求<em class="mw"> dA3 </em>的偏导数，得到<em class="mw"> dW3 </em>和<em class="mw"> dB3 </em>。同样的逻辑适用于得到<em class="mw"> dA2 </em>、<em class="mw"> dW2 </em>、<em class="mw"> dB2 </em>、<em class="mw"> dA1 </em>、<em class="mw"> dW1 </em>和<em class="mw"> dB1 </em>。生成导数列表的过程称为反向传播。最后应用梯度下降并更新参数。然后，新一轮迭代从更新的参数开始。该算法不会停止，直到它收敛。</p><h1 id="ac78" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">创建测试数据</strong></h1><p id="5e4a" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">创建一小组测试数据来验证所创建的功能。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><h1 id="2d24" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">初始化参数</strong></h1><p id="a061" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">在参数初始化阶段，权重被初始化为接近零的随机值。如果权重接近于零，则 sigmoid 的操作部分大致是线性的，因此神经网络崩溃为近似线性的模型[1]sigmoid 函数在零点附近的梯度较陡，利用梯度下降可以快速更新参数。不要使用零和大的权重，这会导致糟糕的解决方案。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="cdbd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我在 Excel 中手动计算了一次神经网络的迭代训练，这有助于您验证每一步创建的函数的准确性。下面是参数初始化的输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/58aa3d05648e8ce5d7d73206a2ddcc74.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*4-sdBMfBFnSYuHmrFO4iNg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 1: Parameters Initialization Testing Result</figcaption></figure><h1 id="f270" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">正向传播</strong></h1><p id="ec6f" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">在神经网络中，输入<em class="mw"> Xn </em>被输入，信息通过整个网络向前流动。输入<em class="mw"> Xn </em>提供初始信息，该信息向上传播到每层的隐藏单元，并最终产生预测。这个过程称为正向传播。正向传播包括两个步骤。第一步是权重和来自最后一层的输出(或输入<em class="mw"> Xn </em>)的线性组合，以生成<em class="mw"> Z </em>。第二步是应用激活函数进行非线性变换。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nf"><img src="../Images/ccc69c30b18fa2b02c9734c9b717741f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZDH95sJdBRgcFvwsqYqdmQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 2: Matrix Calculation in forward propagation</figcaption></figure><p id="3202" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第一步，你需要注意投入和产出的维度。假设您有一个维度为[2，3]的输入矩阵<em class="mw"> X </em>，矩阵中的一列代表一条记录。隐含层有 5 个隐含单元，所以权重矩阵<em class="mw"> W </em>的维数为[5，2]。偏置<em class="mw"> B </em>的尺寸为【5，1】。通过应用矩阵乘法，我们可以得到维数为[5，3]的输出矩阵 Z。计算的细节可以在上表中看到。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ng"><img src="../Images/1db8879d07762f2502d2e8f63bd6e0ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UHWZaqcSNQ6ZTW4XflAvkw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 3: How activation is applied in forward propagation</figcaption></figure><p id="b25e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上表显示了激活函数是如何应用于<em class="mw"> Z 的每个分量的。</em>使用激活函数的原因是为了进行非线性变换。没有激活函数，无论模型有多少隐层，它仍然是一个线性模型。有几种流行且常用的激活函数，包括 ReLU、Leaky ReLU、sigmoid 和 tanh 函数。这些激活函数的公式和图形如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/a5f09d78b73440dba1cc57c1975758ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/1*Lw1fajSzTUlsLM7nlU169Q.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/b8f79e703fccce49e6f804dcd69eb6e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/1*uxHNlZm6fbU3qvyYEaeCWQ.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/b40a17b039ce2d4feb92521180086115.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/1*No8qht7RTunQk4U97LtO8Q.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/39a7d734091f073e47142722e3902467.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/1*4RbyU0FQxXnoLFGdSzyp6Q.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nl"><img src="../Images/68ca094e6141dc256dabd81afe3452ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qk7jH7PFbvQDT7E6bYMS0Q.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 3: Activation Function</figcaption></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="481d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，你需要定义 sigmoid 和 ReLU 函数。然后为单层正向传播创建函数。最后，上一步中创建的函数被嵌套到名为完全正向传播的函数中。为简单起见，ReLU 函数用于前 N-1 个隐藏层，sigmoid 函数用于最后一个隐藏层(或输出层)。注意，在二分类问题的情况下，使用 sigmoid 函数；在多类分类问题的情况下，使用 softmax 函数。将每个隐藏层中计算的<em class="mw"> Z </em>和<em class="mw"> A </em>保存到缓存中，用于反向传播。</p><p id="de42" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是测试数据的函数输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/cf8639cb4c286a09cbca1122a399c71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kS4NtbSFvup11dpAT5cVrQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 4: Forward Propagation Testing Result</figcaption></figure><h1 id="c4fe" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">价值函数</h1><p id="7c85" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">正向传播的输出是二元事件的概率。然后将概率与响应变量进行比较，计算成本。在分类问题中使用交叉熵作为代价函数。在回归问题中，均方误差被用作成本函数。交叉熵的公式如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9c12235cc805f5e9d0c7f8075b1e7d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/1*jMv9QelusI27mGR6f_AQhg.gif"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0277" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是测试数据的函数输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a0245ac0cbb809cda2176562df82f7bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*fRGwzT1_76FmKvpupEw8qg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 5: Cost Function Testing Result</figcaption></figure><h1 id="5d02" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">反向传播</h1><p id="747e" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">在训练期间，前向传播可以继续向前，直到它产生成本。反向传播是计算成本函数的导数，并使用微积分中的链式法则将所有信息返回到每一层。</p><p id="8845" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/4ec3ef09bf9a0fd6fb5bcccde06735c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/1*tEWbdyowAtCFnOKVgm8_2Q.gif"/></div></figure><p id="c04c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">和</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/fb2b369c5ec356402301f0b63c6dd82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/1*StD-3cW-WQIJk-Itm5nZmg.gif"/></div></figure><p id="62b4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5724a7d8a830ae772ec6fea6742bb6c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/1*EKRn0ZuJI0QU8d9RrGGvww.gif"/></div></figure><p id="9277" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">链式法则指出</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/deded2b0eeb19e9a09f81180125ee572.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/1*bQQ-uBmJo179sVVxR_DCMQ.gif"/></div></figure><p id="6642" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">激活函数的导数如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/084863a95e19e9fd719fb1d2a3f25fda.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/1*mL0nReXhgWJmM2rOWmNNOA.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6a5975c69ca5b31e7e9f55db66822f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/1*x9mSvzwn78v6U3NoZZYDGQ.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e0964fa7869e140e6125d72cf05bd3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/1*axUzhQ9AMOdzLCaJcg8XVw.gif"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/8bf0ea0bc9289d67dcd86c9550fc83e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/1*jwZ-xwqZRx4bJd_D7HAUYA.gif"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="54f3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">类似于正向传播。首先，你需要为 sigmoid 和 ReLU 的导数创建一个函数。然后定义一个单层反向传播的函数，计算<em class="mw"> dW </em>、<em class="mw"> dB </em>、<em class="mw"> dA_prev </em>。<em class="mw"> dA_prev </em>将被用作前一隐藏层反向传播的输入。最后，上一步中创建的函数被嵌套到名为完全反向传播的函数中。为了与正向传播对齐，前 N-1 个隐藏层使用 ReLU 函数，最后一个隐藏层或输出层使用 sigmoid 函数。你可以修改代码并添加更多的激活功能。将<em class="mw"> dW </em>和<em class="mw"> dB </em>保存到另一个缓存中，用于更新参数。</p><p id="b490" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是测试数据的函数输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nw"><img src="../Images/77b681ea35bc1a81707f83a6392d295c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vLrEWHtPIQOmP3hbE3UWdA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 6: Backward Propagation Testing Result</figcaption></figure><h1 id="3714" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">更新参数</h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="0d96" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦从反向传播计算出梯度，就通过学习速率*梯度来更新当前参数。然后，更新的参数被用于新一轮的正向传播。</p><p id="eda2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是测试数据的函数输出。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/7c469c54cfec5d2782c47a422806d1c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*2S8iPssDo5emRH_uoeSmCw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 7: Parameter Update Testing Result</figcaption></figure><p id="c2a7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">梯度下降的解释可以在我的博客里看到。</p><div class="ny nz gp gr oa ob"><a rel="noopener follow" target="_blank" href="/an-introduction-to-gradient-descent-c9cca5739307"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd jd gy z fp og fr fs oh fu fw jc bi translated">梯度下降导论</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">本博客将涵盖以下问题和主题:</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">towardsdatascience.com</p></div></div><div class="ok l"><div class="ol l om on oo ok op lb ob"/></div></div></a></div><h1 id="7055" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">将功能堆叠在一起</strong></h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7726" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了训练神经网络模型，将前面步骤中创建的函数堆叠在一起。下表提供了功能摘要。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/77325b4e72418aecb2048038706e2e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IjRBhM-JPa_wPpnqianYOA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Table 8: Functions Summary</figcaption></figure><h1 id="050d" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">运行模式</strong></h1><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="bf55" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先使用 make_moons 函数创建两个交错的半圆数据。下面提供了数据的可视化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/b0421a743beabb12ed9094bb4e063e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FqeCkhYcceZ5uQ_gVRzobw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 4: Training Data</figcaption></figure><p id="a779" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后运行该函数来训练神经网络模型。下图显示了培训过程。成本在 8000 个时期后收敛，并且模型准确率收敛到 0.9。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/c132ad6c026991ef193195f97d36f25f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L97YhnOm1eYKEsL5F2NinA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 5: Cost over Time</figcaption></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/00459c6d19e835d1ed5b0068ba585947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQFVARzEnyoW4Mc63ELtzQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Figure 6: Accuracy over Time</figcaption></figure><h1 id="76fc" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">下一步</h1><p id="d678" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">图 5 和图 6 表明存在潜在的过拟合问题。您可以使用包括提前停止、退出和调整在内的方法来解决这个问题。除了 ReLU 和 sigmoid 函数之外，还可以通过添加其他激活函数来玩 model。本博客使用的是批量梯度下降，但也有很多改进的梯度下降算法如 Momentum、RMSprop、Adam 等。</p><h1 id="0e8f" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated">摘要</h1><p id="9903" class="pw-post-body-paragraph lh li it lj b lk mx kd lm ln my kg lp lq mz ls lt lu na lw lx ly nb ma mb mc im bi translated">虽然之前上过网上课程，读过书中的相关章节，但直到我亲自动手写博客，我才完全理解了这种奇特的方法。俗话说，教是最好的学习方法。希望你能从阅读这篇博客中受益。如果你有兴趣，请阅读我的其他博客。</p><div class="ny nz gp gr oa ob"><a href="https://medium.com/@songyangdetang_41589/table-of-contents-689c8af0c731" rel="noopener follow" target="_blank"><div class="oc ab fo"><div class="od ab oe cl cj of"><h2 class="bd jd gy z fp og fr fs oh fu fw jc bi translated">目录</h2><div class="oi l"><h3 class="bd b gy z fp og fr fs oh fu fw dk translated">这一系列博客将从理论和实现两个方面对深度学习进行介绍。</h3></div><div class="oj l"><p class="bd b dl z fp og fr fs oh fu fw dk translated">medium.com</p></div></div><div class="ok l"><div class="ou l om on oo ok op lb ob"/></div></div></a></div><p id="4134" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">参考</strong></p><p id="a836" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[1]特雷弗·哈斯蒂，罗伯特·蒂布拉尼，杰罗姆·弗里德曼，(2008)，<em class="mw">统计学习的要素</em></p><p id="7540" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]伊恩·古德费勒，约舒阿·本吉奥，亚伦·库维尔，(2017) <em class="mw">深度学习</em></p><p id="5e1d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3]<a class="ae ov" href="https://www.coursera.org/specializations/deep-learning?" rel="noopener ugc nofollow" target="_blank">https://www.coursera.org/specializations/deep-learning?</a></p><p id="7514" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4]https://en.wikipedia.org/wiki/Activation_function<a class="ae ov" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank"/></p><p id="2a44" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]https://explained.ai/matrix-calculus/index.html<a class="ae ov" href="https://explained.ai/matrix-calculus/index.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>