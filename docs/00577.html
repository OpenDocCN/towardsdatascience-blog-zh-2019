<html>
<head>
<title>Understanding Markov Decision Processes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解马尔可夫决策过程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-markov-decision-processes-b5862c192ddb?source=collection_archive---------11-----------------------#2019-01-26">https://towardsdatascience.com/understanding-markov-decision-processes-b5862c192ddb?source=collection_archive---------11-----------------------#2019-01-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="312d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从高层次的直觉来看，马尔可夫决策过程(MDP)是一种数学模型，对于机器学习，特别是强化学习非常有用。该模型允许机器和代理确定特定环境内的理想行为，以便最大化模型在环境中实现某个状态甚至多个状态的能力，这取决于您想要实现什么。这个目标是由我们称之为策略的东西决定的，这个策略根据环境应用于代理的行为，MDP 试图优化实现这样一个解决方案所采取的步骤。这种优化是通过奖励反馈系统来完成的，其中不同的行为根据这些行为将导致的预测状态来加权。</p><h1 id="66f1" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">分解它</h1><p id="f8d7" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">为了理解 MDP，我们应该首先看看这个过程的独特组成部分。它包含:</p><ul class=""><li id="699c" class="lo lp iq jp b jq jr ju jv jy lq kc lr kg ls kk lt lu lv lw bi translated">存在于我们指定环境中的一组状态:S</li><li id="b13f" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">在我们指定的环境中，一个或多个代理可以执行的一组可能的操作</li><li id="897c" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">每个动作对当前状态的影响的描述</li><li id="afdb" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">给定期望的状态和动作，给出回报的函数:R(s，a)。</li><li id="41c0" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk lt lu lv lw bi translated">寻求解决 MDP 问题的政策。你可以把它看作是从状态到我们行为的映射。用更简单的术语来说，它表示在状态 s 时要采取的最佳行动 a。</li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/2663f9f83b42f5ff3aca26fd46748d4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tGwmW_pXT-Inqarion6lWg.jpeg"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Image overview of MDP</figcaption></figure><h1 id="eeee" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">了解模型:</h1><p id="f6b1" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">从上图可以看出，我们有 T(S，a，S') ~ P(S'|S，a)。</p><p id="b43c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型通常被称为过渡模型。如果你把事情分解开来，理解起来就变得简单了。t 代表了我们行为的概念化。我们从某个状态 S 开始，我们采取行动 a，我们的代理发现自己处于一个新的状态 S’。(这里不要让我的措辞把你搞糊涂了。我们采取的行动完全有可能会让代理处于同样的状态。)</p><p id="e650" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们定义 P，或者我们在先前状态下采取行动达到新状态的概率。</p><h1 id="8950" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">让我们看看数学:马尔可夫性质</h1><p id="453a" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">p[<strong class="jp ir">S</strong><em class="ms">t+1</em><strong class="jp ir">T5】|</strong>a，<strong class="jp ir"> S0 </strong>，…..、<strong class="jp ir"><em class="ms">S</em></strong><em class="ms">t</em><em class="ms">=</em>P[<strong class="jp ir">S</strong><em class="ms">t+1</em><strong class="jp ir"><em class="ms">| a、</em> S </strong> <em class="ms"> t </em></p><p id="4a35" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的公式是马尔可夫属性状态的定义。还是那句话，我们来分解一下。</p><p id="c4e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">St+1 可以解释为未来的状态，或者我们打算走向的状态和[ <strong class="jp ir"> S </strong> <em class="ms"> 1 </em>，…..，<strong class="jp ir"><em class="ms">S</em></strong><em class="ms">t</em>]表示状态历史中存在的所有相关信息的历史。当然，a 仍然代表正在采取的行动。但是，新状态仅依赖于以前的状态。它不依赖于各国过去的历史。</p><h1 id="40f3" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">给定马尔可夫状态，定义转移概率。</h1><p id="0d2b" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">假设我们的代理处于某个状态 s，有一个概率进入第一个状态，另一个概率进入第二个状态，以此类推，对于每个现有的状态。这是我们的转移概率。</p><p id="2163" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以把这些概率输入一个状态矩阵！让我们定义一个三态天气问题的例子。这是规则</p><ol class=""><li id="b6be" class="lo lp iq jp b jq jr ju jv jy lq kc lr kg ls kk mt lu lv lw bi translated">你住在桑尼镇，但可悲的是，桑尼从来没有连续两天的好天气。永远不会。</li><li id="cb54" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk mt lu lv lw bi translated">如果你有一个好天气，第二天很可能会下雪或下雨。</li><li id="77af" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk mt lu lv lw bi translated">如果下雪或下雨，第二天有 50%的可能会有同样的天气。</li><li id="d398" class="lo lp iq jp b jq lx ju ly jy lz kc ma kg mb kk mt lu lv lw bi translated">如果天气由下雪或下雨转变，也只会有一半时间转变为晴天。</li></ol><p id="a7b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用这个假的环境信息，我们可以构建一个状态矩阵，如下所示:</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/ce39ac791549001ff53f6ee6f4fdc2ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*eWwv420A0J7Wspmq_Q118Q.png"/></div></figure><p id="54db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中矩阵的第一行表示给定雨天的下几天天气的概率。第二行代表正常天气的概率，第三行，正如你可能已经理解的，代表下雪天的概率。这个转移概率矩阵被称为转移矩阵。</p><p id="212a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，让我们从我们的矩阵中尝试一个真实问题的模型，其中 p (ij)。<br/>设 I 代表当前状态，j 代表未来状态。</p><p id="42ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们想知道今天下雨的可能性有多大，两天后下雪的可能性有多大？假设正在下雨，我们可能的状态是什么？</p><p id="c493" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">唯一的限制是不能连续两天天气都很好。从现在到那时的所有其他状态都是可能的。所以可能明天下雨，第二天下雪，可能明天天气好，第二天下雪，也可能明天下雪，第二天下雪。</p><p id="0a9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">用方程式表示:</p><p id="d6c7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">P (Rain⁰Snow ) = P(下雨)*P(下雨+下雪)+ P(正常下雨)*P(正常下雪)+ P(下雨+下雪)*P(下雪)</p><p id="e783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我意识到这看起来很伤人，但是你可能已经意识到这本质上是向量和矩阵数学。我们取第 1 行和第 3 行的点积。</p><h1 id="bd2d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">视觉化让生活更简单</h1><p id="70d8" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">假设我们希望预测给定 6 天时间周期或 6 个状态转换的 P。我们没有一开始就定义一个单一的状态，我们只是希望在给定初始概率的情况下，找到我们的状态在一个过渡时期的概率。这就是所谓的正规马尔可夫链。如你所见，我们继续使用矢量数学，通过点积来计算每个州的概率。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4a87e649f9208d028481346acc2b72b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*386_HcKCBWEHEGsScOyqdQ.png"/></div></figure><h1 id="b7b8" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">好的，但是我们如何确定初始状态呢？它将如何影响我们对概率的计算，或者我如何创建马尔可夫链:</h1><p id="b5db" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我很高兴你问了！我们可以这样定义:u^(n) = uP^(n)</p><p id="9ea6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中 u 是表示初始状态分布的向量，P 是马尔可夫链的转移矩阵。</p><p id="15a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们知道我们有三个状态，所以让我们把它代入。u =向上</p><p id="d113" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们求解第三天的概率。我们制作一个 1×3 的向量来表示初始分布，并取点积来找出给定一个随机起始状态的第三天每个状态的可能性。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/3e715873736aa837efaa0002edd9afe9.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*R2KzzUdMj9HnGHK_VQOZ3w.png"/></div></figure><h1 id="1033" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">太好了。对于一个期望的结果，我如何找到这么多状态的最优解？这就是我想在这里使用强化学习的原因。</h1><p id="00c6" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">为了理解如何计算我们的状态和动作的最优化，我们需要给它们赋值。为了理解价值，我们需要定义一个政策，为了定义一个政策，我们需要理解回报和回报。</p><h1 id="441b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">奖励和回报值</h1><p id="76ce" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">强化代理寻求最大化未来报酬的总和。他们想预测能获得最大回报的行动。这被称为<strong class="jp ir"> <em class="ms">返回</em> </strong>，我们可以这样建模，R 代表奖励，R 代表返回，下标 t 代表时间步长，在我们的例子中是状态转换。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/3ba7421550a8303bb78795f3f2e5c626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*bw2NjrF4Ssh9Xt2tZgBzvA.png"/></div></figure><p id="96ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，正如你所看到的，这个等式允许你趋向于无穷大，但是对于处理很多问题来说，它没有意义。我们希望奖励的总和结束。我们把结束的任务称为情节性的。想一个大富翁棋盘游戏。这一集是垄断游戏，从给所有人分配相同的值开始，给定一系列可能的状态和行为，这一集最终以赢家结束。新的一集可以通过创建游戏的新实例开始。我承认垄断游戏有时会让人觉得它们会变得无穷无尽…</p><p id="45b5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">比较常见的处理返回值的方法是一种叫做<em class="ms">未来累计</em> <strong class="jp ir"> <em class="ms">贴现</em> </strong> <em class="ms">奖励</em>的方法</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi my"><img src="../Images/7ad25dd372222e32b3c07a6e828b0769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*30HnuSAocHsdcow5xcW4mQ.png"/></div></figure><p id="cbde" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中奖励前面的折扣表示 0 到 1 之间的值。这种方法的好处是，回报现在可以更好地模拟无限奖励，并且它给予更直接的奖励更大的权重。该模型现在关心的是更快到来的回报，而不是未来的回报。这个我们可以自己掂量。我们选择的折扣越小，我们就越重视早期奖励。正如您可能想象的那样，折扣 1 成为我们最初的奖励等式，折扣 0 创建一个只关心第一个奖励的模型。这在某种意义上是有用的，因为代理人会知道在那个确切的时刻做什么是绝对最好的，但它对自己的未来缺乏任何洞察力。有点像婴儿对成人。嗯，有些成年人。婴儿只知道在那一刻他需要一件特定的东西，一个成年人可以计划并试图预测他的未来。</p><h1 id="3736" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">更多关于政策</h1><p id="4e13" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">π(s，a)是我们的策略函数。它描述了一种行为方式。它获取当前状态和代理动作，并返回在该状态下采取该动作的概率。有点像我们上面演示的，不是吗？</p><p id="81f4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你想一想这意味着什么，所有状态的集合，给定所有动作，等于概率 1，这一定是真的。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5f0efc01ea80ba72e421d921c3fd4e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*OV6XCrSNrtNaP8hQ6qbjEQ.png"/></div></figure><p id="e3e1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的政策应该描述在每个州如何行动。</p><p id="f87f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">想想乔希·格里夫斯创造的这个政策</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi na"><img src="../Images/23359f570acf3692e34cc6fcc8c094d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p4JW6ibYcdmaeXVB_klivA.png"/></div></div></figure><p id="5a4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">大家可以看到，我们吃饱了有奖励，饿了吃了有奖励，吃饱了不吃也有奖励。如果饥饿，我们会受到极其严重的惩罚；如果吃饱了，我们会受到惩罚；如果吃饱了，我们会受到惩罚；如果饿了，我们也会受到惩罚。很容易看出，在这个非常简单的例子中，最优策略是总是在饥饿时进食。</p><h1 id="ad16" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">价值函数:</h1><p id="6b1b" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">强化学习中的两类价值函数是状态价值函数 V(s)和动作价值函数 Q(s，a)。</p><p id="1d2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">状态值函数解释给定特定策略的状态值。当我们从初始状态 s 开始并在我们的政策规则范围内行动时，它是对回报的计算。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/bed94799e3e2da1dadc94c16f7a0e73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*-t69s96XMGzV47-ANqoq_g.png"/></div></figure><p id="4902" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当遵循我们指定的策略时，action value 函数返回在给定状态下采取某个操作的值。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/65a24848ae4dc76d9b14e3da45d2aced.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*_k7Y5C1F8zdJF0ftf6NZ_Q.png"/></div></figure><p id="6ad4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，假设当选择一个动作时，环境是返回下一个状态的东西，我们必须记住，如果我们的策略改变，那么价值函数也会改变。我们期望看到这些函数的一个给定的返回值，然而，在到达一个特定的状态时会有很大的随机性，转移函数也会影响我们的状态。我们可能没有 100%的概率！</p><h1 id="acb1" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">给定环境，我们如何解释返回状态的随机性？</h1><p id="580c" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">好吧，我们将在下一篇博文中讨论这个话题，但简而言之，我们可以使用贝尔曼方程。这些方程允许我们将状态的值表示为其他状态的值。意思是如果我们知道某个状态的值，我们可以用它来计算其他状态的值。</p><p id="8e81" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以用这些方程，推导出状态值和动作值的贝尔曼方程，我保证我们下次会这么做，等我再研究一些:D 之后</p><p id="3848" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">直到那时！</p><h1 id="6e87" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">来源:</h1><p id="28fd" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">达特茅斯概率正文:<a class="ae nd" href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/book.html" rel="noopener ugc nofollow" target="_blank">http://www . Dartmouth . edu/~ chance/teaching _ AIDS/books _ articles/probability _ book/book . html</a></p><p id="d499" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">麻省理工学院发表论文:【http://www.mit.edu/~jnt/Papers/J083-01-mar-MDP.pdf T2】</p></div></div>    
</body>
</html>