<html>
<head>
<title>Parameters in Machine Learning algorithms.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习算法中的参数。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/parameters-in-machine-learning-algorithms-ba3e3f0e49a?source=collection_archive---------10-----------------------#2019-06-02">https://towardsdatascience.com/parameters-in-machine-learning-algorithms-ba3e3f0e49a?source=collection_archive---------10-----------------------#2019-06-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4f37e080fa8aa7aec5f6cde04c38ae67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGwc4r-dWMJZ-kS0fTV1pg.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Pic Credit: <a class="ae jg" href="https://mljar.com/blog" rel="noopener ugc nofollow" target="_blank">https://mljar.com/blog</a></figcaption></figure><div class=""/><div class=""><h2 id="1217" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">理解 ML 算法的初学者指南。</h2></div><p id="7023" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我与海得拉巴<a class="ae jg" href="https://www.linkedin.com/school/indian-school-of-business/" rel="noopener ugc nofollow" target="_blank">ISB</a>的交往中，我有幸成为<a class="ae jg" href="https://www.linkedin.com/in/shaileshk/" rel="noopener ugc nofollow" target="_blank">沙伊莱什·库马尔</a>的学生。Shailesh 教授对于如何定义一名成功的数据科学家有着独特的观点:</p><ul class=""><li id="96cf" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">数据科学家能够写出针对给定问题必须优化的目标函数。</li><li id="214f" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">数据科学家能够理解在求解目标函数时需要学习的自由参数的数量。</li><li id="c60f" class="lu lv jj la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">数据科学家能够理解控制模型复杂性的旋钮(或超参数)。</li></ul><p id="7d7d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我写这篇文章是为了那些想了解参数在 ML 算法中的作用的人。需要求解的参数数量将直接影响训练过程的时间和输出。下面的信息对那些理解 ML 中各种算法的人是有用的。</p><ul class=""><li id="d457" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><em class="mi">降维方法</em></li></ul><p id="9581" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">主成分分析:</strong></p><p id="72a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">PCA 用于输入数据的降维，方便快捷。PCA 算法的输出是在向量空间中正交的数据集。PCA 的目标函数可以写成<em class="mi"> argmax{W'CW} </em>其中 C 是输入数据的协方差矩阵，它是对称的、半正定的。为<em class="mi"> W </em>求解上述函数将导致<em class="mi"> W </em>是矩阵<em class="mi"> C </em>的特征向量。设数据是一个<em class="mi"> d 维</em>矩阵。<em class="mi"> C </em>将会是<em class="mi">d * d .</em>PCA 中的参数数量由总共处于最大值<em class="mi">‘d’</em>的特征向量的数量给出。每个特征向量的维数为<em class="mi">‘1xd’，</em>，因此需要估计的总参数为<em class="mi">d * dx1 = d。</em>许多软件包也给出了特征值，即每个主成分解释的方差。由于所有特征值的总和必须等于数据中的总方差，因此存在用于估计特征值的<em class="mi"> d-1 </em>自由参数。旋钮是我们在不损失太多方差的情况下需要考虑的主成分数<em class="mi"> (k) </em>。例如:MNIST 数据集中每个字符的数据被安排在一个 28×28 的图像中，该图像构成一个长度为 784 的向量。这一幅图像的协方差矩阵大小为 784x784，因此参数总数为 784*784+783。</p><p id="0a51" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">多维标度(MDS): </strong></p><p id="82f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MDS 的目标是将高维数据投射到低维表面。对于每对观察值，相似性距离<em class="mi">δ</em>作为算法的输入给出。结果将是一个<em class="mi"> x </em>维空间中每个数据点的坐标向量。目标函数是最小化在<em class="mi"> x 维</em>空间中的投影距离<em class="mi"> delta_x </em>与数据中每对点之间的实际距离<em class="mi"> delta </em>的误差。.即<em class="mi"> argmin {(delta_x — delta) }。的编号。要估计的参数是的数量。数据点* <em class="mi"> x </em>(您想要投影的尺寸)。举个例子:如果你想把 5 种不同的菜系投射到一个二维空间中。参数= 5*2 = 10。旋钮就是<em class="mi"> x. </em>的大小</em></p><ul class=""><li id="0f62" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><em class="mi">无监督学习方法</em></li></ul><p id="2fd7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> K 均值聚类:</strong></p><p id="45f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">问题是为给定的输入数据集找到 K 个 T21 代表。这些代表被称为聚类中心(或)质心，并且被选择为使得在同一聚类中从每个点到其质心的距离最小。目标函数是<em class="mi"> argmin I(k)*{(x-m(k)) } </em>其中<em class="mi"> I(k) </em>是一个点属于聚类的指示函数<em class="mi"> k. </em>模型参数除了聚类质心向量之外什么都不是。如果输入数据集为<em class="mi"> d </em>维，则参数总数为<em class="mi"> k*d. </em>旋钮为<em class="mi"> k </em>的值，该值必须作为超级参数传递给算法。</p><p id="659c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> Parzen 窗口:</strong></p><p id="b080" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Parzen 窗口是一种估计单个随机变量(单变量数据)密度的技术。数据的密度只不过是给定数据的真实概率密度函数(pdf)的近似值。然后汇总每个点的 Parzen 窗口估计值，以获得数据的密度估计值。目标函数是计算<em class="mi"> p(x) = SUM(k(x))。</em>在该模型中没有要学习的自由参数，但是您为每个数据点分配一个高斯(影响区域)，该高斯被称为核函数，其均值(即以数据点为中心)和方差(<em class="mi"> sigma </em>)在定义核时已经指定。旋钮是<em class="mi"> sigma </em>的值，它是 parzen 窗口算法的超级参数。</p><p id="ba60" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">单变量正态(Uni 高斯):</strong></p><p id="4de6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">UVN 建模基于基本假设，即输入数据仅由一维组成，其<em class="mi">均值(μ)和方差(σ)</em>将采用高斯概率密度函数(pdf)进行估计。然而，与上述方法不同，模型参数实际上是通过最大化(或最小化负值)似然函数或其对数似然函数来学习的。假设输入数据是独立同分布的样本。目标函数是<em class="mi">arg min-{ prod(1/sqrt(2 * pi * sigma)* e^-(x-mu)/sigma)}。</em>自由参数为<em class="mi">μ</em>和<em class="mi">σ。</em>这款没有旋钮。</p><p id="0b0a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">多变量正态分布(MVN/高斯混合分布):</strong></p><p id="69a1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在上述模型中，用多变量数据集替换单变量数据，我们得到一个多变量正态分布数据，它具有一个完整的协方差矩阵<em class="mi"> (sigma) </em>和一个均值向量<em class="mi"> (mu)。</em>目标是最大化给定输入数据集上的似然函数，假设多变量高斯分布的 pdf 由这里的<a class="ae jg" href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution" rel="noopener ugc nofollow" target="_blank">给出。</a></p><p id="6d7c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于一个<em class="mi"> d 维的输入数据，</em>协方差矩阵<em class="mi"> (sigma) </em>在上三角形区域将有<em class="mi"> d*(d-1)/2 </em>个条目，对角线上有<em class="mi"> d </em>个条目。估计协方差矩阵将涉及学习<em class="mi"> d*(d-1)/2 + d </em>自由参数。估计平均向量<em class="mi">(μ)</em>需要学习<em class="mi"> d </em>参数。因此，自由参数的总数为<em class="mi"> d*(d-1)/2 + 2d。</em>该型号没有任何旋钮。</p><ul class=""><li id="0ed5" class="lu lv jj la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated"><em class="mi">监督学习方法</em></li></ul><p id="9269" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">感知器:</strong></p><p id="257f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个简单的感知是一个单细胞神经元，可以在一个<em class="mi"> n </em>维特征空间中分离两类。感知器是可以模拟两个类别之间的边界线(或平面)的区别分类器的一个例子。这条线的函数可以写成<em class="mi"> y = h(w'x+b)。</em>参数是神经元的权重(<em class="mi"> w 和 b </em>)，总计<em class="mi"> n+1。</em>目标是最小化预期分类误差 aka as loss，可写成<em class="mi"> -SUM(y*log(h(w'x+b))。</em>计算损失函数的梯度，并使用梯度下降更新权重。模型的旋钮是 GD 算法中使用的<em class="mi">学习率(lr) </em>。</p><p id="0598" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">逻辑回归:</strong></p><p id="c7ef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑回归的形式类似于感知器，即它可以解决两类问题。所使用的激活函数是由<em class="mi">h(w ' x+b)= 1/1+e^-(w'x+b).给出的<em class="mi">s 形</em></em>其余论点同上。</p><p id="c146" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">神经网络:</strong></p><p id="3a91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">神经网络中的每个节点都可以理解为一个单独的逻辑回归。前馈神经网络被完全连接。在具有 2 个隐藏层的神经网络中，每个隐藏层具有 5 个神经元，参数的总数将是<em class="mi">5 *(n+1)+(5 * 5)+5 *输出</em>。目标函数是使用<em class="mi">交叉熵</em>损失最小化分类误差。使用从输出到输入的每个连续层的误差梯度的反向投影来调整权重。模型的旋钮或复杂性是隐藏层的数量和每个隐藏层中的单元的数量，这是设计时与学习速率(如果使用梯度下降来解决优化问题)一起考虑的因素，学习速率是超参数。</p><p id="7243" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">朴素贝叶斯分类器:</strong></p><p id="2d21" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与上面不同，NB 是一个生成式分类器。朴素贝叶斯分类器的关键假设是特征是类条件独立的。NB 分类器对条件概率的贝叶斯公式起作用，即<em class="mi"> p(类/数据)~ p(类)* p(数据/类)。p(数据/类别)</em>根据关键假设进行估算。<em class="mi"> p(x1，x2，x3…/c)~ p(x1/c)* p(x2/c)* p(x3/c)…</em>的编号。估计<em class="mi"> p(x/c) </em>所需的参数取决于特征的类型，即分类特征或数字特征。如果特征是分类的，那么你需要为它的所有级别建立概率值<em class="mi"> (l) </em>。的编号。自由参数是(<em class="mi"> l-1) * c …(l-1，因为所有级别的概率加起来是 1)。</em>如果特征是数字，那么你需要估计基础分布的参数，例如高斯分布的均值和方差。因此参数的数量会根据输入数据集而变化。没有最小化或最大化的目标，你只需要计算条件概率，建立先验，并使用贝叶斯规则对测试数据进行分类。然而，可以使用上面给出的相同目标函数，例如铰链损失或交叉熵损失，来调整后验概率，以更接近地反映基本事实。这种算法缺乏对模型复杂性的控制。</p><p id="73c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">K-最近邻:</strong></p><p id="e309" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">KNN 是一种懒惰的算法，也就是说，当一个需要分类的新数据点被呈现给算法时，它在推理时完成大部分工作。基于给定距离的阈值内的最近数据点，新数据点将被多数类标签分类。旋钮或模型复杂度是阈值距离，它是一个超参数。没有目标函数或参数。</p><p id="17f4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">支持向量机:</strong></p><p id="ed9d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">SVM 是一种特殊类型的判别分类器，其目标是最大化给定类别对之间的决策边界。最大化函数可以使用向量代数来导出为<em class="mi"> 1/2*||w||，</em>，其中可以假设<em class="mi"> w </em>是等式<em class="mi">y *(w’x-b)-1&gt;= 0，</em>中的参数向量，该等式是可以分离给定类别对的线(或超平面)的等式。的编号。对于<em class="mi"> d- </em>维输入数据集，需要求解的参数为<em class="mi"> d+1 </em>。旋钮或复杂度由成本参数(被认为是以升为单位的<em class="mi"> gamma </em>)给出。)这将允许对可能导致复杂的过拟合决策边界(当采用非线性<em class="mi">内核</em>时)的训练数据集点 San 的错误分类的一些容忍。</p></div></div>    
</body>
</html>