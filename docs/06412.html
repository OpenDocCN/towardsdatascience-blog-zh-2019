<html>
<head>
<title>CatBoost Demystified</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CatBoost 去神秘化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/catboost-demystified-8b0b538bfa31?source=collection_archive---------17-----------------------#2019-09-14">https://towardsdatascience.com/catboost-demystified-8b0b538bfa31?source=collection_archive---------17-----------------------#2019-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/8d6e7774192145008fd8b12086cb27ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/0*irl4HR2657jhUAt4"/></div></figure><p id="c257" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">梯度推进算法已经成为几乎所有 ML 竞赛和实际项目中的面包和黄油。CatBoost 是梯度增强工具包家族的最新成员之一。它是由 Yandex 的 ML 团队开发的开源库。那么让我们来评估一下 CatBoost 与 XGBoost、LighGBM 和 H2O 相比有什么优势？</p><p id="d055" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">CatBoost 有两个非常独特的进步:有序提升的实现和处理分类特征的过程。这两种技术都有助于对抗由一种特殊的目标泄漏引起的预测偏移，这种目标泄漏存在于梯度增强算法的所有现有实现中。</p><h1 id="ec14" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">有序升压:</h1><p id="4c54" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">在每个步骤中使用的梯度是使用当前模型所基于的相同数据点的目标值来估计的。这导致特征空间的任何域中的估计梯度的分布与该域中的真实梯度分布相比发生偏移，这导致过拟合(预测偏移)。</p><p id="f8e9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">作为对<em class="ly">预测偏移</em>的解决方案，CatBoost 通过将当前模型应用于新的训练示例，在提升的每一步独立采样新的数据集，以获得未偏移的残差。对于具有 100 棵树的模型，为了使剩余的未移位，需要创建没有原始实例的(100–1 = 99)训练数据集。由于需要所有训练样本的无偏残差，因此没有样本可用于训练先前的模型。</p><p id="aaaf" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">实际上，由于数据有限，这似乎是不可能的。但是 CatBoost 维护了一组不同的模型，用于训练的例子各不相同。然后，为了计算一个示例的残差，CatBoost 使用了一个未经训练的模型。这是使用训练示例的随机排列σ来实现的。每个模型仅使用排列中的前 I 个示例来学习。在每一步，为了获得第 j 个样本的残差，CatBoost 使用 Mj 1 模型。这被称为<em class="ly">命令增压</em>。</p><h1 id="b780" class="kv kw it bd kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">分类特征:</h1><p id="7ad6" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">那么，为了从分类变量中提取最大的信息，你有多少次挣扎于处理分类变量的方法呢？当所有/大部分独立特征都是分类特征时，挑战变得更加严峻，如果它们具有高基数，挑战就更加严峻。下面是我最后处理分类变量的方法:</p><ul class=""><li id="fa1f" class="lz ma it jz b ka kb ke kf ki mb km mc kq md ku me mf mg mh bi translated">独热编码</li><li id="be0f" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">标签编码</li><li id="102f" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">目标编码</li><li id="ea3f" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">将不同级别的变量组合成一个变量</li><li id="d6d3" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">特征散列</li></ul><p id="fdb8" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">计算分类特征中每个类别的目标统计数据并使用新的数字特征似乎是处理分类特征最有效的方法，同时信息损失最小。创建此功能的一个简单方法是估计具有相同类别的训练示例的 y 的平均值。现在有一个基本问题，即计算这种类型的贪婪目标统计的<em class="ly">目标泄漏</em>。</p><p id="ea0f" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">为了绕开这个问题，CatBoost 使用了一种叫做<em class="ly">有序目标统计</em>的方法。每个示例的<em class="ly">目标统计</em>的值仅依赖于观察到的历史。为此，他们引入了人工时间，即训练样本的随机排列σ。因此，对于每个示例，所有可用的历史都用于计算其<em class="ly">目标统计数据</em>。此外，如果仅使用一个随机排列，则前面的示例将具有<em class="ly">目标统计</em>，其方差比后面的示例高得多。为了避免这种情况，CatBoost 对梯度增强的不同步骤使用不同的排列。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="8add" class="mw kw it ms b gy mx my l mz na">Few other interesting characteristics I find in CatBoost are:</span></pre><h2 id="db31" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">功能组合:</h2><p id="74ca" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">CatBoost 的另一个重要细节是使用分类特征的组合作为捕获高阶的附加分类特征。CatBoost 以贪婪的方式构造组合。也就是说，对于树的每次分裂，CatBoost 将当前树中先前分裂已经使用的所有分类特征(及其组合)与数据集中的所有分类特征相结合(连接)。组合被动态转换为<em class="ly">目标静态</em>。</p><h2 id="cd5f" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">SHAP 价值观:</h2><p id="9ef1" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">在特征重要性函数中，你可以得到 SHAP 值，只需输入 type='ShapValues '</p><h2 id="76a7" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">对象重要性:</h2><p id="0e27" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">如果您可以从训练数据中知道哪些实例对学习模型的贡献最大，会怎么样？CatBoost 提供此功能来计算训练数据集中的实例对优化度量值的影响。</p><ul class=""><li id="4379" class="lz ma it jz b ka kb ke kf ki mb km mc kq md ku me mf mg mh bi translated">正值反映优化指标增加</li><li id="4b80" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">负值反映出优化的指标降低</li><li id="a09d" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">与 0 的偏差越大，该实例对优化指标的影响就越大</li></ul><p id="88b3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">该方法是在<a class="ae nm" href="https://arxiv.org/abs/1802.06640" rel="noopener ugc nofollow" target="_blank">为梯度增强决策树论文</a>中描述的方法的实现。</p><h2 id="052e" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">计算要素统计数据:</h2><p id="4ba0" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">有了这一功能，我们可以直观地看到算法如何分割每个特征的数据，并评估目标统计数据。我们可以想象以下情况:</p><ul class=""><li id="29a2" class="lz ma it jz b ka kb ke kf ki mb km mc kq md ku me mf mg mh bi translated">每个时段(对于连续特征)或每个类别(对于分类特征)的平均实际目标值</li><li id="3e07" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">每个时段/类别的平均预测目标值</li><li id="f003" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">每个存储桶中的实例数量</li><li id="d9ad" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated">通过改变特征值来平均预测值</li></ul><p id="ed58" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在保持所有其他要素不变的情况下，所选要素的值会发生变化，以便落在每个桶/类别下。该模型预测这些新实例的目标值，并绘制给定存储桶/类别中预测值的平均值。</p><figure class="mn mo mp mq gt ju gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/80502451723518df142c344b3e7ba7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/0*LjebnD1cVMu_xVxZ.png"/></div></figure><h2 id="f634" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">绘制单棵树:</h2><p id="9272" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">您可以通过提供想要可视化的树的索引来绘制单个树。</p><h2 id="ce2c" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">模型训练的视觉化:</h2><p id="bf86" class="pw-post-body-paragraph jx jy it jz b ka lt kc kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku im bi translated">默认情况下，当模型被训练时，CatBoost 提供学习过程的可视化表示。</p><h2 id="b4ac" class="mw kw it bd kx nb nc dn lb nd ne dp lf ki nf ng lj km nh ni ln kq nj nk lr nl bi translated">参考资料:</h2><ul class=""><li id="2a6a" class="lz ma it jz b ka lt ke lu ki no km np kq nq ku me mf mg mh bi translated"><a class="ae nm" href="https://tech.yandex.com/catboost/doc/dg/concepts/about-docpage/" rel="noopener ugc nofollow" target="_blank"> Catboost 文档</a></li><li id="c188" class="lz ma it jz b ka mi ke mj ki mk km ml kq mm ku me mf mg mh bi translated"><a class="ae nm" href="https://arxiv.org/abs/1706.09516" rel="noopener ugc nofollow" target="_blank"> CatBoost 论文</a></li></ul><p id="a666" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">希望这篇博客能为您提供关于 CatBoost 的良好背景，并帮助您探索这种算法的使用。请在评论区告诉我你的想法。</p><p id="fbde" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这篇内容最初发表在我的个人博客网站:【http://datascienceninja.com/<a class="ae nm" href="http://datascienceninja.com/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">。点击</a><a class="ae nm" href="http://datascienceninja.com/2019/09/14/catboost-demystified/" rel="noopener ugc nofollow" target="_blank">此处</a>查看并订阅即时接收最新博客更新。</p></div></div>    
</body>
</html>