# 神经网络中的注意力

> 原文：<https://towardsdatascience.com/attention-in-neural-networks-e66920838742?source=collection_archive---------0----------------------->

## 注意力结构的一些变化

![](img/e498a61cd05f0178b89ebffcb9fb8293.png)

在“ [**注意力**](/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda) 介绍”的早期帖子中，我们看到了那里介绍的注意力架构解决的一些关键挑战(并在下面的*图 1* 中提到)。本着同样的精神，你也可能会遇到其他的变体。在其他方面，这些变体的不同之处在于注意力被用于“何处”(独立的，在 RNN，在 CNN 等)以及注意力是如何产生的(全球与地方，软与硬等)。这篇文章是一些变种的简要列表。

![](img/04057482aef68ff518f069d38d3649a0.png)

**Fig 1**: From “[**Introduction to Attention**](/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda)”, based on [paper by Bahdanau et al](https://arxiv.org/pdf/1409.0473.pdf).

免责声明 1 :这里的想法只是为了了解在不同论文中提出的解决方案是如何利用注意力机制的。因此，重点将不是论文试图解决的任务类型，而是解决方案中注意力机制的使用。

*免责声明 2* :选择这些论文/变体的背后没有严格的理由。这份名单只是随机搜索和热门搜索的结果。

*免责声明 3* :自我关注和变形金刚值得单独发布(真的，我那天失去了动力)，这里不涉及。

**全球关注 vs 局部关注**

全局注意力与“ [**注意力介绍**](/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda) ”帖子中探讨的内容相同。当我们使用**所有**编码器隐藏状态来为每个解码器步骤定义基于注意力的上下文向量时。但正如你可能已经猜到的，这可能会变得昂贵。

另一方面，局部注意力只注意在一个较小窗口内的几个隐藏状态。该窗口以第“p”个编码器隐藏状态为中心，包括出现在“p”两侧的“D”个隐藏状态。因此，这使得该窗口的长度，即所考虑的隐藏状态的数量，为 2D+1。单调对齐是指 p 简单地设置为与解码器位置相同(第 5 个输出将具有 p = 5，如果 D = 2，则只关注 3，4，5，6，7 个隐藏状态)。预测对齐是当“p”被定义为解码器隐藏状态 ht 的函数(论文使用 s .sigmoid(vp⊤双曲正切(Wpht))并且该函数的参数被模型联合学习时。

![](img/f6504d0f8c6a7cff86e22c4bc4cb6fd2.png)

**Fig 2**: Global vs Local Attention as defined by [Luong et al. here](https://arxiv.org/pdf/1508.04025.pdf)

**硬注意力 vs 软注意力**

Luong 等人在他们的论文中提到的以及和徐等人在他们的论文中描述的，软注意是当我们将上下文向量计算为编码器隐藏状态的加权和时，正如我们在上面的图中所看到的。硬注意是指我们用注意力分数来选择一个单独的隐藏状态，而不是所有隐藏状态的加权平均。选择是一个问题，因为我们可以使用类似 argmax 的函数来进行选择，但它是不可微的(当我们使用 argmax 时，我们选择了与 max score 相对应的索引，并轻推权重以稍微移动分数，因为 backprop 的一部分不会改变该索引选择)，因此采用了更复杂的技术。注意，[的论文](http://proceedings.mlr.press/v37/xuc15.pdf)在图像字幕上下文中使用了硬注意，所以“编码器隐藏状态”实际上是 CNN 生成的“特征向量”。

![](img/2ac1025390378834d849f5ff2b302754.png)

**Fig 3**: Soft vs Hard Attention as defined by [Xu et al.](http://proceedings.mlr.press/v37/xuc15.pdf)

**潜在注意力**

我偶然发现了陈等人的这篇论文，这篇论文是关于把自然语言句子翻译成“如果-那么”程序的。即，给定一个类似“将你的 Instagram 照片发布到 Tumblr”的语句，网络应该预测描述触发器(“Instagram 照片”)和动作(Tumblr)的最相关的词，这将有助于获得相应的标签(trigger=Instagram)。Any_new_photo_by_you，action=Tumblr。Create_a_photo_post)。

我们如何运用注意力来达到这一点呢？我们再来看另一个例子，“将你 Dropbox 文件夹中的照片发布到 Instagram”。与前一个相比，这里的“Instagram”与行动最相关，而“Dropbox”是触发器。同一个词既可以是触发器，也可以是动作。因此，要确定这个词的作用，我们需要调查像“to”这样的介词在这样的句子中是如何使用的。本文介绍了一个“潜在注意”模型来做到这一点。

![](img/5c946dd3af0bf7c2cc1a39e15233237b.png)

**Fig 4**: “Latent Attention” presented by Chen et al. [in this paper](https://papers.nips.cc/paper/6284-latent-attention-for-if-then-program-synthesis.pdf)

准备了一个“J”维的“潜在注意力”向量——这里的每个维代表一个单词，softmax 给出了向量中单词的相对重要性。

1.  输入序列的长度为“J”(即“J”个字)。每个单词由一个“d”维嵌入向量表示。因此，整个序列是一个 d×J 矩阵。
2.  计算该矩阵与长度为“d”的可训练向量“u”的乘积，并对其进行 softmax。这给出了长度为“J”的“潜在注意力”向量

接下来，类似于上述准备“主动注意”，但是不使用类似“u”的“d”维向量，而是使用“d×J”维可训练矩阵 V，从而产生“J×J”主动注意矩阵。列级 softmax 是在每个单词的维度之间完成的。

然后“*有效重量*被计算为这两个重量的乘积。然后，用这些“*有效权重*对另一组单词嵌入进行加权，以导出输出，该输出是达到预测的软最大化。

> 对我来说，作为代表输入中每个单词的向量和代表单词重要性的潜在注意力向量的乘积的主动权重的推导是“自我注意”的一种形式，但稍后更多地是关于自我注意。

**基于注意力的卷积神经网络**

在这篇论文中，Yin 等人提出了 ABCNN——基于注意力的 CNN 来建模一对句子，用于答案选择、复述识别和文本蕴涵任务。所提出的基于注意的模型的关键亮点是，它考虑了一个输入句子的不同部分或单词或整个句子之间存在的影响/关系/影响，并提供了可用于后续任务的相互依赖的句子对表示。让我们先快速浏览一下基础网络，然后再来看看注意力是如何被引入其中的。

![](img/79f44dd3b7c7c0252b1419fce071ec71.png)

**Fig 5**: Yin et al. [in this paper](https://arxiv.org/pdf/1512.05193.pdf)

1.  ***输入层*** :从两个分别有 5 个和 7 个单词的句子 s0 和 s1 开始。每个单词由一个嵌入向量表示。如果你正在数盒子，那么图 5 的*表示嵌入向量的长度为 8。所以 s0 是一个 8×5 秩 2 张量，s1 是一个 8×7 秩 2 张量。*
2.  *:可能有一个或多个卷积层。先前 conv 图层的输出将作为当前 conv 图层的输入。这被称为“制图表达要素地图”。对于第一 conv 层，这将是表示输入句子的矩阵。卷积层应用宽度为 3 的过滤器。这意味着对具有 5 个字的 s0 执行卷积运算 7 次(xx1，x12，123，234，345，45x，5xx)，从而创建具有 7 列的特征映射。对于 s1，这变成了具有 9 列的特征映射。在每个步骤中执行的卷积运算是“tanh (W.c+ b)”，其中“c”是在 7 个卷积步骤(xx1，x12，123，234，345，45x，5xx)的每个步骤中单词的级联嵌入。换句话说，c 是一个长度为 24 的向量。如果你在数箱子，那么根据图 5 中的*，W 的尺寸是 8 x 24。**
3.  **:应用“平均池层”对“w”列进行逐列平均，其中“w”是该层中使用的卷积滤波器的宽度。在我们的例子中，这是 3。因此，s0 的平均值为 123，234，345，456，567，将 7 列特征转换回 5 列。s1 也是如此。**
4.  *****最后一层汇集*** :在最后一个卷积层，平均汇集不是在“w”列上进行，而是在所有列上进行，因此将矩阵特征映射转换为表示向量的句子。**
5.  *****输出层*** :根据任务选择处理表示向量的句子的输出层，图中显示了一个逻辑回归层。**

**请注意，第一层的输入是单词，下一层是短短语(在上面的示例中，过滤器宽度 3 使其成为 3 个单词的短语)，下一层是较大的短语，依此类推，直到最后一层，其中输出是句子表示。换句话说，对于每一层，都会产生一个较低到较高粒度的抽象表示。**

**本文介绍了将注意力引入这个基本模型的三种方式。**

****ABCNN-1****

**![](img/dbed3902e0d6d0495e7537e9648c7dfb.png)**

****Fig 6**: ABCNN-1 [in this paper](https://arxiv.org/pdf/1512.05193.pdf)**

1.  **在 ABCNN-1 中，在卷积运算之前引入注意力。句子 s0(8×5)和 S1(8×7)的输入表示特征图(在基本模型描述的#2 中描述，在图 6 的*中显示为红色矩阵)被“匹配”以得到注意矩阵“A”(5×7)。***
2.  **注意矩阵中的每个单元 Aij 代表 s0 中的第 I 个单词和 s1 中的第 j 个单词之间的注意分数。在本文中，该分数计算为 1/(1+| x y |)，其中| |是欧几里德距离。**
3.  **该注意力矩阵然后被转换回“注意力特征图”，其具有与输入表示图(蓝色矩阵)相同的维度，即分别使用可训练权重矩阵 W0 和 W1 的 8×5 和 8×7。**
4.  **现在，卷积运算不仅在像基本模型那样的输入表示上执行，而且在输入表示和刚刚计算的注意力特征图上执行。**
5.  **换句话说，不是如上面基本模型描述的#1 中所述使用秩 2 张量作为输入，而是对秩 3 张量执行卷积运算。**

****ABCNN-2****

**![](img/949b281922a8798e60d03a3f5a601468.png)**

****Fig 7**: ABCNN-2 [in this paper](https://arxiv.org/pdf/1512.05193.pdf)**

1.  **在 ABCNN-2 中，注意矩阵不是使用如在 ABCNN-2 中描述的输入表示特征图，而是在卷积运算的输出上准备的，我们称之为“conv 特征图”。在我们的示例中，这是分别代表 s0 和 s1 的 7 列和 9 列特征映射。因此，与 ABCNN-1 相比，注意力矩阵的维度也将不同，这里是 7 x 9。**
2.  **然后，通过对给定行(对于 s0)或列(对于 s1)中的所有关注值求和，使用该关注矩阵来导出关注权重。例如，对于 s0 的 conv 特征图中的第一列，这将是关注度矩阵中第一行中所有值的总和。对于 s1 的 conv 特征图中的第一列，这将是关注度矩阵的第一列中所有值的总和。换句话说，conv 特征图中的每个单元/列都有一个关注权重。**
3.  **注意力权重然后用于“重新加权”conv 特征地图列。池化输出要素地图中的每一列都被计算为被池化的“w”conv 要素地图列的关注度加权和，在上述示例中为 3。**

**ABCNN-3**

**![](img/b3f69a7ac1e5e30be67134857a75e74c.png)**

****Fig 8**: ABCNN-3 [in this paper](https://arxiv.org/pdf/1512.05193.pdf)**

**ABCNN-3 简单地将两者结合起来，本质上是在汇集的同时关注卷积的输入和卷积的输出。**

****可分解的注意力模型****

**对于自然语言推理，[Parikh 等人的这篇论文](https://arxiv.org/pdf/1606.01933.pdf)首先创建注意力权重矩阵，将一个句子中的每个单词与另一个句子中的所有单词进行比较，并如图所示进行归一化。但在这之后，在下一步中，问题被“分解成子问题”，分别解决。即，前馈网络用于采用级联的单词嵌入和相应的归一化对齐向量来生成“比较向量”。然后将每个句子的比较向量相加，以创建代表每个句子的两个集合比较向量，然后通过另一个前馈网络进行反馈，以进行最终分类。在这个解决方案中，词序并不重要，只需要注意。**

**![](img/f0a4165a4175d36ff1dfdb8d065384cc.png)**

****Fig 9**: From [this paper by Parikh et al](https://arxiv.org/pdf/1606.01933.pdf)**

****用于在线注意力的神经传感器****

**对于在线任务，如实时语音识别，我们没有处理整个序列的奢侈[jait ly](http://bengio.abracadoudou.com/cv/publications/pdf/jaitly_2016_nips.pdf)等人的这篇论文介绍了一种神经转换器，它可以在一次处理输入块的同时进行增量预测，而不是在整个输入序列上进行编码或引起注意。**

**输入序列被分成等长的多个块(可能除了最后一个块)，并且神经换能器模型只为当前块中的输入计算注意力，然后用于生成对应于该块的输出。与先前块的连接仅通过隐藏状态连接存在，隐藏状态连接是编码器和解码器侧的 RNN 的一部分。虽然这在一定程度上类似于前面描述的局部注意力，但是没有如那里描述的明确的“位置对准”。**

**![](img/a17190eddbdacd238fbf4db44bc0857c.png)**

****Fig 10**: Neural Transducer — attending to a limited part of the sequence. From [this paper by Jaitly et al](http://bengio.abracadoudou.com/cv/publications/pdf/jaitly_2016_nips.pdf)**

****区域注意事项****

**回头参考*图 1* ，这是我们在[早先的帖子](/an-introduction-to-attention-transformers-and-bert-part-1-da0e838c7cda)中看到的基础介绍性注意力模型的图示。对齐的一个概括抽象是，它就像在我们生成输出时查询内存的*。*记忆是输入的某种表示，查询是输出的某种表示。在图 1 的*中，存储器或密钥集合是编码器隐藏状态“h”，蓝色节点，查询是当前解码器隐藏状态“s”，绿色节点。然后将导出的比对分数乘以“值”——输入的另一种表示，即图 1* 中的金色节点。**

**区域注意力是当注意力被施加到一个“区域”上时，不一定只是像普通注意力模型那样的一个项目。“区域”被定义为存储器中一组结构上相邻的项目(即一维输入中的输入序列，如单词的句子)。通过组合存储器中的相邻项目来形成一个区域。在像图像这样的二维情况下，该区域将是图像内的任何矩形子集。**

**![](img/8097d86cc2c88a64f3899a247efa7683.png)**

****Fig 11**: Area attention from [this paper](https://arxiv.org/pdf/1810.10126.pdf) by Yang et al.**

**一个区域的“键”向量可以简单地定义为该区域中每个项目的键的平均向量。在序列到序列翻译任务中，这将是该区域中涉及的每个隐藏状态向量的平均值。在图 11 中*的“简单关键向量”下的定义中，“k”是隐藏状态向量。如果我们定义一个包含 3 个相邻字的区域，那么平均向量就是在编码器中三个字的每一个之后产生的隐藏状态向量的平均值。***

**另一方面,“值”被定义为该区域中所有值向量的总和。在我们的基本例子中，这将再次是对应于为其定义区域的三个字的编码器隐藏状态向量。**

**我们还可以定义更丰富的关键向量表示，不仅考虑平均值，还考虑标准偏差和形状向量，如*图 11* 中所述。形状向量在这里被定义为高度和宽度向量的连接，而高度和宽度向量又是使用嵌入矩阵从投影为向量的实际宽度和高度数字中创建的，我假设这是通过模型学习的。该密钥是作为单层感知器的输出导出的，该感知器将平均值、标准差和形状向量作为输入。**

**一旦定义了关键向量和值向量，网络的其余部分可以是任何注意力利用模型。如果我们使用如图 1 中*所示的编码器-解码器 RNN，那么插入导出的基于区域的键和值向量来代替图 1* 中的那些将使其成为基于区域的注意力模型。**

**通读这些论文给了我们一个有趣的视角，让我们了解研究人员如何将注意力机制用于各种任务，以及思维是如何进化的。希望这个快速的研究能让我们知道如何在我们自己的任务中调整和使用其中的一个或一个新的变体。**