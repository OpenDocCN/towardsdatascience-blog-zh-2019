<html>
<head>
<title>Implicit-Decoder part 2–3D generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">隐式解码器第 2 部分–3D 生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implicit-decoder-part-2-3d-generation-80dbcad8a563?source=collection_archive---------40-----------------------#2019-11-16">https://towardsdatascience.com/implicit-decoder-part-2-3d-generation-80dbcad8a563?source=collection_archive---------40-----------------------#2019-11-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e915" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">内部人工智能</h2><div class=""/><div class=""><h2 id="6198" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">深度学习和 3D 的 3D 生成和限制</h2></div><p id="d506" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">前一篇文章— <a class="ae ln" rel="noopener" target="_blank" href="/implicit-decoder-3d-reconstruction-838193f9b760">隐式解码器第 1 部分—3D 重建</a></p><h1 id="c1a3" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">3D 生成</h1><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/d976387923120dbd10fd00b4f819975c.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/1*d_BvBiKdCGof_JOIvV_cHw.gif"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">3D Airplane Generation</figcaption></figure><p id="c645" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">还记得<a class="ae ln" href="https://2d3d.ai/index.php/2019/11/11/the-deep-learning-dictionary/#What-are-generative-networks?" rel="noopener ugc nofollow" target="_blank">甘斯</a>吗？嗯，同样的技术可以用来生成你在左边看到的飞机。</p><p id="eb18" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">它是如何发生的？诀窍是使用相同的解码器网络如下所示。特别是与编码器一起训练的同一个解码器。我们训练一个 GAN 网络来生成一个假的 z 向量。</p><p id="e913" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">鉴别器从编码器-解码器网络获得真实的 z 向量作为输入，同时从生成器网络获得虚假的 z 向量作为输入。生成器网络被训练为仅基于随机输入产生新的 z 向量。由于解码器知道获得 z 向量作为输入，并从中重建 3D 模型，并且生成器被训练以产生类似真实的 z 向量，因此可以使用两个网络组合来重建新的 3D 模型。</p><p id="4b86" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">此外，我们可以看到 gif 显示一个飞机模型变形为一个新的。这是通过获取第一个和最后一个 3D 模型的 z 向量来完成的，让我们将这些向量称为 z_start 和 z_end，然后新的 z 向量被计算为 z_start 和 z_end 的线性组合。具体来说，选取一个介于 0 和 1 之间的数字(假设为 alpha)，然后计算一个新的 z-z _ new:z _ new =(z _ start * alpha+z _ end *(1-alpha))。然后，z_new 被馈送到解码器网络中，并且可以计算中间 3D 模型。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="ab gu cl ms"><img src="../Images/93505e148ac569de1f6087ce7e5efec4.png" data-original-src="https://miro.medium.com/v2/format:webp/1*OWOzBdzqT214bETaRIxPSw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Encoder-Decoder</figcaption></figure><p id="804e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在不同的 3D 模型之间存在这样的平滑过渡的原因是隐式解码器网络被训练来基于 z 向量识别模型的底层 3D 构造，并且更具体地，识别特定模型类别中的模型。因此，z_vector 的微小变化将导致 3D 模型的微小变化，但仍然保持模型类别的 3D 结构，这样就可以从 z_start 到 z_end 连续地改变模型。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/cf86d3485e0971e5d4803e41c8642f3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XGCZ-nwfUjDunX4r.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Generating z-vectors</figcaption></figure><h1 id="f2c1" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">深度学习和 3D 重建\生成的局限性</h1><p id="7438" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">这些结果被带到这里和其他地方，使神经网络似乎是全能的，易于使用和推广到其他场景，用例和产品。有时候是这样，但很多时候不是。在 3D 生成和重建中，神经网络存在局限性。仅举几个例子:</p><h2 id="26fb" class="nd lp it bd lq ne nf dn lu ng nh dp ly la ni nj ma le nk nl mc li nm nn me iz bi translated">数据集限制</h2><p id="a7f0" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">正如我们已经证明的，神经网络每次都需要针对特定的模型类别进行训练。每个类别中需要有足够多的模型(通常至少数百个),并且足够多的类别允许这种类型的神经网络的任何类型的现实生活应用，ShapeNet 正在为学术界做这项工作，即使在学术界，类别和每个类别中的模型的数量也是有限的。为了使它在商业上可行，我们需要更多的型号和种类。此外，每个模型需要贴上标签，以其确切的类别，需要调整翻译和规模准确，需要以正确的格式保存。此外，对于每个模型，我们需要不同角度、不同照明位置和相机参数、不同比例对齐和平移的图像。同样，ShapeNet 和其他研究计划有助于建立这一系统，以帮助科学进步。但是，这也意味着，为了将这项研究转化为产品，在数据集创建和处理方面会有大量开销。</p><h2 id="e97a" class="nd lp it bd lq ne nf dn lu ng nh dp ly la ni nj ma le nk nl mc li nm nn me iz bi translated">准确度测量</h2><p id="2eeb" class="pw-post-body-paragraph kr ks it kt b ku my kd kw kx mz kg kz la na lc ld le nb lg lh li nc lk ll lm im bi translated">一个反复出现的问题是 3D 重建或生成有多精确。对此的一个回答是——如何测量 3D 重建的精确度？假设一个人类 3D 设计师从一幅图像重建一个 3D 模型，我们怎么能说他的工作准确与否呢？即使我们有了原始的 3D 模型，又怎么能说两个 3D 模型是相似的，或者说重建的 3D 模型与原点相似，又怎么能量化这种相似性呢？老派的方法，如 MSE、IoU、F1 分数、切角和法向距离[[添加参考—<a class="ae ln" href="https://2d3d.ai/index.php/2019/10/09/3d-scene-reconstruction-from-single-image/" rel="noopener ugc nofollow" target="_blank">https://2d 3d . ai/index . PHP/2019/10/09/3D-scene-re construction-from-single-image/</a>]]是不考虑对象的 3D 结构的直接度量。例如，IoU 检查与两个形状的联合体积相比，重建的 3D 形状的体积有多少与原始 3D 形状重叠。如果重建的形状被移动到空间中的不同体积中，即使形状是相同的，IoU 也可能为零(因为没有重叠)。</p><p id="6a35" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在隐式解码器论文中，作者使用了一种不同的 3D 形状相似性度量方法—<a class="ae ln" href="https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-8659.00669" rel="noopener ugc nofollow" target="_blank">【LFD】</a>。该度量对于模型的比例、对齐和位置(平移)是不变的。基本想法是从十二面体上的角度拍摄模型的 10 个轮廓图像，并且每个模型拍摄 10 个不同的十二面体。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8c56886e5a5442bd59a74f94fb71fbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/0*1hcui-meAKXVRxcY.png"/></div></figure><p id="2c60" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后，当在两个模型之间进行比较时，使用<a class="ae ln" href="https://en.wikipedia.org/wiki/Fourier_series" rel="noopener ugc nofollow" target="_blank">傅立叶</a>和<a class="ae ln" href="https://en.wikipedia.org/wiki/Zernike_polynomials" rel="noopener ugc nofollow" target="_blank">泽尼克</a>系数来比较来自这 10 个十二面体的图像的视觉相似性。</p><h1 id="1931" class="lo lp it bd lq lr ls lt lu lv lw lx ly ki lz kj ma kl mb km mc ko md kp me mf bi translated">参考</h1><ul class=""><li id="0e50" class="np nq it kt b ku my kx mz la nr le ns li nt lm nu nv nw nx bi translated">陈，，和。"学习生成式形状建模的隐式场."<em class="ny">IEEE 计算机视觉和模式识别会议论文集</em>。2019.</li><li id="ab86" class="np nq it kt b ku nz kx oa la ob le oc li od lm nu nv nw nx bi translated">shape net:<a class="ae ln" href="https://www.shapenet.org/" rel="noopener ugc nofollow" target="_blank">https://www.shapenet.org/</a></li><li id="6413" class="np nq it kt b ku nz kx oa la ob le oc li od lm nu nv nw nx bi translated">，陈，丁云，等，“基于视觉相似性的三维模型检索研究”<em class="ny">计算机图形学论坛</em>。第 22 卷。№3.英国牛津:布莱克威尔出版公司，2003 年。</li></ul></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="1de1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ny">原载于 2019 年 11 月 16 日</em><a class="ae ln" href="https://2d3d.ai/index.php/2019/11/16/implicit-decoder-part-2-3d-generation/?utm_source=2d3d.ai&amp;utm_campaign=cf7b1bfd74-EMAIL_CAMPAIGN_2019_11_16_07_23&amp;utm_medium=email&amp;utm_term=0_9ef5ea9bbc-cf7b1bfd74-" rel="noopener ugc nofollow" target="_blank"><em class="ny">https://2d3d . ai</em></a><em class="ny">。</em></p></div></div>    
</body>
</html>