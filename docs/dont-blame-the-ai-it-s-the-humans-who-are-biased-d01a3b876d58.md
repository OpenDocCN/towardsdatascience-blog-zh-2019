# 不要责怪人工智能，是人类有偏见。

> 原文：<https://towardsdatascience.com/dont-blame-the-ai-it-s-the-humans-who-are-biased-d01a3b876d58?source=collection_archive---------20----------------------->

人工智能编程中的人工智能，无论是有意识的还是无意识的，都是学者、公众和媒体关注的问题。考虑到在招聘、信贷、社会福利、治安和法律决策中使用的含义，他们有充分的理由这样做。当计算机算法基于数据和/或编程规则做出有偏见的决定时，就会出现人工智能偏见。偏见的问题不仅与编码(或编程)有关，也与用于训练人工智能算法的数据集有关，在一些人所谓的“歧视反馈环[2]。”

![](img/93bde2be033678b54ef641d380267d9d.png)

这些数据集包括你的姓名或个人信息、照片、文档或其他历史文物。因为这些数据是由人类收集的，它们是主观的，通常不具有代表性[3]。正如[在我之前的帖子](https://link.medium.com/DP71oxhd4Z)中所概述的，历史记录中女性(和少数民族)的表现(或被压制)存在问题。由于数据集内经济学与性别、种族和民族的交叉，女性和有色人种可能会不成比例地受到人工智能偏见的影响[1，4]。Buolamwini 和 Gebru[5]在他们对商业可用的面部识别人工智能的分析中发现，白人男性的性别在 99%以上的情况下都能被计算机正确识别，但黑人女性的准确率仅为 65%。

社会偏见如何进入人工智能是复杂的，但在深入研究这个话题后，我发现它们可以大致分为 3 个主要问题。作为一个非工程师，请容忍我试图阐明它们。

F 首先，用于[训练 AI 系统](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#training_set)的**数据**本身可能带有系统性的社会偏见。有些人甚至概述了我们的语言本身如何带有偏见，从根本上影响许多数据点。想象一个假设的场景，一所大学使用人工智能来帮助排名新生申请人，使用以前的学生记录作为其训练数据(例如，高中 GPA、SAT 分数或其他与大学成功相关的州测试结果)。人们广泛讨论了美国学校系统和标准化考试中的不公平现象，这引起了人们对性别/种族/民族和年级或分数差异的关注[6]。计算机将寻找子模式，包括性别/种族/民族与大学成功的相关性，并可能最终筛选出不匹配的申请人；因此结果延续了这些偏见。

第二个问题是，因为 ML 和 AI 需要**大型数据集**来进行关联，不可避免地，代表不足人口中的学生也将出现在数据中，这可能导致更大的数据误差。如果你对统计学有所了解，一般来说，你拥有的数据越多，调查结果就越可靠。

最后，操纵计算机算法的**规则**和要使用的变量必须由人类程序员来编码。每个人都有意识和无意识的偏见，这些偏见影响到他们所做的每一件事(例如，行为、观点)，这对人工智能程序员和他/她的代码来说是不可避免的。在 ML 和 AI 中，程序员需要选择使用(或忽略)哪些变量，然后根据某个集合和规则“训练”系统。然后，计算机开始在一个比人类可能处理的大得多的数据集上寻找更多这样的模式。在更先进的人工智能的情况下，一个复杂的问题是，有时人工智能做出的关联和决定往往是不透明的[1]。

> 打造更好的语音应用。在 voicetechpodcast.com 的[获得更多来自语音技术专家的文章和采访](https://voicetechpodcast.com/)

如今，塑造人工智能的计算机科学家主要是男性、白人，而且收入颇丰。因此，考虑到使用人工智能对大学申请人进行排名和选择的情况，具有这种特征的编码人员可能缺乏上下文和文化知识来理解女学生或有色人种的生活；他对学校的概念以及怎样才能成为一名成功的申请人只能基于他的个人经历。这不仅仅适用于性别和种族；在这种情况下，可能有许多其他偏见，如学习挑战和残疾、学习风格或社会经济地位。

在(人类的)现实世界中，亚马逊使用人工智能来帮助筛选求职者(使用过去的招聘成功数据)，导致参加女子大学的女性候选人得分较低，最终被终止[2]。广告中的机器学习最近因住房、工作和信贷广告中的性别和种族歧视而受到审查[7]。

萨菲娅·诺布尔的书*压迫的算法*强调了谷歌的算法，世界上最常用的搜索引擎背后的规则，远非中立，事实上加强了社会种族主义和性别歧视。诺布尔的书的封面上描绘了一个例子，一个简单的谷歌搜索“为什么黑人女性如此……”返回自动填充结果建议，其中许多是负面的(如“懒惰”、“愤怒”和“卑鄙”)。谷歌后来改变了与这个搜索词相关的建议，然而，人们只需要对“男人”、“女人”、“美国人”或“首席执行官”进行自己的图像搜索，就可以看到性别和种族代表性的问题。

这些例子中的技术本身可能不是罪魁祸首，而是反映了人类现实世界中系统性的性别偏见。更大的数据集是技术社区提出的一个解决方案，然而，如果底层系统存在固有的偏见，人工智能只会反映(并放大)这些。虽然人工智能系统在技术上能够做出比人类更少偏见的决定，但最终是人工智能的开发者和编程者拥有改变它的真正权力[2]。

参考

1.郝，k .(2019 . 2 . 4)。这就是人工智能偏见是如何真正发生的——以及为什么它如此难以修复。*麻省理工科技评论*。检索自[https://www . technology review . com/s/612876/this-is-how-ai-bias-really-happens and-why-its-so-hard-to-fix/](https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/)

2.韦斯特，s .，惠特克，m .，克劳福德，K. (2019)。人工智能中的性别、种族和权力。艾现在研究所。检索自:[https://ainowinstitute.org/discriminatingsystems.pdf](https://ainowinstitute.org/discriminatingsystems.pdf)

3.上议院。(2017).人工智能在英国:准备好了，愿意了，有能力了吗？人工智能特别委员会。会议报告 2017–19。检索自:[https://publications . parliament . uk/pa/LD 201719/LD select/ldai/100/100 . pdf](https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf)

4.瓦尔迪维亚，A. (2018)。压迫的算法:搜索引擎如何强化种族主义。女权主义的形成，30(3)，217-220 页。doi: 10.1353/ff.2018.0050

5.Buolamwini，j .，Gebru，T. (2018 年)。性别差异:商业性别分类的交叉准确性差异。*机器学习研究论文集 81*:1–15。检索自[http://proceedings . MLR . press/v81/buolamwini 18a/buolamwini 18a . pdf](http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf)

6.Cimpian，J. (2018 年 4 月 23 日)。*我们的教育体系如何破坏性别平等【博客】*。检索自:[https://www . Brookings . edu/blog/brown-center-chalk board/2018/04/23/how-our-education-system-baskets-gender-equity/](https://www.brookings.edu/blog/brown-center-chalkboard/2018/04/23/how-our-education-system-undermines-gender-equity/)

7.Dopp，t .和 Westbrook，J. (2019 年 3 月 28 日)。脸书违反公平住房法与广告的做法，住房与城市发展部的指控。彭博在线。从 http://www.bloomberg.com[取回](http://www.bloomberg.com)