<html>
<head>
<title>Simple Transformers — Introducing The Easiest Way To Use BERT, RoBERTa, XLNet, and XLM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单的变压器——介绍使用伯特、罗伯塔、XLNet 和 XLM 的最简单方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3?source=collection_archive---------6-----------------------#2019-10-04">https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3?source=collection_archive---------6-----------------------#2019-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0680" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">想要为 NLP 使用 Transformer 模型吗？一页页的代码让你沮丧？不再是了，因为简单的变形金刚正在工作。只需 3 行代码就可以启动、训练和评估变形金刚！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7c479c5160293291ab3d4ea6e98f1f38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HmTmqEntaryJpH5MNgTsdg.jpeg"/></div></div></figure><h1 id="e71f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">前言</h1><p id="7cbe" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">简单的变形金刚库是通过拥抱脸作为优秀的 T2 变形金刚库的包装而构建的。我永远感谢在拥抱脸的人们所做的艰苦工作，使公众能够方便地访问和使用变形金刚模型。没有你们我真不知道该怎么办！</p><h1 id="ec9a" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">介绍</h1><p id="e003" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我相信可以公平地说，Transformer 模型的成功在推进自然语言处理领域取得了惊人的成就。他们不仅在许多他们被设计解决的 NLP 任务上表现出惊人的飞跃，预先训练过的变形金刚在迁移学习上也表现得出奇的好。这意味着任何人都可以利用训练这些模型的长时间和令人难以置信的计算能力来执行无数种 NLP 任务。你不再需要谷歌或脸书的雄厚财力来构建一个最先进的模型来解决你的 NLP 问题了！</p><p id="b2a7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">或者人们可能希望如此。事实是，让这些模型发挥作用仍然需要大量的技术知识。除非你在深度学习方面有专业知识或至少有经验，否则这似乎是一个令人生畏的挑战。我很高兴地说，我以前关于变形金刚的文章(这里是<a class="ae mi" rel="noopener" target="_blank" href="/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca">这里是</a>和<a class="ae mi" href="https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04" rel="noopener">这里是</a>)似乎已经帮助了很多人开始使用变形金刚。有趣的是，我注意到不同背景的人(语言学、医学、商业等等)都在尝试使用这些模型来解决他们自己领域的问题。然而，为了使变压器适应特定的任务，需要克服的技术障碍并非微不足道，甚至可能相当令人沮丧。</p><h1 id="2b04" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">简单变压器</h1><p id="6d85" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这个难题是我决定开发一个简单的库来使用 Transformers 执行(二进制和多类)文本分类(我见过的最常见的 NLP 任务)的主要动机。我们的想法是让它尽可能简单，这意味着抽象出许多实现和技术细节。库的实现可以在<a class="ae mi" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。我强烈建议您查看它，以便更好地了解一切是如何工作的，尽管使用该库并不需要了解内部细节。</p><p id="8145" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为此，我们编写了简单的 Transformers 库，只需 3 行代码就可以初始化 Transformer 模型，在给定的数据集上进行训练，并在给定的数据集上进行评估。让我们看看这是怎么做的，好吗？</p><h1 id="a7ee" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">装置</h1><ol class=""><li id="579b" class="mp mq it lo b lp lq ls lt lv mr lz ms md mt mh mu mv mw mx bi translated">从<a class="ae mi" href="https://www.anaconda.com/distribution/" rel="noopener ugc nofollow" target="_blank">这里</a>安装 Anaconda 或 Miniconda 包管理器</li><li id="3077" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh mu mv mw mx bi translated">创建新的虚拟环境并安装所需的软件包。<br/> <code class="fe nd ne nf ng b">conda create -n transformers python pandas tqdm</code> <br/> <code class="fe nd ne nf ng b">conda activate transformers</code> <br/>如果使用 cuda: <br/> <code class="fe nd ne nf ng b">conda install pytorch cudatoolkit=10.0 -c pytorch</code> <br/>其他:<br/><code class="fe nd ne nf ng b">conda install pytorch cpuonly -c pytorch</code><br/><code class="fe nd ne nf ng b">conda install -c anaconda scipy</code><br/><code class="fe nd ne nf ng b">conda install -c anaconda scikit-learn</code><br/><code class="fe nd ne nf ng b">pip install transformers</code><br/><code class="fe nd ne nf ng b">pip install tensorboardx</code></li><li id="db1e" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh mu mv mw mx bi translated">安装<em class="mo">简单变压器</em>。<br/>T8】</li></ol><h1 id="c528" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">使用</h1><p id="ea69" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">快速浏览一下如何在 Yelp 评论数据集上使用这个库。</p><ol class=""><li id="7c1e" class="mp mq it lo b lp mj ls mk lv nh lz ni md nj mh mu mv mw mx bi translated">下载<a class="ae mi" href="https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz" rel="noopener ugc nofollow" target="_blank"> Yelp 评论数据集</a>。</li><li id="8828" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh mu mv mw mx bi translated">提取<code class="fe nd ne nf ng b">train.csv</code>和<code class="fe nd ne nf ng b">test.csv</code>并放入目录<code class="fe nd ne nf ng b">data/</code>中。</li></ol><p id="25c0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="mo"> (Bash 用户可以使用</em> <a class="ae mi" href="https://github.com/ThilinaRajapakse/pytorch-transformers-classification/blob/master/data_download.sh" rel="noopener ugc nofollow" target="_blank"> <em class="mo">这个脚本</em> </a> <em class="mo">来下载数据集)</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="d13d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这里没有什么特别的，我们只是以正确的形式得到数据。对于任何数据集，这都是您必须做的。</p><ul class=""><li id="340c" class="mp mq it lo b lp mj ls mk lv nh lz ni md nj mh nm mv mw mx bi translated">为训练和评估部分创建两个熊猫<code class="fe nd ne nf ng b">DataFrame</code>对象。</li><li id="31b1" class="mp mq it lo b lp my ls mz lv na lz nb md nc mh nm mv mw mx bi translated">每个<code class="fe nd ne nf ng b">DataFrame</code>应该有两列。第一列包含您想要训练或评估的文本，数据类型为<code class="fe nd ne nf ng b">str</code>。第二列有相应的标签，数据类型为<code class="fe nd ne nf ng b">int</code>。<br/> <em class="mo">更新:现在建议将列命名为</em> <code class="fe nd ne nf ng b"><em class="mo">labels</em></code> <em class="mo">和</em> <code class="fe nd ne nf ng b"><em class="mo">text</em></code> <em class="mo">而不是依赖于列的顺序。</em></li></ul><p id="1d45" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">数据整理好了，就该训练和评估模型了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="b3c8" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">就是这样！</p><p id="2910" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">为了对其他文本进行预测，<code class="fe nd ne nf ng b">TransformerModel</code>附带了一个<code class="fe nd ne nf ng b">predict(to_predict)</code>方法，它给出了一个文本列表，返回模型预测和原始模型输出。</p><p id="d935" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">有关所有可用方法的更多详细信息，请参见<a class="ae mi" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank"> Github repo </a>。repo 还包含一个使用该库的最小示例。</p><h1 id="9c6f" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">默认设置以及如何更改它们</h1><p id="a0d6" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">下面给出了使用的默认参数。这些都可以通过将包含相应键/值对的 dict 传递给 TransformerModel 的 init 方法来覆盖。<em class="mo">(见下面的例子)</em></p><pre class="kj kk kl km gt nn ng no np aw nq bi"><span id="24f1" class="nr kv it ng b gy ns nt l nu nv">self.args = {<br/>   'model_type':  'roberta',<br/>   'model_name': 'roberta-base',<br/>   'output_dir': 'outputs/',<br/>   'cache_dir': 'cache/',</span><span id="f64b" class="nr kv it ng b gy nw nt l nu nv">   'fp16': True,<br/>   'fp16_opt_level': 'O1',<br/>   'max_seq_length': 128,<br/>   'train_batch_size': 8,<br/>   'eval_batch_size': 8,<br/>   'gradient_accumulation_steps': 1,<br/>   'num_train_epochs': 1,<br/>   'weight_decay': 0,<br/>   'learning_rate': 4e-5,<br/>   'adam_epsilon': 1e-8,<br/>   'warmup_ratio': 0.06,<br/>   'warmup_steps': 0,<br/>   'max_grad_norm': 1.0,</span><span id="d6c7" class="nr kv it ng b gy nw nt l nu nv">   'logging_steps': 50,<br/>   'evaluate_during_training': False,<br/>   'save_steps': 2000,<br/>   'eval_all_checkpoints': True,<br/>   'use_tensorboard': True,</span><span id="367c" class="nr kv it ng b gy nw nt l nu nv">   'overwrite_output_dir': False,<br/>   'reprocess_input_data': False,<br/>}</span></pre><p id="a925" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">要覆盖其中任何一个，只需向 TransformerModel 传递一个带有适当的键/值对的字典。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="088a" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">关于每个参数的解释，请参考<a class="ae mi" href="https://github.com/ThilinaRajapakse/simpletransformers" rel="noopener ugc nofollow" target="_blank"> Github repo </a>。</p><p id="3bec" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><em class="mo">更新:当前参数见</em> <a class="ae mi" href="https://simpletransformers.ai/docs/usage/#configuring-a-simple-transformers-model" rel="noopener ugc nofollow" target="_blank"> <em class="mo">文档</em> </a> <em class="mo">。</em></p><h1 id="96ae" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">结论</h1><p id="75aa" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">那都是乡亲们！据我所知，使用变压器模型的最简单方法。</p></div></div>    
</body>
</html>