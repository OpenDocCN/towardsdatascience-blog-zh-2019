<html>
<head>
<title>An introduction to SVD and its widely used applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">奇异值分解及其广泛应用简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-svd-and-its-widely-used-applications-f5b8f19cb6cb?source=collection_archive---------14-----------------------#2019-06-01">https://towardsdatascience.com/an-introduction-to-svd-and-its-widely-used-applications-f5b8f19cb6cb?source=collection_archive---------14-----------------------#2019-06-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jn jo jp jq gh gi paragraph-image"><div class="ab gu cl jr"><img src="../Images/40048642a46759a1626833db7e2b5aed.png" data-original-src="https://miro.medium.com/v2/0*J1mWANZ5afCD9mmJ"/></div></figure><p id="a82d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">如果您在数据科学领域(或与之接近)，即使您没有使用过 SVD，您也可能已经听过一千遍了。无论是对于 PCA(主成分分析)还是推荐算法，SVD 都是一种强大的技术，目前广泛应用于许多模型中——我们将描述它是什么，并展示它如何在一些关键方法中使用。</p><p id="802e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">注意:我们不讨论代码部分，只讨论理论。</p><h1 id="61c2" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">什么是 SVD？</h1><p id="039a" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">SVD 的意思是<strong class="jw ir">奇异值分解</strong>。维数为<em class="ks"> n×d </em>的矩阵<em class="ks"> X </em>的 SVD 由下式给出:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/928ddd864f08740a5ebab32bc7ebdd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*dNMFT1mkqzOFRfovh8aSxg.png"/></div></figure><p id="397a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中:</p><ul class=""><li id="9ea1" class="mb mc iq jw b jx jy kb kc kf md kj me kn mf kr mg mh mi mj bi translated"><em class="ks"> U </em>和<em class="ks"> V </em>为直角正交；</li></ul><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/3f2034e3c15479e50e157d7fa2e4a7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*IipDz2_edMIg7urxAfbS4A.png"/></div></figure><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/0136f32a153b74be4e00f50e32c06565.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*qBGsTIKmLOTInTYyR7QTrg.png"/></div></figure><ul class=""><li id="e003" class="mb mc iq jw b jx jy kb kc kf md kj me kn mf kr mg mh mi mj bi translated"><em class="ks"> D </em>为尺寸<em class="ks"> d×n </em>的对角线</li></ul><p id="e320" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一些附加说明:</p><ul class=""><li id="ea92" class="mb mc iq jw b jx jy kb kc kf md kj me kn mf kr mg mh mi mj bi translated"><strong class="jw ir"> D 不一定是正方形</strong></li><li id="48ef" class="mb mc iq jw b jx mm kb mn kf mo kj mp kn mq kr mg mh mi mj bi translated">矩阵的奇异值分解可以在<strong class="jw ir">对任何矩阵进行</strong></li><li id="e2e3" class="mb mc iq jw b jx mm kb mn kf mo kj mp kn mq kr mg mh mi mj bi translated">奇异值分解<strong class="jw ir">不同于矩阵的特征值分解</strong>。</li></ul><p id="9294" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们来定义一个矩阵的<strong class="jw ir">特征值分解</strong>。首先，对于一个矩阵<em class="ks"> X </em>它存在当且仅当<em class="ks"> X </em>是平方的并且特征向量在矩阵维数空间中形成一个基。如果是这种情况，那么可以写:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/339cd0fa54eca9f8420d00e8e9f75a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*KE9xfHmTURE_4xzZXTE0Ww.png"/></div></figure><p id="15d8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中<em class="ks"> P </em>是特征向量的矩阵，D <em class="ks"> elta </em>是<em class="ks"> X </em>的特征值的对角矩阵——这里 D <em class="ks"> elta </em>是平方。</p><p id="e73e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在某种意义上，<strong class="jw ir"> SVD 是特征值分解</strong>的推广，因为它可以应用于任何矩阵。</p><h1 id="54fd" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">主成分分析中使用的奇异值分解</h1><p id="b33c" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">PCA 表示<strong class="jw ir">主成分分析</strong>。给定一个输入矩阵<em class="ks"> X </em>，它包括<strong class="jw ir">寻找作为原始坐标的线性组合的分量</strong> <em class="ks"> p_i </em>:</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/90149dad3a8b7f23baba11e0c45690bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qnds0TeZJP07r6Y8A04_DA.png"/></div></div></figure><p id="6957" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">以这样的方式:</p><ul class=""><li id="9541" class="mb mc iq jw b jx jy kb kc kf md kj me kn mf kr mg mh mi mj bi translated">分量为<strong class="jw ir">正交</strong> ( <em class="ks"> E[p_ip_j]=0 </em>)</li><li id="1538" class="mb mc iq jw b jx mm kb mn kf mo kj mp kn mq kr mg mh mi mj bi translated">组件以这样的方式<strong class="jw ir">排序</strong>，即<em class="ks">P1</em>解释了最大的方差百分比，而<em class="ks">p2</em>解释了第二大的方差百分比(这一点没有被<em class="ks">P1</em>解释)等等。</li></ul><p id="22be" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我们这里假设<strong class="jw ir"> <em class="ks"> X </em>归一化</strong>(每个特征<em class="ks">E(X _ I)= 0</em><em class="ks">Var(X _ I)= 1</em>)。</p><p id="85fd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">可以看出，分量 p 由下式给出</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/faad908c75434e881c9377fc63456d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*jOm0d64YjaLdRj7WClJ7qg.png"/></div></figure><p id="656e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中<em class="ks">γ</em>是对<em class="ks"> X </em>的方差-协方差矩阵进行<strong class="jw ir">特征值分解时找到的矩阵。注意，这是可能的，因为方差-协方差矩阵是对称的，并且<strong class="jw ir">任何对称矩阵都有特征值分解</strong>。还要注意的是，<em class="ks"> Gamma </em>的正交性意味着</strong></p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/634eb47c71e68e2998d7a3a67de398ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*e4Dvt7xlm6pOlE_hPwckfQ.png"/></div></figure><p id="f447" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">还要注意，因为<em class="ks"> E(x)=0 </em>，<em class="ks"> E(p)=0 </em>。和</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/04c70100e12e2b6d3a0e6768ecd60d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*EZK0ybul2EPMWsHXLoO1uA.png"/></div></figure><p id="5fa8" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">其中<strong class="jw ir"> D <em class="ks"> elta </em>是按降序排列的<em class="ks"> X </em>的方差-协方差矩阵</strong>的特征值的矩阵。这里要记住的关键是<strong class="jw ir">X 的主成分是通过使用方差-协方差矩阵</strong>的特征向量的线性变换得到的。</p><p id="4209" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一旦我们有了组件，我们可以通过只保留顶部的<em class="ks"> k </em> ( <em class="ks"> k </em>是任意的)组件来减少我们的数据。</p><blockquote class="na nb nc"><p id="1fb1" class="ju jv ks jw b jx jy jz ka kb kc kd ke nd kg kh ki ne kk kl km nf ko kp kq kr ij bi translated">但是我们真的需要计算这个特征值分解吗？</p></blockquote><p id="555f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">不要！这就是 SVD 的用武之地。实际上，使用上面的 SVD 公式，方差-协方差矩阵的<strong class="jw ir">特征向量由<em class="ks"> X </em>的 SVD 的矩阵<em class="ks"> U </em> </strong>给出。并且<strong class="jw ir">特征值是来自<em class="ks"> X </em>的 SVD 的奇异值</strong>的平方。</p><p id="d6e9" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">因此我们可以<strong class="jw ir">使用 SVD 来执行 PCA </strong>，并且保持<em class="ks"> X </em>的顶部<em class="ks"> k </em>奇异值近似给定的顶部<em class="ks"> k </em>主分量。</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/56c317fa82df4d77b7b4b07bb9c61466.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*7a-lvSgldIicycgHT8IMyg.png"/></div></figure><p id="298f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在一些常见情况下，通过 SVD 进行<strong class="jw ir"> PCA 在数值上比通过方差-协方差矩阵的特征值分解更有效</strong>(计算方差-协方差矩阵需要一些额外的时间)。</p><h1 id="96ef" class="kt ku iq bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">矩阵补全中使用的奇异值分解</h1><p id="9a93" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">对于大多数推荐算法，输入矩阵非常稀疏，矩阵分解方法是关键，因为空间需要“减少”到一个更小的潜在空间。奇异值分解在其中起着核心作用。</p><p id="abb2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">一般的矩阵分解问题表述为</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/4f3c0bfcbf174bc83881d185af831995.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*p5je7E3wp4Zzr608ltOW1w.png"/></div></figure><p id="b15c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在约束条件<strong class="jw ir">下，M  </strong>的秩小于或等于任意整数<em class="ks"> r </em>。人们可以证明这个问题的<strong class="jw ir">解</strong>是</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/74e7d690d62f1fea6737c2719aed8cdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*5EBQoFj-7ViNqJXXM-MngA.png"/></div></figure><p id="caf4" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在哪里</p><figure class="lx ly lz ma gt jq gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/3fc3e93a4c85ecbd580bd71e18b83080.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*XUYLDUmMlX3ZgfBjC1htDA.png"/></div></figure><p id="6823" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">并且<em class="ks"> D_r </em>是通过仅保留第一个<em class="ks"> r </em>奇异值而从<em class="ks"> D </em>构建的。</p><p id="bd1c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">人们可以注意到，当矩阵<em class="ks"> X </em>有缺失值时，我们不容易解决上述问题。有几种方法可以解决这个问题。其中之一是从一个初始解开始，通过做 SVDs 迭代，直到收敛。我们不会在这里讨论这些方法的细节，但是底线是<strong class="jw ir"> SVD 也可以用于矩阵补全，</strong>因为它涉及到矩阵分解。</p></div></div>    
</body>
</html>