# 使用 MinHash 和 Spark 的可扩展 Jaccard 相似性

> 原文：<https://towardsdatascience.com/scalable-jaccard-similarity-using-minhash-and-spark-85d00a007c5e?source=collection_archive---------7----------------------->

## 一个简单的算法使得大规模计算相似矩阵变得容易得多。

![](img/4e1c86130a290f83f8322ca471e29ac5.png)

不久前我想到，除了算术平均值之外，Jaccard 相似系数在我的作品中出现的次数可能比其他任何统计都多。如果你有两组事物(词、词的一部分、属性、类别或其他)，你可以用集合交集中的事物数除以集合并集中的事物数。由此产生的度量是一个有意义的相似性度量，其额外的优点是非常容易向非技术人员解释。

Jaccard 相似性在规模上直接计算有点困难。如果您有一个非常大的实体-属性对列表，并且您想要一个逐个实体的相似性矩阵，您基本上必须执行一个内部连接，按实体分组并计数，然后执行一个外部连接，按实体分组并计数，然后将两个连接的结果连接在一起。如果你的工作流程使用 Spark，就像我的一样，那就是一大堆混乱。它是昂贵的。

不久前，一位同事给我指出了一件我觉得我应该知道但却不知道的事情: [MinHash](https://en.wikipedia.org/wiki/MinHash#Jaccard_similarity_and_minimum_hash_values) 。对于每个实体，随机排列属性，然后散列它们(将它们转换成整数)，然后取最小值。这样做很多次，然后计算两个实体的相同绘制的最小灰度匹配的百分比。我们可以用解释这两个实体属性集之间的 Jaccard 相似性的方式来解释这个度量。所以它把问题从大量的属性归结为少量的散列；但是更好的是，它带来的问题从可变数量的属性——伴随着[键偏斜](https://coxautomotivedatasolutions.github.io/datadriven/spark/data%20skew/joins/data_skew/)的所有痛苦——到所有实体中相同数量的最小散列。

以下代码片段中从第 36 行到第 52 行的大部分内容都来自 Patrick Nicholson，他是我的同事，告诉我 MinHash 的事情，他改编了 Spark 的`spark.ml.feature.MinHashLSH`实现中的散列算法。我构建了连接逻辑，将 MinHash 结果转化为实际的 Jaccard 相似性，并将整个过程包装在一个函数中，使其更具可移植性。

该函数需要一个 Spark 数据帧，一个指示数据帧中包含节点标签的列的字符串(我们希望在它们之间找到相似性的实体)，以及包含边的列(我们将散列的属性)。该函数输出一个包含两列节点标签的数据框——每列都有一个由`suffixes`关键字参数指定的后缀——以及 Jaccard 相似性。

随着抽牌次数的增加，Jaccard 的相似性变得更加精确。一百次绘制(下面代码中的默认值)给出的精度最高为 0.01。500 次抽奖的精确度高达 0.005。1000 次绘制的精度最高可达 0.001。你明白了。

在过去的几个月里，这个小东西为我节省了很多时间和麻烦。我觉得值得分享一下。