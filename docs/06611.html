<html>
<head>
<title>Classification using different Link Function than logit, probit[Logistic Trilogy, part 3]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用不同于 logit、probit 的链接函数进行分类[逻辑三部曲，第 3 部分]</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/classification-using-different-link-function-than-logit-probit-logistic-trilogy-part-3-df9922b1acf1?source=collection_archive---------17-----------------------#2019-09-21">https://towardsdatascience.com/classification-using-different-link-function-than-logit-probit-logistic-trilogy-part-3-df9922b1acf1?source=collection_archive---------17-----------------------#2019-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="cce3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们学习一些新的东西。做一些新的、创新的事情总是好的。让我们更深入地研究逻辑分类器。实际上不是逻辑分类器，而是类似的东西。</p><p id="e91a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们使用过哪些不同的链接函数？Logit 链接、probit 链接和 less cloglog 链接。我们能自己想出其他链接功能吗？我们能创建自己的线性分类器吗？为了重新思考，我们需要一些提示，一些方向。让我给你一个。</p><blockquote class="ko"><p id="fa13" class="kp kq it bd kr ks kt ku kv kw kx kn dk translated">所有这些上面提到的反向链接函数只不过是一些连续概率分布的 CDF。</p></blockquote><p id="d2a3" class="pw-post-body-paragraph jq jr it js b jt ky jv jw jx kz jz ka kb la kd ke kf lb kh ki kj lc kl km kn im bi translated">逆 logit 环节是<em class="ld">标准物流配送</em>的 CDF。逆 probit 环节是<em class="ld">标准正态分布</em>的 CDF。逆 cloglog 链接是针对最小值的<em class="ld">广义 Gumbel 分布的 CDF。</em></p><p id="13d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中的共同点是相关的随机变量可以取整条实线上的任何值。这个特性非常非常重要。你能想出更多定义在整条实直线上的连续概率分布吗？目前我能想到的有<strong class="js iu">柯西分布</strong>和<strong class="js iu">拉普拉斯分布</strong>。<strong class="js iu">我的目标是在这两个分布的帮助下构建两个不同的二元分类器</strong>。</p><p id="a101" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">标准柯西分布</strong>的 pdf f(x)和 cdf F(x)为:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/668d7fedaac624ae202387d4f3709265.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*atgxwf1qyYonqFlw9O40qQ.png"/></div></figure><p id="e3ed" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">标准拉普拉斯分布</strong>的相同量为:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/a75d9eeaf6684df13a7646910418568b.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*3hE5bWJ7Scl9jt8xkzVh9A.png"/></div></figure><p id="1681" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 sgn(x)是这样的，如果 x 是负的，则 sgn(x)=-1，如果 x 是正的，则 sgn(x)=+1</p><p id="40c6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">两种分布都是关于 0 对称的。因此对于这两者，<strong class="js iu"> F(0)=0.5 </strong>和<strong class="js iu"> F(x)=1-F(-x) </strong>成立。两个 CDF 的图形类似于</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ln"><img src="../Images/9315b70fc76a8be83d01aba3619fdbdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckMnYg1vJoJ-pEnG2xqK7g.jpeg"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk"><strong class="bd lw">Red: Laplace distribution cdf. Blue: Cauchy distribution cdf</strong></figcaption></figure><blockquote class="ko"><p id="42bc" class="kp kq it bd kr ks lx ly lz ma mb kn dk translated">看起来很像 sigmoid 函数，不是吗？我希望现在你能看到为什么这两个函数是 logit 分类器的潜在替代者。</p></blockquote><p id="06db" class="pw-post-body-paragraph jq jr it js b jt ky jv jw jx kz jz ka kb la kd ke kf lb kh ki kj lc kl km kn im bi translated">由于我们的背景已经准备好，让我们投入分析。我们需要一个可以是 0 或 1 的响应变量 Y 和 p 个预测变量 X1，X2，…，Xp。假设观察总数为 N，并假设:</p><ol class=""><li id="a1e9" class="md me it js b jt ju jx jy kb mf kf mg kj mh kn mi mj mk ml bi translated">Yi 是独立的，并且每个 Yi 遵循具有参数<strong class="js iu"> pi </strong>的伯努利分布</li><li id="142b" class="md me it js b jt mm jx mn kb mo kf mp kj mq kn mi mj mk ml bi translated">预测因子 X1，X2，…Xp 是不相关的。</li></ol><p id="b69a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样-</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/894dc3f4155da813c4cedaa59a8bbc27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*KOzZ-lLtZh6WwWbUSq8LdA.png"/></div></figure><p id="33c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中β是维数为(p+1)×1 的参数向量，xi 是 1×(p+1)阶的第 I 个观测向量。那就是<strong class="js iu"><em class="ld">Xi =【1x1i x2i x3i…xpi】。</em> </strong></p><p id="a2d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在柯西分布的情况下，我们将使用-</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a3c9682480b52ce3b298d11829dcb0c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*vn7WdzzOBBRJKqyZqnZ7Cw.png"/></div></figure><p id="c08e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在拉普拉斯分布的情况下-</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/17526671c1fb4398b6f39599494c0d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*MdAI46UOdgw5XMuw_R2F7w.png"/></div></figure><p id="0109" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们为一般的 F(x)建立理论，在编写 R 代码时，我们将考虑特殊情况。(这意味着您也可以使用这个理论来理解和构建<strong class="js iu">概率单位模型</strong>😊😊)</p><p id="e1c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可能性函数是-</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/4cbf1d5b35fe3eb28a350bb105c476e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*cbHIZzSo7-UUFD0wAG2zZg.png"/></div></figure><p id="9f0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于分布关于 0 对称，F(x)=1-F(-x)成立。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a32df468dd20b69d47311914d1dc4c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*spMGZQbKIfHtM874J25zDA.png"/></div></figure><p id="cab6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们能简化一点吗？假设<strong class="js iu"> z=2y-1。</strong>所以当 y=1 那么 z=1，当 y=0 那么 z=-1。这将有助于以更方便的方式写出可能性。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ln"><img src="../Images/ece3f3cbfbc30637d8c4065c069fe21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YP15UjAKsm25e3i1PYL_dw.png"/></div></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mw"><img src="../Images/53f209a1bf06c14f9109cbb93af609db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bRj-Hf6-l4KbFnXKkTulKA.png"/></div></div></figure><p id="66d0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可能性函数现在可以归结为，</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mx"><img src="../Images/08689e1a514d234d2eb740552a29afd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*Vl__i_ikKutGi5FOaDARzQ.png"/></div></div></figure><p id="64d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似地，对数可能性为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/95f5f5f516f82577e853f8318a39e0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*u0Qg98lrwbSZlIKkX6wsuw.png"/></div></figure><p id="f5a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了训练我们的模型，我们需要通过<strong class="js iu">最大似然估计</strong>过程来估计未知参数。参数的最大似然是</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e6999711faefe9bf053cf518c2424520.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*zAzuz_EO-ZrxxEIb0nz8EA.png"/></div></figure><p id="2c42" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对数似然相对于参数向量的一阶导数为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi na"><img src="../Images/59c9bc18dad64ce122a8ff02a6b3fda6.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*Fk1NVWMyDByJhRUKVQjqvQ.png"/></div></figure><p id="b544" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中 f(x)是相应的 pdf。</p><p id="5786" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最大似然估计必须满足条件 D=0。但是求 D=0 的根并不是一件容易的事情，因为它没有封闭形式的解。我们可以借助不同的优化技术。我使用的是传统的牛顿-拉夫森方法。</p><p id="8e0e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这种方法使我们能够通过如下迭代找到方程 f(x)=0 的根</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ead2f7b2750091dfbbfdd6e3d965cb08.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*MAFliULcvlOknAbQ15hJ1Q.png"/></div></figure><p id="5c50" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">并且当两个连续步骤的输出之间的差异变得太小时，我们停止。在这种情况下，我们将使用</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9ad40a2c00be414035d347ba54cf193d.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*sdbELm2KM5kJ9PJSM6W1yA.png"/></div></figure><p id="b931" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们找到对数似然的二阶导数。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nd"><img src="../Images/76ded580741fbabedcb3682c3b711845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7AewVvLt1EUs5hEX69c1AQ.png"/></div></div></figure><p id="1c8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们正在处理多个预测值，因此相应的矩阵格式为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/892d820b8ca37a5bffae1ca155891dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*T173mnOIeEzL244tHXHfxQ.png"/></div></figure><p id="803e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在哪里，</p><ol class=""><li id="1538" class="md me it js b jt ju jx jy kb mf kf mg kj mh kn mi mj mk ml bi translated">x 是 N 阶 x (p+1)预测值的矩阵，第一列全为 1。</li><li id="c610" class="md me it js b jt mm jx mn kb mo kf mp kj mq kn mi mj mk ml bi translated">p 是 N×1 阶的列向量，其中第 I 个元素是 pi。</li><li id="4800" class="md me it js b jt mm jx mn kb mo kf mp kj mq kn mi mj mk ml bi translated">w 是 N×N 对角矩阵，其中第 I 个对角元素为</li></ol><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7894cd20f9c44f90146ffcbb30d579b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*sU9RHnZiIJJ270FGJs2SSA.png"/></div></figure><p id="28d4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">4.n 表示第 n 次迭代。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="d6ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">唷！！！😓😓这需要大量的数学运算。让我们现在开始编码。我在这里用 R。我还在代码中加入了概率单位分类器。在此之前，让我们手边准备好柯西分布和拉普拉斯分布的 pdf 和 cdf。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/668d7fedaac624ae202387d4f3709265.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*atgxwf1qyYonqFlw9O40qQ.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">pdf and cdf of Cauchy distribution</figcaption></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/a75d9eeaf6684df13a7646910418568b.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*3hE5bWJ7Scl9jt8xkzVh9A.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">pdf and cdf of Laplace distribution</figcaption></figure><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="fd13" class="ns nt it no b gy nu nv l nw nx">#At first lets define the functions for creating the pi values for given predictors and parameters</span><span id="15d4" class="ns nt it no b gy ny nv l nw nx">#x is the matrix of parameters, param is the vector of betas, response is the response variable</span><span id="484f" class="ns nt it no b gy ny nv l nw nx">#at first work with probit model</span><span id="7817" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">p_i_finder_probit=function(x,param,response){<br/>  n=length(response)<br/>  p_i=array(dim=1)               </strong>#initializing an array<strong class="no iu"><br/>  for(i in 1:nrow(x)){<br/>    val=0                        </strong>#temporary variable<strong class="no iu"><br/>    for(j in 1:ncol(x)){<br/>      val=val+x[i,j]*param[j]    </strong>#x[i,j]=ith value of jth predictor<strong class="no iu"><br/>    }<br/>    val=val*response[i]          </strong>#pnorm is the cdf of normal<strong class="no iu"><br/>    p_i[i]=dnorm(val,0,1)*response[i]/pnorm(val,0,1)<br/>  }                              </strong>#dnorm is pdf of normal<strong class="no iu"><br/>  return(p_i)                    </strong>#it will return the vector P<strong class="no iu"><br/>}</strong></span><span id="4aa5" class="ns nt it no b gy ny nv l nw nx">#lets define the pdf of standard Cauchy distribution<strong class="no iu"><br/>cauchy_pdf=function(x){        <br/>  return((1/(1+x^2))/pi)<br/>}</strong></span><span id="4728" class="ns nt it no b gy ny nv l nw nx"># similarly function to calculate the cdf of standard Cauchy distribution</span><span id="159a" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">cauchy_cdf=function(x){<br/>  return(0.5+atan(x)/pi)<br/>}</strong></span><span id="9614" class="ns nt it no b gy ny nv l nw nx"># similarly finding the P column vector for Cauchy classifier</span><span id="13ca" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">p_i_finder_cauchy=function(x,param,response){<br/>  n=length(response)<br/>  p_i=array(dim=1)<br/>  for(i in 1:nrow(x)){<br/>    val=0<br/>    for(j in 1:ncol(x)){<br/>      val=val+x[i,j]*param[j]<br/>    }<br/>    val=val*response[i]<br/>    p_i[i]=cauchy_pdf(val)*response[i]/cauchy_cdf(val)<br/>  }<br/>  return(p_i)<br/>}</strong></span><span id="9ffe" class="ns nt it no b gy ny nv l nw nx"># function to calculate the pdf of Laplace distribution</span><span id="554a" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">laplace_pdf=function(x){<br/>  return(exp(-abs(x))/2)<br/>}</strong></span><span id="43fc" class="ns nt it no b gy ny nv l nw nx"># function to calculate the cdf of Laplace distribution</span><span id="22e6" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">laplace_cdf=function(x){<br/>  return(0.5+0.5*sign(x)*(1-exp(-abs(x))))<br/>}</strong></span><span id="22ba" class="ns nt it no b gy ny nv l nw nx"># pi values under Laplace classifier</span><span id="5582" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">p_i_finder_laplace=function(x,param,response){<br/>  n=length(response)<br/>  p_i=array(dim=1)<br/>  for(i in 1:nrow(x)){<br/>    val=0<br/>    for(j in 1:ncol(x)){<br/>      val=val+x[i,j]*param[j]<br/>    }<br/>    val=val*response[i]<br/>    p_i[i]=laplace_pdf(val)*response[i]/laplace_cdf(val)<br/>  }<br/>  return(p_i)<br/>}</strong></span><span id="6d19" class="ns nt it no b gy ny nv l nw nx">#now lets write the function for constructing the W matrix<br/># as input we need the pi value, the matrix of predictors and the parameters</span><span id="4af4" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">W_matrix_finder=function(p_i,x,param){<br/>  wi=array(dim=1)<br/>  for(i in 1:nrow(x)){<br/>    val=0<br/>    for(j in 1:ncol(x)){<br/>      val=val+x[i,j]*param[j]<br/>    }<br/>    wi[i]=p_i[i]*(val+p_i[i])<br/>  }<br/>  W_matrix=diag(wi)     </strong>#diagonal matrix with ith diagonal=wi<strong class="no iu"><br/>  return(W_matrix)      </strong>#returning the matrix<strong class="no iu"><br/>}</strong></span><span id="3040" class="ns nt it no b gy ny nv l nw nx">#finally creating own function equivalent to glm function<br/># as input it will take predictor variables, response variable, the precision for the stopping criteria of Newton Raphson method<br/>and which classifier to use: probit or cauchy or laplace</span><span id="48c3" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">own_classifier=function(predictor,response,precision,type){<br/>  predictor_new=as.matrix(predictor)  </strong>#to be on safe side<strong class="no iu"><br/>  distinct=sort(unique(as.numeric(response)),decreasing=FALSE)<br/>  response=as.numeric(response)       </strong>#to be on safest side :)<strong class="no iu"><br/>  response_new=array(dim=1)<br/>  for(i in 1:length(response)){<br/>    if(response[i]==distinct[1])<br/>      response_new[i]=-1         </strong>#instead of 0-1 encoding, making<strong class="no iu"><br/>    else                          </strong>it -1 and 1 for simplicity<strong class="no iu"><br/>      response_new[i]=1<br/>  }<br/>  constant=rep(1,length(response_new)) </strong>#1st column with all values=1<strong class="no iu"><br/>  X_matrix=cbind(constant,predictor_new)<br/>  beta=rep(0,ncol(X_matrix))     </strong>#initializing the parameters<strong class="no iu"><br/>  if(type=="cauchy"){            </strong>#based on mentioned classifier<strong class="no iu"><br/>    dif=100                    </strong>#R does not have do while loop :(<strong class="no iu"><br/>    while(dif&gt;precision){<br/>      p_i=p_i_finder_cauchy(X_matrix,beta,response_new)<br/>      W_matrix=W_matrix_finder(p_i,X_matrix,beta)<br/>  updated=solve(t(X_matrix)%*%W_matrix%*%X_matrix)%*%t(X_matrix)%*%p_i<br/>      beta=beta+updated       </strong>#updating beta<strong class="no iu"><br/>      dif=sum(updated^2)</strong>#Euclidean distance between old and new beta<strong class="no iu"><br/>    }<br/>  }<br/>  else if(type=="probit"){    </strong># for probit model<strong class="no iu"><br/>    dif=100<br/>    while(dif&gt;precision){<br/>      p_i=p_i_finder_probit(X_matrix,beta,response_new)<br/>      W_matrix=W_matrix_finder(p_i,X_matrix,beta)<br/>      updated=solve(t(X_matrix)%*%W_matrix%*%X_matrix)%*%t(X_matrix)%*%p_i<br/>      beta=beta+updated<br/>      dif=sum(updated^2)<br/>    }<br/>  }<br/>  else if(type=="laplace"){   </strong>#for laplace classifier<strong class="no iu"><br/>    while(dif&gt;precision){<br/>      p_i=p_i_finder_laplace(X_matrix,beta,response_new)<br/>      W_matrix=W_matrix_finder(p_i,X_matrix,beta)<br/>      updated=solve(t(X_matrix)%*%W_matrix%*%X_matrix)%*%t(X_matrix)%*%p_i<br/>      beta=beta+updated<br/>      dif=sum(updated^2)<br/>    }<br/>  }<br/>  return(beta)    </strong>#returning final parameters<strong class="no iu"><br/>}</strong></span></pre><p id="2ab4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们的模特训练完成了。让我们在一些任意数据集上应用，并与内置的 glm 函数进行比较。</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="bcd8" class="ns nt it no b gy nu nv l nw nx"># I am creating own random dataset containing 2 predictors.</span><span id="196f" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">predictor1=rbeta(100,2,4)</strong>#random sample of size 100 from beta(2,4)<br/><strong class="no iu">predictor2=rpois(100,10) </strong>#rs from poisson(10)<br/><strong class="no iu">predictor=cbind(predictor1,predictor2)<br/>response=sample(c(0,1),100,replace=T)<br/>data=as.data.frame(cbind(predictor1,predictor2,response))</strong></span></pre><p id="a532" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据看起来像这样:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2100ec709286054d40b17c2d1aabc1be.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*6dNogMShCBye2mYfk5ePiA.png"/></div></figure><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="ba14" class="ns nt it no b gy nu nv l nw nx">#train-test split. I am using 80-20 ratio.<br/><strong class="no iu">samples=sample(1:nrow(data),size=nrow(data)*0.80,replace=F)<br/>data_train=data[samples,]    </strong>#train data<strong class="no iu"><br/>data_test=data[-samples,]     </strong>#test data</span><span id="db96" class="ns nt it no b gy ny nv l nw nx">#probit model training using inbuilt glm<br/><strong class="no iu">inbuilt=glm(response~predictor1+predictor2,data=data_train,family=binomial(link="probit"))<br/>inbuilt$coefficients</strong></span></pre><p id="f96a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oa"><img src="../Images/2eedb7bdf4edb4d2c56a57ac7406b75e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ZMpE890U4J5NeY6zZb3Lw.png"/></div></div></figure><p id="0a8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们看看我们的函数是如何运行的，</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ob"><img src="../Images/1c908375648d082bd72c2f6629bf0de8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hsXm_g5bxM1lDQvfFDYcAA.png"/></div></div></figure><p id="153b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">挺好的！！！！！！不是吗？最多 5 位小数，使用 glm 函数是正确的。</p><p id="f4ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们将应用柯西和拉普拉斯分类器。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oc"><img src="../Images/ae3cd8cb857e7dc078aae83d3355b60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61CLxkB-kbsoKr23x4F5DA.png"/></div></div></figure><p id="6dd0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是如何衡量它们作为分类器的性能呢？我们将看到他们在测试数据上的表现。为此，让我们构建一个类似于 r 中的<strong class="js iu">预测</strong>函数的函数。</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="936b" class="ns nt it no b gy nu nv l nw nx">#as input it takes the model outputs, test data predictors and the type of classifier to use.<strong class="no iu"><br/>fitted=function(model,test_data_predictor,type){<br/>  predictors=as.matrix(test_data_predictor)<br/>  constant=rep(1,nrow(predictors))<br/>  X_matrix=cbind(constant,predictors)<br/>  pred=array(dim=1)<br/>  if(type=="probit"){<br/>    for(i in 1:nrow(X_matrix)){<br/>      val=0<br/>      for(j in 1:ncol(X_matrix)){<br/>        val=val+X_matrix[i,j]*model[j]<br/>      }<br/>      pred[i]=pnorm(val,0,1) </strong>#cdf of standard normal as inverse link<strong class="no iu"><br/>    }<br/>  }<br/>  else if(type=="cauchy"){<br/>    for(i in 1:nrow(X_matrix)){<br/>      val=0<br/>      for(j in 1:ncol(X_matrix)){<br/>        val=val+X_matrix[i,j]*model[j]<br/>      }<br/>      pred[i]=cauchy_cdf(val)</strong>#cdf of standard Cauchy as inverse link<strong class="no iu"><br/>    }<br/>  }<br/>  else if(type=="laplace"){<br/>    for(i in 1:nrow(X_matrix)){<br/>      val=0<br/>      for(j in 1:ncol(X_matrix)){<br/>        val=val+X_matrix[i,j]*model[j]<br/>      }<br/>      pred[i]=laplace_cdf(val)</strong>#cdf of standard Laplace as inverse <br/>                                link<br/>    <strong class="no iu">}<br/>  }<br/>  return(pred)<br/>}</strong></span></pre><p id="72b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后是对比时间。拟合的概率将判断我们创建的分类器如何表现。手指交叉！！！！！</p><pre class="lf lg lh li gt nn no np nq aw nr bi"><span id="43ef" class="ns nt it no b gy nu nv l nw nx">#probit model using glm function<strong class="no iu"><br/>inbuilt=glm(response~predictor1+predictor2,data=data_train,family=binomial(link="probit"))</strong></span><span id="9d5d" class="ns nt it no b gy ny nv l nw nx">#probit model using our own code</span><span id="ff24" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">model1=own_classifier(data_train[,1:2],data_train[,3],0.000000000000000001,"probit")</strong></span><span id="4996" class="ns nt it no b gy ny nv l nw nx"># Cauchy classifier using our own code</span><span id="55b0" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">model2=own_classifier(data_train[,1:2],data_train[,3],0.000000000000000001,"cauchy")</strong></span><span id="8130" class="ns nt it no b gy ny nv l nw nx"># Laplace classifier using our own code</span><span id="49c2" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">model3=own_classifier(data_train[,1:2],data_train[,3],0.000000000000000001,"laplace")</strong></span><span id="6113" class="ns nt it no b gy ny nv l nw nx">#fitted probabilities based on our probit classifier</span><span id="0044" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">my_probit=fitted(model1,data_test[,1:2],"probit")</strong></span><span id="ec16" class="ns nt it no b gy ny nv l nw nx">#fitted probabilities based on our Cauchy classifier</span><span id="2c11" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">my_cauchy=fitted(model2,data_test[,1:2],"cauchy")</strong></span><span id="c12d" class="ns nt it no b gy ny nv l nw nx">#fitted probabilities based on our Laplace classifier</span><span id="b9f7" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">my_laplace=fitted(model3,data_test[,1:2],"laplace")</strong></span><span id="ee76" class="ns nt it no b gy ny nv l nw nx">#fitted probabilities based on probit model through inbuilt glm</span><span id="c918" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">r_probit=predict(inbuilt,data_test[,1:2],type="response")</strong></span><span id="1d47" class="ns nt it no b gy ny nv l nw nx"><strong class="no iu">cbind(r_probit,my_probit,my_cauchy,my_laplace)</strong></span></pre><p id="8d59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">输出是:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9f68e97e6b2167da016bbe235cecd7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*xZDNLfesgavgMQxyTbKi1g.png"/></div></figure><p id="9aac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">答对了。！！！！！！！！🤩🤩。我们创造的所有模型表现几乎一样。glm 概率单位模型和我们的概率单位模型的拟合值精确到小数点后 6 位。其他两个分类器也给出了与概率单位模型相似的拟合值。</p><blockquote class="ko"><p id="91e3" class="kp kq it bd kr ks kt ku kv kw kx kn dk translated">恭喜你！！！！！！现在，您不仅知道了一些新的链接函数，还知道了如何自己使用它们开发模型。</p></blockquote></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="7c48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这就是逻辑三部曲的结尾。要想在任何领域大放异彩，有三件事非常非常重要。<strong class="js iu">想象力、创造力和创新。</strong></p><blockquote class="oe of og"><p id="3aec" class="jq jr ld js b jt ju jv jw jx jy jz ka oh kc kd ke oi kg kh ki oj kk kl km kn im bi translated">想象力是无限的。你可以想象任何事情，任何事情。但是如果你把逻辑和想象混合起来，你就会创造出新的东西。这就是创造力。创造新的非传统事物。如果你把创意和创造结合起来，你就会开始创新。</p></blockquote><p id="648f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据科学家需要具备这三个素质。这三个都是通过这个三部曲呈现的。</p><p id="16e5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一部分<a class="ae ok" rel="noopener" target="_blank" href="/logistic-regression-derived-from-intuition-d1211fc09b10"> <strong class="js iu">逻辑回归——源自直觉</strong> </a> <strong class="js iu"> </strong>将帮助你通过纯粹的逻辑和想象得出逻辑分布的想法。</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/logistic-regression-derived-from-intuition-d1211fc09b10"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">逻辑回归——源自直觉</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">让我们通过一个故事从头开始推导逻辑回归。我希望这将是有趣和好玩的…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc lk oo"/></div></div></a></div><p id="d5af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第二部分<a class="ae ok" rel="noopener" target="_blank" href="/building-own-logistic-classifier-in-r-logistic-trilogy-part-2-a36be209d2c">在 R 中构建自己的逻辑分类器</a>将帮助你在 R 中创建自己的逻辑分类器函数。这将使你能够创造你自己的东西。</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/building-own-logistic-classifier-in-r-logistic-trilogy-part-2-a36be209d2c"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">在 R 中构建自己的逻辑分类器[逻辑三部曲，第 2 部分]</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一篇关于如何在不使用内置函数的情况下用 R 构建逻辑分类器的独立文章。它会凝固…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc lk oo"/></div></div></a></div><p id="7716" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我希望这最后一部分能让你跳出框框思考。你们很多人都用过 logit 或 probit 分类器。但是你有没有想过还有其他的链接功能存在呢？我们能从 logit 和 probit 中想出点什么吗？你猜怎么着！！！！！！他们有很多。<strong class="js iu">你所需要的是一个连续的概率分布，它定义在整个实线上</strong>。就拿<strong class="js iu"> t 分布</strong>本身来说。通过改变其自由度，您可以创建几个链接函数，并检查其在不同数据集上的性能。深入思考。深入算法。不要只是用算法，从算法开始创新。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="79d6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你不相信，或者有任何疑问或建议，请在评论区提问，或者通过我的 LinkedIn 个人资料联系我。</p><div class="ol om gp gr on oo"><a href="https://www.linkedin.com/in/soumalya-nandi-95176569/" rel="noopener  ugc nofollow" target="_blank"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">SOUMALYA NANDI -联合健康组织(L2)助理数据科学家| LinkedIn</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">查看 SOUMALYA NANDI 在全球最大的职业社区 LinkedIn 上的个人资料。SOUMALYA 有 4 份工作列在…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">www.linkedin.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc lk oo"/></div></div></a></div></div></div>    
</body>
</html>