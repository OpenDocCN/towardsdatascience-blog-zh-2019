<html>
<head>
<title>Reinforcement Learning — Implement TicTacToe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习—实施 TicTacToe</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542?source=collection_archive---------3-----------------------#2019-05-19">https://towardsdatascience.com/reinforcement-learning-implement-tictactoe-189582bea542?source=collection_archive---------3-----------------------#2019-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0616" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">两个代理人游戏介绍</h2></div><p id="a087" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们通过迭代更新<em class="lb"> Q </em>值函数，即<code class="fe lc ld le lf b">(state, action)</code>对的估计值，实现了网格世界游戏。这一次，我们来看看如何在对抗性游戏中利用强化学习——井字游戏，其中有更多的状态和动作，最重要的是，有一个对手与我们的代理人对抗。(查看<a class="ae lg" href="https://medium.com/@zhangyue9306/implement-grid-world-with-q-learning-51151747b455" rel="noopener">之前的帖子</a>)</p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="lu lv di lw bf lx"><div class="gh gi lo"><img src="../Images/87b2e942d433b2607c7a3e781f1cf03c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_mSVXTq-tar_Zr0c"/></div></div></figure><h1 id="b503" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">强化学习的优势</h1><p id="6c06" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">然而，在一般的博弈论方法中，比如最小-最大算法，算法总是假设一个完美的对手是如此理性，以至于它采取的每一步都是为了最大化它的回报和最小化我们的代理人回报，在强化学习中，它甚至没有假设对手的模型，结果可能会出乎意料地好。</p><p id="dc00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过将对手视为智能体可以与之交互的环境的一部分，在一定数量的迭代之后，智能体能够在没有智能体或环境的任何模型的情况下提前计划，或者对可能的未来动作或状态进行任何搜索。优点是显而易见的，因为该方法省去了复杂的数学推导或探索大量搜索空间的努力，但是它能够通过简单的尝试和学习来达到最先进的技能。</p><p id="46da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的会议中，我们将:</p><ul class=""><li id="c79f" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated">首先，训练两个代理相互对战并保存他们的策略</li><li id="52e4" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated">第二，加载策略，让代理人扮演人类</li></ul><h1 id="2175" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">状态设置</h1><p id="54d6" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">首先，我们需要一个州级机构来充当董事会和法官。它具有记录双方玩家棋盘状态的功能，并在任何一方玩家采取行动时更新状态。同时，它能够判断游戏的结束，并相应地给予玩家奖励。(点击 查看<a class="ae lg" href="https://github.com/MJeremy2017/RL/blob/master/TicTacToe/ticTacToe.py" rel="noopener ugc nofollow" target="_blank"> <em class="lb">代码)</em></a></p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a12e0379aae8712749f61c9995511c52.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*cvD8GrXg1FiDgvuux5ML9A.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk">tic-tac-toe board</figcaption></figure><p id="051f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了阐明这个强化学习问题，最重要的是要清楚 3 个主要组成部分——状态<strong class="kh ir">，行动，和奖励</strong>。这个游戏的状态是代理人和它的对手的棋盘状态，所以我们将<strong class="kh ir">初始化一个 3×3 的棋盘，用 0 表示可用的位置，如果玩家 1 移动，用 1 更新位置，如果玩家 2 移动，用-1 更新位置</strong>。动作是玩家根据当前棋盘状态可以选择的位置。奖励在 0 到 1 之间，只在游戏结束时给出。</p><h2 id="61ef" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">初始化</h2><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="fcb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe lc ld le lf b">init</code>功能中，我们初始化一个空棋盘和两个玩家<code class="fe lc ld le lf b">p1</code>和<code class="fe lc ld le lf b">p2</code>(我们初始化<code class="fe lc ld le lf b">p1</code>以先玩)。每个玩家都有一个<code class="fe lc ld le lf b">playSymbol</code>，当玩家采取一个动作时，它的<code class="fe lc ld le lf b">playerSymbol</code>会被填入棋盘并更新棋盘状态。</p><h2 id="7ecf" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">董事会状态</h2><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="f363" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lc ld le lf b">getHash</code>函数对当前电路板状态进行哈希运算，以便将其存储在状态值字典中。</p><p id="1004" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当玩家采取行动时，其对应的符号将被填入棋盘。并且在状态被更新后，棋盘还会更新当前棋盘上的空位，并依次反馈给下一个玩家。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="2b05" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">检查赢家</h2><p id="2925" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">在玩家采取每一个动作后，我们需要一个函数来持续检查游戏是否已经结束，如果结束，判断游戏的赢家，并给予双方玩家奖励。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="5adb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lc ld le lf b">winner</code>函数检查行、列和对角线的总和，如果<code class="fe lc ld le lf b">p1</code>赢则返回 1，如果<code class="fe lc ld le lf b">p2</code>赢则返回-1，如果平局则返回 0，如果游戏尚未结束则返回<code class="fe lc ld le lf b">None</code>。在游戏结束时，1 奖励给赢家，0 奖励给输家。需要注意的一点是，我们认为平局也是一个糟糕的结局，所以我们给我们的代理人<code class="fe lc ld le lf b">p1</code> 0.1 的奖励，即使游戏是平局(可以尝试不同的奖励，看看代理人如何行动)。</p><h1 id="21c8" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">播放器设置</h1><p id="606e" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">我们需要一个代表我们代理的玩家类，玩家能够:</p><ol class=""><li id="829b" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la oe nd ne nf bi translated">基于状态的当前估计选择动作</li><li id="0076" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la oe nd ne nf bi translated">记录游戏的所有状态</li><li id="23a8" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la oe nd ne nf bi translated">每场比赛后更新状态值估计</li><li id="0e3a" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la oe nd ne nf bi translated">保存并加载策略</li></ol><h2 id="2cbc" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">初始化</h2><p id="aa7a" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">我们将初始化一个<code class="fe lc ld le lf b">dict</code>存储状态值对，并在每局游戏结束时更新估计值。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="664a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<code class="fe lc ld le lf b">init</code>函数中，我们在列表<code class="fe lc ld le lf b">self.states</code>中记录玩家在每场比赛中的所有位置，并在<code class="fe lc ld le lf b">self.states_value</code> dict 中更新相应的状态。在行动选择方面，我们使用ϵ-greedy 方法来平衡探索和开发。这里我们设置了<code class="fe lc ld le lf b">exp_rate=0.3</code>，意思是<code class="fe lc ld le lf b">ϵ=0.3</code>，所以 70%的时间我们的代理将采取贪婪的行动，这是基于状态值的当前估计选择行动，30%的时间我们的代理将采取随机行动。</p><h2 id="a6a7" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">选择操作</h2><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="6a4a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将板状态的散列存储到状态值字典中，并且在利用时，我们散列下一个板状态并选择返回下一个状态的最大值的动作。</p><h2 id="c2cd" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">状态值更新</h2><p id="d3ce" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">为了更新状态的值估计，我们将应用基于以下公式更新的值迭代</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="ab gu cl of"><img src="../Images/9db1e1bbf6ee1d5dfc6727747c280780.png" data-original-src="https://miro.medium.com/v2/format:webp/1*DZcvRVaNyQ34pAhk4wwUDQ.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk">Value Iteration(From Reinforcement Learning an Introduction)</figcaption></figure><p id="0d73" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">公式简单的告诉我们<strong class="kh ir"> <em class="lb">状态 t 的更新值等于状态 t 的当前值加上下一个状态的值和当前状态的值之差，再乘以一个学习率α(假设中间状态的奖励为 0) </em> </strong>。</p><p id="1c4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">逻辑是我们基于我们最新的观察慢慢更新当前值。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4007" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我上面提到的，每场比赛的位置存储在<code class="fe lc ld le lf b">self.states</code>中，当代理到达比赛结束时，估计值以<code class="fe lc ld le lf b">reversed</code>的方式更新。</p><h2 id="9d4d" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">培养</h2><p id="5e85" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">既然我们的代理能够通过更新价值评估来学习，并且我们的板都设置好了，那么是时候让两个玩家互相对战了(<strong class="kh ir">这个部分放在 State 类</strong>中)。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="f387" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练期间，每个球员的过程是:</p><ul class=""><li id="ede0" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated">寻找空缺职位</li><li id="5447" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated">选择操作</li><li id="cebe" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated">更新棋盘状态并将动作添加到玩家状态</li><li id="05bd" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated">判断游戏是否结束，并给予相应的奖励</li></ul><h2 id="1fd0" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">保存和加载策略</h2><p id="eb43" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">在训练结束时(在一定数量的回合之后玩)，我们的代理能够学习它的策略，它存储在状态值字典中。我们需要保存这个策略来对抗人类玩家。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><h1 id="b6d9" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">人类 VS 计算机</h1><p id="c1e1" class="pw-post-body-paragraph kf kg iq kh b ki ms jr kk kl mt ju kn ko mu kq kr ks mv ku kv kw mw ky kz la ij bi translated">现在我们的代理已经设置好了，在最后一步，我们需要一个人类类来管理与代理的游戏。</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="a0ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个类只包含了一个可用的函数<code class="fe lc ld le lf b">chooseAction</code>，它要求我们输入我们想要的棋盘位置。</p><p id="da9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而且我们还需要修改一下<code class="fe lc ld le lf b">play</code>函数内部的状态:</p><figure class="lp lq lr ls gt lt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="d08b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大致相同，我们让 1 号玩家(也就是我们的代理人)先玩，每走一步，棋盘就印好了。</p><h2 id="9a84" class="nq mb iq bd mc nr ns dn mg nt nu dp mk ko nv nw mm ks nx ny mo kw nz oa mq ob bi translated">玩得开心！</h2><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/9690bf91696c1e26d9d3d7ae460603c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*UllTzyeBvsUzRcr1Frv9qA.png"/></div><figcaption class="nm nn gj gh gi no np bd b be z dk">Play against human</figcaption></figure><p id="70b7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lc ld le lf b">play2</code>功能我们显示棋盘状态，并要求你在游戏过程中输入你的位置。</p><p id="da9f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自己去试试，玩得开心！(点击 查看<a class="ae lg" href="https://github.com/MJeremy2017/RL/blob/master/TicTacToe/ticTacToe.py" rel="noopener ugc nofollow" target="_blank"> <em class="lb">代码)</em></a></p><p id="e4eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">参考:</p><ul class=""><li id="a422" class="mx my iq kh b ki kj kl km ko mz ks na kw nb la nc nd ne nf bi translated"><a class="ae lg" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></li><li id="a655" class="mx my iq kh b ki ng kl nh ko ni ks nj kw nk la nc nd ne nf bi translated"><a class="ae lg" href="https://github.com/JaeDukSeo/reinforcement-learning-an-introduction" rel="noopener ugc nofollow" target="_blank">https://github . com/JaeDukSeo/enforcement-learning-an-introduction</a></li></ul></div></div>    
</body>
</html>