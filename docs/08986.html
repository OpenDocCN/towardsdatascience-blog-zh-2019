<html>
<head>
<title>Understanding Dimensionality Reduction for Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解机器学习的降维</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-dimensionality-reduction-for-machine-learning-ad9a3811bd89?source=collection_archive---------14-----------------------#2019-11-30">https://towardsdatascience.com/understanding-dimensionality-reduction-for-machine-learning-ad9a3811bd89?source=collection_archive---------14-----------------------#2019-11-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/f3edc222c73d61e896090799bb00f888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RFByZj-yLxhCShGp"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@oliverschwendener?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Oliver Schwendener</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5392" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近，我被要求处理一个有点偏重的数据集。它太大了，以至于我的 Excel 程序在加载时会停止响应几分钟，手动浏览数据开始变得非常痛苦。</p><p id="218a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我还是机器学习界的新手。因此，我不得不多次返回 Excel，以决定在机器学习模型中应该使用哪些功能。</p><p id="7f2a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，在经历了漫长而痛苦的时间后，我终于能够建立一个能给我一些不错的结果的模型了！我开始自我感觉良好，并开始相信我可以成为一名机器学习工程师！</p><p id="13b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但后来我遇到了下一个障碍:我如何向普通人展示我的成果？</p><p id="a068" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我可以创建两列，一列显示真实世界的结果，另一列显示 ML 模型给我的结果。但这并不是视觉上最吸引人的方式。虽然机器在理解大量数字数据方面比我们更好，但人类思维更容易理解图形和绘图等视觉数据。</p><p id="76ef" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以剧情是最好的前进方式。但是我们的屏幕是二维的，我们的数据可以有两个以上的特征，每个特征可以被认为是一个维度。这个图的两个维度之一将会是输出。因此，我们应该问自己的问题是:我们如何在一个维度中表示我们所有的特征，这些特征可能有数百个到数千个。</p><p id="d379" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是降维发挥作用的地方！</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="a9d5" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">什么是降维？</h1><p id="1de5" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">降维是机器学习中的一种技术，可以减少数据集中的要素数量。降维的好处在于，它不会对你的机器学习模型的性能产生负面影响。在某些情况下，这种技术甚至提高了模型的准确性。</p><p id="4bd7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过减少数据集中的要素数量，我们也减少了存储数据所需的存储空间，我们的 python 编译器将需要更少的时间来遍历数据集。</p><p id="aaf4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本帖中，我们将看看两种最流行的降维技术，主成分分析(PCA)和线性判别分析(LDA)。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="cd1d" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">入门指南</h1><p id="47c8" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">在我们继续之前，让我们确保您的系统已经准备好使用一些 python 代码。</p><p id="24c1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Python 有两个主要版本:2.x 和 3.x。我建议您确保在系统上安装了 3.x 版本。如果不是这样，那么你可以从<a class="ae kf" href="https://www.python.org/downloads/" rel="noopener ugc nofollow" target="_blank">这里</a>得到。</p><p id="306d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们需要一个好的代码编辑器，它将有助于使您的编码体验更加容易和愉快。我个人的选择是惊艳的<a class="ae kf" href="https://code.visualstudio.com/" rel="noopener ugc nofollow" target="_blank"> VSCode </a>，不过你也可以试试<a class="ae kf" href="https://www.spyder-ide.org/" rel="noopener ugc nofollow" target="_blank"> Spyder </a>或者<a class="ae kf" href="https://jupyter.org/" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>。还有一些在线代码编辑器，比如<a class="ae kf" href="https://repl.it/languages/python3" rel="noopener ugc nofollow" target="_blank"> Repl </a>和<a class="ae kf" href="http://colab.research.google.com/" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>你可以试试。使用在线编辑器可以让你跳过安装 Python 库的浪费时间和存储的任务。</p><p id="f4c5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将致力于葡萄酒数据集。这个数据集完全是数值型的，不包含任何缺失值，非常适合本文，因为我不必在数据预处理技术上浪费时间，比如将分类数据转换成数值数据，以及将数据输入空单元格。</p><p id="890e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从导入一些脚本中需要的 Python 库开始:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e6e5" class="mx lm it mt b gy my mz l na nb">import pandas as pd<br/>from sklearn.preprocessing import StandardScaler</span></pre><p id="7c42" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还需要将葡萄酒数据集加载到我们的 python 脚本中。让我们使用来自<code class="fe nc nd ne mt b">pandas</code>库的<code class="fe nc nd ne mt b">read_csv()</code>函数来实现:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ddb9" class="mx lm it mt b gy my mz l na nb">dataset_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'</span><span id="24a2" class="mx lm it mt b gy nf mz l na nb">dataset = pd.read_csv(dataset_url, sep=';')</span></pre><p id="dbb8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集看起来像这样:</p><figure class="mo mp mq mr gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/73d5642153b7b972185054334faf70e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXlcAIMBjIYWZrKjt4FDGg.png"/></div></div></figure><p id="979f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne mt b">quality</code>列将是我们机器学习模型的因变量，其余列将是自变量。因此，让我们相应地将数据集分成两部分:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="10d2" class="mx lm it mt b gy my mz l na nb">X = dataset.iloc[:, 0:-1].values<br/>y = dataset.iloc[:, -1].values</span></pre><p id="693e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nc nd ne mt b">X</code>包含 11 列，<code class="fe nc nd ne mt b">y</code>包含第 12 列。</p><p id="27f8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还需要对数据进行一些特征缩放。我们使用<code class="fe nc nd ne mt b">StandardScalar</code>函数来完成这项工作。该函数将以这样一种方式转换我们的数据，即数据的分布将具有平均值 0 和标准差 1。我们将对<code class="fe nc nd ne mt b">independent</code>变量进行特征缩放，因为我们希望<code class="fe nc nd ne mt b">dependent</code>变量尽可能保持原样。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8e4d" class="mx lm it mt b gy my mz l na nb">sc = StandardScaler()<br/>X = sc.fit_transform(X)</span></pre><p id="27c5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在准备执行降维！先从 PCA 开始，再进行 LDA。我建议创建一个新的 python 脚本文件，将您的代码复制到其中，稍后在 LDA 部分使用它。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="f41f" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">主成分分析</h1><p id="034e" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">主成分分析(PCA)是最流行的降维算法之一，也可用于噪声过滤、股市预测、数据可视化等等。</p><p id="d940" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 通过检测数据集中特征之间的相关性来减少特征的数量。当特征之间的相关性足够强时，它们将合并在一起并形成单个特征。</p><p id="0de0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看一看算法背后的数学原理，并试图理解它是如何工作的，但我发现进入代码并查看我们得到的输出更容易。</p><p id="d02c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上一节我们留下代码的地方继续，让我们从从<code class="fe nc nd ne mt b">sklearn</code>导入<code class="fe nc nd ne mt b">PCA</code>类开始:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4a5a" class="mx lm it mt b gy my mz l na nb">from sklearn.decomposition import PCA</span></pre><p id="2805" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们需要创建一个<code class="fe nc nd ne mt b">PCA</code>类的本地实例。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ff98" class="mx lm it mt b gy my mz l na nb">pca = PCA(n_components=None)</span></pre><p id="e491" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们正在传递一个名为<code class="fe nc nd ne mt b">n_components</code>的参数，我们将首先传递一个值<code class="fe nc nd ne mt b">None</code>。</p><p id="163f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将调用<code class="fe nc nd ne mt b">pca</code>上的<code class="fe nc nd ne mt b">fit_transform</code>方法，并将<code class="fe nc nd ne mt b">X_train</code>数据集作为输入传递给它。我们还将调用<code class="fe nc nd ne mt b">transform</code>方法，并将<code class="fe nc nd ne mt b">X_test</code>数据集作为输入传递</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="95f6" class="mx lm it mt b gy my mz l na nb">X = pca.fit_transform(X)</span></pre><p id="bdb3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦这样做了，我们需要得到数据集不同主成分的解释方差。我们的<code class="fe nc nd ne mt b">pca</code>对象有一个名为<code class="fe nc nd ne mt b">explained_variance_ratio_</code>的属性，它包含一个百分比数组。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="3436" class="mx lm it mt b gy my mz l na nb">variances = pca.explained_variance_ratio_</span></pre><p id="f450" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您打印这个变量，您将得到一个类似如下的数组:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="71f7" class="mx lm it mt b gy my mz l na nb">[0.28173931 0.1750827  0.1409585  0.11029387 0.08720837 0.05996439  0.05307193 0.03845061 0.0313311  0.01648483 0.00541439]</span></pre><p id="6be9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以看到这个数组的元素是降序排列的。但是这些价值观是什么意思呢？每个值都解释了数据集中 11 个主成分的方差。</p><p id="5684" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，如果只取第一个主成分，我们将有 28.17%的方差。如果我们取两个主成分，我们将得到数组前两个元素的和，其方差约为 45.68%，依此类推。</p><p id="f27a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们可以使用 Python 编译器，将传递给<code class="fe nc nd ne mt b">PCA</code>内部<code class="fe nc nd ne mt b">n_components</code>的<code class="fe nc nd ne mt b">None</code>值替换为期望的维数。让我们看看选择<code class="fe nc nd ne mt b">1</code>作为所需的维度数量会得到什么</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="99db" class="mx lm it mt b gy my mz l na nb">pca = PCA(n_components=2)<br/>X = pca.fit_transform(X)</span><span id="6560" class="mx lm it mt b gy nf mz l na nb">// Output<br/>[[-1.61952988]  <br/> [-0.79916993]  <br/> [-0.74847909]  <br/> ...  <br/> [-1.45612897]  <br/> [-2.27051793]  <br/> [-0.42697475]]</span></pre><p id="3253" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">瞧啊。我们已经成功地将输入数据集的维数从 11 减少到 1！</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="42fd" class="ll lm it bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">线性判别分析(LDA)</h1><p id="92c4" class="pw-post-body-paragraph kg kh it ki b kj mj kl km kn mk kp kq kr ml kt ku kv mm kx ky kz mn lb lc ld im bi translated">与 PCA 类似，LDA 也是一种降维算法。但与 PCA 不同的是，LDA 也将找到最大化多个类别之间的分离的特征。</p><p id="3e68" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会问自己:为什么我要为这个新算法费心，这个新算法基本上和以前的算法做同样的事情？</p><p id="81b6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 和 LDA 都是降维算法。但是在 PCA 被认为是无监督算法的情况下，LDA 则被认为是有监督的。</p><p id="e0e8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有监督和无监督？这些是常见的机器学习术语，分别定义算法是否使用因变量。所以 LDA 使用因变量，而 PCA，正如我们之前看到的，不使用。</p><p id="ab1a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看一些代码，并理解如何在 python 代码中实现 LDA 算法。</p><p id="6e96" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您还没有这样做，那么从入门部分获取代码，并将其粘贴到一个新的 python 脚本文件中。然后，从<code class="fe nc nd ne mt b">sklearn</code>导入<code class="fe nc nd ne mt b">LinearDiscriminantAnalysis</code>类。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="0076" class="mx lm it mt b gy my mz l na nb">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis</span></pre><p id="dac0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们从<code class="fe nc nd ne mt b">LinearDiscriminantAnalysis</code>创建一个名为<code class="fe nc nd ne mt b">lda</code>的本地对象。我们还会给它传递一个名为<code class="fe nc nd ne mt b">n_components</code>的参数。现在我们可以做所有的事情，找到每个主成分引入的方差的百分比。但是我将跳过这一部分，将<code class="fe nc nd ne mt b">n_components</code>参数设置为<code class="fe nc nd ne mt b">1</code>的值。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="8e8c" class="mx lm it mt b gy my mz l na nb">lda = LinearDiscriminantAnalysis(n_component=1)</span></pre><p id="c9ab" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后一件事！在<code class="fe nc nd ne mt b">lda</code>上调用<code class="fe nc nd ne mt b">fit</code>方法，将<code class="fe nc nd ne mt b">X</code>和<code class="fe nc nd ne mt b">y</code>数据集作为输入传递给它，然后链接一个<code class="fe nc nd ne mt b">transform</code>方法，将<code class="fe nc nd ne mt b">X</code>作为其输入。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="28ed" class="mx lm it mt b gy my mz l na nb">X = lda.fit(X,y).transform(X)</span><span id="aaad" class="mx lm it mt b gy nf mz l na nb">// Output<br/>[[-1.51304437]  <br/> [-1.28152255]  <br/> [-1.11875163]  <br/> ...  <br/> [ 0.71637389]  <br/> [-0.32030997]  <br/> [ 0.8645305 ]]</span></pre><p id="e658" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Tada！🎉我们现在已经成功地将<code class="fe nc nd ne mt b">X</code>的尺寸减小到 1！</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><p id="123e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢你阅读这篇关于降维的文章。我希望它对你学习机器的旅程有一点点帮助。降维算法真的很棒，不仅有助于可视化您的结果，还有助于减少处理时间，并通过防止系统读取大型数据集来减轻系统压力。</p></div></div>    
</body>
</html>