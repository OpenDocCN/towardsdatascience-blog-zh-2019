<html>
<head>
<title>TF IDF | TFIDF Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF IDF | TFIDF Python 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76?source=collection_archive---------0-----------------------#2019-05-05">https://towardsdatascience.com/natural-language-processing-feature-engineering-using-tf-idf-e8b9d00e7e76?source=collection_archive---------0-----------------------#2019-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/27186455a12385f9adfde0e8ca650a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*scI12lmPInJQKAnm"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@raphaelphotoch?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Raphael Schaller</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="d6a3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自然语言处理(NLP)是人工智能的一个子领域，处理理解和处理人类语言。鉴于机器学习的新进展，许多组织已经开始将自然语言处理应用于翻译、聊天机器人和候选人筛选。</p><p id="0758" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不再拖延，让我们深入一些代码。首先，我们将导入必要的库。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="a8b4" class="ln lo jj lj b gy lp lq l lr ls">import pandas as pd<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span></pre><p id="147f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们将使用两个简单的文档，每个文档包含一个句子。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="ea58" class="ln lo jj lj b gy lp lq l lr ls">documentA = 'the man went out for a walk'<br/>documentB = 'the children sat around the fire'</span></pre><p id="58a0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">机器学习算法不能直接处理原始文本。相反，文本必须转换成数字向量。在自然语言处理中，从文本中提取特征的一种常见技术是将文本中出现的所有单词放在一个桶中。这种方法简称为<strong class="ki jk">袋字</strong>模型或<strong class="ki jk">弓</strong>。它被称为一个<strong class="ki jk"> <em class="lt">【袋】</em> </strong>的单词，因为任何关于句子结构的信息都丢失了。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="6d6d" class="ln lo jj lj b gy lp lq l lr ls">bagOfWordsA = documentA.split(' ')<br/>bagOfWordsB = documentB.split(' ')</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/cfa5130250736487a49ce2e8e582e3c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*crrh3ebmn5px4QxxB6FNDg.png"/></div></figure><p id="3c8d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过将单词包转换为一个集合，我们可以自动删除任何重复的单词。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="2775" class="ln lo jj lj b gy lp lq l lr ls">uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))</span></pre><p id="270a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将为语料库(文档集合)中的每个文档创建一个单词及其出现次数的字典。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="ab4c" class="ln lo jj lj b gy lp lq l lr ls">numOfWordsA = dict.fromkeys(uniqueWords, 0)</span><span id="6233" class="ln lo jj lj b gy lv lq l lr ls">for word in bagOfWordsA:<br/>    numOfWordsA[word] += 1</span><span id="5456" class="ln lo jj lj b gy lv lq l lr ls">numOfWordsB = dict.fromkeys(uniqueWords, 0)</span><span id="74dc" class="ln lo jj lj b gy lv lq l lr ls">for word in bagOfWordsB:<br/>    numOfWordsB[word] += 1</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/42a8779790f27041bed7d2495520b557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*d098MGUO188Uwd_wOWWNgg.png"/></div></figure><p id="c221" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词袋方法的另一个问题是它没有考虑噪音。换句话说，某些单词被用来构成句子，但没有给文本增加任何语义。例如，英语中最常用的单词是<strong class="ki jk"><em class="lt"/></strong>，它代表了所有书面或口头单词的 7%。考虑到文本中包含单词<strong class="ki jk">和</strong>，你无法对文本做出任何推断。另一方面，像<strong class="ki jk"> good </strong>和<strong class="ki jk"> awesome </strong>这样的词可以用来确定一个评级是否是正面的。</p><p id="be6d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在自然语言处理中，无用词被称为停用词。python <strong class="ki jk">自然语言工具包</strong>库提供了一个英文停用词列表。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="d920" class="ln lo jj lj b gy lp lq l lr ls">from nltk.corpus import stopwords</span><span id="17c1" class="ln lo jj lj b gy lv lq l lr ls">stopwords.words('english')</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lx"><img src="../Images/bc576bf85a3b3f14a769a125163dc7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zck_tUbnkwzIbCQCUr7Oiw.png"/></div></div></figure><p id="091f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，当以理解文本为目标构建模型时，您会看到所有停用词都被删除了。另一种策略是使用 TF-IDF 对单词的相对重要性进行评分。</p><h2 id="e689" class="ln lo jj bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated"><strong class="ak">词频(TF) </strong></h2><p id="7573" class="pw-post-body-paragraph kg kh jj ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">一个单词在文档中出现的次数除以文档中的总单词数。每个文档都有自己的词频。</p><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/000840f1cf63682a9d1d48251a52dd55.png" data-original-src="https://miro.medium.com/v2/format:webp/1*HM0Vcdrx2RApOyjp_ZeW_Q.png"/></div></figure><p id="0768" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下代码在 python 中实现了词频。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="ad50" class="ln lo jj lj b gy lp lq l lr ls">def computeTF(wordDict, bagOfWords):<br/>    tfDict = {}<br/>    bagOfWordsCount = len(bagOfWords)<br/>    for word, count in wordDict.items():<br/>        tfDict[word] = count / float(bagOfWordsCount)<br/>    return tfDict</span></pre><p id="f3c6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面几行计算了我们每个文档的词频。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="f0ad" class="ln lo jj lj b gy lp lq l lr ls">tfA = computeTF(numOfWordsA, bagOfWordsA)<br/>tfB = computeTF(numOfWordsB, bagOfWordsB)</span></pre><h2 id="0891" class="ln lo jj bd ly lz ma dn mb mc md dp me kr mf mg mh kv mi mj mk kz ml mm mn mo bi translated"><strong class="ak">逆数据频率(IDF) </strong></h2><p id="3506" class="pw-post-body-paragraph kg kh jj ki b kj mp kl km kn mq kp kq kr mr kt ku kv ms kx ky kz mt lb lc ld im bi translated">日志中的文档数除以包含单词<strong class="ki jk"> <em class="lt"> w </em> </strong>的文档数。逆数据频率决定了语料库中所有文档中稀有词的权重。</p><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/60fabff650c6033680d00e65a5c94477.png" data-original-src="https://miro.medium.com/v2/format:webp/1*A5YGwFpcTd0YTCdgoiHFUw.png"/></div></figure><p id="14a5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下代码在 python 中实现了反向数据频率。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="4b3b" class="ln lo jj lj b gy lp lq l lr ls">def computeIDF(documents):<br/>    import math<br/>    N = len(documents)<br/>    <br/>    idfDict = dict.fromkeys(documents[0].keys(), 0)<br/>    for document in documents:<br/>        for word, val in document.items():<br/>            if val &gt; 0:<br/>                idfDict[word] += 1<br/>    <br/>    for word, val in idfDict.items():<br/>        idfDict[word] = math.log(N / float(val))<br/>    return idfDict</span></pre><p id="33d1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有文件的 IDF 计算一次。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="067a" class="ln lo jj lj b gy lp lq l lr ls">idfs = computeIDF([numOfWordsA, numOfWordsB])</span></pre><p id="c8d7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，TF-IDF 就是 TF 乘以 IDF。</p><figure class="le lf lg lh gt iv gh gi paragraph-image"><div class="ab gu cl mu"><img src="../Images/3857462cc042cd90a582ee58f3c67dc5.png" data-original-src="https://miro.medium.com/v2/format:webp/1*nSqHXwOIJ2fa_EFLTh5KYw.png"/></div></figure><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="4d34" class="ln lo jj lj b gy lp lq l lr ls">def computeTFIDF(tfBagOfWords, idfs):<br/>    tfidf = {}<br/>    for word, val in tfBagOfWords.items():<br/>        tfidf[word] = val * idfs[word]<br/>    return tfidf</span></pre><p id="3265" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们可以计算语料库中所有单词的 TF-IDF 分数。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="d540" class="ln lo jj lj b gy lp lq l lr ls">tfidfA = computeTFIDF(tfA, idfs)<br/>tfidfB = computeTFIDF(tfB, idfs)</span><span id="cd15" class="ln lo jj lj b gy lv lq l lr ls">df = pd.DataFrame([tfidfA, tfidfB])</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/200b365554ed0e9b3738881044a154c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CxgfMxV99iZmRFFFSKOgfQ.png"/></div></div></figure><p id="4701" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用 sklearn 提供的类，而不是自己手动实现 TF-IDF。</p><pre class="le lf lg lh gt li lj lk ll aw lm bi"><span id="8c6d" class="ln lo jj lj b gy lp lq l lr ls">vectorizer = TfidfVectorizer()</span><span id="7efd" class="ln lo jj lj b gy lv lq l lr ls">vectors = vectorizer.fit_transform([documentA, documentB])</span><span id="cf61" class="ln lo jj lj b gy lv lq l lr ls">feature_names = vectorizer.get_feature_names()</span><span id="652f" class="ln lo jj lj b gy lv lq l lr ls">dense = vectors.todense()</span><span id="8ff6" class="ln lo jj lj b gy lv lq l lr ls">denselist = dense.tolist()</span><span id="45bd" class="ln lo jj lj b gy lv lq l lr ls">df = pd.DataFrame(denselist, columns=feature_names)</span></pre><figure class="le lf lg lh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/30c77dfea3b51c1f0597d15553f208e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDbZeEM2eN6vIbmqLp6hbA.png"/></div></div></figure><p id="91e3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些值略有不同，因为 sklearn 使用了平滑版本的 idf 和各种其他小优化。在具有更多文本的示例中，单词<strong class="ki jk"><em class="lt"/></strong>的得分将大大降低。</p></div></div>    
</body>
</html>