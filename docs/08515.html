<html>
<head>
<title>Probability Calibration for Imbalanced Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不平衡数据集的概率校准</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probability-calibration-for-imbalanced-dataset-64af3730eaab?source=collection_archive---------6-----------------------#2019-11-18">https://towardsdatascience.com/probability-calibration-for-imbalanced-dataset-64af3730eaab?source=collection_archive---------6-----------------------#2019-11-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6b28" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对欠采样方法的一点建议</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/afaaa6e8844d84f46df2237f94e772e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aVlGKl6J1hEyekOQ"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@bk010397?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Bharathi Kannan</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d400" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们试图为现实世界的问题建立机器学习模型时，我们经常会面临不平衡的数据集。重采样方法，尤其是欠采样是克服类别不平衡最广泛使用的方法之一(我在另一篇<a class="ae ky" href="https://medium.com/analytics-vidhya/can-we-predict-a-price-adjustment-in-an-online-supermarket-by-using-machine-learning-and-7bf3e8fff81a?source=friends_link&amp;sk=f43dc2cb1805864cf37c58deb237f8b1" rel="noopener">中的</a>文章中也展示了那些重采样方法是如何在<a class="ae ky" href="https://www.researchgate.net/publication/324749650_Analyzing_online_price_by_using_machine_learning_techniques" rel="noopener ugc nofollow" target="_blank">我的硕士论文</a>中起作用的)。然而，由于训练和测试集中不同的类别分布，实现这种方法倾向于增加假阳性。这会使分类器产生偏差并增加假阳性。[1] <a class="ae ky" href="https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf" rel="noopener ugc nofollow" target="_blank"> Pozzolo 等人，(2015) </a>认为可以利用贝叶斯最小风险理论来修正由于欠采样而产生的偏差。这有助于我们找到正确的分类阈值。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8b97" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">内容</h1><ol class=""><li id="adf0" class="mu mv it lb b lc mw lf mx li my lm mz lq na lu nb nc nd ne bi translated">概率校准是如何工作的？</li><li id="99ac" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">实验</li><li id="b640" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu nb nc nd ne bi translated">结论</li></ol></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="71a0" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">1.概率校准是如何工作的？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/a2cd4393daf9db3abbae554ae8e434c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iXXeDAwF_qfYcdQg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@johnschno?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">John Schnobrich</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="0de0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，欠采样会导致后验概率出现偏差。这是由于随机欠采样的特性，其通过随机移除它们来缩小多数类，直到两个类具有相同数量的观察值。这使得训练集的类分布不同于测试集中的类分布。那么，在这个问题上，使用贝叶斯最小风险理论的概率校准究竟是如何工作的呢？—这种方法的基本思想是通过考虑欠采样率<em class="nl"> β </em>考虑<em class="nl">来试图减少/消除随机欠采样率引起的偏差。</em>让我们来看看一些定义:</p><p id="0710" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设<em class="nl"> ps </em>为随机欠采样后预测为正类的概率；</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/73003b92950470e378cbe8846e831011.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*AnkhDVTLcIMA8MBGczv3Kw.png"/></div></figure><p id="a250" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">，而<em class="nl"> p </em>是预测给定特征的概率(不平衡)。我们可以把<em class="nl"> ps 写成 p 的函数；</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/8c5fb3fefca5ba19a2dfeeefd38fe623.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*Qp7pxPIDLM4740DHTUFEDw.png"/></div></figure><p id="1f1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">，其中<em class="nl"> β </em>为欠采样情况下选择负类的概率，可表示如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/fc97fb200f16e59d3885b12da8477cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*q7BNiwCiySHLeh6_wpTsUg.png"/></div></figure><p id="52c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">，可以这样写</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a4bec9f4ded3015933877e843e393147.png" data-original-src="https://miro.medium.com/v2/resize:fit:234/format:webp/1*0N0FtZJztIqUodH0fmvUzw.png"/></div></figure><p id="6dc5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上式可解出<em class="nl"> p </em>并表示如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/a41e02ed5733c3f1f92d5c8d8332692d.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*w-VK4WWmFxE5Gb25BhEY3g.png"/></div></figure><p id="8cb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，在应用欠采样率<em class="nl"> β、</em>之后，我们可以计算出<em class="nl"> p </em>，这是无偏概率。</p><p id="1f55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个阈值可以是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/9f9f12a90b2e941ead23ee5adfad0004.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*dd58BiKT3muw3LX3e6zEbA.png"/></div></figure><p id="2ca9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">，这是数据集中正类的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/db7b600020f45578c9c69f8d35454960.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/format:webp/1*2neW8brsdxrUx5lExaVA0g.png"/></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="e9bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简要介绍了利用贝叶斯最小风险理论进行概率校准的方法。现在我们将继续看看这是如何在一个代码示例中工作的。</p><h1 id="dac5" class="mc md it bd me mf nt mh mi mj nu ml mm jz nv ka mo kc nw kd mq kf nx kg ms mt bi translated">2.例子</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/98321dadf1d18e1982df334f6cc6cc55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FHofPNpqNpjkLPvl"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@nate_dumlao?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nathan Dumlao</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f9fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本节中，我们将看到概率校准技术如何在 Kaggle 上著名的<a class="ae ky" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">信用卡欺诈数据集</a>上对二进制分类问题进行建模。该数据集由 28 个 PCA 特征(全部匿名)和数量特征组成。目标特征是二元的，要么是诈骗，要么不是。正类占整个数据集的 0.17%，严重不平衡。让我们看看这个例子的代码。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="5501" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，导入包。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="5bd3" class="oe md it oa b gy of og l oh oi">## config<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import io, os, sys, types, gc, re<br/>from sklearn import preprocessing<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import average_precision_score,<br/>confusion_matrix, precision_score, recall_score, precision_recall_curve, f1_score, log_loss<br/>from sklearn.decomposition import PCA<br/>pca = PCA(n_components=1,random_state=42)<br/>from imblearn.under_sampling import RandomUnderSampler<br/>rus = RandomUnderSampler(random_state=42)<br/>from imblearn.ensemble import BalancedBaggingClassifier<br/>from src.functionalscripts.BMR import *</span><span id="30fb" class="oe md it oa b gy oj og l oh oi">def make_prediction(model,X,threshold):<br/>    y_pred = model.predict_proba(X)<br/>    y_predicted = np.where(y_pred[:,1]&gt;=threshold,1,0)<br/>    return y_pred, y_predicted</span><span id="79bf" class="oe md it oa b gy oj og l oh oi">def evaluation(true, pred):    <br/>    print('F1-score: ' + str(round(f1_score(true,pred),4)), '\n'<br/>'Precision: ' + str(round(precision_score(true,pred),4)), '\n'<br/>'Recall: ' + str(round(recall_score(true,pred),4)), '\n'<br/>'Log loss: ' + str(round(log_loss(true,pred),4)), '\n'<br/>'Cohen-Kappa: ' + str(round(cohen_kappa_score(true,pred),4)), '\n'<br/>'Confusion matrix:' + '\n' + str(confusion_matrix(true,pred)))</span></pre><p id="d20c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在读取数据集。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="f38a" class="oe md it oa b gy of og l oh oi"># read the data<br/>df = pd.read_csv('src/resources/data/creditcard.csv')</span></pre><p id="b982" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是数据集的前几行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/1528799eda0bd22272de854941c6bcfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*c9hb_61dZ3ynqRXs_fWcOQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">First few rows of the dataset</figcaption></figure><p id="02da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Class 列是我们的目标变量，Amount 列是交易金额。现在看正班的比例。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="c7c7" class="oe md it oa b gy of og l oh oi"># The percentage of positive class in this dataset<br/>len(df[df['Class']==1])/len(df)</span></pre><p id="75c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正面类就是我上面说的，0.17%。现在我们将继续参观模型建筑。我们将使用逻辑回归来看看。让我们将 Amount 列标准化，并分成训练和测试数据集。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="a8e3" class="oe md it oa b gy of og l oh oi"># Normalise the Amount feature<br/>df['amt'] = preprocessing.normalize(np.array(df['Amount']).reshape(-1,1),<br/>norm='l2')</span></pre><p id="49cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，数据准备好了，让我们分成训练(整个数据的 80%)和测试(整个数据的 20%)数据集，并实现随机欠采样。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="9fdc" class="oe md it oa b gy of og l oh oi"># Split the dataset into train and test, and drop unnecessary features<br/>tr, te = train_test_split(df.drop(['Amount','Time'],1), test_size=.2,random_state=42)</span><span id="53be" class="oe md it oa b gy oj og l oh oi"># Rundom Under Sampling (RUS)<br/>tr_x_rus, tr_y_rus = rus.fit_resample(tr.drop(['Class'],1),tr.Class)</span></pre><p id="dd92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以在随机欠采样后检查训练、测试和训练集中类的分布。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="d910" class="oe md it oa b gy of og l oh oi"># See the class distribution with features<br/>feats_distribution_rus = pd.DataFrame(pca.fit_transform(tr_x_rus),columns=['after']).reset_index(drop=True)<br/>feats_distribution_rus['Class'] = tr_y_rus</span><span id="737f" class="oe md it oa b gy oj og l oh oi">sns.regplot(x='after',y='Class',data=feats_distribution_rus,logistic=True, n_boot=500, y_jitter=.03)<br/>plt.title('Class distribution of training set after undersampling')<br/>plt.show()</span></pre><div class="kj kk kl km gt ab cb"><figure class="ol kn om on oo op oq paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/faaa5183d0ec95a44e5ecb9124703e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*IkIZuAoJ-XwKO36iFw_uCA.png"/></div></figure><figure class="ol kn om on oo op oq paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/63a0819a25e0301c02ed62619443451a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mdUuG2CruAuF02OefMFazg.png"/></div></figure></div><div class="ab cb"><figure class="ol kn om on oo op oq paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/84f914a488eba86d7556383a7d60f040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CoHOdqEYtTNV2K0oECP-Ig.png"/></div></figure><figure class="ol kn om on oo op oq paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/63a0819a25e0301c02ed62619443451a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*mdUuG2CruAuF02OefMFazg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk or di os ot">Class distributions of before and after undersampling and test set comparison</figcaption></figure></div><p id="3ced" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">左上角是训练集的分布，左下角和右边是随机欠采样后的分布，都是测试集(用于比较)。y 轴表示类别，1 表示交易是欺诈，否则不是欺诈。我们可以在训练集和测试集中看到类似的分布，因为它们是通过随机采样创建的。另一方面，在随机欠采样后的训练集中，显示出与其他分布完全不同的分布。让我们看看这种差异是如何影响分类器的。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="0423" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在建立模型。为了比较，让我们看看 RUS 装袋(随机欠采样+装袋)的性能。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="14fb" class="oe md it oa b gy of og l oh oi"># Logistic regression<br/>logit = LogisticRegression(random_state=42,solver='lbfgs',<br/>max_iter=1000)</span><span id="154d" class="oe md it oa b gy oj og l oh oi"># Rundom Under Sampling (RUS)<br/>tr_x_rus, tr_y_rus = rus.fit_resample(tr.drop(['Class'],1),tr.Class)<br/>logit_rus = logit.fit(tr_x_rus,tr_y_rus)</span><span id="ec31" class="oe md it oa b gy oj og l oh oi"># RUS bagging<br/>bc = BalancedBaggingClassifier(base_estimator=logit,random_state=42)<br/>logit_bc = bc.fit(tr.drop(['Class'],1),tr.Class)</span></pre><p id="2de5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们使用贝叶斯最小风险来实现概率校准方法。这里我们创建 beta(少数选择比率)、tau(阈值)和校准函数。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="9de4" class="oe md it oa b gy of og l oh oi"># BMR (Bayes Minimum Risk) implementation<br/># Pozzolo et al., 2015, Calibrating Probability with Undersampling</span><span id="837c" class="oe md it oa b gy oj og l oh oi">class BMR:<br/>    def beta(binary_target):<br/>        return binary_target.sum()/len(binary_target)</span><span id="5e9f" class="oe md it oa b gy oj og l oh oi">    def tau(binary_target, beta):<br/>        return binary_target.sum()/len(binary_target)</span><span id="e3a9" class="oe md it oa b gy oj og l oh oi">    def calibration(prob, beta):<br/>        return prob/(prob+(1-prob)/beta)</span></pre><p id="d9bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将这些校准技术应用于 RUS 和 RUS·鲍格的预测概率。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="3dba" class="oe md it oa b gy of og l oh oi"># Calibration<br/>beta = BMR.beta(tr.Class)<br/>tau = BMR.tau(tr.Class,beta)</span><span id="3f4a" class="oe md it oa b gy oj og l oh oi"># with RUS<br/>y_pred_calib_rus = BMR.calibration(prob=logit_rus.predict_proba(te.drop(['Class'],1))[:,1],beta=beta)</span><span id="b0a5" class="oe md it oa b gy oj og l oh oi">y_predicted_calib_rus = np.where(y_pred_calib_rus&gt;=tau,1,0)</span><span id="5427" class="oe md it oa b gy oj og l oh oi"># wtih RUS bagging<br/>y_pred_calib_bc = BMR.calibration(prob=logit_bc.predict_proba(te.drop(['Class'],1))[:,1],beta=beta)</span><span id="1dd6" class="oe md it oa b gy oj og l oh oi">y_predicted_calib_bc = np.where(y_pred_calib_bc&gt;=tau,1,0)</span></pre><p id="dcd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了所有的预测，让我们评估它们，看看它们的表现如何。我将 RUS 和 RUS 装袋模型的阈值设为 0.5。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="3115" class="oe md it oa b gy of og l oh oi"># Evaluation<br/>## Random Under Sampling (RUS)<br/>y_pred_rus, y_predicted_rus = make_prediction(model=logit_rus, X=te.drop(['Class'],1), threshold=.5)<br/>evaluation(te.Class, y_predicted_rus)</span><span id="88cf" class="oe md it oa b gy oj og l oh oi">## RUS + Bagging<br/>y_pred_bc, y_predicted_bc = make_prediction(model=logit_bc, X=te.drop(['Class'],1), threshold=.5)<br/>evaluation(te.Class, y_predicted_bc)</span><span id="bff8" class="oe md it oa b gy oj og l oh oi">## Calibration with Rundom undersampling<br/>evaluation(te.Class, y_predicted_calib_rus)</span><span id="57ba" class="oe md it oa b gy oj og l oh oi">## Calibration with RUS bagging<br/>evaluation(te.Class, y_predicted_calib_bc)</span></pre><p id="4c4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是结果。正如我们所料，有这么多的误报，这一点我们可以从精度得分中看出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/07ede2841a0a368a3c3f1653081b16fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aBEeSnCO_9exVoVLIU3MDg.png"/></div></div></figure><p id="39a0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">哦，等等，概率校准根本没有改变什么？一定有什么不对劲。让我们看看校准后的 RUS 装袋预测概率的预测分布。蓝色垂直线是通过校准的 RUS 装袋预测概率的平均值，红色垂直线是通过 RUS 装袋模型预测概率的平均值。显然，它们的平均值相差很远，因为校准后的概率平均值为 0.0021，而校准前为 0.5。考虑到正类在整个数据集中占 0.17%，校准后的概率似乎非常接近实际分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/aa69b696f9248ffcfc66b766337ae721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tf_Pq0yJPchFW70_LhEOfw.png"/></div></div></figure><p id="686b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果我们修改阈值，效果应该会更好，这里我们可以看到修改阈值后的结果。在 RUS 模型上校准之前和校准之后的阈值被设置为 0.99，并且用 RUS 装袋的校准被设置为 0.8。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/bad8586beb729f26b27dce140e89a7ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOOBory5LmWUG1hZiehh9w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Summary of results after thresholds are modified</figcaption></figure><p id="5a3b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，在校准后，这些分数得到了提高，尤其是在随机欠采样模型上校准前后的差异非常显著。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="6c4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，通过使用概率校准来校正有偏差的概率，我们可以看到性能的提高。</p><p id="3225" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3。结论和想法</strong></p><p id="455a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博文中，我们浏览了 Pozzolo 等人(2015)。这回答了我们的经验，即应用随机欠采样后会有更多的假阳性。观察重采样方法如何使分布产生偏差是非常有趣的。这将导致时间序列问题中的类似问题，因为我们预测的目标变量可能与我们训练模型时大不相同。</p><p id="f446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看看这种方法是否适用于不同类型的分类器(如基于树的分类器或神经网络分类器)以及不同类型的重采样方法，会很有意思。</p><p id="d995" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的代码可以在我的<a class="ae ky" href="https://github.com/kyosek/Probability_Calibration_Imbalanced" rel="noopener ugc nofollow" target="_blank"> GitHub 页面</a>上找到。</p><h1 id="f3c9" class="mc md it bd me mf nt mh mi mj nu ml mm jz nv ka mo kc nw kd mq kf nx kg ms mt bi translated">包裹</h1><ul class=""><li id="61de" class="mu mv it lb b lc mw lf mx li my lm mz lq na lu ox nc nd ne bi translated">引入重采样会导致后验分布出现偏差</li><li id="ad4b" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu ox nc nd ne bi translated">利用贝叶斯最小风险理论引入概率校准方法(Pozzoli，et al. 2015)</li><li id="b498" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu ox nc nd ne bi translated">展示了这种方法的例子</li><li id="f07c" class="mu mv it lb b lc nf lf ng li nh lm ni lq nj lu ox nc nd ne bi translated">证实了该方法纠正了偏差，改善了模型结果</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="6c02" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><p id="52ed" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li oy lk ll lm oz lo lp lq pa ls lt lu im bi translated">[1] Pozzolo 等，<a class="ae ky" href="https://www3.nd.edu/~dial/publications/dalpozzolo2015calibrating.pdf" rel="noopener ugc nofollow" target="_blank">不平衡分类欠采样的概率校准</a> (2015)，2015 IEEE 计算智能研讨会系列</p></div></div>    
</body>
</html>