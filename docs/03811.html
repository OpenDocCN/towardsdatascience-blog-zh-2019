<html>
<head>
<title>The game of Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正规化的游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-game-of-regularization-91442b3be862?source=collection_archive---------14-----------------------#2019-06-16">https://towardsdatascience.com/the-game-of-regularization-91442b3be862?source=collection_archive---------14-----------------------#2019-06-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f9e8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">机器学习中使用的不同正则化技术的直观解释</h2></div><p id="158a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在机器学习中，正则化是一种通过在代价函数中添加惩罚项来解决过拟合问题的方法。我们先来了解一下，</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/a6a4a0e6f0a978e380a622ec1417a18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Mfty5lYtifzq6Pp0.jpg"/></div></div></figure><p id="1848" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">什么是过度拟合，为什么它是一个问题。</p><p id="0665" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在解决机器学习问题时，我们首先用训练数据训练我们的模型，最后概括解决方案(测试机器学习模型学习的概念在多大程度上适用于模型学习时看不到的特定数据点)。如果我们的模型太复杂，就会导致</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/ed20beca0caf63098dbfa0e107f5bd8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*S6LZ2e1aEWxl_vg3"/></div></figure><p id="c7c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">较高的测试误差，尽管训练误差要低得多。但一个自然的问题是<strong class="kh ir"> <em class="lo">为什么是</em> </strong>？<strong class="kh ir"> <em class="lo">什么叫复杂型号</em> </strong> <em class="lo">？</em>让我们以<em class="lo"> </em>为例。</p><p id="0816" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们正在用逻辑回归模型解决一个简单的二元分类问题。让我们红色点是 A 类，蓝色点是 b 类。现在</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/bd8ea001777410b61bcba828dddfd004.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*HGmUTZiiCxHj-mY-.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">source : wikepedia</figcaption></figure><p id="b70b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们试图完美地拟合我们的决策边界，也有可能我们也会拟合噪声点(潜在的异常值)。结果，模型将不会学习关于数据的适当模式(并且对于较小的数据集，它将仅仅记忆所有点)，这可能导致更高的概括(读取测试)误差。</p><p id="d06b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了避免模型中的过度拟合，我们添加了一个正则化项，这将有助于减少泛化误差。让我们列出我们拥有的各种选择:</p><ol class=""><li id="0592" class="lu lv iq kh b ki kj kl km ko lw ks lx kw ly la lz ma mb mc bi translated"><strong class="kh ir"> L1 正则化</strong>:如果我们看代价函数，L1 正则化是系数的绝对值之和。简单地说，添加它是为了保持值</li></ol><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/66c45e8e77ea08feec53eb322639150f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*4MlW1d3xszVAGuXiJ1U6Fg.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">cost function with L1 regularization</figcaption></figure><p id="ee3c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在一定范围内的成本函数。使用 l1 正则化的主要优点是它在解决方案中产生稀疏性(解决方案的大多数系数为零)，这意味着不太重要的特征或噪声项将为零。它使 l1 正则化对异常值具有鲁棒性。</p><p id="0aeb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.<strong class="kh ir"> L2 正则化</strong>:在 L2 正则化的情况下，我们添加以下的平方值</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="ab gu cl md"><img src="../Images/1ea20f0f93a036fbf0cfae33715c869b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jgWOhDiGjVp-NCSPa5abmg.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">cost function with L2 regularization</figcaption></figure><p id="31c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">系数而不是绝对值。在这两种方法中，lambda 是一个超参数，用于调整正则化。就结果而言，l1 和 l2 之间的主要区别在于，对于不太重要的特征，l2 项的系数将非常低，但决不会精确为零。</p><p id="89eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.<strong class="kh ir">弹性网</strong>:当 l1 和 l2 正则项一起作为罚项时，称为弹性网正则化。</p><p id="859f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">太酷了。但是<strong class="kh ir"> <em class="lo">如何选择最适合我的正规化</em> </strong>？这主要取决于手头的问题。我们总是可以借助交叉验证来为特定问题选择性能更好的方法。对于大维度数据，l1 通过为不太重要的特征赋予零系数来提供更好的特征可解释性。</p><h1 id="e311" class="me mf iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">结束注释:</h1><p id="e45a" class="pw-post-body-paragraph kf kg iq kh b ki mw jr kk kl mx ju kn ko my kq kr ks mz ku kv kw na ky kz la ij bi translated">在本文中，我们讨论什么是过拟合，以及我们如何使用正则化来最小化泛化误差。在广泛的机器学习中，你会发现这只不过是一个偏差方差权衡的游戏，模型拟合得越好，结果就越好。希望这篇文章能帮助你掌握基本知识。快乐学习！</p><p id="9d31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献:</strong></p><p id="81e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想深潜，请点击下面的链接</p><ol class=""><li id="e445" class="lu lv iq kh b ki kj kl km ko lw ks lx kw ly la lz ma mb mc bi translated"><a class="ae nb" href="http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/" rel="noopener ugc nofollow" target="_blank">http://enhancedatascience . com/2017/07/04/machine-learning-explained-regulatory/</a></li><li id="4b69" class="lu lv iq kh b ki nc kl nd ko ne ks nf kw ng la lz ma mb mc bi translated"><a class="ae nb" href="https://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization" rel="noopener ugc nofollow" target="_blank">https://developers . Google . com/machine-learning/速成课程/正则化稀疏化/l1 正则化</a></li><li id="3070" class="lu lv iq kh b ki nc kl nd ko ne ks nf kw ng la lz ma mb mc bi translated"><a class="ae nb" href="https://stats.stackexchange.com/questions/184019/when-will-l1-regularization-work-better-than-l2-and-vice-versa" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/184019/when-will-L1-regulation-work-better-L2-and-反之亦然</a></li></ol></div></div>    
</body>
</html>