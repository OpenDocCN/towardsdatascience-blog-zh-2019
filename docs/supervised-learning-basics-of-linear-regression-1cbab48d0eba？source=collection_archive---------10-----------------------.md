# 监督学习:线性回归基础

> 原文：<https://towardsdatascience.com/supervised-learning-basics-of-linear-regression-1cbab48d0eba?source=collection_archive---------10----------------------->

![](img/2ee0c32b6e5564c2991d53055d357456.png)

Picture from [Unsplash](https://unsplash.com/photos/3RQnQyyzA9c)

# 1.介绍

回归分析是监督机器学习的一个子领域。它旨在模拟一定数量的特征和连续目标变量之间的关系。

在回归问题中，我们试图给出一个定量的答案，比如预测房子的价格或某人看视频的秒数。

# 2.简单线性回归:通过数据拟合直线

有了一组点，回归算法将对单个特征(解释变量 x)和连续值响应(目标变量 y)之间的关系进行建模。

该模型将通过设置一条任意线并计算从这条线到数据点的距离来模拟这种关系。这个距离，垂直线，是残差或预测的误差。

回归算法将在每次迭代中不断移动直线，试图找到最合适的直线，换句话说，就是误差最小的直线。

有几种技术来执行这项任务，最终目标是使线更接近最大点数。

## **2.1 移动线**

![](img/910cbe8492da61c67fcdbbfea57fffc6.png)

Figure by the Author

绝对的诡计

当有一个点和一条线时，目标是让线更靠近这个点。为了完成这项任务，算法将使用一个称为“学习率”的参数。该学习率是将与函数参数相乘的数字，以便在逼近直线到点时进行小步进。

换句话说，学习率将决定每次迭代中使直线更接近该点的距离长度。它通常被表示为α符号。

![](img/debe9d4b3cef34671e0070f60e875a34.png)

2.1.2 方块戏法

它基于以下前提:如果有一个点更靠近一条线，并且距离很小，那么这条线就移动一点距离。如果远的话，线会移动很多。

![](img/f0c43e6372e12d15b6468c758560a107.png)

# **3。梯度下降**

假设我们有一组点，我们想开发一种算法，找到最适合这组点的直线。如前所述，误差是从直线到点的距离。

移动直线并计算误差。这个过程一遍又一遍地重复，每次减少一点点误差，直到获得完美的线条。这条完美的线将是误差最小的线。为了减小这种误差，我们将使用梯度下降法。

梯度下降法是一种方法，对于每一步，该方法将观察线可以移动的不同方向，以减少误差，并采取措施减少大部分误差。

维基百科中梯度的定义:

> escalar 场(f)的梯度是一个向量场。当它在 f 的定义域的一般点上求值时，它表明了 f 的快速方差的方向。

所以梯度下降会朝着负梯度的方向前进一步。

![](img/7aab6aff760c7d3efe98902003eb4967.png)

Figure by Author

当该算法采取足够的步骤时，如果学习率被设置为适当的值，它将最终达到局部或全局最小值。这是一个非常重要的细节，因为如果学习率太高，算法将会错过最小值，因为它将采取太大的步骤。而如果这个学习率太低，就需要无限的时间才能到达重点。

![](img/6839a569ce61bc7fd4659e0d09a5c553.png)

Figure by Author

# **4。梯度下降法**

**4.1 随机梯度下降**

当梯度下降逐点进行时。

**4.2 批次梯度下降**

当对所有数据点应用平方或绝对技巧时，我们获得一些值来添加到模型的权重中，将它们相加，然后用这些值的总和来更新权重。

**4.3 小批量梯度下降**

实际上，前面两种方法都没有用，因为从计算上来说都很慢。执行线性回归的最佳方法是将数据分成许多小批。每批，点数大致相同。然后使用每个批次来更新权重。这种方法被称为小批量梯度下降。

# **5。更高的尺寸**

当我们有一个输入列和一个输出列时，我们面临的是一个二维问题，回归是一条线。预测将是一个常数由自变量加上其他常数。

如果我们有更多的输入列，这意味着有更多的维度，输出将不再是一条线，而是平面或超平面(取决于维度的数量)。

![](img/9d0c783257909cf85e8018c168ec55ca.png)

# **6。多元线性回归**

自变量也被称为预测因子，它是我们用来预测其他变量的变量。我们试图预测的变量被称为因变量。

当我们试图预测的结果取决于不止一个变量时，我们可以建立一个更复杂的模型，将这种更高维度考虑在内。只要它们与所面临的问题相关，使用更多的预测变量可以帮助我们获得更好的预测。

如前所述，下图显示了一个简单的线性回归:

![](img/a3a132331d035c5fd8eb1e009518cf7a.png)

Figure by Author

下图显示了具有两个特征的多元线性回归的拟合超平面。

![](img/a5f07f8d007ba17f7d7e264ac7fb89e3.png)

Figure by Author

随着我们增加更多的预测因素，我们给问题增加了更多的维度，也变得更加难以想象，但是这个过程的核心仍然是一样的。

# **7。线性回归警告**

线性回归有一套假设，我们应该考虑到它并不是每种情况下的最佳模型。

a)当数据为线性时，线性回归效果最佳:

它从训练数据中产生一条直线。如果训练数据中的关系不是真正的线性，我们将需要进行调整(转换训练数据)、添加特征或使用其他模型。

b)线性回归对异常值敏感:

线性回归试图拟合训练数据中的最佳直线。如果数据集有一些不符合一般模式的异常极值，线性回归模型可能会受到异常值的严重影响。我们将不得不小心这些异常值，然后正常删除。

处理异常值的一种常用方法是使用替代的回归方法，这种方法对这种极值特别稳健。这种方法被称为随机样本一致性(RNASAC)算法，该算法将模型拟合到数据的内联子集。该算法执行以下步骤:

*   它选择随机数量的样本作为内联体并拟合模型。
*   它根据拟合的模型测试所有其他数据点，并添加属于用户选择值的数据点。
*   用新点重复模型的拟合。
*   计算拟合模型相对于内层的误差。
*   如果性能满足某个用户定义的阈值或达到迭代次数，则结束算法。否则，返回第一步。

# 7 .**。多项式回归**

多项式回归是多重线性 [r](https://en.wikipedia.org/wiki/Regression_analysis) 回归分析的特例，其中自变量 *x* 和因变量 *y* 之间的关系被建模为 *x* 中的 *n* 次多项式。换句话说，当我们的数据分布比线性分布更复杂时，我们使用线性模型来拟合非线性数据，从而生成曲线。

由预测变量的多项式展开产生的独立(或解释)变量被称为高次项。它被用来描述非线性现象，如组织的生长速度和疾病流行的进展。

![](img/131b7368f0d289c18a6678f1c09ee9fc.png)

Figure by Author

# **8。正规化**

正则化是一种广泛使用的处理过拟合的方法。这主要通过以下技术实现:

1.  减少模型的规模:减少模型中可学习参数的数量，从而减少其学习能力。目标是在学习能力过多和不足之间找到一个平衡点。不幸的是，没有任何神奇的公式来确定这种平衡，它必须通过设置不同数量的参数和观察其性能来测试和评估。
2.  添加权重正则化:一般来说，模型越简单越好。只要它能很好地学习，一个更简单的模型就不太可能过度拟合。实现这一点的一种常见方法是通过强制其权重仅取小值来限制网络的复杂性，从而使权重值的分布规律化。这是通过向网络的损失函数添加与具有大权重相关联的成本来实现的。成本来自两个方面:

*   L1 正则化:成本与权重系数的绝对值(权重的 L1 范数)成比例。
*   l2 正则化:成本与权重系数(权重的 L2 范数)的值的平方成比例

![](img/ef1f1d8b686f5f75805b98d58f468efd.png)

Figure by Author

要决定哪一个应用于我们的模型，建议记住以下信息并考虑我们问题的性质:

![](img/104d78219c36136ac93e99f858603b70.png)

Figure by Author

*   λ参数:它是通过正则化计算的误差。如果我们有一个大的λ，那么我们是在惩罚复杂性，最终会得到一个更简单的模型。如果λ很小，我们将得到一个复杂的模型。

# **9。评估指标**

为了跟踪我们的模型执行得有多好，我们需要设置一些评估指标。该评估度量是从生成的线(或超平面)到真实点计算的误差，并且将是通过梯度下降最小化的函数。

一些最常见的回归评估指标是:

**9.1 平均绝对误差:**

![](img/5789ce8d0fe5f110bb5663de05e59e42.png)

Figure by Author

平均绝对误差或 MAE 是真实数据点和预测结果之间的绝对差值的平均值。如果我们将此作为遵循的策略，梯度下降的每一步都将减少 MAE。

![](img/47b3f226e30e34f6eca97dea656ae18c.png)

Figure by Author

**9.2 均方误差:**

![](img/e4f7750e9b3428c0cf367fca32299f79.png)

Figure by Author

均方差或 MSE 是真实数据点和预测结果之间的平方差的平均值。这种方法惩罚越大的距离，这是标准的回归问题。

如果我们以此为策略，梯度下降的每一步都会降低 MSE。这将是计算最佳拟合线的首选方法，也称为普通最小二乘法或 OLS。

![](img/3b822e12d1ca39bb01aa6d7281a135a5.png)

Figure by Author

**9.3 均方根误差**

均方根误差或 RMSE 是平方误差平均值的根，它是确定回归模型性能最常用的评估指标，因为根产生的单位与 y 相同

![](img/9767db57a8cf2160e4e7141933d538d1.png)

Figure by Author

**9.4 决定系数或 R**

决定系数可以理解为 MSE 的标准化版本，它为模型的性能提供了更好的可解释性。

从技术上讲，R 是模型捕获的响应方差的分数，换句话说，它是响应的方差。它被定义为:

![](img/e3efecdd517af153effe698e963dcce2.png)

Figure by Author

# **10。其他算法**

虽然在本文中，我们关注的是线性和多元回归模型，但是在流行的机器学习库 Sci-kit learn(这是我们将在本系列中使用的库)中，几乎每种类型的算法都有回归变量。其中一些产生了非常好的结果。

一些例子是:

*   决策树回归器
*   随机森林回归量
*   支持向量回归机
*   套索
*   弹性网
*   梯度推进回归器
*   Ada 增强回归器

# 11.结论

在整篇文章中，我们介绍了回归模型的基础知识，学习了它们的工作原理，主要的危险以及如何应对它们。我们还了解了最常见的评估指标是什么。

我们已经设置了开始使用我们的第一个机器学习模型的知识，这正是下一篇文章将要涵盖的内容。因此，如果您想学习如何使用 Sci-kit learn 库处理回归问题，请继续关注！

*如果你喜欢这篇文章，那么你可以看看我关于数据科学和机器学习的其他文章* [*这里*](https://medium.com/@rromanss23) *。*

*如果你想了解更多关于机器学习、数据科学和人工智能的知识* ***请在 Medium*** *上关注我，敬请关注我的下一篇帖子！*