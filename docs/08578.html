<html>
<head>
<title>Exploring Textual Data using LDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨ LDA æ¢ç´¢æ–‡æœ¬æ•°æ®</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://towardsdatascience.com/exploring-textual-data-using-lda-ef1f53c772a4?source=collection_archive---------29-----------------------#2019-11-19">https://towardsdatascience.com/exploring-textual-data-using-lda-ef1f53c772a4?source=collection_archive---------29-----------------------#2019-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="99b9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">é€šè¿‡åº”ç”¨æœºå™¨å­¦ä¹ åŸç†æ¥ç†è§£éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ã€‚</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2f1f31b7ba55de7672e9c0a6fe499984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2M9d044f6mXCEDNksclm8g.png"/></div></div></figure><h1 id="4aa6" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated"><strong class="ak">ç®€ä»‹</strong></h1><p id="140e" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">æˆ‘æœ€è¿‘åœ¨å·¥ä½œä¸­å®Œæˆäº†æˆ‘çš„ç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œå¹¶å†³å®šå°†è¯¥é¡¹ç›®ä¸­ä½¿ç”¨çš„æ–¹æ³•åº”ç”¨åˆ°æˆ‘è‡ªå·±çš„é¡¹ç›®ä¸­ã€‚æˆ‘åœ¨å·¥ä½œä¸­å®Œæˆçš„é¡¹ç›®å›´ç»•ç€ä½¿ç”¨<a class="ae mf" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…</a> (LDA)å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ã€‚</p><p id="84a6" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­çš„ä¸€ç§æ— ç›‘ç£æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç”±äºå…¶æ— ç›‘ç£çš„æ€§è´¨ï¼ŒLDA ä¸éœ€è¦æ ‡è®°çš„è®­ç»ƒé›†ã€‚è¿™ä½¿å¾—å®ƒéå¸¸é€‚åˆæŸäº›ç”¨ä¾‹ï¼Œæˆ–è€…å½“å¤§å‹çš„ã€å¸¦æ ‡ç­¾çš„æ–‡æœ¬æ•°æ®é›†ä¸å®¹æ˜“è·å¾—æ—¶ã€‚</p><p id="f785" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA ä¸»è¦ç”¨äºä¸»é¢˜å»ºæ¨¡ï¼Œé€šè¿‡ç›¸ä¼¼æ€§å¯¹æ–‡æœ¬æ–‡æ¡£è¿›è¡Œèšç±»ã€‚æ–‡æ¡£å¤§å°å¯ä»¥å°åˆ°ä¸€ä¸ªå•è¯(ä¸ç†æƒ³),å¤§åˆ°æ•´ä¸ªå‡ºç‰ˆç‰©ã€‚LDA èšç±»çš„å†…å®¹æ˜¯ä½¿ç”¨æ¯ä¸ªæ–‡æ¡£ä¸­çš„æœ¯è¯­(å•è¯)ä»¥åŠå®ƒä»¬å‡ºç°çš„é¢‘ç‡æ¥ç¡®å®šçš„ï¼Œæœ‰æ—¶ç”šè‡³æ˜¯å®ƒä»¬å‡ºç°çš„é¡ºåº(ä½¿ç”¨<a class="ae mf" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n-grams </a>)ã€‚è¢«è®¤ä¸ºå½¼æ­¤ç›¸ä¼¼çš„æ–‡æ¡£è¢«èšç±»åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æˆ‘ä»¬å‡è®¾æ¯ä¸ªèšç±»ä»£è¡¨ä¸€ä¸ªä¸»é¢˜ï¼Œå°½ç®¡ç›´åˆ°èšç±»è¢«åˆ›å»ºä¹‹åæˆ‘ä»¬æ‰çŸ¥é“ä¸»é¢˜æœ¬èº«æ˜¯ä»€ä¹ˆã€‚éœ€è¦æŒ‡å‡ºçš„æ˜¯<strong class="ll ir">æ¨¡å‹æ—¢ä¸ç†è§£è¿™äº›é›†ç¾¤ä¸­æ–‡æ¡£çš„å†…å®¹ä¹Ÿä¸ç†è§£å…¶ä¸Šä¸‹æ–‡</strong>ï¼Œå› æ­¤å®é™…ä¸Šä¸èƒ½ç»™é›†ç¾¤ä¸€ä¸ªä¸»é¢˜æ ‡ç­¾ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨æ¥è‡ª(<em class="ml"> 0 </em>ï¼Œ<em class="ml"> n)çš„ç´¢å¼•æ•´æ•°æ¥â€œæ ‡è®°â€æ¯ä¸ªèšç±»ï¼›</em> <em class="ml"> n </em>æ˜¯æˆ‘ä»¬å‘Šè¯‰æ¨¡å‹è¦å¯»æ‰¾çš„ä¸»é¢˜æ•°é‡ã€‚ä¸€ä¸ªäººï¼Œæˆ–è€…è¯´<a class="ae mf" href="https://www.independent.co.uk/news/world/europe/beluga-whale-catch-hvaldimir-russian-spy-programme-video-a9197106.html" rel="noopener ugc nofollow" target="_blank">éå¸¸èªæ˜çš„æ°´ç”Ÿå“ºä¹³åŠ¨ç‰©</a>ï¼Œéœ€è¦åˆ†æè¿™äº›èšç±»ï¼Œå¹¶ç¡®å®šæ¯ä¸ªèšç±»åº”è¯¥å¦‚ä½•è¢«æ ‡è®°ã€‚</p><p id="65a4" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¸…ç†ä¸€äº› Twitter æ•°æ®ï¼Œå¹¶ç¼–å†™ä¸€ä¸ª LDA æ¨¡å‹æ¥å¯¹è¿™äº›æ•°æ®è¿›è¡Œèšç±»ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨<a class="ae mf" href="https://pyldavis.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a>æ¥ç”Ÿæˆé›†ç¾¤çš„äº¤äº’å¼å¯è§†åŒ–ã€‚</p><p id="5496" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><strong class="ll ir">å…³é”®ä¾èµ–:</strong>ç†ŠçŒ«ã€<a class="ae mf" href="https://www.nltk.org" rel="noopener ugc nofollow" target="_blank"> nltk </a>ã€<a class="ae mf" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank"> gensim </a>ã€numpyã€<a class="ae mf" href="https://pyldavis.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a></p><p id="7d21" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><strong class="ll ir">è¿™é‡Œæœ‰ä¸€äº›éœ€è¦äº‹å…ˆç†Ÿæ‚‰çš„å®šä¹‰:</strong></p><ol class=""><li id="f1ea" class="mm mn iq ll b lm mg lp mh ls mo lw mp ma mq me mr ms mt mu bi translated"><em class="ml">æ–‡æ¡£:</em>æ–‡æœ¬å¯¹è±¡(ä¾‹å¦‚ tweet)</li><li id="dee0" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><a class="ae mf" href="https://radimrehurek.com/gensim/corpora/dictionary.html" rel="noopener ugc nofollow" target="_blank"> <em class="ml">å­—å…¸</em> </a> <em class="ml"> : </em>æˆ‘ä»¬çš„æ–‡æ¡£é›†åˆä¸­æ‰€æœ‰æƒŸä¸€æ ‡è®°(å•è¯ã€æœ¯è¯­)çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ ‡è®°éƒ½æœ‰ä¸€ä¸ªæƒŸä¸€çš„æ•´æ•°æ ‡è¯†ç¬¦</li><li id="59ed" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><a class="ae mf" href="https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/" rel="noopener ugc nofollow" target="_blank"><em class="ml"/></a><em class="ml">:</em>æˆ‘ä»¬æ‰€æœ‰æ–‡æ¡£çš„é›†åˆï¼Œæ¯ä¸ªæ–‡æ¡£ç®€åŒ–ä¸ºä¸€ä¸ªçŸ©é˜µåˆ—è¡¨ï¼Œæ–‡æ¡£ä¸­çš„æ¯ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªçŸ©é˜µâ€” <em class="ml">ä½¿ç”¨ gensim çš„</em><a class="ae mf" href="https://kite.com/python/docs/gensim.corpora.Dictionary.doc2bow" rel="noopener ugc nofollow" target="_blank"><em class="ml">doc 2 bow</em></a><em class="ml">ï¼Œæ¯ä¸ªçŸ©é˜µè¡¨ç¤ºä¸ºä¸€ä¸ªå…ƒç»„ï¼Œå¸¦æœ‰ä¸€ä¸ª</em> <strong class="ll ir"> <em class="ml">æœ¯è¯­çš„å”¯ä¸€æ•´æ•° id </em> </strong> <em class="ml">ï¼Œç´¢å¼•ä¸º 0 å’Œ(ä¾‹å¦‚ï¼Œæ–‡æ¡£â€œthe box was in the bigger boxâ€å°†è¢«ç®€åŒ–ä¸ºç±»ä¼¼äº[("the "ï¼Œ2)ï¼Œ(" box "ï¼Œ2)ï¼Œ(" was "ï¼Œ1)ï¼Œ(" in "ï¼Œ1)ï¼Œ(" bigger "ï¼Œ1)]çš„å†…å®¹ï¼Œä½†ç”¨" term "æ›¿æ¢æœ¯è¯­çš„å”¯ä¸€å­—å…¸ id) </em></li><li id="84b8" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><em class="ml"> coherence score: </em>ä¸€ä¸ªèŒƒå›´ä» 0 åˆ° 1 çš„æµ®ç‚¹å€¼ï¼Œç”¨äºè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹å’Œèšç±»æ•°ä¸æˆ‘ä»¬çš„æ•°æ®çš„å»åˆç¨‹åº¦</li><li id="26f3" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><em class="ml">é›†ç¾¤:</em>ä»£è¡¨ä¸€ç»„æ–‡æ¡£çš„èŠ‚ç‚¹ï¼Œä¸€ä¸ªæ¨æ–­çš„ä¸»é¢˜</li></ol><h1 id="de8f" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">1.æ•°æ®</h1><p id="39ab" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">ä»Šå¹´æ—©äº›æ—¶å€™ï¼Œæˆ‘å¼€å§‹æ”¶é›†å‡ åä¸‡æ¡æ”¿æ²»æ¨æ–‡ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯å¯¹æ¨æ–‡åŠå…¶å…ƒæ•°æ®è¿›è¡Œå„ç§åˆ†æï¼Œä¸º 2020 å¹´ç¾å›½æ€»ç»Ÿå¤§é€‰åšå‡†å¤‡ã€‚</p><p id="78ca" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">è¿™ç¯‡æ–‡ç« çš„æ•°æ®é›†å°†ç”± 3500 æ¡æ¨æ–‡ç»„æˆï¼Œå…¶ä¸­è‡³å°‘æåˆ°ä»¥ä¸‹ä¸€æ¡:â€œ@berniesandersâ€ã€â€œkamalaharrisâ€ã€â€œjoebidenâ€ã€â€œewarrenâ€(åˆ†åˆ«æ˜¯ä¼¯å°¼Â·æ¡‘å¾·æ–¯ã€å¡ç›æ‹‰Â·å“ˆé‡Œæ–¯ã€ä¹”Â·æ‹œç™»å’Œä¼Šä¸½èç™½Â·æ²ƒä¼¦çš„æ¨ç‰¹è´¦å·)ã€‚æˆ‘åœ¨ 2019 å¹´ 11 æœˆåˆæ”¶é›†äº†è¿™äº›æ¨æ–‡ï¼Œå¹¶åœ¨è¿™é‡Œæä¾›ä¸‹è½½<a class="ae mf" href="https://drive.google.com/drive/folders/1ebI3pEkrz3JbyF_aZF4DVPX2LSG1hVn_?usp=sharing" rel="noopener ugc nofollow" target="_blank">ã€‚æˆ‘ä»¬å°†ç ”ç©¶è¿™äº›æ•°æ®ï¼Œå¹¶è¯•å›¾æ‰¾å‡ºäººä»¬åœ¨ 11 æœˆåˆå‘æ¨æ–‡çš„å†…å®¹ã€‚</a></p><p id="081a" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">æˆ‘ä¸ä¼šæ·±å…¥ç ”ç©¶å¦‚ä½•æ”¶é›†æ¨æ–‡ï¼Œä½†æˆ‘å·²ç»åŒ…æ‹¬äº†æˆ‘åœ¨ä¸‹é¢ä½¿ç”¨çš„ä»£ç ã€‚æˆåŠŸè¿è¡Œä»£ç éœ€è¦è®¿é—®<a class="ae mf" href="http://tweepy.readthedocs.org" rel="noopener ugc nofollow" target="_blank"> tweepy API </a>ã€‚æˆ‘æ²¡æœ‰æ”¶é›†è½¬å‘ï¼Œä¹Ÿæ²¡æœ‰æ”¶é›†ä¸æ˜¯ç”¨è‹±è¯­å†™çš„æ¨æ–‡(è¯¥æ¨¡å‹éœ€è¦æ›´å¤šçš„è°ƒæ•´ä»¥é€‚åº”å¤šç§è¯­è¨€)ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="2440" class="nf ks iq nb b gy ng nh l ni nj">class Streamer(StreamListener):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.limit = 1000 # Number of tweets to collect.<br/>        self.statuses = []  # Pass each status here.<br/><br/>    def on_status(self, status):<br/>        if status.retweeted or "RT @" <br/>        in status.text or status.lang != "en":<br/>            return   # Remove re-tweets and non-English tweets.<br/>        if len(self.statuses) &lt; self.limit:<br/>            self.statuses.append(status)<br/>            print(len(self.statuses))  # Get count of statuses<br/>        if len(self.statuses) == self.limit:<br/>            with open("/tweet_data.csv", "w") as    file: <br/>                writer = csv.writer(file)  # Saving data to csv. <br/>                for status in self.statuses:<br/>                    writer.writerow([status.id, status.text,<br/>              status.created_at, status.user.name,         <br/>              status.user.screen_name, status.user.followers_count, status.user.location]) <br/>            print(self.statuses)<br/>            print(f"\n*** Limit of {self.limit} met ***")<br/>            return False<br/>        if len(self.statuses) &gt; self.limit:<br/>            return False<br/><br/><br/>streaming = tweepy.Stream(auth=setup.api.auth, listener=Streamer())<br/><br/>items = ["@berniesanders", "@kamalaharris", "@joebiden", "@ewarren"]  # Keywords to track<br/><br/>stream_data = streaming.filter(track=items)</span></pre><p id="69f2" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">è¿™ä¼šå°† tweet æ–‡æœ¬æ•°æ®åŠå…¶å…ƒæ•°æ®(idã€åˆ›å»ºæ—¥æœŸã€åç§°ã€ç”¨æˆ·åã€å…³æ³¨è€…æ•°é‡å’Œä½ç½®)ä¼ é€’ç»™åä¸º tweet_data çš„ csvã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d515" class="nf ks iq nb b gy ng nh l ni nj">import pandas as pd</span><span id="f2ef" class="nf ks iq nb b gy nk nh l ni nj">df = pd.read_csv(r"/tweet_data.csv", names= ["id", "text", "date", "name", "username", "followers", "loc"])</span></pre><p id="91f0" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">ç°åœ¨æˆ‘ä»¬å·²ç»å°†æ•°æ®æ‰“åŒ…åˆ°ä¸€ä¸ªæ•´æ´çš„ csv ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ä¸ºæˆ‘ä»¬çš„ LDA æœºå™¨å­¦ä¹ æ¨¡å‹å‡†å¤‡æ•°æ®äº†ã€‚æ–‡æœ¬æ•°æ®é€šå¸¸è¢«è§†ä¸ºéç»“æ„åŒ–æ•°æ®ï¼Œåœ¨è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†æä¹‹å‰éœ€è¦æ¸…ç†ã€‚ç”±äºä¸ä¸€è‡´çš„æ€§è´¨ï¼Œæ¨æ–‡å°¤å…¶æ··ä¹±ã€‚ä¾‹å¦‚ï¼Œä»»ä½•ç»™å®šçš„ Twitter ç”¨æˆ·å¯èƒ½æŸä¸€å¤©ç”¨å®Œæ•´çš„å¥å­å‘æ¨ï¼Œè€Œç¬¬äºŒå¤©ç”¨å•ä¸ªå•è¯å’Œæ ‡ç­¾å‘æ¨ã€‚å¦ä¸€ä¸ªç”¨æˆ·å¯èƒ½åªå‘é“¾æ¥ï¼Œå¦ä¸€ä¸ªç”¨æˆ·å¯èƒ½åªå‘æ ‡ç­¾ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ç”¨æˆ·å¯èƒ½ä¼šæœ‰æ„å¿½ç•¥çš„è¯­æ³•å’Œæ‹¼å†™é”™è¯¯ã€‚è¿˜æœ‰ä¸€äº›å£è¯­ä¸­ä½¿ç”¨çš„æœ¯è¯­ä¸ä¼šå‡ºç°åœ¨æ ‡å‡†è‹±è¯­è¯å…¸ä¸­ã€‚</p><h2 id="6473" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">æ¸…æ´</h2><p id="03bd" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">æˆ‘ä»¬å°†åˆ é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šå­—ç¬¦å’Œ url é“¾æ¥ï¼Œç„¶åå¯¹æ¯æ¡æ¨æ–‡åº”ç”¨<em class="ml"> lower() </em>ã€‚è¿™ä¸ºæˆ‘ä»¬çš„æ–‡æ¡£å¸¦æ¥äº†ä¸€å®šç¨‹åº¦çš„ä¸€è‡´æ€§(è®°ä½æ¯æ¡ tweet éƒ½è¢«è§†ä¸ºä¸€ä¸ªæ–‡æ¡£)ã€‚æˆ‘è¿˜åˆ é™¤äº†â€œberniesandersâ€ã€â€œkamalaharrisâ€ã€â€œjoebidenâ€å’Œâ€œewarrenâ€çš„å®ä¾‹ï¼Œå› ä¸ºå®ƒä»¬ä¼šæ‰­æ›²æˆ‘ä»¬çš„è¯é¢‘ï¼Œå› ä¸ºæ¯ä¸ªæ–‡æ¡£è‡³å°‘ä¼šåŒ…å«å…¶ä¸­ä¸€é¡¹ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="777b" class="nf ks iq nb b gy ng nh l ni nj">import string</span><span id="ee74" class="nf ks iq nb b gy nk nh l ni nj">ppl = ["berniesanders", "kamalaharris", "joebiden", "ewarren"]</span><span id="4eb2" class="nf ks iq nb b gy nk nh l ni nj">def clean(txt):<br/>    txt = str(txt.translate(str.maketrans("", "", string.punctuation))).lower() <br/>    txt = str(txt).split()<br/>    for item in txt:<br/>        if "http" in item:<br/>            txt.remove(item)<br/>        for item in ppl:<br/>            if item in txt:<br/>                txt.remove(item)<br/>    txt = (" ".join(txt))<br/>    return txt<br/>    <br/>df.text = df.text.apply(clean)</span></pre><h1 id="c3e7" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">2.æ•°æ®å‡†å¤‡</h1><p id="e79f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">ä¸‹é¢æ˜¯æˆ‘ä»¬éœ€è¦å¯¼å…¥çš„åŒ…ï¼Œä»¥ä¾¿åœ¨å°†æ•°æ®è¾“å…¥æ¨¡å‹ä¹‹å‰å‡†å¤‡å¥½æ•°æ®ã€‚<strong class="ll ir">åœ¨ç¼–å†™æ•°æ®å‡†å¤‡çš„ä»£ç æ—¶ï¼Œæˆ‘ä¹Ÿä¼šåŒ…æ‹¬è¿™äº›å¯¼å…¥ã€‚</strong></p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="55b9" class="nf ks iq nb b gy ng nh l ni nj">import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS as stopwords<br/>import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(0)</span></pre><p id="d3f2" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">æˆ‘ä»¬å·²ç»æ¸…ç†äº†ä¸€äº›æ–‡æ¡£ï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬éœ€è¦<a class="ae mf" href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" rel="noopener ugc nofollow" target="_blank">å¯¹å®ƒä»¬è¿›è¡Œè¯æ³•åˆ†æå’Œè¯å¹²åˆ†æã€‚è¯æ±‡åŒ–å°†æ–‡æ¡£ä¸­çš„å•è¯è½¬æ¢ä¸ºç¬¬ä¸€äººç§°ï¼Œå¹¶å°†æ‰€æœ‰åŠ¨è¯è½¬æ¢ä¸ºç°åœ¨æ—¶ã€‚è¯å¹²å¤„ç†å°†æ–‡æ¡£ä¸­çš„å•è¯è¿˜åŸä¸ºå®ƒä»¬çš„æ ¹æ ¼å¼ã€‚å¹¸è¿çš„æ˜¯ï¼Œnltk æœ‰ä¸€ä¸ª lemmatizer å’Œä¸€ä¸ªè¯å¹²åˆ†æå™¨å¯ä¾›æˆ‘ä»¬åˆ©ç”¨ã€‚</a></p><p id="300d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA æ¶‰åŠåˆ°ä¸€ä¸ª<a class="ae mf" href="https://en.wikipedia.org/wiki/Stochastic_process" rel="noopener ugc nofollow" target="_blank">éšæœºè¿‡ç¨‹</a>ï¼Œæ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹éœ€è¦äº§ç”Ÿéšæœºå˜é‡çš„èƒ½åŠ›ï¼Œå› æ­¤æœ‰äº†<em class="ml"> numpy </em>å¯¼å…¥ã€‚æ·»åŠ <em class="ml"> numpy.random.seed(0) </em>å…è®¸æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¯é‡å¤çš„ï¼Œå› ä¸ºå®ƒå°†ç”Ÿæˆå¹¶ä½¿ç”¨ç›¸åŒçš„éšæœºå˜é‡ï¼Œè€Œä¸æ˜¯åœ¨æ¯æ¬¡ä»£ç è¿è¡Œæ—¶ç”Ÿæˆæ–°çš„å˜é‡ã€‚</p><p id="89a9" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">Gensim çš„åœç”¨è¯æ˜¯ä¸€ä¸ªè¢«è®¤ä¸ºä¸ç›¸å…³æˆ–å¯èƒ½æ··æ·†æˆ‘ä»¬è¯æ±‡çš„æœ¯è¯­åˆ—è¡¨ã€‚åœ¨ NLP ä¸­ï¼Œâ€œåœç”¨è¯â€æŒ‡çš„æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹é€‰å–çš„æœ¯è¯­é›†åˆã€‚æ­¤åˆ—è¡¨å°†ç”¨äºä»æˆ‘ä»¬çš„æ–‡æ¡£ä¸­åˆ é™¤è¿™äº›ä¸ç›¸å…³çš„æœ¯è¯­ã€‚æˆ‘ä»¬å¯ä»¥<em class="ml"> print(stopwords) </em>æ¥æŸ¥çœ‹å°†è¦åˆ é™¤çš„æœ¯è¯­ã€‚</p><p id="a471" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">ä»¥ä¸‹æ˜¯åœç”¨è¯ä¸­çš„æœ¯è¯­ã€‚</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/4a4660ca13aabb8f171d89414ecb6f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxUowoI97ML7MsR54QSZHA.png"/></div></div></figure><p id="ce1d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä¿æŒåœç”¨è¯åˆ—è¡¨ä¸å˜ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦æ·»åŠ æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å¿½ç•¥çš„ç‰¹å®šæœ¯è¯­ã€‚ä¸‹é¢çš„ä»£ç æ˜¯å‘åœç”¨è¯æ·»åŠ æœ¯è¯­çš„ä¸€ç§æ–¹æ³•ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="2f71" class="nf ks iq nb b gy ng nh l ni nj">stopwords = stopwords.union(set(["add_term_1", "add_term_2"]))</span></pre><h2 id="3b3e" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">è¯æ±‡åŒ–å’Œè¯å¹²åŒ–</h2><p id="473c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æ•°æ®å‡†å¤‡å†™ä¸€äº›ä»£ç ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="cc65" class="nf ks iq nb b gy ng nh l ni nj">import warnings <br/>warnings.simplefilter("ignore")<br/>import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS as stopwords<br/>import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(0)</span></pre><p id="c499" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">åˆå§‹åŒ–è¯å¹²åˆ†æå™¨ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="acda" class="nf ks iq nb b gy ng nh l ni nj">stemmer = stemm(language="english")</span></pre><p id="5a5f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ—¢èƒ½å¯¹æˆ‘ä»¬çš„æ–‡æ¡£è¿›è¡Œè¯æ±‡åŒ–ï¼Œåˆèƒ½å¯¹å…¶è¿›è¡Œè¯å¹²åˆ†æã€‚GeeksforGeeks æœ‰å…³äºä½¿ç”¨ nltk è¿›è¡Œè¯æ³•åˆ†æçš„<a class="ae mf" href="https://www.geeksforgeeks.org/python-lemmatization-with-nltk/" rel="noopener ugc nofollow" target="_blank">ä¸ªä¾‹å­</a>å’Œå…³äºä½¿ç”¨ nltk è¿›è¡Œè¯å¹²åˆ†æçš„<a class="ae mf" href="https://www.geeksforgeeks.org/python-stemming-words-with-nltk/" rel="noopener ugc nofollow" target="_blank">ä¸ªä¾‹å­</a>ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="8dc5" class="nf ks iq nb b gy ng nh l ni nj">def lemm_stemm(txt):<br/>    return stemmer.stem(lemm().lemmatize(txt, pos="v"))</span></pre><p id="5197" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œå°†åœç”¨è¯ä»æˆ‘ä»¬çš„æ–‡æ¡£ä¸­åˆ é™¤ï¼ŒåŒæ—¶ä¹Ÿåº”ç”¨<em class="ml">lemm _ stem()</em>ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="47d7" class="nf ks iq nb b gy ng nh l ni nj">def preprocess(txt):<br/>    r = [lemm_stemm(token) for token in simple_preprocess(txt) if       token not in stopwords and len(token) &gt; 2]<br/>    return r</span></pre><p id="276c" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">å°†æˆ‘ä»¬æ¸…ç†å’Œå‡†å¤‡å¥½çš„æ–‡æ¡£åˆ†é…ç»™ä¸€ä¸ªæ–°å˜é‡ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="691c" class="nf ks iq nb b gy ng nh l ni nj">proc_docs = df.text.apply(preprocess)</span></pre><h1 id="dfeb" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated"><strong class="ak"> 3ã€‚æ¨¡å‹çš„åˆ¶ä½œ</strong></h1><p id="c94b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¼–å†™æ¨¡å‹äº†ã€‚</p><h2 id="bd7d" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">è¯å…¸</h2><p id="480b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">æ­£å¦‚å¼•è¨€ä¸­æåˆ°çš„ï¼Œå­—å…¸(åœ¨ LDA ä¸­)æ˜¯åœ¨æˆ‘ä»¬çš„æ–‡æ¡£é›†åˆä¸­å‡ºç°çš„æ‰€æœ‰å”¯ä¸€æœ¯è¯­çš„åˆ—è¡¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ gensim çš„è¯­æ–™åº“åŒ…æ¥æ„å»ºæˆ‘ä»¬çš„è¯å…¸ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="b86c" class="nf ks iq nb b gy ng nh l ni nj">dictionary = gensim.corpora.Dictionary(proc_docs)<br/>dictionary.filter_extremes(no_below=5, no_above= .90)<br/>len(dictionary)</span></pre><p id="2719" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="ml"> filter_extremes() </em>å‚æ•°æ˜¯é’ˆå¯¹åœç”¨è¯æˆ–å…¶ä»–å¸¸ç”¨æœ¯è¯­çš„ç¬¬äºŒé“é˜²çº¿ï¼Œè¿™äº›åœç”¨è¯æˆ–å¸¸ç”¨æœ¯è¯­å¯¹å¥å­çš„æ„ä¹‰æ²¡æœ‰ä»€ä¹ˆå®è´¨æ„ä¹‰ã€‚æ‘†å¼„è¿™äº›å‚æ•°å¯ä»¥å¸®åŠ©å¾®è°ƒæ¨¡å‹ã€‚å…³äºè¿™ä¸€ç‚¹æˆ‘å°±ä¸èµ˜è¿°äº†ï¼Œä½†æˆ‘åœ¨ä¸‹é¢é™„ä¸Šäº†æ¥è‡ª<a class="ae mf" href="https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes" rel="noopener ugc nofollow" target="_blank"> gensim çš„å­—å…¸æ–‡æ¡£</a>ä¸­è§£é‡Šå‚æ•°çš„æˆªå›¾ã€‚</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/dd6a330c2e96aa11a879313a4102ccbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMTP88f0Y-tNlwWZeaECEw.png"/></div></div></figure><p id="bb80" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">æˆ‘ä»¬çš„å­—å…¸æœ‰ 972 ä¸ªç‹¬ç‰¹çš„å•è¯(æœ¯è¯­)ã€‚</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/b664daf8f2317fe5a1b408b2ef0a615c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPxWahQ59en9eVi6OKkJxQ.png"/></div></div></figure><h2 id="0898" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">è¯æ±‡è¢‹</h2><p id="077f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">å¦‚å¼•è¨€ä¸­æ‰€è¿°ï¼Œå•è¯åŒ…(åœ¨ LDA ä¸­)æ˜¯æˆ‘ä»¬åˆ†è§£æˆçŸ©é˜µçš„æ‰€æœ‰æ–‡æ¡£çš„é›†åˆã€‚çŸ©é˜µç”±æœ¯è¯­çš„æ ‡è¯†ç¬¦å’Œå®ƒåœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ç»„æˆã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="7360" class="nf ks iq nb b gy ng nh l ni nj">n = 5 # Number of clusters we want to fit our data to<br/>bow = [dictionary.doc2bow(doc) for doc in proc_docs]<br/>lda = gensim.models.LdaMulticore(bow, num_topics= n, id2word=dictionary, passes=2, workers=2)</span><span id="50ac" class="nf ks iq nb b gy nk nh l ni nj">print(bow)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/dda660754596d151ab078d7a417b9725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zMTHjPvWKA63SU6qeSEOCg.png"/></div></div></figure><p id="571f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å®šä¹‰é›†ç¾¤çš„å…³é”®æœ¯è¯­æ¥äº†è§£æˆ‘ä»¬çš„é›†ç¾¤æ˜¯å¦‚ä½•å½¢æˆçš„ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="808b" class="nf ks iq nb b gy ng nh l ni nj">for id, topic in lda.print_topics(-1):<br/>    print(f"TOPIC: {id} \n WORDS: {topic}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/dfd835e776d590711bb614aab00066a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NRDJOaTcf5430nLlDUHSHw.png"/></div></div></figure><p id="d22f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">æŸ¥çœ‹æ¯ä¸ªä¸»é¢˜ç¾¤ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£å®ƒä»¬ä»£è¡¨äº†ä»€ä¹ˆã€‚çœ‹ä¸€ä¸‹é¢˜ç›® 1 å’Œé¢˜ç›® 4ã€‚</p><p id="e4da" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="ml">å…³äºè¯é¢˜ 1: </em>åœ¨è¯é¢˜ 1 ä¸­ï¼Œå…³é”®è¯â€œcenkuygurâ€å’Œâ€œanakasparianâ€æ˜¯æŒ‡<a class="ae mf" href="https://twitter.com/cenkuygur" rel="noopener ugc nofollow" target="_blank"> Cenk ç»´å¾å°”æ—</a> <strong class="ll ir"> </strong>å’Œ<strong class="ll ir"/><a class="ae mf" href="https://twitter.com/AnaKasparian" rel="noopener ugc nofollow" target="_blank">Ana Kasparian</a><strong class="ll ir">ï¼Œ</strong>å…±åŒä¸»æŒäºº<strong class="ll ir"/><a class="ae mf" href="https://tyt.com" rel="noopener ugc nofollow" target="_blank">å°‘å£®æ´¾</a>(æŸæ”¿è®ºäº‹åŠ¡æ‰€åŠèŠ‚ç›®)ã€‚ä¸»é¢˜ 1 è¿˜åŒ…æ‹¬å…³é”®æœ¯è¯­â€œæƒåˆ©â€ã€â€œç‰¹æœ—æ™®â€å’Œâ€œå…¨å›½æ­¥æªåä¼šâ€ã€‚</p><p id="e403" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">11 æœˆ 15 æ—¥ï¼ŒåŠ å·åœ£å¡”å…‹æ‹‰é‡Œå¡”é™„è¿‘çš„ç´¢æ ¼æ–¯é«˜ä¸­å‘ç”Ÿäº†æ ¡å›­æªå‡»æ¡ˆã€‚å…³äºè¿™ä¸€æ‚²å‰§äº‹ä»¶ï¼ŒT2 åª’ä½“è¿›è¡Œäº†å¤§é‡æŠ¥é“ï¼Œç½‘ä¸Šä¹Ÿè®®è®ºçº·çº·ã€‚å¹´è½»çš„åœŸè€³å…¶äºº(TYT)æ˜¯æ›´ä¸¥æ ¼çš„æªæ”¯æ³•å¾‹çš„å£å¤´æ”¯æŒè€…ï¼Œå¹¶ç»å¸¸ä¸å…¨å›½æ­¥æªåä¼šå’Œå…¶ä»–æªæ”¯å›¢ä½“å‘ç”Ÿå†²çªã€‚TYT ç”šè‡³å¸¦å¤´å‘èµ·äº†åä¸º#NeverNRA çš„æ‰¿è¯º<a class="ae mf" href="https://join.tyt.com/nevernra/" rel="noopener ugc nofollow" target="_blank">è¿åŠ¨ã€‚</a></p><p id="8a02" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">è¿™ä¸ªä¸»é¢˜ç¾¤å¯ä»¥è¢«æ ‡ä¸ºâ€œTYT å¯¹å…¨å›½æ­¥æªåä¼šâ€ï¼Œæˆ–ç±»ä¼¼çš„ä¸œè¥¿ã€‚</p><p id="06bf" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="ml">å…³äºä¸»é¢˜ 4: </em>æœ¯è¯­â€œcenkuygurâ€å’Œâ€œanakasparianâ€åœ¨ä¸»é¢˜ 4 ä¸­é‡å¤å‡ºç°ã€‚è¯é¢˜ 4 è¿˜åŒ…æ‹¬â€œtheyoungturkâ€ï¼ŒæŒ‡çš„æ˜¯å¹´è½»çš„åœŸè€³å…¶äººï¼Œä»¥åŠâ€œberniâ€ï¼ŒæŒ‡çš„æ˜¯ä¼¯å°¼Â·æ¡‘å¾·æ–¯ã€‚</p><p id="afc4" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">11 æœˆ 12 æ—¥ï¼Œå²‘å…‹ç»´ä¸ºå€™é€‰äººä¼¯å°¼Â·æ¡‘å¾·æ–¯å‘å¸ƒ<a class="ae mf" href="https://youtu.be/m4mspXXNiqg" rel="noopener ugc nofollow" target="_blank">å…¬å¼€èƒŒä¹¦</a>ã€‚TYT çš„æ¨ç‰¹è´¦æˆ·é‡å¤äº†è¿™ä¸€è¡¨æ€ã€‚ä¼¯å°¼Â·æ¡‘å¾·æ–¯éšåå…¬å¼€æ„Ÿè°¢ä»–ä»¬çš„æ”¯æŒã€‚æ­¤å¤–ï¼Œ11 æœˆ 14 æ—¥ï¼Œç»´å¾å°”å…ˆç”Ÿå®£å¸ƒä»–å°†ç«é€‰å›½ä¼šè®®å‘˜ã€‚è¿™ä¸¤é¡¹è¿›å±•éƒ½åœ¨ Twitter ä¸Šè·å¾—äº†æ˜¾è‘—å…³æ³¨ã€‚</p><p id="d70d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">è¿™ä¸ªä¸»é¢˜ç¾¤å¯ä»¥è¢«ç§°ä¸ºâ€œTYT å’Œä¼¯å°¼Â·æ¡‘å¾·æ–¯â€ï¼Œæˆ–è€…ç±»ä¼¼çš„åç§°ã€‚</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/54e5092405606881b103a2bf6640b83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFGyxTEwyvAc0rYqbYLa7Q.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/d3260669357d052515659a82da1e67ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ux014P2oIP9OiDe5r-n0WA.png"/></div></div></figure><p id="11f5" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">å…¶ä»–ä¸»é¢˜ç¾¤ä¹Ÿæœ‰ç±»ä¼¼çš„è§£é‡Šã€‚</p><h1 id="ae00" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated"><strong class="ak"> 4ã€‚è¯„ä¼°ã€å¯è§†åŒ–ã€ç»“è®º</strong></h1><p id="7051" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">å¤§å¤šæ•°å¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œåº”ç”¨éƒ½æœ‰ä¸€ä¸ªåé¦ˆç¯ã€‚è¿™æ˜¯ä¸€ç§è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€å¯ä¼¸ç¼©æ€§å’Œæ•´ä½“è´¨é‡çš„æ–¹æ³•ã€‚åœ¨ä¸»é¢˜å»ºæ¨¡ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨<a class="ae mf" href="http://qpleple.com/topic-coherence-to-evaluate-topic-models/" rel="noopener ugc nofollow" target="_blank">ä¸€è‡´æ€§åˆ†æ•°</a>æ¥ç¡®å®šæˆ‘ä»¬çš„æ¨¡å‹æœ‰å¤šâ€œä¸€è‡´â€<em class="ml"> </em>ã€‚æ­£å¦‚æˆ‘åœ¨ä»‹ç»ä¸­æåˆ°çš„ï¼Œcoherence æ˜¯ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹å€¼ã€‚ä¸ºæ­¤æˆ‘ä»¬ä¹Ÿå°†ä½¿ç”¨ gensimã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d73b" class="nf ks iq nb b gy ng nh l ni nj"># Eval via coherence scoring</span><span id="f743" class="nf ks iq nb b gy nk nh l ni nj">from gensim import corpora, models<br/>from gensim.models import CoherenceModel<br/>from pprint import pprint</span><span id="85f8" class="nf ks iq nb b gy nk nh l ni nj">coh = CoherenceModel(model=lda, texts= proc_docs, dictionary = dictionary, coherence = "c_v")<br/>coh_lda = coh.get_coherence()<br/>print("Coherence Score:", coh_lda)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/e6759410a9d51303dea0ea26310e3859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4s1jO1KbBrCZyQhv_JX7gA.png"/></div></div></figure><p id="176b" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">æˆ‘ä»¬å¾—åˆ°äº† 0.44 çš„ä¸€è‡´æ€§åˆ†æ•°ã€‚è¿™ä¸æ˜¯æœ€å¥½çš„ï¼Œä½†å®é™…ä¸Šä¹Ÿä¸ç®—å¤ªå·®ã€‚è¿™ä¸ªåˆ†æ•°æ˜¯åœ¨æ²¡æœ‰ä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹è·å¾—çš„ã€‚çœŸæ­£æŒ–æ˜æˆ‘ä»¬çš„å‚æ•°å’Œæµ‹è¯•ç»“æœåº”è¯¥ä¼šå¾—åˆ°æ›´é«˜çš„åˆ†æ•°ã€‚å¾—åˆ†çœŸçš„æ²¡æœ‰å®˜æ–¹é—¨æ§›ã€‚æˆ‘çš„ä¸€è‡´æ€§åˆ†æ•°ç›®æ ‡é€šå¸¸åœ¨ 0.65 å·¦å³ã€‚å‚è§è¿™ç¯‡<a class="ae mf" href="https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/" rel="noopener ugc nofollow" target="_blank">æ–‡ç« </a>å’Œè¿™ä¸ªå †æ ˆæº¢å‡º<a class="ae mf" href="https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad" rel="noopener ugc nofollow" target="_blank">çº¿ç¨‹</a>äº†è§£æ›´å¤šå…³äºä¸€è‡´æ€§è¯„åˆ†çš„ä¿¡æ¯ã€‚</p><h2 id="17d2" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">ç”¨ pyLDAvis å¯è§†åŒ–</h2><p id="b9c6" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ pyLDAvis å¯è§†åŒ–æˆ‘ä»¬çš„é›†ç¾¤ã€‚è¿™ä¸ªåŒ…åˆ›å»ºäº†ä¸€ä¸ªèšç±»çš„è·ç¦»å›¾ï¼Œæ²¿ç€ x å’Œ y è½´ç»˜åˆ¶èšç±»ã€‚è¿™ä¸ªè·ç¦»åœ°å›¾å¯ä»¥é€šè¿‡è°ƒç”¨<em class="ml"> pyLDAvis.display() </em>åœ¨ Jupiter ä¸­æ‰“å¼€ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨<em class="ml"> pyLDAvis.show() </em>åœ¨ web ä¸­æ‰“å¼€ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="508f" class="nf ks iq nb b gy ng nh l ni nj">import pyLDAvis.gensim as pyldavis<br/>import pyLDAvis</span><span id="d004" class="nf ks iq nb b gy nk nh l ni nj">lda_display = pyldavis.prepare(lda, bow, dictionary)<br/>pyLDAvis.show(lda_display)</span></pre><p id="2ce1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">è¿™æ˜¯æˆ‘ä»¬çš„ pyLDAvis è·ç¦»å›¾çš„æˆªå›¾ã€‚</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/4be55562fcd33d1fbf9e4d3a593b83cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7R8bPrb5FIbC2OGj2_yLQ.png"/></div></div></figure><p id="7ccd" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">å°†é¼ æ ‡æ‚¬åœåœ¨æ¯ä¸ªé›†ç¾¤ä¸Šï¼Œä¼šæ˜¾ç¤ºè¯¥é›†ç¾¤ä¸­å…³é”®æœ¯è¯­çš„ç›¸å…³æ€§(çº¢è‰²)ä»¥åŠè¿™äº›ç›¸åŒå…³é”®æœ¯è¯­åœ¨æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­çš„ç›¸å…³æ€§(è“è‰²)ã€‚è¿™æ˜¯å‘é£é™©æ‰¿æ‹…è€…å±•ç¤ºè°ƒæŸ¥ç»“æœçš„æœ‰æ•ˆæ–¹å¼ã€‚</p><h2 id="b8f5" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated"><strong class="ak">ç»“è®º</strong></h2><p id="46f9" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">è¿™é‡Œæ˜¯æˆ‘ä¸Šé¢ä½¿ç”¨çš„æ‰€æœ‰ä»£ç ï¼ŒåŒ…æ‹¬æˆ‘ç”¨æ¥ç”Ÿæˆå•è¯äº‘çš„ä»£ç å’Œæˆ‘ç”¨æ¥æ”¶é›†æ¨æ–‡æ•°æ®çš„ä»£ç ã€‚</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="895f" class="nf ks iq nb b gy ng nh l ni nj">### All Dependencies ###<br/><br/>import pandas as pd<br/>from wordcloud import WordCloud as cloud<br/>import matplotlib.pyplot as plt<br/>import string<br/>import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS as stopwords<br/>import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(0)<br/>from gensim import corpora, models<br/>from gensim.models import CoherenceModel<br/>from pprint import pprint<br/>import pyLDAvis.gensim as pyldavis<br/>import pyLDAvis<br/><br/><br/>### Word Cloud ###<br/><br/>df = pd.read_csv(r"/tweet_data.csv", names=["id", "text", "date", "name",<br/>                                                                 "username", "followers", "loc"])<br/><br/><br/>def clean(txt):<br/>    txt = str(txt).split()<br/>    for item in txt:<br/>        if "http" in item:<br/>            txt.remove(item)<br/>    txt = (" ".join(txt))<br/>    return txt<br/><br/><br/>text = (df.text.apply(clean))<br/><br/><br/>wc = cloud(background_color='white', colormap="tab10").generate(" ".join(text))<br/><br/>plt.axis("off")<br/>plt.text(2, 210, "Generated using word_cloud and this post's dataset.", size = 5, color="grey")<br/><br/>plt.imshow(wc)<br/>plt.show()</span><span id="ff0f" class="nf ks iq nb b gy nk nh l ni nj">### Stream &amp; Collect Tweets ###</span><span id="d357" class="nf ks iq nb b gy nk nh l ni nj">class Streamer(StreamListener):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.limit = 1000 # Number of tweets to collect.<br/>        self.statuses = []  # Pass each status here.<br/><br/>    def on_status(self, status):<br/>        if status.retweeted or "RT @" <br/>        in status.text or status.lang != "en":<br/>            return   # Remove re-tweets and non-English tweets.<br/>        if len(self.statuses) &lt; self.limit:<br/>            self.statuses.append(status)<br/>            print(len(self.statuses))  # Get count of statuses<br/>        if len(self.statuses) == self.limit:<br/>            with open("/tweet_data.csv", "w") as    file: <br/>                writer = csv.writer(file)  # Saving data to csv. <br/>                for status in self.statuses:<br/>                    writer.writerow([status.id, status.text,<br/>              status.created_at, status.user.name,         <br/>              status.user.screen_name, status.user.followers_count, status.user.location]) <br/>            print(self.statuses)<br/>            print(f"\n*** Limit of {self.limit} met ***")<br/>            return False<br/>        if len(self.statuses) &gt; self.limit:<br/>            return False<br/><br/><br/>streaming = tweepy.Stream(auth=setup.api.auth, listener=Streamer())<br/><br/>items = ["@berniesanders", "@kamalaharris", "@joebiden", "@ewarren"]  # Keywords to track<br/><br/>stream_data = streaming.filter(track=items)</span><span id="367e" class="nf ks iq nb b gy nk nh l ni nj">### Data ###<br/><br/><br/>df = pd.read_csv(r"/tweet_data.csv", names= ["id", "text", "date", "name",<br/>                                                                 "username", "followers", "loc"])<br/><br/><br/>### Data Cleaning ###<br/><br/>ppl = ["berniesanders", "kamalaharris", "joebiden", "ewarren"]<br/><br/><br/>def clean(txt):<br/>    txt = str(txt.translate(str.maketrans("", "", string.punctuation))).lower()<br/>    txt = str(txt).split()<br/>    for item in txt:<br/>        if "http" in item:<br/>            txt.remove(item)<br/>        for item in ppl:<br/>            if item in txt:<br/>                txt.remove(item)<br/>    txt = (" ".join(txt))<br/>    return txt<br/><br/><br/>df.text = df.text.apply(clean)<br/><br/><br/><br/>### Data Prep ###<br/><br/># print(stopwords)<br/><br/># If you want to add to the stopwords list: stopwords = stopwords.union(set(["add_term_1", "add_term_2"]))<br/><br/><br/><br/>### Lemmatize and Stem ###<br/><br/>stemmer = stemm(language="english")<br/><br/><br/>def lemm_stemm(txt):<br/>    return stemmer.stem(lemm().lemmatize(txt, pos="v"))<br/><br/><br/>def preprocess(txt):<br/>    r = [lemm_stemm(token) for token in simple_preprocess(txt) if       token not in stopwords and len(token) &gt; 2]<br/>    return r<br/><br/><br/>proc_docs = df.text.apply(preprocess)<br/><br/><br/><br/>### LDA Model ###<br/><br/>dictionary = gensim.corpora.Dictionary(proc_docs)<br/>dictionary.filter_extremes(no_below=5, no_above= .90)<br/># print(dictionary)<br/><br/>n = 5 # Number of clusters we want to fit our data to<br/>bow = [dictionary.doc2bow(doc) for doc in proc_docs]<br/>lda = gensim.models.LdaMulticore(bow, num_topics= n, id2word=dictionary, passes=2, workers=2)<br/># print(bow)<br/><br/>for id, topic in lda.print_topics(-1):<br/>    print(f"TOPIC: {id} \n WORDS: {topic}")<br/><br/><br/><br/>### Coherence Scoring ###<br/><br/>coh = CoherenceModel(model=lda, texts= proc_docs, dictionary = dictionary, coherence = "c_v")<br/>coh_lda = coh.get_coherence()<br/>print("Coherence Score:", coh_lda)<br/><br/>lda_display = pyldavis.prepare(lda, bow, dictionary)<br/>pyLDAvis.show(lda_display)</span></pre><p id="29c2" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA æ˜¯æ¢ç´¢æ–‡æœ¬æ•°æ®çš„ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å‹ï¼Œå°½ç®¡å®ƒéœ€è¦å¤§é‡çš„ä¼˜åŒ–(å–å†³äºç”¨ä¾‹)æ¥ç”¨äºç”Ÿäº§ã€‚åœ¨ç¼–å†™ã€è¯„ä¼°å’Œæ˜¾ç¤ºæ¨¡å‹æ—¶ï¼Œgensimã€nltk å’Œ pyLDAvis åŒ…æ˜¯æ— ä»·çš„ã€‚</p><p id="7ec1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">éå¸¸æ„Ÿè°¢ä½ è®©æˆ‘åˆ†äº«ï¼Œä»¥åè¿˜ä¼šæœ‰æ›´å¤šã€‚ğŸ˜ƒ</p></div></div>    
</body>
</html>