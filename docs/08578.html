<html>
<head>
<title>Exploring Textual Data using LDA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 LDA 探索文本数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-textual-data-using-lda-ef1f53c772a4?source=collection_archive---------29-----------------------#2019-11-19">https://towardsdatascience.com/exploring-textual-data-using-lda-ef1f53c772a4?source=collection_archive---------29-----------------------#2019-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="99b9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过应用机器学习原理来理解非结构化文本数据。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2f1f31b7ba55de7672e9c0a6fe499984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2M9d044f6mXCEDNksclm8g.png"/></div></div></figure><h1 id="4aa6" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated"><strong class="ak">简介</strong></h1><p id="140e" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我最近在工作中完成了我的第一个机器学习项目，并决定将该项目中使用的方法应用到我自己的项目中。我在工作中完成的项目围绕着使用<a class="ae mf" href="https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation" rel="noopener ugc nofollow" target="_blank">潜在狄利克雷分配</a> (LDA)对文本数据进行自动分类。</p><p id="84a6" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA 是自然语言处理领域中的一种无监督机器学习模型。由于其无监督的性质，LDA 不需要标记的训练集。这使得它非常适合某些用例，或者当大型的、带标签的文本数据集不容易获得时。</p><p id="f785" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA 主要用于主题建模，通过相似性对文本文档进行聚类。文档大小可以小到一个单词(不理想),大到整个出版物。LDA 聚类的内容是使用每个文档中的术语(单词)以及它们出现的频率来确定的，有时甚至是它们出现的顺序(使用<a class="ae mf" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n-grams </a>)。被认为彼此相似的文档被聚类在一起，并且我们假设每个聚类代表一个主题，尽管直到聚类被创建之后我们才知道主题本身是什么。需要指出的是<strong class="ll ir">模型既不理解这些集群中文档的内容也不理解其上下文</strong>，因此实际上不能给集群一个主题标签。相反，它使用来自(<em class="ml"> 0 </em>，<em class="ml"> n)的索引整数来“标记”每个聚类；</em> <em class="ml"> n </em>是我们告诉模型要寻找的主题数量。一个人，或者说<a class="ae mf" href="https://www.independent.co.uk/news/world/europe/beluga-whale-catch-hvaldimir-russian-spy-programme-video-a9197106.html" rel="noopener ugc nofollow" target="_blank">非常聪明的水生哺乳动物</a>，需要分析这些聚类，并确定每个聚类应该如何被标记。</p><p id="65a4" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">在这篇文章中，我们将清理一些 Twitter 数据，并编写一个 LDA 模型来对这些数据进行聚类。然后我们将使用<a class="ae mf" href="https://pyldavis.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a>来生成集群的交互式可视化。</p><p id="5496" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><strong class="ll ir">关键依赖:</strong>熊猫、<a class="ae mf" href="https://www.nltk.org" rel="noopener ugc nofollow" target="_blank"> nltk </a>、<a class="ae mf" href="https://pypi.org/project/gensim/" rel="noopener ugc nofollow" target="_blank"> gensim </a>、numpy、<a class="ae mf" href="https://pyldavis.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> pyLDAvis </a></p><p id="7d21" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><strong class="ll ir">这里有一些需要事先熟悉的定义:</strong></p><ol class=""><li id="f1ea" class="mm mn iq ll b lm mg lp mh ls mo lw mp ma mq me mr ms mt mu bi translated"><em class="ml">文档:</em>文本对象(例如 tweet)</li><li id="dee0" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><a class="ae mf" href="https://radimrehurek.com/gensim/corpora/dictionary.html" rel="noopener ugc nofollow" target="_blank"> <em class="ml">字典</em> </a> <em class="ml"> : </em>我们的文档集合中所有惟一标记(单词、术语)的列表，每个标记都有一个惟一的整数标识符</li><li id="59ed" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><a class="ae mf" href="https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/" rel="noopener ugc nofollow" target="_blank"><em class="ml"/></a><em class="ml">:</em>我们所有文档的集合，每个文档简化为一个矩阵列表，文档中的每个单词对应一个矩阵— <em class="ml">使用 gensim 的</em><a class="ae mf" href="https://kite.com/python/docs/gensim.corpora.Dictionary.doc2bow" rel="noopener ugc nofollow" target="_blank"><em class="ml">doc 2 bow</em></a><em class="ml">，每个矩阵表示为一个元组，带有一个</em> <strong class="ll ir"> <em class="ml">术语的唯一整数 id </em> </strong> <em class="ml">，索引为 0 和(例如，文档“the box was in the bigger box”将被简化为类似于[("the "，2)，(" box "，2)，(" was "，1)，(" in "，1)，(" bigger "，1)]的内容，但用" term "替换术语的唯一字典 id) </em></li><li id="84b8" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><em class="ml"> coherence score: </em>一个范围从 0 到 1 的浮点值，用于评估我们的模型和聚类数与我们的数据的吻合程度</li><li id="26f3" class="mm mn iq ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated"><em class="ml">集群:</em>代表一组文档的节点，一个推断的主题</li></ol><h1 id="de8f" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">1.数据</h1><p id="39ab" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">今年早些时候，我开始收集几十万条政治推文，最终目标是对推文及其元数据进行各种分析，为 2020 年美国总统大选做准备。</p><p id="78ca" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这篇文章的数据集将由 3500 条推文组成，其中至少提到以下一条:“@berniesanders”、“kamalaharris”、“joebiden”、“ewarren”(分别是伯尼·桑德斯、卡玛拉·哈里斯、乔·拜登和伊丽莎白·沃伦的推特账号)。我在 2019 年 11 月初收集了这些推文，并在这里提供下载<a class="ae mf" href="https://drive.google.com/drive/folders/1ebI3pEkrz3JbyF_aZF4DVPX2LSG1hVn_?usp=sharing" rel="noopener ugc nofollow" target="_blank">。我们将研究这些数据，并试图找出人们在 11 月初发推文的内容。</a></p><p id="081a" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我不会深入研究如何收集推文，但我已经包括了我在下面使用的代码。成功运行代码需要访问<a class="ae mf" href="http://tweepy.readthedocs.org" rel="noopener ugc nofollow" target="_blank"> tweepy API </a>。我没有收集转发，也没有收集不是用英语写的推文(该模型需要更多的调整以适应多种语言)。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="2440" class="nf ks iq nb b gy ng nh l ni nj">class Streamer(StreamListener):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.limit = 1000 # Number of tweets to collect.<br/>        self.statuses = []  # Pass each status here.<br/><br/>    def on_status(self, status):<br/>        if status.retweeted or "RT @" <br/>        in status.text or status.lang != "en":<br/>            return   # Remove re-tweets and non-English tweets.<br/>        if len(self.statuses) &lt; self.limit:<br/>            self.statuses.append(status)<br/>            print(len(self.statuses))  # Get count of statuses<br/>        if len(self.statuses) == self.limit:<br/>            with open("/tweet_data.csv", "w") as    file: <br/>                writer = csv.writer(file)  # Saving data to csv. <br/>                for status in self.statuses:<br/>                    writer.writerow([status.id, status.text,<br/>              status.created_at, status.user.name,         <br/>              status.user.screen_name, status.user.followers_count, status.user.location]) <br/>            print(self.statuses)<br/>            print(f"\n*** Limit of {self.limit} met ***")<br/>            return False<br/>        if len(self.statuses) &gt; self.limit:<br/>            return False<br/><br/><br/>streaming = tweepy.Stream(auth=setup.api.auth, listener=Streamer())<br/><br/>items = ["@berniesanders", "@kamalaharris", "@joebiden", "@ewarren"]  # Keywords to track<br/><br/>stream_data = streaming.filter(track=items)</span></pre><p id="69f2" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这会将 tweet 文本数据及其元数据(id、创建日期、名称、用户名、关注者数量和位置)传递给名为 tweet_data 的 csv。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d515" class="nf ks iq nb b gy ng nh l ni nj">import pandas as pd</span><span id="f2ef" class="nf ks iq nb b gy nk nh l ni nj">df = pd.read_csv(r"/tweet_data.csv", names= ["id", "text", "date", "name", "username", "followers", "loc"])</span></pre><p id="91f0" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">现在我们已经将数据打包到一个整洁的 csv 中，我们可以开始为我们的 LDA 机器学习模型准备数据了。文本数据通常被视为非结构化数据，在进行有意义的分析之前需要清理。由于不一致的性质，推文尤其混乱。例如，任何给定的 Twitter 用户可能某一天用完整的句子发推，而第二天用单个单词和标签发推。另一个用户可能只发链接，另一个用户可能只发标签。除此之外，还有用户可能会有意忽略的语法和拼写错误。还有一些口语中使用的术语不会出现在标准英语词典中。</p><h2 id="6473" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">清洁</h2><p id="03bd" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">我们将删除所有标点符号、特殊字符和 url 链接，然后对每条推文应用<em class="ml"> lower() </em>。这为我们的文档带来了一定程度的一致性(记住每条 tweet 都被视为一个文档)。我还删除了“berniesanders”、“kamalaharris”、“joebiden”和“ewarren”的实例，因为它们会扭曲我们的词频，因为每个文档至少会包含其中一项。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="777b" class="nf ks iq nb b gy ng nh l ni nj">import string</span><span id="ee74" class="nf ks iq nb b gy nk nh l ni nj">ppl = ["berniesanders", "kamalaharris", "joebiden", "ewarren"]</span><span id="4eb2" class="nf ks iq nb b gy nk nh l ni nj">def clean(txt):<br/>    txt = str(txt.translate(str.maketrans("", "", string.punctuation))).lower() <br/>    txt = str(txt).split()<br/>    for item in txt:<br/>        if "http" in item:<br/>            txt.remove(item)<br/>        for item in ppl:<br/>            if item in txt:<br/>                txt.remove(item)<br/>    txt = (" ".join(txt))<br/>    return txt<br/>    <br/>df.text = df.text.apply(clean)</span></pre><h1 id="c3e7" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">2.数据准备</h1><p id="e79f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">下面是我们需要导入的包，以便在将数据输入模型之前准备好数据。<strong class="ll ir">在编写数据准备的代码时，我也会包括这些导入。</strong></p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="55b9" class="nf ks iq nb b gy ng nh l ni nj">import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS as stopwords<br/>import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(0)</span></pre><p id="d3f2" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我们已经清理了一些文档，但是现在我们需要<a class="ae mf" href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html" rel="noopener ugc nofollow" target="_blank">对它们进行词法分析和词干分析。词汇化将文档中的单词转换为第一人称，并将所有动词转换为现在时。词干处理将文档中的单词还原为它们的根格式。幸运的是，nltk 有一个 lemmatizer 和一个词干分析器可供我们利用。</a></p><p id="300d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA 涉及到一个<a class="ae mf" href="https://en.wikipedia.org/wiki/Stochastic_process" rel="noopener ugc nofollow" target="_blank">随机过程</a>，意味着我们的模型需要产生随机变量的能力，因此有了<em class="ml"> numpy </em>导入。添加<em class="ml"> numpy.random.seed(0) </em>允许我们的模型是可重复的，因为它将生成并使用相同的随机变量，而不是在每次代码运行时生成新的变量。</p><p id="89a9" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">Gensim 的停用词是一个被认为不相关或可能混淆我们词汇的术语列表。在 NLP 中，“停用词”指的是我们不希望模型选取的术语集合。此列表将用于从我们的文档中删除这些不相关的术语。我们可以<em class="ml"> print(stopwords) </em>来查看将要删除的术语。</p><p id="a471" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">以下是停用词中的术语。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/4a4660ca13aabb8f171d89414ecb6f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxUowoI97ML7MsR54QSZHA.png"/></div></div></figure><p id="ce1d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">对于这个模型，我们将保持停用词列表不变，但在某些情况下，可能需要添加我们希望模型忽略的特定术语。下面的代码是向停用词添加术语的一种方法。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="2f71" class="nf ks iq nb b gy ng nh l ni nj">stopwords = stopwords.union(set(["add_term_1", "add_term_2"]))</span></pre><h2 id="3b3e" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">词汇化和词干化</h2><p id="473c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">让我们为我们的数据准备写一些代码。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="cc65" class="nf ks iq nb b gy ng nh l ni nj">import warnings <br/>warnings.simplefilter("ignore")<br/>import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS as stopwords<br/>import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(0)</span></pre><p id="c499" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">初始化词干分析器。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="acda" class="nf ks iq nb b gy ng nh l ni nj">stemmer = stemm(language="english")</span></pre><p id="5a5f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">写一个函数，既能对我们的文档进行词汇化，又能对其进行词干分析。GeeksforGeeks 有关于使用 nltk 进行词法分析的<a class="ae mf" href="https://www.geeksforgeeks.org/python-lemmatization-with-nltk/" rel="noopener ugc nofollow" target="_blank">个例子</a>和关于使用 nltk 进行词干分析的<a class="ae mf" href="https://www.geeksforgeeks.org/python-stemming-words-with-nltk/" rel="noopener ugc nofollow" target="_blank">个例子</a>。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="8dc5" class="nf ks iq nb b gy ng nh l ni nj">def lemm_stemm(txt):<br/>    return stemmer.stem(lemm().lemmatize(txt, pos="v"))</span></pre><p id="5197" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">编写一个函数，将停用词从我们的文档中删除，同时也应用<em class="ml">lemm _ stem()</em>。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="47d7" class="nf ks iq nb b gy ng nh l ni nj">def preprocess(txt):<br/>    r = [lemm_stemm(token) for token in simple_preprocess(txt) if       token not in stopwords and len(token) &gt; 2]<br/>    return r</span></pre><p id="276c" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">将我们清理和准备好的文档分配给一个新变量。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="691c" class="nf ks iq nb b gy ng nh l ni nj">proc_docs = df.text.apply(preprocess)</span></pre><h1 id="dfeb" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated"><strong class="ak"> 3。模型的制作</strong></h1><p id="c94b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在我们已经准备好了数据，我们可以开始编写模型了。</p><h2 id="bd7d" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">词典</h2><p id="480b" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">正如引言中提到的，字典(在 LDA 中)是在我们的文档集合中出现的所有唯一术语的列表。我们将使用 gensim 的语料库包来构建我们的词典。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="b86c" class="nf ks iq nb b gy ng nh l ni nj">dictionary = gensim.corpora.Dictionary(proc_docs)<br/>dictionary.filter_extremes(no_below=5, no_above= .90)<br/>len(dictionary)</span></pre><p id="2719" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="ml"> filter_extremes() </em>参数是针对停用词或其他常用术语的第二道防线，这些停用词或常用术语对句子的意义没有什么实质意义。摆弄这些参数可以帮助微调模型。关于这一点我就不赘述了，但我在下面附上了来自<a class="ae mf" href="https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes" rel="noopener ugc nofollow" target="_blank"> gensim 的字典文档</a>中解释参数的截图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/dd6a330c2e96aa11a879313a4102ccbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMTP88f0Y-tNlwWZeaECEw.png"/></div></div></figure><p id="bb80" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我们的字典有 972 个独特的单词(术语)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/b664daf8f2317fe5a1b408b2ef0a615c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPxWahQ59en9eVi6OKkJxQ.png"/></div></div></figure><h2 id="0898" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">词汇袋</h2><p id="077f" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">如引言中所述，单词包(在 LDA 中)是我们分解成矩阵的所有文档的集合。矩阵由术语的标识符和它在文档中出现的次数组成。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="7360" class="nf ks iq nb b gy ng nh l ni nj">n = 5 # Number of clusters we want to fit our data to<br/>bow = [dictionary.doc2bow(doc) for doc in proc_docs]<br/>lda = gensim.models.LdaMulticore(bow, num_topics= n, id2word=dictionary, passes=2, workers=2)</span><span id="50ac" class="nf ks iq nb b gy nk nh l ni nj">print(bow)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/dda660754596d151ab078d7a417b9725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zMTHjPvWKA63SU6qeSEOCg.png"/></div></div></figure><p id="571f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">让我们通过查看定义集群的关键术语来了解我们的集群是如何形成的。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="808b" class="nf ks iq nb b gy ng nh l ni nj">for id, topic in lda.print_topics(-1):<br/>    print(f"TOPIC: {id} \n WORDS: {topic}")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/dfd835e776d590711bb614aab00066a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NRDJOaTcf5430nLlDUHSHw.png"/></div></div></figure><p id="d22f" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">查看每个主题群，我们可以了解它们代表了什么。看一下题目 1 和题目 4。</p><p id="e4da" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="ml">关于话题 1: </em>在话题 1 中，关键词“cenkuygur”和“anakasparian”是指<a class="ae mf" href="https://twitter.com/cenkuygur" rel="noopener ugc nofollow" target="_blank"> Cenk 维吾尔族</a> <strong class="ll ir"> </strong>和<strong class="ll ir"/><a class="ae mf" href="https://twitter.com/AnaKasparian" rel="noopener ugc nofollow" target="_blank">Ana Kasparian</a><strong class="ll ir">，</strong>共同主持人<strong class="ll ir"/><a class="ae mf" href="https://tyt.com" rel="noopener ugc nofollow" target="_blank">少壮派</a>(某政论事务所及节目)。主题 1 还包括关键术语“权利”、“特朗普”和“全国步枪协会”。</p><p id="e403" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">11 月 15 日，加州圣塔克拉里塔附近的索格斯高中发生了校园枪击案。关于这一悲剧事件，T2 媒体进行了大量报道，网上也议论纷纷。年轻的土耳其人(TYT)是更严格的枪支法律的口头支持者，并经常与全国步枪协会和其他枪支团体发生冲突。TYT 甚至带头发起了名为#NeverNRA 的承诺<a class="ae mf" href="https://join.tyt.com/nevernra/" rel="noopener ugc nofollow" target="_blank">运动。</a></p><p id="8a02" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这个主题群可以被标为“TYT 对全国步枪协会”，或类似的东西。</p><p id="06bf" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated"><em class="ml">关于主题 4: </em>术语“cenkuygur”和“anakasparian”在主题 4 中重复出现。话题 4 还包括“theyoungturk”，指的是年轻的土耳其人，以及“berni”，指的是伯尼·桑德斯。</p><p id="afc4" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">11 月 12 日，岑克维为候选人伯尼·桑德斯发布<a class="ae mf" href="https://youtu.be/m4mspXXNiqg" rel="noopener ugc nofollow" target="_blank">公开背书</a>。TYT 的推特账户重复了这一表态。伯尼·桑德斯随后公开感谢他们的支持。此外，11 月 14 日，维吾尔先生宣布他将竞选国会议员。这两项进展都在 Twitter 上获得了显著关注。</p><p id="d70d" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这个主题群可以被称为“TYT 和伯尼·桑德斯”，或者类似的名称。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/54e5092405606881b103a2bf6640b83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFGyxTEwyvAc0rYqbYLa7Q.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/d3260669357d052515659a82da1e67ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ux014P2oIP9OiDe5r-n0WA.png"/></div></div></figure><p id="11f5" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">其他主题群也有类似的解释。</p><h1 id="ae00" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated"><strong class="ak"> 4。评估、可视化、结论</strong></h1><p id="7051" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">大多数好的机器学习模型和应用都有一个反馈环。这是一种评估模型的性能、可伸缩性和整体质量的方法。在主题建模空间中，我们使用<a class="ae mf" href="http://qpleple.com/topic-coherence-to-evaluate-topic-models/" rel="noopener ugc nofollow" target="_blank">一致性分数</a>来确定我们的模型有多“一致”<em class="ml"> </em>。正如我在介绍中提到的，coherence 是一个介于 0 和 1 之间的浮点值。为此我们也将使用 gensim。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="d73b" class="nf ks iq nb b gy ng nh l ni nj"># Eval via coherence scoring</span><span id="f743" class="nf ks iq nb b gy nk nh l ni nj">from gensim import corpora, models<br/>from gensim.models import CoherenceModel<br/>from pprint import pprint</span><span id="85f8" class="nf ks iq nb b gy nk nh l ni nj">coh = CoherenceModel(model=lda, texts= proc_docs, dictionary = dictionary, coherence = "c_v")<br/>coh_lda = coh.get_coherence()<br/>print("Coherence Score:", coh_lda)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/e6759410a9d51303dea0ea26310e3859.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4s1jO1KbBrCZyQhv_JX7gA.png"/></div></div></figure><p id="176b" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">我们得到了 0.44 的一致性分数。这不是最好的，但实际上也不算太差。这个分数是在没有任何微调的情况下获得的。真正挖掘我们的参数和测试结果应该会得到更高的分数。得分真的没有官方门槛。我的一致性分数目标通常在 0.65 左右。参见这篇<a class="ae mf" href="https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/" rel="noopener ugc nofollow" target="_blank">文章</a>和这个堆栈溢出<a class="ae mf" href="https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad" rel="noopener ugc nofollow" target="_blank">线程</a>了解更多关于一致性评分的信息。</p><h2 id="17d2" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated">用 pyLDAvis 可视化</h2><p id="b9c6" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">最后，我们可以使用 pyLDAvis 可视化我们的集群。这个包创建了一个聚类的距离图，沿着 x 和 y 轴绘制聚类。这个距离地图可以通过调用<em class="ml"> pyLDAvis.display() </em>在 Jupiter 中打开，也可以通过调用<em class="ml"> pyLDAvis.show() </em>在 web 中打开。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="508f" class="nf ks iq nb b gy ng nh l ni nj">import pyLDAvis.gensim as pyldavis<br/>import pyLDAvis</span><span id="d004" class="nf ks iq nb b gy nk nh l ni nj">lda_display = pyldavis.prepare(lda, bow, dictionary)<br/>pyLDAvis.show(lda_display)</span></pre><p id="2ce1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">这是我们的 pyLDAvis 距离图的截图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/4be55562fcd33d1fbf9e4d3a593b83cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T7R8bPrb5FIbC2OGj2_yLQ.png"/></div></div></figure><p id="7ccd" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">将鼠标悬停在每个集群上，会显示该集群中关键术语的相关性(红色)以及这些相同关键术语在整个文档集合中的相关性(蓝色)。这是向风险承担者展示调查结果的有效方式。</p><h2 id="b8f5" class="nf ks iq bd kt nl nm dn kx nn no dp lb ls np nq ld lw nr ns lf ma nt nu lh nv bi translated"><strong class="ak">结论</strong></h2><p id="46f9" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">这里是我上面使用的所有代码，包括我用来生成单词云的代码和我用来收集推文数据的代码。</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="895f" class="nf ks iq nb b gy ng nh l ni nj">### All Dependencies ###<br/><br/>import pandas as pd<br/>from wordcloud import WordCloud as cloud<br/>import matplotlib.pyplot as plt<br/>import string<br/>import gensim<br/>from gensim.utils import simple_preprocess<br/>from gensim.parsing.preprocessing import STOPWORDS as stopwords<br/>import nltk<br/>nltk.download("wordnet")<br/>from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm<br/>from nltk.stem.porter import *<br/>import numpy as np<br/>np.random.seed(0)<br/>from gensim import corpora, models<br/>from gensim.models import CoherenceModel<br/>from pprint import pprint<br/>import pyLDAvis.gensim as pyldavis<br/>import pyLDAvis<br/><br/><br/>### Word Cloud ###<br/><br/>df = pd.read_csv(r"/tweet_data.csv", names=["id", "text", "date", "name",<br/>                                                                 "username", "followers", "loc"])<br/><br/><br/>def clean(txt):<br/>    txt = str(txt).split()<br/>    for item in txt:<br/>        if "http" in item:<br/>            txt.remove(item)<br/>    txt = (" ".join(txt))<br/>    return txt<br/><br/><br/>text = (df.text.apply(clean))<br/><br/><br/>wc = cloud(background_color='white', colormap="tab10").generate(" ".join(text))<br/><br/>plt.axis("off")<br/>plt.text(2, 210, "Generated using word_cloud and this post's dataset.", size = 5, color="grey")<br/><br/>plt.imshow(wc)<br/>plt.show()</span><span id="ff0f" class="nf ks iq nb b gy nk nh l ni nj">### Stream &amp; Collect Tweets ###</span><span id="d357" class="nf ks iq nb b gy nk nh l ni nj">class Streamer(StreamListener):<br/>    def __init__(self):<br/>        super().__init__()<br/>        self.limit = 1000 # Number of tweets to collect.<br/>        self.statuses = []  # Pass each status here.<br/><br/>    def on_status(self, status):<br/>        if status.retweeted or "RT @" <br/>        in status.text or status.lang != "en":<br/>            return   # Remove re-tweets and non-English tweets.<br/>        if len(self.statuses) &lt; self.limit:<br/>            self.statuses.append(status)<br/>            print(len(self.statuses))  # Get count of statuses<br/>        if len(self.statuses) == self.limit:<br/>            with open("/tweet_data.csv", "w") as    file: <br/>                writer = csv.writer(file)  # Saving data to csv. <br/>                for status in self.statuses:<br/>                    writer.writerow([status.id, status.text,<br/>              status.created_at, status.user.name,         <br/>              status.user.screen_name, status.user.followers_count, status.user.location]) <br/>            print(self.statuses)<br/>            print(f"\n*** Limit of {self.limit} met ***")<br/>            return False<br/>        if len(self.statuses) &gt; self.limit:<br/>            return False<br/><br/><br/>streaming = tweepy.Stream(auth=setup.api.auth, listener=Streamer())<br/><br/>items = ["@berniesanders", "@kamalaharris", "@joebiden", "@ewarren"]  # Keywords to track<br/><br/>stream_data = streaming.filter(track=items)</span><span id="367e" class="nf ks iq nb b gy nk nh l ni nj">### Data ###<br/><br/><br/>df = pd.read_csv(r"/tweet_data.csv", names= ["id", "text", "date", "name",<br/>                                                                 "username", "followers", "loc"])<br/><br/><br/>### Data Cleaning ###<br/><br/>ppl = ["berniesanders", "kamalaharris", "joebiden", "ewarren"]<br/><br/><br/>def clean(txt):<br/>    txt = str(txt.translate(str.maketrans("", "", string.punctuation))).lower()<br/>    txt = str(txt).split()<br/>    for item in txt:<br/>        if "http" in item:<br/>            txt.remove(item)<br/>        for item in ppl:<br/>            if item in txt:<br/>                txt.remove(item)<br/>    txt = (" ".join(txt))<br/>    return txt<br/><br/><br/>df.text = df.text.apply(clean)<br/><br/><br/><br/>### Data Prep ###<br/><br/># print(stopwords)<br/><br/># If you want to add to the stopwords list: stopwords = stopwords.union(set(["add_term_1", "add_term_2"]))<br/><br/><br/><br/>### Lemmatize and Stem ###<br/><br/>stemmer = stemm(language="english")<br/><br/><br/>def lemm_stemm(txt):<br/>    return stemmer.stem(lemm().lemmatize(txt, pos="v"))<br/><br/><br/>def preprocess(txt):<br/>    r = [lemm_stemm(token) for token in simple_preprocess(txt) if       token not in stopwords and len(token) &gt; 2]<br/>    return r<br/><br/><br/>proc_docs = df.text.apply(preprocess)<br/><br/><br/><br/>### LDA Model ###<br/><br/>dictionary = gensim.corpora.Dictionary(proc_docs)<br/>dictionary.filter_extremes(no_below=5, no_above= .90)<br/># print(dictionary)<br/><br/>n = 5 # Number of clusters we want to fit our data to<br/>bow = [dictionary.doc2bow(doc) for doc in proc_docs]<br/>lda = gensim.models.LdaMulticore(bow, num_topics= n, id2word=dictionary, passes=2, workers=2)<br/># print(bow)<br/><br/>for id, topic in lda.print_topics(-1):<br/>    print(f"TOPIC: {id} \n WORDS: {topic}")<br/><br/><br/><br/>### Coherence Scoring ###<br/><br/>coh = CoherenceModel(model=lda, texts= proc_docs, dictionary = dictionary, coherence = "c_v")<br/>coh_lda = coh.get_coherence()<br/>print("Coherence Score:", coh_lda)<br/><br/>lda_display = pyldavis.prepare(lda, bow, dictionary)<br/>pyLDAvis.show(lda_display)</span></pre><p id="29c2" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">LDA 是探索文本数据的一个很好的模型，尽管它需要大量的优化(取决于用例)来用于生产。在编写、评估和显示模型时，gensim、nltk 和 pyLDAvis 包是无价的。</p><p id="7ec1" class="pw-post-body-paragraph lj lk iq ll b lm mg jr lo lp mh ju lr ls mi lu lv lw mj ly lz ma mk mc md me ij bi translated">非常感谢你让我分享，以后还会有更多。😃</p></div></div>    
</body>
</html>