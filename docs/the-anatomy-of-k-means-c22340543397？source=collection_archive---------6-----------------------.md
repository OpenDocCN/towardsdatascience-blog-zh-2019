# K 均值的剖析

> 原文：<https://towardsdatascience.com/the-anatomy-of-k-means-c22340543397?source=collection_archive---------6----------------------->

## K 均值聚类算法完全指南

![](img/18e1eaaf741ca7488d2281dd543a2835.png)

Photo by [Ankush Minda](https://unsplash.com/@an_ku_sh?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

假设您想要根据内容和主题对数百(或数千)个文档进行分类，或者出于某种原因您希望将不同的图像组合在一起。或者更重要的是，让我们假设您已经对相同的数据进行了分类，但是您想要挑战这种标记。您想知道数据分类是否有意义，或者是否可以改进。

好吧，我的建议是你把你的数据聚集起来。信息经常被噪声和冗余所掩盖，将数据分组到具有相似特征的簇(聚类)是一种有效的方式来照亮一些信息。

**聚类**是一种广泛用于寻找具有相似特征的观察组(称为聚类)的技术。这个过程不是由特定的目的驱动的，这意味着您不必特别告诉您的算法如何对这些观察值进行分组，因为它会自己进行分组(组是有机形成的)。结果是同一组中的观察值(或数据点)之间比另一组中的其他观察值更相似。目标是获得相同组中尽可能相似的数据点，以及不同组中尽可能不相似的数据点。

K-means 非常适合探索性分析，是了解数据和提供几乎所有数据类型的见解的完美工具。无论是图像、图形还是文本，K-means 都非常灵活，几乎可以处理任何内容。

# **无监督学习领域的巨星之一**

聚类(包括 K 均值聚类)是一种用于数据分类的无监督学习技术。

**无监督学习**意味着没有输出变量来指导学习过程(没有这个或那个，没有对错)，数据由算法探索，以找到模式。我们只观察特征，但没有既定的结果衡量标准，因为我们想找出它们。

与监督学习相反，在监督学习中，你的现有数据已经被标记，并且你知道你想要在你获得的新数据中确定哪种行为，无监督学习技术不使用标记数据，算法自行发现数据中的结构。

在聚类技术领域中， **K-means** 可能是最广为人知和最常用的方法之一。K-means 使用迭代优化方法，根据用户定义的聚类数(由变量 *K* 表示)和数据集产生最终的聚类。例如，如果您将 K 设置为等于 3，那么您的数据集将被分组为 3 个簇，如果您将 K 设置为等于 4，那么您将数据分组为 4 个簇，依此类推。

K-means 从任意选择的数据点开始，如建议的数据组的 **means** ，并迭代地重新计算新的均值，以便收敛到数据点的最终聚类。

但是，如果您只是提供一个值(K)，算法如何决定如何对数据进行分组呢？当你定义 K 的值时，你实际上是在告诉算法你想要多少个均值或*质心*(如果你设置 K=3，你创建了 3 个均值或质心，这占了 3 个聚类)。**质心**是代表聚类中心(平均值)的数据点，它不一定是数据集的成员。

算法是这样工作的:

1.  随机创建 K 个质心(基于预定义的 K 值)
2.  K-means 将数据集中的每个数据点分配到最近的质心(最小化它们之间的欧几里德距离)，这意味着如果一个数据点比任何其他质心都更靠近该聚类的质心，则该数据点被视为在特定的聚类中
3.  然后，K-means 通过取分配给质心的聚类的所有数据点的平均值来重新计算质心，从而相对于前一步骤减少了总的聚类内方差。K-means 中的“means”指的是平均数据并找到新的质心
4.  该算法在步骤 2 和 3 之间迭代，直到满足某些标准(例如，数据点和它们对应的质心之间的距离之和最小化、达到最大迭代次数、质心值没有变化或者没有数据点改变聚类)

![](img/33da2235484d5ad702c84376f3a54bee.png)

In this example, after 5 iterations the calculated centroids remain the same, and data points are not switching clusters anymore (the algorithm converges). Here, each centroid is shown as a dark coloured data point. Source: [http://ai.stanford.edu](http://ai.stanford.edu)

运行该算法的初始结果可能不是最佳结果，使用不同的随机起始质心重新运行该算法可能会提供更好的性能(不同的初始对象可能会产生不同的聚类结果)。因此，通常的做法是以不同的起点运行算法多次，并评估不同的启动方法(例如，Forgy 或 Kaufman 方法)。

但是另一个问题出现了:你如何知道 K 的正确值，或者要创建多少个质心？对此没有通用的答案，尽管质心或聚类的最佳数量不是先验已知的，但是存在不同的方法来尝试估计它。一种常用的方法是测试不同数量的聚类，并测量所得的误差平方和，选择 K 值，在该值处增加将导致误差和非常小的减少，而减少将急剧增加误差和。定义最佳聚类数的这个点被称为**“肘点”**，并且可以被用作寻找 k 值的最佳选择的视觉测量。

![](img/f608c743ad206d1af2cbf564ee17c2dc.png)

In this example, the elbow point is located in 3 clusters

K-means 是您的数据科学工具包中的必备工具，这有几个原因。首先，它易于实施并带来高效的 T2 性能。毕竟，您只需要定义一个参数(K 的值)就可以看到结果。它也是**快**，并且与**大数据集**配合得非常好，使它能够处理当前的海量数据。它非常灵活，几乎可以用于任何数据类型，而且它的结果比其他算法更容易解释。此外，该算法非常受欢迎，以至于你可以在几乎任何学科中找到用例及实现。

# 但是任何事情都有不利的一面

然而，K-means 存在一些缺点。第一个是你需要定义**个集群，**这个决定会严重影响结果。此外，由于初始质心的位置是随机的，结果可能不可比，并显示出缺乏一致性。K-means 生成具有**统一大小**的聚类(每个聚类具有大致相等数量的观察值)，即使数据可能以不同的方式表现，并且它对异常值和噪声数据非常敏感。此外，它假设每个聚类中的数据点被建模为位于该聚类质心周围的球面内(**球面限制**)，但是当违反该条件(或任何先前的条件)时，该算法会以非直观的方式表现。

![](img/1d9bdbda240aa4b6618dda251f441fb9.png)

Example 1

**示例 1:** 在左侧是数据的直观聚类，两组数据点之间有明显的分隔(一个小环被一个大环包围)。在右侧，相同的数据点由 K-means 算法聚类(K 值为 2)，其中每个质心用菱形表示。如您所见，该算法无法识别直观的聚类。

![](img/b6d66e548d4197b79f8327c7cb23d332.png)

Example 2

**例 2:** 左手边是两个可识别数据组的聚类。在右侧，相同数据点上的 K-means 聚类的结果不符合直观的聚类。与示例 1 的情况一样，由于算法的球面限制，K-means 创建的分区不能反映我们视觉识别的内容。它试图寻找周围有整齐数据球体的质心，当星团的几何形状偏离球体时，它的表现很差。

![](img/b64a16b18d46401798772db37984b463.png)

Example 3

**示例 3:** 同样，在左侧有 K-means 未能识别的两个清晰的聚类(一个小且紧密的数据组和另一个较大且分散的数据组)(右侧)。在这里，为了平衡两个数据组之间的聚类内距离并生成大小一致的聚类，该算法混合两个数据组并创建两个不代表数据集的人工聚类。

有趣的是，K-means 不允许彼此远离的数据点共享同一个聚类，不管这些数据点之间的关系可能有多明显。

# **现在该怎么办？**

事实是，现实生活中的数据几乎总是复杂的、杂乱的和嘈杂的。现实世界中的情况很少反映出应用现成算法的明确条件。在 K-means 算法的情况下，预计*至少有一个假设被违反，因此我们不仅需要识别这一点，还需要知道在这种情况下该做什么。*

好消息是有替代方案，缺陷可以被纠正。比如将数据转换为[极坐标](https://www.r-bloggers.com/exploring-assumptions-of-k-means-clustering-using-r/)可以解决我们在例 1 中描述的球面限制。如果您发现严重的局限性，也可以考虑使用其他类型的聚类算法。可能的方法是使用基于[密度的](https://www.datascience.com/blog/k-means-alternatives)或基于[层次的](https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html)算法，它们修复了一些 K-means 的局限性(但也有它们自己的局限性)。

总之，K-means 是一个很棒的算法，有很多潜在的用途，所以它可以用于几乎任何类型的数据分组。但是从来没有免费的午餐:如果你不想被引导到错误的结果，你需要了解它的假设和运作方式。

感谢 [Sabrina Steinert](https://medium.com/@Binsi) 的宝贵意见

> 对这些话题感兴趣？在 Linkedin[或 Twitter](https://www.linkedin.com/in/lopezyse/) 上关注我