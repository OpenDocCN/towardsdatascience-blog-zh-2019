# 时间序列中经典模型上的神经网络

> 原文：<https://towardsdatascience.com/neural-networks-over-classical-models-in-time-series-5110a714e535?source=collection_archive---------19----------------------->

*为什么我们不应该止步于时间序列分析中的经典模型，为什么我们应该进一步释放神经网络的力量。*

本文讨论了各种神经网络在时间序列建模中的能力。

与监督和非监督学习模型相比，基于时间序列预测的经典机器学习模型很难实现，因为数据中存在时间差异:我们在不同的时间步长对相同数据绘制的数据进行处理。这使得模型拟合和模型评估的过程相对困难。

ARIMA 是市场上非常受欢迎的工具，因为它可以用于几乎任何类型的时间序列数据，并且非常容易理解和有效地实施。深度学习方法可以克服经典模型的 3 个主要限制，如下所述。

**经典模型的局限性:(如霍尔特-温特斯，ARIMA，其他指数模型)**

1.  不支持缺少值。
2.  假设数据具有线性关系，即，显示趋势分量具有总体下降或上升趋势

![](img/91ab42fa4522013ae0a8621d1f89fdb8.png)

Types of trends shown in Boston marathon data**

我们可以看到，线性趋势线显示了数据的总体下降趋势。而“三次样条”是一条非线性趋势线，它捕捉了数据中更直观的模式(非线性模式)。

3.这些模型处理单变量数据。时间序列预测中的大多数模型不支持将多个变量作为输入。我们只关注一个变量:感兴趣的结果只是输入变量的未来版本(在未来的时间步)。这种情况的例外是季节性 ARIMA 或萨里玛等模型，它们接受外生变量来建模数据。

**深度学习神经网络**

像多层感知器、RNNs(递归神经网络)和卷积神经网络这样的深度学习网络在时间序列预测方面有自己的一套优势和功能。

*多层感知器*:可以处理缺失值，模拟复杂关系(如非线性趋势)并支持多输入。

但是 MLPs 或多层感知器有一个缺点，即必须提供固定数量的输入来产生固定数量的输出:在模型设计中预先指定时间相关性。在这些前馈神经网络中，输入和输出之间有一个固定的映射函数，当向模型提供一系列输入时，这就产生了一个问题。

*复杂的神经网络*:使用这些网络的主要优势是它们可以很容易地完成特征工程。它们自动从提供给模型的原始输入中提取特征。

它们还共享多变量输入的相同功能，模拟复杂的非线性关系，并且对噪声(缺失值)具有鲁棒性。但是这些神经网络优于前馈神经网络，因为它们不需要直接从滞后观察中学习，而是学习与预测问题最相关的大输入序列的表示。

*递归神经网络，特别是 lst ms:*lst ms 还提供多变量输入、对噪声的鲁棒性、多变量输出、自动特征提取、对数据中更复杂的关系建模。他们还可以将输入序列数据作为单独的输入向量读入模型。

除了 MLP 和 CNN 提供的功能之外，LSTMs 还可以随着时间的推移学习从输入到输出的映射功能。在模型学习过程中，映射函数不再是固定或静态的。

敬请关注时间序列分析中深度学习实践的更多详细文章！

**参考文献:**

**图片摘自:Rob J. Hyndman 和 George Athanasopoulos 的《预测原理和实践》[在线文本:【https://otexts.com/fpp2/nonlinear-regression.html]