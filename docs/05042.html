<html>
<head>
<title>Getting Started with Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Apache Spark 入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-apache-spark-ad9d59e71f6f?source=collection_archive---------7-----------------------#2019-07-29">https://towardsdatascience.com/getting-started-with-apache-spark-ad9d59e71f6f?source=collection_archive---------7-----------------------#2019-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="770b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Java 中 Spark 的体系结构及应用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e5d9c392c4d0dd1873a14e51418d7419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dl0F36mQqB_LNtA_dKd3ug.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@markusspiske?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae ky" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e7e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache Spark 被解释为“大规模数据处理的快速通用引擎”然而，这甚至没有开始概括它成为大数据领域如此突出的参与者的原因。Apache Spark 是一个分布式计算平台，大数据公司对它的采用一直在以令人瞩目的速度增长。</p><h1 id="4100" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">火花建筑</h1><p id="85b6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">spark 的架构如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/fb7c867be1b781ce9cb7106a94c56510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQ5VWad1nu4yYt2MOuSBsQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Spark Eco-System — Image by Author</figcaption></figure><p id="6e07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark 是一个分布式处理引擎，但是它没有自己的分布式存储和资源集群管理器。它运行在现成的集群资源管理器和分布式存储之上。</p><p id="a0b5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark core 由两部分组成:</p><ul class=""><li id="fc05" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">核心 API:非结构化 API(rdd)，结构化 API(数据帧，数据集)。在 Scala、Python、Java 和 r 中可用。</li><li id="3254" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">计算引擎:内存管理、任务调度、故障恢复、与集群管理器交互。</li></ul><blockquote class="nh ni nj"><p id="799b" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:我们将在文章末尾看到核心 API 的 Java 实现。</p></blockquote><p id="acfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在核心 API 之外，Spark 提供了:</p><ul class=""><li id="3dbb" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">Spark SQL:通过类似 SQL 的查询与结构化数据进行交互。</li><li id="d036" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">流:消费和处理连续的数据流。</li><li id="6793" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">机器学习库。但是，我不会推荐在这里训练深度学习模型。</li><li id="0186" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">GraphX:典型的图形处理算法。</li></ul><p id="f483" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上四个都直接依赖于 spark 核心 API 进行分布式计算。</p><h2 id="150b" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">Spark 的优势</h2><ul class=""><li id="5050" class="mt mu it lb b lc mn lf mo li oa lm ob lq oc lu my mz na nb bi translated">Spark 为批处理、结构化数据处理、流等提供了一个统一的平台。</li><li id="b2e9" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">与 Hadoop 的 map-reduce 相比，spark 代码更容易编写和使用。</li><li id="5348" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">Spark 最重要的特性，它抽象了并行编程方面。Spark core 抽象了分布式存储、计算和并行编程的复杂性。</li></ul><p id="2b97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Apache Spark 的主要用例之一是大规模数据处理。我们创建程序并在 spark 集群上执行它们。</p><h2 id="5442" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">程序在集群上的执行</h2><p id="013f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在 spark cluster 上执行程序主要有两种方法:</p><ol class=""><li id="8454" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu od mz na nb bi translated">互动客户端如<a class="ae ky" href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-shell.html" rel="noopener ugc nofollow" target="_blank"><em class="nk"/></a><a class="ae ky" href="https://spark.apache.org/docs/2.2.0/api/python/pyspark.html" rel="noopener ugc nofollow" target="_blank"><em class="nk">py-spark</em></a>，笔记本等。</li><li id="a861" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">提交作业。</li></ol><p id="3d7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数开发过程发生在交互式客户机上，但是当我们必须将应用程序投入生产时，我们使用提交作业方法。</p><p id="8bc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于长时间运行的流作业或定期批处理作业，我们将应用程序打包并提交给 Spark cluster 执行。</p><p id="48f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark 是一个分布式处理引擎，遵循主从架构。在 spark 术语中，主人是<a class="ae ky" href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-driver.html" rel="noopener ugc nofollow" target="_blank"> <em class="nk">驱动者</em> </a>，奴隶是<a class="ae ky" href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-Executor.html" rel="noopener ugc nofollow" target="_blank"> <em class="nk">执行者</em> </a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/c1c4f5b14f962149f16ec9391a95958e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GnakBwO1h7R58gY5blxRiw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="e3b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">司机负责:</p><ol class=""><li id="c707" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu od mz na nb bi translated">分析</li><li id="7cb9" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">分发。</li><li id="1e6a" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">监控。</li><li id="2bfb" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">日程安排。</li><li id="6eef" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated">在 spark 进程的生命周期内维护所有必要的信息。</li></ol><p id="a62c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行者只负责执行驱动程序分配给他们的那部分代码，并将状态报告给驱动程序。</p><p id="9e30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个 spark 进程都有一个单独的驱动程序和独占的执行器。</p><h2 id="9f97" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">执行方式</h2><ol class=""><li id="c8ed" class="mt mu it lb b lc mn lf mo li oa lm ob lq oc lu od mz na nb bi translated"><strong class="lb iu">客户端模式:</strong>驱动程序是本地虚拟机，您可以在这里提交应用程序。默认情况下，spark 以客户端模式提交所有应用程序。由于驱动程序是整个 spark 流程中的主节点，因此在生产设置中，这是不可取的。对于调试，使用客户端模式更有意义。</li><li id="69c1" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu od mz na nb bi translated"><strong class="lb iu">集群模式:</strong>司机是集群中的执行者之一。在 spark-submit 中，您可以如下传递参数:</li></ol><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="5749" class="no lw it og b gy ok ol l om on">--deploy-mode cluster</span></pre><h2 id="b0e8" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated"><strong class="ak">集群资源管理器</strong></h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/5501c263f5e3fceeaffe1a178a98b711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3Qp5D5p4LgC_ScCOANDEQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="ca1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Yarn 和 Mesos 是常用的集群管理器。</p><p id="bc8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Kubernetes 是一个通用容器编制器。</p><blockquote class="nh ni nj"><p id="219f" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:在撰写本文时，Kubernetes 上的 Spark 还没有做好生产准备。</p></blockquote><p id="e524" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Yarn 是 spark 最受欢迎的资源管理器，让我们看看它的内部工作原理:</p><p id="e81a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在客户端模式应用程序中，驱动程序是我们的本地虚拟机，用于启动 spark 应用程序:</p><p id="6176" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 1: </strong>一旦驱动程序启动，spark 会话请求就会发送到 yarn 以创建一个 Yarn 应用程序。</p><p id="8dde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 2: </strong>纱线资源经理创建一个应用程序主程序。对于客户机模式，AM 充当执行器启动器。</p><p id="134b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第 3 步:</strong> AM 将联系纱线资源经理，要求提供更多容器。</p><p id="7e75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">步骤 4: </strong>资源管理器将分配新的容器，AM 将启动每个容器中的执行器。之后，执行者直接与司机沟通。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/e50a8d8b2f1361bc4b6386bd334f7d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1mmg-hFZ9NeJunygJ51Shg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><blockquote class="nh ni nj"><p id="4daf" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:在集群模式下，驱动程序在上午启动。</p></blockquote><h2 id="30ef" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated"><strong class="ak">执行器和内存调优</strong></h2><p id="6598" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">硬件— 6 个节点，每个节点 16 个内核，64 GB RAM </strong></p><p id="4f25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从核心的数量开始。核心的数量代表一个执行器可以运行的并发任务。研究表明，任何具有 5 个以上并发任务的应用程序都会导致糟糕的表现。因此，我建议坚持 5。</p><blockquote class="nh ni nj"><p id="6740" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:上面的数字来自于一个执行器的性能，而不是来自于系统有多少内核。因此，对于 32 核系统来说也是一样的。</p></blockquote><p id="fc27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">操作系统和 Hadoop 守护程序需要 1 个内核和 1 GB RAM。因此，我们只剩下 63 GB 内存和 15 个内核。</p><p id="5bf1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 15 个内核，每个节点可以有 3 个执行器。我们总共有 18 个遗嘱执行人。AM 容器需要 1 个执行器。因此我们可以得到 17 个遗嘱执行人。</p><p id="3cc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到内存，我们得到每个执行器 63/3 = 21 GB。但是，在计算完整的内存请求时，需要考虑少量的开销。</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="f4b7" class="no lw it og b gy ok ol l om on">Formula for that over head = max(384, .07 * spark.executor.memory)</span><span id="4011" class="no lw it og b gy oq ol l om on">Calculating that overhead = .07 * 21 = 1.47</span></pre><p id="019b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，内存下降到大约 19 GB。</p><p id="afd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，该系统得出:</p><pre class="kj kk kl km gt of og oh oi aw oj bi"><span id="49a8" class="no lw it og b gy ok ol l om on">--num-executors <!-- -->17 --executor-memory 19G --executor-cores 5 </span></pre><blockquote class="nh ni nj"><p id="ac54" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:如果我们需要更少的内存，我们可以减少内核的数量来增加执行器的数量。</p></blockquote><h1 id="94e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">火花核心</h1><p id="7aa1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在我们来看看 spark 提供的一些核心 API。Spark 需要一个数据结构来保存数据。我们有三种选择 RDD、数据帧和数据集。从 Spark 2.0 开始，建议只使用数据集和数据帧。这两个内部编译到 RDD 本身。</p><p id="b5d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这三个是弹性的、分布式的、分区的和不可变的数据集合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/c8fe7b7626e2ddf2e14471efe16f9588.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01tTyJMGdr8HPExoCBowCg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by Author</figcaption></figure><p id="4141" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">Task:</strong>Spark 中最小的工作单元，由一个执行者执行。</p><p id="a841" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集提供两种类型的操作:</p><ul class=""><li id="e911" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">转换:</strong>从现有的数据集创建新的数据集。它是懒惰的，数据仍然是分布式的。</li><li id="74d1" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><strong class="lb iu">动作:</strong>动作向驱动返回数据，本质上是非分布式的。数据集上的操作触发作业。</li></ul><p id="69a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">混洗和排序:</strong>对数据集进行重新分区，以便对其执行操作。它是 spark 中的一个抽象，我们不需要为它编写代码。这项活动需要一个新的阶段。</p><h1 id="d08c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">常见操作和转换</h1><h2 id="d66c" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">1) lit，geq，leq，gt，lt</h2><p id="9f10" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">lit:创建一个文字值列。可用于与其他列进行比较。</p><p id="e425" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">geq(大于等于)，leq(小于等于)，gt(大于)，lt(小于):用于与其他列值进行比较。例如:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="f2d9" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">2)加入</h2><p id="5b49" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Spark 允许我们以各种方式连接数据集。我将试着用一个例子来解释</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="7380" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="ec8e" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">3)工会</h2><p id="6af2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Spark 联合函数让我们在两个数据集之间建立一个联合。数据集应该具有相同的模式。</p><h2 id="5c1d" class="no lw it bd lx np nq dn mb nr ns dp mf li nt nu mh lm nv nw mj lq nx ny ml nz bi translated">4)窗户</h2><p id="7201" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Spark 的基本功能之一。它允许您基于一组行(称为<em class="nk">帧</em>)计算表中每个输入行的返回值。</p><p id="31d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark 提供了翻滚窗口、希望窗口、滑动窗口和延迟窗口的 API。</p><p id="df00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用它来排序、求和、普通的窗口等等。一些使用案例包括:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><p id="00ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其他函数，如<em class="nk"> lag </em>、<em class="nk"> lead </em>等等，允许您进行其他操作，使您能够对数据集进行复杂的分析。</p><p id="187c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，如果您仍然需要对数据集执行更复杂的操作，您可以使用 UDF。UDF 的用法示例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="os ot l"/></div></figure><blockquote class="nh ni nj"><p id="1257" class="kz la nk lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated">注意:使用 UDF 应该是最后的手段，因为它们不是为 Spark 优化的；他们可能需要更长的时间来执行死刑。建议在 UDF 上使用本机 spark 函数。</p></blockquote><p id="dcc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这只是 Apache Spark 的冰山一角。它的应用扩展到各个领域，不仅限于数据分析。请关注此空间了解更多信息。</p><h1 id="ff30" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ul class=""><li id="a164" class="mt mu it lb b lc mn lf mo li oa lm ob lq oc lu my mz na nb bi translated">https://www.youtube.com/watch?v=AYZCpxYVxH4&amp;list = plkz 1 SCF 5 IB 4d xipdfd 4 hxwhergrwhmd 6k</li><li id="ec50" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://stackoverflow.com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/37871194/how-to-tune-spark-executor-number-cores-and-executor-memory</a></li><li id="8ade" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://medium.com/@Farox2q/udfs-vs-map-vs-custom-spark-native-functions-91ab2c154b44" rel="noopener">https://medium . com/@ farox 2q/UDFs-vs-map-vs-custom-spark-native-functions-91 ab 2c 154 b 44</a></li><li id="8274" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated"><a class="ae ky" href="https://stackoverflow.com/questions/45990633/what-are-the-various-join-types-in-spark" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/45990633/what-the-variable-join-types-in-spark</a></li></ul></div></div>    
</body>
</html>