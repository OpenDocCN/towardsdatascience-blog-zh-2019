<html>
<head>
<title>Image-to-Image Translation using CycleGAN Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 CycleGAN 模型的图像到图像翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-to-image-translation-using-cyclegan-model-d58cfff04755?source=collection_archive---------14-----------------------#2019-11-15">https://towardsdatascience.com/image-to-image-translation-using-cyclegan-model-d58cfff04755?source=collection_archive---------14-----------------------#2019-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5e0b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种用于图像到图像翻译的无监督方法。</h2></div><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/593534e06b1fb892d400bbb8b09fcc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x9XVnbPYib5hrfTX"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Photo by <a class="ae lg" href="https://unsplash.com/@timmossholder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Tim Mossholder</a> on <a class="ae lg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><blockquote class="lh li lj"><p id="1498" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">如果你对 GANs 领域完全陌生，我建议你在开始写这篇文章之前先看看我以前的文章。</p></blockquote><div class="mh mi gp gr mj mk"><a rel="noopener follow" target="_blank" href="/fake-face-generator-using-dcgan-model-ae9322ccfd65"><div class="ml ab fo"><div class="mm ab mn cl cj mo"><h2 class="bd iu gy z fp mp fr fs mq fu fw is bi translated">基于 DCGAN 模型的人脸生成器</h2><div class="mr l"><h3 class="bd b gy z fp mp fr fs mq fu fw dk translated">概观</h3></div><div class="ms l"><p class="bd b dl z fp mp fr fs mq fu fw dk translated">towardsdatascience.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my la mk"/></div></div></a></div><blockquote class="lh li lj"><p id="4cb0" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">即使你不熟悉 GANs，我仍然建议你通读这篇文章，因为我们需要所有我们在上一篇文章中学到的基本概念来理解这篇文章。也就是说，让我们从这篇文章开始。</p></blockquote></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="a3db" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">循环生成对抗网络<em class="lm"> (CycleGAN) </em> </strong>，是一种训练深度卷积网络用于图像到图像翻译任务的方法。与其他用于图像转换任务的<em class="lm"> GAN </em> s 模型不同，C <em class="lm"> ycleGAN </em>使用<strong class="ln iu">无监督方法</strong>学习一个图像域和另一个图像域之间的映射。例如，如果我们对将马的图像转换成斑马的图像感兴趣，我们不需要将马物理转换成斑马的训练数据集。<em class="lm"> CycleGAN </em>做到这一点的方法是通过训练生成器网络来学习从域 X 到看起来像是来自域 Y 的图像的映射<em class="lm">(反之亦然)</em>。</p><blockquote class="lh li lj"><p id="b07c" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">当你阅读这篇文章时，你会对它是如何做到的有更深的理解。所以让我们开始吧…</p></blockquote><h1 id="84eb" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">CycleGAN</h1><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi ob"><img src="../Images/5b64487d31539135656c10c1f48f80d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_KxtJIVtZjVaxxl-Yl1vJg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk"><strong class="bd oc">CycleGAN architecture</strong>. Image from <a class="ae lg" href="https://modelzoo.co/model/mnist-svhn-transfer" rel="noopener ugc nofollow" target="_blank">https://modelzoo.co/model/mnist-svhn-transfer</a></figcaption></figure><blockquote class="lh li lj"><p id="0ba1" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">出于直觉，我们会参考上面的图像。</p></blockquote><p id="a2f1" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">对于成对的图像集，我们可以直接创建一个<em class="lm"> GAN </em>来借助 Pix2Pix 学习从 x 到 y 的映射。你可以在这里阅读更多关于 Pix2Pix Networks <a class="ae lg" href="https://machinelearningmastery.com/a-gentle-introduction-to-pix2pix-generative-adversarial-network/" rel="noopener ugc nofollow" target="_blank">的内容。</a></p><p id="2dc2" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">但是准备成对的数据集既费时又困难。我的意思是，我们需要一幅斑马的图像，它的位置和马的位置一样，或者有着相同的背景，这样我们才能学会绘制地图。</p><p id="0cb5" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">为了解决这个问题，CycleGAN 架构应运而生。CycleGAN 能够学习从一个域 X 到另一个域 Y 的映射，而不必寻找完全匹配的训练对！我们来看看 CycleGAN 是怎么做到的。</p><p id="9b7e" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">假设我们有一组来自域 X 的图像和一组来自域 y 的不成对的图像，我们希望能够将一组图像转换成另一组图像。为此，我们定义了一个映射 G <em class="lm"> (G: X- &gt; Y) </em>，它尽最大努力将 X 映射到 Y。但是对于不成对的数据，我们不再能够查看真实和虚假的数据对。但是我们知道我们可以改变我们的模型来产生一个属于目标领域的输出。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi od"><img src="../Images/e64ad5c283d4cbafc88720312ca48f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gOk-R94KA11T0B2kOau24w.png"/></div></div></figure><p id="22d1" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">因此，当你推送一匹马的图像时，我们可以训练一个生成器生成逼真的斑马图像。但问题是，我们不能强迫生成器的输出对应于它的输入<em class="lm">(在上面的图像中，第一个转换是正确的图像到图像的转换)。</em>这导致了一个被称为<strong class="ln iu">模式崩溃</strong>的问题，其中一个模型可能将来自域 X 的多个输入映射到来自域 Y 的同一个输出。在这种情况下，给定一匹输入马<em class="lm">(域 X) </em>，我们所知道的就是输出应该看起来像斑马<em class="lm">(域 Y) </em>。但是为了在相应的目标域中获得输入的正确映射，我们引入了一个额外的映射作为逆映射 G' <em class="lm"> (G': Y- &gt; X) </em>，它试图将 Y 映射到 X。这被称为<strong class="ln iu">循环一致性约束</strong>。</p><p id="71da" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">可以这样想，如果我们将一幅马的图像<em class="lm">(域 X) </em>翻译成一幅斑马的图像<em class="lm">(域 Y) </em>，然后我们再从斑马(域 Y)翻译回一匹马(域 X)，我们应该回到开始时的马的图像。</p><p id="c733" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">一个完整的翻译周期应该会让你回到开始时的图像。在从域 X 到 Y 的图像变换的情况下，如果满足以下条件，我们说图像从域 X 到域 Y 的变换是正确的。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bbb1bf4494e0e47a5ec9ea89154e3ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*AatbTQwB4m2F71gfcrlR8A.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">condition -1</figcaption></figure><p id="deaa" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">在循环一致性约束的帮助下，CycleGAN 确保模型学习到从域 X 到域 y 的正确映射。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="2718" class="nj nk it bd nl nm of no np nq og ns nt jz oh ka nv kc oi kd nx kf oj kg nz oa bi translated">图像到图像翻译任务</h1><p id="bd17" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">下面的任务被分解成一系列小任务，从<strong class="ln iu">加载和可视化数据到训练模型</strong>。</p><h2 id="618b" class="op nk it bd nl oq or dn np os ot dp nt ng ou ov nv nh ow ox nx ni oy oz nz pa bi translated">可视化数据集</h2><p id="e41c" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">具体来说，我们将看一组在夏季或冬季拍摄的约塞米蒂国家公园的照片。季节是我们的两个领域！</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pb"><img src="../Images/470e0248be845c9f597b16dd9d4ca888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qt2KKIU-1yPeI_stKYYEUg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Images from the summer season domain.</figcaption></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pc"><img src="../Images/e7ba2e611611ddf11b4e71aa68902139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cYhvnjN-zbImBxQFHC8LMQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Images from the winter season domain.</figcaption></figure><p id="9d27" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">一般来说，你可以看到夏天的图像比冬天的图像更亮更绿。冬天包含了像雪和多云图像这样的东西。在这个数据集中，我们的主要目标是训练一个生成器，学习将图像从夏天转换到冬天，反之亦然。这些图像不包含标签，被称为不成对的训练数据。但是通过使用 CycleGAN，我们可以使用无监督的方法学习从一个图像域到另一个图像域的映射。</p><blockquote class="lh li lj"><p id="e37c" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">你可以点击<a class="ae lg" href="https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be66e78_summer2winter-yosemite/summer2winter-yosemite.zip" rel="noopener ugc nofollow" target="_blank">这里</a>下载以下数据。</p></blockquote><h2 id="1616" class="op nk it bd nl oq or dn np os ot dp nt ng ou ov nv nh ow ox nx ni oy oz nz pa bi translated">定义模型</h2><p id="66b0" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">CycleGAN 包括两个鉴别器(<em class="lm"> D_x </em>和<em class="lm"> D_y </em>)和两个发生器(<em class="lm"> G_xtoy </em>和<em class="lm"> G_ytox </em>)。</p><ul class=""><li id="9726" class="pd pe it ln b lo lp lr ls ng pf nh pg ni ph mg pi pj pk pl bi translated"><em class="lm"> D_x </em> —将来自域 X 的训练图像识别为真实图像，将从域 Y 到域 X 的翻译图像识别为伪造图像。</li><li id="57a9" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated"><em class="lm"> D_y </em> —将来自域 X 的训练图像识别为真实图像，将从域 Y 到域 X 的翻译图像识别为伪造图像。</li><li id="6282" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated"><em class="lm"> G_xtoy </em> —将图像从域 X 转换到域 y。</li><li id="a60c" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated"><em class="lm"> G_ytox </em> —将图像从域 Y 转换到域 x。</li></ul><p id="e9e7" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">鉴别器</strong></p><p id="9367" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">在这个<em class="lm"> CycleGAN </em>中，鉴别器<em class="lm"> D_x </em>和<em class="lm"> D_y </em>是卷积神经网络，它们看到一幅图像并试图将其分类为真实或伪造。在这种情况下，real 由接近 1 的输出表示，而 fake 由接近 0 的输出表示。鉴别器具有以下架构:</p><pre class="kr ks kt ku gt pr ps pt pu aw pv bi"><span id="2950" class="op nk it ps b gy pw px l py pz"># helper conv function<br/>def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):<br/>    """Creates a convolutional layer, with optional batch normalization.<br/>    """<br/>    layers = []<br/>    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, <br/>                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)<br/>    <br/>    layers.append(conv_layer)<br/><br/>    if batch_norm:<br/>        layers.append(nn.BatchNorm2d(out_channels))<br/>    return nn.Sequential(*layers)<br/><br/>class Discriminator(nn.Module):<br/>    <br/>    def __init__(self, conv_dim=64):<br/>        super(Discriminator, self).__init__()<br/><br/>        # Define all convolutional layers<br/>        # Should accept an RGB image as input and output a single value<br/>        self.layer_1 = conv(3,conv_dim,4,batch_norm = False)<br/>        self.layer_2 = conv(conv_dim,conv_dim*2,4)<br/>        self.layer_3 = conv(conv_dim*2,conv_dim*4,4)<br/>        self.layer_4 = conv(conv_dim*4,conv_dim*8,4)<br/>        self.layer_5 = conv(conv_dim*8,1,4,1,batch_norm = False)<br/><br/>    def forward(self, x):<br/>        # define feedforward behavior<br/>        x = F.relu(self.layer_1(x))<br/>        x = F.relu(self.layer_2(x))<br/>        x = F.relu(self.layer_3(x))<br/>        x = F.relu(self.layer_4(x))<br/>        <br/>        x = self.layer_5(x)<br/>        return x</span></pre><p id="f4d9" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">解释</strong></p><ul class=""><li id="fe99" class="pd pe it ln b lo lp lr ls ng pf nh pg ni ph mg pi pj pk pl bi translated">以下架构由输出单个 logit 的五个卷积层组成。这个逻辑定义了图像是否真实。这种体系结构中没有完全连接的层。</li><li id="fb1b" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">除了第一层和最后一层，所有卷积层之后都是<em class="lm">批量归一化(在 conv 帮助函数中定义)</em>。</li><li id="d0bc" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">对于隐藏单元，使用 ReLU 激活功能。</li><li id="3e43" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">每次卷积后特征图的数量基于参数<em class="lm">conv _ 尺寸(在我的实现中 conv _ 尺寸= 64) </em>。</li></ul><blockquote class="lh li lj"><p id="830e" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">D_x 和 D_y 的架构都是一样的，所以我们只需要定义一个类，后面实例化两个鉴别器。</p></blockquote><h2 id="5c6b" class="op nk it bd nl oq or dn np os ot dp nt ng ou ov nv nh ow ox nx ni oy oz nz pa bi translated">剩余块和剩余函数</h2><p id="81d0" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">在定义生成器架构时，我们将在我们的架构中使用称为<strong class="ln iu"> Resnet 块</strong>和<strong class="ln iu">剩余函数</strong>的东西。使用 Resnet 块和剩余函数的思想如下:</p><p id="f114" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">残留块</strong></p><p id="90a1" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">残差块连接编码器和解码器。这种架构背后的动机如下:深度神经网络可能非常难以训练，因为它们更可能具有爆炸或消失的梯度，因此难以达到收敛；批处理规范化对此有所帮助。</p><p id="c55a" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">这个问题的一个解决方案是使用 Resnet 块，允许我们学习所谓的<em class="lm">剩余函数</em>，因为它们被应用于层输入。</p><p id="4c20" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">剩余功能</strong></p><p id="1af4" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">当我们创建深度学习模型时，该模型(应用了激活的若干层)负责学习从输入 x 到输出 y 的映射 M。</p><p id="956d" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><em class="lm"> M(x) = y </em></p><p id="8d7e" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">我们可以定义一个剩余函数，而不是学习从 x 到 y 的直接映射。</p><p id="7521" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><em class="lm"> F(x) = M(x)-x </em></p><p id="a75e" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">这着眼于应用于 x 的映射与原始输入 x 之间的差异。F(x)通常是两个卷积层+归一化层以及其间的 ReLU。这些卷积层应该具有与输出相同数量的输入。这种映射可以写成如下形式:剩余函数和输入 x 的函数。</p><p id="31f2" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><em class="lm"> M(x) = F(x) + x </em></p><p id="930a" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">你可以在这里阅读更多关于深度剩余学习<a class="ae lg" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">。下面是实现 Residual Block 的代码片段。</a></p><pre class="kr ks kt ku gt pr ps pt pu aw pv bi"><span id="d27a" class="op nk it ps b gy pw px l py pz">class ResidualBlock(nn.Module):<br/>    """Defines a residual block.<br/>       This adds an input x to a convolutional layer (applied to x) with the same size input and output.<br/>       These blocks allow a model to learn an effective transformation from one domain to another.<br/>    """<br/>    def __init__(self, conv_dim):<br/>        super(ResidualBlock, self).__init__()<br/>        # conv_dim = number of inputs  <br/>        <br/>        # define two convolutional layers + batch normalization that will act as our residual function, F(x)<br/>        # layers should have the same shape input as output; I suggest a kernel_size of 3<br/>        self.layer_1 = conv(conv_dim,conv_dim,3,1,1,batch_norm = True)<br/>        self.layer_2 = conv(conv_dim,conv_dim,3,1,1,batch_norm = True)<br/>        <br/>    def forward(self, x):<br/>        # apply a ReLu activation the outputs of the first layer<br/>        # return a summed output, x + resnet_block(x)<br/>        out_1 = F.relu(self.layer_1(x))<br/>        out_2 = x + self.layer_2(out_1)<br/>        <br/>        return out_2</span></pre><h2 id="7f3a" class="op nk it bd nl oq or dn np os ot dp nt ng ou ov nv nh ow ox nx ni oy oz nz pa bi translated">发电机</h2><p id="f8ed" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">生成器 G_xtoy 和 G_ytox 由编码器、将图像转换成小特征表示的 conv 网和解码器、负责将特征表示转换成变换图像的转置 conv 网组成。下面是实现生成器的代码片段。</p><pre class="kr ks kt ku gt pr ps pt pu aw pv bi"><span id="9692" class="op nk it ps b gy pw px l py pz">def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):<br/>    """Creates a transpose convolutional layer, with optional batch normalization.<br/>    """<br/>    layers = []<br/>    # append transpose conv layer<br/>    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))<br/>    # optional batch norm layer<br/>    if batch_norm:<br/>        layers.append(nn.BatchNorm2d(out_channels))<br/>    return nn.Sequential(*layers)<br/>    <br/>class CycleGenerator(nn.Module):<br/>    <br/>    def __init__(self, conv_dim=64, n_res_blocks=6):<br/>        super(CycleGenerator, self).__init__()<br/><br/>        # 1. Define the encoder part of the generator<br/>        self.layer_1 = conv(3,conv_dim,4)<br/>        self.layer_2 = conv(conv_dim,conv_dim*2,4)<br/>        self.layer_3 = conv(conv_dim*2,conv_dim*4,4)<br/>        # 2. Define the resnet part of the generator<br/>        layers = []<br/>        for n in range(n_res_blocks):<br/>            layers.append(ResidualBlock(conv_dim*4))<br/>        self.res_blocks = nn.Sequential(*layers)<br/>        # 3. Define the decoder part of the generator<br/>        self.layer_4 = deconv(conv_dim*4,conv_dim*2,4)<br/>        self.layer_5 = deconv(conv_dim*2,conv_dim,4)<br/>        self.layer_6 = deconv(conv_dim,3,4,batch_norm = False)<br/><br/>    def forward(self, x):<br/>        """Given an image x, returns a transformed image."""<br/>        # define feedforward behavior, applying activations as necessary<br/>        <br/>        out = F.relu(self.layer_1(x))<br/>        out = F.relu(self.layer_2(out))<br/>        out = F.relu(self.layer_3(out))<br/>        <br/>        out = self.res_blocks(out)<br/>        <br/>        out = F.relu(self.layer_4(out))<br/>        out = F.relu(self.layer_5(out))<br/>        out = F.tanh(self.layer_6(out))<br/>        <br/>        return out</span></pre><p id="1987" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">解说</strong></p><ul class=""><li id="5f9e" class="pd pe it ln b lo lp lr ls ng pf nh pg ni ph mg pi pj pk pl bi translated">下面的架构由编码器的三个卷积层和解码器的三个转置卷积层组成，它们都使用一系列残差块<em class="lm">(在我们的例子中为 6)连接。</em></li><li id="2b6f" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">所有卷积层之后是<em class="lm">批归一化</em>。</li><li id="d77d" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">除了最后一层，所有转置卷积层之后都是<em class="lm">批量归一化</em>。</li><li id="3159" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">对于隐藏单元，使用 ReLU 激活函数，除了最后一层，我们使用 tanh 激活函数，这是基于我们在之前的<a class="ae lg" rel="noopener" target="_blank" href="/fake-face-generator-using-dcgan-model-ae9322ccfd65">文章</a> <em class="lm">(训练 DCGAN 的技巧)</em>中的讨论。</li><li id="b98d" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">编码器和解码器中每次卷积后的特征图数量基于参数<em class="lm"> conv_dim </em>。</li></ul><blockquote class="lh li lj"><p id="cf30" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">G_xtoy 和 G_ytox 的架构都是一样的，所以我们只需要定义一个类，后面实例化两个生成器。</p></blockquote><h1 id="6997" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">培训过程</h1><blockquote class="lh li lj"><p id="1ba4" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">训练过程包括定义损失函数、选择优化器以及最后训练模型。</p></blockquote><h2 id="13ca" class="op nk it bd nl oq or dn np os ot dp nt ng ou ov nv nh ow ox nx ni oy oz nz pa bi translated">鉴频器和发电机损耗</h2><p id="1f80" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">我们已经看到，常规 GANs 将鉴别器视为具有 sigmoid 交叉熵损失函数的分类器。然而，这种损失函数可能导致学习过程中的消失梯度问题。为了解决这个问题，我们将对鉴别器使用最小二乘损失函数。这种结构通常被称为最小二乘 GANs，你可以从 LSGANs 的<a class="ae lg" href="https://arxiv.org/pdf/1611.04076.pdf" rel="noopener ugc nofollow" target="_blank">原始论文中读到更多关于它们的内容。</a></p><p id="a27d" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">鉴频器损耗</strong></p><p id="2e28" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">鉴别器损耗将是鉴别器的输出(给定图像)和目标值(0 或 1)之间的均方误差，这取决于它应该将该图像分类为假还是真。例如，对于一个<em class="lm">真实的</em>图像 x，我们可以通过观察它与使用均方误差将 x 识别为真实图像的接近程度来训练 D_x:</p><blockquote class="lh li lj"><p id="331b" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">out = D_x(x)</p><p id="fb93" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">real_error = torch.mean((out-1))(用于 Pytorch)</p></blockquote><p id="5120" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">发电机损耗</strong></p><p id="7c6b" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">在这种情况下，我们将生成看起来像是属于域 X 但却基于域 Y 的图像的假图像，反之亦然。我们将通过观察应用于这些伪图像的鉴频器的输出来计算这些生成图像的真实损失。</p><p id="0de0" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">除了对抗性损失，发电机损失项将包括循环一致性损失。这种损失是重建图像与原始图像相比有多好的量度。例如，我们有一个假的生成图像 x^和一个真实的图像 y，在 g _ xtoy<em class="lm">(g_xtoy(x^)= y^】</em>的帮助下，我们可以从 x^生成 y^。这里，周期一致性损失将是原始图像和重建图像之间的绝对差异。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/9783f7ebd477b74a34eb28a12f147cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*vQ9vNQ9VjnlPR748-Aizrg.png"/></div><figcaption class="lc ld gj gh gi le lf bd b be z dk"><strong class="bd oc">Cycle consistency loss</strong>. Image from <a class="ae lg" href="https://ssnl.github.io/better_cycles/report.pdf" rel="noopener ugc nofollow" target="_blank">https://ssnl.github.io/better_cycles/report.pdf</a></figcaption></figure><p id="8717" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">下面是定义损失的代码片段。</p><pre class="kr ks kt ku gt pr ps pt pu aw pv bi"><span id="21cf" class="op nk it ps b gy pw px l py pz">def real_mse_loss(D_out):<br/>    # how close is the produced output from being "real"?<br/>    return torch.mean((D_out - 1)**2)<br/><br/>    <br/>def fake_mse_loss(D_out):<br/>    # how close is the produced output from being "fake"?<br/>    return torch.mean(D_out**2)<br/><br/>def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):<br/>    # calculate reconstruction loss <br/>    # return weighted loss<br/>    loss = torch.mean(torch.abs(real_im - reconstructed_im))<br/>    return loss*lambda_weight</span></pre><p id="1c16" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><em class="lm">在周期一致性损失中，lambda 项是一个权重参数，它将对批次中的平均绝对误差进行加权。建议大家看一下</em> <a class="ae lg" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lm">原文，CycleGAN 论文</em> </a> <em class="lm">得到一个 lambda_weight 的起始值。</em></p><h2 id="992c" class="op nk it bd nl oq or dn np os ot dp nt ng ou ov nv nh ow ox nx ni oy oz nz pa bi translated">【计算机】优化程序</h2><p id="5453" class="pw-post-body-paragraph lk ll it ln b lo ok ju lq lr ol jx lt ng om lw lx nh on ma mb ni oo me mf mg im bi translated">对于 CycleGAN，我们为生成器<em class="lm"> (G_xtoy 和 G_ytox) </em>和 D_x 和 D_y 定义了三个优化器。所有数值超参数均选自<a class="ae lg" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank">原始 CycleGAN 文件</a>。</p><pre class="kr ks kt ku gt pr ps pt pu aw pv bi"><span id="9447" class="op nk it ps b gy pw px l py pz"># hyperparams for Adam optimizers<br/>lr= 0.0002<br/>beta1= 0.5<br/>beta2= 0.999<br/><br/>g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters<br/><br/># Create optimizers for the generators and discriminators<br/>g_optimizer = optim.Adam(g_params, lr, [beta1, beta2])<br/>d_x_optimizer = optim.Adam(D_X.parameters(), lr, [beta1, beta2])<br/>d_y_optimizer = optim.Adam(D_Y.parameters(), lr, [beta1, beta2])</span></pre><p id="9d90" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated"><strong class="ln iu">培训</strong></p><p id="11f8" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">当 CycleGAN 进行训练并看到来自集合 X 和 Y 的一批真实图像时，它通过执行以下步骤进行训练:</p><p id="b748" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">对于鉴别器:</p><ul class=""><li id="1fd7" class="pd pe it ln b lo lp lr ls ng pf nh pg ni ph mg pi pj pk pl bi translated">计算实际图像上的鉴别器 D_x 损失。</li><li id="6e70" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">在 G_ytox 的帮助下，使用集合 Y 中的图像生成伪图像，然后计算 D_x 的伪损失。</li><li id="0ea0" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">计算总损失并进行反向传播和优化。对 D_y 做同样的事情，你的域就转换了。</li></ul><p id="f172" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">对于发电机:</p><ul class=""><li id="bf03" class="pd pe it ln b lo lp lr ls ng pf nh pg ni ph mg pi pj pk pl bi translated">根据域 Y 中的实像生成看起来像域 X 的假像，然后根据 D_x 如何响应假像 X 计算发电机损耗。</li><li id="640a" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">基于步骤 1 中的伪 x 射线图像生成重建图像 Y^图像。</li><li id="9d72" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">计算重建和真实 Y 图像的周期一致性损失。</li><li id="b801" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated">重复步骤 1 至 4，仅交换域，添加所有发电机损耗，并执行反向传播和优化。</li></ul><p id="06b5" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">下面是这样做的代码片段。</p><pre class="kr ks kt ku gt pr ps pt pu aw pv bi"><span id="a7a2" class="op nk it ps b gy pw px l py pz">def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, <br/>                  n_epochs=1000):<br/>    <br/>    print_every=10<br/>    <br/>    # keep track of losses over time<br/>    losses = []<br/><br/>    test_iter_X = iter(test_dataloader_X)<br/>    test_iter_Y = iter(test_dataloader_Y)<br/><br/>    # Get some fixed data from domains X and Y for sampling. These are images that are held<br/>    # constant throughout training, that allow us to inspect the model's performance.<br/>    fixed_X = test_iter_X.next()[0]<br/>    fixed_Y = test_iter_Y.next()[0]<br/>    fixed_X = scale(fixed_X) # make sure to scale to a range -1 to 1<br/>    fixed_Y = scale(fixed_Y)<br/><br/>    # batches per epoch<br/>    iter_X = iter(dataloader_X)<br/>    iter_Y = iter(dataloader_Y)<br/>    batches_per_epoch = min(len(iter_X), len(iter_Y))<br/><br/>    for epoch in range(1, n_epochs+1):<br/><br/>        # Reset iterators for each epoch<br/>        if epoch % batches_per_epoch == 0:<br/>            iter_X = iter(dataloader_X)<br/>            iter_Y = iter(dataloader_Y)<br/><br/>        images_X, _ = iter_X.next()<br/>        images_X = scale(images_X) # make sure to scale to a range -1 to 1<br/><br/>        images_Y, _ = iter_Y.next()<br/>        images_Y = scale(images_Y)<br/>        <br/>        # move images to GPU if available (otherwise stay on CPU)<br/>        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")<br/>        images_X = images_X.to(device)<br/>        images_Y = images_Y.to(device)<br/><br/><br/>        # ============================================<br/>        #            TRAIN THE DISCRIMINATORS<br/>        # ============================================<br/><br/>        ##   First: D_X, real and fake loss components   ##<br/><br/>        # 1. Compute the discriminator losses on real images<br/>        d_x_optimizer.zero_grad()<br/>        real_D_loss = real_mse_loss(D_X(images_X))<br/>        # 3. Compute the fake loss for D_X<br/>        fake_D_loss = fake_mse_loss(D_X(G_YtoX(images_Y)))<br/>        # 4. Compute the total loss and perform backprop<br/>        d_x_loss = real_D_loss + fake_D_loss<br/>        d_x_loss.backward()<br/>        d_x_optimizer.step()<br/>        <br/>        ##   Second: D_Y, real and fake loss components   ##<br/>        d_y_optimizer.zero_grad()<br/>        real_D_y_loss = real_mse_loss(D_Y(images_Y))<br/>        <br/>        fake_D_y_loss = fake_mse_loss(D_Y(G_XtoY(images_X)))<br/>        <br/>        d_y_loss = real_D_y_loss + fake_D_y_loss<br/>        d_y_loss.backward()<br/>        d_y_optimizer.step()<br/><br/><br/>        # =========================================<br/>        #            TRAIN THE GENERATORS<br/>        # =========================================<br/><br/>        ##    First: generate fake X images and reconstructed Y images    ##<br/>        g_optimizer.zero_grad()<br/>        # 1. Generate fake images that look like domain X based on real images in domain Y<br/>        out_1 = G_YtoX(images_Y)<br/>        # 2. Compute the generator loss based on domain X<br/>        loss_1 = real_mse_loss(D_X(out_1))<br/>        # 3. Create a reconstructed y<br/>        out_2 = G_XtoY(out_1)<br/>        # 4. Compute the cycle consistency loss (the reconstruction loss)<br/>        loss_2 = cycle_consistency_loss(real_im = images_Y, reconstructed_im = out_2, lambda_weight=10)<br/><br/>        ##    Second: generate fake Y images and reconstructed X images    ##<br/>        out_3 = G_XtoY(images_X)<br/>        # 5. Add up all generator and reconstructed losses and perform backprop<br/>        loss_3 = real_mse_loss(D_Y(out_3))<br/>        out_4 = G_YtoX(out_3)<br/>        loss_4 =  cycle_consistency_loss(real_im = images_X, reconstructed_im = out_4, lambda_weight=10)<br/><br/>        g_total_loss = loss_1 + loss_2 + loss_3 + loss_4<br/>        g_total_loss.backward()<br/>        g_optimizer.step()<br/>        <br/>        # Print the log info<br/>        if epoch % print_every == 0:<br/>            # append real and fake discriminator losses and the generator loss<br/>            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))<br/>            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f}'.format(<br/>                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))<br/><br/>            <br/>        sample_every=100<br/>        # Save the generated samples<br/>        if epoch % sample_every == 0:<br/>            G_YtoX.eval() # set generators to eval mode for sample generation<br/>            G_XtoY.eval()<br/>            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16)<br/>            G_YtoX.train()<br/>            G_XtoY.train()<br/><br/>        # uncomment these lines, if you want to save your model<br/>#         checkpoint_every=1000<br/>#         # Save the model parameters<br/>#         if epoch % checkpoint_every == 0:<br/>#             checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)<br/><br/>    return losses</span></pre><blockquote class="lh li lj"><p id="0bca" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">使用 GPU 进行了超过 5000 个时期的训练，这就是为什么我必须将我的模型和输入从 CPU 移动到 GPU。</p></blockquote><h1 id="fc47" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">结果</h1><ul class=""><li id="c50b" class="pd pe it ln b lo ok lr ol ng qb nh qc ni qd mg pi pj pk pl bi translated">以下是在每个时期之后记录的发生器和鉴别器的训练损失的曲线图。</li></ul><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qe"><img src="../Images/a1fa23830ce8dc39eae6d03cd67b2fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0kMJG8jU8sVhvjDpBECvQ.png"/></div></div></figure><p id="d294" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">我们可以观察到，发生器开始时有很高的误差，但随着时间的推移，它开始产生像样的图像转换，从而有助于降低误差。</p><p id="a9ad" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">两种鉴频器误差都显示出非常小的误差波动。但是到 5000 个纪元结束时，我们可以看到两个鉴别器误差都减少了，从而迫使生成器进行更真实的图像转换。</p><ul class=""><li id="2b85" class="pd pe it ln b lo lp lr ls ng pf nh pg ni ph mg pi pj pk pl bi translated">可视化样品。</li></ul><p id="9bdb" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">经过 100 次迭代之后—</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qf"><img src="../Images/d20cdc9a42255ba95914025973638e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3ncnyxWIrNqT6wmuEY0UQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Translation from X to Y after 100 iterations</figcaption></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qg"><img src="../Images/c2d6dbb0f13b1eca965ec2d40c135b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-cxviwKLUGr_LQRNZd6t2A.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Translation from Y to X after 100 iterations</figcaption></figure><p id="3b35" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">经过 5000 次迭代后—</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qh"><img src="../Images/722aa20a5a3f90442862e7e634f8786a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UYzN0aTRyQFIgLmIC4IIMg.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Translation from X to Y after 5000 iterations</figcaption></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi qh"><img src="../Images/33593b72515047e4ea310e5be0500663.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBcrPcijSacKXDErSMO7tQ.png"/></div></div><figcaption class="lc ld gj gh gi le lf bd b be z dk">Translation from Y to X after 5000 iterations</figcaption></figure><p id="53da" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">我们可以观察到 CycleGAN 模型产生低分辨率图像，这是一个正在进行的研究领域，您可以通过点击<a class="ae lg" href="https://arxiv.org/abs/1711.11585" rel="noopener ugc nofollow" target="_blank">此处</a>了解更多关于使用多个生成器的高分辨率公式。</p><p id="b124" class="pw-post-body-paragraph lk ll it ln b lo lp ju lq lr ls jx lt ng lv lw lx nh lz ma mb ni md me mf mg im bi translated">这种型号很难精确匹配颜色。这是因为，如果 G_xtoy 和 G_ytox 可能会改变图像的色调；周期一致性损失可能不受影响，并且仍然很小。你可以选择引入一个新的、基于颜色的损失项来比较 G_ytox(y)和 y，以及 G_xtoy(x)和 x，但这变成了一种监督学习方法。也就是说，CycleGAN 能够做出令人满意的翻译。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><blockquote class="lh li lj"><p id="b6ef" class="lk ll lm ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf mg im bi translated">如果你想保持联系，你可以在<a class="ae lg" href="http://www.linkedin.com/in/rahil-vijay" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我。</p></blockquote><h1 id="3f9c" class="nj nk it bd nl nm nn no np nq nr ns nt jz nu ka nv kc nw kd nx kf ny kg nz oa bi translated">参考</h1><ul class=""><li id="f1d3" class="pd pe it ln b lo ok lr ol ng qb nh qc ni qd mg pi pj pk pl bi translated"><a class="ae lg" href="https://arxiv.org/pdf/1703.10593.pdf" rel="noopener ugc nofollow" target="_blank"> CycleGAN 原创论文</a></li><li id="12a5" class="pd pe it ln b lo pm lr pn ng po nh pp ni pq mg pi pj pk pl bi translated"><a class="ae lg" href="https://www.udacity.com/course/deep-learning-nanodegree--nd101" rel="noopener ugc nofollow" target="_blank"> Udacity 深度学习纳米学位课程</a></li></ul><blockquote class="qi"><p id="a65b" class="qj qk it bd ql qm qn qo qp qq qr mg dk translated">查看我关于这篇文章的 Github 报告。</p></blockquote></div></div>    
</body>
</html>