<html>
<head>
<title>An Introduction to Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-logistic-regression-8136ad65da2e?source=collection_archive---------10-----------------------#2019-03-27">https://towardsdatascience.com/an-introduction-to-logistic-regression-8136ad65da2e?source=collection_archive---------10-----------------------#2019-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="aabd" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习和深度学习之旅</h2><div class=""/><div class=""><h2 id="1160" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">逻辑回归从理论到实践的深度探讨</h2></div></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><p id="ccaa" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本博客将涵盖五个主题和问题:</p><p id="9ed5" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">1.什么是逻辑回归？</p><p id="aefe" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2.为什么不用线性回归？</p><p id="d7cb" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3.最大似然估计</p><p id="d656" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">4.逻辑回归的梯度下降</p><p id="36ef" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">5.用 Python 实现逻辑回归</p><ol class=""><li id="a07c" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ja">什么是逻辑回归？</strong></li></ol><p id="c2f0" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Logistic 回归是一种传统而经典的统计模型，在学术界和工业界得到了广泛的应用。与用于预测数值响应的线性回归不同，逻辑回归用于解决分类问题。例如，当一个人向银行申请贷款时，银行感兴趣的是这个申请人将来是否会违约？(默认或非默认)</p><p id="3098" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一种解决方案是直接对申请人的未来状态进行预测，如感知器，这是 SVM 和神经网络的基础。请阅读我在感知器上的博客:</p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/an-introduction-to-perceptron-algorithm-40f2ab4e2099"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">感知器算法简介</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">从理论到实践，学习感知机的基本原则，并在数据集上实现它与随机…</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr ms md"/></div></div></a></div><p id="5fd5" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">另一种解决方案，如逻辑回归，是对申请人违约的概率进行预测。由于概率的性质，预测将落在[0，1]内。凭经验，如果预测概率大于或等于 0.5，那么我们可以给这个申请人贴上‘违约’的标签；如果预测概率小于 0.5，那么我们可以将该申请人标记为“不违约”。但是线性回归的范围是从负无穷大到正无穷大，而不是在[0，1]中。然后引入 sigmoid 函数来解决这个问题。sigmoid 函数的表达式为:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/6531d17e280453e508344e41aa06dd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/1*-XcDnVV0LLpV5XyZ2fqcig.gif"/></div></figure><p id="9a33" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">sigmoid 函数给出了一条 S 形曲线，当它的自变量非常正或非常负时，它会饱和。花点时间记下这个公式。我们将在最大可能估计中应用它。</p><p id="943c" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">sigmoid 函数的图形:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/69e99b38b69abfc0191c051223cd318c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UnSW1b5LdpFlBx5hR54J0w.png"/></div></div></figure><p id="0abb" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Sigmoid 函数有许多特性，包括:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f539ae8b9181ce4d056f470fde1f0942.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/1*2cKo1jg_HLA7AhYAqNZfSQ.gif"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/6a6ab5c70bff8d9c5ddac99267db9d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/1*uKOTCWT5_6AwsIxifnb0dQ.gif"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/88e0e246cee18582b3037b884e7d570c.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/1*jW-tKM7coTbuMpb966aLRg.gif"/></div></figure><p id="7873" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在逻辑回归中，我们可以写成:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/50a69518b3346b61606fc29a8de36d7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/1*I0lW7YdvTn3mHXh56pYxZQ.gif"/></div></figure><p id="6e0d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">函数的导数如下所示，将用于计算成本函数的梯度。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nj"><img src="../Images/5a19ecedb5cc240f97e4326fdec54273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IrxhWNL9ZmOP4yW3T_62Ew.png"/></div></div></figure><p id="b012" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 2。为什么不用线性回归？</strong></p><p id="ce48" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">《统计学习导论》直接解释了为什么使用逻辑回归而不是线性回归来解决分类问题。首先，线性回归的范围是负无穷大到正无穷大，超出了[0，1]的边界。如果线性回归和逻辑回归都对概率进行预测，线性模型甚至会产生负面的预测，而逻辑回归不存在这样的问题。见下图。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bdab62b874077b58a2b15c439a6fb315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*cOt3Hlg4iXuTIDktpWWblg.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Figures Source: Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, 2017, An Introduction to Statistical Learning</figcaption></figure><p id="b5a4" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">线性回归的另一个问题是线性模型的预测总是基数的而不是名义的。即使在某些情况下，响应的值确实采用了自然排序，例如坏、中性和好。看起来坏和中性的差距和中性和好的差距是一样的，所以把坏、中性和好相应地编码成 1、2、3 是合理的。然而，没有自然或直接的方法将名义响应转换成数字响应。</p><p id="01d1" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 3。最大似然估计(MLE) </strong></p><p id="2f6d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从全部人口中抽取一个样本。该记录遵循伯努利分布。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi np"><img src="../Images/92347bbe5725c0e267aaa91ef00cc93d.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/1*ayxQCn3xz6sm41KRjf3Ygw.gif"/></div></figure><p id="534d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个公式中，<em class="nq"> y </em>是 1 或 0 的指标，<em class="nq"> p </em>是事件发生的概率。</p><p id="b113" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果总共有<em class="nq"> N </em>条记录呢，概率是多少？简而言之，假设每条记录都是独立且同分布的(<em class="nq"> I.I.D </em>)，我们可以一起计算<em class="nq"> N </em>条记录的概率。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b963f117808503b9351afd0094d30140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/1*U7tef7LJzgvbipSiKpIshQ.gif"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/74d4dd11a93618b7cf8a35a40165a6bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/1*DaUtcIiIoAD8kuhnRK8phQ.gif"/></div></figure><p id="2a86" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后取两边的对数，我们就可以得到对数似然。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/a7b3da561077af6463559f0ea4861293.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/1*D6RnanSaQ3REykJ2B8sUBA.gif"/></div></figure><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0f993eb998a38cc012b211997e5cf0ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/1*6MTXtB4zipiDMguZrlXSlA.gif"/></div></figure><p id="f2d1" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，在公式中，<em class="nq"> p </em>是需要估计的参数(概率)，等于:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/bcef3d81438867f251a048aa3b3682b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/1*BUtaRIGY5IoeDhOq8Ud1nQ.gif"/></div></figure><p id="2cea" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在统计学中，最大似然估计(MLE)被广泛用于获得分布的参数。在这个范例中，最大化对数似然等于最小化成本函数<em class="nq"> J </em>。它是凸优化中的一个对偶问题。成本函数<em class="nq"> J </em>如下:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ae2a37fe280a3f7b0f0ba3fd5102bd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/1*VAb-6NSg2vwUtqCtfNdjrA.gif"/></div></figure><p id="a2dc" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本节展示了 MLE 和成本函数之间的关系，以及 sigmoid 函数是如何嵌入 MLE 的。下一个问题是如何计算<em class="nq"> p </em>并进一步计算<em class="nq"> w </em>以最小化成本函数。</p><p id="26aa" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 4。逻辑回归的梯度下降</strong></p><p id="4d0e" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与具有封闭形式解决方案的线性回归不同，梯度下降应用于逻辑回归。梯度下降的一般思想是反复调整参数<em class="nq"> w </em>和<em class="nq"> b </em>以最小化成本函数。有三种典型的梯度下降法，包括批量梯度下降法、小批量梯度下降法和随机梯度下降法。在这个博客中，批量渐变体面使用。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d13ee43f51e5adf1c9b6afefc2bfa6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk">Figure Source: <a class="ae ny" href="https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://saugatbhattarai.com.np/what-is-gradient-descent-in-machine-learning/</a></figcaption></figure><p id="e270" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">成本函数<strong class="kx ja"> <em class="nq"> J </em> </strong>的梯度为:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nz"><img src="../Images/9a6ce118a375e8202b172dd0141235d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bZyyNge9SXeJKy_VQQ5xZQ.png"/></div></div></figure><p id="a28f" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在第一部分中应用 sigmoid 函数的导数，然后我们可以得到:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/c7296682c6704be7a07c5d405db46ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/1*61EJOXvXZsMdXtkyWjPe3A.gif"/></div></figure><p id="66a3" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">初始值被分配给<em class="nq">w；</em>然后通过学习率*代价函数的梯度迭代更新<em class="nq"> w </em>。该算法不会停止，直到它收敛。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/adafd97d3127c7a6efa069d25e2f0dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/1*kUmtH0lRS-euZriSNaqgHQ.gif"/></div></figure><p id="64d4" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降请看我的博客:</p><div class="ma mb gp gr mc md"><a rel="noopener follow" target="_blank" href="/an-introduction-to-gradient-descent-c9cca5739307"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">梯度下降导论</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">本博客将涵盖以下问题和主题:</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">towardsdatascience.com</p></div></div><div class="mm l"><div class="oc l mo mp mq mm mr ms md"/></div></div></a></div><p id="b98d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja"> 5。用 Python 实现逻辑回归</strong></p><p id="c811" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一部分中，我将使用众所周知的数据 iris 来展示梯度体面如何工作以及逻辑回归如何处理分类问题。</p><p id="244c" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，导入包</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="ec9e" class="oi oj iq oe b gy ok ol l om on">from sklearn import datasets<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import matplotlib.lines as mlines</span></pre><p id="0fe7" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来，加载数据。为简单起见，我只选择 2 种虹膜。</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="c777" class="oi oj iq oe b gy ok ol l om on"># Load data<br/>iris = datasets.load_iris()<br/>X=iris.data[0:99,:2]<br/>y=iris.target[0:99]</span><span id="f1fa" class="oi oj iq oe b gy oo ol l om on"># Plot the training points<br/>x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5<br/>y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5<br/>plt.figure(2, figsize=(8, 6))</span></pre><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi op"><img src="../Images/c72e7ca0faf0257842eedf87ac67ba45.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*fjNuC59ux5tsVUG3pD6HBQ.png"/></div></figure><p id="4f80" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降的伪代码</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="5381" class="oi oj iq oe b gy ok ol l om on">   1. Initialize the parameters<br/>Repeat {<br/>   2. Make a prediction on y<br/>   3. Calculate cost function<br/>   4. Get gradient for cost function<br/>   5. Update parameters<br/> }</span></pre><p id="9e31" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降代码</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="a5c5" class="oi oj iq oe b gy ok ol l om on">#Step 1: Initial Model Parameter<br/>Learning_Rate=0.01<br/>num_iterations=100000<br/>N=len(X)<br/>w=np.zeros((2,1))<br/>b=0<br/>costs=[]</span><span id="8398" class="oi oj iq oe b gy oo ol l om on">for i in range(num_iterations):<br/>  #Step 2: Apply sigmoid Function and get y prediction<br/>    Z=np.dot(w.T,X.T)+b<br/>    y_pred=1/(1+1/np.exp(Z))</span><span id="e0a5" class="oi oj iq oe b gy oo ol l om on">  #Step 3: Calculate Cost Function<br/>    cost=-(1/N)*np.sum(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))</span><span id="c4c0" class="oi oj iq oe b gy oo ol l om on">  #Step 4: Calculate Gradient<br/>    dw=1/N*np.dot(X.T,(y_pred-y).T)<br/>    db=1/N*np.sum(y_pred-y)</span><span id="2216" class="oi oj iq oe b gy oo ol l om on">  #Step 5: Update w &amp; b<br/>    w = w - Learning_Rate*dw<br/>    b = b - Learning_Rate*db<br/>   #Records cost<br/>    if i%100==0:<br/>    costs.append(cost)<br/>    print(cost)</span></pre><p id="815a" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可视化成本函数随时间的变化</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="fee1" class="oi oj iq oe b gy ok ol l om on"># Plot cost function<br/>Epoch=pd.DataFrame(list(range(100,100001,100)))<br/>Cost=pd.DataFrame(costs)<br/>Cost_data=pd.concat([Epoch, Cost], axis=1)<br/>Cost_data.columns=['Epoch','Cost']<br/>plt.scatter(Cost_data['Epoch'], Cost_data['Cost'])<br/>plt.xlabel('Epoch')<br/>plt.ylabel('Cost')</span></pre><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/64aaa43ad7a55b02d312de64aed66647.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*ptQYCzb4SbotNTj463T9-w.png"/></div></figure><p id="dffa" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上图中，我们可以看到，起初成本急剧下降；经过 40000 轮迭代，变得稳定。</p><p id="88be" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">可视化线性分类</p><pre class="mu mv mw mx gt od oe of og aw oh bi"><span id="02df" class="oi oj iq oe b gy ok ol l om on"># Plot linear classification<br/>fig, ax = plt.subplots()<br/>ax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1,edgecolor='k')<br/>line=mlines.Line2D([3.701,7],[2,4.1034],color='red')<br/>ax.add_line(line)<br/>ax.set_xlabel('Sepal length')<br/>ax.set_ylabel('Sepal width')<br/>plt.show()</span></pre><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi or"><img src="../Images/ff005b3d4b4e630052dee6c7c388d3bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*RL-1tCz5btppDz4ufmV3AA.png"/></div></figure><p id="2f88" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图中的红线是 logistic 回归的决策边界。因为虹膜数据只包含二维，所以判定边界是一条线。在某些情况下，当存在 3 个或更多维时，决策边界将是超平面。</p><p id="9d5d" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja">总结</strong></p><p id="4e52" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇博客中，我从理论到实践解释了逻辑回归。希望你看完这篇博客后对逻辑回归有更好的理解。如果您对其他博客感兴趣，请点击以下链接:</p><div class="ma mb gp gr mc md"><a href="https://medium.com/@songyangdetang_41589/table-of-contents-689c8af0c731" rel="noopener follow" target="_blank"><div class="me ab fo"><div class="mf ab mg cl cj mh"><h2 class="bd ja gy z fp mi fr fs mj fu fw iz bi translated">机器学习和深度学习之旅</h2><div class="mk l"><h3 class="bd b gy z fp mi fr fs mj fu fw dk translated">这一系列博客将从理论和实现两个方面对深度学习进行介绍。</h3></div><div class="ml l"><p class="bd b dl z fp mi fr fs mj fu fw dk translated">medium.com</p></div></div><div class="mm l"><div class="os l mo mp mq mm mr ms md"/></div></div></a></div><p id="8f7c" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ja">参考</strong></p><p id="68f4" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1]伊恩·古德菲勒，约舒阿·本吉奥，亚伦·库维尔，(2017) <em class="nq">深度学习</em></p><p id="f8cd" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] Gareth James，Daniela Witten，Trevor Hastie，Robert Tibshirani，(2017) <em class="nq">统计学习介绍</em></p><p id="a2c9" class="pw-post-body-paragraph kv kw iq kx b ky kz ka la lb lc kd ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae ny" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_descent</a></p></div></div>    
</body>
</html>