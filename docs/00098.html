<html>
<head>
<title>Back Propagation, the Easy Way (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播，简单的方法(第 1 部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/back-propagation-the-easy-way-part-1-6a8cde653f65?source=collection_archive---------5-----------------------#2019-01-05">https://towardsdatascience.com/back-propagation-the-easy-way-part-1-6a8cde653f65?source=collection_archive---------5-----------------------#2019-01-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1f6e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">反向传播的简单详细解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/667c326178240e336277fb4b9754f72f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMEOMyEvwuqv6FkhJgYJ6w.jpeg"/></div></div></figure><p id="ad9b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae lq" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="202f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在关于<a class="ae lq" rel="noopener" target="_blank" href="/gradient-descent-the-easy-way-5240ca9a08da">梯度下降</a>的文章之后，反向传播很好地利用了这种技术来计算神经网络中权重的“正确”值。</p><p id="ada2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">重要提示:</strong>这篇文章包含了相当多的等式，然而它们都是有逻辑联系的。读者应该有耐心阅读它们，并理解它们是如何构建的。这对很好地理解反向传播技术是必不可少的。</p><p id="4d58" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">与梯度下降方法一样，我们再次借用无线电比喻来帮助理解反向传播的直觉。在另一篇文章中，为了得到可接受的输出，只需要处理一个按钮，而在这里，我们有许多需要调整的按钮。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lr"><img src="../Images/b871994856c669033dd4606ee7d9f281.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OL0NXIpbzQIfVW7gT-xjBw.png"/></div></div></figure><p id="9569" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上图显示，在连接两个节点的每条线上都有一个微调按钮(即使它们没有全部显示出来)。</p><p id="231e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如前所述，我们的工作是调整这些按钮，使输出误差最小，这意味着有效输出尽可能接近预期结果。</p><p id="3575" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在梯度下降中，我们讨论了一个微调参数𝜃和误差或损耗𝓛之间的简单关系，在这种情况下，我们有一个更复杂的关系。<br/>然而，我们将从仅考虑一个神经元开始慢慢地进行。</p><h1 id="e266" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">一个神经元的情况</h1><p id="c353" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">作为提醒，单个神经元接收一个或多个输入<strong class="kw iu"> <em class="mp"> x </em> </strong>并将每个输入乘以某个权重<strong class="kw iu"> <em class="mp"> w </em> </strong> <em class="mp">，</em>将它们相加并加上偏差<strong class="kw iu"> <em class="mp"> b </em> </strong>，<strong class="kw iu"> <em class="mp"> </em> </strong>然后应用激活函数<strong class="kw iu"> <em class="mp"> g() </em> </strong>来产生输出<strong class="kw iu"> <em class="mp"/></strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mq"><img src="../Images/bd3afe611d76cb88b05c32a64ea901b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TzEmhdoonDOdtNBmD1ByOQ.png"/></div></div></figure><p id="0f9c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了能够评估神经元的输出，使用了损失函数𝓛。一个这样的损失函数可以是𝓛(y，ŷ) = (y - ŷ)</p><p id="cd01" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">当然，目的是确定<strong class="kw iu"> <em class="mp"> w </em> </strong>和<strong class="kw iu"> <em class="mp"> b </em> </strong>的值，以便最小化𝓛.</p><p id="73f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过查看神经元架构，我们可以很容易地注意到以下的<strong class="kw iu"> <em class="mp">事件链</em></strong>:<br/><strong class="kw iu"><em class="mp">wᵢ</em></strong>的变化导致<strong class="kw iu"> <em class="mp"> z </em> </strong>变化，<br/>变化导致<strong class="kw iu"><em class="mp">z</em></strong>g(z)<em class="mp"/>变化，<br/>变化导致<strong class="kw iu"> <em class="mp"> g(z) </em></strong></p><p id="4aaa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为简单起见，让我们称之为<strong class="kw iu"> <em class="mp"> a = g(z) </em> </strong>，由此可见<strong class="kw iu"> <em class="mp"> ŷ = a. </em> </strong> <br/>上述链式反应可以使用<a class="ae lq" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式法则</a>导出，该法则规定输出相对于<strong class="kw iu"> wᵢ </strong>的变化是两者之间相位的偏导数的乘积:</p><p id="06f6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">∂𝓛/∂wᵢ=∂𝓛/∂a * ∂a/∂z *∂z/∂wᵢ</strong></p><p id="22b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们单独考虑每个术语:</p><p id="7607" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">∂𝓛/∂a</strong>= ∂((y-ŷ))/∂a = ∂((y-a))/∂a =-(y-a)=<strong class="kw iu">a-y</strong></p><p id="40e8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">∂a/∂z</strong>= ∂g(z)/∂z =<strong class="kw iu">g’(z)</strong>(其中 g’(z)是 g(z)的导数)。注意 g(z)可以是任何可导函数)</p><p id="78a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">∂z/∂wᵢ</strong>= ∂(x1*w1+x2 * w2+…+xᵢ*wᵢ+…xn *wn)/∂wᵢ<br/>=∂(xᵢ*wᵢ)/∂wᵢ=<strong class="kw iu"><em class="mp">xᵢ</em></strong></p><p id="c2b9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">将每一项替换为它的值将得到∂𝓛/∂wᵢ<br/><strong class="kw iu">∂𝓛/∂wᵢ=(a-y)* g’(z)*xᵢ</strong></p><p id="411a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们定义𝛿如𝛿 = (a - y) * g'(z)，∂𝓛/∂wᵢ的最终形式将是:</p><p id="d0d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">∂𝓛/∂wᵢ<em class="mp">=𝛿*xᵢ</em>t65】</strong></p><p id="f202" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你还记得那篇<a class="ae lq" rel="noopener" target="_blank" href="/gradient-descent-the-easy-way-5240ca9a08da">梯度下降</a>的文章，为了找到最小化𝓛的𝜃，我们迭代下面的等式，直到∆𝓛变得太小或为零:<br/> 𝜃𝑛₊₁ = 𝜃𝑛 -𝛂。∆𝓛</p><p id="ba87" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样的逻辑也适用于神经网络。<br/>对于每个 I，<strong class="kw iu">wᵢ⁺=wᵢ-</strong>𝛂*<strong class="kw iu">∂𝓛/∂wᵢ=wᵢ-</strong>𝛂*<strong class="kw iu"><em class="mp">𝛿*xᵢ<br/></em></strong><em class="mp">其中</em>w \u\u\u\u 是 w \u 的新值。<br/> <strong class="kw iu">重要:</strong>对于每一个 I 我们迭代<strong class="kw iu"> wᵢ </strong>足够的次数，直到<strong class="kw iu"> ∂𝓛/∂wᵢ </strong>变得足够小</p><p id="dd56" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用相同的逻辑我们也可以找到变化的效果<strong class="kw iu"><em class="mp">b</em></strong>:<br/><strong class="kw iu">∂𝓛/∂<em class="mp">b</em>=∂𝓛/∂a * ∂a/∂z *∂z/∂<em class="mp">b<br/></em></strong>由于∂z/∂b = 1 并且所有其他项已经被计算过<strong class="kw iu"><em class="mp"><br/></em>∂𝓛/∂<em class="mp">b =𝛿<br/></em></strong>接下来是<strong class="kw iu"><em class="mp">b</em>⁺=<em class="mp">b<em class="mp"> <strong class="kw iu">∂𝓛/∂<em class="mp">b</em>=<em class="mp">b</em>-</strong>𝛂*<strong class="kw iu"><em class="mp">𝛿<br/></em></strong><em class="mp">其中 b </em> ⁺是<em class="mp"> b </em>的新值。 <br/>再次我们保持迭代<strong class="kw iu"> <em class="mp"> b </em> </strong>足够的次数，直到<strong class="kw iu"> ∂𝓛/∂ <em class="mp"> b </em> </strong>变得足够小。</em></em></strong></p><h1 id="1515" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">两个神经元的情况</h1><p id="3e41" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">现在让我们以两个神经元按顺序排列为例(它们形成两层)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mr"><img src="../Images/21a627c5d95eeca1328723c83d316e37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASmuhi4Uo7vzMNH2d22zPw.png"/></div></div></figure><p id="fca0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在开始之前，值得注意的是符号已经改变，以适应新的架构。上标数字表示层。例如，z、b、a、g 属于层#2，而 z、b、a、g 属于层#3。<br/>另外值得一提的是，层#2 的输出是单值<strong class="kw iu"> a </strong>，乘以下一层的权重<strong class="kw iu"> v </strong>。</p><h2 id="427a" class="ms lt it bd lu mt mu dn ly mv mw dp mc ld mx my me lh mz na mg ll nb nc mi nd bi translated">第三层</h2><p id="adef" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">我们将首先从最后一层(层#3)开始，因为它的结果将用于前一层。</p><p id="f2ed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一连串的事件并不奇怪。<br/>v<strong class="kw iu">变化</strong>引起<strong class="kw iu"> z </strong>变化，<br/>z 变化引起<strong class="kw iu"> g (z ) </strong>变化，<strong class="kw iu"> g (z ) </strong>变化<strong class="kw iu">𝓛(y</strong>变化。</p><p id="108b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">使用链式法则，我们计算导数∂𝓛/∂v:<br/><strong class="kw iu">∂𝓛/∂v =∂𝓛/∂a * ∂a /∂z * ∂z /∂v</strong></p><p id="99d1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如前所述，我们单独计算每一项:</p><p id="bfeb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">∂𝓛/∂a = a - y </p><p id="88e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu"> ∂a /∂z = g '(z ) </strong>(其中 g '(z)是 g (z)的导数)</p><p id="edac" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">∂z /∂v = a </p><p id="05c0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以∂𝓛/∂v 变成了<strong class="kw iu">∂𝓛/∂v =(a-y)* g’(z)* a</strong></p><p id="7a85" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们来定义一下<strong class="kw iu">𝛿=</strong><strong class="kw iu">(a-y)*</strong><strong class="kw iu">【g’(z)<br/></strong>接下来就是<strong class="kw iu"> ∂𝓛/∂v = 𝛿 * a </strong></p><p id="8de6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样我们可以找到<strong class="kw iu">∂𝓛/∂<em class="mp">b =𝛿</em>t93】</strong></p><h2 id="8cbb" class="ms lt it bd lu mt mu dn ly mv mw dp mc ld mx my me lh mz na mg ll nb nc mi nd bi translated">第二层</h2><p id="b8cb" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">现在让我们移动到第二层。<br/>链式反应开始于对<strong class="kw iu"> wᵢ </strong>施加影响<strong class="kw iu"> z </strong>，<br/>z<strong class="kw iu">的变化</strong>影响<strong class="kw iu">g(z)</strong><br/>g(z)的变化影响<strong class="kw iu"> z </strong>(注意在这一点上<strong class="kw iu"> v </strong>被认为是固定的)<br/>z<strong class="kw iu">的变化</strong>影响<strong class="kw iu">g(z)<br/></strong></p><p id="e526" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">导数方程为<br/><strong class="kw iu">∂𝓛/∂wᵢ=(∂𝓛/∂a * ∂a /∂z * ∂z /∂a)* ∂a /∂z * ∂z/∂wᵢ</strong></p><p id="c56b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们已经计算了<strong class="kw iu"> ∂𝓛/∂a </strong>和<strong class="kw iu">∂a /∂z</strong>，至于其余的可以很容易地推导出来。<br/><strong class="kw iu">∂z /∂a</strong>= ∂(a * v))/∂a =<strong class="kw iu">v</strong><br/><strong class="kw iu">∂a /∂z</strong>= ∂g(z )/∂z =<strong class="kw iu">g '(z)</strong><br/><strong class="kw iu">∂z/∂wᵢ=xᵢ</strong></p><p id="3037" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">替换完每一项但其值<br/>∂𝓛/∂wᵢ=((a<strong class="kw iu"/>-y)* g<strong class="kw iu">'【z】)* v)* g '(z)*xᵢ<br/>我们已经将𝛿定义为𝛿=(a-y)* g '【z】<br/><strong class="kw iu">∂𝓛/∂wᵢ=𝛿* v * g '(z)*xᵢ</strong></strong></p><p id="46fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们定义<strong class="kw iu"> 𝛿 </strong>为<strong class="kw iu"> 𝛿 = 𝛿 * v * g '(z ) </strong> <br/>这将给出∂𝓛/∂wᵢ: <strong class="kw iu"> ∂𝓛/∂wᵢ = 𝛿 * xᵢ </strong>的最终形式</p><p id="fd30" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">同样我们可以找到∂𝓛/∂b =𝛿</p><h1 id="de6c" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">L 层序列</h1><p id="2584" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">现在考虑一个有 L 层的网络，每层只包含一个神经元。这样的网络看起来会像下图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/eb6bb3e89c448d8b3fa849240d1f7d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6kGi3pMn0apc2EWlnPRaQ.png"/></div></div></figure><p id="7080" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">通过使用与上述相同的逻辑，我们可以找到反向传播所需的不同组件。</p><p id="be6b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以对于 l 层<br/><strong class="kw iu">𝛿ᴸ=</strong><strong class="kw iu">(aᴸ-y)*</strong><strong class="kw iu">gᴸ'(zᴸ)<br/></strong>对于任何其他层𝒍&lt;l<br/><strong class="kw iu">𝛿ˡ=𝛿ˡ⁺* wˡ⁺*gˡ'(zˡ)</strong></p><p id="bdca" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于任何层，𝒍≤l<strong class="kw iu"><br/>∂𝓛/∂wˡ=𝛿ˡ*aˡ⁻<br/>∂𝓛/∂bˡ=𝛿ˡ<br/></strong>其中 a <strong class="kw iu"> ˡ </strong> ⁻是层 1 的输出，或者如果我们在层 1，它将是输入 x</p><p id="f317" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">梯度下降的公式求<strong class="kw iu">w<em class="mp">ᵢ</em>ˡ</strong>(𝒍层的权重，神经元 I)即<br/>t5】w<em class="mp">ᵢ</em>ˡ⁺= w<em class="mp">ᵢ</em>ˡ-𝛂*<strong class="kw iu">∩w<em class="mp">t = w<em class="mp">t = w</em><br/> <strong class="kw iu">重要:</strong>对于每个<strong class="kw iu"> w <em class="mp"> ᵢ </em> ˡ </strong>我们迭代<strong class="kw iu"> </strong>足够的次数，直到<strong class="kw iu"> ∂𝓛/∂w <em class="mp"> ᵢ </em> ˡ </strong>变得足够小</em></strong></p><h1 id="685d" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">一般情况</h1><p id="798c" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">具有多个神经元的多个层</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/b62b584483b66b0c4966e05284e4ed65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NnvGTjl5SxEIKA5W3cuzUw.png"/></div></div></figure><p id="6655" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在这一节中，我们有多层神经网络，每层有多个神经元，而不是我们迄今为止使用的一个。<br/>回想一下，在前面的例子中，我们每层有一个神经元，对于层 l，我们得到了<strong class="kw iu">𝛿ᴸ=</strong><strong class="kw iu">(aᴸ-y)*</strong><strong class="kw iu">【gᴸ'(zᴸ】</strong><br/>和<strong class="kw iu"><br/>𝛿ˡ=𝛿ˡ⁺* wˡ⁺*gˡ'(zˡ)</strong>对于层𝒍 &lt; L <br/>这只是针对每层中的一个神经元。<br/>当我们每层有多个神经元时，我们必须为每层的每个神经元计算<strong class="kw iu"> 𝛿 </strong>。<br/>所以现在只有上标字母的变量如<strong class="kw iu"> 𝛿ᴸ，w ˡ </strong>是<strong class="kw iu">向量</strong>和<strong class="kw iu">矩阵，</strong>而同时有上标和下标字母的变量如<strong class="kw iu"> 𝛿 <em class="mp"> ᵢ </em> ˡ </strong>和<strong class="kw iu"> wˡ <em class="mp"> ᵢᵣ </em> </strong>都是单值。由于我们有多个输出层，损失函数𝓛(y，ŷ) = (y - ŷ)的定义不再充分。我们必须考虑所有的输出神经元。于是我们定义代价函数为所有输出神经元的平方和<br/><strong class="kw iu">c =∑<em class="mp">ᵢ</em>(y<em class="mp">ᵢ</em>−ŷ<em class="mp">ᵢ</em>)。</strong></p><p id="4499" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，到目前为止，我们计算的公式将具有向量形式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c1d8629ca381119aa0582d9e25141c2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*GqWmzxmUFZfbaWeg-M5izg.png"/></div></figure><p id="a46c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中∇aC <strong class="kw iu"> </strong>是成本<strong class="kw iu"> C </strong>相对于网络输出的变化向量<strong class="kw iu"> a <em class="mp"> ᵢ </em> ᴸ </strong>，也就是<strong class="kw iu"> ∂C/∂a <em class="mp"> ᵢ </em> ᴸ。<br/></strong>⊙运算符是成员向量/矩阵乘法。</p><h1 id="e2a1" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">反向传播算法</h1><ol class=""><li id="d6b0" class="nh ni it kw b kx mk la ml ld nj lh nk ll nl lp nm nn no np bi translated"><strong class="kw iu">输入 x: </strong>设置第一层的输入。</li><li id="be7e" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">向前:</strong>对于每层 l = 2，3，…，l 我们计算<br/><strong class="kw iu">zˡ=wˡaˡ⁻+bˡ</strong><br/>和<br/> <strong class="kw iu"> aˡ=gˡ(zˡ) </strong>。</li><li id="d9bb" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">输出误差𝛿ᴸ: </strong>在输出层我们计算矢量<br/><strong class="kw iu">𝛿ᴸ</strong>=∇ac⊙g<strong class="kw iu">ᴸ</strong>’(z<strong class="kw iu">ᴸ</strong>)。这将是反向传播的开始。</li><li id="6989" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">反向传播:</strong>我们向后移动，对于每一层 l=L-1，L-2，L-3，…，2 我们计算每一层的误差<br/><strong class="kw iu">𝛿ˡ</strong>=(<strong class="kw iu">wˡ⁺</strong>)<strong class="kw iu">ᵀ</strong>*<strong class="kw iu">𝛿ˡ⁺</strong>)⊙<strong class="kw iu">gˡ'(zˡ</strong>。<br/>然后我们使用梯度下降公式更新各层的权重:<br/><strong class="kw iu">wˡ⁺<em class="mp">ᵢᵣ=</em>wˡ<em class="mp">ᵢᵣ-</em></strong>𝛂*<strong class="kw iu">𝛿ˡ<em class="mp">ᵢ</em>* a<em class="mp">ᵣ</em>̿<br/></strong>和<br/><strong class="kw iu">b̿<em class="mp"/></strong></li><li id="e57b" class="nh ni it kw b kx nq la nr ld ns lh nt ll nu lp nm nn no np bi translated"><strong class="kw iu">输出:</strong>最后，我们将计算每层的权重<strong class="kw iu"> w </strong>和偏差<strong class="kw iu"> b </strong>，以最小化成本函数<strong class="kw iu"> C </strong>。</li></ol><h1 id="3347" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">结论</h1><p id="b974" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">反向传播可能很难理解，在代码中实现更难，因为它很容易与矩阵和向量及其维数纠缠在一起。然而，对于初学者来说，重要的是付出足够的努力来获得关于这项技术的足够的直觉，因为这将帮助他们获得神经网络的深入知识。</p><h2 id="c3bd" class="ms lt it bd lu mt mu dn ly mv mw dp mc ld mx my me lh mz na mg ll nb nc mi nd bi translated">相关文章</h2><p id="e9d6" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">第二部分:<a class="ae lq" rel="noopener" target="_blank" href="/back-propagation-the-easy-way-part-2-bea37046c897">反向传播的实际实现</a> <br/>第三部分:<a class="ae lq" href="https://medium.com/@zsalloum/back-propagation-the-easy-way-part-3-cc1de33e8397" rel="noopener">如何处理矩阵的维数</a></p></div></div>    
</body>
</html>