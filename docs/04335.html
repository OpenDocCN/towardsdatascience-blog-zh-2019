<html>
<head>
<title>Is Random Forest better than Logistic Regression? (a comparison)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林比逻辑回归好吗？(比较)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4?source=collection_archive---------2-----------------------#2019-07-06">https://towardsdatascience.com/is-random-forest-better-than-logistic-regression-a-comparison-7a0f068963e4?source=collection_archive---------2-----------------------#2019-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="185d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入研究随机森林的本质，遍历一个示例，并将其与逻辑回归进行比较。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5a00d1f508160c7b14ed0a978a34720f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9fheh6kRizn3gUCfZJE5NA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://unsplash.com/photos/sp-p7uuT0tw" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h1 id="c8eb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">简介:</h1><p id="c763" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">随机森林是从一组数据中提取信息的另一种方式。这种模式的吸引力在于:</p><ul class=""><li id="d647" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">它强调功能选择——将某些功能看得比其他功能更重要。</li><li id="364c" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">它不像回归模型那样假设模型具有线性关系。</li><li id="cb73" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">它利用集成学习。如果我们只使用一个决策树，我们就不会使用集成学习。一个随机森林随机抽取样本，形成很多决策树，然后平均掉叶子节点，得到一个更清晰的模型。</li></ul><p id="e2a3" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">在此分析中，我们将使用随机森林对数据进行分类，<a class="ae ky" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6050737/" rel="noopener ugc nofollow" target="_blank">使用逻辑回归对结果进行比较</a>，并讨论差异。<em class="ng">看一看前面的</em> <a class="ae ky" rel="noopener" target="_blank" href="/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc"> <em class="ng">逻辑回归分析</em> </a> <em class="ng">看看我们将把它与什么进行比较。</em></p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="a491" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">目录:</h1><p id="bcdb" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">1.数据理解(摘要)</p><p id="d1c6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">2.数据探索/可视化(摘要)</p><p id="5372" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">4.构建模型</p><p id="bb4b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">5.测试模型</p><p id="7a26" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">6.结论</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="7b65" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">数据背景:</h1><p id="e201" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们有一个 255 名患者的样本，想要测量 4 种蛋白质水平和癌症生长之间的关系。</p><p id="23c2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">我们知道:</strong></p><ul class=""><li id="6fec" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated">每位患者测得的每种蛋白质的浓度。</li><li id="319d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated">每个患者是否已经被诊断患有癌症(0 =没有癌症；1=癌症)。</li></ul><blockquote class="nt nu nv"><p id="1254" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated"><strong class="lt iu">我们的目标是:</strong></p><p id="9c05" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated">通过从我们样本中的蛋白质水平和癌症之间的关系中提取信息来预测未来的患者是否患有癌症。</p></blockquote><p id="9f91" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><strong class="lt iu">我们将关注的 4 种蛋白质:</strong></p><p id="f3a6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">甲胎蛋白</p><p id="e2c2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">癌胚抗原</p><p id="ab6d" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">癌抗原 125</p><p id="3087" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">癌抗原 50</p><p id="56a6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我从 UAB 的 MBA 项目那里得到了这套用于教育目的的数据。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="3dfb" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">数据探索/可视化</h1><p id="03c9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">再来看一下<a class="ae ky" rel="noopener" target="_blank" href="/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc">逻辑回归分析</a>得到更深入的了解。以下是要点:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="ee5f" class="og la it oc b gy oh oi l oj ok">import numpy as np<br/>import pandas as pd<br/>from sklearn import tree<br/>from sklearn.ensemble import RandomForestClassifier<br/>import matplotlib.pyplot as plt<br/>inputdata= r”C:\Users\Andrew\Desktop\cea.xlsx”<br/>df = pd.read_excel(inputdata)<br/><strong class="oc iu">df</strong>.describe()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ol"><img src="../Images/9e35e9062bfc9e16ddff544fceec489b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*N-e5STjJnKZrr9NxUAy7DA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1</figcaption></figure><h2 id="6dee" class="og la it bd lb om on dn lf oo op dp lj ma oq or ll me os ot ln mi ou ov lp ow bi translated">目标变量(Y)</h2><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c876" class="og la it oc b gy oh oi l oj ok">yhist = plt.hist('class (Y)', data = df, color='g')<br/>plt.xlabel('Diagnosis (1) vs no diagnosis (0)')<br/>plt.ylabel('Quantity')<br/>plt.title('Class (Y) Distribution')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ol"><img src="../Images/d636f19f53be4b8bec66d9f5cfa27657.png" data-original-src="https://miro.medium.com/v2/format:webp/1*4j68kVXV3LlQsw0dukUkjg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2</figcaption></figure></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="833f" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">构建模型</h1><p id="42a8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">要刷新逻辑回归输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ol"><img src="../Images/78dd481e420a9008fb788f33c655c7f8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*B6gjtZ9qFwhBcS2xg2eDBQ.png"/></div></figure><p id="2cdd" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">CEA 和 CA125 是最有预测性的，它们的 pvalues 低于 alpha 5 %,它们的系数比其他的高。由于 AFP 和 CA50 具有较高的 p 值，我们将其从逻辑回归中剔除。</p><blockquote class="nt nu nv"><p id="577c" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated">但是，我们会将它们保留在随机森林模型中。这个练习的全部目的是比较这两个模型，而不是将它们结合起来。</p></blockquote><p id="145b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们将构建决策树，并想象它的样子:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="ebeb" class="og la it oc b gy oh oi l oj ok">#Defining variables and building the model<br/>features = list(df.columns[1:5])<br/>y = df['class (Y)']<br/>X = df[features]<br/>clf = tree.DecisionTreeClassifier()<br/>clf = clf.fit(X,y)</span><span id="a5c4" class="og la it oc b gy ox oi l oj ok">#Visualizing the tree<br/>from sklearn.externals.six import StringIO  <br/>from IPython.display import Image  <br/>from sklearn.tree import export_graphviz<br/>import pydotplus<br/>dot_data = StringIO()<br/>export_graphviz(clf, out_file=dot_data,  <br/>                filled=True, rounded=True,<br/>                special_characters=True)<br/>graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  <br/>Image(graph.create_png())</span></pre><h2 id="3b01" class="og la it bd lb om on dn lf oo op dp lj ma oq or ll me os ot ln mi ou ov lp ow bi translated">决策树:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/f5b44c6a995551b2987b04c79d44b5e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErWbNmD6q7PppO2GLOMnqw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 3</figcaption></figure><p id="5b20" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">对于新来的人来说，这是一棵令人生畏的树。让我们把它分解一下:</p><blockquote class="oz"><p id="cc7e" class="pa pb it bd pc pd pe pf pg ph pi mm dk translated"><strong class="ak"> <em class="pj">特色:</em> </strong> <em class="pj"> X0(法新社)。X1 (CEA)。X2 (CA125)。X3 (CA50) </em></p><p id="776e" class="pa pb it bd pc pd pe pf pg ph pi mm dk translated"><strong class="ak"> <em class="pj">第一层:</em> </strong> <em class="pj"> CEA ≤ 3.25，基尼 0.492，利差= 144，111 </em></p></blockquote><p id="accc" class="pw-post-body-paragraph lr ls it lt b lu pk ju lw lx pl jx lz ma pm mc md me pn mg mh mi po mk ml mm im bi translated">根节点显示了分支之前整个数据集的基尼指数。基尼系数越低，数据越纯净。最糟糕的混合数据给出的基尼指数为 0.5。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/21d53cd6f78eec27edaec840e89a28a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*Mu5SI5DkeM1uKEfrFLkO0Q.png"/></div></figure><p id="fef6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">更新一下，我们的数据中有 144 名非癌症患者和 111 名癌症患者。这方面的基尼指数将是 0.492，这意味着它非常混杂。但不用担心，随着新的分支和节点的形成，这棵树会降低基尼指数。</p><blockquote class="nt nu nv"><p id="6c02" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated">基尼指数= 1−((144/255)^2)+((111/255)^2)= 0.4916</p></blockquote><p id="30da" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">回归模型告诉我们 CEA 是最具预测性的特征，具有最高的系数和最低的 p 值。决策树通过将 CEA 放在根节点上来同意这一点。每隔一个节点都是根节点 split 的衍生物。该算法选择在 CEA 水平 3.25 处分割，因为该点将目标变量分割成癌性和非癌性，比任何其他属性中的任何其他点更纯粹。CEA 值低于 3.25 (180 个样本)的情况更可能是非癌性的；高于 3.25 (75 个样本)的实例更有可能是癌性的。参考根下面的连接内部节点，查看实例是如何进一步划分的。</p><p id="685f" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">树的第二层分析两个新的数据桶(CEA 3.25 以下的 180 个样本和以上的 75 个样本)的方式与分析根节点的方式相同:</p><blockquote class="nt nu nv"><p id="015a" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated">它运行 ID3 算法，找到将目标变量分割为其最大纯度的属性，确定最佳分界点，然后进行分割。</p></blockquote><p id="c716" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">CEA 级别 3.25 以上拆分的第二层节点是基于 CA125 级别 38.6 以上。这种分割产生了 72 个样本的另一个内部节点和 3 个样本的第一个叶节点。该叶节点的基尼指数为 0，因为该节点中的所有 3 个样本都被分类为非癌。该算法考虑基于该特定叶节点对未来数据进行分类的方式是:</p><blockquote class="nt nu nv"><p id="647f" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated"><strong class="lt iu">如果:</strong> CEA ≥ 3.25 <strong class="lt iu">且:</strong> CA125 ≥ 38.65 <strong class="lt iu"> → </strong>患者=非癌(0)</p></blockquote><p id="28d4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">该过程继续，直到树在所有叶节点结束，并且对于每一系列分裂都有一个决策。</p><h2 id="8268" class="og la it bd lb om on dn lf oo op dp lj ma oq or ll me os ot ln mi ou ov lp ow bi translated">随机森林</h2><p id="44a0" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将实现一个随机森林，而不是停留在那里，将我们的模型建立在树叶的基础上:获取随机样本，形成许多决策树，并取这些决策的平均值来形成一个更精确的模型。在这个模型中，我们取 1000 个树木样本的平均值。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="5a6a" class="og la it oc b gy oh oi l oj ok">#Importing<br/>from sklearn import metrics<br/>from sklearn.model_selection import train_test_split as tts</span><span id="ccb3" class="og la it oc b gy ox oi l oj ok">#Dividing into training(70%) and testing(30%)X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=None)</span><span id="1e89" class="og la it oc b gy ox oi l oj ok">#Running new regression on training data<br/>treeclass = RandomForestClassifier(n_estimators=1000)<br/>treeclass.fit(X_train, y_train)</span><span id="20ff" class="og la it oc b gy ox oi l oj ok">#Calculating the accuracy of the training model on the testing data<br/>y_pred = treeclass.predict(X_test)<br/>y_pred_prob = treeclass.predict_proba(X_test)<br/>accuracy = treeclass.score(X_test, y_test)<br/>print(‘The accuracy is: ‘ + str(accuracy *100) + ‘%’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/1b2c655843cb72bf2f20dc6d37deb877.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*3K6IeNtElC1vU9_sLrUc0g.png"/></div></figure><p id="8733" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">与逻辑斯蒂模型 74%的准确率相比，这一准确率为 71%。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="3fd3" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">测试模型</h1><h2 id="c0c0" class="og la it bd lb om on dn lf oo op dp lj ma oq or ll me os ot ln mi ou ov lp ow bi translated">混淆矩阵</h2><blockquote class="nt nu nv"><p id="73a7" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated"><em class="it">编辑:我正在和生物状态的一个朋友谈论我的分析，这个领域的惯例是这种疾病被认为是阳性的。我武断地将癌症设为阴性，因为当时我并不知道。</em></p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl ol"><img src="../Images/63bc03d23c6438de3a20d27770c228aa.png" data-original-src="https://miro.medium.com/v2/format:webp/1*PQzpzwv3h98UGENcWwsPGA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 4</figcaption></figure><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="5210" class="og la it oc b gy oh oi l oj ok">#Confusion Matrix<br/>from sklearn.metrics import confusion_matrix<br/>confusion_matrix = confusion_matrix(y_test, y_pred)<br/>print(confusion_matrix)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/df2dff10d34824e321ee26b252c56ba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*_68k7iVdBfVkp9Qyf0j4xA.png"/></div></figure><p id="e8f6" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">将上面的矩阵与图 4 相匹配，了解它的意思:</p><ul class=""><li id="afd2" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu"> 34 我们模型的</strong>猜测为<strong class="lt iu">真阳性</strong>:模型认为患者没有癌症，他们确实没有癌症。</li><li id="dbb6" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> 21 我们模型的</strong>猜测为<strong class="lt iu">真阴性</strong>:模型认为患者有癌症，他们确实有癌症。</li><li id="4571" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> 14 我们的模型</strong>的猜测是<strong class="lt iu">假阴性</strong>:模型认为患者有癌症，但实际上他们没有癌症</li><li id="2537" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> 8 本模型</strong>猜测为<strong class="lt iu">假阳性</strong>:本模型认为患者无癌症，但实际确实有癌症。</li></ul><p id="66c2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">我们总数据的 30%进入了测试组，剩下 255 个(. 3) = 77 个实例被测试。矩阵的和是 77。将“真”数除以总数，就可以得出我们模型的准确度:55/77 = 71%。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="92e2" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">形成准确度图和 ROC 曲线的新数据框架:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c650" class="og la it oc b gy oh oi l oj ok">#Formatting for ROC curve<br/>y_pred_prob = pd.DataFrame(y_pred_prob)<br/>y_1_prob = y_pred_prob[1]<br/>y_test_1 = y_test.reset_index()<br/>y_test_1 = y_test_1['class (Y)']<br/>X_test = X_test.reset_index()<br/>CEA = X_test['CEA']<br/>CA125 = X_test['CA125']</span><span id="c596" class="og la it oc b gy ox oi l oj ok">#Forming new df for ROC Curve and Accuracy curve<br/>df = pd.DataFrame({ 'CEA': CEA, 'CA125': CA125, 'y_test': y_test_1, 'model_probability': y_1_prob})<br/>df = df.sort_values('model_probability')</span><span id="e52c" class="og la it oc b gy ox oi l oj ok">#Creating 'True Positive', 'False Positive', 'True Negative' and 'False Negative' columns <br/>df['tp'] = (df['y_test'] == int(0)).cumsum()<br/>df['fp'] = (df['y_test'] == int(1)).cumsum()<br/>total_0s = df['y_test'].sum()<br/>total_1s = abs(total_0s - len(df))<br/>df['total_1s'] = total_1s<br/>df['total_0s']= total_0s<br/>df['total_instances'] = df['total_1s'] + df['total_0s']<br/>df['tn'] = df['total_0s'] - df['fp']<br/>df['fn'] = df['total_1s'] - df['tp']<br/>df['fp_rate'] = df['fp'] / df['total_0s']<br/>df['tp_rate'] = df['tp'] / df['total_1s']</span><span id="de51" class="og la it oc b gy ox oi l oj ok">#Calculating accuracy column<br/>df['accuracy'] = (df['tp'] + df['tn']) / (df['total_1s'] + df['total_0s'])</span><span id="0e04" class="og la it oc b gy ox oi l oj ok">#Deleting unnecessary columns<br/>df.reset_index(inplace = True)<br/>del df['total_1s']<br/>del df['total_0s']<br/>del df['total_instances']<br/>del df['index']</span><span id="88cc" class="og la it oc b gy ox oi l oj ok">#Export the log into excel to show your friends<br/>export_excel = df.to_excel (r"C:\Users\Andrew\Desktop\df1.xlsx", index = None, header=True)</span></pre><p id="c8d4" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为了理解下面数据框架中发生的事情，让我们一行一行地分析它。</p><ul class=""><li id="1dfd" class="mn mo it lt b lu mp lx mq ma mr me ms mi mt mm mu mv mw mx bi translated"><strong class="lt iu">索引</strong>:该数据框架按 model_probability 排序，为方便起见，我重新进行了索引。</li><li id="4765" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> CA125 与 CEA </strong>:蛋白水平原始检测数据。</li><li id="0e6d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> model_probability </strong>:该列来自我们的训练数据的 logistic 模型，输出它基于输入的测试蛋白水平被分类为“1”(癌变)的概率预测。第一行是分类为癌性的可能性最小的实例，其 CA125 水平高，CEA 水平低。</li><li id="6db0" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> y_test </strong>:我们用来检验模型性能的测试数据的实际分类。</li></ul><blockquote class="oz"><p id="643e" class="pa pb it bd pc pd ps pt pu pv pw mm dk translated">其余的列完全基于“y_test”，而不是我们模型的预测。将这些值视为它们自己的混淆矩阵。这将有助于我们确定最佳截止点的位置。</p></blockquote><ul class=""><li id="5636" class="mn mo it lt b lu pk lx pl ma px me py mi pz mm mu mv mw mx bi translated"><strong class="lt iu"> tp(真阳性)</strong>:此列从 0 开始。如果 y_test 为“0”(良性)，则该值增加 1。它是所有潜在的真正积极因素的累积追踪器。第一行就是一个例子。</li><li id="5c39" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> fp(假阳性)</strong>:本列从 0 开始。如果 y_test 为“1”(癌变)，则该值增加 1。它是所有潜在假阳性的累积跟踪器。第四行就是一个例子。</li><li id="695d" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> tn(真阴性)</strong>:该列从 32 开始(测试集中 1 的总数)。如果 y_test 为‘1’(癌变)，则该值减少 1。它是所有潜在真阴性的累积追踪器。第四行就是一个例子。</li><li id="9990" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> fn(假阴性)</strong>:该列从 45 开始(测试集中 0 的总数)。如果 y_test 为‘0’(良性)，则该值减少 1。它是所有潜在假阴性的累积跟踪器。第四行就是一个例子。</li><li id="fe92" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu"> fp_rate(假阳性率)</strong>:这是通过计算行的假阳性计数并除以阳性总数(在我们的例子中是 45)得到的。它让我们知道我们可以通过在该行设置分界点来分类的假阳性的数量。我们希望尽可能降低成本。</li><li id="b33a" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">TP _ Rate(True Positive Rate)</strong>:也称为 sensitivity，计算方法是取该行的真阳性计数，然后除以阳性总数。它让我们知道我们可以通过在那一行设置分界点来分类的真阳性的数量。<strong class="lt iu">我们希望保持尽可能高的价格。</strong></li><li id="78ea" class="mn mo it lt b lu my lx mz ma na me nb mi nc mm mu mv mw mx bi translated"><strong class="lt iu">准确性</strong>:真阳性和真阴性的总和除以总实例数(在我们的例子中是 77)。我们一行一行地计算潜在的精确度，基于我们混淆矩阵的可能性。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/eb89029c1a1bdbc7c384fd19a4495558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*jvVrSqrQoIYWPgQAe_a9Qg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 5</figcaption></figure><p id="db12" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">查看数据帧中的混淆矩阵后，尝试找到最高的<strong class="lt iu">准确率</strong>。如果您可以找到它，您可以将其与相应的<strong class="lt iu"> model_probability </strong>进行匹配，以发现分类的最佳分界点。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c75d" class="og la it oc b gy oh oi l oj ok">#Plot<br/>plt.plot(df[‘model_probability’],df[‘accuracy’], color = ‘c’)<br/>plt.xlabel(‘Model Probability’)<br/>plt.ylabel(‘Accuracy’)<br/>plt.title(‘Optimal Cutoff’)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/0682de2a8c35f2f61f509c8cc92ad378.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*C-SRDByvZNQeiqdYMyP0Zg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 6</figcaption></figure><p id="293c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">随机森林模型将截止点设置为 60%的模型概率，即 75%的准确度。</p><blockquote class="nt nu nv"><p id="d279" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated">这可能看起来违背直觉，但这意味着如果我们在将患者归类为癌症患者时使用 60%而不是 50%，那么使用这一特定模型实际上会更准确。</p></blockquote><p id="dd0e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">作为比较，逻辑模型将它的最佳截止点设置为 54%的概率，同样的准确度为 75%。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="e4f8" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">最后，让我们绘制 ROC 曲线并找出 AUC:</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="221f" class="og la it oc b gy oh oi l oj ok">#Calculating AUC<br/>AUC = 1-(np.trapz(df[‘fp_rate’], df[‘tp_rate’]))<br/>#Plotting ROC/AUC graph<br/>plt.plot(df[‘fp_rate’], df[‘tp_rate’], color = ‘k’, label=’ROC Curve (AUC = %0.2f)’ % AUC)<br/>#Plotting AUC=0.5 red line<br/>plt.plot([0, 1], [0, 1],’r — ‘)<br/>plt.xlabel(‘False Positive Rate’)<br/>plt.ylabel(‘True Positive Rate (Sensitivity)’)<br/>plt.title(‘Receiver operating characteristic’)<br/>plt.legend(loc=”lower right”)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qc"><img src="../Images/e29fa4697f876e77f8dd74db91d798e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SAS0s6_yo3vFWRt5ymZjgQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 7</figcaption></figure><p id="9ada" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">黑色的 ROC 曲线显示了我们的测试数据的真阳性率和假阳性率之间的权衡。穿过图表中心的红色虚线是为了提供一种最坏可能模型看起来像 ROC 曲线的感觉。</p><blockquote class="nt nu nv"><p id="1cc7" class="lr ls ng lt b lu mp ju lw lx mq jx lz nw nd mc md nx ne mg mh ny nf mk ml mm im bi translated">ROC 线越靠近左上方，我们的模型越有预测性。它越像红色虚线，预测性就越低。</p></blockquote><p id="f62c" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">这就是曲线下面积(AUC)的由来。AUC 是位于 ROC 曲线下的空间面积。直观上，这个值越接近 1，我们的分类模型就越好。虚线的 AUC 是 0.5。完美模型的 AUC 应该是 1。我们的随机森林的 AUC 为 0.71。</p><p id="bd0b" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">为了比较，提供了 AUC 为 0.82 的 logistic ROC 曲线:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qd"><img src="../Images/0176ce292d23d35ae9dc280c35b6204d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9PrbPDVVZzV7d93EnCG7Dg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 8</figcaption></figure></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="cad1" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">结论</h1><p id="807a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">比较模型之间的准确率和 AUC，这次是 logistic 回归胜出。这两种模式都各有利弊。</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="a4b7" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated">如果你觉得这有帮助，请订阅。如果你喜欢我的内容，请查看其他几个项目:</p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><p id="9d9e" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/linear-vs-polynomial-regression-walk-through-83ca4f2363a3"> <em class="ng">简单线性 vs 多项式回归</em> </a></p><p id="ede0" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/predicting-cancer-with-logistic-regression-in-python-7b203ace16bc"> <em class="ng">用 Python 中的逻辑回归预测癌症</em> </a></p><p id="e848" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/univariate-logistic-regression-example-in-python-acbefde8cc14">二元逻辑回归示例(python) </a></p><p id="5daf" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/r-squared-recipe-5814995fa39a"> <em class="ng">从头开始计算 R 平方(使用 python) </em> </a></p><p id="0976" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/better-programming/risk-board-game-battle-automation-5e2d955cc9b3" rel="noopener"> <em class="ng">风险棋盘游戏战斗自动化</em> </a></p><p id="a333" class="pw-post-body-paragraph lr ls it lt b lu mp ju lw lx mq jx lz ma nd mc md me ne mg mh mi nf mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/better-programming/risk-board-game-battle-probability-grid-program-f3073fb34e5c" rel="noopener"> <em class="ng">风险棋盘游戏——战斗概率网格程序</em> </a></p></div><div class="ab cl nh ni hx nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="im in io ip iq"><h1 id="5b65" class="kz la it bd lb lc no le lf lg np li lj jz nq ka ll kc nr kd ln kf ns kg lp lq bi translated">延伸阅读:</h1><p id="fd2e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">随机森林回归:何时失败，为什么？</p></div></div>    
</body>
</html>