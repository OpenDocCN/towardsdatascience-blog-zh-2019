<html>
<head>
<title>Learning Parameters, Part 1: Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习参数，第 1 部分:梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb?source=collection_archive---------14-----------------------#2019-05-14">https://towardsdatascience.com/learning-parameters-part-1-eb3e8bb9ffbb?source=collection_archive---------14-----------------------#2019-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a3ed" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/learning-parameters" rel="noopener" target="_blank">学习参数</a></h2><div class=""/><div class=""><h2 id="9342" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">梯度下降是一种迭代优化算法，用于寻找函数的(局部)最小值。</h2></div><p id="cf51" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">梯度下降是最优化中最流行的技术之一，非常常用于训练神经网络。给定基本微积分的正确背景，它是直观的和可解释的。看一看我的这篇博客文章— <a class="ae ln" rel="noopener" target="_blank" href="/learning-parameters-part-0-5cfffd647bdc">第 0 部分</a>，它涵盖了更好地理解这个系列所需的一些先决条件。你可以点击这个帖子顶部的标签查看<strong class="kt jd"> <em class="lo">学习参数</em> </strong>系列的所有帖子。</p><p id="a764" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇博文中，我们用一个玩具神经网络来建立梯度下降的动机。我们还从零开始推导梯度下降更新规则，并使用相同的玩具神经网络解释几何发生的事情。</p><blockquote class="lp lq lr"><p id="6c4f" class="kr ks lo kt b ku kv kd kw kx ky kg kz ls lb lc ld lt lf lg lh lu lj lk ll lm im bi translated">引用说明:本博客中的大部分内容和图表直接取自 IIT 马德拉斯大学教授 Mitesh Khapra 提供的深度学习课程第 5 讲。</p></blockquote><h1 id="b600" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">陈腐的比喻</h1><p id="f5ee" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">想象你站在一座山上，有很多可能的路径可供你下山。梯度下降(GD)简单来说就是给你一个下山的原则性方法。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/7db44f12dec87f6948353e9f08b85c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*i3Gsdnr4srsBoLhPVoEUWQ.png"/></div></figure><p id="2cc5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">全球动力局鼓励你首先找到一个方向，在这个方向上，山有最陡的上升，并要求你去完全相反的方向。你可能会说这个看似简单的想法并不总是有效。你是对的，GD 可以让你走得慢，即使是在平坦的地面上(当你可以跑的时候)，但是我们会在文章的最后解决这个限制。</p><h1 id="5803" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">动机</h1><p id="d187" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">以一个玩具为例，假设我们想要训练一个只有一个神经元的玩具神经网络。前提是:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi na"><img src="../Images/2d522840b2c561f81d47082d045ed509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBDrFo0woUzqNPjP0TMs-Q.png"/></div></div></figure><p id="e7d5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">训练目标是找到使函数<em class="lo"> </em> <strong class="kt jd"> <em class="lo"> L(w，b) </em> </strong> <em class="lo"> </em>输出其<em class="lo"> </em>最小值<em class="lo">的<strong class="kt jd"><em class="lo"/></strong><em class="lo">和</em> <strong class="kt jd"> <em class="lo"> b </em> </strong>的最佳组合。</em></p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/6346b24ac043c9dda78ac8b7293922a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*HaUe7o3HKmyS-JpdgS_iuQ.png"/></div></figure><p id="4ca8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，训练一个网络意味着什么？假设我们用(<em class="lo"> x，y </em> ) <em class="lo"> = </em> (0.5，0.2) <em class="lo"> </em>和(2.5，0.9)训练玩具网络，在训练结束时，我们期望找到<strong class="kt jd"><em class="lo">*</em></strong>和<strong class="kt jd"> <em class="lo"> b* </em> </strong>使得<em class="lo"> f </em> (0.5)输出 0.2 和<em class="lo"> f </em> (2.5)我们希望看到一个 sigmoid 函数，使得(0.5，0.2)和(2.5，0.9)位于 sigmoid 上。</p><h2 id="44ab" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">你永远不会做的暴力行为</h2><p id="3ac2" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">我们能不能试着手动找到这样一个<strong class="kt jd"><em class="lo">w *</em></strong><strong class="kt jd"><em class="lo">b *</em></strong>？让我们试着随机猜测一下..(假设 w = 0.5，b = 0)</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nr"><img src="../Images/65a98fbd2cb5af3aee9007084e0fa8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NkCdVlIPhoMnVhW-Ntu4Kw.png"/></div></div></figure><p id="2978" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们继续猜。我们走吧。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ns"><img src="../Images/26d443237e7417a37a65084281681a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bj0KLAu8bluOe1AxIu2VNA.png"/></div></div></figure><h2 id="4384" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">你永远不会做的事情的几何解释</h2><p id="2000" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">我们能把猜测形象化吗？我们可以！由于我们只有 2 个点和 2 个参数<strong class="kt jd"> <em class="lo"> (w，b) </em> </strong>，我们可以很容易地为<strong class="kt jd"> <em class="lo"> (w，b) </em> </strong>的不同值绘制<strong class="kt jd"><em class="lo">【L(w，b)】</em></strong>，并选择其中<strong class="kt jd"><em class="lo">【L(w，b) </em> </strong>最小的一个。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/eeac649b1b5ebf715319b05b561ee3ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*rg7pGh5YSNlm2ghoum83wA.png"/></div></figure><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nu"><img src="../Images/dbb4ebcbe8c400fe2c4fcad867ef6cea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LPxbwM51k2iXDsM5vrqcpw.gif"/></div></div></figure><p id="070d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是当然，一旦有了更多的数据点和更多的参数，这就变得棘手了！此外，这里我们只绘制了来自(6，6)的<strong class="kt jd"> <em class="lo"> (w，b)</em></strong>的小范围误差表面，而不是来自(inf，inf)的误差表面。梯度下降救援！</p><h1 id="1e99" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">梯度下降</h1><p id="232d" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">如果还不清楚，手头的任务就是找到使损失函数最小的最佳参数组合。梯度下降为我们提供了一种遍历误差面的原则性方法，这样我们就可以快速达到最小值，而无需诉诸蛮力搜索。</p><h2 id="8141" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">推导梯度下降规则</h2><p id="5da2" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">合理的做法是随机初始化<strong class="kt jd"> <em class="lo"> w </em> </strong>和<strong class="kt jd"> <em class="lo"> b </em> </strong>，然后以<strong class="kt jd">最佳方式</strong>迭代更新它们，以达到我们最小化损失函数的目标。让我们用数学来定义它。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nv"><img src="../Images/634fadea048f91012bade93a0ed6ab9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jKpqYCSvhEEv-jynCwNKdQ.png"/></div></div></figure><p id="9b87" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是我们如何找到<em class="lo"> θ </em>中最‘想要’的变化呢？什么是正确的<strong class="kt jd">δ</strong><em class="lo">θ</em>使用？答案来自于<a class="ae ln" href="http://fourier.eng.hmc.edu/e176/lectures/NM/node45.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">泰勒级数</strong> </a>。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nw"><img src="../Images/fcc11e5bc64302558f3604d495b8acb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9B5TFde4a7Yh0Yt35nsBeA.png"/></div></div></figure><p id="47a4" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这意味着我们要移动的方向<em class="lo"> u </em>或<em class="lo"/><strong class="kt jd">δ</strong><em class="lo">θ</em>应该与坡度成 180 度角。在损失面上的给定点，我们的移动方向与该点损失函数的梯度方向相反。这就是黄金梯度下降法则！！</p><h2 id="4f92" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">权重/参数更新规则</h2><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nx"><img src="../Images/b1553cbf9f09d3f634f8465240c5ec6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*imRrzCELAOGiGHPg6YMlsg.png"/></div></div></figure><h2 id="d9e8" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">算法</h2><p id="25ca" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">既然我们在<strong class="kt jd"> <em class="lo"> w-b </em> </strong> <em class="lo"> </em>平面上有了比我们的蛮力算法更有原则的移动方式。让我们根据这个规则创建一个算法…</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/9e8207957ad3281c450bd230801b3301.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Gnc1n67aNhaqaByWe-y5HA.png"/></div></figure><h1 id="7fd0" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">梯度下降在行动</h1><p id="78c0" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">为了在实践中看到 GD，我们首先必须为我们的玩具神经网络导出∇ <strong class="kt jd"> <em class="lo"> w </em> </strong>和∇ <strong class="kt jd"> <em class="lo"> b </em> </strong>。如果你算出来，你会看到它们如下:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/41171c6d6cff1532c4f98f7d8bcec5bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*RjdP7GPIZjtsj3YQUn5oNA.png"/></div></figure><h2 id="03a8" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">Python 代码</h2><figure class="mt mu mv mw gt mx"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h2 id="9d68" class="ng lw it bd lx nh ni dn mb nj nk dp mf la nl nm mh le nn no mj li np nq ml iz bi translated">几何解释</h2><p id="99f4" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">让我们从误差面上的一个随机点开始，看看更新情况。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ff599214be7e1dc6ff521d15b80cfe34.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/1*ghMZkEOArRtRVGOFMu4aOQ.gif"/></div></figure><h1 id="e393" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">限制</h1><p id="1510" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">原始形式的梯度下降有一个明显的缺点。让我们看一个示例曲线<em class="lo"> f(x) = x + 1，</em>如下所示:</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi od"><img src="../Images/115db4ff25e16f27260b0c1c6ef050af.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*_V2aSf3uNzbWEy_JRYak-A.png"/></div></figure><ul class=""><li id="c336" class="oe of it kt b ku kv kx ky la og le oh li oi lm oj ok ol om bi translated">当曲线很陡时，梯度(∇y1/∇x1)很大。</li><li id="ecb5" class="oe of it kt b ku on kx oo la op le oq li or lm oj ok ol om bi translated">当曲线平缓时(∇y2/∇x2)很小。</li></ul><p id="2f4f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">回想一下，我们的权重更新与梯度<strong class="kt jd"> <em class="lo"> w = w — η∇w </em> </strong> <em class="lo">成正比。</em>因此，在曲线平缓的区域，更新较小，而在曲线陡峭的区域，更新较大。让我们看看当我们从曲面上的不同点开始时会发生什么。</p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5f662f4c47fbfb4b66ac7ee00d08df35.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/1*qJ4t5n_9-6AnpOufeYBmMA.gif"/></div></figure><p id="9f88" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">不管我们从哪里开始，一旦我们碰到一个缓坡的表面，前进的速度就会减慢。由于这个原因，训练方法可能永远无法收敛。</p><h1 id="c345" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">结论</h1><p id="afce" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">在这篇博文中，我们用一个玩具神经网络来强调梯度下降的必要性。我们还从零开始推导梯度下降更新规则，并使用相同的玩具神经网络以几何方式解释每次更新发生的情况。我们还解决了 GD 的一个重要限制，在平缓区域减速的问题，以曲线图示的原始形式。<em class="lo">基于动量的梯度下降</em>通过让最近梯度更新的“动量”控制当前步骤中的更新幅度，在一定程度上克服了这个缺点。</p><p id="d4e6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请阅读本系列的下一篇文章，网址是:</p><ul class=""><li id="a1b7" class="oe of it kt b ku kv kx ky la og le oh li oi lm oj ok ol om bi translated"><a class="ae ln" rel="noopener" target="_blank" href="/learning-parameters-part-2-a190bef2d12">学习参数，第 2 部分:基于动量和内斯特罗夫加速梯度下降</a></li></ul><h1 id="27f2" class="lv lw it bd lx ly lz ma mb mc md me mf ki mg kj mh kl mi km mj ko mk kp ml mm bi translated">承认</h1><p id="4a3b" class="pw-post-body-paragraph kr ks it kt b ku mn kd kw kx mo kg kz la mp lc ld le mq lg lh li mr lk ll lm im bi translated">非常感谢 IIT·马德拉斯教授的<a class="ae ln" href="https://www.cse.iitm.ac.in/~miteshk/" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd">和<a class="ae ln" href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kt jd"> CS7015:深度学习</strong> </a> <strong class="kt jd"> </strong>课程的丰富内容和创造性的可视化。我只是简单地整理了提供的课堂讲稿和视频。</strong></a></p><figure class="mt mu mv mw gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi ot"><img src="../Images/31223c7659a9f173beddaf102e33c1b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYKQqkRCBSwSnry_4VAAQQ.png"/></div></div><figcaption class="ou ov gj gh gi ow ox bd b be z dk">Source: CS229 — Machine Learning Lecture Notes, Stanford University</figcaption></figure></div></div>    
</body>
</html>