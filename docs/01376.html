<html>
<head>
<title>Part-1: Breaking Enigma! Let’s Detect Objects</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第 1 部分:打破谜！让我们检测物体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/part-1-breaking-enigma-lets-detect-objects-269ab175632c?source=collection_archive---------34-----------------------#2019-03-04">https://towardsdatascience.com/part-1-breaking-enigma-lets-detect-objects-269ab175632c?source=collection_archive---------34-----------------------#2019-03-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f9f3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">现在，让我们开始寻找一个鲁棒和准确的对象检测算法。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/44a9ade5e6640646a2bca0c2bf85204d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SItdW7bTUDFnNHPyWu3QaQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Hawa Mahal Road, Jaipur, Rajasthan (Image Source:<a class="ae ky" href="https://unsplash.com/photos/6rDbvXzIVpQ" rel="noopener ugc nofollow" target="_blank"> unsplash</a>)</figcaption></figure><p id="cb09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">大家好！我是尼特莱普尔的巴拉特。我在印度蒂鲁帕蒂印度理工学院计算机视觉实验室第四学期结束后的研究实习期间。我的导师给了我一个任务，找到并实现一个健壮而准确的目标检测算法，这将有助于他们正在进行的研究工作。</p><h2 id="a831" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">1)对象检测:</h2><p id="5b57" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">对象检测是一种与计算机视觉和图像处理相关的计算机技术，用于检测某一类语义对象(如人、建筑物、汽车等)的实例。)在数字图像和视频中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/9c3fb4fe3c2d9b80af0d6de12573eb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RKputDyZX3unf5nAMMPTw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.2 Machine Learning: Stanford University- Coursera</figcaption></figure><h2 id="fd8c" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated"><strong class="ak"> 2)物体探测器类型:</strong></h2><blockquote class="nd ne nf"><p id="e623" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><strong class="lb iu"> A .)基于区域的对象检测器或两阶段方法:</strong>解决这个问题的一个简单方法是从图像中提取不同的感兴趣区域，并使用 CNN 对该区域内对象的存在进行分类。</p></blockquote><p id="eb34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用一些其他的计算机视觉技术提取建议，然后调整大小为分类网络的固定输入，分类网络充当特征提取器。然后训练一个 SVM 来在对象和背景之间进行分类(每个类别一个 SVM)。此外，一个边界框回归器被<br/>训练，输出一些建议框的校正(偏移)。总体思路如图 3 所示。这些方法非常精确，但计算量很大(低 fps)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/53da8d85d2e17cbefe6ce79c0a900e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-o2WVnm960psoIAiYcR4A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.3 Google Search Pixel</figcaption></figure><p id="e804" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主要技术有:</strong></p><p id="965f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ng"> 1 &gt;滑动窗口</em> </strong></p><p id="cd5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ng"> 2 &gt; R-CNN </em> </strong></p><p id="876d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ng"> 3 &gt;快速 R-CNN </em> </strong></p><p id="a791" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">T22】4&gt;更快 R-CNNT24】</strong></p><p id="5492" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ng"> 5 &gt; FPN…..等等。</em> </strong></p><blockquote class="nd ne nf"><p id="6c3d" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated"><strong class="lb iu"> B .)单镜头物体检测器或统一方法:</strong>这里的区别是，不是产生提议，而是预先定义一组盒子来寻找物体。<br/>使用来自网络后续层的卷积特征图，在这些特征图上运行另一个网络，以预测类别分数和边界框偏移。</p></blockquote><p id="1497" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">步骤如下:<br/> 1。用回归和分类目标训练 CNN。<br/> 2。从后面的层收集激活，以推断全连接或卷积层的分类和位置。<br/> 3。在训练期间，使用“Jaccard 距离”将预测与实际情况联系起来。<br/> 4。在推断过程中，使用非最大值抑制来过滤同一对象周围的多个框。</p><p id="a1cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">主要技术有:</strong></p><blockquote class="nd ne nf"><p id="4f19" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">单一激活图:</p></blockquote><p id="639a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">1&gt;</strong><a class="ae ky" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">YOLO:</strong></a><strong class="lb iu">你只看一次:统一、实时的物体检测</strong></p><p id="db15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">2&gt;</strong><a class="ae ky" href="https://arxiv.org/abs/1612.08242" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">yolo 9000</strong></a><strong class="lb iu">:更好、更快、更强</strong></p><blockquote class="nd ne nf"><p id="9060" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">多重激活图:</p></blockquote><p id="5324" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">1&gt;</strong><a class="ae ky" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">SSD</strong></a><strong class="lb iu">:单发多盒探测器</strong></p><p id="7c7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">2&gt;</strong><a class="ae ky" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">yolov 3</strong></a><strong class="lb iu">:增量改进</strong></p><p id="a43e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">3&gt;</strong><a class="ae ky" href="https://arxiv.org/abs/1701.06659" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">DSSD</strong></a><strong class="lb iu">:解卷积单次检测器</strong></p><p id="9bad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ng">使用多种比例有助于更好地检测图像上不同尺寸的物体，从而获得更高的地图(平均精度<br/>)。</em> <strong class="lb iu"> <em class="ng">因此在这个项目/博客中使用的技术是 YOLO(单激活地图)和 YOLOv3(多激活地图)。</em>T46】</strong></p><h2 id="069b" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated"><strong class="ak"> 3)你只看一次:统一的、实时的物体检测(YOLO)——</strong></h2><p id="ad3d" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated"><strong class="lb iu"> YOLO </strong>模特(<strong class="lb iu">“你只看一次”</strong>；<a class="ae ky" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"> Redmon 等人，2016 </a>)是构建快速实时物体探测器的第一次尝试。因为 YOLO 不经历区域提议步骤，并且仅在有限数量的包围盒上进行预测，所以它能够超快速地进行推断。这里是在 keras 中实现经过训练的 YOLO 的<strong class="lb iu"> GitHub </strong> <a class="ae ky" href="https://github.com/tarahb2103/Object_Dtection-YOLO" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">库</strong> </a>。</p><p id="8536" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.1)网格单元:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/22c00e0cc089cd6c514305cde7b69f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3OO1RwdXuTCM1ZoQ1yssQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.4 YOLO Official Paper</figcaption></figure><p id="41c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">YOLO 将输入图像分成一个<strong class="lb iu"> S </strong> × <strong class="lb iu"> S </strong>网格。每个网格单元只预测<strong class="lb iu">一个</strong>物体。例如，下面的红色网格单元试图预测中心落在网格单元内的“狗”对象。</p><p id="9c12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.2)它是如何工作的？:</strong></p><ul class=""><li id="2071" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">使用整个图像的特征来预测每个边界框。</li><li id="f5d5" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">同时预测图像所有类别的所有边界框。</li><li id="056d" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">将输入图像划分为一个 s*s 网格。如果对象的中心落入网格单元，则该单元负责检测该对象。</li><li id="5907" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">每个网格单元预测 B 边界框和这些框的置信度得分。</li><li id="797e" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">每个网格还预测 C 条件(以包含对象的网格单元为条件)分类概率。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/c85d0df9b253d6b6ee934e827220e3f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gKJLZuZO4RnqpF9DLXg-Ug.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><em class="ob">Fig.5 The workflow of YOLO model. (Image source: </em><a class="ae ky" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ob">original paper</em></a><em class="ob">)</em></figcaption></figure><p id="cb88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它将图像划分为一个 S × S 网格，每个网格单元预测 B 边界框、这些框的置信度和 C 类概率。这些预测被编码为 S×S×(B∫5+C)张量。</p><p id="1099" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.3)特性:</strong></p><ul class=""><li id="9a15" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">工具？—单个神经网络、统一的体系结构(24 个卷积层、4 个最大池和 2 个全连接层)</li><li id="92a6" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">框架？— Darknet —最初的实现是在 C 和 CUDA 中。</li><li id="7353" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">技术背景？—相关方法速度慢、非实时且缺乏推广能力。</li></ul><p id="4342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.4)网络架构:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/cc9c553d4a2de0fa97e8c03d62cc1b11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqgTwsEC_R_IXeljHyPhWA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 6 The Architecture — Detection Network has 24 conv layers and 2 fully connected layers which<br/>are converted into feature map of S * S*(B*5 +C). <em class="ob">(Image source: </em><a class="ae ky" href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ob">original paper</em></a><em class="ob">)</em></figcaption></figure><p id="c5f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.5)术语和公式:</strong></p><ul class=""><li id="97e9" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">置信度得分:</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8cb27347855ca3c3b17691e67627648a.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*RAbYMotZ6E-T3w07YshxCQ.png"/></div></figure><p id="31c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">反映了对盒子包含一个对象的置信度+盒子的准确度。</p><ul class=""><li id="cb67" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">条件类概率:</strong></li></ul><p id="fbd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">&gt;以包含一个对象的网格单元为条件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/566a044139e7c5e8b01fee0136d22aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*yUS_p8CpjBL6-5KCWBa1Xw.png"/></div></div></figure><p id="f5c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">&gt;在测试时，我们将条件类概率和单个盒子置信度预测相乘。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/9cbe5b67e48828904c75ca409706f861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*JGEmfpSR3UjCW7E9nJvzyg.png"/></div></figure><ul class=""><li id="dab9" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">总损失函数:定位+置信度+分类损失:</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5d9e6c209f23c02b6afe23902f2da632.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*M8_RIPeRbTP1-DWuMpYqCQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/6253967f1ec6f5945cabfa942c477455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gl6iML7cSvmc7S027v-GxQ.png"/></div></div></figure><ul class=""><li id="e60b" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated"><strong class="lb iu">非最大抑制:</strong></li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7dc6d503234191aa6ac112b62cb1953c.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*jS4t2C1K3AHMroyJOf4Rrw.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig.7 Non-Max Suppression (Image source <a class="ae ky" href="https://www.google.co.in/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiZ7sfMtOjgAhUG5o8KHWWZCqwQjhx6BAgBEAM&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-deep-into-object-detection-bed442d92b34&amp;psig=AOvVaw0hoQNIUYYBgx8nlVqoH6Ec&amp;ust=1551786292803339" rel="noopener ugc nofollow" target="_blank">Blog</a>)</figcaption></figure><p id="d277" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在预测期间，使用非最大值抑制来过滤每个对象的多个框，这些框可以如图所示进行匹配。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/48b01676822ec255fb8f1cf1456e2ad9.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*yhTA_DmLL_nX5ls5JQVZdQ.png"/></div></figure><ul class=""><li id="a3ff" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">丢弃所有 pc &lt;0.6</li><li id="f93b" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">Pick the box with largest pc. Output that as a prediction.</li></ul><p id="3aea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Discard any remaining box with IOU &gt; = 0.5 的盒子，并输出上一步的盒子。</p><p id="89ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.5)结果:定性分析- </strong></p><p id="427a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> &gt;真检测</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/00e43443d76b4f172ed2744bf24bd47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNqWX4ijKX-JI48ND4tN0Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Table 1: Predicted output results from MS-COCO datasets.</figcaption></figure><p id="d4f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> &gt;错误或变异检测:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/15515880b1e8b8586438e23edfbc6bb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RT5VVGqe0T4neS7b4b7dzw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Table 2: False Detections and Table 3: False Detections on custom images</figcaption></figure><p id="bb4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 3.6)观察到的问题:</strong></p><ul class=""><li id="d754" class="nm nn it lb b lc ld lf lg li no lm np lq nq lu nr ns nt nu bi translated">当与小物体一起出现时，较大物体占优势，如图 a 所示</li><li id="eda6" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">遮挡给检测带来了问题。如图 b 所示，被遮挡的鸟没有被正确检测到。</li><li id="6883" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">图像的分辨率必须很高，否则边界框可能会偏离其位置。</li><li id="ce5d" class="nm nn it lb b lc nv lf nw li nx lm ny lq nz lu nr ns nt nu bi translated">如图 c 所示。为了解决上述问题，继 YOLO 之后出现了许多统一的检测方法:</li></ul><blockquote class="nd ne nf"><p id="0dd7" class="kz la ng lb b lc ld ju le lf lg jx lh nh lj lk ll ni ln lo lp nj lr ls lt lu im bi translated">1.)YOLO9000:更好更快更强<br/> 2。)SSD —单次多盒检测器。<br/> 3。)DSSD-去卷积单触发多盒检测器。<br/> 4。)YOLOv3:增量改进。</p></blockquote><h2 id="b926" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated">在下一部分，我将把 YOLOv3 描述为一项重大改进。</h2><p id="3241" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">喀拉斯 YOLO 的 Github 知识库。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><h1 id="8cb9" class="ot mf it bd mg ou ov ow mj ox oy oz mm jz pa ka mp kc pb kd ms kf pc kg mv pd bi translated">谢谢你。</h1></div></div>    
</body>
</html>