<html>
<head>
<title>PCA| K-means Clustering | |Unsupervised Learning Algorithms|</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA| K 均值聚类| |无监督学习算法|</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/into-to-pca-k-means-clustering-unsupervised-learning-algorithms-5cc5acea274d?source=collection_archive---------20-----------------------#2019-12-03">https://towardsdatascience.com/into-to-pca-k-means-clustering-unsupervised-learning-algorithms-5cc5acea274d?source=collection_archive---------20-----------------------#2019-12-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="73ca" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">看完这个保证你理解无监督学习的原理。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/de97475f66e72aa04e4c0e3117e45b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppDpUaZxU3PkqYelyL_61Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Andrew looks very young when he taught this course!</figcaption></figure><h1 id="7d40" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">摘要</h1><p id="1bf1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">和往常一样，为了节省你的时间(我知道这里的文章太多了，你的时间很宝贵)，我准备先写这篇文章的摘要。</p><p id="3b1b" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在这篇文章中，我谈到了无监督学习算法，包括 K-means 聚类，PCA。对于这些算法，我谈到了它们的应用、优缺点。这里没有代码和数学公式，如果你想学习算法的数学理论，谷歌一下就行了，维基百科上有很多；如果你想看一些真实项目中无监督学习的代码示例，只需<strong class="lp ir">给我留言</strong>，我会再贴一个。</p><p id="a185" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">我<strong class="lp ir">确定</strong>你看完这个就能明白什么是无监督学习了；里面有 gif 图片帮助你理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/af37653ec0abb80b7e587bc2bd0cbbc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNIixlUFwhJvLQjlilI7eQ.jpeg"/></div></div></figure><blockquote class="mp mq mr"><p id="820c" class="ln lo ms lp b lq mj jr ls lt mk ju lv mt ml ly lz mu mm mc md mv mn mg mh mi ij bi translated"><em class="iq">“因为我们不给它答案，所以是无监督学习”</em></p></blockquote><p id="1412" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">监督学习和非监督学习的主要区别在于，在非监督学习中，数据没有标签。</p><p id="18dc" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在监督学习中，系统试图从给定的样本中学习(另一方面，在无监督学习中，系统试图直接从给定的样本中找到模式)。所以如果数据集有标签，那就是监督问题。如果数据集没有标签，那就是无监督问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/77497a6f2008aba31bf9484c90c04bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*ZVQJzbv5AeMbNjUzAoY7dA.png"/></div></figure><p id="56c3" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">上面的左图是监督学习的一个例子:我们使用回归方法来寻找特征之间的最佳拟合线。</p><p id="b6b8" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在无监督学习中，基于特征隔离输入，基于特征所属的聚类生成预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/1810ad5cf1ad626e6471326743606bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M7ylZfy9H78aBlyGmG6GdQ.png"/></div></div></figure><h1 id="4f5c" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">使聚集</h1><h2 id="faec" class="my kw iq bd kx mz na dn lb nb nc dp lf lw nd ne lh ma nf ng lj me nh ni ll nj bi translated">k 均值聚类</h2><p id="d351" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">K-均值聚类的过程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/2e0e7247a439f78f63bd493160cad155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSeJ9fUXOPBqCm6y3ROx5A.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/68e8b2771b55a98cab1478fdb9446e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/1*Nx6IyGfRAV1ly6uDGnVCxQ.gif"/></div></figure><p id="b957" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">优点</strong></p><ol class=""><li id="a8a5" class="nm nn iq lp b lq mj lt mk lw no ma np me nq mi nr ns nt nu bi translated">快速、健壮且易于理解。</li><li id="701e" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">相对高效:O(tknd)，其中 n 是#个对象，k 是#个簇，d 是每个对象的#个维度，t 是#次迭代。正常情况下，k、t、d &lt;&lt; n.</li><li id="c24d" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">Gives the best result when the data set is distinct or well separated from each other.</li></ol><p id="7abb" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><strong class="lp ir">缺点</strong></p><ol class=""><li id="153a" class="nm nn iq lp b lq mj lt mk lw no ma np me nq mi nr ns nt nu bi translated">学习算法需要预先指定聚类中心的数量。</li><li id="61b6" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">使用排他赋值-如果有两个高度重叠的数据，那么 k-means 将无法解析出有两个聚类。</li><li id="c946" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">学习算法对于非线性变换不是不变的，即，对于不同的数据表示，我们得到不同的结果(以笛卡尔坐标和极坐标形式表示的数据将给出不同的结果)。</li><li id="f37c" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">欧几里德距离度量可以不相等地加权潜在的因素。5)学习算法提供平方误差函数的局部最优。</li><li id="c93d" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">随机选择聚类中心无法让我们获得丰硕的成果。</li><li id="c6ef" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">仅在定义平均值时适用，即分类数据不适用。</li><li id="63dc" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">无法处理噪音数据和异常值。</li><li id="b379" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">该算法不适用于非线性数据集。</li></ol><h1 id="083d" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">主成分分析</h1><p id="f85e" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">特征降维是无监督学习的另一个应用。</p><p id="c3e3" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">有两个目的:</p><ol class=""><li id="10ae" class="nm nn iq lp b lq mj lt mk lw no ma np me nq mi nr ns nt nu bi translated">第一，我们在实际项目中经常会遇到特征维度非常高的训练样本，往往我们无法用自己的领域知识人工构造有用的特征；</li><li id="d118" class="nm nn iq lp b lq nv lt nw lw nx ma ny me nz mi nr ns nt nu bi translated">第二，在数据表现上，我们无法用人眼观察到超过三个维度的特征。</li></ol><p id="bb75" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">因此，特征降维不仅重构了有效的低维特征向量，而且为呈现的数据提供了一种方法。</p><p id="3d6f" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">在特征降维方法中，主成分分析是最经典、最实用的特征降维技术，尤其是在图像识别领域。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/fefa31b7d1922fb556a2cd5985ba9a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IWQfJ8r6NrDLafj6kaAa3g.png"/></div></div></figure><p id="72e0" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated">参考资料:</p><div class="ob oc gp gr od oe"><a href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener  ugc nofollow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd ir gy z fp oj fr fs ok fu fw ip bi translated">k 均值聚类</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">k-means 聚类是一种矢量量化的方法，起源于信号处理，在聚类分析中很流行。</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">en.wikipedia.org</p></div></div><div class="on l"><div class="oo l op oq or on os kp oe"/></div></div></a></div><p id="923c" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><a class="ae ot" href="https://www.youtube.com/watch?v=Ev8YbxPu_bQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=Ev8YbxPu_bQ</a></p></div><div class="ab cl ou ov hu ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="ij ik il im in"><p id="a893" class="pw-post-body-paragraph ln lo iq lp b lq mj jr ls lt mk ju lv lw ml ly lz ma mm mc md me mn mg mh mi ij bi translated"><em class="ms">最初发表于</em><a class="ae ot" href="https://www.linkedin.com/pulse/unsupervised-learning-summary-pengyuan-patrick-li/" rel="noopener ugc nofollow" target="_blank">T5【https://www.linkedin.com】</a><em class="ms">。</em> <strong class="lp ir"> <em class="ms">但是，</em> </strong> <em class="ms">这是一个</em> <strong class="lp ir"> <em class="ms">修改后的</em> </strong> <em class="ms"> </em> <strong class="lp ir"> <em class="ms">版本。</em>T25】</strong></p></div></div>    
</body>
</html>