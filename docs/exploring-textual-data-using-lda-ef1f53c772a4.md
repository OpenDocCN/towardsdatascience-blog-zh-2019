# ä½¿ç”¨ LDA æ¢ç´¢æ–‡æœ¬æ•°æ®

> åŸæ–‡ï¼š<https://towardsdatascience.com/exploring-textual-data-using-lda-ef1f53c772a4?source=collection_archive---------29----------------------->

## é€šè¿‡åº”ç”¨æœºå™¨å­¦ä¹ åŸç†æ¥ç†è§£éç»“æ„åŒ–æ–‡æœ¬æ•°æ®ã€‚

![](img/2f1f31b7ba55de7672e9c0a6fe499984.png)

# **ç®€ä»‹**

æˆ‘æœ€è¿‘åœ¨å·¥ä½œä¸­å®Œæˆäº†æˆ‘çš„ç¬¬ä¸€ä¸ªæœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œå¹¶å†³å®šå°†è¯¥é¡¹ç›®ä¸­ä½¿ç”¨çš„æ–¹æ³•åº”ç”¨åˆ°æˆ‘è‡ªå·±çš„é¡¹ç›®ä¸­ã€‚æˆ‘åœ¨å·¥ä½œä¸­å®Œæˆçš„é¡¹ç›®å›´ç»•ç€ä½¿ç”¨[æ½œåœ¨ç‹„åˆ©å…‹é›·åˆ†é…](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) (LDA)å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œè‡ªåŠ¨åˆ†ç±»ã€‚

LDA æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸä¸­çš„ä¸€ç§æ— ç›‘ç£æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç”±äºå…¶æ— ç›‘ç£çš„æ€§è´¨ï¼ŒLDA ä¸éœ€è¦æ ‡è®°çš„è®­ç»ƒé›†ã€‚è¿™ä½¿å¾—å®ƒéå¸¸é€‚åˆæŸäº›ç”¨ä¾‹ï¼Œæˆ–è€…å½“å¤§å‹çš„ã€å¸¦æ ‡ç­¾çš„æ–‡æœ¬æ•°æ®é›†ä¸å®¹æ˜“è·å¾—æ—¶ã€‚

LDA ä¸»è¦ç”¨äºä¸»é¢˜å»ºæ¨¡ï¼Œé€šè¿‡ç›¸ä¼¼æ€§å¯¹æ–‡æœ¬æ–‡æ¡£è¿›è¡Œèšç±»ã€‚æ–‡æ¡£å¤§å°å¯ä»¥å°åˆ°ä¸€ä¸ªå•è¯(ä¸ç†æƒ³),å¤§åˆ°æ•´ä¸ªå‡ºç‰ˆç‰©ã€‚LDA èšç±»çš„å†…å®¹æ˜¯ä½¿ç”¨æ¯ä¸ªæ–‡æ¡£ä¸­çš„æœ¯è¯­(å•è¯)ä»¥åŠå®ƒä»¬å‡ºç°çš„é¢‘ç‡æ¥ç¡®å®šçš„ï¼Œæœ‰æ—¶ç”šè‡³æ˜¯å®ƒä»¬å‡ºç°çš„é¡ºåº(ä½¿ç”¨ [n-grams](https://en.wikipedia.org/wiki/N-gram) )ã€‚è¢«è®¤ä¸ºå½¼æ­¤ç›¸ä¼¼çš„æ–‡æ¡£è¢«èšç±»åœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æˆ‘ä»¬å‡è®¾æ¯ä¸ªèšç±»ä»£è¡¨ä¸€ä¸ªä¸»é¢˜ï¼Œå°½ç®¡ç›´åˆ°èšç±»è¢«åˆ›å»ºä¹‹åæˆ‘ä»¬æ‰çŸ¥é“ä¸»é¢˜æœ¬èº«æ˜¯ä»€ä¹ˆã€‚éœ€è¦æŒ‡å‡ºçš„æ˜¯**æ¨¡å‹æ—¢ä¸ç†è§£è¿™äº›é›†ç¾¤ä¸­æ–‡æ¡£çš„å†…å®¹ä¹Ÿä¸ç†è§£å…¶ä¸Šä¸‹æ–‡**ï¼Œå› æ­¤å®é™…ä¸Šä¸èƒ½ç»™é›†ç¾¤ä¸€ä¸ªä¸»é¢˜æ ‡ç­¾ã€‚ç›¸åï¼Œå®ƒä½¿ç”¨æ¥è‡ª( *0* ï¼Œ *n)çš„ç´¢å¼•æ•´æ•°æ¥â€œæ ‡è®°â€æ¯ä¸ªèšç±»ï¼›* *n* æ˜¯æˆ‘ä»¬å‘Šè¯‰æ¨¡å‹è¦å¯»æ‰¾çš„ä¸»é¢˜æ•°é‡ã€‚ä¸€ä¸ªäººï¼Œæˆ–è€…è¯´[éå¸¸èªæ˜çš„æ°´ç”Ÿå“ºä¹³åŠ¨ç‰©](https://www.independent.co.uk/news/world/europe/beluga-whale-catch-hvaldimir-russian-spy-programme-video-a9197106.html)ï¼Œéœ€è¦åˆ†æè¿™äº›èšç±»ï¼Œå¹¶ç¡®å®šæ¯ä¸ªèšç±»åº”è¯¥å¦‚ä½•è¢«æ ‡è®°ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¸…ç†ä¸€äº› Twitter æ•°æ®ï¼Œå¹¶ç¼–å†™ä¸€ä¸ª LDA æ¨¡å‹æ¥å¯¹è¿™äº›æ•°æ®è¿›è¡Œèšç±»ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/) æ¥ç”Ÿæˆé›†ç¾¤çš„äº¤äº’å¼å¯è§†åŒ–ã€‚

**å…³é”®ä¾èµ–:**ç†ŠçŒ«ã€ [nltk](https://www.nltk.org) ã€ [gensim](https://pypi.org/project/gensim/) ã€numpyã€ [pyLDAvis](https://pyldavis.readthedocs.io/en/latest/)

**è¿™é‡Œæœ‰ä¸€äº›éœ€è¦äº‹å…ˆç†Ÿæ‚‰çš„å®šä¹‰:**

1.  *æ–‡æ¡£:*æ–‡æœ¬å¯¹è±¡(ä¾‹å¦‚ tweet)
2.  [*å­—å…¸*](https://radimrehurek.com/gensim/corpora/dictionary.html) *:* æˆ‘ä»¬çš„æ–‡æ¡£é›†åˆä¸­æ‰€æœ‰æƒŸä¸€æ ‡è®°(å•è¯ã€æœ¯è¯­)çš„åˆ—è¡¨ï¼Œæ¯ä¸ªæ ‡è®°éƒ½æœ‰ä¸€ä¸ªæƒŸä¸€çš„æ•´æ•°æ ‡è¯†ç¬¦
3.  [](https://www.geeksforgeeks.org/bag-of-words-bow-model-in-nlp/)**:*æˆ‘ä»¬æ‰€æœ‰æ–‡æ¡£çš„é›†åˆï¼Œæ¯ä¸ªæ–‡æ¡£ç®€åŒ–ä¸ºä¸€ä¸ªçŸ©é˜µåˆ—è¡¨ï¼Œæ–‡æ¡£ä¸­çš„æ¯ä¸ªå•è¯å¯¹åº”ä¸€ä¸ªçŸ©é˜µâ€” *ä½¿ç”¨ gensim çš„*[*doc 2 bow*](https://kite.com/python/docs/gensim.corpora.Dictionary.doc2bow)*ï¼Œæ¯ä¸ªçŸ©é˜µè¡¨ç¤ºä¸ºä¸€ä¸ªå…ƒç»„ï¼Œå¸¦æœ‰ä¸€ä¸ª* ***æœ¯è¯­çš„å”¯ä¸€æ•´æ•° id*** *ï¼Œç´¢å¼•ä¸º 0 å’Œ(ä¾‹å¦‚ï¼Œæ–‡æ¡£â€œthe box was in the bigger boxâ€å°†è¢«ç®€åŒ–ä¸ºç±»ä¼¼äº[("the "ï¼Œ2)ï¼Œ(" box "ï¼Œ2)ï¼Œ(" was "ï¼Œ1)ï¼Œ(" in "ï¼Œ1)ï¼Œ(" bigger "ï¼Œ1)]çš„å†…å®¹ï¼Œä½†ç”¨" term "æ›¿æ¢æœ¯è¯­çš„å”¯ä¸€å­—å…¸ id)**
4.  **coherence score:* ä¸€ä¸ªèŒƒå›´ä» 0 åˆ° 1 çš„æµ®ç‚¹å€¼ï¼Œç”¨äºè¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹å’Œèšç±»æ•°ä¸æˆ‘ä»¬çš„æ•°æ®çš„å»åˆç¨‹åº¦*
5.  **é›†ç¾¤:*ä»£è¡¨ä¸€ç»„æ–‡æ¡£çš„èŠ‚ç‚¹ï¼Œä¸€ä¸ªæ¨æ–­çš„ä¸»é¢˜*

# *1.æ•°æ®*

*ä»Šå¹´æ—©äº›æ—¶å€™ï¼Œæˆ‘å¼€å§‹æ”¶é›†å‡ åä¸‡æ¡æ”¿æ²»æ¨æ–‡ï¼Œæœ€ç»ˆç›®æ ‡æ˜¯å¯¹æ¨æ–‡åŠå…¶å…ƒæ•°æ®è¿›è¡Œå„ç§åˆ†æï¼Œä¸º 2020 å¹´ç¾å›½æ€»ç»Ÿå¤§é€‰åšå‡†å¤‡ã€‚*

*è¿™ç¯‡æ–‡ç« çš„æ•°æ®é›†å°†ç”± 3500 æ¡æ¨æ–‡ç»„æˆï¼Œå…¶ä¸­è‡³å°‘æåˆ°ä»¥ä¸‹ä¸€æ¡:â€œ@berniesandersâ€ã€â€œkamalaharrisâ€ã€â€œjoebidenâ€ã€â€œewarrenâ€(åˆ†åˆ«æ˜¯ä¼¯å°¼Â·æ¡‘å¾·æ–¯ã€å¡ç›æ‹‰Â·å“ˆé‡Œæ–¯ã€ä¹”Â·æ‹œç™»å’Œä¼Šä¸½èç™½Â·æ²ƒä¼¦çš„æ¨ç‰¹è´¦å·)ã€‚æˆ‘åœ¨ 2019 å¹´ 11 æœˆåˆæ”¶é›†äº†è¿™äº›æ¨æ–‡ï¼Œå¹¶åœ¨è¿™é‡Œæä¾›ä¸‹è½½[ã€‚æˆ‘ä»¬å°†ç ”ç©¶è¿™äº›æ•°æ®ï¼Œå¹¶è¯•å›¾æ‰¾å‡ºäººä»¬åœ¨ 11 æœˆåˆå‘æ¨æ–‡çš„å†…å®¹ã€‚](https://drive.google.com/drive/folders/1ebI3pEkrz3JbyF_aZF4DVPX2LSG1hVn_?usp=sharing)*

*æˆ‘ä¸ä¼šæ·±å…¥ç ”ç©¶å¦‚ä½•æ”¶é›†æ¨æ–‡ï¼Œä½†æˆ‘å·²ç»åŒ…æ‹¬äº†æˆ‘åœ¨ä¸‹é¢ä½¿ç”¨çš„ä»£ç ã€‚æˆåŠŸè¿è¡Œä»£ç éœ€è¦è®¿é—® [tweepy API](http://tweepy.readthedocs.org) ã€‚æˆ‘æ²¡æœ‰æ”¶é›†è½¬å‘ï¼Œä¹Ÿæ²¡æœ‰æ”¶é›†ä¸æ˜¯ç”¨è‹±è¯­å†™çš„æ¨æ–‡(è¯¥æ¨¡å‹éœ€è¦æ›´å¤šçš„è°ƒæ•´ä»¥é€‚åº”å¤šç§è¯­è¨€)ã€‚*

```
*class Streamer(StreamListener):
    def __init__(self):
        super().__init__()
        self.limit = 1000 # Number of tweets to collect.
        self.statuses = []  # Pass each status here.

    def on_status(self, status):
        if status.retweeted or "RT @" 
        in status.text or status.lang != "en":
            return   # Remove re-tweets and non-English tweets.
        if len(self.statuses) < self.limit:
            self.statuses.append(status)
            print(len(self.statuses))  # Get count of statuses
        if len(self.statuses) == self.limit:
            with open("/tweet_data.csv", "w") as    file: 
                writer = csv.writer(file)  # Saving data to csv. 
                for status in self.statuses:
                    writer.writerow([status.id, status.text,
              status.created_at, status.user.name,         
              status.user.screen_name, status.user.followers_count, status.user.location]) 
            print(self.statuses)
            print(f"\n*** Limit of {self.limit} met ***")
            return False
        if len(self.statuses) > self.limit:
            return False

streaming = tweepy.Stream(auth=setup.api.auth, listener=Streamer())

items = ["@berniesanders", "@kamalaharris", "@joebiden", "@ewarren"]  # Keywords to track

stream_data = streaming.filter(track=items)*
```

*è¿™ä¼šå°† tweet æ–‡æœ¬æ•°æ®åŠå…¶å…ƒæ•°æ®(idã€åˆ›å»ºæ—¥æœŸã€åç§°ã€ç”¨æˆ·åã€å…³æ³¨è€…æ•°é‡å’Œä½ç½®)ä¼ é€’ç»™åä¸º tweet_data çš„ csvã€‚*

```
*import pandas as pddf = pd.read_csv(r"/tweet_data.csv", names= ["id", "text", "date", "name", "username", "followers", "loc"])*
```

*ç°åœ¨æˆ‘ä»¬å·²ç»å°†æ•°æ®æ‰“åŒ…åˆ°ä¸€ä¸ªæ•´æ´çš„ csv ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ä¸ºæˆ‘ä»¬çš„ LDA æœºå™¨å­¦ä¹ æ¨¡å‹å‡†å¤‡æ•°æ®äº†ã€‚æ–‡æœ¬æ•°æ®é€šå¸¸è¢«è§†ä¸ºéç»“æ„åŒ–æ•°æ®ï¼Œåœ¨è¿›è¡Œæœ‰æ„ä¹‰çš„åˆ†æä¹‹å‰éœ€è¦æ¸…ç†ã€‚ç”±äºä¸ä¸€è‡´çš„æ€§è´¨ï¼Œæ¨æ–‡å°¤å…¶æ··ä¹±ã€‚ä¾‹å¦‚ï¼Œä»»ä½•ç»™å®šçš„ Twitter ç”¨æˆ·å¯èƒ½æŸä¸€å¤©ç”¨å®Œæ•´çš„å¥å­å‘æ¨ï¼Œè€Œç¬¬äºŒå¤©ç”¨å•ä¸ªå•è¯å’Œæ ‡ç­¾å‘æ¨ã€‚å¦ä¸€ä¸ªç”¨æˆ·å¯èƒ½åªå‘é“¾æ¥ï¼Œå¦ä¸€ä¸ªç”¨æˆ·å¯èƒ½åªå‘æ ‡ç­¾ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œè¿˜æœ‰ç”¨æˆ·å¯èƒ½ä¼šæœ‰æ„å¿½ç•¥çš„è¯­æ³•å’Œæ‹¼å†™é”™è¯¯ã€‚è¿˜æœ‰ä¸€äº›å£è¯­ä¸­ä½¿ç”¨çš„æœ¯è¯­ä¸ä¼šå‡ºç°åœ¨æ ‡å‡†è‹±è¯­è¯å…¸ä¸­ã€‚*

## *æ¸…æ´*

*æˆ‘ä»¬å°†åˆ é™¤æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ã€ç‰¹æ®Šå­—ç¬¦å’Œ url é“¾æ¥ï¼Œç„¶åå¯¹æ¯æ¡æ¨æ–‡åº”ç”¨ *lower()* ã€‚è¿™ä¸ºæˆ‘ä»¬çš„æ–‡æ¡£å¸¦æ¥äº†ä¸€å®šç¨‹åº¦çš„ä¸€è‡´æ€§(è®°ä½æ¯æ¡ tweet éƒ½è¢«è§†ä¸ºä¸€ä¸ªæ–‡æ¡£)ã€‚æˆ‘è¿˜åˆ é™¤äº†â€œberniesandersâ€ã€â€œkamalaharrisâ€ã€â€œjoebidenâ€å’Œâ€œewarrenâ€çš„å®ä¾‹ï¼Œå› ä¸ºå®ƒä»¬ä¼šæ‰­æ›²æˆ‘ä»¬çš„è¯é¢‘ï¼Œå› ä¸ºæ¯ä¸ªæ–‡æ¡£è‡³å°‘ä¼šåŒ…å«å…¶ä¸­ä¸€é¡¹ã€‚*

```
*import stringppl = ["berniesanders", "kamalaharris", "joebiden", "ewarren"]def clean(txt):
    txt = str(txt.translate(str.maketrans("", "", string.punctuation))).lower() 
    txt = str(txt).split()
    for item in txt:
        if "http" in item:
            txt.remove(item)
        for item in ppl:
            if item in txt:
                txt.remove(item)
    txt = (" ".join(txt))
    return txt

df.text = df.text.apply(clean)*
```

# *2.æ•°æ®å‡†å¤‡*

*ä¸‹é¢æ˜¯æˆ‘ä»¬éœ€è¦å¯¼å…¥çš„åŒ…ï¼Œä»¥ä¾¿åœ¨å°†æ•°æ®è¾“å…¥æ¨¡å‹ä¹‹å‰å‡†å¤‡å¥½æ•°æ®ã€‚**åœ¨ç¼–å†™æ•°æ®å‡†å¤‡çš„ä»£ç æ—¶ï¼Œæˆ‘ä¹Ÿä¼šåŒ…æ‹¬è¿™äº›å¯¼å…¥ã€‚***

```
*import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS as stopwords
import nltk
nltk.download("wordnet")
from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm
from nltk.stem.porter import *
import numpy as np
np.random.seed(0)*
```

*æˆ‘ä»¬å·²ç»æ¸…ç†äº†ä¸€äº›æ–‡æ¡£ï¼Œä½†æ˜¯ç°åœ¨æˆ‘ä»¬éœ€è¦[å¯¹å®ƒä»¬è¿›è¡Œè¯æ³•åˆ†æå’Œè¯å¹²åˆ†æã€‚è¯æ±‡åŒ–å°†æ–‡æ¡£ä¸­çš„å•è¯è½¬æ¢ä¸ºç¬¬ä¸€äººç§°ï¼Œå¹¶å°†æ‰€æœ‰åŠ¨è¯è½¬æ¢ä¸ºç°åœ¨æ—¶ã€‚è¯å¹²å¤„ç†å°†æ–‡æ¡£ä¸­çš„å•è¯è¿˜åŸä¸ºå®ƒä»¬çš„æ ¹æ ¼å¼ã€‚å¹¸è¿çš„æ˜¯ï¼Œnltk æœ‰ä¸€ä¸ª lemmatizer å’Œä¸€ä¸ªè¯å¹²åˆ†æå™¨å¯ä¾›æˆ‘ä»¬åˆ©ç”¨ã€‚](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)*

*LDA æ¶‰åŠåˆ°ä¸€ä¸ª[éšæœºè¿‡ç¨‹](https://en.wikipedia.org/wiki/Stochastic_process)ï¼Œæ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹éœ€è¦äº§ç”Ÿéšæœºå˜é‡çš„èƒ½åŠ›ï¼Œå› æ­¤æœ‰äº† *numpy* å¯¼å…¥ã€‚æ·»åŠ  *numpy.random.seed(0)* å…è®¸æˆ‘ä»¬çš„æ¨¡å‹æ˜¯å¯é‡å¤çš„ï¼Œå› ä¸ºå®ƒå°†ç”Ÿæˆå¹¶ä½¿ç”¨ç›¸åŒçš„éšæœºå˜é‡ï¼Œè€Œä¸æ˜¯åœ¨æ¯æ¬¡ä»£ç è¿è¡Œæ—¶ç”Ÿæˆæ–°çš„å˜é‡ã€‚*

*Gensim çš„åœç”¨è¯æ˜¯ä¸€ä¸ªè¢«è®¤ä¸ºä¸ç›¸å…³æˆ–å¯èƒ½æ··æ·†æˆ‘ä»¬è¯æ±‡çš„æœ¯è¯­åˆ—è¡¨ã€‚åœ¨ NLP ä¸­ï¼Œâ€œåœç”¨è¯â€æŒ‡çš„æ˜¯æˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹é€‰å–çš„æœ¯è¯­é›†åˆã€‚æ­¤åˆ—è¡¨å°†ç”¨äºä»æˆ‘ä»¬çš„æ–‡æ¡£ä¸­åˆ é™¤è¿™äº›ä¸ç›¸å…³çš„æœ¯è¯­ã€‚æˆ‘ä»¬å¯ä»¥ *print(stopwords)* æ¥æŸ¥çœ‹å°†è¦åˆ é™¤çš„æœ¯è¯­ã€‚*

*ä»¥ä¸‹æ˜¯åœç”¨è¯ä¸­çš„æœ¯è¯­ã€‚*

*![](img/4a4660ca13aabb8f171d89414ecb6f15.png)*

*å¯¹äºè¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å°†ä¿æŒåœç”¨è¯åˆ—è¡¨ä¸å˜ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¯èƒ½éœ€è¦æ·»åŠ æˆ‘ä»¬å¸Œæœ›æ¨¡å‹å¿½ç•¥çš„ç‰¹å®šæœ¯è¯­ã€‚ä¸‹é¢çš„ä»£ç æ˜¯å‘åœç”¨è¯æ·»åŠ æœ¯è¯­çš„ä¸€ç§æ–¹æ³•ã€‚*

```
*stopwords = stopwords.union(set(["add_term_1", "add_term_2"]))*
```

## *è¯æ±‡åŒ–å’Œè¯å¹²åŒ–*

*è®©æˆ‘ä»¬ä¸ºæˆ‘ä»¬çš„æ•°æ®å‡†å¤‡å†™ä¸€äº›ä»£ç ã€‚*

```
*import warnings 
warnings.simplefilter("ignore")
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS as stopwords
import nltk
nltk.download("wordnet")
from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm
from nltk.stem.porter import *
import numpy as np
np.random.seed(0)*
```

*åˆå§‹åŒ–è¯å¹²åˆ†æå™¨ã€‚*

```
*stemmer = stemm(language="english")*
```

*å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ—¢èƒ½å¯¹æˆ‘ä»¬çš„æ–‡æ¡£è¿›è¡Œè¯æ±‡åŒ–ï¼Œåˆèƒ½å¯¹å…¶è¿›è¡Œè¯å¹²åˆ†æã€‚GeeksforGeeks æœ‰å…³äºä½¿ç”¨ nltk è¿›è¡Œè¯æ³•åˆ†æçš„[ä¸ªä¾‹å­](https://www.geeksforgeeks.org/python-lemmatization-with-nltk/)å’Œå…³äºä½¿ç”¨ nltk è¿›è¡Œè¯å¹²åˆ†æçš„[ä¸ªä¾‹å­](https://www.geeksforgeeks.org/python-stemming-words-with-nltk/)ã€‚*

```
*def lemm_stemm(txt):
    return stemmer.stem(lemm().lemmatize(txt, pos="v"))*
```

*ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œå°†åœç”¨è¯ä»æˆ‘ä»¬çš„æ–‡æ¡£ä¸­åˆ é™¤ï¼ŒåŒæ—¶ä¹Ÿåº”ç”¨*lemm _ stem()*ã€‚*

```
*def preprocess(txt):
    r = [lemm_stemm(token) for token in simple_preprocess(txt) if       token not in stopwords and len(token) > 2]
    return r*
```

*å°†æˆ‘ä»¬æ¸…ç†å’Œå‡†å¤‡å¥½çš„æ–‡æ¡£åˆ†é…ç»™ä¸€ä¸ªæ–°å˜é‡ã€‚*

```
*proc_docs = df.text.apply(preprocess)*
```

# ***3ã€‚æ¨¡å‹çš„åˆ¶ä½œ***

*ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½äº†æ•°æ®ï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹ç¼–å†™æ¨¡å‹äº†ã€‚*

## *è¯å…¸*

*æ­£å¦‚å¼•è¨€ä¸­æåˆ°çš„ï¼Œå­—å…¸(åœ¨ LDA ä¸­)æ˜¯åœ¨æˆ‘ä»¬çš„æ–‡æ¡£é›†åˆä¸­å‡ºç°çš„æ‰€æœ‰å”¯ä¸€æœ¯è¯­çš„åˆ—è¡¨ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ gensim çš„è¯­æ–™åº“åŒ…æ¥æ„å»ºæˆ‘ä»¬çš„è¯å…¸ã€‚*

```
*dictionary = gensim.corpora.Dictionary(proc_docs)
dictionary.filter_extremes(no_below=5, no_above= .90)
len(dictionary)*
```

**filter_extremes()* å‚æ•°æ˜¯é’ˆå¯¹åœç”¨è¯æˆ–å…¶ä»–å¸¸ç”¨æœ¯è¯­çš„ç¬¬äºŒé“é˜²çº¿ï¼Œè¿™äº›åœç”¨è¯æˆ–å¸¸ç”¨æœ¯è¯­å¯¹å¥å­çš„æ„ä¹‰æ²¡æœ‰ä»€ä¹ˆå®è´¨æ„ä¹‰ã€‚æ‘†å¼„è¿™äº›å‚æ•°å¯ä»¥å¸®åŠ©å¾®è°ƒæ¨¡å‹ã€‚å…³äºè¿™ä¸€ç‚¹æˆ‘å°±ä¸èµ˜è¿°äº†ï¼Œä½†æˆ‘åœ¨ä¸‹é¢é™„ä¸Šäº†æ¥è‡ª [gensim çš„å­—å…¸æ–‡æ¡£](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes)ä¸­è§£é‡Šå‚æ•°çš„æˆªå›¾ã€‚*

*![](img/dd6a330c2e96aa11a879313a4102ccbc.png)*

*æˆ‘ä»¬çš„å­—å…¸æœ‰ 972 ä¸ªç‹¬ç‰¹çš„å•è¯(æœ¯è¯­)ã€‚*

*![](img/b664daf8f2317fe5a1b408b2ef0a615c.png)*

## *è¯æ±‡è¢‹*

*å¦‚å¼•è¨€ä¸­æ‰€è¿°ï¼Œå•è¯åŒ…(åœ¨ LDA ä¸­)æ˜¯æˆ‘ä»¬åˆ†è§£æˆçŸ©é˜µçš„æ‰€æœ‰æ–‡æ¡£çš„é›†åˆã€‚çŸ©é˜µç”±æœ¯è¯­çš„æ ‡è¯†ç¬¦å’Œå®ƒåœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°ç»„æˆã€‚*

```
*n = 5 # Number of clusters we want to fit our data to
bow = [dictionary.doc2bow(doc) for doc in proc_docs]
lda = gensim.models.LdaMulticore(bow, num_topics= n, id2word=dictionary, passes=2, workers=2)print(bow)*
```

*![](img/dda660754596d151ab078d7a417b9725.png)*

*è®©æˆ‘ä»¬é€šè¿‡æŸ¥çœ‹å®šä¹‰é›†ç¾¤çš„å…³é”®æœ¯è¯­æ¥äº†è§£æˆ‘ä»¬çš„é›†ç¾¤æ˜¯å¦‚ä½•å½¢æˆçš„ã€‚*

```
*for id, topic in lda.print_topics(-1):
    print(f"TOPIC: {id} \n WORDS: {topic}")*
```

*![](img/dfd835e776d590711bb614aab00066a9.png)*

*æŸ¥çœ‹æ¯ä¸ªä¸»é¢˜ç¾¤ï¼Œæˆ‘ä»¬å¯ä»¥äº†è§£å®ƒä»¬ä»£è¡¨äº†ä»€ä¹ˆã€‚çœ‹ä¸€ä¸‹é¢˜ç›® 1 å’Œé¢˜ç›® 4ã€‚*

**å…³äºè¯é¢˜ 1:* åœ¨è¯é¢˜ 1 ä¸­ï¼Œå…³é”®è¯â€œcenkuygurâ€å’Œâ€œanakasparianâ€æ˜¯æŒ‡ [Cenk ç»´å¾å°”æ—](https://twitter.com/cenkuygur) å’Œ**[Ana Kasparian](https://twitter.com/AnaKasparian)**ï¼Œ**å…±åŒä¸»æŒäºº**[å°‘å£®æ´¾](https://tyt.com)(æŸæ”¿è®ºäº‹åŠ¡æ‰€åŠèŠ‚ç›®)ã€‚ä¸»é¢˜ 1 è¿˜åŒ…æ‹¬å…³é”®æœ¯è¯­â€œæƒåˆ©â€ã€â€œç‰¹æœ—æ™®â€å’Œâ€œå…¨å›½æ­¥æªåä¼šâ€ã€‚*****

*****11 æœˆ 15 æ—¥ï¼ŒåŠ å·åœ£å¡”å…‹æ‹‰é‡Œå¡”é™„è¿‘çš„ç´¢æ ¼æ–¯é«˜ä¸­å‘ç”Ÿäº†æ ¡å›­æªå‡»æ¡ˆã€‚å…³äºè¿™ä¸€æ‚²å‰§äº‹ä»¶ï¼ŒT2 åª’ä½“è¿›è¡Œäº†å¤§é‡æŠ¥é“ï¼Œç½‘ä¸Šä¹Ÿè®®è®ºçº·çº·ã€‚å¹´è½»çš„åœŸè€³å…¶äºº(TYT)æ˜¯æ›´ä¸¥æ ¼çš„æªæ”¯æ³•å¾‹çš„å£å¤´æ”¯æŒè€…ï¼Œå¹¶ç»å¸¸ä¸å…¨å›½æ­¥æªåä¼šå’Œå…¶ä»–æªæ”¯å›¢ä½“å‘ç”Ÿå†²çªã€‚TYT ç”šè‡³å¸¦å¤´å‘èµ·äº†åä¸º#NeverNRA çš„æ‰¿è¯º[è¿åŠ¨ã€‚](https://join.tyt.com/nevernra/)*****

*****è¿™ä¸ªä¸»é¢˜ç¾¤å¯ä»¥è¢«æ ‡ä¸ºâ€œTYT å¯¹å…¨å›½æ­¥æªåä¼šâ€ï¼Œæˆ–ç±»ä¼¼çš„ä¸œè¥¿ã€‚*****

******å…³äºä¸»é¢˜ 4:* æœ¯è¯­â€œcenkuygurâ€å’Œâ€œanakasparianâ€åœ¨ä¸»é¢˜ 4 ä¸­é‡å¤å‡ºç°ã€‚è¯é¢˜ 4 è¿˜åŒ…æ‹¬â€œtheyoungturkâ€ï¼ŒæŒ‡çš„æ˜¯å¹´è½»çš„åœŸè€³å…¶äººï¼Œä»¥åŠâ€œberniâ€ï¼ŒæŒ‡çš„æ˜¯ä¼¯å°¼Â·æ¡‘å¾·æ–¯ã€‚*****

*****11 æœˆ 12 æ—¥ï¼Œå²‘å…‹ç»´ä¸ºå€™é€‰äººä¼¯å°¼Â·æ¡‘å¾·æ–¯å‘å¸ƒ[å…¬å¼€èƒŒä¹¦](https://youtu.be/m4mspXXNiqg)ã€‚TYT çš„æ¨ç‰¹è´¦æˆ·é‡å¤äº†è¿™ä¸€è¡¨æ€ã€‚ä¼¯å°¼Â·æ¡‘å¾·æ–¯éšåå…¬å¼€æ„Ÿè°¢ä»–ä»¬çš„æ”¯æŒã€‚æ­¤å¤–ï¼Œ11 æœˆ 14 æ—¥ï¼Œç»´å¾å°”å…ˆç”Ÿå®£å¸ƒä»–å°†ç«é€‰å›½ä¼šè®®å‘˜ã€‚è¿™ä¸¤é¡¹è¿›å±•éƒ½åœ¨ Twitter ä¸Šè·å¾—äº†æ˜¾è‘—å…³æ³¨ã€‚*****

*****è¿™ä¸ªä¸»é¢˜ç¾¤å¯ä»¥è¢«ç§°ä¸ºâ€œTYT å’Œä¼¯å°¼Â·æ¡‘å¾·æ–¯â€ï¼Œæˆ–è€…ç±»ä¼¼çš„åç§°ã€‚*****

*****![](img/54e5092405606881b103a2bf6640b83c.png)**********![](img/d3260669357d052515659a82da1e67ff.png)*****

*****å…¶ä»–ä¸»é¢˜ç¾¤ä¹Ÿæœ‰ç±»ä¼¼çš„è§£é‡Šã€‚*****

# *******4ã€‚è¯„ä¼°ã€å¯è§†åŒ–ã€ç»“è®º*******

*****å¤§å¤šæ•°å¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œåº”ç”¨éƒ½æœ‰ä¸€ä¸ªåé¦ˆç¯ã€‚è¿™æ˜¯ä¸€ç§è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€å¯ä¼¸ç¼©æ€§å’Œæ•´ä½“è´¨é‡çš„æ–¹æ³•ã€‚åœ¨ä¸»é¢˜å»ºæ¨¡ç©ºé—´ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨[ä¸€è‡´æ€§åˆ†æ•°](http://qpleple.com/topic-coherence-to-evaluate-topic-models/)æ¥ç¡®å®šæˆ‘ä»¬çš„æ¨¡å‹æœ‰å¤šâ€œä¸€è‡´â€ã€‚æ­£å¦‚æˆ‘åœ¨ä»‹ç»ä¸­æåˆ°çš„ï¼Œcoherence æ˜¯ä¸€ä¸ªä»‹äº 0 å’Œ 1 ä¹‹é—´çš„æµ®ç‚¹å€¼ã€‚ä¸ºæ­¤æˆ‘ä»¬ä¹Ÿå°†ä½¿ç”¨ gensimã€‚*****

```
*****# Eval via coherence scoringfrom gensim import corpora, models
from gensim.models import CoherenceModel
from pprint import pprintcoh = CoherenceModel(model=lda, texts= proc_docs, dictionary = dictionary, coherence = "c_v")
coh_lda = coh.get_coherence()
print("Coherence Score:", coh_lda)*****
```

*****![](img/e6759410a9d51303dea0ea26310e3859.png)*****

*****æˆ‘ä»¬å¾—åˆ°äº† 0.44 çš„ä¸€è‡´æ€§åˆ†æ•°ã€‚è¿™ä¸æ˜¯æœ€å¥½çš„ï¼Œä½†å®é™…ä¸Šä¹Ÿä¸ç®—å¤ªå·®ã€‚è¿™ä¸ªåˆ†æ•°æ˜¯åœ¨æ²¡æœ‰ä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹è·å¾—çš„ã€‚çœŸæ­£æŒ–æ˜æˆ‘ä»¬çš„å‚æ•°å’Œæµ‹è¯•ç»“æœåº”è¯¥ä¼šå¾—åˆ°æ›´é«˜çš„åˆ†æ•°ã€‚å¾—åˆ†çœŸçš„æ²¡æœ‰å®˜æ–¹é—¨æ§›ã€‚æˆ‘çš„ä¸€è‡´æ€§åˆ†æ•°ç›®æ ‡é€šå¸¸åœ¨ 0.65 å·¦å³ã€‚å‚è§è¿™ç¯‡[æ–‡ç« ](https://datascienceplus.com/evaluation-of-topic-modeling-topic-coherence/)å’Œè¿™ä¸ªå †æ ˆæº¢å‡º[çº¿ç¨‹](https://stackoverflow.com/questions/54762690/coherence-score-0-4-is-good-or-bad)äº†è§£æ›´å¤šå…³äºä¸€è‡´æ€§è¯„åˆ†çš„ä¿¡æ¯ã€‚*****

## *****ç”¨ pyLDAvis å¯è§†åŒ–*****

*****æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ pyLDAvis å¯è§†åŒ–æˆ‘ä»¬çš„é›†ç¾¤ã€‚è¿™ä¸ªåŒ…åˆ›å»ºäº†ä¸€ä¸ªèšç±»çš„è·ç¦»å›¾ï¼Œæ²¿ç€ x å’Œ y è½´ç»˜åˆ¶èšç±»ã€‚è¿™ä¸ªè·ç¦»åœ°å›¾å¯ä»¥é€šè¿‡è°ƒç”¨ *pyLDAvis.display()* åœ¨ Jupiter ä¸­æ‰“å¼€ï¼Œä¹Ÿå¯ä»¥é€šè¿‡è°ƒç”¨ *pyLDAvis.show()* åœ¨ web ä¸­æ‰“å¼€ã€‚*****

```
*****import pyLDAvis.gensim as pyldavis
import pyLDAvislda_display = pyldavis.prepare(lda, bow, dictionary)
pyLDAvis.show(lda_display)*****
```

*****è¿™æ˜¯æˆ‘ä»¬çš„ pyLDAvis è·ç¦»å›¾çš„æˆªå›¾ã€‚*****

*****![](img/4be55562fcd33d1fbf9e4d3a593b83cf.png)*****

*****å°†é¼ æ ‡æ‚¬åœåœ¨æ¯ä¸ªé›†ç¾¤ä¸Šï¼Œä¼šæ˜¾ç¤ºè¯¥é›†ç¾¤ä¸­å…³é”®æœ¯è¯­çš„ç›¸å…³æ€§(çº¢è‰²)ä»¥åŠè¿™äº›ç›¸åŒå…³é”®æœ¯è¯­åœ¨æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­çš„ç›¸å…³æ€§(è“è‰²)ã€‚è¿™æ˜¯å‘é£é™©æ‰¿æ‹…è€…å±•ç¤ºè°ƒæŸ¥ç»“æœçš„æœ‰æ•ˆæ–¹å¼ã€‚*****

## *******ç»“è®º*******

*****è¿™é‡Œæ˜¯æˆ‘ä¸Šé¢ä½¿ç”¨çš„æ‰€æœ‰ä»£ç ï¼ŒåŒ…æ‹¬æˆ‘ç”¨æ¥ç”Ÿæˆå•è¯äº‘çš„ä»£ç å’Œæˆ‘ç”¨æ¥æ”¶é›†æ¨æ–‡æ•°æ®çš„ä»£ç ã€‚*****

```
*****### All Dependencies ###

import pandas as pd
from wordcloud import WordCloud as cloud
import matplotlib.pyplot as plt
import string
import gensim
from gensim.utils import simple_preprocess
from gensim.parsing.preprocessing import STOPWORDS as stopwords
import nltk
nltk.download("wordnet")
from nltk.stem import WordNetLemmatizer as lemm, SnowballStemmer as stemm
from nltk.stem.porter import *
import numpy as np
np.random.seed(0)
from gensim import corpora, models
from gensim.models import CoherenceModel
from pprint import pprint
import pyLDAvis.gensim as pyldavis
import pyLDAvis

### Word Cloud ###

df = pd.read_csv(r"/tweet_data.csv", names=["id", "text", "date", "name",
                                                                 "username", "followers", "loc"])

def clean(txt):
    txt = str(txt).split()
    for item in txt:
        if "http" in item:
            txt.remove(item)
    txt = (" ".join(txt))
    return txt

text = (df.text.apply(clean))

wc = cloud(background_color='white', colormap="tab10").generate(" ".join(text))

plt.axis("off")
plt.text(2, 210, "Generated using word_cloud and this post's dataset.", size = 5, color="grey")

plt.imshow(wc)
plt.show()### Stream & Collect Tweets ###class Streamer(StreamListener):
    def __init__(self):
        super().__init__()
        self.limit = 1000 # Number of tweets to collect.
        self.statuses = []  # Pass each status here.

    def on_status(self, status):
        if status.retweeted or "RT @" 
        in status.text or status.lang != "en":
            return   # Remove re-tweets and non-English tweets.
        if len(self.statuses) < self.limit:
            self.statuses.append(status)
            print(len(self.statuses))  # Get count of statuses
        if len(self.statuses) == self.limit:
            with open("/tweet_data.csv", "w") as    file: 
                writer = csv.writer(file)  # Saving data to csv. 
                for status in self.statuses:
                    writer.writerow([status.id, status.text,
              status.created_at, status.user.name,         
              status.user.screen_name, status.user.followers_count, status.user.location]) 
            print(self.statuses)
            print(f"\n*** Limit of {self.limit} met ***")
            return False
        if len(self.statuses) > self.limit:
            return False

streaming = tweepy.Stream(auth=setup.api.auth, listener=Streamer())

items = ["@berniesanders", "@kamalaharris", "@joebiden", "@ewarren"]  # Keywords to track

stream_data = streaming.filter(track=items)### Data ###

df = pd.read_csv(r"/tweet_data.csv", names= ["id", "text", "date", "name",
                                                                 "username", "followers", "loc"])

### Data Cleaning ###

ppl = ["berniesanders", "kamalaharris", "joebiden", "ewarren"]

def clean(txt):
    txt = str(txt.translate(str.maketrans("", "", string.punctuation))).lower()
    txt = str(txt).split()
    for item in txt:
        if "http" in item:
            txt.remove(item)
        for item in ppl:
            if item in txt:
                txt.remove(item)
    txt = (" ".join(txt))
    return txt

df.text = df.text.apply(clean)

### Data Prep ###

# print(stopwords)

# If you want to add to the stopwords list: stopwords = stopwords.union(set(["add_term_1", "add_term_2"]))

### Lemmatize and Stem ###

stemmer = stemm(language="english")

def lemm_stemm(txt):
    return stemmer.stem(lemm().lemmatize(txt, pos="v"))

def preprocess(txt):
    r = [lemm_stemm(token) for token in simple_preprocess(txt) if       token not in stopwords and len(token) > 2]
    return r

proc_docs = df.text.apply(preprocess)

### LDA Model ###

dictionary = gensim.corpora.Dictionary(proc_docs)
dictionary.filter_extremes(no_below=5, no_above= .90)
# print(dictionary)

n = 5 # Number of clusters we want to fit our data to
bow = [dictionary.doc2bow(doc) for doc in proc_docs]
lda = gensim.models.LdaMulticore(bow, num_topics= n, id2word=dictionary, passes=2, workers=2)
# print(bow)

for id, topic in lda.print_topics(-1):
    print(f"TOPIC: {id} \n WORDS: {topic}")

### Coherence Scoring ###

coh = CoherenceModel(model=lda, texts= proc_docs, dictionary = dictionary, coherence = "c_v")
coh_lda = coh.get_coherence()
print("Coherence Score:", coh_lda)

lda_display = pyldavis.prepare(lda, bow, dictionary)
pyLDAvis.show(lda_display)*****
```

*****LDA æ˜¯æ¢ç´¢æ–‡æœ¬æ•°æ®çš„ä¸€ä¸ªå¾ˆå¥½çš„æ¨¡å‹ï¼Œå°½ç®¡å®ƒéœ€è¦å¤§é‡çš„ä¼˜åŒ–(å–å†³äºç”¨ä¾‹)æ¥ç”¨äºç”Ÿäº§ã€‚åœ¨ç¼–å†™ã€è¯„ä¼°å’Œæ˜¾ç¤ºæ¨¡å‹æ—¶ï¼Œgensimã€nltk å’Œ pyLDAvis åŒ…æ˜¯æ— ä»·çš„ã€‚*****

*****éå¸¸æ„Ÿè°¢ä½ è®©æˆ‘åˆ†äº«ï¼Œä»¥åè¿˜ä¼šæœ‰æ›´å¤šã€‚ğŸ˜ƒ*****