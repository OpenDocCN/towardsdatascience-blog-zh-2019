<html>
<head>
<title>Neural Networks Intuitions: 3. Focal Loss for Dense Object Detection — Paper Explanation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络直觉:3。密集物体探测的焦点损失—论文解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-intuitions-3-focal-loss-for-dense-object-detection-paper-explanation-61bc0205114e?source=collection_archive---------2-----------------------#2019-03-29">https://towardsdatascience.com/neural-networks-intuitions-3-focal-loss-for-dense-object-detection-paper-explanation-61bc0205114e?source=collection_archive---------2-----------------------#2019-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="f3af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嘿大家好！</p><p id="a4fc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">今天，在神经网络直觉系列中，我将讨论<strong class="jp ir"> RetinaNet:密集物体检测的焦点损失</strong>论文。本文讨论了 RetinaNet，它是一种单次目标检测器，与其他两级检测器相比速度更快，并且还解决了所有单次目标检测器的一个共同问题——单次目标检测器不如两级目标检测器精确。</p><p id="23bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">论文链接:<a class="ae kl" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">密集物体探测的焦损失</a></p><p id="fc1c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么 RetinaNet 如何解决所有单次检测器普遍存在的不准确问题呢？像往常一样，让我们深入探讨一下单次检测器的核心问题，看看 RetinaNet 与其他的有何不同:-)。</p><blockquote class="km"><p id="3674" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">问题:单次目标检测器不如两级目标检测器精确。</p></blockquote><p id="3755" class="pw-post-body-paragraph jn jo iq jp b jq kw js jt ju kx jw jx jy ky ka kb kc kz ke kf kg la ki kj kk ij bi translated">我们来看看为什么上面的说法是真的。</p><p id="ded3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在传统的两级对象检测器中，<strong class="jp ir">第一级</strong>接收输入图像并输出<strong class="jp ir">区域建议</strong>，即对象可能存在的位置，并且<strong class="jp ir">第二级</strong>涉及<strong class="jp ir">将每个建议分类到哪个类别</strong>。这意味着两级检测器中的分类头通常接收包含更多肯定类别(即相关对象)的建议，并且所提供的否定(背景)样本的数量相对较少(或者在理想情况下甚至可能为零)。因此，分类头不会偏向负面(或背景)类别。这是至关重要的，因为神经网络有这种倾向，偏向于更经常显示给它们的类。</p><blockquote class="lb lc ld"><p id="21ac" class="jn jo le jp b jq jr js jt ju jv jw jx lf jz ka kb lg kd ke kf lh kh ki kj kk ij bi translated"><strong class="jp ir">第一阶段</strong>接收输入图像并输出<strong class="jp ir">区域建议</strong>即物体可能存在的位置，并且<strong class="jp ir">第二阶段</strong>涉及<strong class="jp ir">将每个建议分类到哪个类别。</strong></p></blockquote><p id="550d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们看看在单级检测器中发生了什么——单次检测器通常接收输入图像，并输出图像中存在的对象(类)及其相应的边界框坐标。</p><blockquote class="lb lc ld"><p id="2305" class="jn jo le jp b jq jr js jt ju jv jw jx lf jz ka kb lg kd ke kf lh kh ki kj kk ij bi translated">他们是如何在单镜头中输出班级分数和盒子坐标的？</p></blockquote><p id="3699" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单发探测器使用来自更快的 R-CNN 的<strong class="jp ir">锚盒(或默认盒)</strong>的概念。锚点是分配给特征图中每个单元的逻辑框，<strong class="jp ir">相对于其进行边界框回归和分类</strong>。它们可以有不同的纵横比(取决于我们手头的问题)。我不打算进入锚的细节。这里的关键是<strong class="jp ir">不是用另一个神经网络(或任何算法)来生成区域建议，而是将特征图的整个网格视为区域建议</strong>，该区域建议又由同一个神经网络分类以产生类分数和边界框偏移。</p><blockquote class="km"><p id="a881" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">单发探测器与 fast-RCNN 中的区域建议网络(RPN)相同，除了 RPN 执行前景与背景分类，而单发探测器执行多类分类，即包括所有对象类和背景。</p></blockquote><blockquote class="lb lc ld"><p id="4a04" class="jn jo le jp b jq kw js jt ju kx jw jx lf ky ka kb lg kz ke kf lh la ki kj kk ij bi translated">没错。我们现在知道了单触发探测器的工作原理，但是这个过程是如何影响它们的精度的呢？或者更准确地说，这如何使它们不如两级检测器精确？</p></blockquote><p id="fc29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单发检测器中分类头将特征图网格中存在的每个锚点视为其输入的区域提议集。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/a8a70bd5c59e4c8de34f5e44cc84d8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z24VtkItitWykVT9cDI9xg.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Default boxes(or anchors) as mentioned in <a class="ae kl" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank">SSD: Single Shot MultiBox Detector</a></figcaption></figure><blockquote class="km"><p id="f05f" class="kn ko iq bd kp kq ly lz ma mb mc kk dk translated">与包含负面或背景类别的锚的数量相比，包含正面类别(相关对象)的锚的数量非常少(例如 1:1000)。因此，与正样本相比，分类器获得更多的负样本(或者更具体地说，更容易的训练样本)，从而导致更有偏向的学习。</p></blockquote><blockquote class="lb lc ld"><p id="75dd" class="jn jo le jp b jq kw js jt ju kx jw jx lf ky ka kb lg kz ke kf lh la ki kj kk ij bi translated">如前所述，两级检测器通过级联分类器减少了背景样本的数量。</p></blockquote><p id="bafd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这就是为什么单级检测器的性能不如两级检测器的原因。请注意，这些准确性基准测试是使用 COCO 等标准对象检测数据集进行的。对于用户可能正在处理的其他自定义数据集，情况不一定如此。</p><blockquote class="km"><p id="7dc3" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">在<a class="ae kl" href="https://arxiv.org/abs/1512.02325" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> SSD:单次多盒检测器</strong> </a>中，通过使用每个默认盒的<strong class="ak">最高置信损失对负样本进行排序，并挑选顶部的样本</strong>，从而使负样本和正样本之间的比率最多为 3:1，从而解决了这种类别不平衡问题。</p></blockquote><p id="ccc7" class="pw-post-body-paragraph jn jo iq jp b jq kw js jt ju kx jw jx jy ky ka kb kc kz ke kf kg la ki kj kk ij bi translated">现在让我们看看 RetinaNet 如何通过仅仅调整一个对象分类器的损失函数来以一种优雅的方式解决这个类不平衡的问题。</p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="2be4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">解决方案:</strong>本文的作者引入了一个称为<strong class="jp ir">焦点损失</strong>的损失函数，它对容易分类的例子(即我们案例中的背景)进行惩罚。</p><p id="9430" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑用于分类的典型交叉熵损失</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/36cbf8768f4923c8cd7e0240a376e2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*bwAOckwT5er7zzONYmbF8g.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Cross entropy Loss</figcaption></figure><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/4fda26566decf0b0b564779e4b91367b.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*IyexRrfRufTi0pGbp416IA.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">y ∈ {±1} specifies the ground-truth class and p ∈ [0, 1] is the model’s estimated probability for the class with label y = 1</figcaption></figure><p id="1c29" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里要解决的问题是分类损失被来自“<strong class="jp ir">简单背景</strong>”示例的贡献淹没——如果一个示例不能很好地解释特定类的模式(或者几乎不能帮助区分该类和其他类),则该示例可以被称为简单</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/4890513cf05dcd2a8f17149ed3900c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*ODJ31kwtkl9qoITUcqaSKA.png"/></div></figure><p id="2d0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种简单的例子被很好地分类，即概率&gt; &gt; 0.5，因此它们对损失的贡献非常小。但是如果这样的例子远远多于正面例子的数量呢？—阴性与阳性的比例为 10000:1。那么它对损失的贡献是巨大的，从而造成网络偏向背景类。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/24514cab2eec84e74fa7887f27cb3b01.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*Td8ovSPGqtIEokeoVvYQQg.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Focal loss</figcaption></figure><p id="da9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了避免这些简单的例子对损失的贡献，1-它们的概率乘以它们的原始损失值，最终减少它们的损失。</p><blockquote class="km"><p id="6ed2" class="kn ko iq bd kp kq kr ks kt ku kv kk dk translated">因此，如果一个例子很容易分类，那么它的概率 p 将是&gt; &gt; 0.5(接近 0.9–1.0)，而 1-p(接近 0)导致 C.E .产生一个非常小的值，最终导致该例子的学习非常低或没有学习。γ项是聚焦参数，它调整简单示例的向下加权速率。</p></blockquote><figure class="mp mq mr ms mt ln gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/e784e0854569c62a6c8dcd68f9bef7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*Qrw4Xcem0hnlVg2Csk2FnQ.png"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">A Balanced C.E variant of Focal Loss is usually used</figcaption></figure><p id="0523" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用平衡交叉熵(看看我在本系列中的第一篇文章<a class="ae kl" rel="noopener" target="_blank" href="/neural-networks-intuitions-1-balanced-cross-entropy-331995cd5033">和<strong class="jp ir">调制因子(1pt)^γ</strong>)的组合，因为它产生了更好的精度，如本文所述。</a></p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/765380bbb6259aa37da0ef5491200825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*qd7u4PjpaKgO-pUX8vi7XQ.png"/></div></figure><p id="d7ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看一下上图，它更直观地给出了如何通过使用调制因子来减少<strong class="jp ir">良好/简单分类示例的损失。</strong></p><p id="7a6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里要注意的另一件重要事情是，它们显示了与 SSD 中使用的硬负挖掘相比，单独使用<strong class="jp ir">焦损失如何产生更好的结果。</strong></p></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="c209" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> RetinaNet 架构:</strong> RetinaNet 使用带有 Resnet 主干的功能金字塔网络(FPN)。FPN 包括在进行预测之前添加顶层要素地图及其下的要素地图。将顶层特征图与下面的特征图相加通常涉及放大顶层图，使用 1x1 conv 对下面的图进行维度匹配，并对两者进行元素相加。查看<a class="ae kl" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank">用于对象检测的特征金字塔网络</a>论文，了解关于 FPN 的更多详细信息。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mv"><img src="../Images/ef39caa6fa8dee11d8e3e03f20882e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IIuPgetzAtJM0OAW35QiVA.png"/></div></div></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="fa23" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">训练视网膜网:</strong>另一个更重要的方面是在开始训练之前初始化前景类的模型概率。<strong class="jp ir">所有正面锚被分配 0.01 </strong>的先验概率，以便它们对损失有更大的贡献，并确保大量负面例子不会妨碍初始阶段的训练。</p><p id="2485" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我对密集物体探测的纸焦损失的解释。如果我的理解有漏洞，欢迎提问或指正。</p><p id="161c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">干杯:-)</p></div></div>    
</body>
</html>