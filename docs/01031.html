<html>
<head>
<title>Review: MultiChannel — Segment Colon Histology Images (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:多通道—分割结肠组织学图像(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc?source=collection_archive---------15-----------------------#2019-02-17">https://towardsdatascience.com/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc?source=collection_archive---------15-----------------------#2019-02-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2b55" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用 FCN 的前景分割+使用 HED 的边缘检测+使用更快的 R-CNN 的对象检测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f22d44a195a456156659109a4d9e10ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcOZoe2bvVadukqYw9SPHw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Gland Haematoxylin and Eosin (H&amp;E) stained slides and ground truth labels</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kw"><img src="../Images/835b09acb2af6a787f70b5ba5c1361e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FpvUcaPHbomIrJbbSeMG9g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Foreground Segmentation using </strong><a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"><strong class="bd kv">FCN</strong></a><strong class="bd kv"> + Edge Detection Using HED + Object Detection Using </strong><a class="ae kx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"><strong class="bd kv">Faster R-CNN</strong></a></figcaption></figure><p id="2687" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di">在</span>这个故事里，<strong class="la ir">多路</strong>简要回顾。它是一个<strong class="la ir">深度多通道神经网络</strong>，用于<strong class="la ir">腺体实例分割</strong>。这种方法，如上图所示，<strong class="la ir">融合来自 3 个子网络的结果:前景分割使用</strong> <a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> <strong class="la ir"> FCN </strong> </a> <strong class="la ir">，边缘检测使用 HED，对象检测使用</strong> <a class="ae kx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="la ir">更快的 R-CNN </strong> </a> <strong class="la ir">。</strong>使用 2015 MICCAI 腺体分割挑战数据集获得了最先进的结果。<strong class="la ir">作者在 2016 年首次发表多通道 MICCAI </strong>，仅使用 2 个子网络:使用<a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>的前景分割和使用 hed 的边缘检测。然后他们增强了会议版本，使用更快的 R-CNN 添加了对象检测。<strong class="la ir">本增强版于 2017 年 TBME </strong>出版。由于事务版本要详细得多，虽然我已经阅读了这两个版本，但我将在这里呈现事务版本。(<a class="md me ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d7e57902fbfc--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="6d93" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">概述</h1><ol class=""><li id="4237" class="ne nf iq la b lb ng le nh lh ni ll nj lp nk lt nl nm nn no bi translated"><strong class="la ir">第一子网:前景分割通道</strong></li><li id="fe0c" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated"><strong class="la ir">第二子网:边缘检测通道</strong></li><li id="c82d" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated"><strong class="la ir">第三子网:物体检测通道</strong></li><li id="8424" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated"><strong class="la ir">定影多通道</strong></li><li id="4629" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated"><strong class="la ir">与最先进方法的比较</strong></li><li id="7b3b" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt nl nm nn no bi translated"><strong class="la ir">进一步消融研究</strong></li></ol></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="eea3" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 1。第一子网:前景分割通道</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f2bbe165e7703346f9a6cfa4acaae9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*u41FzDTo0CMtyC7_ZebbnQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">1st Sub-Network: Foreground Segmentation Channel</strong></figcaption></figure><ul class=""><li id="b8e0" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> <strong class="la ir"> FCN-32s </strong> </a> <strong class="la ir">用作网络中前景分割通道。</strong></li><li id="8032" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">然而，由于<a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-32s </a>产生的输出特征图较小，不利于分割。在<a class="ae kx" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">的扩展网</a>中提出的扩展卷积用于增强<a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">的 FCN </a>。</li><li id="e689" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">pool4 和 pool5 的步距是 1。</li><li id="3d90" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">并且随后的回旋层通过扩大的回旋来扩大感受野。</li><li id="9153" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">训练时使用 Softmax 交叉熵损失。</li><li id="32e7" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">使用预先训练好的<a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN-32s </a>。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="934c" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated">2.第二子网:<strong class="ak">边缘检测通道</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1311cb0b08c220f2931db6ecbc3e9ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*IxxfASJr6REQDZnJUpc99w.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">2nd Sub-Network: Edge Detection Channel</strong></figcaption></figure><ul class=""><li id="2d3f" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated"><strong class="la ir">边缘通道基于整体嵌套的边缘检测器(HED)。</strong></li><li id="8f90" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">它学习分层嵌入的多尺度边缘场，以说明轮廓和对象边界的低级、中级和高级信息。</li><li id="6210" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">对于第<em class="nz"> m </em>次预测:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e3d22596c52856f37ee37812bfb2fdeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*ElWLFfMtPlEjEHnEIc8nNQ.png"/></div></figure><ul class=""><li id="9b97" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">输出为特征图<em class="nz"> h </em> () 的<strong class="la ir"> sigmoid 函数</strong> <strong class="la ir"> σ </strong> <strong class="la ir">。</strong></li><li id="5153" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">最后是不同尺度边缘场的加权融合。</li><li id="6a00" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">在训练期间使用 Sigmoid 交叉熵损失。</li><li id="04e3" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">使用 Xavier 初始化。</li><li id="7d3b" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">地面实况边缘标签由区域标签生成。如果所有相邻(上、下、左、右)像素都是前景或背景，则该像素不是边缘。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="6af9" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir"> 3。第三子网:物体检测通道</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bf83d996f4ea9c48aacedf8250445e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*ruiZDFN3ZAUZBe5462hNww.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">3rd Sub-Network: Object Detection Channel</strong></figcaption></figure><ul class=""><li id="61aa" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated"><a class="ae kx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202"> <strong class="la ir">更快 R-CNN </strong> </a> <strong class="la ir">这里用的是，但是有修饰。</strong></li><li id="e7df" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir">填充操作</strong>在生成区域建议后完成。</li><li id="5b7c" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">边界框覆盖的区域中的每个像素的值等于它所属的边界框的数量。</li><li id="c96d" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">例如，如果一个像素位于三个边界框的重叠区域，则该像素的值将为 3。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/48b39eddf638e0bf6aada6c4122f4f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*KqHx_TFmR2PhbPVUbs00kw.png"/></div></figure><ul class=""><li id="9658" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">𝜙是灌装操作。</li><li id="0b2b" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">损耗与<a class="ae kx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">fast R-CNN</a>中的一样，即分类损耗和回归损耗之和。</li><li id="d8b8" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">使用预训练的<a class="ae kx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>。</li><li id="51d0" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">使用包围每个腺体的最小矩形来生成地面真实边界框。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="7d91" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 4。定影多通道</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/9d3e6699a090b77c8fa2ec58d7ad4390.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*DK3n7RavWg-gnCuyB_6kwg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Fusing Multichannel</strong></figcaption></figure><ul class=""><li id="f6a8" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">使用 7 层 CNN。</li><li id="e4bf" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">同样，在<a class="ae kx" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5"> DilatedNet </a>中使用的扩张卷积在这里被用来代替下采样。</li><li id="6e5b" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">使用 Xavier 初始化。</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="3c2d" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 5。与最先进方法的比较</strong></h1><h2 id="4ea7" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">5.1.资料组</h2><ul class=""><li id="7dfd" class="ne nf iq la b lb ng le nh lh ni ll nj lp nk lt ny nm nn no bi translated">MICCAI 2015 腺体分割挑战大赛</li><li id="f188" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">165 标记的结肠直肠癌组织学图像</li><li id="f91d" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">原图，大部分是 775×522。</li><li id="968b" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">训练集:85 幅图像</li><li id="e09e" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">测试集:80 幅图像。(测试集 A 包含 60 幅图像，测试集 B 包含 20 幅图像)。训练集中有 37 个良性部分和 48 个恶性部分，测试集 A 中有 33 个良性部分和 27 个恶性部分，测试集 b 中有 4 个良性部分和 16 个恶性部分</li></ul><h2 id="9a21" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">5.2.数据扩充</h2><ul class=""><li id="d567" class="ne nf iq la b lb ng le nh lh ni ll nj lp nk lt ny nm nn no bi translated">数据增强策略一:水平翻转和 0、90、180、270°旋转。</li><li id="aeb7" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">数据扩充策略二:弹性转换就像<a class="ae kx" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760"> U-Net </a>里的那个。</li></ul><h2 id="d431" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">5.3.估价</h2><ul class=""><li id="f013" class="ne nf iq la b lb ng le nh lh ni ll nj lp nk lt ny nm nn no bi translated">使用了三个指标:<strong class="la ir"> F1 得分</strong>、<strong class="la ir"> ObjectDice </strong>和<strong class="la ir"> ObjectHausdorff </strong>。</li><li id="c10d" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir"> F1 得分</strong>:由 precision P 和 recall R 测得的得分，超过 50%的重叠定义为真阳性。</li><li id="71a5" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir"> ObjectDice </strong>:分割的度量标准。</li><li id="7808" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir"> ObjectHausdorff </strong>:测量形状相似度的度量。</li><li id="4e2f" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">(详情请看我对<a class="ae kx" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener"> CUMedVision2 / DCAN </a>的点评。)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/1da5d1e27ab832ab3ad8c16867e2b867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sc8upfDsEfCFDrmVO57VeQ.png"/></div></div></figure><ul class=""><li id="a4d0" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">RS 和 WRS 分别是基于 F1 评分的秩和与加权秩和，ObjectDice 和 ObjectHausdorff。</li><li id="495a" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">我们可以看到，在 A 部分和 B 部分测试集中，多通道几乎获得了所有的 rank 1，这意味着多通道优于，<a class="ae kx" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener"> CUMedVision1 </a>，<a class="ae kx" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener"> CUMedVision2 / DCAN </a>，<a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>和<a class="ae kx" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">expanded FCN(DeepLab)</a>。</li><li id="3d1d" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">一些定性结果:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/362fb31a8eb78b9a4dedca9addc122f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eFg-ujF9NogyTdXHt_ygA.png"/></div></div></figure><h2 id="b06e" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">5.3.与实例分割方法的比较</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/3a3e5e5be014282a52f3b503210c3abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cPr50cloOkJspqUZ5Yg0Q.png"/></div></div></figure><ul class=""><li id="b5b6" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">多通道比所有的实例分割方法都要好，例如<a class="ae kx" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a>。</li><li id="5603" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">当仅在边界框内(即倒数第二行)分割时，结果也不如融合方法。</li><li id="b226" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">边缘 3 表示边缘被半径为 3 的圆盘过滤器扩大。</li><li id="cd69" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated">一些定性结果:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/97b3ff0fe0f9eb37aa9146d9b674130b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-qBrBEM0sUzRam7A2aDg7g.png"/></div></div></figure></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="aa76" class="mm mn iq bd mo mp mq mr ms mt mu mv mw jw mx jx my jz mz ka na kc nb kd nc nd bi translated"><strong class="ak"> 6。进一步消融研究</strong></h1><h2 id="9fd5" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">6.1.数据扩充</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/3716cfef9cf89641a622f030bb6d0981.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*EzCntuyjRZkUWrjPjPGe-g.png"/></div></figure><ul class=""><li id="8df1" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">使用数据增强策略 II(弹性变换)更好。</li></ul><h2 id="e103" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">6.2.多通道的不同融合变体</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/42d95d51643fd18934d2974f0f7cda39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*wA33ywA38ldXK5OFlmRb5Q.png"/></div></figure><ul class=""><li id="b518" class="ne nf iq la b lb lc le lf lh nv ll nw lp nx lt ny nm nn no bi translated">边缘 3 表示边缘被半径为 3 的圆盘过滤器扩大。这意味着增加边缘的宽度，以处理训练过程中边缘和非边缘像素的不平衡。</li><li id="e210" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir">前 3 行</strong>:不使用扩张卷积，性能较差。</li><li id="f78a" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir">最后 2 排</strong>:只有 2 个通道(或子网)进行融合，性能也较逊色。</li><li id="c362" class="ne nf iq la b lb np le nq lh nr ll ns lp nt lt ny nm nn no bi translated"><strong class="la ir">中间 3 排:带扩张卷积，加 3 个通道，性能最好。</strong></li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h2 id="d84e" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">参考</h2><p id="41ab" class="pw-post-body-paragraph ky kz iq la b lb ng jr ld le nh ju lg lh ow lj lk ll ox ln lo lp oy lr ls lt ij bi translated">【2016 MICCAI】【多通道】<br/> <a class="ae kx" href="https://arxiv.org/abs/1607.03222" rel="noopener ugc nofollow" target="_blank">汽封实例分割由深多通道侧监督</a></p><p id="3a10" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">【2017 TBE】【多通道】<br/> <a class="ae kx" href="https://arxiv.org/abs/1611.06661" rel="noopener ugc nofollow" target="_blank">利用深度多通道神经网络进行腺体实例分割</a></p><h2 id="9ba0" class="oe mn iq bd mo of og dn ms oh oi dp mw lh oj ok my ll ol om na lp on oo nc op bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph ky kz iq la b lb ng jr ld le nh ju lg lh ow lj lk ll ox ln lo lp oy lr ls lt ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(是)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(情)(况)(。</p><p id="8b77" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir">物体检测<br/></strong><a class="ae kx" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae kx" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae kx" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae kx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae kx" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae kx" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae kx" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae kx" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae kx" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a> yolo 9000[<a class="ae kx" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolov 3</a>][<a class="ae kx" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae kx" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>][<a class="ae kx" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir">语义切分<br/></strong><a class="ae kx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae kx" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae kx" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae kx" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】【parse net<a class="ae kx" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae kx" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae kx" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a><a class="ae kx" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a></p><p id="fc65" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir">生物医学图像分割<br/></strong>[<a class="ae kx" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae kx" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae kx" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U 网</a>][<a class="ae kx" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae kx" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U 网+ResNet </a> ]</p><p id="3134" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="la ir"> 实例分段 <br/> </strong> <a class="ae kx" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae kx" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae kx" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae kx" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae kx" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae kx" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a>】</p><p id="58de" class="pw-post-body-paragraph ky kz iq la b lb lc jr ld le lf ju lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p></div></div>    
</body>
</html>