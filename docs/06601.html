<html>
<head>
<title>Tags recommendation algorithm using Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于潜在狄利克雷分配的标签推荐算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tags-recommendation-algorithm-using-latent-dirichlet-allocation-lda-3f844abf99d7?source=collection_archive---------7-----------------------#2019-09-21">https://towardsdatascience.com/tags-recommendation-algorithm-using-latent-dirichlet-allocation-lda-3f844abf99d7?source=collection_archive---------7-----------------------#2019-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="549e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们将用 python 开发一个 LDA 模型，根据用户的 StackOverflow 帖子向用户推荐标签</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c4c174beadf8727a4ceee733ef9822b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zEi3upbdEgU2CmRVe8WBiA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Word Cloud of tags in Stack Overflow</figcaption></figure><p id="b1a6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一般来说，LDA 用于对原始数据集进行降维，然后再应用其他机器学习算法，这些算法将受益于更少的维数。这里我们将把它用于另一个目的，即实现一个推荐系统。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><h1 id="c3ec" class="ly lz iq bd ma mb mc md me mf mg mh mi jw mj jx mk jz ml ka mm kc mn kd mo mp bi translated">资料组</h1><p id="06ec" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">我们的数据集来自<a class="ae mv" href="https://data.stackexchange.com/stackoverflow/query/new" rel="noopener ugc nofollow" target="_blank"> stackexchange explorer，</a>要从网站导出帖子，您必须进行如下 SQL 查询:<code class="fe mw mx my mz b">SELECT * FROM posts WHERE Id &lt; 50000</code>默认情况下，每个 SQL 查询的执行时间都有时间限制，这使得很难一次恢复所有数据。要检索更多的结果，请记住使用对 id 的约束进行查询。</p><p id="3186" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们有了数据集，下一步是做一些文本预处理，也就是说:</p><ul class=""><li id="1ac1" class="na nb iq kx b ky kz lb lc le nc li nd lm ne lq nf ng nh ni bi translated">使用<a class="ae mv" href="https://www.crummy.com/software/BeautifulSoup/bs3/documentation.html" rel="noopener ugc nofollow" target="_blank">美汤</a>库移除 html 格式</li><li id="31a3" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">降低文本</li><li id="31f7" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">转换缩写</li><li id="d5ef" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">删除停用词</li><li id="3485" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">单词的词形变化——将单词的词形变化组合在一起</li><li id="2ea2" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">去掉动词和形容词，因为它们不能提供关于文章的有价值的信息</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Pre-processing functions</figcaption></figure><h1 id="ff36" class="ly lz iq bd ma mb nq md me mf nr mh mi jw ns jx mk jz nt ka mm kc nu kd mo mp bi translated">TF-IDF</h1><p id="3cbf" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">TF-IDF 是 Text Frequency-Inverse Document Frequency 的缩写，它旨在衡量一个术语在文档语料库中对一个文档的重要性。<em class="nv">术语权重=术语频率×idf(术语)</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0e04b9f4ef7f2e8d2287fd2568cd417e.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*vYyWsBNJBWgLhlVKYwopIA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">definition of inverse document frequency</figcaption></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Converting training and test post to matrices of TF-IDF features</figcaption></figure><h1 id="f624" class="ly lz iq bd ma mb nq md me mf nr mh mi jw ns jx mk jz nt ka mm kc nu kd mo mp bi translated">皱胃向左移</h1><p id="2f2b" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated"><strong class="kx ir"> LDA </strong>(潜在狄利克雷分配)是一种基于以下假设的生成式非监督方法:</p><ul class=""><li id="f9a2" class="na nb iq kx b ky kz lb lc le nc li nd lm ne lq nf ng nh ni bi translated">语料库中的每个文档都是一组没有顺序的单词(单词包)</li><li id="cddd" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">每个文档以不同的比例涵盖了多个主题<em class="nv"> p(θm) </em></li><li id="942e" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">每个单词都有一个与每个主题相关的分布<em class="nv"> p(ϕk) </em>。因此，每个主题可以由每个单词的概率来表示</li><li id="d435" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated"><em class="nv"> zn </em>代表单词<em class="nv"> wn </em>的主题</li></ul><p id="19f3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为访问受限于文档，所以有必要确定什么是主题、每个词在主题上的分布、每个主题在语料库上出现的频率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/10c9a13f74324ac5c7b6c500f007e38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*VTHd8nB_PBsDtd2hd87ybg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">LDA representation in the form of a graphical probabilistic model</figcaption></figure><p id="2a9f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们使用<strong class="kx ir">sk learn . decomposition . latentdirichletallocation、</strong>训练不同的 lda 模型，每个模型都有不同数量的主题，然后我们使用度量困惑度在测试集上评估不同的模型。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/65c6047f251ba0a6a50fd7106bdfcb9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*7ZZp9dxx6hR6_kV1fyH4ng.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">definition of perplexity</figcaption></figure><p id="39f8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">L( <strong class="kx ir"> <em class="nv"> w </em> </strong>)是看不见的文档<strong class="kx ir"> <em class="nv"> w </em> </strong>的对数似然；困惑度越低，模型越好。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">lda algorithm</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/10649eac3dfd6c03ea91961c0d5aef84.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*cXQ207ae3dNMQM2LymWgww.png"/></div></figure><p id="f1dd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最低的困惑分数是针对 10 个主题的，所以我们将学习包含 10 个主题的 lda 模型。</p><h1 id="ff9f" class="ly lz iq bd ma mb nq md me mf nr mh mi jw ns jx mk jz nt ka mm kc nu kd mo mp bi translated">推荐算法</h1><p id="e1d7" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">现在我们有了模型，我们将研究推荐算法。它将基于两个要点，即:</p><ul class=""><li id="98bf" class="na nb iq kx b ky kz lb lc le nc li nd lm ne lq nf ng nh ni bi translated">得分=文档被分配给主题的概率×主题生成单词的概率</li><li id="dabd" class="na nb iq kx b ky nj lb nk le nl li nm lm nn lq nf ng nh ni bi translated">当一个单词的分数高于定义的阈值时，该单词被认为是相关标签</li></ul><p id="600e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在尝试了阈值的不同值之后，我们选择了阈值<strong class="kx ir"> 0.010 </strong>，这是因为使用该值，测试集标签具有看起来像训练集标签的分布，并且还因为使用阈值等于 0.010，超过 90% 的帖子具有推荐标签。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Tags recommender using LDA</figcaption></figure><h1 id="557e" class="ly lz iq bd ma mb nq md me mf nr mh mi jw ns jx mk jz nt ka mm kc nu kd mo mp bi translated">估价</h1><p id="0727" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">我们将使用 Jaccard 评分来评估我们的模型，该评分用于将样本的预测标签集与相应的标签集进行比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/5575844ea3499e285b8dafc75ca86a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*FZV6QCR9p4en8cP0-yJpVw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Jaccard score definition</figcaption></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Jaccard score function</figcaption></figure><p id="c5b2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的 LDA 模型在测试集上的 Jaccard 得分是<strong class="kx ir"> 3.98% </strong>，而使用 OneVsRest 方法(简单基线)的哑分类器使得<strong class="kx ir"> 0.90% </strong>。这表明我们的 LDA 模型肯定比随机模型好，因为它已经成功地正确预测了一些标签，尽管事实上它是一个无监督的模型。但是为了有一个我们推荐系统正常运行的真正指标，我们必须对一些帖子，模型输出的标签与我们手动认为相关的所有标签进行比较。</p><h1 id="56c9" class="ly lz iq bd ma mb nq md me mf nr mh mi jw ns jx mk jz nt ka mm kc nu kd mo mp bi translated">例子</h1><p id="7412" class="pw-post-body-paragraph kv kw iq kx b ky mq jr la lb mr ju ld le ms lg lh li mt lk ll lm mu lo lp lq ij bi translated">下面的两个例子表明，LDA 推荐器的性能取决于帖子的精确度和用户问题的详细程度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/55f46aacfe3298c8d9961dbbddc04199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_aHo-VG82Ji6vYv3OJZK2g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Recommended tags for example 1</figcaption></figure><p id="c9e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第一个例子是一个详细帖子的完美例子，所以对于基于无监督算法的推荐器来说，返回相关标签很简单，这里就是这种情况。帖子是关于 html/css 的问题，程序显示这 2 个标签。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/13ded7e68a293fe0e3efc532ceaa0d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wt7a41yi83dzOZrvSAdv6Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Recommended tags for example 2</figcaption></figure><p id="87d3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个例子(2)中，帖子很短，没有具体的细节来帮助程序推荐感兴趣的标签。这里唯一推荐的标签是<em class="nv">‘table’</em>，不能真正认为是相关标签。</p><p id="7102" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想有更多相关的标签，你必须给算法更多关于文章的信息，比如在你的程序中考虑文章的标题。</p></div><div class="ab cl lr ls hu lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="ij ik il im in"><p id="a13a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">源代码可以在<a class="ae mv" href="https://github.com/koukou10/Tags_recommender" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。请随意分享你的想法和想法。</p></div></div>    
</body>
</html>