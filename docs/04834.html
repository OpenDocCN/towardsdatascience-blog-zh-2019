<html>
<head>
<title>Production Data Processing with PySpark on AWS EMR</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PySpark 在自动气象站电子病历上处理生产数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/production-data-processing-with-apache-spark-96a58dfd3fe7?source=collection_archive---------9-----------------------#2019-07-22">https://towardsdatascience.com/production-data-processing-with-apache-spark-96a58dfd3fe7?source=collection_archive---------9-----------------------#2019-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="852f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 AWS CLI 在集群上提交 PySpark 应用程序，分步指南</h2></div><p id="bff9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">带有 PySpark 和 AWS EMR 的数据管道</strong>是一个多部分系列。这是第二部分。如果你需要 AWS EMR 的入门知识，请查看<a class="ae le" rel="noopener" target="_blank" href="/getting-started-with-pyspark-on-amazon-emr-c85154b6b921">第 1 部分</a>。</p><ol class=""><li id="6481" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated"><a class="ae le" rel="noopener" target="_blank" href="/getting-started-with-pyspark-on-amazon-emr-c85154b6b921">在 AWS EMR 上开始使用 PySpark</a></li><li id="f8e5" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">用 PySpark 在 AWS EMR 上处理生产数据<strong class="kk iu">(本文)</strong></li></ol><figure class="lu lv lw lx gt ly gh gi paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="gh gi lt"><img src="../Images/d7fab68eaeabe1a12ffb7d56a5601e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kave-NZvGxW7nuyVlsmzCQ.jpeg"/></div></div></figure><h1 id="788c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">动机</h1><p id="fde3" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">Apache Spark 在大规模数据处理和分析领域风靡一时，这是有充分理由的。借助 Spark，组织能够从不断增长的数据堆中提取大量价值。正因为如此，能够构建 Spark 应用的数据科学家和工程师受到企业的高度重视。本文将向您展示如何从命令行在 Amazon EMR 集群上运行 Spark 应用程序。</p><p id="d0fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大多数 PySpark 教程都使用 Jupyter 笔记本来演示 Spark 的数据处理和机器学习功能。原因很简单。在集群上工作时，笔记本通过在 UI 中提供快速反馈和显示错误消息，使得测试语法和调试 Spark 应用程序变得更加容易。否则，您将不得不挖掘日志文件来找出哪里出错了——这对于学习来说并不理想。</p><p id="69c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦您确信您的代码可以工作，您可能想要将您的 Spark 应用程序集成到您的系统中。在这里，笔记本就没那么有用了。要按计划运行 PySpark，我们需要将代码从笔记本转移到 Python 脚本中，并将该脚本提交给集群。在本教程中，我将向您展示如何操作。</p><h1 id="3f64" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">我们开始吧</h1><p id="0555" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">一开始，从命令行向集群提交 Spark 应用程序可能会令人生畏。我的目标是揭开这个过程的神秘面纱。本指南将向您展示如何使用 AWS 命令行界面来:</p><ol class=""><li id="ac6c" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">创建一个能够处理比本地计算机容量大得多的数据集的集群。</li><li id="d277" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">向集群提交 Spark 应用程序，集群读取数据、处理数据并将结果存储在可访问的位置。</li><li id="007c" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">该步骤完成后自动终止集群，因此您只需在使用集群时付费。</li></ol><h1 id="88ea" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">Spark 开发工作流程</h1><p id="808d" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">当开发 Spark 应用程序来处理数据或运行机器学习模型时，我的首选是从使用 Jupyter 笔记本开始，原因如上所述。这里有一个关于<a class="ae le" rel="noopener" target="_blank" href="/getting-started-with-pyspark-on-amazon-emr-c85154b6b921">创建一个亚马逊 EMR 集群并用 Jupyter 笔记本</a>连接到它的指南。</p><p id="ad44" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我知道我的代码起作用了，我可能想把这个过程作为一个预定的工作来进行。我会把代码放在一个脚本中，这样我就可以用<a class="ae le" href="https://www.ostechnix.com/a-beginners-guide-to-cron-jobs/" rel="noopener ugc nofollow" target="_blank"> Cron </a>或<a class="ae le" href="https://airflow.apache.org/start.html" rel="noopener ugc nofollow" target="_blank"> Apache Airflow </a>把它放在一个时间表中。</p><h1 id="232c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">生产火花应用</h1><p id="c3a4" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated"><em class="nc">重要更新:本指南使用 AWS CLI 版本 1 —以下命令需要进行一些调整才能与版本 2 配合使用。</em></p><p id="fd9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank">创建您的 AWS 帐户</a>如果您还没有。<a class="ae le" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html#install-tool-pip" rel="noopener ugc nofollow" target="_blank">安装</a>和<a class="ae le" href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html#post-install-configure" rel="noopener ugc nofollow" target="_blank">配置</a>AWS 命令行界面。要配置 AWS CLI，您需要添加您的凭据。您可以按照这些说明创建凭证<a class="ae le" href="https://docs.aws.amazon.com/IAM/latest/UserGuide/getting-started_create-admin-group.html" rel="noopener ugc nofollow" target="_blank">。您还需要指定您的默认区域。对于本教程，我们使用<code class="fe nd ne nf ng b">us-west-2</code>。您可以使用任何您想要的区域。只是要确保所有的资源都使用相同的区域。</a></p><h2 id="ed33" class="nh mg it bd mh ni nj dn ml nk nl dp mp kr nm nn mr kv no np mt kz nq nr mv ns bi translated">定义 Spark 应用程序</h2><p id="b617" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">对于这个例子，我们将从 S3 加载亚马逊书评数据，执行基本处理，并计算一些聚合。然后，我们将把聚合数据帧写回 S3。</p><p id="54ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个例子很简单，但这是 Spark 的一个常见工作流。</p><ol class=""><li id="af53" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld lk ll lm ln bi translated">从源(本例中为 S3)读取数据。</li><li id="7a4e" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">使用 Spark ML 处理数据或执行模型工作流。</li><li id="4e66" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld lk ll lm ln bi translated">将结果写在我们的系统可以访问的地方(在这个例子中是另一个 S3 桶)。</li></ol><p id="59aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您还没有，<a class="ae le" href="https://docs.aws.amazon.com/quickstarts/latest/s3backup/step-1-create-bucket.html" rel="noopener ugc nofollow" target="_blank">现在就创建一个 S3 存储桶</a>。<strong class="kk iu"> <em class="nc">确保你创建桶的区域与你在本教程剩余部分使用的区域相同。我将使用地区“美国西部(俄勒冈州)”。</em> </strong>复制下面的文件。确保编辑<code class="fe nd ne nf ng b">main()</code>中的<code class="fe nd ne nf ng b">output_path</code>来使用你的 S3 桶。然后把<code class="fe nd ne nf ng b">pyspark_job.py</code>上传到你的桶里。</p><pre class="lu lv lw lx gt nt ng nu nv aw nw bi"><span id="f5fa" class="nh mg it ng b gy nx ny l nz oa"># pyspark_job.py</span><span id="8afe" class="nh mg it ng b gy ob ny l nz oa">from pyspark.sql import SparkSession<br/>from pyspark.sql import functions as F</span><span id="6f75" class="nh mg it ng b gy ob ny l nz oa">def create_spark_session():<br/>    """Create spark session.</span><span id="cc1b" class="nh mg it ng b gy ob ny l nz oa">Returns:<br/>        spark (SparkSession) - spark session connected to AWS EMR<br/>            cluster<br/>    """<br/>    spark = SparkSession \<br/>        .builder \<br/>        .config("spark.jars.packages",<br/>                "org.apache.hadoop:hadoop-aws:2.7.0") \<br/>        .getOrCreate()<br/>    return spark</span><span id="7f8a" class="nh mg it ng b gy ob ny l nz oa">def process_book_data(spark, input_path, output_path):<br/>    """Process the book review data and write to S3.</span><span id="1508" class="nh mg it ng b gy ob ny l nz oa">Arguments:<br/>        spark (SparkSession) - spark session connected to AWS EMR<br/>            cluster<br/>        input_path (str) - AWS S3 bucket path for source data<br/>        output_path (str) - AWS S3 bucket for writing processed data<br/>    """<br/>    df = spark.read.parquet(input_path)<br/>    # Apply some basic filters and aggregate by product_title.<br/>    book_agg = df.filter(df.verified_purchase == 'Y') \<br/>        .groupBy('product_title') \<br/>        .agg({'star_rating': 'avg', 'review_id': 'count'}) \<br/>        .filter(F.col('count(review_id)') &gt;= 500) \<br/>        .sort(F.desc('avg(star_rating)')) \<br/>        .select(F.col('product_title').alias('book_title'),<br/>                F.col('count(review_id)').alias('review_count'),<br/>                F.col('avg(star_rating)').alias('review_avg_stars'))<br/>    # Save the data to your S3 bucket as a .parquet file.<br/>    book_agg.write.mode('overwrite')\<br/>        .save(output_path)</span><span id="0e07" class="nh mg it ng b gy ob ny l nz oa">def main():<br/>    spark = create_spark_session()<br/>    input_path = ('s3://amazon-reviews-pds/parquet/' +<br/>                  'product_category=Books/*.parquet')<br/>    output_path = 's3://spark-tutorial-bwl/book-aggregates'<br/>    process_book_data(spark, input_path, output_path)</span><span id="82ea" class="nh mg it ng b gy ob ny l nz oa">if __name__ == '__main__':<br/>    main()</span></pre><h2 id="ff75" class="nh mg it bd mh ni nj dn ml nk nl dp mp kr nm nn mr kv no np mt kz nq nr mv ns bi translated">使用 AWS 命令行界面</h2><p id="d68a" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">是时候创建集群并提交应用程序了。一旦我们的应用程序完成，我们将告诉集群终止。自动终止允许我们仅在需要时才支付资源费用。</p><p id="94ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据我们的使用案例，我们可能不想在完成时终止集群。例如，如果您有一个依赖 Spark 来完成数据处理任务的 web 应用程序，那么您可能希望有一个一直运行的专用集群。</p><p id="e5e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行下面的命令。确保用你自己的文件替换<strong class="kk iu"> <em class="nc">粗体斜体</em> </strong>部分。关于<code class="fe nd ne nf ng b">--ec2-attributes</code>和<code class="fe nd ne nf ng b">--bootstrap-actions</code>以及所有其他参数的细节包括在下面。</p><pre class="lu lv lw lx gt nt ng nu nv aw nw bi"><span id="d865" class="nh mg it ng b gy nx ny l nz oa">aws emr create-cluster --name "Spark cluster with step" \<br/>    --release-label emr-5.24.1 \<br/>    --applications Name=Spark \<br/>    --log-uri <strong class="ng iu"><em class="nc">s3://your-bucket/logs/</em></strong> \<br/>    --ec2-attributes KeyName=<strong class="ng iu"><em class="nc">your-key-pair</em></strong> \<br/>    --instance-type m5.xlarge \<br/>    --instance-count 3 \<br/>    --bootstrap-actions Path=<strong class="ng iu"><em class="nc">s3://your-bucket/emr_bootstrap.sh</em></strong> \<br/>    --steps Type=Spark,Name="Spark job",ActionOnFailure=CONTINUE,Args=[--deploy-mode,cluster,--master,yarn,<strong class="ng iu"><em class="nc">s3://your-bucket/pyspark_job.py</em></strong>] \<br/>    --use-default-roles \<br/>    --auto-terminate</span></pre><p id="c304" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"/><code class="fe nd ne nf ng b"><strong class="kk iu">aws emr create-cluster</strong></code>重要<strong class="kk iu">论据:</strong></p><ul class=""><li id="57a9" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--steps</code>告诉您的集群在集群启动后要做什么。确保将<code class="fe nd ne nf ng b">--steps</code>参数中的<code class="fe nd ne nf ng b">s3://your-bucket/pyspark_job.py</code>替换为 Spark 应用程序的 S3 路径。您还可以将应用程序代码放在 S3 上，并传递一个 S3 路径。</li><li id="31d4" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--bootstrap-actions</code>允许您指定要安装在所有集群节点上的软件包。只有当您的应用程序使用非内置 Python 包而不是<code class="fe nd ne nf ng b">pyspark</code>时，这一步才是必需的。要使用这样的包，使用下面的例子作为模板创建您的<code class="fe nd ne nf ng b">emr_bootstrap.sh</code>文件，并将其添加到您的 S3 桶中。在<code class="fe nd ne nf ng b">aws emr create-cluster</code>命令中包含<code class="fe nd ne nf ng b">--bootstrap-actions Path=s3://your-bucket/emr_bootstrap.sh</code>。</li></ul><pre class="lu lv lw lx gt nt ng nu nv aw nw bi"><span id="bd7c" class="nh mg it ng b gy nx ny l nz oa">#!/bin/bash<br/>sudo pip install -U \<br/>    matplotlib \<br/>    pandas \<br/>    spark-nlp</span></pre><ul class=""><li id="7b17" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--ec2-attributes</code>允许您指定许多不同的 EC2 属性。使用以下语法设置您的密钥对<code class="fe nd ne nf ng b">--ec2-attributes KeyPair=your-key-pair</code>。<strong class="kk iu"> <em class="nc">注意:这只是你的密钥对的名字，不是文件路径。</em> </strong>你可以在这里了解更多关于创建密钥对文件<a class="ae le" href="https://medium.com/@brent_64035/create-a-key-pair-file-for-aws-ec2-b71c6badb16" rel="noopener">的信息。</a></li><li id="a52c" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--log-uri</code>需要一个 S3 桶来存储你的日志文件。</li></ul><p id="88a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">其他</strong> <code class="fe nd ne nf ng b"><strong class="kk iu">aws emr create-cluster</strong></code> <strong class="kk iu">论据解释:</strong></p><ul class=""><li id="a707" class="lf lg it kk b kl km ko kp kr lh kv li kz lj ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--name</code>给你正在创建的集群一个标识符。</li><li id="0fed" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--release-label</code>指定使用哪个版本的 EMR。我推荐使用最新版本。</li><li id="a42b" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--applications</code>告诉 EMR 您将在集群上使用哪种类型的应用程序。要创建火花簇，使用<code class="fe nd ne nf ng b">Name=Spark</code>。</li><li id="30d3" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--instance-type</code>指定要为集群使用哪种类型的 EC2 实例。</li><li id="415e" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--instance-count</code>指定集群中需要多少个实例。</li><li id="055d" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--use-default-roles</code>告诉集群使用 EMR 的默认 IAM 角色。如果这是你第一次使用 EMR，你需要运行<code class="fe nd ne nf ng b">aws emr create-default-roles</code>才能使用这个命令。如果您已经在配置了 AWS CLI 的区域中的 EMR 上创建了一个集群，那么您应该已经准备好了。</li><li id="ba27" class="lf lg it kk b kl lo ko lp kr lq kv lr kz ls ld oc ll lm ln bi translated"><code class="fe nd ne nf ng b">--auto-terminate</code>告诉集群在<code class="fe nd ne nf ng b">--steps</code>中指定的步骤完成后立即终止。如果您想让您的集群保持运行，请排除此命令—请注意，只要您保持集群运行，您就是在为它付费。</li></ul><h2 id="f1ca" class="nh mg it bd mh ni nj dn ml nk nl dp mp kr nm nn mr kv no np mt kz nq nr mv ns bi translated">检查 Spark 应用程序的进度</h2><p id="8ee1" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在您执行了<code class="fe nd ne nf ng b">aws emr create-cluster</code>命令之后，您应该会得到一个响应:</p><pre class="lu lv lw lx gt nt ng nu nv aw nw bi"><span id="7d30" class="nh mg it ng b gy nx ny l nz oa">{<br/>    "ClusterId": "j-xxxxxxxxxx"<br/>}</span></pre><p id="2cfa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">登录到 AWS 控制台并导航到 EMR 仪表板。您的集群状态应该是“正在启动”。您的集群启动、引导和运行您的应用程序大约需要 10 分钟(如果您使用了我的示例代码)。一旦该步骤完成，您应该会在 S3 存储桶中看到输出数据。</p><p id="833d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样！</p><h1 id="054c" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">最后的想法</h1><p id="4ae1" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">您现在知道了如何创建 Amazon EMR 集群并向其提交 Spark 应用程序。该工作流是使用 Spark 构建生产数据处理应用程序的重要组成部分。我希望您现在对使用所有这些工具更有信心了。</p><p id="1a42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦你的工作顺利进行，考虑在亚马逊上建立一个气流环境来安排和监控你的管道。</p><h1 id="c11f" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">取得联系</h1><p id="609f" class="pw-post-body-paragraph ki kj it kk b kl mx ju kn ko my jx kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">感谢您的阅读！请让我知道你是否喜欢这篇文章，或者你是否有任何批评。如果你觉得这个指南有用，一定要关注我，这样你就不会错过我以后的文章。</p><p id="5bcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你在一个数据项目上需要帮助或者想打个招呼，<strong class="kk iu">在</strong><a class="ae le" href="https://www.linkedin.com/in/brent-lemieux/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">LinkedIn</strong></a>上联系我。干杯！</p></div></div>    
</body>
</html>