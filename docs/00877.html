<html>
<head>
<title>Reinforcement Learning: The Other Type of Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:机器学习的另一种类型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-other-type-of-machine-learning-97ab81306ce9?source=collection_archive---------8-----------------------#2019-02-10">https://towardsdatascience.com/the-other-type-of-machine-learning-97ab81306ce9?source=collection_archive---------8-----------------------#2019-02-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/faf802a4cd322aec9e81e24f01f2f730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YAnaykCe7-S92ZD9dEa-0g.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="ba21" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">强化学习简介</h2></div><p id="0b9f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是强化学习和 OpenAI 健身房系列文章的第一篇。</p><h1 id="be2a" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">介绍</h1><p id="0ce5" class="pw-post-body-paragraph kt ku je kv b kw mh kf ky kz mi ki lb lc mj le lf lg mk li lj lk ml lm ln lo im bi translated">假设你在玩电子游戏。你进入一个有两扇门的房间。1 号门后面是 100 金币，后面是通道。门 2 后面是一枚金币，接着是另一个不同方向的第二个通道。一旦你穿过其中一扇门，就没有回头路了。你应该选择哪一扇门？</p><p id="bb9a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果你仅仅基于最大化你的直接回报(或分数)来做决定，那么你的答案将是 1 号门。然而，大多数视频游戏的目的不是最大化你在游戏的单个部分的分数，而是最大化你在整个游戏中的分数。毕竟，据你所知，在 2 号门后面的通道尽头可能有 1000 枚金币。</p><p id="16af" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">真正回答这个问题的唯一方法是多次玩这个游戏；每次尝试不同的门；然后根据你收集的信息，确定你的最佳策略。</p><p id="c1b1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我刚才描述的场景是强化学习(RL)的一个经典例子，这是一种经常被忽视的“其他”类型的机器学习。在这篇文章中，我将带您了解 RL 的基本特征，并介绍 Q-learning，这是一种基本的 RL 算法。</p><h1 id="ce0b" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">什么是强化学习？</h1><p id="549a" class="pw-post-body-paragraph kt ku je kv b kw mh kf ky kz mi ki lb lc mj le lf lg mk li lj lk ml lm ln lo im bi translated">强化学习与监督学习和非监督学习是机器学习的三种基本类型之一。</p><p id="bfab" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在监督学习中，给你一个带标签的数据集，目的是使用该数据集来确定一个通用规则，该规则允许你标记你可能遇到的任何新的数据点。</p><p id="95f7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如，使用一个带有标签的宠物图片的数据集，创建一个模型，允许您标记您可能遇到的任何新的宠物图片。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mm"><img src="../Images/050d2d46785b4acaf701a2b2b986de12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LJ7TKKNvCQNxNEG9bh0lRg.jpeg"/></div></div></figure><p id="7f53" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在无监督学习中，给你一个未标记的数据集，目的是简单地通过检查数据点之间存在的关系，得出关于数据底层结构的结论。</p><p id="3f2f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如，在 MNIST 手写数字数据集中识别相似图像的聚类。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b584581e9c13f263a915089862d5a1b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*01Qn7vbmsYpd5WXfvsWk6A.jpeg"/></div></figure><p id="13f2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，在强化学习中，你通常会看到一个最初未知的“环境”(如迷宫、美国证券交易所，甚至是真实世界)，而不是预先看到一个数据集，你必须通过谈论该环境中的行为(例如，在视频游戏中选择两扇门中的哪一扇门)来收集数据，并观察结果。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/50556b10a97ee6a7a3cfd79b0c888127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*eh2ZooqJFrwKmZJmsw7x2w.jpeg"/></div></figure><p id="7dac" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">RL 应用于的问题类型被称为“顺序决策问题”,最终目标是确定将使您的长期收益最大化的最佳行动顺序。</p><p id="73e9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">环境最初是未知的这一事实很重要，因为如果环境是完全已知的，那么您就不需要为了收集关于它的信息而与它进行交互。</p><p id="198b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">RL 问题的主要特征是:</p><ul class=""><li id="f085" class="mt mu je kv b kw kx kz la lc mv lg mw lk mx lo my mz na nb bi translated"><em class="nc">一个最初未知的环境</em> : <em class="nc"> </em>你必须探索这个环境以便获得关于它的信息；</li><li id="ac83" class="mt mu je kv b kw nd kz ne lc nf lg ng lk nh lo my mz na nb bi translated"><em class="nc">延迟反馈</em>:在采取一项行动后，可能需要一段时间才能完全意识到那项行动的长期后果；和</li><li id="5600" class="mt mu je kv b kw nd kz ne lc nf lg ng lk nh lo my mz na nb bi translated"><em class="nc">顺序决策</em>:收到的整体奖励通常是一系列多项行动的结果，而不是单一的独立行动。</li></ul><p id="b43b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">许多研究已经投入到开发用于解决 RL 问题的算法中。其中最著名的是 Q-learning。</p><h1 id="a7f9" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">q 学习</h1><p id="f0ff" class="pw-post-body-paragraph kt ku je kv b kw mh kf ky kz mi ki lb lc mj le lf lg mk li lj lk ml lm ln lo im bi translated">RL 算法有两种类型:“基于模型的”和“无模型的”。</p><p id="97d8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在“基于模型”的 RL 中，我们使用预先确定的环境模型(可能准确，也可能不准确)来确定最佳行动方案(也称为最佳<em class="nc">策略</em>),而在“无模型”RL 中，我们的目标是通过与环境互动来确定最佳策略。</p><p id="e26e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">反过来，无模型 RL 算法可以分为两种类型:“策略上的”和“策略外的”，与我们如何与环境交互以收集有关它的信息有关。</p><p id="9073" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">使用“政策上”算法，我们在做出决策时根据我们的“最佳”政策来决定我们的行动，然后使用从采取该行动中收集的信息来改进最佳政策。</p><p id="c350" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">另一方面，对于“非策略”算法，我们在与环境交互(或探索)时的行为可能与我们认为在采取行动时最佳的行为无关。</p><p id="6cd2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如，我们可能会故意选择采取我们知道在短期内是次优的行动，以确定它是否会在长期内带来更大的回报。</p><p id="d3c7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Q-Learning 是一种“无模型、无策略”的 RL 算法。</p><p id="2392" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Q-learning 的工作方式是构建一个 Q 值表 Q(s，a)，表示在环境状态<em class="nc"> s </em>下采取行动<em class="nc"> a </em>的预期贴现(长期)回报，然后通过与环境交互来迭代改进该表，直到找到最佳 Q 表。</p><p id="ed57" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最初，Q 表中的所有值都设置为小随机数(除了终端状态的 Q(s，a)值设置为 0)。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/1d8d351f8ebc1557b36080ef7e4cad01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*g0rUy9yjg7UNJgUcVRdzvg.jpeg"/></div></figure><p id="275a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">每次在环境中采取一个动作，都会产生一个(状态、动作、奖励、新状态)元组。这表示紧接在行动之前的环境状态，<em class="nc"> s </em>，在该状态下采取的行动，<em class="nc"> a </em>，在该行动之后收到的立即奖励<em class="nc">，r </em>，以及在该行动之后的新环境状态，<em class="nc">s’</em>。</p><p id="d26d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这些(状态、动作、回报、新状态)元组用于迭代地更新 Q 表中的值，其中更新的 Q(s，a)值，表示为 Q’(s，a)，被设置为等于旧的 Q(s，a)值和由新的观察暗示的 Q 值的加权平均值。即(I)即时奖励和(ii)从新状态开始收到的预期折扣奖励之和，假设你总是选择最优行动。</p><p id="4188" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这可以用下面的更新规则来表示:</p><blockquote class="nj nk nl"><p id="8a98" class="kt ku nc kv b kw kx kf ky kz la ki lb nm ld le lf nn lh li lj no ll lm ln lo im bi translated">Q'(s，a) = (1 — w) * Q(s，a) + w *(r + d * Q(s '，argmax a' : Q(s '，a '))</p></blockquote><p id="c6e1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">其中:</p><ul class=""><li id="7d78" class="mt mu je kv b kw kx kz la lc mv lg mw lk mx lo my mz na nb bi translated"><em class="nc"> w </em>是应用于新信息的权重(也称为学习率)；和</li><li id="153b" class="mt mu je kv b kw nd kz ne lc nf lg ng lk nh lo my mz na nb bi translated">d 是贴现率，它考虑了这样一个事实，即现在收到的 1 美元比将来收到的 1 美元更有价值。</li></ul><p id="31a5" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一旦找到最优 Q 表，给定状态下的最优动作就是使该状态下的 Q(s，a)最大化的动作(即 argmax a: Q(s，a))。</p><p id="5702" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这种算法的效果如何取决于用于与环境交互的探索策略的选择——如果你从未访问过环境的特定状态或在该状态下采取特定行动，那么你永远不会知道在该状态下采取行动的后果。</p><p id="2d28" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，我们也不只是想在我们的环境中随意移动。理想情况下，一旦我们收集了一些关于我们环境的信息，我们会通过将我们未来的探索集中在我们认为可能产生最大回报的状态和行动上来利用这些信息。一种方法是使用ε贪婪策略。</p><p id="0ebf" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在ε贪婪策略下，在给定状态下，<em class="nc"> s </em>，以概率(1—ε)选择贪婪最佳行动(即对于该状态使 Q(s，a)最大化的行动)，否则选择随机行动。</p><p id="87a6" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">通常ε会随时间衰减。这鼓励在算法的早期迭代中探索环境，但随后，随着时间的推移，减少了探索的数量，使我们能够更专注于利用这些信息。</p><p id="1444" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果所有的状态-动作对被访问足够多的次数，Q-学习算法保证最终收敛到最优表上，尽管收敛需要多长时间是另一回事。</p><h1 id="2ff8" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated">例子</h1><p id="4b84" class="pw-post-body-paragraph kt ku je kv b kw mh kf ky kz mi ki lb lc mj le lf lg mk li lj lk ml lm ln lo im bi translated">考虑以下迷宫(摘自<a class="ae np" href="https://samyzaf.com/ML/rl/qmaze.html" rel="noopener ugc nofollow" target="_blank">此处</a>):</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6d4e44d28d33975f134ca018a45f7315.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*Ql_Be5NpJiWYTD_MlehPNg.jpeg"/></div></figure><p id="0657" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你的目标是训练一个机器人找到通过这个迷宫的最佳路径，从单元(0，0)开始，到单元(6，6)结束，给定没有环境的先验知识。</p><p id="34d4" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了鼓励机器人找到最短的路径，机器人每次移动到一个空的(白色)单元时，应用 0.04 个单位的小惩罚，并且在迷宫周围放置障碍物(用灰色标记)，如果机器人进入包含其中一个障碍物的单元，则导致 0.75 个单位的较大惩罚。</p><p id="ab09" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">机器人只能向上、向下、向左或向右移动(即不允许对角线移动)。然而，每次移动都有一定程度的不确定性，例如，机器人只有 80%的机会沿预定方向移动，20%的机会沿与预定方向成直角的方向移动(两种可能性各半)。</p><p id="8884" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">机器人无法移动到迷宫的边界之外，如果它试图这样做，会撞到墙上，其位置保持不变。</p><p id="a81e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果机器人成功到达迷宫的尽头，它将获得 1 个单位的奖励。</p><p id="9090" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">假设贴现率为 0.9，学习率为 0.3，ε贪婪探索策略(常数)ε等于 0.5，在 Q 学习算法的 50，000 次迭代之后，我们得到以下策略。该图显示了机器人在每个方格中采取的最佳方向。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1a35584ffcd053877931f9e5f4da78c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*bVd14IDB42seUYCGVqvLlw.jpeg"/></div></figure><p id="76d3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">寻找这个解决方案的 Python 代码可以在<a class="ae np" href="https://github.com/gkhayes/maze_reinforcement_learning" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="e12d" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated"><strong class="ak">现实世界中的强化学习</strong></h1><p id="2f18" class="pw-post-body-paragraph kt ku je kv b kw mh kf ky kz mi ki lb lc mj le lf lg mk li lj lk ml lm ln lo im bi translated">虽然从上面给出的例子中可能看起来不明显，但研究人员已经找到了许多将 RL 应用于现实世界的方法。RL 已经成功地用于开发自动股票交易系统；优化化学反应；训练自动驾驶汽车；教电脑比任何人都玩得好；除了别的以外。</p><p id="964c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然而，对于那些希望开始 RL 的人来说，历史上最大的障碍之一是获得一个有趣和具有挑战性的实验环境。这就是 RL 是机器学习家族中最不为人知的成员的原因。然而，自从 OpenAI 健身房推出以来，情况就不再是这样了。</p><p id="0a3c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><a class="ae np" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>是一个 Python 包，包含一系列 RL 环境，从简单的“玩具”环境，如示例中描述的环境，到更具挑战性的环境，包括模拟机器人环境和 Atari 视频游戏环境。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/6339d2829c022193d75775251564fe7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nJntS8YxqND3ktAI6ElzfQ.jpeg"/></div></div><figcaption class="nt nu gj gh gi nv nw bd b be z dk">Examples of some of the environments available in OpenAI Gym</figcaption></figure><p id="14cf" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这个软件包使业余和专业的计算机科学家有可能试验一系列不同的 RL 算法，甚至有可能开发他们自己的算法。</p><h1 id="7c77" class="lp lq je bd lr ls lt lu lv lw lx ly lz kk ma kl mb kn mc ko md kq me kr mf mg bi translated"><strong class="ak">总结</strong></h1><p id="5895" class="pw-post-body-paragraph kt ku je kv b kw mh kf ky kz mi ki lb lc mj le lf lg mk li lj lk ml lm ln lo im bi translated">强化学习是机器学习的一个分支，处理从与反馈可能延迟的环境交互中进行学习。在本教程中，我们讨论了 RL 的基本特征，并介绍了所有 RL 算法中最著名的一种，Q-learning。</p><p id="4a40" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">Q-learning 包括为所有状态-动作对创建一个 Q(s，a)值表，然后通过与环境交互来优化该表。只要每个状态-动作对被访问足够多的次数，该算法就保证收敛。但是，当可能的状态和/或动作的数量变得如此之大以至于不再可能时，会发生什么呢？</p><p id="755f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我的下一篇文章中，我将介绍如何开始使用 OpenAI Gym，它包括几个允许我们更详细地探索这个问题的环境。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><p id="972f" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><em class="nc">Genevieve Hayes 博士是数据科学家、教育家和人工智能及分析专家，拥有</em><a class="ae np" href="https://www.genevievehayes.com/" rel="noopener ugc nofollow" target="_blank"><em class="nc">Genevieve Hayes Consulting</em></a><em class="nc">。你可以在</em><a class="ae np" href="https://www.linkedin.com/in/gkhayes/" rel="noopener ugc nofollow" target="_blank"><em class="nc">LinkedIn</em></a><em class="nc">或者</em><a class="ae np" href="https://twitter.com/genevievekhayes" rel="noopener ugc nofollow" target="_blank"><em class="nc">Twitter</em></a><em class="nc">上关注她。她还是</em> <a class="ae np" href="https://www.genevievehayes.com/episodes/" rel="noopener ugc nofollow" target="_blank"> <em class="nc">价值驱动数据科学</em> </a> <em class="nc">的主持人，这是一个每月两次的播客，面向希望最大化其数据和数据团队价值的企业。</em></p><p id="6735" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><em class="nc">想要发掘企业数据的价值，但不知道从哪里开始？</em><strong class="kv jf"><em class="nc"/></strong><a class="ae np" href="https://www.genevievehayes.com/discovery-guide/" rel="noopener ugc nofollow" target="_blank"><strong class="kv jf"><em class="nc">下载免费的数据科学项目发现指南。</em></strong>T37】</a></p></div></div>    
</body>
</html>