<html>
<head>
<title>Gaussian Mixture Models Clustering Algorithm Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型聚类算法讲解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e?source=collection_archive---------0-----------------------#2019-07-15">https://towardsdatascience.com/gaussian-mixture-models-d13a5e915c8e?source=collection_archive---------0-----------------------#2019-07-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3b0816dd2c5542b8ac06242d61d7f90a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CdFlXo-yh-4zkTry"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/macbook-pro-beside-spiral-notebook-669616/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/macbook-pro-beside-spiral-notebook-669616/</a></figcaption></figure><div class=""/><p id="2e18" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">高斯混合模型可以像 k-means 一样用于聚类未标记的数据。然而，与 k-均值相比，使用高斯混合模型有几个优点。</p><p id="c473" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，k 均值不考虑方差。方差是指钟形曲线的宽度。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi le"><img src="../Images/d1084b476638cdbce9aed001f6abe500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tBFK650dBxOxqn2H.png"/></div></div></figure><p id="6a61" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在二维中，方差(确切地说是协方差)决定了分布的形状。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi lj"><img src="../Images/e29a974b6ee57b1a878bb39854abf1fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*D41a20c7S8gy8UD3.png"/></div></div></figure><p id="2173" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">思考<em class="lk">k</em>-均值模型的一种方式是，它在每个集群的中心放置一个圆(或者，在更高维度中，一个超球体)，半径由集群中最远的点定义。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/b175f13170bf49c5cfc742b6c1cbcf80.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Zl8_vxkkqv-ZV3IWZdRCmw.png"/></div></figure><p id="2648" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当你的数据是循环的时候，这很好。然而，当您的数据呈现不同的形状时，您最终会得到这样的结果。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/cefded41b82a2363aee0a0a35a4df54f.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*D2nunNrckTdV9n7gTUC4Zg.png"/></div></figure><p id="00b6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，高斯混合模型甚至可以处理非常长方形的集群。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/fed30dca40f25ede9f046683d3b933e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*eTAFs5cTUjb_kt-4RgE3uw.png"/></div></figure><p id="5ba2" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">k-means 和高斯混合模型之间的第二个区别是前者执行硬分类，而后者执行软分类。换句话说，k-means 告诉我们什么数据点属于哪个聚类，但不会给我们提供给定数据点属于每个可能的聚类的概率。</p><p id="95fb" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在调用<code class="fe lm ln lo lp b">predict</code>函数时，模型会将每个数据点分配给其中一个聚类。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="f326" class="lu lv jj lp b gy lw lx l ly lz">gmm.predict(X)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/2a21fb0289ef4aabc61aa053c936f04c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*RM1Z0sLZNqs5lbEXZek-bQ.png"/></div></figure><p id="4cc8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，我们可以调用<code class="fe lm ln lo lp b">predict_proba</code>函数来返回一个数据点属于每个<em class="lk"> K </em>聚类的概率。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="044c" class="lu lv jj lp b gy lw lx l ly lz">gmm.predict_proba(X)</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/997aad67c721428661f44a579106cdcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*DA6vEtLrS75sGQlTY9_Quw.png"/></div></figure><h1 id="54f5" class="mc lv jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">高斯混合模型一览</h1><p id="00ee" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">顾名思义，高斯混合模型涉及多个高斯分布的混合(即叠加)。为了便于解释，假设我们有三个由三个不同类别的样本组成的分布。</p><p id="6851" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">蓝色高斯表示构成下层阶级的人的教育水平。红高斯表示构成中产阶级的人的教育水平，绿高斯表示构成上层阶级的人的教育水平。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/2190d842f5c50f5eed5c42f40996d8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNtjQbi2cyH8-mL4D8AZ3A.png"/></div></div></figure><p id="a045" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在不知道哪些样本来自哪个类的情况下，我们的目标是使用高斯混合模型将数据点分配到适当的聚类中。</p><p id="901c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练完模型后，我们理想的结果是在同一轴上有三个分布。然后，根据给定样本的教育水平(它在轴上的位置)，我们将它放在三个类别中的一个。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nf"><img src="../Images/9aff725c67084c7ab862b60c7a55291e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4-BjVX30dNm7EHSjAxA_w.png"/></div></div></figure><p id="05d0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个分布都乘以一个权重π，以说明我们在每个类别中没有相同数量的样本。换句话说，我们可能只包括了 1000 名来自上层阶级的人和 10 万名来自中产阶级的人。</p><p id="e157" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们处理的是概率，所以权重加起来应该是 1。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ce06d4c956017a12124c06a3fdc7a4dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*TAE-IHEZ0HeDKjcM4JwCaw.png"/></div></figure><p id="665a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们决定添加另一个维度，比如孩子的数量，那么，它可能看起来像这样。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/ab07f4eb74d00ba51964b61b739cad3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9RzFPKbQP4jKAiaqV4jmEg.png"/></div></div></figure><h1 id="a006" class="mc lv jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">高斯混合模型算法</h1><p id="66c6" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">对于那些对数学不感兴趣的人，我提前向你们道歉，因为下一节很重。</p><p id="d530" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们想知道第 I 个样本来自高斯<em class="lk">k</em>T3】的<strong class="ki jk">概率是多少。我们可以这样表达:</strong></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/fdba21bed777611397b4bf741c30c6d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHv9sdKWfVqzJqUIEjrgJg.png"/></div></div></figure><p id="fc8c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中θ代表每个高斯的平均值、协方差和权重。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/8d23598a63f97889438b0386820db5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMz6cA9HWLqzjVQPWr5uHw.png"/></div></div></figure><p id="bedc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你也可能会碰到写成π的方程。这不要和每个高斯相关的权重混淆(我知道的混淆)。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ca96c75b0dc8cd389b7e2152a9fbee3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*6QZjtruR1MNwmrTRyd53VQ.png"/></div></figure><p id="1384" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们将观察到来自高斯 K 的数据点的<strong class="ki jk">可能性表示为:</strong></p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/0e2db870cccaf9405e86a5334c95dd1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*lK3gSMSJnh0C8WiUfPKvDA.png"/></div></figure><p id="110f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后者有时候是这样写的(我相信<strong class="ki jk"> <em class="lk"> N </em> </strong>来源于<strong class="ki jk"> <em class="lk"> N </em> </strong>正规分布):</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/f42bed47744fb2fba3ed5c2fd928868a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWYyRPdP7kt2ki0VCl_YPg.png"/></div></div></figure><p id="3c6c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有一个高斯分布，横轴是一个人可能得到的不同智商分数，从最低到最高。我们可以通过从沿着 x 轴的位置到曲线画一条垂直线，然后查看 y 轴上相应的值，来找出个人智商为 120 的可能性有多大。任意一点的 y 值都等于上面的等式。</p><p id="6fa8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想知道在考虑所有不同分布的情况下观察样本<strong class="ki jk"> <em class="lk"> i </em> </strong>的可能性，我们只需对观察样本的可能性求和，假设样本来自每个可能的高斯分布。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/6621df97a5382da1851cfd624c9ab21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHwyh5K0Mk0Ov6Imf5ghjw.png"/></div></div></figure><p id="59da" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，我们从数据集中选取一个样本(行)，查看单个特征(即教育水平)，在 x 轴上绘制其位置，并对每个分布的相应 y 值(可能性)求和。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/9c1c8918877048486302bf3044d0724e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wIw9aiDQYBmr-sSP213Uhg.png"/></div></div></figure><p id="3282" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了将此扩展到我们数据集中的所有样本。我们假设观察一个样本的可能性独立于所有其他样本，然后我们可以简单地将它们相乘。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/19562b7993377523567ca535b4907622.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hu5dfstYu0D5SW_1uBNoEg.png"/></div></div></figure><p id="54f4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用之前看到的命名法重写该等式，如下所示:</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nq"><img src="../Images/8a4dc642ba1c38f8326b905b546634ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6pbvHn9anlDawbsd_q-tsQ.png"/></div></div></figure><p id="09c8" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常情况下，我们采用可能性的对数，因为对数中两个数字的乘积等于其组成部分的对数之和，并且数字相加比相乘更容易。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3fe5965c0d801b7e76f8bfe1f9d27ce8.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*pY7LTt8RY4mESp0AKsH-Lw.png"/></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/933076f8c3f7c5125cee4659ba2bb1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_OYwR8-J6EytsuxGKN_fxQ.png"/></div></div></figure><h1 id="2111" class="mc lv jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">期望最大化算法</h1><p id="1d7e" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">我们还需要解决这样一个事实，即我们需要每个高斯的参数(即方差、均值和权重)以便对我们的数据进行聚类，但是我们需要知道哪个样本属于哪个高斯，以便估计那些完全相同的参数。</p><p id="c87d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是期望最大化发挥作用的地方。在高层次上，期望值最大化算法可以描述如下:</p><ol class=""><li id="c60b" class="nt nu jj ki b kj kk kn ko kr nv kv nw kz nx ld ny nz oa ob bi translated">从随机高斯参数(θ)开始</li><li id="7489" class="nt nu jj ki b kj oc kn od kr oe kv of kz og ld ny nz oa ob bi translated">重复以下步骤，直到我们收敛:</li></ol><p id="df20" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">a) <strong class="ki jk">期望步骤</strong>:计算 p(zi = k | xi，θ)。换句话说，样本<em class="lk"> i </em>看起来像是来自聚类 k 吗？</p><p id="a5b5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">b) <strong class="ki jk">最大化步骤</strong>:更新高斯参数(θ) <strong class="ki jk"> </strong>以适合分配给它们的点。</p><h1 id="cf85" class="mc lv jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">最大化步骤</h1><p id="b6c8" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">在最大化步骤中，我们希望最大化每个样本来自分布的可能性。回想一下，可能性是曲线在 x 轴某一点的高度。因此，我们希望修改分布的方差和均值，以使每个数据点的图形高度最大化。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/d5d89863cd1dd17c24379b85ded40afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CFOb_meu_ebZcY4zU-mq1A.png"/></div></div></figure><p id="2fc6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就带来了一个问题，“我们应该如何着手选择方差和均值的最优值。”使用<em class="lk">期望最大化</em>的一般形式，我们可以推导出均值、方差和权重的一组方程。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/69635370aa36529bc1965fbc0d4ac0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WxVTak_ImMOyIKwapAGXdw.png"/></div></div></figure><p id="ca9b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以遵循相同的过程来获得协方差和权重的方程。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/6df78c8b47088ee6d2a3719485194e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8PUJPF1hj57tgwvU9_jXuw.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ok"><img src="../Images/3c921d14116c4119205acc474189d19e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nv_mwshXeiw9SUEJwg1WSQ.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/86fcfb6be22deb95b4dcf9c7f0ddcf2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*cXjzyo1_zewSqSOnArXM1g.png"/></div></figure><p id="a69a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们有了方程，我们只需在最大化步骤中应用它们。也就是说，我们在每个方程中插入数字，以确定最佳均值、协方差、权重，然后相应地设置高斯参数。</p><p id="d633" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来看看数学在起作用。最初，我们不知道哪些点与哪个分布相关联。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/c3ba770aebfc6ef492094cb6bd86be3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bdzCO9M2W6XGI4RL5BPnEg.png"/></div></div></figure><p id="5948" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们从具有随机均值、方差和权重的 K 个高斯分布(在这种情况下，K=2)开始。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/d668141bf5fd3aea864d600e9964a576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wXCbbjEgiRqoQiLqLS2n0Q.png"/></div></div></figure><p id="5ef6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们重复期望和最大化步骤，直到θ几乎没有变化。</p><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/dceee06f67902b6274aff267bc71720c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfjMltXjRPeKJ5jQlZE_2g.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/41915c8b9ca55cbce8bdf54383b9ec69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rBu2X9HrVNW9e6DATyA9TA.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/4df4e2ecb240be59be6bf1ab25ce768f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BEPQ7NC-Oq_N6SNNUpe71Q.png"/></div></div></figure><figure class="lf lg lh li gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/8238efa1f7310369cdafa7055c3ee8f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8L3Qq1vupYdZ8aIz5PLmmw.png"/></div></div></figure><p id="6478" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">值得注意的是，该算法易受局部极大值的影响。</p><h1 id="d78d" class="mc lv jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">密码</h1><p id="e084" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">现在，我们已经掌握了高斯混合模型的工作原理，让我们来看看如何实现它们。首先，导入以下库。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="88d4" class="lu lv jj lp b gy lw lx l ly lz">import numpy as np<br/>from sklearn.datasets.samples_generator import make_blobs<br/>from sklearn.mixture import GaussianMixture<br/>from matplotlib import pyplot as plt<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="1d04" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们随机生成 4 个集群。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="deb6" class="lu lv jj lp b gy lw lx l ly lz">X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)<br/>plt.scatter(X[:,0], X[:,1])</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi os"><img src="../Images/2da7a87e4f9c5629caa90366af15dcdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*4ddVlFTxtT_AomF5LTTUgQ.png"/></div></figure><p id="3d7e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最佳聚类数(K)是最小化<a class="ae jg" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank">赤池信息准则(AIC) </a>或<a class="ae jg" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion" rel="noopener ugc nofollow" target="_blank">贝叶斯信息准则(BIC) </a>的值。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="92f8" class="lu lv jj lp b gy lw lx l ly lz">n_components = np.arange(1, 21)<br/>models = [GaussianMixture(n, covariance_type='full', random_state=0).fit(X) for n in n_components]</span><span id="f1ec" class="lu lv jj lp b gy ot lx l ly lz">plt.plot(n_components, [m.bic(X) for m in models], label='BIC')<br/>plt.plot(n_components, [m.aic(X) for m in models], label='AIC')<br/>plt.legend(loc='best')<br/>plt.xlabel('n_components');</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c348796d2c014dd60d6b02ca385d415e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*s0LOlXe_yy6hFodNRghmfA.png"/></div></figure><p id="c434" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用最佳数量的聚类(在本例中为 4)来训练我们的模型。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="b8da" class="lu lv jj lp b gy lw lx l ly lz">gmm = GaussianMixture(n_components=4)</span><span id="ca68" class="lu lv jj lp b gy ot lx l ly lz">gmm.fit(X)</span></pre><p id="40ea" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用<code class="fe lm ln lo lp b">predict</code>方法来获得一系列的点和它们各自的集群。</p><pre class="lf lg lh li gt lq lp lr ls aw lt bi"><span id="abb4" class="lu lv jj lp b gy lw lx l ly lz">labels = gmm.predict(X)</span><span id="070d" class="lu lv jj lp b gy ot lx l ly lz">plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis');</span></pre><figure class="lf lg lh li gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a2fe02aaee9996f0143233aa6113544f.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*Mils2mhgQCFfFZEvm3yo0A.png"/></div></figure><h1 id="2be3" class="mc lv jj bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">最后的想法</h1><p id="f577" class="pw-post-body-paragraph kg kh jj ki b kj mz kl km kn na kp kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">与 k-means 不同，高斯混合模型考虑了方差并返回数据点属于每个<em class="lk"> K </em>聚类的概率。</p></div></div>    
</body>
</html>