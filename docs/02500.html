<html>
<head>
<title>Exploration in Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的探索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploration-in-reinforcement-learning-e59ec7eeaa75?source=collection_archive---------2-----------------------#2019-04-24">https://towardsdatascience.com/exploration-in-reinforcement-learning-e59ec7eeaa75?source=collection_archive---------2-----------------------#2019-04-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a59a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">应该在勘探和开发上花费多少努力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3e97a3a675a4eef78067bb6b730acfc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7S-9IPC8XjUiFVBR"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@dariuszsankowski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Dariusz Sankowski</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e2f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">更新</strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae ky" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="dbd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个人每天都面临着同样的困境:我应该继续做我正在做的事情，还是应该尝试其他事情。例如，我应该去我喜欢的餐馆还是应该尝试一个新的，我应该保持目前的工作还是应该找到一个新的，等等…</p><p id="d5e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在强化学习中，当你坚持做你正在做的事情时，这种类型的决定被称为<strong class="lb iu"> <em class="lv">开发</em> </strong>，当你尝试新事物时，这种类型的决定被称为<strong class="lb iu"> <em class="lv">探索</em> </strong>。</p><p id="8c32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">很自然，这就提出了一个问题:开发多少，勘探多少。</p><p id="6eb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，在探索中有一个问题，那就是我们并不真正知道结果会是什么，它可能比目前的情况更好，也可能更糟。</p><p id="7a1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人类在采取行动之前，试图获得尽可能多的信息，例如，在我们尝试一家新餐馆之前，我们会阅读评论或询问已经尝试过的朋友。另一方面，在强化学习中，这是不可能的，但是有一些技术可以帮助找出最佳策略。</p><h1 id="bd4d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated"><strong class="ak">后悔的概念</strong></h1><p id="11bf" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">从逻辑上讲，当我们尝试新事物，结果不令人满意时，我们会后悔我们的决定。如果新餐馆很差，我们会后悔去那里，我们认为无论我们付多少钱都是完全的损失。所以我们很遗憾这个金额。</p><p id="61ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们不断为错误的决定买单时，损失的数量和后悔的程度都会增加。因此，将损失金额和后悔程度保持在最低水平应该是个好主意。</p><p id="e450" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能做到吗？答案是，是的，这是可能的，至少在 RL 中是这样。</p><h1 id="397d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">强化学习中的遗憾</h1><p id="84b3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">首先我们需要定义 RL 中的后悔。为此，我们首先将最佳行动 a*定义为给予最高回报的行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/ae967c1ad872d7b9f50d9ce305635e6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*Ny8twv5X9H4HE_tB3AwVbw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Optimal action</figcaption></figure><p id="cd64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们将 T 次尝试过程中的后悔 L 定义为最佳行动产生的奖励 a*乘以 T 与任意行动的每次奖励从 1 到 T 的总和之差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/48eafc895b2294946f8d3031cdcfc7c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*Ufv_IM67nXV0VdxMd_ZCPg.png"/></div></figure><p id="8d9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以证明，当 T 趋于无穷大时，后悔趋于 C . log T 的一个下界。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/06b83532f479f33481bcdfb8f8951622.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*xejETlLnCnVL4mXkUwHZtQ.png"/></div></figure><h1 id="e534" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">贪婪&amp;ε贪婪</h1><p id="51a2" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Greedy 和 Epsilon Greedy 探索方法相当容易理解和实现，但是它们遭受重大挫折，即它们具有次优遗憾。事实上，贪婪者和贪婪者的后悔都是随着时间线性增长的。</p><p id="7d62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很容易直观地理解，因为贪婪者会锁定一个在某个时间点碰巧有好结果的行动，但它实际上并不是最佳行动。所以贪婪者会继续利用这个行为，而忽略其他可能更好的行为。它利用得太多了。</p><p id="0748" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，贪婪的 Epsilon 探索太多，因为即使当一个行为看起来是最佳的，方法仍然分配固定百分比的时间用于探索，因此错过了机会，增加了总的遗憾。</p><p id="7685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，随着时间的推移，衰减的ε贪婪方法试图减少专用于探索的百分比。如下图所示，这可以产生最佳的遗憾。</p><p id="78f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，挑战在于能够执行正确的衰减过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/b5f3b8972daaf0b9b7dd84ae5a5a5453.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2BKtzg0zwMax5HuQOpxBXg.png"/></div></div></figure><h1 id="4473" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">面对不确定性保持乐观</h1><p id="b8e4" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">面对不确定性的乐观是一种加强探索和减少总遗憾的方法。</p><p id="4be2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们有三个概率分布不同的老虎机，我们并不确切知道，但经过几次试验后，我们认为我们有以下分布形状。请记住，这些都是根据经验得出的，并不一定反映事实。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/bebbddb2e38dc445a36015d61066e8db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LxBW_m4R23c0Tv2jfx6ItA.png"/></div></div></figure><p id="a322" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绿色 Q(a3)分布具有相当窄的基数，因为间隔很小[1.8，3.2]，红色 Q(a2)具有较大的间隔[0，4]，最后蓝色 Q(a1)具有最大的间隔[-1.8，5.2]。所以问题是我们下一步应该采取什么行动。</p><p id="3d6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据面对不确定性的乐观主义原则，即使不确定性较高，我们也会选择回报较高的行动。查看图表，我们很容易发现 Q(a1)可能比其他人有更高的回报(5.2)，所以我们选择了它，即使它有更高的不确定性(它可能是 5.2，因为它可能是-1.8)。</p><p id="f9f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设采取了行动 a1，分布如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/49303559a18f8b82ec31bd161e256f6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8znJdnbVHdEJUXeSq1wplA.png"/></div></div></figure><p id="1ab3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在知道蓝色分布的最大值比另外两个小。所以我们对它不太确定，下一个动作将从另一个分布中选择，例如红色的。</p><p id="5924" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是原则，但如何在现实中实施呢？<br/>这就是 UCB1 的意义所在。</p><h1 id="2520" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">UCB1</h1><p id="43fa" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在上一段中，我们谈到了采取行动，这可能会带来最大的回报，即使这是不确定的。<br/>然而，问题仍然是如何找到这个最大值？</p><p id="cf7a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一种方法是对每个动作的 Q <em class="lv"> 𝑡 </em> (a)加上一个称为置信上限 U <em class="lv"> 𝑡 </em> (a)的项，然后选择具有最大 Q <em class="lv"> 𝑡 </em> (a) + U <em class="lv"> 𝑡 </em> (a)的动作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/c41529028c922049ba2f6dea97d29e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLskMFQgmy23JO-4SYRJEA.png"/></div></div></figure><p id="19ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不言而喻，U <em class="lv"> 𝑡 </em> (a)不是常数，而是应该随时间和经验而变化。<br/>我们的意思是，随着时间的推移，我们频繁地选择行动(a)，我们越来越确定 Q(a)是什么，它本身是一个平均值，所以 U <em class="lv"> 𝑡 </em> (a)应该随着我们对 Q(a)越来越有信心而缩小。</p><p id="9353" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">U <em class="lv"> 𝑡 </em> (a)的公式如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/fcdd0993c55b9258f7ad7e1007fc5b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u5PpIUSAvAI8YnkOAlejjA.png"/></div></div></figure><p id="c4af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> <em class="lv"> t </em> </strong>为试验总次数，N <em class="lv"> 𝑡 </em> (a)为动作(a)被选择的次数。<br/>该公式清楚地表明，随着<strong class="lb iu"> <em class="lv"> t </em> </strong>的增加，分子也以对数方式增加(例如缓慢增加)，而当选择动作(a)时，N <em class="lv"> 𝑡 </em> (a)增加 1，然而 U <em class="lv"> 𝑡 </em> (a)急剧减少。</p><p id="e189" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/f354a68ecac0b769eebe36442b6f2579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bQosOFwT3pY96hHFaPp3zw.png"/></div></div></figure><p id="cc46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，对于 t 从 100 到 1000，N <em class="lv"> 𝑡 </em> (a)为 1，那么 U <em class="lv"> 𝑡 </em> (a)从 2 到 2.45 增长非常缓慢，但是当 N <em class="lv"> 𝑡 </em> (a)增加 1 时，U <em class="lv"> 𝑡 </em> (a)急剧下降到 1.73。</p><p id="f3cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以 U <em class="lv"> 𝑡 </em> (a)是一种调节选择动作(a)的方式，基于过去我们已经多次选择了这个动作。换句话说，它为我们提供了更多回报的确定性。</p><p id="f807" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终算法规定，对于每次迭代，我们计算所有动作的 Q <em class="lv"> 𝑡 </em> (a) + U <em class="lv"> 𝑡 </em> (a)，并且我们选择具有最高 Q <em class="lv"> 𝑡 </em> (a) + U <em class="lv"> 𝑡 </em> (a)值的动作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/967e83acb0f47bc18bf2b9ea4bcddb52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2n-N8BhHPizu-5hqbu6Vg.png"/></div></div></figure><p id="eda0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种探索 vs 利用的方法保证了有一个对数遗憾。</p><p id="6f8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重要提示</strong> : UCB1 没有对分布的类型做任何假设，尽管在上面的图表中它们看起来像高斯分布，但是它们可以是任何分布。</p><h2 id="4daa" class="nd lx it bd ly ne nf dn mc ng nh dp mg li ni nj mi lm nk nl mk lq nm nn mm no bi translated">正态分布</h2><p id="47b7" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">假设我们知道报酬分布是高斯分布:<br/> ℛ(r) = <strong class="lb iu"> 𝒩 </strong> (r，μ，σ)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/8072986b1b2db3dadf7c86d694ac4308.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fhv38LWh6ZhZ30TVNnXNXw.png"/></div></div></figure><p id="de7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">算法变成选择最大化 Q(a)的标准偏差的动作</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/88b4d821c5fe542a5e7ef9248fcc103c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*01h-HD1DGFxLlV_jLGlMlw.png"/></div></div></figure><h1 id="fd5a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">汤普森取样</h1><p id="9c70" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">也称为后验抽样，这是一种从每个分布中抽取样本，然后从样本中选择具有最大值的动作的方法。<br/>采取行动并收集奖励后，我们更新采取行动的分配，以反映我们得到的结果。</p><p id="8b44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，这需要更多的解释。</p><p id="626f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑一个骰子，我们不知道它是否有偏差，实际上我们没有任何关于它的信息。所以我们假设，作为开始，这是一个公平的骰子，得到任何数字的概率都等于 1/6。<br/>所以最初的信念是概率分布是均匀的，等于 1/6。这称为先验分布，它构成了我们对骰子潜在概率分布的一种信念或假设(可能是错误的)。<br/>现在我们开始掷骰子并收集结果，随着每个结果我们更新先验分布，使其反映实验的现实。新的分布现在被称为后验分布。我们不断重复这个过程大量的迭代，其中迭代(I)处的后验分布变成迭代(i+1)处的先验分布，直到我们到达最终后验分布非常接近真实的基础分布的阶段。<br/>例如，如果我们注意到，在大量投掷后，我们有一个数字比其他数字占优势，这意味着分布不可能是均匀的，最有可能的是骰子有偏差。</p><p id="9b97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在出现的问题是，我们如何更新先验分布来获得后验分布？</p><p id="c996" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要回答这个问题，让我们从定义贝塔分布开始:<br/>贝塔分布是定义在区间[0，1]上的一族连续概率分布，区间由两个正参数α和β参数化，它们决定了分布的形状。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/a737e6edac774b3dea03d9958e3c5ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJyeEY_kHazXsmHX2nZWTg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Formula of the Beta distribution, where C is a constant and 𝝰, β are parameters, and x is between 0 and 1</figcaption></figure><p id="24b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以将β分布视为一个分布族，其中每个成员根据参数𝝰，β的值而各不相同。</p><p id="f462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是β分布在𝝰，β值之后的一些形状示例。</p><div class="kj kk kl km gt ab cb"><figure class="ns kn nt nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/153ccbded67ef0199408db2753cd7d4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*5Lw2zFl9uyDLoIdAEVW-ZA.png"/></div></figure><figure class="ns kn ny nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/5f0263cb9e9ace2eae1b8a328ab0027e.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*QL1CMTGq-D2jJ8ANUlGjMg.png"/></div></figure><figure class="ns kn nz nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/e436fbbad08f7c117593a64d236f3bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*fWPidT9FwKfGzR5x2PymLg.png"/></div></figure></div><div class="ab cb"><figure class="ns kn oa nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/8e3de80886a3d4e034b4e50e4c53b55d.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*vwm8HKM3DJQ-da6H2igptg.png"/></div></figure><figure class="ns kn ob nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0b276b25deb3402e7170c0894044085f.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*zVkjtkhUHdrENVKwfwwu9A.png"/></div></figure><figure class="ns kn oc nu nv nw nx paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d09d83d49cdc139f44f3aa7635b7c5cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*J0WLe9pLl-kpGseTQNtixQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk od di oe of">Shapes of <strong class="bd og">Beta</strong> distribution relative to the values of 𝝰, β parameters</figcaption></figure></div><p id="d455" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如何采样？</p><p id="d374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">采样包括从分布中随机获取值，但是由于分布可能不均匀，因此采样返回的值很可能来自分布达到峰值的区域。</p><p id="f1f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，以上面这组图表中第二行的第三个图表为例。这种分布在大约 0.84 处达到峰值。因此，当从该分布中采样时，它更可能具有大约 0.84 的值，而不是例如具有小于 0.4 的值。</p><p id="f791" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在假设你有多个行动，每个行动都有未知的回报分布。我们使用汤普森抽样来发现哪种行为是最好的(它使平均回报最大化)。我们首先假设某个分布，如 Beta(1，1)，它是每个动作的均匀分布，然后我们对这些分布进行采样，并选择具有最高采样值的动作。<br/>一旦选择的动作被执行并且奖励被收集，选择的动作的分布被更新。</p><p id="6061" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面举一个多臂土匪(老虎机)的例子。<br/>例子由三个多臂土匪组成:蓝、红、紫。由于我们对这些匪徒的潜在分布一无所知，我们假设他们是统一的。我们从这些分布中采样，得到如下结果:蓝色= 0.4，红色= 0.5，紫色= 0.7 <br/>结果是紫色的采样值最高，所以我们拉紫色机器的手臂。假设我们没有赢，那么我们需要通过将β参数增加 1 来更新我们对紫色分布的信念，使之成为β(1，2)，这将给出下面的第一个图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/cd72276bde4a96fede9ced4c7c5c3e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RrfbDag4lvv_V4pjjne97w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits" rel="noopener ugc nofollow" target="_blank">https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits</a></figcaption></figure><p id="bfc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们再次采样，这一次红色分布得到了最高的采样值，所以我们拉动红色机器的手臂，我们就赢了。我们通过将𝝰增加 1 来更新红色分布，并且我们得到第二个图(第 2 列，第 1 行)。<br/>当我们继续从所有分布中取样时，我们注意到蓝色的分布比其他分布获得更多的胜利，因此更新使它比其他分布更有优势，并且分布开始呈现向高值(&gt; 0.8)的可能性峰值。这意味着我们更多地利用蓝色机器，而不是其他两个。换句话说，我们越来越相信我们已经找到了最佳行动。</p><h1 id="ec39" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结论</h1><p id="293b" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">本文展示了强化学习中最常用的一些探索技术。从中学到的主要知识是，探索那些优先级低的行为是浪费时间和资源。因此，重要的是使用一种探索的方法来减少遗憾，这样学习阶段会变得更快更有效。</p></div></div>    
</body>
</html>