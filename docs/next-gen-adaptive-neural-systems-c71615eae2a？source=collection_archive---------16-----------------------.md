# 下一代自适应神经系统

> 原文：<https://towardsdatascience.com/next-gen-adaptive-neural-systems-c71615eae2a?source=collection_archive---------16----------------------->

![](img/9299d9978bf71e96d371b6e4571e25a6.png)

Photo by [Joshua Sortino](https://unsplash.com/@sortino?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

自从深度学习革命开始以来，神经网络中就存在一种趋势，这种趋势可以简单地用一个词来概括:规模。

缩放这些模型的自然维度是计算和数据。十多年来，从硬件加速器到高级软件工具，计算扩展在堆栈的每个级别都是一项艰巨的任务。大型数据集已成为新数字经济中的一个关键优势，它一直是能够吸收和理解此类数据集的模型背后的驱动因素。

在大多数情况下，最好的模型处于我们扩展这些资源的能力的最前沿。这成为一个实际问题，因为预测有时需要在低延迟的情况下大量计算，但通常学习最好的模型不是最快的。与解决这个问题相关的研究领域叫做模型压缩。有许多技术，有些感觉有点粗糙，比如将小权重设置为零，并引入稀疏数据表示。其他方法似乎更有原则，如知识提炼，其中一个训练有素的重量级模型用于“教授”一个小得多的网络，或者另一种称为条件计算的技术，其中控制器决定为给定的输入样本激活网络的哪些部分。

# 模型适应和解压缩

相对于最初训练的网络，压缩模型在泛化能力方面损失了多少，这是一个公开的问题。如果你读了论文，你当然会发现在实验中，压缩模型的表现和原始模型一样好，甚至比原始模型的评价集还要好。但我在现实世界中部署模型时学到的一件事是，故事永远不会结束。环境总是可以在你的带领下改变，有时很慢，有时相当快。

通往真正智能和适应性系统的道路可能依赖于我从旧的创造力和天才之神那里得到的灵感，我称之为*模型解压*。原理很简单:当你发现你的模型在性能上有偏差时，在压缩模型中添加一些容量，然后动态学习。

从技术角度来看，这听起来像是一个愚蠢的想法，因为我们今天处理模型漂移的方式要有效得多。我们只是重新训练或不断训练大网络，并在我们想要推出新型号时重复压缩过程。理解为什么*认为*是次优的，需要一点关于边缘计算的想象力。

# 边缘

模型压缩得到真正关注的原因是因为我们希望部署这些令人惊叹的技术，而不会受到网络的瓶颈限制。网络通信通常是最慢和最糟糕的资源，因此，如果你需要一个大型云服务器来完成所有的 ML 并将其发送回手机或远程设备，你将面临可靠性和延迟的问题。模型压缩使我们能够运输这些模型，并以低延迟和低功耗在设备上运行它们。

我会回头说我们当前的方法有点傻，因为当环境彻底改变时，你不能指望一个超专门化的模型简单地完全适应自己。如果这是真的，我们将永远不需要一个重量级的模型。最近有一些关于使用被称为“教学助手”的中间模型的工作，来帮助建立一个蒸馏的层次结构，以帮助学生网络更好地学习。从解决边缘模型漂移的逻辑角度来看，似乎中间网络可以满足教师和学生之间的一系列资源需求。

当然，对于较小的漂移来说，这更有意义，甚至可能不需要任何新的资源(除了计算能力)。这都是相对的，当人类以人工智能工程师的形式被警告模型突然倒退时，他们不应该调整自己。更有意义的是，我们向这个系统引入新的组件来帮助自动化这个过程。

对于我们在训练和推理阶段之间的区别，有很多批评。这个想法是，训练应该像在大脑中一样一直进行，而不仅仅是一次性的。这和*持续学习者*的学习有些关系。

不幸的是，这并不容易符合硬件的故事。培训被认为是非常计算密集型的，如果没有硬件加速，这种情况是相当没有希望的。但现实并非总是如此:从头开始训练*众所周知是计算密集型的，但我们可能不需要对训练数据进行几十次或几百次检查来适应新的情况。也许有必要重新审视一下利用当前硬件进行边缘培训的想法。*

# *下一代神经系统*

*由于对资源的严格限制，这些问题在今天对 edge 来说是很难解决的。它们在云中的负担要小得多，所以我们为什么没有能够自动调整自己并不断从各种流中摄取数据的系统，这是一个奇怪的问题。嗯，我们有他们，但在我所知道的这样做的组织中，他们的 ML 团队的规模在几十到几百人之间。一个很大的原因是，构建可伸缩的数据处理系统仍然很重要。但更大的原因是，由于各种原因，我们的模型仍然很难训练，这涉及到许多人尝试不同的想法来提高性能。*

*这在手工生成特征的 ML 时代是有意义的。但是深度学习已经改变了这一点，或者至少它承诺会改变。在过去的几十年里，特别是最近的几十年里，我们已经证明了这是可行的。但大多数现实世界的场景都没有受益于一些非常聪明的科学家和多年来对归纳偏差和数据准备策略的研究思考。他们努力的结果是神经结构搜索、超参数优化和迁移学习。这些也在谷歌内部得到了证明，以至于它现在可以作为云服务使用。随着不同数据领域中新形式的无监督预训练方法的出现，深度学习的典型成功可能最终会在跨领域和行业的许多现实世界问题中实现。*

*这些是我们下一代系统的基本要素。*

# *障碍*

*到目前为止，这些元素还没有集成到一个系统中。我们没有实现目标有几个原因，我将简要介绍一下。*

*有些部分似乎不兼容。例如，在通过神经架构搜索找到最佳架构后，如何从预训练的模型中进行迁移学习？有没有软件可以存储模型，当你想建立一个新的模型时可以查询它们，检索最好的模型作为预训练的起点？在将原始数据转换成当前网络可以有效学习的最佳形式方面，是否存在类似的事情？当然，这些算法存在超参数等价物，应该结合模型进行调整吗？我预计我们将及时解决其中的一些问题，这必须由一个在系统工程和深度学习科学方面都具有专业知识的行业研究实验室来推动。*

*运行这样一个系统也有很大的成本，忽略了建设它所必需的前期资本。但是如果我们把视野扩展到时间维度，成本就没那么有趣了。因为每个人都知道，计算成本总是会大幅下降，半个世纪以来一直如此，尽管摩尔定律已经终结，但它仍将继续，而且现在整个全球经济都在推动这一进步。*

*成本也不那么令人感兴趣，因为迁移学习承诺消除大量必要的计算能力。人们只需要训练一个好的基础模型一次，并根据需要多次微调它，成本要小几个数量级。这引发了该领域另一项高级技术的应用:联邦学习。*

*通常情况下，公司拥有不想共享的专有数据。然而，由于对其应用深度学习存在严重成本限制，他们也可能不愿意利用这些数据。但是，如果我们拥有下一代自适应深度学习系统，并具备联合学习的能力，也就是说，在本地进行培训(就像在公司网络内部一样)，并且只向外部世界报告梯度，他们可能会更愿意。具体来说，这可以使多个医疗记录公司或律师事务所联合创建针对他们的问题的最佳模型，将其真正的应用留在下游，以针对他们的具体问题进行微调，而他们不必分享关于该模型的任何信息(其他各方可能也不太关心它，因为它对他们来说只有很小的价值)。联合学习甚至可以进一步降低初始预训练模型的成本。*

*也许我们根本不需要等待更好的计算机硬件。*