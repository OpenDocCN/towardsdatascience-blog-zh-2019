<html>
<head>
<title>Neural Networks from scratch with Numpy — Part 2: Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Numpy 从零开始的神经网络第 2 部分:线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a?source=collection_archive---------7-----------------------#2019-01-25">https://towardsdatascience.com/neural-networks-with-numpy-for-absolute-beginners-part-2-linear-regression-e53c0c7dea3a?source=collection_archive---------7-----------------------#2019-01-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c7f1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在本教程中，您将详细学习使用 Numpy 实现预测的线性回归，并直观了解算法如何逐时段学习。除此之外，您将探索两层神经网络。</h2></div><p id="de41" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上一个教程中，你对感知器有一个非常简单的概述。</p><div class="lb lc gp gr ld le"><a rel="noopener follow" target="_blank" href="/neural-networks-with-numpy-for-absolute-beginners-introduction-c1394639edb2"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">神经网络从零开始与 Numpy:简介</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">在本教程中，您将简要了解什么是神经网络以及它们是如何发展起来的。在…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls lt le"/></div></div></a></div><p id="72b8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，您将深入研究实现一个线性感知器(线性回归),从中您将能够预测问题的结果！</p><p id="e3c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本教程显然会包括更多的数学知识，因为这是不可避免的，但没有必要担心，因为我会解释它们。不管这些，必须认识到，所有的机器学习算法基本上都是最终以代码形式实现的数学公式。</p><p id="0eb3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们开始之前，还记得我们用阈值激活函数来模拟与门和或非门的功能吗？！</p><p id="06c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们将使用另一个极其简单的激活函数，叫做线性激活函数(相当于没有任何激活！).</p><p id="02d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来看看这个激活功能能带来什么样的奇迹吧！</p><h1 id="d977" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">线性激活函数</h1><p id="15ae" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">假设感知器只有一个输入和偏差，如下所示:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/df1f2470cb15e4e2d0a57419a55d5779.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZYS_CQJmhBQlELU8"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Computation graph of Linear Regression</figcaption></figure><p id="bbf4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">得到的线性输出(即和)将是</p><p id="158e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">。这是直线的方程式，如下图所示。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/893671e7f08e2f0ef32fd765847dafa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/0*LQE8GOCX7W-NrkEo.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Graph of Linear Equation</figcaption></figure><p id="0fd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里必须注意，当没有使用激活函数时，我们可以说激活函数是线性的。</p><p id="b7d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个<strong class="kh ir">多元(多变量)线性方程。</strong></p><p id="ca6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看这是如何用于预测下一部分中的<strong class="kh ir"> <em class="nh"> y </em> </strong>的实际输出，即<strong class="kh ir"> <em class="nh">线性回归</em> </strong>。</p><h1 id="c1a9" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">线性回归</h1><p id="95a8" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在<strong class="kh ir"> <em class="nh"> n </em> </strong>维空间中对给定的一组数据拟合一个线性方程称为<strong class="kh ir">线性回归</strong>。下面的 GIF 图像显示了一个线性回归的例子。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="ab gu cl ni"><img src="../Images/2264b7082925c840cdb17db66cedc233.png" data-original-src="https://miro.medium.com/v2/1*eeIvlwkMNG1wSmj3FR6M2g.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Linear Regression [<a class="ae nj" href="https://camo.githubusercontent.com/310f1cd8eb881d776474abb62acf1d17a911c2e4/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a656549766c776b4d4e473177536d6a334652364d32672e676966" rel="noopener ugc nofollow" target="_blank">Source Link</a>]</figcaption></figure><p id="fc94" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简单来说，你试着找出最符合上图所示点集的<strong class="kh ir"> <em class="nh"> m </em> </strong>和<strong class="kh ir"> <em class="nh"> b </em> </strong>的最佳值。当我们获得了可能的最佳拟合时，我们可以预测给定<strong class="kh ir"><em class="nh">×T33】的<strong class="kh ir"> <em class="nh"> y </em> </strong>值。</em></strong></p><p id="2327" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个很通俗的例子就是<em class="nh">房价预测</em>问题。在这个问题中，给你一组值，比如房子的面积和房间的数量等等。你必须预测给定这些价值的房子的价格。</p><p id="ce8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，最大的问题是…预测算法是如何工作的？它是如何学会预测的？</p><p id="78b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们边走边学吧！</p><p id="be4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从导入所需的包开始。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="3b6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您将使用<code class="fe nm nn no np b">sklearn</code>数据集生成器来创建数据集。您还将使用该包将数据分为训练数据和测试数据。如果你不知道<code class="fe nm nn no np b">sklearn</code>，它是一个丰富的包，有很多机器学习算法。尽管您获得了用于执行线性回归的预建函数，但在本教程中，您将从头开始构建它。</p><p id="e7c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了创建数据集，您必须首先设置一个超参数列表，而<strong class="kh ir"> <em class="nh"> m </em> </strong>和<strong class="kh ir"> <em class="nh"> b </em> </strong>是参数、样本数量、输入特征数量、神经元数量、学习速率、训练的迭代次数/次数等。被称为超参数。在实现算法时，您将了解这些超参数。</p><p id="5e5f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，您应该设置训练样本的数量、输入特征的数量、学习速率和时期。你很快就会明白学习的速度和时代。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a0a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您的第一个任务是导入或生成数据。在本教程中，您将使用<code class="fe nm nn no np b">sklearn</code>的<code class="fe nm nn no np b">make_regression</code>函数生成数据集。</p><p id="8e38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">出于学习的目的，我们将保持特征的数量最少，以便于可视化。因此，您必须只选择一个特征。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6ec0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，是时候想象一下数据生成器做了什么了！</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/f84d5c4e1a65284b6b733cab9d4f959a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*HpraXuiV2NrrDrkM.png"/></div></figure><p id="66f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查矢量形状的一致性。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="f806" class="nv lv iq np b gy nw nx l ny nz">Shape of vector X: (200, 1) Shape of vector y: (200,)</span></pre><p id="6dc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们需要将<strong class="kh ir"> <em class="nh"> y </em> </strong>的大小重置为<code class="fe nm nn no np b">(200, 1)</code>，这样我们就不会在向量乘法过程中出错。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="e85b" class="nv lv iq np b gy nw nx l ny nz">(200, 1)</span></pre><p id="763b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，您必须将数据集分为定型集和测试集，以便在定型模型后，可以使用数据集的一部分来测试回归模型的准确性。</p><p id="5dde" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们将数据分为训练集和测试集。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="cdcb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，训练集是 80%，测试集是 20%。</p><p id="f4a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查所创建的训练和测试数据集的形状。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="5ef3" class="nv lv iq np b gy nw nx l ny nz">(160, 1) (160, 1) (40, 1) (40, 1)</span></pre><p id="d19b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，80%的数据，即 200 个数据点中的 80%是正确的 160。</p><blockquote class="oa ob oc"><p id="8b55" class="kf kg nh kh b ki kj jr kk kl km ju kn od kp kq kr oe kt ku kv of kx ky kz la ij bi translated">那么，到目前为止我们取得了什么成就？</p></blockquote><p id="2f59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经完成了最初的<strong class="kh ir">数据预处理</strong>，并且<strong class="kh ir">还通过可视化对数据</strong>进行了探索。这通常是建模任何机器学习算法的第一步。我们还拆分了数据，以便在模型训练完成后测试其准确性。</p><blockquote class="oa ob oc"><p id="ddad" class="kf kg nh kh b ki kj jr kk kl km ju kn od kp kq kr oe kt ku kv of kx ky kz la ij bi translated">我们下一步做什么？</p></blockquote><p id="1db2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">清晰的如上面的线性回归 GIF 图所示，我们首先需要考虑一条随机线，然后通过训练将其拟合到数据上。</p><p id="3b33" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，下一步是随机<strong class="kh ir">生成一条具有随机斜率和截距(偏差)</strong>的直线。目标是实现与生产线的最佳匹配。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="6520" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，给定<strong class="kh ir"><em class="nh">m</em></strong>&amp;<strong class="kh ir"><em class="nh">b</em></strong>，我们就可以绘制生成的直线了。</p><p id="70bd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们更新函数<code class="fe nm nn no np b">plot_graph</code>来显示预测线。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/73b5c285a163f04bf6c907d7b622bed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*bh8kfO0qQDojVuPV.png"/></div></figure><p id="9d98" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于现在已经生成了直线，您需要预测它为给定的<strong class="kh ir"> <em class="nh"> x </em> </strong>值产生的值。根据这个值，我们要做的就是计算它们的均方误差。为什么？</p><blockquote class="oa ob oc"><p id="e477" class="kf kg nh kh b ki kj jr kk kl km ju kn od kp kq kr oe kt ku kv of kx ky kz la ij bi translated">我们怎样才能发现实际产量和预测产量之间的差异呢？</p></blockquote><p id="5232" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最简单的方法是减去这两个差值。我们有一条随机线，它为每个给定的<strong class="kh ir"> <em class="nh"> x </em> </strong>给出一个输出<code class="fe nm nn no np b">y_pred</code>，但它肯定不是实际输出。幸运的是，我们也有所有<strong class="kh ir"> <em class="nh"> x </em> </strong>的实际输出！因此，我们要做的不是直接取差(技术上称为绝对距离或 L1 距离)，而是将其平方(称为欧几里德距离或 L2 距离)，并取所有给定点的平均值&amp;这称为<em class="nh">均方误差</em>。</p><p id="cf9d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们通过定义一个函数<code class="fe nm nn no np b">forward_prop</code>根据参数<strong class="kh ir"><em class="nh">m</em></strong>&amp;<strong class="kh ir"><em class="nh">b</em></strong>来预测<code class="fe nm nn no np b">y_pred</code>的值。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="160c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">成本/损失函数</h1><p id="bf9e" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">如前所述，现在您已经得到了<code class="fe nm nn no np b">X_train</code>的相应值和<code class="fe nm nn no np b">y_pred</code>的预测值，您将计算成本/误差/损失函数。</p><p id="41c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nh">损失(均方误差)</em>为:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/72dd6d36a172d78a24d99ae0c348ef93.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/0*QY2A9VTA3eHK4cjy"/></div></figure><p id="6222" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对所有<em class="nh"> M </em>例求和，得到<em class="nh">损失 fn。</em>如下:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/f7f3a8c55babff5f8058a287a5f252ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/0*YqV_WGROudwCDptA"/></div></figure><p id="4959" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的目标是将<em class="nh">损失</em>明显降至最低，使回归线预测更准确。</p><p id="00b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们现在就把它编纂成法典。</p><p id="4c1a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您还可以保存<strong class="kh ir"> <em class="nh">损耗</em> </strong>的每个值，这些值将被计算出来，以图形方式显示它在训练中的变化。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="c56d" class="nv lv iq np b gy nw nx l ny nz">4005.265725705774</span></pre><p id="624f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们修改上面定义的<code class="fe nm nn no np b">plot_graph</code>函数来绘制损失。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="dca2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您会看到由参数<strong class="kh ir"> <em class="nh"> m </em> </strong>和<strong class="kh ir"> <em class="nh"> b </em> </strong>创建的直线。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/af1cf1ebf68f66d3ff07b8fe01d33340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K2zwEtYuIHgK8aeb.png"/></div></div></figure><p id="c489" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然你已经计算了损失，让我们把它减到最小。</p><h1 id="e813" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">线性回归的梯度下降法</h1><p id="7f44" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">由于<em class="nh">损失</em>是因变量而<strong class="kh ir"><em class="nh">m</em></strong>&amp;<strong class="kh ir"><em class="nh">b</em></strong>是自变量，我们将不得不更新<strong class="kh ir"><em class="nh">m</em></strong>&amp;<strong class="kh ir"><em class="nh">b</em></strong>以便找到最小的<strong class="kh ir"> <em class="nh">损失</em> </strong>。</p><p id="acdf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，迫在眉睫的问题是…</p><blockquote class="oa ob oc"><p id="1113" class="kf kg nh kh b ki kj jr kk kl km ju kn od kp kq kr oe kt ku kv of kx ky kz la ij bi translated">如何更新参数<strong class="kh ir"> m </strong>和<strong class="kh ir"> b </strong>？</p></blockquote><p id="65c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，让我们仅考虑如下所示的单个参数<strong class="kh ir"> <em class="nh"> p </em> </strong>，并让<strong class="kh ir"><em class="nh"/></strong><em class="nh">(目标)</em>成为必须预测的值。我们看到，随着<strong class="kh ir"> <em class="nh">成本</em> </strong>收敛到最小值，参数<strong class="kh ir"> <em class="nh"> p </em> </strong>达到一个称为最优值的特定值。假设<strong class="kh ir"> <em class="nh"> p </em> </strong>的最优值为<strong class="kh ir"> <em class="nh"> a </em> </strong>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="ab gu cl ni"><img src="../Images/e86a33236e53de634212aba3251bb2b4.png" data-original-src="https://miro.medium.com/v2/1*pwPIG-GWHyaPVMVGG5OhAQ.gif"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Gradient Descent w.r.t <strong class="bd oj">p. </strong>[<a class="ae nj" href="https://camo.githubusercontent.com/a401a48f5503c52004369148a784e779aa7e3411/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f313630302f312a70775049472d475748796150564d564747354f6841512e676966" rel="noopener ugc nofollow" target="_blank">Source Link</a>]</figcaption></figure><p id="b8e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以从这张图表中观察到一些情况。</p><p id="09be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图中可以清楚地看到，随着<strong class="kh ir"> <em class="nh"> p </em> </strong>向<strong class="kh ir"><em class="nh">【a(minima)</em></strong>移动，成本降低，随着远离它，成本增加。</p><p id="02e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如何才能让<strong class="kh ir"> <em class="nh"> p </em> </strong>朝着<strong class="kh ir"><em class="nh"/></strong>a 移动，而不管是如图所示的<strong class="kh ir"> <em class="nh"> a </em> </strong>的左边还是右边？</p><p id="9b5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们考虑曲线的<strong class="kh ir"> <em class="nh"> p </em> </strong>。从微积分中我们知道，一条曲线在一点的<strong class="kh ir">斜率 </strong>由<strong class="kh ir">d<em class="nh">y/</em>d<em class="nh">x</em>T13】给出(这里是<strong class="kh ir">d<em class="nh">L/</em>d<em class="nh">p</em></strong><em class="nh"/>其中<strong class="kh ir"> <em class="nh"> L →损耗</em> </strong>)。从图中可以看出，当<strong class="kh ir"> <em class="nh"> p </em> </strong>在<strong class="kh ir"> <em class="nh"> a </em> </strong>的左边时，<strong class="kh ir"> <em class="nh">斜率</em> </strong>显然是<strong class="kh ir"><em class="nh"/></strong>而当它在右边时，<strong class="kh ir"> <em class="nh">斜率</em> </strong>就会是<strong class="kh ir"> <em class="nh"> +ve </em> </strong>。但是我们看到，如果<strong class="kh ir"> <em class="nh"> p </em> </strong>在<strong class="kh ir"> <em class="nh"> a </em> </strong>的左边，就必须给<strong class="kh ir"> <em class="nh"> p </em> </strong>加一些值。同样，当<strong class="kh ir"> <em class="nh"> p </em> </strong>在<strong class="kh ir"> <em class="nh"> a </em> </strong>的右边时，必须减去一些值。</strong></p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/2612ac2a0891863595015e5c5d40c67a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*R2tiN4d9QhsEdBA7.jpg"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Source: <a class="ae nj" href="https://imgflip.com/i/2dz47q" rel="noopener ugc nofollow" target="_blank">https://imgflip.com/i/2dz47q</a></figcaption></figure><p id="89b5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就意味着当<strong class="kh ir"><em class="nh"/></strong><em class="nh"/>是<em class="nh"/><strong class="kh ir"><em class="nh">ve</em></strong><em class="nh"/>时就隐含着<strong class="kh ir"> <em class="nh"> p = p +(有些 val。)</em> </strong>和当<strong class="kh ir"> <em class="nh">斜率</em> </strong> <em class="nh">是</em><strong class="kh ir"><em class="nh">+ve</em></strong><em class="nh"/>寓意<strong class="kh ir"><em class="nh">p = p ‖(有些 val。)</em> </strong>向<strong class="kh ir"> <em class="nh">移动一个</em> </strong>。</p><p id="e89f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> ∴ </strong>我们从<strong class="kh ir"> <em class="nh"> p </em> </strong>中减去<strong class="kh ir"> <em class="nh">斜率</em> </strong>。这样，<strong class="kh ir"> <em class="nh">斜率</em> </strong>被取反，确保其始终向<strong class="kh ir"> <em class="nh"> a </em> </strong>移动。得到的等式将是，</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/41961246db77e9ce28ebe16077191647.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/0*zfxH9ExbXp7s3_4S"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi om"><img src="../Images/8f52b539489c99b51e74dae1d30b6314.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/0*iUnAJW3Ed1_6kZTd"/></div></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/cfbd07606f3e21b482c5228c38e311c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/0*r-WLMj0WOIsd1dVQ"/></div></figure><p id="e6ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还必须观察到，如果<strong class="kh ir"> <em class="nh">成本</em> </strong>过高，则<strong class="kh ir"> <em class="nh">斜率</em> </strong>也会过高。于是，当从<strong class="kh ir"><em class="nh"/></strong><strong class="kh ir"><em class="nh">p</em></strong>值中减去<strong class="kh ir"><em class="nh"/></strong>斜率时，可能会过冲<strong class="kh ir"><em class="nh"/></strong>。因此，有必要减小<strong class="kh ir"> <em class="nh">斜率</em> </strong>的值，使得<strong class="kh ir"> <em class="nh"> p </em> </strong>不会过冲<strong class="kh ir"> <em class="nh"> a </em> </strong>。因此，我们给<strong class="kh ir"><em class="nh"/></strong>斜率引入一个叫做<strong class="kh ir"> <em class="nh">学习率【α】</em></strong>的阻尼因子。稍后你会看到，通过改变<strong class="kh ir"><em class="nh"/></strong>α，误差下降的速率会发生变化。</p><p id="4832" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们最终得到的是，</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/817f97fac7aee7acf2c1e07890305e9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/0*t4RRJ5gRSnpla5vd"/></div></figure><p id="fc1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如图所示，<strong class="kh ir"> <em class="nh"> p </em> </strong>对<strong class="kh ir"> <em class="nh"> cost </em> </strong>所走的轨迹是一条钟形曲线。</p><p id="7f24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法叫做<strong class="kh ir">梯度下降</strong>。</p></div><div class="ab cl op oq hu or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="ij ik il im in"><p id="c203" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的例子中，我们使用两个参数<strong class="kh ir"> <em class="nh"> m </em> </strong>和<strong class="kh ir"> <em class="nh"> b </em> </strong>。因此，钟形曲线将是<em class="nh"> 3- </em>尺寸，如下图所示。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ow"><img src="../Images/c6a5b40f184d1e8609a4ca71f222cb2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Jz3amoEa5RKp_-Kh.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Gradient Descent w.r.t to parameters <strong class="bd oj">m</strong> and <strong class="bd oj">b</strong>.[<a class="ae nj" href="https://camo.githubusercontent.com/b9d3586045436ffb04c29c82923bec26d494eb1c/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f4f3972635a566d5263454771492f67697068792e676966" rel="noopener ugc nofollow" target="_blank">Source</a>]</figcaption></figure><p id="6348" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，您将计算损失函数 w.r.t 对参数<strong class="kh ir"><em class="nh">m</em></strong>&amp;<strong class="kh ir"><em class="nh">b</em></strong>的偏导数。【<strong class="kh ir">注:</strong>通常期望你知道偏导数的基本概念。然而，如果你没有，你可以参考这个奇妙的<a class="ae nj" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction" rel="noopener ugc nofollow" target="_blank">可汗学院视频</a></p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/ca33b06aafadc92b4cd16da5ed6537f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/0*hks-gAJeD1bPEiny"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/94c74889f0cc260ce207bb6904e560ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:26/0*XWXoT9UchQqqi0N6"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/5c3774814f0487ddfb072a23d8cec89f.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/0*8I9mP0Mphg1A4fWg"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9ee2192c918d3daf852b9f167021932d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*fDI_kdjjDRTXxeSD"/></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Source: <a class="ae nj" href="https://i.chzbgr.com/full/2689942016/h8B269F68/" rel="noopener ugc nofollow" target="_blank">https://i.chzbgr.com/full/2689942016/h8B269F68/</a></figcaption></figure><h1 id="3747" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">更新参数</h1><p id="d296" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">现在我们从它们各自的导数中减去参数<strong class="kh ir"> <em class="nh"> m </em> </strong>和<strong class="kh ir"> <em class="nh"> b </em> </strong>的斜率以及阻尼因子<strong class="kh ir"> <em class="nh"> α(alpha) </em> </strong>。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/d256a962318d1fdfb770f20f3e208c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/0*L9w9vuysW4idDLrR"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/aea8da99deae0dd1feb73992574151c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/0*_Bd7cWn7GUiOBdIl"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="a165" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从减小<strong class="kh ir"> <em class="nh"> m </em> </strong>和<strong class="kh ir"> <em class="nh"> b </em> </strong>的值开始，它们逐渐向最小值移动。因此，以这种方式更新参数必须进行多次迭代，这被称为<strong class="kh ir"> <em class="nh">时期</em> </strong>。</p><p id="6333" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们定义一个函数<code class="fe nm nn no np b">grad_desc</code>，它同时调用<code class="fe nm nn no np b">gradient</code>和<code class="fe nm nn no np b">update_params</code>。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="e01a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在已经定义了我们需要的一切，所以让我们把所有的函数编译成一个，看看我们的算法是如何工作的。因此，在实际运行代码之前，您必须设置超参数。</p><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6c9327678b5d8799162ddaa2f076783c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*ueb2TbtHsoP6W-ie.png"/></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="7485" class="nv lv iq np b gy nw nx l ny nz">Epoch: 0 <br/>Loss = 2934.082243250548</span><span id="affd" class="nv lv iq np b gy pc nx l ny nz">Epoch: <br/>10 Loss = 1246.3617292447889</span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/f5e4a08700c7fb8e30d3a74bcd86d679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7pJeL8W7gS5sLtA5.png"/></div></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="2b74" class="nv lv iq np b gy nw nx l ny nz">Epoch: 20 <br/>Loss = 546.310951004311</span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/d30b133cb6007837a51bbc9a8f3abd2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*N5whTnoINapcGvLx.png"/></div></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="09bf" class="nv lv iq np b gy nw nx l ny nz">Epoch: 30 <br/>Loss = 255.88020867147344</span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/0c138b2940b326c9bf3fd066ed48084e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tr39EhDVUQUqk8km.png"/></div></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="a370" class="nv lv iq np b gy nw nx l ny nz">Epoch: 40 <br/>Loss = 135.36914932067438</span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/2e75ee74d89323487e136f108d9c9b5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OEnX2ft7i6McBuXD.png"/></div></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="4810" class="nv lv iq np b gy nw nx l ny nz">Epoch: 50 <br/>Loss = 85.35744394597806</span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/158fdef6f62f9165647e91ec4bedcef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0sm0i6n_fngJKRWy.png"/></div></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="8f4f" class="nv lv iq np b gy nw nx l ny nz">Epoch: 60 <br/>Loss = 64.60029693013243</span></pre><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi oi"><img src="../Images/ba3c4632e127a029ddc8ec260de93987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cd1yc_QQDPwp_umK.png"/></div></div></figure><p id="1f2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于您已经训练了 60 个时期的参数，并且回归线看起来符合数据，您可以前进到最后一个阶段，即预测我们的测试数据并检查准确性。</p><h1 id="4e1e" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">预言；预测；预告</h1><p id="f417" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">为了检查准确性，您可以取所有测试数据点的百分比误差的平均值。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/f24776954b4ce0602fcdc28a0a70c675.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/0*W-n40aFixcPhy91I"/></div></figure><figure class="ms mt mu mv gt mw"><div class="bz fp l di"><div class="nk nl l"/></div></figure><pre class="ms mt mu mv gt nr np ns nt aw nu bi"><span id="8c9a" class="nv lv iq np b gy nw nx l ny nz">Prediction: <br/>Loss = 56.53060443946197 <br/>Accuracy = 80.1676%</span><span id="ab09" class="nv lv iq np b gy pc nx l ny nz">Hence <br/>m = 82.34083095217943 <br/>b = 0.46491578390750576</span></pre><p id="8f27" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上图所示，考虑到数据中的差异，准确度为 80%,这是“可以的”。</p><p id="93ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望在文章中引入一些真正有趣的东西，作为奖励，我还加入了神经网络的介绍。但这肯定伴随着一个陷阱！</p><h1 id="4a7c" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">具有线性激活函数的两层神经网络</h1><p id="dbcb" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">神经网络如下所示。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/316e321ab573b575ae587cfafc8af6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gy9PEwXMInUjR0g4"/></div></div></figure><p id="3fb0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从图像中，我们观察到第一层中的两个神经元各有两个输入，第二层中有一个输出神经元。</p><p id="7c09" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用矩阵来表示上述方程。我们可以用向量(单列矩阵)形式将它们表示为:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/26886361a2e88da7af2bb70eaeeb8679.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/0*VgOTW_X5XsDhSizY"/></div></figure><p id="8b9b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在进行矩阵计算时，我们需要考虑维数和乘法。因此，我们重新排列一位以达到所需的输出。</p><p id="68c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">方程的展开是不需要的，因此让我们坚持</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/0bce3a39a3c8933b8be9424a3d899e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/0*21UMoI-Sj-K1j0B-"/></div></figure><p id="b760" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，的价值</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/6f65aa4859eb561787e06ef15688c8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/0*MRu-lW-IyOZnBscd"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/90fcd90ef2178d518c338dc5ea330a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/0*w5iRdKKV_G2yttGF"/></div></figure><p id="71a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，2ⁿᵈ图层的输出将是:</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/a13f3bad2e5310723c376044cefdc0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/0*aFM7Bx_Jy8xsC4kW"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/08ba26d9d74f0c17591b534e46a6cfe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/0*haHxK2bPIVjmM4HW"/></div></figure><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/ae850e42c7ec8247754304d77b0153ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/0*2mC41vPSZgTrjE2z"/></div></figure><p id="113a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的一组方程中，我们看到一个具有线性激活函数的神经网络简化为一个<em class="nh">线性方程</em>。</p><p id="c8cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络的整个目的是创建一个非常复杂的函数，可以适应任何类型的数据，正如可以清楚地看到的那样，具有线性激活函数的神经网络没有达到目的。<strong class="kh ir">因此，需要严格注意的是，线性函数不能用作神经网络的激活函数</strong> <em class="nh">，尽管它只能用于回归问题的最后一层</em>。</p><p id="c85e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那么我想你必须耐心等待下一个教程来实现它！</p><h1 id="41df" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak">这里是 Jupyter 笔记本中完整实现的链接:</strong></h1><blockquote class="pk"><p id="caef" class="pl pm iq bd pn po pp pq pr ps pt la dk translated">继续克隆它，并开始在你的 Colab 上运行细胞，看看<strong class="ak">梯度下降</strong>的奇迹！！</p></blockquote><div class="pu pv pw px py le"><a href="https://github.com/SurajDonthi/Article-Tutorials/blob/master/NN%20with%20Numpy%202/Neural_Networks_for_Absolute_Beginners_Part_2_Linear_Regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">SurajDonthi/文章-教程</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">神经网络相关的文章会贴在这里。-SurajDonthi/文章-教程</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">github.com</p></div></div><div class="ln l"><div class="pz l lp lq lr ln ls lt le"/></div></div></a></div><h1 id="7d64" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">结论</h1><p id="fb8b" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">在本教程中，您学习了</p><ol class=""><li id="ee2a" class="qa qb iq kh b ki kj kl km ko qc ks qd kw qe la qf qg qh qi bi translated">线性激活函数执行回归任务，即学习预测和预报值。这种方法到处都叫<em class="nh">线性回归</em>。</li><li id="b3f4" class="qa qb iq kh b ki qj kl qk ko ql ks qm kw qn la qf qg qh qi bi translated">具有线性激活函数的 MLP(多层感知器)简化为正常的线性回归任务。因此，线性激活不能用于网络的隐藏层。但是，它可以在最后一层用于回归/预测任务。</li></ol><p id="a24c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一个教程中，您将学习 Sigmoid 激活函数并执行逻辑回归，这是实现神经网络的最重要的关键。</p><p id="c8a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在这里阅读下一篇关于逻辑回归的文章:</p><div class="lb lc gp gr ld le"><a rel="noopener follow" target="_blank" href="/neural-networks-with-numpy-for-absolute-beginners-part-3-logistic-regression-18b474096a4e"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">用 Numpy 从零开始的神经网络第 3 部分:逻辑回归</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">sigmoid 激活函数是神经网络中最基本的概念。在本教程中，您将学习…</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">towardsdatascience.com</p></div></div><div class="ln l"><div class="qo l lp lq lr ln ls lt le"/></div></div></a></div></div><div class="ab cl op oq hu or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="ij ik il im in"><p id="1d63" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你在做什么很酷的深度学习项目吗？</p><p id="4dbf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在 Linkedin 上联系我:</p><div class="lb lc gp gr ld le"><a href="https://www.linkedin.com/in/suraj-donthi/" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">Suraj Donthi | LinkedIn</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">Suraj Donthi 是一名计算机视觉顾问|机器学习和深度学习实践者和培训师。和他联系...</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">www.linkedin.com</p></div></div><div class="ln l"><div class="qp l lp lq lr ln ls lt le"/></div></div></a></div><p id="a879" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如有任何疑问，请在 Twitter 上给我发消息:</p><div class="lb lc gp gr ld le"><a href="https://twitter.com/suraj_donthi" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">Suraj Donthi (@suraj_donthi) |推特</h2><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">twitter.com</p></div></div><div class="ln l"><div class="qq l lp lq lr ln ls lt le"/></div></div></a></div></div></div>    
</body>
</html>