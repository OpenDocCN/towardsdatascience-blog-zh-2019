<html>
<head>
<title>“Attention-guided Network for Ghost-free High Dynamic Range Imaging” (CVPR 2019) — Paper review</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“用于无重影高动态范围成像的注意力引导网络”(CVPR，2019)——论文综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-review-attention-guided-network-for-ghost-free-high-dynamic-range-imaging-4df2ec378e8?source=collection_archive---------19-----------------------#2019-09-18">https://towardsdatascience.com/paper-review-attention-guided-network-for-ghost-free-high-dynamic-range-imaging-4df2ec378e8?source=collection_archive---------19-----------------------#2019-09-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fcbd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">HDR 重建的最新进展</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ca93a6badc7cc9435fabb9e1ba4c43ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XQTvMhuOvww-TJ8X.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Source: <a class="ae kv" href="https://donggong1.github.io/ahdr" rel="noopener ugc nofollow" target="_blank">Attention-guided Network for Ghost-free High Dynamic Range Imaging (AHDRNet)</a></figcaption></figure><blockquote class="kw kx ky"><p id="691f" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><em class="iq">“你只是想要关注，你不想要我的心”——查理·普斯</em></p></blockquote><h1 id="38bf" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">关于报纸</h1><p id="a9c1" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated"><em class="lb">用于无重影高动态范围成像的注意力引导网络</em> (AHDRNet)是目前 HDR 使用包围曝光图像生成图像的最先进技术。它出现在 2019 年 CVPR，可以在这里阅读<a class="ae kv" href="https://arxiv.org/abs/1904.10293" rel="noopener ugc nofollow" target="_blank"/>。第一作者，<a class="ae kv" href="https://donggong1.github.io/" rel="noopener ugc nofollow" target="_blank">董公</a>是阿德莱德大学的博士后研究员。他的兴趣包括机器学习和计算机视觉中的优化。</p><blockquote class="kw kx ky"><p id="2d28" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated"><strong class="lc ir"> <em class="iq">注</em> </strong> <em class="iq">:本帖使用的图片结果、网络表示、公式、表格均来源于</em> <a class="ae kv" href="https://arxiv.org/abs/1904.10293" rel="noopener ugc nofollow" target="_blank"> <em class="iq">论文</em> </a> <em class="iq">。</em></p></blockquote><p id="877d" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">在我们开始之前，我建议你看看我播客中的一个片段，我们在这里讨论了计算摄影和 HDR 成像的现状。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h1 id="6846" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">埃尔问题 a</h1><p id="cd2e" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">为了从多重曝光包围的 LDR 图像生成 HDR 图像，LDR 图像的对准对于具有帧内运动的动态场景非常重要。合并前未考虑未对准，导致重影(以及其他)伪像。已经有几次成功的(几乎)尝试通过使用光流估计来补偿帧之间的这种运动。事实证明，基于流量的方法的缺点并没有很好地服务于 HDR 事业。</p><p id="3fd2" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">这可以从<a class="ae kv" href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener ugc nofollow" target="_blank">卡兰塔里等人的一次尝试中看出。al </a>，其中不考虑饱和图像区域中的精确空间重建，对于具有严重运动的输入帧，可以观察到对准伪像。这可以从 AHDRNet 的作者在下图中提供的结果中看出。专门针对高度动态的的重建的另一个尝试包括输入(<a class="ae kv" href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener ugc nofollow" target="_blank"> Wu et .al </a>)声称利用 CNN 架构在学习失调和补偿鬼影伪像方面的明显实力。然而，以下结果表明仍有改进的余地。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/328614e70d72aa3c68221d757d6eb269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*30oeBm3mjtt2UsDCSqu0gA.png"/></div></div></figure><h1 id="aa5e" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">注意营救</h1><p id="6702" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">作者建议使用注意机制来解决这一双面问题，即通过使用注意机制来减少对准伪影+精确的 HDR 重建。如果你想一想，注意力网络只是堆叠在一起的几个 Conv 层，然后(通常)是一个 sigmoid 激活，允许网络专注于重要的和相关的应用程序。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/5c6de98616da23976cf9159e096ade11.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*kOhxdmePmBQfgrhhKMiwwA.png"/></div></figure><p id="f693" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">这里，注意力网络被用于抑制对准伪影，并且通过关注括号中的图像相对于参考图像的空间动态，集中于将曝光更好的图像区域注入到生成的图像中。对应于参考图像的区域被突出显示，而具有严重运动和饱和度的区域被抑制。我们将会看到注意力信息矩阵是如何被处理和实现的，就其背后的数学而言。</p><p id="4495" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">网络的注意力部分集中于决定哪些图像区域对网络输出的准确性有更好的贡献。接下来是一个基于注意力输出的合并网络，试图从 LDR 输入中创建 HDR 内容。注意力机制越好，对合并网络的输入就越好，从而允许它利用输入的更相关部分中的信息。使用扩展的密集残差块开发了合并网络，这改进了梯度流、分层学习和收敛。整个网络以端到端的方式被训练，因此两个子网络相互影响，并且一起学习。</p><h1 id="333a" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">履行</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/9cee8c3282006fe61c38d24a39021d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IJfbHMvhiFWC5yQe6PCxxg.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Overview</figcaption></figure><h2 id="c9d0" class="nb lx iq bd ly nc nd dn mc ne nf dp mg mq ng nh mi ms ni nj mk mu nk nl mm nm bi translated">预处理</h2><p id="2586" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">非线性 LDR 输入(<em class="lb"> I1，I2，I3 </em>)通过应用逆 CRF(这里是伽马校正)并除以它们相应的曝光时间而被转移到线性域。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2649360f8252ae059e07f2c654543885.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*iy2Do0TiEa70L4ZZOrUU1A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">γ = 2.2</figcaption></figure><p id="9342" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">线性和非线性输入(<em class="lb"> Ii，Hi </em>)沿着信道维度被连接以形成<em class="lb"> Xi </em>。<em class="lb"> X1 </em>、<em class="lb"> X2 </em>和<em class="lb"> X3 </em>馈入网络产生相应的 HDR 输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/412cfae9d4695097046f23ae0d304f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*MgAQuh0sfypm-2aEUM0g6A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><em class="np">H</em> is the generated image, <em class="np">f</em> (.) represents the AHDRNet network and <em class="np">θ</em> the network parameters</figcaption></figure><p id="1f9e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">当网络拥有可供其支配的线性化输入信息时，它表现得更好。这已在<a class="ae kv" href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener ugc nofollow" target="_blank"> Kalantari 等人的研究中得到观察和利用。艾尔</a>以及<a class="ae kv" href="https://arxiv.org/pdf/1711.08937.pdf" rel="noopener ugc nofollow" target="_blank">吴等人。艾尔</a>。</p><h1 id="b18a" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">体系结构</h1><p id="4b79" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">整个网络由两个子网络组成——注意网络和融合网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/71500cb9d630e0d21a9cdb17d1fa246f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHnHzLbQz58BHcRMFhOd1w.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Attention network</figcaption></figure><p id="3cf4" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">如上所述，注意力网络通过突出显示和使用来自与参考图像相对应的相邻图像(非参考图像)中的区域的信息，有助于避免对准伪影。它通过以下方式实现。</p><p id="e5f1" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">注意力不是从连接的图像对中提取的，而是直接应用于连接的图像对。首先将<em class="lb"> Xi </em>通过 Conv 层提取 64 通道特征图<em class="lb">子</em>。</p><p id="3cb7" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">然后，参考特征图(<em class="lb"> Z2 </em>或<em class="lb"> Zr </em>)连同相邻图像特征图(<em class="lb"> Z1 </em>和<em class="lb"> Z3 </em>)被馈送到关注模块，该关注模块参照<em class="lb"> Zr </em>生成关注图来标记非参考特征图中的重要区域。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/d1aff2e9a3d33443899f9a91c6a5d51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/0*TeylpymAABwM2B6X.png"/></div></figure><p id="7d98" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">这是对两对(<em class="lb"> Z1 </em>、<em class="lb"> Z2 </em>)和(<em class="lb"> Z3 </em>、<em class="lb"> Z2 </em>)进行的。这在上面的网络表示中可以看得很清楚。</p><p id="a077" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">在我们进入注意力模块做什么之前，让我们看看如何处理生成的注意力地图。生成的注意力图本质上是一个包含[0，1]之间的值的 64 通道矩阵。该矩阵用作一种权重矩阵，其中每个元素表示相邻图像的特征矩阵中相应元素的重要性，参考<em class="lb"> Z2 </em>。这是通过使用从(<em class="lb"> Z1 </em>，<em class="lb"> Z2 </em>)生成的注意力图，通过进行注意力图和<em class="lb"> Z1 </em>的逐元素乘法来获得<em class="lb">Z’1</em>来实现的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5bae0c20c2fbdf375fb9ca6e4e35d889.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/0*AyY92BJFOzyEAatL.png"/></div></figure><p id="7cdd" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">该操作导致<em class="lb"> Z1 </em>中的重要特征(关注度更接近 1)获得较高的数值，而不太重要的特征获得相应的较低值。这仅表现在来自<em class="lb"> Z1 </em>的重要图像区域，其在网络中前进以对最终的 HDR 输出做出贡献。同样的事情发生在(<em class="lb"> Z3 </em>，<em class="lb"> Z2 </em>)之间得到<em class="lb">Z’3</em>。</p><p id="beb8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">现在我们有了所有与构建 HDR 图像最相关的输入片段，我们沿着通道维度将它们连接起来，如下所示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/2c1d3176216bde1e7346788ab1023800.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/0*oulHopStLvaePUth.png"/></div></figure><h2 id="058c" class="nb lx iq bd ly nc nd dn mc ne nf dp mg mq ng nh mi ms ni nj mk mu nk nl mm nm bi translated">注意模块</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/03f62c739dd1a04c404a4f33743eb94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/0*VJ5ooOqafNOC5v7O.png"/></div></figure><p id="1bde" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">我们来看看这些注意力地图是如何产生的。本文中使用的注意模块由 2 个 Conv2d 层组成，输出 64 通道矩阵，之后分别是 ReLU 和 sigmoid 激活。它将相邻和参考图像(2×3 = 6 个通道)的连接特征向量作为输入。最终，sigmoid 激活用于将输出包含在[0，1]范围内。</p><h2 id="116a" class="nb lx iq bd ly nc nd dn mc ne nf dp mg mq ng nh mi ms ni nj mk mu nk nl mm nm bi translated">关注结果</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/473cbc13c7c0f5559f5dfd6f4e8d3df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*op6pjaA5-eo-eecZ.jpg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Attention map examples from the paper. ( a ) to ( c )— Alignment attention; ( d ) to ( f ) — Exposure attention</figcaption></figure><p id="bfaf" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">在(a)至(c)中，从上述结果可以观察到，非参考图像中具有运动差异的区域是如何被抑制的(深蓝色区域)，而与参考图像具有对应关系的区域被突出显示(较亮的蓝绿色)。在(d)到(f)中，在相邻帧中曝光较好的区域被突出显示，而饱和区域被抑制。</p><h1 id="ecd4" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">合并网络</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/4d906d766f61b8cf7ac66b0bcdac4116.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*nq0yC0HIakfLZcdZ.jpg"/></div></div></figure><p id="f2b8" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">级联的特征图(<em class="lb"> Zs </em>)作为输入被提供给合并网络。作者所用的归并网络是张等人提出的剩余稠密网络。艾尔。代替传统的 Conv 操作，作者使用了扩张卷积来传播更大的感受野，因此称之为扩张残余致密块(DRDB)。在合并网络中有 3 个这样的块，它们由基于密集级联的跳跃连接和剩余连接组成，已经证明它们对于 CNN 在解决梯度消失方面非常有效，允许更好的反向传播、分级学习，并因此帮助和改善收敛性能。在建议的 AHDRNet 网络中，每个 DRDB 包括 6 个 Conv 层，增长率为 32。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/c237f33a7c7d24e6193629b67b2625e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1ZrNm30PPxTHo9Iw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">DRDB</figcaption></figure><p id="3da0" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">作者还采用了局部和全局剩余跳跃连接，将低级特征旁路到高级特征。局部残差学习在 DRDBs 内实现，而全局残差学习用于将包含<em class="lb">纯</em>信息的浅层特征图从参考图像转移到后面的阶段。这一点以及其他网络规格可以在合并网络图中观察到。</p><h1 id="53b5" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">损失函数</h1><p id="f08e" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">就像<a class="ae kv" href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener ugc nofollow" target="_blank">卡兰塔里 et 一样。al </a>，在<em class="lb"> μ </em>定律色调映射生成的图像和色调映射的地面真实图像之间计算损失。所有实验的<em class="lb"> μ </em>都被设置为 5000。<em class="lb"> μ </em>定律可以定义为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/314bcbf755b64fc4c26c577d9a5958c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/0*W_ebdoq-1Dus37Gi.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><em class="np">μ</em>-law</figcaption></figure><p id="ab4c" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">L1 损失也是如此。定量比较 PSNR 和 HDR-VDP-2 分数在本文中提出传达 L1 损失更好地重建更好的细节相比，L2 损失。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d6c718c997e375e5e6f0e94ed9d6d88a.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*In81bfClnA_qHrlV.png"/></div></figure><h2 id="8087" class="nb lx iq bd ly nc nd dn mc ne nf dp mg mq ng nh mi ms ni nj mk mu nk nl mm nm bi translated">实施规范</h2><p id="4899" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">该架构是使用 PyTorch 实现的。规格和超参数是-</p><ul class=""><li id="a900" class="nx ny iq lc b ld le lg lh mq nz ms oa mu ob lv oc od oe of bi translated">重量初始化:Xavier</li><li id="8351" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">优化器:ADAM</li><li id="cbca" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">学习率:1 X10–5</li><li id="8e9f" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">批量:8 个</li><li id="2b8e" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">GPU: NVIDIA GeForce 1080 Ti</li><li id="f13f" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">一幅图像(1500x1000)的推断时间:0.32 秒</li></ul><h2 id="89bd" class="nb lx iq bd ly nc nd dn mc ne nf dp mg mq ng nh mi ms ni nj mk mu nk nl mm nm bi translated">结果</h2><p id="3f6e" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">这些网络在 Kalantari et 提供的数据集上进行了训练和测试。艾尔。作者提供了定量和定性的比较之间的几个变量的网络，在 PSNR 和 HDR-VDP-2 评分。</p><ul class=""><li id="278c" class="nx ny iq lc b ld le lg lh mq nz ms oa mu ob lv oc od oe of bi translated">ahdr net—ahdr net 的完整模型。</li><li id="d63a" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">DRDB 网络(即无注意事项的 AHDRNet)</li><li id="e117" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">RDB 网(即没有膨胀的阿德尔网)</li><li id="e174" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">RDB 网络(即没有注意力和扩张的 AHDRNet)</li><li id="5924" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">RB-Net(即 AHDRNet，不包含注意、扩张和密集连接)。RBs 取代 DRDBs。</li><li id="3281" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">深 RB 网。使用更多的 RBs。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/2aecee552cc0695c52bf5f427ab887bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/0*EUnWQxWutnMEJg6i.png"/></div></figure><p id="7003" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">结果显示了 AHDRNet 的每个组件对于整个网络的功效是如何重要的，即注意力是重要的，扩展卷积是重要的，密集连接是重要的，剩余学习是重要的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/a9519858957591c7a4376e4a0ae073b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/0*WC-uKJVWhFLumxbb.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Visual results, from the paper</figcaption></figure><p id="bfac" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">与最先进技术的比较</p><p id="ae9e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">与当前最先进的方法(基于学习和不基于学习)的比较揭示了 AHDRNet 如何击败现有的方法。最接近的竞争对手显然是<a class="ae kv" href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener ugc nofollow" target="_blank"> Kalantari et。al </a>的实现仅次于 AHDRNet。作者还提供了使用光流对齐图像(AHDRNet + OF)的 AHDRNet 变体的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/6b12fe033e8ab7cc4697b84d474dfd05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iWVJeN5urpZVz9Ka.png"/></div></div></figure><p id="898f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mq lk ll lm ms lo lp lq mu ls lt lu lv ij bi translated">视觉结果显示了网络在生成的 HDR 输出中注入细致细节的功效，即使在剧烈运动的情况下也不会产生任何对准伪影。这是从论文中摘录的一些结果-</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/01e197c6795bb6cde6e9b8ec0f2c67b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*muRv_BXdBEHgJOxd.jpg"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/aa3eaed0130f87c1523f36db15411a5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*H1TAnwS6l0whMraX.png"/></div></div></figure><h1 id="e32a" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">结论</h1><p id="c2a1" class="pw-post-body-paragraph kz la iq lc b ld mo jr lf lg mp ju li mq mr ll lm ms mt lp lq mu mv lt lu lv ij bi translated">AHDRNet 是第一个解决 HDR 图像生成问题的基于注意力的方法。注意力机制的技巧已经被用于对齐输入的 LDR 图像。先前的图像对准尝试使用了基于光流的方法，这些方法具有一些不准确性，并且对于帧之间的剧烈运动表现不佳。然而，基于注意力的方法在 HDR 重建以及消除对准伪影方面表现得非常好。大量的实验揭示了 AHDRNet 如何在定性和定量上取代现有的方法，并且它已经成为 HDR 图像生成中新的最先进的技术。</p><h1 id="49d3" class="lw lx iq bd ly lz ma mb mc md me mf mg jw mh jx mi jz mj ka mk kc ml kd mm mn bi translated">参考</h1><ul class=""><li id="9bc8" class="nx ny iq lc b ld mo lg mp mq op ms oq mu or lv oc od oe of bi translated">闫庆森，董工，施勤峰，安东·范·登·亨格尔，，伊恩·里德，，，<a class="ae kv" href="https://arxiv.org/abs/1904.10293" rel="noopener ugc nofollow" target="_blank">注意力引导网络无鬼高动态范围成像</a> (2019)，2019</li><li id="21a4" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">名词（noun 的缩写）K. Kalantari 和 R. Ramamoorthi，<a class="ae kv" href="https://cseweb.ucsd.edu/~viscomp/projects/SIG17HDR/" rel="noopener ugc nofollow" target="_blank">动态场景的深高动态范围成像</a> (2017)，ACM Trans。图表</li><li id="5afb" class="nx ny iq lc b ld og lg oh mq oi ms oj mu ok lv oc od oe of bi translated">吴尚哲、许家瑞、戴宇荣、唐志强，<a class="ae kv" href="https://arxiv.org/abs/1711.08937" rel="noopener ugc nofollow" target="_blank">具有大前景运动的深高动态范围成像</a> (2018)，欧洲计算机视觉会议(ECCV)，2018 年 9 月</li></ul></div></div>    
</body>
</html>