<html>
<head>
<title>Kernel Functions in Non-linear Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非线性分类中的核函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kernel-functions-in-non-linear-classification-91a6c9ab14d6?source=collection_archive---------12-----------------------#2019-08-25">https://towardsdatascience.com/kernel-functions-in-non-linear-classification-91a6c9ab14d6?source=collection_archive---------12-----------------------#2019-08-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c96f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解内核函数如何将特性映射到更高维度。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3222921ad51b2fafd72453bd1da16e4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fuYHjgQmXfNi34fW8sRyhw.png"/></div></div></figure><p id="774c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi lq translated">由于数据点在其原始特征空间中是非线性可分的，线性分类器可能无法确定决策边界在哪里。然而，将原始特征空间(<em class="lz"> x </em> ∈ ℝᵈ)映射到更高维的特征空间(<em class="lz"> ϕ </em> ( <em class="lz"> x </em> ) ∈ ℝᵉ，e &gt; d)可以帮助复活线性分类器来正确地完成工作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/9f91b155f123589b6fc27ed65c18609c.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*TfUSC60rUOZAmYZ5QOOGUw.png"/></div></figure><p id="3a3c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">图一。阐释了通过特征映射对数据点进行分类的概念。本来，二维空间中具有特征向量<em class="lz"> x </em> = [ <em class="lz"> x </em> ₁，<em class="lz"> x </em> ₂]的数据点具有同心圆分布(这里不是严格的数学描述)。不可能使用线性分类器来区分决策边界。</p><p id="0b27" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">尽管如此，通过结合某种映射函数<em class="lz"> ϕ </em> ( <em class="lz"> x </em>)，特征向量可以被变换到三维特征空间。具有三维特征向量<em class="lz">ϕ</em>(<em class="lz">x</em>)=[<em class="lz">x</em>₁，<em class="lz">x</em>₂,(<em class="lz">x</em>₁+<em class="lz">x</em>₂]的新数据点现在可以使用线性分类器来确定决策边界超平面。这就是特性映射的力量，它可以让我们用更强的表达能力来处理更复杂的数据分布模式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mb"><img src="../Images/8f54e4e9a6104e455838d55e1758d7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHelTqNV7MyOJrsZgWoiOw.png"/></div></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 1. Mapping data points with 2-D feature vectors into 3-D feature vectors</figcaption></figure><p id="8715" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是，直接使用<em class="lz"> ϕ </em> ( <em class="lz"> x </em>)的缺点是</p><ol class=""><li id="5543" class="mg mh it kw b kx ky la lb ld mi lh mj ll mk lp ml mm mn mo bi translated">有时很难直接显式构造一个<em class="lz"> ϕ </em> ( <em class="lz"> x </em>)。</li><li id="3fd1" class="mg mh it kw b kx mp la mq ld mr lh ms ll mt lp ml mm mn mo bi translated">随着特征尺寸的增加，计算能力迅速提高。</li></ol><p id="e24c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是内核函数可以提供一种有效的方法来解决这个问题。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="5788" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">核函数</h1><p id="7cd4" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">核函数的思想是在两个特征向量之间取内积，并且计算内积不需要很大的计算量。然后，我们可以在算法中只利用内积的结果。例如，如果我们想让<em class="lz"> ϕ </em> ( <em class="lz"> x </em>)如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/2e2c079eb98b2f7a6dab7fe2b24792fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*Ry5zanqAcAuUdA6A55--WA.png"/></div></figure><p id="4c55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">核函数取两个特征向量之间的内积如下，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/6c476c35cb084f22380fc950e140d897.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*1btyG-vQRonGHspi9v-sbw.png"/></div></figure><p id="f3f2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因此，核函数的形式对于我们来说将比在更高的特征维度中直接使用映射函数更容易构造。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="2b56" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">内核合成规则</h1><p id="3754" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">有几个内核组合规则可以用来构造更复杂的内核函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7a0f2ad7624c27a597cf05e9da797471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*_K91RAdMLmj5qJJ5yB9NSQ.png"/></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="3124" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">径向基核</h1><p id="f4ec" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">核函数甚至可以赋予特征向量无限维的能力。一种常见的核函数是径向基核。定义如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0f543e38a987f311bf74ca472a0472b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*2TrrPfzuAeTHJiNhFG-ppg.png"/></div></figure><p id="23bf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为指数可以扩展到无穷幂级数，所以径向基核赋予特征映射更多的表现力。下面是径向基核是核函数的证明。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/5fd07eec3e71d6e32d3eae8b5c7f0cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*ncu1MN8R9mpVg9N8uupAHA.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/797f3e34b0ef5d78f3feafcad88c7bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*brM22oD-XOy44yf0eLBu6A.png"/></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="12b5" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">核感知器算法</h1><p id="7d9f" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">这里回忆一下感知器算法<a class="ae od" rel="noopener" target="_blank" href="/perceptron-algorithms-for-linear-classification-e1bb3dcc7602"/>，感知器算法一旦一个数据点被误分类，就更新<em class="lz">θ=θ+</em>y⁽ʲ<em class="lz"/>⁾<em class="lz">x</em>⁽ʲ<em class="lz"/>⁾。换句话说，<em class="lz"> θ </em>可以替换地表达如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/234efbcf0da1d5cedf3a0e7e26f6fefd.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*Nc9LQTLHEDGouTGYKqfkXg.png"/></div></figure><p id="8a44" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">其中，α <strong class="kw iu"> ⱼ </strong>是感知器在第<em class="lz"> j </em>个数据点上犯的错误数。如果在映射特征空间中，则<em class="lz"> θ </em>可以表示如下。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/0236be70ec96010a3f7f1a7d214f7df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*eMArI8mhitnpmOa70og-Ug.png"/></div></figure><p id="91dd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">利用<em class="lz"> θ的核心形式，</em>核心感知器算法的伪代码可以描述如下。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="7ebf" class="ol nc it oh b gy om on l oo op"><em class="lz"># Kernel Perceptron Algorithm</em></span><span id="82b2" class="ol nc it oh b gy oq on l oo op"><em class="lz"># initialize </em>α<strong class="oh iu">ⱼ</strong><br/><em class="lz"># totally m data points<br/></em>for i = 1 .. m do<br/>    αᵢ = 0</span><span id="c74c" class="ol nc it oh b gy oq on l oo op"><em class="lz"># totally T epoches to iterate</em><br/>for t = 1 .. T do                     <br/>   <em class="lz"> # totally m data points</em>    <br/>    for i = 1 .. m do<br/>        <em class="lz"># misclassify data points</em>                 <br/>        if y⁽ⁱ⁾<strong class="oh iu"><em class="lz">∑</em>ⱼ(</strong>α<strong class="oh iu">ⱼ</strong>y⁽ʲ⁾<em class="lz">K</em>(x⁽ʲ⁾,x⁽ⁱ⁾)<strong class="oh iu">)</strong> ≦ 0     <br/>        then<br/>            αᵢ = αᵢ + 1</span><span id="f615" class="ol nc it oh b gy oq on l oo op">θ<em class="lz">ϕ</em>(x⁽ⁱ⁾)= <strong class="oh iu"><em class="lz">∑</em>ⱼ(</strong>α<strong class="oh iu">ⱼ</strong>y⁽ʲ⁾<em class="lz">K</em>(x⁽ʲ⁾,x⁽ⁱ⁾)<strong class="oh iu">)</strong></span></pre></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="f579" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">内核在运行</h1><p id="2680" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">图二。通过径向基核感知器算法可视化决策边界的更新。注意，由径向基核感知器算法绘制的决策边界可以在 2 个 epoches 内收敛于这种数据分布。径向基核的γ在这里使用 0.3。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/cfa7178290bc5929cee026d0a586aef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*t28AsbPCYUwHDaL9o0gkwg.gif"/></div><figcaption class="mc md gj gh gi me mf bd b be z dk">Figure 2. Updating the decision boundaries by the RBK perceptron algorithm. May take time to load.</figcaption></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="a2fa" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">示例代码</h1><p id="587b" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">在 Jupyter 笔记本上写的感知器算法的样本代码可以在<a class="ae od" href="https://github.com/AnHungTai/Medium-SampleCode/blob/master/Kernel%20Functions%20in%20Nonlinear%20Classification/Visualizing%20Kernel.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。您可以自己处理数据和超参数，看看内核感知器算法的表现如何。</p></div></div>    
</body>
</html>