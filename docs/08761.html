<html>
<head>
<title>Binary Classification with Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用逻辑回归进行二元分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/binary-classification-with-logistic-regression-31b5a25693c4?source=collection_archive---------10-----------------------#2019-11-24">https://towardsdatascience.com/binary-classification-with-logistic-regression-31b5a25693c4?source=collection_archive---------10-----------------------#2019-11-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ccf2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在线广告点击率的估算</h2></div><p id="bcd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在绩效营销中，一个重要的关键绩效指标(KPI)是由点击率(CTR)给出的。点击率是点击特定链接的用户与查看页面、电子邮件或广告(ad)的用户总数的比率。</p><p id="575f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">估计 CTR 是一个二元分类问题。当用户观看广告时，他要么点击<code class="fe le lf lg lh b">(y=1)</code>要么不点击<code class="fe le lf lg lh b">(y=0)</code>。只有两种可能的结果，让我们使用<em class="li">逻辑回归</em>作为我们的模型。与用于推断<em class="li">连续</em>变量的线性回归相反，逻辑回归用于估计任意数量的<em class="li">离散</em>类。我给出了一个简单的可视化，它为三个主要的数据科学问题提供了正确的模型:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/6d29cea9a5d54b346bbad0ff52399c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0MIKRdwybn-T_fPmZ1cfA.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">How to choose a model</figcaption></figure><p id="f184" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个故事中，在将学到的一切应用于 Kaggle 的“点击率预测”挑战之前，我想先引导您了解逻辑回归的技术细节。</p><h1 id="d3ab" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">二元逻辑回归:理论</h1><p id="e5a5" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">逻辑回归的特征在于一个逻辑函数<em class="li">来模拟标签 Y 变量 X 的条件概率</em></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/72be8fa566db54fb9937d6a916028350.png" data-original-src="https://miro.medium.com/v2/resize:fit:168/format:webp/1*2nWLxivyEb3n1DqV6WFu-Q.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The conditional probability.</figcaption></figure><p id="9941" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，Y 表示被点击或未被点击的状态，X 表示我们想要选择的特征(例如设备类型)。</p><p id="a7fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用<em class="li"> m </em>个观察值，每个包含<em class="li"> n </em>个特征。对于它们中的每一个，我们将有 m 个 n+1 维的行向量 xᵢ。我们的标签 Y 只能是 0 或 1。参数将在 n+1 维的列向量θ中给出。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mx"><img src="../Images/05ab41703fc743984ad0dacd16846add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZCo4J5mCs5eQxaXDynHqw.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Definitions of Y, X and Θ.</figcaption></figure><p id="a003" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用户点击给定观察值 X 的条件概率可以建模为<em class="li"> sigmoid </em>函数。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi my"><img src="../Images/cc82f46c9aa47153e5ac2d141f84807b.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*ROkaYrPGotg5YotkcBAVIg.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The conditional probability modeled with the sigmoid logistic function.</figcaption></figure><p id="d0c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">逻辑回归的核心是 sigmoid 函数。sigmoid 函数将连续变量映射到闭集[0，1]，然后可以将其解释为概率。右手边的每个数据点被解释为<code class="fe le lf lg lh b">y=1</code>，左手边的每个数据点被推断为<code class="fe le lf lg lh b">y=0</code>。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi mz"><img src="../Images/1ba387995bdffac449be344a9adc8b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6BLktaLLBRHMIV556X-vew.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">A plot of the sigmoid function with labeled sample data.</figcaption></figure><h2 id="e7cb" class="na ma it bd mb nb nc dn mf nd ne dp mj kr nf ng ml kv nh ni mn kz nj nk mp nl bi translated">衍生(可选)</h2><p id="1930" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">在推导条件概率时，sigmoid 函数自然出现。我们可以用贝叶斯定理来表示 P(Y|X)</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/b3168149e594b76baf88f3393e4c5221.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*EUc4iyPGh9jFT5yonL2CbQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Bayes’ theorem</figcaption></figure><p id="ec53" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">根据贝叶斯解释</p><ul class=""><li id="0e95" class="nn no it kk b kl km ko kp kr np kv nq kz nr ld ns nt nu nv bi translated">P(Y|X)为<em class="li">后验，</em></li><li id="9d18" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">P(Y)为<em class="li">先验，</em></li><li id="fe7f" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">和 P(X)作为归一化因子。</li></ul><p id="16e2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将拟合数据的后验和先验，并且必须去掉未知的概率 P(X)。这可以通过使用补充条件概率来完成。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/72d53c41ddab465d7e5c627b50657584.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*vTpv1ldNQYgGA7BD1goM5w.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Complement conditional probability.</figcaption></figure><p id="c872" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当将后验概率除以互补条件概率并取对数时，我们得到<em class="li">对数优势(logit) </em></p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi oc"><img src="../Images/45f5da88d164d5fb2345fd81cc54f6a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eGO-01Xm4F1gmsQ2uEiHrg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The logit can be modeled as a linear function of X.</figcaption></figure><p id="ef7f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这里我们假设 logit 是 X 中的线性函数！</strong>现在，我们只需撤销对数并求解后验概率，即可导出 sigmoid 函数</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi od"><img src="../Images/2d67b6a64d9eaa9099dcd55fb13a33c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TU4Jf0fdXWMEU8YtP5BQgg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Sigmoid derivation.</figcaption></figure><h2 id="05d1" class="na ma it bd mb nb nc dn mf nd ne dp mj kr nf ng ml kv nh ni mn kz nj nk mp nl bi translated">最大似然估计</h2><p id="023d" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">到目前为止，我们已经用一组参数θ模拟了后验概率。我们如何确定θ的最佳选择？用户点击的条件概率等于 sigmoid 函数。所有情况的概率总和必须等于 1。因为我们只有两种情况，所以我们可以找到一种优雅的方式在一个表达式中表达这两种概率:</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi oe"><img src="../Images/1c9607764a5b5cc05532aee354bbf860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkAUw5Q_iilq7qTvmgBDCw.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The probability mass function of the Bernoulli distribution.</figcaption></figure><p id="44be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">右边是伯努利分布的概率质量函数(PMF)。伯努利分布描述了一个随机变量，它可以采取两种结果中的一种，比如我们的标签被点击或未被点击。现在，为了确定我们的参数θ，我们需要在只给定一个样本的情况下，最大化复制总体分布的概率。这种方法被称为<em class="li">最大似然估计</em> (MLE)。我们主要是将样本中每个事件的所有概率结合起来。这种联合概率被称为似然性，它与概率有许多共同之处，但主要集中在参数上</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi of"><img src="../Images/8b7ef0a515e6b50688986c62993a5269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SRNBgLpleqTlzQb57DIPkw.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The likelihood.</figcaption></figure><p id="63de" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将上面的函数最大化，但是为了方便起见(为了获得更漂亮的导数),我们将对数应用于可能性。我们可以这样做，因为对数是单调递增的，因此保留了最大值的位置。通过应用对数，乘积变成和</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi og"><img src="../Images/7be8bfc2340d04ba52142e2074db3106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofjv2am1VcS9zuAs7UZRMg.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The log-likelihood.</figcaption></figure><p id="b0b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了最大化对数似然，我们可以使用微积分。极值点的导数必须等于零</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi oh"><img src="../Images/6c5a495a2c73f3c64ce3e36e7ce4dd97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHPSK-8wjdlKg83yPh2vUA.png"/></div></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The first derivative of the log-likelihood.</figcaption></figure><h2 id="db8d" class="na ma it bd mb nb nc dn mf nd ne dp mj kr nf ng ml kv nh ni mn kz nj nk mp nl bi translated">sigmoid 函数的导数(可选)</h2><p id="7370" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">在最后一个结果中，我们使用了 sigmoid 函数对θ的导数。推导过程如下</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d8241838d7a76908ec73b1568f2e28d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*BEYq_s07omSz-mhvKVcbFQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Derivation of the derivative of the sigmoid function.</figcaption></figure><h2 id="538a" class="na ma it bd mb nb nc dn mf nd ne dp mj kr nf ng ml kv nh ni mn kz nj nk mp nl bi translated">牛顿-拉夫森</h2><p id="bbe2" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">为了执行 MLE，我们必须找到对数似然的一阶导数的根。我们可以使用<em class="li">牛顿-拉夫森</em>求根算法来完成这项任务。牛顿-拉夫森法是最大化对数似然的标准方法。它需要计算二阶导数。在我们的例子中，我们可以通过分析来确定它。在其他情况下，二阶导数在计算上是昂贵的，我们可以使用<em class="li">梯度下降(上升)</em>进行优化。二阶导数由下式给出</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2533c8c1da3adef516978f42ca8cbdbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*JhTzt1ifRb9pDHS023j5VQ.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">The second derivative of the log-likelihood.</figcaption></figure><p id="1075" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">牛顿-拉夫森方法告诉我们如何更新每次迭代的参数。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/92222a1696358ffad8a29a9492708d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*a6NM8KWVIYnbsddh6QqjIA.png"/></div><figcaption class="lv lw gj gh gi lx ly bd b be z dk">Newton-Raphson iteration.</figcaption></figure><h1 id="9c6f" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">估计 CTR</h1><p id="f77f" class="pw-post-body-paragraph ki kj it kk b kl mr ju kn ko ms jx kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">在这个故事的第二部分，我们想编写我们自己的逻辑回归实现。我制作的 Jupyter 笔记本已经作为<a class="ae ol" href="https://gist.github.com/Knowledge91/8b839bec47c8113c8f279e0cc574a40a" rel="noopener ugc nofollow" target="_blank">要点</a>出版。我们将使用“点击率预测”Kaggle 竞赛的数据。下载数据后，我们对其进行解包，并在对完整集进行训练之前准备一个 10000 行的样本。</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="8019" class="na ma it lh b gy oq or l os ot">unzip avazu-ctr-prediction.zip<br/>gunzip train.gz<br/>head -n10000 train &gt; train_sample.csv</span></pre><p id="8176" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们将 CSV 加载到 panda 数据帧中，并将其分为训练集和测试集</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="7617" class="na ma it lh b gy oq or l os ot">df = pd.read_csv('train_sample.csv')<br/>msk = np.random.rand(len(df)) &lt; 0.8<br/>train = df[msk]<br/>test = df[~msk]</span></pre><p id="f32e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在你应该关注特性探索，但是为了简单起见，我选择了列<em class="li"> device_type、C1、C15 </em>和<em class="li"> C16 </em>作为特性列。然后我可以准备我的特征矩阵 X 并使用<em class="li">点击</em>列作为标签</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="81df" class="na ma it lh b gy oq or l os ot">m = len(train)<br/>X_train = np.ones((m, 5))<br/>X_train[:,1] = train.device_type.to_numpy()<br/>X_train[:,2] = train.C1.to_numpy()<br/>X_train[:,3] = train.C15.to_numpy()<br/>X_train[:,4] = train.C16.to_numpy()</span><span id="ff6c" class="na ma it lh b gy ou or l os ot">y_train = train.click.to_numpy()</span></pre><p id="c4b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使我们的算法工作，我们需要先前导出的对数似然的一阶和二阶导数，其可以编码如下</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="8383" class="na ma it lh b gy oq or l os ot">def DLogLikelihood(X, y, theta):<br/>    res = np.zeros(theta.shape[0])<br/>    for i in range(0, X.shape[0]):<br/>        x_i = X[i]<br/>        y_i = y[i]<br/>        res += x_i * (y_i - sigmoid(np.dot(theta, x_i)) )<br/>    return res</span><span id="3c13" class="na ma it lh b gy ou or l os ot">def DDLogLikelihood(X, theta):<br/>    res = np.zeros((theta.shape[0], theta.shape[0]))<br/>    for i in range(0, X.shape[0]):<br/>        x_i = X[i]<br/>        sigma = sigmoid(np.dot(theta, x_i))<br/>        res += np.outer(x_i, x_i) * sigma * ( 1 - sigma ) <br/>    return -res</span></pre><p id="c2b7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">迭代 Netwon-Raphons 步骤和我们的逻辑回归算法然后</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="c23a" class="na ma it lh b gy oq or l os ot">def NewtonRaphsonTheta(X, y, theta):<br/>    return theta - np.dot(<br/>        np.linalg.inv(DDLogLikelihood(X, theta)),<br/>        DLogLikelihood(X, y, theta))</span><span id="f6a5" class="na ma it lh b gy ou or l os ot">def logisticRegression(X, y, epochs=100):<br/>    theta = np.zeros(X.shape[1])<br/>    for i in range(epochs):<br/>        theta = NewtonRaphsonTheta(X, y, theta)<br/>    return theta</span></pre><p id="315f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通过调用<code class="fe le lf lg lh b">logisticRegression(X, y)</code>,我们将迭代计算参数θ，然后可以用它来预测用户的点击概率</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="d764" class="na ma it lh b gy oq or l os ot">def predict(X, theta):<br/>    res = np.zeros(X.shape[0])<br/>    for i in range(len(res)):<br/>        x = X[i]<br/>        res[i] = sigmoid(np.dot(theta, x))<br/>    return res</span></pre><p id="1095" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于试运行，我们得到以下概率</p><pre class="lk ll lm ln gt om lh on oo aw op bi"><span id="b08d" class="na ma it lh b gy oq or l os ot">theta = logisticRegression(X_train, y_train, epochs=100)<br/>y_pred = predict(X_test, theta)</span><span id="f804" class="na ma it lh b gy ou or l os ot">print(y_pred)<br/>[0.18827126 0.16229901 … 0.16229901 0.16229901 0.16229901]</span></pre><p id="b092" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了评估该模型，我将测试集的预测与它们的实际值进行了比较，结果显示该模型相当差。为了改进，我们可以在特征选择上花更多的时间，在更多的数据上进行训练，同时不断地用评估指标测量模型性能，如<em class="li">对数损失</em>或<em class="li"> ROC </em>曲线。</p><h1 id="d0a1" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated"><strong class="ak">总结</strong></h1><ul class=""><li id="aa9d" class="nn no it kk b kl mr ko ms kr ov kv ow kz ox ld ns nt nu nv bi translated">逻辑回归用于多分类问题</li><li id="f801" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">如果我们只有两个类，则使用二元逻辑回归</li><li id="c764" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">P(Y|X)由 sigmoid 函数建模，该函数从(-∞，∞)映射到(0，1)</li><li id="106d" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">我们假设 logit 可以建模为线性函数</li><li id="b824" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">为了估计参数θ，我们最大化对数似然</li><li id="628a" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">伯努利分布是具有两种可能结果的离散分布，用于二元分类</li><li id="09fe" class="nn no it kk b kl nw ko nx kr ny kv nz kz oa ld ns nt nu nv bi translated">我们使用牛顿-拉夫森作为求根器，因为我们可以很容易地计算对数似然的二阶导数</li></ul></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="1cd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]: Kaggle，点击率预测<a class="ae ol" href="https://www.kaggle.com/c/avazu-ctr-prediction" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/avazu-ctr-prediction</a></p><p id="6e9e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]:统计学习的要素，t .哈斯蒂，r .蒂布拉尼，j .弗里德曼<a class="ae ol" href="https://web.stanford.edu/~hastie/ElemStatLearn/" rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~hastie/ElemStatLearn/</a></p><p id="84bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]:牛顿-拉夫逊法<a class="ae ol" href="https://en.wikipedia.org/wiki/Newton%27s_method" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Newton%27s_method</a></p></div></div>    
</body>
</html>