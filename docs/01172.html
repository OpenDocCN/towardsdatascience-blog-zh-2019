<html>
<head>
<title>The Complete Reinforcement Learning Dictionary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">完全强化学习词典</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e?source=collection_archive---------2-----------------------#2019-02-23">https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e?source=collection_archive---------2-----------------------#2019-02-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b16a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习术语，从 A 到 Z</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="20fa" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">在媒体<em class="ll">上阅读此文，而</em>不是使用此</strong> <a class="ae lm" rel="noopener" target="_blank" href="/the-complete-reinforcement-learning-dictionary-e16230b7d24e?sk=9d29f303db3650387c489c9565b43636"> <strong class="kr iu">好友链接</strong> </a> <strong class="kr iu">的媒体成员！</strong></p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/4bf6dbf426e36071a147c6a779fc5d46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z0JjTZ2DnEK8S-FPgktbNQ.jpeg"/></div></div></figure><p id="a480" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">每当我开始学习一门新的学科时，我发现最难应付的是它的新术语。每个领域都有许多术语和定义，对于一个局外人来说完全是晦涩难懂的，这会使一个新手迈出第一步相当困难。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="30b0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">当我第一次踏入强化学习的世界时，我被每隔一行就弹出的新术语所淹没，并且总是让我惊讶的是在这些复杂的单词背后隐藏着非常简单和符合逻辑的想法。因此，我决定用自己的话把它们都写下来，这样我就可以随时查阅，以防忘记。这本字典就是这样产生的。</p><p id="f7f0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这不是强化学习的入门文章，而是学习时的辅助工具。如果你也想在这个领域开始你的职业生涯，我可以给你以下建议:</p><ul class=""><li id="8c81" class="lz ma it kr b ks kt kv kw ky mb lc mc lg md lk me mf mg mh bi translated">如果你正在寻找一个包含代码示例的快速、10 分钟速成课程，请查看我的<em class="ll"> Qrash 课程</em>系列:<a class="ae lm" href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" rel="noopener">RL 和 Q-Learning 简介</a>和<a class="ae lm" href="https://medium.com/@shakedzy/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c" rel="noopener">政策梯度和行动者-批评家</a>。</li><li id="5f07" class="lz ma it kr b ks mi kv mj ky mk lc ml lg mm lk me mf mg mh bi translated">如果你有更深的兴趣，并且想学习和编写几种不同的 RL 算法并获得更多的直觉，我可以推荐托马斯·西蒙尼的这个系列和亚瑟·朱利安尼的这个系列。</li><li id="a368" class="lz ma it kr b ks mi kv mj ky mk lc ml lg mm lk me mf mg mh bi translated">如果你已经准备好掌握 RL，我会把你引向强化学习的“圣经”——理查德·萨顿和安德鲁·巴尔托的《强化学习导论》。第二版(从 2018 年开始)作为<a class="ae lm" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank"> PDF 文件</a>免费(合法)提供。</li></ul><p id="8db4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我将尽我最大的努力不断更新这本词典。如果我错过了什么重要的事情或者做错了什么，请随时告诉我。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="6195" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">字典</h1><p id="c678" class="pw-post-body-paragraph kp kq it kr b ks nf ju ku kv ng jx kx ky nh la lb lc ni le lf lg nj li lj lk im bi translated"><strong class="kr iu">动作值功能:</strong>见<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a> <em class="ll">。</em></p><p id="8751" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">动作:</strong>动作是<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a> <em class="ll">的</em>方法，允许其交互并改变其<a class="ae lm" href="#4311" rel="noopener ugc nofollow"> <em class="ll">环境</em> </a>，从而在<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>之间转移。代理人执行的每一个动作都会从环境中获得一个<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>。选择哪个动作的决定由<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a>做出。</p><p id="8f35" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">演员-评论家:</strong>当试图解决一个<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a> <em class="ll"> </em>问题时，有两种主要方法可供选择:计算每个状态的<a class="ae lm" href="#680c" rel="noopener ugc nofollow"> <em class="ll">值函数</em> </a>或<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a>并根据这些来选择动作，或者直接计算一个<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a>，该策略定义了每个动作应该被采取的概率 Actor-Critic 算法结合了这两种方法，以创建一种更健壮的方法。一个很棒的插图漫画解释可以在<a class="ae lm" href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="efa6" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">优势函数:</strong>通常表示为<em class="ll"> A(s，a) </em>，优势函数是在给定某个<a class="ae lm" href="#c274" rel="noopener ugc nofollow"><em class="ll"/></a>状态的情况下，衡量某个<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a> <em class="ll"> </em>好坏的一个决策——或者更简单的说，从某个状态中选择某个动作的优势是什么。它在数学上定义为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/c96e04bfbbdbbca9b91845092041da36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*CbxBGj9dxC741D0z0DL2Kg.png"/></div></figure><p id="c06e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中<em class="ll"> r(s，a) </em>是从状态<em class="ll"> s </em>开始的动作<em class="ll"> a </em>的期望<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>，并且<em class="ll"> r(s) </em>是在选择动作之前整个状态<em class="ll"> s </em>的期望奖励。它也可以被视为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/ed0d0061ce6318ddeb8e909f11701016.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*GuzTDUuNkuEboEhhdILnEQ.png"/></div></figure><p id="d10c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中<em class="ll"> Q(s，a) </em>为<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em></a><em class="ll">V(s)</em>为<a class="ae lm" href="#680c" rel="noopener ugc nofollow"> <em class="ll">值函数</em> </a> <em class="ll">。</em></p><p id="b6a2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">代理:</strong>学习和<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">演戏</em> </a>一部分<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>问题，即试图最大化<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>它是由<a class="ae lm" href="#4311" rel="noopener ugc nofollow"> <em class="ll">环境</em> </a>给出的。简单地说，代理就是你试图设计的模型。</p><p id="790b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">盗匪:</strong>正式命名为“k-Armed 盗匪”后昵称“独臂盗匪”给予<a class="ae lm" href="https://en.wikipedia.org/wiki/Slot_machine" rel="noopener ugc nofollow" target="_blank">吃角子老虎</a>，这些被认为是最简单类型的<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a> <em class="ll"> </em>任务。盗匪没有不同的<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>，只有一个——而考虑中的<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>只是眼前的一个。因此，土匪可以被认为是有单态<a class="ae lm" href="#601d" rel="noopener ugc nofollow"> <em class="ll">集</em> </a>。每个 k 臂被认为是一个<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>，目标是学习在每个动作(或拉臂)后将最大化期望回报的<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a>。<br/>尽管如此，这个任务仍然是一个单一状态的阶段性任务，一个上下文不能对其他上下文产生影响。</p><p id="fbd3" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">贝尔曼方程:</strong>形式上，贝尔曼方程定义了一个给定的<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>(或状态- <a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>对)与其后继者之间的关系。虽然存在许多形式，但在<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>任务中通常遇到的最常见形式是最优<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a> <em class="ll"> </em>的贝尔曼方程，其由下式给出:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/284b90d6a2b4ff17d50c769b274d31a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Re6kADukp4wKFEnGzhImzw.png"/></div></div></figure><p id="0abe" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">或者当不存在不确定性时(即概率为 1 或 0):</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/fdbecb709e67ea6d179aa7e35cf2a6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HTYLQwcKYerhL9620zALMA.png"/></div></div></figure><p id="49d8" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中星号表示<em class="ll">最佳值</em>。一些算法，如<a class="ae lm" href="#9d8f" rel="noopener ugc nofollow"> <em class="ll"> Q-Learning </em> </a>，就是以此为基础进行学习的。</p><p id="2692" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">连续任务:</strong> <a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>不是由<a class="ae lm" href="#601d" rel="noopener ugc nofollow"> <em class="ll">集</em> </a>组成，而是永远持续的任务。这个任务没有终端<em class="ll"> </em> <a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <em class="ll"> s. </em>为了简单起见，通常假设它们由一个永无止境的情节组成。</p><p id="ac07" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">深度 Q-Networks (DQN) </strong>:参见<a class="ae lm" href="#9d8f" rel="noopener ugc nofollow"> <em class="ll"> Q-Learning </em> </a></p><p id="aa91" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">深度强化学习:</strong>使用带有深度神经网络的<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>算法作为学习部分的近似器。这样做通常是为了处理可能的<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>和<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>的数量迅速增加，并且精确的解决方案不再可行的问题。</p><p id="4ee6" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">贴现因子(γ) </strong>:贴现因子，通常表示为γ，是乘以未来期望<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">报酬</em> </a>的因子，在[0，1]的范围内变化。它控制着未来回报相对于眼前回报的重要性。折扣因子越低，未来的回报就越不重要，而<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a> <em class="ll"> </em>会倾向于专注于只会产生即时回报的<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">行动</em> </a>。</p><p id="4311" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">环境:</strong>除了<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow">之外的一切<em class="ll">代理</em></a>；代理可以直接或间接与之交互的一切。环境随着代理执行<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>而变化；每一个这样的变化都被认为是一个<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em></a>-转换。代理执行的每个动作都会产生一个由代理接收的<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>。</p><p id="601d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">插曲:</strong>介于初态和终态之间的所有<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">态</em></a>；比如:一盘棋。<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em></a><em class="ll"/>的目标是让它在一集里收到的总<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>最大化。在没有终极状态的情况下，我们考虑一个无限事件。重要的是要记住，不同的情节是完全相互独立的。</p><p id="15bc" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">阶段性任务:<em class="ll"> </em> </strong> <a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>任务由不同的<a class="ae lm" href="#601d" rel="noopener ugc nofollow"> <em class="ll">剧集</em> </a>(意思是，每一集都有一个终端<em class="ll"> </em> <a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>)。</p><p id="a3de" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">预期回报:</strong>有时被称为“整体报酬”，偶尔被表示为<em class="ll"> G </em>，是在整个<a class="ae lm" href="#601d" rel="noopener ugc nofollow"><em class="ll"/></a>剧集中的预期<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"><em class="ll"/></a>。</p><p id="5ecb" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">体验回放:</strong>作为<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>任务，没有预先生成的训练集可供其学习，<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>必须保留其遇到的所有<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <em class="ll"> - </em>转换的记录，以便以后可以从中学习。用于存储此内容的内存缓冲区通常被称为<em class="ll">体验回放</em>。这些内存缓冲区有几种类型和架构，但一些非常常见的是<a class="ae lm" href="https://en.wikipedia.org/wiki/Circular_buffer" rel="noopener ugc nofollow" target="_blank">循环内存缓冲区</a>(它确保代理保持对其新行为的训练，而不是可能不再相关的事情)和<a class="ae lm" href="https://en.wikipedia.org/wiki/Reservoir_sampling" rel="noopener ugc nofollow" target="_blank">基于库采样</a>的内存缓冲区(它保证记录的每个状态转换都有均匀的概率被插入到缓冲区中)。</p><p id="b903" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">开发&amp;探索:</strong> <a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>任务没有可以学习的预先生成的训练集——它们创建自己的<a class="ae lm" href="#5ecb" rel="noopener ugc nofollow"> <em class="ll">经验</em> </a>并“动态”学习。为了能够这样做，<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>需要在许多不同的<a class="ae lm" href="#c274" rel="noopener ugc nofollow"><em class="ll"/></a>状态下尝试许多不同的<em class="ll">动作</em>，以便尝试和学习所有可用的可能性，并找到将最大化其总体<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"><em class="ll"/></a>回报的路径；这被称为<em class="ll">探索</em>，因为代理探索<a class="ae lm" href="#4311" rel="noopener ugc nofollow"> <em class="ll">环境</em> </a>。另一方面，如果代理只做探索，它永远不会最大化整体回报——它还必须使用它学到的信息来这样做。这被称为<em class="ll">利用</em>，因为代理利用其知识来最大化其收到的回报。<br/>这两者之间的权衡是强化学习问题的最大挑战之一，因为这两者必须平衡，以便让代理既能充分探索环境，又能利用它所学到的东西，并重复它找到的最有价值的路径。</p><p id="15c5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">贪策，<em class="ll">ε</em>——贪策:</strong>一贪<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策</em> </a>意为<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a> <em class="ll"> </em>不断地执行<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>即被认为能产生最高预期<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">报酬</em> </a>。显然，这样的政策根本不会允许代理人<a class="ae lm" href="#b903" rel="noopener ugc nofollow"> <em class="ll">探索</em> </a> <em class="ll"> </em>。为了仍然允许一些探索，通常使用一种<em class="ll"> ε- </em>贪婪策略来代替:选择[0，1]范围内的一个数(名为<em class="ll"> ε </em> ) <em class="ll"> </em>，并且在选择动作之前，选择[0，1]范围内的一个随机数。如果该数字大于<em class="ll"> ε </em>，则选择贪婪动作，但如果该数字小于该数字，则选择随机动作。注意，如果<em class="ll"> ε </em> =0，则该策略成为贪婪策略，如果<em class="ll"> ε </em> =1，则始终探索。</p><p id="42c0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">k-武装土匪:</strong>见<a class="ae lm" href="#790b" rel="noopener ugc nofollow"> <em class="ll">土匪</em> </a> <em class="ll">。</em></p><p id="4a1d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">马尔可夫决策过程(MDP): </strong>马尔可夫属性是指每个<a class="ae lm" href="#c274" rel="noopener ugc nofollow"><em class="ll"/></a><em class="ll"/>状态完全依赖于它之前的状态，从那个状态中取出的被选择的<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>和在那个动作被执行之后立即收到的<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>。数学上的意思是:<em class="ll"> s' = s'(s，a，r) </em>，其中<em class="ll"> s' </em>是未来状态，<em class="ll"> s </em>是其之前状态，<em class="ll"> a </em>和<em class="ll"> r </em>是动作和奖励。不需要事先知道在<em class="ll"> s </em>之前发生了什么——马尔可夫属性假设<em class="ll"> s </em>包含所有相关信息。马尔可夫决策过程是基于这些假设的决策过程。</p><p id="6d88" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">基于模型&amp;无模型:</strong>基于模型和无模型是一个<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>在试图优化其<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a>时可以选择的两种不同的方法。最好用一个例子来解释:假设你正试图学习如何玩<a class="ae lm" href="https://en.wikipedia.org/wiki/Blackjack" rel="noopener ugc nofollow" target="_blank">21 点</a>。你可以通过两种方式做到这一点:一，你预先计算，在游戏开始之前，所有<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>的获胜概率，以及所有可能的<a class="ae lm" href="#8751" rel="noopener ugc nofollow"><em class="ll"/></a>动作的所有状态转移概率，然后简单地按照你的计算去行动。第二种选择是在没有任何先验知识的情况下简单地玩，并使用“试错法”来获取信息。注意，使用第一种方法，你基本上是在<em class="ll">建模</em>你的<a class="ae lm" href="#4311" rel="noopener ugc nofollow"> <em class="ll">环境</em> </a>，而第二种方法不需要关于环境的任何信息。这正是基于模型和无模型的区别；第一种方法是基于模型的，而后者是无模型的。</p><p id="f9e0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">蒙特卡洛(MC): </strong>蒙特卡洛方法是一种算法，它使用重复的随机抽样来获得一个结果。它们经常在<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a>算法中使用，以获得期望值；例如—计算一个<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <a class="ae lm" href="#680c" rel="noopener ugc nofollow"> <em class="ll">值函数</em> </a>通过反复返回同一状态，对每次收到的实际累计<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>进行平均。</p><p id="44d1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">On-Policy&amp;Off-Policy:</strong>每一个<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a> <em class="ll"> </em>算法都必须遵循一些<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a>以决定在每一个<a class="ae lm" href="#c274" rel="noopener ugc nofollow"><em class="ll"/></a>状态下执行哪些<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>。尽管如此，算法的学习过程在学习时不必考虑该策略。关注产生过去状态行为决策的策略的算法被称为<em class="ll">符合策略的</em>算法，而忽略它的算法被称为<em class="ll">不符合策略的</em>。<br/>一个众所周知的非策略算法是<a class="ae lm" href="#9d8f" rel="noopener ugc nofollow"> <em class="ll"> Q-Learning </em> </a>，因为它的更新规则使用将产生最高<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a>的动作，而实际使用的策略可能会限制该动作或选择另一个动作。Q-Learning 的基于策略的变体被称为<a class="ae lm" href="#da43" rel="noopener ugc nofollow"> <em class="ll"> Sarsa </em> </a>，其中更新规则使用由跟随的策略选择的动作。</p><p id="04db" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">独臂土匪:</strong>见<a class="ae lm" href="#790b" rel="noopener ugc nofollow"> <em class="ll">土匪</em> </a> <em class="ll">。</em></p><p id="532b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">一步到位 TD: </strong>见<a class="ae lm" href="#d0d7" rel="noopener ugc nofollow"> <em class="ll">时间差</em> </a> <em class="ll">。</em></p><p id="a76c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">策略(π): </strong>策略，记为<em class="ll"> π </em>(有时也称为<em class="ll"> π(a|s) </em>)，是从某个<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <em class="ll"> s </em>到给定该状态下选择每个可能的<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a> <em class="ll"> </em>的概率的映射。例如，一个<a class="ae lm" href="#15c5" rel="noopener ugc nofollow"> <em class="ll">贪婪策略</em> </a>为每个状态输出具有最高期望<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a>的动作。</p><p id="9d8f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> Q-Learning: </strong> Q-Learning 是一种<a class="ae lm" href="#44d1" rel="noopener ugc nofollow"><em class="ll">off-policy</em></a><em class="ll"/><a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"><em class="ll">强化学习</em> </a>算法，被认为是非常基础的算法之一。在其最简化的形式中，它使用一个表来存储所有可能的<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <em class="ll"> - </em> <a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>对的所有<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a>。它使用<a class="ae lm" href="#fbd3" rel="noopener ugc nofollow"> <em class="ll">贝尔曼方程</em> </a>更新该表，而动作选择通常使用<a class="ae lm" href="#15c5" rel="noopener ugc nofollow"><em class="ll">ε-贪婪策略</em> </a> <em class="ll">进行。<br/> </em>以其最简单的形式(无不确定性<a class="ae lm" href="#c274" rel="noopener ugc nofollow"><em class="ll"/></a>-状态跃迁和预期<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"><em class="ll"/></a>)，Q-Learning 的更新规则是:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/cb41eb039317524e654db727dcb8db06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8ZP4tTFqDGyezwJ8jR8eA.png"/></div></div></figure><p id="a9f2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">它的一个更复杂的版本，尽管更受欢迎，是<em class="ll">深度 Q-网络</em>变体(有时甚至简称为<em class="ll">深度 Q-学习</em>或简称为<em class="ll">Q-学习</em>)。这种变体用神经网络代替状态-动作表，以便处理大规模任务，其中可能的状态-动作对的数量可能是巨大的。你可以在<a class="ae lm" href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" rel="noopener">这篇博文</a>中找到这个算法的教程。</p><p id="f366" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu"> Q 值(Q 函数):</strong>通常表示为<em class="ll"> Q(s，a) </em>(有时带π下标，有时表示为<em class="ll"> Q(s，a；θ) </em>中<a class="ae lm" href="#aa91" rel="noopener ugc nofollow"> <em class="ll">深 RL </em> </a>)，Q 值是整体预期<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>假设<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>处于<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em></a><em class="ll"/>并执行<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>它的名字是“质量”一词的缩写，数学上定义为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/1c34d3fce90a522a8b116e50292132a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*OQqePNtxQV177JeQR1O16g.png"/></div></figure><p id="c8d9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中<em class="ll"> N </em>是从状态<em class="ll"> s </em>到终止状态的状态数，γ是<a class="ae lm" href="#4ee6" rel="noopener ugc nofollow"> <em class="ll">折扣因子</em> </a>，r⁰是在状态<em class="ll"> s </em>执行动作<em class="ll"> a </em>后收到的即时奖励。</p><p id="41f4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">强化算法:</strong>强化算法是<a class="ae lm" href="#fc9f" rel="noopener ugc nofollow"> <em class="ll">强化学习</em> </a> <em class="ll"> </em>算法<strong class="kr iu"> </strong>的一个家族，它根据策略相对于策略参数的梯度来更新自己的<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a> <em class="ll"> </em>参数<em class="ll"/><a class="ae lm" href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。该名称通常只使用大写字母，因为它最初是原始算法组设计的缩写:"<strong class="kr iu">RE</strong>ward<strong class="kr iu">I</strong>Non negative<strong class="kr iu">F</strong>actor<em class="ll">x</em>T42】Offset<strong class="kr iu">R</strong>E enforcement<em class="ll">x</em>Ccharacter istic<strong class="kr iu">E</strong></p><p id="fc9f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">强化学习(RL): </strong>与监督学习和非监督学习一样，强化学习是机器学习和人工智能的主要领域之一。它关注的是一个任意存在的学习过程，正式名称为<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">智能体</em> </a>，在其周围的世界中，称为<a class="ae lm" href="#4311" rel="noopener ugc nofollow"> <em class="ll">环境</em> </a>。智能体寻求从环境中获得的<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>最大化，并执行不同的<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>以学习环境如何反应并获得更多奖励。RL 任务的最大挑战之一是将行为与延迟奖励相关联——延迟奖励是代理在产生奖励的行为做出很久之后才收到的奖励。因此，它被大量用于解决不同类型的游戏，从井字游戏，国际象棋，雅达利 2600 和所有的方式去和星际争霸。</p><p id="6a6f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">奖励:</strong>由<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>从<a class="ae lm" href="#4311" rel="noopener ugc nofollow"> <em class="ll">环境</em> </a>中接收的一个数值，作为对代理<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>的直接响应。代理人的目标是在一集<a class="ae lm" href="#601d" rel="noopener ugc nofollow"><em class="ll"/></a>中最大化其收到的总奖励，因此奖励是代理人以期望的行为行动所需的动机。所有的行动都会产生奖励，大致可以分为三种类型:<em class="ll">积极奖励</em>强调想要的行动，<em class="ll">消极奖励</em>强调行动者应该偏离的行动，<em class="ll">零</em>，这意味着行动者没有做任何特殊或独特的事情。</p><p id="da43" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">Sarsa:</strong>Sarsa 算法基本上是一种<a class="ae lm" href="#9d8f" rel="noopener ugc nofollow"> <em class="ll"> Q-Learning </em> </a>算法，只是稍加修改，使其成为一种<a class="ae lm" href="#44d1" rel="noopener ugc nofollow"> <em class="ll"> on-policy </em> </a>算法。Q-学习更新规则是基于<a class="ae lm" href="#fbd3" rel="noopener ugc nofollow"><em class="ll"/></a>贝尔曼方程求最优<a class="ae lm" href="#f366" rel="noopener ugc nofollow"><em class="ll"/></a>的 Q 值，因此在<a class="ae lm" href="#c274" rel="noopener ugc nofollow"><em class="ll"/></a>-状态-跃迁和期望<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"><em class="ll"/></a>不确定的情况下，Q-学习更新规则是:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/cb41eb039317524e654db727dcb8db06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8ZP4tTFqDGyezwJ8jR8eA.png"/></div></div></figure><p id="b7e4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了将其转换为基于策略的算法，最后一项被修改:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/477de2cdeb079d3efd4f4a900d785e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdnBhX5TYGLSj7xGAwe6Xw.png"/></div></div></figure><p id="90a9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">当此处，两个<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a> <em class="ll"> a </em>和<em class="ll">a’</em>都由同一个<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em> </a>选择。算法的名字来源于它的更新规则，更新规则基于(<em class="ll"> s，a，r，s’，a’</em>)，都来自同一个策略。</p><p id="c274" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">状态:</strong>在<a class="ae lm" href="#4311" rel="noopener ugc nofollow"><em class="ll"/></a>环境中<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">智能体</em> </a>遭遇的每一个场景都正式称为<em class="ll">状态。</em>代理通过执行<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>在不同状态之间转换。还值得一提的是<em class="ll">终端状态</em>，它标志着一集<a class="ae lm" href="#601d" rel="noopener ugc nofollow"><em class="ll"/></a><em class="ll">的结束。</em>达到终态后就没有可能的状态了，新的一集开始了。很多时候，一个终止状态被表示为一个特殊的状态，其中所有动作都转换到同一个终止状态，并且<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a> 0。</p><p id="40ff" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">状态-值函数:</strong>见<a class="ae lm" href="#680c" rel="noopener ugc nofollow"> <em class="ll">值函数</em> </a> <em class="ll">。</em></p><p id="d0d7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">时间差分(TD): </strong>时间差分是一种结合了<a class="ae lm" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态规划</a>和<a class="ae lm" href="#f9e0" rel="noopener ugc nofollow"> <em class="ll">蒙特卡罗</em> </a>原理的学习方法；它像蒙特卡罗一样“在飞行中”学习，但像动态编程一样更新它的估计。最简单的时间差分算法之一被称为<em class="ll">一步 TD </em>或<em class="ll"> TD(0) </em>。根据以下更新规则更新<a class="ae lm" href="#680c" rel="noopener ugc nofollow"> <em class="ll">值函数</em> </a>:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nm"><img src="../Images/08fb27fa67287074aba35194ec74bcf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jLeVw2pIpOfknG5V8ySOhQ.png"/></div></div></figure><p id="cd60" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中<em class="ll"> V </em>为价值函数，<em class="ll"> s </em>为<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a>，<em class="ll"> r </em>为<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>，<em class="ll"> γ </em>为<a class="ae lm" href="#4ee6" rel="noopener ugc nofollow"> <em class="ll">贴现因子</em> </a>，<em class="ll"> α </em>为学习率，方括号中的项被称为<em class="ll">时间差误差。</em></p><p id="4371" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">端子状态:</strong>见<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <em class="ll">。</em></p><p id="930e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">上自信界(UCB):</strong>【UCB】是一种<a class="ae lm" href="#b903" rel="noopener ugc nofollow"> <em class="ll">探索</em> </a>方法，它试图确保每个<a class="ae lm" href="#8751" rel="noopener ugc nofollow"> <em class="ll">动作</em> </a>都被很好地探索。考虑一个完全随机的探索<a class="ae lm" href="#a76c" rel="noopener ugc nofollow"> <em class="ll">策略</em></a><em class="ll"/>——意思是，每个可能的动作都有相同的机会被选中。有些行动可能会比其他行动探索得更多。一个动作被选中得越少，这个<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>就越没有信心能左右其预期的<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>，而其<a class="ae lm" href="#b903" rel="noopener ugc nofollow"> <em class="ll">剥削</em> </a>阶段就可能受到伤害。UCB 的探索考虑了每个动作被选择的次数，并给予那些探索较少的动作额外的权重。从数学上来说，所选择的动作是通过以下方式挑选的:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nl"><img src="../Images/26df397e459c852ca806c0fb2b6bb122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5haNfed96VexQNtBOw2tYg.png"/></div></div></figure><p id="8f30" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中<em class="ll"> R(a) </em>是动作<em class="ll"> a </em>的预期总体回报，<em class="ll"> t </em>是采取的步骤数(总共选择了多少个动作)，<em class="ll"> N(a) </em>是选择动作<em class="ll"> a </em>的次数，<em class="ll"> c </em>是可配置的超参数。这种方法有时也被称为“通过乐观进行探索”，因为它赋予较少探索的行为更高的价值，鼓励模型选择它们。</p><p id="680c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated"><strong class="kr iu">价值函数:</strong>通常表示为<em class="ll"> V(s) </em>(有时带有π下标)，价值函数是对整体预期<a class="ae lm" href="#6a6f" rel="noopener ugc nofollow"> <em class="ll">奖励</em> </a>的度量假设<a class="ae lm" href="#b6a2" rel="noopener ugc nofollow"> <em class="ll">代理</em> </a>处于<a class="ae lm" href="#c274" rel="noopener ugc nofollow"> <em class="ll">状态</em> </a> <em class="ll"> s </em>然后继续播放直到<a class="ae lm" href="#601d" rel="noopener ugc nofollow"> <em class="ll">第一集</em> </a>结束它在数学上定义为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/71f601f23a25c771eb144cc7a23961b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*UOfJo9BjoQfNcC2x0hV_Zw.png"/></div></figure><p id="8354" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">虽然它看起来确实类似于<a class="ae lm" href="#f366" rel="noopener ugc nofollow"> <em class="ll"> Q 值</em> </a>的定义，但有一个隐含的——但很重要的——区别:对于<em class="ll">n = 0</em>,<em class="ll">v(s)</em>的奖励 r⁰是刚处于状态<em class="ll"> s </em>、<em class="ll">在</em>进行任何动作之前的预期奖励，而在 q 值中，r⁰是在进行某个动作之后的预期奖励<em class="ll">。这种差异也产生了<a class="ae lm" href="#efa6" rel="noopener ugc nofollow"> <em class="ll">优势函数</em> </a>。</em></p></div></div>    
</body>
</html>