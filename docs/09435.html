<html>
<head>
<title>An accidental side effect of text mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本挖掘的意外副作用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-accidental-side-effect-of-text-mining-4b43f8ee1273?source=collection_archive---------25-----------------------#2019-12-12">https://towardsdatascience.com/an-accidental-side-effect-of-text-mining-4b43f8ee1273?source=collection_archive---------25-----------------------#2019-12-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/349f0355d83452bfc34a13cf88fe9051.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8O1lWQkX05rv3vPJ00FBqg.png"/></div></div></figure><div class=""/><div class=""><h2 id="8383" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">文本挖掘真的有用吗？第一手测试。</h2></div><blockquote class="kq"><p id="12af" class="kr ks jb bd kt ku kv kw kx ky kz la dk translated">“我们探索的终点将是到达我们开始的地方，并第一次了解这个地方。”</p><p id="0097" class="kr ks jb bd kt ku lb lc ld le lf la dk translated">—t·s·艾略特</p></blockquote><p id="8dd4" class="pw-post-body-paragraph lg lh jb li b lj lk kc ll lm ln kf lo lp lq lr ls lt lu lv lw lx ly lz ma la ij bi translated">我最近读了很多关于生产力和自我发展的书，经常会发现一些我想以后再读的建议。kindle 上的高亮选项使这变得非常容易。通过持续阅读和强调，我积累了大量的文本，这些文本很好地代表了我读过的书。</p><p id="c78e" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">因为我对内容非常了解。我想对这些数据应用<strong class="li jc">文本挖掘</strong>和<strong class="li jc">情感分析</strong>，这样我就可以将结果与我对书籍的真实看法进行比较。如果它们匹配，我会更有信心将其应用于我的下一个业务问题。</p><p id="fa6a" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">在某种程度上，这也是一次自我探索，因为我可以回答一些问题，比如我更喜欢什么样的内容，他们的情感诉求是什么？</p><p id="6b19" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">让我们来寻找答案。</strong></p><p id="c98c" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">客观的说，我会创造一个独立的角色，叫做“蝙蝠”。他的下一个任务是黑进我的数据并分析它，为卖给我更多的书收集见解。可惜读书不是他的特长。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="b214" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi mn translated">他进来时，房间里乱七八糟。当他把 USB 驱动器插在笔记本电脑上时，他几乎听不到还开着的收音机。记者:</p><blockquote class="mw mx my"><p id="a5ea" class="lg lh mz li b lj mb kc ll lm mc kf lo na md lr ls nb me lv lw nc mf lz ma la ij bi translated">从我们一大早打开手机处理 Whatsapp 或脸书信息或推文形式的大量信息的那一刻起，直到我们在晚上睡觉时重写或阅读产品评论，我们在互联网上留下了面包屑给自己的个人口味。</p><p id="da0c" class="lg lh mz li b lj mb kc ll lm mc kf lo na md lr ls nb me lv lw nc mf lz ma la ij bi translated">许多企业使用这种非结构化数据，通过有针对性的产品推荐进行更好的营销来推动销售，或者隔离他们的客户…</p></blockquote><p id="5899" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">看到 28 本书 21000 行文字的数据，他咬紧牙关。他的第一次接触是卡罗尔·德韦克的书<a class="ae nd" href="https://www.amazon.com/Mindset-Psychology-Carol-S-Dweck/dp/0345472322" rel="noopener ugc nofollow" target="_blank">“心态”</a>，书中她介绍了<a class="ae nd" href="https://www.mindsetworks.com/science/" rel="noopener ugc nofollow" target="_blank"> <strong class="li jc">成长心态</strong>的概念。背后的想法是，有成长心态的人相信他们的能力可以通过努力来提高，而有固定心态的人相信他们的能力在出生时就固定了。结果，有固定思维模式的人错过了在许多事情上做得更好的机会，尽管他们可以做到。很简单，因为他们一开始就不相信。</a></p><p id="5deb" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">如果你对这个概念不熟悉，这里有一段卡罗尔·德韦克的视频，解释她对成长心态的研究。</p><figure class="ne nf ng nh gt is"><div class="bz fp l di"><div class="ni nj l"/></div><figcaption class="nk nl gj gh gi nm nn bd b be z dk">Carol Dweck explaining her research on growth mindset.</figcaption></figure><p id="804e" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">长长的文字让他疲惫不堪，他没有意识到时间是如何流逝的。现在，他是一个成长中的人。他决定学习文本挖掘。</p><p id="eb9b" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">他开始学习用于文本挖掘的 R 包，他不喜欢这个包的名字<a class="ae nd" href="https://www.tidytextmining.com/" rel="noopener ugc nofollow" target="_blank"> tidytext </a>，但是他正在稍微失去他的偏见。这是一个漫长的夜晚。太阳慢慢升起时，他趴在桌子上睡着了。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="a847" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">它照亮了我的后花园，在那里我可以不时地瞥一眼被一夜⛄️.的雪涂成的树我不知道镇上的另一个地方发生了什么，我继续阅读，一边喝着红酒一边点着我的 kindle🍷。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="4c70" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我们稍后会知道我们的黑客发生了什么。首先，我们一起去翻翻他的笔记本。</p><p id="ac76" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">黑客笔记</strong></p><p id="985b" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">这是导出的 kindle highlights 的样子。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/3c0355471333b87ee106f93204c0b536.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHFpWG-jUr297bzgWXe_fQ.png"/></div></div></figure><p id="129b" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">读取并解析文本文件</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="41d9" class="nu nv jb nq b gy nw nx l ny nz"># Use readLines function to parse the text file</span><span id="c590" class="nu nv jb nq b gy oa nx l ny nz">highlights &lt;- readLines("Kindle_highlights_Serdar.Rmd", encoding = "UTF-8")</span><span id="9d01" class="nu nv jb nq b gy oa nx l ny nz"># Create a dataframe where each row is a line from the text</span><span id="0470" class="nu nv jb nq b gy oa nx l ny nz">df &lt;- data.frame(highlights)</span><span id="d0c2" class="nu nv jb nq b gy oa nx l ny nz"># Packages<br/>library(tidyverse)   <br/><em class="mz"># includes </em><strong class="nq jc"><em class="mz">ggplot2</em></strong><em class="mz">, </em><strong class="nq jc"><em class="mz">dplyr</em></strong><em class="mz">, tidyr, readr, purrr, </em><strong class="nq jc"><em class="mz">tibble</em></strong><em class="mz">, </em><strong class="nq jc"><em class="mz">stringr</em></strong><em class="mz">, forcats</em></span><span id="8340" class="nu nv jb nq b gy oa nx l ny nz">library(tidytext)<br/>library(wordcloud2)</span></pre><p id="6648" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">每个数据科学项目都需要某种数据准备。<strong class="li jc">停用词</strong>通常是语言中最常见的词，通常在处理文本数据之前被过滤掉。</p><p id="93fe" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">让我们看看 tidytext 包中的<strong class="li jc"> stop_words </strong>数据集。因为这是一个很长的单词列表(&gt; 1K) <strong class="li jc">我将打印每第五个单词作为例子。</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="cc9d" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc">data(stop_words)</strong></span><span id="8ffc" class="nu nv jb nq b gy oa nx l ny nz"># print every 50th word </span><span id="98c6" class="nu nv jb nq b gy oa nx l ny nz">stop_words_small &lt;- <strong class="nq jc">stop_words[seq(1, nrow(stop_words), 50),]</strong></span><span id="fee8" class="nu nv jb nq b gy oa nx l ny nz">stop_words_small %&gt;% print(n=50)</span><span id="a138" class="nu nv jb nq b gy oa nx l ny nz"># A tibble: 23 x 2<br/>   word       lexicon <br/>   &lt;chr&gt;      &lt;chr&gt;   <br/> 1 a          SMART   <br/> 2 at         SMART   <br/> 3 contain    SMART   <br/> 4 few        SMART   <br/> 5 hers       SMART   <br/> 6 last       SMART   <br/> 7 nine       SMART   <br/> 8 presumably SMART   <br/> 9 some       SMART   <br/>10 they'd     SMART   <br/>11 very       SMART   <br/>12 without    SMART   <br/>13 what       snowball<br/>14 they'll    snowball<br/>15 during     snowball<br/>16 again      onix    <br/>17 but        onix    <br/>18 finds      onix    <br/>19 if         onix    <br/>20 much       onix    <br/>21 parted     onix    <br/>22 since      onix    <br/>23 under      onix</span></pre><p id="50fe" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">仔细观察可以发现，停用词使用单引号，而在文本数据中使用撇号(')。</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="c207" class="nu nv jb nq b gy nw nx l ny nz">e.g. <strong class="nq jc">they'll  </strong>in stop_words</span></pre><p id="1acf" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">以及单词<strong class="li jc">他们将</strong>如何出现在文本中:</p><blockquote class="mw mx my"><p id="4887" class="lg lh mz li b lj mb kc ll lm mc kf lo na md lr ls nb me lv lw nc mf lz ma la ij bi translated">黄色高亮| Page: 200 <br/>记忆随着我们从中获得的意义不断被修正，因此在未来<strong class="li jc">它们会</strong>更加有用。</p></blockquote><p id="3d3a" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我们必须使停用词和我们的数据兼容，否则一些词如<strong class="li jc">他们会，不会，不能可能出现在我们的结果中。</strong></p><p id="5b7a" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我们可以使用 stringr 包中的<strong class="li jc"> str_replace_all() </strong>函数，找出所有的撇号，转换成单引号。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/0ee70253d22ff3002fb1c1c2c3bf7b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wGd8zGdlOuKKLP5Ixd8hYA.png"/></div></div></figure><p id="fe11" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">现在，文本已经准备好进行频率分析了。文本挖掘项目中的单词称为<strong class="li jc">记号。</strong>我们可以通过 tidytext 包中的<strong class="li jc"> unnest_tokens() </strong>函数将文本拆分成单个单词，过滤<strong class="li jc"> stop_words </strong>并计数。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="a45a" class="nu nv jb nq b gy nw nx l ny nz">df$highlights &lt;- str_replace_all(df$highlights, "’", "'")</span><span id="0b3f" class="nu nv jb nq b gy oa nx l ny nz">df &lt;- df %&gt;% unnest_tokens(word, highlights) %&gt;%<br/>             anti_join(stop_words) %&gt;% <br/>             filter(!word %in% c("highlights","highlight", "page", <br/>                      "location", "yellow", "pink", "orange", "blue"))</span></pre><p id="0dfe" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">他还在这里添加了一些经常出现在 kindle highlights 输出中的额外单词。</p><p id="be95" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">Dplyr()包函数对于对数据框中的单词进行分组和计数非常有用。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="72fa" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc">top_kindle_highlights</strong> &lt;- df %&gt;% <br/> group_by(word) %&gt;% <br/> count() %&gt;% <br/> arrange(desc(n))</span></pre><p id="58b9" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">他记下了他的第一个见解。<strong class="li jc">我的 kindle 最常用的 10 个单词。</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="91f0" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc">people</strong> 592   <br/><strong class="nq jc">story</strong>  340   <br/>life   318   <br/>time   309   <br/><strong class="nq jc">mind </strong>  213   <br/><strong class="nq jc">change</strong> 212   <br/>feel   211   <br/>world  171   <br/>person 170   <br/><strong class="nq jc">habits</strong> 157   </span></pre><p id="a3d5" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">如果你不喜欢看一长串单词<strong class="li jc">的话，单词云</strong>和<strong class="li jc">T3 都是不错的选择。Wordcloud2 包为你的 Wordcloud 提供了额外的定制选项，例如你可以使用任何图像作为标记。</strong></p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/48cf04c91d03cde3c72aa8b42ed54006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AjQEHBc_CDSZlOjX9T9aCg.png"/></div></div></figure><p id="4b7b" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">上面的单词云代码是:</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="1c46" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc">wordcloud2</strong>(top_kindle_highlights, figPath = bat, size = 1, backgroundColor = "white", color = color_vector(data$freq) )</span></pre><p id="d0fa" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">一些想法开始出现在他的脑海里。他认为做出这些亮点的人是对讲故事、写作和良好的沟通感兴趣的人，是有良好习惯的人。想以积极的方式影响自己生活的人。<strong class="li jc">他对书籍越来越感兴趣。他想深入挖掘。</strong></p><p id="743d" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">二元模型分析</strong></p><p id="189c" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">单词是书籍内容的一个很好的起点。<strong class="li jc">但是没有上下文，它们就受到限制。</strong>还可以执行频率分析来测量单词对<strong class="li jc">(二元模型)</strong>在文本中出现的频率。这使我们能够捕捉到文本中更细微的细节。</p><p id="59d2" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">为此，他将上面隔离的未嵌套的单个标记组合回一个连续文本，然后执行二元语法分析。您可以使用 stringr 包中的<strong class="li jc"> str_c() </strong>函数来连接单个单词。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="6c46" class="nu nv jb nq b gy nw nx l ny nz">df_com &lt;- <strong class="nq jc">str_c</strong>(df$word, “ “) <br/>df_com &lt;- data.frame(df_com)</span></pre><p id="d8e2" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">让我们将文本拆分成二元模型，并找出最常见的。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="01ac" class="nu nv jb nq b gy nw nx l ny nz">df_bigram &lt;- df_com %&gt;% <br/> <strong class="nq jc">unnest_tokens</strong>(bigram, df_com, token = “ngrams”, <br/> n = 3, n_min = 2)</span><span id="8e16" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">top_bigrams</strong> &lt;- df_bigram %&gt;% <br/> group_by(bigram) %&gt;% <br/> count() %&gt;% <br/> arrange(desc(n))%&gt;% <br/> print(n=20)</span></pre><p id="8cd3" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">并将它们可视化在图上</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="26dd" class="nu nv jb nq b gy nw nx l ny nz">top &lt;- top_bigrams[1:25,]<br/> <br/>top %&gt;% ungroup() %&gt;% mutate(bigram = fct_reorder(bigram, n)) %&gt;% <br/> ggplot(aes(x=bigram, y=n)) + <br/> geom_col() + <br/> coord_flip() +<br/> theme_classic() + <br/> theme(legend.position = “none”,<br/> text = element_text(size=18)) </span></pre><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/ea57c7de08a3cb69ebcaee7632c143ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*__cRQTsUYd1Nj9CQ9BZoMw.png"/></div></div></figure><p id="1441" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我发现最常见的二元模型之一是<strong class="li jc">行为变化。我们可以利用这些信息来理解我们之前的发现。例如，一个最常见的词是变化。我们从二元模型分析中看到,“改变”这个词主要用在行为改变的上下文中。<strong class="li jc">所以二元模型是获得关于文本内容的更深层次见解的有用工具。</strong></strong></p><p id="05fb" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我突出显示的文本数据来自 28 本不同的书，通过查看整个文档中最常见的单词和二元模型，我们对它们有了一个概述。</p><p id="4fa9" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">为了了解每本书的不同之处，我们可以对每本书重复这一过程。</p><p id="b87d" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">但是我们如何单独捕捉它们呢？</p><p id="c972" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">让我们先把课文再看一遍。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/80918f8f4a124aa70f286e0fa730ca98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*BBWoKfPi5ttH8dCSourjRw.png"/></div></figure><p id="4eab" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">在每本书之前都会出现<strong class="li jc">“您的 Kindle 笔记:”</strong>。让我们找出每本书的开头和结尾的行号，并使用这些索引来找出每本书。</p><p id="e9b4" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我们将重用上面创建的数据帧 df。<strong class="li jc"> str_which() </strong>函数返回包含给定输入模式的行索引号。</p><p id="bd69" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">在最后一步，一个 for 循环<strong class="li jc">捕获两个连续索引之间的文本</strong>将给出它们之间的书。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="1e74" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc"># Getting the index number for each book</strong></span><span id="9e18" class="nu nv jb nq b gy oa nx l ny nz">indexes &lt;- <strong class="nq jc">str_which</strong>(df$highlights, <strong class="nq jc">pattern</strong> = fixed(<strong class="nq jc">"Your Kindle Notes For"</strong>))<br/><strong class="nq jc">book_names</strong> &lt;- df$highlights[indexes + 1]<br/><strong class="nq jc">indexes</strong> &lt;-  c(indexes,nrow(df))</span><span id="1e0c" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc"># Create an empty list </strong></span><span id="75be" class="nu nv jb nq b gy oa nx l ny nz">books &lt;- list()</span><span id="b5d3" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc"># Now the trick. Capture each 28 book separately in a list. </strong></span><span id="8f18" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">for</strong>(i in 1:(length(indexes)-1)) {<br/>    books[[i]] &lt;- data.frame(df$highlights[(indexes[i]:indexes[i+1]-1)])<br/>    colnames(books[[i]]) &lt;- "word_column"<br/>    books[[i]]$word_column &lt;- as.character(books[[i]]$word_column)<br/>}<br/></span></pre><p id="cd84" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">让我们看看它是否有效，例如你可以在我们的列表中查找第五本书。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="36fa" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc">head(books[[5]])</strong></span><span id="80aa" class="nu nv jb nq b gy oa nx l ny nz">                                         word_column<br/>1                                                    <br/>2                              Your Kindle Notes For:<br/>3 Bird by Bird: Some Instructions on Writing and Life<br/>4                                         Anne Lamott<br/>5             Last accessed on Saturday July 27, 2019<br/>6                         75 Highlight(s) | 4 Note(s)</span></pre><p id="7570" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">现在，我们捕获了所有 28 本书，我们可以应用相同的过程，通过另一个 for 循环来分析它们。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="ef31" class="nu nv jb nq b gy nw nx l ny nz">top &lt;- list()</span><span id="7d13" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">for</strong>(i in 1:28){<br/><strong class="nq jc">books[[i]]</strong> &lt;- books[[i]] %&gt;% <strong class="nq jc">unnest_tokens</strong>(word, word_column) %&gt;%<br/>             anti_join(stop_words) %&gt;% <br/>             filter(!word %in% c("highlights","highlight", "page", <br/>                      "location", "yellow", "pink", "orange", "blue"))</span><span id="2219" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc"># Find out the top words in each book and capture them in a list (top)</strong></span><span id="d2f4" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc"><br/>top[[i]]</strong> &lt;- books[[i]] %&gt;% <br/>              group_by(word) %&gt;% <br/>              count() %&gt;% <br/>              arrange(desc(n))</span><span id="e831" class="nu nv jb nq b gy oa nx l ny nz">}</span><span id="5533" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">for(i in 1:28)</strong>{<br/>  print(book_names[[i]])<br/>  print(top[[i]])<br/>}</span></pre><p id="bfb8" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">这是上面代码输出的一部分。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="ed92" class="nu nv jb nq b gy nw nx l ny nz">## [1] "Crucial Conversations Tools for Talking When Stakes Are High, Second Edition"</span><span id="d67f" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 1,834 x 2<br/>## # Groups:   word [1,834]<br/>##    word          n<br/>##    &lt;chr&gt;     &lt;int&gt;<br/>##  1 people       84<br/>##  2 dialogue     40<br/>##  3 stories      40<br/>##  4 due          34<br/>##  5 export       33<br/>##  6 feel         33<br/>##  7 hidden       33<br/>##  8 limits       33<br/>##  9 truncated    33<br/>## 10 crucial      31<br/>## # ... with 1,824 more rows</span><span id="0239" class="nu nv jb nq b gy oa nx l ny nz">## [1] "Pre-Suasion: A Revolutionary Way to Influence and Persuade"</span><span id="844e" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 526 x 2<br/>## # Groups:   word [526]<br/>##    word             n<br/>##    &lt;chr&gt;        &lt;int&gt;<br/>##  1 attention        6<br/>##  2 influence        5<br/>##  3 mental           5<br/>##  4 trust            5<br/>##  5 visitors         5<br/>##  6 comfort          4<br/>##  7 emotional        4<br/>##  8 experience       4<br/>##  9 message          4<br/>## 10 associations     3<br/>## # ... with 516 more rows</span><span id="f368" class="nu nv jb nq b gy oa nx l ny nz">## [1] "Made to Stick: Why some ideas take hold and others come unstuck"</span><span id="3beb" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 1,754 x 2<br/>## # Groups:   word [1,754]<br/>##    word          n<br/>##    &lt;chr&gt;     &lt;int&gt;<br/>##  1 people       64<br/>##  2 knowledge    27<br/>##  3 story        25<br/>##  4 ideas        24<br/>##  5 concrete     18<br/>##  6 surprise     17<br/>##  7 care         16<br/>##  8 time         15<br/>##  9 attention    14<br/>## 10 core         14</span></pre><p id="4553" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">现在，看看每本书中最常用的单词，我们可以更深入地了解它们是关于什么的。</p><p id="dc4c" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">同一本书的二元模型。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="62b9" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc">## [1] "Crucial Conversations Tools for Talking When Stakes Are High, Second Edition"</strong></span><span id="0e35" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 8,774 x 2<br/>## # Groups:   bigram [8,774]<br/>##    bigram                    n<br/>##    &lt;chr&gt;                 &lt;int&gt;<br/><strong class="nq jc">##  1 due export               33<br/>##  2 due export limits        33<br/>##  3 export limits            33<br/>##  4 hidden truncated         33<br/>##  5 hidden truncated due     33<br/>##  6 truncated due            33<br/>##  7 truncated due export     33<br/>##  8 crucial conversations    19<br/>##  9 export limits 27         10<br/>## 10 limits 27                10</strong><br/>## # ... with 8,764 more rows</span><span id="6f84" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">## [1] "Pre-Suasion: A Revolutionary Way to Influence and Persuade"</strong><br/>## # A tibble: 1,265 x 2<br/>## # Groups:   bigram [1,265]<br/>##    bigram                      n<br/>##    &lt;chr&gt;                   &lt;int&gt;<br/>##  1 attention goal              2<br/>##  2 concept audience            2<br/>##  3 levels importance           2<br/>##  4 mandel johnson              2<br/>##  5 mental activity             2<br/>##  6 social proof                2<br/>##  7 thousand dollars            2<br/>##  8 twenty thousand             2<br/>##  9 twenty thousand dollars     2<br/>## 10 writing session             2<br/>## # ... with 1,255 more rows</span><span id="6c7a" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">## [1] "Made to Stick: Why some ideas take hold and others come unstuck"</strong></span><span id="fbbe" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 6,376 x 2<br/>## # Groups:   bigram [6,376]<br/>##    bigram                      n<br/>##    &lt;chr&gt;                   &lt;int&gt;<br/>##  1 curse knowledge             7<br/>##  2 guessing machines           6<br/>##  3 people care                 6<br/>##  4 goodyear tires              5<br/>##  5 knowledge gaps              5<br/>##  6 people's attention          5<br/>##  7 popcorn popper              5<br/>##  8 security goodyear           5<br/>##  9 security goodyear tires     5<br/>## 10 sinatra test                5</span></pre><p id="c740" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">现在，你可能在第一本书里看到了奇怪的二元模型。Kindle 限制你可以高亮显示的文本长度，例如你不能高亮显示 5 页的文本。这可以防止人们突出显示整本书并导出到 word 文档。</p><p id="1535" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">因为我偶尔会高亮显示很长的文本，所以像“到期”、“导出”和“限制”这样的词会作为警告出现在我导出的高亮显示上。</p><p id="49a9" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">现在，我将返回并通过将这些单词添加到 filter()函数中来进行更多的清理。</p><p id="4659" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">一本书一本书地看着，他对我 kindle 里的书越来越着迷。他决定订购几个。</strong></p><p id="e3be" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><em class="mz">如果你想看这个捕捉过程的另一个例子，你可以在这里看看我最近的</em> <a class="ae nd" href="https://dataatomic.com/r/data-wrangling-text-mining/" rel="noopener ugc nofollow" target="_blank"> <em class="mz">帖子。</em>T9】</a></p><p id="0e37" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我们可以使用<strong class="li jc">情感分析</strong>来评估文本数据中的情感负荷。最常见的用途是社交媒体监控、客户体验管理和客户之声，以了解他们的感受。</p><blockquote class="mw mx my"><p id="131d" class="lg lh mz li b lj mb kc ll lm mc kf lo na md lr ls nb me lv lw nc mf lz ma la ij bi translated">bing 词典以二进制方式将单词分为积极和消极两类。nrc 的词典使用了积极、消极、愤怒、期待、厌恶、恐惧、快乐、悲伤、惊讶和信任等类别。</p></blockquote><p id="22f5" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">使用必应词典</strong></p><p id="dd66" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我列出了对每个情感类别贡献最大的单词。例如成功和有效对于积极情绪，不好和困难对于消极情绪。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/958633982dcc78f15674c13c612a346d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AydS4md-jH3_mWAfKYpowQ.png"/></div></div></figure><p id="9fa2" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">下面是 R 是如何制作出上述情节的:</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="615b" class="nu nv jb nq b gy nw nx l ny nz">bing_word_counts &lt;- df %&gt;% inner_join(get_sentiments("bing")) %&gt;% <br/>  count(word, sentiment, sort = TRUE) %&gt;%<br/>  ungroup()</span><span id="2d47" class="nu nv jb nq b gy oa nx l ny nz">bing_word_counts</span><span id="1879" class="nu nv jb nq b gy oa nx l ny nz"># <strong class="nq jc">Sentiment plot for top positive negative contributors</strong><br/># Select top 10 positive and negative words</span><span id="ae84" class="nu nv jb nq b gy oa nx l ny nz">bing &lt;- bing_word_counts %&gt;% <br/>  group_by(sentiment) %&gt;% <br/>  top_n(10) %&gt;% <br/>  ggplot(aes(reorder(word, n), n, fill=sentiment)) + <br/>  geom_bar(alpha=0.8, stat="identity", show.legend = FALSE)+<br/>  facet_wrap(~sentiment, scales = "free_y") +<br/>  labs(y= "Contribution to sentiment", x = NULL) +<br/>  coord_flip()</span><span id="27ce" class="nu nv jb nq b gy oa nx l ny nz">bing</span></pre><p id="14ab" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">使用 nrc lexion </strong></p><p id="fd04" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">如果一篇文章是正面的而不是负面的，我更有可能突出它。还有信任、期待和喜悦，而不是悲伤或愤怒。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="b125" class="nu nv jb nq b gy nw nx l ny nz">sentiment &lt;- df %&gt;%<br/>        left_join(get_sentiments("nrc")) %&gt;%<br/>        filter(!is.na(sentiment)) %&gt;%<br/>        count(sentiment, sort = TRUE)<br/>sentiment</span><span id="dbdd" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 10 x 2<br/>##    <strong class="nq jc">sentiment        n</strong><br/>##    &lt;chr&gt;        &lt;int&gt;<br/>##  1 positive      8471<br/>##  2 trust         4227<br/>##  3 negative      3963<br/>##  4 anticipation  3466<br/>##  5 joy           2701<br/>##  6 fear          2467<br/>##  7 sadness       1853<br/>##  8 anger         1814<br/>##  9 surprise      1353<br/>## 10 disgust       1102</span></pre><p id="68dd" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">正常化情绪</strong></p><p id="b81c" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">重要的是要补充一点，因为每种情感类别在一种语言中有不同数量的单词。字数少的情感类，在给定文本中出现的可能性更小。因此，<strong class="li jc">我想根据它们在词典中的出现频率对它们进行标准化，看看它与上面的结果有何不同。</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="2e49" class="nu nv jb nq b gy nw nx l ny nz"><strong class="nq jc"><em class="mz"># I will add numbers of each categories from the NRC lexicon</em></strong></span><span id="b850" class="nu nv jb nq b gy oa nx l ny nz">lexicon &lt;- c(2317, 3338, 1234, 842, 1483, 691, 1250, 1195, 1060, 535)<br/>polarity &lt;-  c(1,1,1,1,1,0,0,0,0,0)<br/>sentiment &lt;- data.frame(sentiment, lexicon)<br/>norm_sentiment &lt;- sentiment %&gt;% mutate( normalized = n/lexicon) %&gt;% arrange(desc(normalized))<br/>sentiment &lt;- data.frame(norm_sentiment, polarity)<br/>sentiment</span><span id="3d56" class="nu nv jb nq b gy oa nx l ny nz">##       sentiment    n lexicon <strong class="nq jc">normalized</strong> polarity<br/>## 1  anticipation 3466     842   4.116390        1<br/>## 2      positive 8471    2317   3.656021        1<br/>## 3          fear 2467     691   3.570188        1<br/>## 4      negative 3963    1234   3.211507        1<br/>## 5       disgust 1102     535   2.059813        1<br/>## 6           joy 2701    1483   1.821308        0<br/>## 7         anger 1814    1195   1.517992        0<br/>## 8       sadness 1853    1250   1.482400        0<br/>## 9      surprise 1353    1060   1.276415        0<br/>## 10        trust 4227    3338   1.266327        0</span></pre><p id="b930" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">现在，<strong class="li jc">期待</strong>是文本中发现的最高情感。在我看来，这不是巧合。因为我们分析的大部分书籍都是关于生产力和自我发展的。生产力提示和工具通常包含与预期相关的词汇。</p><p id="7079" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated"><strong class="li jc">同样，我可以看看个人对书籍的看法</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="b79a" class="nu nv jb nq b gy nw nx l ny nz">sentiment &lt;- list()<br/><strong class="nq jc">for</strong> (i <strong class="nq jc">in</strong> 1:28){<br/>sentiment[[i]] &lt;- books[[i]] %&gt;%<br/>        left_join(get_sentiments("nrc")) %&gt;%<br/>        filter(!is.na(sentiment)) %&gt;%<br/>        count(sentiment, sort = TRUE)<br/>        print(book_names[i])<br/>        print(sentiment[[i]])<br/>}</span></pre><p id="c93b" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">对个人书籍的看法。我在这里展示了其中的几个。</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="7de2" class="nu nv jb nq b gy nw nx l ny nz">## [1] "Crucial Conversations Tools for Talking When Stakes Are High, Second Edition"</span><span id="e595" class="nu nv jb nq b gy oa nx l ny nz">## # A tibble: 10 x 2<br/>##    sentiment        n<br/>##    &lt;chr&gt;        &lt;int&gt;<br/>##  1 positive       758<br/>##  2 negative       496<br/>##  3 trust          412<br/>##  4 fear           282<br/>##  5 anticipation   258<br/>##  6 anger          243<br/>##  7 joy            216<br/>##  8 sadness        196<br/>##  9 disgust        142<br/>## 10 surprise       108</span><span id="851c" class="nu nv jb nq b gy oa nx l ny nz">## Joining, by = "word"</span><span id="10df" class="nu nv jb nq b gy oa nx l ny nz">## [1] "Pre-Suasion: A Revolutionary Way to Influence and Persuade"<br/>## # A tibble: 10 x 2<br/>##    sentiment        n<br/>##    &lt;chr&gt;        &lt;int&gt;<br/>##  1 positive        84<br/>##  2 trust           51<br/>##  3 negative        31<br/>##  4 anticipation    27<br/>##  5 fear            24<br/>##  6 joy             22<br/>##  7 anger           14<br/>##  8 sadness         12<br/>##  9 surprise         9<br/>## 10 disgust          3</span><span id="347c" class="nu nv jb nq b gy oa nx l ny nz">## Joining, by = "word"</span><span id="6e9e" class="nu nv jb nq b gy oa nx l ny nz">## [1] "Made to Stick: Why some ideas take hold and others come unstuck"<br/>## # A tibble: 10 x 2<br/>##    sentiment        n<br/>##    &lt;chr&gt;        &lt;int&gt;<br/>##  1 positive       499<br/>##  2 trust          236<br/>##  3 anticipation   198<br/>##  4 negative       167<br/>##  5 joy            156<br/>##  6 fear           123<br/>##  7 surprise       107<br/>##  8 sadness         74<br/>##  9 anger           65<br/>## 10 disgust         60</span></pre><p id="cd1d" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">为了有一个概述，你可以通过绘制每本书的阳性率来总结数据。</p><figure class="ne nf ng nh gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi og"><img src="../Images/23e2fb0ffdc9bcc2790f852d316e81b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CFiaUMwbd-rZ6ZAlLJ8Sg.png"/></div></div></figure><p id="3202" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我们来看看积极性得分最低的那本书。<a class="ae nd" href="https://www.amazon.com/Mans-Search-Meaning-Viktor-Frankl/dp/080701429X" rel="noopener ugc nofollow" target="_blank"><strong class="li jc">男人的寻找意义</strong> </a> <strong class="li jc">。这本书是根据维克多·弗兰克在第二次世界大战期间的遭遇写成的。这也是意料之中的。</strong></p><blockquote class="mw mx my"><p id="8291" class="lg lh mz li b lj mb kc ll lm mc kf lo na md lr ls nb me lv lw nc mf lz ma la ij bi translated">我越来越感受到文本挖掘的力量。</p></blockquote><p id="f249" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">《局外人》这本书出现在积极性图的顶部，是一个真正的局外人。😮</p><p id="e219" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">从头开始了解一切是很难的，我们将回去做一些额外的清理。《局外人》一书的字数是 107 个。<strong class="li jc">这个真的很低。因此在下一次迭代中，我将从分析中删除它，因为它不可靠。</strong></p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="6de3" class="nu nv jb nq b gy nw nx l ny nz">book_names[[27]]</span><span id="dc99" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">## [1] "Outliers: The Story of Success"</strong></span><span id="3fe9" class="nu nv jb nq b gy oa nx l ny nz">top[[27]]</span><span id="8145" class="nu nv jb nq b gy oa nx l ny nz"># A tibble: 107 x 2<br/># Groups:   <strong class="nq jc">word [107]</strong><br/>word             n<br/>&lt;chr&gt;        &lt;int&gt;<br/>1 ability          3<br/>2 knowing          3<br/>3 sense            3<br/>4 communicate      2<br/>5 distance         2<br/>6 family           2<br/>7 intelligence     2<br/>8 power            2<br/>9 practical        2<br/>10 sternberg        2<br/># ... with 97 more rows</span></pre><p id="8bae" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">以下是我用来绘制积极性得分的代码:</p><pre class="ne nf ng nh gt np nq nr ns aw nt bi"><span id="7b18" class="nu nv jb nq b gy nw nx l ny nz">books &lt;- <strong class="nq jc">str_trunc</strong>(book_names, width=22) # Shorten the book names</span><span id="fcfe" class="nu nv jb nq b gy oa nx l ny nz">all &lt;- list()</span><span id="7926" class="nu nv jb nq b gy oa nx l ny nz">for (i in 1:28) {<br/>all[[i]] &lt;- sentiment[[i]] %&gt;% <br/>  filter(sentiment %in% c('positive','negative')) %&gt;% <br/>  mutate(n2 = n/sum(n)) %&gt;% <br/>  print()<br/>}</span><span id="4b29" class="nu nv jb nq b gy oa nx l ny nz">all_bound &lt;- do.call("rbind", all) %&gt;% filter(sentiment == "positive")</span><span id="71a2" class="nu nv jb nq b gy oa nx l ny nz"><strong class="nq jc">library(ggrepel) # Useful for preventing overlapping labels</strong></span><span id="5eb5" class="nu nv jb nq b gy oa nx l ny nz">all_bound %&gt;% ggplot(aes(x= book_names, y=n2)) + <br/>              geom_point() + <br/>              <strong class="nq jc">geom_label_repel</strong>(aes(label=books, color = ifelse(n2 &lt;0.55, "red", "blue")), size = 4) + <br/>              theme_classic() + <br/>              theme(legend.position = "none",<br/>                    text = element_text(size=18), <br/>                    axis.text.x = element_blank()) + <br/>              xlab("Books") + <br/>              ylab("Positivity score")</span></pre><h1 id="4f7d" class="oh nv jb bd oi oj ok ol om on oo op oq kh or ki os kk ot kl ou kn ov ko ow ox bi translated">摘要</h1><p id="1500" class="pw-post-body-paragraph lg lh jb li b lj oy kc ll lm oz kf lo lp pa lr ls lt pb lv lw lx pc lz ma la ij bi translated">阅读数百万页来检查文本挖掘是否可靠是不可行的。但在这里，我得到了一些我知道内容的数据，并应用了文本挖掘和情感分析。</p><p id="6088" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">字母组合或字母组合都表明了这些书的相似之处。这种情绪和我 kindle 里的书的类型有关系。</p><p id="9333" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">让我们回到我们的黑客。</p><p id="5cfa" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">文本挖掘的一个意想不到的副作用永远改变了他。分析我的书并从中获得真知灼见使他对阅读越来越感兴趣。他开始关心他周围的世界。世界变了。</p><p id="aa11" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我为自己做的事，他也为自己做了。他变成了一个更好的自己。</p><p id="414d" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">世界变得更加明亮。☀️</p><p id="8115" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">收音机打破了寂静。</p><blockquote class="mw mx my"><p id="26ad" class="lg lh mz li b lj mb kc ll lm mc kf lo na md lr ls nb me lv lw nc mf lz ma la ij bi translated">“brrring…..br 正在…..br 正在……”</p></blockquote><p id="347a" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">我醒了。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><p id="d27b" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">感谢您的阅读。你可以在我的<a class="ae nd" href="https://github.com/korur/textmining" rel="noopener ugc nofollow" target="_blank"> github repo 中找到数据和代码。</a></p><p id="8f65" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">希望你从中有所收获或者有所启发。请随时留下评论、建议和问题。(你可以通过电子邮件联系我，地址是 serdar.korur@gmail.com)</p><p id="2d87" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">下次见！</p><p id="3f7a" class="pw-post-body-paragraph lg lh jb li b lj mb kc ll lm mc kf lo lp md lr ls lt me lv lw lx mf lz ma la ij bi translated">塞尔达尔</p></div></div>    
</body>
</html>