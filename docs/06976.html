<html>
<head>
<title>Simple neural network implementation in C</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 C 语言实现简单的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-neural-network-implementation-in-c-663f51447547?source=collection_archive---------3-----------------------#2019-10-03">https://towardsdatascience.com/simple-neural-network-implementation-in-c-663f51447547?source=collection_archive---------3-----------------------#2019-10-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/af987a5f757b3bae0d030153f43af961.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W1ohPWGX7T48uVTh"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by <a class="ae kf" href="https://unsplash.com/@jjying?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JJ Ying</a> on <a class="ae kf" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="74b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为深入研究机器学习概念的一部分，我决定在没有任何向量或矩阵库的帮助下，用 C 从头开始编写一个简单的神经网络。</p><p id="bbbf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">“为什么 C 并没有向量或矩阵库？…" </em></p><p id="7da0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网上发布的大多数样本神经网络都是用 Python 编写的，并且使用了 numpy 等强大的数学库。虽然这些示例中的代码简洁明了，但是当复杂的矩阵运算被压缩到一条语句中时，可能很难掌握反向传播背后的细节。当你被迫用 C 写出循环时，概念背后的数学变得更加清晰。我强烈建议这样做，即使你最终会使用一个健壮的库，比如 TensorFlow。</p><p id="cd56" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文假设您已经理解了神经网络和反向传播背后的概念。如果你还没有到那里，我建议从这里开始<a class="ae kf" href="https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e" rel="noopener">。我将通过讨论下面的关键代码片段来浏览代码。如果你想看看完整的源代码，你可以在这里找到它</a><a class="ae kf" href="https://gist.github.com/espiritusanti/b7485c68a06ef2c8c76d8c62c8c39d8f" rel="noopener ugc nofollow" target="_blank"/>。</p></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="cc6d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们开始吧！我们首先定义几个辅助函数，包括激活函数及其相应的导数。第三个函数用于初始化 0.0 和 1.0 之间的权重:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="2bef" class="lv lw it lr b gy lx ly l lz ma">// Activation function and its derivative<br/><strong class="lr iu">double</strong> sigmoid(<strong class="lr iu">double</strong> x) { <strong class="lr iu">return</strong> 1 / (1 + exp(-x)); }<br/><strong class="lr iu">double</strong> dSigmoid(<strong class="lr iu">double</strong> x) { <strong class="lr iu">return</strong> x * (1 — x); }</span><span id="0d93" class="lv lw it lr b gy mb ly l lz ma">// Init all weights and biases between 0.0 and 1.0<br/><strong class="lr iu">double</strong> init_weight() { <strong class="lr iu">return</strong> ((<strong class="lr iu">double</strong>)rand())/((<strong class="lr iu">double</strong>)RAND_MAX); }</span></pre><p id="4b53" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的网络将由一个具有 2 个节点的隐藏层和一个输出层节点组成。这是学习 XOR 函数的最低配置:</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mc"><img src="../Images/4f11b5c6590914e91426b555578224e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HdAy5ODliu-qcRG_bHdVQ.png"/></div></div></figure><p id="5704" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面我们定义网络的维度，并为层、偏差和权重分配数组:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="1c76" class="lv lw it lr b gy lx ly l lz ma"><strong class="lr iu">static</strong> <strong class="lr iu">const</strong> <strong class="lr iu">int</strong> numInputs = 2;<br/><strong class="lr iu">static</strong> <strong class="lr iu">const</strong> <strong class="lr iu">int</strong> numHiddenNodes = 2;<br/><strong class="lr iu">static</strong> <strong class="lr iu">const</strong> <strong class="lr iu">int</strong> numOutputs = 1;</span><span id="a50e" class="lv lw it lr b gy mb ly l lz ma"><strong class="lr iu">double</strong> hiddenLayer[numHiddenNodes];<br/><strong class="lr iu">double</strong> outputLayer[numOutputs];</span><span id="3d27" class="lv lw it lr b gy mb ly l lz ma"><strong class="lr iu">double</strong> hiddenLayerBias[numHiddenNodes];<br/><strong class="lr iu">double</strong> outputLayerBias[numOutputs];</span><span id="8906" class="lv lw it lr b gy mb ly l lz ma"><strong class="lr iu">double</strong> hiddenWeights[numInputs][numHiddenNodes];<br/><strong class="lr iu">double</strong> outputWeights[numHiddenNodes][numOutputs];</span></pre><p id="67f9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就反向传播算法而言，我考虑了与如何迭代训练数据相关的两个选项。虽然有许多口味可供选择，但最基本的两种是<em class="le">随机梯度下降(SGD) </em>和<em class="le">批量梯度下降</em>。在这种情况下，我选择了<em class="le"> SGD </em>，其中权重基于一对输入/预期输出进行更新。用这种方法更容易将算法概念化。使用这种方法时，为了最大化网络收敛到正确解的机会，随机化训练对的顺序是很重要的。</p><p id="b00c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是训练集:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="542c" class="lv lw it lr b gy lx ly l lz ma"><strong class="lr iu">static</strong> <strong class="lr iu">const</strong> <strong class="lr iu">int</strong> numTrainingSets = 4;</span><span id="6454" class="lv lw it lr b gy mb ly l lz ma"><strong class="lr iu">double</strong> training_inputs[numTrainingSets][numInputs] = { {0.0f,0.0f},{1.0f,0.0f},{0.0f,1.0f},{1.0f,1.0f} };</span><span id="41c5" class="lv lw it lr b gy mb ly l lz ma"><strong class="lr iu">double</strong> training_outputs[numTrainingSets][numOutputs] = { {0.0f},{1.0f},{1.0f},{0.0f} };</span></pre><p id="c6c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主循环首先迭代一定数量的历元(本例中为 10，000 个)，对于每个历元，它挑选一对输入和预期输出进行操作。因为<em class="le"> SGD </em>要求输入/输出对是随机的，所以我在每个时期打乱一个索引数组，以便选择一个随机对，同时确保每个时期使用所有输入:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="53a7" class="lv lw it lr b gy lx ly l lz ma">// Iterate through the entire training for a number of epochs<strong class="lr iu"><br/>for</strong> (<strong class="lr iu">int</strong> n=0; n &lt; epochs; n++) {</span><span id="d07d" class="lv lw it lr b gy mb ly l lz ma">  // As per SGD, shuffle the order of the training set<br/>  <strong class="lr iu">int</strong> trainingSetOrder[] = {0,1,2,3};<br/>  shuffle(trainingSetOrder,numTrainingSets);</span><span id="30ce" class="lv lw it lr b gy mb ly l lz ma">  // Cycle through each of the training set elements<br/>  <strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> x=0; x&lt;numTrainingSets; x++) {<br/>    <strong class="lr iu">int</strong> i = trainingSetOrder[x];</span></pre><p id="91b9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其余的代码片段在上面的循环中运行，构成了反向传播算法的“核心”。根据<em class="le"> SGD </em>，我们采用单对训练输入，由变量<em class="le"> i </em>指定，并执行以下操作:</p><p id="2b74" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一部分是在给定当前权重的情况下计算网络的输出，从根据以下公式计算隐藏层激活开始:</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi md"><img src="../Images/78c1f59ed8d94da8433a92c153c0460a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lVVFxRmQwpCDsTrbh7-vCw.png"/></div></div></figure><p id="8bce" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="le"> j </em>是隐藏节点的数量(2)，而<em class="le"> k </em>是输入的数量(2):</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="fe0f" class="lv lw it lr b gy lx ly l lz ma">// Compute hidden layer activation<strong class="lr iu"><br/>for</strong> (<strong class="lr iu">int</strong> j=0; j&lt;numHiddenNodes; j++) {<br/>  <strong class="lr iu">double</strong> activation=hiddenLayerBias[j];<br/>    <strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> k=0; k&lt;numInputs; k++) {<br/>      activation+=training_inputs[i][k]*hiddenWeights[k][j];<br/>    }<br/>  hiddenLayer[j] = sigmoid(activation);<br/>}</span></pre><p id="6ba9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们通过下式计算输出层激活:</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi me"><img src="../Images/7ae4ed497ae48b01759842fe02f8c66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JUo-6i0A4H0ACq9u0dg2_A.png"/></div></div></figure><p id="9324" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="le"> j </em>是输出节点(1)的数量，而<em class="le"> k </em>是隐藏节点(2)的数量:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="db36" class="lv lw it lr b gy lx ly l lz ma">// Compute output layer activation<br/><strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> j=0; j&lt;numOutputs; j++) {<br/>  <strong class="lr iu">double</strong> activation=outputLayerBias[j];<br/>  <strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> k=0; k&lt;numHiddenNodes; k++) {<br/>    activation+=hiddenLayer[k]*outputWeights[k][j];<br/>  }<br/>  outputLayer[j] = sigmoid(activation);<br/>}</span></pre><p id="f2e6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下一步包括计算网络权重的微小增量变化，这将使网络朝着最小化网络刚刚计算的输出误差的方向移动。这一步从输出节点开始，向后进行。我们通过计算误差的导数并将其乘以该节点输出的导数来计算权重的变化(<em class="le"> deltaOutput </em>)。这里我们使用<em class="le">均方误差(MSE) </em>函数来计算误差。然而，请注意，对于反向传播本身，我们只需计算误差的导数，对于<em class="le"> MSE </em>而言，该导数就是预期输出和计算输出之间的差值。参见<a class="ae kf" rel="noopener" target="_blank" href="/back-propagation-demystified-in-7-minutes-4294d71a04d7">这篇文章</a>以获得这背后更完整的数学解释。对于输出层，我们计算每个输出节点的增量:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="8f8b" class="lv lw it lr b gy lx ly l lz ma">// Compute change in output weights<strong class="lr iu"><br/>double</strong> deltaOutput[numOutputs];<br/><strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> j=0; j&lt;numOutputs; j++) {<br/>  <strong class="lr iu">double</strong> dError = (training_outputs[i][j]-outputLayer[j]);<br/>  deltaOutput[j] = dError*dSigmoid(outputLayer[j]);<br/>}</span></pre><p id="aa72" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于隐藏层，这是一个类似的过程，除了给定隐藏节点的误差计算是所有输出节点的误差之和(对其应用适当的权重):</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="194e" class="lv lw it lr b gy lx ly l lz ma">// Compute change in hidden weights<strong class="lr iu"><br/>double</strong> deltaHidden[numHiddenNodes];<br/>  <strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> j=0; j&lt;numHiddenNodes; j++) {<br/>    <strong class="lr iu">double</strong> dError = 0.0f;<br/>    <strong class="lr iu">for</strong>(<strong class="lr iu">int</strong> k=0; k&lt;numOutputs; k++) {<br/>      dError+=deltaOutput[k]*outputWeights[j][k];<br/>    }<br/>  deltaHidden[j] = dError*dSigmoid(hiddenLayer[j]);<br/>}</span></pre><p id="5db6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经有了每个输出和隐藏节点的增量，最后一步是将它们应用到各自的权重矩阵和偏差单位:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="32b7" class="lv lw it lr b gy lx ly l lz ma">// Apply change in output weights<strong class="lr iu"><br/>for</strong> (<strong class="lr iu">int</strong> j=0; j&lt;numOutputs; j++) {<br/>  outputLayerBias[j] += deltaOutput[j]*lr;<br/>    <strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> k=0; k&lt;numHiddenNodes; k++) {<br/>      outputWeights[k][j]+=hiddenLayer[k]*deltaOutput[j]*lr;<br/>    }<br/>}<br/>// Apply change in hidden weights<br/><strong class="lr iu">for</strong> (<strong class="lr iu">int</strong> j=0; j&lt;numHiddenNodes; j++) {<br/>  hiddenLayerBias[j] += deltaHidden[j]*lr;<br/>  <strong class="lr iu">for</strong>(<strong class="lr iu">int</strong> k=0; k&lt;numInputs; k++) {<br/>    hiddenWeights[k][j]+=training_inputs[i][k]*deltaHidden[j]*lr;<br/>  }<br/>}</span></pre><p id="a46f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">差不多就是这样！最后一个细节是学习率，在这个应用中设置为 0.1。这是一个值，它必须随着时期的数量进行调整，以最大化网络有效学习问题的机会。对于这个问题，0.1 的学习率和 10，000 的纪元的组合似乎是一个很好的组合。这是一张追踪训练过程中误差的图表。对于该图，我每 10 个时期绘制一个样本:</p><figure class="lm ln lo lp gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mf"><img src="../Images/a11bc8a74bbbe014226e04a01265cf3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DLV6-rTuJf4t-kfJ-uSIg.png"/></div></div></figure></div><div class="ab cl lf lg hx lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="im in io ip iq"><p id="0f8e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然编写这段代码时没有考虑到可扩展性，但是扩展它的功能，使之包括以下内容会很有趣:可配置的隐藏层数、批量梯度下降以及激活和错误函数的附加选项。请随意摆弄代码，不要犹豫提出问题或反馈！</p></div></div>    
</body>
</html>