<html>
<head>
<title>Training a Convolutional Neural Network from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始训练卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-convolutional-neural-network-from-scratch-2235c2a25754?source=collection_archive---------0-----------------------#2019-06-06">https://towardsdatascience.com/training-a-convolutional-neural-network-from-scratch-2235c2a25754?source=collection_archive---------0-----------------------#2019-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0623" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个为 CNN 派生反向传播并在 Python 中从头实现它的简单演练。</h2></div><p id="7067" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们将深入探讨大多数卷积神经网络(CNN)介绍所缺乏的东西:<strong class="kk iu">如何训练 CNN </strong>，包括推导梯度，从头实现反向投影<em class="le"/>(仅使用<a class="ae lf" href="https://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> numpy </a>)，并最终建立一个完整的训练管道！</p><p id="0d2a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这篇文章假设你对 CNN 有基本的了解</strong>。我对 CNN 的介绍涵盖了你需要知道的一切，所以我强烈建议你先读一下。如果你来这里是因为你已经读过了，欢迎回来！</p><p id="a890" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这篇文章的部分内容也假设了多变量微积分的基础知识</strong>。如果你想的话，你可以跳过这些章节，但是我建议即使你不理解所有的内容，也要阅读它们。当我们得到结果时，我们将逐步编写代码，即使是表面的理解也会有所帮助。</p><h1 id="485c" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">1.搭建舞台</h1><p id="e344" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">系好安全带。是时候进入状态了。</p><p id="2066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将继续我对 CNN 的介绍。我们用 CNN 来解决<a class="ae lf" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>手写数字分类问题:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi md"><img src="../Images/bbd80fae2d272a83e9493ac9e6fc85ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/0*G4sNjkXEV1jDJrBu.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Sample images from the MNIST dataset</figcaption></figure><p id="c17f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的(简单的)CNN 由一个 Conv 层、一个 Max 池层和一个 Softmax 层组成。这是我们 CNN 的图表:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/0a37e0fe9340647862f34309ad232587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1YtJE0DL3TXRK9nDPGC3oA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Our CNN takes a 28x28 grayscale MNIST image and outputs 10 probabilities, 1 for each digit.</figcaption></figure><p id="db21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们写了 3 个类，每层一个:<code class="fe mu mv mw mx b">Conv3x3</code>、<code class="fe mu mv mw mx b">MaxPool</code>和<code class="fe mu mv mw mx b">Softmax</code>。每个类都实现了一个<code class="fe mu mv mw mx b">forward()</code>方法，我们用它来构建 CNN 的前向传递:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="74e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在你的浏览器 中<strong class="kk iu">查看代码或者</strong> <a class="ae lf" href="https://repl.it/@vzhou842/A-CNN-from-scratch-Part-1" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">运行 CNN。在<a class="ae lf" href="https://github.com/vzhou842/cnn-from-scratch" rel="noopener ugc nofollow" target="_blank"> Github </a>上也有。</strong></a></p><p id="fd43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们 CNN 现在的输出:</p><pre class="me mf mg mh gt na mx nb nc aw nd bi"><span id="53f1" class="ne lh it mx b gy nf ng l nh ni">MNIST CNN initialized!<br/>[Step 100] Past 100 steps: Average Loss 2.302 | Accuracy: 11%<br/>[Step 200] Past 100 steps: Average Loss 2.302 | Accuracy: 8%<br/>[Step 300] Past 100 steps: Average Loss 2.302 | Accuracy: 3%<br/>[Step 400] Past 100 steps: Average Loss 2.302 | Accuracy: 12%</span></pre><p id="b005" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">显然，我们想做到 10%以上的准确率…让我们给 CNN 一个教训。</p><h1 id="ee45" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">2.培训概述</h1><p id="cbd7" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">训练神经网络通常包括两个阶段:</p><ol class=""><li id="7d36" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">前向阶段，输入完全通过网络。</li><li id="d427" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated"><strong class="kk iu">反向</strong>阶段，其中梯度被反向传播(反向传播)并且权重被更新。</li></ol><p id="082a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将遵循这种模式来训练我们的 CNN。我们还将使用两个主要的特定于实现的想法:</p><ul class=""><li id="b843" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld nx np nq nr bi translated">在向前阶段，每一层将<strong class="kk iu">缓存</strong>向后阶段所需的任何数据(如输入、中间值等)。这意味着任何反向阶段之前必须有相应的正向阶段。</li><li id="fb2c" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">在反向阶段，每一层将<strong class="kk iu">接收一个梯度</strong>，并且<strong class="kk iu">返回一个梯度</strong>。它将接收关于其<em class="le">输出</em> (∂L / ∂out)的损耗梯度，并返回关于其<em class="le">输入</em> (∂L / ∂in).)的损耗梯度</li></ul><p id="6717" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两个想法将有助于保持我们的培训实施干净和有组织。了解原因的最佳方式可能是查看代码。训练我们的 CNN 最终会是这样的:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="7d89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看到那看起来多漂亮多干净了吗？现在想象建立一个有 50 层而不是 3 层的网络——拥有好的系统更有价值。</p><h1 id="08e6" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">3.反向投影:Softmax</h1><p id="bc42" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们将从结尾开始，然后朝着开头前进，因为这就是反向传播的工作方式。首先，回忆一下<strong class="kk iu">交叉熵损失</strong>:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ny"><img src="../Images/b9c4296a0c6d79a3431614bcc01de348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ymMrWTttRNXaJV0eJaRTSA.png"/></div></div></figure><p id="db07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中 p_c 是正确类别 c 的预测概率(换句话说，我们当前图像<em class="le">实际上</em>是多少位)。</p><blockquote class="nz oa ob"><p id="5bb2" class="ki kj le kk b kl km ju kn ko kp jx kq oc ks kt ku od kw kx ky oe la lb lc ld im bi translated"><em class="it">想要更长的解释？阅读我的 CNN 简介的</em> <a class="ae lf" href="https://victorzhou.com/blog/intro-to-cnns-part-1/#52-cross-entropy-loss" rel="noopener ugc nofollow" target="_blank"> <em class="it">交叉熵损失</em> </a> <em class="it">部分。</em></p></blockquote><p id="2546" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们需要计算的第一件事是 Softmax 层的反向相位∂L / ∂out_s 的输入，其中 out_s 是 Softmax 层的输出:10 个概率的向量。这很简单，因为只有 p_i 出现在损耗方程中:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi of"><img src="../Images/335fe150dd5e41a352486880373f1c9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*zabU5ZND46RKEZUZeBYL9A.png"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Reminder: c is the correct class.</figcaption></figure><p id="8a74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是你在上面看到的初始梯度:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="a256" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们几乎已经准备好实现我们的第一个反向阶段——我们只需要首先执行我们之前讨论过的正向阶段缓存:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="268d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在这里缓存了 3 个对实现后向阶段有用的东西:</p><ul class=""><li id="ce51" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld nx np nq nr bi translated">在变平之前<code class="fe mu mv mw mx b">input</code>的形状<em class="le">。</em></li><li id="e993" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">后的<code class="fe mu mv mw mx b">input</code> <em class="le">我们把它压平。</em></li><li id="7ce2" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated"><strong class="kk iu">总计</strong>，是传递给 softmax 激活的值。</li></ul><p id="a8d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样一来，我们就可以开始推导反投影阶段的梯度了。我们已经导出了 Softmax 反向阶段的输入:∂L / ∂out_s。关于∂L / ∂out_s，我们可以利用的一个事实是<em class="le">只有正确的类</em> c <em class="le">才是非零的。这意味着除了 out_s(c)之外，我们可以忽略任何东西！</em></p><p id="1bf3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们计算 out_s(c)相对于总数的梯度(传递给 softmax 激活的值)。设 t_i 为第 I 类的总和，那么我们可以写出 _s(c)为:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi og"><img src="../Images/c840f31730b8eae0ff617a8a93ece710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpL4t7ghKjV0VQ6I2kWOSA.png"/></div></div></figure><blockquote class="nz oa ob"><p id="c778" class="ki kj le kk b kl km ju kn ko kp jx kq oc ks kt ku od kw kx ky oe la lb lc ld im bi translated"><em class="it">你应该从我的 CNN 教程的</em> <a class="ae lf" href="https://victorzhou.com/blog/intro-to-cnns-part-1/#5-softmax" rel="noopener ugc nofollow" target="_blank"> <em class="it"> Softmax </em> </a> <em class="it">部分认出了上面的等式。</em></p></blockquote><p id="5c06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，考虑某个类 k，使得 k 不是 c，我们可以将 out_s(c)重写为:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oh"><img src="../Images/2905725e451e671f06f419ad191c3b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7AQI1hS6g2xh8WDGYt4NQ.png"/></div></div></figure><p id="29e1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并使用链式法则推导出:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oi"><img src="../Images/1f5109beb7ef2ecb1aac267a90511367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J4KA1bSgLBy6SCrdjtGRJA.png"/></div></div></figure><p id="0b2d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">记住，这是假设 k 不等于 c。现在让我们对 c 进行推导，这次使用<a class="ae lf" href="https://en.wikipedia.org/wiki/Quotient_rule" rel="noopener ugc nofollow" target="_blank">商法则</a>:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oj"><img src="../Images/0108f624c0d96ec277c366b0e44b356c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2xwvKzclbqVtn1Z8DQBTNQ.png"/></div></div></figure><p id="7cea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">唷。这是整篇文章中最难的部分——从这里开始只会变得更容易！让我们开始实现它:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="d8e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还记得∂L / ∂out_s 只对正确的类 c 是非零的吗？我们通过在<code class="fe mu mv mw mx b">d_L_d_out</code>中寻找非零梯度来开始寻找 c。一旦我们发现了这一点，我们就可以使用上面得出的结果来计算梯度∂out_s(i) / ∂t ( <code class="fe mu mv mw mx b">d_out_d_totals</code>):</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ok"><img src="../Images/aa179347ce16dfb584044049efa68c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyJ34X-MOq2WWAvPznopkQ.png"/></div></div></figure><p id="cd23" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们继续吧。我们最终想要损失相对于权重、偏差和输入的梯度:</p><ul class=""><li id="7853" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld nx np nq nr bi translated">我们将使用权重梯度，∂L / ∂w，来更新我们的层的权重。</li><li id="dce7" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">我们将使用偏差梯度∂L / ∂b 来更新图层的偏差。</li><li id="bd46" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">我们将从我们的<code class="fe mu mv mw mx b">backprop()</code>方法返回输入渐变，∂L / ∂input，这样下一层可以使用它。这是我们在培训概述部分谈到的回报梯度！</li></ul><p id="072b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了计算这 3 个损失梯度，我们首先需要导出另外 3 个结果:总数<em class="le">相对于权重、偏差和输入的梯度。这里的相关等式是:</em></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ol"><img src="../Images/545fef7e6784ba9b09bf0967bc167248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bh0PufS_8vfJo_k4k5S-OA.png"/></div></div></figure><p id="dd8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些渐变很容易！</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi om"><img src="../Images/8345335b59a30af22a68f8c18c480e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sa5uFVVCLS8Rd0BKljoi9w.png"/></div></div></figure><p id="276e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将所有东西放在一起:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi on"><img src="../Images/df9b3a4725a75a2864ea6eeb90759689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jO5Xm-rVvDYaew1RlfQZSg.png"/></div></div></figure><p id="cf75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将它写入代码就不那么简单了:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="731c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们预计算<code class="fe mu mv mw mx b">d_L_d_t</code>，因为我们将多次使用它。然后，我们计算每个梯度:</p><ul class=""><li id="a098" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld nx np nq nr bi translated"><code class="fe mu mv mw mx b"><strong class="kk iu">d_L_d_w</strong></code>:我们需要 2d 数组来做矩阵乘法(<code class="fe mu mv mw mx b">@</code>)，但是<code class="fe mu mv mw mx b">d_t_d_w</code>和<code class="fe mu mv mw mx b">d_L_d_t</code>是 1d 数组。<a class="ae lf" href="https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html#numpy.newaxis" rel="noopener ugc nofollow" target="_blank"> np.newaxis </a>让我们很容易地创建一个新的长度为 1 的轴，所以我们最终将维度为(<code class="fe mu mv mw mx b">input_len</code>，1)和(1，<code class="fe mu mv mw mx b">nodes</code>)的矩阵相乘。因此，<code class="fe mu mv mw mx b">d_L_d_w</code>的最终结果将具有 shape ( <code class="fe mu mv mw mx b">input_len</code>，<code class="fe mu mv mw mx b">nodes</code>)，与<code class="fe mu mv mw mx b">self.weights</code>相同！</li><li id="55fb" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated"><code class="fe mu mv mw mx b"><strong class="kk iu">d_L_d_b</strong></code>:这个很简单，因为<code class="fe mu mv mw mx b">d_t_d_b</code>是 1。</li><li id="039d" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated"><code class="fe mu mv mw mx b"><strong class="kk iu">d_L_d_inputs</strong></code>:我们将维度为(<code class="fe mu mv mw mx b">input_len</code>，<code class="fe mu mv mw mx b">nodes</code>)和(<code class="fe mu mv mw mx b">nodes</code>，1)的矩阵相乘，得到长度为<code class="fe mu mv mw mx b">input_len</code>的结果。</li></ul><blockquote class="nz oa ob"><p id="7b04" class="ki kj le kk b kl km ju kn ko kp jx kq oc ks kt ku od kw kx ky oe la lb lc ld im bi translated"><em class="it">试着做上面计算的小例子，特别是</em> <code class="fe mu mv mw mx b"><em class="it">d_L_d_w</em></code> <em class="it">和</em> <code class="fe mu mv mw mx b"><em class="it">d_L_d_inputs</em></code> <em class="it">的矩阵乘法。这是理解为什么这段代码能正确计算梯度的最好方法。</em></p></blockquote><p id="6a59" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着所有的梯度计算，所有剩下的是实际训练 Softmax 层！我们将使用随机梯度下降(SGD)来更新权重和偏差，就像我们在我的<a class="ae lf" href="https://victorzhou.com/blog/intro-to-neural-networks/#training-stochastic-gradient-descent" rel="noopener ugc nofollow" target="_blank">神经网络简介</a>中所做的那样，然后返回<code class="fe mu mv mw mx b">d_L_d_inputs</code>:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="f66a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，我们添加了一个<code class="fe mu mv mw mx b">learn_rate</code>参数来控制我们更新权重的速度。此外，在返回<code class="fe mu mv mw mx b">d_L_d_inputs</code>之前，我们必须<code class="fe mu mv mw mx b">reshape()</code>，因为我们在向前传球时拉平了输入:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="4a77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">整形为<code class="fe mu mv mw mx b">last_input_shape</code>确保该层返回其输入的渐变，格式与输入最初提供给它的格式相同。</p><h2 id="c331" class="ne lh it bd li oo op dn lm oq or dp lq kr os ot ls kv ou ov lu kz ow ox lw oy bi translated">试驾:Softmax Backprop</h2><p id="1ea5" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们已经完成了我们的第一个反向投影实现！让我们快速测试一下，看看它有什么好的。我们将从我的<a class="ae lf" href="https://victorzhou.com/blog/intro-to-cnns-part-1/" rel="noopener ugc nofollow" target="_blank">CNN 简介</a>开始实现一个<code class="fe mu mv mw mx b">train()</code>方法:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5605" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行此命令会得到类似于以下内容的结果:</p><pre class="me mf mg mh gt na mx nb nc aw nd bi"><span id="71f4" class="ne lh it mx b gy nf ng l nh ni">MNIST CNN initialized!<br/>[Step 100] Past 100 steps: Average Loss 2.239 | Accuracy: 18%<br/>[Step 200] Past 100 steps: Average Loss 2.140 | Accuracy: 32%<br/>[Step 300] Past 100 steps: Average Loss 1.998 | Accuracy: 48%<br/>[Step 400] Past 100 steps: Average Loss 1.861 | Accuracy: 59%<br/>[Step 500] Past 100 steps: Average Loss 1.789 | Accuracy: 56%<br/>[Step 600] Past 100 steps: Average Loss 1.809 | Accuracy: 48%<br/>[Step 700] Past 100 steps: Average Loss 1.718 | Accuracy: 63%<br/>[Step 800] Past 100 steps: Average Loss 1.588 | Accuracy: 69%<br/>[Step 900] Past 100 steps: Average Loss 1.509 | Accuracy: 71%<br/>[Step 1000] Past 100 steps: Average Loss 1.481 | Accuracy: 70%</span></pre><p id="1eb9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">损失在下降，准确性在上升——我们的 CNN 已经在学习了！</p><h1 id="0e01" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">4.反向推进:最大池化</h1><p id="b981" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">Max Pooling 层不能被训练，因为它实际上没有任何权重，但是我们仍然需要为它实现一个方法来计算梯度。我们将从再次添加前向阶段缓存开始。这次我们需要缓存的只是输入:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="9d3e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在向前传递期间，最大池层通过在 2x2 块上选取最大值来获取输入体积并将其宽度和高度尺寸减半。反向过程则相反:<strong class="kk iu">我们将通过将每个梯度值分配给<strong class="kk iu">来加倍损失梯度的宽度和高度</strong>，其中原始最大值是其对应的 2x2 块中的</strong>。</p><p id="7738" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个例子。考虑最大池层的这个前进阶段:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi oz"><img src="../Images/2bc078069585ad58222a84c21e61d58c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vy_xlr3d9ltGWkFzQXMx9Q.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">An example forward phase that transforms a 4x4 input to a 2x2 output</figcaption></figure><p id="5636" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同一层的反向阶段将如下所示:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pa"><img src="../Images/16d78078a8b50fed3ab2d07375ad5979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lSD_vA0285ta3K0j6jjEw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">An example backward phase that transforms a 2x2 gradient to a 4x4 gradient</figcaption></figure><p id="6c39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个渐变值都被分配到原始最大值所在的位置，其他值为零。</p><p id="2657" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为什么最大池层的反向阶段是这样工作的？思考一下∂L / ∂inputs 直觉上应该是什么样子。不是其 2x2 块中最大值的输入像素对损失的边际影响为零，因为稍微改变该值根本不会改变输出！换句话说，对于非最大像素，∂L / ∂inputs = 0。另一方面，<em class="le">为</em>最大值的输入像素会将其值传递给输出，因此∂output / ∂input = 1，意味着∂L / ∂input = ∂L / ∂output.</p><p id="653e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用我在 CNN 简介中写的助手方法很快实现这一点。我将再次把它包括在内作为提醒:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="e55d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个过滤器中每个 2×2 图像区域中的每个像素，如果它是正向传递期间的最大值，我们将梯度从<code class="fe mu mv mw mx b">d_L_d_out</code>复制到<code class="fe mu mv mw mx b">d_L_d_input</code>。</p><p id="3f1c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就是这样！我们的最后一层。</p><h1 id="5d3b" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">5.背景:Conv</h1><p id="8145" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们终于来了:通过 Conv 层反向传播是训练 CNN 的核心。前向阶段缓存很简单:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><blockquote class="nz oa ob"><p id="dcc3" class="ki kj le kk b kl km ju kn ko kp jx kq oc ks kt ku od kw kx ky oe la lb lc ld im bi translated"><em class="it">提醒一下我们的实现:为了简单起见，</em> <strong class="kk iu"> <em class="it">我们假设 conv 层的输入是一个 2d 数组</em> </strong> <em class="it">。这只对我们有用，因为我们把它作为网络的第一层。如果我们正在构建一个需要多次使用</em> <code class="fe mu mv mw mx b"><em class="it">Conv3x3</em></code> <em class="it">的更大的网络，我们必须使输入成为一个</em> <strong class="kk iu"> <em class="it"> 3d </em> </strong> <em class="it">数组。</em></p></blockquote><p id="b4b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们主要对 conv 图层中滤镜的损耗梯度感兴趣，因为我们需要它来更新我们的滤镜权重。我们已经有了∂L / ∂out 的 conv 层，所以我们只需要∂out / ∂filters。为了计算，我们问自己:改变过滤器的重量会如何影响 conv 层的输出？</p><p id="8ad1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实是<strong class="kk iu">改变任何滤波器权重都会影响该滤波器的整个输出图像</strong>，因为在卷积期间<em class="le">每个</em>输出像素使用<em class="le">每个</em>像素权重。为了更容易理解，让我们一次只考虑一个输出像素:<strong class="kk iu">修改一个滤镜会如何改变一个特定输出像素的输出？</strong></p><p id="7118" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个超级简单的例子来帮助思考这个问题:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pb"><img src="../Images/ca3fde150c912e3a649f9c2201a7a25a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktzJkzBjlGe0JX5-k63g1g.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">A 3x3 image (left) convolved with a 3x3 filter (middle) to produce a 1x1 output (right)</figcaption></figure><p id="0aa3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将一个 3×3 图像与一个全零 3×3 滤波器进行卷积，产生 1×1 输出。如果我们将中央过滤器的重量增加 1 会怎么样？输出将增加中心图像值 80:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pc"><img src="../Images/64c50e5f27f0fd8a3141c6e941ca1d15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYIyuQybw6AOEIKX_GeZ8w.png"/></div></div></figure><p id="0975" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，将任何其他过滤器权重增加 1 将使输出增加相应图像像素的值！这表明特定输出像素相对于特定滤波器权重的导数就是相应的图像像素值。计算证实了这一点:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi ny"><img src="../Images/f98801e9eeb5e288b028d9b0278dc0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArHfhNtvh8kGwQxmvQG4BA.png"/></div></div></figure><p id="ab91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以将所有这些放在一起，找出特定滤波器权重的损耗梯度:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi pd"><img src="../Images/33b5501b4b40f6bcdeb9a817b1e46333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qjkhHc01xELoul9CwKm9Lw.png"/></div></div></figure><p id="bb4c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们已经准备好为我们的 conv 层实现反向投影了！</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="a295" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们通过迭代每个图像区域/滤波器并递增地构建损失梯度来应用我们导出的方程。一旦我们涵盖了所有内容，我们就像以前一样使用 SGD 更新<code class="fe mu mv mw mx b">self.filters</code>。请注意解释我们为什么要返回的注释——输入损耗梯度的推导与我们刚才所做的非常相似，留给读者作为练习:)。</p><p id="e6b0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就这样，我们结束了！我们已经通过我们的 CNN 实现了一个完整的向后传递。是时候测试一下了…</p><h1 id="ae19" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">6.训练 CNN</h1><p id="8a6e" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们将训练 CNN 几个时期，在训练过程中跟踪它的进展，然后在单独的测试集上测试它。以下是完整的代码:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="e6bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行代码的输出示例:</p><pre class="me mf mg mh gt na mx nb nc aw nd bi"><span id="174c" class="ne lh it mx b gy nf ng l nh ni">MNIST CNN initialized!<br/>--- Epoch 1 ---<br/>[Step 100] Past 100 steps: Average Loss 2.254 | Accuracy: 18%<br/>[Step 200] Past 100 steps: Average Loss 2.167 | Accuracy: 30%<br/>[Step 300] Past 100 steps: Average Loss 1.676 | Accuracy: 52%<br/>[Step 400] Past 100 steps: Average Loss 1.212 | Accuracy: 63%<br/>[Step 500] Past 100 steps: Average Loss 0.949 | Accuracy: 72%<br/>[Step 600] Past 100 steps: Average Loss 0.848 | Accuracy: 74%<br/>[Step 700] Past 100 steps: Average Loss 0.954 | Accuracy: 68%<br/>[Step 800] Past 100 steps: Average Loss 0.671 | Accuracy: 81%<br/>[Step 900] Past 100 steps: Average Loss 0.923 | Accuracy: 67%<br/>[Step 1000] Past 100 steps: Average Loss 0.571 | Accuracy: 83%<br/>--- Epoch 2 ---<br/>[Step 100] Past 100 steps: Average Loss 0.447 | Accuracy: 89%<br/>[Step 200] Past 100 steps: Average Loss 0.401 | Accuracy: 86%<br/>[Step 300] Past 100 steps: Average Loss 0.608 | Accuracy: 81%<br/>[Step 400] Past 100 steps: Average Loss 0.511 | Accuracy: 83%<br/>[Step 500] Past 100 steps: Average Loss 0.584 | Accuracy: 89%<br/>[Step 600] Past 100 steps: Average Loss 0.782 | Accuracy: 72%<br/>[Step 700] Past 100 steps: Average Loss 0.397 | Accuracy: 84%<br/>[Step 800] Past 100 steps: Average Loss 0.560 | Accuracy: 80%<br/>[Step 900] Past 100 steps: Average Loss 0.356 | Accuracy: 92%<br/>[Step 1000] Past 100 steps: Average Loss 0.576 | Accuracy: 85%<br/>--- Epoch 3 ---<br/>[Step 100] Past 100 steps: Average Loss 0.367 | Accuracy: 89%<br/>[Step 200] Past 100 steps: Average Loss 0.370 | Accuracy: 89%<br/>[Step 300] Past 100 steps: Average Loss 0.464 | Accuracy: 84%<br/>[Step 400] Past 100 steps: Average Loss 0.254 | Accuracy: 95%<br/>[Step 500] Past 100 steps: Average Loss 0.366 | Accuracy: 89%<br/>[Step 600] Past 100 steps: Average Loss 0.493 | Accuracy: 89%<br/>[Step 700] Past 100 steps: Average Loss 0.390 | Accuracy: 91%<br/>[Step 800] Past 100 steps: Average Loss 0.459 | Accuracy: 87%<br/>[Step 900] Past 100 steps: Average Loss 0.316 | Accuracy: 92%<br/>[Step 1000] Past 100 steps: Average Loss 0.460 | Accuracy: 87%<br/><br/>--- Testing the CNN ---<br/>Test Loss: 0.5979384893783474<br/>Test Accuracy: 0.78</span></pre><p id="4160" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的代码有效！仅在 3000 个训练步骤中，我们就从一个损失 2.3、准确率 10%的模型，变成了损失 0.6、准确率 78%的模型。</p><p id="f5b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想自己尝试或修改这段代码吗？ <a class="ae lf" href="https://repl.it/@vzhou842/A-CNN-from-scratch-Part-2" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">在浏览器中运行本 CNN</strong></a><strong class="kk iu">。</strong>在<a class="ae lf" href="https://github.com/vzhou842/cnn-from-scratch" rel="noopener ugc nofollow" target="_blank"> Github </a>上也有。</p><p id="f25e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了节省时间，我们在这个例子中只使用了整个 MNIST 数据集的一个子集——我们的 CNN 实现不是特别快。如果我们真的想训练一个 MNIST CNN，我们会使用一个 ML 库，比如<a class="ae lf" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>。为了展示我们 CNN 的强大，我使用 Keras 来实现和训练我们刚刚从头构建的<em class="le">完全相同的</em> CNN:</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="184f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<em class="le">完整的</em> MNIST 数据集(60k 训练图像)上运行该代码会得到如下结果:</p><pre class="me mf mg mh gt na mx nb nc aw nd bi"><span id="c241" class="ne lh it mx b gy nf ng l nh ni">Epoch 1<br/>loss: 0.2433 - acc: 0.9276 - val_loss: 0.1176 - val_acc: 0.9634<br/>Epoch 2<br/>loss: 0.1184 - acc: 0.9648 - val_loss: 0.0936 - val_acc: 0.9721<br/>Epoch 3<br/>loss: 0.0930 - acc: 0.9721 - val_loss: 0.0778 - val_acc: 0.9744</span></pre><p id="c91c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们用这个简单的 CNN 实现了<strong class="kk iu"> 97.4% </strong>的测试准确率！有了更好的 CNN 架构，我们可以进一步改进——在这个<a class="ae lf" href="https://keras.io/examples/mnist_cnn/" rel="noopener ugc nofollow" target="_blank">官方的 Keras MNIST CNN 示例</a>中，他们在 12 个时期后达到了<strong class="kk iu"> 99.25% </strong>的测试准确度。这个<em class="le">真的</em>准确度好。</p><p id="d64e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这篇帖子的所有代码都可以在</strong><a class="ae lf" href="https://github.com/vzhou842/cnn-from-scratch" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu">Github</strong></a><strong class="kk iu">上获得。</strong></p><h1 id="ae5a" class="lg lh it bd li lj lk ll lm ln lo lp lq jz lr ka ls kc lt kd lu kf lv kg lw lx bi translated">现在怎么办？</h1><p id="1af0" class="pw-post-body-paragraph ki kj it kk b kl ly ju kn ko lz jx kq kr ma kt ku kv mb kx ky kz mc lb lc ld im bi translated">我们完了！在这篇文章中，我们做了一个完整的演练如何训练一个卷积神经网络。然而，这仅仅是开始。您还可以做更多的事情:</p><ul class=""><li id="8be5" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld nx np nq nr bi translated">使用适当的 ML 库，如<a class="ae lf" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>、<a class="ae lf" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>或<a class="ae lf" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>，尝试更大/更好的 CNN。</li><li id="7722" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">了解如何将<a class="ae lf" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批处理规范化</a>用于 CNN。</li><li id="86b5" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">了解<strong class="kk iu">数据扩充</strong>如何用于改善图像训练集。</li><li id="f5d4" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld nx np nq nr bi translated">阅读<a class="ae lf" href="https://en.wikipedia.org/wiki/ImageNet" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>项目及其著名的计算机视觉竞赛 ImageNet 大规模视觉识别挑战赛(<a class="ae lf" href="http://image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> ILSVRC </a>)。</li></ul></div><div class="ab cl pe pf hx pg" role="separator"><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj pk"/><span class="ph bw bk pi pj"/></div><div class="im in io ip iq"><p id="7ab0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">最初发表于</em><a class="ae lf" href="https://victorzhou.com/blog/intro-to-cnns-part-2/" rel="noopener ugc nofollow" target="_blank"><em class="le">【https://victorzhou.com】</em></a><em class="le">。</em></p></div></div>    
</body>
</html>