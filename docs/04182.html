<html>
<head>
<title>Bayesian inference problem, MCMC and variational inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯推理问题、MCMC 和变分推理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29?source=collection_archive---------1-----------------------#2019-07-01">https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29?source=collection_archive---------1-----------------------#2019-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="95ce" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">统计学中的贝叶斯推断问题综述。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/79bf8444450a6cdbbedd62be3472d5b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVHeNa3MrFxAnsXHukwu2g.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Credit: <a class="ae kv" href="https://pixabay.com/fr/users/free-photos-242387/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1030852" rel="noopener ugc nofollow" target="_blank">Free-Photos</a> on <a class="ae kv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="1e87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">本帖与</em> <a class="lt lu ep" href="https://medium.com/u/20ad1309823a?source=post_page-----25a8aa9bce29--------------------------------" rel="noopener" target="_blank"> <em class="ls">巴蒂斯特·罗卡</em> </a> <em class="ls">共同撰写。</em></p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="89bb" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">介绍</h1><p id="5cc2" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">贝叶斯推理是统计学中的一个主要问题，也是许多机器学习方法中遇到的问题。例如，用于分类的高斯混合模型或用于主题建模的潜在狄利克雷分配都是在拟合数据时需要解决这种问题的图形模型。</p><p id="219d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同时，可以注意到，根据模型设置(假设、维度等)，贝叶斯推理问题有时很难解决。在大型问题中，精确的解决方案确实需要大量的计算，这些计算往往变得难以处理，必须使用一些近似技术来克服这个问题，并构建快速和可扩展的系统。</p><p id="499a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将讨论两种主要的方法来解决贝叶斯推理问题:马尔可夫链蒙特卡罗(MCMC)，这是一种基于抽样的方法，和变分推理(VI)，这是一种基于近似的方法。</p><h2 id="59e6" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">概述</h2><p id="8653" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">在第一部分，我们将讨论贝叶斯推理问题，并看到一些经典的机器学习应用程序的例子，其中这个问题自然出现。然后在第二部分，我们将全面介绍 MCMC 技术来解决这一问题，并给出一些关于两个 MCMC 算法的细节:Metropolis-Hasting 和 Gibbs 抽样。最后，在第三节中，我们将引入变分推理，看看如何在一个参数化的分布族上通过优化过程得到一个近似解。</p><blockquote class="nl nm nn"><p id="a302" class="kw kx ls ky b kz la jr lb lc ld ju le no lg lh li np lk ll lm nq lo lp lq lr ij bi translated"><strong class="ky ir">注。标有 a (∞)的小节非常数学化，可以跳过，不会影响对这篇文章的整体理解。还要注意，在这篇文章中，p(.)用于表示取决于上下文的概率、概率密度或概率分布。</strong></p></blockquote></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="c0bd" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">贝叶斯推理问题</h1><p id="3762" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">在本节中，我们将介绍贝叶斯推理问题，并讨论一些计算困难，然后给出潜在狄利克雷分配的示例，这是一种主题建模的具体机器学习技术，其中会遇到该问题。</p><h2 id="0689" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">什么是推论？</h2><p id="af60" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">统计推断在于<strong class="ky ir">根据我们观察到的东西来了解我们没有观察到的东西</strong>。换句话说，它是根据这个群体或这个群体的一个样本中的一些观察变量(通常是效果)得出关于群体中一些潜在变量(通常是原因)的结论如准时估计、置信区间或分布估计的过程。</p><p id="9c67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别是，<strong class="ky ir">贝叶斯推断</strong>是从贝叶斯的角度产生统计推断的过程。简而言之，贝叶斯范式是一种统计/概率范式，其中由概率分布建模的先验知识在每次记录新的观察时被更新，该新的观察的不确定性由另一个概率分布建模。支配贝叶斯范式的整个思想嵌入在所谓的贝叶斯定理中，该定理表达了更新的知识(“后验”)、先验知识(“先验”)和来自观察的知识(“可能性”)之间的关系。</p><p id="c705" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个经典的例子是参数的<strong class="ky ir">贝叶斯推断。让我们假设一个模型，其中数据 x 由依赖于未知参数θ的概率分布产生。我们还假设我们有一个关于参数θ的先验知识，它可以表示为一个概率分布 p(θ)。然后，当观察到数据 x 时，我们可以使用贝叶斯定理更新关于该参数的先验知识，如下所示</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/63203f65fe6d60193af78a0227070a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04pd7c6QIHXYHgAelzzWlg@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Illustration of the Bayes theorem applied to the inference of a parameter given observed data.</figcaption></figure><h2 id="22b5" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">计算困难</h2><p id="3814" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">贝叶斯定理告诉我们，后验概率的计算需要三个条件:先验、似然和证据。前两个可以很容易地表达出来，因为它们是假设模型的一部分(在许多情况下，先验和可能性是明确已知的)。然而，第三项，即归一化因子，需要计算如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e8e93c74f274ed3b4a581906a0e5c8c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*A5g85OCd_hFhnmL-dMnauA@2x.png"/></div></figure><p id="f3ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然在低维中计算这个积分没有太多困难，<strong class="ky ir">但是在高维中它会变得难以处理</strong>。在最后一种情况下，后验分布的精确计算实际上是不可行的，必须使用一些近似技术来解决需要知道该后验分布的问题(例如，均值计算)。</p><p id="4240" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以注意到，一些其他的计算困难可能产生于贝叶斯推理问题，例如，当一些变量是离散的时，组合学问题。在最常用于克服这些困难的方法中，我们找到了<strong class="ky ir">马尔可夫链蒙特卡罗</strong>和<strong class="ky ir">变分推断</strong>方法。在这篇文章的后面，我们将描述这两种方法，特别是集中在“归一化因子问题”上，但是人们应该记住，当面临与贝叶斯推理相关的其他计算困难时，这些方法也可能是宝贵的。</p><p id="8b9a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使接下来的章节更具一般性，我们可以观察到，假设 x 是给定的，因此可以作为一个参数，我们面临这样一种情况，θ的概率分布定义为一个归一化因子</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/15dc4e620c59cbdc065fbfee1ea317d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*DflXhaT1KDrX286xSwyDjA@2x.png"/></div></div></figure><p id="b04c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下两节描述 MCMC 和 VI 之前，我们先举一个具体的例子，用潜在的狄利克雷分配来说明机器学习中的贝叶斯推理问题。</p><h2 id="62ca" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">例子</h2><p id="159c" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">贝叶斯推理问题自然地出现，例如，在假设概率图形模型的机器学习方法中，并且在给定一些观察的情况下，我们想要恢复模型的潜在变量。在主题建模中，<strong class="ky ir">潜在狄利克雷分配(LDA) </strong>方法为语料库中的文本描述定义了这样的模型。因此，给定大小为 V 的完整语料库词汇表和给定数量的主题 T，该模型假设:</p><ul class=""><li id="c3f7" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated">对于每个主题，存在词汇表上的“主题-词”概率分布(假设有 Dirichlet 先验)</li><li id="af2b" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">对于每个文档，存在主题上的“文档-主题”概率分布(假设另一个狄利克雷先验)</li><li id="8c4d" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">对文档中的每个单词进行采样，首先，我们从文档的“文档-主题”分布中采样主题，其次，我们从附加到采样主题的“主题-单词”分布中采样单词</li></ul><p id="9660" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该方法的名称来源于模型中假设的 Dirichlet 先验，其目的是推断观察到的语料库中的潜在主题以及每个文档的主题分解。即使我们不深入 LDA 的细节，我们也可以非常粗略地说，表示 w 是语料库中的单词向量，z 是与这些单词相关联的主题向量，我们希望以贝叶斯方式基于观察到的 w 来推断 z:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/b07f18aab275e42731a2437782905847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*6fqMubkBgSFOO5pnb2mvaQ@2x.png"/></div></figure><p id="c4b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，除了归一化因子由于巨大的维度而绝对难以处理的事实之外，我们还面临着组合学的挑战(因为问题的一些变量是离散的),需要使用 MCMC 或 VI 来获得近似解。对主题建模及其具体的底层贝叶斯推理问题感兴趣的读者可以看看这篇关于 LDA 的<a class="ae kv" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank">参考论文。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/d6f73f7312b97de8bf377b17e99598ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4iN2zgkYIQ0p-Ij4bgmldA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Illustration of the Latent Dirichlet Allocation method.</figcaption></figure></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="2bf4" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">马尔可夫链蒙特卡罗(MCMC)</h1><p id="733d" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">正如我们之前提到的，处理贝叶斯推理问题时面临的主要困难之一来自规范化因素。在本节中，我们描述了<strong class="ky ir"> MCMC 采样方法</strong>，它构成了一个可能的解决方案来克服这个问题以及一些其他与贝叶斯推理相关的计算困难。</p><h2 id="c600" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">抽样方法</h2><p id="ffd9" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">抽样方法的思想如下。让我们首先假设我们有一个方法(MCMC)来从一个概率分布中抽取样本，这个概率分布被定义为一个因子。然后，我们可以从该分布中获取样本(仅使用非标准化部分定义),并使用这些样本来计算各种准时统计，如均值和方差，甚至通过核密度估计来近似分布，而不是试图处理涉及后验的棘手计算。</p><p id="cce5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与下一节描述的 VI 方法相反，<strong class="ky ir"> MCMC 方法对研究的概率分布(贝叶斯推理情况下的后验概率)不采用模型</strong>。因此，这些方法具有较低的偏差，但具有较高的方差，这意味着大多数时候获得结果的成本更高，但也比我们从 VI 中获得的结果更准确。</p><p id="dce3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了结束这一小节，我们再次概述这样一个事实，即我们刚刚描述的这个抽样过程并不局限于后验分布的贝叶斯推断，而且更一般地说，还可以用于概率分布被定义为其归一化因子的任何情况。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/9caf8ab3937a4397732f0cfa838c13c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3nBb4AqcriLcENdpBp4fpQ@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Illustration of the sampling approach (MCMC).</figcaption></figure><h2 id="fcb2" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">MCMC 的理念</h2><p id="fad0" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">在统计学中，<strong class="ky ir">马尔可夫链蒙特卡罗算法旨在从给定的概率分布中生成样本</strong>。该方法名称中的“蒙特卡罗”部分是由于采样的目的，而“马尔可夫链”部分来自我们获取这些样本的方式(我们让读者参考我们关于马尔可夫链的<a class="ae kv" rel="noopener" target="_blank" href="/brief-introduction-to-markov-chains-2c8cab9c98ab">介绍文章</a>)。</p><p id="f695" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了产生样本，我们的想法是建立一个马尔可夫链，它的平稳分布就是我们想要从中取样的分布。然后，我们可以从马尔可夫链中模拟一个随机的状态序列，它足够长，足以(几乎)达到稳态，然后保留一些生成的状态作为我们的样本。</p><p id="445f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在随机变量生成技术中，MCMC 是一种非常先进的方法(我们已经在关于 GANs 的<a class="ae kv" rel="noopener" target="_blank" href="/understanding-generative-adversarial-networks-gans-cd6e4651a29">帖子中讨论了另一种方法)，它使得<strong class="ky ir">从一个非常困难的概率分布中获取样本成为可能，该概率分布可能只定义到一个乘法常数</strong>。反直觉的事实是，我们可以用 MCMC 从一个没有很好归一化的分布中获得样本，这来自于我们定义对这些归一化因子不敏感的马尔可夫链的特定方式。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/70c56cd87fb5347142c52094d51c2290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZBh2kDanLoTFmb3yzErGQ@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Markov Chain Monte Carlo approach is aimed at generating samples from a difficult probability distribution that can be defined up to a factor.</figcaption></figure><h2 id="3442" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">马尔可夫链的定义</h2><p id="6b44" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">整个 MCMC 方法是基于<strong class="ky ir">建立马尔可夫链的能力，该马尔可夫链的平稳分布就是我们想要从</strong>采样的分布。为了做到这一点，Metropolis-Hasting 和 Gibbs 抽样算法都使用了马尔可夫链的一个特殊性质:可逆性。</p><p id="c6bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">状态空间 E 上的马尔可夫链，转移概率表示为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/55d2e6d38d129ca6dd0335a0231af5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*1PwsQcZwvQ-Id9abB_SFBA@2x.png"/></div></figure><p id="381c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果存在一个概率分布γ，使得</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/7e060b6b15b146f1268522f177bb8904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8zbV1ejwDoommiUKixZ-A@2x.png"/></div></div></figure><p id="eac9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这样的马尔可夫链，我们可以很容易地证明我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/a53d3fdefaa54247510ed7454d21319f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0oahfPmyqcN_Q_D51klqdA@2x.png"/></div></div></figure><p id="e344" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，γ是一个平稳分布(如果马氏链是不可约的，它是唯一的)。</p><p id="cd47" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们假设我们想要采样的概率分布<strong class="ky ir"> π </strong>只定义了一个因子</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c6d684b28aacc9e4c9fdb7123b934a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*S7u499hk1Ki2ZtnIW8gEdw@2x.png"/></div></figure><p id="9144" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(其中 C 是未知的乘法常数)。我们可以注意到下面的等价关系成立</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/71ea6ffc5f72a7f02f507d985b7cd34f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZQSR1QtwXNz2GuSXG1Cyg@2x.png"/></div></div></figure><p id="8d55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后是转移概率为 k(.,.)定义来验证最后的等式将有，如预期的那样，<strong class="ky ir"> π </strong>为平稳分布。因此，我们可以定义一个马尔可夫链，它对于平稳分布有一个不能明确计算的概率分布<strong class="ky ir"> π </strong>。</p><h2 id="dc2e" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">吉布斯采样跃迁(∞)</h2><p id="7590" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">让我们假设我们想要定义的马尔可夫链是 D 维的，这样</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/c76bcb139f710fe96653b92668eb015e.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*G84c79iIolzx-pUPPXa65A@2x.png"/></div></figure><p id="4694" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">吉布斯抽样</strong>方法基于这样的假设，即使联合概率难以处理，也可以计算出给定其他维度的单个维度的条件分布。基于这一思想，定义转移，使得在迭代 n+1 时，要访问的下一个状态由以下过程给出。</p><p id="7ad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们在 X_n 的 D 维中随机选择一个整数 D。然后，在所有其他维保持固定的情况下，我们根据相应的条件概率为该维采样一个新值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/ab188c0d7f07b49e2488d8078b0e72b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tOmfzlCurY2ZelR-gnjQsw@2x.png"/></div></div></figure><p id="cd1a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/3fabd020389d282c770492c93d8bcd34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9AaCuxvTs89Xy0LlxOAUNQ@2x.png"/></div></div></figure><p id="ba9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是在给定所有其他维度的情况下，第 d 个维度的条件分布。</p><p id="c96d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">形式上，如果我们表示</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/2e8b65368fa52bd6d23fbde7a8e16791.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*p_jqeiUWQwO7CVdTY7wuIA@2x.png"/></div></figure><p id="39d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后可以写出转移概率</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/2d0937a095b6edb996f615935135af86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_oKGB41_jNwCHWAEM3O4Ew@2x.png"/></div></div></figure><p id="8d49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，局部平衡如预期的那样被验证，对于唯一重要的情况，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/be02bf6a0a81714fae9751b4e2c73c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HktHvBfTyfYFZWKmbTnArw@2x.png"/></div></div></figure><h2 id="4225" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">大都市的急速转变(∞)</h2><p id="02fc" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">有时，甚至吉布斯方法中的条件分布也太复杂而难以获得。在这种情况下，可以使用<strong class="ky ir"> Metropolis-Hasting </strong>。为此，我们从定义侧跃迁概率 h(.,.)这将用于建议过渡。然后，在迭代 n+1 时，马尔可夫链要访问的下一个状态由下面的过程定义。我们首先从 h 画出一个“建议转移”x，并计算接受它的相关概率 r:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/cd598bb2ae44c8affeb14afbbaa535b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUX15HQj_63FkcPd1XeYVQ@2x.png"/></div></div></figure><p id="8adc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么选择有效转换，使得</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/8596f5c99d1da099f4518695b5d2e6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQB_Nidk0B4cUfAxcV1rnQ@2x.png"/></div></div></figure><p id="87aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">形式上，转移概率可以写成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/280854bcf128727acd41320bfa7063a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b5CSJJYndXdiHrD3iOyDrA@2x.png"/></div></div></figure><p id="0f55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，本地余额如预期的那样得到验证</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/cf4e7942dd71ea8efa24679eaee4b930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HPcyKNv6YIIN92XGD60E9A@2x.png"/></div></div></figure><h2 id="799e" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">取样过程</h2><p id="0db1" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">一旦定义了我们的马尔可夫链，我们就可以模拟一个随机的状态序列(随机初始化),并选择其中的一些状态，以获得既遵循目标分布又相互独立的样本。</p><p id="bc7c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，为了使样本(几乎)遵循目标分布，我们需要<strong class="ky ir">只考虑离生成序列的开始足够远的状态，以几乎达到马尔可夫链</strong>的稳态(理论上，稳态只是渐近达到)。因此，第一个模拟状态不可用作样本，我们将达到稳态所需的这个阶段称为<strong class="ky ir">老化时间</strong>。注意，实际上很难知道这个老化时间需要多长。</p><p id="9b9b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二，为了有(几乎)独立的样本，<strong class="ky ir">我们不能在老化时间</strong>之后保持序列的所有连续状态。事实上，马尔可夫链的定义意味着两个连续状态之间的强相关性，因此我们只需要保留彼此相距足够远的状态作为样本，以被认为是几乎独立的。实际上，两个状态之间被认为几乎独立所需的<strong class="ky ir">滞后</strong>可以通过自相关函数的分析来估计(仅针对数值)。</p><p id="836c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，为了获得遵循目标分布的独立样本，<strong class="ky ir">我们保留来自所生成序列的状态，这些状态彼此相隔一个滞后 L，并且在老化时间 B </strong>之后出现。因此，如果马尔可夫链的连续状态被表示为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/31432c8725fe8b364617c9b2902722f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*QWslKPLEjVCx1B-yO4rWeg@2x.png"/></div></figure><p id="fdc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只保留州作为我们的样本</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0df34368a625c2f23c38dacede4f9447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*OqoOZPmEhycQJLlO5ZqSRA@2x.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/8af44f212566acba80bc194f9268eb00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IH4LadJXK8wBHTAMvVQkfw@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">MCMC sampling requires to consider both a burn-in time and a lag.</figcaption></figure></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="42b8" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">变分推理(六)</h1><p id="7488" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">另一种克服与推理问题相关的计算困难的可能方法是使用<strong class="ky ir">变分推理方法，该方法在于在参数化族</strong>中寻找分布的最佳近似值。为了找到最佳近似值，我们遵循一个优化流程(针对系列参数),该流程只要求将目标分布定义为一个因子。</p><h2 id="8412" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">近似方法</h2><p id="52f4" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">VI 方法包括在给定的家族中寻找一些复杂目标概率分布的最佳近似。更具体地说，其思想是定义一个参数化的分布族，并对参数进行优化，以获得相对于明确定义的误差度量最接近目标的<strong class="ky ir">元素。</strong></p><p id="8a16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们仍然考虑定义为归一化因子 C 的概率分布<strong class="ky ir"> π </strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/3b787bfc1fe505f06577689ff2746b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*YHrSdFqdCV9itQezC1QD2Q@2x.png"/></div></figure><p id="01f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，用更数学的术语来说，如果我们表示参数化的分布族</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/b0b468fa00432f128d9e845dca1f5930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlqxS1j0bs5gGibUutDi5w@2x.png"/></div></div></figure><p id="7ae7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">并且我们考虑两个分布 p 和 q 之间的误差度量 E(p，q ),我们搜索最佳参数使得</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/b86da590e46c2b4ce4c670f1e09e3be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*nz1whiWrVRTJef1iVNURug@2x.png"/></div></figure><p id="df11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们可以解决这个最小化问题，而不必显式归一化<strong class="ky ir"> π </strong>，我们就可以使用 f_𝜔*作为一种近似来估计各种量，而不是处理棘手的计算。变分推理方法所隐含的最优化问题实际上被认为比直接计算(归一化、组合学等)要简单得多。</p><p id="2943" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与抽样方法相反，<strong class="ky ir">假设了一个模型(参数化家族)，这意味着一个偏差和一个较低的方差</strong>。总的来说，VI 方法不如 MCMC 方法精确，但产生结果的速度快得多:这些方法更适合于大规模、非常统计的问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/85889e25c6a7d4f6e1e4eeb8a6b46482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ei3-DC1_4RRzaDYfOOHbZA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Illustration of the approximation approach (Variational Inference).</figcaption></figure><h2 id="9d1d" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">分布族</h2><p id="55ce" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">我们需要设置的第一件事是参数化的分布族，它定义了我们搜索最佳近似的空间。</p><p id="f3d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">家庭的选择定义了一个控制方法的偏倚和复杂性的模型</strong>。如果我们假设一个非常严格的模型(简单的家庭)，那么我们有一个高偏差，但优化过程是简单的。相反，如果我们假设一个相当自由的模型(复杂的家庭)，偏差会低得多，但优化会更困难(如果不是难以处理的话)。因此，我们必须在一个足够复杂以确保最终逼近的良好质量的族和一个足够简单以使优化过程易于处理的族之间找到正确的平衡。我们应该记住，如果族中没有一个分布接近目标分布，那么即使是最好的近似也只能给出很差的结果。</p><p id="a90c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">平均场变分族</strong>是一个概率分布族，其中所考虑的随机向量的所有分量都是独立的。这个系列的分布具有产品密度，使得每个独立的成分由产品的不同因素控制。因此，属于平均场变分族的分布具有可以写成的密度</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d17e0e49b43a275a1ece353be383f46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*0B1XsYaMnhpEVJ0rgMZ3MA@2x.png"/></div></figure><p id="b867" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里我们假设一个 m 维随机变量 z，注意，即使在符号中省略了它，所有的密度 f_j 都是参数化的。因此，例如，如果每个密度 f_j 是具有均值和方差参数的高斯分布，则全局密度 f 由来自所有独立因素的一组参数来定义，并且在整个参数组上进行优化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/599de4f7d4b00a298f992dd186987d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6N1m9kwigxbMVn1OHlun3w@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The choice of the family in variational inference sets both the difficulty of the optimisation process and the quality of the final approximation.</figcaption></figure><h2 id="e5bd" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">库尔贝克-莱布勒散度</h2><p id="783c" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">一旦定义了这个族，一个主要的问题仍然存在:如何在这个族中找到一个给定概率分布的最佳近似(明确定义到它的归一化因子)？即使最佳近似值明显取决于我们考虑的误差测量的性质，但似乎很自然地假设<strong class="ky ir">最小化问题对归一化因子不敏感，因为我们更想比较质量分布而不是质量本身</strong>(对于概率分布必须是单一的)。</p><p id="3ed6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，现在让我们定义一下<strong class="ky ir"> Kullback-Leibler (KL) </strong>散度，看看这个度量使问题对归一化因子不敏感。如果 p 和 q 是两个分布，KL 散度定义如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/44ee8d8c57d75ea086f7a10689340b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7S-OMKGN8_Yd0P8LrGRxLQ@2x.png"/></div></div></figure><p id="0fd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据这个定义，我们可以很容易地看到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/9e683afa8e75fe8c8b6cacef95b49f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yUBblR5YfIosVc-FjF_Cvg@2x.png"/></div></div></figure><p id="8ca5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的最小化问题，这意味着以下等式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/48fb235592c8e7c4a9f6e328fa5a8d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b--W7Snp2Qg2a7aVA-r6eA@2x.png"/></div></div></figure><p id="acf0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，当选择 KL 散度作为我们的误差度量时，优化过程对乘法系数不敏感，并且我们可以在我们的参数化分布族中搜索最佳近似，而不必像预期的那样计算目标分布的痛苦的归一化因子。</p><p id="1c6c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，作为一个附带事实，我们可以通过为感兴趣的读者注意到 KL 散度是交叉熵减去熵来结束这一小节，并且在信息论中有很好的解释。</p><h2 id="df4a" class="mz md iq bd me na nb dn mi nc nd dp mm lf ne nf mo lj ng nh mq ln ni nj ms nk bi translated">优化过程和直觉</h2><p id="c566" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">一旦定义了参数族和误差测量，我们就可以初始化参数(随机或根据明确定义的策略)并进行优化。可以使用几个经典的优化技术，例如<strong class="ky ir">梯度下降或坐标下降</strong>，这将在实践中导致局部最优。</p><p id="1eee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更好地理解这个优化过程，让我们举一个例子，回到贝叶斯推理问题的具体情况，我们假设一个后验概率如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/7ac8f84e85c4a5978bbfb0af2fd9030f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TT2LYMNYyMYwhnBj64C51Q@2x.png"/></div></figure><p id="0576" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，如果我们想要使用变分推断获得这个后验的近似，我们必须解决下面的优化过程(假设定义的参数化族和 KL 散度作为误差度量)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/f8645876221e9aadd724d020c7ed9b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mo37rvVTkcpcL7ksO0dfhQ@2x.png"/></div></div></figure><p id="0987" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一个等式有助于我们更好地理解如何鼓励近似值分配其质量。<strong class="ky ir">第一项是预期对数似然</strong>，其倾向于调整参数，以便将近似值的质量放在解释最佳观察数据的潜在变量 z 的值上。<strong class="ky ir">第二项是近似值和先验分布</strong>之间的负 KL 散度，其倾向于调整参数，以便使近似值接近先验分布。因此，这个目标函数很好地表达了通常的先验/似然平衡。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/088cde773a90b4fb36a65e573357e2e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z53f0HAcO4DkAU1KWVVrQg@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Optimisation process of the Variational Inference approach.</figcaption></figure></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h1 id="8192" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">外卖食品</h1><p id="548d" class="pw-post-body-paragraph kw kx iq ky b kz mu jr lb lc mv ju le lf mw lh li lj mx ll lm ln my lp lq lr ij bi translated">这篇文章的主要观点是:</p><ul class=""><li id="b149" class="nu nv iq ky b kz la lc ld lf nw lj nx ln ny lr nz oa ob oc bi translated">贝叶斯推理是统计学和机器学习中的一个非常经典的问题，它依赖于众所周知的贝叶斯定理，其主要缺点是，大多数时候，在一些非常繁重的计算中</li><li id="2208" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">马尔可夫链蒙特卡罗(MCMC)方法旨在模拟来自密度的样本，这些密度可能非常复杂和/或定义到某个因子</li><li id="1456" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">MCMC 可以用在贝叶斯推理中，以便直接从后验的“非归一化部分”生成样本来处理，而不是处理棘手的计算</li><li id="6798" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">变分推断(VI)是一种近似分布的方法，使用参数的优化过程在给定的族中找到最佳近似</li><li id="b83a" class="nu nv iq ky b kz od lc oe lf of lj og ln oh lr nz oa ob oc bi translated">VI 优化过程对目标分布中的乘法常数不敏感，因此，该方法可用于近似仅定义到归一化因子的后验</li></ul><p id="a43f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如已经提到的，MCMC 和 VI 方法具有不同的属性，这意味着不同的典型用例。一方面，MCMC 方法的采样过程相当繁重，但是没有偏差，因此，当期望精确的结果时，这些方法是优选的，而不考虑所花费的时间。另一方面，尽管 VI 方法中家族的选择会明显引入偏差，但它伴随着合理的优化过程，使这些方法特别适合于需要快速计算的超大规模推理问题。</p><p id="6895" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">MCMC 和 VI 之间的其他比较可以在优秀的<a class="ae kv" href="https://arxiv.org/pdf/1601.00670.pdf" rel="noopener ugc nofollow" target="_blank">变分推断:统计学家回顾</a>中找到，我们也强烈推荐给仅对 VI 感兴趣的读者。关于 MCMC 的进一步阅读，我们推荐这本<a class="ae kv" href="https://pdfs.semanticscholar.org/21a9/2825dcec23c743e77451ff5b5ee6b1091651.pdf" rel="noopener ugc nofollow" target="_blank">总论</a>以及这本<a class="ae kv" href="https://www.cs.ubc.ca/~arnaud/andrieu_defreitas_doucet_jordan_intromontecarlomachinelearning.pdf" rel="noopener ugc nofollow" target="_blank">面向机器学习的导论</a>。有兴趣了解更多关于应用于 LDA 的 Gibbs 抽样的读者可以参考这篇关于主题建模和 Gibbs 抽样的<a class="ae kv" href="http://u.cs.biu.ac.il/~89-680/darling-lda.pdf" rel="noopener ugc nofollow" target="_blank">教程</a>(结合这些<a class="ae kv" href="http://www2.cs.uh.edu/~arjun/courses/advnlp/LDA_Derivation.pdf" rel="noopener ugc nofollow" target="_blank">关于 LDA Gibbs 抽样器的讲义</a>进行谨慎推导)。</p><p id="fe66" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，让我们以一点调侃来结束，并提到在即将到来的帖子中，我们将讨论变分自动编码器，这是一种基于变分推理的深度学习方法…所以请保持关注！</p><p id="75d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您的阅读，如果您认为值得分享，请随意分享！</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="71c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">巴蒂斯特·罗卡的最后一篇文章:</p><div class="pk pl gp gr pm pn"><a rel="noopener follow" target="_blank" href="/introduction-to-recommender-systems-6c66cf15ada"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd ir gy z fp ps fr fs pt fu fw ip bi translated">推荐系统简介</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">几种主要推荐算法综述。</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">towardsdatascience.com</p></div></div><div class="pw l"><div class="px l py pz qa pw qb kp pn"/></div></div></a></div><div class="pk pl gp gr pm pn"><a rel="noopener follow" target="_blank" href="/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205"><div class="po ab fo"><div class="pp ab pq cl cj pr"><h2 class="bd ir gy z fp ps fr fs pt fu fw ip bi translated">整体方法:装袋、助推和堆叠</h2><div class="pu l"><h3 class="bd b gy z fp ps fr fs pt fu fw dk translated">理解集成学习的关键概念。</h3></div><div class="pv l"><p class="bd b dl z fp ps fr fs pt fu fw dk translated">towardsdatascience.com</p></div></div><div class="pw l"><div class="qc l py pz qa pw qb kp pn"/></div></div></a></div></div></div>    
</body>
</html>