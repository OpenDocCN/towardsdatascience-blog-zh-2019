<html>
<head>
<title>Generative Adversarial Networks: Revitalizing old video game textures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生成性对抗网络:重振旧的视频游戏纹理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generative-adversarial-networks-revitalizing-old-video-game-textures-669493f883a0?source=collection_archive---------16-----------------------#2019-02-28">https://towardsdatascience.com/generative-adversarial-networks-revitalizing-old-video-game-textures-669493f883a0?source=collection_archive---------16-----------------------#2019-02-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/77f286cbe537a81aa5938e697a2d4751.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UWntnHzvXP3I3btlUPBSAA.gif"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Comparison of traditional resampling vs ESRGAN output, and the original image on the right.</figcaption></figure><p id="43a5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你的屏幕顶部有一个大的绿色条，等一分钟！正在加载！</p><p id="0109" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果有一件事是人们喜欢在视频游戏中谈论的，那就是他们的图形保真度。这些年来，衡量游戏进步的标准是现代硬件带来下一波“新一代图形”的能力。但这并不意味着我们不再喜欢昔日的游戏。我们很多人都有一种强烈的怀旧情绪，对过去的游戏(和它们的图像)的热爱。生活在这样一个时代是多么令人兴奋啊，机器学习可以直接应用在一种叫做单图像超分辨率的方法中，以增强那些过去遗迹的纹理，并使它们更多地融入现代世界。</p><p id="8f85" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我在这里使用的模型 SRGAN 基于这篇论文【https://arxiv.org/abs/1609.04802 T2】</p><p id="05c9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以及在 https://github.com/xinntao/ESRGAN<a class="ae la" href="https://github.com/xinntao/ESRGAN" rel="noopener ugc nofollow" target="_blank">ECCV 2018</a>展示的原始模型的扩展</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lb"><img src="../Images/46f2fbad6fd2e84039ccc95e7f1e1473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*SlMco1u_D5jo6_zTZjgw0g.gif"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Before and after comparisons of Ocarina of Time textures</figcaption></figure><p id="dba4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在我们深入这个模型的内部工作之前，理解 SISR 是什么以及它是如何工作的是很重要的。它在图片和游戏纹理领域之外有许多应用，如显微镜和雷达，但为了清楚起见，我们将只讨论数字媒体的应用。</p><p id="7bcf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">SISR 模型试图从低分辨率图像中吸收高分辨率图像，同时保持图像的保真度不变。意味着没有图像噪声，没有伪像。直到最近几年，很难在不丢失大量纹理细节的情况下缩放图像。通常，超分辨率算法会试图创建一个成本函数来最小化增强的高分辨率图像和基础图像之间的 MSE(均方误差)。MSE 的降低与峰值信噪比直接相关。(PSNR)不幸的是，这种测量是通过逐个像素的比较来完成的，通常会导致高度模糊的图像，例如双三次图像放大或最近邻变换。SISR 模型通常比那些重采样方法实现更高的保真度，尽管随着分辨率的提高，您开始看到类似的行为模式，或者在某些情况下像素开始融合在一起。比较 SRGAN，以其增强的未来版本如下。正如你所看到的，当我们继续向上缩放时，眼睛开始有点塌陷。但这无疑是对简单取样方法的改进。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lg"><img src="../Images/0b9cfff3b54013231f7296bf58e7e10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZLqMZ-orWy5S8UcqMXbriA.png"/></div></div></figure><p id="be63" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">那么我们来谈谈 GAN 网络到底是什么。分解一下。</p><p id="6f71" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">生成+对抗+网络。</p><p id="2aa2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个术语的生成部分来自于这样一个事实，即甘寻求创建内容作为算法的输出。对抗性术语暗指算法的生成部分需要某种东西与之竞争，一个对手。网络指的是如何将生成性和对抗性模型联系在一起，以最终合作实现模型的最终目标。对于图像，GAN 网络试图从它们的训练集中生成新的图像，而对抗模型试图确定生成的图像是真是假。像 SISR 一样，基于图像的 GAN 网络寻求最小化成本函数，尽管这里有更广泛的误差测量，例如由 GAN 网络生成的特征映射之间的欧几里德距离。</p><h1 id="3e88" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="ak"> SRGAN </strong></h1><p id="8b9c" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">所以现在你可能会猜到这只是超分辨率生成对抗网络的缩写。基于 GAN 的 SISR 方法。SRGAN 用从 VGG(视觉几何组)网络的特征地图计算的新损失函数来代替 MSE 成本最小化(记住，这是在逐个像素的基础上测量的，并且可以留给我们过度平滑的表示)。</p><p id="0f66" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">2017 年，SRGAN 是最先进的，是同类中最好的。但是世界不会为任何人而停止。输入新的竞争对手…</p><h1 id="40d4" class="lh li iq bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">ESRGAN:简介</h1><p id="42a7" class="pw-post-body-paragraph kc kd iq ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated"><strong class="ke ir">以及一些理解其重要性的必备知识</strong></p><p id="a247" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">ESRGAN 向 SRGAN 引入了残差中残差密集块。通常，当模型中有许多复杂层时，层数越多，效率越低，因为越接近完全精度，最小化误差的总和就越低，并且这些层在某个点开始相互竞争。使用传统的方法，当层数超过 25 层时，你实际上开始失去准确性。</p><p id="cff7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">最小化是如何计算的？</strong></p><p id="bc11" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">由于消失梯度问题，深度网络在训练期间很难参数化。渐变用于计算模型早期层的权重。这种重复经常会使梯度变得无限小，正如你所想象的，网络越深，梯度就越小。我们使用梯度来计算成本函数。如果我们不能引入一种方法来处理这个问题，最终这个成本函数将会开始增加。</p><p id="3dd3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">剩余连接的引入</strong></p><p id="f4e0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">2015 年，ResNet 将通过实施剩余网络改变世界。他们的理论是“身份快捷连接”将允许连接的层符合残差映射。</p><p id="25ae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">设 H(x)是所需的底层映射。我们试图使堆叠的非线性层符合另一个映射 F(x):= H(x)-xF(x):= H(x)-x。</p><p id="fd74" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">原始映射可以重铸为 F(x)+x。他们理论上认为优化残差映射比优化原始映射更容易。</p><p id="6ef0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">他们是对的。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/957780c39be5034f5e4e77d85b0e5028.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*piEuydQM3bAiFyT8wTC9_w.png"/></div></figure><p id="19ce" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">恒等式快捷连接寻求学习恒等式函数，该函数简单地将当前层的计算权重和前一层(l+1)的偏差值的权重矩阵设置为 0。通过这样做，新层(l + 2)的激活与层 l 相同</p><p id="0ed2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">吴恩达对此解释得比我好得多，并且通过一些数学推导，绝对值得一看。</p><p id="9fee" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">结果是，与标准网络相比，该网络停止了梯度成本函数的退化，并提高了性能。太神奇了！</p><p id="a6fd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">改善剩余网络</strong></p><p id="bd25" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">此后不久，该社区爆发了寻求优化这一新方法的研究人员的涌入。提出了剩余层的预激活，这将允许梯度使用捷径连接来跳过任何层而不受阻碍。这进一步改进了剩余方法，将 1001 层网络带入世界，其性能优于更浅的同类网络。</p><p id="6d6e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> ESRGAN 定制的 SRGAN 及其开山鼻祖 ResNet </strong></p><p id="5f9a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了 ESRGAN 的 RRDB 方法之外，他们还放弃了批量归一化图层，并替换为残差缩放和一分钟初始化学习速率。然后，它利用 RaGAN 或相对论平均 GAN 来衡量一个图像与另一个图像的相关性，即这个图像更真实还是更不真实，而不是衡量一个图像是否是假的。</p><p id="7e14" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">SRGAN 的感知损失函数在它的计算中使用 VGG 特征<em class="ml">在</em>激活后，ESRGAN 在激活前移动该利用率<em class="ml">。</em></p><p id="7b02" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">所有这些导致图像比以往任何时候都更清晰。</p><p id="9fd4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">最终想法:</strong></p><p id="3378" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我已经使用这个模型，并在一系列现代纹理上训练它，并将其应用于旧 N64 和 PlayStation 1 纹理，结果令人惊叹。</p><p id="7f38" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然而，仍有改进的余地。当图像中有大量像素时，或者对于没有太多重复图案的图像，模型有时可以放大成几乎像浮雕一样的实例，如第一幅图像所示。我的假设是，艺术形式与任何其他类型的图像都非常不同，我需要收集适当的训练数据，即高分辨率像素艺术。这可能很难找到，甚至更难(耗时！)来制作。</p><p id="a3ae" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果你知道任何艺术家或者你自己愿意为这个项目做贡献，请在 otillieodd@gmail.com 联系我</p><p id="f934" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我希望能够编辑这篇文章，并与社区分享升级的任天堂 64 和 Playstation 纹理，因为我认为它可以对游戏进行一些非常酷的修改，并给我们老前辈一些新的理由来重播我们的最爱，但我仍在研究这样做是否合法。</p><p id="5cbb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">到那时，感谢阅读！</p></div></div>    
</body>
</html>