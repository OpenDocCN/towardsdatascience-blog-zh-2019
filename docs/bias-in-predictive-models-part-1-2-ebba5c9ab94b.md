# 预测模型中的偏差—第 1/2 部分

> 原文：<https://towardsdatascience.com/bias-in-predictive-models-part-1-2-ebba5c9ab94b?source=collection_archive---------27----------------------->

## 对抗偏见的第一步是定义它。

![](img/17d87fc531d6c9920019c8545a0e239d.png)

# 摘要

人工智能已经在我们的生活中发挥了重要作用，很快它将做出越来越多改变生活的决定。人工智能的公平和偏见问题正在吸引越来越多的研究和立法者的关注。关于什么是偏倚以及如何测量偏倚，有几种可能的定义，每一种都有自己的优点，但也有适用性的限制。

使用哪一种是一个重要的(也不是很简单的)选择，公司，可能还有监管者都需要做出这个选择。

# 介绍

还记得最近关于亚马逊人工智能招聘工具对女性产生偏见的[故事](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)吗？幸运的是，他们的团队能够在模型投入生产之前发现问题。人工智能已经在我们的生活中发挥了重要作用，很快它将做出越来越多改变生活的决定。你会得到那份工作吗？贷款被批准了吗？你的产品会被推荐吗？你的社交媒体账户会被屏蔽吗？你的孩子会被那所大学录取吗？你会被“随机”选择进行深入的税务审计吗？甚至，你会提前出狱吗？并不是这个列表上的所有决定都已经被人工智能完成了，但是很多已经完成了，而且这个列表每年都在增长。

在[这个关于布劳沃德县累犯率的 ProPublica 分析](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)中，显示(见下表)没有再犯的非裔美国人被(错误地)标记为高风险的可能性几乎是白人的两倍。

![](img/8e2599b24f17771637d04ede5d158d5d.png)

大多数类型的人工智能，如果任其自生自灭，将会延续它们被训练的历史数据中存在的任何趋势(包括偏见)。如果女性以前是劳动力的一小部分，这就是人工智能要学习的(就像亚马逊招聘工具的故事一样)。

有了这么多的利害关系，难怪这个话题越来越受到关注，无论是来自研究人员的[还是现在来自立法者的](/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)。

![](img/d21cfbb8820b55cc3073864a968b6ce9.png)

数字摘自[本文](/a-tutorial-on-fairness-in-machine-learning-3ff8ba1040cb)

这个问题提出了两个高层次的问题:

1.  什么是偏差，如何衡量偏差？
2.  如果存在偏见，又能做些什么呢？

这两个问题都不容易回答。在这篇文章中，我们集中讨论偏见的定义和衡量方法。解决这个问题的方法将在后续讨论。

# 偏见的定义

在一个理想的世界里，每个人都希望他们的模型是“公平的”，没有人希望他们是“有偏见的”，但是这到底意味着什么呢？不幸的是，关于偏见或公平，似乎没有一个普遍认可的定义。

在这篇关于衡量模型公平性的不同方法的伟大文章中，谷歌 People + AI Research (PAIR)的常驻作家大卫·温伯格(David Weinberger)提出了不少于五个(！)定义什么是公平的方法各不相同，各有利弊。

有趣的是，一些更直观的方法，如“群体无意识”或“人口统计均等”有时并不适合现实生活中的问题。

**【群体无意识】**方法规定，如果完全忽略受保护的班级信息(如性别)，结果将反映申请人的客观优点，这是公平的。如果结果是某个性别的代表比例失调，这就是“数据所显示的”。然而，在现实生活中，事情往往更复杂。性别信息可能隐藏在一些其他变量(代理)中，而源于过去可能存在的歧视的女性比例较低将会悄悄进入我们的新模型(就像亚马逊案例或这个[谷歌翻译示例](https://qz.com/1141122/google-translates-gender-bias-pairs-he-with-hardworking-and-she-with-lazy-and-other-examples/)中发生的那样)。有些问题，比如医学诊断，可能直接取决于性别。在其他问题中，一些参数的意义可能间接取决于性别。例如，在简历分析中，女性就业中的某些差距可能表示产假，与一个人不能或不愿找到工作时的就业差距相比，产假具有不同的含义。总的来说，有时引用受保护的变量是可以的，甚至是可取的，不引用它们并不能保证不会有偏差。

另一种常见的方法，**“人口统计均等”**有效地假设了结果的均等。根据这一假设，如果有 30%的橙色申请人，我们希望看到 30%的橙色批准。“人口均等”的概念与“不同影响”标准非常相似，后者用于美国反歧视法律框架，监管信贷、教育、就业等领域。然而，这种方法的应用远非显而易见。这些橙色的申请者和蓝色的一样合格吗？什么应该算 100%？**一些缩小差距的方法可能导致相似的人受到不同的待遇，这本身就可以被视为歧视。**一个这样的案件甚至[到达了](https://en.wikipedia.org/wiki/Ricci_v._DeStefano)最高法院，而一般的话题是[法律讨论](https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1526&context=articles)的主题，这超出了本概述的范围。

这种同等结果的假设在某些情况下可能是正确的，但它总是正确的吗？有时，由于与偏差无关的原因，受保护变量与输出相关。例如，某些疾病的发生率在不同性别或种族之间可能有很大差异。

最近，英特尔[宣布](https://www.theregister.co.uk/2018/10/31/intel_diversity_report_2018/)他们实现了“全性别代表”，只有大约 27%的女性员工。“人口均等”的天真应用当然会失败，因为 27%远低于女性在总人口中的比例。英特尔可以这样说，因为他们只使用相关大学学位的毕业生作为参考，因为根据英特尔的说法，只有她们才是合格的女性候选人。

**在统计学中，估计量的偏差定义为估计量的期望值与被估计参数的真值之间的差值。然而，在现实生活中，往往很难知道这个“真实价值”是什么。如果被拒绝的借款人获得了贷款批准，他们真的会偿还贷款吗？**

有时我们有机会知道所有候选人的实际结果，包括被拒绝的候选人。例如，[在这种情况下](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)，该模型预测现有罪犯未来犯罪的可能性，在他们被释放几年后，我们可以知道谁实际上犯了另一个罪行，谁没有。在贷款案例中，被特定银行拒绝贷款的人可能仍然能够在其他地方获得贷款，并且信用局可能有关于他们随后付款的信息。对于这种情况，在已知*实际结果*的情况下，由谷歌大脑团队和加州大学伯克利分校“机器学习的公平性”教师 Moritz Hardt 提出的以下标准似乎是适用的:

> *应该调整系统，使两个类别的批准和拒绝总数中出错的百分比相同*

**换句话说，我们将*对不良申请人的批准*或*对优秀申请人的拒绝*因不同阶层(如性别)**差异太大的情况定义为有偏见(并希望避免)。注意，我们不需要假设任何关于*正确*批准或拒绝的比率。尽管很吸引人，但这一指标有些不太直观，当然，只有在实际结果已知时才适用，例如在对历史数据进行回溯测试时。

总之，有多种方法来定义和衡量偏见，这可能或多或少适合你的特殊问题。使用哪一种是一个重要的(也不是很简单的)选择，公司，可能还有监管者都需要做出这个选择。

在选择了某种检测偏差的方法后，**下一个挑战是减轻偏差**。一些减轻偏见的方法，包括现有的和新的，将在下一篇文章中讨论。