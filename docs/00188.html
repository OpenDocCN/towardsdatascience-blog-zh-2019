<html>
<head>
<title>Similarity of Documents with Random Walks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机游走文档的相似性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/similarity-of-documents-with-random-walks-98f94fd2c76c?source=collection_archive---------20-----------------------#2019-01-08">https://towardsdatascience.com/similarity-of-documents-with-random-walks-98f94fd2c76c?source=collection_archive---------20-----------------------#2019-01-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="100b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python 代码和解释，用于计算带有重启的随机行走的图中节点的相似性</h2></div><p id="407b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑具有 5 个节点的无向图，其中节点的连接如下。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/d3f3c90ed1aec816e30342eaf06cc211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-BZLW3JUHd1MZbNq1MjQA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Document-topic assignment graph</figcaption></figure><p id="27d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们看到有两种类型的节点。一类是<em class="lr">文档</em>另一类是<em class="lr">题目。</em>在这篇博客文章中，我将解释如何使用重启随机漫步来计算基于主题的文档之间的相似性。您还可以想到一个更大的图，它的节点类型是<em class="lr">单词</em>而不是<em class="lr">主题</em>，其连接告诉我们哪个单词出现在哪个文档中。为了简单起见，我将选择主题。</p><h2 id="1089" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">随机行走</h2><p id="483e" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">现在想象我们站在文档 1 的节点上，随机(统一)决定走哪条边到达另一个节点。因为我们只有两条边，所以我们去<em class="lr">主题 1 </em>的概率是 0.5，就像我们去<em class="lr">主题 2 </em>的概率一样。我们总共有 1 英镑。</p><p id="32c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们对所有的节点进行这样的想象，我们最终会得到一个矩阵 M，它的起点是列，概率在相应的行中。这个矩阵可以用 python 创建，代码如下:</p><pre class="lc ld le lf gt mq mr ms mt aw mu bi"><span id="bd04" class="ls lt iq mr b gy mv mw l mx my"><strong class="mr ir">import</strong> numpy <strong class="mr ir">as</strong> np<br/><br/><br/>M = np.array([<br/>    [0, 0, 0, 1./3., 0.5],<br/>    [0, 0, 0, 1./3., 0],<br/>    [0, 0, 0, 1./3., 0.5],<br/>    [0.5, 1, 0.5, 0, 0],<br/>    [0.5, 0, 0.5, 0, 0]<br/>])</span></pre><p id="361a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通常的随机漫步不会只有一步，而是两步。因此，两步后，您从<em class="lr">文档 1 </em>到达节点<em class="lr">文档 3 </em>的概率就是这些概率的组合。为了计算一个步行者在两步后到达的概率，下面的脚本会有所帮助。</p><pre class="lc ld le lf gt mq mr ms mt aw mu bi"><span id="d97b" class="ls lt iq mr b gy mv mw l mx my">v = np.array([1., .0, .0, .0, .0]) # starting point<br/><strong class="mr ir">for</strong> step <strong class="mr ir">in</strong> range(n):<br/>    v = np.dot(M, v)</span></pre><p id="e860" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">变量 v 告诉我们当前节点的概率。因为我们从<em class="lr">文档 1 </em>开始，所以在该节点的概率是 100%。点乘取这个概率，并与下一个可能节点的概率执行向量乘法。因此，我们得到一个向量，它告诉我们，基于我们之前去过的地方的概率，我们可能到达的地方的概率。对于第一步，这只是 m 的第一列。从那里开始，我们可以在<em class="lr">主题 1 </em>或<em class="lr">主题 2 </em>处，并且必须将这个概率乘以下一个路径的概率。</p><h2 id="8bf6" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">重新启动</h2><p id="76a8" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">带重启的随机行走算法包括重启的概念。在这里，一个步行者有β的概率继续行走而不需要重新开始，因此 1-β再次传送到它的起点并从那里开始行走。这有助于探索更多样化的道路。</p><p id="af8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现相当容易。我们不只是用转移矩阵 M 乘以我们现在可能在哪里的概率，而是包括我们在开始节点结束的概率。</p><pre class="lc ld le lf gt mq mr ms mt aw mu bi"><span id="f29a" class="ls lt iq mr b gy mv mw l mx my">start = np.array([1., .0, .0, .0, .0]) # starting point<br/>v = start<br/><strong class="mr ir">for</strong> step <strong class="mr ir">in</strong> range(n):<br/>    v = beta*np.dot(M, v) + (1-beta)*start</span></pre><p id="e2da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们将通常的步行乘以β，并加上到达起点的剩余机会(1-β)。</p><h2 id="69b9" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">实验</h2><p id="d580" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">如果我们进行 n 步随机行走，我们得到以下概率:</p><pre class="lc ld le lf gt mq mr ms mt aw mu bi"><span id="5f4f" class="ls lt iq mr b gy mv mw l mx my">array([0.34461028, 0.06633499, 0.14461028, 0.24875622, 0.19568823])</span></pre><p id="3ff0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它们会告诉我们可能会到达哪个节点。有 34%的概率我们会再次回到起点，7%的概率我们会在<em class="lr">文档 2 </em>、<em class="lr">T3】处，14%的概率我们会在<em class="lr">文档 3 </em>处，依此类推。因此，出现在<em class="lr">文档 3 </em>的概率是<em class="lr">文档 2 </em>的两倍，因此我们可以说<em class="lr">文档 1 </em>(我们的起点)与<em class="lr">文档 3 </em>的相似度是<em class="lr">文档 2 </em>的两倍。</em></p><h2 id="c046" class="ls lt iq bd lu lv lw dn lx ly lz dp ma ko mb mc md ks me mf mg kw mh mi mj mk bi translated">结论</h2><p id="fea6" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">由于事先的题目分配，已经清楚<em class="lr">文件 3 </em>与<em class="lr">文件 1 </em>相似。无论如何，这个简单的例子可以推广到更复杂的图形和实体之间的关系。结合简单的实现，这是一种计算相似性的好方法。</p></div></div>    
</body>
</html>