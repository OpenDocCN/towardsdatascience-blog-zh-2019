<html>
<head>
<title>Visualizing The Non-linearity of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可视化神经网络的非线性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-the-non-linearity-of-neural-networks-c55b2a14ad7a?source=collection_archive---------14-----------------------#2019-07-14">https://towardsdatascience.com/visualizing-the-non-linearity-of-neural-networks-c55b2a14ad7a?source=collection_archive---------14-----------------------#2019-07-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0478" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将通过一个基本的例子来展示神经网络中非线性激活函数的能力。为此，我创建了一个人工数据集。每个数据点都有两个特征和一个类别标签 0 或 1。所以我们有一个二元分类问题。如果我们把这些特征称为 x₁和 x₂，那么这些数据在(x₂)-spacex₁)的情节如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/55a71b19c537024e7c8d5a715862f5eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*I5mPlecJ1RxcIwP1BgMKRw.png"/></div></figure><p id="68ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里，红点对应负类，蓝点对应正类。请注意，数据不是线性可分的，这意味着没有线来分隔蓝点和红点。因此，对于给定的特征表示，线性分类器没有用。现在，我们将训练一个具有两个单元和非线性<code class="fe kt ku kv kw b">tanh</code>激活函数的一个隐藏层的神经网络，并可视化该网络学习的特征。</p><p id="d9e7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了创建模型，我将使用 Tensorflow 2.0 和<code class="fe kt ku kv kw b">tf.keras</code>:</p><pre class="km kn ko kp gt kx kw ky kz aw la bi"><span id="2181" class="lb lc iq kw b gy ld le l lf lg">inputs = tf.keras.Input(shape=(2,))<br/>x = tf.keras.layers.Dense(2, activation=tf.nn.tanh)(inputs)<br/>outputs = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)(x)<br/>model = tf.keras.Model(inputs=inputs, outputs=outputs)</span><span id="ab2a" class="lb lc iq kw b gy lh le l lf lg">model.compile(optimizer='adam',<br/>              loss='binary_crossentropy',<br/>              metrics=['accuracy'])</span></pre><p id="f4d5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将隐藏层的权重和偏差表示如下:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi li"><img src="../Images/8b66b731a94bb24807ec7e4ac7563fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*Z72QQq-t_h3153HELQI0Eg.png"/></div></figure><p id="f9d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">权重和偏差的初始值定义了(x₁，x₂)-space，</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/f0c401ff0c88f4416e9fc1168d5287f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*L-L-g9vw22blWP5E-5ImsA.png"/></div></figure><p id="de4b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练前的这些初始行以及数据如下所示:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/e91a551184278f6f044e5c28e02414bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*WKeqd2QLs8H-KtoMVWsuMw.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">Data and the initial lines defined by the hidden layer in (x₁,x₂)-space</figcaption></figure><p id="0638" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，这两个初始行没有很好地划分给定的两个类。现在让我们训练模型，看看这些线会发生什么。</p><pre class="km kn ko kp gt kx kw ky kz aw la bi"><span id="9eef" class="lb lc iq kw b gy ld le l lf lg">model.fit(x_train, y_train, batch_size = 16, epochs=100)</span><span id="e997" class="lb lc iq kw b gy lh le l lf lg">Epoch 100/100<br/>400/400 [==============================] - 0s 80us/sample - loss: 0.1116 - accuracy: 1.0000</span></pre><p id="93a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 100 个时期后，我们的训练准确度是 100%,因此模型正确地分类了训练数据中的所有点。</p><p id="7a9c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，您可以通过使用<code class="fe kt ku kv kw b">model.weights</code>获得 keras 模型的参数，它返回一个权重和偏差列表:</p><pre class="km kn ko kp gt kx kw ky kz aw la bi"><span id="6910" class="lb lc iq kw b gy ld le l lf lg">[&lt;tf.Variable ‘dense/kernel:0’ shape=(2, 2) dtype=float32, numpy= array([[-3.2753847, 3.2302036], [ 3.3264563, -3.2554653]], dtype=float32)&gt;, &lt;tf.Variable ‘dense/bias:0’ shape=(2,) dtype=float32, numpy=array([1.5562934, 1.5492057], dtype=float32)&gt;, &lt;tf.Variable ‘dense_1/kernel:0’ shape=(2, 1) dtype=float32, numpy= array([[2.625529], [2.670275]], dtype=float32)&gt;, &lt;tf.Variable ‘dense_1/bias:0’ shape=(1,) dtype=float32, numpy=array([-2.0918093], dtype=float32)&gt;]</span></pre><p id="33f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此列表中的前两个元素是隐藏层的权重和偏差，最后两个元素是输出层的权重和偏差。所有这些元素都是张量，您可以使用 numpy()方法获得 numpy 数组形式的值。例如，下面将以 numpy 数组的形式给出隐藏层的权重，</p><pre class="km kn ko kp gt kx kw ky kz aw la bi"><span id="7019" class="lb lc iq kw b gy ld le l lf lg">model.weights[0].numpy()</span><span id="6f17" class="lb lc iq kw b gy lh le l lf lg">array([[-3.2753847,  3.2302036],        [ 3.3264563, -3.2554653]], dtype=float32)</span></pre><p id="7b7f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一行的系数是-3.27 和 3.32；第二行的系数是 3.23 和-3.25。你可以通过运行<code class="fe kt ku kv kw b">model.weights[1].numpy()</code>看到他们的偏见</p><p id="37ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们想象由学习到的隐藏层的权重和偏差定义的线，</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/358fe1af35c86955a718a843c217fbd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*12KakjSKmmEm2nqmku6FYQ.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">The data and the lines defined by the hidden layer after training in (x₁,x₂)-space</figcaption></figure><p id="ab31" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如您现在所看到的，这些线以一种将类包含在独立区域中的方式划分了空间。这些线的系数告诉我们每条线的正面。对于蓝线，正方向是上侧，对于橙线，正方向是下侧。所以蓝点在两条线的正侧，红点在一条线的负侧，另一条线的正侧。</p><p id="ee98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们应用非线性激活函数<code class="fe kt ku kv kw b">tanh</code>并在新的特征空间(a₂).a₁)中可视化数据隐藏层的激活计算如下，</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/d62bbe483c7fb296be0386588c5ee643.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*Kihf42f9t0HT3dJmlRZNtQ.png"/></div></figure><p id="0f2f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每个数据点，<code class="fe kt ku kv kw b">tanh</code>的参数由该数据点相对于上述线的位置决定。我们可以认为 a₁和 a₂是新的特征，a₁,a₂)-space 是新的特征空间。输出层的权重和偏差在这个新的特征空间中定义了一条线，由以下等式给出</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/41a5ea0238af07618c77d6b92f68d843.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*6ed4_7zRsvhceIWf2bV4gA.png"/></div></figure><p id="8144" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这条线和这个新特征空间中的数据一起绘制如下，</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/7871f1a064cbcb7865af7e21482fbe6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*kW5qQJeonRdjhEPWTjlsTQ.png"/></div><figcaption class="lk ll gj gh gi lm ln bd b be z dk">The features learned by the model in (a₁, a₂)-space and the line defined by the output layer.</figcaption></figure><p id="c92f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，在这个新的特征空间中，我们的数据变得可线性分离，由输出图层定义的线将两个类分开。蓝点的 a₁和 a₂坐标都是正的，因为(x₁，x₂)空间中的这些点位于由隐藏层参数定义的两条线的正侧，并且在应用<code class="fe kt ku kv kw b">tanh</code>后，两个坐标都是正的。对于红点，a₁和 a₂中的一个是正的，因为在 x₂)-space 的 x₁中，红点仅在由隐藏层参数定义的一条线的正侧，并且根据这条线，它们在新特征空间中只有一个坐标是正的，而另一个是负的。这解释了上图(a₁,a₂)-space)中的数据图。</p><p id="ddc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论:</strong>神经网络学习数据的新表示，这使得相对于该新表示进行分类变得容易。</p><p id="c39d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里有一个<a class="ae lq" href="https://gist.github.com/CihanSoylu/6967249574192728a9fba367065e8949" rel="noopener ugc nofollow" target="_blank">链接</a>指向我在这篇文章中使用的 google colab 笔记本。</p><p id="924c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读。</p></div></div>    
</body>
</html>