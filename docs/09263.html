<html>
<head>
<title>Multi Class Text Classification with LSTM using TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 TensorFlow 2.0 的 LSTM 多类文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35?source=collection_archive---------0-----------------------#2019-12-08">https://towardsdatascience.com/multi-class-text-classification-with-lstm-using-tensorflow-2-0-d88627c10a35?source=collection_archive---------0-----------------------#2019-12-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7fb9cdfde1d58f6ab771acbe48339d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ZEtdMpfuB37n0EtzaUgnw.png"/></div></div></figure><div class=""/><div class=""><h2 id="6ac0" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">递归神经网络，长短期记忆</h2></div><p id="c0db" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">NLP 的许多创新是如何将上下文添加到单词向量中。一种常见的方法是使用<a class="ae lm" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">循环神经网络</a>。以下是<a class="ae lm" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>的概念:</p><ul class=""><li id="0497" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">他们利用连续的信息。</li><li id="fe4e" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">他们有一种记忆，可以捕捉到目前为止已经计算过的内容，也就是说，我上一次讲的内容会影响我接下来要讲的内容。</li><li id="4b97" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">rnn 是文本和语音分析的理想选择。</li><li id="1b2e" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">最常用的 rnn 是 LSTMs。</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mb"><img src="../Images/ed9deb2b21e434815deafa2a632e4307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UiZX9ZKImH9B3gCp9VVRmw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Source: <a class="ae lm" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><p id="1e69" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">以上是<a class="ae lm" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a>的架构。</p><ul class=""><li id="654b" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">“A”是一层<a class="ae lm" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>。</li><li id="f0e0" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">如果我们只看右边，它确实循环地通过每个序列的元素。</li><li id="a3b1" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">如果我们打开左边，它看起来会和右边一模一样。</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/8c475cfea37a4eb7144bdaba2d19bfaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*zMgvcjtb5oW6UwMO2uO7Gw.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Source: <a class="ae lm" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs</a></figcaption></figure><p id="2381" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">假设我们正在解决一个新闻文章数据集的文档分类问题。</p><ul class=""><li id="e075" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">我们输入每个单词，单词在某些方面相互关联。</li><li id="4f2e" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">当我们看到文章中的所有单词时，我们在文章的结尾进行预测。</li><li id="9efa" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">rnn 通过从最后一个输出传递输入，能够保留信息，并能够在最后利用所有信息进行预测。</li></ul><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ml"><img src="../Images/44f61989cea99585b8520c26e26a55ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbnLhYD_9GL23UE_XNQwGw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk"><a class="ae lm" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs</a></figcaption></figure><ul class=""><li id="1290" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">这对短句很有效，当我们处理一篇长文章时，会有一个长期依赖的问题。</li></ul><p id="1bea" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">因此，我们一般不使用普通的 RNNs，而是使用<a class="ae lm" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">长短期记忆</a>。LSTM 是一种可以解决这种长期依赖问题的 RNNs。</p><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mm"><img src="../Images/c0f32ef819dace2a00829c1127d9dacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8687P-BnBvNWt9QPjgdrYw.png"/></div></div></figure><p id="0a41" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在我们的新闻文章文档分类示例中，我们有这种多对一的关系。输入是单词序列，输出是一个单独的类或标签。</p><p id="52a0" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在，我们将使用<a class="ae lm" href="https://blog.tensorflow.org/2019/09/tensorflow-20-is-now-available.html" rel="noopener ugc nofollow" target="_blank">tensor flow 2.0</a>&amp;<a class="ae lm" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank">Keras</a>解决 LSTM 的 BBC 新闻文档分类问题。数据集可以在这里找到<a class="ae lm" href="https://raw.githubusercontent.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/master/bbc-text.csv" rel="noopener ugc nofollow" target="_blank">。</a></p><ul class=""><li id="ae7e" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">首先，我们导入库并确保 TensorFlow 是正确的版本。</li></ul><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/85dc7ae68ff4dcd290aa854b88c60f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:212/format:webp/1*hUXS5eJZ3X-4SXCwoL5iqw.png"/></div></figure><ul class=""><li id="4b0e" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">像这样把超参数放在顶部，这样更容易修改和编辑。</li><li id="715d" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">当我们到达那里时，我们将解释每个超参数是如何工作的。</li></ul><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">hyperparameter.py</figcaption></figure><ul class=""><li id="0e55" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">定义两个包含文章和标签的列表。同时，我们删除停用词。</li></ul><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">articles_labels.py</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/7dd38fb658862700d1e4d166062853cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/format:webp/1*p_OAeSRvO0l2KFRoXXf7Fw.png"/></div></figure><p id="730d" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">数据中有 2，225 篇新闻文章，我们把它们分成训练集和验证集，根据我们之前设置的参数，80%用于训练，20%用于验证。</p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">train_valid.py</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/8b327f7f7f0a4c7c655bba8ba32dbdce.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*3YggImk2p0_1TJlhuoqS7Q.png"/></div></figure><p id="79b4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">Tokenizer 为我们做了所有繁重的工作。在我们的文章中，它是令牌化的，它将需要 5000 个最常见的单词。<code class="fe ms mt mu mv b">oov_token</code>就是遇到看不见的词的时候放一个特殊值进去。这意味着我们希望<code class="fe ms mt mu mv b">&lt;OOV&gt;</code>用于不在<code class="fe ms mt mu mv b">word_index</code>中的单词。<code class="fe ms mt mu mv b">fit_on_text</code>将遍历所有文本并创建如下词典:</p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">tokenize.py</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/98201b01364ca082f2c4ccb842fa5434.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*jC_lw0x9RbKcM3FGbyqDNg.png"/></div></figure><p id="6834" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可以看到“<oov>”是我们语料库中最常见的标记，其次是“said”，再其次是“mr”等等。</oov></p><p id="d5be" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在标记化之后，下一步是将这些标记转换成序列列表。下面是训练数据中已经变成序列的第 11 条。</p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="b1e5" class="nb nc jb mv b gy nd ne l nf ng">train_sequences = tokenizer.texts_to_sequences(train_articles)<br/>print(train_sequences[10])</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/d6d8398b1a158cb8cf7834745ad2c5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbaZfyhvdPNczwZUu_fRrg.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 1</figcaption></figure><p id="6456" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">当我们为 NLP 训练神经网络时，我们需要序列大小相同，这就是为什么我们使用填充。如果你向上看，我们的<code class="fe ms mt mu mv b">max_length</code>是 200，所以我们使用<code class="fe ms mt mu mv b">pad_sequences</code>使我们所有的文章长度相同，都是 200。结果，你会看到第一篇文章的长度是 426，变成了 200，第二篇文章的长度是 192，变成了 200，依此类推。</p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="5c14" class="nb nc jb mv b gy nd ne l nf ng">train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)</span><span id="0372" class="nb nc jb mv b gy ni ne l nf ng">print(len(train_sequences[0]))<br/>print(len(train_padded[0]))<br/><br/>print(len(train_sequences[1]))<br/>print(len(train_padded[1]))<br/><br/>print(len(train_sequences[10]))<br/>print(len(train_padded[10]))</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/cc559e85635fcc995fe9db418edd2615.png" data-original-src="https://miro.medium.com/v2/resize:fit:170/format:webp/1*orKxbvw3jWJTR9HYpsJHew.png"/></div></figure><p id="8d81" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">另外，还有<code class="fe ms mt mu mv b">padding_type</code>和<code class="fe ms mt mu mv b">truncating_type</code>，都是<code class="fe ms mt mu mv b">post</code>，意思是比如第 11 条，长度是 186，我们填充到 200，我们填充到最后，也就是加了 14 个零。</p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="82f0" class="nb nc jb mv b gy nd ne l nf ng">print(train_padded[10])</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/fe5430c41995e8c79f357be5bf5dc752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cykLg9TVrkyGhiSWUoU8_g.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 2</figcaption></figure><p id="7f99" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">对于第一篇文章，它的长度是 426，我们将其截断为 200，并且我们在结尾也进行了截断。</p><p id="9230" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">然后我们对验证序列做同样的事情。</p><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">val_tok.py</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a1d27150cd38492da02ae6004c58c088.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*pjnHNurRrBf_KeMOEuyUog.png"/></div></figure><p id="b661" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在我们来看看标签。因为我们的标签是文本，所以我们将对它们进行标记，在训练时，标签应该是 numpy 数组。因此，我们将把标签列表转换成 numpy 数组，如下所示:</p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="d425" class="nb nc jb mv b gy nd ne l nf ng">label_tokenizer = Tokenizer()<br/>label_tokenizer.fit_on_texts(labels)<br/><br/>training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))<br/>validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))</span><span id="144e" class="nb nc jb mv b gy ni ne l nf ng">print(training_label_seq[0])<br/>print(training_label_seq[1])<br/>print(training_label_seq[2])<br/>print(training_label_seq.shape)<br/><br/>print(validation_label_seq[0])<br/>print(validation_label_seq[1])<br/>print(validation_label_seq[2])<br/>print(validation_label_seq.shape)</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/b475439ceb71592ab3ce31a2a967feed.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*VtqE4PQUwPTih2fRsWuKCQ.png"/></div></figure><p id="3a77" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在训练深度神经网络之前，我们要探索自己的原始文章和填充后的文章是什么样子的。运行下面的代码，我们浏览第 11 篇文章，我们可以看到一些单词变成了“<oov>”，因为它们没有进入前 5000 名。</oov></p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="8510" class="nb nc jb mv b gy nd ne l nf ng">reverse_word_index = dict([(value, key) <strong class="mv jc">for</strong> (key, value) <strong class="mv jc">in</strong> word_index.items()])<br/><br/><strong class="mv jc">def</strong> decode_article(text):<br/>    <strong class="mv jc">return</strong> ' '.join([reverse_word_index.get(i, '?') <strong class="mv jc">for</strong> i <strong class="mv jc">in</strong> text])<br/>print(decode_article(train_padded[10]))<br/>print('---')<br/>print(train_articles[10])</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nn"><img src="../Images/7ad770fcaa2f45471f83989cac686471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EUa4EbZwfFzDdH0p1aiJMw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 3</figcaption></figure><p id="22e4" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">现在是实施 LSTM 的时候了。</p><ul class=""><li id="588b" class="ln lo jb ks b kt ku kw kx kz lp ld lq lh lr ll ls lt lu lv bi translated">我们构建一个<code class="fe ms mt mu mv b">tf.keras.Sequential</code>模型，从嵌入层开始。嵌入层为每个单词存储一个向量。当被调用时，它将单词索引序列转换成向量序列。经过训练，意思相近的单词往往有相似的向量。</li><li id="7d53" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">双向包装器与 LSTM 层一起使用，它通过 LSTM 层向前和向后传播输入，然后连接输出。这有助于 LSTM 了解长期依赖。然后，我们将它与密集的神经网络相匹配，以进行分类。</li><li id="c140" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">我们用<code class="fe ms mt mu mv b">relu</code>代替<code class="fe ms mt mu mv b">tahn</code>函数，因为它们是很好的替代物。</li><li id="2cc7" class="ln lo jb ks b kt lw kw lx kz ly ld lz lh ma ll ls lt lu lv bi translated">我们添加一个有 6 个单位的密集层和<code class="fe ms mt mu mv b">softmax</code>激活。当我们有多个输出时，<code class="fe ms mt mu mv b">softmax</code>将输出层转换成一个概率分布。</li></ul><figure class="mc md me mf gt is"><div class="bz fp l di"><div class="mn mo l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">lstm_model.py</figcaption></figure><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/81bfb60a51aab76f434515a3d73493ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOGSuCjY2-pOrg-yxTcz_w.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 4</figcaption></figure><p id="f70b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在我们的模型摘要中，我们有我们的嵌入，我们的双向包含 LSTM，后面是两个密集层。双向的输出是 128，因为它是我们在 LSTM 的两倍。我们也可以堆叠 LSTM 层，但我发现结果更糟。</p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="63f2" class="nb nc jb mv b gy nd ne l nf ng">print(set(labels))</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/a89ef56496de6666e689d71c9dd70196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sCw-aQY7g3JyToDjvPKlbA.png"/></div></div></figure><p id="1233" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们总共有 5 个标签，但是因为我们没有一次性编码标签，我们必须使用<code class="fe ms mt mu mv b">sparse_categorical_crossentropy</code>作为损失函数，它似乎认为 0 也是一个可能的标签，而 tokenizer 对象从整数 1 开始标记，而不是整数 0。结果，最后的密集层需要标签 0、1、2、3、4、5 的输出，尽管 0 从未被使用过。</p><p id="d646" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">如果希望最后一个密集层为 5，则需要从训练和验证标签中减去 1。我决定让它保持原样。</p><p id="277c" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我决定训练 10 个纪元，你会看到很多纪元。</p><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="1cda" class="nb nc jb mv b gy nd ne l nf ng">model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="7727" class="nb nc jb mv b gy ni ne l nf ng">num_epochs = 10<br/>history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nq"><img src="../Images/305f920c35f71c1796d42c51fc73a431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUye-BDNoVXlQKE4-TuM5A.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 5</figcaption></figure><pre class="mc md me mf gt mx mv my mz aw na bi"><span id="9e5e" class="nb nc jb mv b gy nd ne l nf ng">def plot_graphs(history, string):<br/>  plt.plot(history.history[string])<br/>  plt.plot(history.history['val_'+string])<br/>  plt.xlabel("Epochs")<br/>  plt.ylabel(string)<br/>  plt.legend([string, 'val_'+string])<br/>  plt.show()<br/>  <br/>plot_graphs(history, "accuracy")<br/>plot_graphs(history, "loss")</span></pre><figure class="mc md me mf gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/0504d966b972e630e8b8e0b9e83755af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*ATjn95TdxbZAIk1avTKc4Q.png"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Figure 6</figcaption></figure><p id="c9c9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">我们可能只需要 3 或 4 个纪元。在训练结束时，我们可以看到有一点点过度适应。</p><p id="e054" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">在以后的文章中，我们将致力于改进这个模型。</p><p id="1ae9" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated"><a class="ae lm" href="https://github.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/blob/master/BBC%20News_LSTM.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter 笔记本</a>可以在<a class="ae lm" href="https://github.com/susanli2016/PyCon-Canada-2019-NLP-Tutorial/blob/master/BBC%20News_LSTM.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。享受余下的周末吧！</p><p id="b37b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">参考资料:</p><div class="ip iq gp gr ir ns"><a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd jc gy z fp nx fr fs ny fu fw ja bi translated">Coursera |顶尖大学的在线课程。免费加入</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">斯坦福和耶鲁等学校的 1000 多门课程——无需申请。培养数据科学方面的职业技能…</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">www.coursera.org</p></div></div><div class="ob l"><div class="oc l od oe of ob og ix ns"/></div></div></a></div><div class="ip iq gp gr ir ns"><a href="https://learning.oreilly.com/videos/oreilly-strata-data/9781492050681/9781492050681-video327451" rel="noopener  ugc nofollow" target="_blank"><div class="nt ab fo"><div class="nu ab nv cl cj nw"><h2 class="bd jc gy z fp nx fr fs ny fu fw ja bi translated">纽约州纽约市 2019 年奥莱利地层数据会议</h2><div class="nz l"><h3 class="bd b gy z fp nx fr fs ny fu fw dk translated">选自 2019 年纽约奥莱利地层数据会议[视频]</h3></div><div class="oa l"><p class="bd b dl z fp nx fr fs ny fu fw dk translated">learning.oreilly.co</p></div></div><div class="ob l"><div class="oh l od oe of ob og ix ns"/></div></div></a></div></div></div>    
</body>
</html>