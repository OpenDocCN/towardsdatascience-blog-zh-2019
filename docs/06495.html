<html>
<head>
<title>Why is Boosting Fitting Residual</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么升压拟合有残差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-is-boosting-fitting-residual-9ac6f4e77550?source=collection_archive---------25-----------------------#2019-09-17">https://towardsdatascience.com/why-is-boosting-fitting-residual-9ac6f4e77550?source=collection_archive---------25-----------------------#2019-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="05c6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Boosting 算法的一般解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/44aa9561c37058817546e4c8f8ef8a74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFiS2p8fKJhVN-CnhTHLZw.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Gardens By The Bay Singapore</figcaption></figure></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><blockquote class="lc ld le"><p id="429a" class="lf lg lh li b lj lk jr ll lm ln ju lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">Boosting 算法在每次迭代中搜索最优的弱学习器函数，该函数在训练数据上<strong class="li ir">最小化损失函数。每个新的学习者都会试图“纠正”以前的学习者所犯的错误。</strong></p></blockquote></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><h1 id="719b" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">动机</h1><p id="4277" class="pw-post-body-paragraph lf lg iq li b lj mu jr ll lm mv ju lo mw mx lr ls my mz lv lw na nb lz ma mb ij bi translated">不用多介绍，大部分数据科学家应该都有使用各种 boosting 模型的经验，比如 XGBoost，用于项目或者 Kaggle 竞赛。许多文章将 boosting 算法描述为“递归地添加弱学习器以拟合先前学习器产生的残差”。嗯，是真的，但是为什么呢？本文旨在从更广泛的意义上解释助推的概念。</p></div><div class="ab cl kv kw hu kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="ij ik il im in"><h1 id="b8d3" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">增压解释</h1><p id="2f29" class="pw-post-body-paragraph lf lg iq li b lj mu jr ll lm mv ju lo mw mx lr ls my mz lv lw na nb lz ma mb ij bi translated"><strong class="li ir"> Boosting </strong>或<strong class="li ir">Forward Stagewise Additive Modeling</strong>是一种集成学习方法，它将许多弱学习者组合起来，形成一个行为类似于强学习者的集合。它一般可以描述如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/47508840fb26641e4bcf36891d338431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qlrsUWENgOB5m49JPj11pw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Boosting algorithm (Source: <a class="ae nd" href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf" rel="noopener ugc nofollow" target="_blank">ESLII</a>)</figcaption></figure><p id="dd5a" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">在详细解释每个步骤之前，让我们澄清一下这些符号。</p><ul class=""><li id="9aae" class="ne nf iq li b lj lk lm ln mw ng my nh na ni mb nj nk nl nm bi translated"><em class="lh"> x，y </em>:训练数据和实际值</li><li id="8fc3" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated"><em class="lh"> f </em> ᵢ <em class="lh"> (x) </em>:在<em class="lh"> i </em>迭代中收集弱学习者</li><li id="f7e4" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated"><em class="lh"> M </em>:要添加的树数</li><li id="16ac" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated"><em class="lh"> N </em>:训练数据点数</li><li id="2204" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated"><em class="lh"> β </em>:扩展系数或集合中后续弱学习者的“权重”</li><li id="b13e" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated"><em class="lh">b(x；γ) </em>:由一组参数<em class="lh"> γ </em>表征的弱学习函数</li><li id="932f" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated"><em class="lh"> L(y，ŷ) </em>:损失函数</li></ul><p id="f633" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">现在让我们来看看每一步。</p><ol class=""><li id="ac74" class="ne nf iq li b lj lk lm ln mw ng my nh na ni mb ns nk nl nm bi translated">初始化基本集合以预测所有定型数据的 0</li><li id="298c" class="ne nf iq li b lj nn lm no mw np my nq na nr mb ns nk nl nm bi translated">对于添加到集合中的每个后续弱学习者:</li></ol><ul class=""><li id="35da" class="ne nf iq li b lj lk lm ln mw ng my nh na ni mb nj nk nl nm bi translated">a)在训练数据上找到最小化损失函数的最佳展开系数[<em class="lh">β</em>和参数组[<em class="lh">γ</em>。注意，损失函数接受实际值和来自先前集合[<em class="lh">fₘ₋₁(x]</em>和当前弱学习器函数[<em class="lh">b(x；γ) </em> ]</li><li id="6cff" class="ne nf iq li b lj nn lm no mw np my nq na nr mb nj nk nl nm bi translated">b)找到最优的<em class="lh"> β </em>和<em class="lh"> γ </em>后，将学习能力弱的系数添加到集合中</li></ul><p id="94b6" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">最重要的步骤是 2a，它搜索最佳弱学习器函数以添加到集合中。当选择平方误差作为损失函数时，2a 中的公式可改写为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/6803afa2618bd5097644981ac4f561a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZBQg8BCW76Yf4n0r7i0Aw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Square Error Loss Function (Source: <a class="ae nd" href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf" rel="noopener ugc nofollow" target="_blank">ESLII</a>)</figcaption></figure><p id="727f" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">在这个表达式中，<em class="lh"> fₘ₋₁ </em>代表来自先前树的预测值。<em class="lh"> yᵢ </em>代表实际值。<em class="lh"> yᵢ - fₘ₋₁ </em>将输出前一个学习器的残差，<em class="lh"> rᵢₘ </em>。因此，为了最小化平方损失，每个新的弱学习器将拟合残差。</p><p id="107d" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">对于回归问题的其他常见损失函数，例如绝对误差和 Huber 损失，这种说法仍然成立，因为残差总是被计算的。</p><p id="c80d" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">那么，分类问题呢？</p><p id="5f72" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">对于 2 类分类问题，AdaBoost 是由 Freund 和<a class="ae nd" href="https://en.wikipedia.org/wiki/Robert_Schapire" rel="noopener ugc nofollow" target="_blank"> Schapire </a>在 1997 年推出的一种著名的 boosting 算法，它使用指数损失。在每次迭代中，训练样本将根据先前树的预测误差进行重新加权。错误分类的样本将被赋予更高的权重。后续树将适合重新加权的训练样本。尽管新树不直接与预测误差或偏差相匹配，但预测误差在匹配新树时起着重要作用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/6921f6aed272940413ca999bdae2fa2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*9ivV7_saGnC_i4gAAQBAJw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Exponential Loss (Source: <a class="ae nd" href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf" rel="noopener ugc nofollow" target="_blank">ESLII</a>)</figcaption></figure><p id="ea77" class="pw-post-body-paragraph lf lg iq li b lj lk jr ll lm ln ju lo mw lq lr ls my lu lv lw na ly lz ma mb ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>