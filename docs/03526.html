<html>
<head>
<title>Depth Estimation on Camera Images using DenseNets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 DenseNets 的相机图像深度估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/depth-estimation-on-camera-images-using-densenets-ac454caa893?source=collection_archive---------10-----------------------#2019-06-05">https://towardsdatascience.com/depth-estimation-on-camera-images-using-densenets-ac454caa893?source=collection_archive---------10-----------------------#2019-06-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5318" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用数据做很酷的事情！</h2></div><h1 id="13f0" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">介绍</h1><p id="9075" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当观看二维场景时，人脑具有推断深度的非凡能力，即使是单点测量，就像观看照片一样。然而，从单幅图像进行精确的深度映射仍然是计算机视觉中的一个挑战。来自场景的深度信息对于增强现实、机器人、自动驾驶汽车等许多任务都很有价值。在这篇博客中，我们探索如何在<a class="ae lw" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank"> NYU 深度数据集</a>上训练深度估计模型。该模型在该数据集上获得了最先进的结果。我们还添加了代码，以在用户收集的图像+视频上测试该模型。</p><p id="2679" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">经过训练的模型在来自野外的数据上表现非常好，如下面的视频所示。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/742cabc0b5737225ddc60047fee2af07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*NT0HiofHrDQUdz1TAxqBNQ.gif"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Actual Image on left and predicted depth map on the right</figcaption></figure><p id="9e50" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">完整代码在我的<a class="ae lw" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/depth_estimation" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上开源。</p><p id="0909" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">深度信息是什么样的？深度可以存储为图像帧中每个像素到相机的距离，单位为米。下图显示了单一 RGB 图像的深度图。深度图在右边，实际深度已经用这个房间的最大深度转换成了相对深度。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ms"><img src="../Images/9f25a7918f2280100f2e93344bce0262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pf9nYoeoG6uXunSBM4xOkA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">RGB Image and its corresponding depth map</figcaption></figure><h1 id="53ee" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">数据集</h1><p id="581e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了建立深度估计模型，我们需要 RGB 图像和相应的深度信息。深度信息可以通过像<a class="ae lw" href="https://www.jameco.com/jameco/workshop/howitworks/xboxkinect.html" rel="noopener ugc nofollow" target="_blank"> Kinect </a>这样的低成本传感器来收集。对于这个练习，我使用了流行的<a class="ae lw" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank"> NYU v2 深度数据集</a>来建立一个模型。该数据集由超过 400，000 幅图像及其相应的深度图组成。我在训练任务中使用了全部数据集中的 50，000 幅图像的子集。</p><h1 id="5656" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">模型概述</h1><p id="0d8e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我阅读了几篇执行深度估计任务的论文，其中许多论文使用了编码器解码器类型的神经网络。对于深度估计任务，模型的输入是 RGB 图像，输出是深度图像，该深度图像或者是与输入图像相同的维度，或者有时是具有相同纵横比的输入图像的缩小版本。用于该任务的标准损失函数考虑了实际深度图和预测深度图之间的差异。这可能是 L1 或 L2 的损失。</p><p id="ff75" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我决定使用来自 Alhashim 和 Wonka 的<a class="ae lw" href="https://arxiv.org/abs/1812.11941" rel="noopener ugc nofollow" target="_blank">密集深度模型。这个模型的简单性和准确性令人印象深刻。很容易理解，训练起来也比较快。它使用图像增强和自定义损失函数来获得比更复杂的架构更好的结果。该型号使用功能强大的</a><a class="ae lw" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>型号，带有预先调整的砝码，为编码器提供动力。</p><p id="f0d9" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">密集深度模型</strong></p><p id="8cc3" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">本文介绍的编码器是一个预训练的 DenseNet 169。<br/>编码器由 4 个密集块组成，出现在 DenseNet 169 模型中完全连接的层之前。它不同于其他深度模型，因为它使用非常简单的解码器。每个解码器模块由单个 bi- <br/>线性上采样层和两个卷积层组成。遵循编码器解码器架构中的另一标准实践，上采样层与编码器中的相应层连接。下图更详细地解释了该架构。关于层的更多细节，请阅读<a class="ae lw" href="https://arxiv.org/abs/1812.11941" rel="noopener ugc nofollow" target="_blank">原文</a>。写得非常好！</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mt"><img src="../Images/906d8117c0de6a2f10961dda702cbc5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pbbg3JqEzIEQrs4vJWTifw.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Encoder Decoder model from Dense Depth Paper</figcaption></figure><h1 id="c498" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">训练和测试深度估计模型</h1><p id="a1ea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在 NYU-v2 数据集的 50K 样本上训练密集深度。输入是 640×480 分辨率 RGB 图像，输出是 320×240 分辨率的深度图。使用 Adam optimizer 在 Keras 中训练该模型。我利用密集深度作者的回购开始。</p><p id="f7a3" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">训练了三种不同的模型架构:</p><ol class=""><li id="4b56" class="mu mv it lc b ld lx lg ly lj mw ln mx lr my lv mz na nb nc bi translated">原代码提供了 DensetNet 169 en- <br/>编码器的实现。这个模型被训练了 8 个纪元(9 个小时在<br/> NVIDIA 1080 Ti 上)</li><li id="c58d" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">原始代码被修改以实现 DenseNet 121 编码器，其具有比 DenseNet 169 更少的参数。该模型被训练了 6 个时期(在 GPU 上 5 个小时),因为验证损失在此<br/>点已经稳定</li><li id="a2d1" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated">修改了原始代码以实现 Resnet <br/> 50 编码器，该编码器具有比 DenseNet 169 更多的参数。<br/>我试验了连接编码器和解码器的不同方式。这个模型被训练了<br/>5 个时期(在 GPU 上 8 个小时),并且训练是不连续的，因为模型已经开始过度拟合。</li></ol><p id="c529" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">所有这些代码修改都被推送到<a class="ae lw" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/depth_estimation" rel="noopener ugc nofollow" target="_blank"> github repo </a>上，并附有如何进行培训、评估和测试的解释。</p><h1 id="09b7" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">评估不同的模型</h1><p id="49c6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们使用三种不同的指标来比较模型性能——预测深度和实际深度的平均相对误差(rel)、RMSE (rms) —实际深度和预测深度的均方根误差以及两个深度之间的平均对数误差(log)。所有这些指标的值越低，表示模型越强。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/f12261792a2e038be3f84a8faa38420d.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*Cv09Hl7oeTblqwo8rnhXcQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Model Comparison</figcaption></figure><p id="dd7b" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">如上表所示，DenseNet 169 型号的性能优于 DenseNet121 和 ResNet 50。此外，我训练的 DenseNet 169 在性能上与原作者(Alhashim 和 Wonka)的非常接近。</p><h1 id="2b60" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">结论</h1><p id="3652" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们希望这篇博客能成为理解深度估计工作原理的一个好的起点。我们提供了一个管道来使用一个强大的、简单的和易于训练的深度估计模型。我们还分享了代码，可用于从您收集的室内图像或视频中获取深度图像。希望您自己尝试一下这些代码。</p><p id="0614" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">我有自己的深度学习咨询公司，喜欢研究有趣的问题。我已经帮助许多初创公司部署了基于人工智能的创新解决方案。请到 http://deeplearninganalytics.org/来看看我们吧。如果你有一个我们可以合作的项目，那么请通过我的网站或在 info@deeplearninganalytics.org<strong class="lc iu">联系我</strong></p><p id="9712" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated">你也可以在 https://medium.com/@priya.dwivedi<a class="ae lw" href="https://medium.com/@priya.dwivedi" rel="noopener">的</a>看到我的其他作品</p><p id="3185" class="pw-post-body-paragraph la lb it lc b ld lx ju lf lg ly jx li lj lz ll lm ln ma lp lq lr mb lt lu lv im bi translated"><strong class="lc iu">参考文献:</strong></p><ul class=""><li id="7e76" class="mu mv it lc b ld lx lg ly lj mw ln mx lr my lv nj na nb nc bi translated"><a class="ae lw" href="https://github.com/ialhashim/DenseDepth" rel="noopener ugc nofollow" target="_blank">密集深度原始 Github </a></li><li id="33c2" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv nj na nb nc bi translated"><a class="ae lw" href="https://arxiv.org/abs/1812.11941" rel="noopener ugc nofollow" target="_blank">密集深度纸</a></li><li id="c5c5" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv nj na nb nc bi translated"><a class="ae lw" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html" rel="noopener ugc nofollow" target="_blank"> NYU V2 数据集</a></li></ul></div></div>    
</body>
</html>