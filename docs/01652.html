<html>
<head>
<title>Simple Reinforcement Learning: Q-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单强化学习:Q-学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56?source=collection_archive---------1-----------------------#2019-03-18">https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56?source=collection_archive---------1-----------------------#2019-03-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/43bd5d5e4a6edd0945a443e064a27b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OjizsKNU6_LyM3QW8is0Sg.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Typical Exploring Image for RL - Credit <a class="ae kf" href="https://www.instagram.com/mike.shots/" rel="noopener ugc nofollow" target="_blank">@mike.shots</a></figcaption></figure><h1 id="7e6f" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">简介</strong></h1><p id="2fe7" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我在参加强化学习课程时学到的最喜欢的算法之一是 q-learning。可能是因为它对我来说最容易理解和编码，但也因为它似乎有意义。在这篇快速的帖子中，我将讨论 q-learning，并提供理解该算法的基本背景。</p><h1 id="127f" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">什么是 q 学习？</strong></h1><p id="1ce0" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Q-learning 是一种非策略强化学习算法，它寻求在给定的当前状态下采取最佳行动。它被认为是不符合策略的，因为 q-learning 函数从当前策略之外的动作中学习，比如采取随机动作，因此不需要策略。更具体地说，q-learning 寻求学习一种使总回报最大化的策略。</p><h1 id="e050" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">什么是‘Q’？</h1><p id="cc81" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">q-learning 中的“q”代表质量。在这种情况下，质量代表了一个给定的行为在获得未来回报方面的有用程度。</p><h1 id="7464" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">创建一个 q 表</strong></h1><p id="2652" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当进行 q-learning 时，我们创建一个所谓的<em class="mc"> q-table </em>或遵循<code class="fe md me mf mg b">[state, action]</code>形状的矩阵，并将我们的值初始化为零。然后，在一集之后，我们更新并存储我们的<em class="mc"> q 值</em>。这个 q 表成为我们的代理根据 q 值选择最佳行动的参考表。</p><pre class="mh mi mj mk gt ml mg mm mn aw mo bi"><span id="2ffa" class="mp kh it mg b gy mq mr l ms mt">import numpy as np</span><span id="6b60" class="mp kh it mg b gy mu mr l ms mt"># Initialize q-table values to 0</span><span id="d086" class="mp kh it mg b gy mu mr l ms mt">Q = np.zeros((state_size, action_size))</span></pre><h1 id="236b" class="kg kh it bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated"><strong class="ak">Q-学习和更新</strong></h1><p id="aa0d" class="pw-post-body-paragraph le lf it lg b lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下一步只是让代理与环境交互，并更新我们的 q 表<code class="fe md me mf mg b">Q[state, action]</code>中的状态动作对。</p><p id="0bf9" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><em class="mc">采取行动:探索或利用</em></p><p id="b46f" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">代理以两种方式之一与环境交互。第一种是使用 q 表作为参考，并查看给定状态的所有可能动作。然后，代理根据这些操作的最大值选择操作。这就是所谓的<strong class="lg iu"> <em class="mc">利用</em> </strong>，因为我们利用现有的信息做出决定。</p><p id="2f0d" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">第二种采取行动的方式是随机行动。这叫做<strong class="lg iu"> <em class="mc">探索</em> </strong>。我们不是根据最大未来回报来选择行动，而是随机选择行动。随机行动很重要，因为它允许代理探索和发现新的状态，否则在利用过程中可能不会被选择。您可以使用 epsilon ( <em class="mc"> ε </em>)来平衡探索/利用，并设置您希望探索和利用的频率值。这里有一些粗略的代码，将取决于如何设置状态和动作空间。</p><pre class="mh mi mj mk gt ml mg mm mn aw mo bi"><span id="e4a9" class="mp kh it mg b gy mq mr l ms mt">import random</span><span id="26ca" class="mp kh it mg b gy mu mr l ms mt"># Set the percent you want to explore<br/>epsilon = 0.2</span><span id="37ce" class="mp kh it mg b gy mu mr l ms mt">if random.uniform(0, 1) &lt; epsilon:<br/>    """<br/>    Explore: select a random action</span><span id="247c" class="mp kh it mg b gy mu mr l ms mt">    """<br/>else:<br/>    """<br/>    Exploit: select the action with max value (future reward)</span><span id="37da" class="mp kh it mg b gy mu mr l ms mt">    """</span></pre><p id="ace4" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><em class="mc">更新 q 表</em></p><p id="1389" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">更新发生在每个步骤或动作之后，并在一集结束时结束。在这种情况下，完成意味着代理到达某个端点。例如，终端状态可以是任何类似于登录到结帐页面、到达某个游戏的结尾、完成某个期望的目标等的状态。在单个情节之后，代理将不会学到太多，但是最终通过足够的探索(步骤和情节)，它将收敛并学习最优 q 值或 q 星(<code class="fe md me mf mg b">Q∗</code>)。</p><p id="33fe" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">以下是 3 个基本步骤:</p><ol class=""><li id="79f0" class="na nb it lg b lh mv ll mw lp nc lt nd lx ne mb nf ng nh ni bi translated">代理从一个状态(s1)开始，采取一个动作(a1)并收到一个奖励(r1)</li><li id="a229" class="na nb it lg b lh nj ll nk lp nl lt nm lx nn mb nf ng nh ni bi translated">代理通过随机(epsilon，ε)引用具有最高值(max) <strong class="lg iu">或</strong>的 Q 表来选择动作</li><li id="e765" class="na nb it lg b lh nj ll nk lp nl lt nm lx nn mb nf ng nh ni bi translated">更新 q 值</li></ol><p id="e56f" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">以下是 q-learning 的基本更新规则:</p><pre class="mh mi mj mk gt ml mg mm mn aw mo bi"><span id="1bdc" class="mp kh it mg b gy mq mr l ms mt"># Update q values</span><span id="fb32" class="mp kh it mg b gy mu mr l ms mt">Q[state, action] = Q[state, action] + lr * (reward + gamma * np.max(Q[new_state, :]) — Q[state, action])</span></pre><p id="5361" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在上面的更新中，有几个变量我们还没有提到。这里发生的是，我们根据贴现后的新值和旧值之间的差异来调整 q 值。我们使用 gamma 来贴现新值，并使用学习率(lr)来调整步长。以下是一些参考资料。</p><p id="6906" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><strong class="lg iu">学习率:</strong> <code class="fe md me mf mg b">lr</code>或学习率，常被称为<em class="mc">α</em>或<em class="mc"> </em> α，可以简单定义为你接受新值相对于旧值的程度。上面我们取新旧之间的差值，然后用这个值乘以学习率。这个值然后被加到我们先前的 q 值上，这实质上使它朝着我们最新更新的方向移动。</p><p id="4af3" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><strong class="lg iu">γ:</strong><code class="fe md me mf mg b">gamma</code>或<em class="mc"> γ </em>是贴现因子。它用于平衡当前和未来的奖励。从上面的更新规则中，您可以看到我们将折扣应用于未来的奖励。通常，该值可以在 0.8 到 0.99 的范围内。</p><p id="49cf" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><strong class="lg iu">奖励:</strong> <code class="fe md me mf mg b">reward</code>是在给定状态下完成某个动作后获得的数值。奖励可以发生在任何给定的时间步或只在终端时间步。</p><p id="6f83" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><strong class="lg iu"> Max: </strong> <code class="fe md me mf mg b">np.max()</code>使用 numpy 库，并获取未来奖励的最大值，并将其应用于当前状态的奖励。这是通过未来可能的回报来影响当前的行为。这就是 q-learning 的妙处。我们将未来奖励分配给当前行动，以帮助代理在任何给定状态下选择最高回报的行动。</p><p id="b164" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><strong class="lg iu">结论</strong></p><p id="6a99" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">就这样，简短而甜蜜(希望如此)。我们讨论过 q-learning 是一种非策略强化学习算法。我们使用一些基本的 python 语法展示了 q-learning 的基本更新规则，并回顾了算法所需的输入。我们了解到，q-learning 使用未来的奖励来影响给定状态下的当前行为，从而帮助代理选择最佳行为，使总奖励最大化。</p><p id="1bfd" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">关于 q-learning 还有很多，但希望这足以让你开始并有兴趣学习更多。我在下面添加了几个资源，它们对我学习 q-learning 很有帮助。尽情享受吧！</p><p id="8087" class="pw-post-body-paragraph le lf it lg b lh mv lj lk ll mw ln lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><strong class="lg iu">资源</strong></p><ol class=""><li id="d3f7" class="na nb it lg b lh mv ll mw lp nc lt nd lx ne mb nf ng nh ni bi translated">使用<a class="ae kf" href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym taxi </a>环境的绝佳 RL 和 q 学习示例</li><li id="17fc" class="na nb it lg b lh nj ll nk lp nl lt nm lx nn mb nf ng nh ni bi translated"><a class="ae kf" href="http://www.incompleteideas.net/book/RLbook2018trimmed.pdf" rel="noopener ugc nofollow" target="_blank">强化学习:导论</a>(萨顿的免费书籍)</li><li id="9313" class="na nb it lg b lh nj ll nk lp nl lt nm lx nn mb nf ng nh ni bi translated">Quora <a class="ae kf" href="https://www.quora.com/How-does-Q-learning-work-1" rel="noopener ugc nofollow" target="_blank"> Q 学习</a></li><li id="65f1" class="na nb it lg b lh nj ll nk lp nl lt nm lx nn mb nf ng nh ni bi translated">维基百科<a class="ae kf" href="https://en.wikipedia.org/wiki/Q-learning" rel="noopener ugc nofollow" target="_blank"> Q-learning </a></li><li id="3009" class="na nb it lg b lh nj ll nk lp nl lt nm lx nn mb nf ng nh ni bi translated">大卫·西尔弗关于 RL 的<a class="ae kf" href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html" rel="noopener ugc nofollow" target="_blank">讲座</a></li></ol></div></div>    
</body>
</html>