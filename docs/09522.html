<html>
<head>
<title>Reinforcement learning framework and toolkits (Gym and Unity)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习框架和工具包(Gym 和 Unity)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a?source=collection_archive---------10-----------------------#2019-12-15">https://towardsdatascience.com/reinforcement-learning-framework-and-toolkits-gym-and-unity-1e047889c59a?source=collection_archive---------10-----------------------#2019-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cb13" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍强化学习框架，以及环境 Cart-pole(健身房)和 Banana collector(Unity)</h2></div><p id="b897" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化学习为学习策略提供了一个框架，该学习策略将状态映射为行动，目标是最大化累积回报。在这篇文章中，我们提出了这个框架是如何数学公式，以及哪些算法可以用来解决 RL 问题。我们还将讨论(1) Gym 和(2) Unity 的使用，这两个工具包用于开发和比较强化学习算法。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/acb7d4b561fad22d4fa9584f931e63da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BpLk3KFMFCKMgBkL_EFgww.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Gym — classic control environments <a class="ae lu" href="https://gym.openai.com/envs/#classic_control" rel="noopener ugc nofollow" target="_blank">https://gym.openai.com/envs/#classic_control</a></figcaption></figure><h1 id="a5d8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习框架</h1><p id="f4cd" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><strong class="kk iu">强化学习</strong>是基于迭代学习思想的<strong class="kk iu">机器学习</strong>的一个领域。学习者，或者决策者，被称为<strong class="kk iu">代理</strong>，它与<strong class="kk iu">环境</strong>互动，接收被称为<strong class="kk iu">奖励</strong>的反馈。代理的目标是通过与环境的迭代来最大化回报。在强化学习问题中，我们认为时间步长是离散的。在第一时间步，代理观察环境的<strong class="kk iu">状态</strong>，选择<strong class="kk iu">动作</strong>作为响应。随后，环境向代理呈现一个新的状态和一个奖励，显示它的动作是多么恰当。这个过程在下面的时间步骤中继续，获得一系列的状态、动作和奖励。在反复试验之后，代理人学会执行适当的行动，以最大化<strong class="kk iu">预期累积报酬</strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/dbd8441201b0098889c3415ffecfb97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*TmaEbKtp407WpDFbe-3VMQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Sequence of states, actions, and rewards</figcaption></figure><p id="7d36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">建立<strong class="kk iu">奖励</strong>，以便当代理试图最大化它时，也学习特定的行为(例如，玩视频游戏或驾驶汽车)。智能体的设计不是为了最大化即时报酬，即时报酬是执行一个动作后环境提供的报酬，而是期望的累积报酬。预期累积奖励是从下一个时间步开始的奖励总和，用 Gt 表示；我们使用预期这个术语，因为代理人不能确定地预测未来的回报是什么。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/8af5cff984a83d09f17c0954be9a4fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*i6o5quP53LXC7Qk2TyQEZA.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Expected cumulative reward at time step t</figcaption></figure><p id="7071" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了强调比未来奖励来得更早的奖励，我们计算了<strong class="kk iu">折现奖励，</strong>代表γ<strong class="kk iu">折现率</strong>。折扣率是介于 0 和 1 之间的数字，由开发者在定义强化学习任务时设置。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/a174c4e1eeee6254e141f2b45404c1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*BKCx_olU1qd970AYKTRuSQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Discounted reward at time step t</figcaption></figure><p id="4828" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">折现率γ=1 代表未折现的奖励，而γ=0 代表最直接的奖励，这意味着γ越大，代理人考虑的未来奖励就越多。</p><p id="c70e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用<strong class="kk iu">马尔可夫决策过程</strong>来描述强化学习的环境。这个过程定义为:(1)一组状态<strong class="kk iu"> <em class="mv"> S </em> </strong>，(2)一组动作<strong class="kk iu"> <em class="mv"> A </em> </strong>，(3)一组奖励<strong class="kk iu"> <em class="mv"> R </em> </strong>，(4)环境的一步动力学<strong class="kk iu"><em class="mv">p(S′，r|s，a) </em> </strong>，(5)一个贴现率<strong class="kk iu"> <em class="mv"> γ </em> </strong>。环境在时间步长 t+1 做出响应，仅考虑前一时间步长的状态和动作。单步动态和奖励描述了环境如何工作；因此，代理不知道它们，它通过与环境迭代来学习如何采取适当的行动。</p><p id="c949" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个<strong class="kk iu">策略</strong>描述了代理的行为，是从一组状态<strong class="kk iu"><em class="mv"/></strong>到一组动作<strong class="kk iu"><em class="mv"/></strong>的映射。该策略可以是(1)确定性的，或者(2)随机的。一个<strong class="kk iu">确定性策略</strong>将状态映射到动作<strong class="kk iu"> <em class="mv"> π:S→A </em> </strong>，将一个状态作为输入，提供一个动作作为输出。相反，一个<strong class="kk iu">随机策略</strong><strong class="kk iu"><em class="mv">π:SxA→【0，1】</em></strong>取一个状态和一个动作，输出在那个状态下采取那个动作的概率。</p><p id="5562" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代理的目标是学习<strong class="kk iu">最优策略</strong>。如果预期收益大于或等于所有州的所有其他策略的预期收益，则认为策略是最优的。在我们深入了解最优性的概念之前，我们先介绍两个在强化学习中经常会遇到的函数:(1)状态值函数<strong class="kk iu">和(2)动作值函数<strong class="kk iu"/>。状态值函数是从状态<strong class="kk iu"> <em class="mv"> s </em> </strong>开始并遵循策略<strong class="kk iu"> <em class="mv"> π </em> </strong>时所有时间步长的预期收益，用小写的<strong class="kk iu"> <em class="mv"> v. </em> </strong>表示</strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/dc2bc82999ef366d85a48c70a4050f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*dxUkXX8fbPC1PMNO9oHt7Q.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">State-value function</figcaption></figure><p id="b118" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有一个公式可以简化状态值函数的计算，称为<strong class="kk iu">贝尔曼期望公式</strong>。根据这个等式，可以使用下一个状态的期望值和期望的即时报酬来计算任意状态<strong class="kk iu"> <em class="mv"> s </em> </strong>处的状态值函数。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/a4ebbbee646045d1f5ae5ef0b7b21eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*gW9m77zREIhIXTb-rBbPPw.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Bellman expectation equation</figcaption></figure><p id="129e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一个有趣的函数是用小写字母<strong class="kk iu"> <em class="mv"> q </em> </strong>表示的动作值函数。该函数不仅依赖于环境的状态，还依赖于代理的动作，表示期望的回报，如果代理在状态<strong class="kk iu"> <em class="mv"> s </em> </strong>开始，采取动作<strong class="kk iu"> <em class="mv"> a </em> </strong>，然后对于所有未来的时间步骤遵循策略<strong class="kk iu"><em class="mv">【π</em></strong>。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/a27cc40a93d376f4c73e7c0794c11fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*bR3rdOuY68xtJdA9iyiE-w.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Action-value function</figcaption></figure><p id="52aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">智能体的目标是通过与环境的迭代获得最优动作值函数<strong class="kk iu"><em class="mv">q∫</em></strong>，然后使用这个动作值函数获得最优策略<strong class="kk iu"><em class="mv">π∫</em></strong>。我们可以通过选择动作<strong class="kk iu"> <em class="mv"> a </em> </strong>来轻松实现，该动作为每个状态<strong class="kk iu"><em class="mv"/></strong>提供最大<strong class="kk iu"><em class="mv">q∫</em></strong></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/0127b82a49e4fff76668ece58bb0981e.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*Kn80U6nWskihxNbn6oQe3g.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Optimal policy</figcaption></figure><p id="e950" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强化任务的目标是获得代表最优代理行为的最优策略。为此，我们可以采用多种多样的算法，这些算法通常分为两组:(1) <strong class="kk iu">基于值的方法</strong>，以及(2) <strong class="kk iu">基于策略的方法</strong>。基于价值的方法通过学习最优行动价值函数<strong class="kk iu"><em class="mv">Q∫(s，a) </em> </strong>间接计算最优策略。然后，我们可以通过为每个状态选择最大化<strong class="kk iu"> <em class="mv"> Q </em> </strong>的动作来获得最优策略。相反，基于策略的方法直接找到最优策略，而不必计算动作值函数估计。下表显示了最著名的算法。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c5e7c1061242e6fc7f19de6999a1bd99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*2FUQOF0iiX2lUvHc-w0keQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Reinforcement learning algorithms</figcaption></figure><p id="c633" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所采用的算法取决于状态和动作空间的类型:(1)离散的，或(2)连续的。<strong class="kk iu">离散的</strong>空间呈现有限的一组状态和动作。在离散空间中，我们可以将 Q 表(动作值函数估计)表示为字典或查找表。<strong class="kk iu"> </strong>前述算法<strong class="kk iu"> Q-Learning </strong>和<strong class="kk iu"> Sarsa </strong>只能在离散空间中运行。然而，大多数强化学习应用需要<strong class="kk iu">连续的</strong>状态和动作空间。例如，在机器人学中，通常使用速度、位置和扭矩等连续变量。连续空间不呈现有限的可能性集合；它们可以取一系列的值。为了处理连续空间，我们可以采用两种策略:(1) <strong class="kk iu">离散化</strong>，以及(2) <strong class="kk iu">函数逼近</strong>。离散化允许我们使用算法 Q-Learning 和 Sarsa，只需很少或不需要修改。当需要的离散空间数量非常大时，离散化是不可行的，函数逼近成为唯一的选择。深度神经网络成为处理连续空间的最有吸引力的替代方案，可以轻松捕捉状态和动作之间的非线性关系。<strong class="kk iu">深度 Q 学习</strong>算法可以应用于呈现连续状态空间的强化学习问题，但是动作空间仍然必须是离散的。为了与连续状态和动作空间一起工作，必须使用基于策略的算法，例如<strong class="kk iu">增强</strong>或<strong class="kk iu">近似策略优化</strong>。另一种选择是使用<strong class="kk iu">行动者-批评家</strong>方法(基于价值的方法和基于政策的方法的结合)，例如<strong class="kk iu">深度确定性政策梯度</strong>。</p><p id="8c0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在以后的文章中，我们将解释如何使用 Pytorch 编写这些算法来训练代理。然而，本文主要关注于理解强化学习框架和工具包(Gym 和 Unity)。这两个库都提供了用于训练的代理，这意味着我们可以将上述算法之一应用于这些代理，以便它们学习特定的任务。</p><h1 id="7556" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习工具包——健身房</h1><p id="11d3" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><a class="ae lu" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> Gym </strong> </a>是开发强化学习算法的开源库。该工具包提供了从 Atari 游戏到机器人的各种环境。用户可以很容易地与代理交互，目的是应用一种算法来教会他们一个特定的任务(例如，在游戏中最大化得分，训练机器人行走，或平衡汽车上的杆子)。</p><p id="056e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一步是在你的电脑上安装 Gym。要做到这一点，我们可以查阅官方网页，</p><div class="nb nc gp gr nd ne"><a href="https://gym.openai.com/docs/#installation" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">Gym:开发和比较强化学习算法的工具包</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">健身房图书馆是一个测试问题——环境——的集合，你可以用它来制定你的强化…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">gym.openai.com</p></div></div></div></a></div><p id="33c1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者，我们可以阅读 Genevieve Hayes 在《走向数据科学》上发表的以下文章。</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/how-to-install-openai-gym-in-a-windows-environment-338969e24d30"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">如何在 Windows 环境下安装 OpenAI Gym</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">一步一步的指导如何建立和运行 OpenAI 健身房</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns lo ne"/></div></div></a></div><p id="4588" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了了解库是如何工作的，我们将解释 Gym 提供的<a class="ae lu" href="https://gym.openai.com/envs/CartPole-v1/" rel="noopener ugc nofollow" target="_blank"> CartPole-v1 </a>环境。我们开始吧💪！</p><h2 id="cd67" class="nt lw it bd lx nu nv dn mb nw nx dp mf kr ny nz mh kv oa ob mj kz oc od ml oe bi translated">CartPole-v1 环境</h2><p id="8505" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">小车-V1 环境由一根通过非驱动关节连接到小车上的杆组成，小车沿无摩擦轨道运动。代理可以向推车施加-1 到 1 之间的力，目的是尽可能长时间地保持杆的平衡。柱子保持直立的每一步都获得+1 的奖励。当柱子偏离垂直方向超过 15 度，或者手推车偏离中心超过 2.4 个单位时，该集结束。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c97dd1c5b52fa5275b9c115c0dad13a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*X6ORPSvdujPdpkEMkH1kwQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">CartPole-v1 Environment</figcaption></figure><p id="dac3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一步是导入库<strong class="kk iu"> gym </strong>，使用<strong class="kk iu"> gym.make </strong>函数加载 CartPole-v1 环境。一旦环境被创建，我们需要一个初步的观察。第一个状态通过调用<strong class="kk iu">复位</strong>函数获得，由一个(4)维<strong class="kk iu"> numpy 数组</strong>组成，包含以下信息:(1)小车位置，(2)小车速度，(3)电极角度，以及(4)尖端的电极速度。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="738f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">状态空间</strong>指的是我们的代理可能遇到的所有可能情况的集合。类似地，<strong class="kk iu">动作空间</strong>是我们的代理在特定状态下可以采取的所有可能动作的集合。两个空间的类型和大小可以通过使用<strong class="kk iu">环境观察 _ 空间</strong>和<strong class="kk iu">环境动作 _ 空间</strong>方法来查询。</p><p id="7041" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">健身房空间可以是:(1)离散的，(2)多离散的，(3)盒子，和(4)元组。在 CartPole-v1 环境中，动作空间是离散的，这意味着该空间包含从 0 到 n-1 的 n 个离散点。我们可以通过键入<strong class="kk iu"> env.action_space.n </strong>来获得动作的总数。相反，状态空间是一个多维连续空间(箱式)，其中每个变量都位于区间[low，high]内，通过使用<strong class="kk iu">env . observation _ space . low</strong>和<strong class="kk iu">env . observation _ space . high</strong>获得两个边界。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="ab08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用<strong class="kk iu">env . action _ space . sample()</strong>获得一个随机动作。然而，强化学习问题的目标不是随机选择一个动作，而是找到要采取的最佳动作，这意味着我们使用一种算法来输出我们的代理应该采取的动作，以使其报酬最大化。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="32db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">选择动作后，环境向代理呈现一个新的状态和一个奖励，显示这个动作是多么合适。为了获得下一步和奖励，我们使用了<strong class="kk iu"> env.step() </strong>方法。此方法使环境步进一个时间步长，并返回:</p><ol class=""><li id="4185" class="oi oj it kk b kl km ko kp kr ok kv ol kz om ld on oo op oq bi translated">观察→代表环境的下一个状态。</li><li id="c3ab" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">奖励→采取行动后获得的奖励。</li><li id="dbc5" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">完成→表示剧集是否已经结束。</li><li id="4aa0" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">信息→用于调试目的的诊断信息。</li></ol><p id="5a4e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面可以看到 step 方法返回的对象的值和类型。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9daa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在是时候把所有的放在一起了！:)我们创建一个代理，它采取随机的行动，直到剧集结束，返回该剧集中获得的总分数(奖励的总和)。请注意，当我们训练代理时，操作将通过策略(训练的算法)获得，而不是像这里一样随机获得。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="0644" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">车杆环境具有连续的<strong class="kk iu">状态空间</strong>，但是<strong class="kk iu">动作空间</strong>是离散的。为了训练这个代理，可以采用<strong class="kk iu">基于值的</strong>方法，例如<strong class="kk iu">深度 Q 学习</strong>。该算法为给定状态选择最佳动作；因此，使用该方法获得的策略是确定性的。为了获得<strong class="kk iu">随机策略</strong>，可以应用基于<strong class="kk iu">策略的</strong>方法，例如<strong class="kk iu">近似策略优化</strong>。</p><h1 id="f3ea" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习工具包— Unity</h1><p id="3198" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated"><strong class="kk iu">Unity Machine Learning Agents(ML-Agents)</strong>是一个开源的 Unity 插件，使游戏和模拟成为训练智能代理的环境。</p><p id="5e5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了了解图书馆是如何工作的，我们将解释香蕉收集器的环境。在这种环境下，代理可以拿黄色或蓝色的香蕉，目标是收集尽可能多的黄色香蕉，避免蓝色的。</p><p id="d9a1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们克隆存储库，然后导入<strong class="kk iu"> Unity </strong>并加载<strong class="kk iu">Banana collector</strong>环境。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="3cc1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Unity 使用了大脑的概念。学习环境中的每个代理都与一个大脑相连，这个大脑负责为所有相连的代理做出决策。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ow"><img src="../Images/944730f4297139c1a14bf6abb57710e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6siqOwM8RvRpJg3Tu20zUg.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Unity Learning Environment</figcaption></figure><p id="bfc2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以检查可用的大脑，并将其中一个设置为我们要用 Python 控制的大脑，如下所示。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="3467" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在香蕉收集器环境中，状态空间是连续的，有 37 个维度，包含代理的线速度和代理前进方向上基于光线的物体感知。动作空间是离散的，具有 4 个维度:(1)向前移动，(2)向后移动，(3)向左转，以及(4)向右转。收集一个黄色香蕉的奖励为+1，收集一个蓝色香蕉的奖励为-1。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0950481ae714296bd2f58cbe422e1e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*tLk7XI_ywviLpHnm-kQ9tg.png"/></div></figure><p id="88f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过使用以下函数来检查状态和动作空间(大小和类型):</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="9dc7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Unity </strong>提供了一个类似<strong class="kk iu">健身房的</strong>界面来培训代理。为了得到一个初步的观察结果，我们需要调用<strong class="kk iu"> env.reset() </strong>函数，就像我们之前对 gym 所做的那样。类似地，为了向环境发送一个动作，我们使用了<strong class="kk iu"> env.step() </strong>函数。</p><p id="e0c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们之前在健身房推车杆示例中所做的那样，我们创建了一个代理，它采取随机行动，直到该集结束，返回该集的总得分(奖励的总和)。请注意，当我们训练代理时，操作将由策略(训练的算法)获得，而不是像这里一样随机获得。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="0221" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所观察到的，获得的奖励是-1，这意味着代理人比黄色的香蕉摘得更多。训练后，代理人区分两者，获得的总回报更高。</p><p id="6e41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与车杆环境一样，状态空间是连续的，但动作空间是离散的。因此，为了训练这个代理，我们可以使用基于值的方法，例如深度 Q 学习，或者基于策略的方法，例如近似策略优化。</p><p id="b6ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在以后的文章中，我们将解释如何训练这两者:推车杆和香蕉收集代理。所以，敬请期待！😊</p><h1 id="7821" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">文学</h1><ul class=""><li id="298c" class="oi oj it kk b kl mn ko mo kr oy kv oz kz pa ld pb oo op oq bi translated">Udaciy 课程深度强化学习<a class="ae lu" href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893" rel="noopener ugc nofollow" target="_blank">https://www . uda city . com/course/Deep-Reinforcement-Learning-nano degree-nd 893</a></li><li id="0005" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld pb oo op oq bi translated"><a class="ae lu" href="https://gym.openai.com/envs/CartPole-v0/" rel="noopener ugc nofollow" target="_blank">https://gym.openai.com/envs/CartPole-v0/</a></li><li id="215a" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld pb oo op oq bi translated"><a class="ae lu" href="https://unity3d.com/machine-learning" rel="noopener ugc nofollow" target="_blank">https://unity3d.com/machine-learning</a></li></ul><h1 id="265c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">有趣的读物</strong></h1><ul class=""><li id="5c43" class="oi oj it kk b kl mn ko mo kr oy kv oz kz pa ld pb oo op oq bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287">https://towards data science . com/introduction-to-variable-reinforcement-learning-algorithms-I-q-learning-sarsa-dqn-ddpg-72 a5 E0 CB 6287</a></li><li id="12ba" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld pb oo op oq bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/the-complete-reinforcement-learning-dictionary-e16230b7d24e">https://towards data science . com/the-complete-reinforcement-learning-dictionary-e 16230 b 7d 24 e</a></li></ul><p id="3e13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读🍀 😃</p></div></div>    
</body>
</html>