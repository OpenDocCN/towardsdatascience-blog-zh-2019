<html>
<head>
<title>Understand Kaiming Initialization and Implementation Detail in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解 PyTorch 中明凯初始化和实现的细节</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138?source=collection_archive---------4-----------------------#2019-08-06">https://towardsdatascience.com/understand-kaiming-initialization-and-implementation-detail-in-pytorch-f7aa967e9138?source=collection_archive---------4-----------------------#2019-08-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1c81" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">初始化很重要！知道如何用明凯均匀函数设置扇入和扇出模式</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e07cb2e0d6c087bb6886dc984ba81549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjet9qSO4O8fX2-FXvxflw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@tateisimikito?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Jukan Tateisi</a> on <a class="ae ky" href="https://unsplash.com/search/photos/challenge?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="4c64" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="22e9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你<strong class="lt iu">通过创建一个线性层</strong>来隐式地创建权重，你应该设置<code class="fe mn mo mp mq b">modle='fan_in'</code>。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="2fcb" class="mv la it mq b gy mw mx l my mz">linear = torch.nn.Linear(node_in, node_out)<br/>init.kaiming_normal_(linear.weight, mode=’fan_in’)<br/>t = relu(linear(x_valid))</span></pre><p id="b0f3" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">如果你<strong class="lt iu">通过创建一个随机矩阵</strong>显式地创建权重，你应该设置<code class="fe mn mo mp mq b">modle='fan_out'</code>。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="fd0c" class="mv la it mq b gy mw mx l my mz">w1 = torch.randn(node_in, node_out)<br/>init.kaiming_normal_(w1, mode=’fan_out’)<br/>b1 = torch.randn(node_out)<br/>t = relu(linear(x_valid, w1, b1))</span></pre><p id="4097" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">内容结构如下。</p><ol class=""><li id="d5cb" class="nf ng it lt b lu na lx nb ma nh me ni mi nj mm nk nl nm nn bi translated">重量初始化很重要！</li><li id="e3d4" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">什么是明凯初始化？</li><li id="949e" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">为什么明凯初始化有效？</li><li id="49d3" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm nk nl nm nn bi translated">理解 Pytorch 实现中的扇入和扇出模式</li></ol><h1 id="894c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">重量初始化很重要！</h1><p id="6bb3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">初始化是一个创建权重的过程。在下面的代码片段中，我们随机创建了一个大小为<code class="fe mn mo mp mq b">(784, 50)</code>的权重<code class="fe mn mo mp mq b">w1</code>。</p><blockquote class="nt nu nv"><p id="fe18" class="lr ls nw lt b lu na ju lw lx nb jx lz nx nc mc md ny nd mg mh nz ne mk ml mm im bi translated"><code class="fe mn mo mp mq b"><em class="it">torhc.randn(*sizes)</em></code>从均值为 0、方差为 1 的<strong class="lt iu">正态分布</strong>返回一个填充了随机数的张量(也称为<strong class="lt iu">标准正态分布</strong>)。张量的形状由变量 argument <code class="fe mn mo mp mq b"><em class="it">sizes</em></code>定义。</p></blockquote><p id="0b5d" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">并且该权重将在训练阶段被更新。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="f29e" class="mv la it mq b gy mw mx l my mz"><em class="nw"># random init</em><br/>w1 = torch.randn(784, 50) <br/>b1 = torch.randn(50)<br/><br/><strong class="mq iu">def</strong> linear(x, w, b):<br/>    <strong class="mq iu">return</strong> x@w + b<br/><br/>t1 = linear(x_valid, w1, b1)</span><span id="88ab" class="mv la it mq b gy oa mx l my mz">print(t1.mean(), t1.std())</span><span id="232e" class="mv la it mq b gy oa mx l my mz">############# output ##############<br/>tensor(3.5744) tensor(28.4110)</span></pre><p id="f3c5" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">你可能想知道如果权重可以在训练阶段更新，为什么我们需要关心初始化。无论如何初始化权重，最终都会被“很好地”更新。</p><p id="3799" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">但现实并不那么甜蜜。如果我们随机初始化权重，就会引起两个问题，即<strong class="lt iu"> <em class="nw">消失渐变问题</em> </strong>和<strong class="lt iu"> <em class="nw">爆炸渐变问题</em> </strong>。</p><p id="3256" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu"> <em class="nw">消失渐变问题</em> </strong>表示权重消失为 0。因为这些权重在反向传播阶段与层一起被相乘。如果我们将权重初始化得很小(&lt; 1)，在反向传播过程中，当我们使用隐藏层进行反向传播时，梯度会变得越来越小。前几层的神经元比后几层的神经元学习慢得多。这导致轻微的重量更新。</p><p id="bf4d" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu"> <em class="nw">爆炸渐变问题</em> </strong>表示权重爆炸到无穷大(NaN)。因为这些权重在反向传播阶段与层一起被相乘。如果我们将权重初始化得很大(&gt; 1)，当我们在反向传播过程中使用隐藏层时，梯度会变得越来越大。早期层中的神经元以巨大的步长更新，<code class="fe mn mo mp mq b">W = W — ⍺ * dW</code>，并且向下的力矩将增加。</p><h1 id="996b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是明凯初始化？</h1><p id="df62" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">明凯等人</a>通过对 ReLUs 的非线性进行谨慎建模，推导出一种合理的初始化方法，使得极深模型(&gt; 30 层)收敛。下面是明凯初始化函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0be870ab668ca9a2f52f7d70bb311771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/0*DwUan_QhBFIKHFfy.png"/></div></figure><ul class=""><li id="e2d5" class="nf ng it lt b lu na lx nb ma nh me ni mi nj mm oc nl nm nn bi translated">a:该层之后使用的整流器的负斜率(默认情况下，ReLU 为 0)</li><li id="efcf" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm oc nl nm nn bi translated">fan_in:输入维数。如果我们创建一个<code class="fe mn mo mp mq b">(784, 50)</code>，扇入是 784。<code class="fe mn mo mp mq b">fan_in</code>用于前馈阶段的<strong class="lt iu">。如果我们将其设置为<code class="fe mn mo mp mq b">fan_out</code>，则扇出为 50。<code class="fe mn mo mp mq b">fan_out</code>用于<strong class="lt iu">反向传播阶段</strong>。后面我会详细解释两种模式。</strong></li></ul><h1 id="7187" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">为什么明凯初始化有效？</h1><p id="86a4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们通过比较随机初始化和明凯初始化来说明明凯初始化的有效性。</p><p id="d063" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu">随机初始化</strong></p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="0b9b" class="mv la it mq b gy mw mx l my mz"># random init<br/>w1 = torch.randn(784, 50) <br/>b1 = torch.randn(50)<br/>w2 = torch.randn(50, 10) <br/>b2 = torch.randn(10)<br/>w3 = torch.randn(10, 1) <br/>b3 = torch.randn(1)</span><span id="1fcd" class="mv la it mq b gy oa mx l my mz">def linear(x, w, b):<br/>    return x@w + b</span><span id="0be1" class="mv la it mq b gy oa mx l my mz">def relu(x):<br/>    return x.clamp_min(0.)</span><span id="9078" class="mv la it mq b gy oa mx l my mz">t1 = relu(linear(x_valid, w1, b1))<br/>t2 = relu(linear(t1, w2, b2))<br/>t3 = relu(linear(t2, w3, b3))</span><span id="98b9" class="mv la it mq b gy oa mx l my mz">print(t1.mean(), t1.std())<br/>print(t2.mean(), t2.std())<br/>print(t3.mean(), t3.std())</span><span id="bdf8" class="mv la it mq b gy oa mx l my mz">############# output ##############<br/>tensor(13.0542) tensor(17.9457)<br/>tensor(93.5488) tensor(113.1659)<br/>tensor(336.6660) tensor(208.7496)</span></pre><p id="8af7" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我们用均值为 0、方差为 1 的正态分布来初始化权重，ReLU 后的理想权重分布应该是<strong class="lt iu">逐层略微递增的均值</strong>和接近 1 的<strong class="lt iu">方差</strong>。但是在前馈阶段，经过一些层之后，分布变化很大。</p><p id="900c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu">为什么权重的均值要逐层小幅递增？</strong></p><p id="b766" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">因为我们使用 ReLU 作为激活函数。如果输入值大于 0，ReLU 将返回提供的值，如果输入值小于 0，将返回 0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/570cb774c4d701b0f33861e21f865ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/0*1dhXvUnpOzcp3Tna.png"/></div></figure><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="3833" class="mv la it mq b gy mw mx l my mz">if input &lt; 0:<br/>    return 0<br/>else:<br/>    return input</span></pre><p id="61c6" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">ReLU 之后，所有负值都变成 0。当层变得更深时，平均值将变得更大。</p><p id="8ad8" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu">明凯初始化</strong></p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="d87a" class="mv la it mq b gy mw mx l my mz"># kaiming init<br/>node_in = 784<br/>node_out = 50</span><span id="356a" class="mv la it mq b gy oa mx l my mz"># random init<br/>w1 = torch.randn(784, 50) * math.sqrt(2/784)<br/>b1 = torch.randn(50)<br/>w2 = torch.randn(50, 10) * math.sqrt(2/50)<br/>b2 = torch.randn(10)<br/>w3 = torch.randn(10, 1) * math.sqrt(2/10)<br/>b3 = torch.randn(1)</span><span id="16d9" class="mv la it mq b gy oa mx l my mz">def linear(x, w, b):<br/>    return x@w + b</span><span id="f633" class="mv la it mq b gy oa mx l my mz">def relu(x):<br/>    return x.clamp_min(0.)</span><span id="5c97" class="mv la it mq b gy oa mx l my mz">t1 = relu(linear(x_valid, w1, b1))<br/>t2 = relu(linear(t1, w2, b2))<br/>t3 = relu(linear(t2, w3, b3))</span><span id="9b1b" class="mv la it mq b gy oa mx l my mz">print(t1.mean(), t1.std())<br/>print(t2.mean(), t2.std())<br/>print(t3.mean(), t3.std())</span><span id="557a" class="mv la it mq b gy oa mx l my mz">############# output ##############<br/>tensor(0.7418) tensor(1.0053)<br/>tensor(1.3356) tensor(1.4079)<br/>tensor(3.2972) tensor(1.1409)</span></pre><p id="587a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">我们用均值为 0、方差为 <code class="fe mn mo mp mq b"><strong class="lt iu">std</strong></code>的正态分布<strong class="lt iu">初始化权重，relu 后的理想权重分布应该是均值逐层略微递增，方差接近 1。我们可以看到输出接近我们的预期。<strong class="lt iu">在前馈阶段，平均增量缓慢，std 接近 1 </strong>。并且这种稳定性将<strong class="lt iu">避免在</strong>反向传播阶段中的消失梯度问题和爆炸梯度问题。</strong></p><p id="9b05" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">明凯初始化显示出比随机初始化更好的稳定性。</p><h1 id="254f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">了解 Pytorch 实现中的 fan_in 和 fan_out 模式</strong></h1><p id="370f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><code class="fe mn mo mp mq b"><a class="ae ky" href="https://pytorch.org/docs/stable/nn.html#torch.nn.init.kaiming_normal_" rel="noopener ugc nofollow" target="_blank">nn.init.kaiming_normal_()</a></code>将返回具有从均值 0 和方差<code class="fe mn mo mp mq b">std</code>中采样的值的张量。有两种方法可以做到。</p><p id="7296" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">一种方法是<strong class="lt iu">通过创建线性层</strong>来隐式创建权重。我们设置<code class="fe mn mo mp mq b">mode='fan_in'</code>来表示使用<code class="fe mn mo mp mq b">node_in</code>计算 std</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="95b6" class="mv la it mq b gy mw mx l my mz">from torch.nn import init</span><span id="4d4c" class="mv la it mq b gy oa mx l my mz"># linear layer implementation<br/>node_in, node_out = 784, 50</span><span id="83c4" class="mv la it mq b gy oa mx l my mz"><strong class="mq iu">layer = torch.nn.Linear(node_in, node_out)<br/>init.kaiming_normal_(layer.weight, mode='<em class="nw">fan_in</em>')<br/>t = relu(layer(x_valid))</strong></span><span id="10bc" class="mv la it mq b gy oa mx l my mz">print(t.mean(), t.std())</span><span id="796c" class="mv la it mq b gy oa mx l my mz">############# output ##############<br/>tensor(0.4974, grad_fn=&lt;MeanBackward0&gt;) tensor(0.8027, grad_fn=&lt;StdBackward0&gt;)</span></pre><p id="2461" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">另一种方法是<strong class="lt iu">通过创建随机矩阵</strong>来显式创建权重，你应该设置<code class="fe mn mo mp mq b">mode='fan_out'</code>。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="4b04" class="mv la it mq b gy mw mx l my mz">def linear(x, w, b):<br/>    return x@w + b</span><span id="3e77" class="mv la it mq b gy oa mx l my mz"># weight matrix implementation<br/>node_in, node_out = 784, 50</span><span id="d0ce" class="mv la it mq b gy oa mx l my mz"><strong class="mq iu">w1 = torch.randn(node_in, node_out)<br/>init.kaiming_normal_(w1, mode='<em class="nw">fan_out</em>')<br/></strong>b1 = torch.randn(node_out)<br/><strong class="mq iu">t = relu(linear(x_valid, w1, b1))</strong></span><span id="51dc" class="mv la it mq b gy oa mx l my mz">print(t.mean(), t.std())</span><span id="7b9d" class="mv la it mq b gy oa mx l my mz">############# output ##############<br/>tensor(0.6424) tensor(0.9772)</span></pre><p id="03d7" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">两种实现方法都是对的。平均值接近 0.5，标准差接近 1。但是等一下，你有没有发现一些奇怪的事情？</p><p id="03c8" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu">为什么模式不同？</strong></p><p id="0058" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">根据<a class="ae ky" href="https://pytorch.org/docs/stable/nn.html#torch.nn.init.kaiming_normal_" rel="noopener ugc nofollow" target="_blank">文件</a>，选择<code class="fe mn mo mp mq b">'fan_in'</code>保留了正向传递中权重<strong class="lt iu">的方差大小。选择<code class="fe mn mo mp mq b">'fan_out'</code>保留反向过程中的幅度。我们可以这样写。</strong></p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="e8bc" class="mv la it mq b gy mw mx l my mz">node_in, node_out = 784, 50</span><span id="ff2c" class="mv la it mq b gy oa mx l my mz"># fan_in mode<br/>W = torch.randn(node_in, node_out) * math.sqrt(2 / node_in)</span><span id="5257" class="mv la it mq b gy oa mx l my mz"># fan_out mode<br/>W = np.random.randn(node_in, node_out) * math.sqrt(2/ node_out)</span></pre><p id="47f2" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">在线性层实现中，我们设置<code class="fe mn mo mp mq b">mode='fan_in'</code>。是的，这是前馈阶段，我们应该设置<code class="fe mn mo mp mq b">mode='fan_in'</code>。没什么问题。</p><p id="ca6f" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><strong class="lt iu">但是为什么我们在权重矩阵实现中设置模式为</strong> <code class="fe mn mo mp mq b"><strong class="lt iu">fan_out</strong></code> <strong class="lt iu">？</strong></p><p id="490a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated"><code class="fe mn mo mp mq b"><a class="ae ky" href="https://pytorch.org/docs/stable/nn.html#torch.nn.init.kaiming_normal_" rel="noopener ugc nofollow" target="_blank">nn.init.kaiming_normal_()</a></code>的<a class="ae ky" href="https://github.com/pytorch/pytorch/blob/d58059bc6fa9b5a0c9a3186631029e4578ca2bbd/torch/nn/init.py#L202" rel="noopener ugc nofollow" target="_blank">源代码</a>背后的原因</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="f16a" class="mv la it mq b gy mw mx l my mz">def _calculate_fan_in_and_fan_out(tensor):<br/>    dimensions = tensor.dim()<br/>    if dimensions &lt; 2:<br/>        raise ValueError("Fan in and fan out can not be computed for tensor with fewer than 2 dimensions")</span><span id="0fbb" class="mv la it mq b gy oa mx l my mz">if dimensions == 2:  # Linear<br/>        <strong class="mq iu">fan_in = tensor.size(1)<br/>        fan_out = tensor.size(0)</strong><br/>    else:<br/>        num_input_fmaps = tensor.size(1)<br/>        num_output_fmaps = tensor.size(0)<br/>        receptive_field_size = 1<br/>        if tensor.dim() &gt; 2:<br/>            receptive_field_size = tensor[0][0].numel()<br/>        fan_in = num_input_fmaps * receptive_field_size<br/>        fan_out = num_output_fmaps * receptive_field_size</span><span id="eec6" class="mv la it mq b gy oa mx l my mz">return fan_in, fan_out</span></pre><p id="abc2" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这是获得正确模式的源代码。<code class="fe mn mo mp mq b">tensor</code>是尺寸为(784，50)的<code class="fe mn mo mp mq b">w1</code>。所以<code class="fe mn mo mp mq b">fan_in = 50, fan_out=784</code>。当我们在权重矩阵实现中将模式设置为<code class="fe mn mo mp mq b">fan_out</code>时。<code class="fe mn mo mp mq b">init.kaiming_normal_()</code>实际计算如下。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="0bb6" class="mv la it mq b gy mw mx l my mz">node_in, node_out = 784, 50</span><span id="3ab7" class="mv la it mq b gy oa mx l my mz">W = np.random.randn(node_in, node_out)<br/><strong class="mq iu">init.kaiming_normal_(W, mode='<em class="nw">fan_out</em>')</strong></span><span id="4c1a" class="mv la it mq b gy oa mx l my mz"># what <!-- -->init.kaiming_normal_() actually does<br/>    <!-- --># fan_in = 50<br/>    # fan_out = 784</span><span id="e276" class="mv la it mq b gy oa mx l my mz">W = W * torch.sqrt(784 / 2)</span></pre><p id="473c" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">好吧，有道理。<strong class="lt iu">但是如何解释在线性层实现中使用</strong> <code class="fe mn mo mp mq b"><strong class="lt iu">fan_in</strong></code> <strong class="lt iu">？</strong></p><p id="f91f" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">当我们使用线性隐式创建权重时，权重被隐式转置。下面是<a class="ae ky" href="https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L1354" rel="noopener ugc nofollow" target="_blank">torch . nn . functional . linear</a>的源代码。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="04fc" class="mv la it mq b gy mw mx l my mz">def linear(input, weight, bias=None):<br/>    # type: (Tensor, Tensor, Optional[Tensor]) -&gt; Tensor<br/>    r"""<br/>    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.<br/>    Shape:<br/>        - Input: :math:`(N, *, in\_features)` where `*` means any number of<br/>          additional dimensions<br/>        - <strong class="mq iu">Weight: :math:`(out\_features, in\_features)`</strong><br/>        - Bias: :math:`(out\_features)`<br/>        - Output: :math:`(N, *, out\_features)`<br/>    """<br/>    if input.dim() == 2 and bias is not None:<br/>        # fused op is marginally faster<br/>        ret = torch.addmm(bias, input, <strong class="mq iu">weight.t()</strong>)<br/>    else:<br/>        <strong class="mq iu">output = input.matmul(weight.t())</strong><br/>        if bias is not None:<br/>            output += bias<br/>        ret = output<br/>    return ret</span></pre><p id="cb21" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">权重初始化为<code class="fe mn mo mp mq b">(out_features, in_features)</code>的大小。例如，如果我们输入尺寸<code class="fe mn mo mp mq b">(784, 50)</code>，重量的大小实际上是<code class="fe mn mo mp mq b">(50, 784)</code>。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="2584" class="mv la it mq b gy mw mx l my mz">torch.nn.Linear(784, 50).weight.shape</span><span id="9dbe" class="mv la it mq b gy oa mx l my mz">############# output ##############<br/>torch.Size([50, 784])</span></pre><p id="f71a" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这就是为什么 linear 需要先转置权重，再做 matmul 运算的原因。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="6d9a" class="mv la it mq b gy mw mx l my mz">    if input.dim() == 2 and bias is not None:<br/>        # fused op is marginally faster<br/>        ret = torch.addmm(bias, input, <strong class="mq iu">weight.t()</strong>)<br/>    else:<br/>        output = input.matmul(<strong class="mq iu">weight.t()</strong>)</span></pre><p id="0088" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">因为线性层中的权重大小为<code class="fe mn mo mp mq b">(50, 784)</code>，所以<code class="fe mn mo mp mq b">init.kaiming_normal_()</code>实际计算如下。</p><pre class="kj kk kl km gt mr mq ms mt aw mu bi"><span id="c0f4" class="mv la it mq b gy mw mx l my mz">node_in, node_out = 784, 50</span><span id="ef5e" class="mv la it mq b gy oa mx l my mz"><strong class="mq iu">layer = torch.nn.Linear(node_in, node_out)</strong><br/><strong class="mq iu">init.kaiming_normal_(layer.weight, mode='<em class="nw">fan_out</em>')</strong></span><span id="1a89" class="mv la it mq b gy oa mx l my mz"># the size of layer.weight<strong class="mq iu"> </strong>is (50, 784)</span><span id="5cc6" class="mv la it mq b gy oa mx l my mz"># what <!-- -->init.kaiming_normal_() actually does<br/>    <!-- --># fan_in = 784<br/>    # fan_out = 50</span><span id="1004" class="mv la it mq b gy oa mx l my mz">W = W * torch.sqrt(784 / 2)</span></pre><h1 id="5048" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">摘要</h1><p id="47ba" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在这篇文章中，我首先谈到了为什么初始化很重要，什么是明凯初始化。我将分解如何使用 PyTorch 来实现它。希望这个帖子对你有帮助。有什么建议就留言吧。</p><p id="f41b" class="pw-post-body-paragraph lr ls it lt b lu na ju lw lx nb jx lz ma nc mc md me nd mg mh mi ne mk ml mm im bi translated">这个<a class="ae ky" href="https://gist.github.com/BrambleXu/feb892476202ecc55d03f1f377869755" rel="noopener ugc nofollow" target="_blank">片段中的完整代码</a>。</p><blockquote class="nt nu nv"><p id="6680" class="lr ls nw lt b lu na ju lw lx nb jx lz nx nc mc md ny nd mg mh nz ne mk ml mm im bi translated"><strong class="lt iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae ky" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="lt iu"> <em class="it">中等</em> </strong> </a> <strong class="lt iu"> <em class="it">同</em> </strong> <a class="ae ky" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu"> <em class="it">分类查看</em> </strong> </a> <strong class="lt iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae ky" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="it">bramble Xu</em></strong></a><strong class="lt iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae ky" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="lt iu"><em class="it">徐亮</em> </strong> </a> <strong class="lt iu"> <em class="it"> <br/>博客:</em></strong><a class="ae ky" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><em class="it">bramble Xu</em></a></p></blockquote><h1 id="a249" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated"><strong class="ak">参考</strong></h1><ul class=""><li id="06c1" class="nf ng it lt b lu lv lx ly ma oe me of mi og mm oc nl nm nn bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/what-is-weight-initialization-in-neural-nets-and-why-it-matters-ec45398f99fa">为什么谨慎初始化深度神经网络很重要？</a></li><li id="94ce" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm oc nl nm nn bi translated"><a class="ae ky" href="https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94" rel="noopener">深度学习最佳实践(1) —权重初始化</a></li><li id="d8aa" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm oc nl nm nn bi translated">明凯初始化论文:<a class="ae ky" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">深入研究整流器:在 ImageNet 分类上超越人类水平的性能</a></li><li id="3c5c" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm oc nl nm nn bi translated"><a class="ae ky" href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">整流线性单元(ReLU)简介</a></li><li id="2808" class="nf ng it lt b lu no lx np ma nq me nr mi ns mm oc nl nm nn bi translated"><a class="ae ky" href="https://course.fast.ai/videos/?lesson=8" rel="noopener ugc nofollow" target="_blank"> Fast.ai 的《程序员深度学习》课程第八课</a></li></ul></div></div>    
</body>
</html>