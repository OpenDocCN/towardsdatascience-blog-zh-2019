<html>
<head>
<title>Human-Like Machine Hearing With AI (3/3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能的类人机器听觉(3/3)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/human-like-machine-hearing-with-ai-3-3-fd6238426416?source=collection_archive---------10-----------------------#2019-02-27">https://towardsdatascience.com/human-like-machine-hearing-with-ai-3-3-fd6238426416?source=collection_archive---------10-----------------------#2019-02-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/09cea819c71f6ca8574f1953d432491d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nNeTXnntUdN83LFQ"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: <a class="ae jd" href="https://unsplash.com/@maltewingen?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Malte Wingen</a></figcaption></figure><div class=""/><div class=""><h2 id="c1b2" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">结果和观点。</h2></div><p id="bfad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我关于<strong class="kx jh"> <em class="lr">【类人】机器听觉</em> </strong>系列文章的最后一部分:用 AI 对人类听觉的部分进行建模来做音频信号处理。</p><h2 id="1473" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">本系列的最后一部分将提供:</h2><ul class=""><li id="4538" class="ml mm jg kx b ky mn lb mo le mp li mq lm mr lq ms mt mu mv bi translated">对主要观点的总结。</li><li id="b241" class="ml mm jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">经验测试的结果。</li><li id="37c7" class="ml mm jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated">相关工作和未来展望。</li></ul><p id="d970" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你错过了之前的文章，这里有:</p><p id="f93b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">背景</strong>:<a class="ae jd" rel="noopener" target="_blank" href="/the-promise-of-ai-in-audio-processing-a7e4996eb2ca">AI 在音频处理上的承诺</a> <br/> <strong class="kx jh">批评</strong>:<a class="ae jd" rel="noopener" target="_blank" href="/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd">CNN 和 spectrograms 做音频处理有什么问题？</a> <br/> <strong class="kx jh">第一部分</strong> : <a class="ae jd" rel="noopener" target="_blank" href="/human-like-machine-hearing-with-ai-1-3-a5713af6e2f8">具有 AI (1/3)听觉的类人机器</a> <br/> <strong class="kx jh">第二部分:</strong> <a class="ae jd" rel="noopener" target="_blank" href="/human-like-machine-hearing-with-ai-2-3-f9fab903b20a">具有 AI (2/3)听觉的类人机器</a></p><h1 id="0be0" class="nb lt jg bd lu nc nd ne lx nf ng nh ma km ni kn md kp nj kq mg ks nk kt mj nl bi translated">总结。</h1><p id="7601" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">在抽象层次上理解和处理信息不是一件容易的事情。人工神经网络已经在这一领域移山。特别是对于计算机视觉:深度 2D CNN 已经被证明可以捕捉视觉特征的层次，随着网络的每一层，复杂性都在增加。卷积神经网络的灵感来自于大脑皮层，而大脑皮层又受到了人类视觉系统的启发。</p><p id="551c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">人们已经尝试在音频领域重新应用风格转换等技术，但结果很少令人信服。视觉方法似乎不适用于声音。</p><p id="6771" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我认为声音完全是一种不同的动物。在进行特征提取和设计深度学习架构时，这是需要记住的事情。声音表现不同。就像计算机视觉受益于对视觉系统的建模一样，当在神经网络中处理声音时，我们可以受益于考虑人类的听觉。</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/f62d1f80c946682c9cf16e3ccc27973a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jzs7GEoT7gOjTbQA"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: <a class="ae jd" href="https://unsplash.com/@trommelkopf?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Steve Harvey</a></figcaption></figure><h2 id="b811" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">声音表现。</h2><p id="8ef2" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">为了开始探索建模方法，我们可以建立一个人类基线:</p><p id="d803" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对大脑来说，声音是用光谱表示的。压力波由耳蜗处理，并被分成大约 20-20，000 Hz 范围内的大约 3500 个对数间隔的频带。</p><p id="3068" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">声音是以 2-5 毫秒的时间分辨率听到的。比这更短的声音(或声音的间隙)人类几乎察觉不到。</p><p id="1a3f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">基于这些信息，<strong class="kx jh">我推荐使用一个</strong> <a class="ae jd" href="https://en.wikipedia.org/wiki/Gammatone_filter" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">伽玛通滤波器组</strong> </a>来代替傅立叶变换。<a class="ae jd" href="https://www.sciencedirect.com/science/article/pii/S0165027016302898" rel="noopener ugc nofollow" target="_blank"> Gammatone 滤波器组是听觉建模中的常用工具</a>，通常，滤波器组允许分离频谱和时间分辨率。<strong class="kx jh">通过这种方式，您可以拥有多个光谱带和一个较短的分析窗口</strong>。</p><h2 id="1c38" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">内存和缓冲器。</h2><p id="39e5" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">人类被认为具有短期储存感官印象的记忆，以便进行比较和整合。尽管实验结果略有不同，但它们表明人类有大约 0.25-4 秒的回声记忆(专门针对声音的感官记忆)。</p><blockquote class="nu"><p id="3041" class="nv nw jg bd nx ny nz oa ob oc od lq dk translated"><strong class="ak">任何人类能听懂的声音</strong> <strong class="ak">都可以在这些限度内表现出来！</strong></p></blockquote><p id="c0ea" class="pw-post-body-paragraph kv kw jg kx b ky oe kh la lb of kk ld le og lg lh li oh lk ll lm oi lo lp lq ij bi translated">几秒钟的 2-5 ms 窗口，每个窗口有大约 3500 个对数间隔的频带。但这确实积累了大量数据。</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div class="ab gu cl oj"><img src="../Images/7d947dee75b0114ea417bc5f2782aeea.png" data-original-src="https://miro.medium.com/v2/0*FjbbdS65FKlCLBbC"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Two dilated buffers covering ~1.25s of sound with 8 time steps.</figcaption></figure><p id="558e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了减少一点维度，我提出了<strong class="kx jh"> <em class="lr">扩展缓冲区</em> </strong>的想法，其中时间序列的时间分辨率随着旧时间步长的增加而降低。这样，可以覆盖更大的时间范围。</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/901fdc94a776b2260ff6adb58a45c174.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q3zKnVN7HskpCA2N"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo credit: <a class="ae jd" href="https://unsplash.com/@alireza_attari?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Alireza Attari</a></figcaption></figure><h2 id="c26e" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">监听器处理器架构。</h2><p id="5f55" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">我们可以<strong class="kx jh">将内耳概念化为一个光谱特征提取器，将听觉皮层概念化为一个分析处理器</strong>，从听觉印象的记忆中导出<em class="lr">“认知属性】</em>。</p><p id="c274" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这两者之间，还有一系列经常被遗忘的步骤。<a class="ae jd" href="https://en.wikipedia.org/wiki/Cochlear_nucleus" rel="noopener ugc nofollow" target="_blank">它们被称为<em class="lr">耳蜗核</em> </a>。关于这些我们还有很多不知道的，但是它们对声音做了一种初步的神经编码:对定位和声音识别的基本特征进行编码。</p><p id="5aa0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这让我探索了一个<strong class="kx jh"> <em class="lr">监听器-处理器模型</em> </strong>，其中声音缓冲区由通用<a class="ae jd" href="https://machinelearningmastery.com/lstm-autoencoders/" rel="noopener ugc nofollow" target="_blank"> LSTM 自动编码器</a>(一个<strong class="kx jh"><em class="lr"/></strong>)嵌入到一个低维空间，然后被传递到一个特定任务的神经网络(一个<strong class="kx jh"> <em class="lr">【处理器】</em> </strong>)。这使得 autoencoder 成为一个可重用的预处理步骤，用于对声音进行一些特定任务的分析工作。</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ol"><img src="../Images/5f101b2997831b2aaac873d2c5884e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vnWzWT1CQRCuaRD78zpatw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">A listener-processor architecture to do sound classification.</figcaption></figure><h1 id="c081" class="nb lt jg bd lu nc nd ne lx nf ng nh ma km ni kn md kp nj kq mg ks nk kt mj nl bi translated">结果。</h1><p id="369a" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">我认为这一系列文章提出了一些在神经网络中处理声音的新想法。利用这些原则，我建立了一个模型，使用<a class="ae jd" href="https://urbansounddataset.weebly.com/urbansound8k.html" rel="noopener ugc nofollow" target="_blank"> UrbanSound8K 数据集</a>进行环境声音分类。</p><p id="1c06" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于计算资源有限，我只能勉强接受:</p><ul class=""><li id="5dde" class="ml mm jg kx b ky kz lb lc le om li on lm oo lq ms mt mu mv bi translated"><strong class="kx jh"> 100 个伽马酮过滤器。</strong></li><li id="c4f7" class="ml mm jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated"><strong class="kx jh"> 10 ms 分析窗口。</strong></li><li id="3395" class="ml mm jg kx b ky mw lb mx le my li mz lm na lq ms mt mu mv bi translated"><strong class="kx jh"> 8 级扩展缓冲器，覆盖约 1.25 秒的声音。</strong></li></ul><p id="e533" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了训练听者，我将数千个膨胀的缓冲区输入到一个自动编码器中，每边有两个 LSTM 层，将 800 维的连续输入编码到一个 250 维的“静态”潜在空间中。</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi op"><img src="../Images/47df04d9133ead43bd2c8c49396c9e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WjA2cxbcxHnbN64o7-dutw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">An illustration of the LSTM autoencoder architecture.</figcaption></figure><p id="ad90" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">经过大约 50 个时期的训练后，<strong class="kx jh">自动编码器能够捕捉大多数输入缓冲器的粗略结构</strong>。能够产生一种嵌入来捕捉声音中复杂的频率顺序运动是非常有趣的！</p><h2 id="120f" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">但是有用吗？</h2><p id="f368" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">这是要问的问题。为了测试这一点，<strong class="kx jh">我训练了一个 5 层自归一化神经网络，使用来自 LSTM 编码器的嵌入来预测声音类别</strong> (UrbanSound8K 定义了 10 个可能的类别)。</p><blockquote class="nu"><p id="4235" class="nv nw jg bd nx ny nz oa ob oc od lq dk translated">经过 50 代的训练后，该网络预测声音类别的准确率约为 70%。</p></blockquote><p id="309e" class="pw-post-body-paragraph kv kw jg kx b ky oe kh la lb of kk ld le og lg lh li oh lk ll lm oi lo lp lq ij bi translated">2018 年，<a class="ae jd" href="https://arxiv.org/pdf/1808.08405.pdf" rel="noopener ugc nofollow" target="_blank"> Z .张等人。艾尔。在 UrbanSound8K 数据集</a>上实现了 77.4%的预测准确率。他们通过对γ谱图进行 1D 卷积实现了这一点。随着数据的增加，这种准确性可以被推得更高。我没有资源来探索数据增强，所以我将在系统的非增强版本上进行比较。</p><p id="ed1e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">相比之下，我的方法准确率低了 7.4%。然而，我的技术在 10 ms 的时间范围内工作(通常附带一些内存)，这意味着 70%的准确率涵盖了数据集中声音在任何给定时间的 10 ms 时刻。<strong class="kx jh">这将系统的延迟降低了 300 倍，非常适合实时处理</strong>。简而言之，这种方法不太准确，但在处理时引入的延迟明显更少。</p><figure class="nq nr ns nt gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oq"><img src="../Images/bdb995c3412d17fb2b4c40b78018627d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liVcDhz5dLwHhztCYnkN7g.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">A “happy accident” I encountered during initial experiments.</figcaption></figure><h1 id="c50b" class="nb lt jg bd lu nc nd ne lx nf ng nh ma km ni kn md kp nj kq mg ks nk kt mj nl bi translated">视角。</h1><p id="50fe" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">完成这个项目确实非常有趣。我希望已经为你提供了一些如何在神经网络中处理声音的想法。虽然我对我的初步结果很满意，但我相信，如果有更好的光谱分辨率和用于训练神经网络的计算资源，这些结果可以得到显著改善。</p><p id="feb7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望有人会拿起这个，在新问题上用一种<strong class="kx jh">监听器-处理器</strong>的方法进行实验。特别是，我很好奇尝试一种<a class="ae jd" href="http://kvfrans.com/variational-autoencoders-explained/" rel="noopener ugc nofollow" target="_blank">变分自动编码器</a>方法，看看当它们的潜在空间被调整时，重建的声音会发生什么——也许这可以揭示一些关于声音本身的基本统计特征的直觉。</p><h1 id="e218" class="nb lt jg bd lu nc nd ne lx nf ng nh ma km ni kn md kp nj kq mg ks nk kt mj nl bi translated">相关工作。</h1><p id="8a0d" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">如果您对使用自动编码器的声音表现感兴趣，这里有一些启发了我的项目，我推荐您去看看:</p><h2 id="daca" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><a class="ae jd" href="https://arxiv.org/abs/1603.00982" rel="noopener ugc nofollow" target="_blank">音字 2Vec </a></h2><p id="f83c" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">这些人使用 MFCC 和 Seq2Seq 自动编码器研究了一种类似的语音编码方法。他们发现语音结构可以用这种方式充分表达。</p><h2 id="cf38" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><a class="ae jd" href="https://research.fb.com/publications/a-universal-music-translation-network/" rel="noopener ugc nofollow" target="_blank">全球音乐翻译网</a></h2><p id="b8ab" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">这是非常令人印象深刻的，也是我见过的最接近声音风格的转变。它是去年由脸书人工智能研究所发布的。通过使用共享的 WaveNet 编码器，他们将许多原始样本音乐序列压缩到一个潜在空间，然后使用单独的 WaveNet 解码器对每个所需的输出“风格”进行解码。</p><h2 id="28bb" class="ls lt jg bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated"><a class="ae jd" href="https://arxiv.org/abs/1810.06603v1" rel="noopener ugc nofollow" target="_blank">用神经网络模拟非线性音频效果</a></h2><p id="b13e" class="pw-post-body-paragraph kv kw jg kx b ky mn kh la lb mo kk ld le nm lg lh li nn lk ll lm no lo lp lq ij bi translated">马克·马丁内斯和约书亚·赖斯成功地用神经网络模拟了非线性音频效果(如失真)。他们通过使用 1D 卷积来编码原始样本序列，变换这些编码(！)与深度神经网络，然后通过去卷积将这些编码重新合成回原始样本。</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="21d8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">亲爱的读者，非常感谢你和我一起踏上这段旅程。我对这个系列文章得到的大量积极的、批评性的和信息丰富的回复感到荣幸。</p><p id="bd56" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">忙得不可开交，这篇最后的文章姗姗来迟。现在它已经结束了，我期待着下一章，新的项目和想法来探索。</p></div></div>    
</body>
</html>