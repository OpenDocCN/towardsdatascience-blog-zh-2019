# 数据科学和机器学习中的法规和伦理

> 原文：<https://towardsdatascience.com/regulation-and-ethics-in-data-science-and-machine-learning-8120aeb7cd59?source=collection_archive---------22----------------------->

![](img/028ae4ad14090507fd48fa845a1f4faa.png)

Artwork by: [Nina Michailidou](https://www.instagram.com/ninamichailidou/)

# 不管我们喜欢与否，机器学习改变了我们的生活。

统计推断、强化学习、深度神经网络和其他术语最近吸引了很多关注，事实上，这是有根本原因的。统计推断扩展了我们决策的基础，并改变了决策的审议过程。这种变化构成了从我称之为*前数据科学*到随后的*数据科学时代*的本质区别。在数据科学时代，决策是基于数据和算法做出的。通常情况下，决策完全由算法做出，只有在收集、清理、构建数据和建立算法选择框架的过程中，人类才是重要的参与者(通常情况下，算法本身是通过一个指标来选择的)。鉴于这一根本性变化，在数据科学时代做出决策时，密切关注决策的扩展基础以及在考虑这一扩展基础时思维过程的变化非常重要。

# “所有的模型都是错的，但有些是有用的。”

额外的步骤导致:

# "所有的模型都是错误的，有些是有用的，但有些也可能是危险的."

另一方面，大量研究证明，人类行为——以及决策——是情境驱动的[cit]。当将人类的准确性与算法的预测进行比较时，算法的[预测准确性一贯甚至将专家的判断打得落花流水。对这一点的详尽讨论超出了本文的范围，但是在**附录 1、**中有一些不错的资料，以防读者有兴趣进一步研究这个主题。](https://www.sciencedirect.com/science/article/pii/S0749597818303388)

好吧，接下来是什么？就像每一次技术进步一样，在某个时刻，关于西方资本主义最关键的辩论，一些悄悄话会变成声音:我们把监管的门槛放在哪里？我们需要更多的监管吗？具体来说:我们应该审核我们的算法和数据，以确保它们满足准确性、有效性和偏差的最低要求吗？

# 为什么监管至关重要？

每个统计学习算法都应该满足三个要求，即准确性、有效性和偏倚。简而言之，**准确性**表示模型的性能与基于可用数据集的可接受基线或预定义正确答案的比较。**有效性**考虑如何收集数据的更广泛背景，以及由于测量误差、校准失败等，数据在多大程度上没有反映真实世界。**偏差**是指对特定人群的预测系统性地偏低或偏高的情况，通常是由人工标注和不完善的数据质量、缺失数据，以及更一般地说，采样偏差和模型设定错误造成的。

那么，我们可以安全地假设算法应该被审计，并且它们满足上面提到的要求吗？最后，我认为与社会和健康决策系统相关的算法需要监管。然而，从数据科学的角度来看，我们应该以更广阔的视角开始开发算法，而不是仅仅依赖 MSE 和 AUC 来说明准确性。遵循并向风险评估工具延伸*负责任的机器学习原则，*算法需要确保我们的预测满足以下要求:

1.  数据科学和机器学习过程应该通过设计包括人在内的系统来尽可能地增加**人的干预**。此外，特别是对于风险评估工具，统计决策工具的关键挑战之一是自动化偏差现象，即机器提供的信息被认为是内在可信的，不容置疑的。这可能导致人类过度依赖自动化系统的准确性或正确性。最后，**主题专家**应该能够验证结果和过程。他们还应该接受足够的教育，以便理解伴随算法开发的潜在假设。
2.  **统计模型中的偏差**必须在估计值的方差容许范围内进行测量和减轻。在风险评估工具中训练算法时应该考虑的一个关键因素是所谓的遗漏变量偏差问题。每当从不包括所有相关因果因素的数据中训练模型时，就会出现遗漏变量偏差。
3.  工具不得合并多个不同的预测。**对于不同的风险，应衡量不同的得分**，而不是反映各种结果风险的单一风险。
4.  **模型再现性**。使机器学习模型可再现需要抽象其组成组件的过程，即数据、配置/环境和计算图。如果把这三点都抽象出来，就有可能有模型再现性的基础。决定抽象级别通常是至关重要的，因为可以专注于构建非常复杂的层来抽象具有特定数据输入/输出格式的多个机器学习库。
5.  **信任靠隐私**。适当级别的隐私。与用户和相关利益方建立信任的一个基本方法是通过展示适当的流程和技术来保护个人数据。技术专家应该做出明确的努力来理解所涉及的元数据的潜在含义，以及元数据是否会暴露相关用户或利益相关者的意外个人信息。

# 给我问题而不是解决方案，尽管有时后者会有所帮助。

为了应对这些挑战并降低其对业务的风险(包括道德实践的实施)，通过算法开发风险评估的公司应在以下方面投入时间:

**首先**，在**探索性数据分析** (EDA)阶段，应进行适当的数据分析，以确定数据不平衡。此外，对特征进行相关性分析以及适当地平衡训练-验证-测试分割是非常重要的。

**第二个**，**模型可解释性**至关重要，尤其是对于复杂的数据集，它包含了所谓的黑色模型(梯度推进、神经网络等)。).为此，开发了不同的方法，包括 SHapley Additive explainions(SHAP)和 LIME 等模型。前者基于可能的联盟如何有助于作为一个整体的群体的影响的想法，这是一个从合作博弈论借用的概念，而后者侧重于局部近似。(LIME 更快，但不太准确，目前不在讨论范围内)。而且很少有像 Pachyderm 和 ModelDB 这样优秀的开源库，可以帮助数据科学家和机器学习工程师在透明性和模型**再现性**的基础上进行构建。

**第三**，当将模型投入生产时，应结合性能的连续诊断，以捕捉与模型训练数据集的任何偏差。例如，学习系统的行为应该包括随着时间的推移腐蚀发生的漂移。此外，KL Divergence 和 Wasserstein 等分数也应该用于捕捉数据偏差。

**最后**、**在处理关键事件诊断和行动时，应始终考虑自动化偏差**。结果应该由主题专家阅读，他们可以批判性地质疑结果并理解输入/输出和遵循的过程。

Ps 1:这篇文章的内容旨在告诉读者一个相当活跃的话题，所有数据科学家迟早都要解决这个问题。我试图涵盖我用作参考资料的大部分材料，以供希望进一步加深知识的读者参考。

Ps 2:请联系我以获得更正和讨论点。

**参考文献**

[1]:该格言一般归功于[统计学家](https://en.wikipedia.org/wiki/Statistician) [乔治·博克斯](https://en.wikipedia.org/wiki/George_E._P._Box)。

【2】:[https://ethical.institute/](https://ethical.institute/)

[3]:[https://standards.ieee.org/industry-connections/ecpais.html](https://standards.ieee.org/industry-connections/ecpais.html)

[4]:[https://sloanreview . MIT . edu/article/the-regulation-of-ai-should-organizations-be-worried/](https://sloanreview.mit.edu/article/the-regulation-of-ai-should-organizations-be-worried/)

[5]:[https://sloanreview . MIT . edu/article/the-risk-of-machine-learning-bias-and-how-to-prevent-it/](https://sloanreview.mit.edu/article/the-risk-of-machine-learning-bias-and-how-to-prevent-it/)

[6]:[https://hbr.org/2018/11/why-we-need-to-audit-algorithms](https://hbr.org/2018/11/why-we-need-to-audit-algorithms)

[7]:[https://www.blog.google/technology/ai/ai-principles/](https://www.blog.google/technology/ai/ai-principles/)