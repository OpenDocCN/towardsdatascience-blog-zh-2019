<html>
<head>
<title>Arctic Monkeys Lyrics Generator with Data Augmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有数据增强的北极猴子歌词生成器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/arctic-monkeys-lyrics-generator-with-data-augmentation-b9b1f7989db0?source=collection_archive---------8-----------------------#2019-04-26">https://towardsdatascience.com/arctic-monkeys-lyrics-generator-with-data-augmentation-b9b1f7989db0?source=collection_archive---------8-----------------------#2019-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/6f265d95862a1558254b1d2f9ee8e24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XQP08H308R3kjqHq.jpg"/></div></div></figure><p id="9756" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">AM:不要相信炒作。艾:嗯..</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="7645" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">简介。</strong></h1><p id="f717" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi mh translated">外部发生器很酷，对吗？大约两年前，当我第一次看到类似于“莎士比亚发电机”的东西时，我惊叹不已。</p><p id="8926" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">通常，文本生成器将是具有递归神经网络或 LSTM 的语言模型，并尝试基于先前的种子词来预测接下来的词。</p><p id="eb44" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所以我决定基于北极猴子的歌词创建一个歌词生成器。这个想法分为三个主要部分</p><ul class=""><li id="3789" class="mq mr iq ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated">创建数据语料库并清洗数据</li><li id="9e33" class="mq mr iq ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">文本数据扩充</li><li id="0be5" class="mq mr iq ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">语言模型和生成器</li></ul></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="91ad" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">创建数据语料库</h1><p id="4202" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我发现乔纳森·戴顿的博客真的很有帮助。它使用 spotifyAPI 获取 spotify 上的艺术家 ID，列出所有专辑 ID，获取所有曲目 ID 的列表。然后使用 GeniusAPI 保存歌曲的所有歌词。这里有一些获取数据的代码片段。</p><p id="ed3a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">依赖关系:</p><ul class=""><li id="84ab" class="mq mr iq ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated"><em class="kw">歌词天才</em></li></ul><figure class="nf ng nh ni gt jr"><div class="bz fp l di"><div class="nj nk l"/></div></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="391c" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">文本扩充</h1><p id="16cb" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">该数据集包括 144 首歌曲，即 167887 个单词。我真的很想对亚历克斯写的歌曲数量发表评论，这些歌曲甚至不包括上一张皮影戏和他的个人专辑中的歌曲——我开始分心了！</p><p id="d1f5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果数据集没有语言建模任务预期的那么大，可以应用文本扩充。</p><p id="d275" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里使用的两种类型的文本增强是</p><ul class=""><li id="2d67" class="mq mr iq ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated">替换-用语言模型通常预测的单词替换当前单词。</li><li id="deb7" class="mq mr iq ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">插入—使用单词作为预测下一个单词的特征。</li></ul><p id="7547" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我为此使用了<a class="ae ne" href="https://pypi.org/project/nlpaug/" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>，在这篇文章中可以找到一个非常好的概述——马志威<a class="ae ne" href="https://towardsdatascience.com/@makcedward" rel="noopener" target="_blank"/><br/>的<a class="ae ne" rel="noopener" target="_blank" href="/data-augmentation-library-for-text-9661736b13ff">文本数据扩充库</a>。为了生成歌词的合成数据，我认为使用单词级模型更有益，并且像“naf.sequential”这样的流增强器用于顺序应用不同的增强。</p><figure class="nf ng nh ni gt jr"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="1abf" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我使用了两种类型的增强——Bert aug 和 FasttextAug。它们都基于上下文插入/替换相似的单词。BertAug 使用<a class="ae ne" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>语言模型来预测被替换的单词或者在插入的情况下预测下一个单词。FasstextAug 基于上下文化的单词嵌入替换或插入单词。</p><p id="d4bb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【BERTAug 插入和替换后的结果</p><blockquote class="nl nm nn"><p id="0d22" class="jy jz kw ka b kb kc kd ke kf kg kh ki no kk kl km np ko kp kq nq ks kt ku kv ij bi translated">进:总有更高更机智的人<br/>出:<em class="iq">是</em>总有更高更机智的人</p></blockquote><p id="09c2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="kw"> weeeirrrdddd..但听起来差不多是对的。</em></p><p id="3f58" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">【FasttextAug 插入和替换后的结果</p><blockquote class="nl nm nn"><p id="4894" class="jy jz kw ka b kb kc kd ke kf kg kh ki no kk kl km np ko kp kq nq ks kt ku kv ij bi translated">总有更高的人更有智慧</p></blockquote><p id="2787" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">还有一件有趣的事情发生了，由于子词嵌入，FasttestAug 的未知词没有 ValueError 异常——我使用 wiki-news-300d-1M-subword.vec 来加载模型——</p><p id="9c90" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">除了——嗯——<em class="kw">“I . d . s . t . I . d . s . t . I . d . s . t . I . d . s . t”，“啾啾！啾啾！啾啾！”和“咻-咻-咻-咻-咻”。</em>我老实说不怪。</p><figure class="nf ng nh ni gt jr gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b5bebd5eb10a36b3fc1299e3989bbfde.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/0*muo16D39A6rFZXRk.jpg"/></div></figure><p id="f9b2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">扩充后，语料库中有 334524 个单词。这意味着新数据是原始数据的两倍。</p><p id="f6e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">创建扩充数据集确实花了不少时间。(大约一小时左右)我确实有。最终文集<a class="ae ne" href="https://drive.google.com/file/d/1g3WkrGCmU5IzMGoIqWMjYRgpoHETbVLl/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">的 txt 文件上传到 google drive </a>。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="fb63" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">LSTM 模型</h1><p id="df8c" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">一个理想的文本生成模型将接受一个种子单词/句子，并给出单词的历史记录<em class="kw"> w0，…，wk </em>，它将预测下一个<em class="kw"> wn+p </em>单词。因为递归神经网络和 LSTMs 具有记忆，所以它们基于先前的状态计算下一个观察值。</p><p id="63ad" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">LSTMs 是特殊的，因为它们有输入、输出和遗忘门以及单元存储器。因此，它们能够在更长的时间间隔内存储信息。这里我用了一个 0.5 辍学和 0.5 <a class="ae ne" href="https://arxiv.org/pdf/1512.05287.pdf" rel="noopener ugc nofollow" target="_blank">经常辍学</a>的 LSTM。</p><p id="e0a9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">目前，有一些非递归模型在使用转换器的语言建模中表现非常好，比如 OpenAIs <a class="ae ne" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 文本生成模型</a>。</p><figure class="nf ng nh ni gt jr"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="49f9" class="ns lf iq bd lg nt nu dn lk nv nw dp lo kj nx ny ls kn nz oa lw kr ob oc ma od bi translated">结果:</h2><blockquote class="nl nm nn"><p id="e55d" class="jy jz kw ka b kb kc kd ke kf kg kh ki no kk kl km np ko kp kq nq ks kt ku kv ij bi translated">结果:</p></blockquote><figure class="nf ng nh ni gt jr gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3233e77f1588dc46cc5f3d22c540020c.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*0wswKkUGlodj0p8L8mTxoA.png"/></div></figure></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="57d7" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">使用 OpenAI 的 GPT-2 进行微调</h1><p id="1b65" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">我使用了<a class="ae ne" href="https://github.com/minimaxir/gpt-2-simple" rel="noopener ugc nofollow" target="_blank">GPT _ 2 _ simple</a>——“一个简单的 Python 包，它包装了用于<a class="ae ne" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>的<a class="ae ne" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 文本生成模式</a> l 的现有模型微调和生成脚本”</p><p id="6546" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将带有 BERTAug 和 FasttextAug 的原始数据集合并并导出为文本以形成<em class="kw"> am_corpus.txt </em></p><figure class="nf ng nh ni gt jr"><div class="bz fp l di"><div class="nj nk l"/></div></figure><blockquote class="nl nm nn"><p id="317c" class="jy jz kw ka b kb kc kd ke kf kg kh ki no kk kl km np ko kp kq nq ks kt ku kv ij bi translated">带有随机前缀的结果:</p></blockquote><div class="nf ng nh ni gt ab cb"><figure class="of jr og oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/dba35a24425bfc919e518412e323e0ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*zcJFShR2OyVLWnqNBvalAw.png"/></div></figure><figure class="of jr ol oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/c688066d46804c476b61209e0a1edc9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*ziJzKe9bPAn_M-Jh0XJmIA.png"/></div></figure><figure class="of jr om oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/d243a3a5c30c6a8a42491e3a6b443fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*-sOVvzA0LGD-DnrgvGxjSA.png"/></div><figcaption class="on oo gj gh gi op oq bd b be z dk or di os ot">Generated Lyrics</figcaption></figure></div><p id="80f3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">为了评估这个结果，我使用了<a class="ae ne" href="https://www.aclweb.org/anthology/W04-1013" rel="noopener ugc nofollow" target="_blank">胭脂</a>。它代表面向回忆的替角，用于 Gisting 评估。我发现——<a class="ae ne" href="http://What Is ROUGE And How It Works For Evaluation Of Summarization Tasks?" rel="noopener ugc nofollow" target="_blank">什么是 ROUGE，它是如何对摘要任务进行评估的？</a> —对了解胭脂真的很有帮助。</p><div class="nf ng nh ni gt ab cb"><figure class="of jr ou oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/ddb69832bfe5784c4207c02def99fe64.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*3_0n_iMrLn0BdkuVHFClyg.png"/></div></figure><figure class="of jr ov oh oi oj ok paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><img src="../Images/69e4a82d03cef865fd5ca63291e9f08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*8U055kIV2NsJt_CpM7cbBg.png"/></div></figure></div><p id="bf63" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">本质上，GPT-2 模型生成的歌词比 LSTM 模型生成的歌词更有意义！虽然公平地说，LSTM 模型没有得到一个战斗的机会，只有 5 个纪元的训练。</p></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><ul class=""><li id="3378" class="mq mr iq ka b kb kc kf kg kj ms kn mt kr mu kv mv mw mx my bi translated"><a class="ae ne" href="https://drive.google.com/file/d/1ZY66gnohTa6mYJZNnVGLELIVKPz47RCt/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">谷歌 Colab 笔记本</a>为歌词生成器。</li></ul></div><div class="ab cl kx ky hu kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="ij ik il im in"><h1 id="151c" class="le lf iq bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">参考</h1><ul class=""><li id="7dae" class="mq mr iq ka b kb mc kf md kj ow kn ox kr oy kv mv mw mx my bi translated"><a class="ae ne" rel="noopener" target="_blank" href="/data-augmentation-library-for-text-9661736b13ff">文本</a>的数据扩充库<a class="ae ne" href="https://towardsdatascience.com/@makcedward" rel="noopener" target="_blank">马志威</a></li><li id="9c98" class="mq mr iq ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated"><a class="ae ne" href="http://jdaytn.com/posts/download-blink-182-data/" rel="noopener ugc nofollow" target="_blank">是什么让一些 blink-182 歌曲比其他歌曲更受欢迎？第 1 部分</a>乔纳森·戴顿</li><li id="61ba" class="mq mr iq ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated">OpenAI 的<a class="ae ne" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 文本生成模型</a>。</li><li id="d5c7" class="mq mr iq ka b kb mz kf na kj nb kn nc kr nd kv mv mw mx my bi translated"><a class="ae ne" href="https://arxiv.org/pdf/1512.05287.pdf" rel="noopener ugc nofollow" target="_blank">递归神经网络中基于理论的辍学应用</a></li></ul><p id="f444" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">编辑:添加评估指标和更多结果。</strong></p></div></div>    
</body>
</html>