<html>
<head>
<title>Gliding into Model-Based</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">滑向基于模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gliding-into-model-based-94aaf077d381?source=collection_archive---------21-----------------------#2019-05-02">https://towardsdatascience.com/gliding-into-model-based-94aaf077d381?source=collection_archive---------21-----------------------#2019-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dd5a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">逻辑直观的解释</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/51ebb6cd593db70449735f7006b711d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeYYt1UdvAwuSsH5yOXqkg.jpeg"/></div></div></figure><p id="ec3e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">由于术语和复杂的数学公式，强化学习(RL)对于该领域的新手来说可能是一个令人畏惧的领域。然而，其背后的原理比最初想象的更直观。让我们想象 RL 是一个新的尚未发行的塞尔达游戏，发生在遥远的未来，2119 年。</p><p id="8c04" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">林克在 2119 年的任务是从里顿豪斯那里拯救人类，这是一个位于 2019 年的秘密机构，拥有强大的人工智能和时间机器。因为里顿豪斯是邪恶的，他们设计了一个在 2119 年毁灭人类的计划，并及时派遣他们的特工去完成任务。</p><p id="0e55" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">游戏开始时，林克降落在一个荒凉的小岛上，他必须在那里找到人类文明的第一个迹象，并警告他们里顿豪斯的特工即将到来。当他降落在一座塔的顶部时，一位年长的智者出现并送给他一个滑翔伞作为礼物。林克现在的任务是滑翔伞，并找到一个人类文明的城镇，以警告他们里顿豪斯。</p><p id="5d7a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">林克是一个电脑外星人，可以在他的大脑中安装任何软件。他不知道如何玩滑翔伞。在 Ritten-House 的代理到达那里之前，你的目标是编写一个程序来教 Link 滑翔伞和所有的技巧，他尽可能快地到达人类文明，并尽可能避免 Ritten-House。你的计划是在里顿豪斯的特工来阻止他之前把这个模拟发送给林克，这样他就会知道如何保护自己。</p><p id="7694" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">你的计划是在未来通过光波向你发送这个程序。</p><p id="0a4d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了设计这个程序，你需要知道一种叫做强化学习的东西。在强化学习中，有一个代理正在与环境交互。</p><p id="63de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们基于称为<a class="ae lo" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">马尔可夫决策过程</a> (MDP)的数学框架，对代理与环境的交互进行建模。每个代理从状态“X”开始，在每个时间步采取一个动作“A”，得到奖励“R”，并进入下一个状态“Xt+1”，重复这个循环，直到代理达到目标状态“X`”。</p><p id="94c2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在，在你正在制作的模拟中，Link 作为滑翔伞是一个在天空中处于初始状态 X 的代理。在每个时间步，Link 从一组可能的动作中采取一个动作。在这里，向左或向右操纵他的风筝被认为是他可能的行动，他去了天空中一个新的状态或新的地方。他的目标是降落在目标州 x `，这是未来的几个州，在那里他可以找到人类文明。</p><p id="532d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在每一个时间步，基于他采取的每一个动作，他将在空间中处于不同的位置。例如，如果他把他的风筝转向右边，他会在一个不同的地方，当他把他的风筝转向左边。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi lp"><img src="../Images/f1b2226dbee7cc4d7684bd6b3617b2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T01dG80td92timev"/></div></div></figure><p id="7bb3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">然而，并非所有这些可能的行动都是同样有利的。他的目标是找到一系列最佳行动来实现他的目标。</p><p id="de7d" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这意味着你想让林克选择一条最有效的路线去接近人类。为了做到这一点，我们最好使用基于模型的方法。在基于模型的方法中，为了采取最优的行动，Link 还需要预测理想的未来状态，因此可以选择到达那里的最佳路线。</p><p id="1150" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">当 Link 在滑翔伞飞行时，他想知道他应该如何操纵他的风筝以便不坠落(即，找到最佳动作)。但是，他也想避开敌人并降落在最佳位置，如城镇所在的位置(即，预测下一个最佳状态)。</p><p id="46d5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">对未来状态的预测以及在哪里着陆才是正确的，这反过来会影响林克在当前时刻如何操纵他的风筝。你想根据他对未来状态的预测找到最佳行动。</p><p id="c50c" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">林克可以预测敌人会在某个地点，为了避开敌人，他应该把风筝转向其他方向。</p><p id="d68b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">对未来状态的预测解释了环境是如何变化的。环境相对于当前状态和动作的这种变化被描述为一个函数，我们将这个函数称为<em class="lq">模型</em>。</p><p id="46b1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">你的目标是教林克学习这个模型。</p><p id="71ec" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">模型的输入是当前状态<strong class="ku ir"> <em class="lq"> x </em> </strong>和动作<strong class="ku ir"> <em class="lq"> u </em> </strong>、<strong class="ku ir"> <em class="lq"> </em> </strong>，目标是预测未来状态<strong class="ku ir"> <em class="lq"> x t+1 </em> </strong>。我们可以这样写:x t+1 =f(xt，ut)</p><p id="9c81" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们称选择行动顺序直到剧集结束的过程为<em class="lq">策略</em>。直觉上，政策意味着林克如何在每一个时间点选择最佳方式来驾驶他的风筝，直到他最终到达城镇拯救人类。我们可以用下面的符号来描述策略:ut= лθ(xt)。这意味着在每个状态 xt，策略лθ告诉 Link 最佳动作 ut 是什么。</p><p id="481b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">成本或奖励函数</strong></p><p id="1f8a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们使用成本函数或回报函数来寻找最优策略，或者换句话说，轨迹上的最优行动。根据不同的设置，我们使用成本或回报函数。</p><p id="73f9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">请注意，奖励只是成本函数的负值。我们试图最小化成本或最大化回报。在这个设置中，我们使用一个成本函数 c(xt，ut)。</p><p id="a287" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">寻找最佳行动与预测未来状态有什么关系？换句话说，最优策略与模型有什么关系？</p><p id="8df3" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">在回答这个问题之前，我想让你想象一个世界，在这个世界里，这些功能中只有一个起作用。</p><p id="86c2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">你认为会发生什么？</p><p id="bcdf" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">第一种情况，模型能够预测下一个状态，但不能采取好的行动。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/e547851e7787baa927e0fe63a2ba63ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/0*57lv3s1YwCx61x2I"/></div></figure><p id="483a" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这里绿色是最佳的未来状态，模型有绿色和红色的<strong class="ku ir">预测</strong>。</p><p id="1bf1" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这意味着，即使林克能够预测未来状态，他知道敌人在哪里，但他不能根据他对未来状态的了解采取好的行动。他不知道如何驾驶他的滑翔伞，并且向里顿豪斯坠落或消失，因为他不知道如何驾驶他的风筝。</p><p id="013e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">现在，想象相反的场景，林克能够采取好的行动，并且是滑翔伞专家，但是他不能预测他的行动将带他去哪里。他知道如何驾驶他的风筝，但是他不知道去哪里。这也可能让他陷入困境。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/ca0167adeef72d20604e59ba5591b46a.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/0*Q9jcW3Ay0-QOEzNL"/></div></figure><p id="01ca" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这意味着他对未来没有任何预测。他可能会花很多时间根据他当前的情况和需要采取行动，而不是根据对未来状态的预测，也可能会随机搜索整个区域，直到他找到有人居住的城镇。这类似于许多无模型环境。预测使林克成为一个聪明的人，他能预测自己行为的后果，并根据这些预测做出决定。这也是理性的人类会做的。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi lt"><img src="../Images/2d7bd538ed63bb135ec1ab19bb211fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gN4pWPe5V_yKMKuQ"/></div></div></figure><p id="fd66" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">再次以滑翔伞为例，林克能够在给定的时刻控制他的风筝不落下，但是他不记得敌人在哪里，结果，他不断地遇到他们，结果，他不清楚环境如何变化，他不能确切地知道哪个方向会把他带到城镇。</p><p id="0964" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">林克不仅需要知道如何驾驶他的风筝，还需要预测他应该去哪里，并根据他的预测驾驶他的风筝。现在，你可能对这两个函数为什么相关有了更好的直觉。</p><p id="1282" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">让我们从数学角度更详细地阐述这一点。</strong></p><p id="0c62" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这是总损失的计算公式，我们正努力使其最小化。注意，这个损失由两个函数组成。</p><p id="5372" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">1.代价函数:c(xt，ut)。</p><p id="6fb5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">2.，我们的模型~转移函数:f(xt-1，ut-1)</p><p id="46eb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">代替 c(xt，ut)中的 xt，我们可以把 x 写成它之前状态和它动作的转移函数。c(f(xt-1，ut-1)，ut)。我们可以将上述公式改写如下:</p><p id="94b5" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因为我们有一系列的步骤，所以我们试图将每一步的成本降到最低。因此，总损失是每个时间步的成本总和。</p><p id="d830" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这直观地意味着，林克想要在任何时刻采取好的行动，而且是针对整个领域，直到他接触到人类。</p><p id="bd92" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">该模型的承诺基于我们如何基于转移函数 f(xt，ut)找到预测未来状态 x t+1 的最优策略лθ之间的关系。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi lu"><img src="../Images/0f8ddf4e92e0b42a1859364a9a5a7ef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4z2__8hEzFpo2Nh9"/></div></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/81ee0b28c5154c8a8343796e41045acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/0*-fOSDVbyCBrOXMA5"/></div></figure><p id="f4a2" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">因此，现在我们要优化整个序列的总损失函数，包括两个函数:每个时间步的成本函数和转移函数。我们希望找到一种优化技术，使这两个函数的损失最小化。我们可以使用线性或非线性优化。我们可以使用神经网络来最小化总成本函数。</p><p id="757b" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">如果我们的环境是可区分的，我们可以通过反向传播进行优化。为了计算总损失，我们需要下列导数:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/e0ce2890820b5f36b3897e876789b158.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/0*1lr5pG63rG-JMgkM"/></div></figure><p id="372e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这部分就是我们所说的模型。我们想了解关于状态和动作的模型:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/fbc2f29e64362b1bbeb0a5609b6bd8d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/0*Dcf-2zFY_mvNfQAJ"/></div></figure><p id="ccc7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这部分是我们的成本。我们想了解状态和行为的成本:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/259aff2494927b3d0f4e9b1176c81145.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/0*AJbyrL91_UMxLQFj"/></div></figure><p id="c0b9" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">世界模型</p><p id="3171" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">到目前为止，您已经了解了基于模型的方法，我们可以使用基于模型的方法之一——世界模型——作为 Link 模拟软件背后的核心机制。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi lz"><img src="../Images/3fd0fed78b13609a32cd1ede629d1fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NznuP9ZEjg7I4pVc"/></div></div></figure><p id="1476" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">这里，MD-RNN 类似于用于预测未来状态的转移函数 x t+1 =f(xt，ut)。然而，它只是与我们所学的略有不同。MD-RNN 增加了自己的隐藏状态来预测未来状态和 ht+1。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ma"><img src="../Images/946c78daa2227fc8d3e18aaf3171cda8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P1uz4h8tvYzySFQ2"/></div></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mb"><img src="../Images/f5600f961f290d512181dc69728966a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9u0cahrYAvg0_0no"/></div></div></figure><p id="f80e" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">你可能还记得，之前我们讨论过根据设置，我们使用成本或回报函数。世界模型使用奖励而不是成本函数。这里，控制器网络充当奖励函数，其目标是找到在整个推广过程中最大化累积奖励的策略。控制器的输入是 zt 和 ht，输出是 at 的最优动作。ht 是控制器网络用来预测最佳动作的附加变量。我鼓励你自己阅读《世界模型》。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mc"><img src="../Images/728dea4288461c631018a677dd8b9ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uvveX-g5jfSsxIr6"/></div></div></figure><p id="0fe7" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">谢谢你救了林克·❤</p></div></div>    
</body>
</html>