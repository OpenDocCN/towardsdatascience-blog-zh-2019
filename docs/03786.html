<html>
<head>
<title>Reinforcement Learning — Solving Blackjack</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习—解决 21 点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f?source=collection_archive---------6-----------------------#2019-06-15">https://towardsdatascience.com/reinforcement-learning-solving-blackjack-5e31a7fb371f?source=collection_archive---------6-----------------------#2019-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d288" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用 Q-learning 实现 21 点</h2></div><p id="8ee6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们已经讨论了如何使用蒙特卡罗方法来评估强化学习中的策略<a class="ae lb" rel="noopener" target="_blank" href="/monte-carlo-methods-estimate-blackjack-policy-fcc89df7f029">在这里</a>，我们以 21 点为例，设定了一个固定的策略，通过重复采样，我们能够获得策略和状态的无偏估计，以及沿途的值对。下一个直接的想法是，我们能否通过使用价值迭代来解决 21 点问题，我们在前面的文章中已经介绍过了。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/238c12b39178d92baa699b8e3c4ef556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jn4rzHGFg_rqm1K-V4wnag.jpeg"/></div></div></figure><p id="d7be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">答案听起来是肯定的，因为我们可以清楚地定义 21 点的三个主要组成部分<code class="fe lv lw lx ly b">(state, action, reward</code>，并且我们也知道对手的策略，这也将被视为环境的一部分，所以这个过程将类似于我们在<a class="ae lb" rel="noopener" target="_blank" href="/reinforcement-learning-implement-tictactoe-189582bea542">井字游戏实现</a>中讨论过的，我们站在 1 个玩家的立场上，对手的相应动作将被视为环境的反馈(这样我们就不需要对手的任何模型，这是强化学习的优势之一)。</p><h1 id="af28" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">二十一点规则</h1><p id="5ee3" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">简单回顾一下<a class="ae lb" href="https://en.wikipedia.org/wiki/Blackjack" rel="noopener ugc nofollow" target="_blank">21 点</a>规则和庄家采取的一般策略:</p><p id="639b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mw">游戏从发给庄家和玩家的两张牌开始。庄家的一张牌面朝上，另一张面朝下。如果玩家立即有 21(一张 a 和一张 10 的牌)，这叫自然牌。然后他赢了，除非庄家也有一个自然牌，在这种情况下，游戏是平局。如果玩家没有自然牌，那么他可以一张接一张地要求额外的牌(命中)，直到他停止(坚持)或超过 21(破产)。如果他破产了，他就输了；如果他坚持，那么就轮到庄家了。庄家根据固定的策略不加选择地打或坚持:他坚持任何 17 或更大的和，否则就打。</em>  <em class="mw">如果庄家破产，那么玩家获胜；否则，结果——赢、输或平——取决于谁的最终总和更接近 21。如果玩家拿着一张可以算作 11 而不会破产的王牌，那么这张王牌就可以用了。</em></p><h1 id="cd90" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">履行</h1><p id="358a" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">先明确一下<strong class="kh ir">状态，动作，奖励</strong>。<strong class="kh ir">游戏的状态是决定和影响胜算的因素</strong>。首先，最重要的是卡的金额，目前手头的价值。其次，还有两个因素有助于赢得游戏，这是我们在上面的规则介绍中描述的，是可用的 ace 和庄家的扑克牌。因此<strong class="kh ir">状态将有 3 个组成部分:玩家当前的牌总数、可用的 ace 和庄家的出牌</strong>。<strong class="kh ir">动作</strong>清晰，因为在 21 点中一个人只能有 2 个动作，要么<strong class="kh ir">击中，要么</strong>站立。奖励将基于游戏的结果，赢了给 1，平了给 0，输了给-1。</p><p id="4c19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我已经谈到了 21 点上的 MC 方法，在下面的部分中，我将介绍两者实现的主要差异，并尝试使代码更加简洁。(<a class="ae lb" href="https://github.com/MJeremy2017/RL/blob/master/BlackJack/blackjack_solution.py" rel="noopener ugc nofollow" target="_blank">全码</a>)</p><h2 id="7d88" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">初始化</h2><p id="9af5" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在 init 函数中，我们定义了将在下面的函数中频繁使用或更新的全局值。与我们的玩家遵循固定策略的 MC 实现相反，这里我们控制的玩家不使用固定策略，因此我们需要更多组件来更新其 Q 值估计。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="5a9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个<code class="fe lv lw lx ly b">init</code>函数中定义的组件通常用于强化学习问题的大多数情况。与 MC 方法中的<code class="fe lv lw lx ly b">init</code>函数相比，增加的部分包括<code class="fe lv lw lx ly b">self.player_Q_Values</code>，它是<code class="fe lv lw lx ly b">(state, action)</code>的初始化估计，每集之后会更新，<code class="fe lv lw lx ly b">self.lr</code>，它用于控制更新速度，以及<code class="fe lv lw lx ly b">self.exp</code>，它用于采取行动。</p><h2 id="658f" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">交易卡和经销商政策</h2><p id="432f" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated"><code class="fe lv lw lx ly b">giveCard</code>和<code class="fe lv lw lx ly b">dealerPolicy</code>功能完全相同。因为我们的对手，庄家，在这场游戏中仍然持有相同的策略，我们正在探索一种可以与庄家的策略一样有竞争力甚至更好的策略。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nj nk l"/></div></figure><h2 id="c2da" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">行动选择</h2><p id="8c98" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">这一次我们的玩家不再遵循固定的政策，所以它需要考虑在平衡探索和开发方面采取什么行动。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="fba0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的玩家有两个动作要做，其中 0 代表站立，1 代表击中。当当前牌的和等于或小于 11 时，一个人将总是击中，因为击中另一张牌没有害处。当牌的总数超过 11 时，我们的玩家将采取ϵ-greedy 策略，其中<code class="fe lv lw lx ly b">exp_rate</code>百分比的时间，它采取随机行动，否则采取贪婪行动，这是基于 q 值的当前估计获得最多奖励的行动。</p><h2 id="5769" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">判断下一个状态</h2><p id="51d6" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">通过采取一个动作，我们的玩家从当前状态移动到下一个状态，因此<code class="fe lv lw lx ly b">playerNxtState</code>函数将<strong class="kh ir">采取一个动作并输出下一个状态，并判断是否游戏结束</strong>。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="d29a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了进入下一个状态，函数需要知道当前的状态。它在开始时通过将当前状态分配给固定变量来实现这一点。<em class="mw">下面的逻辑是如果我们的动作是 1，代表 HIT，我们的玩家再抽一张牌，根据抽的牌是不是 ace，相应的加上当前的牌和。另一方面，如果动作是 STAND，游戏马上结束，返回当前状态。值得注意的是，在函数的最后我们增加了另一个部分，根据玩家手上是否有可用的 ace 来判断游戏是否结束。</em></p><h2 id="07c1" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">给予奖励并更新 Q 值</h2><p id="1fae" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">像所有的强化学习更新一样，在游戏结束时(这被认为是一集),我们的玩家会收到基于自己和庄家的牌值的奖励，并将该值向后传播以更新对<code class="fe lv lw lx ly b">(state, action)</code>的估计。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nj nk l"/></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi nl"><img src="../Images/d63d3b23c912123c472a47bda9ef5ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmQXi79_h7oK-Lf_6QniMQ.png"/></div></div><figcaption class="nm nn gj gh gi no np bd b be z dk">Q-value update</figcaption></figure><p id="fb59" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个功能可以合并成一个，我把它们分开是为了在结构上更清晰。<code class="fe lv lw lx ly b">winner</code>函数判断游戏的获胜者并相应返回奖励，<code class="fe lv lw lx ly b">_giveCredit</code>函数根据上面的公式更新奖励，这与我们在<a class="ae lb" rel="noopener" target="_blank" href="/implement-grid-world-with-q-learning-51151747b455">网格世界 Q-learning </a>中介绍的完全相同，<strong class="kh ir">Q 值以相反的方式更新，而最后更新的值将用于更新当前的 Q 值</strong>。</p><h2 id="0890" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">培养</h2><p id="80c2" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在培训阶段，我们将模拟许多游戏，让我们的玩家与庄家对打，以更新 Q 值。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nj nk l"/></div></figure><p id="4372" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与 21 点的 MC 方法不同，在开始时我增加了一个功能<code class="fe lv lw lx ly b">deal2cards</code>,它只是简单地向玩家连续发 2 张牌。原因是遵循规则<strong class="kh ir">如果任一玩家用前 2 张牌得到 21 分，游戏直接结束，而不是继续等待下一个玩家到达其终点</strong>。这避免了这样的情况，一个玩家用前 2 张牌得到 21 分，而另一个玩家也用 2 张以上的牌得到 21 分，但是游戏以平局结束。</p><h1 id="f302" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">玩庄家和结果</h1><p id="a62f" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">有了我们配备了智能的代理，我们就可以让它与庄家对弈了(<em class="mw">保存和加载策略功能与 MC 21 点中的相同，</em> <code class="fe lv lw lx ly b"><em class="mw">playWithDealer</em></code> <em class="mw">功能与训练过程的结构类似，只是将</em> <code class="fe lv lw lx ly b"><em class="mw">exp_rate</em></code> <em class="mw">调至 0 </em>)。</p><p id="6f34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用<code class="fe lv lw lx ly b">exp_rate=0.2</code>和<code class="fe lv lw lx ly b">lr=0.1</code>训练了一万回合后，我保存了策略，让它和庄家打一万回合，得到的结果是:</p><ul class=""><li id="c1bf" class="nq nr iq kh b ki kj kl km ko ns ks nt kw nu la nv nw nx ny bi translated">wining: 4122</li><li id="b5fd" class="nq nr iq kh b ki nz kl oa ko ob ks oc kw od la nv nw nx ny bi translated">图纸:1639 年</li><li id="8cc0" class="nq nr iq kh b ki nz kl oa ko ob ks oc kw od la nv nw nx ny bi translated">失败:4239</li></ul><p id="564b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一项无法超越经销商的政策。肯定存在比 HIT17 执行得更好的策略(事实上，这是一个公开的秘密)，我相信，我们的代理没有学习到最优策略并且执行得同样好的原因是，</p><ol class=""><li id="3a96" class="nq nr iq kh b ki kj kl km ko ns ks nt kw nu la oe nw nx ny bi translated">没有足够的训练</li><li id="b711" class="nq nr iq kh b ki nz kl oa ko ob ks oc kw od la oe nw nx ny bi translated">探索率和学习率的调整(修正它们可能对这种情况不好)</li><li id="6017" class="nq nr iq kh b ki nz kl oa ko ob ks oc kw od la oe nw nx ny bi translated">一步 Q 值更新有其自身的局限性</li></ol><h2 id="c38e" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">你能做什么？</h2><p id="36d6" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我强烈建议你在当前实现的基础上进行更多的尝试，这既有趣又有利于加深你对强化学习的理解。你可以试试:</p><ul class=""><li id="5442" class="nq nr iq kh b ki kj kl km ko ns ks nt kw nu la nv nw nx ny bi translated"><strong class="kh ir"> n 步更新而不是 1 步</strong>(我也会在以后的文章中介绍)</li><li id="8c4f" class="nq nr iq kh b ki nz kl oa ko ob ks oc kw od la nv nw nx ny bi translated"><strong class="kh ir">根据当前保存的策略</strong> <em class="mw">(这是一个有趣的想法，值得一试，因为我们知道 AlphaGo 通过与大师对弈而变得强大，但 AlphaGo Zero 通过学习与自己对弈以 100:0 击败了它。你也可以利用这个想法，试着训练一个代理和自己一起玩)</em></li></ul><p id="14a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请点击查看<a class="ae lb" href="https://github.com/MJeremy2017/RL/blob/master/BlackJack/blackjack_solution.py" rel="noopener ugc nofollow" target="_blank">的完整代码。欢迎您投稿，如果您有任何问题或建议，请在下面发表评论！</a></p><p id="8142" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考</strong></p><p id="099e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]<a class="ae lb" href="http://incompleteideas.net/book/the-book-2nd.html" rel="noopener ugc nofollow" target="_blank">http://incompleteideas.net/book/the-book-2nd.html</a></p></div></div>    
</body>
</html>