# 变分图自动编码器教程

> 原文：<https://towardsdatascience.com/tutorial-on-variational-graph-auto-encoders-da9333281129?source=collection_archive---------4----------------------->

图适用于许多现实世界的数据集，如社会网络、引用网络、化学图等。对图结构数据日益增长的兴趣增加了对图神经网络的研究。

变分自动编码器体现了变分贝叶斯方法在深度学习中的成功，并激发了广泛的研究。变分图自动编码器(VGAE)将 VAE 的思想应用于图结构数据，显著提高了在大量引用网络数据集(如 Cora 和 Citesser)上的预测性能。

我在网上搜了一下，还没有看到关于 VGAE 的详细教程。在这篇文章中，我将简单地谈谈传统的自动编码器和变化的自动编码器。此外，我将讨论将 VAE 应用于图结构数据(VGAE)的想法。

# 传统自动编码器

![](img/9d38e6f67bc1ad756b7e9695155092a5.png)

Figure 1: The architecture of the traditional autoencoder

传统的自动编码器是包含编码器和解码器的神经网络。编码器将数据点 *X* 作为输入，并将其转换为低维表示(嵌入) *Z* 。解码器采用低维表示 *Z* 并返回原始输入 *X-hat* 的重构，看起来像输入 *X* 。嵌入的质量决定输出的质量 *X-hat* 。然而，编码器不可能编码所有的信息，因为嵌入的维数比输入的维数低。因此，如果嵌入从输入中捕获更多的信息，输出将具有更好的性能。

## 编码器和解码器的架构

![](img/5c7a4d4bdedc30c9a7bcef81a137a9f8.png)

Figure 2: Traditional autoencoder for MNIST dataset

让我们看一个例子。图 2 显示了一个自动编码器的例子，它将 MNIST 图像作为输入。MNIST 数据集包含灰度图像，每个图像的形状为 28 x 28 像素。编码器获取该图像并将其转换为低维嵌入 *Z* 。解码器采用嵌入 *Z* 并返回重构的输入图像，该图像是自动编码器的输出。

## 损失函数

自动编码器的损失函数测量重建期间的信息损失。我们希望最小化重建损失，以使 *X-hat* 更接近 *X* 。我们经常使用均方误差作为损失函数，

![](img/f9518505fe119fc0b0bb4507193f47a1.png)

其测量 *X-hat* 与 *X* 的接近程度。

## 为什么我们要将输入转换为低维嵌入？

让我们以图像为例。有几种方法可以使用图像的低维嵌入。与存储图像的像素强度相比，存储图像的低维嵌入可以节省存储空间。存储图像的低维嵌入也可以节省计算能力，因为输入维度更低。

靠近图像嵌入的点也是嵌入空间中的图像嵌入，这是使自动编码器嵌入有用的另一个原因。例如，如果您修改 MNIST 影像的像素强度，您将得到一个看起来不像 MNIST 数据集中的影像的噪声影像。但是，如果您对 MNIST 图像的嵌入进行少量修改，然后对修改后的嵌入进行解码，您仍然可以得到看起来像 MNIST 图像的东西。

![](img/386cd86619290cb49efdae351422f55f.png)

Figure 3: Modify the pixel intensities of an MNIST image

图 3 显示了修改 MNIST 图像的*像素强度*的例子。在每个像素的强度中添加一些随机噪声后，输出的是一幅噪声图像，看起来不像 MNIST 数据集中的图像。

![](img/8903489b69cc5b40d3b9a4311f61e580.png)

Figure 4: Modify the embedding of MNIST image by a small amount

图 4 显示了一个修改 MNIST 图像的*嵌入*的例子。少量修改嵌入与嵌入空间的小位移是一样的。从图 4 中我们可以看到，在嵌入 *Z* 中每个元素加 4 后，输出图像 *X2 帽*仍然与原始输出 *X1 帽*非常相似。

![](img/0f4dc9ccaa631e2286b58ddcc1cf5374.png)

Figure 5: An example of plotting MNIST data on embedding space

我们也可以把这些低维嵌入标绘到 n-d 坐标上。图 5 显示了在嵌入空间上绘制 MNIST 数据的例子。嵌入空间上的每个点代表一个输入数据。这种可视化很方便，因为它可以系统地组织坐标中的所有输入数据。

低维嵌入也可以用于下游的机器学习任务。我们可以使用预先计算的嵌入来解决另一个机器学习问题，即分类问题。我们可以对这些嵌入而不是原始输入进行分类。我们还可以在迁移学习或半监督学习中使用嵌入，这可能会给我们带来比原始输入更好的结果。

# 可变自动编码器

## 为什么我们需要可变的自动编码器？

变分自动编码器的最大优势之一是 VAE 可以从原始源数据集生成新数据。相比之下，传统的自动编码器只能生成与原始输入相似的图像。假设你想建一个长满灌木的花园。每一株灌木都需要与众不同，这样你的花园才会看起来真实。你肯定不能自己画每一棵灌木，一个更聪明的方法是使用自动编码器自动生成新的灌木。

## 主要思想

变分自动编码器的主要思想是将输入 *X* 嵌入到一个分布中，而不是一个点中。然后随机样本 *Z* 取自分布，而不是直接从编码器产生。

## 编码器和解码器的架构

VAE 的编码器通常写成 *qφ(z|x)* ，它取一个数据点 *X* 并产生一个分布。该分布通常被参数化为多元高斯分布。因此，编码器预测高斯分布的平均值和标准偏差。低维嵌入 *Z* 从此分布中采样。解码器是一个变分近似， *pθ(x|z)* ，它采用一个嵌入 *Z* 并产生输出 *X-hat* 。

![](img/ecedea5c9948576c30cde607fe63f089.png)

Figure 6: An example of a variational autoencoder

## 损失函数

VAE 的损失函数有两部分。损失函数的第一部分称为变分下界，它衡量网络重构数据的好坏。如果重建数据 *X* 与原始数据差别很大，那么重建损失将会很高。损失函数的第二部分作为正则项工作。它是近似值与真实后验值( *p(z)* )的 KL 散度，它衡量输出分布( *qφ(z|x)* )与 *p(z)* 的匹配程度。

![](img/8662f5a26ef3453e1db58a11e6f56947.png)

## 摘要

VAE 的想法可以用下图来概括:

![](img/1d74bd1cb631180414dbc9cc03aeec15.png)

Figure 7: The architecture of the variational autoencoder

编码器将数据点 *X* 作为输入，并生成μ和 logσ作为输出。我们使用 logσ而不是σ的原因是σ是非负的，因此我们将需要一个额外的激活函数。但是 logσ可能是正的，也可能是负的。在我们得到μ和 logσ之后，我们试图让μ和 logσ都接近 0，也就是说μ接近 0，σ接近 1。因此，最终分布将接近于 *N(0，1)* 。最后，我们想通过 *z = μ + σ * ε* 由μ和σ生成嵌入 *Z* ，其中ε ~ *N(0，1)* 。这被称为*重新参数化技巧*。现在有了潜在变量 *Z* ，我们可以通过解码器生成我们的输出 *X-hat* 。

# 变分图自动编码器

## 介绍

我们希望构建一个变分图自动编码器，将 VAE 的思想应用于图结构数据。我们希望我们的变分图自动编码器能够生成新的图形或关于图形的推理。然而，我们不能直接应用 VAE 的思想，因为图结构的数据是不规则的。每个图都有不同大小的无序节点，图中的每个节点都有不同数量的邻居，所以我们不能再直接使用卷积了。让我们弄清楚如何以神经网络可以理解的方式来表示图形。

## 邻接矩阵

我们使用一个邻接矩阵 *A* 来表示输入图。通常我们假设邻接矩阵是二进制的。第 *i* 行和第 *j* 列的值 1 表示顶点 *i* 和顶点 *j* 之间有一条边。行 *m* 和列 *n* 的值 0 表示顶点 *m* 和顶点 *n* 之间没有边。

![](img/78beec26c8da8b66378818b847c2ef01.png)

Figure 8: An example of the adjacency matrix

图 8 显示了当输入图是有向和无向时邻接矩阵的例子。在图的左侧，当图是无向图时，邻接矩阵是对称的。顶点 0 和顶点 1 是相连的，所以 *A[0，1] = 1* 和 *A[1，0] = 1* 。当图是有向图时，右边的邻接矩阵是不对称的。顶点 0 指向顶点 1，意味着顶点 1 聚合了来自顶点 0 的信息，所以 *A[1，0] = 1* 。

## 特征矩阵

我们使用特征矩阵 *X* 来表示输入图中每个节点的特征。特征矩阵 *X* 的行 *i* 代表顶点 *i* 的特征嵌入。

![](img/d40a85c477eacaf58d09333be4807a25.png)

Figure 9: An example of the feature matrix

图 9 显示了特征矩阵的一个例子。每个节点 *i* 都有自己的特征矩阵 *X_i* ，我们可以将它们组合在一起得到整个图的特征矩阵 *X* ，其中每一行 *i* 代表节点 *i* 的特征矩阵。

## 编码器和解码器的架构

![](img/54ca6f229aa7ffa38d27a97f91f7bc71.png)

Figure 10: The architecture of the variational graph autoencoder

VGAE 的编码器(推理模型)由[图卷积网络](https://tkipf.github.io/graph-convolutional-networks/) ( *GCNs* )组成。它以邻接矩阵 *A* 和特征矩阵 *X* 作为输入，生成潜变量 *Z* 作为输出。第一 GCN 层生成低维特征矩阵。它被定义为

![](img/6f1c297b7b7d23784223a34cf6d18355.png)![](img/5b5b995641ba77a5a5384f8d053dce4a.png)

*A-tilde* 是对称归一化邻接矩阵。

第二 GCN 层产生μ和 logσ，其中

![](img/9608e6c1fe0f10a574a55f90414bef65.png)![](img/994516f083585b9781f66aef1be923d8.png)

现在如果我们把两层 GCN 的数学结合在一起，我们得到

![](img/7f8e78707f931344d4256a16ce87d1a7.png)

从而产生μ和 logσ。

然后我们可以使用参数化技巧计算出 *Z*

![](img/aac25864ee9e78ed1dc399cddfef2325.png)

其中ε ~ *N(0，1)。*

解码器(生成模型)由潜在变量 *Z* 之间的内积定义。我们解码器的输出是一个重构的邻接矩阵 *A-hat* ，其定义为

![](img/050ca306ad7e011d9aac21122775024e.png)

其中σ()是逻辑 sigmoid 函数。

总之，编码器表示为

![](img/74c8fbb3134790e3c8d140acf1982b00.png)

并且解码器被表示为

![](img/6198d22989902bf104f0b2c7af64587b.png)

## 损失函数

变分图自动编码器的损失函数和以前几乎一样。第一部分是输入邻接矩阵和重构邻接矩阵之间的重构损失。更具体地说，它是目标(A)和输出(A’)逻辑之间的二元交叉熵。第二部分是 *q (Z | X，A)* 与 *p(Z)* 之间的 KL-散度，其中 *p(Z) = N(0，1)* 。它测量我们的 *q (Z | X，A)* 与 *p(Z)* 的匹配程度。

![](img/30d627dd0e1dc2fbd8d06158c10b1227.png)

## 使用内积解码器的优势

在我们得到潜变量 *Z* 之后，我们要想办法学习潜变量中每一行的相似度(因为一行代表一个顶点)来生成输出邻接矩阵。内积可以计算两个向量的余弦相似性，这在我们需要一个对向量大小不变的距离度量时很有用。因此，通过对潜在变量 *Z* 和 *Z^T* 应用内积，我们可以学习 *Z* 内部每个节点的相似度来预测我们的邻接矩阵。

## 链路预测实验

在[变分图自动编码器论文](https://arxiv.org/pdf/1611.07308.pdf)中，作者还创建了一个传统的图自动编码器模型(GAE)作为 VGAE 模型的对比。GAE 模型有一个单 GCN 层作为编码器，直接生成潜变量 *Z* ，还有一个内积解码器，和 VGAE 模型一样。作者在三个数据集(Cora、citeser 和 Pubmed)上测试了这两个模型，VGAE 模型在 Cora 和 citeser 数据集上都获得了更高的预测性能。

![](img/7eeadb33bebfbb26d32a5ea58b5e1e18.png)

# 结论

在这篇文章中，你已经学习了传统自动编码器、变分自动编码器的基本思想，以及如何将 VAE 的思想应用于图结构数据。

图结构数据在当今各个领域发挥着越来越重要的作用。我相信在不久的将来，图结构数据分析将成为机器学习中最受欢迎的主题之一。

# 参考

[1] [自动编码变分贝叶斯](https://arxiv.org/pdf/1312.6114.pdf)

[2] [变分图自动编码器](https://arxiv.org/pdf/1611.07308.pdf)

[3] [图形自动编码器 TensorFlow 实现](https://github.com/tkipf/gae)

[4] [变分自动编码器教程](https://arxiv.org/pdf/1606.05908.pdf)

[5] [库尔贝克-莱布勒散度](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)