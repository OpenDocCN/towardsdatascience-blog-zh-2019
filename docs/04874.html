<html>
<head>
<title>Data Science over the Movies Dataset with Spark, Scala and some SQL. And some Python.(Part 1).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Spark、Scala 和一些 SQL 对电影数据集进行数据科学分析。和一些蟒蛇。(第一部分)。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-over-the-movies-dataset-with-spark-scala-and-some-sql-and-some-python-part-1-f5fd4ee8509e?source=collection_archive---------6-----------------------#2019-07-23">https://towardsdatascience.com/data-science-over-the-movies-dataset-with-spark-scala-and-some-sql-and-some-python-part-1-f5fd4ee8509e?source=collection_archive---------6-----------------------#2019-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9c21" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak"> <em class="kf"> Spark SQL </em> </strong> <em class="kf">是一个用于结构化数据处理的</em> <strong class="ak"> <em class="kf"> Spark </em> </strong> <em class="kf">模块。它提供了一个名为 DataFrames 的编程抽象，也可以作为一个分布式的</em><strong class="ak"><em class="kf">SQL</em></strong><em class="kf">查询引擎。让我们在数据块上使用它来对电影数据集执行查询。</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/7967a46a00b8964eb7899b1a02e9d9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*j02oknA5Ak3mj5w9"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Photo by <a class="ae kw" href="https://unsplash.com/@jakehills?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jake Hills</a> on <a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5b9a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一天，我的一位 SQL 开发人员朋友告诉我，她对大数据处理引擎感兴趣，但除了 SQL 之外，她没有任何其他编程语言的经验。我想向她展示，了解 SQL 是学习大数据的良好起点，因为借助 Spark，可以对表执行平面 SQL 查询，而且其代码 sintax 与 SQL 类似。我希望你喜欢这个故事。</p><p id="71e5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">为了证明这一点，我执行了一些查询和描述性统计，以从一个奇特的数据集<strong class="kz ir">电影镜头数据集</strong>中提取洞察力，该数据集<strong class="kz ir">可在【https://grouplens.org/datasets/movielens/】<a class="ae kw" href="https://grouplens.org/datasets/movielens/" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir"/></a>的</strong>上获得，包含了超过近 3 万部电影的不同用户的大量评分。该报告可能有助于了解如何进行聚合和执行基本的 spark 查询。我既不是 Spark 专家，也不是 Scala 专家，所以代码可能不会以最有效的方式实现，但我不会停止学习，我非常欢迎建议和评论。</p><p id="0e3f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">当我们想要学习大数据并在 Hadoop 生态系统上使用 Scala 的<strong class="kz ir"> Spark 时，我们面临的最大问题之一</strong>总是安装和集成来自这些框架的所有工具。然而，<strong class="kz ir"> Databricks 社区版</strong>将把我们从那个问题中解救出来。实际上，它拥有我们需要使用的一切:它自己的文件系统，以及我们将要使用的所有已安装(并正常工作)的 API(<strong class="kz ir">Hive 与 Spark 和 Scala </strong>以及其余的 Spark 库如 MLlib 或 Graphx 很好地集成)。为了不让这篇文章写得太长，我将不涉及那些技术，但是关于的好文档可以在他们的网站上找到:<a class="ae kw" href="https://databricks.com/spark/about" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir">https://databricks.com/spark/about</strong></a>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/7a3a76ee5b5f05c7e9a619d9c059d0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*2WmsKKnj-conUld-OWJ4Gw.png"/></div></figure><p id="896b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">本文的主题不是数据块的使用，而是电影数据集<strong class="kz ir">(统计、查询、聚合…) </strong>上的<strong class="kz ir"> scala-Spark 编码</strong>。<strong class="kz ir">一些查询将在 SQL 中显示它们的等价物。Databricks 还将允许我们管理这个巨大的数据集，它可能适合我们本地机器的内存。Spark 将为我们提供处理数据的有效方法。</strong></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi lu"><img src="../Images/acd75a79f261d3e2d77985a9751f15df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFGZBsfLFW5qEY_Zqb1FzA.png"/></div></div></figure><h1 id="f4e3" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">1)导入数据</h1><p id="1f01" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">第一步也是必要的一步是下载两个<strong class="kz ir">长格式数据集</strong>，它们在新研究的推荐部分。之后，我们必须<strong class="kz ir">将它们导入 databricks 文件系统，然后将它们加载到 Hive 表</strong>。现在我们可以对两个数据集/表执行一些基本的查询，一个是关于电影的信息，另一个是关于电影的价格。<strong class="kz ir">描述和打印模式方法</strong>总是一个很好的切入点:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="52ee" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">val movies = table("movies")<br/>val ratings = sql("select userId, movieId, rating from ratingsB")<br/>ratings.describe().show<br/>movies.printSchema<br/>ratings.printSchema</strong></span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nc"><img src="../Images/f6e0e63ab8ca846d0e9bd104e081e143.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uTlHVua6Rj9jFMJeGFJomA.png"/></div></div></figure><h1 id="a518" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">2)执行一些查询和知识提取。</h1><p id="9761" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">为了改进我们的 spark 编码，我们可以执行任何我们能想到的带有学习目的的查询。首先，让我们检查对电影评分最多和最少的用户:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="48a0" class="mx lw iq mt b gy my mz l na nb">import org.apache.spark.sql.functions.count<br/>import org.apache.spark.sql.functions.{desc,asc} </span><span id="2df1" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">ratings.groupBy("userId").agg(count("*").alias("number_cnt"))<br/>.orderBy(desc("number_cnt"))</strong> </span><span id="a7f9" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">ratings.groupBy("userId").agg(count("*").alias("number_cnt"))<br/>.orderBy(asc("number_cnt"))</strong></span></pre><p id="fd7a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">等价的 sql 查询(重要的是要注意，只要我们可以，我们应该用 spark 编写这个<strong class="kz ir">查询，因为它会像纯 SQL 查询</strong>一样在编译时给我们带来错误和运行时带来错误<strong class="kz ir">，以避免将来浪费时间，特别是在大型进程上):</strong></p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="d7a7" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">sql("select userId, count(*) from ratingsB group by userId order by count(*) desc")</strong></span></pre><p id="edac" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有一些用户评价了超过 5000 部电影！！这太疯狂了。</p><p id="05fb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这让我想知道他们看电影花了多长时间<strong class="kz ir">考虑到电影 100 分钟的平均时间。</strong>这个结果值得展示！！一些查询将被打包到函数中，以便在我们想要/需要它们时使用，甚至在笔记本的不同单元或部分代码中使用:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="78c8" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">ratings.groupBy("userId").agg(count("*").alias("number_cnt")).withColumn("years_Watching", round($"number_cnt" * 100/ 60 / 24 / 365,3)).orderBy(desc("years_Watching")).show(10)</strong></span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/2ff8cea2efe0d380878ac9a2a01c7d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*SpzK1rrBBPWyMV8iWdNPLQ.png"/></div></figure><p id="b9d3" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以看到，<strong class="kz ir">用户数 123100 花了超过 4.5 年的时间看电影</strong>和<strong class="kz ir">看了超过 20000 部电影</strong>。真是个电影迷！！关于代码，它有一些有用的方法，比如一个适当的 gropuBy agregation 和一个 count，这个 count 和 round 函数一起应用于用。带列(因为我们不需要那么多小数火花显示)。现在，我们将连接两个数据帧，将所有信息放在一个 df 中进行处理:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="27cb" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">val df_full = ratings.join(movies, $"ratingsb.movieId" === $"movies.movieId").drop($"ratingsb.movieId")</strong></span></pre><p id="3c40" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你有兴趣知道哪些电影被影迷评为 3 星或 5 星吗？我们可以知道，这个函数接收一个<strong class="kz ir"> scala 整数序列</strong>作为输入，以及我们想要查询的星星数和我们要查询的用户数。如果 seq 为空，将显示该用户评价的所有电影(使用 scala <strong class="kz ir">检查)。isEmpty </strong>方法):</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="56bc" class="mx lw iq mt b gy my mz l na nb">import org.apache.spark.sql.functions.{asc, desc}</span><span id="815c" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">def movie_query (df: org.apache.spark.sql.DataFrame, stars: Seq[Double], user: Int ) : Unit = <br/>{<br/> if (stars.isEmpty)<br/> {<br/> println(s"All movies rated by user $user")<br/> df.filter(df("userId") ===user).orderBy(desc("rating")).show(20, false)<br/> }<br/> else <br/> stars.foreach(starsNum =&gt;{ <br/> println(s"All movies rated by user $user with $starsNum stars:")<br/> df.filter(df("userId") === user).filter(df("rating") === starsNum).orderBy(asc("movieId")).show(7, false)})<br/>}</strong> </span><span id="6fe9" class="mx lw iq mt b gy nd mz l na nb">movie_query(df_full, Seq((3.0, 4.5), 21)</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nf"><img src="../Images/cf9a84faf29aae095ae6409aa474234d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvdmiXYaCVyWd7FuLHHfBA.png"/></div></div></figure><p id="f6e7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">通常，我们都会从朋友和家人那里得到很多电影推荐，我们会把这些电影列一个清单。在那之后，当我犹豫应该先看哪部电影时，我总是去 filmaffinity 或 imdb 查看我列表中评价最好的电影。原来我们也可以做类似的事情。要做到这一点，我们需要获得电影的平均评级，并按照从最好到最差的评级对它们进行检索。值得注意的是，一个优秀的电影搜索者可能不会收到电影的确切名称，这可以用 scala contains 方法解决:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="fc4d" class="mx lw iq mt b gy my mz l na nb">import org.apache.spark.sql.functions._</span><span id="5670" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">def movie_grade (df: org.apache.spark.sql.DataFrame, movie: String ) : Unit = df.filter((df("title").contains(movie))).groupBy("title").agg((round(avg($"rating"),3)).as("averageStars")).orderBy(desc("averageStars")).show(false)</strong></span></pre><p id="7e0d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将在两个著名的传奇故事上测试我们的功能:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="adac" class="mx lw iq mt b gy my mz l na nb">movie_grade(df_full, "Wild Bunch")<br/>movie_grade(df_full, "Lord of the Rings")</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ng"><img src="../Images/8c5f03baf8d6676d643c40811e081215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FgOseMSm1IGAjs5t8O3qdw.png"/></div></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nh"><img src="../Images/4ae688a55068d5055c018e95173d7eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uvAnn-WZquqZCsL3"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Photo by <a class="ae kw" href="https://unsplash.com/@thomasschweighofer_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Thomas Schweighofer</a> on <a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="23b0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">耶！我们爱魔戒！评分最低的电影对应的是我还没看过的动画版。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ni"><img src="../Images/5dcd003f4e2dc0d0c33263f2bf93e90b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*756PdnKSaD6kOZEtEssfsQ.jpeg"/></div></div></figure><p id="9664" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">阿拉贡和两个哈比人在指环王动画电影中。</p><p id="85a2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我相信也有很多星战迷，所以让我们用类似的 SQL 查询得到所有电影的分数:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="4afa" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">sql("Select title, avg(rating) as averageRate from full_movies where title like '%Star Wars%' group by title order by averageRate desc").show(false)</strong></span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nj"><img src="../Images/0fc0545316dcef62cbd5786b8d7a2290.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hgHRcYnrbFlZYsXXJoZULw.png"/></div></div></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nk"><img src="../Images/d5d2cc781789e08a968f28eaf73d235c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bvUkij67ouJW5hNZ"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Photo by <a class="ae kw" href="https://unsplash.com/@danielkcheung?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Daniel Cheung</a> on <a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2719" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">正如我们所看到的，老电影是最好的评级，但在我看来，前 3 集并没有那么糟糕，得分低于 3.5 星。但那只是观点！！我们还获得了一些衍生产品和平行电影，对《星球大战》迷来说也很有趣。</p><p id="27fd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我想知道的下一件事是哪些是收视率最高和最低的电影。由于这是一个相当大的数据集，它包含了大量可视化效果差的电影，其中一些电影的评分低于 5 分。当然，如果我们想做一个连贯的研究，我们应该忽略那些电影。我只会关注那些收视率超过 600 的电影，这代表了总收视率的 1%(完全不是主流电影)。为此，我们将创建一个新的数据框架，其中包含每部电影的总评分和平均评分。之后，我们将筛选出超过 600 个评分的电影，并按其平均评分以升序和降序的方式显示，以检查最差和最好的电影。</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="5dac" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">val df_count = df_full.groupBy("title").agg(avg("rating").as("averageRating"), <br/>count("title").as("RatesCount"))</strong> </span><span id="9af6" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">df_count.withColumn("roundedAverageRating", round(df_count("averageRating"),2)).filter(df_count("RatesCount") &gt; 600).orderBy(asc("averageRating")).drop("averageRating").show(20, false)</strong> </span><span id="519b" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">df_count.withColumn("roundedAverageRating", round(df_count("averageRating"),2)).filter(df_count("RatesCount") &gt; 600).orderBy(desc("averageRating")).drop("averageRating").show(20, false)</strong></span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nl"><img src="../Images/6b44ea9a8e9b1bf41f8159159717d4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SasEZzAaqEYXXqET4ZHQWg.png"/></div></div></figure><p id="c0ec" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">收视率超过 600 的排名最低的电影。</p><p id="5ddb" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如你所见,《闪光》是最差的电影。我在 imbd 上搜索过，确实看起来很恐怖:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f30ff3a328c1f31b17900808e16d040c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*Alb6YvTbJF8f6r6xKft26g.png"/></div></figure><p id="e9b6" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在好的部分来了，注意列表的头:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nn"><img src="../Images/99dc7e13cbd420fc280fb64ca30b7136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GI1-KEa2tYMM9kjpVkGBFA.png"/></div></div></figure><p id="8300" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这也是其在 imdb 上评级的结果:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi no"><img src="../Images/37d6b2706d36ce4b71c32276c8daeb99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*ZyCnRnENEnbQOpZGoZbpVQ.png"/></div></figure><h1 id="057d" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">3)推荐系统(一)</h1><p id="b457" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">这个数据集对于改进和理解推荐系统是如何工作的是完美的。我正在使用 spark 机器学习库<strong class="kz ir"> MLlib </strong>研究推荐系统的机器学习模型，它们将在下一篇文章中展示。我刚刚提出了一个简单的解决方案，只需要执行一个大的查询。这是一个简单的模型，可以作为虚拟解决方案。解决方案是关于<strong class="kz ir">检索用户观看最多的类别，并从用户尚未观看的类别中输出评价最高的电影。</strong>这是一个打包到<strong class="kz ir"> easy_recommender </strong>函数中的更复杂的查询，该函数将接收我们想要推荐的用户数量、我们将对一部电影使用的作为最小阈值的数量或比率(请记住，从统计的角度来看<strong class="kz ir">不是检索只有一点比率的电影的好做法</strong>)以及我们想要显示的电影数量。这些是我们将要遵循的步骤:</p><ol class=""><li id="ed59" class="np nq iq kz b la lb ld le lg nr lk ns lo nt ls nu nv nw nx bi translated">获取用户 X 看到最多的类别。要做到这一点，我们需要根据用户进行过滤，根据类型进行分组，进行计数，然后根据计数进行排序。一旦有了这些，我们只需选择“流派”列并用 map 映射该列，然后执行一个收集操作，然后使用:。<strong class="kz ir">map(r =&gt;r . getstring(0)). collect . to list .</strong></li><li id="c089" class="np nq iq kz b la ny ld nz lg oa lk ob lo oc ls nu nv nw nx bi translated">之后，我们将使用一个新列“ToDelete”来标记用户 X 看过的电影，该列包含一个简单的字符串，例如“Delete ”,一旦我们执行连接，就可以很容易地找到它。这样我们就可以很好地识别用户 X 看过的电影。</li><li id="fd88" class="np nq iq kz b la ny ld nz lg oa lk ob lo oc ls nu nv nw nx bi translated">我们将把那个数据帧和大数据帧连接起来。我们想忽略的电影标有“删除”(所以我们会用<strong class="kz ir">过滤掉那一栏为空的。过滤器(" ToDelete 为空")</strong>)。</li><li id="5b33" class="np nq iq kz b la ny ld nz lg oa lk ob lo oc ls nu nv nw nx bi translated">最后，我们将使用 foreach scala 方法遍历我们想要过滤的类别，现在我们已经选择了用户 X 没有看过的电影。现在，我们只需按标题分组，获得平均评分，再一次按电影筛选出 Y 个以上的评分(记住，出于统计原因)，并按平均评分降序排列。</li></ol><p id="ca61" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是一个有点复杂的过程，我相信有更好的方法来做这件事。在这里你可以看到代码:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="7b16" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">def easy_recommender(nUser: Int, nRates: Int, nMovies: Int) : Unit = {<br/> val mostSeenGenresList = df_full.filter(df_full("userId") === nUser).groupBy("genres").agg(count("*").alias("cuenta")).orderBy(desc("cuenta")).limit(3).select("genres").map(r =&gt; r.getString(0)).collect.toList<br/> println(s"List of genres user $nUser has seen the most : $mostSeenGenresList")</strong></span><span id="1092" class="mx lw iq mt b gy nd mz l na nb"> <strong class="mt ir"> val movies_watched_by_userX = df_full.filter($"userId" === nUser).withColumn("ToDelete", lit("DELETE")).select($"ToDelete", $"title".as("title2"))</strong></span><span id="2583" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir"> var df_filt = df_full.join(movies_watched_by_userX, $"title" === $"title2", "left_outer")</strong></span><span id="0e41" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir"> df_filt = df_filt.filter("ToDelete is null").select($"title", $"rating", $"genres")</strong> <strong class="mt ir"> </strong></span><span id="fe56" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">mostSeenGenresList.foreach(e =&gt; {<br/> println(s"Top $nMovies movies user number $nUser has not seen from category $e with more than $nRates rates: ")</strong></span><span id="674b" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir"> df_filt.filter($"genres" === e).groupBy("title").agg(avg("rating").as("avgByRating"), count("*").alias("nRates")).filter($"nRates" &gt; nRates).orderBy(desc("avgByRating")).show(nMovies, false)<br/> })<br/>}</strong> </span><span id="d36c" class="mx lw iq mt b gy nd mz l na nb">easy_recommender(134596, 1000, 5)</span></pre><p id="d883" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们从“推荐系统”获得的结果用 scala <strong class="kz ir">字符串插值:</strong>打印出来</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi od"><img src="../Images/aff06d652ba35c4775a3bc7aeb3f9419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PPQr3f4RtNEYsdPvgcpM2w.png"/></div></div></figure><p id="bdb2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">你可以看到，首先我们打印出用户 X 看到最多的类别。我们可以通过<strong class="kz ir">控制要推荐的类别数量。</strong>极限③法。正如你所看到的，我们可以通过函数的输入来控制我们想要包含在模型中的大多数参数。</p><h1 id="4df7" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">4)可视化时刻</h1><p id="c371" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">总之，如果没有适当的可视化/绘图，这<strong class="kz ir">就不是一部好的数据科学作品。为了实现这一点，Python 总是一个很好的选择，我将向你展示 databricks 的另一个精彩特性，它允许我们在同一个笔记本上运行<strong class="kz ir"> Python 和 Scala 代码。</strong></strong></p><p id="0804" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">第一步是将我们的 df_full 保存到一个<strong class="kz ir">临时配置单元表</strong>(它只在当前会话期间持续存在):</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="199d" class="mx lw iq mt b gy my mz l na nb"><strong class="mt ir">val ratesDF = df_full.select("rating").groupBy("rating").agg(count("rating").as("NumberOfRates")).orderBy(asc("rating"))<br/></strong> <br/><strong class="mt ir">ratesDF.createOrReplaceTempView("ratedDF")<br/>spark.sql("select * from ratedDF").show</strong></span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e6d410ef726f924b0e53970b73310b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*Y7uOtb5npd8t858Skchw3Q.png"/></div></figure><p id="635a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在是魔法降临的时刻了。只需在单元格顶部键入<strong class="kz ir"> %python </strong>，我们就可以执行具有所有 Spark (pyspark)优势和特性的 python 代码。我们<strong class="kz ir">将表加载到 pyspark 数据帧中，并使用这些<strong class="kz ir"> Python 命令行程序</strong>将两列转换成 Python 列表</strong>:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="f0b1" class="mx lw iq mt b gy my mz l na nb">%python</span><span id="ef6e" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">df = table("ratedDF")<br/>rating_list = df.select("rating").collect()<br/>number_rates = df.select("NumberOfRates").collect()<br/>rate_category = [float(row.rating) for row in rating_list]<br/>n_rates = [int(row.NumberOfRates) for row in number_rates]</strong></span></pre><p id="faa9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后，我们将使我们的视觉化。这是一个完全自由式的过程，一些情节会比其他的更漂亮。记住<strong class="kz ir">分类变量</strong>应该用<strong class="kz ir">条形图</strong>表示:</p><pre class="kh ki kj kk gt ms mt mu mv aw mw bi"><span id="ad33" class="mx lw iq mt b gy my mz l na nb">%python<br/>import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="c893" class="mx lw iq mt b gy nd mz l na nb"><strong class="mt ir">fig, ax = plt.subplots()<br/>ax.bar(rate_category, n_rates, align='center', width=0.4, facecolor='b', edgecolor='b', linewidth=3, alpha=.3)<br/>plt.title('Number of Rates vs Stars')<br/>plt.xlabel('Stars (*)')<br/>plt.xlim(0,5.5)<br/>plt.ylim(0,8000000)<br/>ax.set_xticks(rate_category)<br/>display(fig)</strong> </span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi of"><img src="../Images/70ee0db6f14fb11f9e16ab84afcfd250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZUoHAiL-McD5FnxTxV8UA.png"/></div></div></figure><p id="c972" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">除了<strong class="kz ir">没有 0 星的电影</strong>以及<strong class="kz ir">看起来不像人们预期的那样是正态分布</strong>之外，没有太多评论。</p><h1 id="7ce7" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">5)总结</h1><p id="87d7" class="pw-post-body-paragraph kx ky iq kz b la mn jr lc ld mo ju lf lg mp li lj lk mq lm ln lo mr lq lr ls ij bi translated">这就是所有的人。我希望你和我一样喜欢这篇文章，了解 scala、spark 和 Databricks，思考关于电影数据集的见解。现在我正在使用 spark 的机器学习库<strong class="kz ir"> Spark MLlib 在这个数据集上实现和改进推荐系统的性能</strong>。在未来的文章中，这些模型可能会与数据集上更复杂的查询和描述性统计一起显示。我玩了一会儿流派专栏，获得了更深入的统计数据，但我不希望这篇文章过于密集。</p><p id="2e3d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">由于这是我的第一篇文章，再次感谢任何反馈和评论。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi og"><img src="../Images/147a51f7232b6d0e4dfd8312295af91b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*anVZ9F1qMM59U2-J"/></div></div><figcaption class="ks kt gj gh gi ku kv bd b be z dk">Photo by <a class="ae kw" href="https://unsplash.com/@gwenong?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Gwen Ong</a> on <a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl oh oi hu oj" role="separator"><span class="ok bw bk ol om on"/><span class="ok bw bk ol om on"/><span class="ok bw bk ol om"/></div><div class="ij ik il im in"><p id="b028" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="oo">原载于 2019 年 7 月 23 日</em><a class="ae kw" href="https://medium.com/@borjagg042/some-data-science-on-the-movies-dataset-with-spark-scala-and-some-sql-and-some-python-part-1-b52ec62b04c1" rel="noopener"><em class="oo">https://medium.com</em></a><em class="oo">。</em></p></div></div>    
</body>
</html>