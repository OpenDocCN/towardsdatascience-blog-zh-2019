<html>
<head>
<title>The Approximation Power of Neural Networks (with Python codes)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的逼近能力(使用 Python 代码)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58?source=collection_archive---------15-----------------------#2019-01-07">https://towardsdatascience.com/the-approximation-power-of-neural-networks-with-python-codes-ddfc250bdb58?source=collection_archive---------15-----------------------#2019-01-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e650934ca6a58d7a9601be703807545d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*382Mhd7Xry6tJ7SIpx0naw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Image by <a class="ae jg" href="https://pixabay.com/fr/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2926087" rel="noopener ugc nofollow" target="_blank">Gerd Altmann</a> from <a class="ae jg" href="https://pixabay.com/fr/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2926087" rel="noopener ugc nofollow" target="_blank">Pixabay</a>.</figcaption></figure><div class=""/><div class=""><h2 id="f069" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">为什么神经网络可以预测(几乎)任何过程的结果</h2></div><h1 id="e7ee" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">介绍</h1><p id="f03f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">众所周知，神经网络可以逼近任何连续数学函数的输出，不管它有多复杂。以下面的函数为例:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/e21b766426ca4dbc84dd598e30995898.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*mV98Hjz5qQ5OGn0LbWkgVA@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Generic function (generated using <a class="ae jg" href="https://www.wolfram.com/mathematica/" rel="noopener ugc nofollow" target="_blank">Mathematica</a>).</figcaption></figure><p id="79fd" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">尽管它的形状非常复杂，我们稍后将讨论的定理保证了我们可以建立某种神经网络，它可以像我们想要的那样精确地逼近<em class="mw"> f(x) </em>。因此，神经网络显示了一种普遍的行为。</p><p id="23de" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">神经网络受到如此多关注的原因之一是，除了这些相当显著的通用属性，<a class="ae jg" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">它们还拥有许多强大的学习功能算法</a>。</p><h1 id="85d3" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">普遍性和数学基础</h1><p id="2566" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这篇文章将给出人工神经网络这些近似能力背后的一些基本数学结果(定理)的非正式概述。</p><blockquote class="mx"><p id="94a9" class="my mz jj bd na nb nc nd ne nf ng ml dk translated">“几乎任何你能想象到的过程都可以被认为是函数计算……[例子包括]根据一段音乐的简短样本命名一段音乐[…]，将一段中文文本翻译成英文[…]，获取一个 mp4 电影文件并生成电影情节的描述，以及对表演质量的讨论。”</p><p id="fa00" class="my mz jj bd na nb nc nd ne nf ng ml dk translated">—迈克尔·尼尔森</p></blockquote><h1 id="5123" class="ky kz jj bd la lb lc ld le lf lg lh li kp nh kq lk ks ni kt lm kv nj kw lo lp bi translated">使用神经网络作为逼近器的动机:Kolmogorov 定理</h1><p id="df98" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">1957 年，俄罗斯数学家<a class="ae jg" href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov" rel="noopener ugc nofollow" target="_blank">安德雷·柯尔莫哥洛夫</a>，<a class="ae jg" href="https://link.springer.com/chapter/10.1007/978-94-011-3030-1_56" rel="noopener ugc nofollow" target="_blank">证明了</a>一个关于多变量实函数表示的重要定理，他因在广泛的数学课题(如概率论、拓扑学、湍流、计算复杂性等)上的贡献而闻名。根据 Kolmogorov 定理，多元函数可以通过(有限个)一元函数的和与合成的组合来表示。</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/16aba77f17c0566dead5a744c278daec.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*QaPof9Po6bD9S2onTnwbZg.jpeg"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">The Russian mathematician Andrey Kolmogorov (Wikipedia)</figcaption></figure><p id="4830" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">稍微正式一点，该定理陈述了在<em class="mw"> n </em>维超立方体(其中<em class="mw"> n </em> ≥ 2)中定义的实变量的连续函数<em class="mw"> f </em>可以表示如下:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/aa05ba0a4024bc8fe756a22b1d556ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W-wYyeJdSo3M0K_7J-CAjA@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Kolmogorov’s Theorem (1957)</figcaption></figure><p id="e068" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">在这个表达式中，<em class="mw"> g </em> s 是一元函数，ϕs 是连续的、完全(单调)递增的函数(如下图所示)，不依赖于<em class="mw"> f. </em>的选择</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/fba666084955ec6100a7a3e112e3443d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*ursqbQ8rIECr3v9J0w08wQ@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Examples of monotonous functions (Wikipedia).</figcaption></figure><h1 id="f37c" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">通用逼近定理(UAT)</h1><p id="7287" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">UAT 指出，包含具有有限数量节点的单个隐藏层的前馈神经网络可以用于逼近任何连续函数，只要满足关于激活函数形式的相当温和的假设。现在，由于我们能够想象的几乎任何过程都可以用某种数学函数来描述，神经网络至少在原则上可以预测几乎每个过程的结果。</p><p id="f23a" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">使用不同激活函数的前馈人工神经网络的通用性有几种严格的证明。为了简洁起见，我们只讨论 sigmoid 函数。Sigmoids 是“S”形的，包括作为特例的<a class="ae jg" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">逻辑函数</a>、<a class="ae jg" href="https://en.wikipedia.org/wiki/Gompertz_curve" rel="noopener ugc nofollow" target="_blank"> Gompertz 曲线</a>和<a class="ae jg" href="https://en.wikipedia.org/wiki/Ogee_curve" rel="noopener ugc nofollow" target="_blank"> ogee 曲线</a>。</p><h1 id="294a" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">乙状结肠的 Python 代码</h1><p id="654d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">构建和绘制 sigmoid 函数的快速 Python 代码如下:</p><pre class="mn mo mp mq gt nn no np nq aw nr bi"><span id="0e63" class="ns kz jj no b gy nt nu l nv nw">import numpy as np<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="7839" class="ns kz jj no b gy nx nu l nv nw">upper, lower = 6, -6<br/>num = 100</span><span id="1556" class="ns kz jj no b gy nx nu l nv nw">def sigmoid_activation(x):<br/>    if x &gt; upper:<br/>        return 1<br/>    elif x &lt; lower:<br/>        return 0<br/>    return 1/(1+np.exp(-x))</span><span id="2914" class="ns kz jj no b gy nx nu l nv nw">vals = [sigmoid_activation(x) for <br/>       x in np.linspace(lower, upper, num=num)]<br/>plt.plot(np.linspace(lower, <br/>                     upper, <br/>                     num=num), vals);<br/>plt.title('Sigmoid');</span></pre><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/d6550c42aac0cfd8bd655854870212c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fIu1vrSy1A2LcksP0TQBA@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">A plot of the sigmoid function. In the code, the conditionals inside the function are included to avoid problematic computations at large values.</figcaption></figure><h1 id="f55b" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">乔治·西本科的证明</h1><p id="20dd" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">Cybenko (1989) 给出的证明以其优雅、简单和简洁而闻名。在他的文章中，他证明了下面的说法。设ϕ是 sigmoid 型的任何连续函数(见上面的讨论)。给定任何多元连续函数</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/eabe9f69486b3a3fff0436c3ee9ece8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*7vjwpMEHV_NHe-dXukGeHg@2x.png"/></div></figure><p id="d978" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">在<em class="mw"> N </em>维实空间的<a class="ae jg" href="https://en.wikipedia.org/wiki/Compact_space" rel="noopener ugc nofollow" target="_blank">紧</a>子集和任何正ϵ内，都有向量</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/797b685b04d935a214454619b170f3ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*SBO8hvfAmDb-LCSVeDfAhQ@2x.png"/></div></figure><p id="ee67" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">(权重<strong class="ls jk">、常数</strong></p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e005abf92d96c062fbebca611fb33e22.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*jHEIq6F99q-BDVAjQP2cgQ@2x.png"/></div></figure><p id="9eb3" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">(<strong class="ls jk">偏置</strong>术语)和</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/4af7a23ca5232ae0bb4a067f8fe0c7b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*Fzfl6O2Cw-vtusPkUGLt-w@2x.png"/></div></figure><p id="46a3" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">到这样的程度</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bd9a34a2a99d02d9269cbfd0a55b9843.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*_j7-MnmiFRMGHqWb6sfC9w@2x.png"/></div></figure><p id="42ee" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">对于紧凑子集内的任何<em class="mw">x</em>(NN 输入)，其中函数<em class="mw"> G </em>由下式给出:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/8dd26cd6390638ffba0572f749a348d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*532teQE_TPHTpAE12g7bEQ@2x.png"/></div></figure><p id="0437" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">选择适当的参数，神经网络可以用来表示各种不同形式的函数。</p><h1 id="858d" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">使用 Python 的示例</h1><p id="3ec6" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">为了使这些陈述不那么抽象，让我们构建一个简单的 Python 代码来说明到目前为止所讨论的内容。以下分析基于迈克尔·尼尔森(Michael Nielsen)的<a class="ae jg" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">伟大的在线书籍</a>，以及<a class="ae jg" href="http://argmatt.com" rel="noopener ugc nofollow" target="_blank">马特·布雷姆斯</a>和<a class="ae jg" href="http://linkedin.com/in/pounders" rel="noopener ugc nofollow" target="_blank">贾斯汀·庞德斯</a>在<a class="ae jg" href="https://generalassemb.ly/education/data-science-immersive" rel="noopener ugc nofollow" target="_blank">大会数据科学沉浸式(DSI) </a>上的精彩演讲。</p><p id="fc8c" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">我们将构建一个神经网络来近似以下简单函数:</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/33acb70e3672381b54e5c86fea8e2c2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ZMMDlwskCPrQK2EBqo5oQ@2x.png"/></div></div></figure><p id="75f8" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">在深入研究代码之前，需要做一些说明:</p><ul class=""><li id="4e2d" class="og oh jj ls b lt mr lw ms lz oi md oj mh ok ml ol om on oo bi translated">为了使分析更加直观，我将使用 sigmoid 函数的简单极限情况。当重量非常大时，s 形接近<a class="ae jg" href="https://en.m.wikipedia.org/wiki/Heaviside_step_function" rel="noopener ugc nofollow" target="_blank">亥维赛阶梯函数</a>。因为我们将需要添加来自几个神经元的贡献，所以使用阶跃函数比一般的 sigmoids 更方便。</li></ul><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/5ddb87cb50224cd6960fb37b508283c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*2Aep1Y5QPjsWJEFZwF3UiQ@2x.png"/></div></figure><ul class=""><li id="687f" class="og oh jj ls b lt mr lw ms lz oi md oj mh ok ml ol om on oo bi translated">在 sigmoid 逼近阶跃函数的极限中，我们只需要一个参数来描述它，即阶跃发生的点。<a class="ae jg" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">s 的值可以表示为</a>等于<em class="mw"> s=-b/w </em>其中<em class="mw"> b </em>和<em class="mw"> w </em>分别是神经元的偏差和权重。</li><li id="5b13" class="og oh jj ls b lt oq lw or lz os md ot mh ou ml ol om on oo bi translated">我要建立的神经网络将是一个非常简单的网络，有一个输入、一个输出和一个隐藏层。如果两个隐藏神经元对应的权值绝对值相等，符号相反，则它们的输出变成一个“<a class="ae jg" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">凸起</a>”，其高度等于权值的绝对值，宽度等于每个神经元的<em class="mw"> s </em>值之差(见下图)。</li></ul><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/1bd1a84340bf4b76175af620a236bd24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LSoLmRpuMeFOCpiuajQ7Wg@2x.png"/></div></div></figure><ul class=""><li id="4781" class="og oh jj ls b lt mr lw ms lz oi md oj mh ok ml ol om on oo bi translated">我们使用下面的<a class="ae jg" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">符号</a>，因为权重的绝对值是凸起的<strong class="ls jk">h</strong>8。</li></ul><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/41dba4df2654ae1613aa63c982bfc6e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*xAIjKDUX78BYMDoG7fWT7A@2x.png"/></div></figure><ul class=""><li id="1250" class="og oh jj ls b lt mr lw ms lz oi md oj mh ok ml ol om on oo bi translated">因为我们想要近似<em class="mw"> g，</em>隐藏层的加权输出必须包含 sigmoid 的逆。事实上，它必须等于:</li></ul><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/f334cb0af5250761d5b2bc2d73069f64.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*nBUaXSdijDbNavFjyZ_jdA@2x.png"/></div></figure><p id="c87a" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">为了重现该函数，我们选择<em class="mw"> h </em> s 的值为(见下图):</p><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/87b5950b68de4fd561195d7d97e13508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5nHiXlPDz0NPa5G8PQcHpg@2x.png"/></div></div></figure><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/829f6821508a0b55d80473475873945c.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*cbE8OFO3zckMJ2qZm-J3rw@2x.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Taken from <a class="ae jg" href="http://neuralnetworksanddeeplearning.com/chap4.html" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><h1 id="63de" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">代码</h1><p id="c09d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">代码以下列定义开始:</p><ul class=""><li id="01fc" class="og oh jj ls b lt mr lw ms lz oi md oj mh ok ml ol om on oo bi translated">我们首先导入需要构建反 sigmoid 函数的<code class="fe pa pb pc no b">inversefunc</code></li><li id="4e30" class="og oh jj ls b lt oq lw or lz os md ot mh ou ml ol om on oo bi translated">然后我们选择一个非常大的权重给 sigmoid，使其类似于<a class="ae jg" href="https://en.m.wikipedia.org/wiki/Heaviside_step_function" rel="noopener ugc nofollow" target="_blank"> Heaviside </a>函数(正如刚刚讨论的)。</li><li id="9df4" class="og oh jj ls b lt oq lw or lz os md ot mh ou ml ol om on oo bi translated">我们选择输出激活作为身份函数<code class="fe pa pb pc no b">identify_activation</code></li><li id="e970" class="og oh jj ls b lt oq lw or lz os md ot mh ou ml ol om on oo bi translated">该函数的作用是从<em class="mw"> s </em>和<em class="mw"> w </em>中恢复原来的<em class="mw"> (w，b) </em>参数化(记得 s 是步进位置)。</li><li id="0128" class="og oh jj ls b lt oq lw or lz os md ot mh ou ml ol om on oo bi translated">架构设置好了，所有的<em class="mw"> w </em> s 和<em class="mw"> b </em> s 都选择好了。数组<code class="fe pa pb pc no b">weight_outputs</code>的元素是从上一节给出的输出权重值中获得的。</li></ul><pre class="mn mo mp mq gt nn no np nq aw nr bi"><span id="d018" class="ns kz jj no b gy nt nu l nv nw">from pynverse import inversefunc</span><span id="f606" class="ns kz jj no b gy nx nu l nv nw">w = 500</span><span id="abd4" class="ns kz jj no b gy nx nu l nv nw">def identity_activation(x):<br/>    return(x)</span><span id="16fe" class="ns kz jj no b gy nx nu l nv nw">def solve_for_bias(s, w=w):<br/>    return(-w * s)</span><span id="5332" class="ns kz jj no b gy nx nu l nv nw">steps = [0,.2,.2,.4,.4,.6,.6,.8,.8,1]</span><span id="f0c4" class="ns kz jj no b gy nx nu l nv nw">bias_hl = np.array([solve_for_bias(s) for s in steps])<br/>weights_hl = np.array([w] * len(steps))<br/>bias_output = 0<br/>weights_output =np.array([-1.2, 1.2, -1.6, 1.6, <br/>                          -.3, .3, -1])<br/>                          1, 1, 1, -1])</span></pre><p id="65ab" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">最后的步骤是:</p><ul class=""><li id="429a" class="og oh jj ls b lt mr lw ms lz oi md oj mh ok ml ol om on oo bi translated">编写一个我称之为<code class="fe pa pb pc no b">nn</code>的<code class="fe pa pb pc no b">Python</code>函数来构建和运行网络</li><li id="cfa4" class="og oh jj ls b lt oq lw or lz os md ot mh ou ml ol om on oo bi translated">打印出近似值和实际函数之间的比较。</li></ul><pre class="mn mo mp mq gt nn no np nq aw nr bi"><span id="3187" class="ns kz jj no b gy nt nu l nv nw">def nn(input_value):<br/>    <br/>    Z_hl = input_value * weights_hl + bias_hl<br/>    activation_hl = np.array([sigmoid_activation(Z) <br/>                              for Z in Z_hl])</span><span id="5153" class="ns kz jj no b gy nx nu l nv nw">Z_output = np.sum(activation_hl * weights_output) <br/>               + bias_output</span><span id="84ff" class="ns kz jj no b gy nx nu l nv nw">activation_output = identity_activation(Z_output) <br/>    <br/>    return activation_output</span><span id="d331" class="ns kz jj no b gy nx nu l nv nw">x_values = np.linspace(0,1,1000)<br/>y_hat = [nn(x) for x in x_values]</span><span id="4879" class="ns kz jj no b gy nx nu l nv nw">def f(x):<br/>    return 0.2 + 0.4*(x**2) + 0.3*x*np.sin(15*x)+ 0.05*np.cos(50*x))</span><span id="4078" class="ns kz jj no b gy nx nu l nv nw">y = [f(x) for x in x_values]</span><span id="5230" class="ns kz jj no b gy nx nu l nv nw">inv_sigmoid = inversefunc(sigmoid_activation)</span><span id="c8d7" class="ns kz jj no b gy nx nu l nv nw">y_hat = [nn(x) for x in x_values]<br/>y_invsig = [inv_sigmoid(i) for i in y]<br/>_ = plt.plot(x_values, y_invsig)<br/>_ = plt.plot(x_values, y_hat)<br/>_ = plt.xlim((0,1))</span></pre><figure class="mn mo mp mq gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pd"><img src="../Images/56bfdceee160fa5b155231ff5cf2d8bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YryNBGz5VYBOQ-2oZqesUA@2x.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Approximating the weighted output of the hidden layer using sigmoid functions with very large weights (such that the sigmoids approach step functions).</figcaption></figure><p id="f036" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">这个近似值远非理想。然而，是否可以直接改进它，例如，通过增加节点数或层数(但同时避免过度拟合)。</p><h1 id="9e8d" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结论</h1><p id="0e19" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本文中，我描述了神经网络通用属性背后的一些基本数学，并展示了一个简单的 Python 代码，它实现了一个简单函数的近似。</p><p id="e370" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">尽管文章中已经包含了完整的代码，但我的 Github 和我的个人网站 www.marcotavora.me(希望)还有一些关于数据科学和物理学的有趣内容。</p><p id="fb97" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">感谢您的阅读，再见！</p><p id="82b1" class="pw-post-body-paragraph lq lr jj ls b lt mr kk lv lw ms kn ly lz mt mb mc md mu mf mg mh mv mj mk ml im bi translated">顺便说一下，建设性的批评和反馈总是受欢迎的！</p></div></div>    
</body>
</html>