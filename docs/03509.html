<html>
<head>
<title>Analyzing Text Classification Techniques on Youtube Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Youtube 数据上的文本分类技术分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analyzing-text-classification-techniques-on-youtube-data-7af578449f58?source=collection_archive---------13-----------------------#2019-06-04">https://towardsdatascience.com/analyzing-text-classification-techniques-on-youtube-data-7af578449f58?source=collection_archive---------13-----------------------#2019-06-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/6bd4d33dc1efdef72b539f23e86ea714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6nPN6naiH7lApcvg.png"/></div></figure><p id="d950" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">文本分类是自然语言处理所要解决的一个经典问题，它指的是分析原始文本的内容并确定其所属的类别。这类似于有人读了罗宾·夏尔马的书，把它归类为‘垃圾’。它有广泛的应用，如情感分析，主题标记，垃圾邮件检测和意图检测。</p><p id="28bd" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">今天，我们将进行一项相当简单的任务，使用不同的技术(朴素贝叶斯、支持向量机、Adaboost 和 LSTM)并分析其性能，根据视频的标题和描述将视频分类为不同的类别。这些类别被选择为(但不限于):</p><ul class=""><li id="da1f" class="ks kt iq jw b jx jy kb kc kf ku kj kv kn kw kr kx ky kz la bi translated">旅游博客</li><li id="ccef" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">科学与技术</li><li id="42cd" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">食物</li><li id="09d0" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">制造业</li><li id="2da7" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">历史</li><li id="c31e" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">艺术和音乐</li></ul><p id="1b40" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">事不宜迟，就像一个刚刚开始园艺的中年父亲会说，“让我们把手弄脏吧！”。</p><h2 id="30d7" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">收集数据</h2><p id="acb3" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">当处理像这样的定制机器学习问题时，我发现收集自己的数据非常有用，如果不是简单地令人满意的话。对于这个问题，我需要一些关于属于不同类别的视频的元数据。如果你有点笨，我欢迎你手动收集数据并构建数据集。然而我不是，所以我会用<strong class="jw ir"> Youtube API v3 </strong>。它是由谷歌自己创建的，通过一段专门为我们这样的程序员编写的代码与 Youtube 进行交互。前往<a class="ae me" href="https://console.developers.google.com/" rel="noopener ugc nofollow" target="_blank">谷歌开发者控制台</a>，创建一个示例项目并开始。我选择这样做的原因是，我需要收集成千上万的样本，我发现使用任何其他技术都不可能做到这一点。</p><blockquote class="mf mg mh"><p id="ceb7" class="ju jv mi jw b jx jy jz ka kb kc kd ke mj kg kh ki mk kk kl km ml ko kp kq kr ij bi translated">注意:Youtube API，像谷歌提供的任何其他 API 一样，基于配额系统工作。根据你的计划，每封邮件每天/每月都有固定的配额。在我的免费计划中，我只能向 Youtube 发出大约 2000 次请求，这造成了一点问题，但我通过使用多个电子邮件帐户克服了这个问题。</p></blockquote><p id="8a25" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">API 的文档非常简单，在使用了超过 8 个电子邮件帐户来补偿所需的配额后，我收集了以下数据并将其存储在一个. csv 文件中。如果你想在你的项目中使用这个数据集，你可以在这里  <strong class="jw ir">下载<a class="ae me" href="https://github.com/agrawal-rohit/Text-Classification-Analysis/blob/master/Collected_data_raw.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">。</strong></a></strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi mm"><img src="../Images/2348a7684a9d52f78022d87fb05eb326.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5uN2M0dLrizHWDyf-u7wVw.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Collected Raw Data</figcaption></figure><blockquote class="mf mg mh"><p id="0a30" class="ju jv mi jw b jx jy jz ka kb kc kd ke mj kg kh ki mk kk kl km ml ko kp kq kr ij bi translated">注意:你可以自由探索一种被称为<strong class="jw ir">网络抓取</strong>的技术，它被用来从网站中提取数据。Python 有一个名为<em class="iq"> BeautifulSoup </em>的漂亮库用于同样的目的。然而，我发现在从 Youtube 搜索结果中抓取数据的情况下，对于一个搜索查询，它只返回 25 个结果。这对我来说是一个障碍，因为我需要大量的样本来创建一个准确的模型，而这并不能解决问题。</p></blockquote><h2 id="86e1" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">数据清理和预处理</h2><p id="ee71" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">我的数据预处理过程的第一步是处理丢失的数据。因为丢失的值应该是文本数据，所以没有办法估算它们，因此唯一的选择是删除它们。幸运的是，在总共 9999 个样本中，只存在 334 个缺失值，因此它不会影响训练期间的模型性能。</p><p id="bd8b" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">“视频 Id”列对于我们的预测分析并不真正有用，因此它不会被选为最终训练集的一部分，所以我们没有任何预处理步骤。</p><p id="3368" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这里有 2 个重要的栏目，即— <strong class="jw ir">标题</strong>和<strong class="jw ir">描述</strong>，但它们都是未经加工的原文。因此，为了过滤掉噪声，我们将遵循一种非常常见的方法来清理这两列的文本。这种方法分为以下几个步骤:</p><ol class=""><li id="49cc" class="ks kt iq jw b jx jy kb kc kf ku kj kv kn kw kr mz ky kz la bi translated"><strong class="jw ir">转换成小写:</strong>执行这一步是因为大写对单词的语义重要性没有影响。“travel”和“Travel”应该被同等对待。</li><li id="d1ec" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated"><strong class="jw ir">删除数值和标点:</strong>数值和标点中使用的特殊字符($，！等等。)无助于确定正确的类别</li><li id="31e1" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated"><strong class="jw ir">删除多余的空格:</strong>这样每个单词都由一个空格分隔，否则在标记化过程中可能会出现问题</li><li id="b53b" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated"><strong class="jw ir">记号化成单词:</strong>这指的是将一个文本串分割成一个“记号”列表，其中每个记号是一个单词。例如，句子“我有巨大的二头肌”将转换为[“我”、“有”、“巨大”、“二头肌”]。</li><li id="bfbb" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated"><strong class="jw ir">去除非字母单词和“停用词”:</strong>“停用词”指的是 and、the、is 等单词，这些单词在学习如何构造句子时很重要，但对我们的预测分析没有用处。</li><li id="2b52" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated"><strong class="jw ir">词汇化:</strong>词汇化是一种非常棒的技术，可以将相似的单词转换成它们的基本含义。例如，单词“flying”和“flyed”都将被转换成它们最简单的意思“fly”。</li></ol><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="gh gi na"><img src="../Images/1501a1837e0890acb8bfe1b6c09e5063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JqGVBG9gdeArpQzXxy-weQ.png"/></div></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Dataset after text cleaning</figcaption></figure><p id="c04f" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">“现在文字干净了，万岁！让我们开一瓶香槟庆祝吧！”</p><p id="f22c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">不，还没有。即使今天的计算机可以解决世界问题，玩超逼真的视频游戏，但它们仍然是不懂我们语言的机器。因此，我们无法将文本数据原样提供给我们的机器学习模型，无论它有多干净。因此，我们需要将它们转换成基于数字的特征，这样计算机就可以构建一个数学模型作为解决方案。这就构成了<strong class="jw ir">数据预处理步骤</strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/14873348b6e7a5bcdbb82339cc2b0dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*FtqdScCp3aClB1u8-CHAUw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">Category column after LabelEncoding</figcaption></figure><p id="4b26" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">由于输出变量(' Category ')也是分类的，我们需要将每个类编码为一个数字。这叫做<em class="mi">标签编码</em>。</p><p id="da66" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">最后，让我们关注一下每个样本的主要信息—原始文本数据。为了从文本中提取数据作为特征并以数字格式表示它们，一种非常常见的方法是<strong class="jw ir">将它们矢量化</strong>。Scikit-learn 库包含用于此目的的“TF-IDF 矢量器”。<strong class="jw ir"> TF-IDF </strong>(词频-逆文档频率)计算每个词在多个文档内部和跨文档的频率，以识别每个词的重要性。</p><h2 id="f679" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">数据分析和特征探索</h2><p id="0ba2" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">作为一个额外的步骤，我决定显示类的分布，以便检查样本数量的不平衡。</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/1f0b1edad5ef35040ea103120ffc2d25.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*FPJ6qrcBXJVZrkg0Wqh4Cg.png"/></div></figure><p id="e5d7" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">此外，我想检查使用 TF-IDF 矢量化提取的特征是否有意义，因此我决定使用标题和描述特征为每个类找到最相关的单字和双字。</p><pre class="mn mo mp mq gt nd ne nf ng aw nh bi"><span id="14a7" class="lg lh iq ne b gy ni nj l nk nl"># USING TITLE FEATURES</span><span id="5723" class="lg lh iq ne b gy nm nj l nk nl"># 'art and music':<br/>Most correlated unigrams:<br/>------------------------------<br/>. paint<br/>. official<br/>. music<br/>. art<br/>. theatre<br/>Most correlated bigrams:<br/>------------------------------<br/>. capitol theatre<br/>. musical theatre<br/>. work theatre<br/>. official music<br/>. music video</span><span id="5ad8" class="lg lh iq ne b gy nm nj l nk nl"># 'food':<br/>Most correlated unigrams:<br/>------------------------------<br/>. foods<br/>. eat<br/>. snack<br/>. cook<br/>. food<br/>Most correlated bigrams:<br/>------------------------------<br/>. healthy snack<br/>. snack amp<br/>. taste test<br/>. kid try<br/>. street food</span><span id="5acf" class="lg lh iq ne b gy nm nj l nk nl"># 'history':<br/>Most correlated unigrams:<br/>------------------------------<br/>. discoveries<br/>. archaeological<br/>. archaeology<br/>. history<br/>. anthropology<br/>Most correlated bigrams:<br/>------------------------------<br/>. history channel<br/>. rap battle<br/>. epic rap<br/>. battle history<br/>. archaeological discoveries</span><span id="2c32" class="lg lh iq ne b gy nm nj l nk nl"># 'manufacturing':<br/>Most correlated unigrams:<br/>------------------------------<br/>. business<br/>. printer<br/>. process<br/>. print<br/>. manufacture<br/>Most correlated bigrams:<br/>------------------------------<br/>. manufacture plant<br/>. lean manufacture<br/>. additive manufacture<br/>. manufacture business<br/>. manufacture process</span><span id="e6c9" class="lg lh iq ne b gy nm nj l nk nl"># 'science and technology':<br/>Most correlated unigrams:<br/>------------------------------<br/>. compute<br/>. computers<br/>. science<br/>. computer<br/>. technology<br/>Most correlated bigrams:<br/>------------------------------<br/>. science amp<br/>. amp technology<br/>. primitive technology<br/>. computer science<br/>. science technology</span><span id="6ec3" class="lg lh iq ne b gy nm nj l nk nl"># 'travel':<br/>Most correlated unigrams:<br/>------------------------------<br/>. blogger<br/>. vlog<br/>. travellers<br/>. blog<br/>. travel<br/>Most correlated bigrams:<br/>------------------------------<br/>. viewfinder travel<br/>. travel blogger<br/>. tip travel<br/>. travel vlog<br/>. travel blog</span><span id="5496" class="lg lh iq ne b gy nm nj l nk nl"># USING DESCRIPTION FEATURES</span><span id="4a19" class="lg lh iq ne b gy nm nj l nk nl"># 'art and music':<br/>Most correlated unigrams:<br/>------------------------------<br/>. official<br/>. paint<br/>. music<br/>. art<br/>. theatre<br/>Most correlated bigrams:<br/>------------------------------<br/>. capitol theatre<br/>. click listen<br/>. production connexion<br/>. official music<br/>. music video</span><span id="e535" class="lg lh iq ne b gy nm nj l nk nl"># 'food':<br/>Most correlated unigrams:<br/>------------------------------<br/>. foods<br/>. eat<br/>. snack<br/>. cook<br/>. food<br/>Most correlated bigrams:<br/>------------------------------<br/>. special offer<br/>. hiho special<br/>. come play<br/>. sponsor series<br/>. street food</span><span id="4b78" class="lg lh iq ne b gy nm nj l nk nl"># 'history':<br/>Most correlated unigrams:<br/>------------------------------<br/>. discoveries<br/>. archaeological<br/>. history<br/>. archaeology<br/>. anthropology<br/>Most correlated bigrams:<br/>------------------------------<br/>. episode epic<br/>. epic rap<br/>. battle history<br/>. rap battle<br/>. archaeological discoveries</span><span id="372f" class="lg lh iq ne b gy nm nj l nk nl"># 'manufacturing':<br/>Most correlated unigrams:<br/>------------------------------<br/>. factory<br/>. printer<br/>. process<br/>. print<br/>. manufacture<br/>Most correlated bigrams:<br/>------------------------------<br/>. process make<br/>. lean manufacture<br/>. additive manufacture<br/>. manufacture business<br/>. manufacture process</span><span id="23cd" class="lg lh iq ne b gy nm nj l nk nl"># 'science and technology':<br/>Most correlated unigrams:<br/>------------------------------<br/>. quantum<br/>. computers<br/>. science<br/>. computer<br/>. technology<br/>Most correlated bigrams:<br/>------------------------------<br/>. quantum computers<br/>. primitive technology<br/>. quantum compute<br/>. computer science<br/>. science technology</span><span id="91b8" class="lg lh iq ne b gy nm nj l nk nl"># 'travel':<br/>Most correlated unigrams:<br/>------------------------------<br/>. vlog<br/>. travellers<br/>. trip<br/>. blog<br/>. travel<br/>Most correlated bigrams:<br/>------------------------------<br/>. tip travel<br/>. start travel<br/>. expedia viewfinder<br/>. travel blogger<br/>. travel blog</span></pre><h2 id="792e" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">建模和培训</h2><p id="b28f" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">我们将分析的四个模型是:</p><ul class=""><li id="092e" class="ks kt iq jw b jx jy kb kc kf ku kj kv kn kw kr kx ky kz la bi translated">朴素贝叶斯分类器</li><li id="67a3" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">支持向量机</li><li id="6628" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">Adaboost 分类器</li><li id="a359" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr kx ky kz la bi translated">LSTM</li></ul><p id="6a5a" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">数据集被拆分为训练集和测试集，拆分比例为 8:2。标题和描述的特征被独立计算，然后被连接以构建最终的特征矩阵。这用于训练分类器(除了 LSTM)。</p><p id="4680" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">对于使用 LSTM，数据预处理步骤与前面讨论的非常不同。这是它的流程:</p><ol class=""><li id="0994" class="ks kt iq jw b jx jy kb kc kf ku kj kv kn kw kr mz ky kz la bi translated">将每个样本的标题和描述组合成一个句子</li><li id="f1ad" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated"><strong class="jw ir">将组合句子标记为填充序列:</strong>将每个句子转换为标记列表，为每个标记分配一个数字 id，然后通过填充较短的序列并截断较长的序列，使每个序列具有相同的长度。</li><li id="e638" class="ks kt iq jw b jx lb kb lc kf ld kj le kn lf kr mz ky kz la bi translated">对“Category”变量进行一次性编码</li></ol><p id="6c05" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">LSTM 的学习曲线如下所示:</p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b2d9a4df148a6b44294371cb925a6d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*dMRG0E9q1BZ6H5VSxHFe0w.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">LSTM Loss Curve</figcaption></figure><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/21fd2a836ae8bb23968bc12545b9d974.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*5_h3QdwCAaC6cizb7atnHw.png"/></div><figcaption class="mv mw gj gh gi mx my bd b be z dk">LSTM Accuracy Curve</figcaption></figure><h2 id="fd81" class="lg lh iq bd li lj lk dn ll lm ln dp lo kf lp lq lr kj ls lt lu kn lv lw lx ly bi translated">分析性能</h2><p id="4f28" class="pw-post-body-paragraph ju jv iq jw b jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn md kp kq kr ij bi translated">以下是所有不同分类器的精度-召回曲线。要获得额外的指标，请查看<a class="ae me" href="https://github.com/agrawal-rohit/Text-Classification-Analysis/blob/master/Text%20Classification%20Analysis.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="jw ir">完整代码</strong> </a> <strong class="jw ir">。</strong></p><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/39ce8d7b8cd8843be5454b1da150868a.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ZeCRVkXNTUyInTouo2MW_g.png"/></div></figure><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b52a58fe4236c4060d7a4d64c2f4fd27.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*ZYH9pD4HjVWvHvrwQ-o3tw.png"/></div></figure><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/a39125a28619c5193ae637e39bf0cec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*shYMumLw6hchAks1RHzC4w.png"/></div></figure><figure class="mn mo mp mq gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f2f3247d9fa16146afd41254b9801cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*7Ps1tyjPpfvgtLCn26i6tw.png"/></div></figure><p id="86ac" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">在我们的项目中观察到的每个分类器的等级如下:</p><p id="8d86" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated"><strong class="jw ir"> LSTM &gt; SVM &gt;朴素贝叶斯&gt; AdaBoost </strong></p><p id="0b4d" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">LSTMs 在自然语言处理的多个任务中表现出了优异的性能，包括这个任务。LSTMs 中多个“门”的存在允许它们学习序列中的长期依赖性。深度学习 10 分！</p><p id="20b2" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">支持向量机是高度鲁棒的分类器，它们尽最大努力寻找我们提取的特征之间的相互作用，但是学习到的相互作用与 LSTMs 不一致。另一方面，朴素贝叶斯分类器认为特征是独立的，因此它的性能比支持向量机稍差，因为它没有考虑不同特征之间的任何交互作用。</p><p id="7b24" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">AdaBoost 分类器对超参数的选择非常敏感，因为我使用了默认模型，所以它没有最佳参数，这可能是性能差的原因</p></div><div class="ab cl np nq hu nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="ij ik il im in"><p id="f878" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我希望这对你和对我一样有益。完整的代码可以在我的 Github 上找到。</p><div class="nw nx gp gr ny nz"><a href="https://github.com/agrawal-rohit" rel="noopener  ugc nofollow" target="_blank"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">agrawal-rohit —概述</h2><div class="og l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">拥有 3 年以上项目工作经验的初级数据科学家和软件开发人员。高度熟练的机器…</h3></div><div class="oh l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">github.com</p></div></div><div class="oi l"><div class="oj l ok ol om oi on js nz"/></div></div></a></div><p id="7370" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">再见</p></div></div>    
</body>
</html>