<html>
<head>
<title>Augmenting Neural Networks with Constrained Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用约束优化增强神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/augmenting-neural-networks-with-constraints-optimization-ac747408432f?source=collection_archive---------9-----------------------#2019-10-23">https://towardsdatascience.com/augmenting-neural-networks-with-constraints-optimization-ac747408432f?source=collection_archive---------9-----------------------#2019-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e24e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">添加包含领域知识的约束是一种用世界知识增加神经网络并提高其性能的有趣方式，特别是在低数据设置中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/92e96b587f93dca5458d19360baf223d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Yme-vkY123cWauV"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><em class="le">Image from: Conditional Random Fields as Recurrent Neural Networks, </em>Zheng et al (2015)</figcaption></figure><p id="5cdd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在神经网络之上使用约束优化和/或逻辑模块已经成为 NLP 和计算机视觉中许多任务的结构化预测的相当普遍的实践。例如:BiLSTM-CRF 用于 NLP 中的序列对序列任务，或者使用来自神经网络的具有潜在功能的 CRF 用于图像分割任务。</p><p id="9447" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如今，已经有很多积极的研究将这些优化模块直接整合到神经网络中，从而允许网络以端到端的方式进行训练。本文探讨了在神经架构中合并约束的流行方法，并提供了在尝试从数据中学习约束方面的最新进展的调查。</p><h2 id="d777" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">如何将约束纳入深度学习架构？</h2><p id="841f" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">有四种流行的方法可以尝试将域约束合并到神经架构中:</p><ul class=""><li id="2fef" class="md me it js b jt ju jx jy kb mf kf mg kj mh kn mi mj mk ml bi translated">在神经网络上使用约束优化层</li><li id="3751" class="md me it js b jt mm jx mn kb mo kf mp kj mq kn mi mj mk ml bi translated">添加约束违反惩罚</li><li id="1fb0" class="md me it js b jt mm jx mn kb mo kf mp kj mq kn mi mj mk ml bi translated">强制约束架构设计</li><li id="a5f0" class="md me it js b jt mm jx mn kb mo kf mp kj mq kn mi mj mk ml bi translated">数据扩充</li></ul><p id="3da8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">约束优化层</strong></p><p id="f3f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一些流行的约束优化层是条件随机场、维特比解码、整数线性编程(ILP)或非线性编程(NLP)解算器。</p><p id="fce0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，在通过约束优化层合并约束时发生的情况是，您获取神经网络的输出，并将该输出用作实施约束的优化层的潜在函数。</p><p id="965a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们用一个使用这种技术的流行架构来理解它吧— <em class="mr"> BiLSTM CRF。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/12aa376d2c526dcaa92500d971bde01a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*j07mYdi_Ww6d9X0EI-Qoeg.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">BiLSTM Architecture</figcaption></figure><p id="3dfe" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设给你一个句子，你必须在上面做<a class="ae mt" href="https://en.wikipedia.org/wiki/Part-of-speech_tagging" rel="noopener ugc nofollow" target="_blank">词性标注</a>。双向 LSTM(或 BiLSTM)结构通常用于这种序列标记任务。BiLSTM 考虑要标记的单词及其前后的单词，以生成用于预测当前单词的标签的局部嵌入。</p><p id="9792" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在我们可以看到，在输出空间上有许多自然约束。例如，我们不会经常看到一个名词后跟一个形容词，或者一个副词后跟一个副词(所有这些都是软约束，因为存在一些例外)。有人会认为神经架构应该知道这些限制，但事实往往并非如此。CRFs 比 BiLSTM 表现更好就说明了这一点。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/65d29683a11ad81766c4dea47e788f38.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/0*LCADPBUHGax5klqu"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">BiLSTM CRF architecture</figcaption></figure><p id="7cb4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一种创新的方法是将神经模型的优势与 CRF 相结合，这产生了 BiLSTM-CRF 或 LSTM-CRF 架构。BiLSTM 产生的丰富嵌入充当 CRF 层的特征。点击了解更多信息。</p><p id="1e9e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似的技术也已经在计算机视觉中使用，通常用于通过将 HMM 或 CRFs 与神经网络相结合的分割任务。在这里看一个例子<a class="ae mt" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">。</a></p><p id="fda7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">约束违反处罚</strong></p><p id="1368" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">合并约束的另一种流行方法是使用约束违反惩罚作为正则化方法。我们引入了一个对应于违反约束惩罚的辅助损失项。这个增加的项给出了神经网络有多接近满足约束的可微分测量。这种约束条件的一个例子可以在<a class="ae mt" href="https://arxiv.org/abs/1711.11157" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="b3e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些项的添加也为半监督学习开辟了一条途径，因为即使没有输出标签，这种正则化项也可以用于调整模型以更加满足约束。事实上，对于某些问题，任何满足一组给定约束的非平凡假设都是最佳假设的良好近似。对这一假设的研究可以在没有数据(或数据量极低)的情况下进行。更多参考见<a class="ae mt" href="https://arxiv.org/abs/1609.05566" rel="noopener ugc nofollow" target="_blank">具有物理和领域知识的神经网络的无标签监督</a>。</p><p id="5321" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">实施约束的神经架构设计</strong></p><p id="7f93" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是最早也是最难以捉摸的强制约束方法之一。在这种方法中，我们设计的架构可以自动执行约束。我们可以通过使用推进这种方法的最近工作得到这种方法的直觉:<a class="ae mt" href="https://arxiv.org/pdf/1906.06298.pdf" rel="noopener ugc nofollow" target="_blank">用一阶逻辑增强神经网络</a>。这项工作仅限于对具有语义基础的神经元施加约束(因此，对大多数实际网络来说，这种方法仅限于输出神经元或注意神经元)。此外，约束的形式为<code class="fe mv mw mx my b">L -&gt; R</code>。因此，每当 L 的激活为高时，R 的激活也应该为高，因此对应于 L 的激活的偏置项被添加到负责 R 的神经元，从而每当 L 为高时推动 R 变得更高。</p><p id="312f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">加强约束的数据扩充</strong></p><p id="be1d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">人们还可以通过增加数据来加强约束，以激励网络更加注意约束。再次获得这种方法的直觉，让我们考虑<a class="ae mt" href="https://svivek.com/research/publications/li2019logic-driven.pdf" rel="noopener ugc nofollow" target="_blank">一个逻辑驱动的神经模型一致性框架</a>。于是这个小组与<a class="ae mt" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank"> SNLI </a>一起完成任务。给定两个陈述 P 和 Q，我们需要判断 P 是否包含 Q，P 是否与 Q 矛盾，或者这两个陈述是中性的。自然，人们可以看到蕴涵的传递性和矛盾的交换性。然而，该论文表明，SOTA 模型的 SNLI 往往不执行这些一致性。因此，论文的主要贡献是使用数据扩充，使这些一致性得到加强。例如，如果(P，Q)在带有标签“矛盾”的数据集中，那么(Q，P)也应该加上标签“矛盾”，从而强制模型学习交换性。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h2 id="b879" class="lf lg it bd lh li lj dn lk ll lm dp ln kb lo lp lq kf lr ls lt kj lu lv lw lx bi translated">怎样才能学会约束？</h2><p id="2016" class="pw-post-body-paragraph jq jr it js b jt ly jv jw jx lz jz ka kb ma kd ke kf mb kh ki kj mc kl km kn im bi translated">这是一个相当新的研究领域，我们让神经模型在有限的人工监督下从数据本身学习约束。我们将关注这一领域的两项工作。</p><p id="aae5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">用于结构化预测的对抗性约束学习，<em class="mr">任等人</em> </strong> </p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7c0a3660149ed9364966c6a9bfd1ddb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/0*1OpQ613hPGW6GJ8G"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Adversarial training to train a generator which can model output label space</figcaption></figure><p id="9108" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">它们为结构化预测任务提供了一种创新的半监督学习方法。他们的工作假设遵循真实输出空间分布的大量输出标签是可用的(可能来自模拟器)。然后，他们训练一个生成器网络，该网络可以使用对抗训练技术生成能够模拟真实输出标签空间的输出标签。他们使用有限量的标记训练数据来将输入标签与输出标签相关联，从而进一步调整该生成器。</p><p id="3874" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae mt" href="https://arxiv.org/pdf/1905.12149.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu"> SATNet:使用可微可满足性求解器桥接深度学习和逻辑推理，<em class="mr">王等</em> </strong> </a></p><p id="7d8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是一个我个人认为非常惊人的研究领域。基本上，他们试图构建一个能够从数据本身学习约束的架构。此外，他们设计的架构可以使用反向传播进行训练，因此可以插在任何深度学习架构之上，用于约束优化任务。他们通过学习解决数独来演示这个模型，只使用数独的实例，没有人为监督来识别约束。</p><p id="4348" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">他们试图学习一个 MAX2SAT 实例，该实例通过将手头的问题公式化为半确定的编程实例来对其建模。在正向传递中，它们求解当前 SDP 实例，以找到优化 SDP 的变量分配。然后，他们计算这个变量赋值的损失，并反向传播以修改 SDP 实例。</p><pre class="kp kq kr ks gt nh my ni nj aw nk bi"><span id="e305" class="lf lg it my b gy nl nm l nn no"><strong class="my iu">Forward Pass </strong>: Solve current approximated problem<br/><strong class="my iu">Backward Pass</strong>: Improve approximated problem</span></pre><p id="f535" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，拥有反向传播能力的伟大之处在于，你可以将这个模块插在任何深度学习架构之上。考虑给你一个数独的图像，然后你可以有一个 CNN 来识别数字，在此之上，你可以有一个 SATNet 层来解决解码的数独实例。这整个架构可以进行端到端的训练。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/8100555048d036593f7c506ac85239e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*smKxLJ7pZP6G9GYtfnoKzg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Solving visual Sudoku with SATNet</figcaption></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="2af5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">向神经架构添加约束不仅有助于提高性能，而且还提供了更新的方法来利用未标记的数据，修剪输出空间，从而提高模型的可学习性并增加泛化能力。此外，强制约束一致性有助于使网络健壮和可靠。</p><p id="0839" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇架构和算法的综述并不详尽，但我希望它能让你对架构和算法中的大多数常见约束有所了解。</p><p id="8604" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="mr">干杯！</em></p></div></div>    
</body>
</html>