<html>
<head>
<title>Using NLP to Find Similar Movies Based on Plot Summaries</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于情节摘要使用 NLP 查找相似的电影</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-nlp-to-find-similar-movies-based-on-plot-summaries-b1481a2ba49b?source=collection_archive---------14-----------------------#2019-09-23">https://towardsdatascience.com/using-nlp-to-find-similar-movies-based-on-plot-summaries-b1481a2ba49b?source=collection_archive---------14-----------------------#2019-09-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="b091" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">人人都爱电影，对吧？本文将介绍 NLP 中的一些基本概念，并展示如何使用 scikit-learn 和 NLTK 库实现一个简单的模型来比较电影摘要之间的相似性。</p><p id="5a30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的内容基于这个<a class="ae ko" href="https://www.datacamp.com/projects/648" rel="noopener ugc nofollow" target="_blank"> DataCamp 项目</a>，并且这篇文章中使用的数据由来自维基百科和 IMDb 的 100 部电影的情节摘要组成。</p><h1 id="fc00" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">准备数据</h1><p id="f045" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">在处理文本数据时，我们需要对数据集进行一些特殊的调整，比如标记化、词干化和矢量化。我们将会看到这些步骤的作用。</p><h2 id="6b02" class="ls kq it bd kr lt lu dn kv lv lw dp kz kb lx ly ld kf lz ma lh kj mb mc ll md bi translated">标记化</h2><p id="418a" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">给定一个字符序列，标记化是将它分解成具有有用和基本意义的基本语义单元的过程。这些碎片被称为代币。比如:句子“你今天过得怎么样？”可以拆分为“如何”、“s”、“你的”、“日”和“日”等标记。".每一个都有特定的含义。</p><h2 id="abb1" class="ls kq it bd kr lt lu dn kv lv lw dp kz kb lx ly ld kf lz ma lh kj mb mc ll md bi translated">堵塞物</h2><p id="2f8a" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">想想这些句子:“我们需要更多的计算能力”和“我们需要计算机更强大”。两者意思都差不多，只是主词的写法不同(computational/计算机，power/强大)。我们可以将这些词的屈折形式简化为词根或基本形式，例如“comput”和“power”具有相同的意思。这样，词汇表的大小就减少了，使模型更容易在有限的数据集上进行训练。</p><h2 id="ddaf" class="ls kq it bd kr lt lu dn kv lv lw dp kz kb lx ly ld kf lz ma lh kj mb mc ll md bi translated">…向量化…</h2><p id="bee9" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">除了数字，计算机不能处理任何东西。为了实现对文本的任何操作，我们首先需要将文本转换成数字。这个过程被称为矢量化，顾名思义，它们被组织成向量。有多种向量化文本的方法。这里我们将使用单词袋和 TF-IDF。</p><ul class=""><li id="a2eb" class="me mf it js b jt ju jx jy kb mg kf mh kj mi kn mj mk ml mm bi translated"><strong class="js iu">单词包(BoW) </strong>:每个句子被表示为一个固定大小的向量，其大小等于词汇表中的单词数，每个位置表示一个特定的单词。这个位置的值是该特定单词在句子中出现的次数。</li><li id="55c8" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated"><strong class="js iu">词频-逆文档频率(TF-IDF) </strong>:当我们需要比较两个文档之间的相似性时，为文档中的每个单词指定一个重要性度量是很有帮助的，这样我们就可以专注于特定的部分。TF-IDF 在于通过两个术语的乘积来发现这种“重要性”: TF 术语表示该单词在文档中的出现频率，而 IDF 术语表示文档中出现该特定单词的频率。基本思想是:如果这个词在很少的文档中出现很多，那么它一定很重要。如果在很多文档中出现很多，那一定不重要(比如“the”、“of”、“a”)。</li></ul><h1 id="93f1" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">代码</h1><p id="8793" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们现在准备实施这些概念。首先，我们将导入一些库，如<strong class="js iu"> Pandas </strong>来操作我们的数据，<strong class="js iu"> scikit-learn </strong>来创建我们的管道并训练我们的模型，以及<strong class="js iu"> NLTK </strong>用于自然语言的分词器、停用词和词干。</p><p id="fc49" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">CSV 文件被加载到熊猫数据帧中。电影数据集有两列用于绘图:<code class="fe ms mt mu mv b">wiki_plot</code>和<code class="fe ms mt mu mv b">imdb_plot</code>。我们将它们连接成一个名为<code class="fe ms mt mu mv b">plot</code>的新列，这样我们就可以有一个包含更多信息的单列。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="4e81" class="ls kq it mv b gy ne nf l ng nh">import numpy as np<br/>import pandas as pd<br/>import re<br/>import nltk</span><span id="79c3" class="ls kq it mv b gy ni nf l ng nh">movies_df = pd.read_csv('datasets/movies.csv')</span><span id="7449" class="ls kq it mv b gy ni nf l ng nh">movies_df['plot'] = movies_df['wiki_plot'].astype(str) + "\n" + movies_df['imdb_plot'].astype(str)</span></pre><p id="bd85" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后定义函数<code class="fe ms mt mu mv b">normalize</code>，它将对数据集的每个文档的每个单词中的特殊字符进行分词、词干提取和过滤。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="c739" class="ls kq it mv b gy ne nf l ng nh">from nltk.stem.snowball import SnowballStemmer<br/>stemmer = SnowballStemmer("english", ignore_stopwords=False)</span><span id="f08b" class="ls kq it mv b gy ni nf l ng nh">def normalize(X): <br/>  normalized = []<br/>  for x in X:<br/>    words = nltk.word_tokenize(x)<br/>    normalized.append(' '.join([stemmer.stem(word) for word in words if re.match('[a-zA-Z]+', word)]))<br/>  return normalized</span></pre><p id="d7af" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">被定义为具有三个步骤的管道:</p><ul class=""><li id="513a" class="me mf it js b jt ju jx jy kb mg kf mh kj mi kn mj mk ml mm bi translated">应用<code class="fe ms mt mu mv b">normalize</code>功能</li><li id="f553" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated">使用单词包对所有文档进行矢量化处理(这一步还会删除停用词)</li><li id="4449" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated">将单词包转换成 TF-IDF 矩阵</li></ul><p id="2f4d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们在管道上调用<code class="fe ms mt mu mv b">fit_transform</code>，并将所有电影的情节列作为参数传递。该方法将按顺序运行每个步骤，沿途转换数据并返回上一步的结果，这是一个(n_movies，n_words)矩阵，其中包含每个电影的相应 TF-IDF 向量。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="9b4e" class="ls kq it mv b gy ne nf l ng nh">from sklearn.pipeline import Pipeline<br/>from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer<br/>from sklearn.preprocessing import FunctionTransformer<br/><br/>pipe = Pipeline([<br/>  ('normalize', FunctionTransformer(normalize, validate=False)),<br/>  ('counter_vectorizer', CountVectorizer(<br/>    max_df=0.8, max_features=200000,<br/>    min_df=0.2, stop_words='english',<br/>    ngram_range=(1,3)<br/>  )),<br/>  ('tfidf_transform', TfidfTransformer())<br/>])<br/><br/>tfidf_matrix = pipe.fit_transform([x for x in movies_df['plot']])</span></pre><p id="356c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于句子被转换成向量，我们可以计算它们之间的余弦相似度，并表示这些向量之间的“距离”。使用<code class="fe ms mt mu mv b">tfidf_matrix</code>计算余弦相似度，并返回一个具有维数(n_movies，n_movies)的矩阵，其中包含它们之间的相似度。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="b023" class="ls kq it mv b gy ne nf l ng nh">from sklearn.metrics.pairwise import cosine_similarity similarity_distance = 1 - cosine_similarity(tfidf_matrix)</span></pre><p id="48f4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用相似性距离，可以创建一个树状图:</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="bee4" class="ls kq it mv b gy ne nf l ng nh">import matplotlib.pyplot as plt<br/>from scipy.cluster.hierarchy import linkage, dendrogram<br/><br/>mergings = linkage(similarity_distance, method='complete')<br/>dendrogram_ = dendrogram(mergings,<br/>               labels=[x for x in movies_df["title"]],<br/>               leaf_rotation=90,<br/>               leaf_font_size=16,<br/>              )<br/><br/>fig = plt.gcf()<br/>_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]<br/>fig.set_size_inches(108, 21)<br/><br/>plt.show()</span></pre><figure class="mw mx my mz gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nj"><img src="../Images/66f815d2eaed40274cd15925b0623664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgpMWij6CMUekviXPqULUg.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk">Dendrogram of similar movies</figcaption></figure><p id="f571" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们甚至可以创建一个功能来搜索与另一部电影最相似的电影。这里我们使用一个 numpy 方法<code class="fe ms mt mu mv b">argsoft</code>来寻找矩阵中第二相似的电影。这是因为一部电影的最小距离是和自己。这可以在主对角线的<code class="fe ms mt mu mv b">similarity_distance</code>中看到(所有值都为零)。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="9d9d" class="ls kq it mv b gy ne nf l ng nh">def find_similar(title):<br/>  index = movies_df[movies_df['title'] == title].index[0]<br/>  vector = similarity_distance[index, :]<br/>  most_similar = movies_df.iloc[np.argsort(vector)[1], 1]<br/>  return most_similar</span><span id="0b69" class="ls kq it mv b gy ni nf l ng nh">print(find_similar('Good Will Hunting')) # prints "The Graduate"</span></pre><h1 id="9143" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">结论</h1><p id="a1a2" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">为 NLP 中基本任务训练机器学习模型是简单的。首先，对数据进行令牌化和过滤，以便可以用称为令牌的单元来表示。我们还可以将单词表示为它们的词根形式，因此词汇量也可以减少，然后我们使用一种算法对数据集进行矢量化，这种算法取决于我们试图解决的问题。之后，我们可以训练一些机器学习模型，生成更多的文本，可视化我们的数据，等等。</p><p id="5958" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下次见！</p></div></div>    
</body>
</html>