<html>
<head>
<title>Elucidating Policy Iteration in Reinforcement Learning — Jack’s Car Rental Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阐释强化学习中的策略迭代——杰克的租车问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elucidating-policy-iteration-in-reinforcement-learning-jacks-car-rental-problem-d41b34c8aec7?source=collection_archive---------12-----------------------#2019-10-10">https://towardsdatascience.com/elucidating-policy-iteration-in-reinforcement-learning-jacks-car-rental-problem-d41b34c8aec7?source=collection_archive---------12-----------------------#2019-10-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/a169322a9b6e7814d5051ef0cd424467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*udhphWhqjadT-osAQhL6AQ.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">[1]: Generalized Policy Iteration</figcaption></figure><p id="c676" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇博文中，我将尝试通过使用强化学习中的策略迭代算法来解决 Jack 的租车问题，从而阐明该算法。这个问题和它的变体分别在萨顿和巴尔托的书(强化学习:导论，第二版)中的例子 4.2 和练习 4.5 中给出。</p><h1 id="d983" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">问题陈述</h1><p id="ef6c" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">杰克为一家全国性的汽车租赁公司管理着两个地点。每天，都会有一定数量的客户来到每个地点租车。如果杰克有一辆车，他就把它租出去，国家公司给他 10 美元。如果他在那个位置没有车，那么生意就没了。汽车在归还的第二天就可以出租了。为了帮助确保汽车在需要的地方可用，Jack 可以在两个地点之间连夜移动汽车，每移动一辆汽车的费用为 2 美元。</p><p id="b006" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们假设每个位置请求和返回的汽车数量是一个泊松随机变量。回想一下，如果 X 是泊松随机变量，那么</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi md"><img src="../Images/bfa2284c8531bebddca92b0a1d0f0b3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWI3KWikpvZG7HZLIKej6Q.png"/></div></div></figure><p id="a428" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">假设第一个和第二个位置的租赁请求的λ是 3 和 4，返回请求的λ是 3 和 2。</p><p id="27db" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了稍微简化问题，我们假设每个位置不能超过 20 辆汽车(任何额外的汽车都返回给全国性的公司，因此从问题中消失),并且在一个晚上最多可以将五辆汽车从一个位置移动到另一个位置。我们将贴现率γ设为 0.9，并将其公式化为一个连续的有限马尔可夫决策过程，其中时间步长为天，状态为一天结束时每个位置的汽车数量，而行动为一夜之间在两个位置之间移动的汽车净数量。</p><h1 id="9b5e" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">但是解决这个问题意味着什么呢？</h1><p id="9ab8" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">解决这个问题意味着解决两件事，首先，杰克应该在一夜之间在每个地点之间移动多少辆车，以最大化他的总期望回报，即，在给定的情况(状态)下，他的策略(政策)应该是什么；其次，如果杰克知道这个策略，他如何比较哪些情况比其他情况更好(价值)？</p><h1 id="fc5d" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">初始设置</h1><p id="7d55" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">让我们先用代码写出我们知道的最初的东西。从问题中，我们知道杰克可以获得两种类型的奖励，第一种是他租车时的 10 美元奖励，第二种是他从一个位置移动到另一个位置的每辆车的-2 美元奖励(注意，后一种奖励是负的)。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="e6a2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们定义一个 poisson_ class，它取一个参数λ，计算对应的概率质量函数。它有两个数据成员α和β，表示 pmf 值大于ε(此处为 0.01)的 n 值的区间[α，β]。我们将ε以下的 pmf 值设置为零，然后对结果分布进行归一化。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="4eb0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如，对于λ = 3，数据成员α、β和 val 的值为:</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><h1 id="b86f" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">策略迭代算法</h1><p id="1db9" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">在我们继续之前，为了确保所有的读者都在同一页上，让我们快速修改一下这里的术语价值和策略的含义。</p><p id="2b8c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在我们的汽车租赁示例中，系统在任何时候的状态都是一对两个数字，第一个和第二个位置的汽车数量。给定一个状态，杰克必须选择一个动作，即他可以从第一个位置移动到第二个位置的汽车数量，反之亦然。根据问题，可以在-5 到+5 之间变化，其中+n 代表 Jack 将 n 辆车从第一个位置移动到第二个位置。</p><p id="3f95" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">策略是从状态到行动的映射，例如，给定一个状态，Jack 应该在一夜之间移动多少辆车。现在，假设杰克有某个策略π，那么给定这个π，<strong class="ke ir">一个状态(比如 s)的值就是杰克从 s 出发，之后跟随π时会得到的期望回报。</strong></p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mk"><img src="../Images/b41fa1d321273be9499a98b7e4835a63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYq8bX-q4Z8-8aPndQaz9w.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">[1]: The Policy Iteration Algorithm</figcaption></figure><p id="9657" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如上图所示，策略迭代算法由三部分组成。让我们在解决租赁问题的背景下分别讨论这些组件。</p><p id="47ee" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第一部分是初始化。如上图所示，我们任意初始化值和策略矩阵。将它们初始化为零也可以。请注意，给定一个策略，我们为每个状态定义一个值，因为我们的状态是一对两个数字，其中每个数字取 0 到 20 之间的值，因此我们用形状矩阵(21 x 21)来表示值。策略取一个状态，输出一个动作；因此，它也可以用相同形状的矩阵来表示。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="a53f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第二个组成部分是政策评估。通过政策评估，我们的意思是遵循这个政策，任何国家的价值应该是什么。如上所述，给定一个政策π，一个状态(比如 s)的值就是杰克从 s 出发，之后跟随π时会得到的期望回报。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ml"><img src="../Images/9b88371eecb25e4bd5bc37c83d95a0f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A9czURkUF4F_uOUJBQL51Q.png"/></div></div></figure><p id="dc4b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这种与状态值相关联的期望概念可以写成上图所示的形式，由此可以推导出贝尔曼方程，如图所示。</p><p id="c5be" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个贝尔曼方程形成了在策略评估组件中显示的值更新的基础。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mm"><img src="../Images/d4e4d697d1fadadab1de630572a9f9ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45UrrNMFfujO2mVTTwLLuA.png"/></div></div><figcaption class="jy jz gj gh gi ka kb bd b be z dk">Value Update</figcaption></figure><p id="9331" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在许多这样的更新之后，V(s)收敛到一个几乎满足(最多有一些θ误差)贝尔曼方程的数，因此代表状态 s 的值。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="27b7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第三个组成部分是政策改进。给定一个状态(比如 s)，我们指定π(s)等于使期望报酬最大化的动作。我们说，当任何状态下的行动最大化步骤都没有引起策略的变化时，策略变得稳定。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="8670" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们循环运行策略评估和改进组件，直到策略变得稳定。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><h1 id="a026" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">结果</h1><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/7eaad59c4ed95121a97035c1050f4f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pRhHj3YPpYuOIaWlH1ZzfA.png"/></div></div></figure><p id="a7fd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">上图以热图的形式显示了通过评估和改进后的策略。回想一下，策略由包含[-5，5]范围内的动作值的矩阵表示。</p><h1 id="5c0c" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">给原来的租赁问题增加了非线性</h1><p id="8118" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">让我们看看，如果我们在上面的问题中加入一些非线性因素，会发生什么:</p><ol class=""><li id="2295" class="mo mp iq ke b kf kg kj kk kn mq kr mr kv ms kz mt mu mv mw bi translated">Jack 在第一个地点的一名员工每天晚上乘公交车回家，并且住在第二个地点附近。她很乐意免费接送一辆车到第二个地点。每增加一辆车仍然要花 2 美元，所有向相反方向移动的车也是如此。</li><li id="4919" class="mo mp iq ke b kf mx kj my kn mz kr na kv nb kz mt mu mv mw bi translated">此外，Jack 在每个位置都有有限的停车位。如果超过 10 辆汽车在一个地点过夜(在任何汽车移动之后)，那么使用第二个停车场必须产生 4 美元的额外费用(与有多少辆汽车停在那里无关)。</li></ol><p id="56a1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">可以很容易地添加这些条件来修改原始代码。如果需要，我们将有一个额外的奖励-第二个停车场 4 美元(注意，这个奖励是负的)。</p><figure class="me mf mg mh gt jr"><div class="bz fp l di"><div class="mi mj l"/></div></figure><p id="975b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">生成的策略(通过评估和改进)如下图所示。</p><figure class="me mf mg mh gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/4b710348f930545e294cf6b345d7d407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jDL3jHa_GEIjVvX-W7xzwg.png"/></div></div></figure><h1 id="1d5c" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">结论</h1><p id="1960" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">我们使用动态规划解决了上述问题。我们存储了中间值和策略矩阵，并在策略评估和改进功能中使用它们。然而，为了使用贝尔曼更新，我们需要知道环境的动态，就像我们在租赁示例中知道奖励和下一个状态的概率一样。如果只能从底层分布中采样，不知道分布本身，那么可以用蒙特卡罗方法来解决相应的学习问题。</p><p id="7e3e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Github 库:<a class="ae nd" href="https://github.com/thunderInfy/JacksCarRental" rel="noopener ugc nofollow" target="_blank">https://github.com/thunderInfy/JacksCarRental</a></p><h1 id="c314" class="la lb iq bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">参考</h1><p id="caea" class="pw-post-body-paragraph kc kd iq ke b kf ly kh ki kj lz kl km kn ma kp kq kr mb kt ku kv mc kx ky kz ij bi translated">[1]萨顿和巴尔托(2017 年)。强化学习:导论。剑桥，麻省理工学院出版社</p><p id="115f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[2] S. David 关于通过动态规划进行规划的讲座(【https://www.youtube.com/watch?v=Nd1-UUMVfz4】T2)。</p></div></div>    
</body>
</html>