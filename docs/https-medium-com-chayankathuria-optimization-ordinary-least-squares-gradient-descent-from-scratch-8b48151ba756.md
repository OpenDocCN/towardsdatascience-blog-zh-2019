# 优化:普通最小二乘法与梯度下降法——从头开始

> 原文：<https://towardsdatascience.com/https-medium-com-chayankathuria-optimization-ordinary-least-squares-gradient-descent-from-scratch-8b48151ba756?source=collection_archive---------7----------------------->

![](img/a047c77b7af56d303e46e710d2c3bd13.png)

Photo by [Trần Ngọc Vân](https://unsplash.com/@aaaaaaaaaaaaaaaa?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

什么是优化？、优化技术—数值方法和迭代方法，以及最终的 Python 实现。

# 最佳化

> 优化是机器学习的核心。

用非常严格的术语来说，优化就是寻找你的*成本函数*给出最小值*的过程。*对于任何关于机器学习的优化问题，可以有数值方法或分析方法。数值问题是确定性的，这意味着它们有一个不变的封闭解。因此它也被称为时不变问题。这些封闭形式的解是解析可解的。但这些都不是优化问题。

当你有函数 f(x)的“最小”或“最大”这样的词时，优化就来了——目标函数**或成本函数。这个目标函数可以定义与你正在优化的问题相关的任何东西。这可能是一家公司的成本，另一家公司的损失，甚至是收入等等。**

这个函数在特定的点 X*将是最优的。这个 X*就是最优点。因此，你的优化问题可能是——找出 X*,其中 f(x)是最小值/最大值。这也可以写成 argmin(f(x)) —其中函数 f(x)是最小值的自变量(或者反过来是 argmax(f(x))。

# 普通最小二乘法

我很确定你知道线性回归的基本知识。如果你没有，也不用担心。看看这个就知道了。基本上，回归意味着找到最适合你的数字数据的*线/曲线——数据的函数近似值。也就是说，您需要一个从输入数据到输出数据(目标)的映射函数。这个映射函数被写成:*

![](img/6d1f088896db1d35a16abfe497ca3714.png)

其中 W0 是截距，W1 是直线的斜率，ŷ是预测输出。需要找到 W0 和 W1 的最佳值。让我们考虑这个非常小的数据集:

X = 1，2，3

Y = 5，12，18

所以我们需要优化的问题是:

![](img/6c9efea3485bdf8a8823df7bdd55842e.png)

其中 L 是损失函数或成本函数或误差函数。现在下一步是为我们的优化问题找到正确的损失函数。因为如果你最小化错误的目标函数，你最终会得到错误的最优点。我们将使用的损失函数 L 是均方误差，计算公式如下:

![](img/a71289ef0fcc906bf032a4d14d82d419.png)

求解上述损失函数，我们得到以下用于找到最佳权重的公式:

![](img/5fe2d802e27034851dd6367c856fd107.png)![](img/247e32de55450add5c7d6927a78cf3b5.png)

使用 python 计算上述权重，我们得到以下值:

![](img/3e16b8f74c2e98e5c12911c1a2c42b48.png)![](img/a4dda785596169f80ef93c30877a29ee.png)

这是普通的最小二乘解，也就是解析解。因为我们找到了误差平方的最小值。

但是这种解决方案是不可扩展的。将此应用于线性回归相当容易，因为我们有很好的系数和线性方程。将此应用于复杂的非线性算法(如支持向量机)是不可行的。因此，我们将通过迭代法找到这个解的*数值近似值*——它将接近(但不完全等于)OLS 解——它给了我们精确的解。

# 梯度下降

我们先来了解一下梯度下降优化背后的直觉。假设 20 个人(包括你自己)被随机空投到一个山脉中。你的任务是在 30 天内找到完整区间的最高峰。你们每个人都有对讲机可以交流，还有高度计可以测量高度。每天，你们都要花几个小时寻找可能的最高峰，并向其他人报告他们在指定区域内发现的当天最高海拔——这就是他们的健康值。

假设在第一天你报告 1000 英尺。有人报告高度 1230 英尺。诸如此类。然后有一个人报告 5000 英尺。这是最大值。记住你的任务是集体到达山脉的最高峰。第二天你接下来做什么？第二天，每个人都会聚集到昨天发现最高海拔的地方。他们会认为山脉的最高峰很可能就在这个地区。如果有另一个区域已经有 5000 英尺，为什么昨天报告 500 英尺的人会再次搜索那个区域。于是所有的搜索者*贪婪地*向报道的最高点移动。现在，这种贪婪可能会把你带到这个范围的最高峰，但也可能会导致一个彻底的错误。

那个在 500 英尺高的家伙。昨天可能是在一座 10000 英尺高的山峰的底部。！他贪婪地忽略了它，走向另一个最大化 5000 英尺，比如 7000 或 8000 英尺。你实际上陷入了局部最大值/最优值(突变在某种程度上会有所帮助！).没有办法知道你是否停留在局部最优。

**模拟退火**也是一种可以拯救我们的算法。其中搜索者将彻底搜索整个搜索空间，而不会偏向于最有可能找到全局最大值。

现在回到我们用 OLS 定义的优化问题。让我们用梯度下降法求解。损失函数也是一样的。但这次我们将一步一步地迭代，以达到最佳点。从任意权重值开始，检查该点的梯度。我们的目标是到达最低点，也就是谷底。所以我们的梯度应该总是负的。

![](img/ee9111798be5960033417bf7eb940f99.png)

[source](http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/)

接下来，我们需要更新权重，使它们更接近最小值。我们有下面的等式:

![](img/472902d45ea31c2df701359309257c54.png)

这意味着下一次迭代的权重将是前一次迭代的权重减去更新的权重。现在这个更新有两个组成部分:*方向* —斜率或梯度，以及*值* —步长。梯度将为:

![](img/fe102324d54eb80d8392b783bd26833e.png)

所以如果在我们的初始重量下，斜率是负的，我们在正确的方向上。我们只需要增加权重的值来使它更接近。这正是上面的等式所做的。如果斜率在特定点为负，则第二项将被添加到上一次迭代的权重值中。相反，如果它是正的，那就意味着我们需要向相反的方向去达到最小值。在这种情况下，该等式从上一个等式的权重值中减去第二项。干净利落。

我们需要考虑的第二个因素是步长α。这是一个*超参数*，您需要在算法开始之前决定。如果α太大，那么你的优化器将会跳跃很大，永远找不到最小值。相反，如果你把它设置得太小，优化器将会花很长时间来达到最小值。因此，我们需要预先设定α的最佳值。

![](img/660f97cb17da55e67f070b1084ae4827.png)

[source](https://datascience.stackexchange.com/questions/50948/gradient-descent)

如果你理解正确，你会意识到我们在这里讨论的梯度本质上是误差的总和。我们实际上只是接受了这个错误的一小部分。并且我们传播该误差来更新我们的权重。

因此，我们的权重更新等式变为:

![](img/884fd5049e7febcf1262d2463ca0c362.png)![](img/e650ed56e3084503df7bd6e0b439645a.png)

因此，让我们记下我们执行的清晰步骤:

1.  初始化权重 W0、W1 的值(可以是任何值)和步长α(需要是一个好值)。
2.  求目标 Ŷ的预测值= W0 + W1。所有 x 的 x。
3.  计算误差值(Ŷ-Y)和 MSE。
4.  根据梯度下降更新规则更新权重。
5.  重复 2–4。

这被称为**批次梯度下降**，因为我们在每次迭代中获取整个批次或数据集的误差。我们还有以下梯度下降的变体:

*   **随机梯度下降**，其中权重的更新在每次迭代中完成
*   **小批量梯度下降**，介于批量和随机之间，将完整数据集分成小批量，然后在每个批量后应用权重更新。

最后，让我们用 Python 中的几行代码完成所有这些工作！

让我们从初始化我们的小数据集开始:

![](img/351c7dd5f7aab98fb79650ef1f45b6f2.png)

现在进入第一步，初始化权重和步长，我选择了 0.04。您可以尝试调整该值并亲自查看结果:

![](img/4eb68acf17791f58ddf19ed3514e8719.png)

初始化后，我们对整个数据集进行多次迭代，计算每次迭代的均方误差，并更新权重:

![](img/d3c3593b46b0c2d8764a5cc111b6d20b.png)

所以我们迭代 10 次，希望我们的算法已经足够收敛。让我们看看我们的最终重量，看看它们与我们的 OLS 解决方案有多接近:

![](img/94c4cedd8415e01f882279e88bcfec84.png)

相当接近！现在让我们检查预测的目标变量，Ŷ和误差:

![](img/1b2eaf4994a3b71611063df09b1760a2.png)

如你所见，预测变量非常接近实际值。最后，让我们画出每次迭代的均方误差值，看看我们的算法表现如何:

![](img/4b4adb158590eabca940cc0232aa698f.png)

太好了！所以我们讨论了:

*   优化到底是什么，
*   关于机器学习，它有什么样的解决方案，
*   它的分析方法和直觉是什么
*   线性回归在 python 中的实现

这就是本文的全部内容。如需进一步阅读，您可以关注以下精彩内容:

*   [https://machine learning mastery . com/gradient-descent-for-machine-learning/](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)
*   https://www.youtube.com/watch?v=sDv4f4s2SB8
*   [https://www . Amazon . in/Engineering-Optimization-Practice-Singiresu-Rao/DP/0470183527](https://www.amazon.in/Engineering-Optimization-Practice-Singiresu-Rao/dp/0470183527)

希望这篇文章有所帮助。请在下面分享和评论你的想法！