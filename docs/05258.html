<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-support-vector-classifier-maximal-margin-classifier-22648a38ad9c?source=collection_archive---------3-----------------------#2019-08-06">https://towardsdatascience.com/support-vector-machine-support-vector-classifier-maximal-margin-classifier-22648a38ad9c?source=collection_archive---------3-----------------------#2019-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2e1c" class="io ip iq bd ir is it dn iu iv iw dp ix iy iz ja jb jc jd je jf jg jh ji jj jk bi translated">支持向量机(详细解释)</h2><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi jl"><img src="../Images/f7cad4cc8461c87d967dd26aca2e175d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UWZ8N1Yv5zBtvuOAbYMeQg.jpeg"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk"><a class="ae kb" href="https://www.quantamagazine.org/new-algorithm-solves-cake-cutting-problem-20161006/" rel="noopener ugc nofollow" target="_blank">https://www.quantamagazine.org/new-algorithm-solves-cake-cutting-problem-20161006/</a></figcaption></figure><p id="b7ee" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">最大间隔分类器</strong></p><p id="5d5c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">什么是超平面:<br/>如果我们有 p 维空间，那么超平面就是维数为 p-1 的平坦子空间。比如二维空间中超平面是直线，三维空间中超平面是二维子空间。想象一把小刀切开一块立方形的奶酪，把它分成两部分。然后把两片放回去，观察两片之间的空间，那是三维空间中的二维子空间。对于大于 3 的 p，想象可能更难，但直觉保持不变。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi ky"><img src="../Images/dfee1bc319c8c4bca963698fcf743d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*sNSMYpoZN7RTcJbrVk-unA.png"/></div></figure><p id="0c8b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">这是二维空间中超平面的方程。类似地，该等式可以扩展到 p 维设置，如下所示:</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/76abf4986ad20f7ec86b27b491519ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*iY_S4VG-2EvW6P3i94yNyw.png"/></div></figure><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi la"><img src="../Images/caefab94a273dd1842a4315b906c063f.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*sQlbl9gZtFn-ktmDsNFQjg.png"/></div></figure><p id="0f5a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">每个β都是我们空间中众多维度之一的参数。</p><p id="ef8a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">因此，如果我们有一个满足上述方程的点 X，那么它意味着这个点在超平面上。换句话说，如果我们有一个点，比方说在 5 维空间中，那么 X1 到 X4 对于该空间中的一个点将具有不同的值，如果在你将这些值代入上面的方程后，该方程最终等于零，那么这意味着该点位于超平面上。</p><p id="3fdb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">这意味着如果方程的值小于零，那么这个点就不在平面上，而是在超平面下面。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/d20a2fc3294104988ca71d669745024b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*OSLQ4NUx3iSEdOXcBmxrBA.png"/></div></figure><p id="bd58" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">如果这些点位于平面的上侧，则等式成立，如下所示:</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/fd091afecd21e58fe822e05ec172e25c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*Wq1phijR6-Z2-v_2OyGpxQ.png"/></div></figure><p id="fd95" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">以上两个等式定义了这两类不同的点。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi ld"><img src="../Images/4d86f5ae553d4d96bc840b66703a964a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o35ejlDT50z0YlrhBueLHA.png"/></div></div></figure><p id="b9bd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">使用分离超平面分类</strong></p><p id="8a3c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">假设我们有一个训练和测试数据矩阵。训练数据矩阵具有 n×p 维。意味着它有 n 个观测值，并且是 p 维的。每个观察值都属于这两类中的任何一类，即<em class="le"> y1…yn </em>可以是-1 也可以是 1。</p><p id="fc87" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">假设如果基于训练数据，我们可以构建一个超平面，它可以根据标记的类别完美地分离所有训练观察。分类器将像这样工作:</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi lf"><img src="../Images/dba78622eca69789e5813b173d41e44f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DjyNZCYF7DBCmZhOaXLqoA.png"/></div></div></figure><p id="ace5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">每个测试观察根据它们位于超平面的哪一侧而被分配一个类别。如果观察给出的上述等式的值小于零，那么它属于-1 类，对于 1 类也是如此。</p><p id="8934" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">也就是说，对于任何测试观察值<em class="le"> x(theta) </em>，我们可以根据以下等式的符号对其进行分类。如果符号为负，则分配 1 级，如果符号为正，则分配 1 级。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/4d5838176a62c0c611e46f0a6027af7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*7QjGIKimzUUOzPmQqH3N-Q.png"/></div></figure><p id="4285" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">此外，对于任何特定的测试观测，一旦我们将 X 的所有值放入上面的等式中，并观察它是小于 1 还是大于 1，那么这些值的大小就决定了该特定观测点在平原每一侧的远近。</p><p id="a117" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">最大间隔分类器</strong></p><p id="e49f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">现在的问题是，如果我们基于超平面划分数据，那么可能有许多可能的超平面来划分同一组数字。我们如何决定从有限数量的超平面中进行选择？</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi lh"><img src="../Images/5e903b3acd584d6c2f9121e4ab6340aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6i5enXAly94sqo2YcYAfw.png"/></div></div></figure><p id="dc8e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><em class="le">解决方案:最大间隔分类器</em></p><p id="58d4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">这是离训练观察值最远的分类器。通过计算超平面到训练观测值之间的垂直距离。最短的这种距离称为超平面和观测之间的最小距离，它被称为余量。</p><p id="a414" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">因此，最大边缘超平面是具有最大边缘的超平面，也就是说，它在超平面和训练观察值之间具有最大的距离。使用这个超平面，我们可以对测试数据进行分类。如果我们的模型</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi li"><img src="../Images/590c22048b84a807fbd899a02b895e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*4VCSIlD9GsbPA36oYlKyCA.png"/></div></figure><p id="ae0e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">然后，最大间隔分类器可以基于符号对新的测试观察进行分类</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/7f85ba62d8154a10ff1e945f16dc0daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*oGNpALJe8GG450wSnZ2yAQ.png"/></div></figure><p id="8f2a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">简而言之，对于每个测试观察，我们将所有变量放入上面的等式中，并基于<em class="le"> f(x)的符号来决定该特定观察属于超平面的哪一侧。</em></p><p id="c370" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">最大间隔分类器看起来是这样的，它有一个平面穿过 p 维空间并将其分成两部分，然后它有两条线，分别位于该平面的一侧和另一侧。这些线被称为边缘，接触这两条线的观察值被称为向量。它们定义了边缘的长度，这些点越远，这两条线离中间超平面越远。那些接触点被称为向量，因为它们是 p 维空间中的向量。</p><p id="979f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">有趣的是，这个分类器仅仅依赖于这些向量，而不依赖于训练集中所有其他可用的观察值。也就是说，如果你移动向量，分类器会改变它的边界，但是如果你移动所有其他的观察值，边界不会改变。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lk"><img src="../Images/12bdaf6354a8eae70de7b1bca2e696f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*La9CfJ3DAnfhz-rJZ28Oeg.png"/></div></figure><p id="35cf" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><em class="le">建筑:</em></p><p id="16a6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">考虑到基于一组 n 个训练观察值<em class="le"> x1 构建最大边缘超平面的任务，…xn </em>关联类"<em class="le"> y" </em>标注为-1 或者 1，我们需要考虑三个等式。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi ll"><img src="../Images/31e932e0d0bbe98e3523ae80d10d7cf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-3xCQ_TcKr5vbdJVomOBsA.png"/></div></div></figure><p id="dba7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">第一个等式意味着通过调整所有变量的系数来定义余量 M，使得余量最大化。第二，每个观测值的类别与其方程的乘积应大于裕度。</p><p id="dba3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">这两个方程确保每个观测值都在超平面的正确一侧，并且距离超平面至少 M。换句话说，一旦我们将一个观察值放入下面的等式中</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/d4fd989635696e0688e34caf952a9eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*goSbBNqHPX77AF4iGojWeQ.png"/></div></figure><p id="fb11" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">我们得到一个小于或大于零的值。除了有一个符号之外，值也有一个大小，如果我们把这个大小乘以它所属的类的标号，那么我们就得到这个点到超平面的距离。</p><p id="8a42" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">解释:定义所有<em class="le">β(β1，β2…β_ p)</em>的值，使得存在具有最大可能值的余量 M，然后对每个观察值进行分类，使得如果将其输出(-1 或 1)乘以其等式</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lm"><img src="../Images/d4fd989635696e0688e34caf952a9eb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*goSbBNqHPX77AF4iGojWeQ.png"/></div></figure><p id="c538" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">你得到一个正数，它的大小大于距离 M，因此，如果你得到一个大于 M 的数，这是最大可能的余量，那么这个数落在这个余量上，或者离它更远。</p><p id="5605" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><em class="le">问题:不可分的情况:</em></p><p id="2ce4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">在某些情况下，这种方法可能行不通。举个例子，图中显示一个简单的超平面无法将那两类分开。为了解决分离不可分离案例的问题，出现了支持向量分类的概念。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi ln"><img src="../Images/362c72bebaba05699b6cd3eef0880ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sl8mbP7qba4nUmiiyOiNlQ.png"/></div></div></figure><p id="0ea1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">支持向量分类器</strong></p><p id="9a0a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">心中的问题:我们知道向量落在两边的幅度决定了我们对其分类的信心。这可以帮助我们区分出一些比另一些更强的观察结果。</p><p id="98cd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">我们已经知道，支持向量定义了最大裕度的长度，即使在定义裕度时没有涉及所有其他的观察，这种方法仍然有一个缺点。问题是最大余量变得对支持向量的变化太敏感。也就是说，少量的向量具有显著改变裕度的能力，并且这种敏感性也可能导致过度拟合。在过度拟合中，模型将试图拟合所有的观察值，这样做它不会泛化，从而在测试观察值时给我们错误的结果。</p><p id="5f37" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">简而言之，如果我们的观察中有一个异常值，它会改变定义边界的方式，这种改变会错误地对测试观察进行分类。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi lo"><img src="../Images/a43a949f9130741dfffd44b092c826d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6cjaRgbm0vqbeIGlXqOPeQ.png"/></div></div></figure><p id="5709" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">然后，我们可能愿意定义一个分类器，对一些观察值进行错误分类，以便:</p><p id="15b2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">1-个体观察的稳健性。</p><p id="fb6f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">2-更好地对大多数训练观察进行分类。</p><p id="02e2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">因此，有了定义最大裕度以使观察值落在裕度的正确一侧的先前属性，我们可以考虑错误地对一些观察值进行分类的另一个属性，以便更好地概括模型并处理异常值。</p><p id="4497" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><em class="le">支持向量分类器=软间隔分类器</em></p><p id="dee4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">详情:</strong></p><p id="3818" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">加入这个新条件，方程看起来像这样:</strong></p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi lp"><img src="../Images/f68c1408361a7bc8a2eb9d09c2039ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wGKvgQEyRZ3f1QcVoJJqQ.png"/></div></div></figure><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/a9eece6bb5dbeb6089d544816d88804e.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*Bjrg3cHzsjTBSxj9pLM4ig.png"/></div></figure><p id="068c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">如果松弛变量等于零，则意味着“第 I 个”观察值在边缘的正确一侧，如果松弛变量大于 0，则意味着“第 I 个”观察值在边缘的错误一侧。<strong class="ke kx">如果松弛变量大于 1，则“第 I 次”观察位于超平面的错误一侧。</strong></p><p id="2c4c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">现在考虑变量 C 的作用，它是所有 n 次观察的所有<em class="le">松弛变量</em>的总和。如果 C=0，这意味着没有观察违反了边界。请注意，松弛变量的值也可以是分数，例如，假设 C=0.6，那么我们可能有三个松弛变量= 0.2 的点或两个松弛变量= 0.3 的点；或者我们有 C=1.6，其中一个松弛变量=0.6(边缘的错误一侧)，另一个松弛变量=1(平面的错误一侧)。因此，要考虑的事情是，对于 C &gt; 0，不超过 C 的观察可以错误的超平面的一边。</p><p id="8b74" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">现在是偏差和方差之间的权衡。这里的 c 是一个通过交叉验证选择的调整参数。当 C 较小时，模型寻求较窄的边界，因为它希望允许少量的观察值落在错误的一边，因此模型变得高度拟合但具有高方差。类似地，当 C 较大时，它更宽容，允许边界更经常地被违反。该模型有较高的偏倚，但方差较低。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi lr"><img src="../Images/f33008bec8c494fca15a65dd22f5435c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJA8j3RjwEj5D6utTtGceQ.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">In left image 8 and 1 has crossed the margin (9, 3, 5, 2 are support vectors), whereas on the right image 1 and 8 has crossed the margin(9, 7, 12, 2 are support vectors), and 11 has crossed the hyperplane</figcaption></figure><p id="4311" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">支持向量分类器决策基于称为支持向量的少量训练观察的事实意味着它对于远离超平面的观察行为是鲁棒的。这使得支持向量分类器不同于任何其他分类器。</p><p id="c03f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">支持向量机</strong></p><p id="1a34" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">虽然这一部分有很多数学知识，但我写它是为了将来的直观理解。此外，这部分有点难以理解，我自己也没有完全掌握。</p><p id="a0a3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">问题</p><p id="f750" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">有些情况下，我们无法以线性方式划分两个类别。例子在下图中。我们看到，线性飞机不能分离属于两个不同类别的训练观测值。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="gh gi ls"><img src="../Images/84b4c7b42282c2828c034f0d24769a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5MP-84vM7lGjmMsDGydFuA.png"/></div></div></figure><p id="e25a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">解决方案:</p><p id="7ee3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">在这种情况下，考虑使用诸如二次或三次项的预测函数来扩大特征空间，以便解决非线性。在支持向量分类器的情况下，我们可以通过使用二次、三次甚至更高阶的多项式来扩大特征空间，从而以类似的方式解决类别之间可能存在非线性边界的问题。</p><p id="988a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">例如，如果我们有 2-D 特征空间，训练观察值为<em class="le"> (X1，Y1)，(X1，Y1)…。(X_n，Y_n) </em>使得它们使用一个线性平面是不可分的，那么我们可以通过有一个第三维<em class="le"> (X1，Y1，Z1)，(X2，Y2，Z2)…..(X_n，Y_n，Z_n) </em></p><p id="80fd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">换句话说，不是使用 p 特征来拟合支持向量分类器(左侧)，而是我们可以使用 2p 特征来拟合支持向量分类器(右侧)。</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/942dbceba0e1dd266c538f082ae7aed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*YpywwO-NOMvObszNQ7jCtg.png"/></div></figure><p id="4720" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">特征空间中的变化可以是任何人喜欢的，只要它成功地将该空间转换成更高维度的空间，从而可以使用线性平面来分离两个类别。</p><p id="37a6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">然而，我们需要某种标准化的计算方法，可以将我们的特征空间转换到更高维度。内核技巧来了。</p><p id="7858" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated"><strong class="ke kx">内核</strong></p><p id="71ed" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">核技巧是扩大特征空间的有效计算方法。内核技巧使用两个向量的内积。两个 r 向量 a 和 b 的内积定义为</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/65cd5a89cb8bf813bfb86678bba8e479.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*pKtdwGVOEVSlJ1Tr1RVDIA.png"/></div></figure><p id="6e11" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">其中 a 和 b 只不过是两个不同的观察值。</p><p id="0c3e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">假设我们有两个向量 X 和 Z，都是二维数据。</p><p id="e05c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">X= (x1，x2)</p><p id="a20e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">Z= (z1，z2)</p><p id="670d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">应用点积的内核技巧会给我们—</p><figure class="jm jn jo jp gt jq gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/ec139934cfffb3e472676c797584efaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*Oj3XPSQp6_rqeWwahPOu1Q.png"/></div></figure><p id="e5a8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">这种核的优点是优化问题的复杂性仅取决于输入特征的维度。当新的测试特征出现时，它们的点积将通过每个训练观察来计算，并且结果将表现为更高维的特征空间。</p><p id="fe3a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">点积是核的一种形式，根据目标，还有许多其他不同类型的核可用。</p><p id="d33d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">注意:</p><p id="499a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">我们知道，每个观察值都有不同的参数或变量来定义其属性，我们可以将它们画在平面上，以查看它们相对于彼此的位置。但是，如果我们从两个不同的来源获得数据，但具有相似的变量或参数，在这种情况下，如果我们在一个平面上绘制它们，那么两个不同来源的观察结果可能会相互混淆。在这种情况下，SVM 是有帮助的。</p><p id="2afd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">参考资料:</p><p id="7a3d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">j 加雷斯。d，威滕。t，哈斯蒂。t 罗伯特。(2013 年 6 月 14 日)。统计学习导论(第九章)。http://faculty.marshall.usc.edu/gareth-james/ISL/<a class="ae kb" href="http://faculty.marshall.usc.edu/gareth-james/ISL/" rel="noopener ugc nofollow" target="_blank"/></p><p id="ee61" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km iy kn ko kp jc kq kr ks jg kt ku kv kw ij bi translated">马丁。(2016 年 6 月 26 日)。“支持向量机——内核和内核技巧”。<a class="ae kb" href="https://pdfs.semanticscholar.org/6c41/c29257597af6b7da10fbb335cd2c2f9bde75.pdf" rel="noopener ugc nofollow" target="_blank">https://pdfs . semantic scholar . org/6c 41/c 29257597 af 6 b 7 da 10 fbb 335 CD 2c 2f 9 bde 75 . pdf</a></p></div></div>    
</body>
</html>