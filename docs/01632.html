<html>
<head>
<title>Attention in Deep Networks with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 深层网络中的注意力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39?source=collection_archive---------0-----------------------#2019-03-17">https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39?source=collection_archive---------0-----------------------#2019-03-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="162e" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/light-on-math" rel="noopener" target="_blank">点亮数学机器学习</a></h2><div class=""/><div class=""><h2 id="e0b9" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">将所有错综复杂的注意力转移到喀拉斯的一条优雅的线上</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1b88455455109f0215537cf0f5961368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x3DdmKFtvjxKuaqHs-EDDw.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Courtesy of Pixabay</figcaption></figure><p id="9f8f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个故事向您介绍了一个 Github 存储库，其中包含一个使用 Keras 后端操作实现的原子最新关注层。可在<a class="ae md" href="https://github.com/thushv89/attention_keras" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">关注 _keras </strong> </a> <strong class="lj jd">获得。</strong></p><p id="2be0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">要访问本系列中我以前的文章，请使用以下信件。</p><p id="0bff" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-ml-attention-with-keras-dc8dbc1fad39"><strong class="lj jd">A</strong></a><strong class="lj jd">B</strong><a class="ae md" href="http://www.thushv.com/computer_vision/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks/" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">C</strong></a><strong class="lj jd"/><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7"><strong class="lj jd">D</strong></a><strong class="lj jd">* E F G H I J</strong><a class="ae md" href="http://www.thushv.com/machine-learning/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence/" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">K</strong></a><strong class="lj jd"/><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"><strong class="lj jd">L</strong><strong class="lj jd">*</strong></a><a class="ae md" href="https://medium.com/p/bee5af0c01aa" rel="noopener"><strong class="lj jd">M</strong></a></p><p id="bb68" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> [ </strong>🔈🔥<strong class="lj jd">最新文章</strong>🔥🔈<strong class="lj jd"/>:<a class="ae md" href="https://medium.com/p/bee5af0c01aa" rel="noopener">M—矩阵分解</a></p><h1 id="8193" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">为什么是 Keras？</h1><p id="7b71" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">随着 TensorFlow 2.0 的推出，很难忽视引人注目的关注(没有双关语！)送给 Keras。有更多的重点是倡导 Keras 实施深度网络。TensorFlow 2.0 中的 Keras 将提供三个强大的 API 来实现深度网络。</p><ul class=""><li id="75f7" class="nb nc it lj b lk ll ln lo lq nd lu ne ly nf mc ng nh ni nj bi translated">顺序 API——这是最简单的 API，首先调用<code class="fe nk nl nm nn b">model = Sequential()</code>并不断添加层，例如<code class="fe nk nl nm nn b">model.add(Dense(...))</code>。</li><li id="4a21" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">功能 API —高级 API，您可以在其中创建具有任意输入/输出的自定义模型。定义一个模型需要非常小心，因为在用户端有很多事情要做。可以使用<code class="fe nk nl nm nn b">model = Model(inputs=[...], outputs=[...])</code>定义模型。</li><li id="8a1d" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">子类化 API——另一个高级 API，可以将模型定义为 Python 类。在这里，您可以在类中定义模型的向前传递，Keras 会自动计算向后传递。那么这个模型可以像使用任何 Keras 模型一样正常使用。</li></ul><p id="bd6e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">更多信息，<a class="ae md" href="https://www.tensorflow.org/guide/keras" rel="noopener ugc nofollow" target="_blank">从 TensorFlow 团队获得第一手信息</a>。然而，请记住，虽然选择高级 API 为实现复杂模型提供了更多的“回旋空间”，但它们也增加了出现错误和各种兔子洞的机会。</p><h1 id="b3d2" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">为什么发这个帖子？</h1><p id="aedc" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">最近，我正在为我正在做的一个项目寻找一个基于 Keras 的注意力层实现或库。我处理了几个已经引起关注的回复。然而，我的努力是徒劳的，试图让他们与以后的 TF 版本。由于几个原因:</p><ul class=""><li id="e772" class="nb nc it lj b lk ll ln lo lq nd lu ne ly nf mc ng nh ni nj bi translated">实现注意力的方式缺乏模块化(对整个解码器而不是解码器的各个展开步骤实现注意力</li><li id="5058" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">使用早期 TF 版本中不推荐使用的函数</li></ul><p id="4ec0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">他们做出了巨大的努力，我尊重所有做出贡献的人。但是我想我会介入并实现一个<strong class="lj jd"> AttentionLayer </strong>，它适用于更多的原子级别，并随着新的 TF 版本而更新。这个库在<a class="ae md" href="https://github.com/thushv89/attention_keras" rel="noopener ugc nofollow" target="_blank">这里</a>可用。</p><p id="a008" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">注</strong>:这是数学机器学习 A-Z  上<strong class="lj jd"> <em class="nt">光系列的一篇文章。你可以在下面的信中找到以前的博客文章。</em></strong></p><p id="98d5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">A B</strong><a class="ae md" href="http://www.thushv.com/computer_vision/light-on-math-machine-learning-intuitive-guide-to-convolution-neural-networks/" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">C</strong></a><strong class="lj jd"/><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-decision-trees-adb2165ccab7"><strong class="lj jd">D</strong></a><strong class="lj jd">* E F G H I J</strong><a class="ae md" href="http://www.thushv.com/machine-learning/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence/" rel="noopener ugc nofollow" target="_blank"><strong class="lj jd">K</strong></a><strong class="lj jd"/><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158"><strong class="lj jd">L</strong></a><strong class="lj jd">* M</strong><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-neural-style-transfer-ef88e46697ee"><strong class="lj jd">N</strong></a><strong class="lj jd">O P Q R S T U V</strong><a class="ae md" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-understanding-word2vec-e0128a460f0f"/></p><h1 id="dfd5" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">介绍</h1><p id="cb6d" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">在这篇文章中，首先你会探究什么是序列对序列模型，然后是为什么注意力对序列模型很重要？接下来，你将学习注意力机制的本质。这篇博文将以解释如何使用注意力层来结束。</p><h1 id="5cab" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">序列到序列模型</h1><p id="c994" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">Sequence to sequence 是一个强大的深度学习模型家族，旨在解决 ML 领域中最疯狂的问题。举个例子，</p><ul class=""><li id="71ba" class="nb nc it lj b lk ll ln lo lq nd lu ne ly nf mc ng nh ni nj bi translated">机器翻译</li><li id="ac35" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">聊天机器人</li><li id="8e3d" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">文本摘要</li></ul><p id="ae4a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有着非常独特和独特的挑战。比如机器翻译要处理不同的<a class="ae md" href="https://en.wikipedia.org/wiki/Word_order" rel="noopener ugc nofollow" target="_blank">语序拓扑</a>(即主语-动词-宾语顺序)。因此它们是解决复杂 NLP 问题的必备武器。</p><p id="7157" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看如何将序列对序列模型用于英法机器翻译任务。</p><p id="2bef" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">序列对序列模型有两个组件，一个<strong class="lj jd">编码器</strong>和一个<strong class="lj jd">解码器</strong>。编码器将源句子编码成一个简洁的向量(称为<strong class="lj jd">上下文向量</strong>)，解码器将上下文向量作为输入，并使用编码的表示来计算翻译。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/9ef7ddbbf4852f7b7404e43af109e39d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkVKlV-Pk7POJUhlWHRgDA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Sequence to sequence model</figcaption></figure><h1 id="a233" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">这种方法有问题吗？</h1><p id="541a" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">这种方法有一个巨大的瓶颈。上下文向量负责将给定源句子中的所有信息编码成一个包含几百个元素的向量。现在给出一点背景，这个向量需要保持:</p><ul class=""><li id="fa4b" class="nb nc it lj b lk ll ln lo lq nd lu ne ly nf mc ng nh ni nj bi translated">关于主语、宾语和动词的信息</li><li id="e29a" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">这些实体之间的相互作用</li></ul><p id="931f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这可能是相当令人生畏的，尤其是对于长句。因此，需要一种更好的解决方案来突破极限。</p><h1 id="a827" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">输入关注！</h1><p id="2469" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">如果解码器能够访问编码器的所有过去状态，而不是仅仅依赖于上下文向量，会怎么样？这正是注意力在做的事情。在每个解码步骤中，解码器都会查看编码器的任何特定状态。这里我们将讨论<a class="ae md" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank"> Bahdanau 注意力</a>。下图描绘了注意力的内部运作。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nv"><img src="../Images/67b72586bcb8e59b83196ff1f2518c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wcxAAgQ0n9gOXLRqhmaLGA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Sequence to sequence with attention</figcaption></figure><p id="f26a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，如图所示，上下文向量已经成为所有过去编码器状态的<strong class="lj jd">加权和。</strong></p><h1 id="6541" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">介绍 attention_keras</h1><p id="bf40" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">由于我前面解释的原因，让一些注意力层在那里工作可能会很麻烦。<a class="ae md" href="https://github.com/thushv89/attention_keras" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="d661" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">使用注意力层</h1><p id="c095" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">您可以将它用作任何其他层。举个例子，</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="953d" class="oa mf it nn b gy ob oc l od oe">attn_layer = AttentionLayer(name='attention_layer')([encoder_out, decoder_out])</span></pre><p id="570d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我还提供了一个玩具神经机器翻译器(NMT)的例子，展示了如何在 NMT ( <a class="ae md" href="https://github.com/thushv89/attention_keras/blob/master/examples/nmt/train.py" rel="noopener ugc nofollow" target="_blank"> nmt/train.py </a>)中使用注意力层。但是让我带你了解一些细节。</p><h1 id="554a" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">用心实施 NMT</h1><p id="ba93" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">在这里，我将简要介绍一下实现 NMT 的步骤。</p><p id="94f3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先定义编码器和解码器输入(源/目标字)。两者都是形状(batch_size，timesteps，vocabulary_size)。</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="d427" class="oa mf it nn b gy ob oc l od oe">encoder_inputs = Input(batch_shape=(batch_size, en_timesteps, en_vsize), name='encoder_inputs')<br/>decoder_inputs = Input(batch_shape=(batch_size, fr_timesteps - 1, fr_vsize), name='decoder_inputs')</span></pre><p id="5255" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">定义编码器(注意<code class="fe nk nl nm nn b">return_sequences=True</code>)</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="b526" class="oa mf it nn b gy ob oc l od oe">encoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='encoder_gru')<br/>encoder_out, encoder_state = encoder_gru(encoder_inputs)</span></pre><p id="29ea" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">定义解码器(注意<code class="fe nk nl nm nn b">return_sequences=True</code></p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="218f" class="oa mf it nn b gy ob oc l od oe">decoder_gru = GRU(hidden_size, return_sequences=True, return_state=True, name='decoder_gru')<br/>decoder_out, decoder_state = decoder_gru(decoder_inputs, initial_state=encoder_state)</span></pre><p id="083e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">定义关注层。注意层的输入是<code class="fe nk nl nm nn b">encoder_out</code>(编码器输出序列)和<code class="fe nk nl nm nn b">decoder_out</code>(解码器输出序列)</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="de5f" class="oa mf it nn b gy ob oc l od oe">attn_layer = AttentionLayer(name='attention_layer')<br/>attn_out, attn_states = attn_layer([encoder_out, decoder_out])</span></pre><p id="369a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">连接<code class="fe nk nl nm nn b">attn_out</code>和<code class="fe nk nl nm nn b">decoder_out</code>作为 softmax 层的输入。</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="f2b9" class="oa mf it nn b gy ob oc l od oe">decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_out, attn_out])</span></pre><p id="8128" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">定义<code class="fe nk nl nm nn b">TimeDistributed</code> Softmax 层并提供<code class="fe nk nl nm nn b">decoder_concat_input</code>作为输入。</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="159b" class="oa mf it nn b gy ob oc l od oe">dense = Dense(fr_vsize, activation='softmax', name='softmax_layer')<br/>dense_time = TimeDistributed(dense, name='time_distributed_layer')<br/>decoder_pred = dense_time(decoder_concat_input)</span></pre><p id="8b95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">定义完整模型。</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="4662" class="oa mf it nn b gy ob oc l od oe">full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)<br/>full_model.compile(optimizer='adam', loss='categorical_crossentropy')</span></pre><p id="152b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就是这样！</p><h1 id="2140" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">甚至支持注意力可视化…</h1><p id="f2a6" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">这不仅实现了注意力，也给了你一个很容易窥视注意力机制的方法。这是可能的，因为这一层返回两者，</p><ul class=""><li id="205c" class="nb nc it lj b lk ll ln lo lq nd lu ne ly nf mc ng nh ni nj bi translated">注意上下文向量(用作解码器的 Softmax 层的额外输入)</li><li id="98c5" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">注意能量值(注意机制的 Softmax 输出)</li></ul><p id="1da1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于每个解码步骤。因此，通过可视化注意力能量值，你可以完全了解注意力在训练/推理过程中在做什么。下面，我来说说这个过程的一些细节。</p><h1 id="4bb3" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">从 NMT 推断并获得关注权重</h1><p id="8b3c" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">从 NMT 推断是繁琐的！因为你必须这么做，</p><ul class=""><li id="aee8" class="nb nc it lj b lk ll ln lo lq nd lu ne ly nf mc ng nh ni nj bi translated">获取编码器输出</li><li id="7816" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">定义一个执行解码器的单个步骤的解码器(因为我们需要提供该步骤的预测作为下一步的输入)</li><li id="70d5" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">使用编码器输出作为解码器的初始状态</li><li id="95db" class="nb nc it lj b lk no ln np lq nq lu nr ly ns mc ng nh ni nj bi translated">执行解码，直到我们得到一个无效字/ <eos>作为输出/或固定步数</eos></li></ul><p id="2f1c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我不打算讨论模型定义。详情请参考<code class="fe nk nl nm nn b">examples/nmt/train.py</code>。让我们来看看如何利用这一点来获得关注权重。</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="072f" class="oa mf it nn b gy ob oc l od oe">for i in range(20):<br/><br/>    dec_out, attention, dec_state = decoder_model.predict([enc_outs, dec_state, test_fr_onehot_seq])<br/>    dec_ind = np.argmax(dec_out, axis=-1)[0, 0]<br/><br/>    ...<br/><br/>    attention_weights.append((dec_ind, attention))<br/></span></pre><p id="aa21" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如你所见，我们正在为每个解码步骤收集注意力权重。</p><p id="d235" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，您只需将这个注意力权重列表传递给<code class="fe nk nl nm nn b">plot_attention_weights</code> ( <a class="ae md" href="https://github.com/thushv89/attention_keras/blob/master/examples/nmt/train.py" rel="noopener ugc nofollow" target="_blank"> nmt/train.py </a>)，以便获得带有其他参数的注意力热图。绘图后的输出可能如下所示。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi of"><img src="../Images/aa769458d19069ddcfbfec0cd9fbc54a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T6oosC9Y9vS5AIh4GzcsNQ.png"/></div></div></figure><h1 id="69fa" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">2022 年 6 月更新</h1><p id="31fc" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">最近有一个关于 AttentionLayer <a class="ae md" href="https://github.com/thushv89/attention_keras/issues/59" rel="noopener ugc nofollow" target="_blank">在 TensorFlow 2.4+版本</a>上不工作的 bug 报告。这导致了如下的神秘错误，</p><pre class="ks kt ku kv gt nw nn nx ny aw nz bi"><span id="1620" class="oa mf it nn b gy ob oc l od oe">TypeError: Exception encountered when calling layer "tf.keras.backend.rnn" (type TFOpLambda).<br/><br/>You are passing KerasTensor(type_spec=TensorSpec(shape=(None, 101), dtype=tf.float32, name=None), name='tf.compat.v1.nn.softmax_1/Softmax:0', description="created by layer 'tf.compat.v1.nn.softmax_1'"), an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.<br/><br/>Call arguments received:<br/>  • step_function=&lt;function AttentionLayer.call.&lt;locals&gt;.energy_step at 0x7f1d5ff279e0&gt;<br/>  • inputs=tf.Tensor(shape=(None, None, 256), dtype=float32)<br/>  • initial_states=['tf.Tensor(shape=(None, 101), dtype=float32)']<br/>  • go_backwards=False<br/>  • mask=None<br/>  • constants=None<br/>  • unroll=False<br/>  • input_length=None<br/>  • time_major=False<br/>  • zero_output_for_mask=False</span></pre><p id="afb7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该错误是由于基于图形的<code class="fe nk nl nm nn b">KerasTensor</code>对象和渴望的<code class="fe nk nl nm nn b">tf.Tensor</code>对象之间的混淆造成的。即将合并的 https://github.com/thushv89/attention_keras/tree/tf2-fix 分公司正在进行调整。</p><h1 id="ee19" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="3955" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">在本文中，我向您介绍了 AttentionLayer 的一个实现。注意力对于序列模型甚至其他类型的模型都是非常重要的。然而，当前的实现要么不是最新的，要么不是非常模块化。所以我稍微挖了一下，用 Keras 后端操作实现了一个注意力层。所以我希望你能在这一层做得很好。如果您有任何问题/发现任何 bug，请随时在 Github 上提交问题。</p><h1 id="b98a" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">欢迎贡献者</h1><p id="77a9" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">我将非常感谢有贡献者，修复任何错误/实施新的注意机制。所以欢迎投稿！</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><p id="a369" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你喜欢我分享的关于数据科学和机器学习的故事，考虑成为会员吧！</p><div class="on oo gp gr op oq"><a href="https://thushv89.medium.com/membership" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd jd gy z fp ov fr fs ow fu fw jc bi translated">通过我的推荐链接加入媒体</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">thushv89.medium.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe lb oq"/></div></div></a></div></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><h1 id="4b75" class="me mf it bd mg mh pf mj mk ml pg mn mo ki ph kj mq kl pi km ms ko pj kp mu mv bi translated">想在深度网络和 TensorFlow 上做得更好？</h1><p id="241a" class="pw-post-body-paragraph lh li it lj b lk mw kd lm ln mx kg lp lq my ls lt lu mz lw lx ly na ma mb mc im bi translated">检查我在这个课题上的工作。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/c902b07566ddcbe9ec0bc8a9c98954cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVW0Dql9IQhFYMY7JLG7YA.png"/></div></div></figure><p id="c3f0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[1] <a class="ae md" href="https://www.manning.com/books/tensorflow-in-action" rel="noopener ugc nofollow" target="_blank">(书)TensorFlow 2 在行动——曼宁</a></p><p id="f769" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2] <a class="ae md" href="https://www.datacamp.com/courses/machine-translation-in-python" rel="noopener ugc nofollow" target="_blank">(视频课程)Python 中的机器翻译</a> — DataCamp</p><p id="ddfc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3] <a class="ae md" href="https://www.amazon.com.au/Natural-Language-Processing-TensorFlow-Ganegedara/dp/1788478312/ref=sr_1_25?dchild=1&amp;keywords=nlp+with+tensorflow&amp;qid=1603009947&amp;sr=8-25" rel="noopener ugc nofollow" target="_blank">(书)TensorFlow 中的自然语言处理 1 </a> — Packt</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><h1 id="ebd3" class="me mf it bd mg mh pf mj mk ml pg mn mo ki ph kj mq kl pi km ms ko pj kp mu mv bi translated">新的！加入我的新 YouTube 频道</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://www.youtube.com/channel/UC1HkxV8PtmWRyQ39MfzmtGA/"><div class="gh gi pl"><img src="../Images/9aa6e53203c7a7784377ba29519f211f.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*TduhS2vQpck51QCn.png"/></div></a></figure><p id="579c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你渴望看到我关于各种机器学习/深度学习主题的视频，请确保加入<a class="ae md" href="https://www.youtube.com/channel/UC1HkxV8PtmWRyQ39MfzmtGA/" rel="noopener ugc nofollow" target="_blank"> DeepLearningHero </a>。</p></div></div>    
</body>
</html>