<html>
<head>
<title>Hacks for Doing Black Magic of Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的黑魔法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56?source=collection_archive---------24-----------------------#2019-10-16">https://towardsdatascience.com/hacks-for-doing-black-magic-of-deep-learning-ab5be5ff5f56?source=collection_archive---------24-----------------------#2019-10-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="6895" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习</h2><div class=""/><div class=""><h2 id="b03f" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">建议，这将有助于你掌握 CNN 的培训</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/976b094d4a2a1314d466723c6b0efd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lTUnlasTVv-8LMg1"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@grakozy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Greg Rakozy</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h2 id="1d77" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">总是超负荷</h2><p id="60fc" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">深度神经网络被称为“黑盒”，在那里很难进行调试。编写完训练脚本后，您无法确定脚本中没有任何错误，也无法预见您的模型是否有足够的参数来学习您需要的转换。</p><p id="b64f" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">安德烈·卡帕西(Andrej Karpathy)关于过度饮食的建议就来自这里。</p><p id="75ff" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在训练开始时，在将所有数据输入到你的网络之前，试着在一个固定的批次上过量，没有任何增加，学习率非常小。如果它不会被过度拟合，这意味着，要么你的模型没有足够的学习能力来进行你需要的转换，要么你的代码中有一个 bug。</p><p id="47da" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">只有成功过拟合后，才合理的开始对整个数据进行训练。</p><h2 id="52f5" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">选择您的标准化</h2><p id="145b" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">归一化是一种强有力的技术，用于克服消失梯度并以更高的学习速率训练网络，而无需仔细的参数初始化。最初在 S.Ioffe 的论文中，提出了对整个批次的特征进行归一化，并且将激活转向单位高斯分布，以学习一个用于所有数据分布(包括测试数据)的通用均值和方差。当您需要预测影像的一个(或多个，如果是多标签分类)标签时，这种方法适用于所有分类任务。但是当你进行图像到图像的翻译任务时，情况就不同了。在这里，对整个数据集学习<em class="my">一个</em>移动平均线和<em class="my">一个</em>移动平均线可能会导致失败。在这种情况下，对于每一幅图像，作为网络的一个输出，你要获得独一无二的结果。<br/>这就是实例规范化的由来。相反，在实例标准化中，为批中的每个图像独立计算统计数据。这种独立性有助于成功地训练网络，以完成图像超分辨率、神经风格转换、图像修复等任务。<br/>所以要小心，不要在图像转换任务中使用迁移学习的常见做法，最著名的预训练网络有 ResNet、MobileNet、Inception。</p><h2 id="7a62" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">越大(不总是)越好</h2><p id="8705" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">众所周知，在训练深度神经网络的过程中，批量越大，收敛越快。而且，经验表明，在某个点之后，批量大小的增加会损害模型的最终性能。在工作中，N.S. Keskar 等人。艾尔。这与以下事实有关:在大批量的情况下，训练倾向于收敛到训练函数的尖锐极小值，而在小批量的情况下，收敛到平坦极小值。因此，在第一种情况下，来自训练函数的灵敏度很高，数据分布的微小变化将损害测试阶段的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/2178812016577d0583c79194bbbc17eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*eyQJUhR5R2ZwSizHErb6Bw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">A Conceptual Sketch of Flat and Sharp Minima. The Y-axis indicates the value of the loss function and the X-axis the parameters.</figcaption></figure><p id="8e5f" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">但是 P. Goyal 等人。艾尔。⁴在论文《精确、大型迷你批处理 SGD:在 1 小时内训练 ImageNet》中表明，可以用高达 8K 的批处理大小训练 ImageNet，而性能不会下降。正如作者所说，<em class="my">优化困难</em>是大型迷你批处理的主要问题，而不是泛化能力差(至少在 ImageNet 上)。作者提出了一个线性比例规则的学习率，取决于批量大小。规则如下</p><blockquote class="na nb nc"><p id="cd5d" class="ma mb my mc b md mt ka mf mg mu kd mi nd mv mk ml ne mw mn mo nf mx mq mr ms ij bi translated">当小批量乘以 k 时，将学习率乘以 k。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/84c17cb30345db4c90b66bef08c9ffae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*DVrRMi9pUy1YoXdGHfcIyw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">ImageNet top-1 validation error vs. minibatch size.</figcaption></figure><p id="7c30" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">小批量也可以被认为是正则化的形式，因为在这种情况下，您将有嘈杂的更新，这有助于避免快速收敛到局部最小值并提高泛化能力。</p><h2 id="3063" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">深度方向可分卷积并不总是你的救星</h2><p id="e1d9" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">近年来，随着性能的提高，神经网络中的参数数量急剧增加，设计高效、低成本的神经网络成为当今的一个课题。<br/>作为 Tensorflow⁵框架的一部分，谷歌提出的解决方案之一是深度方向可分离卷积，这是传统卷积层的一种修改，需要的参数更少。</p><p id="196a" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">让我们假设，我们有一层</p><p id="f8f8" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> fi - </em>输入过滤器</p><p id="c2bf" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my">fo</em>-输出滤波器</p><p id="282d" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my">KH</em>-内核的高度</p><p id="c7c5" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my">kw</em>-内核宽度</p><p id="f295" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在卷积的情况下，层中的参数数量将为</p><blockquote class="na nb nc"><p id="2ea3" class="ma mb my mc b md mt ka mf mg mu kd mi nd mv mk ml ne mw mn mo nf mx mq mr ms ij bi translated">N = kh * kw * fi * fo</p></blockquote><p id="934c" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">我们按照输出滤波器的次数对每个输入滤波器进行卷积，然后求和。</p><p id="2d26" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在深度方向可分离卷积的情况下，它将是</p><blockquote class="na nb nc"><p id="8d56" class="ma mb my mc b md mt ka mf mg mu kd mi nd mv mk ml ne mw mn mo nf mx mq mr ms ij bi translated">N = kh * kw * fi + 1 * 1 * fo</p></blockquote><p id="90a7" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">我们用内核<em class="my"> (kh，kw) </em>对每个输入滤波器进行一次卷积，然后用内核<em class="my"> (1，1)</em>按照输出滤波器的次数对这些中间滤波器进行卷积。</p><p id="28cc" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">现在，让我们来看两个例子。</p><h2 id="f186" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">示例 1</h2><p id="5e09" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">假设该层有以下值</p><p id="b2ae" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> fi = 128 </em></p><p id="0c48" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> fo = 256 </em></p><p id="3b4d" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> kh = 3 </em></p><p id="f3f6" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> kw = 3 </em></p><p id="04e2" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">卷积层中的参数数量将是</p><p id="ae42" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> 3 * 3 * 128 * 256 = 294.912 </em></p><p id="d0d0" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">深度方向可分离卷积中的参数数量为</p><p id="2c23" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> 3 * 3 * 128 + 1 * 1 * 256 = 99.456 </em></p><p id="ac45" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">在深度方向可分离卷积的情况下优势是明显的！！！</p><h2 id="0929" class="lf lg iq bd lh li lj dn lk ll lm dp ln lo lp lq lr ls lt lu lv lw lx ly lz iw bi translated">示例 2</h2><p id="ed8d" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">现在让我们假设该层有其他值</p><p id="f775" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> fi = 128 </em></p><p id="2ba3" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> fo = 256 </em></p><p id="2279" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> kh = 1 </em></p><p id="0b45" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> kw = 1 </em></p><p id="e14c" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">卷积层中的参数数量将是</p><p id="a029" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> 1 * 1* 128 * 256 = 32.768 </em></p><p id="e784" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">深度方向可分离卷积中的参数数量为</p><p id="2c1a" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated"><em class="my"> 1 * 1* 128 + 1 * 1 * 256 = 32.896 </em></p><p id="0764" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">因此，我们可以看到，在第二种情况下，我们没有减少，而是增加了参数的数量。</p></div><div class="ab cl nh ni hu nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="ij ik il im in"><h1 id="bd61" class="no lg iq bd lh np nq nr lk ns nt nu ln kf nv kg lr ki nw kj lv kl nx km lz ny bi translated">参考</h1><p id="498d" class="pw-post-body-paragraph ma mb iq mc b md me ka mf mg mh kd mi lo mj mk ml ls mm mn mo lw mp mq mr ms ij bi translated">[1]安德烈·卡帕西博客，<a class="ae le" href="http://karpathy.github.io/2019/04/25/recipe/" rel="noopener ugc nofollow" target="_blank">http://karpathy.github.io/2019/04/25/recipe/</a></p><p id="deba" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">[2]S.Ioffe，C. Szegedy <a class="ae le" href="https://arxiv.org/pdf/1502.03167.pdf" rel="noopener ugc nofollow" target="_blank">批量归一化:通过减少内部协变量移位加速深度网络训练</a>，2015，第 32 届机器学习国际会议论文集，</p><p id="5af2" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">[3]N. S. Keskar，D. Mudigere，J. Nocedal，M. Smelyanskiy，和 P. T. P. Tang，<a class="ae le" href="https://arxiv.org/pdf/1609.04836.pdf" rel="noopener ugc nofollow" target="_blank">关于深度学习的大批量训练:泛化差距和尖锐极小值</a>，2017，</p><p id="50a0" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">[4]P. Goyal，P. Dollar，R. Girshick，P. Noordhuis，L. Wesolowski，A. Kyrola，，Y. Jia，K. He .，<a class="ae le" href="https://arxiv.org/pdf/1706.02677.pdf" rel="noopener ugc nofollow" target="_blank">精准，大迷你批量 SGD:1 小时训练 ImageNet</a>，2017，arXiv:1706.02677</p><p id="146d" class="pw-post-body-paragraph ma mb iq mc b md mt ka mf mg mu kd mi lo mv mk ml ls mw mn mo lw mx mq mr ms ij bi translated">[5] <a class="ae le" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow 官网</a></p></div></div>    
</body>
</html>