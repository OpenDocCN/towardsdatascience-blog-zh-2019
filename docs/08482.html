<html>
<head>
<title>The Math behind Artificial Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络背后的数学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-heart-of-artificial-neural-networks-26627e8c03ba?source=collection_archive---------9-----------------------#2019-11-17">https://towardsdatascience.com/the-heart-of-artificial-neural-networks-26627e8c03ba?source=collection_archive---------9-----------------------#2019-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="19c5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">像大脑一样的神经元和像心脏一样的数学</h2></div><blockquote class="kf"><p id="7d46" class="kg kh iq bd ki kj kk kl km kn ko kp dk translated">就像大脑由数十亿个高度连接的神经元组成一样，神经网络中的一个基本操作单元是一个类似神经元的节点。它从其他节点获取输入，并将输出发送给其他节点。—费·</p></blockquote><figure class="kr ks kt ku kv kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kq"><img src="../Images/ce537d283d919353ea505850188a41b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjkDbsIw9wPp6BXxmEgSFg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">image by <a class="ae lh" href="https://unsplash.com/@dulgier" rel="noopener ugc nofollow" target="_blank">Anastasia Dulgier</a></figcaption></figure><blockquote class="li lj lk"><p id="bec7" class="ll lm ln lo b lp lq jr lr ls lt ju lu lv lw lx ly lz ma mb mc md me mf mg kp ij bi translated">由于语音识别、计算机视觉和文本处理方面的许多突破性成果，人工神经网络在机器学习研究和行业中产生了许多兴奋。</p></blockquote><p id="0e6b" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">在这篇文章中，我将关注神经元的基本结构，神经元如何工作以及神经网络背后的数学。</p></div><div class="ab cl mk ml hu mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="ij ik il im in"><h1 id="014a" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">感知器</h1><p id="3e9a" class="pw-post-body-paragraph ll lm iq lo b lp nj jr lr ls nk ju lu mh nl lx ly mi nm mb mc mj nn mf mg kp ij bi translated">具有输入层和输出层的简单人工神经元称为感知器。</p><p id="f5ca" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">这个神经元包含什么？</p><ol class=""><li id="1827" class="no np iq lo b lp lq ls lt mh nq mi nr mj ns kp nt nu nv nw bi translated">求和函数</li><li id="3f0d" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">激活功能</li></ol><p id="a68f" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">给感知器的输入由求和函数处理，然后由激活函数处理以获得期望的输出。</p><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/374f9f40164127a01bb073ec3cb276c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*nEzbfyiHLsNUoyJX2sJcMw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Perceptron</figcaption></figure><p id="75e2" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">这是一个简单的感知器，但如果我们有许多输入和巨大的数据，一个感知器是不够的，对不对？？我们必须继续增加神经元。这是基本的神经网络，有输入层，隐藏层，输出层。</p><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/63a64991966b93776db4ad74ee991942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*emS3cwp0ezpMuYsB7YOROw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Neural network</figcaption></figure><p id="6da5" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">我们应该永远记住，神经网络只有一个输入层，输出层，但它可以有多个隐藏层。在上图中，我们可以看到样本神经网络有一个输入层、两个隐藏层和一个输出层。</p><blockquote class="li lj lk"><p id="5940" class="ll lm ln lo b lp lq jr lr ls lt ju lu lv lw lx ly lz ma mb mc md me mf mg kp ij bi translated">作为神经网络的先决条件，让我们知道什么是激活函数和激活函数的类型。</p></blockquote><h1 id="f0fe" class="mr ms iq bd mt mu oi mw mx my oj na nb jw ok jx nd jz ol ka nf kc om kd nh ni bi translated">激活功能</h1><blockquote class="li lj lk"><p id="eee6" class="ll lm ln lo b lp lq jr lr ls lt ju lu lv lw lx ly lz ma mb mc md me mf mg kp ij bi translated">激活函数的主要目的是将神经元输入信号的加权和转换成输出信号。并且该输出信号被用作下一层的输入。</p></blockquote><p id="09a5" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">任何激活函数都应该是可微分的，因为我们使用反向传播机制来减少误差并相应地更新权重。</p><h2 id="1955" class="on ms iq bd mt oo op dn mx oq or dp nb mh os ot nd mi ou ov nf mj ow ox nh oy bi translated">激活功能的类型</h2><figure class="od oe of og gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/c8509c92349fba791ffda76d09cd33f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*QKU2I5_8iNVsM_ewBFtpmg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">image <a class="ae lh" href="http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="ffa2" class="on ms iq bd mt oo op dn mx oq or dp nb mh os ot nd mi ou ov nf mj ow ox nh oy bi translated">乙状结肠的</h2><ol class=""><li id="54c1" class="no np iq lo b lp nj ls nk mh oz mi pa mj pb kp nt nu nv nw bi translated">范围从 0 到 1。</li><li id="72ee" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">x 的小变化会导致 y 的大变化。</li><li id="7f60" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">通常用于二进制分类的输出层。</li></ol><h2 id="82f8" class="on ms iq bd mt oo op dn mx oq or dp nb mh os ot nd mi ou ov nf mj ow ox nh oy bi translated">双曲正切</h2><ol class=""><li id="8da4" class="no np iq lo b lp nj ls nk mh oz mi pa mj pb kp nt nu nv nw bi translated">范围在-1 和 1 之间。</li><li id="f760" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">输出值以零为中心。</li><li id="053d" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">通常用于隐藏层。</li></ol><h2 id="83b4" class="on ms iq bd mt oo op dn mx oq or dp nb mh os ot nd mi ou ov nf mj ow ox nh oy bi translated">RELU(校正线性单位)</h2><ol class=""><li id="45f6" class="no np iq lo b lp nj ls nk mh oz mi pa mj pb kp nt nu nv nw bi translated">范围在 0 和最大值(x)之间。</li><li id="16d8" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">与 sigmoid 和 tanh 函数相比，计算成本较低。</li><li id="2f97" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">隐藏层的默认功能。</li><li id="b726" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">它会导致神经元死亡，这可以通过应用漏 RELU 函数来补偿。</li></ol><blockquote class="li lj lk"><p id="9645" class="ll lm ln lo b lp lq jr lr ls lt ju lu lv lw lx ly lz ma mb mc md me mf mg kp ij bi translated">到目前为止，我们已经学习了感知机和激活功能的先决条件。现在让我们深入研究神经网络(神经网络的核心)的工作原理。</p></blockquote><h1 id="0d6d" class="mr ms iq bd mt mu oi mw mx my oj na nb jw ok jx nd jz ol ka nf kc om kd nh ni bi translated">神经网络的工作</h1><p id="395a" class="pw-post-body-paragraph ll lm iq lo b lp nj jr lr ls nk ju lu mh nl lx ly mi nm mb mc mj nn mf mg kp ij bi translated">神经网络基于两个原理工作</p><ol class=""><li id="563a" class="no np iq lo b lp lq ls lt mh nq mi nr mj ns kp nt nu nv nw bi translated">正向传播</li><li id="d8b6" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">反向传播</li></ol><p id="7eba" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">让我们借助一个例子来理解这些积木。这里我考虑的是单一的输入层，隐藏层，输出层，使理解清晰。</p><h2 id="04b9" class="on ms iq bd mt oo op dn mx oq or dp nb mh os ot nd mi ou ov nf mj ow ox nh oy bi translated">正向传播</h2><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/57d2a5596f1685235f1f17fcdbff8aa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*XSGcCUoLv4GXl9_ONgb4xg.png"/></div></figure><ol class=""><li id="016f" class="no np iq lo b lp lq ls lt mh nq mi nr mj ns kp nt nu nv nw bi translated">考虑到我们有数据，并希望应用二元分类来获得所需的输出。</li><li id="f9cf" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">取一个具有 X1、X2 特征的样本，这些特征将在一组过程中被操作以预测结果。</li><li id="cc58" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">每个特征与一个权重相关联，其中 X1，X2 作为特征，W1，W2 作为权重。这些被用作神经元的输入。</li><li id="2b1b" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">神经元执行这两种功能。a)求和 b)激活。</li><li id="4df4" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">在求和过程中，所有特征都乘以它们的权重，然后对偏差求和。(Y=W1X1+W2X2+b)。</li><li id="a44a" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">这个求和函数应用于激活函数。该神经元的输出乘以权重 W3，并作为输入提供给输出层。</li><li id="e799" class="no np iq lo b lp nx ls ny mh nz mi oa mj ob kp nt nu nv nw bi translated">同样的过程发生在每个神经元中，但我们改变了隐藏层神经元的激活函数，而不是输出层。</li></ol><blockquote class="li lj lk"><p id="7286" class="ll lm ln lo b lp lq jr lr ls lt ju lu lv lw lx ly lz ma mb mc md me mf mg kp ij bi translated">我们只是随机初始化权重，然后继续这个过程。有许多初始化权重的技术。但是，你可能会怀疑这些权重是如何更新的，对吗？？？这将使用反向传播来回答。</p></blockquote><h2 id="e4e0" class="on ms iq bd mt oo op dn mx oq or dp nb mh os ot nd mi ou ov nf mj ow ox nh oy bi translated">反向传播</h2><p id="8219" class="pw-post-body-paragraph ll lm iq lo b lp nj jr lr ls nk ju lu mh nl lx ly mi nm mb mc mj nn mf mg kp ij bi translated">让我们回到我们的微积分基础，我们将使用学生时代学到的链式法则来更新权重。</p><p id="9bbd" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated"><strong class="lo ir">链式法则</strong></p><p id="935c" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated"><strong class="lo ir">链规则</strong>为我们提供了一种寻找复合函数导数的技术，组成复合函数的函数数量决定了需要多少个微分步骤。例如，如果复合函数<em class="ln"> f </em> ( <em class="ln"> x </em>)被定义为</p><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/9853ae5a3d1efd270083b15d99d27b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/0*D97O-4EMsKuqF83w.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Chain rule</figcaption></figure><p id="cfb7" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">让我们将链式法则应用于单个神经元，</p><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/0b1fe56cd82f3c5b704d5ebf1f6fde08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*OMxpIK864eQU3hpafYCynw.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Chain Rule</figcaption></figure><p id="52ec" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">在神经网络中，我们的主要目标是减少误差，为了使之成为可能，我们必须通过反向传播来更新所有的权重。我们需要找到权重的变化，以使误差最小。为此，我们计算 dE/dW1 和 dE/dW2。</p><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/4781335de35a688b4f00802c21454565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*dCj29HqMkzi5NJrARPbNWg.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Change in Weights</figcaption></figure><figure class="od oe of og gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/261525b88bf563dea66ee8aa653a086d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*R01P7HxAuf9YehCn41gfgQ.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Backward Propagation</figcaption></figure><p id="3ef7" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">一旦你计算出与误差有关的重量变化，我们的下一步将是使用梯度下降程序更新重量。请点击查看关于梯度下降<a class="ae lh" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/" rel="noopener ugc nofollow" target="_blank">的更多详情。</a></p><figure class="od oe of og gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f6d3ae4f70e55f50d9d43ae9cfae422c.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*yq_u6PVZQ2h8IlRE5tgAsQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">New weights</figcaption></figure><p id="ae46" class="pw-post-body-paragraph ll lm iq lo b lp lq jr lr ls lt ju lu mh lw lx ly mi ma mb mc mj me mf mg kp ij bi translated">对于所有样本，前向传播和后向传播将是连续的，直到误差达到最小值。</p><h1 id="8b26" class="mr ms iq bd mt mu oi mw mx my oj na nb jw ok jx nd jz ol ka nf kc om kd nh ni bi translated">包扎</h1><p id="6e42" class="pw-post-body-paragraph ll lm iq lo b lp nj jr lr ls nk ju lu mh nl lx ly mi nm mb mc mj nn mf mg kp ij bi translated">在这里，我们了解了什么是神经元，神经网络如何工作的基础知识。但是，这还不够，在更新权重时还存在许多问题，如神经元死亡、权重超出范围等。我将发表一篇新文章，讨论激活函数面临的挑战以及如何减少神经元以减少训练错误。</p><h1 id="5293" class="mr ms iq bd mt mu oi mw mx my oj na nb jw ok jx nd jz ol ka nf kc om kd nh ni bi translated">参考</h1><p id="bfe3" class="pw-post-body-paragraph ll lm iq lo b lp nj jr lr ls nk ju lu mh nl lx ly mi nm mb mc mj nn mf mg kp ij bi translated">链式法则，克里夫笔记，<a class="ae lh" href="https://www.cliffsnotes.com/study-guides/calculus/calculus/the-derivative/chain-rule" rel="noopener ugc nofollow" target="_blank">https://www . cliffs Notes . com/study-guides/calculus/calculus/the-derivative/chain-Rule</a></p><blockquote class="kf"><p id="7f99" class="kg kh iq bd ki kj pf pg ph pi pj kp dk translated">希望你喜欢它！！！敬请期待！！！请对任何疑问或建议发表评论！！！！</p></blockquote></div></div>    
</body>
</html>