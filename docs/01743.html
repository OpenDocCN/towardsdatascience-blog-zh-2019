<html>
<head>
<title>Review: MR-CNN &amp; S-CNN — Multi-Region &amp; Semantic-aware CNNs (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:MR-CNN 和 S-CNN——多区域语义感知 CNN(目标检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=collection_archive---------15-----------------------#2019-03-21">https://towardsdatascience.com/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde?source=collection_archive---------15-----------------------#2019-03-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="14e2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用多区域特征和语义分割特征进行目标检测</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bed30b93639a1f1d518316fcf3480dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GK18z6FfsyE8cRqEM9kEoA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC 2012 Dataset</strong></figcaption></figure><p id="5f23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>这个故事中，回顾了<strong class="ky ir">巴黎东方大学</strong>使用<strong class="ky ir"> MR-CNN &amp; S-CNN </strong>的物体检测方法。提出了两条卷积神经网络(CNN)路径:</p><ul class=""><li id="e640" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">多区域 CNN (MR-CNN) </strong>:使用多个区域捕捉一个对象的几个不同方面的对象表示。</li><li id="cfca" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">分割感知 CNN (S-CNN) </strong>:语义分割信息也被用来提高对象检测的准确性。</li></ul><p id="fb75" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，还提出了细化包围盒的<strong class="ky ir">定位机制。而这是一篇<strong class="ky ir"> 2015 ICCV </strong>论文，有超过<strong class="ky ir"> 200 篇引用</strong>。(<a class="mp mq ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----3bd4e5648fde--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</strong></p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="f9b3" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">概述</h1><ol class=""><li id="4684" class="mb mc iq ky b kz nq lc nr lf ns lj nt ln nu lr nv mh mi mj bi translated"><strong class="ky ir">多区域 CNN (MR-CNN) </strong></li><li id="6015" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr nv mh mi mj bi translated"><strong class="ky ir">分段感知 CNN (S-CNN) </strong></li><li id="8da2" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr nv mh mi mj bi translated"><strong class="ky ir">物体定位</strong></li><li id="65db" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr nv mh mi mj bi translated"><strong class="ky ir">迭代定位机制</strong></li><li id="df13" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr nv mh mi mj bi translated"><strong class="ky ir">结果</strong></li></ol></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="7997" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated"><strong class="ak"> 1。多区域 CNN (MR-CNN) </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/966305ae96926095309f023a8eb1d8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rDCZfACOKQW5jVLE82kXA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Multi-Region CNN (MR-CNN)</strong></figcaption></figure><h2 id="3697" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">1.1.网络体系结构</h2><ul class=""><li id="ec31" class="mb mc iq ky b kz nq lc nr lf ns lj nt ln nu lr mg mh mi mj bi translated">首先，输入图像通过激活图模块，如上所示，并输出激活图。</li><li id="7f9b" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">使用选择性搜索生成区域提议或边界框候选。</li><li id="b984" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">对于每个候选包围盒<em class="oj"> B </em>，生成一组区域{ <em class="oj"> Ri </em> }，其中<em class="oj"> i </em> =1 到<em class="oj"> k，</em>，这就是为什么它被称为多区域。下一小节将详细介绍多区域选择。</li><li id="4079" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">在每个区域适配模块中，对每个区域<em class="oj"> Ri </em>执行 ROI 合并，合并或裁剪的区域经过完全连接的(FC)层。</li><li id="11fb" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">最后，来自所有 FC 层的输出被连接在一起以形成 1D 特征向量，该向量是边界框<em class="oj"> B </em>的对象表示。</li><li id="6aef" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">这里使用的是<a class="ae ok" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG-16 </a> ImageNet 预训练模型。移除最后一个 conv 图层后的最大池化图层。</li></ul><h2 id="cfb9" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">1.2.区域组件</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/dcb7062f1c1be47e3e1c046f07bad0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0gXCzEvG6kbpiRnsN3Bew.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Regions Used in Multi-Region CNN</strong></figcaption></figure><ul class=""><li id="0945" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">区域有两种类型:<strong class="ky ir">矩形((a)-(f)) </strong>和<strong class="ky ir">矩形圆环((g)-(j)) </strong>，如上图。</li><li id="c79f" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">原盒(a)</strong>:R-CNN 中使用的那个。</li><li id="747d" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">半框，(b)-(e) </strong>:这些区域旨在使对象表示相对于遮挡更加<strong class="ky ir">健壮。</strong></li><li id="1560" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">中心区域，(f)-(g) </strong>:这些区域是为了使物体表现<strong class="ky ir">较少受到它旁边的其他物体或其背景</strong>的干扰。</li><li id="b297" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">边界区域，(h)-(i) </strong>:这些区域旨在使对象表示<strong class="ky ir">对不准确的定位</strong>更加敏感。</li><li id="0c3d" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">上下文区域(j) </strong>:该区域<strong class="ky ir">聚焦围绕对象</strong>的上下文外观。</li><li id="5f95" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">使用这些区域有帮助的原因有两个。</li><li id="0ea0" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">被屏蔽区域被设置为零。</li></ul><h2 id="aab3" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">区别特征多样化</h2><ul class=""><li id="ed54" class="mb mc iq ky b kz nq lc nr lf ns lj nt ln nu lr mg mh mi mj bi translated">这有助于使整体识别模型捕获的区别因素多样化。此处进行消融研究的模型 A 使用(A)和(I ),模型 B 使用(A)和与(A)尺寸相同的改良(I)。在 PASCAL VOC 2007 测试集上，模型 A 得到 64.1%的 mAP，模型 B 得到 62.9%，比模型 A 低 1.2 个百分点。</li></ul><h2 id="54d5" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">本地化感知表示</h2><ul class=""><li id="ffb3" class="mb mc iq ky b kz nq lc nr lf ns lj nt ln nu lr mg mh mi mj bi translated">对于给定的候选检测框，多区域的使用对每种类型的区域上允许的可视内容施加了软约束。</li></ul></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="b532" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated"><strong class="ak"> 2。分段感知 CNN (S-CNN) </strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/c738a240e466eb453f6759631cc0f888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2v5JXdWTFjHaq16SBdMdQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Multi-Region CNN (MR-CNN) Extended with Segmentation-aware CNN (S-CNN)</strong></figcaption></figure><ul class=""><li id="b3c0" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">分割和检测之间有着密切的联系。和分割相关的线索通常有助于对象检测。</li><li id="1d30" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">增加了两个模块:<strong class="ky ir">支持语义分割特征的激活图模块</strong>和<strong class="ky ir">支持语义分割特征的区域自适应模块。</strong></li><li id="0bf1" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">这里没有用于训练的附加注释。</li><li id="1c7b" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><a class="ae ok" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>用于激活地图模块。</li><li id="d2b6" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">最后一个 FC7 层通道数从 4096 更改为 512。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/2b7d684fd8ec95653d33d3d3cc7ff935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*Rf6oJ6O-tSF-utZ5m9ppUg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Bounding Box (Left), Segmentation Mask Based on Bounding Box (Middle), Foreground Probabilities (Right)</strong></figcaption></figure><ul class=""><li id="cacd" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">使用弱监督训练</strong>策略。<strong class="ky ir">使用边界框注释</strong>创建人工前景特定类别分割掩模。</li><li id="3d4f" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">更具体地说，图像的地面真实边界框被投影到<a class="ae ok" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>的最后一个隐藏层的空间域上，位于投影框内的“像素”被标记为前景，而其余的被标记为背景。</li><li id="4647" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">在使用掩码训练<a class="ae ok" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>之后，最后一个分类层被删除。只有剩余的<a class="ae ok" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>被使用。</li><li id="5851" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">尽管它是弱监督训练，如上所示的前景概率仍然携带一些信息，如上所示。</li><li id="25fa" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">使用的边界框比原始边界框大 1.5 倍。</li></ul></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="0d8b" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated"><strong class="ak"> 3。物体定位</strong></h1><h2 id="15a6" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">3.1.用于包围盒回归的 CNN 区域适应模块</h2><ul class=""><li id="6e6f" class="mb mc iq ky b kz nq lc nr lf ns lj nt ln nu lr mg mh mi mj bi translated">训练额外的区域适应模块来预测对象边界框。</li><li id="450c" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">它由两个隐藏的 FC 层和一个预测层组成，每个类别输出 4 个值(即边界框)。</li><li id="d54f" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">将候选框放大 1.3 倍提供了显著的提升。</li></ul><h2 id="b22b" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">3.2.迭代定位</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9f52c2a8f3d30e048d4fdb9c14773a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/1*7vBsm9dRZ4qy6Reaqr8WVw.png"/></div></figure><ul class=""><li id="b2b6" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><em class="oj"> Bt_c </em>:类<em class="oj"> c </em>和图像<em class="oj"> X </em>迭代<em class="oj"> t </em>生成的<em class="oj"> Nc，t </em>包围盒集合。</li><li id="e5ec" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">一开始，<em class="oj"> t </em> =1，建议<em class="oj"> B0_c </em>通过选择性搜索产生。</li><li id="a006" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">对于从 t=1 开始的每次迭代，…，T，<em class="oj"> Bt_c </em>被更新。T=2 通常就足够了。</li></ul><h2 id="6225" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">3.3.包围盒投票</h2><ul class=""><li id="2e4b" class="mb mc iq ky b kz nq lc nr lf ns lj nt ln nu lr mg mh mi mj bi translated">在迭代定位之后，执行包围盒投票。</li><li id="26d9" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">最后一次迭代<em class="oj"> T </em>后，从<em class="oj"> t </em> =1 到<em class="oj"> t </em> = <em class="oj"> T </em>的候选检测{ <em class="oj"> Dt_c </em> }合并形成<em class="oj"> D_c </em>。<em class="oj"> D_c </em> = { <em class="oj"> st_i，c </em>，<em class="oj"> Bt_i，c </em> }其中<em class="oj"> s </em>为分类得分，<em class="oj"> B </em>为对应的边界框。</li><li id="b9f7" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">首先，非最大抑制(NMS)应用于<em class="oj"> D_c </em>，使用 0.3 的 IoU 阈值，并产生检测<em class="oj"> Y_c </em>。</li><li id="61fb" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">然后基于权重执行进一步的细化:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/80f64a734363799bf2569f81efe95638.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*QJ5YuPVKM5870GFhfmjKFw.png"/></div></figure><ul class=""><li id="89d6" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">权重<em class="oj"> w </em> =max(0，<em class="oj"> s </em>)，其中<em class="oj"> s </em>为分类分数。</li></ul><h2 id="4db0" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">3.4.程序总结</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/9f226dcb575f98ee413f2394ca661f24.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*Wnxwkc_0F5jCns7jcIE7QA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">Object Localization: Candidates (Blue), Ground Truth (Green), and False Positives (Red)</strong></figcaption></figure><ul class=""><li id="533b" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">第 1 步:初始箱式建议书(仅显示相关的建议书)。</li><li id="03da" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">第二步:第一次 CNN 包围盒回归后。</li><li id="2bb7" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">第三步:第二次 CNN 包围盒回归后。</li><li id="54c4" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">步骤 4:步骤 2 中的包围盒加上步骤 3 中的包围盒。</li><li id="2960" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated">步骤 5:投票后的边界框。</li></ul></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="9df3" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">4.结果</h1><h2 id="7797" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">4.1.PASCAL VOC2007</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/9419db492de9afd3c8036e168305884a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g20gfrH_MPRACu_XJw26Lw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC2007 Test Set</strong></figcaption></figure><ul class=""><li id="41ab" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">所提出的方法单独使用原始盒优于所有其他单独使用的盒，并且也优于单独使用语义感知区域。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/ba2dcb495fca0366bf48791ae1713cd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RFHSoCc4neNhQNVgybKr5g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC2007 Test Set</strong></figcaption></figure><ul class=""><li id="2167" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated"><strong class="ky ir">只有单个原盒</strong> : 61.7%图。</li><li id="2feb" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir"> MR-CNN </strong>:采用多区域，66.2%地图，可见其新颖之处。</li><li id="d75e" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">MR-CNN&amp;S-CNN</strong>:67.5%图。</li><li id="eb06" class="mb mc iq ky b kz mk lc ml lf mm lj mn ln mo lr mg mh mi mj bi translated"><strong class="ky ir">MR-CNN&amp;S-CNN&amp;Loc</strong>:74.9% mAP，胜过<a class="ae ok" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener"> R-CNN </a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/e2ab797703cd1b5b72f0586503182c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RZjakOnbk9rwSKmy-lxug.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC2007 Test Set</strong></figcaption></figure><ul class=""><li id="e171" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">使用 0.7 IoU 阈值，<strong class="ky ir">MR-CNN&amp;S-CNN&amp;Loc</strong>依然表现最佳。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/4f7c1adb5c7d27af29800a49739f176e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Co3dILD7GmuOjvWGObVibA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC2007 Test Set, Trained with Extra Data</strong></figcaption></figure><ul class=""><li id="cc7c" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">经过额外数据训练，<strong class="ky ir"> MR-CNN &amp; S-CNN &amp; Loc </strong>获得 78.2%的 mAP，优于<a class="ae ok" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener"> NoC </a>、<a class="ae ok" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> Fast R-CNN </a>和<a class="ae ok" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">Fast R-CNN</a>。</li></ul><h2 id="1a9f" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">4.2.帕斯卡 VOC2012</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/0c866f9081993dff0e51933c9e24fe29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7fzWazJBsdD_o0K7Q9hSQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC2012 Test Set</strong></figcaption></figure><ul class=""><li id="2ccd" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">与 VOC2007 类似，<strong class="ky ir"> MR-CNN &amp; S-CNN &amp; Loc </strong>以 70.7%的 mAP 表现最好。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/775756cdc65535ab9c6f183476773505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7iI-Exu9iJR69zPNSP31cQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><strong class="bd kv">PASCAL VOC2012 Test Set, Trained with Extra Data</strong></figcaption></figure><ul class=""><li id="1c62" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">经过额外数据训练，<strong class="ky ir">MR-CNN&amp;S-CNN&amp;Loc</strong>获得 73.9% mAP，优于<a class="ae ok" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener"> NoC </a>、<a class="ae ok" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89"> YOLOv1 </a>、<a class="ae ok" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener"> Fast R-CNN </a>和<a class="ae ok" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快 R-CNN </a>。</li></ul></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h2 id="99ad" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">参考</h2><p id="07b2" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ow lh li lj ox ll lm ln oy lp lq lr ij bi translated">【2015 ICCV】【MR-CNN &amp; S-CNN】<br/><a class="ae ok" href="https://arxiv.org/abs/1505.01749" rel="noopener ugc nofollow" target="_blank">通过多区域的对象检测&amp;语义分割感知的 CNN 模型</a></p><h2 id="470a" class="nx mz iq bd na ny nz dn ne oa ob dp ni lf oc od nk lj oe of nm ln og oh no oi bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ow lh li lj ox ll lm ln oy lp lq lr ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(情)(况)(,)(还)(是)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">物体检测<br/></strong><a class="ae ok" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae ok" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae ok" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae ok" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae ok" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae ok" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae ok" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a><a class="ae ok" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">ION</a><a class="ae ok" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath Net</a>【T21 [ <a class="ae ok" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae ok" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae ok" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae ok" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae ok" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae ok" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">语义切分<br/></strong><a class="ae ok" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae ok" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae ok" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae ok" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae ok" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae ok" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae ok" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae ok" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae ok" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">生物医学图像分割<br/></strong>[<a class="ae ok" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae ok" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae ok" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae ok" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae ok" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae ok" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae ok" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实例分割 <br/> </strong> <a class="ae ok" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS </a> <a class="ae ok" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">DeepMask </a> <a class="ae ok" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">SharpMask </a> <a class="ae ok" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">MultiPathNet </a> <a class="ae ok" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC </a>】 <a class="ae ok" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">InstanceFCN </a> <a class="ae ok" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS </a></p><p id="58de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(T38) 人类姿势估计 (T39) <br/> [(T41) 汤普森 NIPS'14 [T42)]</p></div></div>    
</body>
</html>