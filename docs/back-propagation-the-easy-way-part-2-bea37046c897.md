# åå‘ä¼ æ’­ï¼Œç®€å•çš„æ–¹æ³•(ç¬¬ 2 éƒ¨åˆ†)

> åŸæ–‡ï¼š<https://towardsdatascience.com/back-propagation-the-easy-way-part-2-bea37046c897?source=collection_archive---------7----------------------->

## åå‘ä¼ æ’­çš„å®é™…å®ç°

![](img/1061245b7af2b26566ce6e638336203e.png)

**æ›´æ–°**:å­¦ä¹ å’Œç»ƒä¹ å¼ºåŒ–å­¦ä¹ çš„æœ€å¥½æ–¹å¼æ˜¯å» http://rl-lab.com

åœ¨[ç¬¬ä¸€éƒ¨åˆ†](/back-propagation-the-easy-way-part-1-6a8cde653f65)ä¸­ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°åå‘ä¼ æ’­æ˜¯å¦‚ä½•ä»¥æœ€å°åŒ–æˆæœ¬å‡½æ•°çš„æ–¹å¼å¯¼å‡ºçš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å®ç°æ–¹é¢ï¼Œä»¥åŠä¸€äº›é¿å…å¸¸è§é™·é˜±çš„æœ€ä½³å®è·µã€‚

æˆ‘ä»¬ä»ç„¶å¤„äºç®€å•æ¨¡å¼ï¼Œä¸€æ¬¡å¤„ç†ä¸€ä¸ªè¾“å…¥ã€‚

## å›¾å±‚ç±»åˆ«

è€ƒè™‘ä¸‹å›¾æ‰€ç¤ºçš„å…¨è¿æ¥ç¥ç»ç½‘ç»œã€‚

![](img/728553a46ba5b5c6a4c18b92f699faac.png)

æ¯ä¸ªå±‚å°†ç”±åŒ…å«æƒé‡ã€æ¿€æ´»å€¼(å±‚çš„è¾“å‡º)ã€æ¢¯åº¦ dZ(å›¾åƒä¸­æœªç¤ºå‡º)ã€ç´¯ç§¯è¯¯å·®Î´(ğš«)ã€ä»¥åŠæ¿€æ´»å‡½æ•° ***f(x)*** åŠå…¶å¯¼æ•°***fâ€™(x)***çš„å±‚å¯¹è±¡æ¥å»ºæ¨¡ã€‚å­˜å‚¨ä¸­é—´å€¼çš„åŸå› æ˜¯ä¸ºäº†é¿å…æ¯æ¬¡éœ€è¦æ—¶éƒ½è¦è®¡ç®—å®ƒä»¬ã€‚

**å»ºè®®:**æœ€å¥½å›´ç»•å‡ ä¸ªç±»æ¥ç»„ç»‡ä»£ç ï¼Œé¿å…æŠŠæ‰€æœ‰ä¸œè¥¿éƒ½å¡è¿›æ•°ç»„ï¼Œå› ä¸ºå¾ˆå®¹æ˜“ä¸¢å¤±ã€‚

![](img/755921a64729226da58daff5a856629f.png)

è¯·æ³¨æ„ï¼Œè¾“å…¥å›¾å±‚ä¸ä¼šç”±å›¾å±‚å¯¹è±¡è¡¨ç¤ºï¼Œå› ä¸ºå®ƒåªåŒ…å«ä¸€ä¸ªçŸ¢é‡ã€‚

```
**class** Layer:

    **def** __init__(self, dim, id, act, act_prime, 
                 isoutputLayer = **False**):
        self.weight = 2 * np.random.random(dim) - 1
        self.delta = **None** self.A = **None** self.activation = act
        self.activation_prime = act_prime
        self.isoutputLayer = isoutputLayer
        self.id = id
```

Layer ç±»çš„æ„é€ å‡½æ•°å°†ä»¥ä¸‹å†…å®¹ä½œä¸ºå‚æ•°:

*   dim:æƒé‡çŸ©é˜µçš„ç»´æ•°ï¼Œ
*   id:æ•´æ•°ä½œä¸ºå±‚çš„ idï¼Œ
*   actï¼Œact_prime:æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°ï¼Œ
*   isoutputlayer:å¦‚æœè¯¥å±‚æ˜¯è¾“å‡ºï¼Œåˆ™ä¸º Trueï¼Œå¦åˆ™ä¸º Falseã€‚

å®ƒå°†æƒé‡éšæœºåˆå§‹åŒ–ä¸º-1 åˆ°+1 ä¹‹é—´çš„æ•°å­—ï¼Œå¹¶è®¾ç½®è¦åœ¨å¯¹è±¡å†…éƒ¨ä½¿ç”¨çš„ä¸åŒå˜é‡ã€‚

å›¾å±‚å¯¹è±¡æœ‰ä¸‰ç§æ–¹æ³•:

*   å‘å‰ï¼Œè®¡ç®—å±‚è¾“å‡ºã€‚
*   å‘åï¼Œå°†ç›®æ ‡å’Œè¾“å‡ºä¹‹é—´çš„è¯¯å·®ä¼ æ’­å›ç½‘ç»œã€‚
*   æ›´æ–°ï¼Œæ ¹æ®æ¢¯åº¦ä¸‹é™æ›´æ–°æƒé‡ã€‚

```
**def** forward(self, x):
    z = np.dot(x, self.weight)
    self.A = self.activation(z)
    self.dZ = self.activation_prime(z);
```

forward å‡½æ•°é€šè¿‡è¾“å…¥ **x** è®¡ç®—å¹¶è¿”å›å±‚çš„è¾“å‡ºï¼Œå¹¶è®¡ç®—å’Œå­˜å‚¨è¾“å‡º A = activation (W.X)ã€‚å®ƒè¿˜è®¡ç®—å¹¶å­˜å‚¨ dZï¼Œå³è¾“å‡ºç›¸å¯¹äºè¾“å…¥çš„å¯¼æ•°ã€‚

åå‘å‡½æ•°é‡‡ç”¨ä¸¤ä¸ªå‚æ•°ï¼Œç›®æ ‡ y å’Œ rightLayerï¼Œå³å‡è®¾å½“å‰å±‚æ˜¯ğ“.çš„å±‚(ğ“-1)

å®ƒè®¡ç®—ä»è¾“å‡ºå‘å·¦ä¼ æ’­åˆ°ç½‘ç»œèµ·ç‚¹çš„ç´¯ç§¯è¯¯å·®å¢é‡ã€‚

**é‡è¦æç¤º**:ä¸€ä¸ªå¸¸è§çš„é”™è¯¯æ˜¯è®¤ä¸ºåå‘ä¼ æ’­æ˜¯æŸç§ç¯å›ï¼Œå…¶ä¸­è¾“å‡ºè¢«å†æ¬¡æ³¨å…¥ç½‘ç»œã€‚æ‰€ä»¥ä¸ç”¨***dZ = self . activation _ prime(z)ï¼›*** æœ‰çš„ç”¨é€” ***self.activation_prime(ä¸€)*** *ã€‚*è¿™æ˜¯é”™è¯¯çš„ï¼Œå› ä¸ºæˆ‘ä»¬è¦åšçš„åªæ˜¯è®¡ç®—å‡ºè¾“å‡º a ç›¸å¯¹äºè¾“å…¥ z çš„å˜åŒ–ï¼Œè¿™æ„å‘³ç€æ ¹æ®é“¾å¼æ³•åˆ™è®¡ç®—å¯¼æ•°**âˆ‚a/âˆ‚z**= âˆ‚g(z)/âˆ‚z =**gâ€™(z)**ã€‚è¿™ä¸ªè¯¯å·®å¯èƒ½æ˜¯å› ä¸ºåœ¨ sigmoid æ¿€æ´»å‡½æ•° **a = ğœ(z)** çš„æƒ…å†µä¸‹ï¼Œå¯¼æ•°**ğœ'(z)= ğœ(z)*(1-ğœ(z)= a *(1-a)ã€‚**è¿™ç»™äººä¸€ç§è¾“å‡ºè¢«æ³¨å…¥ç½‘ç»œçš„é”™è§‰ï¼Œè€Œäº‹å®æ˜¯æˆ‘ä»¬æ­£åœ¨è®¡ç®— **ğœ'(z).**

```
**def** backward(self, y, rightLayer):
    **if** self.isoutputLayer:
        error =  self.A - y
        self.delta = np.atleast_2d(error * self.dZ)
    **else**:
        self.delta = np.atleast_2d(
            rightLayer.delta.dot(rightLayer.weight.T)
            * self.dZ)
    **return** self.delta
```

backward å‡½æ•°çš„ä½œç”¨æ˜¯æ ¹æ®ä»¥ä¸‹å…¬å¼è®¡ç®—å¹¶è¿”å›å¢é‡:

![](img/80833e999934d3145cdb64d25f7175ad.png)

æœ€åï¼Œæ›´æ–°å‡½æ•°ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ›´æ–°å½“å‰å±‚çš„æƒé‡ã€‚

```
**def** update(self, learning_rate, left_a):
    a = np.atleast_2d(left_a)
    d = np.atleast_2d(self.delta)
    ad = a.T.dot(d)
    self.weight -= learning_rate * ad
```

## ç¥ç»ç½‘ç»œç±»

æ­£å¦‚äººä»¬å¯èƒ½çŒœæµ‹çš„é‚£æ ·ï¼Œå±‚å½¢æˆäº†ä¸€ä¸ªç½‘ç»œï¼Œå› æ­¤ç±» NeuralNetwork ç”¨äºç»„ç»‡å’Œåè°ƒå±‚ã€‚
å®ƒçš„æ„é€ å™¨é‡‡ç”¨å±‚çš„é…ç½®ï¼Œè¿™æ˜¯ä¸€ä¸ªé•¿åº¦å†³å®šç½‘ç»œå±‚æ•°çš„æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ å®šä¹‰ç›¸åº”å±‚ä¸­çš„èŠ‚ç‚¹æ•°ã€‚
ä¾‹å¦‚[2ï¼Œ4ï¼Œ5ï¼Œ]è¡¨ç¤ºç½‘ç»œæœ‰ 4 å±‚ï¼Œè¾“å…¥å±‚æœ‰ 2 ä¸ªèŠ‚ç‚¹ï¼Œæ¥ä¸‹æ¥çš„éšè—å±‚åˆ†åˆ«æœ‰ 4 ä¸ªå’Œ 5 ä¸ªèŠ‚ç‚¹ï¼Œè¾“å‡ºå±‚æœ‰ 1 ä¸ªèŠ‚ç‚¹ã€‚ç¬¬äºŒä¸ªå‚æ•°æ˜¯ç”¨äºæ‰€æœ‰å±‚çš„æ¿€æ´»å‡½æ•°çš„ç±»å‹ã€‚

fit å‡½æ•°æ˜¯æ‰€æœ‰è®­ç»ƒå‘ç”Ÿçš„åœ°æ–¹ã€‚å®ƒé¦–å…ˆé€‰æ‹©ä¸€ä¸ªè¾“å…¥æ ·æœ¬ï¼Œè®¡ç®—æ‰€æœ‰å±‚ä¸Šçš„å‰å‘ï¼Œç„¶åè®¡ç®—ç½‘ç»œè¾“å‡ºå’Œç›®æ ‡å€¼ä¹‹é—´çš„è¯¯å·®ï¼Œå¹¶é€šè¿‡ä»¥ç›¸åçš„é¡ºåºè°ƒç”¨æ¯å±‚çš„åå‘å‡½æ•°(ä»æœ€åä¸€å±‚å¼€å§‹åˆ°ç¬¬ä¸€å±‚)å°†è¯¥è¯¯å·®ä¼ æ’­åˆ°ç½‘ç»œã€‚
æœ€åï¼Œä¸ºæ¯ä¸€å±‚è°ƒç”¨æ›´æ–°å‡½æ•°æ¥æ›´æ–°æƒé‡ã€‚

è¿™äº›æ­¥éª¤é‡å¤çš„æ¬¡æ•°ç”±å‚æ•° epoch ç¡®å®šã€‚

è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥è°ƒç”¨é¢„æµ‹å‡½æ•°æ¥æµ‹è¯•è¾“å…¥ã€‚é¢„æµ‹åŠŸèƒ½åªæ˜¯æ•´ä¸ªç½‘ç»œçš„ä¸€ä¸ªå‰é¦ˆã€‚

```
**class** NeuralNetwork:

    **def** __init__(self, layersDim, activation=**'tanh'**):
        **if** activation == **'sigmoid'**:
            self.activation = sigmoid
            self.activation_prime = sigmoid_prime
        **elif** activation == **'tanh'**:
            self.activation = tanh
            self.activation_prime = tanh_prime
        **elif** activation == **'relu'**:
            self.activation = relu
            self.activation_prime = relu_prime

        self.layers = []
        **for** i **in** range(1, len(layersDim) - 1):
            dim = (layersDim[i - 1] + 1, layersDim[i] + 1)
            self.layers.append(Layer(dim, i, self.activation, self.activation_prime))

        dim = (layersDim[i] + 1, layersDim[i + 1])
        self.layers.append(Layer(dim, len(layersDim) - 1, self.activation, self.activation_prime, **True**))# train the network
    **def** fit(self, X, y, learning_rate=0.1, epochs=10000):
        *# Add column of ones to X
        # This is to add the bias unit to the input layer* ones = np.atleast_2d(np.ones(X.shape[0]))
        X = np.concatenate((ones.T, X), axis=1)

        **for** k **in** range(epochs):

            i = np.random.randint(X.shape[0])
            a = X[i]

            *# compute the feed forward* **for** l **in** range(len(self.layers)):
                a = self.layers[l].forward(a)

            *# compute the backward propagation* delta = self.layers[-1].backward(y[i], **None**)

            **for** l **in** range(len(self.layers) - 2, -1, -1):
                delta = self.layers[l].backward(delta, self.layers[l+1])

            *# update weights* a = X[i]
            **for** layer **in** self.layers:
                layer.update(learning_rate, a)
                a = layer.A# predict input
    **def** predict(self, x):
        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)
        **for** l **in** range(0, len(self.layers)):
            a = self.layers[l].forward(a)
        **return** a
```

## è¿è¡Œç½‘ç»œ

ä¸ºäº†è¿è¡Œç½‘ç»œï¼Œæˆ‘ä»¬ä»¥ Xor å‡½æ•°çš„è¿‘ä¼¼ä¸ºä¾‹ã€‚

![](img/bdf56f3643ce4cbeec7299b83171cf1f.png)

æˆ‘ä»¬å°è¯•äº†å‡ ç§ç½‘ç»œé…ç½®ï¼Œä½¿ç”¨ä¸åŒçš„å­¦ä¹ é€Ÿç‡å’Œå†å…ƒè¿­ä»£ã€‚ç»“æœå¦‚ä¸‹æ‰€ç¤º:

```
 Result with tanh
[0 0] [-0.00011187]
[0 1] [ 0.98090146]
[1 0] [ 0.97569382]
[1 1] [ 0.00128179]Result with sigmoid
[0 0] [ 0.01958287]
[0 1] [ 0.96476513]
[1 0] [ 0.97699611]
[1 1] [ 0.05132127]Result with relu
[0 0] [ 0.]
[0 1] [ 1.]
[1 0] [ 1.]
[1 1] [ 4.23272528e-16]
```

å»ºè®®æ‚¨å°è¯•ä¸åŒçš„é…ç½®ï¼Œè‡ªå·±çœ‹çœ‹å“ªç§é…ç½®èƒ½æä¾›æœ€ä½³å’Œæœ€ç¨³å®šçš„ç»“æœã€‚

## æºä»£ç 

å®Œæ•´çš„ä»£ç å¯ä»¥ä»[è¿™é‡Œ](https://gist.github.com/ZSalloum/54703842f8a06e38fd76934579a6c814)ä¸‹è½½ã€‚

## ç»“è®º

åå‘ä¼ æ’­å¯èƒ½ä¼šä»¤äººå›°æƒ‘å¹¶ä¸”éš¾ä»¥å®ç°ã€‚ä½ å¯èƒ½ä¼šæœ‰ä¸€ç§é”™è§‰ï¼Œè®¤ä¸ºä½ é€šè¿‡ç†è®ºæŒæ¡äº†å®ƒï¼Œä½†äº‹å®æ˜¯ï¼Œå½“å®æ–½å®ƒæ—¶ï¼Œå¾ˆå®¹æ˜“é™·å…¥è®¸å¤šé™·é˜±ã€‚ä½ åº”è¯¥æœ‰è€å¿ƒå’Œæ¯…åŠ›ï¼Œå› ä¸ºåå‘ä¼ æ’­æ˜¯ç¥ç»ç½‘ç»œçš„åŸºçŸ³ã€‚

## ç›¸å…³æ–‡ç« 

ç¬¬ä¸€éƒ¨åˆ†:[åå‘ä¼ æ’­çš„ç®€å•è¯¦è§£](/back-propagation-the-easy-way-part-1-6a8cde653f65)
ç¬¬ä¸‰éƒ¨åˆ†:[å¦‚ä½•å¤„ç†çŸ©é˜µçš„ç»´æ•°](https://medium.com/@zsalloum/back-propagation-the-easy-way-part-3-cc1de33e8397)