<html>
<head>
<title>Attention Craving RNNS: Building Up To Transformer Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">注意力渴望 RNNS:建立变压器网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-craving-rnns-a-journey-into-attention-mechanisms-eec840fbc26f?source=collection_archive---------5-----------------------#2019-04-04">https://towardsdatascience.com/attention-craving-rnns-a-journey-into-attention-mechanisms-eec840fbc26f?source=collection_archive---------5-----------------------#2019-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/104665b2c14e94aba7d2692a51bd4cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*3tQEkx5YJPmkbOHd.gif"/></div></div></figure><p id="f004" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将注意力放在你的神经网络上有点像想在工作时小睡一会儿。你知道这对你更好，每个人都想这样做，但每个人都害怕。</p><p id="eeab" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我今天的目标是什么都不假设，用动画解释细节，让数学再次伟大(MMGA？呃…)</p><p id="2021" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">这里我们将介绍:</strong></p><ol class=""><li id="c7c3" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">简短的 RNN 评论。</li><li id="0bff" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">短序列到序列模型审查。</li><li id="661c" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">RNNs 中的注意。</li><li id="a46a" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">注意力的改善。</li><li id="f7c2" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">变压器网络介绍。</li></ol><h1 id="1f8e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk bi translated"><strong class="ak">递归神经网络(RNN) </strong></h1><p id="1529" class="pw-post-body-paragraph kb kc it kd b ke ml kg kh ki mm kk kl km mn ko kp kq mo ks kt ku mp kw kx ky im bi translated">RNNs 让我们在神经网络中模拟序列。虽然有其他方法来模拟序列，rnn 是特别有用的。rnn 有两种类型，LSTMs ( <a class="ae mq" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> Hochreiter 等人，1997 </a>)和 GRUs ( <a class="ae mq" href="https://arxiv.org/pdf/1406.1078.pdf" rel="noopener ugc nofollow" target="_blank"> Cho 等人，2014 </a>)。想要更深入的教程，请查看克里斯·科拉的教程。</p><p id="c8f6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">让我们来看一个 RNN 的具体例子的机器翻译。</strong></p><ol class=""><li id="c567" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">想象我们有一个有 56 个隐藏单元的 RNN。</li></ol><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="d647" class="na lo it mw b gy nb nc l nd ne">rnn_cell = rnn_cell(input_dim=100, output_dim=56)</span></pre><p id="9337" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">2.我们有一个单词“NYU ”,它由整数 12 表示，这意味着它是我创建的 vocab 中的第 12 个单词。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="55e2" class="na lo it mw b gy nb nc l nd ne"># 'NYU' is the 12th word in my vocab<br/>word = 'NYU'<br/>word = VOCAB[word]</span><span id="a204" class="na lo it mw b gy nf nc l nd ne">print(word)<br/># 11</span></pre><p id="0bda" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">除了我们没有将整数输入 RNN，我们使用了一个更高维的表示，这是我们目前通过<a class="ae mq" rel="noopener" target="_blank" href="/neural-network-embeddings-explained-4d028e6f0526">嵌入</a>获得的。嵌入让我们将一系列离散的记号映射到连续的空间中(<a class="ae mq" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank"> Bengio 等人，2003 </a>)。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="df83" class="na lo it mw b gy nb nc l nd ne">embedding_layer = Embedding(vocab_size=120, embedding_dim=10)</span><span id="c9e0" class="na lo it mw b gy nf nc l nd ne"># project our word to 10 dimensions<br/>x = embedding_layer(x)</span></pre><p id="2e46" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个 RNN 单元接受两个输入，一个字<strong class="kd iu"> <em class="ng"> x </em> </strong>，以及一个来自前一时间步<strong class="kd iu"> <em class="ng"> h </em> </strong>的隐藏状态。在每一个时间步，它输出一个新的<em class="ng"> </em> <strong class="kd iu"> <em class="ng"> h </em> </strong>。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f6cabed369973cef711c83c78a874665.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/1*7Tx5Y6E7dDZMcAwLM6d_jw.gif"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">RNN CELL: next_h= f(x, prev_h).</strong></figcaption></figure><p id="7db8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">提示:对于第一步，h 通常为零。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="e2c1" class="na lo it mw b gy nb nc l nd ne"># 1 word, RNN has 56 hidden units <br/>h_0 = np.zeros(1, 56)</span></pre><p id="9e5b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这很重要:RNN 细胞与 RNN 细胞是不同的。</p><p id="eaa2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在 RNN 术语中有一个主要的混淆点。在深度学习框架中，如<a class="ae mq" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html#RNNCell" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>和<a class="ae mq" href="https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell/RNNCell" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>，RNN 细胞是执行这种计算的单位:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="cbe0" class="na lo it mw b gy nb nc l nd ne">h1 = rnn_cell(x, h0)</span></pre><p id="2e96" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">RNN <em class="ng">网络</em> for 在时间步长上循环单元</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="c848" class="na lo it mw b gy nb nc l nd ne">def RNN(sentence):  <br/>  current_h = h_0</span><span id="482e" class="na lo it mw b gy nf nc l nd ne">  all_h = []<br/>  for word in sentence:<br/>    # use the RNN CELL at each time step<br/>    current_h = rnn_cell(embed(word), current_h)<br/>    all_h.append(current_h)</span><span id="2033" class="na lo it mw b gy nf nc l nd ne">  # RNNs output a hidden vector h at each time step<br/>  return all_h</span></pre><p id="1ebc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">这里有一个 RNN 随着时间推移移动<em class="ng">同一个</em> RNN 单元格的图示:</strong></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/5369a57afa314f12541d55e7d2676e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*m8ia9jflD1pUUkZaU_UWfA.gif"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">The RNN moves the RNN cell over time. For attention, we’ll use ALL the h’s produced at each timestep</strong></figcaption></figure></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="df56" class="ln lo it bd lp lq nv ls lt lu nw lw lx ly nx ma mb mc ny me mf mg nz mi mj mk bi translated"><strong class="ak">序列对序列模式(Seq2Seq) </strong></h1><p id="8313" class="pw-post-body-paragraph kb kc it kd b ke ml kg kh ki mm kk kl km mn ko kp kq mo ks kt ku mp kw kx ky im bi translated">现在你是 RNNs 的专家了，但是让我们放松一下。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7b102eb04a8241c027ff3de6b789f05b.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*au9O39Y0n9K5LWP6Z_eKSg.gif"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Chill</strong></figcaption></figure><p id="06f3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">rnn 可以用作更大深度学习系统的模块。</p><p id="fe97" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一个这样的系统是由 Bengio 的小组(<a class="ae mq" href="https://arxiv.org/pdf/1406.1078.pdf" rel="noopener ugc nofollow" target="_blank"> Cho 等人，2014 </a>)和 Google ( <a class="ae mq" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank"> Sutskever 等人，2014 </a>)引入的 Seq2Seq 模型，它可以用于将一个序列翻译成另一个序列。你可以把很多问题框定为翻译:</p><ol class=""><li id="d091" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">把英语翻译成西班牙语。</li><li id="733a" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">将一个视频序列转换成另一个序列。</li><li id="d8e0" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">将一系列指令翻译成程序代码。</li><li id="9027" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">将用户行为转化为未来的用户行为</li><li id="7285" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi">…</li><li id="2e78" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">唯一的限制是你的创造力！</li></ol></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="3d7a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">seq2seq 模型无非是 2 个 rnn，一个编码器(E)，一个解码器(D)。</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="de67" class="na lo it mw b gy nb nc l nd ne">class Seq2Seq(object):</span><span id="99ae" class="na lo it mw b gy nf nc l nd ne">  def __init__():<br/>      self.encoder = RNN(...)<br/>      self.decoder = RNN(...)</span></pre><p id="7b4a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">seq2seq 模型有两个主要步骤:</p><p id="5310" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第一步:<em class="ng">编码</em>一个序列:</strong></p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="6e3f" class="na lo it mw b gy nb nc l nd ne">sentence = ["NYU", "NLP", "rocks", "!"]<br/>all_h = Seq2Seq.encoder(sentence)</span><span id="675a" class="na lo it mw b gy nf nc l nd ne"># all_h now has 4 h (activations)</span></pre><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/5369a57afa314f12541d55e7d2676e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*m8ia9jflD1pUUkZaU_UWfA.gif"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Encoding</strong></figcaption></figure><p id="dbff" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">第二步:解码生成“翻译”</strong></p><p id="b10a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这部分真的很复杂。前一步中的编码器一次处理完整的序列(即:这是一个普通的 RNN)。</p><p id="f678" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在第二步中，我们一次运行解码器 RNN <em class="ng">一个</em>步骤，以生成自回归预测(这是为了将前一步的输出用作下一步的输入)。</p><p id="b4bd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有两种主要的解码方式:</p><p id="abd7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">选项一:贪婪解码</p><ol class=""><li id="6535" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">运行解码器的 1 个步骤。</li><li id="39aa" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">选择概率最高的输出。</li><li id="9b14" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">使用此输出作为下一步的输入</li></ol><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="7609" class="na lo it mw b gy nb nc l nd ne"># you have to seed the first x since there are no predictions yet<br/># SOS means start of sentence<br/>current_X_token = '&lt;SOS&gt;'</span><span id="6228" class="na lo it mw b gy nf nc l nd ne"># we also use the last hidden output of the encoder (or set to zero)<br/>h_option_1 = hs[-1]<br/>h_option_2 = zeros(...)</span><span id="b108" class="na lo it mw b gy nf nc l nd ne"># let's use option 1 where it's the last h produced by the encoder<br/>dec_h = h_option_1</span><span id="0568" class="na lo it mw b gy nf nc l nd ne"># run greedy search until the RNN generates an End-of-Sentence token<br/>while current_X_token != 'EOS':</span><span id="0735" class="na lo it mw b gy nf nc l nd ne">   # keep the output h for next step<br/>   next_h = decoder(dec_h, current_X_token)</span><span id="0179" class="na lo it mw b gy nf nc l nd ne">   # use new h to find most probable next word using classifier<br/>   next_token = max(softmax(fully_connected_layer(next_h)))</span><span id="27af" class="na lo it mw b gy nf nc l nd ne">   # *KEY* prepare for next pass by updating pointers<br/>   current_X_token = next_token<br/>   dec_h = next_h</span></pre><p id="d8c0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这叫做贪婪，因为我们总是选择概率最高的下一个词。</p><p id="d786" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">选项 2:光束搜索</strong></p><p id="03fc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">有一种更好的技术叫做波束搜索，它在解码过程中考虑多条路径。通俗地说，宽度为 5 的波束搜索意味着我们考虑具有最大对数似然的 5 个可能的序列(数学术语为 5 个最可能的序列)。</p><p id="441e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在高层次上，我们保留顶部 k(波束大小= k)，而不是采用最高概率预测。注意下面，在每一步我们有 5 个选项(5 个可能性最大)。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ob"><img src="../Images/9c605e34e3286b6a2801db57daa05c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7oslzA6EGizJAvmOLqZRZA.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Beam search figure found </strong><a class="ae mq" href="https://www.researchgate.net/publication/317377611_Retrosynthetic_Reaction_Prediction_Using_Neural_Sequence-to-Sequence_Models/figures?lo=1&amp;utm_source=google&amp;utm_medium=organic" rel="noopener ugc nofollow" target="_blank"><strong class="bd nm">here</strong></a></figcaption></figure><p id="016c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><a class="ae mq" href="https://www.youtube.com/watch?v=UXW6Cs82UKo" rel="noopener ugc nofollow" target="_blank">这个 youtube 视频</a>有详细的光束搜索教程！</p><p id="c1e7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以，用 greedy decoding 作为动画把<em class="ng">【NYU NLP 很牛逼】</em>翻译成西班牙语的完整 seq2seq 过程看起来是这样的:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oc"><img src="../Images/29db285d85e89f88de351006f8e31013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*tS-MYHa63gU9FyryQp-uPQ.gif"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Seq2Seq is made up of 2 RNNs an encoder and decoder</strong></figcaption></figure><p id="d047" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">这个模型有各种部件:</strong></p><ol class=""><li id="5bd0" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">蓝色 RNN 是编码器。</li><li id="4695" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">红色 RNN 是解码器</li><li id="546c" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">解码器顶部的蓝色矩形是一个带有 softmax 的完全连接层。这将挑选最有可能的下一个单词。</li></ol></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="fe55" class="ln lo it bd lp lq nv ls lt lu nw lw lx ly nx ma mb mc ny me mf mg nz mi mj mk bi translated"><strong class="ak">注意机制</strong></h1><p id="0af7" class="pw-post-body-paragraph kb kc it kd b ke ml kg kh ki mm kk kl km mn ko kp kq mo ks kt ku mp kw kx ky im bi translated">好了，现在我们已经讨论了所有的先决条件，让我们进入正题。</p><p id="d2fd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你注意到前面的动画，解码器只查看编码器生成的最后一个隐藏向量。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7e21074041feb97521febdb06a32e010.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*Mwr-wnG1pD6shmtK3BEUbg.png"/></div></figure><p id="6b81" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">事实证明，RNN 很难记住<em class="ng">在这个单一向量的序列中发生的一切</em>(<a class="ae mq" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">Bahdanau 等人，2015 </a>)。例如，当编码器处理完输入序列时，可能已经忘记了单词“NYU”。</p><p id="10b4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意力试图解决这个问题。</p><p id="df19" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当你给一个模型一个注意机制时，你允许它在每个解码步骤中查看由编码器产生的所有 h。</p><p id="4630" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了做到这一点，我们使用一个单独的网络，通常是一个完全连接的层，它计算解码器想要查看多少 h。这被称为<em class="ng">注意力机制</em>。</p><p id="e0ec" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">想象一下，对于我们生成的所有 h，我们实际上只取其中的一点。它们的总和被称为上下文向量<em class="ng"> c. </em></p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/e01a2f6c3780586d44065c5967c552ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*4apBy1pAE8ZRhTRDgftA1g.png"/></div></figure><p id="e78f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">标量 0.3、0.2、0.4、0.1 称为<em class="ng">注意力权重。</em>在原始论文中，你会在第 3 页找到同样的等式:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/297b64bf4f52ddd799e9109cf9b14c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:262/format:webp/1*btzwgYUUkFuNZ9n2vxqN5A.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk"><strong class="bd nm">Alphas are generated by a neural network + softmax.</strong></figcaption></figure><p id="a586" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这些权重由一个小型神经网络以这种方式生成:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="13f1" class="na lo it mw b gy nb nc l nd ne"># attention is just a fully connected layer and a final projection<br/>attention_mechanism = nn.Linear(input=h_size+x_size, attn_dim=20)<br/>final_proj_V = weight_matrix(attn_dim)</span><span id="fd68" class="na lo it mw b gy nf nc l nd ne"># encode the full input sentence to get the hs we want to attend to<br/>all_h = encoder(["NYU", "NLP", "is", "awesome"]</span><span id="ab45" class="na lo it mw b gy nf nc l nd ne"># greedy decoding 1 step at a time until end of sentence token<br/>current_token = '&lt;SOS&gt;'<br/>while current_token != '&lt;EOS&gt;':</span><span id="2705" class="na lo it mw b gy nf nc l nd ne">   # attend to the hs first<br/>    attn_energies = []<br/>    for h in all_h:<br/>      attn_score = attention_mechanism([h,current_token])<br/>      attn_score = tanh(attn_score)<br/>      attn_score = final_proj_V.dot(attn_score)</span><span id="ae6b" class="na lo it mw b gy nf nc l nd ne">      # attn_score is now a scalar (called an attn energy)<br/>      attn_energies.append(attn_score)</span><span id="1f47" class="na lo it mw b gy nf nc l nd ne">   # turn the attention energies into weights by normalizing<br/>   attn_weights = softmax(attn_energies)<br/>   # attn_weights = [0.3, 0.2, 0.4, 0.1]</span></pre><p id="8b3c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在我们有了权重，我们用它们来提取可能与被解码的特定令牌相关的 h</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="44d6" class="na lo it mw b gy nb nc l nd ne">context_vector = attn_weights.dot(all_h)<br/># this is now a vector which mixes a bit of all the h's</span></pre><p id="ff0b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们把它分成几个步骤:</p><ol class=""><li id="f9ab" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">我们对完整的输入序列进行了编码，生成了一个 h 列表。</li><li id="549a" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">我们开始用解码器用贪婪搜索解码。</li><li id="de43" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">我们没有给解码器 h4，而是给了它一个上下文向量。</li><li id="04a7" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">为了生成上下文向量，我们使用另一个网络和可学习的权重 V 来评分每个 h 与当前被解码的令牌的相关程度。</li><li id="7162" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">我们将那些<strong class="kd iu"> <em class="ng">注意力</em> </strong>归一化，并使用它们将所有的 h 混合成一个 h，这有希望捕获所有 h 的相关部分，即:一个<strong class="kd iu"> <em class="ng">上下文向量</em> </strong>。</li><li id="9119" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">现在，我们再次执行解码步骤，但这一次，使用上下文向量而不是 h4。</li></ol><p id="2aef" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">视觉化注意力</strong></p><p id="916e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意力权重告诉我们每个 h 有多重要。这意味着我们还可以可视化每个解码步骤的权重。以下是最初关注文件中的一个例子:</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi og"><img src="../Images/2e22ec90fdbafdce26e325cc2fd8f9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*rx6Rn3SOsGvUy9Ih7b7EXA.jpeg"/></div></figure><p id="fb68" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在第一行中，为了翻译“L”，网络在单词“the”上使用了一个字母，而将其余的字母置零。</p><p id="e3d1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了生成单词“economique ”,该网络实际上在“欧洲经济”中放置了一些字母权重，而将其余的字母置零。这表明当翻译关系是多对一或一对多时，注意力是有用的。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="f90c" class="ln lo it bd lp lq nv ls lt lu nw lw lx ly nx ma mb mc ny me mf mg nz mi mj mk bi translated">注意力会变得复杂</h1><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/e1256788d3a251666aefb820909f2ee8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*mXAizm8C-kDFxryDzrwdFQ.gif"/></div></figure><p id="13c0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">注意力类型</strong></p><p id="3ecb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这种类型的注意力只使用编码器产生的 h。有大量关于改进这一过程的研究。例如:</p><ol class=""><li id="c809" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">只使用一些 h，也许是你正在解码的时间步附近的 h(局部注意)。</li><li id="fb54" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">除了 h 的<strong class="kd iu"> <em class="ng">还有</em> </strong>使用我们之前扔掉的解码器生成的 h。</li><li id="b4c7" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi">…</li></ol><p id="4b88" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">如何计算注意力</strong></p><p id="48e4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">另一个研究领域涉及如何计算注意力得分。除了 V 的点积，研究人员还尝试了:</p><ol class=""><li id="54dd" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">缩放点积。</li><li id="72f3" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">余弦(s，h)</li><li id="269e" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">不使用 V 矩阵，并将 softmax 应用于完全连接的层。</li><li id="0c24" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi">…</li></ol><p id="41ac" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">计算注意力时用什么</strong></p><p id="4f86" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">研究的最后一个领域是，与 h 矢量相比，到底应该研究什么。</p><p id="f683" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了对我的意思建立一些直觉，考虑像键值字典一样计算注意力。关键是你给注意力网络“查找”最相关的上下文。该值是最相关的上下文。</p><p id="5af8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我在这里描述的方法只使用当前令牌和每个 h 来计算关注度分数。那就是:</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="291e" class="na lo it mw b gy nb nc l nd ne"># calculate how relevant that h is<br/>score_1 = attn_network([embed("&lt;SOS"), h1])<br/>score_2 = attn_network([embed("&lt;SOS"), h2])<br/>score_3 = attn_network([embed("&lt;SOS"), h3])<br/>score_4 = attn_network([embed("&lt;SOS"), h4])</span></pre><p id="8215" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但实际上，我们可以给它任何我们认为有用的东西，来帮助注意力网络做出最好的决定。也许我们也给它最后一个上下文向量！</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="1e9f" class="na lo it mw b gy nb nc l nd ne">score_1 = attn_network([embed("&lt;SOS&gt;"), h1, last_context])</span></pre><p id="ce6d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">或者也许我们给它一些不同的东西，也许一个令牌让它知道它在解码西班牙语</p><pre class="mr ms mt mu gt mv mw mx my aw mz bi"><span id="7450" class="na lo it mw b gy nb nc l nd ne">score_1 = attn_network([embed("&lt;SOS&gt;"), h1, last_context, embed('&lt;SPA&gt;')])</span></pre><p id="34b4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">可能性是无限的！</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="1675" class="ln lo it bd lp lq nv ls lt lu nw lw lx ly nx ma mb mc ny me mf mg nz mi mj mk bi translated">实施细节</h1><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5f60159fa88a6df5a28b9fca586b2f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/1*S67pSthjRoDr2cvzcvmIjg.gif"/></div></figure><p id="01c8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你决定实现你自己的，这里有一些提示供你考虑。</p><ol class=""><li id="20f9" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">使用<a class="ae mq" href="https://github.com/pytorch/fairseq/blob/master/fairseq/models/lstm.py" rel="noopener ugc nofollow" target="_blank">脸书的实现</a>，它已经真正优化。</li></ol><p id="0ec5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">好吧，好吧，那是逃避。以下是实际的建议。</p><ol class=""><li id="8342" class="kz la it kd b ke kf ki kj km lb kq lc ku ld ky le lf lg lh bi translated">记住 seq2seq 有两个部分:解码器 RNN 和编码器 RNN。这两个是分开的。</li><li id="d7df" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">大部分工作都是在构建解码器。编码器只是在整个输入序列上运行编码器。</li><li id="1305" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">记住解码器 RNN 一步一步地操作。这是关键！</li><li id="896f" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">记住解码器 RNN 一步一步地操作。值得说两遍；)</li><li id="db75" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">解码算法有两种选择，贪婪或波束搜索。贪心更容易实现，但是波束搜索大多数时候会给你更好的结果。</li><li id="065e" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">关注是可选的！但是…当你拥有它时，影响是巨大的…</li><li id="143d" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">注意力是一个独立的网络……把这个网络想象成字典，其中的关键是你想让网络用来决定每个特定的 h 有多相关的东西的集合。</li><li id="2df0" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">记住你是在计算每个 h 的关注度，这意味着你有一个 for 循环[h1，…，hn]。</li><li id="6676" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">嵌入 dim 的注意力网络可以任意高。这会让你的 RAM 爆炸。请确保将它放在一个单独的 GPU 上，或者保持 dim 较小。</li><li id="b98d" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">让大型模型运行起来的一个技巧是将编码器放在一个 gpu 上，解码器放在第二个 gpu 上，注意力网络放在第三个 gpu 上。这样，您可以保持较低的内存占用。</li><li id="d7e2" class="kz la it kd b ke li ki lj km lk kq ll ku lm ky le lf lg lh bi translated">如果您真的部署这个模型，您将需要批量实现它。我在这里解释的一切都是针对批量=1，但是你可以通过改变到张量积和聪明的线性代数来扩展到更大的批量。我在这里详细解释一下这个<a class="ae mq" rel="noopener" target="_blank" href="/taming-lstms-variable-sized-mini-batches-and-why-pytorch-is-good-for-your-health-61d35642972e">。</a></li></ol><p id="f347" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">同样，大多数情况下，您应该只使用开源实现，但是自己做也是很好的学习体验！</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="fa48" class="ln lo it bd lp lq nv ls lt lu nw lw lx ly nx ma mb mc ny me mf mg nz mi mj mk bi translated">关注后的生活</h1><p id="1d23" class="pw-post-body-paragraph kb kc it kd b ke ml kg kh ki mm kk kl km mn ko kp kq mo ks kt ku mp kw kx ky im bi translated">原来……注意力网络本身被证明是<strong class="kd iu"> <em class="ng">真正的</em> </strong>强大。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3efa8b6b81ff9a4be65a4d6b5d599838.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/1*u3SNiCTkZfwnZQmYJhsSxg.gif"/></div></figure><p id="0484" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以至于研究人员决定放弃 RNNs 和序列对序列的方法。相反，他们创造了一种叫做变压器模型的东西。</p><p id="086a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在高层次上，<a class="ae mq" href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" rel="noopener ugc nofollow" target="_blank">变压器</a>仍然有一个编码器和解码器，只是各层完全连接，并立即查看完整的输入。然后，当输入在网络中移动时，注意力集中在重要的事情上。</p><figure class="mr ms mt mu gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ok"><img src="../Images/dc07177876e5c9027f575627a941c636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dtz4GnZ1uYNMIxRC5mKScg.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Transformer illustration <a class="ae mq" href="https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.XKaIwutKh-U" rel="noopener ugc nofollow" target="_blank">from here</a>.</figcaption></figure><p id="dbe9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这种模型在很大程度上取代了 seq2seq 模型的翻译，并落后于目前最强大的模型，<a class="ae mq" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank"> BERT </a>和<a class="ae mq" rel="noopener" target="_blank" href="/openai-gpt-2-understanding-language-generation-through-visualization-8252f683b2f8"> OpenAI 的 GPT </a>。</p></div></div>    
</body>
</html>