<html>
<head>
<title>Logistic Regression from Scratch with NumPy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NumPy 从头开始进行逻辑回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-from-scratch-with-numpy-da4cc3121ece?source=collection_archive---------3-----------------------#2019-08-02">https://towardsdatascience.com/logistic-regression-from-scratch-with-numpy-da4cc3121ece?source=collection_archive---------3-----------------------#2019-08-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/c241d030c80a359b9db51403c7424253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YHOsfiENyBPlNzpx"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">At the end, it’s all about creating something valuable with your bare hands!</figcaption></figure><p id="45f9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi ld translated">欢迎来到另一个实现机器学习算法的帖子！今天，我们将从头开始实现的算法是<strong class="kh iu">逻辑回归</strong>。除了它心爱的姐妹算法<em class="lm">线性回归</em>，由于其简单性和鲁棒性，这一算法也被高度用于机器学习。尽管它被称为逻辑<em class="lm">回归</em>，但它实际上是一种分类算法，用于将输入数据分类到其类别(标签)中。</p><p id="88f6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这个强大的机器学习模型可以用来回答一些问题，例如:</p><ul class=""><li id="cd3c" class="ln lo it kh b ki kj km kn kq lp ku lq ky lr lc ls lt lu lv bi translated">一封电子邮件是否是垃圾邮件</li><li id="7a67" class="ln lo it kh b ki lw km lx kq ly ku lz ky ma lc ls lt lu lv bi translated">如果客户会流失</li><li id="8ae1" class="ln lo it kh b ki lw km lx kq ly ku lz ky ma lc ls lt lu lv bi translated">肿瘤是良性的还是恶性的</li></ul><p id="4473" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">以上所有问题都是简单的<em class="lm">是-否</em>问题，因此它们可用于将输入数据分为两类。因此，术语<strong class="kh iu"> <em class="lm">二进制分类</em> </strong>在数据可以被分类为两个不同类别时使用。</p><p id="eccc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">显然，<strong class="kh iu"> <em class="lm">多类分类</em> </strong>处理的是有两个以上标签(类)的数据。在掌握了进行二元分类的逻辑回归的来龙去脉之后，过渡到多类分类是相当直接的，因此，我们现在将只处理具有两个类的数据。</p><p id="256e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">请记住，在线性回归中，我们根据输入和模型参数预测数值。这里，在逻辑回归中，我们也可以接近模型，因为我们试图预测数值，但这次这些值对应于属于特定类别的输入数据的<strong class="kh iu"> <em class="lm">概率</em> </strong>。</p><p id="5611" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">使用逻辑回归中的术语<em class="lm">逻辑</em>是因为我们将另一个函数应用于输入数据和模型参数的加权和，该函数称为<strong class="kh iu"> <em class="lm"> logit (sigmoid)函数</em> </strong>。</p><p id="09ca" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">Sigmoid 函数始终输出介于 0 和 1 之间的值，将值映射到一个范围，因此可用于计算属于某个类别的输入数据的概率:</p><p id="ab27" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><em class="lm">sigmoid(x)= 1/(1+e⁻</em><strong class="kh iu"><em class="lm">ˣ</em></strong><em class="lm">)</em></p><figure class="mc md me mf gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mb"><img src="../Images/e460e37fc1774fbbd6572f2f2dc92756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5IxgCMTQlym0Q9zk1PtmcQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Sigmoid (logit) function</figcaption></figure><p id="440f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">事不宜迟，让我们开始编写这个实现的代码。必要时，我会边走边解释代码。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="854f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们从导入必要的库开始。和往常一样，<strong class="kh iu"> NumPy </strong>是我们用来实现逻辑回归算法的唯一一个包。</p><p id="7f0e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">所有其他工具只能帮助我们完成一些小任务，比如可视化手头的数据或创建数据集。因此，我们不会使用已经实现的逻辑回归包解决方案。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="efe2" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里，我们为前面提到的 sigmoid (logit)函数编写代码。值得注意的是，这个函数可以单独应用于一个<code class="fe mi mj mk ml b">numpy</code>数组的所有元素，因为我们使用了<strong class="kh iu"> NumPy </strong>包中的指数函数。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="e5fd" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们为逻辑回归写成本函数。请注意，逻辑回归中使用的成本函数不同于线性回归中使用的成本函数。</p><p id="c73c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">请记住，在线性回归中，我们计算输入数据和参数的加权和，并将该和提供给成本函数来计算成本。当我们绘制成本函数时，发现它是凸的，因此局部最小值也是全局最小值。</p><p id="b4ab" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然而，在逻辑回归中，我们将 sigmoid 函数应用于加权和，这使得结果<em class="lm">是非线性的</em>。</p><p id="16d9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果我们把非线性结果输入到成本函数中，我们得到的将是一个非凸的函数，我们不能保证只找到一个局部最小值，也就是全局最小值。</p><p id="cec1" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，我们使用另一个成本函数来计算成本，该成本保证在优化期间给出一个局部最小值<em class="lm">。</em></p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="b51b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这里的梯度下降实现与我们在线性回归中使用的没有太大不同。显然，唯一需要注意的区别是应用于加权和的 sigmoid 函数。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><p id="847e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">当写出预测函数时，我们不要忘记我们在这里处理的是<em class="lm">概率</em>。</p><p id="256a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">因此，如果结果值高于 0.50，我们将其向上舍入到 1，这意味着数据样本属于类 1。因此，如果属于类别 1 的数据样本的概率低于 0.50，这仅仅意味着它是另一个类别(类别 0)的一部分。</p><p id="b950" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">记住这是二元分类，所以我们只有两个类(类 1 和类 0)。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/de4b86097150e131c6cb4c348c0a23d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*5rtqFLoucmepz3NwJVaxPg.png"/></div></figure><p id="6f37" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">写完必要函数的代码后，让我们用来自<code class="fe mi mj mk ml b">sklearn.datasets</code>的<code class="fe mi mj mk ml b">make_classification</code>函数创建我们自己的数据集。我们用两个类创建 500 个样本点，并在<code class="fe mi mj mk ml b">seaborn</code>库的帮助下绘制数据集。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><pre class="mc md me mf gt mn ml mo mp aw mq bi"><span id="dc4d" class="mr ms it ml b gy mt mu l mv mw">Initial Cost is: [[0.69312718]] <br/><br/>Optimal Parameters are: <br/> [[-0.45293068]<br/> [ 3.26552327]<br/> [ 0.03334871]]</span></pre><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/cd4c1c6944adfd99a7a2dfe78e70627f.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*JgJAVxPu-NjHux_-F4t-CQ.png"/></div></figure><p id="9816" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，让我们运行算法，计算模型的参数。</p><p id="08c4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">看到该图，我们现在可以确定我们已经实现了逻辑回归算法，没有错误，因为它随着每次迭代而减少，直到<em class="lm">减少如此之小，以至于成本收敛到最小值</em>，这正是我们真正想要的。</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><pre class="mc md me mf gt mn ml mo mp aw mq bi"><span id="3ed6" class="mr ms it ml b gy mt mu l mv mw">0.966</span></pre><p id="43bc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">运行算法并获得最佳参数后，我们想知道我们的模型在预测数据类别方面有多成功。事实证明，我们获得的准确度分数是一个相当高的值，所以一定要鼓励自己！</p><figure class="mc md me mf gt ju"><div class="bz fp l di"><div class="mg mh l"/></div></figure><figure class="mc md me mf gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/ea87ee8790e09aab68b2e3087bf8626c.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*lqSs1yAqwi96ZiWmMZoMXA.png"/></div></figure><p id="8e74" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">现在，为了形象化，让我们沿着模型的决策边界绘制数据集。</p><p id="64b9" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们只需使用最佳参数计算截距和斜率值，并绘制将数据分为两类的边界。</p><p id="475d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">从图中我们可以看出，分类不是 100%正确的，因为类的分离自然不是线性的。然而，错误分类的点(假阳性和假阴性)真的很少，所以我们在逻辑回归的实现方面做得很好。</p><p id="9929" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">请随意继续并再次编写所有这些代码。</p><p id="8ee8" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">但是这一次，<em class="lm">靠自己</em>去做。</p><p id="3224" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">你也可以查看我的<a class="ae mz" href="https://github.com/leventbass/logistic_regression" rel="noopener ugc nofollow" target="_blank"> GitHub 简介</a>，沿着<em class="lm"> jupyter 笔记本</em>阅读代码，或者简单地使用代码来实现。</p><p id="1a4b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">将来，我一定会带来更多的实现。</p><p id="f14f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">编码快乐！</p><h2 id="0396" class="mr ms it bd na nb nc dn nd ne nf dp ng kq nh ni nj ku nk nl nm ky nn no np nq bi translated">有问题吗？评论？通过<a class="ae mz" href="http://leventbas92@gmail.com" rel="noopener ugc nofollow" target="_blank">leventbas92@gmail.com</a>或<a class="ae mz" href="https://github.com/leventbass" rel="noopener ugc nofollow" target="_blank"> GitHub </a>联系我。</h2></div></div>    
</body>
</html>