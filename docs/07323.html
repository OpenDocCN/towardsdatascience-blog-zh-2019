<html>
<head>
<title>Trade and Invest Smarter — The Reinforcement Learning Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更明智地交易和投资——强化学习方式</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315?source=collection_archive---------2-----------------------#2019-10-15">https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315?source=collection_archive---------2-----------------------#2019-10-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d56f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深入探究 TensorTrade——使用深度强化学习进行交易和投资的 Python 框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a8a4e78d83cb9ff03eb5fcbc0425ab96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zXav--HypuqP2vnr7yaMvw.jpeg"/></div></div></figure><blockquote class="ku kv kw"><p id="f5b1" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="it">注来自《走向数据科学》的编辑:</em> </strong> <em class="it">虽然我们允许独立作者根据我们的</em> <a class="ae lu" rel="noopener" target="_blank" href="/questions-96667b06af5"> <em class="it">规则和指导方针</em> </a> <em class="it">发表文章，但我们不认可每个作者的贡献。你不应该在没有寻求专业建议的情况下依赖一个作者的作品。详见我们的</em> <a class="ae lu" rel="noopener" target="_blank" href="/readers-terms-b5d780a700a4"> <em class="it">读者术语</em> </a> <em class="it">。</em></p></blockquote><p id="dce9" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><a class="ae lu" href="https://www.nature.com/articles/d41586-019-02156-9" rel="noopener ugc nofollow" target="_blank">赢得高赌注扑克锦标赛</a>，<a class="ae lu" href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" rel="noopener ugc nofollow" target="_blank">击败世界级星际争霸玩家</a>，以及<a class="ae lu" href="https://www.tesla.com/autopilot" rel="noopener ugc nofollow" target="_blank">自动驾驶特斯拉的未来跑车</a>。他们都有什么共同点？长期以来，这些极其复杂的任务中的每一项都被认为是机器不可能完成的，直到深度强化学习的最新进展表明它们今天是可能的。</p><p id="c4bb" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">强化学习开始接管世界。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ly"><img src="../Images/b719707f40cf8e9fcbb90ef1ab0e4422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gidYwyTpP1IbJ7VnNexLgw.gif"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Source: <a class="ae lu" href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" rel="noopener ugc nofollow" target="_blank">https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii</a></figcaption></figure><p id="2b76" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">两个多月前，我决定参加这场革命，于是我开始了一段旅程，利用最先进的深度强化学习算法创建一个有利可图的比特币交易策略。虽然我在这方面取得了相当大的进展，但我意识到这类项目的工具可能会令人望而生畏，因此很容易迷失在细节中。</p><p id="9323" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">在优化我之前的分布式高性能计算(HPC)系统项目之间；迷失在无尽的数据和功能优化管道中；围绕高效的模型建立、调整、培训和评估，我忙得团团转；我意识到一定有更好的做事方法。在对现有项目进行了无数小时的研究，花了无数个晚上观看 PyData 会议演讲，并与数百名 RL trading Discord 社区成员进行了多次来回交谈后，我意识到没有任何现有的解决方案是那么好。</p><p id="ada8" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">互联网上散布着许多零碎的强化学习交易系统，但没有一个是完整的。出于这个原因，我决定创建一个开源的 Python 框架，使用深度强化学习，高效地将任何交易策略从想法变成产品。</p><p id="ff4b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">输入 TensorTrade。想法是创建一个高度模块化的框架，以一种可组合、可维护的方式构建高效的强化学习交易策略。依我看，这听起来像是一大堆术语，所以让我们进入正题吧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><blockquote class="ku kv kw"><p id="a986" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">目录</strong></p></blockquote><p id="ddbc" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><a class="ae lu" href="#85ac" rel="noopener ugc nofollow"> <strong class="la iu">概述</strong> </a></p><ul class=""><li id="7cc8" class="mf mg it la b lb lc le lf lv mh lw mi lx mj lt mk ml mm mn bi translated"><a class="ae lu" href="#2b7a" rel="noopener ugc nofollow"> RL 引物</a></li><li id="0626" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#7e3a" rel="noopener ugc nofollow">入门</a></li><li id="e120" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#20b4" rel="noopener ugc nofollow">安装</a></li></ul><p id="9b61" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><a class="ae lu" href="#9940" rel="noopener ugc nofollow"> <strong class="la iu"> TensorTrade 组件</strong> </a></p><ul class=""><li id="4636" class="mf mg it la b lb lc le lf lv mh lw mi lx mj lt mk ml mm mn bi translated"><a class="ae lu" href="#a323" rel="noopener ugc nofollow"> <strong class="la iu">交易环境</strong> </a></li><li id="273b" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#a178" rel="noopener ugc nofollow">交易所</a></li><li id="0c5f" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#02f5" rel="noopener ugc nofollow">特征管线</a></li><li id="735b" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#0f23" rel="noopener ugc nofollow">行动策略</a></li><li id="fd03" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#4ded" rel="noopener ugc nofollow">奖励策略</a></li><li id="e6fd" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#6f18" rel="noopener ugc nofollow"> <strong class="la iu">学习代理</strong> </a></li><li id="9d39" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#8cec" rel="noopener ugc nofollow">稳定基线</a></li><li id="e1f9" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#8cec" rel="noopener ugc nofollow">张量力</a></li><li id="d531" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#0ea3" rel="noopener ugc nofollow"> <strong class="la iu">交易策略</strong> </a></li></ul><p id="69c8" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><a class="ae lu" href="#4761" rel="noopener ugc nofollow"><strong class="la iu"/></a></p><ul class=""><li id="f06c" class="mf mg it la b lb lc le lf lv mh lw mi lx mj lt mk ml mm mn bi translated"><a class="ae lu" href="#d343" rel="noopener ugc nofollow">创建环境</a></li><li id="0e35" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#cde4" rel="noopener ugc nofollow">定义代理</a></li><li id="8a25" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#e048" rel="noopener ugc nofollow"> <strong class="la iu">训练一个策略</strong> </a></li><li id="0501" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#f04b" rel="noopener ugc nofollow"> <strong class="la iu">保存和恢复</strong> </a></li><li id="3d28" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#d17a" rel="noopener ugc nofollow">调整你的策略</a></li><li id="58f9" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#5936" rel="noopener ugc nofollow"> <strong class="la iu">策略评估</strong> </a></li><li id="86e4" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#2667" rel="noopener ugc nofollow">现场交易</a></li></ul><p id="4133" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><a class="ae lu" href="#99f2" rel="noopener ugc nofollow"> <strong class="la iu">未来</strong> </a></p><ul class=""><li id="17e8" class="mf mg it la b lb lc le lf lv mh lw mi lx mj lt mk ml mm mn bi translated"><a class="ae lu" href="#af85" rel="noopener ugc nofollow">最终想法</a></li><li id="88d1" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#6681" rel="noopener ugc nofollow">投稿</a></li><li id="ecd4" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><a class="ae lu" href="#10c5" rel="noopener ugc nofollow">参考文献</a></li></ul><h1 id="85ac" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">概观</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/763c2169bcc4f6919f98f1b9c2e79354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xZe_NhjEXSVmRbOaG5H5SA.jpeg"/></div></div></figure><p id="0aea" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">TensorTrade 是一个开源的 Python 框架，使用深度强化学习来训练、评估和部署稳健的交易策略。该框架的重点是高度可组合和可扩展，以允许系统从单个 CPU 上的简单交易策略扩展到在分布的 HPC 机器上运行的复杂投资策略。</p><p id="74a7" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">在引擎盖下，该框架使用现有机器学习库中的许多 API 来维护高质量的数据管道和学习模型。TensorTrade 的主要目标之一是通过利用由<code class="fe nm nn no np b">numpy</code>、<code class="fe nm nn no np b">pandas</code>、<code class="fe nm nn no np b">gym</code>、<code class="fe nm nn no np b">keras</code>和<code class="fe nm nn no np b">tensorflow</code>提供的现有工具和管道，实现算法交易策略的快速试验。</p><p id="6381" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">框架的每一部分都被分割成可重用的组件，允许您利用社区构建的通用组件，同时保留您的专有特性。目的是使用深度强化学习来简化测试和部署稳健交易代理的过程，让你我专注于创造盈利策略。</p><h2 id="2b7a" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">RL 底漆</h2><p id="c27d" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">如果你的强化学习技能有点生疏，让我们快速回顾一下基本概念。</p><blockquote class="ku kv kw"><p id="c2ac" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">强化学习</strong> ( <strong class="la iu"> RL </strong>)是<a class="ae lu" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>的一个领域，涉及<a class="ae lu" href="https://en.wikipedia.org/wiki/Software_agent" rel="noopener ugc nofollow" target="_blank">软件代理</a>应该如何在一个环境中采取<a class="ae lu" href="https://en.wikipedia.org/wiki/Action_selection" rel="noopener ugc nofollow" target="_blank">行动</a>，以便最大化一些累积回报的概念。</p></blockquote><p id="e4d0" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">每个强化学习问题都始于一个环境和一个或多个可以与环境交互的代理。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://www.researchgate.net/publication/319121340_Enabling_Cognitive_Smart_Cities_Using_Big_Data_and_Machine_Learning_Approaches_and_Challenges"><div class="gh gi oh"><img src="../Images/a21e6bdd0bb222f98c4c63bb38d121a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*OlqIw1eCrAAg5CZE.jpg"/></div></a><figcaption class="lz ma gj gh gi mb mc bd b be z dk">This technique is based off Markov Decision Processes (MDP) dating back to the 1950s.</figcaption></figure><p id="5105" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">代理将首先观察环境，然后建立当前状态的模型以及该环境中动作的期望值。基于该模型，代理将采取它认为具有最高期望值的行动。</p><p id="e460" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">基于所选动作在环境中的效果，代理将被奖励与该动作的实际价值相对应的金额。然后，强化学习代理可以通过试错过程(即，通过强化学习)来改进其底层模型，并随着时间的推移学习采取更有益的行动。</p><p id="753f" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">如果在这个问题上你还需要一点新鲜的东西，在这篇文章的参考资料中有一篇文章的链接，标题是<em class="kz">深度强化学习简介</em>，它更深入地介绍了细节。我们继续吧。</p><h2 id="7e3a" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">入门指南</h2><p id="0b1a" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">下面的教程将提供足够的例子，让你开始使用 TensorTrade 创建简单的交易策略，尽管你会很快发现这个框架能够处理更复杂的配置。</p><p id="191f" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">你可以跟随<a class="ae lu" href="https://colab.research.google.com/drive/1hzbugXnkGWO6l3vpQ0bSqnJJxBGiogar" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>或者<a class="ae lu" href="https://github.com/notadamking/tensortrade/blob/master/examples/TensorTrade_Tutorial.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上的教程。</p><h2 id="20b4" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">装置</h2><p id="4dfd" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">TensorTrade 需要 Python 3.6 或更高版本，所以在 pip 安装框架之前，请确保您使用的是有效版本。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="8bfd" class="nq mu it np b gy om on l oo op">pip install git+https://github.com/notadamking/tensortrade.git</span></pre><p id="bd47" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">为了完成整个教程，您需要安装一些额外的依赖项，比如<code class="fe nm nn no np b">tensorflow</code>、<code class="fe nm nn no np b">tensorforce</code>、<code class="fe nm nn no np b">stable-baselines</code>、<code class="fe nm nn no np b">ccxt</code>、<code class="fe nm nn no np b">ta</code>和<code class="fe nm nn no np b">stochastic</code>。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="3d55" class="nq mu it np b gy om on l oo op">pip install git+<a class="ae lu" href="https://github.com/notadamking/tensortrade.git#egg=tensortrade[tf,tensorforce,baselines,ccxt,fbm" rel="noopener ugc nofollow" target="_blank">https://github.com/notadamking/tensortrade.git#egg=tensortrade[tf,tensorforce,baselines,ccxt,ta,fbm</a>] -U</span></pre><p id="3fe4" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">这就是所有必要的安装！让我们进入代码。</p><h1 id="9940" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">TensorTrade 组件</h1><p id="d1ab" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">TensorTrade 是围绕模块化组件构建的，这些组件共同构成了交易策略。交易策略以<code class="fe nm nn no np b">gym</code>环境的形式将强化学习代理与可组合的交易逻辑结合起来。交易环境是由一组模块组成的，这些模块可以混合和匹配，以创建高度多样化的交易和投资策略。稍后我将进一步详细解释这一点，但现在知道一些基本知识就足够了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/93399bd9aa205e4f733cde9d00badfd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xx0Q0PrHXWn40f3d5v0kXw.jpeg"/></div></div></figure><p id="fdc1" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">就像电气组件一样，TensorTrade 组件的目的是能够根据需要混合和匹配它们。</p><blockquote class="ku kv kw"><p id="a641" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本节中的代码片段应该作为创建新策略和组件的指南。随着更多组件的定义，可能会遗漏一些实现细节，这些细节将在后面的章节中变得更加清晰。</p></blockquote><h1 id="a323" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">贸易环境</h1><p id="96bb" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">交易环境是遵循 OpenAI 的<code class="fe nm nn no np b">gym.Env</code>规范的强化学习环境。这允许我们在交易代理中利用许多现有的强化学习模型，如果我们愿意的话。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/dc15e424f111c1cd81eadd4cddda679c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t7hGvsEheauOOrcWKrNG1A.jpeg"/></div></div></figure><p id="6f7b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">交易环境是完全可配置的<code class="fe nm nn no np b">gym</code>环境，具有高度可组合的<code class="fe nm nn no np b">Exchange</code>、<code class="fe nm nn no np b">FeaturePipeline</code>、<code class="fe nm nn no np b">ActionScheme</code>和<code class="fe nm nn no np b">RewardScheme</code>组件。</p><ul class=""><li id="cb3a" class="mf mg it la b lb lc le lf lv mh lw mi lx mj lt mk ml mm mn bi translated"><code class="fe nm nn no np b">Exchange</code>提供对环境的观察并执行代理的交易。</li><li id="bdf2" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated">在将交换输出传递给代理之前，<code class="fe nm nn no np b">FeaturePipeline</code>可选地将交换输出转换成一组更有意义的特性。</li><li id="e5bf" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><code class="fe nm nn no np b">ActionScheme</code>将代理的行为转换成可执行的交易。</li><li id="7d4b" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><code class="fe nm nn no np b">RewardScheme</code>根据代理的表现计算每个时间步的奖励。</li></ul><p id="c59c" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">如果说现在看起来有点复杂，其实真的不是。这就是全部内容，现在只需要将这些组件组成一个完整的环境。</p><p id="ada7" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">当<code class="fe nm nn no np b">TradingEnvironment</code>的<code class="fe nm nn no np b">reset</code>方法被调用时，所有的子组件也将被重置。每个交换、特性管道、变形金刚、行动方案和奖励方案的内部状态将被设置回默认值，为下一集做好准备。</p><p id="4b7c" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">让我们从一个示例环境开始。如前所述，初始化一个<code class="fe nm nn no np b">TradingEnvironment</code>需要一个交换，一个行动方案，一个奖励方案，特征流水线是可选的。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="739e" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.environments <strong class="np iu">import </strong>TradingEnvironment</span><span id="2fb5" class="nq mu it np b gy or on l oo op">environment = TradingEnvironment(exchange=exchange,<br/>                                 action_scheme=action_scheme,<br/>                                 reward_scheme=reward_scheme,<br/>                                 feature_pipeline=feature_pipeline)</span></pre><blockquote class="ku kv kw"><p id="098d" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然推荐的用例是将交易环境插入到交易策略中，但是很明显，您可以单独使用交易环境，就像使用<code class="fe nm nn no np b">gym</code>环境一样。</p></blockquote><h2 id="a178" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">交换</h2><p id="5942" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">交易所确定交易环境中可交易工具的范围，在每个时间步将观察结果返回给环境，并在该环境中执行交易。有两种类型的交流:现场交流和模拟交流。</p><p id="7c47" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">实时交易所是由实时定价数据和实时交易执行引擎支持的<code class="fe nm nn no np b">Exchange</code>的实现。例如，<code class="fe nm nn no np b">CCXTExchange</code>是一个实时交易所，它能够返回定价数据，并在数百个实时加密货币交易所执行交易，如<a class="ae lu" href="https://www.binance.com/en/register?ref=PDOJ9XB8" rel="noopener ugc nofollow" target="_blank">币安</a>和<a class="ae lu" href="http://coinbase-consumer.sjv.io/c/1949163/626313/9251" rel="noopener ugc nofollow" target="_blank">比特币基地</a>。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="39fa" class="nq mu it np b gy om on l oo op"><strong class="np iu">import </strong>ccxt</span><span id="43ed" class="nq mu it np b gy or on l oo op"><strong class="np iu">from </strong>tensortrade.exchanges.live <strong class="np iu">import </strong>CCXTExchange</span><span id="a5f3" class="nq mu it np b gy or on l oo op">coinbase = ccxt.coinbasepro()</span><span id="1e5e" class="nq mu it np b gy or on l oo op">exchange = CCXTExchange(exchange=coinbase, base_instrument='USD')</span></pre><blockquote class="ku kv kw"><p id="1195" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也有股票和 ETF 交易的交易所，如<code class="fe nm nn no np b">RobinhoodExchange</code>和<code class="fe nm nn no np b">InteractiveBrokersExchange</code>，但这些仍在进行中。</p></blockquote><p id="8420" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">另一方面，模拟交易所是由模拟定价数据和交易执行支持的<code class="fe nm nn no np b">Exchange</code>的实现。</p><p id="1d76" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">例如，<code class="fe nm nn no np b">FBMExchange</code>是一个模拟的交易所，它使用分数布朗运动(FBM)生成定价和交易量数据。因为它的价格是模拟的，所以它执行的交易也必须模拟。该交易所使用简单的滑点模型来模拟交易的价格和交易量滑点，尽管像 TensorTrade 中的几乎所有东西一样，这个滑点模型可以很容易地被更复杂的东西取代。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="c689" class="nq mu it np b gy om on l oo op"><strong class="np iu">from </strong>tensortrade.exchanges.simulated <strong class="np iu">import </strong>FBMExchange</span><span id="64a9" class="nq mu it np b gy or on l oo op">exchange = FBMExchange(base_instrument='BTC', timeframe='1h')</span></pre><p id="fb3e" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">虽然<code class="fe nm nn no np b">FBMExchange</code>使用随机模型生成虚假的价格和交易量数据，但它只是<code class="fe nm nn no np b">SimulatedExchange</code>的一个实现。在幕后，<code class="fe nm nn no np b">SimulatedExchange</code>只需要一个价格历史的<code class="fe nm nn no np b">data_frame</code>来生成它的模拟。这个<code class="fe nm nn no np b">data_frame</code>既可以由编码实现(如<code class="fe nm nn no np b">FBMExchange</code>)提供，也可以在运行时提供，如下例所示。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="00f5" class="nq mu it np b gy om on l oo op"><strong class="np iu">import </strong>pandas <strong class="np iu">as </strong>pd</span><span id="dae0" class="nq mu it np b gy or on l oo op"><strong class="np iu">from </strong>tensortrade.exchanges.simulated <strong class="np iu">import</strong> SimulatedExchange</span><span id="4de7" class="nq mu it np b gy or on l oo op">df = pd.read_csv('./data/btc_ohclv_1h.csv')</span><span id="a541" class="nq mu it np b gy or on l oo op">exchange = SimulatedExchange(data_frame=df, base_instrument='USD')</span></pre><h2 id="02f5" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">特征管线</h2><p id="6770" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">特征管道意味着将来自环境的观察转换成有意义的特征，以便代理从中学习。如果一个管道已经被添加到一个特定的交换中，那么在输出到环境之前，观察结果将通过<code class="fe nm nn no np b">FeaturePipeline</code>。例如，特性管道可以标准化所有价格值，使时间序列稳定，添加移动平均列，并删除不必要的列，所有这些都在观察结果返回给代理之前完成。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/0a8b929a9d1c21505ef86d28786dbf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jHeLOtoFsptZfQN2N0qNKg.jpeg"/></div></div></figure><p id="164a" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">可以用任意数量的逗号分隔的转换器初始化特征管线。每个<code class="fe nm nn no np b">FeatureTransformer</code>都需要用要转换的列集进行初始化，否则如果没有传递任何内容，所有输入列都将被转换。</p><p id="18cd" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">每个特征转换器都有一个<code class="fe nm nn no np b">transform</code>方法，该方法将从更大的数据集中转换单个观察值(a <code class="fe nm nn no np b">pandas.DataFrame</code>),在内存中保留任何必要的状态以转换下一帧。因此，经常需要定期对<code class="fe nm nn no np b">reset</code>和<code class="fe nm nn no np b">FeatureTransformer</code>进行调整。每次重置父<code class="fe nm nn no np b">FeaturePipeline</code>或<code class="fe nm nn no np b">Exchange</code>时，这将自动完成。</p><p id="5a0d" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">让我们创建一个示例管道，并将其添加到现有的交换中。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="3dbc" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.features <strong class="np iu">import </strong>FeaturePipeline<br/><strong class="np iu">from</strong> tensortrade.features.scalers <strong class="np iu">import </strong>MinMaxNormalizer<br/><strong class="np iu">from</strong> tensortrade.features.stationarity <strong class="np iu">import </strong>FractionalDifference<br/><strong class="np iu">from </strong>tensortrade.features.indicators <strong class="np iu">import </strong>SimpleMovingAverage</span><span id="48bd" class="nq mu it np b gy or on l oo op">price_columns = ["open", "high", "low", "close"]</span><span id="6937" class="nq mu it np b gy or on l oo op">normalize_price = MinMaxNormalizer(price_columns)<br/>moving_averages = SimpleMovingAverage(price_columns)<br/>difference_all = FractionalDifference(difference_order=0.6)</span><span id="f715" class="nq mu it np b gy or on l oo op">feature_pipeline = FeaturePipeline(steps=[normalize_price,<br/>                                          moving_averages,<br/>                                          difference_all])</span><span id="4a1a" class="nq mu it np b gy or on l oo op">exchange.feature_pipeline = feature_pipeline</span></pre><blockquote class="ku kv kw"><p id="3d62" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此功能管道在添加一些移动平均列并通过连续值的微小差异使整个时间序列平稳之前，将价格值标准化为 0 到 1 之间。</p></blockquote><h2 id="0f23" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">行动计划</h2><p id="53c5" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">行动方案定义了环境的行动空间，并将代理的行动转换成可执行的交易。例如，如果我们使用 3 个动作的离散动作空间(0 = <code class="fe nm nn no np b">hold</code>，1 = <code class="fe nm nn no np b">buy 100%</code>，2 = <code class="fe nm nn no np b">sell 100%</code>)，我们的学习代理不需要知道返回一个动作 1 等同于购买一个乐器。相反，我们的代理需要知道在特定情况下返回动作 1 的回报，并且可以将动作转换为交易的实现细节留给<code class="fe nm nn no np b">ActionScheme</code>。</p><p id="ded3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">每个动作方案都有一个<code class="fe nm nn no np b">get_trade</code>方法，它会将代理的指定动作转化为可执行的<code class="fe nm nn no np b">Trade</code>。通常需要在方案中存储额外的状态，例如跟踪当前交易的头寸。每次调用动作方案的<code class="fe nm nn no np b">reset</code>方法时都应该重置该状态，这是在重置父<code class="fe nm nn no np b">TradingEnvironment</code>时自动完成的。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="122d" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.actions <strong class="np iu">import </strong>DiscreteActions</span><span id="c98a" class="nq mu it np b gy or on l oo op">action_scheme = DiscreteActions(n_actions=20,      <br/>                                instrument_symbol='BTC')</span></pre><blockquote class="ku kv kw"><p id="5675" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该离散动作方案使用 20 个离散动作，相当于 5 种交易类型(市场买入/卖出、限价买入/卖出和持有)中每种交易类型的 4 个离散金额。例如[0，5，10，15]= <code class="fe nm nn no np b">hold</code>，1= <code class="fe nm nn no np b">market buy 25%</code>，2= <code class="fe nm nn no np b">market sell 25%</code>，3= <code class="fe nm nn no np b">limit buy 25%</code>，4= <code class="fe nm nn no np b">limit sell 25%</code>，6= <code class="fe nm nn no np b">market buy 50%</code>，7= <code class="fe nm nn no np b">market sell 50%</code>等…</p></blockquote><h2 id="cd57" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">奖励计划</h2><p id="419c" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">奖励方案接收在每个时间步进行的交易，并返回一个<code class="fe nm nn no np b">float</code>，对应于特定行动的收益。例如，如果这一步采取的行动是导致正利润的出售，我们的<code class="fe nm nn no np b">RewardScheme</code>可以返回一个正数，以鼓励更多这样的交易。另一方面，如果行动是导致损失的销售，该方案可以返回负奖励，以教导代理人在未来不要做出类似的行动。</p><p id="4413" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">该示例算法的一个版本在<code class="fe nm nn no np b">SimpleProfit</code>组件中实现，然而显然可以使用更复杂的策略来代替。</p><p id="ba87" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">每个奖励方案都有一个<code class="fe nm nn no np b">get_reward</code>方法，它接受在每个时间步执行的交易，并返回一个与该动作的<em class="kz">值</em>相对应的浮点数。与动作方案一样，出于各种原因，经常需要在奖励方案中存储附加状态。每次调用奖励方案的<code class="fe nm nn no np b">reset</code>方法时都应该重置该状态，这是在重置父<code class="fe nm nn no np b">TradingEnvironment</code>时自动完成的。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="2da4" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.rewards <strong class="np iu">import </strong>SimpleProfit</span><span id="a03f" class="nq mu it np b gy or on l oo op">reward_scheme = SimpleProfit()</span></pre><blockquote class="ku kv kw"><p id="2db9" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单利润方案返回的回报为-1 表示不持有交易，1 表示持有交易，2 表示购买工具，如果出售工具，则对应于交易所得(正/负)利润的值。</p></blockquote><h1 id="6f18" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">学习代理</h1><p id="57f0" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">到目前为止，我们还没有看到深度强化学习框架的“深度”部分。这就是学习代理的用武之地。学习代理是数学(阅读:魔术)发生的地方。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/c9f57de1176e4c1ae0ae8da8fedb1dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*thRJ6ENkP7sRepftGiq3tA.jpeg"/></div></div></figure><p id="0547" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">在每个时间步，代理将来自环境的观察作为输入，通过其底层模型(大部分时间是神经网络)运行它，并输出要采取的行动。例如，观察值可能是交易所以前的开盘价、最高价、最低价和收盘价。学习模型将这些值作为输入，并输出对应于要采取的动作的值，例如购买、出售或持有。</p><p id="9137" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">重要的是要记住，学习模型对这些值所代表的价格或交易没有直觉。相反，该模型只是学习对于特定的输入值或输入值序列输出哪些值，以获得最高的回报。</p><h2 id="8cec" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated"><a class="ae lu" href="https://stable-baselines.readthedocs.io/en/master" rel="noopener ugc nofollow" target="_blank">稳定基线</a></h2><p id="a07b" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">在本例中，我们将使用<a class="ae lu" href="https://stable-baselines.readthedocs.io/en/master" rel="noopener ugc nofollow" target="_blank">稳定基线</a>库为我们的交易策略提供学习代理，然而，TensorTrade 框架与许多强化学习库兼容，如<a class="ae lu" href="https://tensorforce.readthedocs.io/en/0.4.4" rel="noopener ugc nofollow" target="_blank"> Tensorforce </a>、<a class="ae lu" href="https://ray.readthedocs.io/en/latest/rllib.html" rel="noopener ugc nofollow" target="_blank"> Ray 的 RLLib </a>、<a class="ae lu" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank"> OpenAI 的基线</a>、<a class="ae lu" href="https://github.com/NervanaSystems/coach" rel="noopener ugc nofollow" target="_blank">英特尔的蔻驰</a>，或 TensorFlow 系列中的任何库，如<a class="ae lu" href="https://github.com/tensorflow/agents" rel="noopener ugc nofollow" target="_blank"> TF 代理</a>。</p><p id="eb1a" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">自定义 TensorTrade 学习代理可能会在未来添加到该框架中，尽管该框架的目标始终是与尽可能多的现有强化学习库进行互操作，因为该领域有如此多的并发增长。</p><p id="96e4" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">但是现在，稳定的基线对于我们的需求来说足够简单和强大。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="0a3d" class="nq mu it np b gy om on l oo op"><strong class="np iu">from </strong>stable_baselines.common.policies <strong class="np iu">import </strong>MlpLnLstmPolicy<br/><strong class="np iu">from </strong>stable_baselines <strong class="np iu">import </strong>PPO2</span><span id="ea2d" class="nq mu it np b gy or on l oo op">model = PPO2<br/>policy = MlpLnLstmPolicy<br/>params = { "learning_rate": 1e-5 }</span><span id="29c5" class="nq mu it np b gy or on l oo op">agent = model(policy, environment, model_kwargs=params)</span></pre><blockquote class="ku kv kw"><p id="27cb" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意:使用 TensorTrade 并不需要稳定的基线，尽管在本教程中它是必需的。这个例子使用了一个支持 GPU 的近似策略优化模型和一个层标准化的 LSTM 感知器网络。如果您想了解更多关于稳定基线的信息，您可以查看<a class="ae lu" href="https://stable-baselines.readthedocs.io/en/master/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p></blockquote><p id="3851" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><a class="ae lu" href="https://tensorforce.readthedocs.io/en/0.4.4" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu">张量力</strong> </a></p><p id="a6ec" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">我还将快速介绍一下<a class="ae lu" href="https://tensorforce.readthedocs.io/en/0.4.4" rel="noopener ugc nofollow" target="_blank"> Tensorforce </a>库，以展示在强化学习框架之间切换是多么简单。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="1a99" class="nq mu it np b gy om on l oo op"><strong class="np iu">from </strong>tensorforce.agents <strong class="np iu">import </strong>Agent</span><span id="244d" class="nq mu it np b gy or on l oo op">agent_spec = {<br/>    "type": "ppo_agent",<br/>    "step_optimizer": {<br/>        "type": "adam",<br/>        "learning_rate": 1e-4<br/>    },<br/>    "discount": 0.99,<br/>    "likelihood_ratio_clipping": 0.2,<br/>}</span><span id="651f" class="nq mu it np b gy or on l oo op">network_spec = [<br/>    dict(type='dense', size=64, activation="tanh"),<br/>    dict(type='dense', size=32, activation="tanh")<br/>]</span><span id="9bc2" class="nq mu it np b gy or on l oo op">agent = Agent.from_spec(<em class="kz">spec</em>=agent_spec,<br/>                        <em class="kz">kwargs</em>=<em class="kz">dict</em>(<em class="kz">network</em>=network_spec,<br/>                                    <em class="kz">states</em>=environment.states,<br/>                                    <em class="kz">actions</em>=environment.actions))</span></pre><blockquote class="ku kv kw"><p id="18b5" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你想了解更多关于 Tensorforce 代理的信息，你可以查看<a class="ae lu" href="https://tensorforce.readthedocs.io/en/0.4.4" rel="noopener ugc nofollow" target="_blank">文档</a>。</p></blockquote><h1 id="0ea3" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">交易策略</h1><p id="f9b9" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">一个<code class="fe nm nn no np b">TradingStrategy</code>由一个学习代理和一个或多个交易环境组成，用于调整、训练和评估。如果只提供一个环境，它将用于调优、培训和评估。否则，可以在每个步骤提供单独的环境。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="620b" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.strategies <strong class="np iu">import </strong>TensorforceTradingStrategy,<br/>                                   StableBaselinesTradingStrategy</span><span id="2a3e" class="nq mu it np b gy or on l oo op">a_strategy = TensorforceTradingStrategy(environment=environment,<br/>                                        agent_spec=agent_spec,<br/>                                        network_spec=network_spec)</span><span id="2c78" class="nq mu it np b gy or on l oo op">b_strategy = StableBaselinesTradingStrategy(environment=environment,<br/>                                            model=PPO2,<br/>                                            policy=MlpLnLSTMPolicy)</span></pre><blockquote class="ku kv kw"><p id="cb6f" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您还不理解策略初始化，请不要担心，稍后会有更详细的解释。</p></blockquote><h1 id="4761" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">把所有的放在一起</h1><p id="b978" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">现在我们知道了组成<code class="fe nm nn no np b">TradingStrategy</code>的每个组件，让我们构建并评估一个组件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/19124a7c009dd59c6e7c27a5f76beb91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fX-iNWdA0OUzdNcOrgAsSw.jpeg"/></div></div></figure><p id="a58c" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">简单回顾一下，<code class="fe nm nn no np b">TradingStrategy</code>由<code class="fe nm nn no np b">TradingEnvironment</code>和学习代理组成。一个<code class="fe nm nn no np b">TradingEnvironment</code>是一个<code class="fe nm nn no np b">gym</code>环境，它接受一个<code class="fe nm nn no np b">Exchange</code>、一个<code class="fe nm nn no np b">ActionScheme</code>、一个<code class="fe nm nn no np b">RewardScheme</code>和一个可选的<code class="fe nm nn no np b">FeaturePipeline</code>，并返回观察结果和奖励，学习代理可以在这些观察结果和奖励上进行培训和评估。</p><h2 id="d343" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">创造环境</h2><p id="374c" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">第一步是使用上面概述的组件创建一个<code class="fe nm nn no np b">TradingEnvironment</code>。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="6f9e" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.exchanges.simulated <strong class="np iu">import</strong> FBMExchange<br/><strong class="np iu">from</strong> tensortrade.features.scalers <strong class="np iu">import</strong> MinMaxNormalizer<br/><strong class="np iu">from</strong> tensortrade.features.stationarity <strong class="np iu">import</strong> FractionalDifference<br/><strong class="np iu">from</strong> tensortrade.features <strong class="np iu">import</strong> FeaturePipeline<br/><strong class="np iu">from</strong> tensortrade.rewards <strong class="np iu">import</strong> SimpleProfit<br/><strong class="np iu">from</strong> tensortrade.actions <strong class="np iu">import</strong> DiscreteActions<br/><strong class="np iu">from</strong> tensortrade.environments <strong class="np iu">import</strong> TradingEnvironment</span><span id="880d" class="nq mu it np b gy or on l oo op">normalize_price = MinMaxNormalizer(["open", "high", "low", "close"])<br/>difference = FractionalDifference(difference_order=0.6)<br/>feature_pipeline = FeaturePipeline(steps=[normalize_price, <br/>                                          difference])</span><span id="052b" class="nq mu it np b gy or on l oo op">exchange = FBMExchange(timeframe='1h',<br/>                       base_instrument='BTC',<br/>                       feature_pipeline=feature_pipeline)</span><span id="a90e" class="nq mu it np b gy or on l oo op">reward_scheme = SimpleProfit()</span><span id="f2e2" class="nq mu it np b gy or on l oo op">action_scheme = DiscreteActions(n_actions=20, <br/>                                instrument_symbol='ETH/BTC')</span><span id="3e46" class="nq mu it np b gy or on l oo op">environment = TradingEnvironment(exchange=exchange,<br/>                                 action_scheme=action_scheme,<br/>                                 reward_scheme=reward_scheme,<br/>                                 feature_pipeline=feature_pipeline)</span></pre><p id="8260" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">很简单，现在<code class="fe nm nn no np b">environment</code>是一个<code class="fe nm nn no np b">gym</code>环境，可以被任何兼容的交易策略或学习代理使用。</p><h2 id="cde4" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">定义代理</h2><p id="c8ac" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">既然环境已经设置好了，是时候创建我们的学习代理了。同样，我们将为此使用稳定的基线，但也可以在这里随意添加任何其他强化学习代理。</p><p id="d0a7" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">由于我们使用的是<code class="fe nm nn no np b">StableBaselinesTradingStrategy</code>，所以我们需要做的就是为要训练的底层神经网络提供模型类型和策略类型。对于这个例子，我们将使用一个简单的近似策略优化(PPO)模型和一个层标准化的 LSTM 策略网络。</p><p id="cafd" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">有关模型和策略规范的更多示例，请参见稳定基线<a class="ae lu" href="https://stable-baselines.readthedocs.io/en/master" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="7784" class="nq mu it np b gy om on l oo op"><strong class="np iu">from </strong>stable_baselines.common.policies <strong class="np iu">import </strong>MlpLnLstmPolicy<br/><strong class="np iu">from </strong>stable_baselines <strong class="np iu">import </strong>PPO2</span><span id="0d2f" class="nq mu it np b gy or on l oo op">model = PPO2<br/>policy = MlpLnLstmPolicy<br/>params = { "learning_rate": 1e-5 }</span></pre><h2 id="e048" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">训练策略</h2><p id="9354" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">创建我们的交易策略就像插入我们的代理和环境一样简单。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="38e0" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.strategies <strong class="np iu">import </strong>StableBaselinesTradingStrategy</span><span id="253e" class="nq mu it np b gy or on l oo op">strategy = StableBaselinesTradingStrategy(environment=environment,<br/>                                          model=model,<br/>                                          policy=policy,<br/>                                          model_kwargs=params)</span></pre><p id="2c75" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">然后，为了训练策略(即，在当前环境下训练代理)，我们需要做的就是调用<code class="fe nm nn no np b">strategy.run()</code>并告知您想要运行的步骤或剧集的总数。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="91aa" class="nq mu it np b gy om on l oo op">performance = strategy.run(steps=100000,<br/>                           episode_callback=stop_early_callback)</span></pre><p id="37f3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">瞧啊。三个小时和数以千计的打印报表后，你会看到你的代理如何做的结果！</p><p id="5a7f" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">如果这个反馈循环对你来说有点慢，可以给<code class="fe nm nn no np b">run</code>传递一个回调函数，每集结束时都会调用。回调函数将传入一个包含代理在那一集的表现的数据帧，并期待一个<code class="fe nm nn no np b">bool</code>作为返回。如果<code class="fe nm nn no np b">True</code>，代理将继续培训，否则，代理将停止并返回其整体绩效。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/db5ab9d36c03e60f48dfffb6606d8643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zZnyEN2N3UTsTmI4g4-sYw.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">A simple performance output at the end of the episode, including the final 5 balances and net worths of the agent.</figcaption></figure><h2 id="f04b" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">保存和恢复</h2><p id="6883" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">所有的交易策略都能够将它们的代理保存到一个文件中，以便以后恢复。环境没有被保存，因为它没有我们关心保存的状态。为了将我们的<code class="fe nm nn no np b">TensorflowTradingStrategy</code>保存到一个文件中，我们只需要将文件的<code class="fe nm nn no np b">path</code>提供给我们的策略。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="0fc2" class="nq mu it np b gy om on l oo op">strategy.save_agent(path="../agents/ppo_btc_1h")</span></pre><p id="c83b" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">为了从文件中恢复代理，在调用<code class="fe nm nn no np b">restore_agent</code>之前，我们首先需要实例化我们的策略。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="9176" class="nq mu it np b gy om on l oo op"><strong class="np iu">from</strong> tensortrade.strategies <strong class="np iu">import </strong>StableBaselinesTradingStrategy</span><span id="35f9" class="nq mu it np b gy or on l oo op">strategy = StableBaselinesTradingStrategy(environment=environment,<br/>                                          model=model,<br/>                                          policy=policy,<br/>                                          model_kwargs=params)</span><span id="386b" class="nq mu it np b gy or on l oo op">strategy.restore_agent(path="../agents/ppo_btc/1h")</span></pre><p id="77b0" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">我们的策略现在恢复到了之前的状态，并准备再次使用。</p><h2 id="d17a" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">调整您的策略</h2><p id="d318" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">有时，交易策略需要在一个环境中调整一组超参数或特征，以达到最佳性能。在这种情况下，每个<code class="fe nm nn no np b">TradingStrategy</code>提供一个可选的可实现的<code class="fe nm nn no np b">tune</code>方法。</p><p id="39de" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">调整模型类似于训练模型，但是除了调整和保存最佳执行模型的权重和偏差之外，该策略还调整和保持产生该模型的超参数。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="415a" class="nq mu it np b gy om on l oo op"><strong class="np iu">from </strong>tensortrade.environments <strong class="np iu">import </strong>TradingEnvironment<br/><strong class="np iu">from </strong>tensortrade.exchanges.simulated <strong class="np iu">import </strong>FBMExchange</span><span id="7d21" class="nq mu it np b gy or on l oo op">exchange = FBMExchange(timeframe='1h',<br/>                       base_instrument='BTC',<br/>                       feature_pipeline=feature_pipeline)</span><span id="96c3" class="nq mu it np b gy or on l oo op">environment = TradingEnvironment(exchange=exchange,<br/>                                 action_scheme=action_scheme,<br/>                                 reward_scheme=reward_scheme)</span><span id="6730" class="nq mu it np b gy or on l oo op">strategy.environment = environment</span><span id="d5d3" class="nq mu it np b gy or on l oo op">tuned_performance = strategy.tune(episodes=10)</span></pre><blockquote class="ku kv kw"><p id="091e" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种情况下，代理将被训练 10 集，每集有一组不同的超参数。最佳设置将保存在策略中，并在此后调用<code class="fe nm nn no np b">strategy.run()</code>时使用。</p></blockquote><h2 id="299c" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">战略评估</h2><p id="1cbd" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">现在我们已经调优并训练了我们的代理，是时候看看它的表现如何了。为了评估我们的策略在看不见的数据上的性能，我们需要在这样的数据支持的新环境上运行它。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="ef0c" class="nq mu it np b gy om on l oo op"><strong class="np iu">from </strong>pandas <strong class="np iu">import </strong>pd</span><span id="553c" class="nq mu it np b gy or on l oo op"><strong class="np iu">from </strong>tensortrade.environments <strong class="np iu">import </strong>TradingEnvironment<br/><strong class="np iu">from </strong>tensortrade.exchanges.simulated <strong class="np iu">import </strong>SimulatedExchange</span><span id="0e34" class="nq mu it np b gy or on l oo op">df = pd.read_csv('./btc_ohlcv_1h.csv')</span><span id="ba67" class="nq mu it np b gy or on l oo op">exchange = SimulatedExchange(data_frame=df, <br/>                             base_instrument='BTC',<br/>                             feature_pipeline=feature_pipeline)</span><span id="0152" class="nq mu it np b gy or on l oo op">environment = TradingEnvironment(exchange=exchange,<br/>                                 action_scheme=action_scheme,<br/>                                 reward_scheme=reward_scheme)</span><span id="b2ef" class="nq mu it np b gy or on l oo op">strategy.environment = environment</span><span id="7546" class="nq mu it np b gy or on l oo op">test_performance = strategy.run(episodes=1, testing=True)</span></pre><p id="10a3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">完成后，<code class="fe nm nn no np b">strategy.run</code>返回代理绩效的 Pandas 数据框架，包括代理在每个时间步的净值和余额。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/1eaad7ad7be62ed5a15019ed80209ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0K0d1yj7SXhYVDLbX18CBQ.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Example evaluation performance — this agent has not been trained on this feature set, so performance is arbitrary.</figcaption></figure><h2 id="2667" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">现场交易</h2><p id="6c3e" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">一旦你建立了一个有利可图的交易策略，训练了一个代理来正确交易它，并确保它对新数据集的“泛化能力”，剩下要做的就是盈利。使用像<code class="fe nm nn no np b">CCXTExchange</code>这样的实时交流，你可以插入你的策略并让它运行！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/bbf175d212b8252d6c150b8409457a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZCVKPbgkxxgAYt7iZWK_w.jpeg"/></div></div></figure><p id="0fa7" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">虽然你可能喜欢开始一个策略，并让它无限制地运行，但你可以使用一个<code class="fe nm nn no np b">trade_callback</code>，它将在每次策略交易时被调用。这个回调函数类似于剧集回调，将传入一个包含代理整体表现的数据帧，并期望得到一个<code class="fe nm nn no np b">bool</code>作为回报。如果<code class="fe nm nn no np b">True</code>，代理将继续交易，否则，代理将停止交易并返回其在整个交易期间的表现。</p><pre class="kj kk kl km gt oi np oj ok aw ol bi"><span id="b066" class="nq mu it np b gy om on l oo op"><strong class="np iu">import </strong>ccxt</span><span id="1da7" class="nq mu it np b gy or on l oo op"><strong class="np iu">from </strong>tensortrade.environments <strong class="np iu">import </strong>TradingEnvironment<br/><strong class="np iu">from </strong>tensortrade.strategies <strong class="np iu">import </strong>StableBaselinesTradingStrategy<br/><strong class="np iu">from </strong>tensortrade.exchanges.live <strong class="np iu">import </strong>CCXTExchange</span><span id="2137" class="nq mu it np b gy or on l oo op">coinbase = ccxt.coinbasepro(...)</span><span id="6860" class="nq mu it np b gy or on l oo op">exchange = CCXTExchange(exchange=coinbase,<br/>                        timeframe='1h',<br/>                        base_instrument='USD', <br/>                        feature_pipeline=feature_pipeline)</span><span id="8fbb" class="nq mu it np b gy or on l oo op">environment = TradingEnvironment(exchange=exchange,<br/>                                 action_scheme=action_scheme,<br/>                                 reward_scheme=reward_scheme)</span><span id="4ff1" class="nq mu it np b gy or on l oo op">strategy.environment = environment</span><span id="7f06" class="nq mu it np b gy or on l oo op">strategy.restore_agent(path="../agents/ppo_btc/1h")</span><span id="9839" class="nq mu it np b gy or on l oo op"><em class="kz">live_performance = strategy.run(steps=0, trade_callback=episode_cb)</em></span></pre><blockquote class="ku kv kw"><p id="9ac1" class="kx ky kz la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过<code class="fe nm nn no np b">steps=0</code>指示策略运行，直到停止。</p></blockquote><p id="89b6" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">这就是全部了！正如你所看到的，使用简单的组件和深度强化学习来构建复杂的交易策略是非常简单的。你还在等什么？投入其中，亲自动手，看看使用 TensorTrade 能做些什么。</p><h1 id="99f2" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">未来</h1><p id="3da1" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">目前，该框架正处于早期阶段。到目前为止，重点是获得一个工作原型，具有创建高利润战略所需的所有必要构件。下一步是构建未来的路线图，并决定哪些即将到来的构件对社区是重要的。</p><p id="3dd7" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">很快，我们将看到框架中增加了高度信息化的可视化环境，以及更多交易所、交易更多工具的更深入的策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/a65b7a5d8ffb05c4b766b34350d303c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/1*CjZTzPsMcHu2c_i338Ft8g.gif"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">An example environment visualization, created in a <a class="ae lu" rel="noopener" target="_blank" href="/using-reinforcement-learning-to-trade-bitcoin-for-massive-profit-b69d0e8f583b">previous article</a>.</figcaption></figure><p id="255a" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">天空是极限。基础(即框架)已经奠定，现在要由社区来决定下一步做什么。我希望你能成为其中的一员。</p><h2 id="af85" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">最后的想法</h2><p id="2645" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">TensorTrade 是一个强大的框架，能够构建高度模块化、高性能的交易系统。尝试新的交易和投资策略是相当简单和容易的，同时允许你在另一个策略中利用一个策略的成分。但不要相信我的话，创建一个自己的策略，并开始教你的机器人接管世界！</p><p id="00c3" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">虽然这个教程应该足以让你开始，但如果你想创造一个有利可图的交易策略，还有很多东西要学。我鼓励你去<a class="ae lu" href="https://github.com/notadamking/tensortrade" rel="noopener ugc nofollow" target="_blank"> Github </a>看看代码库，或者看看我们在 tensortrade.org<a class="ae lu" href="https://tensortrade.org" rel="noopener ugc nofollow" target="_blank">的文档。还有一个相当活跃的</a><a class="ae lu" href="https://discord.gg/ZZ7BGWh" rel="noopener ugc nofollow" target="_blank"> Discord 社区</a>，共有近 1000 名成员，所以如果你有问题、反馈或功能请求，请随时在那里提出！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/f3145ab8e673c9c1bd3dcf748d17d7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*msW28avu4hBSsDZu9j-kqA@2x.png"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">This is my GitHub commit history for the TensorTrade framework — you could say I’ve been busy.</figcaption></figure><p id="5b98" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated">我已经让这个项目达到了高度可用的状态。不过，我的时间有限，我相信你们中有很多人可以为开源代码库做出有价值的贡献。因此，如果你是一个对构建最先进的交易系统感兴趣的开发人员或数据科学家，我希望看到你打开一个拉请求，即使它只是一个简单的测试案例！</p><h2 id="6681" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">贡献的</h2><p id="efc1" class="pw-post-body-paragraph kx ky it la b lb oc ju ld le od jx lg lv oe lj lk lw of ln lo lx og lr ls lt im bi translated">其他人问他们如何在不写代码的情况下为项目做贡献。目前有三种方法可以做到这一点。</p><ol class=""><li id="6803" class="mf mg it la b lb lc le lf lv mh lw mi lx mj lt pa ml mm mn bi translated">为 TensorTrade 框架编写代码或文档。<a class="ae lu" href="https://github.com/notadamking/tensortrade" rel="noopener ugc nofollow" target="_blank"> Github </a>上的许多问题都是通过<a class="ae lu" href="https://gitcoin.co/profile/notadamking/active" rel="noopener ugc nofollow" target="_blank"> Gitcoin 智能合约</a>资助的，所以你可以通过贡献获得报酬。迄今为止，社区捐赠的近 10 ETH(～2000 美元)已经用于支付开源开发者对框架的贡献。</li><li id="d1b6" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt pa ml mm mn bi translated">用<a class="ae lu" href="https://www.blockchain.com/btc/address/1Lc47bhYvdyKGk1qN8oBHdYQTkbFLL3PFw" rel="noopener ugc nofollow" target="_blank">比特币</a>或<a class="ae lu" href="https://www.blockchain.com/eth/address/0x9907A0cF64Ec9Fbf6Ed8FD4971090DE88222a9aC" rel="noopener ugc nofollow" target="_blank">以太坊</a>资助这个项目。这些捐赠用于资助我们的<a class="ae lu" href="https://gitcoin.co/profile/notadamking/active" rel="noopener ugc nofollow" target="_blank"> Gitcoin 智能合同</a>，它允许任何贡献优质代码和文档的人获得报酬。</li><li id="b2a1" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt pa ml mm mn bi translated">在 Github 上赞助我。Github 目前正在 1:1 匹配所有捐款，最高可达 5000 美元，因此这是赞助我的工作和 TensorTrade 发展的最佳时机。所有的赞助都直接用于资助框架的开源开发。</li></ol><p id="4807" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><em class="kz">感谢阅读！一如既往，本教程的所有代码都可以在我的</em> <a class="ae lu" href="https://github.com/notadamking/tensortrade" rel="noopener ugc nofollow" target="_blank"> <em class="kz"> GitHub </em> </a> <em class="kz">上找到。如果您有任何问题或反馈，请在下面留下评论，我很乐意收到您的来信！我也可以通过@notadamking 上的</em><a class="ae lu" href="https://twitter.com/notadamking" rel="noopener ugc nofollow" target="_blank"><em class="kz">Twitter</em></a><em class="kz">联系到。</em></p><p id="b2bc" class="pw-post-body-paragraph kx ky it la b lb lc ju ld le lf jx lg lv li lj lk lw lm ln lo lx lq lr ls lt im bi translated"><em class="kz">你也可以通过下面的链接在</em> <a class="ae lu" href="https://github.com/users/notadamking/sponsorship" rel="noopener ugc nofollow" target="_blank"> <em class="kz"> Github 赞助商</em> </a> <em class="kz">或者</em><a class="ae lu" href="https://www.patreon.com/join/notadamking" rel="noopener ugc nofollow" target="_blank"><em class="kz">Patreon</em></a><em class="kz">上赞助我。</em></p><div class="pb pc gp gr pd pe"><a href="https://github.com/users/notadamking/sponsorship" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">GitHub 赞助商</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">嗨，我是亚当。我是一名开发人员、作家和企业家，尤其对深度…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">github.com</p></div></div><div class="pn l"><div class="po l pp pq pr pn ps ks pe"/></div></div></a></div><div class="pb pc gp gr pd pe"><a href="https://patreon.com/notadamking" rel="noopener  ugc nofollow" target="_blank"><div class="pf ab fo"><div class="pg ab ph cl cj pi"><h2 class="bd iu gy z fp pj fr fs pk fu fw is bi translated">亚当·金正在创造改变世界的内容</h2><div class="pl l"><h3 class="bd b gy z fp pj fr fs pk fu fw dk translated">嗨，我是亚当。我是一名开发人员、作家和企业家，尤其对深度…</h3></div><div class="pm l"><p class="bd b dl z fp pj fr fs pk fu fw dk translated">patreon.com</p></div></div><div class="pn l"><div class="pt l pp pq pr pn ps ks pe"/></div></div></a></div><h2 id="3251" class="nq mu it bd mv nr ns dn mz nt nu dp nd lv nv nw nf lw nx ny nh lx nz oa nj ob bi translated">参考</h2><ul class=""><li id="06af" class="mf mg it la b lb oc le od lv pu lw pv lx pw lt mk ml mm mn bi translated"><strong class="la iu">【1。】深度强化学习介绍</strong>惠，乔纳森。" RL-深度强化学习入门."<em class="kz">中</em>2019 年 1 月 7 日<a class="ae lu" href="https://medium.com/@jonathan_hui/rl-introduction-to-deep-reinforcement-learning-35c25e04c199." rel="noopener">https://Medium . com/@ Jonathan _ hui/rl-introduction-to-deep-reinforcement-learning-35 c 25 e04c 199。</a></li><li id="9a38" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><strong class="la iu">【2。]政策梯度算法</strong> <br/>翁，李莲。“政策梯度算法。”<em class="kz">Lil ' log</em>2018 年 4 月 8 日<a class="ae lu" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#reinforce." rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/Lil-log/2018/04/08/policy-gradient-algorithms . html #加强。</a></li><li id="4c7b" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><strong class="la iu">【3。]干净的代码:敏捷软件技术手册</strong> <br/> <a class="ae lu" href="https://amzn.to/2XANX1X" rel="noopener ugc nofollow" target="_blank">马丁，罗伯特 C. <em class="kz">干净的代码:敏捷软件技术手册</em>。普伦蒂斯霍尔，2010 年</a>。</li><li id="8a9a" class="mf mg it la b lb mo le mp lv mq lw mr lx ms lt mk ml mm mn bi translated"><strong class="la iu">【4。]金融机器学习进展<br/> </strong> <a class="ae lu" href="https://amzn.to/2J6YCrW" rel="noopener ugc nofollow" target="_blank">普拉多马科斯 Ló pez de。<em class="kz">金融机器学习的进展</em>。威利，2018 </a>。</li></ul></div></div>    
</body>
</html>