<html>
<head>
<title>Generating optical flow using NVIDIA flownet2-pytorch implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NVIDIA flownet2-pytorch 实现生成光流</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-optical-flow-using-nvidia-flownet2-pytorch-implementation-d7b0ae6f8320?source=collection_archive---------5-----------------------#2019-07-04">https://towardsdatascience.com/generating-optical-flow-using-nvidia-flownet2-pytorch-implementation-d7b0ae6f8320?source=collection_archive---------5-----------------------#2019-07-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ee55" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">视频分类算法中使用的光流文件创建指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/71edc921aeb1895babe3a36f8f4bbe3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Un3UEItWCuxit6Uf1BkaDA.png"/></div></div></figure><p id="98ec" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这篇博客最初发表于<a class="ae lq" href="https://blog.dancelogue.com/generating-optical-flow-using-flownet-for-human-action-deep-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">blog.dancelogue.com</a>。在之前的<a class="ae lq" href="https://blog.dancelogue.com/what-is-optical-flow-and-why-does-it-matter/" rel="noopener ugc nofollow" target="_blank">帖子</a>中，进行了光流的介绍，以及基于<a class="ae lq" href="https://arxiv.org/abs/1612.01925" rel="noopener ugc nofollow" target="_blank"> FlowNet 2.o 论文</a>的光流架构概述。本博客将重点深入光流，这将通过从标准 Sintel 数据和自定义舞蹈视频生成光流文件来完成。将使用<a class="ae lq" href="https://github.com/NVIDIA/flownet2-pytorch" rel="noopener ugc nofollow" target="_blank"> NVIDIA flownet2-pytorch </a>代码库的<a class="ae lq" href="https://github.com/dancelogue/flownet2-pytorch" rel="noopener ugc nofollow" target="_blank"> fork </a>进行，该代码库可在<a class="ae lq" href="https://github.com/dancelogue/flownet2-pytorch" rel="noopener ugc nofollow" target="_blank"> Dancelogue 关联回购</a>中找到。</p><p id="a366" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本博客的目标是:</p><ul class=""><li id="a989" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">启动并运行 flownet2-pytorch 代码库。</li><li id="7b95" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">下载相关数据集，如原始存储库中提供的示例所述。</li><li id="a725" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">生成光流文件，然后研究光流文件的结构。</li><li id="8ed3" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">将流文件转换为颜色编码方案，使其更易于人类理解。</li><li id="ca75" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">将光流生成应用于舞蹈视频并分析结果。</li></ul><h1 id="4b05" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">系统需求</h1><p id="8450" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">flownet2-pytorch 实现被设计为与 GPU 一起工作。不幸的是，这意味着如果你没有访问权限，就不可能完全关注这个博客。为了缓解这个问题，我们提供了模型生成的样本数据，并允许读者继续阅读博客的其余部分。</p><p id="2496" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本教程的其余部分是使用 ubuntu 18.04 和 NVIDIA GEFORCE GTX 1080 Ti GPU 进行的。Docker 是必需的，并且必须支持 GPU，这可以通过使用<a class="ae lq" href="https://github.com/NVIDIA/nvidia-docker" rel="noopener ugc nofollow" target="_blank">NVIDIA-docker</a>r 包来实现。</p><h1 id="e1cc" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">下载代码库和数据集</h1><p id="8261" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">这里列出了继续写博客所需的所有代码和数据(下载数据是自动进行的，因此读者不必手动完成，请参见入门部分):</p><ul class=""><li id="462d" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">这个博客的代码库可以从下面的<a class="ae lq" href="https://github.com/dancelogue/flownet2-pytorch" rel="noopener ugc nofollow" target="_blank"> repo </a>中克隆。</li><li id="5351" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">点击下面的<a class="ae lq" href="http://files.is.tue.mpg.de/sintel/MPI-Sintel-complete.zip" rel="noopener ugc nofollow" target="_blank">链接</a>可以下载 Sintel 数据，压缩后的文件是 5.63 GB，解压后增加到 12.24 GB。</li><li id="57e8" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">可以下载自定义数据，包括<a class="ae lq" href="https://dancelogue.s3.amazonaws.com/open_source/datasets/generating-optical-flow-using-flownet-for-human-action-deep-learning-algorithms/000835.flo" rel="noopener ugc nofollow" target="_blank">样本光流</a> <code class="fe nc nd ne nf b">.flo</code>文件，从样本光流文件生成的<a class="ae lq" href="https://dancelogue.s3.amazonaws.com/open_source/datasets/generating-optical-flow-using-flownet-for-human-action-deep-learning-algorithms/000835.flo.png" rel="noopener ugc nofollow" target="_blank">颜色编码方案</a>，进行光流的<a class="ae lq" href="https://dancelogue.s3.amazonaws.com/open_source/datasets/generating-optical-flow-using-flownet-for-human-action-deep-learning-algorithms/sample-video.mp4" rel="noopener ugc nofollow" target="_blank">舞蹈视频</a>，舞蹈视频的<a class="ae lq" href="https://dancelogue.s3.amazonaws.com/open_source/datasets/generating-optical-flow-using-flownet-for-human-action-deep-learning-algorithms/sample-optical-flow-video.mp4" rel="noopener ugc nofollow" target="_blank">光流视频表示</a>。</li></ul><p id="874b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">完成这篇博客所需的内存空间大约是 32 GB。其原因将在后面解释。</p><h1 id="d876" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">叉子的差异</h1><p id="4dc5" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">如前所述，创建了原始 flownet2-pytorch 的一个分支，这是因为在撰写本博客时，原始存储库在构建和运行 docker 映像时出现了问题，例如 python 包版本问题、c 库编译问题等。这些更新包括:</p><ul class=""><li id="c451" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">通过修复 python 包版本、更新 cuda 和 pytorch 版本、运行相关层的自动构建和安装、添加 ffmpeg、添加第三方 github 包来修改 Dockerfile，该第三方 github 包将允许流文件的读取、处理和转换到颜色编码方案。</li><li id="652f" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">为数据集和训练模型编写下载脚本以便更容易开始，其灵感来自 NVIDIA 的<a class="ae lq" href="https://github.com/NVIDIA/vid2vid" rel="noopener ugc nofollow" target="_blank"> vid2vid </a>存储库。</li></ul><h1 id="6dab" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">入门指南</h1><p id="e075" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">考虑到这一点，我们开始吧。第一件事是从<a class="ae lq" href="https://github.com/dancelogue/flownet2-pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/dancelogue/flownet2-pytorch</a>中克隆原始存储库的 dancelogue 分支。然后使用以下命令运行 docker 脚本:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="eaa7" class="nk mg it nf b gy nl nm l nn no">bash launch_docker.sh</span></pre><p id="ffab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">设置应该需要几分钟时间，之后，应该将终端上下文更改为 docker 会话。</p><p id="190a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来是下载相关的数据集，初始设置所需的所有数据都可以通过在 docker 上下文中运行以下命令来获得:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="3b41" class="nk mg it nf b gy nl nm l nn no">bash scripts/download.sh</span></pre><p id="0189" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这会将<code class="fe nc nd ne nf b">FlowNet2_checkpoint.pth.tar</code>模型权重下载到 models 文件夹，并将<code class="fe nc nd ne nf b">MPI-Sintel</code>数据下载到 datasets 文件夹。这是必需的，以便遵循《flownet2-pytorch 入门指南》中指示的推理示例的说明。定制的舞蹈视频以及样本光流<code class="fe nc nd ne nf b">.flo</code>文件也被下载。</p><p id="bddb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本博客中的其余命令已经自动化，可以通过以下方式运行:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="6ca8" class="nk mg it nf b gy nl nm l nn no">bash scripts/run.sh</span></pre><h1 id="f258" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">运行推理示例</h1><p id="2d18" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">运行原始推理示例的命令如下:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="8409" class="nk mg it nf b gy nl nm l nn no">python main.py --inference --model FlowNet2 --save_flow \ <br/>--inference_dataset MpiSintelClean \<br/>--inference_dataset_root /path/to/mpi-sintel/clean/dataset \<br/>--resume /path/to/checkpoints</span></pre><p id="146e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是，根据 fork，这已修改为:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="f7da" class="nk mg it nf b gy nl nm l nn no">python main.py --inference --model FlowNet2 --save_flow \ <br/>--inference_dataset MpiSintelClean \<br/>--inference_dataset_root datasets/sintel/training \<br/>--resume checkpoints/FlowNet2_checkpoint.pth.tar \<br/>--save datasets/sintel/output</span></pre><p id="2004" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们来分解一下:</p><ul class=""><li id="e521" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--model</code>表示要使用的型号变体。从<a class="ae lq" href="https://blog.dancelogue.com/what-is-optical-flow-and-why-does-it-matter/" rel="noopener ugc nofollow" target="_blank">之前的博客</a>我们看到这个可以是<code class="fe nc nd ne nf b">FlowNetC</code>、<code class="fe nc nd ne nf b">FlowNetCSS</code>或<code class="fe nc nd ne nf b">FlowNet2</code>，但是对于这个博客它被设置为<code class="fe nc nd ne nf b">FlowNet2</code>。</li><li id="b248" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--resume</code>参数指示训练模型权重的位置。已使用下载脚本将其下载到检查点文件夹中。请注意，训练模型权重有一定的许可限制，如果您需要在本博客之外使用它们，您应该遵守这些限制。</li><li id="7fbe" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--inference</code>参数简单地意味着，基于由来自训练数据的模型权重定义的学习能力，你能告诉我关于新数据集的什么。这不同于训练模型，其中模型权重将改变。</li><li id="95a5" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--inference_dataset</code>表示将输入何种类型的数据。在当前情况下，它是由<code class="fe nc nd ne nf b">MpiSintelClean</code>指定的 sintel。更多选项可以在<a class="ae lq" href="https://github.com/dancelogue/flownet2-pytorch/blob/master/datasets.py" rel="noopener ugc nofollow" target="_blank">https://github . com/dance Logue/flownet 2-py torch/blob/master/datasets . py</a>中找到，并被定义为类，例如<code class="fe nc nd ne nf b">FlyingChairs</code>。还有一个<code class="fe nc nd ne nf b">ImagesFromFolder</code>类，这意味着我们可以输入自定义数据，例如来自视频的帧，我们可以从中得出推论。</li><li id="4499" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--inference_dataset_root</code>表示将用于推理过程的数据的位置，该数据已被下载并解压缩到<code class="fe nc nd ne nf b">datasets/sintel</code>文件夹中。</li><li id="dd24" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--save_flow</code>参数表示推断的光流应该保存为<code class="fe nc nd ne nf b">.flo</code>文件。</li><li id="fb09" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><code class="fe nc nd ne nf b">--save</code>参数指示推断的光流文件以及日志应该保存到的位置。这是一个可选字段，默认为<code class="fe nc nd ne nf b">work/</code>位置。</li></ul><p id="65d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行上述命令将生成的光流文件保存到<code class="fe nc nd ne nf b">datasets/sintel/output/inference/run.epoch-0-flow-field</code>文件夹中。生成的光流文件的扩展名为<code class="fe nc nd ne nf b">.flo</code>，是流场的表示。</p><h1 id="b376" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">分析和可视化光流文件</h1><p id="92f3" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">现在光流文件已经生成，是时候分析结构了，以便更好地理解结果，并将其转换为流场颜色编码方案。本节使用的样本流文件可从以下<a class="ae lq" href="https://dancelogue.s3.amazonaws.com/open_source/datasets/generating-optical-flow-using-flownet-for-human-action-deep-learning-algorithms/000835.flo" rel="noopener ugc nofollow" target="_blank">链接</a>下载。</p><h1 id="62be" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">分析流文件</h1><p id="df6c" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">将光流文件加载到 numpy 是一个相当简单的过程，可以按如下方式进行:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="66ba" class="nk mg it nf b gy nl nm l nn no">path = Path('path/to/flow/file/&lt;filename&gt;.flo')<br/>with path.open(mode='r') as flo:<br/>    np_flow = np.fromfile(flo, np.float32)<br/>    print(np_flow.shape)</span></pre><p id="3e0a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">上面的语法基于 python3，其中文件被加载到一个缓冲区中，然后被送入 numpy。接下来的事情是试图理解由打印语句实现的流文件的基本特性。假设您使用提供的样本流文件，print 语句应该输出<code class="fe nc nd ne nf b">(786435,)</code>。这意味着对于每个流文件，它包含一个数组，数组中有 786453 个元素。单个流文件的内存占用大约为 15.7 MB，尽管看起来很小，但增长非常快，尤其是在查看具有数千帧的视频时。</p><p id="1368" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在继续之前，我们需要看看 http://vision.middlebury.edu/flow/code/flow-code/README.txt<a class="ae lq" href="http://vision.middlebury.edu/flow/code/flow-code/README.txt" rel="noopener ugc nofollow" target="_blank">中定义的光流规范。我们关心的是以下内容:</a></p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="1d59" class="nk mg it nf b gy nl nm l nn no">".flo" file format used for optical flow evaluation</span><span id="66e2" class="nk mg it nf b gy np nm l nn no">Stores 2-band float image for horizontal (u) and vertical (v) flow components.<br/>Floats are stored in little-endian order.<br/>A flow value is considered "unknown" if either |u| or |v| is greater than 1e9.</span><span id="7eb0" class="nk mg it nf b gy np nm l nn no">  bytes  contents</span><span id="e0df" class="nk mg it nf b gy np nm l nn no">  0-3     tag: "PIEH" in ASCII, which in little endian happens to be the float 202021.25<br/>          (just a sanity check that floats are represented correctly)<br/>  4-7     width as an integer<br/>  8-11    height as an integer<br/>  12-end  data (width*height*2*4 bytes total)<br/>          the float values for u and v, interleaved, in row order, i.e.,<br/>          u[row0,col0], v[row0,col0], u[row0,col1], v[row0,col1], ...</span></pre><p id="f8b3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基于上述规范，下面的代码将允许我们正确地读取流文件(借用自<a class="ae lq" href="https://github.com/georgegach/flowiz/blob/master/flowiz/flowiz.py#L25" rel="noopener ugc nofollow" target="_blank">https://github . com/georgegach/flowiz/blob/master/flowiz/flowiz . p</a>y)。</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="60cc" class="nk mg it nf b gy nl nm l nn no">with path.open(mode='r') as flo:<br/>  tag = np.fromfile(flo, np.float32, count=1)[0]<br/>  width = np.fromfile(flo, np.int32, count=1)[0]<br/>  height = np.fromfile(flo, np.int32, count=1)[0]</span><span id="241c" class="nk mg it nf b gy np nm l nn no">  print('tag', tag, 'width', width, 'height', height)</span><span id="9841" class="nk mg it nf b gy np nm l nn no">  nbands = 2<br/>  tmp = np.fromfile(flo, np.float32, count= nbands * width * height)<br/>  flow = np.resize(tmp, (int(height), int(width), int(nbands)))</span></pre><p id="abbf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">基于光流格式规范，希望上面的代码对正在发生的事情更有意义，即我们得到标签，然后是宽度，接着是高度。打印语句的输出是<code class="fe nc nd ne nf b">tag 202021.25 width 1024 height 384</code>。从给定的规范中，我们可以看到标签匹配健全性检查值，流文件的宽度是 1024，高度是 384。请注意，在读取文件缓冲区并将其加载到 numpy 时，正确的顺序很重要，这是因为 python 中读取文件的方式(字节是顺序读取的)，否则标签、高度和宽度会混淆。现在我们已经有了宽度和高度，我们可以读取其余的光流数据，并将其调整为更熟悉的形状，这是使用<code class="fe nc nd ne nf b">np.resize</code>方法完成的。</p><p id="f12f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">理解流向量如何被调整大小的一个快速方法是将它们打印到终端，这是通过运行以下代码来完成的:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="109e" class="nk mg it nf b gy nl nm l nn no">&gt;&gt; print(flow.shape)<br/>(384, 1024, 2)</span><span id="47de" class="nk mg it nf b gy np nm l nn no">&gt;&gt; print(flow[0][0])<br/>[-1.2117167 -1.557275]</span></pre><p id="6282" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">正如我们所料，新表示的形状意味着高度为 384，宽度为 1024，并且具有由 2 个值组成的位移向量。关注位置<code class="fe nc nd ne nf b">0, 0</code>处的像素，我们可以看到该点的位移向量似乎指向左侧和底部，即 x，y 图的左下象限，这意味着我们预计该位置的颜色代码是浅蓝色，甚至是基于下面给出的颜色编码方案的绿色。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4c6696b9b53a093c1eefb8911db8faa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/0*XM70oE5lMioSb7UL.jpg"/></div></figure><h1 id="2424" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">可视化流文件</h1><p id="bada" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">有不少开源代码库是为了可视化光流文件而编写的。为此选择的那个可以在 github 库<a class="ae lq" href="https://github.com/georgegach/flowiz" rel="noopener ugc nofollow" target="_blank">https://github.com/georgegach/flowiz</a>中找到。这样做的原因是，它允许从颜色编码方案中生成视频剪辑，这在稍后阶段将是有用的。假设使用了本教程开头提供的 docker 上下文，可以使用以下命令来生成光流的彩色编码图像文件。</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="a2de" class="nk mg it nf b gy nl nm l nn no">python -m flowiz \<br/>datasets/sintel/output/inference/run.epoch-0-flow-field/*.flo \<br/>-o datasets/sintel/output/color_coding \<br/>-v datasets/sintel/output/color_coding/video \<br/>-r 30</span></pre><p id="9f03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这将获取光流文件并生成图像文件，其中的位移向量用颜色编码，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/ee9e2653da065c1cdbf41fb3a50b14ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yPdv6c1Axd-Yym5Z.png"/></div></div></figure><p id="74fd" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了理解颜色编码方案，请查看<a class="ae lq" href="https://blog.dancelogue.com/what-is-optical-flow-and-why-does-it-matter/" rel="noopener ugc nofollow" target="_blank">之前关于光流的博客</a>。在位置 0，0，即图像的右下部分，我们确实可以看到浅蓝色，这是我们从位移向量中预期的颜色，即它是指向左侧和底部的向量的颜色。</p><h1 id="4ed5" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">将光流应用于舞蹈视频</h1><p id="399e" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">在这一节中，我们将使用一个舞蹈视频，并从中生成光流文件。舞蹈视频是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="c68e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它包括一个真实世界中的舞蹈编排课程。</p><h1 id="51bd" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">生成帧</h1><p id="fe14" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">当 flownet 代码库接收图像时，我们需要做的第一件事是将视频转换成帧，这可以通过以下使用 ffmpeg 的命令来完成。</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="1c6b" class="nk mg it nf b gy nl nm l nn no">ffmpeg -i datasets/dancelogue/sample-video.mp4 \<br/>datasets/dancelogue/frames/output_%02d.png</span></pre><p id="f112" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">它将在帧文件夹中按顺序输出帧，顺序很重要，因为 flownet 算法使用相邻图像来计算图像之间的光流。生成的帧占用 1.7 GB 的内存，而视频只有 11.7 MB，每帧大约 2 MB。</p><h1 id="ba49" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">产生光流</h1><p id="e6a1" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">光流表示可以通过运行以下命令来生成。</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="dcba" class="nk mg it nf b gy nl nm l nn no">python main.py --inference --model FlowNet2 --save_flow \<br/>--inference_dataset ImagesFromFolder \<br/>--inference_dataset_root datasets/dancelogue/frames/ \<br/>--resume checkpoints/FlowNet2_checkpoint.pth.tar \<br/>--save datasets/dancelogue/output</span></pre><p id="2ca8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这类似于我们在 sintel 数据集上运行的推理模型，不同之处在于从<code class="fe nc nd ne nf b">--inference_dataset</code>参数变为<code class="fe nc nd ne nf b">ImagesFromFolder</code>，并在<a class="ae lq" href="https://github.com/dancelogue/flownet2-pytorch/blob/master/datasets.py#L320" rel="noopener ugc nofollow" target="_blank">代码库</a>中定义。<code class="fe nc nd ne nf b">--inference_dataset_root</code>是生成的视频帧的路径。生成的光流文件占用 14.6 GB 的存储器，这是因为对于这个例子，每个光流文件大约为 15.7 MB。</p><h1 id="4d60" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">生成颜色编码方案</h1><p id="8d37" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">生成颜色编码方案的命令是:</p><pre class="kj kk kl km gt ng nf nh ni aw nj bi"><span id="991d" class="nk mg it nf b gy nl nm l nn no">python -m flowiz \<br/>datasets/dancelogue/output/inference/run.epoch-0-flow-field/*.flo \<br/>-o datasets/dancelogue/output/color_coding \<br/>-v datasets/dancelogue/output/color_coding/video \<br/>-r 30</span></pre><p id="c6c7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这使用了<a class="ae lq" href="https://github.com/georgegach/flow2image" rel="noopener ugc nofollow" target="_blank"> flowviz 库</a>和 ffmpeg。它不仅将光流颜色编码生成为<code class="fe nc nd ne nf b">.png</code>文件，而且<code class="fe nc nd ne nf b">-v -r 30</code>参数在<code class="fe nc nd ne nf b">30 fps</code>从图像文件生成视频。生成的彩色编码帧占用 422 MB 的内存，其中包括一个 8.7 MB 的视频文件，如果你正在浏览这个博客，它的名称是<code class="fe nc nd ne nf b">000000.flo.mp4</code>。</p><h1 id="cc9d" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结果</h1><p id="45bd" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">生成的光流的视频表示如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div></figure><p id="73b4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">舞蹈动作的要点可以从生成的视频中看出，不同的颜色表示动作的方向。然而，尽管视频中没有明显的运动，但可以看到有很多背景噪音，特别是在中心舞者周围。不幸的是，不清楚为什么会这样。</p><h1 id="674b" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">尺寸影响</h1><p id="b6f2" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">当运行 flownet 算法时，需要注意大小含义，例如，一个 11.7 MB 的视频在提取时会生成一个 1.7 GB 的单个帧文件。然而，当生成光流时，这变成包含所有光流表示的 14.6 GB 文件。这是因为每个光流文件占用大约 15.7 MB 的存储器，然而每个图像帧占用 2 MB 的存储器(对于所提供的例子的情况)。因此，当运行光流算法时，需要注意计算需求与空间的权衡。在为视频构建深度学习系统时，这种权衡将影响架构，这意味着要么以计算时间为代价按需(即，懒惰地)生成光流文件，要么以存储空间为代价提前生成所有需要的格式和表示并将其保存到文件系统。</p><h1 id="5b5e" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">结论</h1><p id="f6ca" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我们已经看到了如何使用 NVIDIA 的 flownet2-pytorch 实现的分支来生成光流文件，并对光流文件有了一个大致的了解。下一篇博客将介绍如何使用光流表示来理解视频内容，并将重点放在 2 流网络上。</p><p id="f3e5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果您有任何问题或需要澄清的事情，您可以在<a class="ae lq" href="https://mbele.io/mark" rel="noopener ugc nofollow" target="_blank">https://mbele.io/mark</a>和我预约时间</p><h1 id="563b" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">参考</h1><ul class=""><li id="9e37" class="lr ls it kw b kx mx la my ld nu lh nv ll nw lp lw lx ly lz bi translated"><a class="ae lq" href="https://blog.dancelogue.com/what-is-optical-flow-and-why-does-it-matter/" rel="noopener ugc nofollow" target="_blank">https://blog . dance Logue . com/什么是光流，为什么有关系/ </a></li><li id="fd1b" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://blogs.nvidia.com/blog/2016/08/22/difference-deep-learning-training-inference-ai/" rel="noopener ugc nofollow" target="_blank">https://blogs . NVIDIA . com/blog/2016/08/22/差异-深度学习-训练-推理-ai/ </a></li><li id="377e" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated"><a class="ae lq" href="https://stackoverflow.com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/28013200/reading-middlebury-flow-files-with-python-bytes-array-numpy</a></li></ul></div></div>    
</body>
</html>