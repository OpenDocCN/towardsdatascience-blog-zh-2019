<html>
<head>
<title>Differentiable Architecture Search for RNN with fastai</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 fastai 寻找 RNN 的差异化建筑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/differentiable-architecture-search-for-rnn-with-fastai-a5e247aeb937?source=collection_archive---------29-----------------------#2019-04-15">https://towardsdatascience.com/differentiable-architecture-search-for-rnn-with-fastai-a5e247aeb937?source=collection_archive---------29-----------------------#2019-04-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="e658" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">刘韩啸等人提出的可微分结构搜索(<a class="ae kl" href="https://openreview.net/pdf?id=S1eYHoC5FX" rel="noopener ugc nofollow" target="_blank"> DARTS </a>)是一种使神经网络结构设计过程自动化的算法。它最初是在纯 pytorch 中实现的<a class="ae kl" href="https://github.com/quark0/darts" rel="noopener ugc nofollow" target="_blank">。这篇文章触及了飞镖背后的关键思想，并展示了我如何使用</a><a class="ae kl" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank"> fastai </a>重新实现飞镖，以达到清晰和易用的目的。</p><p id="69a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">代码在<a class="ae kl" href="https://github.com/tinhb92/rnn_darts_fastai" rel="noopener ugc nofollow" target="_blank">https://github.com/tinhb92/rnn_darts_fastai</a>。</p><h1 id="74f7" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">飞镖</strong></h1><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi lk"><img src="../Images/35b86cd0db40f2cc798366d3620eab4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kVbmFbmwmDCbYnROzRe4pg.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">Overview of DARTS — from the paper</figcaption></figure><p id="c262" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<a class="ae kl" href="https://github.com/salesforce/awd-lstm-lm" rel="noopener ugc nofollow" target="_blank"> AWD-LSTM </a>作为主干，目标是找到一个好的 rnn 单元来填充模型的“递归”部分(RNNModel.rnns)。尝试不同的配置以最终提取最合适的 rnn 小区。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ma"><img src="../Images/f28f7055ded9be088e8b682ead5ca08c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QJn4EMjjMsxo5aXlM36Y8g.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">DARTS Algorithm — from the paper</figcaption></figure><p id="9f7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们来解释一下算法:</p><ol class=""><li id="80da" class="mb mc iq jp b jq jr ju jv jy md kc me kg mf kk mg mh mi mj bi translated"><strong class="jp ir"> train_search:搜索好的 rnn 细胞基因型</strong></li><li id="7273" class="mb mc iq jp b jq mk ju ml jy mm kc mn kg mo kk mg mh mi mj bi translated"><strong class="jp ir">训练:导出基因型，训练其收敛并在测试集上评估</strong></li></ol><p id="8fbc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第一部分，搜索好的 rnn 细胞基因型，包括连续松弛(即创建混合操作)和使用梯度下降交替更新架构α和权重θ</p><p id="7a26" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 rnn 单元的一个节点上，我们不仅仅使用<em class="mp"> 1 运算</em> (tanh，sigmoid …)，而是应用几个运算，并获得这些运算的加权和。实验中有<em class="mp"> 5 种运算</em> : none，tanh，relu，sigmoid，identity。在每个节点给予这些<em class="mp"> 5 操作</em>中的每一个的权重是可学习的参数。</p><p id="fcf9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在 Pytorch 中，我们使用<em class="mp"> torch.rand() </em>对此进行初始化，并设置<em class="mp"> requires_grad = True </em>。作者称此为<strong class="jp ir">α/架构参数</strong>以区别于<strong class="jp ir">θ/网络的正常参数</strong>。更新θ需要通常的向前和向后传递。</p><p id="de11" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">α的梯度在论文的等式(7)中描述:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi mq"><img src="../Images/2f38ce6451ec420be644bad18e58de51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0I9UCSFTwa2YHvlMsAUjCQ.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">Eq (7) in paper: Gradient for alpha</figcaption></figure><p id="7406" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">“ω”表示一步向前模型的权重。</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/58584e9d6e914be78967d2668b2c2d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*YA5rduIgVYUxZiVQU4jETg.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">ω’</figcaption></figure><p id="1aa0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式(7)的第一部分通过使用ω’对验证数据进行 1 次正向和 1 次反向传递来计算。等式(7)的第二部分使用有限差分近似来计算:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="gh gi ms"><img src="../Images/4aeea42a7e1aae5e87915bd4cff4c1bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZsy7JYYpp3aZgq0XTjCaw.png"/></div></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">Finite difference approximation</figcaption></figure><p id="0372" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ω+和ω-定义为:</p><figure class="ll lm ln lo gt lp gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/05a0349d4014770984fbc9c50eea00ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*pDfaI3UjMVNELfyGg1XTXw.png"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">ω+ and ω-</figcaption></figure><p id="9d7e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">评估有限差分只需要对权重进行 2 次向前传递，对α进行 2 次向后传递。复杂度从 O(|α| |θ|)降低到 O(|α|+|θ|)。</p><p id="4915" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练后，我们选择概率最高的操作(忽略无)将连续松弛转换为单元的 1 个离散规格。然后，这种基因型被训练收敛并在测试集上进行评估</p><h1 id="1d56" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">用 fastai 实现</strong></h1><p id="d8a1" class="pw-post-body-paragraph jn jo iq jp b jq mu js jt ju mv jw jx jy mw ka kb kc mx ke kf kg my ki kj kk ij bi translated">在最初的实现中，<strong class="jp ir"> train_seach </strong>和<strong class="jp ir"> train </strong>这两个阶段的训练循环用纯 pytorch 编码。如果循环不太复杂，这是没问题的，但是它会很快变得混乱，使代码难以破译。fastai 已经开发了一个<a class="ae kl" href="https://t.co/zPGakJihrZ" rel="noopener ugc nofollow" target="_blank">回调</a> <a class="ae kl" href="https://www.youtube.com/watch?v=roc-dOSeehM&amp;t=6s" rel="noopener ugc nofollow" target="_blank">系统</a>来将训练循环的组件归类为独立的部分，以保持清晰。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">callbacks at a glance — thanks to Jeremy’s <a class="ae kl" href="https://twitter.com/jeremyphoward/status/1117473237489688576" rel="noopener ugc nofollow" target="_blank">tweet</a> — <a class="ae kl" href="https://github.com/fastai/fastai_docs/blob/0df9565eb91bbb51fd920476d11263668ecd63a0/dev_course/dl2/09b_learner.ipynb" rel="noopener ugc nofollow" target="_blank">full notebook</a></figcaption></figure><p id="4b34" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以指定在训练的每个阶段做什么:on_train_begin、on_epoch_begin、on_batch_begin、on_loss_begin …等等，而不会弄乱代码。阅读<a class="ae kl" href="https://docs.fast.ai/callback.html" rel="noopener ugc nofollow" target="_blank">文档</a>了解更多信息！</p><p id="7341" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在最初的实现之后，我还有 DartsCell、DartsRnn 模块(用于 train 阶段)和它们各自的子类 DartsCellSearch、DartsRnnSearch(用于 train_search 阶段)。不同的是，为了便于理解，其他有趣的附加组件被放入它们自己的回调中。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">train search loop — <a class="ae kl" href="https://github.com/quark0/darts/blob/f276dd346a09ae3160f8e3aca5c7b193fda1da37/rnn/train_search.py#L169" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><p id="d6d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上面的大<strong class="jp ir"> train_search </strong>循环被一个带回调的学习者所取代，在回调中你可以快速看到所需内容的概述。</p><figure class="ll lm ln lo gt lp"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="lw lx gj gh gi ly lz bd b be z dk">learner with callbacks — <a class="ae kl" href="https://github.com/tinhb92/rnn_darts_fastai/blob/master/train_search_nb.ipynb" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><p id="ad6e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Train_search 有自己的 ArchParamUpdate 回调函数，上面提到的更新 alpha/architecture 参数的所有逻辑都存储在那里。<br/>alpha(代码中的 arch_p)的优化器(arch_opt)是这个回调的一部分。</p><p id="a5c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">类似地，训练阶段有一个单独回调来触发 ASGD 优化器，且触发条件在回调中，与训练循环分开。<strong class="jp ir"> train </strong>和<strong class="jp ir"> train_search </strong>共享的其他任务如<em class="mp">正规化</em>、<em class="mp">隐藏初始化</em>、<em class="mp">保存</em>、<em class="mp">恢复</em>训练也有各自的回调。</p><p id="d0e8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更多细节，你可以查看我的<a class="ae kl" href="https://github.com/tinhb92/rnn_darts_fastai" rel="noopener ugc nofollow" target="_blank">代码</a>并运行提供的笔记本。</p></div></div>    
</body>
</html>