<html>
<head>
<title>An Introduction to Perceptron Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">感知器算法简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-perceptron-algorithm-40f2ab4e2099?source=collection_archive---------9-----------------------#2019-06-19">https://towardsdatascience.com/an-introduction-to-perceptron-algorithm-40f2ab4e2099?source=collection_archive---------9-----------------------#2019-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d56b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习和深度学习之旅</h2><div class=""/><div class=""><h2 id="e08b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从理论到实践，学习感知器的基本原理，并用随机梯度下降法实现算法</h2></div><p id="8e4f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">本博客将涵盖以下问题和主题</p><p id="2919" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">1.什么是感知器？</p><p id="32b2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">2.感知器的随机梯度下降</p><p id="a2c5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">3.用 Python 实现</p><p id="cd28" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 1。什么是感知器？</strong></p><p id="c4ca" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感知器在 20 世纪 80 年代为神经网络模型奠定了基础。该算法由弗兰克·罗森布拉特开发，并被封装在 1962 年发表的论文“神经动力学原理:感知机和大脑机制理论”中。当时，罗森布拉特的工作受到马文·明克斯和西蒙·派珀特的批评，认为神经网络有缺陷，只能解决线性分离问题。然而，这种限制只出现在单层神经网络中。</p><p id="2078" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">感知器可以用来解决两类分类问题。算法的一般形式可以写成:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/04640dde3f64f36d31953c66cbb57e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/1*O_Ybe1xv6istM--bwnv3ig.gif"/></div></figure><p id="f4a1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">非线性激活<strong class="kt jd">的<em class="lv">标志</em>的</strong>功能是:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/a9908e885b7095a132297b7a56217d50.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/1*4EkHyCMmurM4uO-Qo1dRCQ.gif"/></div></figure><p id="a023" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">而逻辑回归是针对事件发生或不发生的概率，所以目标值的范围是[0，1]。感知器对第一类使用更方便的目标值 t=+1，对第二类使用 t=-1。因此，该算法不提供概率输出，也不处理 K&gt;2 分类问题。另一个限制来自于该算法只能处理固定基函数的线性组合的事实。</p><p id="e009" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 2。感知器的随机梯度下降</strong></p><p id="7054" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">根据前面的两个公式，如果记录分类正确，则:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/e0213b91a7bd541dbf2019da3107f4c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*y7BwAt4Kup0_ICfUzQ9JoA.png"/></div></figure><p id="83f3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">否则，</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/66956ac5530755177f78d365bea16a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*-5gA-HuoYcitJJXDUMOiGg.png"/></div></figure><p id="2bab" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，为了最小化感知器的成本函数，我们可以写出:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/6356b27156e29152c4163df51becb5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*SZo0ba9FcE-pX-m95vXy3w.png"/></div></figure><p id="3a11" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">m 表示错误分类记录的集合。</p><p id="7532" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">通过偏导数，我们可以得到成本函数的梯度:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi lz"><img src="../Images/8860f9572ddda5d4731f8229b61f5170.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*JIp6qxN16FmVbAnI-raoIg.png"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/e50d63938fe515466780935716efc3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*eDc_-W-uZ9CyKLbbM1Q60w.png"/></div></figure><p id="4ae3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">与逻辑回归可以应用批量梯度下降、小批量梯度下降和随机梯度下降来计算参数不同，感知器只能使用随机梯度下降。我们需要初始化参数<strong class="kt jd"> <em class="lv"> w </em> </strong>和<strong class="kt jd"> <em class="lv"> b </em> </strong>，然后随机选择一条错误分类的记录，使用随机梯度下降迭代更新参数<strong class="kt jd"> <em class="lv"> w </em> </strong>和<strong class="kt jd"> <em class="lv"> b </em> </strong>，直到所有记录都被正确分类:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/5e970a512ba9512aa73953fbe3cc4bc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*cAaocjs9s0jfbGJNNo91LQ.png"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mc"><img src="../Images/cd23deb6ccd7f1b8131a946811eef286.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*5yBMQKDv-IoB8b2LGM64MA.png"/></div></div></figure><p id="9d1b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">注意学习率<strong class="kt jd"> <em class="lv"> a </em> </strong>的范围是从 0 到 1。</p><p id="b6de" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">例如，我们有 3 条记录，<strong class="kt jd"> <em class="lv">、Y1 = (3，3)、Y2 = (4，3)、Y3 = (1，1) </em>、T3。<strong class="kt jd"> <em class="lv"> Y1 </em> </strong>和<strong class="kt jd"> <em class="lv"> Y2 </em> </strong>标注为+1<strong class="kt jd"><em class="lv">Y3</em></strong>标注为-1。假设初始参数都是 0。因此，所有的点都将被归类为 1 类。</strong></p><p id="3dee" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随机梯度下降循环通过所有训练数据。在第一轮中，通过应用前两个公式，<strong class="kt jd"> <em class="lv"> Y1 </em> </strong>和<strong class="kt jd"> <em class="lv"> Y2 </em> </strong>可以被正确分类。但是<strong class="kt jd"> <em class="lv"> Y3 </em> </strong>会被误分类。假设学习率等于 1，通过应用上面所示的梯度下降，我们可以得到:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f229ed77d77eefe7a1db421b63ed98cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/1*BRe46WLZhWn0n90HzcJfYQ.gif"/></div></figure><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/c37702217b33ff61a2d5d4a76778ffe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/1*oUojPXlvrKGgjC_nOUqiMg.gif"/></div></figure><p id="cf13" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">那么线性分类器可以写成:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/c39c2dc5d5b2a70c44ed0f700fcc8f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*Zqm2jIZd1wRK0A8LaCVu9g.png"/></div></figure><p id="9d5a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">即 1 轮梯度下降迭代。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/3ae0292036530d6f84d2ec45d951f7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*K-pO87oIB0C4_VH-1IsI9w.png"/></div></figure><p id="995a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上表显示了感知器的随机梯度下降的整个过程。如果一条记录被正确分类，那么权重向量<strong class="kt jd"> <em class="lv"> w </em> </strong>和<strong class="kt jd"> <em class="lv"> b </em> </strong>保持不变；否则，我们在<strong class="kt jd"> <em class="lv"> y=1 </em> </strong>时将向量<strong class="kt jd"> <em class="lv"> x </em> </strong>加到当前权重向量上，并且在<strong class="kt jd"> <em class="lv"> y=-1 </em> </strong>时从当前权重向量<strong class="kt jd"> <em class="lv"> w </em> </strong>中减去向量<strong class="kt jd"><em class="lv"/></strong>。请注意，最后 3 列是预测值，错误分类的记录以红色突出显示。如果我们反复执行梯度下降，在第 7 轮中，所有 3 个记录都被正确标记。那么算法就会停止。线性分类器的最终公式为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/19804f3bcd8780f604f9e8e9deee0e00.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/1*14wuQJWf1Cs1DSnP-XdbKg.gif"/></div></figure><p id="33d5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">请注意，这种算法总是存在收敛问题。当数据可分时，有多种解，选择哪种解取决于起始值。当数据不可分时，算法不会收敛。有关详细信息，请参见下面参考资料中的相应段落。</p><p id="a483" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd"> 3。用 Python 实现</strong></p><pre class="lo lp lq lr gt mm mn mo mp aw mq bi"><span id="064f" class="mr ms it mn b gy mt mu l mv mw">from sklearn import datasets<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import matplotlib.lines as mlines</span><span id="4147" class="mr ms it mn b gy mx mu l mv mw">np.random.seed(10)</span><span id="18d7" class="mr ms it mn b gy mx mu l mv mw"># Load data<br/>iris=datasets.load_iris()<br/>X = iris.data[0:99,:2]<br/>y = iris.target[0:99]</span></pre><p id="429e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，加载虹膜数据。</p><pre class="lo lp lq lr gt mm mn mo mp aw mq bi"><span id="a3f9" class="mr ms it mn b gy mt mu l mv mw"># Plot figure<br/>plt.plot(X[:50, 0], X[:50, 1], 'bo', color='blue', label='0')<br/>plt.plot(X[50:99, 0], X[50:99, 1], 'bo', color='orange', label='1')<br/>plt.xlabel('sepal length')<br/>plt.ylabel('sepal width')<br/>plt.legend()</span></pre><p id="6876" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然后可视化数据</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi my"><img src="../Images/a4ff0935a1a41f75e73a732f8846cc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_RP-MX6tQ_Bq6w8SvKsJbw.png"/></div></div></figure><pre class="lo lp lq lr gt mm mn mo mp aw mq bi"><span id="7091" class="mr ms it mn b gy mt mu l mv mw"># Update y into -1 and 1<br/>y=np.array([1 if i==1 else -1 for i in y])</span></pre><p id="2b40" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">将<strong class="kt jd"> <em class="lv"> y=0 </em> </strong>更新为<strong class="kt jd"> <em class="lv"> y=-1 </em> </strong></p><pre class="lo lp lq lr gt mm mn mo mp aw mq bi"><span id="8873" class="mr ms it mn b gy mt mu l mv mw">#################################<br/># Gradient Descent<br/>#################################</span><span id="37ea" class="mr ms it mn b gy mx mu l mv mw"># Initialize parameters<br/>w=np.ones((X.shape[1],1));<br/>b=1;<br/>learning_rate=0.1;<br/>Round=0;<br/>All_Correct=False;</span><span id="14a2" class="mr ms it mn b gy mx mu l mv mw"># Start Gradient Descent<br/>while not All_Correct:<br/>    misclassified_count=0<br/>    for i in range(X.shape[0]):<br/>        XX=X[i,]<br/>        yy=y[i]<br/>        if yy * (np.dot(w.T,XX.T)+b)&lt;0:<br/>            w+=learning_rate * np.dot(XX,yy).reshape(2,1)<br/>            b+=learning_rate * yy</span><span id="58d6" class="mr ms it mn b gy mx mu l mv mw">            misclassified_count +=1</span><span id="d7a0" class="mr ms it mn b gy mx mu l mv mw">    if misclassified_count==0:<br/>        All_Correct=True<br/>    else:<br/>        All_Correct=False<br/>    Round += 1<br/>    print(Round)</span><span id="f899" class="mr ms it mn b gy mx mu l mv mw">print(w)<br/>print(b)</span></pre><p id="eb00" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">应用随机梯度下降后，我们得到<strong class="kt jd"> <em class="lv"> w=(7.9，-10.07) </em> </strong>和<strong class="kt jd"> <em class="lv"> b=-12.39 </em> </strong></p><pre class="lo lp lq lr gt mm mn mo mp aw mq bi"><span id="d75c" class="mr ms it mn b gy mt mu l mv mw">x_points = np.linspace(4,7,10)<br/>y_ = -(w[0]*x_points + b)/w[1]<br/>plt.plot(x_points, y_)</span><span id="aa4d" class="mr ms it mn b gy mx mu l mv mw">plt.plot(X[:50, 0], X[:50, 1], 'bo', color='blue', label='0')<br/>plt.plot(X[50:99, 0], X[50:99, 1], 'bo', color='orange', label='1')<br/>plt.xlabel('sepal length')<br/>plt.ylabel('sepal width')<br/>plt.legend()</span></pre><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mz"><img src="../Images/26d18c8cb9cc1f8bd78cf0fe7e830a59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K6n6x0RCiYllwpCRPObMsQ.png"/></div></div></figure><p id="2f3e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上图显示了感知器的最终结果。我们可以看到，线性分类器(蓝线)可以正确地分类所有训练数据集。在这种情况下，虹膜数据集仅包含 2 维，因此决策边界是一条线。在数据集包含 3 个或更多维度的情况下，决策边界将是超平面。</p><p id="841c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">结论</strong></p><p id="ded6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在这篇博客中，我解释了感知器背后的理论和数学，将该算法与逻辑回归进行了比较，并最终用 Python 实现了该算法。希望看完这篇博客，你能对这个算法有更好的理解。如果您对其他博客感兴趣，请点击以下链接:</p><div class="na nb gp gr nc nd"><a href="https://medium.com/@songyangdetang_41589/table-of-contents-689c8af0c731" rel="noopener follow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd jd gy z fp ni fr fs nj fu fw jc bi translated">机器学习和深度学习之旅</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">这一系列博客将从理论和实现两个方面对深度学习进行介绍。</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">medium.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr lt nd"/></div></div></a></div><p id="1895" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><strong class="kt jd">参考</strong></p><p id="8e1c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[1] Christopher M. Bishop，(2009)，<em class="lv">模式识别与机器学习</em></p><p id="43e2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[2]特雷弗·哈斯蒂，罗伯特·蒂布拉尼，杰罗姆·弗里德曼，(2008)，<em class="lv">《统计学习的要素》</em></p></div></div>    
</body>
</html>