<html>
<head>
<title>Linear Regression from Scratch with NumPy — Implementation (Finally!)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 NumPy 从头开始线性回归—实现(最终！)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-from-scratch-with-numpy-implementation-finally-8e617d8e274c?source=collection_archive---------3-----------------------#2019-08-01">https://towardsdatascience.com/linear-regression-from-scratch-with-numpy-implementation-finally-8e617d8e274c?source=collection_archive---------3-----------------------#2019-08-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/5d8c782f7bc68848418e04201da584e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bq1VY4jlv32_L62g"/></div></div></figure><p id="cad2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">欢迎使用 NumPy 系列从零开始学习<em class="kz">线性回归的第二部分！在解释了线性回归背后的直觉之后，现在是时候深入研究实现线性回归的代码了。如果你想了解线性回归的直觉，你可以从<a class="ae la" href="https://medium.com/@leventbas92/linear-regression-from-scratch-with-numpy-5485abc9f2e4" rel="noopener">这里</a>阅读本系列的前一部分。现在，让我们把手弄脏吧！</em></p><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><p id="f069" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，我们首先导入必要的库来帮助我们。正如我之前提到的，我们不会使用任何给我们已经实现的算法模型的包，比如<code class="fe lh li lj lk b">sklearn.linear_model</code>，因为它不会帮助我们掌握实现算法的基本原理，因为它是一个开箱即用的(因此是现成的)解决方案。我们想用强硬的方式，而不是简单的方式。</p><p id="692b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">此外，请注意，我们可以使用<code class="fe lh li lj lk b">sklearn</code>包(或其他包)来利用它的有用功能，例如加载数据集，只要我们不使用它已经实现的算法模型。</p><p id="9662" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将使用:</p><ul class=""><li id="99f0" class="ll lm it kd b ke kf ki kj km ln kq lo ku lp ky lq lr ls lt bi translated"><code class="fe lh li lj lk b">numpy</code>(显然)在数据集上进行所有的矢量化数值计算，包括算法的实现，</li><li id="794f" class="ll lm it kd b ke lu ki lv km lw kq lx ku ly ky lq lr ls lt bi translated"><code class="fe lh li lj lk b">matplotlib</code>借助一些视觉辅助工具绘制图表，以便更好地理解手头的问题，</li><li id="c4c3" class="ll lm it kd b ke lu ki lv km lw kq lx ku ly ky lq lr ls lt bi translated"><code class="fe lh li lj lk b">sklearn.datasets</code>加载一个玩具数据集来玩我们写的代码。</li></ul><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><pre class="lb lc ld le gt lz lk ma mb aw mc bi"><span id="00c9" class="md me it lk b gy mf mg l mh mi">Total samples in our dataset is: 506</span></pre><p id="ea9a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">现在，是时候加载我们将在本文中使用的数据集了。<code class="fe lh li lj lk b">sklearn.datasets</code>包提供了一些玩具数据集来说明一些算法的行为，我们将使用<code class="fe lh li lj lk b">load_boston()</code>函数返回一个回归数据集。这里，<code class="fe lh li lj lk b">dataset.data</code>代表特征样本，<code class="fe lh li lj lk b">dataset.target</code>返回目标值，也称为<strong class="kd iu">标签</strong>。</p><p id="4aee" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">值得注意的是，当我们加载目标值时，我们向数据添加了一个新的维度(<code class="fe lh li lj lk b">dataset.target[:,np.newaxis]</code>)，这样我们就可以将数据用作列向量。记住，线性代数区分了行向量和列向量。</p><p id="a85f" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，在<strong class="kd iu"> NumPy </strong>中，只有 n 维数组，本质上没有行列向量的概念。我们可以使用形状为<code class="fe lh li lj lk b">(n, 1)</code>的数组来模拟列向量，使用<code class="fe lh li lj lk b">(1, n)</code>来模拟行向量。因此，我们可以通过显式添加一个轴，将形状<code class="fe lh li lj lk b">(n, )</code>的目标值用作形状<code class="fe lh li lj lk b">(n, 1)</code>的列向量。幸运的是，我们可以用<strong class="kd iu"> NumPy </strong>自己的<code class="fe lh li lj lk b">newaxis</code>函数来做到这一点，该函数用于在使用一次后将数组的维度增加一个维度。</p><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><p id="c643" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们已经选择了(<em class="kz"> 1/2) x 均方误差(MSE) </em>作为我们的成本函数，所以我们将在这里实现它。<code class="fe lh li lj lk b">h</code>表示我们的假设函数，它只是我们从输入(<code class="fe lh li lj lk b">X</code>)到输出(<code class="fe lh li lj lk b">y</code>)的映射的候选函数。</p><p id="237c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当我们用参数(<code class="fe lh li lj lk b">X @ params</code>)取我们的特征的内积时，我们明确地声明我们将从其他机器学习算法的广泛列表中为我们的假设使用线性回归，也就是说，我们已经决定特征和目标值之间的关系最好由线性回归来描述。</p><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><p id="30bd" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们现在可以实现梯度下降算法。这里，<code class="fe lh li lj lk b">n_iters</code>表示梯度下降的迭代次数。我们希望保存每次迭代中由成本函数返回的成本历史，因此我们使用了一个<strong class="kd iu"> NumPy </strong>数组<code class="fe lh li lj lk b">J_history</code>来实现这个目的。</p><p id="2054" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">关于更新规则，<code class="fe lh li lj lk b">1/n_samples) * X.T @ (X @ params - y)</code>对应于成本函数相对于参数的偏导数。因此，<code class="fe lh li lj lk b">params</code>根据更新规则保存更新的参数值。</p><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><p id="ea27" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在我们对数据集运行梯度下降算法之前，我们对数据进行标准化。标准化是一种技术，通常作为机器学习管道中数据准备的一部分，通常意味着将值重新调整到<code class="fe lh li lj lk b">[0,1]</code>的范围内，以提高我们的准确性，同时降低成本(误差)。另外，请注意，我们将参数(<code class="fe lh li lj lk b">params</code>)初始化为零。</p><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><pre class="lb lc ld le gt lz lk ma mb aw mc bi"><span id="bd46" class="md me it lk b gy mf mg l mh mi">Initial cost is:  296.0734584980237 </span><span id="0f66" class="md me it lk b gy mj mg l mh mi">Optimal parameters are: <br/> [[22.53279993]<br/> [-0.83980839]<br/> [ 0.92612237]<br/> [-0.17541988]<br/> [ 0.72676226]<br/> [-1.82369448]<br/> [ 2.78447498]<br/> [-0.05650494]<br/> [-2.96695543]<br/> [ 1.80785186]<br/> [-1.1802415 ]<br/> [-1.99990382]<br/> [ 0.85595908]<br/> [-3.69524414]] </span><span id="5d2e" class="md me it lk b gy mj mg l mh mi">Final cost is:  [11.00713381]</span></pre><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/b47d2bfb7902319e846a0bc974d7fa31.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*kMXmkkGonuNJvj2Ks0dF9A.png"/></div></figure><p id="e87c" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你有它！我们已经成功地运行了该算法，因为我们可以清楚地看到，成本从 296 急剧下降到 11。<code class="fe lh li lj lk b">gradient_descent</code>函数返回最佳参数值，因此，我们现在可以使用它们来预测新的目标值。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="7826" class="ms me it bd mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no bi translated">线性回归的类实现</h1><p id="9e85" class="pw-post-body-paragraph kb kc it kd b ke np kg kh ki nq kk kl km nr ko kp kq ns ks kt ku nt kw kx ky im bi translated">最后，从零开始实现线性回归后，我们可以重新安排我们到目前为止编写的代码，添加更多的代码，进行一些修改，并将其转换为一个类实现，这样我们就有了自己的线性回归模块！给你:</p><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><p id="3002" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意我们的实现和<code class="fe lh li lj lk b">sklearn</code>自己的线性回归实现之间的相似之处。</p><p id="ddd2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">当然，我是故意这样做的，向您展示我们可以编写一个广泛使用的模块的简化版本，其工作方式类似于<code class="fe lh li lj lk b">sklearn</code>的实现。</p><p id="d12e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不过，我这么做主要是为了好玩！</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="4229" class="ms me it bd mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni nj nk nl nm nn no bi translated">将我们的实现与 Sklearn 的线性回归进行比较</h1><figure class="lb lc ld le gt ju"><div class="bz fp l di"><div class="lf lg l"/></div></figure><figure class="lb lc ld le gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/d8d72427e78150cc4b96199db2b6dd44.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*LwJVIiDwM851XB_bEEib8Q.png"/></div></figure><p id="d44e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们在实施方面做得很好，不是吗？我们的训练精度几乎和<code class="fe lh li lj lk b">sklearn</code>的精度一样。此外，与<code class="fe lh li lj lk b">sklearn</code>的测试精度相比，测试精度并不差。</p><p id="a4e3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">毫无疑问，这很有趣。我鼓励你在读完这篇文章后自己编写代码。</p><p id="d701" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你也可以查看我的<a class="ae la" href="https://github.com/leventbass/linear_regression" rel="noopener ugc nofollow" target="_blank"> GitHub 简介</a>，沿着<em class="kz"> jupyter 笔记本</em>阅读代码，或者简单地使用代码来实现。</p><p id="6345" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">将来我会带着更多的实现和博客文章回来。</p><p id="3921" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">编码快乐！</p><h2 id="8cad" class="md me it bd mt nv nw dn mx nx ny dp nb km nz oa nf kq ob oc nj ku od oe nn of bi translated">有问题吗？评论？通过<a class="ae la" href="http://leventbas92@gmail.com" rel="noopener ugc nofollow" target="_blank">leventbas92@gmail.com</a>和<a class="ae la" href="https://github.com/leventbass" rel="noopener ugc nofollow" target="_blank"> GitHub </a>联系我。</h2></div></div>    
</body>
</html>