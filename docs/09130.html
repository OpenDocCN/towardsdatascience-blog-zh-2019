<html>
<head>
<title>Gradient Descent Training With Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用逻辑回归的梯度下降训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gradient-descent-training-with-logistic-regression-c5516f5344f7?source=collection_archive---------3-----------------------#2019-12-04">https://towardsdatascience.com/gradient-descent-training-with-logistic-regression-c5516f5344f7?source=collection_archive---------3-----------------------#2019-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/29b99f5132ab1b006330efd299e8c7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aab2JuauXrpNSi9zI9GoxA.jpeg"/></div></div></figure><h1 id="a195" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">梯度下降</h1><p id="f21e" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">梯度下降算法及其变体(Adam、SGD 等。)已经成为许多机器学习应用中非常流行的训练(优化)算法。优化算法可以非正式地分为两类——基于梯度的和无梯度的(例如粒子群、遗传算法等。).你可以猜到，梯度下降是一种基于梯度的算法。为什么梯度在训练机器学习中很重要？</p><p id="1842" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">训练机器学习模型的目的是通过改变可训练参数来最小化基础事实和预测之间的损失或误差。梯度是导数在多维空间中的延伸，它告诉我们损失或误差最佳最小化的方向。如果你还记得向量微积分课上，梯度被定义为最大变化率。因此，梯度下降的公式很简单:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/d4f8d97ea96e3cd83807f56887873049.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*SzinwRriPA6v_USiftpLsQ.png"/></div></figure><p id="80a0" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">θj 是可训练参数，j. α是学习率。J(θ)是一个成本函数。</p><p id="cb88" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">在下图中，从起点(峰)到最佳点(谷)的最短距离是沿着渐变轨迹。相同的原理适用于多维空间，这通常是机器学习训练的情况。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mh"><img src="../Images/bcb86b20325382ecf5efa51a25384819.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E9RL93pdfzEjMfvw.png"/></div></div></figure><p id="dddb" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">为了演示梯度下降如何应用于机器学习训练，我们将使用逻辑回归。</p><h1 id="5edd" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">逻辑回归</h1><h2 id="be9c" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">二元情况</h2><p id="0748" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为了理解 LR 的工作原理，让我们想象以下场景:我们想根据年龄(x1)、年收入(x2)和教育程度(x3)来预测一个人的性别(男性= 0，女性= 1)。如果 Y 是预测值，则此问题的逻辑回归模型将采用以下形式:</p><p id="e729" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb iu"><em class="mu">Z = B0+B1(x1)+B2(x2)+B3(x3)</em>T3】</strong></p><p id="6b03" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb iu"> <em class="mu"> Y = 1.0 / (1.0 + e^-Z) </em> </strong></p><p id="1743" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">b0 通常称为“偏差”，b1、b2 和 b3 称为“权重”。</p><p id="d95c" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">z 具有与线性回归相同的形式，而 Y 是 sigmoid 激活函数。y 取 0 到 1 之间的值。如果 Y 小于 0.5，我们推断预测输出为 0，如果 Y 大于 0.5，我们推断输出为 1。</p><p id="42c0" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">现在，我们准备看看下面 LR 的更正式的形式:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/c6745e12ec89695656aa39dd073d4306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R42KgECSLeGbZq6K510ykQ.png"/></div></div></figure><p id="1e9a" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">φn 是 Xn 在特征空间中的增广变换。tn 是类标签。σ是一个 s 形激活。w 是权重向量(包括偏差项)。p(C1 |φ)和 p(C2 |φ)分别是给定φ时分配给 C1 和 C2 的概率。</p><p id="bc0d" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">给定上述公式，这里的主要目标是在给定权重(W)的情况下最大化观察数据的可能性。似然函数是观测数据的联合分布，如下所示:</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/d67d353cdb32fbcb33ce92e4d62fdede.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mXdkf_NnQ4IMKkBwgSW01A.png"/></div></div></figure><p id="562e" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">π是乘积算子。</p><p id="bef4" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">从似然函数可以看出，y 是伯努利分布。</p><p id="52a3" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">当处理概率时，最好转换成对数，因为对数将乘积转换成总和，从而避免了取非常小的数的乘积的问题(通常用于概率)。下面是负对数似然(NLL)及其相对于权重的梯度。NLL 用于将最大化问题转化为最小化问题。本质上，最小化 NLL 相当于最大化可能性。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/edd3b36c4f43b6bfd667a395fb483cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HeglJABZa1W4xPN8lQn2Rw.png"/></div></div></figure><h2 id="ae48" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">多类案件</h2><p id="e01e" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">LR 的二进制情况可以通过改变符号扩展到多类情况。</p><p id="23e6" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">假设有 K 类。因此，p(Ck)是在给定φ的情况下分配给类 k 的概率。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div class="gh gi my"><img src="../Images/5b9e515b5281ead1a418d0573e61f7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*H3NvM4FWXdRUWzlHPZ9p8A.png"/></div></figure><p id="9e8d" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">代替 sigmoid 激活，softmax 激活用于将类分数(ak)转换成适当的概率。</p><p id="4195" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">w 是权重矩阵(DxK) — D 是特征空间维度。</p><p id="95c7" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">似然函数和负似然(NLL)如下所示。</p><figure class="md me mf mg gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/8f074aab3b96dcfbf38bf321095694c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LF7uAVHBWrqkZbf2yeg8bw.png"/></div></div></figure><p id="a9b7" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">y 现在是多元分布的。</p><h1 id="bf81" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">MNIST 分类</h1><p id="c307" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><a class="ae na" href="http://deeplearning.net/data/mnist/" rel="noopener ugc nofollow" target="_blank"> MNIST </a>是一个经典数据集，由手绘数字(0 到 9 之间)的黑白图像组成。我们将使用 PyTorch 实现多类逻辑回归来对 MNIST 的数字进行分类。因为我们想要演示梯度下降算法，所以我们不使用 torch.optim 中的内置算法。为了简单起见，我们将使用 torch.autograd，而不是手动计算梯度。这个演示来自 PyTorch 网站。</p><h2 id="0729" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">下载数据集</h2><p id="cf94" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">以下代码下载数据集(训练集和验证集)并转换成相应的 numpy 数组。</p><pre class="md me mf mg gt nb nc nd ne aw nf bi"><span id="eae3" class="mi kc it nc b gy ng nh l ni nj">from pathlib import Path<br/>import requests<br/>import pickle<br/>import gzip</span><span id="ff4a" class="mi kc it nc b gy nk nh l ni nj">DATA_PATH = Path("data")<br/>PATH = DATA_PATH / "mnist"<br/>PATH.mkdir(parents=True, exist_ok=True)<br/>URL = "http://deeplearning.net/data/mnist/"</span><span id="792a" class="mi kc it nc b gy nk nh l ni nj">FILENAME = "mnist.pkl.gz"<br/>if not (PATH / FILENAME).exists():<br/>  content = requests.get(URL + FILENAME).content<br/>  (PATH / FILENAME).open("wb").write(content)</span><span id="fab3" class="mi kc it nc b gy nk nh l ni nj">with gzip.open((PATH / FILENAME).as_posix(), "rb") as f:<br/>  ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f,   encoding="latin-1")</span></pre><h2 id="6430" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">转换为张量</h2><p id="cfb3" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在使用 PyTorch 时，我们需要将上面的 numpy 数组转换成张量。</p><pre class="md me mf mg gt nb nc nd ne aw nf bi"><span id="1faf" class="mi kc it nc b gy ng nh l ni nj">import torch<br/>x_train, y_train, x_valid, y_valid = map(torch.tensor, (x_train,  y_train, x_valid, y_valid))</span></pre><h2 id="0cd3" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">初始化权重和偏差</h2><p id="864a" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">接下来，我们将创建和初始化权重和偏差张量。我们使用<a class="ae na" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank"> Xavier 初始化</a>用于权重，同时用零值初始化偏置。因为我们希望 torch.autograd 负责梯度计算，所以我们需要将 requires_grad 设置为 True，以便 PyTorch 可以跟踪梯度计算所需的操作。</p><pre class="md me mf mg gt nb nc nd ne aw nf bi"><span id="266c" class="mi kc it nc b gy ng nh l ni nj">import math<br/>weights = torch.randn(784, 10) / math.sqrt(784)<br/>weights.requires_grad_()<br/>bias = torch.zeros(10, requires_grad=True)</span></pre><h2 id="05cb" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">似然函数</h2><p id="a701" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们需要评估的可能性是 yk(φ)。要得到 yk(φ)，我们首先需要对 ak 求值。它不返回 yk(φ)，而是返回 log(yk(φ))，这对以后计算损失函数很有用。</p><pre class="md me mf mg gt nb nc nd ne aw nf bi"><span id="4868" class="mi kc it nc b gy ng nh l ni nj"><strong class="nc iu">def</strong> <strong class="nc iu">log_softmax</strong>(x):<br/>    <strong class="nc iu">return</strong> x <strong class="nc iu">-</strong> x<strong class="nc iu">.</strong>exp()<strong class="nc iu">.</strong>sum(<strong class="nc iu">-</strong>1)<strong class="nc iu">.</strong>log()<strong class="nc iu">.</strong>unsqueeze(<strong class="nc iu">-</strong>1)<br/><br/><strong class="nc iu">def</strong> <strong class="nc iu">model</strong>(xb):<br/>    <strong class="nc iu">return</strong> log_softmax(xb <strong class="nc iu">@</strong> weights <strong class="nc iu">+</strong> bias)</span></pre><h2 id="3efd" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">损失函数—负对数似然(NLL)</h2><p id="6509" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">现在，我们可以使用似然来计算总体负对数似然，这是 MNIST 逻辑回归的损失函数。</p><pre class="md me mf mg gt nb nc nd ne aw nf bi"><span id="0721" class="mi kc it nc b gy ng nh l ni nj"><strong class="nc iu">def</strong> <strong class="nc iu">nll</strong>(input, target):<br/>    <strong class="nc iu">return</strong> <strong class="nc iu">-</strong>input[range(target<strong class="nc iu">.</strong>shape[0]), target]<strong class="nc iu">.</strong>mean()</span></pre><h2 id="c46c" class="mi kc it bd kd mj mk dn kh ml mm dp kl lk mn mo kp lo mp mq kt ls mr ms kx mt bi translated">训练循环</h2><p id="9764" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">到目前为止，我们已经实现了训练 MNIST 逻辑回归所需的所有必要功能。我们将实施小批量培训。</p><pre class="md me mf mg gt nb nc nd ne aw nf bi"><span id="ea59" class="mi kc it nc b gy ng nh l ni nj">bs <strong class="nc iu">=</strong> 64  <em class="mu"># batch size<br/></em>loss_func <strong class="nc iu">=</strong> nll<br/>lr <strong class="nc iu">=</strong> 0.5  <em class="mu"># learning rate</em><br/>epochs <strong class="nc iu">=</strong> 2  <em class="mu"># how many epochs to train for</em><br/><br/><strong class="nc iu">for</strong> epoch <strong class="nc iu">in</strong> range(epochs):<br/>    <strong class="nc iu">for</strong> i <strong class="nc iu">in</strong> range((n <strong class="nc iu">-</strong> 1) <strong class="nc iu">//</strong> bs <strong class="nc iu">+</strong> 1):<br/>        start_i <strong class="nc iu">=</strong> i <strong class="nc iu">*</strong> bs<br/>        end_i <strong class="nc iu">=</strong> start_i <strong class="nc iu">+</strong> bs<br/>        xb <strong class="nc iu">=</strong> x_train[start_i:end_i]<br/>        yb <strong class="nc iu">=</strong> y_train[start_i:end_i]<br/>        pred <strong class="nc iu">=</strong> model(xb)<br/>        loss <strong class="nc iu">=</strong> loss_func(pred, yb)<br/><br/>        loss<strong class="nc iu">.</strong>backward()<br/>        <strong class="nc iu">with</strong> torch<strong class="nc iu">.</strong>no_grad():<br/>            weights <strong class="nc iu">-=</strong> weights<strong class="nc iu">.</strong>grad <strong class="nc iu">*</strong> lr<br/>            bias <strong class="nc iu">-=</strong> bias<strong class="nc iu">.</strong>grad <strong class="nc iu">*</strong> lr<br/>            weights<strong class="nc iu">.</strong>grad<strong class="nc iu">.</strong>zero_()<br/>            bias<strong class="nc iu">.</strong>grad<strong class="nc iu">.</strong>zero_()</span></pre><p id="cb97" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">。loss_func 上的 backward()执行参数更新所需的所有梯度计算。一旦计算出梯度。backward()，权重和偏差由梯度和学习率的乘积更新。学习率(LR)用于控制收敛。大 LR 会过冲，而小 LR 会减慢收敛。</p><p id="692a" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">一旦权重和偏差被更新，它们的梯度被设置为零；否则，梯度将在下一批中累积。</p><h1 id="14d3" class="kb kc it bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">总结</h1><p id="508f" class="pw-post-body-paragraph kz la it lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">上面实现的梯度下降非常基本，但足以演示它是如何工作的。PyTorch 和 TensorFlow 等现代机器学习框架有更复杂的梯度下降变体，如 SGD、Adam 等。尽管如此，当我们需要训练机器学习模型时，理解梯度下降如何工作是有益的。</p><p id="2043" class="pw-post-body-paragraph kz la it lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">本文中的等式摘自 Christopher M. Bishop 的《模式识别和机器学习》。</p></div></div>    
</body>
</html>