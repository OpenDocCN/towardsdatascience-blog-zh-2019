<html>
<head>
<title>Machine Learning for Beginners: An Introduction to Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者的机器学习:神经网络导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9?source=collection_archive---------1-----------------------#2019-03-05">https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9?source=collection_archive---------1-----------------------#2019-03-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ac8e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简单解释它们如何工作，以及如何用 Python 从头实现一个。</h2></div><p id="c8cb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有些东西可能会让你吃惊:<strong class="kk iu">神经网络并没有那么复杂！</strong>术语“神经网络”经常被用作时髦词汇，但实际上它们往往比人们想象的要简单得多。</p><p id="d151" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这篇文章是为完全的初学者而写的，假设你没有机器学习的知识</strong>。我们将理解神经网络是如何工作的，同时用 Python 从头实现一个神经网络。</p><p id="9a75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们开始吧！</p><blockquote class="le lf lg"><p id="d429" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">注意:我推荐阅读 victorzhou.com<a class="ae ll" href="https://victorzhou.com/blog/intro-to-neural-networks/" rel="noopener ugc nofollow" target="_blank">的这篇文章——这篇文章的大部分格式在那里看起来更好。</a></p></blockquote><h1 id="74d0" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">1.构建模块:神经元</h1><p id="76ef" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">首先，我们必须谈谈神经元，它是神经网络的基本单位。一个神经元接受输入，用它们做一些数学运算，然后产生一个输出。这是一个双输入神经元的样子:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/8ce26f5ab8ebd6f5a6e7df2485e623ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*JRRC_UDsW1kDgPK3MW1GjQ.png"/></div></figure><p id="22f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里发生了三件事。首先，每个输入乘以一个权重:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mr"><img src="../Images/de6a27ce42e7fa745ecd8bde67f9143d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Iq76QGqSTJfYRztFhwK0yw.png"/></div></div></figure><p id="3191" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，所有加权输入与偏差 b 相加:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mw"><img src="../Images/c39d81a3ce8a9c3ae4327d763104cec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CE-YfWhFQ2yQSGq9Zaxd9Q.png"/></div></div></figure><p id="cbe7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，总和通过一个激活函数传递:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mx"><img src="../Images/9fc1f180189938b9b6ef50eba7cf2704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9BFMXPkoAqN_EW7XTPvuGg.png"/></div></div></figure><p id="b21b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">激活函数用于将一个无界的输入转换成一个具有良好的、可预测的形式的输出。一个常用的激活功能是<a class="ae ll" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>功能:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi my"><img src="../Images/e532ea75bf6f53a6ed1b3e585c102af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ul8Yu_r8GKSFillzbPFrPQ.png"/></div></div></figure><p id="d3d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sigmoid 函数只输出(0，1)范围内的数字。你可以把它想成是把(∞，+∞)压缩到(0，1)——大负数变成~0，大正数变成~1。</p><h1 id="a850" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">简单的例子</h1><blockquote class="le lf lg"><p id="0655" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">提醒:这篇文章的大部分格式在 victorzhou.com<a class="ae ll" href="https://victorzhou.com/blog/intro-to-neural-networks/" rel="noopener ugc nofollow" target="_blank">上的原帖中看起来更好。</a></p></blockquote><p id="c2d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有一个 2 输入神经元，它使用 sigmoid 激活函数并具有以下参数:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mz"><img src="../Images/b45f810cc61e17ea98ca284a1490f27b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpb0aE2eu2xsV1X9Sp17UQ.png"/></div></div></figure><p id="db5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh"> w </em> =[0，1]只是向量形式的<em class="lh"> w </em> 1 =0，<em class="lh"> w </em> 2 =1 的一种写法。现在，让我们给神经元一个<em class="lh">x</em>=【2，3】的输入。我们将使用<a class="ae ll" href="https://simple.wikipedia.org/wiki/Dot_product" rel="noopener ugc nofollow" target="_blank">点积</a>来更简洁地写东西:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi na"><img src="../Images/4ad9c19c22bbebd3e0a6c1c1f4e2e9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pOMfnp8FI3KTcpuNwBDPSA.png"/></div></div></figure><p id="655c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定输入<em class="lh">x</em>=【2，3】，神经元输出 0.999。就是这样！这个向前传递输入以获得输出的过程被称为<strong class="kk iu">前馈</strong>。</p><h1 id="9140" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">编码一个神经元</h1><p id="731d" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">是时候实现一个神经元了！我们将使用<a class="ae ll" href="http://www.numpy.org/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>，一个流行且强大的 Python 计算库，来帮助我们做数学:</p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="a559" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">认得这些号码吗？这就是我们刚才做的例子！我们得到同样的答案 0.999。</p><h1 id="bc95" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">2.将神经元组合成神经网络</h1><p id="f852" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">神经网络只不过是一堆连接在一起的神经元。下面是一个简单的神经网络可能的样子:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/aa00326b380b6ea7e4e88997edfb6ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*x6KWjKTOBhUYL0MRX4M3oQ.png"/></div></figure><p id="643e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个网络有两个输入，一个隐层有两个神经元(<em class="lh"> h </em> 1 和<em class="lh"> h </em> 2)，一个输出层有一个神经元(<em class="lh"> o </em> 1)。请注意， 1 的<em class="lh">输入是来自<em class="lh"> h </em> 1 和<em class="lh"> h </em> 2 的输出——这就是网络的构成。</em></p><blockquote class="le lf lg"><p id="3f1e" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">隐藏层是输入(第一个)层和输出(最后一个)层之间的任何层。可以有多个隐藏层！</p></blockquote><h1 id="08ad" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">一个例子:前馈</h1><p id="448c" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">让我们使用上图所示的网络，并假设所有神经元都具有相同的权重<em class="lh"> w </em> =[0，1】，相同的偏差<em class="lh"> b </em> =0，以及相同的 sigmoid 激活函数。设<em class="lh"> h </em> 1、<em class="lh"> h </em> 2、<em class="lh"> o </em> 1 表示它们所代表的神经元的<em class="lh">输出</em>。</p><p id="edb1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们传入输入<em class="lh"> x </em> =[2，3]会发生什么？</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ne"><img src="../Images/7f9b33b8c4167e788ea6da2bb0774fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6oNvFWIYvAxH0whBFCxsKw.png"/></div></div></figure><p id="c2ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">输入<em class="lh"> x </em> =[2，3]的神经网络输出为 0.7216。很简单，对吧？</p><p id="3942" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个神经网络可以有<strong class="kk iu">任意数量的层</strong>，在这些层中有<strong class="kk iu">任意数量的神经元</strong>。基本思想保持不变:通过网络中的神经元将输入前馈，最终得到输出。为了简单起见，我们将在本文的剩余部分继续使用上图所示的网络。</p><h1 id="2ad9" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">神经网络编码:前馈</h1><p id="f828" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">让我们为我们的神经网络实现前馈。这里的网络形象再次供参考:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/aa00326b380b6ea7e4e88997edfb6ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*x6KWjKTOBhUYL0MRX4M3oQ.png"/></div></figure><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="nb nc l"/></div></figure><p id="2575" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们又得了 0.7216！看起来很有效。</p><h1 id="3a38" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">3.训练神经网络，第 1 部分</h1><p id="2250" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">假设我们有以下测量值:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nf"><img src="../Images/e9ac3082a316188cde059fce4ff80abd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qU-kcw71_o_Ok3rshQAmOw.png"/></div></div></figure><p id="3656" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们训练我们的网络，根据某人的体重和身高来预测他们的性别:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/bbf0431edd20a6d3d0ca978d278ec260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Z9ZsW5wBSzxHbgPOujGPDA.png"/></div></figure><p id="e70d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将用 0 表示男性，用 1 表示女性，我们还将移动数据以使其更易于使用:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nf"><img src="../Images/48b93a1a75df551d8c5fbef35c3e80af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WivwN9tOfvA9Uz0akikgLA.png"/></div></div></figure><h1 id="5b47" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">失败</h1><p id="196b" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">在我们训练我们的网络之前，我们首先需要一种方法来量化它做得有多“好”，以便它可以尝试做得“更好”。这就是<strong class="kk iu">损失</strong>的原因。</p><p id="c85e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将使用<strong class="kk iu">均方误差</strong> (MSE)损失:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ng"><img src="../Images/90cab9a0eef585c0e65d5fcd5c8e308c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AGjwUIJ62a9He2K919OJug.png"/></div></div></figure><p id="0b62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来分解一下:</p><ul class=""><li id="164c" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated"><em class="lh"> n </em>为样本数，为 4(爱丽丝、鲍勃、查理、戴安娜)。</li><li id="b070" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><em class="lh"> y </em>代表被预测的变量，是性别。</li><li id="41b4" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><em class="lh"> y_true </em>是变量的<em class="lh"> true </em>值(“正确答案”)。例如，爱丽丝的<em class="lh"> y_true </em>将为 1(女性)。</li><li id="c135" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><em class="lh"> y_pred </em>是变量的<em class="lh">预测值</em>。它是我们网络输出的任何东西。</li></ul><p id="abd5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(<em class="lh">y _ true</em>-<em class="lh">y _ pred</em>)被称为<strong class="kk iu">平方误差</strong>。我们的损失函数只是取所有平方误差的平均值(因此名字<em class="lh">表示</em>平方误差)。我们的预测越准确，我们的损失就越低！</p><p id="c048" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更好的预测=更低的损失。</p><p id="4fcc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">训练一个网络=尽量减少它的损失。</strong></p><h1 id="40da" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">损失计算示例</h1><p id="65fb" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">假设我们的网络总是输出 00——换句话说，它确信所有人类都是男性🤔。我们会有什么损失？</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nv"><img src="../Images/078ef14599ba81917d049ea0407394e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*10R8hLYmut9xe3zxIjz1Dg.png"/></div></div></figure><h2 id="57f6" class="nw ln it bd lo nx ny dn ls nz oa dp lw kr ob oc ly kv od oe ma kz of og mc oh bi translated">代码:MSE 损失</h2><p id="ec26" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">下面是一些为我们计算损失的代码:</p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="nb nc l"/></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">If you don’t understand why this code works, read the NumPy <a class="ae ll" href="https://docs.scipy.org/doc/numpy/user/quickstart.html#basic-operations" rel="noopener ugc nofollow" target="_blank">quickstart</a> on array operations.</figcaption></figure><p id="2458" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很好。向前！</p><blockquote class="le lf lg"><p id="1ebc" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">喜欢这个帖子吗？我写了很多初学者友好的 ML 文章。<a class="ae ll" href="https://victorzhou.com/subscribe/?src=intro-to-nn-medium" rel="noopener ugc nofollow" target="_blank">订阅我的简讯</a>让它们进入你的收件箱！</p></blockquote><h1 id="fca3" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">4.训练神经网络，第 2 部分</h1><p id="5c68" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">我们现在有一个明确的目标:<strong class="kk iu">最小化神经网络的损失</strong>。我们知道我们可以改变网络的权重和偏差来影响它的预测，但是我们怎样才能减少损失呢？</p><blockquote class="le lf lg"><p id="b89e" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">本节使用了一点多变量微积分。如果你对微积分感到不舒服，可以跳过数学部分。</p></blockquote><p id="0082" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了简单起见，让我们假设我们的数据集中只有 Alice:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi om"><img src="../Images/f128767ece2689df9b36e163eb816181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*783WcLm4Zlu0snto1NaYbg.png"/></div></div></figure><p id="6eb8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么均方误差损失就是爱丽丝的平方误差:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi on"><img src="../Images/d6c57688230562361f20ff15972e979f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Fn15kWdz4VpPymonQuHGg.png"/></div></div></figure><p id="8e2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种思考损失的方式是作为权重和偏差的函数。让我们给网络中的每个权重和偏差贴上标签:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0356388fb575e99951bdd46042f36752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*JuCFYUaqd7WTX8PKHkfuQw.png"/></div></figure><p id="d354" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们可以将损失写成一个多变量函数:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi oo"><img src="../Images/20c1a3f5435f2a9a2c09654aa3b65c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OHMn7EMtIG77EAmccwgowg.png"/></div></div></figure><p id="66f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们想要调整<em class="lh"> w </em> 1。如果我们改变<em class="lh"> w </em> 1，损失<em class="lh"> L </em>会如何变化？这个问题<a class="ae ll" href="https://simple.wikipedia.org/wiki/Partial_derivative" rel="noopener ugc nofollow" target="_blank">偏导数</a>可以回答。我们怎么算？</p><blockquote class="le lf lg"><p id="485f" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">这就是数学开始变得更加复杂的地方。不要气馁！我建议带上纸和笔——这会帮助你理解。</p><p id="4613" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">如果你读这个有困难:下面的数学格式在 victorzhou.com 的原始帖子里看起来更好。</p></blockquote><p id="4a26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们用∂<em class="lh">y _ pred/</em>∂<em class="lh">w</em>1 来重写偏导数:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mz"><img src="../Images/9607a5b372a4cd3e330bc3c2646e4b06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ojh2mA6NWye18NnTUne24Q.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">This works because of the <a class="ae ll" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">Chain Rule</a>.</figcaption></figure><p id="2670" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以计算∂ <em class="lh"> L/ </em> ∂ <em class="lh"> y_pred </em>，因为我们在上面计算了 l =(1-t34)y _ pred:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi op"><img src="../Images/c824720c3bc2df161c749d74cb7712a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKP2ce3tNUUj-vsKjQsSvw.png"/></div></div></figure><p id="bc71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们弄清楚如何处理∂<em class="lh">y _ pred/</em>∂<em class="lh">w</em>1。就像之前一样，设<em class="lh"> h </em> 1、<em class="lh"> h </em> 2、<em class="lh"> o </em> 1 为它们所代表的神经元的输出。然后</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi mz"><img src="../Images/418c986750f50286c4ad32312fd03d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IAPqA69MXq8_fwQeYcv7bg.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">f is the sigmoid activation function, remember?</figcaption></figure><p id="a87c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于<em class="lh"> w </em> 1 只影响<em class="lh"> h </em> 1(而不是<em class="lh"> h </em> 2)，我们可以写</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi oq"><img src="../Images/5541011eae623f177e2fa3bb2921c37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pFc6605O7Xf0lI8l7cWsEw.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">More Chain Rule.</figcaption></figure><p id="fecc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们为∂做同样的事情:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi om"><img src="../Images/954ae6b38d16b07f706d4bf68759ea76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBYqQUbNNB0pSGfR0pDq2w.png"/></div></div><figcaption class="oi oj gj gh gi ok ol bd b be z dk">You guessed it, Chain Rule.</figcaption></figure><p id="24e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh"> x </em> 1 这里是体重，<em class="lh"> x </em> 2 是身高。这是我们现在第二次看到<em class="lh">f</em>'(<em class="lh">x</em>)(sigmoid 函数的导数)了！我们来推导一下:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi or"><img src="../Images/c4fc0384360cf8fe6c2ed6d27da0e9dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9L8M67L_KGSZguJFo9zVww.png"/></div></div></figure><p id="a22e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">稍后我们将使用这个漂亮的形式来表示<em class="lh">f</em>'(<em class="lh">x</em>)。</p><p id="c2c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们完了！我们已经设法将∂1 分解成我们可以计算的几个部分:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi os"><img src="../Images/645c94f9cbda8e5ea00f8192ceeab3f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnBzd86OgPXHsF7rV0tAcQ.png"/></div></div></figure><p id="6ac9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种通过逆向计算偏导数的系统被称为<strong class="kk iu">反向传播</strong>，或“反向传播”。</p><p id="43e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">唷。这是一大堆符号——如果你还是有点困惑，没关系。让我们做一个例子来看看这是怎么回事！</p><h1 id="c1f1" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">示例:计算偏导数</h1><p id="c4e1" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">我们将继续假设只有 Alice 在我们的数据集中:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi om"><img src="../Images/f128767ece2689df9b36e163eb816181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*783WcLm4Zlu0snto1NaYbg.png"/></div></div></figure><p id="609a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们将所有权重初始化为 1，将所有偏差初始化为 0。如果我们通过网络进行前馈，我们得到:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ot"><img src="../Images/5f8fd1c8907771ad5ee23950cd0dd4d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hrczz7ekQ3D1Hktf3_1evQ.png"/></div></div></figure><p id="8a58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">网络输出<em class="lh"> y_pred </em> =0.524，不强烈偏向男性(0)或女性(1)。我们来计算一下∂ <em class="lh"> L/ </em> ∂ <em class="lh"> w </em> 1:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ou"><img src="../Images/0977bd66008d524ff255ed158dcd52f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DIIlLoIuqDTEbeblLJyMkQ.png"/></div></div></figure><blockquote class="le lf lg"><p id="15bc" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">提醒:我们在前面为我们的乙状结肠激活函数推导了 f<em class="it">'(</em>x<em class="it">)=</em>f<em class="it">(</em>x<em class="it">)∫(1-T35】f<em class="it">(</em>x<em class="it">)</em>。</em></p></blockquote><p id="d321" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们做到了！这告诉我们，如果我们要增加<em class="lh"> w </em> 1，<em class="lh"> L </em>结果会增加一个<em class="lh">t iiny</em>位。</p><h1 id="ad70" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">训练:随机梯度下降</h1><p id="7ded" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">我们现在已经拥有了训练神经网络所需的所有工具！我们将使用一种叫做<a class="ae ll" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a> (SGD)的优化算法，告诉我们如何改变我们的权重和偏差，以最小化损失。基本上就是这个更新方程式:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi ov"><img src="../Images/da81e8b684505eb032112873617ece3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kX2Av8AoG8VX42kXhhFHZw.png"/></div></div></figure><p id="38f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh"> η </em>是一个叫做<strong class="kk iu">学习率</strong>的常数，它控制着我们训练的速度。我们所做的就是从<em class="lh"> w </em> 1 中减去<em class="lh">η</em>∂<em class="lh">w</em>1/∂<em class="lh">l</em>:</p><ul class=""><li id="9653" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated">如果∂ <em class="lh"> L/ </em> ∂ <em class="lh"> w </em> 1 为正，<em class="lh"> w </em> 1 将减少，这使得<em class="lh"> L </em>减少。</li><li id="7a0e" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">如果∂ <em class="lh"> L/ </em> ∂ <em class="lh"> w </em> 1 为负，<em class="lh"> w </em> 1 将增加，这使得<em class="lh"> L </em>减少。</li></ul><p id="44c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们对网络中的每一个权重和偏差都这样做，那么损耗会慢慢减少，我们的网络也会改善。</p><p id="17a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的培训过程将是这样的:</p><ol class=""><li id="2033" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld ow nn no np bi translated">从我们的数据集中选择一个样本。这就是随机梯度下降的原因——我们一次只对一个样本进行操作。</li><li id="6b97" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld ow nn no np bi translated">计算损失相对于权重或偏差的所有偏导数(例如∂ <em class="lh"> L/ </em> ∂ <em class="lh"> w </em> 1，∂ <em class="lh"> L </em> /∂ <em class="lh"> w </em> 2 等)。</li><li id="1960" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld ow nn no np bi translated">使用更新等式来更新每个权重和偏差。</li><li id="8262" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld ow nn no np bi translated">回到步骤 1。</li></ol><p id="30db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看它的实际效果吧！</p><h1 id="4944" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">代码:一个完整的神经网络</h1><p id="4412" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">终于到了<em class="lh">实现完整神经网络的</em>时刻:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div role="button" tabindex="0" class="ms mt di mu bf mv"><div class="gh gi nf"><img src="../Images/48b93a1a75df551d8c5fbef35c3e80af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WivwN9tOfvA9Uz0akikgLA.png"/></div></div></figure><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/0356388fb575e99951bdd46042f36752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*JuCFYUaqd7WTX8PKHkfuQw.png"/></div></figure><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="nb nc l"/></div></figure><blockquote class="le lf lg"><p id="1948" class="ki kj lh kk b kl km ju kn ko kp jx kq li ks kt ku lj kw kx ky lk la lb lc ld im bi translated">你可以<a class="ae ll" href="https://repl.it/@vzhou842/An-Introduction-to-Neural-Networks" rel="noopener ugc nofollow" target="_blank">自己运行/玩这个代码</a>。在<a class="ae ll" href="https://github.com/vzhou842/neural-network-from-scratch" rel="noopener ugc nofollow" target="_blank"> Github </a>上也有。</p></blockquote><p id="c77e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的损失稳步下降，因为网络了解到:</p><figure class="mk ml mm mn gt mo gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/3e3921bcd277a908be3e48fa6b6da46f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/1*meeIavVtb0G0hNF6UvkkGw.png"/></div></figure><p id="0020" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以利用网络来预测性别:</p><figure class="mk ml mm mn gt mo"><div class="bz fp l di"><div class="nb nc l"/></div></figure><h1 id="4847" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">现在怎么办？</h1><p id="d91e" class="pw-post-body-paragraph ki kj it kk b kl me ju kn ko mf jx kq kr mg kt ku kv mh kx ky kz mi lb lc ld im bi translated">你成功了！快速回顾一下我们的工作:</p><ul class=""><li id="62d9" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated">介绍了<strong class="kk iu">神经元</strong>，神经网络的构建模块。</li><li id="eb75" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">在我们的神经元中使用了<strong class="kk iu">乙状窦激活功能</strong>。</li><li id="bde1" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">我发现神经网络只是连接在一起的神经元。</li><li id="e64e" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">创建了一个数据集，将体重和身高作为输入(或<strong class="kk iu">特征</strong>)，将性别作为输出(或<strong class="kk iu">标签</strong>)。</li><li id="e249" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">了解了<strong class="kk iu">损失函数</strong>和<strong class="kk iu">均方误差</strong> (MSE)损失。</li><li id="a23e" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">意识到训练一个网络只是最小化它的损失。</li><li id="7bc5" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">使用<strong class="kk iu">反向传播</strong>计算偏导数。</li><li id="c073" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">使用<strong class="kk iu">随机梯度下降</strong> (SGD)来训练我们的网络。</li></ul><p id="29ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">还有很多事情要做:</p><ul class=""><li id="a774" class="nh ni it kk b kl km ko kp kr nj kv nk kz nl ld nm nn no np bi translated">使用适当的机器学习库，如<a class="ae ll" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>、<a class="ae ll" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae ll" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>，尝试更大/更好的神经网络。</li><li id="fbe8" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated"><a class="ae ll" href="https://victorzhou.com/blog/keras-neural-network-tutorial/" rel="noopener ugc nofollow" target="_blank">用 Keras </a>建立你的第一个神经网络。</li><li id="b4a1" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">在你的浏览器中修补神经网络。</li><li id="28ee" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">发现除了 sigmoid 之外的其他激活功能，如<a class="ae ll" href="https://victorzhou.com/blog/softmax/" rel="noopener ugc nofollow" target="_blank"> Softmax </a>。</li><li id="f275" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">发现除了 SGD 之外的其他优化器。</li><li id="45bf" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">看我的<a class="ae ll" href="https://victorzhou.com/blog/intro-to-cnns-part-1/" rel="noopener ugc nofollow" target="_blank">卷积神经网络介绍</a>(CNN)。CNN 彻底改变了<a class="ae ll" href="https://victorzhou.com/tag/computer-vision/" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>的领域，并且非常强大。</li><li id="d0c7" class="nh ni it kk b kl nq ko nr kr ns kv nt kz nu ld nm nn no np bi translated">看我的<a class="ae ll" href="https://victorzhou.com/blog/intro-to-rnns/" rel="noopener ugc nofollow" target="_blank">介绍递归神经网络</a> (RNNs)，经常用于<a class="ae ll" href="https://victorzhou.com/tag/natural-language-processing/" rel="noopener ugc nofollow" target="_blank">自然语言处理</a> (NLP)。</li></ul><p id="90dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我将来可能会写这些话题或类似的话题，所以如果你想得到新帖子的通知，请订阅<a class="ae ll" href="https://victorzhou.com/subscribe/?src=intro-to-nn-medium" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="c2c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">感谢阅读！</p></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="ace1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lh">原贴于</em><a class="ae ll" href="https://victorzhou.com/blog/intro-to-neural-networks/" rel="noopener ugc nofollow" target="_blank"><em class="lh">victorzhou.com</em></a><em class="lh">。</em></p></div></div>    
</body>
</html>