<html>
<head>
<title>Exploring wild west of natural language generation — from n-gram and RNNs to Seq2Seq</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索自然语言生成的西部——从 n-gram 和 RNNs 到 Seq2Seq</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-wild-west-of-natural-language-generation-from-n-gram-and-rnns-to-seq2seq-2e816edd89c6?source=collection_archive---------12-----------------------#2019-09-20">https://towardsdatascience.com/exploring-wild-west-of-natural-language-generation-from-n-gram-and-rnns-to-seq2seq-2e816edd89c6?source=collection_archive---------12-----------------------#2019-09-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/891cad0d92fd015b6b188a8814586992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*crtxywGS5TQMHPow"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">NLP Wild West</figcaption></figure><div class=""/><div class=""><h2 id="7341" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">斯坦福大学 NLP 课程中讲授的自然语言模型介绍</h2></div><p id="05de" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最初，文本生成是基于模板或基于规则的系统，对于特定的狭义应用程序来说，它们具有相当好的可解释性和良好的行为。但是扩展这样的系统需要大量不可行的手工工作。简单地说，语言没有清晰明确的规则，这就是为什么它经常被许多人比作西部荒野。由于这个原因，这个领域转移到了<em class="lt">统计语言模型</em>。</p><blockquote class="lu"><p id="9070" class="lv lw ji bd lx ly lz ma mb mc md ls dk translated"><strong class="ak"> <em class="me">语言建模(LM)是预测下一个单词是什么的任务，或者更一般地说，是一个为一段文本序列分配概率的系统。</em> </strong></p></blockquote><p id="d8ab" class="pw-post-body-paragraph kx ky ji kz b la mf kj lc ld mg km lf lg mh li lj lk mi lm ln lo mj lq lr ls im bi translated"><strong class="kz jj"> <em class="lt"> N-gram </em> </strong>是<em class="lt">最简单的</em>语言模型，其性能受到缺乏复杂性的限制。像这种简单的模型无法实现流畅、足够的语言变化和正确的长文本写作风格。由于这些原因，<em class="lt"> </em> <strong class="kz jj"> <em class="lt">神经网络</em> ( <em class="lt"> NN </em> ) </strong>被探索为新的黄金标准，尽管它们很复杂，并且<strong class="kz jj"> <em class="lt">递归神经网络(RNN) </em> </strong>成为用于任何种类的序列的<em class="lt">基本架构</em>。如今，RNN 被认为是文本的“香草”架构，但 rnn 也有自己的问题:它不能长时间记住过去的内容，并且它很难创建<em class="lt">长的相关文本序列</em>。由于这些原因，其他架构如<strong class="kz jj"><em class="lt">【LSTM】</em></strong>和<strong class="kz jj"> <em class="lt">门控循环单元(GRU) </em> </strong>被开发出来，并成为许多 NLG 任务的<em class="lt">最先进解决方案</em>。在本文中，我们将探索这些<em class="lt">基本思想和模型</em>的<em class="lt">演变</em>。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mk"><img src="../Images/f82bde916ad8177a4a81f6ab812073d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bd3wdO4VydaG-MtUZ-7Epg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Different flavours of RNN: RNN, GRU and LSTM (<a class="ae mp" href="https://minimalistbaker.com/" rel="noopener ugc nofollow" target="_blank">source of images</a>)</figcaption></figure><p id="a75e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">毫无疑问，<strong class="kz jj"><em class="lt">【NLP】</em></strong>和<strong class="kz jj"><em class="lt">【NLG】</em></strong>已经经历了重大的进步，尤其是最近五年，这就是为什么我们每天多次使用语言模型并从中受益。每当我们在<em class="lt">搜索框</em>中键入文字或者在<em class="lt">手机</em>上书写文字时，幕后的一个模型就在预测下一个即将出现的字符或单词。<em class="lt"> Gmail </em>和<em class="lt"> LinkedIn </em>根据我们之前的对话和<em class="lt">虚拟助手</em>(例如<em class="lt"> Siri </em>或<em class="lt"> Alexa </em>生成<em class="lt">类似人类的回复</em>。聊天机器人正在成为客户服务的重要组成部分，谷歌翻译为大量语言提供了更好的翻译。生成流畅连贯的冗长文本，同时<em class="lt">保持对输出</em>语义的控制，这是一个挑战。这可以通过类似于<strong class="kz jj"> <em class="lt">序列到序列(Seq2Seq) </em> </strong>的架构来解决，该架构用于解决条件文本生成问题，但我们将在本文的后面对此进行更多讨论。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/ebdd627aae68ffb1657140f8f316c1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwaBCbjgK8oZApQs_x1ALA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Examples of applications with NLG models</figcaption></figure><h1 id="b5bc" class="mr ms ji bd mt mu mv mw mx my mz na nb ko nc kp nd kr ne ks nf ku ng kv nh ni bi translated"><strong class="ak"> N-gram </strong></h1><p id="be66" class="pw-post-body-paragraph kx ky ji kz b la nj kj lc ld nk km lf lg nl li lj lk nm lm ln lo nn lq lr ls im bi translated">最直接的语言模型是一个<em class="lt">n</em>gram 模型。文本只不过是一系列字符(或单词)。词汇<em class="lt"> V </em> = { <em class="lt"> W </em> ₁,…，<em class="lt"> W </em> |ᵥ|}下一个单词  <em class="lt"> x </em> ⁽ᵗ⁺ ⁾的概率分布为:</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b1a512a0a779bf5d44d9426bf9ba6194.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/0*4INerAzqqNQu_ree"/></div></figure><p id="96b0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">因此，单词序列<em class="lt"> x </em> ⁽ ⁾，<em class="lt"> x </em> ⁽ ⁾,…，<em class="lt"> x </em> ⁽ᵀ⁾的概率为:</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/e3c9f45f9e2f6ca5529b9e2dadf35b29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DsRegqWL14jv7pie"/></div></div></figure><blockquote class="lu"><p id="b7fe" class="lv lw ji bd lx ly nq nr ns nt nu ls dk translated"><strong class="ak"> <em class="me"> N-gram </em> </strong>假设<strong class="ak"> <em class="me">下一个字 x </em> ⁽ᵗ⁺ ⁾只依赖前面的<em class="me"> n-1 个</em>字。</strong></p></blockquote><p id="5bcc" class="pw-post-body-paragraph kx ky ji kz b la mf kj lc ld mg km lf lg mh li lj lk mi lm ln lo mj lq lr ls im bi translated">用数学术语来说，这意味着:</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/260617b7e626cac76d4c7c42c8584e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/0*aZdvyUmkdmlZkGSe"/></div></figure><p id="6a54" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">N-gram 又称为<strong class="kz jj"> <em class="lt">非光滑极大似然估计</em> </strong>。对于<em class="lt">最大</em> <em class="lt">似然估计</em>，我们统计每个单词在 n-1 个单词(历史)之后出现的次数，并除以这 n-1 个单词在训练数据中出现的次数。<em class="lt">未平滑</em>部分意味着，如果我们在训练数据中没有看到 n-1 个单词历史之后的给定单词，我们将为其分配零概率。</p><blockquote class="nw nx ny"><p id="4c08" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated"><em class="ji">4 克 LM 的示例:</em></p><p id="63ee" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated"><em class="ji"> H̵e̵l̵l̵o̵,̵ ̵I̵想 ____ </em></p><p id="f9ab" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">p(字| <em class="ji">想)= count(想</em>字)/ <em class="ji"> count(想</em>)</p><p id="8c4a" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">P(观| <em class="ji">想)=0.03，</em> P(书| <em class="ji">想)=0.04 </em></p></blockquote><p id="60f1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">完成这些计算后，文本生成就很容易了。生成下一个消息 的<strong class="kz jj"> <em class="lt">步骤为:</em></strong></p><ol class=""><li id="1651" class="oc od ji kz b la lb ld le lg oe lk of lo og ls oh oi oj ok bi translated">看最后 n-1 克(历史)。</li><li id="9506" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">从 n-gram 模型中获得下一个单词的分布。</li><li id="d83b" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">根据相应的分布抽取一克样本。(注意，有不同的方法来选择下一个 gram，并在本文的结尾进行了解释。)</li></ol><p id="4a3d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了生成一系列的 k 线图，我们简单地<em class="lt">循环上面的过程</em>，在每一轮用生成的 k 线图更新历史。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oq"><img src="../Images/9ce57c6cdc96cf08488710a583e29995.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OebP6FpR4kBKf3HN_2VbUw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Sparsity problem</figcaption></figure><p id="092f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这种简单化的方法有一个<strong class="kz jj"> <em class="lt">稀疏性问题</em> </strong>:</p><ol class=""><li id="f6ff" class="oc od ji kz b la lb ld le lg oe lk of lo og ls oh oi oj ok bi translated">正如我们已经提到的，如果一个 gram 从未出现在历史数据中，n-gram 分配<em class="lt"> 0 概率</em> ( <strong class="kz jj"> 0 分子</strong>)。一般来说，我们应该<strong class="kz jj"> <em class="lt">平滑</em> </strong>的概率分布，因为每件事都应该至少有一个小概率分配给它。</li><li id="f2b9" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">如果历史数据中从未出现过 n-1 个 gram 序列，我们<em class="lt">就无法计算出下一个 gram 的</em> <em class="lt">概率</em>(<strong class="kz jj">0 分母</strong>)。在这种情况下，我们应该只考虑一个<em class="lt">更短的 n-gram </em>(即使用 n-1 gram 模型代替)。这种技术叫做<strong class="kz jj"> <em class="lt">回退</em> </strong>。</li></ol><p id="f12d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由于模型的简化假设，如果我们保持上下文和长期依赖性，我们就不会像我们一样擅长预测即将到来的单词。于是我们以<strong class="kz jj"><em class="lt"/></strong><em class="lt"/>等不连贯的文字收场</p><blockquote class="nw nx ny"><p id="caa1" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">"在鞋楦和制鞋业生产的同时，股市也随之上涨"</p></blockquote><p id="0fcb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然后，有人可能认为更大的上下文可能给出更好的预测，但是通过增加模型中的克数(n)<em class="lt">稀疏性</em>成为更大的问题，并且整个模型实际上变得不正常。此外，更大的 n 会导致型号的<em class="lt">更大，根据经验，<strong class="kz jj"> <em class="lt"> n 不应超过 5 </em> </strong>。</em></p><h1 id="7b0e" class="mr ms ji bd mt mu mv mw mx my mz na nb ko nc kp nd kr ne ks nf ku ng kv nh ni bi translated"><strong class="ak"> NN </strong></h1><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/499c8f4901415f1b56fd84ae225cbd4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/0*CoW4RGvRRPRWaVDf"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Trade-offs between these two types of systems. (<a class="ae mp" href="https://cs.stanford.edu/~zxie/textgen.pdf" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="cc7d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">需要一种具有上下文感知的更复杂的模型。有了 NN，就没有稀疏性问题，因为它可以给我们以前从未见过的组合的概率分布，并且不需要存储所有观察到的 n-gram。一个 NN 似乎是现在每个硬数据问题的解决方案，这里的情况是这样吗？</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/db0753b1e00e01c82b9820439eea684d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wp8x2j0WZiwlbreOS2Wu0A.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Fixed window neural language model</figcaption></figure><p id="523c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">例如，固定窗口神经 LM 查看固定长度窗口中的单词，其输入层是连接的单词嵌入向量。但是我们仍然在看一个有限大小的固定窗口，并且<em class="lt">扩大窗口</em>大小导致模型参数 不可维护的<strong class="kz jj"> <em class="lt">增加。此外，在处理输入的方式上也没有<strong class="kz jj"> <em class="lt">对称性</em></strong>——看上面的图，我们可以看到单词被乘以不同的权重。因此，我们有效地多次学习相似的函数。</em></strong></p><blockquote class="lu"><p id="0931" class="lv lw ji bd lx ly lz ma mb mc md ls dk translated"><em class="me">在我们如何处理引入的单词嵌入方面应该有很多共性</em>。</p></blockquote><p id="426d" class="pw-post-body-paragraph kx ky ji kz b la mf kj lc ld mg km lf lg mh li lj lk mi lm ln lo mj lq lr ls im bi translated">顾名思义，递归神经网络(RNN)通过对所有单词输入 循环使用相同的网络和权重矩阵来解决这个问题<strong class="kz jj"> <em class="lt">。因此，RNN 可以被认为是同一个网络的多个副本，每个副本都向后继者传递消息。这使得 RNN 成为比固定窗口 NNs 更紧凑的模型。下一节将使这一陈述更加清晰。</em></strong></p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/517fb9c0319d51cbd572f6563c3b0d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y21cQnOKux5Sh-CkqOHslg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Unrolling of an RNN (modified figure from <a class="ae mp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="564a" class="mr ms ji bd mt mu mv mw mx my mz na nb ko nc kp nd kr ne ks nf ku ng kv nh ni bi translated"><strong class="ak">rnn、gru 和 lstm</strong></h1><p id="dfec" class="pw-post-body-paragraph kx ky ji kz b la nj kj lc ld nk km lf lg nl li lj lk nm lm ln lo nn lq lr ls im bi translated">在 RNN 中，<strong class="kz jj"> <em class="lt">输入序列</em> </strong>可以是任意长度的<strong class="kz jj"><em class="lt"/></strong><em class="lt"/>，因为我们对每个时间步(字)应用相同的权重矩阵。这意味着型号<em class="lt">的尺寸不会因输入</em>变长而增加，并且在输入处理方式上存在<strong class="kz jj"> <em class="lt">对称性</em> </strong>。此外，RNN 具有一系列隐藏状态(而不是像我们在固定窗口神经 LM 中那样只有一个隐藏状态)。某个步骤的输出是基于先前的隐藏状态和该步骤的输入计算的。隐藏状态就像一个单一的状态，随着时间的推移而变异，形成网络的记忆。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/a287a3e326fd33404ff479d2feed2baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6MtCqYAbuILuVr84HGeGg.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Vanilla RNN architecture where Xt is the input (eg. encoding of word at step t) and ht is output value (eg. encoding of the next word) (modified figure from <a class="ae mp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="b410" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">隐藏状态是前一个隐藏状态和当前模型输入的线性组合，然后通过非线性处理(如 sigmoid，tanh)。我们正在尝试学习一个<strong class="kz jj"> <em class="lt">通用函数</em> </strong>(上图中的 A)关于我们应该如何处理一个单词<em class="lt">给定的前一个单词</em>的上下文，而<em class="lt">学习语言和上下文</em>的一般表示。</p><p id="b57e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在字符级而不是单词级预测即将到来的上下文是一个分类问题，因为字符比词汇表中的单词范围少，所以分类数量明显较少。在训练了一个预测下一个字符的 RNN 之后，我们可以按照下面的<strong class="kz jj"> <em class="lt">步骤生成新的文本</em> </strong>:</p><ol class=""><li id="954b" class="oc od ji kz b la lb ld le lg oe lk of lo og ls oh oi oj ok bi translated">首先选择一个字符串，初始化 RNN 状态，并设置要生成的字符数。</li><li id="a525" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">使用起始字符串和 RNN 状态，将预测作为下一个字符的分布。使用分类分布计算预测字符的索引。</li><li id="e2f5" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">使用这个预测的字符作为模型的下一个输入。</li><li id="80f6" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">由模型返回的 RNN 状态被反馈到模型中，以便它现在具有更多的上下文，而不仅仅是输入(预测的先前字符)。(见上面 RNN 展开的图)</li></ol><p id="4387" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对文本序列中的一个字符的计算(理论上)使用来自许多步骤之前的信息，但是循环字符计算仍然很慢，并且在实践中很难访问来自许多步骤之前的信息。</p><blockquote class="nw nx ny"><p id="8990" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">预测“太阳在<strong class="kz jj">天空</strong>照耀”这句话的最后一个单词是可行的。但是“11 月的天气主要是<strong class="kz jj">晴</strong>”这句话的最后一个字就比较难预测了当 100 个单词之前有一句话“我计划去塞浦路斯旅行。”。</p></blockquote><p id="4fe0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj"> RNNs 挣扎</strong>记住长范围依赖关系主要是由于所谓的<strong class="kz jj"> <em class="lt">消失梯度问题</em></strong><em class="lt"/><em class="lt"/><em class="lt"/><strong class="kz jj"><em class="lt">上下文信息的线性衰减</em> </strong>。梯度的大小受激活函数的权重和导数的影响(由于链式法则),它们在网络中反复出现。如果这些因子中的任何一个小于 1(当我们将小值相乘时)，梯度可能会及时消失，如果这些因子大于 1，梯度可能会爆炸。在这两种情况下，我们都会丢失信息，尤其是当文本较长时。训练期间非常小的梯度转化为重量的微小变化，因此，<em class="lt">没有显著的学习</em>。</p><blockquote class="lu"><p id="41be" class="lv lw ji bd lx ly lz ma mb mc md ls dk translated">LSTM 和后来的 GRU 的设计是为了缓解控制模型更新记忆的消失梯度问题。</p></blockquote><p id="75f4" class="pw-post-body-paragraph kx ky ji kz b la mf kj lc ld mg km lf lg mh li lj lk mi lm ln lo mj lq lr ls im bi translated"><strong class="kz jj">lstm</strong>和<strong class="kz jj"> GRUs </strong>中的重复模块(图中的 A)具有不同的结构，这使得它们<strong class="kz jj"> <em class="lt">能够学习长期依赖</em> </strong>。它们的结构使用由非线性(例如 sigmoid)神经网络层和乘法或加法运算组成的门。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/b2aee4a3d2c1350d8ee362226b625e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcTWNLcHCYMIIk2f2wtGtw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">LSTM architecture and equations of its repeating module (modified figure from <a class="ae mp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="68e5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">有不同类型的<strong class="kz jj"> <em class="lt">门</em> </strong>可用于<strong class="kz jj"> <em class="lt">更新状态</em> </strong>。例如，<em class="lt">遗忘门</em>决定我们要丢弃什么信息，而<em class="lt">输入门</em>决定我们要存储什么新信息。LSTM 的一个关键特征是<em class="lt">单元状态</em>(上图中 LSTM 顶部的水平线)，它通过网络，只有一些微小的线性交互，允许信息沿着网络流动。<strong class="kz jj"> <em class="lt"> GRU 是 LSTM </em> </strong>的一个更快(但功能更弱)的变体，它合并了单元格状态和隐藏状态。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ou"><img src="../Images/0a7e4d4ce658562cf983e17a1c8f5e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GMuZ0O8uARPkOF0rYEGVMA.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">GRU architecture and equations of its repeating module (modified figure from <a class="ae mp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><h1 id="f941" class="mr ms ji bd mt mu mv mw mx my mz na nb ko nc kp nd kr ne ks nf ku ng kv nh ni bi translated"><strong class="ak">条件语言模型— Seq2Seq </strong></h1><p id="48d4" class="pw-post-body-paragraph kx ky ji kz b la nj kj lc ld nk km lf lg nl li lj lk nm lm ln lo nn lq lr ls im bi translated">到目前为止，我们已经看到了可以生成文本的不同模型(n-gram、RNN、LSTM、GRU ),但是我们还没有探索如何基于除起始文本之外的条件来生成文本。一些条件文本生成<strong class="kz jj">应用</strong>如下:</p><blockquote class="nw nx ny"><p id="acd2" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">作者姓名<strong class="kz jj"> &gt; </strong>作者风格的文本</p><p id="714b" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated"><strong class="kz jj">话题&gt;话题</strong>关于那个话题的文章</p><p id="04d9" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">电子邮件<strong class="kz jj"> &gt; </strong>电子邮件主题行(又名总结)</p><p id="5d7f" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">问题<strong class="kz jj"> &gt; </strong>自由形式问题回答(又名聊天机器人)</p><p id="d60e" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated"><strong class="kz jj">条&gt;条</strong>条的总结(又名总结)</p><p id="e20d" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">图像<strong class="kz jj"> &gt; </strong>描述图像的文本(又名图像字幕)</p><p id="c60e" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated">自然语言代码描述<strong class="kz jj"> &gt; </strong>代码(又名代码生成)</p><p id="241e" class="kx ky lt kz b la lb kj lc ld le km lf nz lh li lj oa ll lm ln ob lp lq lr ls im bi translated"><strong class="kz jj">句&gt;句</strong>句翻译成不同的语言(又名机器翻译)</p></blockquote><p id="6b0c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在条件语言模型中，我们希望在给定一些条件上下文(<em class="lt"> x </em>)的情况下，将概率分配给单词序列(<em class="lt"> y </em>):</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ov"><img src="../Images/233e4b7604e6cfb95cd064c0fe56f68d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O8p583DgliA0GGd-"/></div></div></figure><p id="e358" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这种范式被称为<strong class="kz jj"> <em class="lt"> Seq2Seq </em> </strong>建模，它是 2014 年<em class="lt">神经机器翻译</em>的一大突破。从那时起，它一直是一种领先的标准方法，并在 2016 年使用了<strong class="kz jj"> <em class="lt">谷歌翻译</em> </strong>。为了训练这样一个模型，我们需要大量的<em class="lt">并行数据</em>。</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/91b280d3479afc525f89c357ff06819a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fUziQUkUZuNMY6-U"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">The Rosetta Stone: First parallel language dataset</figcaption></figure><p id="3e3b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">Seq2Seq 的架构是由两个 rnn 组成的单一神经网络模型:</p><ul class=""><li id="f4d0" class="oc od ji kz b la lb ld le lg oe lk of lo og ls ox oi oj ok bi translated"><strong class="kz jj">一个<em class="lt">编码器</em> </strong>:创建一个固定长度的编码(一个实数向量)<em class="lt">封装关于输入</em>的信息。在机器翻译中，编码器从源句子中提取所有相关信息以产生编码。</li><li id="c8af" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated"><strong class="kz jj">一个<em class="lt">解码器</em> </strong>(本质上是一个条件 LM):一个语言模型，<em class="lt">用编码器创建的编码生成目标序列条件</em>。在机器翻译中，解码器生成源句子的翻译句子。</li></ul><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oy"><img src="../Images/2f23a6b2160d372b062fba5e709379bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPw-Dra7nsUfbt67ckeY8g.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Seq2Seq example for generating a message to property agents based on some search criteria</figcaption></figure><p id="ce31" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">解码器是用一种叫<em class="lt"> </em> <strong class="kz jj"> <em class="lt">老师强制</em> </strong>的方法训练的。这意味着“老师”迫使解码器的输入成为黄金目标输入，而不是它的预测。目标序列是偏移了一个单词的输入序列，以便网络学习预测下一个单词。Seq2Seq 中的<em class="lt">反向传播</em>端到端操作<em class="lt"/>(一端是损耗函数，另一端是编码器 RNN 的起点)，我们<strong class="kz jj"> <em class="lt">相对于<strong class="kz jj"><em class="lt"/></strong>学习整个系统</em>。</strong></p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oz"><img src="../Images/f3f09aef75e24ba2f1203ee84c14fc67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t_McQ_a1M0_x8bJqBiAN7Q.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">End-to-end training using one loss function <em class="me">J</em></figcaption></figure><p id="1ce7" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">由编码器创建的嵌入成为解码器 RNN 的初始状态。我们强制在这个向量中捕获关于源句子的所有信息，因为这是提供给解码器的唯一信息。这样我们就陷入了一种<em class="lt"> </em> <strong class="kz jj"> <em class="lt">的信息瓶颈</em> </strong>的问题中。如果一些信息不在这个向量中，那么解码器就无法正确翻译这个句子。<strong class="kz jj"> <em class="lt">注意</em> </strong>提供了这个问题的解决方案，同时也帮助解决了其他问题，包括<strong class="kz jj"> <em class="lt">消失渐变问题</em> </strong>。</p><blockquote class="lu"><p id="f2c3" class="lv lw ji bd lx ly lz ma mb mc md ls dk translated"><strong class="ak"> <em class="me">注意</em>是通用的<em class="me">深度学习技术</em>。给定一组<em class="me">向量值</em>和一个<em class="me">向量查询</em>，它计算<em class="me">依赖于查询的值的加权和</em>。</strong></p></blockquote><figure class="pb pc pd pe pf iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pa"><img src="../Images/ff2e76492487213c90afc675bdb33037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pfjsL8SjXXIbTa429agOTw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Seq2Seq with Attention</figcaption></figure><p id="6d90" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在 Seq2Seq 情况下，每个解码器隐藏状态 s𝘵(查询)关注所有编码器隐藏状态 h₁,…,h_N(值)。因此，在<strong class="kz jj">解码器</strong>的每一步，我们都有一个<strong class="kz jj">直接连接到编码器</strong>，但是我们<strong class="kz jj">关注源句子的不同部分</strong>。</p><p id="580d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">上面的网络图在数学上<strong class="kz jj">所做的</strong>是:</p><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pg"><img src="../Images/9c8e19a29b5da727c3c71c8f219cc0d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EHIKmLNZDk0oJ7A9plIZfw.png"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Attention (a_t) translated in mathematical equations</figcaption></figure><ol class=""><li id="96e8" class="oc od ji kz b la lb ld le lg oe lk of lo og ls oh oi oj ok bi translated">计算解码器隐藏状态(步骤 t 中的 s)与每个编码器隐藏状态(h₁,…,h_N)的点积，以便获得注意力分数(eᵗ)</li><li id="0639" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">使用软蜡获得这一步的注意力分布(αᵗ)</li><li id="fd1d" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">使用该分布来获得注意力输出(步骤 t 中的 a ),作为编码器隐藏状态的加权和。</li><li id="e6b5" class="oc od ji kz b la ol ld om lg on lk oo lo op ls oh oi oj ok bi translated">将注意力输出与解码器隐藏状态([a；s])并按照非注意 Seq2Seq 模型进行。</li></ol><p id="5cf9" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">最后，在训练 Seq2Seq 模型之后，可以遵循不同的<strong class="kz jj">策略来通过解码器预测下一个字</strong>:</p><ul class=""><li id="df7e" class="oc od ji kz b la lb ld le lg oe lk of lo og ls ox oi oj ok bi translated"><strong class="kz jj"> <em class="lt">贪婪</em> </strong>:我们总是<strong class="kz jj">挑选概率最高的词</strong>(又名 argmax)。我们现在做了最好的选择，没有回头路了。然而，这不一定会给我们整个句子的 argmax。</li><li id="7f80" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated"><strong class="kz jj"> <em class="lt">光束搜索</em> </strong>:光束搜索<strong class="kz jj">跟踪每一步的<em class="lt"> k </em>可能变量</strong>，避免被局部最大值引入歧途。当 k 为<em class="lt">小</em> ( <em class="lt"> k=1 表示贪婪</em>)时，我们可能会得到<em class="lt">不合语法的不自然的</em>、<em class="lt">无意义的不正确的</em>句子。当 k 比<em class="lt">大</em>时，这些问题会减少，但会更加<em class="lt">昂贵</em>并给出更多<em class="lt">通用</em>回复，汇聚成<em class="lt">安全“正确”</em>响应，然而<em class="lt">不太相关</em>。</li><li id="bf59" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated"><strong class="kz jj"> <em class="lt">采样</em> </strong>:我们<strong class="kz jj">从一个截断的条件词概率分布</strong>中采样，即从前 k 个最可能的词中采样。用这种方法，通常句子由于随机性而没有太大的意义。注意<em class="lt"> k=1 </em>与<em class="lt">贪婪</em>解码相同，<em class="lt"> k= </em>大小的<em class="lt">词汇</em>与纯<em class="lt">采样</em>相同。</li></ul><h1 id="844d" class="mr ms ji bd mt mu mv mw mx my mz na nb ko nc kp nd kr ne ks nf ku ng kv nh ni bi translated"><strong class="ak">遗言</strong></h1><p id="151a" class="pw-post-body-paragraph kx ky ji kz b la nj kj lc ld nk km lf lg nl li lj lk nm lm ln lo nn lq lr ls im bi translated">我们讨论了生成文本的不同方法，从简单的<strong class="kz jj"><em class="lt"/></strong>方法到不同的<strong class="kz jj"> <em class="lt"> RNN </em> </strong>架构，包括<strong class="kz jj"><em class="lt"/></strong>和<strong class="kz jj"> <em class="lt"> GRU </em> </strong>。我们还研究了<strong class="kz jj"> S <em class="lt"> eq2Seq </em> </strong>架构，其中用<strong class="kz jj"><em class="lt"/></strong>可以处理<strong class="kz jj"> <em class="lt">条件文本生成</em> </strong>机器翻译等问题。</p><p id="5139" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">这些模型在谷歌和我的团队<strong class="kz jj">之外有</strong><a class="ae mp" href="https://www.zoopla.co.uk/" rel="noopener ugc nofollow" target="_blank"><strong class="kz jj">Zoopla</strong></a><strong class="kz jj">的数据科学团队用这些知识挑战财产门户体验</strong>。希望在读完这篇文章后，你对文本生成有了更好的直觉，并且你现在也能够为你的行业找到有用的应用。如果你想了解更多，以下是我发现的一些有用且有见地的资源:</p><ul class=""><li id="b75b" class="oc od ji kz b la lb ld le lg oe lk of lo og ls ox oi oj ok bi translated"><a class="ae mp" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank"> CS224n </a>:深度学习的自然语言处理。斯坦福大学，2019 年冬季(该课程是本博客的灵感来源)</li><li id="5673" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated">A.卡帕西。<a class="ae mp" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a>，2015</li><li id="6eae" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated">C.奥拉。<a class="ae mp" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解 LSTM 网络</a>，2015</li><li id="4c68" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated">Z.谢。神经文本生成:实用指南。arXiv 预印本 arXiv: 1711.09534v1，2017 年</li><li id="b660" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated">Z.胡。<a class="ae mp" href="https://arxiv.org/pdf/1703.00955.pdf" rel="noopener ugc nofollow" target="_blank">走向文本的受控生成</a>，2018</li><li id="fa68" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated"><a class="ae mp" href="https://www.tensorflow.org/beta/tutorials/text/text_generation" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/beta/tutorials/text/text _ generation</a></li><li id="dbd0" class="oc od ji kz b la ol ld om lg on lk oo lo op ls ox oi oj ok bi translated"><a class="ae mp" href="https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html" rel="noopener ugc nofollow" target="_blank">https://py torch . org/tutorials/intermediate/seq 2 seq _ translation _ tutorial . html</a></li></ul><figure class="ml mm mn mo gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ph"><img src="../Images/7cdcc961224e50ce65ffde32bad20d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*7XNnFDABPItxPLNlHRC4Qw.png"/></div></div></figure><p id="4c7a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">特别感谢我在 Zoopla 的同事们，他们重视知识分享和新想法的实验，并感谢 Jan Teichmann 对本文的反馈和支持。</p></div></div>    
</body>
</html>