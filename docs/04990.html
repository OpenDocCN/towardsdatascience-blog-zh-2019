<html>
<head>
<title>How to Train Your Quadcopter</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练你的四轴飞行器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-your-quadcopter-adventures-in-machine-learning-algorithms-e6ee5033fd61?source=collection_archive---------7-----------------------#2019-07-27">https://towardsdatascience.com/how-to-train-your-quadcopter-adventures-in-machine-learning-algorithms-e6ee5033fd61?source=collection_archive---------7-----------------------#2019-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="98ab" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">教四轴飞行器飞行的完全初学者指南(附代码！)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1ff19238227f01b1b223662096ffb321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ldtibm2AOhW-d_ZqL1NdA.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/photo/animal-avian-beak-bird-416202/" rel="noopener ugc nofollow" target="_blank">Pixaby</a> via <a class="ae ky" href="https://www.pexels.com/photo/animal-avian-beak-bird-416202/" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><p id="b16a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，我在尝试训练四轴飞行器飞行时获得了很多乐趣。</p><p id="4d63" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太有趣了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/e7d41768ee60759b2691da90dddc80ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u93iKYOWVxppjvUik5x57A.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@366671?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">наталья семенкова </a>from <a class="ae ky" href="https://www.pexels.com/photo/selective-focus-photography-of-sphinx-cat-lying-on-bedspread-991831/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h2 id="4755" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">关于四轴飞行器的一切！</h2><p id="6152" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">四轴飞行器是很神奇的东西。</p><p id="53f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它们体积小，机动性强，可以在室内和室外飞行。他们对摄影和新闻业很感兴趣。你可以用它们来送货，人道主义行动，搜索和救援，以及拍摄电影。当然，它们也有军事和执法应用。它们也非常适合研究！有了四轴飞行器，你可以测试飞行控制理论、实时系统、导航和机器人等领域的新想法。</p><p id="6fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，他们可以由业余爱好者建造和维护！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mu mv l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">GIF via <a class="ae ky" href="https://media.giphy.com/media/1fkd6ZTBsxSosV4UTS" rel="noopener ugc nofollow" target="_blank">GIPHY</a></figcaption></figure><p id="4ee4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一个非常基本的层面上，四轴飞行器是一种由四个旋翼(大部分时间)提升和推进的直升机。他们通常使用两对相同的固定螺距螺旋桨。两个顺时针运行，两个逆时针运行。对于控制，他们使用每个转子速度的独立变化。通过改变每个转子的速度，您可以产生所需的总推力，定位推力中心(横向和纵向)，并产生所需的总扭矩。</p><p id="fcca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">四轴飞行器真正酷的地方在于，你可以通过使用机器学习技术训练的神经网络来控制它！</p><p id="cbf0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用强化学习，你可以训练一个网络直接将状态映射到执行器命令。甚至可以用模拟训练的神经网络完全控制四轴飞行器！<a class="ae ky" href="https://arxiv.org/pdf/1707.05110.pdf" rel="noopener ugc nofollow" target="_blank">如果你感兴趣的话，Jemin Hwangbo 等人写了一篇很棒的论文概述了他们的研究。</a></p><p id="3220" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然大多数四轴飞行器有四个发动机提供推力(……将“四轴飞行器”中的“四轴飞行器”)，但一些实际上有 6 个或 8 个。有更多的推力点可以提高稳定性，并实现一系列酷的飞行行为。但更复杂的系统意味着手动控制每个发动机的推力几乎是不可能的。</p><p id="bd33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，您可以使用强化学习来构建能够自主学习行为的代理！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/5958b51211dbb1ee031f364703b77a1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRWjmzqxPoilKCThrtn2Ug.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@jimbear?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Jimmy Chan </a>from <a class="ae ky" href="https://www.pexels.com/photo/photography-of-a-baby-monkey-eating-vegetable-1000529/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h2 id="e915" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">我为什么要这么做？</h2><p id="4124" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">这是<a class="ae ky" href="http://udacity.com" rel="noopener ugc nofollow" target="_blank"> Udacity 的</a>机器学习工程师 Nanodegree 的必考项目之一。这是一个非常酷的项目，你可能想<a class="ae ky" href="https://github.com/udacity/RL-Quadcopter" rel="noopener ugc nofollow" target="_blank">看看</a>！你得到了一些坚实的基本代码，一些有用的文件，然后你插入你自己的代码，让它运行起来。</p><p id="67aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">听起来很简单，对吧？</p><p id="9cb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">没有那么多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/57030a6ef618711813db709737022d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PKw-hBy28T0l4oxZwxd9fg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/photo/adorable-angry-animal-animal-portrait-208984/" rel="noopener ugc nofollow" target="_blank">Pixabay via Pexels</a></figcaption></figure><h2 id="3707" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">让我们把手弄脏吧！</h2><p id="152b" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">要让这个东西发挥作用，你需要设计自己的强化学习任务和一个代理来完成它。为您提供了一个样例代理来开始使用，以及一个四轴飞行器模拟器。</p><p id="2f9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">剩下的就看你的了。</p><p id="afe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代理通过设置四个旋翼的每秒转数来控制四轴飞行器。您可以通过包含速度信息来扩展状态向量的大小。(Udacity 使用四轴飞行器的 6 维姿态来构建每个时间步的环境状态)。也可以使用姿势、速度和角速度的任意组合。</p><p id="15be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您将使用<code class="fe my mz na nb b">__init__()</code>方法初始化几个变量，包括您的任务。如果您浏览一下代码，您会看到模拟器在这里被初始化为<code class="fe my mz na nb b">physics_sim</code>类的一个实例。受<a class="ae ky" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">这篇深度确定性政策梯度论文</a>的启发，Udacity 还利用了动作重复。您将设置状态向量中的元素数量，并将目标位置指定为一个变量。</p><p id="e25f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们为您提供了一个示例代理。它使用非常简单的线性策略来直接计算动作向量，作为状态向量和权重矩阵的点积。它通过添加一些<a class="ae ky" href="https://en.wikipedia.org/wiki/Gaussian_noise" rel="noopener ugc nofollow" target="_blank">高斯噪声</a>来随机扰动参数，以产生不同的策略。根据每集获得的平均奖励，它会跟踪到目前为止找到的最佳参数集以及分数的变化情况。然后，它调整一个比例因子来扩大或收紧噪声。</p><p id="55bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们给你安排的那个简单的不太好用…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f18dcbd10f4f471fb8d438566b2bc35c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*J_XG_AcwSQheFjafTIl6nQ.png"/></div></figure><p id="1aa1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来由你来指定你选择的任务。您将定义自己的代理，并为您的任务调整各种超参数和奖励函数，直到您获得一些值得骄傲的行为。</p><h2 id="c055" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">我们来训练一架四轴飞行器吧！</h2><p id="d20f" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">我们需要在 task.py 文件中定义我们的任务(环境)。我们还会有一个文件夹存放我们的增援特工。我们将在那里保存一个 agent.py 文件，我们在其中定义代理，并利用包含四轴飞行器模拟器的 physics_sim.py 文件。你可能想考虑创建更多的文件，比如一个定义神经网络的文件，如果你想尝试的话。</p><p id="3287" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们的任务是:</p><h2 id="d116" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">任务. py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="ea1d" class="lw lx it nb b gy nh ni l nj nk">import numpy as np<br/>from physics_sim import PhysicsSim<br/>    <br/>class Task2():<br/>    """Task that defines the goal and provides feedback to the agent."""<br/>    def __init__(self, init_pose=None, init_velocities=None, <br/>        init_angle_velocities=None, runtime=5., target_pos=None):<br/>        """Initialize a Task object.<br/>        Params<br/>        ======<br/>            init_pose: initial position of the quadcopter in (x,y,z) dimensions and the Euler angles<br/>            init_velocities: initial velocity of the quadcopter in (x,y,z) dimensions<br/>            init_angle_velocities: initial radians/second for each of the three Euler angles<br/>            runtime: time limit for each episode<br/>            target_pos: target/goal (x,y,z) position for the agent<br/>        """<br/>        # Simulation<br/>        self.sim = PhysicsSim(init_pose, init_velocities, init_angle_velocities, runtime) <br/>        self.action_repeat = 3</span><span id="d75e" class="lw lx it nb b gy nl ni l nj nk">self.state_size = self.action_repeat * 6<br/>        self.action_low = 0<br/>        self.action_high = 900<br/>        self.action_size = 4</span><span id="86d8" class="lw lx it nb b gy nl ni l nj nk"># Goal<br/>        self.target_pos = target_pos if target_pos is not None else np.array([0., 0., 10.])</span><span id="d480" class="lw lx it nb b gy nl ni l nj nk">def get_reward(self):<br/>        """Uses current pose of sim to return reward."""<br/>        #reward = 1-(0.3*(abs(self.sim.pose[:3] - self.target_pos))).sum()<br/>        #reward = np.tanh(reward)<br/>        reward = np.tanh(1 - 0.0005*(abs(self.sim.pose[:3] - self.target_pos)).sum())<br/>        return reward</span><span id="6633" class="lw lx it nb b gy nl ni l nj nk">def step(self, rotor_speeds):<br/>        """Uses action to obtain next state, reward, done."""<br/>        reward = 0<br/>        pose_all = []<br/>        for _ in range(self.action_repeat):<br/>            done = self.sim.next_timestep(rotor_speeds) # update the sim pose and velocities<br/>            reward += (self.get_reward()/2)<br/>            #if reward &gt; 1: reward = 1<br/>            #if reward &lt; -1: reward = -1<br/>            #reward = np.tanh(0.5*reward)<br/>            pose_all.append(self.sim.pose)<br/>        next_state = np.concatenate(pose_all)<br/>        return next_state, reward, done</span><span id="bc29" class="lw lx it nb b gy nl ni l nj nk">def reset(self):<br/>        """Reset the sim to start a new episode."""<br/>        self.sim.reset()<br/>        state = np.concatenate([self.sim.pose] * self.action_repeat) <br/>        return state</span></pre><p id="f4a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们肯定需要设计代理。在浪费了大量时间和无用的浪费之后，我决定使用 actor、critical、policy search、replay buffer 和 ou_noise 文件。(<a class="ae ky" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noopener ugc nofollow" target="_blank">你可以在这里阅读奥恩斯坦-乌伦贝克进程</a>。)</p><h1 id="4ace" class="nm lx it bd ly nn no np mb nq nr ns me jz nt ka mh kc nu kd mk kf nv kg mn nw bi translated">显示您的代理文件夹</h1><h2 id="d68b" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">agent.py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="1806" class="lw lx it nb b gy nh ni l nj nk">from agents.actor import Actor<br/>from agents.critic import Critic<br/>from agents.replay_buffer import ReplayBuffer<br/>from agents.ou_noise import OUNoise<br/>import numpy as np<br/>import random<br/>from collections import namedtuple, deque</span><span id="02f5" class="lw lx it nb b gy nl ni l nj nk">class DDGP():<br/>    """Reinforcement Learning agent that learns using DDPG."""<br/>    def __init__(self, task):<br/>        self.task = task<br/>        self.state_size = task.state_size<br/>        self.action_size = task.action_size<br/>        self.action_low = task.action_low<br/>        self.action_high = task.action_high</span><span id="98cf" class="lw lx it nb b gy nl ni l nj nk"># Actor (Policy) Model<br/>        self.actor_local = Actor(self.state_size, self.action_size, self.action_low, self.action_high)<br/>        self.actor_target = Actor(self.state_size, self.action_size, self.action_low, self.action_high)</span><span id="a97c" class="lw lx it nb b gy nl ni l nj nk"># Critic (Value) Model<br/>        self.critic_local = Critic(self.state_size, self.action_size)<br/>        self.critic_target = Critic(self.state_size, self.action_size)</span><span id="128c" class="lw lx it nb b gy nl ni l nj nk"># Initialize target model parameters with local model parameters<br/>        self.critic_target.model.set_weights(self.critic_local.model.get_weights())<br/>        self.actor_target.model.set_weights(self.actor_local.model.get_weights())</span><span id="527f" class="lw lx it nb b gy nl ni l nj nk"># Noise process<br/>        self.exploration_mu = 0<br/>        self.exploration_theta = 0.15<br/>        self.exploration_sigma = 0.3<br/>        self.noise = OUNoise(self.action_size, self.exploration_mu, self.exploration_theta, self.exploration_sigma)</span><span id="bba2" class="lw lx it nb b gy nl ni l nj nk"># Replay memory<br/>        self.buffer_size = 1000000<br/>        self.batch_size = 64<br/>        self.memory = ReplayBuffer(self.buffer_size, self.batch_size)</span><span id="a074" class="lw lx it nb b gy nl ni l nj nk"># Algorithm parameters<br/>        self.gamma = 0.99  # discount factor<br/>        self.tau = 0.001  # for soft update of target parameters</span><span id="28d7" class="lw lx it nb b gy nl ni l nj nk">def reset_episode(self):<br/>        self.noise.reset()<br/>        state = self.task.reset()<br/>        self.last_state = state<br/>        return state</span><span id="15a6" class="lw lx it nb b gy nl ni l nj nk">def step(self, action, reward, next_state, done):<br/>         # Save experience / reward<br/>        self.memory.add(self.last_state, action, reward, next_state, done)</span><span id="0be8" class="lw lx it nb b gy nl ni l nj nk"># Learn, if enough samples are available in memory<br/>        if len(self.memory) &gt; self.batch_size:<br/>            experiences = self.memory.sample()<br/>            self.learn(experiences)</span><span id="1668" class="lw lx it nb b gy nl ni l nj nk"># Roll over last state and action<br/>        self.last_state = next_state</span><span id="37c1" class="lw lx it nb b gy nl ni l nj nk">def act(self, states):<br/>        """Returns actions for given state(s) as per current policy."""<br/>        state = np.reshape(states, [-1, self.state_size])<br/>        action = self.actor_local.model.predict(state)[0]<br/>        return list(action + self.noise.sample())  # add some noise for exploration</span><span id="ef6b" class="lw lx it nb b gy nl ni l nj nk">def learn(self, experiences):<br/>        """Update policy and value parameters using given batch of experience tuples."""<br/>        # Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)<br/>        states = np.vstack([e.state for e in experiences if e is not None])<br/>        actions = np.array([e.action for e in experiences if e is not None]).astype(np.float32).reshape(-1, self.action_size)<br/>        rewards = np.array([e.reward for e in experiences if e is not None]).astype(np.float32).reshape(-1, 1)<br/>        dones = np.array([e.done for e in experiences if e is not None]).astype(np.uint8).reshape(-1, 1)<br/>        next_states = np.vstack([e.next_state for e in experiences if e is not None])</span><span id="7707" class="lw lx it nb b gy nl ni l nj nk"># Get predicted next-state actions and Q values from target models<br/>        #     Q_targets_next = critic_target(next_state, actor_target(next_state))<br/>        actions_next = self.actor_target.model.predict_on_batch(next_states)<br/>        Q_targets_next = self.critic_target.model.predict_on_batch([next_states, actions_next])</span><span id="86ac" class="lw lx it nb b gy nl ni l nj nk"># Compute Q targets for current states and train critic model (local)<br/>        Q_targets = rewards + self.gamma * Q_targets_next * (1 - dones)<br/>        self.critic_local.model.train_on_batch(x=[states, actions], y=Q_targets)</span><span id="3dcb" class="lw lx it nb b gy nl ni l nj nk"># Train actor model (local)<br/>        action_gradients = np.reshape(self.critic_local.get_action_gradients([states, actions, 0]), (-1, self.action_size))<br/>        self.actor_local.train_fn([states, action_gradients, 1])  # custom training function</span><span id="18c5" class="lw lx it nb b gy nl ni l nj nk"># Soft-update target models<br/>        self.soft_update(self.critic_local.model, self.critic_target.model)<br/>        self.soft_update(self.actor_local.model, self.actor_target.model)</span><span id="3e64" class="lw lx it nb b gy nl ni l nj nk">def soft_update(self, local_model, target_model):<br/>        """Soft update model parameters."""<br/>        local_weights = np.array(local_model.get_weights())<br/>        target_weights = np.array(target_model.get_weights())</span><span id="c227" class="lw lx it nb b gy nl ni l nj nk">assert len(local_weights) == len(target_weights), "Local and target model parameters must have the same size"</span><span id="6f52" class="lw lx it nb b gy nl ni l nj nk">new_weights = self.tau * local_weights + (1 - self.tau) * target_weights<br/>        target_model.set_weights(new_weights)<br/>       <br/>class OUNoise:<br/>    """Ornstein-Uhlenbeck process."""</span><span id="0514" class="lw lx it nb b gy nl ni l nj nk">def __init__(self, size, mu, theta, sigma):<br/>        """Initialize parameters."""<br/>        self.mu = mu * np.ones(size)<br/>        self.theta = theta<br/>        self.sigma = sigma<br/>        self.reset()</span><span id="59fa" class="lw lx it nb b gy nl ni l nj nk">def reset(self):<br/>        """Reset the internal state mean (mu)."""<br/>        self.state = self.mu</span><span id="4f66" class="lw lx it nb b gy nl ni l nj nk">def sample(self):<br/>        """Update internal state and return it as a noise sample."""<br/>        x = self.state<br/>        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))<br/>        self.state = x + dx<br/>        return self.state<br/>    <br/>class ReplayBuffer:<br/>    """Fixed-size buffer to store experience tuples."""</span><span id="3290" class="lw lx it nb b gy nl ni l nj nk">def __init__(self, buffer_size, batch_size):<br/>        """Initialize a ReplayBuffer object.<br/>        Params<br/>        ======<br/>            buffer_size: maximum size of buffer<br/>            batch_size: size of each training batch<br/>        """<br/>        self.memory = deque(maxlen=buffer_size) # internal memory (deque)<br/>        self.batch_size = batch_size<br/>        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])</span><span id="e0a8" class="lw lx it nb b gy nl ni l nj nk">def add(self, state, action, reward, next_state, done):<br/>        """Add a new experience to memory."""<br/>        e = self.experience(state, action, reward, next_state, done)<br/>        self.memory.append(e)</span><span id="68c3" class="lw lx it nb b gy nl ni l nj nk">def sample(self, batch_size=64):<br/>        """Randomly sample a batch of experiences from memory."""<br/>        return random.sample(self.memory, k=self.batch_size)</span><span id="aadf" class="lw lx it nb b gy nl ni l nj nk">def __len__(self):<br/>        """Return the current size of internal memory."""<br/>        return len(self.memory)</span></pre><h2 id="559d" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">actor.py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="d664" class="lw lx it nb b gy nh ni l nj nk">from keras import layers, models, optimizers, regularizers<br/>from keras import backend as K</span><span id="59d5" class="lw lx it nb b gy nl ni l nj nk">from keras import layers, models, optimizers, regularizers<br/>from keras import backend as K</span><span id="b3d0" class="lw lx it nb b gy nl ni l nj nk">class Actor:<br/>    """Actor (Policy) Model."""</span><span id="9511" class="lw lx it nb b gy nl ni l nj nk">def __init__(self, state_size, action_size, action_low, action_high):<br/>        """Initialize parameters and build model.<br/>        Params<br/>        ======<br/>            state_size (int): Dimension of each state<br/>            action_size (int): Dimension of each action<br/>            action_low (array): Min value of each action dimension<br/>            action_high (array): Max value of each action dimension<br/>        """<br/>        self.state_size = state_size<br/>        self.action_size = action_size<br/>        self.action_low = action_low<br/>        self.action_high = action_high<br/>        self.action_range = self.action_high - self.action_low</span><span id="f7cb" class="lw lx it nb b gy nl ni l nj nk">self.build_model()</span><span id="2f3b" class="lw lx it nb b gy nl ni l nj nk">def build_model(self):<br/>        """Build an actor (policy) network that maps states to actions."""<br/>        # Define states (input layer)<br/>        states = layers.Input(shape=(self.state_size,), name='states')<br/>        <br/>        net = layers.Dense(units=512, kernel_regularizer=regularizers.l2(0.01))(states)<br/>        net = layers.BatchNormalization()(net)<br/>        net = layers.Activation('relu')(net)<br/>        <br/>        net = layers.Dense(units=256, kernel_regularizer=regularizers.l2(0.01))(net)<br/>        net = layers.BatchNormalization()(net)<br/>        net = layers.Activation('relu')(net)<br/>        <br/>        #net = layers.Dense(units=128, kernel_regularizer=regularizers.l2(0.01))(net)<br/>        #net = layers.BatchNormalization()(net)<br/>        #net = layers.Activation('relu')(net)</span><span id="416c" class="lw lx it nb b gy nl ni l nj nk"># Add final output layer with sigmoid activation<br/>        raw_actions = layers.Dense(units=self.action_size, activation='sigmoid',name='raw_actions',<br/>                                   kernel_initializer=layers.initializers.RandomUniform(minval=-3e-3,maxval=3e-3))(net)</span><span id="5459" class="lw lx it nb b gy nl ni l nj nk"># Scale [0, 1] output for each action dimension to proper range<br/>        actions = layers.Lambda(lambda x: (x * self.action_range) + self.action_low,<br/>            name='actions')(raw_actions)</span><span id="9e7a" class="lw lx it nb b gy nl ni l nj nk"># Create Keras model<br/>        self.model = models.Model(inputs=states, outputs=actions)</span><span id="02a8" class="lw lx it nb b gy nl ni l nj nk"># Define loss function using action value (Q value) gradients<br/>        action_gradients = layers.Input(shape=(self.action_size,))<br/>        loss = K.mean(-action_gradients * actions)</span><span id="8cec" class="lw lx it nb b gy nl ni l nj nk"># Incorporate any additional losses here (e.g. from regularizers)</span><span id="236f" class="lw lx it nb b gy nl ni l nj nk"># Define optimizer and training function<br/>        optimizer = optimizers.Adam(lr=0.001)<br/>        updates_op = optimizer.get_updates(params=self.model.trainable_weights, loss=loss)<br/>        self.train_fn = K.function(<br/>            inputs=[self.model.input, action_gradients, K.learning_phase()],<br/>            outputs=[],<br/>            updates=updates_op)</span></pre><h2 id="1455" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">critic.py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="2760" class="lw lx it nb b gy nh ni l nj nk">from keras import layers, models, optimizers, regularizers<br/>from keras import backend as K</span><span id="0070" class="lw lx it nb b gy nl ni l nj nk">class Critic:<br/>    """Critic (Value) Model."""</span><span id="71a2" class="lw lx it nb b gy nl ni l nj nk">def __init__(self, state_size, action_size):<br/>        """Initialize parameters and build model.<br/>        Params<br/>        ======<br/>            state_size (int): Dimension of each state<br/>            action_size (int): Dimension of each action<br/>        """<br/>        self.state_size = state_size<br/>        self.action_size = action_size</span><span id="e214" class="lw lx it nb b gy nl ni l nj nk">self.build_model()</span><span id="bd51" class="lw lx it nb b gy nl ni l nj nk">def build_model(self):<br/>       <br/>        # Define input layers<br/>        states = layers.Input(shape=(self.state_size,), name='states')<br/>        actions = layers.Input(shape=(self.action_size,), name='actions')<br/>        <br/>        net_states = layers.Dense(units=512, kernel_regularizer=regularizers.l2(0.01))(states)<br/>        net_states = layers.BatchNormalization()(net_states)<br/>        net_states = layers.Activation('relu')(net_states)<br/>        #net_states = layers.Dropout(0.2)(net_states)<br/>        <br/>        net_states = layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(net_states)<br/>        #net_states = layers.BatchNormalization()(net_states)<br/>        #net_states = layers.Activation('relu')(net_states)</span><span id="2283" class="lw lx it nb b gy nl ni l nj nk"># Add hidden layers for action pathway<br/>       <br/>        net_actions = layers.Dense(units=256, activation='relu', kernel_regularizer=regularizers.l2(0.01))(actions)</span><span id="2e75" class="lw lx it nb b gy nl ni l nj nk"># Combine state and action pathways<br/>        net = layers.Add()([net_states, net_actions])<br/>        net = layers.Activation('relu')(net)</span><span id="030e" class="lw lx it nb b gy nl ni l nj nk"># Add final output layer to produce action values (Q values)<br/>        Q_values = layers.Dense(units=1, name='q_values',<br/>                                kernel_initializer=layers.initializers.RandomUniform(minval=-3e-3,maxval=3e-3))(net)</span><span id="019f" class="lw lx it nb b gy nl ni l nj nk"># Create Keras model<br/>        self.model = models.Model(inputs=[states, actions], outputs=Q_values)</span><span id="1db0" class="lw lx it nb b gy nl ni l nj nk"># Define optimizer and compile model for training with built-in loss function<br/>        optimizer = optimizers.Adam()<br/>        self.model.compile(optimizer=optimizer, loss='mse')</span><span id="d0e9" class="lw lx it nb b gy nl ni l nj nk"># Compute action gradients (derivative of Q values w.r.t. to actions)<br/>        action_gradients = K.gradients(Q_values, actions)</span><span id="6708" class="lw lx it nb b gy nl ni l nj nk"># Define an additional function to fetch action gradients (to be used by actor model)<br/>        self.get_action_gradients = K.function(<br/>            inputs=[*self.model.input, K.learning_phase()],<br/>            outputs=action_gradients)</span></pre><h2 id="502f" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">策略 _ 搜索. py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="c88a" class="lw lx it nb b gy nh ni l nj nk">import numpy as np<br/>from task import Task</span><span id="b5a7" class="lw lx it nb b gy nl ni l nj nk">class PolicySearch_Agent():<br/>    def __init__(self, task):<br/>        # Task (environment) information<br/>        self.task = task<br/>        self.state_size = task.state_size<br/>        self.action_size = task.action_size<br/>        self.action_low = task.action_low<br/>        self.action_high = task.action_high<br/>        self.action_range = self.action_high - self.action_low</span><span id="4bfb" class="lw lx it nb b gy nl ni l nj nk">self.w = np.random.normal(<br/>            size=(self.state_size, self.action_size),  # weights for simple linear policy: state_space x action_space<br/>            scale=(self.action_range / (2 * self.state_size))) # start producing actions in a decent range</span><span id="b0f5" class="lw lx it nb b gy nl ni l nj nk"># Score tracker and learning parameters<br/>        self.best_w = None<br/>        self.best_score = -np.inf<br/>        self.noise_scale = 0.1</span><span id="ca73" class="lw lx it nb b gy nl ni l nj nk"># Episode variables<br/>        self.reset_episode()</span><span id="b3ca" class="lw lx it nb b gy nl ni l nj nk">def reset_episode(self):<br/>        self.total_reward = 0.0<br/>        self.count = 0<br/>        state = self.task.reset()<br/>        return state</span><span id="0e55" class="lw lx it nb b gy nl ni l nj nk">def step(self, reward, done):<br/>        # Save experience / reward<br/>        self.total_reward += reward<br/>        self.count += 1</span><span id="a132" class="lw lx it nb b gy nl ni l nj nk"># Learn, if at end of episode<br/>        if done:<br/>            self.learn()</span><span id="cc45" class="lw lx it nb b gy nl ni l nj nk">def act(self, state):<br/>        # Choose action based on given state and policy<br/>        action = np.dot(state, self.w)  # simple linear policy<br/>        return action</span><span id="d8cf" class="lw lx it nb b gy nl ni l nj nk">def learn(self):<br/>        # Learn by random policy search, using a reward-based score<br/>        self.score = self.total_reward / float(self.count) if self.count else 0.0<br/>        if self.score &gt; self.best_score:<br/>            self.best_score = self.score<br/>            self.best_w = self.w<br/>            self.noise_scale = max(0.5 * self.noise_scale, 0.01)<br/>        else:<br/>            self.w = self.best_w<br/>            self.noise_scale = min(2.0 * self.noise_scale, 3.2)<br/>        self.w = self.w + self.noise_scale * np.random.normal(size=self.w.shape)  # equal noise in all directions</span></pre><h2 id="81c7" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">ou_noise.py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="ba17" class="lw lx it nb b gy nh ni l nj nk">import numpy as np<br/>from task import Task</span><span id="e239" class="lw lx it nb b gy nl ni l nj nk">class PolicySearch_Agent():<br/>    def __init__(self, task):<br/>        # Task (environment) information<br/>        self.task = task<br/>        self.state_size = task.state_size<br/>        self.action_size = task.action_size<br/>        self.action_low = task.action_low<br/>        self.action_high = task.action_high<br/>        self.action_range = self.action_high - self.action_low</span><span id="03cb" class="lw lx it nb b gy nl ni l nj nk">self.w = np.random.normal(<br/>            size=(self.state_size, self.action_size),  # weights for simple linear policy: state_space x action_space<br/>            scale=(self.action_range / (2 * self.state_size))) # start producing actions in a decent range</span><span id="23b5" class="lw lx it nb b gy nl ni l nj nk"># Score tracker and learning parameters<br/>        self.best_w = None<br/>        self.best_score = -np.inf<br/>        self.noise_scale = 0.1</span><span id="22f5" class="lw lx it nb b gy nl ni l nj nk"># Episode variables<br/>        self.reset_episode()</span><span id="998e" class="lw lx it nb b gy nl ni l nj nk">def reset_episode(self):<br/>        self.total_reward = 0.0<br/>        self.count = 0<br/>        state = self.task.reset()<br/>        return state</span><span id="aba0" class="lw lx it nb b gy nl ni l nj nk">def step(self, reward, done):<br/>        # Save experience / reward<br/>        self.total_reward += reward<br/>        self.count += 1</span><span id="97a4" class="lw lx it nb b gy nl ni l nj nk"># Learn, if at end of episode<br/>        if done:<br/>            self.learn()</span><span id="1202" class="lw lx it nb b gy nl ni l nj nk">def act(self, state):<br/>        # Choose action based on given state and policy<br/>        action = np.dot(state, self.w)  # simple linear policy<br/>        return action</span><span id="a0fd" class="lw lx it nb b gy nl ni l nj nk">def learn(self):<br/>        # Learn by random policy search, using a reward-based score<br/>        self.score = self.total_reward / float(self.count) if self.count else 0.0<br/>        if self.score &gt; self.best_score:<br/>            self.best_score = self.score<br/>            self.best_w = self.w<br/>            self.noise_scale = max(0.5 * self.noise_scale, 0.01)<br/>        else:<br/>            self.w = self.best_w<br/>            self.noise_scale = min(2.0 * self.noise_scale, 3.2)<br/>        self.w = self.w + self.noise_scale * np.random.normal(size=self.w.shape)  # equal noise in all directions</span></pre><h2 id="e2c0" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">replay _ buffer.py</h2><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="aa82" class="lw lx it nb b gy nh ni l nj nk">import random<br/>from collections import namedtuple, deque</span><span id="bf57" class="lw lx it nb b gy nl ni l nj nk">class ReplayBuffer:<br/>    """Fixed-size buffer to store experience tuples."""</span><span id="804a" class="lw lx it nb b gy nl ni l nj nk">def __init__(self, buffer_size, batch_size):<br/>        """Initialize a ReplayBuffer object.<br/>        Params<br/>        ======<br/>            buffer_size: maximum size of buffer<br/>            batch_size: size of each training batch<br/>        """<br/>        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)<br/>        self.batch_size = batch_size<br/>        self.experience = namedtuple("Experience", field_names=["state", "action", "reward", "next_state", "done"])</span><span id="7750" class="lw lx it nb b gy nl ni l nj nk">def add(self, state, action, reward, next_state, done):<br/>        """Add a new experience to memory."""<br/>        e = self.experience(state, action, reward, next_state, done)<br/>        self.memory.append(e)</span><span id="d9cb" class="lw lx it nb b gy nl ni l nj nk">def sample(self, batch_size=64):<br/>        """Randomly sample a batch of experiences from memory."""<br/>        return random.sample(self.memory, k=self.batch_size)</span><span id="3ce6" class="lw lx it nb b gy nl ni l nj nk">def __len__(self):<br/>        """Return the current size of internal memory."""<br/>        return len(self.memory)</span></pre><p id="b9ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是时候训练我们的特工了！</p><h2 id="efe4" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">我们开始吧！</h2><p id="e766" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">首先，设置您的环境</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="a490" class="lw lx it nb b gy nh ni l nj nk">conda create -n quadcop python=3.6 matplotlib numpy pandas<br/>source activate quadcop</span></pre><p id="5257" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在去你的笔记本上拿你的进口货</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="0f54" class="lw lx it nb b gy nh ni l nj nk">conda create -n quadcop python=3.6 matplotlib numpy pandas<br/>source activate quadcop</span></pre><p id="7900" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后继续看你的笔记本。从你的进口开始</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="3e87" class="lw lx it nb b gy nh ni l nj nk">import sys<br/>import pandas as pd<br/>from agents.agent import DDGP<br/>from task import Task2<br/>import csv</span><span id="f49a" class="lw lx it nb b gy nl ni l nj nk">import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="ee22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们飞吧！</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="8de7" class="lw lx it nb b gy nh ni l nj nk">num_episodes = 500<br/>target_pos = np.array([0., 0., 10.])<br/>task = Task2(target_pos=target_pos)<br/>agent = DDGP(task) <br/>best_score = -1000<br/>best_x = 0<br/>best_y = 0<br/>best_z = 0<br/>data = {}<br/>reward_log = "reward.txt"</span><span id="c988" class="lw lx it nb b gy nl ni l nj nk">reward_labels = ['episode', 'reward']<br/>reward_results = {x : [] for x in reward_labels}</span><span id="c8bf" class="lw lx it nb b gy nl ni l nj nk">for i_episode in range(1, num_episodes+1):<br/>    state = agent.reset_episode()<br/>    score = 0<br/>    <br/>    while True:<br/>        action = agent.act(state) <br/>        next_state, reward, done = task.step(action)<br/>        agent.step(action, reward, next_state, done)<br/>        state = next_state<br/>        score += reward<br/>        if score &gt; best_score:<br/>            best_x = task.sim.pose[0]<br/>            best_y = task.sim.pose[1]<br/>            best_z = task.sim.pose[2]<br/>        best_score = max(score, best_score)<br/>        data[i_episode] = {'Episode': i_episode, 'Reward':score,'Action':action,'Best_Score':best_score,<br/>                            'Position_x':task.sim.pose[0],'Position_y':task.sim.pose[1],'Position_z':task.sim.pose[2]}<br/>        if done:<br/>            print("\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), last_position = ({:5.1f},{:5.1f},{:5.1f}), best_position = ({:5.1f},{:5.1f},{:5.1f})".format(<br/>                i_episode, score, best_score, task.sim.pose[0], task.sim.pose[1], task.sim.pose[2], best_x, best_y, best_z), end="")<br/>            break<br/>    reward_results['episode'].append(i_episode)<br/>    reward_results['reward'].append(score)<br/>    sys.stdout.flush()</span><span id="e5c6" class="lw lx it nb b gy nl ni l nj nk"><strong class="nb iu">Episode =  500, score =  94.111 (best =  95.349), last_position = ( -4.6, 23.7,158.8), best_position = (-14.1, 31.0, 25.8))</strong></span></pre><p id="e5a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以这样规划奖励:</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="d858" class="lw lx it nb b gy nh ni l nj nk">plt.figure(figsize=(16,4))</span><span id="98ce" class="lw lx it nb b gy nl ni l nj nk">plt.plot(reward_results[‘episode’], reward_results[‘reward’], label=’reward/episode’, color=’indigo’)</span><span id="8e64" class="lw lx it nb b gy nl ni l nj nk">plt.title(‘Reward Results’, color=’indigo’, fontsize=18)</span><span id="d15b" class="lw lx it nb b gy nl ni l nj nk">plt.legend()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/cbdbe7d0e20d0e53b4132946c5009ccf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QleIix3Fz8iQGwEVBSS9Wg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/690a0806530e64713037a50105ab2c95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSv4skUnLp_jiT5latRLsQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://www.pexels.com/@jvdm?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Jean van der Meulen </a>from <a class="ae ky" href="https://www.pexels.com/photo/brown-and-gray-bird-1526410/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">Pexels</a></figcaption></figure><h2 id="45ec" class="lw lx it bd ly lz ma dn mb mc md dp me li mf mg mh lm mi mj mk lq ml mm mn mo bi translated">影响最大的是什么？</h2><p id="a9d2" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">首先，我们可以看看奖励函数。无人机在每集的每一步都会根据其当前位置和 x、y、z 维度的目标位置之间的差异获得-1 到 1 之间的奖励。因为最初的奖励函数没有给我们好的结果，我们切换到 tanh 奖励函数。</p><p id="3cff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从这个奖励开始:</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="c5f4" class="lw lx it nb b gy nh ni l nj nk">reward = 1-(0.3*(abs(self.sim.pose[:3] — self.target_pos))).sum()`</span></pre><p id="9c90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，在确定提供的奖励函数不令人满意之后，我们使用了 tanh 奖励函数:</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="32cd" class="lw lx it nb b gy nh ni l nj nk">np.tanh(1–0.3*(abs(self.sim.pose[:3] — self.target_pos)).sum())`</span></pre><p id="4d52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">降低“0.3”常数会得到更好的结果。经过反复试验，我决定使用:</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="9f00" class="lw lx it nb b gy nh ni l nj nk">reward = np.tanh(1–0.0005*(abs(self.sim.pose[:3] — self.target_pos)).sum())`</span></pre><p id="425a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">(除了成绩提升之外，无人机也随着这个变化学习快了很多！)</p><p id="06f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Actor 模型有两个密集层，分别包含 512 个和 256 个单元。在这两层上，我们使用了 12 个正则化，批量归一化，和<a class="ae ky" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)" rel="noopener ugc nofollow" target="_blank"> ReLU 激活函数</a>。最后一层具有四个细胞的致密层和 s 形激活函数。Adam optimizer 工作良好，学习率为 0.001。</p><p id="83cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Critic 模型类似于 Actor 模型，也有两个密集层，分别是 512 和 256 个单元。批处理规范化和 ReLU 激活函数在这里也有意义。</p><p id="c8a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据参数的随机初始化，您的代理可能会在前 20-50 集学习一项任务。但是大多数算法在 500-1000 集内学习任务。他们也有可能陷入局部极小，永远出不来(或者很久以后才出不来)。你的训练算法可能需要更长时间。这取决于你选择的学习率参数等细节。</p><p id="cb56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个代理学的很快！它的学习曲线非常陡峭。无人机获得的奖励周期性大幅增加，而不是一个渐进的学习曲线。过去 10 集的平均回报约为 93.3。</p><pre class="kj kk kl km gt nd nb ne nf aw ng bi"><span id="9165" class="lw lx it nb b gy nh ni l nj nk">print("Final Performance (Mean Reward over last 10 episodes): {}".format(np.sum(reward_results['reward'][-10:])/10))</span><span id="2efb" class="lw lx it nb b gy nl ni l nj nk"><strong class="nb iu">Final Performance (Mean Reward over last 10 episodes): 93.29302608958609</strong></span></pre><p id="2236" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我不是机器人或四轴飞行器方面的专家，这个项目很艰难！但是我在尝试把这个项目组合在一起时学到的东西是惊人的。如果你有任何兴趣，你可能想尝试一下这个项目！</p><p id="e44a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/udacity/RL-Quadcopter" rel="noopener ugc nofollow" target="_blank">如果你想看看原来的项目，你可以在这里找到</a>。这是您可以找到所有原始代码和指令的地方。</p><p id="1163" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对我的代码感兴趣，<a class="ae ky" href="https://github.com/bonn0062/quadcopter2" rel="noopener ugc nofollow" target="_blank">它在 GitHub </a>上！</p><p id="8f6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！和往常一样，如果你对这些信息做了什么很酷的事情，请在下面的评论中让所有人都知道，或者联系 LinkedIn <a class="ae ky" href="https://www.linkedin.com/in/annebonnerdata/" rel="noopener ugc nofollow" target="_blank"> @annebonnerdata </a>！</p></div></div>    
</body>
</html>