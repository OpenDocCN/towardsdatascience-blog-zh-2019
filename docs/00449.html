<html>
<head>
<title>Combining supervised learning and unsupervised learning to improve word vectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">结合监督学习和非监督学习改进词向量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b?source=collection_archive---------15-----------------------#2019-01-20">https://towardsdatascience.com/combining-supervised-learning-and-unsupervised-learning-to-improve-word-vectors-d4dea84ec36b?source=collection_archive---------15-----------------------#2019-01-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="58d6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">生成性预培训简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/87edc51832affd44e75cdad3ba13833b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zLkeT_gH6YgPaADH"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="ef48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在 NLP 任务中获得最先进结果，研究人员尝试了大量方法让机器理解语言并解决下游任务，如文本蕴涵、语义分类。OpenAI 发布了一个新的模型，命名为生成式预训练(GPT)。</p><p id="36ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看完这篇文章，你会明白:</p><ul class=""><li id="0484" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">微调变压器 LM 设计</li><li id="7fdc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">体系结构</li><li id="b6af" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实验</li><li id="716b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">履行</li><li id="56b5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">拿走</li></ul><h1 id="7c73" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">微调变压器 LM 设计</h1><p id="145c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这种方法包括两个步骤。首先，通过基于大量数据的无监督学习来训练模型。第二部分是使用目标数据集(领域数据)通过监督学习对上一步的模型进行微调。</p><h2 id="8d54" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated">无监督学习</h2><p id="6b34" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">不可否认，NLP 的无标签数据是无限的。拉德福德等人认为，利用无限的语料库有助于训练一个通用模型，就像<a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a"> word2vec </a>(单词嵌入)和<a class="ae ky" rel="noopener" target="_blank" href="/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c"> skip-thought </a>(句子嵌入)。我们不需要考虑训练数据的数量，因为我们可以很容易地获得大量的语料。</p><p id="8fe5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，仍然有一个限制。尽管我们可以尽可能多地使用语料库，但在大多数情况下，语料库与我们的领域数据是脱节的。从我以前的工作中，我注意到我的领域数据中的大部分单词并不存在于大量现成的单词嵌入模型中。</p><p id="f4cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拉德福德等人没有使用 RNN 架构，而是使用变压器架构来训练第一个模型。因为他们相信变压器架构能够捕捉更远距离的信号(语言特征)。限制是高计算时间。由于拉德福德等人使用了 12 个自我注意块和高维内部状态，即使使用 GPU 也需要几周时间来训练初始模型。</p><h2 id="a7f3" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated"><strong class="ak"> <em class="ns">监督学习</em> </strong></h2><p id="df18" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">之后，目标数据集(与之前的数据集相比，在大多数情况下，它应该是一个小数据集)将被用于通过监督学习来微调模型。</p><h1 id="12b0" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">体系结构</h1><h2 id="61bc" class="ng mk it bd ml nh ni dn mp nj nk dp mt li nl nm mv lm nn no mx lq np nq mz nr bi translated"><strong class="ak"> <em class="ns">【无监督学习模型(Transformer)】</em></strong></h2><p id="b53a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">序列对序列(又名 RNN)模型有一个局限性，我们需要定义固定长度的上下文向量，这损害了记忆很长句子的能力。与此同时，注意力机制应运而生。该架构被称为“变压器”，是多头自我关注。在注意机制家族中，我们有许多不同的注意，拉德福德等人决定使用自我注意。</p><ul class=""><li id="105f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">多头</strong>是指使用具有不同参数多个自我注意来计算表示。想想看，我们希望有多个专家来帮助找到一个更好的结果。多头机制并行执行具有不同参数相同计算。来自不同注意块的计算结果将被连接并转换到期望的维度。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/3c3bb9f99cdb5b5683dd85b30dc47f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*S3oB7KilYR7h0TCt"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@joshstyle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">JOSHUA COLEMAN</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ul class=""><li id="483b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">注意</strong>正在利用 CNN 的优势。自注意不依赖于先前的信息，因此它可以并行运行以实现更低的计算时间。同时，单词是直接计算所有已定义的单词，而不仅仅是环境。它克服了 RNN 缺乏记忆长句子能力的缺点。<strong class="lb iu">自我注意</strong>是注意机制家族的成员之一。注意力输入是 Q(查询)、K(键)和 V(值)。与其他成员不同，所有输入(Q、K 和 V)都是相等的。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/91358efeb7e88255e8cd91b4ddc283a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qNVEyJoBIE5C3adk"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@ari_spada?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ari Spada</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="aa34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于变压器的细节，你可以查看这张<a class="ae ky" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。回到架构，输入特征是文本和文本的位置来计算单词向量。文本位置是指输入的单词位置。流程是:</p><ol class=""><li id="212b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nv mb mc md bi translated">文本和位置将被转换成一个矢量</li><li id="a51a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated">传递给多头自我关注</li><li id="5450" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated">结合步骤 1 和步骤 2 的结果并执行归一化</li><li id="bf96" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated">传递到全连接的前馈网络</li><li id="7d38" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nv mb mc md bi translated">结合步骤 3 和 4 的结果并执行归一化</li></ol><p id="53d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，将多头(共 12 个自关注块)组合在一起计算向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/c31388070fc2ccb49ee522b60460662f.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*jbcwhhB8PEpJRk781rML_g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Transformer architecture (Radford et al., 2018)</figcaption></figure><p id="23b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">型号规格为:</p><ul class=""><li id="039c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">总共 12 台变压器</li><li id="f3db" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">自我注意中的 768 维状态</li><li id="5f22" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">位置前馈网络中的 3072 维内部态。</li><li id="d1c7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">使用 Adam 优化，最大学习率为 2.5e-4</li><li id="bc8d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">64 个小批量的 100 个时代</li><li id="ac2a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">辍学率为 0.1%</li></ul><p id="227c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">监督学习</em> </strong></p><p id="181d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在训练了上一步的模型后，这种受监督的微调过程有助于获得目标任务的向量。假设输入是带有标签的输入符号序列，我们可以从预先训练的模型中得到一个符号的向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/22bbfcd89af7d86d272ee992a376edcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c5MbHe6HVXVHOLT9r8szng.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Input Transformations for fine-tuning on different tasks (Radford et al., 2018)</figcaption></figure><h1 id="e825" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">实验</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/222a8949fe63f0b5700037f37b130fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkexWpqaM6_Pz_Ij_FSfZQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Experimental Result on Natural Language Inference Tasks ( Radford et al., 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/ba31fb9689f7071a380129e1a5df6ae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O4KBTwb3hVeS6R0FexHuPg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Experimental Result on Question Answering and Commonsense Reasoning Tasks ( Radford et al., 2018)</figcaption></figure><h1 id="f3ad" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">拿走</h1><ul class=""><li id="3fa3" class="lv lw it lb b lc nb lf nc li ob lm oc lq od lu ma mb mc md bi translated">展示了针对特定领域数据的微调能力。</li><li id="3bed" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>的设计与该模型相似，而 BERT 进一步改进了该模型的局限性。</li><li id="505b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">作者注意到这个架构设计没有进一步的改进。(在 github 中提到)</li></ul><h1 id="d688" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="9b17" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="25f0" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">拉德福德·a·纳拉辛汉·k·萨利曼斯·蒂姆，苏茨基弗一世..2018.<a class="ae ky" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">通过生成性预训练提高语言理解</a>。</p><p id="304a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瓦斯瓦尼 a，沙泽尔 n，帕尔马 n，乌兹科雷特 j，琼斯 L，戈麦斯 a n，凯泽 L..2017.你所需要的只是注意力。</p><p id="a53c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/openai/finetune-transformer-lm" rel="noopener ugc nofollow" target="_blank">tensor flow 中的微调变压器 LM</a>(原装)</p><p id="3cba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/huggingface/pytorch-openai-transformer-lm" rel="noopener ugc nofollow" target="_blank">py torch 中的微调变压器 LM</a></p></div></div>    
</body>
</html>