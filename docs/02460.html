<html>
<head>
<title>A deeper look at descent algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入了解下降算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-deeper-look-at-descent-algorithms-13340b82db49?source=collection_archive---------21-----------------------#2019-04-22">https://towardsdatascience.com/a-deeper-look-at-descent-algorithms-13340b82db49?source=collection_archive---------21-----------------------#2019-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1004" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">不同下降算法的概述和比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e9a068bde23d0b43b24e0875d1a9558a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*35_sXUva1Qt8UUYc"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@katekerdi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Katerina Kerdi</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2b39" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">理解本文的核心要求</h1><ul class=""><li id="8494" class="lr ls it lt b lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">线性代数</li><li id="924a" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">多变量微积分</li><li id="6d61" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">凸函数的基本思想</li></ul><p id="c609" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">众所周知，优化是机器学习中最重要的因素之一。因此，我们感兴趣的是找到一种在合理的时间内优化函数的算法。今天最常用的算法之一是梯度下降。今天我们就来看看其他的优化算法，对它们有一个理论上的了解。<br/>本文将讨论的核心算法有:</p><ul class=""><li id="3788" class="lr ls it lt b lu mq lw ms ly nd ma ne mc nf me mf mg mh mi bi translated">牛顿方法</li><li id="7bef" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">最陡下降</li><li id="c061" class="lr ls it lt b lu mj lw mk ly ml ma mm mc mn me mf mg mh mi bi translated">梯度下降</li></ul><p id="8685" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">你可以从教材<a class="ae ky" href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf#page=469" rel="noopener ugc nofollow" target="_blank">凸优化:第三部分</a>中了解更多关于这些算法的知识。在本文中，我们将主要关注二次/多项式函数</p><h1 id="9ed7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">对我们的功能所做的假设</h1><p id="926f" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">我们总是假设我们所处理的函数及其导数是连续的(即<strong class="lt iu"> <em class="nj"> f ∈ C </em> </strong>)。在牛顿法的情况下，我们还需要假设二阶导数是连续的。(即<strong class="lt iu"> <em class="nj"> f ∈ C </em> </strong>)。我们做的最后一个假设是，我们试图最小化的函数是凸的。因此，如果我们的算法收敛到一个点(通常称为局部最小值)，那么我们保证它是一个全局优化器。</p><h1 id="1eb4" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">牛顿方法</h1><h2 id="5444" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">单变量函数的算法</h2><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="2a77" class="nk la it nx b gy ob oc l od oe">x_n = starting point<br/>x_n1 = x_n - (f'(x_n)/f''(x_n))<br/>while (f(x_n) != f(x_n1)):<br/>  x_n = x_n1 <br/>  x_n1 = x_n - (f'(x_n)/f''(x_n))</span></pre><p id="0f51" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">牛顿方法背后的思想是被最小化的函数<em class="nj"> f </em>由二次函数局部逼近。然后我们找到那个二次函数的精确最小值，并把下一个点设为那个。然后我们重复这个过程。</p><h1 id="7f70" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">多元函数的情况</h1><p id="eb6c" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">到目前为止，看起来牛顿的方法是一个可靠的候选方法。但通常情况下，我们不会处理很多单变量函数。大多数时候，我们将需要优化有很多参数的函数(例如，ℝn).函数这是多变量情况下的算法:</p><p id="b409" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">假设 x∈ ℝn，我们有:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="cfe2" class="nk la it nx b gy ob oc l od oe">x_n = starting_point<br/>x_n1 = x_n - inverse(hessian_matrix) (gradient(x_n))</span><span id="d337" class="nk la it nx b gy of oc l od oe">while (f(x_n) != f(x_n1)):<br/>  x_n = x_n1<br/>  x_n1 = x_n - inverse(hessian_matrix) (gradient(x_n))</span></pre><p id="d5f0" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">其中<code class="fe og oh oi nx b">gradient(x_n)</code>是在<code class="fe og oh oi nx b">x_n</code>的梯度向量，<code class="fe og oh oi nx b">hessian_matrix</code>是一个 n×n hessian 对称矩阵，其元素由在<code class="fe og oh oi nx b">x_n</code>的二阶导数组成。<br/>众所周知，求一个矩阵的逆矩阵代价很高(<em class="nj"/><strong class="lt iu"><em class="nj">O</em>(n)</strong>)，因此这种方法并不常用。</p><h1 id="c99a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降</h1><p id="faef" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">这是迄今为止机器学习和其他近似优化中使用的最流行的优化方法。这是一种算法，涉及到在每次迭代中在梯度方向上迈出一步。它还包括一个常数α，它决定了每次迭代中要采取的步长。算法是这样的:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="0daa" class="nk la it nx b gy ob oc l od oe">alpha = small_constant<br/>x_n = starting_point<br/>x_n1 = x_n - alpha * gradient(x_n)</span><span id="a775" class="nk la it nx b gy of oc l od oe">while (f(x_n) != f(x_n1)): # May take a long time to converge<br/>  x_n = x_n1<br/>  x_n1 = x_n - alpha * gradient(x_n)</span></pre><p id="cdf1" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这里，alpha 是一个值，你必须在每次迭代中选择它来更新<code class="fe og oh oi nx b">x_n</code>(这被称为超参数)。我们将分析我们选择的α值</p><p id="a097" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">如果我们为 alpha 选择一个大的值，我们将会超出优化点。其实选的太大就可以发散。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/281f0e738201a6fc5c5f73f672ccc243.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*m333awL5rsfYzqa_pp1PNg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Gradient Descent after 10 iterations with an alpha value that is too large.</figcaption></figure><p id="c150" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">另一方面，如果我们选择α很小，那么将需要很多次迭代才能收敛到最优值。随着越来越接近最佳值，梯度趋于零。因此，如果你的阿尔法值太小，那么它可能会永远收敛到最小点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/bd99ee9efc260f7bd69f5499685b40e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*2dOg3hOWpLujQXM0I4bSNA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Gradient Descent after 10 iterations with an alpha value that is too small.</figcaption></figure><p id="515d" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">因此，如你所见，你有责任为α选择一个好的常数。然而，如果您选择了一个好的 alpha，那么您可以在每次迭代中节省很多时间</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a7825903ca4291149ac8443a08e24d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*H-EIxXGHWOqFwRix5V1YxQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Gradient Descent after 10 iterations with an alpha value that is good enough.</figcaption></figure><h1 id="5c2a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">最陡下降</h1><p id="74a9" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">最速下降非常类似于梯度下降，只是它更严格，因为每次迭代中采取的步骤都保证是最佳步骤。算法是这样工作的:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="f1d4" class="nk la it nx b gy ob oc l od oe">x_n = starting_point<br/>alpha_k = get_optimizer(f(x_n - alpha * gradient(x_n)))<br/>x_n1 = x_n - alpha_n * gradient(x_n)</span><span id="1f7e" class="nk la it nx b gy of oc l od oe">while (f(x_n) != f(x_n1)):<br/>  x_n = x_n1<!-- --> <br/>  alpha_k = get_optimizer(f(x_n - alpha * gradient(x_n)))<br/>  x_n1 = x_n - alpha_n * gradient(x_n)</span></pre><p id="b225" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">其中<code class="fe og oh oi nx b">x_n</code>和<code class="fe og oh oi nx b">x_n1</code>是ℝn 的输入向量，<code class="fe og oh oi nx b">gradient</code>是<strong class="lt iu"> <em class="nj"> f </em> </strong>在<code class="fe og oh oi nx b">x_n</code>处的梯度，<code class="fe og oh oi nx b">alpha_k</code>为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/317761d705e52715d9593b25ecf5a6cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*vTl1hrDdZfPZAYoQUp0Rog.png"/></div></figure><p id="824a" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">因此，在优化我们的原始函数时，我们需要在每次迭代中优化一个内部函数。好消息是，这个函数是一个单变量函数，这意味着它并不复杂(例如，我们可以在这里使用牛顿法)。然而，在大多数情况下，在每一步都优化这样一个函数确实有点昂贵。</p><h2 id="aa69" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">二次函数的一个特例</h2><p id="6427" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">考虑<em class="nj">平方误差损失函数</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/9d45040124c27785f3f3c70693735884.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*3qd69l2_rVKZ8u_qgwtXmQ.png"/></div></figure><p id="5a3f" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">其中<strong class="lt iu"> <em class="nj"> I </em> </strong>为单位矩阵<strong class="lt iu"> <em class="nj"> y=Qw+b </em> </strong> <br/>为简单起见，我们只考虑寻找权重 w 的最优值(假设 b 为常数)。通过代入 y 并简化一切，我们得到如下结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b956d3897c0bb3b86c53c5f1f0fad471.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*cdK3_6xCS2mo-nBNGx46fw.png"/></div></figure><p id="58fc" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">回头看一下<em class="nj"> g(α) </em>，我们知道如果我们在<strong class="lt iu"> <em class="nj"> αk </em> </strong>取梯度，由于是极小值，所以应该是 0。利用这一点，我们有以下优势:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/41664a0ecf1c11e4ed20bc850311b6a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*ON4Kk8MQ2F6S-miu5wS-Iw.png"/></div></figure><p id="c8f1" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">简化上面的混乱，代入两点处的<strong class="lt iu"> <em class="nj"> f </em> </strong>的梯度，我们得到αk 如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9b348ea9c9b592c7ea5083df35d56cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*upVhFQadIIcKmfET5WJFsg.png"/></div></figure><p id="f567" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">现在，在二次函数的情况下，我们有了αk 的具体值。</p><h2 id="c371" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">二次函数的收敛性分析</h2><p id="1895" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">以ℝ的二次函数为例，最速下降通常会在不到十步的时间内非常接近最优值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/bfb7accd3845e70bab96757112443550.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*BViOSQImhP7CB9z3ta49xQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Steepest descent in 2 dimensions after 4 iterations.</figcaption></figure><p id="d2fa" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">在上图中，请注意方向的变化在每次迭代中都是垂直的。经过 3 到 4 次迭代后，我们注意到导数的变化几乎可以忽略不计。</p><h2 id="99b1" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">为什么不用最速下降法？</h2><p id="8c41" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">那么为什么这种算法不经常使用呢？很明显，它不再需要超参数来调整，并且保证收敛到局部最小值。这里的问题是，在每次迭代中，我们需要优化 alpha_k，考虑到我们必须在每一步都这样做，这有点昂贵。</p><p id="402f" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">例如，在二次函数的情况下，我们必须在每次迭代中计算多个矩阵乘法和向量点积。相比之下，梯度下降，在每一步，我们只需要计算导数，并更新新的价值，这是更便宜的方式。最速下降法在非二次函数的情况下也很难推广，在这种情况下，α_ k 可能没有具体值</p><h1 id="883e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降法和最速下降法的比较</h1><p id="fe70" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">我们将对梯度下降和最速下降进行比较，并分析它们的时间复杂度。首先，我们将对两种算法的时间进行比较。我们将创建一个二次函数<strong class="lt iu"> <em class="nj"> f:ℝ ⁰⁰⁰→ℝ </em> </strong>(涉及一个 2000×2000 的矩阵)。然后，我们将优化函数，并将迭代次数限制为 1000 次。然后，我们将比较两种算法所用的时间以及 x_n 值与优化器的接近程度。</p><p id="bf04" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">让我们先来看看最速下降:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="ff8a" class="nk la it nx b gy ob oc l od oe">0   Diff: 117727672.56583363 alpha value: 8.032725864804974e-06 <br/>100 Diff: 9264.791000127792 alpha value: 1.0176428564615889e-05 <br/>200 Diff: 1641.154644548893 alpha value: 1.0236993350903281e-05 <br/>300 Diff: 590.5089467763901 alpha value: 1.0254560482036439e-05 <br/>400 Diff: 279.2355946302414 alpha value: 1.0263893422517941e-05 <br/>500 Diff: 155.43169915676117 alpha value: 1.0270028681773919e-05 <br/>600 Diff: 96.61812579631805 alpha value: 1.0274280663010468e-05 <br/>700 Diff: 64.87719237804413 alpha value: 1.027728512597358e-05 <br/>800 Diff: 46.03102707862854 alpha value: 1.0279461929697766e-05 <br/>900 Diff: 34.00975978374481 alpha value: 1.0281092917213468e-05 <br/>Optimizer found with x = [-1.68825261  5.31853629 -3.45322318 ...  1.59365232 -2.85114689   5.04026352] and f(x)=-511573479.5792374 in 1000 iterations<br/>Total time taken: 1min 28s</span></pre><p id="369e" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这是渐变下降的输出，alpha = 0.000001</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="92cc" class="nk la it nx b gy ob oc l od oe">0   Diff: 26206321.312622845 alpha value: 1e-06 <br/>100 Diff: 112613.38076114655 alpha value: 1e-06 <br/>200 Diff: 21639.659786581993 alpha value: 1e-06 <br/>300 Diff: 7891.810685873032 alpha value: 1e-06 <br/>400 Diff: 3793.90934664011 alpha value: 1e-06 <br/>500 Diff: 2143.767760157585 alpha value: 1e-06 <br/>600 Diff: 1348.4947955012321 alpha value: 1e-06 <br/>700 Diff: 914.9099299907684 alpha value: 1e-06 <br/>800 Diff: 655.9336211681366 alpha value: 1e-06 <br/>900 Diff: 490.05882585048676 alpha value: 1e-06 <br/>Optimizer found with x = [-1.80862488  4.66644055 -3.08228401 ...  2.46891076 -2.57581774   5.34672724] and f(x)=-511336392.26658595 in 1000 iterations<br/>Total time taken: 1min 16s</span></pre><p id="d296" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">正如你所看到的，梯度下降往往更快，虽然不是很多(几秒或几分钟)。但更重要的是，虽然α的值没有被选为梯度下降中的最佳参数，但最速下降比梯度下降采取的步骤要好得多。在上例中，第 900 个梯度下降的<strong class="lt iu"> <em class="nj"> f(xprev) </em> </strong>和<strong class="lt iu"> <em class="nj"> f(xcurr) </em> </strong>之差为 450。这种差异在最陡下降中很早就过去了(大约在迭代 300 和 400 之间)。</p><p id="74fb" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">因此，如果我们只考虑最速下降的 300 次迭代，我们得到如下结果:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="60dc" class="nk la it nx b gy ob oc l od oe">0   Diff: 118618752.30065191 alpha value: 8.569151292666038e-06 <br/>100 Diff: 8281.239207088947 alpha value: 1.1021416896567156e-05 <br/>200 Diff: 1463.1741587519646 alpha value: 1.1087402059869253e-05 <br/>300 Diff: 526.3014997839928 alpha value: 1.1106776689082503e-05 Optimizer found with x = [-1.33362899  5.89337889 -3.31827817 ...  1.77032789 -2.86779156   4.56444743] and f(x)=-511526291.3367646 in 400 iterations<br/>Time taken: 35.8s</span></pre><p id="c730" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">因此，最陡下降实际上更快。这只是表明，如果你想接近最优，你真的需要每次迭代更少的步骤。事实上，如果您的目标是逼近最优值，那么最速下降法只需 10 步就能生成更接近最优值的小维函数，而梯度下降法需要 1000 步！</p><p id="eeb2" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这里有一个例子，我们有一个来自ℝ ⁰→ℝ.的二次函数 10 步，最陡下降产生<code class="fe og oh oi nx b">f(x) = -62434.18</code>。1000 步内，梯度下降生成<code class="fe og oh oi nx b">f(x) = -61596.84</code>。在短短 10 步中，最速下降降低到一个<strong class="lt iu"><em class="nj">f</em></strong>-比 1000 步中的梯度下降值还低！</p><p id="4539" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">请记住，上面的工作真的很好，只是因为我们正在处理二次函数。一般来说，每次迭代都很难找到αk 的值。优化 g(α)并不总是让你找到αk 的具体值，通常我们倾向于使用迭代算法来最小化这样的函数。在这种情况下，事情变得繁琐，比梯度下降慢得多。这就是为什么最速下降不那么受欢迎。</p><h1 id="4cf3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="a8bf" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">总之，我们学习了三种算法:</p><h2 id="6ae1" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">牛顿方法</h2><p id="8a0a" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">牛顿的方法提供了一个函数的二次近似，并在每一步进行优化。最大的缺点是，它涉及到对多变量情况下的矩阵求逆(当处理具有许多特征的向量时，这可能是昂贵的)</p><h2 id="4f7b" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">梯度下降</h2><p id="8777" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">梯度下降是最常见的优化算法。它非常快，因为每一步最昂贵的事情就是计算导数。然而，它确实涉及到“猜测”或“调整”一个告诉你每一步应该走多远的超参数。</p><h2 id="8929" class="nk la it bd lb nl nm dn lf nn no dp lj ly np nq ll ma nr ns ln mc nt nu lp nv bi translated">最陡下降</h2><p id="98a8" class="pw-post-body-paragraph mo mp it lt b lu lv ju mr lw lx jx mt ly ng mv mw ma nh my mz mc ni nb nc me im bi translated">最速下降算法是在给定函数梯度向量的情况下找到最佳步长的算法。唯一的问题是，它涉及到在每次迭代中优化一个函数，这通常是昂贵的。在二次函数的情况下，最速下降通常表现良好，尽管它确实涉及每步大量的矩阵计算。</p><p id="8f81" class="pw-post-body-paragraph mo mp it lt b lu mq ju mr lw ms jx mt ly mu mv mw ma mx my mz mc na nb nc me im bi translated">这篇文章的笔记本版本可以在<a class="ae ky" href="https://colab.research.google.com/gist/nasirhemed/0026e5e6994d546b4debed8f1ed543c0/a-deeper-look-into-descent-algorithms.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p></div></div>    
</body>
</html>