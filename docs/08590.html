<html>
<head>
<title>Python Predicts PUBG Mobile</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python 预测 PUBG 移动</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/python-predicts-pubg-mobile-6876720b978c?source=collection_archive---------41-----------------------#2019-11-19">https://towardsdatascience.com/python-predicts-pubg-mobile-6876720b978c?source=collection_archive---------41-----------------------#2019-11-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="ab5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用 Python 预测视频数据中未来帧的简单方法</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/13c5371ef28defd43f689a2f4a1e6473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LnaIcbXQ-8tLfYrxl9taiw.jpeg"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">My PUBG character :D</figcaption></figure><h1 id="af34" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated"><strong class="ak">简介:</strong></h1><p id="e2b6" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">预测未来是不可能的！(<em class="mh">除非你有</em> <a class="ae mi" href="https://en.wikipedia.org/wiki/Infinity_Gems" rel="noopener ugc nofollow" target="_blank"> <em class="mh">时间石</em> </a> <em class="mh"> -:) </em>)。但是预测不远的将来对我们来说并不难。我们在现实生活中经常这样做——在玩游戏或看电影时，人们可以很容易地预测即将发生的事情。我们大脑的这种能力有助于我们提前计划行动和做出决定。(例如，在空中接球，躲避迎面而来的石头……等等)。</p><p id="f054" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们不能躲避子弹，因为我们不够快。如果我们能把大脑的这种能力给愚蠢的机器，然后造一个<a class="ae mi" href="https://en.wikipedia.org/wiki/Ultron" rel="noopener ugc nofollow" target="_blank">奥创</a> -:)会怎么样。</p><p id="3618" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">问题是——机器真的能做到吗？</p><p id="de96" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">答案是肯定的。这个小实验证明了这一点。这也是一个活跃的研究领域——给定一个小视频，你能生成未来的视频吗？这是这个实验产生的数据的一瞥—</p><blockquote class="mj mk ml"><p id="b315" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">下面的视频完全是人工的，由深度学习模型生成。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mp"><img src="../Images/f8ab3f679b041b482276dbeae9aee77d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*POSNEGn-iZ1rUJqSW91sIg.gif"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk"><strong class="bd mq">Artificially Generated Video</strong></figcaption></figure><blockquote class="mj mk ml"><p id="3b71" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">如何在我的电脑上完成？</p></blockquote><p id="1152" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">只需几个小时，你就可以在自己的电脑上完成。也不需要 GPU。</p><p id="2626" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果你是计算机视觉和深度学习的新手，我建议你在进一步阅读之前对以下主题有一个基本的了解—</p><ol class=""><li id="866d" class="mr ms it js b jt ju jx jy kb mt kf mu kj mv kn mw mx my mz bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/OpenCV" rel="noopener ugc nofollow" target="_blank">打开简历</a></li><li id="b8f6" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a></li><li id="8e25" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated"><a class="ae mi" href="https://en.wikipedia.org/wiki/Autoencoder" rel="noopener ugc nofollow" target="_blank">自动编码器</a></li></ol><p id="eafa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们把整件事分成五个小部分。我们将一个接一个地检查每一部分——</p><ol class=""><li id="cb07" class="mr ms it js b jt ju jx jy kb mt kf mu kj mv kn mw mx my mz bi translated"><strong class="js iu">数据生成</strong></li><li id="b185" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated"><strong class="js iu">数据准备</strong></li><li id="b58a" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated"><strong class="js iu">型号</strong></li><li id="99d4" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated"><strong class="js iu">结果</strong></li><li id="a646" class="mr ms it js b jt na jx nb kb nc kf nd kj ne kn mw mx my mz bi translated"><strong class="js iu">结论</strong></li></ol></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="4bf0" class="le lf it bd lg lh nm lj lk ll nn ln lo lp no lr ls lt np lv lw lx nq lz ma mb bi translated"><strong class="ak">数据生成:</strong></h1><p id="5ac4" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">生成数据非常简单。在我的移动设备上的屏幕录制器的帮助下，我能够捕捉到一个 15 分钟长的视频(事实证明这对我们的小实验来说是足够的数据)。在这个视频中，我将我的 PUBG 角色设置为冲刺模式，让它在随机方向上连续跑大约 15 分钟。</p><p id="1dfa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我有了这个视频，我需要做的就是按固定的时间间隔将它切割成多个帧。使用<code class="fe nr ns nt nu b">OpenCV toolkit</code>，这种视频到帧的转换非常简单快捷。这些帧相距大约<code class="fe nr ns nt nu b">~25ms</code>，这使得它成为<code class="fe nr ns nt nu b"><strong class="js iu">40fps (frames per second)</strong></code>。两个连续帧之间的差异很小，但相当明显。</p><blockquote class="mj mk ml"><p id="cc9c" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">这是一个简单的 python 代码，它将视频转换为帧。(*不同机器上的帧速率可能有所不同，因为它取决于您系统的处理速度。)</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/48da5c4f1fdceea0fa4604fe481a4c63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vB-y43xA-V-LAIWF0EMTzg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Video to Frame conversion</figcaption></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="5453" class="le lf it bd lg lh nm lj lk ll nn ln lo lp no lr ls lt np lv lw lx nq lz ma mb bi translated"><strong class="ak">数据准备:</strong></h1><p id="119f" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">之前的练习给了我们大约 30k 帧。在这 30k 帧中，前 29k 帧作为训练数据，其余 1k 帧作为验证数据。</p><p id="0309" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这些相框的尺寸真的很大<code class="fe nr ns nt nu b">800 * 1200</code>。为了使模型简单快速，在将每一帧传递给模型进行训练之前，将每一帧的大小调整为<code class="fe nr ns nt nu b">240 * 160</code>。</p><blockquote class="mj mk ml"><p id="4a52" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">下面是一个用于数据预处理的简单 python 函数。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/c316da32ff2597f628c3580e74a2f945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cc5qjbljqIPI0scZo7tGAA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Preprocessing function</figcaption></figure><blockquote class="mj mk ml"><p id="fc10" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">这是处理过的图像的样子——</p><p id="973e" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">在 1 之后。裁剪图像，仅保留图像的相关部分。</p><p id="45e3" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">第二年。将图像调整为较小的尺寸(240*160)，因为原始图像非常大。(调整图像大小时，<a class="ae mi" href="https://en.wikipedia.org/wiki/Aspect_ratio_(image)" rel="noopener ugc nofollow" target="_blank">纵横比</a>保持不变)。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/3fa90ee4b1670866b1de4d3979322423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UTZfXBxgnYvLJG495h1aTw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Image preprocessing</figcaption></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="3c77" class="le lf it bd lg lh nm lj lk ll nn ln lo lp no lr ls lt np lv lw lx nq lz ma mb bi translated"><strong class="ak">型号:</strong></h1><ol class=""><li id="9419" class="mr ms it js b jt mc jx md kb ny kf nz kj oa kn mw mx my mz bi translated"><strong class="js iu">训练数据:</strong></li></ol><p id="1115" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">来自先前练习的预处理帧成对排列，使得每对帧<code class="fe nr ns nt nu b">(frame_x, frame_y) </code>具有两个连续的帧——例如，如果<code class="fe nr ns nt nu b">frame_x</code>出现在视频中的<code class="fe nr ns nt nu b">n’th</code>位置，那么<code class="fe nr ns nt nu b">frame_y</code>应该来自<code class="fe nr ns nt nu b">(n+1)’th</code>位置。</p><p id="534f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以这种方式，我们为训练数据保留的那些 29k 帧可以构成<code class="fe nr ns nt nu b">29k — 1</code>这样的训练数据对。类似地，剩余的 1k 验证帧可以构成 1k 验证数据对。</p><p id="63c8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2。模型架构:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/b10b58cdd9eafdfca1182a7f688d01b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlSJO2PT6Vgk2s-ZU5zpDw.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Encoder — Decoder Architecture</figcaption></figure><p id="e2f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在该实验中使用了简单的<a class="ae mi" href="https://en.wikipedia.org/wiki/Autoencoder" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">编码器-解码器</strong> </a>模型架构。这里的<code class="fe nr ns nt nu b">encoder</code>部分模型是一个三层的<code class="fe nr ns nt nu b">2D CNN (convolutional neural network)</code>和<code class="fe nr ns nt nu b">MaxPooling</code>层。该模型的<code class="fe nr ns nt nu b">decoder</code>部分也是<code class="fe nr ns nt nu b">2D CNN with UpSampling layers</code>或(转置卷积神经网络)。卷积层的核大小保持不变，以便我们在解码后得到相同大小的图像。</p><blockquote class="mj mk ml"><p id="5b18" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">下面是使用 tensorflow 的 keras api 实现的模型的架构。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oc"><img src="../Images/f4164746ed3492bc5587055b87747861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPvc4xnKqtvIT6icSSGyTg.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Model Architecture</figcaption></figure><p id="127d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3。模型训练:</strong></p><p id="d547" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，对于每个训练对，模型将<code class="fe nr ns nt nu b">frame_x</code>作为输入，并学习猜测<code class="fe nr ns nt nu b">frame_y</code>(下一个直接帧)。</p><p id="58d3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">采用梯度下降算法训练模型，以<code class="fe nr ns nt nu b">mean squared error</code>为损失函数……</p><pre class="kp kq kr ks gt od nu oe of aw og bi"><span id="153e" class="oh lf it nu b gy oi oj l ok ol"><strong class="nu iu">--- Model Parameters ---</strong><br/>batch_size    : 32<br/>epochs        : 10<br/>optimizer     : 'adam'<br/>learning_rate : 0.001<br/>loss          : 'mse'</span></pre><p id="c7e6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在仅仅训练<code class="fe nr ns nt nu b">10 epochs</code>之后，学习的权重被保存用于推断。</p><pre class="kp kq kr ks gt od nu oe of aw og bi"><span id="37a7" class="oh lf it nu b gy oi oj l ok ol">Training time : approximately 8 hours (on CPU)<br/>Hardware      : Macbook Pro, 16GB, 2.6 GHz, Intel Core i7</span></pre><blockquote class="mj mk ml"><p id="6ef1" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">培训和验证损失图</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/d8b477e15667d5a56a901bbff2a8c494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zN8c3kmw2a0zTscI5vx86A.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Loss Chart</figcaption></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="ae64" class="le lf it bd lg lh nm lj lk ll nn ln lo lp no lr ls lt np lv lw lx nq lz ma mb bi translated"><strong class="ak">结果:</strong></h1><p id="3d6a" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">为了检查模型在看不见的数据上的性能，从验证集中选取一个随机帧。现在，使用这个单一的帧，我们可以通过将预测的帧一次又一次地传递给模型作为输入来生成任意数量的未来帧。</p><p id="da94" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型能够从单个输入帧生成前 10-12 个未来帧，具有相当高的精度。在第 15 帧之后，它变得非常嘈杂，因为预测误差在每次新的预测时都会增加。需要注意的一件重要事情是<code class="fe nr ns nt nu b">static parts of the frames (my PUBG control buttons) are intact as model is able to learn what is static and what is changing</code>这部分也不会模糊。</p><blockquote class="mj mk ml"><p id="a059" class="jq jr mh js b jt ju jv jw jx jy jz ka mm kc kd ke mn kg kh ki mo kk kl km kn im bi translated">下面是由模型生成的两个示例图像序列，以及顶部的地面真实图像。</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/195e7d896ad7e74bb31009535732f793.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Im9WfYon5eFY91Sd0PjfZA.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Image sequences generated by model</figcaption></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><h1 id="28c5" class="le lf it bd lg lh nm lj lk ll nn ln lo lp no lr ls lt np lv lw lx nq lz ma mb bi translated"><strong class="ak">结论:</strong></h1><p id="edcd" class="pw-post-body-paragraph jq jr it js b jt mc jv jw jx md jz ka kb me kd ke kf mf kh ki kj mg kl km kn im bi translated">有趣的是，这个模型能够很容易地用一张图像生成这么多未来的图像。这很简单，因为这个实验是受控的 PUBG 视频中的人只进行一项活动(总是跑步)。因此，模型只需要学习他的运动以及背景如何随时间变化。</p><p id="3c31" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">预测真实的生活场景并不容易。这也是一个活跃的研究领域。这很难，因为在现实世界中有无限的可能性。多个对象可以同时改变环境。为了对这样的场景建模，我们需要一个更好、更强大的模型架构。还有大量高质量的真实世界数据。</p><p id="56b8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢<a class="oo op ep" href="https://medium.com/u/dff4008c1908?source=post_page-----6876720b978c--------------------------------" rel="noopener" target="_blank"> Raghav Bali </a>和<a class="oo op ep" href="https://medium.com/u/6278d12b0682?source=post_page-----6876720b978c--------------------------------" rel="noopener" target="_blank"> Dipanjan (DJ) Sarkar </a>的点评。</p><p id="40ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">请与我分享您的意见/反馈。</p></div></div>    
</body>
</html>