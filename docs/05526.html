<html>
<head>
<title>Striking a Balance between Exploring and Exploiting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在探索和开发之间取得平衡</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/striking-a-balance-between-exploring-and-exploiting-5475d9c1e66e?source=collection_archive---------24-----------------------#2019-08-14">https://towardsdatascience.com/striking-a-balance-between-exploring-and-exploiting-5475d9c1e66e?source=collection_archive---------24-----------------------#2019-08-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b795" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">强化学习中探索与利用的困境</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6064ebc02ac71a72f1675826fba48948.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0nYRM7oJ7Q1mRfRZA9FZbw.png"/></div></div></figure><p id="053e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的代理在学习玩井字游戏[ <a class="ae lq" rel="noopener" target="_blank" href="/reinforcement-learning-value-function-57b04e911152"> Medium article </a> ]时面临的探索-开发困境。这种困境是强化学习以及现实生活中的一个基本问题，当我们在选项之间做出选择时，你会选择:</p><ul class=""><li id="06e2" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">挑选你熟悉的东西，以最大化获得你想要的东西的机会</li><li id="f39d" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">或者选择一些你没有尝试过的东西，可能会学到更多，这可能会(也可能不会)让你在未来做出更好的决定</li></ul><p id="d313" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这种权衡将会影响你是尽快获得奖励，还是先了解环境再获得奖励。</p><h1 id="f70b" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">直觉</h1><p id="4a9f" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">假设你搬到了一个新城镇，你正在考虑从你的办公室到你的新家该走哪条路。你快速搜索了一下，发现有几种方法:</p><ol class=""><li id="2a11" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp nc lx ly lz bi translated">坐地铁 A 线，跟着 B 线</li><li id="f74a" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nc lx ly lz bi translated">乘地铁 A 线，然后乘 123 路公共汽车</li><li id="276b" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp nc lx ly lz bi translated">步行到地铁 C 线，然后是 B 线</li></ol><p id="c9fb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最初，您不知道哪种方式是最好的，但是您信任技术，所以选择了选项 1，因为它的持续时间最短。你设法在第一天到家，所以你决定选择 1 作为你的回家路线。你继续<em class="nd">利用</em>你所知道的有用的东西，并通过选项 1 旅行。</p><p id="9a7d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">也许有一天，你决定去<em class="nd">探索</em>，认为从办公室回家可能有更好的方式，所以你决定尝试选项 2。结果可能是你设法在更短的时间内回到家，或者花了你更多的时间(也许还会花更多的钱)。在尝试了选项 2 之后，确实比选项 1 花了更长的时间。可能是因为在高峰时段，交通对 123 路公交车不好。所以你决定选择 2 不好。但这是否意味着你再也不会尝试第二种选择了呢？在非高峰时间，这可能是一个更好的选择。</p><p id="b36f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果选项 3 实际上是回家的最佳方式，但决定不再探索而坚持选项 1，该怎么办？那么你将永远不会意识到选项 3 比选项 1 更好。</p><h1 id="f6ae" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">取得平衡</h1><p id="59f9" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">学习做出这些选择是强化学习的一部分。这意味着有时你不得不故意决定不选择你认为最有回报的行动，以便获得新的信息，但有时在探索的过程中最终会做出一些糟糕的决定。但与此同时，你想通过<em class="nd">利用</em>你知道最有效的方法来最大化你的回报。</p><p id="ff2d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，我们如何在充分探索未知和利用最佳行动之间取得平衡呢？</p><ul class=""><li id="28a7" class="lr ls it kw b kx ky la lb ld lt lh lu ll lv lp lw lx ly lz bi translated">充分的初步探索，以便确定最佳方案</li><li id="7304" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">利用最佳选择以最大化总回报</li><li id="4f9f" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">继续留出一个小概率来试验次优和未开发的选项，以防它们在未来提供更好的回报</li><li id="5196" class="lr ls it kw b kx ma la mb ld mc lh md ll me lp lw lx ly lz bi translated">如果这些实验选项表现良好，算法必须更新并开始选择这个选项</li></ul><h1 id="e9af" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">ε贪婪</h1><p id="75d8" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">在强化学习中，我们可以决定要做多少探索。这是ε贪婪参数，范围从 0 到 1，它是探测的概率，通常在 5%到 10%之间。如果我们设置 0.1ε-greedy，该算法将在 10%的时间里探索随机选项，在 90%的时间里利用最佳选项。</p><h1 id="f7ae" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">评估不同的ε贪婪</h1><p id="4beb" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">我已经创建了一个<a class="ae lq" href="https://jinglescode.github.io/reinforcement-learning-tic-tac-toe/" rel="noopener ugc nofollow" target="_blank">井字游戏</a>，代理可以通过相互对抗来学习游戏。首先，让我向你介绍我们的代理人，他们是代理人 X 和代理人 o。代理人 X 总是先走，实际上比代理人 o 更有优势赢得更多的机会。</p><p id="c82b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了找出最适合这个游戏中每个代理的ε贪婪值，我将测试不同的ε贪婪值。首先，用ε贪婪值 0.01 初始化代理 X，意味着有 1%的概率代理 X 会选择<em class="nd">探索</em>而不是<em class="nd">利用</em>。然后，特工们互相对战 10000 场，我记录下 X 特工获胜的次数。在 10，000 场游戏之后，我将ε-greedy 值增加到 0.02，代理将再玩 10，000 场游戏。</p><p id="8fc9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">代理 X (eps 增量)vs 代理 O (eps 0.05)，结果如下:</p><div class="kj kk kl km gt ab cb"><figure class="ne kn nf ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/d9e2fd1f61b7bd4a4527ac77af680383.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*PxV3bjZemcrr0WeFQzSpqg.png"/></div></figure><figure class="ne kn nk ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/18ea62c7ddebb783192c5fb1fe00365f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*W0UNrYFBu_t-5zMxOWg2YA.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk np di nq nr">Number of games (out of 10,000) won by agent X on different epsilon-greedy value</figcaption></figure></div><p id="7dfe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">蓝线是代理 X 赢代理 o 的次数。胜率随着ε-贪婪值的增加而降低，并在ε-贪婪值为 0.05 时达到赢得 9268 场游戏的峰值(代理 X 探索 5%的时间)。特工 O 开始赢得更多的游戏，因为特工 X 有超过 50%的时间在探索。</p><p id="3381" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们尝试一下，代理 O 用最优的 5%ε贪婪挑战代理 X，让我们看看代理 O 在不同ε贪婪值下的表现。</p><div class="kj kk kl km gt ab cb"><figure class="ne kn ns ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/a5f8ab846438105994971d1b59d9a7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*0kPQmSH7jttqIdurvhvpJw.png"/></div></figure><figure class="ne kn nt ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/790c045baaa53b6135518c73400a50fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*BI0y0J3IRJgD4LydK4LnSQ.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk np di nq nr">Number of games won by agent O on different epsilon-greedy value</figcaption></figure></div><p id="c66b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">给定代理 X 具有最优ε贪婪并且在游戏中首先开始的优势，代理 O 在它能够学习游戏之前输掉了大部分游戏。</p><p id="093a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们把代理人 X 的 epsilon greedy 调整到 100%，代理人 X 将一直玩随机动作。</p><div class="kj kk kl km gt ab cb"><figure class="ne kn nu ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/7c5ac0e0bc08acfd5e889973633b6c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*QcyHEpQgwSep_eJq4dtboQ.png"/></div></figure><figure class="ne kn nv ng nh ni nj paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4c3c2c697f4ae175fedb3ebf510836c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*G34HXWdvN7x6ef7oA6d8dw.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk nw di nx nr">Number of games won by agent O on different epsilon-greedy value, where agent X play randomly</figcaption></figure></div><p id="96c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">代理人 O 能够学习这个游戏并战胜代理人 X，在 4% epsilon greedy 时达到顶峰，在 30%时开始失败。</p><h1 id="eb5a" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">最后</h1><p id="9d87" class="pw-post-body-paragraph ku kv it kw b kx mx ju kz la my jx lc ld mz lf lg lh na lj lk ll nb ln lo lp im bi translated">探索在线演示并在井字游戏中挑战我们的代理。</p><p id="3a99" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">只想以这张图结束。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/64a1210ae1b8e508481d6f39319f9126.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*ZxkW_KFFfBJSI-0hR9EkVQ.jpeg"/></div></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><div class="kj kk kl km gt og"><a rel="noopener follow" target="_blank" href="/data-scientist-the-dirtiest-job-of-the-21st-century-7f0c8215e845"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">数据科学家:21 世纪最肮脏的工作</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">40%的吸尘器，40%的看门人，20%的算命师。</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks og"/></div></div></a></div><p id="11c6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嗨！，我是叮当。我喜欢构建机器学习项目/产品，我在<a class="ae lq" href="https://towardsdatascience.com/@jinglesnote" rel="noopener" target="_blank">向数据科学</a>写关于它们的文章。在<a class="ae lq" href="https://medium.com/@jinglesnote" rel="noopener">媒体</a>上关注我或者在<a class="ae lq" href="https://www.linkedin.com/in/jingles/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p><div class="kj kk kl km gt ab cb"><figure class="ne kn ov ng nh ni nj paragraph-image"><a href="https://towardsdatascience.com/@jinglesnote"><img src="../Images/37b3c6bbeb134b28c91c9382ab95f170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*i2NzU4j49rZ36Mxz4gp4Sg.png"/></a></figure><figure class="ne kn ov ng nh ni nj paragraph-image"><a href="https://jingles.substack.com/subscribe"><img src="../Images/2e369202b596d5b518dca78b96ab5f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*oENDSDMTwXi2CJdO1gryug.png"/></a></figure></div></div></div>    
</body>
</html>