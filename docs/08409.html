<html>
<head>
<title>Feature Engineering Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征工程技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-engineering-techniques-9a57e4901545?source=collection_archive---------12-----------------------#2019-11-15">https://towardsdatascience.com/feature-engineering-techniques-9a57e4901545?source=collection_archive---------12-----------------------#2019-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c01e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍一些可用于为机器学习分析准备原始特征的主要技术。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8393166cc8f07e4b67fd0d44a6fc4ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qppSymXYGZ7PxA24"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@bamagal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">"My Life Through A Lens"</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="kz la l"/></div></figure><h1 id="9989" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">介绍</h1><p id="87bc" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">在开始机器学习分析之前，特征工程是最重要的步骤之一。创建尽可能最好的机器学习/深度学习模型当然有助于实现良好的结果，但选择正确格式的正确功能来输入模型可以大大提高性能，从而带来以下好处:</p><ul class=""><li id="a48f" class="mp mq it lv b lw mr lz ms mc mt mg mu mk mv mo mw mx my mz bi translated">使我们能够使用更简单的机器学习模型来实现良好的模型性能。</li><li id="277f" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated">使用更简单的机器学习模型，增加了我们模型的透明度，从而使我们更容易理解如何进行预测。</li><li id="db78" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated">减少使用集成学习技术的需求。</li><li id="a212" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated">减少执行<a class="ae ky" rel="noopener" target="_blank" href="/hyperparameters-optimization-526348bb8e2d">超参数优化</a>的需求。</li></ul><p id="bd88" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">为了充分利用给定的数据，可以使用的其他常用技术是<a class="ae ky" rel="noopener" target="_blank" href="/feature-selection-techniques-1bfab5fe0784">特征选择</a>和<a class="ae ky" rel="noopener" target="_blank" href="/feature-extraction-techniques-d619b56e31be">提取</a>，我在以前的帖子中已经谈到过。</p><p id="03a8" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">我们现在将介绍一些最常见的特征工程技术。大多数基本特征工程技术包括<strong class="lv iu">发现数据中的不一致</strong>和<strong class="lv iu">通过组合/分割现有特征来创建新特征</strong>。</p><p id="e062" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">本文使用的所有代码都可以在我的 GitHub 账户上的<a class="ae ky" href="https://github.com/pierpaolo28/Artificial-Intelligence-Projects/blob/master/Features%20Analysis/FeatureEngineering.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>中找到。</p><p id="150e" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">对于这个示例，我决定创建一个简单的数据集，它受到数据分析过程中面临的一些最常见问题的影响(例如，缺失数字、异常值、缩放问题等)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/020211831518d455c7a50f21d4c3cb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*MUn1-8Jg8AWF4ounzdPVfw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1: Dataset Head</figcaption></figure><h2 id="6a4a" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">对数变换</h2><p id="48de" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">使用对数变换时，原始要素的分布被变换为更接近高斯分布。这可能特别有用，尤其是当使用机器学习模型时，例如线性判别分析(LDA)和朴素贝叶斯分类器，它们假设它们的输入数据遵循高斯分布。</p><p id="6bb2" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">在本例中，我将对数据集中所有可用的数字要素应用对数变换。此外，我还决定用各自的最小值减去原始特征，然后将它们加到一起，以确保这些列中的每个元素都是正的(对数只支持正值)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5b473d207e8f8f99232e7fa8f0e5b2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*j8LVuDLzk70Ss003dwIuRg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2: Logarithmically Transformed Dataset</figcaption></figure><h2 id="d8c3" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">归罪</h2><p id="2c0e" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">插补是使用适当的值识别和替换数据集中缺失值的艺术。数据集中缺失值的存在可能是由许多可能的因素造成的，例如:隐私问题、使用传感器记录数据时的技术故障、人为错误等</p><p id="cdbd" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">有两种主要的插补类型:</p><ul class=""><li id="b7ed" class="mp mq it lv b lw mr lz ms mc mt mg mu mk mv mo mw mx my mz bi translated"><strong class="lv iu">数字插补</strong>:数字特征中缺失的数字可以使用许多不同的技术进行插补。使用的一些主要方法是用受影响列的总体平均值或模式替换缺失值。如果你有兴趣了解更多先进技术，你可以在这里找到更多信息<a class="ae ky" rel="noopener" target="_blank" href="/why-using-a-mean-for-missing-data-is-a-bad-idea-alternative-imputation-algorithms-837c731c1008">。</a></li><li id="1d6c" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><strong class="lv iu">分类插补</strong>:对于分类特征，缺失值通常使用整体列模式替换。在某些特殊情况下，如果分类列结构没有很好地定义，那么最好替换丢失的值，创建一个新的类别，并将其命名为“未知”或“其他”。</li></ul><p id="f264" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">我们现在可以，首先，通过运行下面几行来检查哪些特性受到 NaNs(不是一个数字)的影响。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e0c3355dde42639329ac8e64bebf67dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*qDUYyoU3JgYg6_bJuPCSlQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 3: Percentage of NaNs in each Feature</figcaption></figure><p id="8f21" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">处理缺失数字的最简单方法之一是删除受其影响的所有行。不过，最好设置一个阈值(例如 20%)，只删除缺失数超过阈值的行。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/0329f4e426504f0572f8f939ef38bbeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*hFpdFcdFux5v8vk3SJzzHQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 4: Imputation by deleting features with excessive NaNs</figcaption></figure><p id="ba84" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">另一个可能的解决方案是，对于我们的数值和分类数据，用列模式替换所有 nan。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/2ef2a0876ba5f9bf36e6265bc72c7c51.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*gxze2yfVe6n_vWEHZ-yh9g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 5: Imputation using column mode</figcaption></figure><h2 id="95a7" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">处理日期</h2><p id="88f3" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated"><a class="ae ky" href="https://www.w3schools.com/python/python_datetime.asp" rel="noopener ugc nofollow" target="_blank">日期对象</a>由于其格式，对于机器学习模型来说可能很难处理。因此，有时有必要将一个日期分成多列。同样的考虑可以应用于数据分析中的许多其他常见情况(例如，自然语言处理)。</p><p id="780a" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">在我们的示例中，我们现在要将日期列分成三个不同的列:年、月和日。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/68cafa77509a377439ce01d53d48d0dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*xng97NxC5Hx5C7BliwDbqQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 6: Dealing with Dates</figcaption></figure><h2 id="84c6" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">极端值</h2><p id="ef9c" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">异常值是数据点的一小部分，这些数据点与特征中的其余观察值相距甚远。异常值可能被引入到数据集中，主要是因为收集数据时的错误，或者是因为我们的特定特征所特有的特殊异常。</p><p id="6604" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">使用四种主要技术来识别异常值:</p><ul class=""><li id="b0f0" class="mp mq it lv b lw mr lz ms mc mt mg mu mk mv mo mw mx my mz bi translated"><strong class="lv iu">数据可视化</strong>:通过目视检查数据分布来确定异常值。</li><li id="2b85" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><strong class="lv iu"> Z-Score </strong>:如果我们知道我们的特征分布是高斯分布，就使用 Z-Score。事实上，当使用高斯分布时，我们知道分布的大约 2 个标准偏差意味着大约 95%的数据将被覆盖，而远离平均值的 3 个标准分布将覆盖大约 99.7%的数据。因此，使用介于 2 和 3 之间的因子值，我们能够非常准确地删除所有异常值。如果你有兴趣了解更多关于高斯分布的信息，你可以在这里找到更多信息<a class="ae ky" rel="noopener" target="_blank" href="/probability-distributions-in-data-science-cce6e64873a7"/>。</li><li id="638a" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><strong class="lv iu">百分位数</strong>:是另一种识别异常值的统计方法。当使用百分位数时，我们假设数据的某个顶部和底部百分比是异常值。使用这种方法的关键点是找到最佳百分比值。一种有用的方法是在应用百分位数法检查总体结果之前和之后，将数据可视化。</li><li id="f8c6" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><strong class="lv iu">封顶</strong>:我们不是删除异常值，而是用我们列中最高的正常值来替换它们。</li></ul><p id="5719" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">其他常用于检测异常值的更高级技术有<a class="ae ky" href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener ugc nofollow" target="_blank"> DBSCAN </a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" rel="noopener ugc nofollow" target="_blank">隔离林</a>。</p><p id="4ac4" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">继续我们的例子，我们可以从左边的两个数字特征(X2，X3)开始。通过使用 Seaborn 创建一个简单的箱线图，我们可以清楚地看到 X2 有一些异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a282ac33df8a3163b3705cb3b629f4da.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*Fv6VXKdn9O_sxtWujVBNGA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 7: Examining Outliers using Data Visualization</figcaption></figure><p id="2987" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">使用 Z 分数(因子为 2)和百分位数方法，我们现在可以测试在 X2 将识别出多少异常值。如下面的输出框所示，使用 Z 分数确定了 234 个值，而使用百分位数方法删除了 800 个值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="38af" class="nk lc it od b gy oh oi l oj ok">8000<br/>7766<br/>7200</span></pre><p id="30c7" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">此外，还可以通过限制异常值来处理异常值，而不是丢弃它们。</p><h2 id="8ccc" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">扔掉</h2><p id="f431" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">宁滨是一种用于平滑噪声数据的常用技术，通过将数字或分类特征划分到不同的箱中。因此，这可以帮助我们降低过度拟合的风险(尽管可能会降低我们的模型精度)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/dcd8f7da583ed558e68242bb950e4cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*NSkqxTCr7EHFO4PnLyImDw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 8: Binning Numeric and Categoric Data</figcaption></figure><h2 id="27e9" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">分类数据编码</h2><p id="a548" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">大多数机器学习模型目前不能处理分类数据，因此通常需要在将所有分类特征输入到机器学习模型之前将其转换为数字。</p><p id="f70a" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">可以在 Python 中实现不同的技术，例如:One Hot Encoding(转换要素)和 Label Encoder(转换标注)。</p><p id="61d7" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">一种热编码采用一个特征，并将它分割成与原始列中不同类别的数量一样多的列。然后，它给所有没有该特定类别的行分配一个 0，给所有有该类别的行分配一个 1。使用 Pandas<strong class="lv iu"><em class="om">get _ dummies()</em></strong>函数可以在 Python 中实现一个热编码。</p><p id="e20f" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">Label Encoder 通过为所有分类案例分配不同的编号并将它们存储在一列中来替代所有分类案例。</p><p id="bb61" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">不使用具有正常特征的标签编码器是非常优选的，因为一些机器学习模型可能会混淆，并认为具有比其他值更高的值的编码情况可能对它们更重要(按层次顺序考虑它们)。当使用一种热编码时，这种情况不会发生。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/0c2507ab9ef83fcd50c61b5e8a65aacb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ezGOW_jDDvZ9jCjHTj7hLg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 9: Difference between One Hot Encoding and Label Encoding [1]</figcaption></figure><p id="284b" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">我们现在可以继续将数据集划分为特征(<strong class="lv iu"> <em class="om"> X </em> </strong>)和标签(<strong class="lv iu"> <em class="om"> Y </em> </strong>)，然后分别应用一个热编码和标签编码器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><h2 id="463c" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">缩放比例</h2><p id="4a8f" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">在大多数数据集中，数字特征都有不同的范围(例如，身高与体重)。虽然，对于一些机器学习算法来说，将我们的输入特征限制在一个定义的范围内是很重要的。事实上，对于一些基于距离的模型，如朴素贝叶斯、<a class="ae ky" rel="noopener" target="_blank" href="/svm-feature-selection-and-kernels-840781cc1a6c">支持向量机</a>和聚类算法，如果它们都有不同的范围，那么几乎不可能比较不同的特征。</p><p id="c19f" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">缩放特征的两种常见方式是:</p><ul class=""><li id="0e1c" class="mp mq it lv b lw mr lz ms mc mt mg mu mk mv mo mw mx my mz bi translated"><strong class="lv iu">标准化:</strong>缩放输入要素，同时考虑它们的标准偏差(使用标准化，我们的变换后的要素将看起来类似于正态分布)。这种方法可以降低异常值的重要性，但由于标准差的差异，可能会导致要素之间的不同范围。通过使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"><strong class="lv iu"><em class="om">StandardScaler()</em></strong></a>可以在 scikit-learn 中实现标准化。</li><li id="92ed" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><strong class="lv iu">归一化:</strong>在 0 和 1 之间的范围内缩放所有特征，但是会增加异常值的影响，因为没有考虑每个不同特征的标准偏差。使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler" rel="noopener ugc nofollow" target="_blank"><strong class="lv iu"><em class="om">minmax scaler()</em></strong></a>可以在 scikit-learn 中实现归一化。</li></ul><p id="2a02" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated">在本例中，我们将使用标准化，然后我们将处理异常值。如果您正在处理的数据集没有受到离群值的广泛影响，scikit-learn 还提供了另一个名为<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lv iu"> <em class="om">的标准化函数，该函数可以在默认情况下减少离群值的影响。</em></strong></a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><h2 id="05bc" class="nk lc it bd ld nl nm dn lh nn no dp ll mc np nq ln mg nr ns lp mk nt nu lr nv bi translated">自动化特征工程</h2><p id="7d4d" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">为了使特征工程过程自动化，在过去几年中已经开发了不同的技术和软件包。当对我们的数据集进行第一次分析时，这些肯定会产生有用的结果，但是它们仍然不能完全自动化整个特征工程过程。关于数据的领域知识和数据科学家在对原始数据建模以最适合分析目的方面的专业知识是不可替代的。Python 中最流行的自动特征选择库之一是<a class="ae ky" href="https://docs.featuretools.com/#" rel="noopener ugc nofollow" target="_blank"> Featuretools </a>。</p><h1 id="7000" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">结论</h1><p id="fa5b" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">现在是时候通过使用随机森林分类器来最终测试我们抛光的数据集预测准确性了。如下所示，我们的分类器现在成功地实现了 100%的预测准确率。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni la l"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="5e45" class="nk lc it od b gy oh oi l oj ok">1.40625<br/>[[1204    0]<br/> [   0 1196]]<br/>              precision    recall  f1-score   support<br/><br/>           0       1.00      1.00      1.00      1204<br/>           1       1.00      1.00      1.00      1196<br/><br/>   micro avg       1.00      1.00      1.00      2400<br/>   macro avg       1.00      1.00      1.00      2400<br/>weighted avg       1.00      1.00      1.00      2400</span></pre></div><div class="ab cl oo op hx oq" role="separator"><span class="or bw bk os ot ou"/><span class="or bw bk os ot ou"/><span class="or bw bk os ot"/></div><div class="im in io ip iq"><p id="722d" class="pw-post-body-paragraph lt lu it lv b lw mr ju ly lz ms jx mb mc nf me mf mg ng mi mj mk nh mm mn mo im bi translated"><em class="om">希望您喜欢这篇文章，感谢您的阅读！</em></p><h1 id="4545" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">联系人</h1><p id="c8e1" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">如果你想了解我最新的文章和项目<a class="ae ky" href="https://medium.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener">，请在媒体</a>上关注我，并订阅我的<a class="ae ky" href="http://eepurl.com/gwO-Dr?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">邮件列表</a>。以下是我的一些联系人详细信息:</p><ul class=""><li id="1c76" class="mp mq it lv b lw mr lz ms mc mt mg mu mk mv mo mw mx my mz bi translated"><a class="ae ky" href="https://uk.linkedin.com/in/pier-paolo-ippolito-202917146?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="9c4f" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/blog/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人博客</a></li><li id="975f" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><a class="ae ky" href="https://pierpaolo28.github.io/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">个人网站</a></li><li id="b803" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><a class="ae ky" href="https://towardsdatascience.com/@pierpaoloippolito28?source=post_page---------------------------" rel="noopener" target="_blank">中型简介</a></li><li id="80b8" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><a class="ae ky" href="https://github.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> GitHub </a></li><li id="8c55" class="mp mq it lv b lw na lz nb mc nc mg nd mk ne mo mw mx my mz bi translated"><a class="ae ky" href="https://www.kaggle.com/pierpaolo28?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">卡格尔</a></li></ul><h1 id="c1e1" class="lb lc it bd ld le lf lg lh li lj lk ll jz lm ka ln kc lo kd lp kf lq kg lr ls bi translated">文献学</h1><p id="97ef" class="pw-post-body-paragraph lt lu it lv b lw lx ju ly lz ma jx mb mc md me mf mg mh mi mj mk ml mm mn mo im bi translated">[1]什么是 One Hot Encoding，如何进行？迈克尔·德尔索尔，中号。访问地址:<a class="ae ky" href="https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179" rel="noopener">https://medium . com/@ michaeldelsole/what-one-hot-encoding-and-how-do-it-f0ae 272 f 1179</a></p></div></div>    
</body>
</html>