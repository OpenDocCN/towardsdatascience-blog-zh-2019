<html>
<head>
<title>Suggestive Computer-Aided Design</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">暗示性计算机辅助设计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/suggestive-computer-aided-design-a9c7698c1cc9?source=collection_archive---------16-----------------------#2019-07-03">https://towardsdatascience.com/suggestive-computer-aided-design-a9c7698c1cc9?source=collection_archive---------16-----------------------#2019-07-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0308" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过机器学习辅助设计</h2></div><p id="072e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://www.linkedin.com/in/stanislas-chaillou-b1931090/" rel="noopener ugc nofollow" target="_blank"> <em class="lf">斯塔尼斯拉斯 Chaillou</em></a>T4<em class="lf"/>T8 |哈佛设计研究院| 2018 年春季</p><p id="4dbb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">协同</em> <a class="ae le" href="https://www.linkedin.com/in/thomastrinelle/" rel="noopener ugc nofollow" target="_blank"> <em class="lf">托马斯·特雷诺</em> </a></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/3899e3b2ee60a7d7146ce74294eda070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kCuNL07RRQvPWiiaTpWARg.gif"/></div></div></figure><p id="800f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> T </span> <strong class="kk iu">基于机器的推荐已经在无数行业得到了应用，从网络上的暗示性搜索到图片股票图像推荐。</strong>作为推荐引擎的核心，它可以在海量数据库中查询相关信息<em class="lf">文本、图像等</em>，并在用户与给定界面交互时呈现给用户。随着当今大型 3D 数据仓库的聚合，架构&amp;设计可以从类似实践中受益。</p><p id="8c5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">实际上，我们专业的设计过程大多是通过 3D 软件<em class="lf">rhino 3D、Maya、3DSmax、AutoCAD </em>等进行的。<strong class="kk iu">建筑师无论是通过<a class="ae le" href="https://en.wikipedia.org/wiki/Computer-aided_design" rel="noopener ugc nofollow" target="_blank"> CAD </a>软件<em class="lf">计算机辅助设计</em>还是如今的<a class="ae le" href="https://en.wikipedia.org/wiki/Building_information_modeling" rel="noopener ugc nofollow" target="_blank"> BIM </a>引擎<em class="lf">建筑信息建模</em>不断地将自己的意图转化为三维空间中的线和面</strong>。建议相关的 3D 对象，取自外部数据源，可能是一种方式，以加强他们的设计过程。</p><p id="e193" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是本文的目的:<strong class="kk iu">研究并提出一种辅助设计师的方法，通过<em class="lf">暗示造型</em>。</strong>随着建筑师在 3D 空间中作画，一系列基于机器学习的分类器将能够搜索相关建议并提出替代、相似或互补的设计选项。</p><p id="84ae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，我们从 3D 形状识别与分类领域的先例中汲取灵感，建立了一套能够在设计者绘图时向他们推荐模型的方法和工具集。实际上，我们的目标有两个:<strong class="kk iu"> (1)利用预先建模的建议加快 3D 建模过程</strong>，而<strong class="kk iu">则通过替代或互补的设计选项激励设计师</strong>。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="1358" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">卷积神经网络</h2><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nb"><img src="../Images/e641ac3dfd5a2866ac02ba5ef6acbc8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ioaVVwuFuD1t286YM6SIZQ.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 1:<strong class="bd ng"> Convolutional Neural Network Architecture | </strong>Source: <a class="ae le" href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" rel="noopener ugc nofollow" target="_blank">3b1b</a></figcaption></figure><p id="d724" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">能够查询与用户绘制的设计特征相匹配的 3D 对象依赖于特征比较。远离标准的几何描述性度量，卷积神经网络(CNN)提供了一种简化且更全面的比较形状的选项。</p><p id="6c0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CNN 不是匹配简单的指标——也称为<em class="lf">特征提取</em>——而是将图像作为输入，将像素表示传递给一连串的“<em class="lf">神经元</em>”(<em class="lf">图 1 </em>)。CNN 模型调整每个神经元的权重，同时在其最后一层输出预测。通过训练和验证的连续阶段，我们能够估计模型的准确性，并进一步调整权重以最大化准确性。一旦充分训练，CNN 模型将预测给定对象图像表示的“<em class="lf">类</em>”或类别。</p><p id="73dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种类型的网络是机器学习的标准实践，本身并不代表突破或复杂的架构。然而，<strong class="kk iu">它仅基于空间特征建立一定数量的直觉的能力比我们的应用更相关</strong>:公开可用的 3D 对象的格式异构性使得特征提取和元数据比较成为一个具有挑战性的过程。能够使用图像简单地从它们的拓扑特征来考虑对象，为我们提供了一个用于 3D 形状比较和检索的健壮的统一框架。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="67e1" class="nh mj it bd mk ni nj nk mn nl nm nn mq jz no ka mt kc np kd mw kf nq kg mz nr bi translated">一.先例</h1><p id="bd97" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated"><strong class="kk iu">我们的工作建立在 3 个主要研究项目之上，这些项目最近构建了 3D 对象识别和分类领域。</strong>这些论文揭示了<a class="ae le" href="https://en.wikipedia.org/wiki/Convolution" rel="noopener ugc nofollow" target="_blank">卷积</a>作为理解&amp;描述 3D 形状的理想工具的相关性。</p><p id="8f45" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">【1】用于 3D 形状识别的多视角卷积神经网络</strong></p><p id="9845" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf" rel="noopener ugc nofollow" target="_blank">这第一篇论文</a><strong class="kk iu">【1】</strong>提出了一个标准的 CNN 架构，该架构被训练来识别彼此独立的形状的渲染视图，并表明甚至可以以远高于使用最先进的 3D 形状描述符的准确度从单个视图识别 3D 形状。当提供形状的多个视图时，识别率进一步增加。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nx"><img src="../Images/5fc8b4c3e30f5d630f958edb6799daa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-NYNr4qpRSVlPFRCAy1ajA.jpeg"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 2: <strong class="bd ng">Multi-view CNN for 3D shape recognition</strong> | Source: <a class="ae le" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><p id="6412" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，还引入了一种新颖的 CNN 架构，它将来自 3D 形状的多个视图的信息组合到一个紧凑的形状描述符中，提供了更好的识别性能。</p><p id="e69e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">【2】vox net:用于实时物体识别的 3D 卷积神经网络</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ny"><img src="../Images/b02fa9c0fe8e84d03af2e710988d8672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmQVTn4_Ia1hBjYo_Pd-Iw.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 3: <strong class="bd ng">The VoxNet Architecture</strong> | Source: <a class="ae le" href="https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><p id="7e4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了利用因激光雷达和 RGBD 扫描仪的可用性增加而导致的点云数据库数量的增长，<a class="ae le" href="https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf" rel="noopener ugc nofollow" target="_blank">本文</a><strong class="kk iu">【2】</strong>提出了一个“VoxNet”。该模型的架构旨在通过将体积占用网格表示与受监督的 3D 卷积神经网络(3D CNN)集成来解决海量点云处理和标记的问题。</p><p id="895b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 LiDAR、RGBD 和 CAD 数据在公开可用的基准上评估结果。VoxNet 最终实现了超越现有技术水平的准确性，同时每秒标记数百个实例。</p><p id="8092" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">【3】设计中的体积表示和机器学习</strong></p><p id="ae14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="http://njstudio.co.kr/main/project/2017_thesisVoxelHarvardGSD/public/" rel="noopener ugc nofollow" target="_blank">这最后一篇论文</a><strong class="kk iu">【3】</strong>探讨了体素建模与机器学习的机会。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nz"><img src="../Images/3aac43875c318147451a28198152a34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GYrg06ln5pAWHcqTcew_Kw.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 4: <strong class="bd ng">Summary of Work Flow | </strong>Source: <a class="ae le" href="http://njstudio.co.kr/main/project/2017_thesisVoxelHarvardGSD/public/" rel="noopener ugc nofollow" target="_blank">link</a></figcaption></figure><p id="4af9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先介绍了体素建模的概念，并与传统建模技术进行了比较。解释了像像素图和图形表示这样的概念，以最终检查基于从空间和几何光栅化到机器学习的过程的拟议设计系统或工作流的原型实现。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="99e4" class="nh mj it bd mk ni nj nk mn nl nm nn mq jz no ka mt kc np kd mw kf nq kg mz nr bi translated">二。模型定义</h1><p id="8536" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi ls translated">在这个项目中，我们的方法是识别用户正在绘制的对象，并通过简单地使用对象的形状作为代理来提供相似的对象。</p><p id="6e4a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简而言之，我们在这里设计的模型处理两个主要任务:</p><ul class=""><li id="0c2d" class="oa ob it kk b kl km ko kp kr oc kv od kz oe ld of og oh oi bi translated"><strong class="kk iu"> (1)分类:</strong>识别用户正在绘制的对象的类型，即找到合适的标签(<em class="lf">“椅子”、“长凳”、“床”</em>等)。)对于任何给定的图像输入，结合预测置信度得分。</li><li id="8823" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu"> (2)匹配:</strong>在 3D 对象的数据库中查询与用户的模型化输入的方面最匹配的一些形状，即返回在我们的数据库中找到的对象列表，从最相似到最不相似排序。</li></ul><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi oo"><img src="../Images/8666de4676dc000490c3769c70ac10bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkzqhjL_6tPkfJ6TZeu_EQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 5: <strong class="bd ng">Typical Pipeline | </strong>Source: Author</figcaption></figure><p id="bdea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如图<em class="lf">图 5 </em>所示，通过嵌套两个不同层次的模型(一个<strong class="kk iu">分类器</strong>和一个<strong class="kk iu">匹配器</strong>),我们也许能够执行这个两步过程。每个模型将在 3D 对象的图像上被训练，然后将在由用户建模的对象的图像上被测试。</p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="0d81" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">A.数据来源和生成</h2><p id="3d4c" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated">第一步是生成一个数据库来训练我们的模型。由于我们的方法是为了共享，我们想在此详述这一关键步骤，并分享我们为实现这一目标而使用和构建的资源。</p><p id="bfe5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们首先从现有的公共 3D 对象仓库中提取信息，例如:</p><ul class=""><li id="dce3" class="oa ob it kk b kl km ko kp kr oc kv od kz oe ld of og oh oi bi translated"><strong class="kk iu"> ShapeNet | </strong> <a class="ae le" href="https://www.shapenet.org/" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="61f8" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">谷歌 3D 仓库| </strong> <a class="ae le" href="https://3dwarehouse.sketchup.com/?hl=en" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="62d4" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu"> ModelNet | </strong> <a class="ae le" href="http://modelnet.cs.princeton.edu/" rel="noopener ugc nofollow" target="_blank">链接</a></li></ul><p id="3941" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从<strong class="kk iu"> ShapeNet </strong>数据库中，我们可以下载多达 2.330 个带标签的 3D 模型，分为 14 个特定类别(<em class="lf">图 6 </em>)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi op"><img src="../Images/0a757dd9857492dbc93ba08b1880e835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Grt2AjGuhBWw9IpCrBC6g.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 6: <strong class="bd ng">Training Set Classes </strong>| Source: Author</figcaption></figure><p id="4dad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用<em class="lf"> </em> </strong> <a class="ae le" href="https://www.rhino3d.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lf">犀牛</em> </strong> </a> <strong class="kk iu"> <em class="lf"> </em>和</strong> <a class="ae le" href="https://www.grasshopper3d.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lf">蚱蜢</em> </strong> </a> <strong class="kk iu">，我们然后编写一个工具来创建我们的训练集和验证集。</strong>在这个脚本中，围绕每个连续对象旋转的照相机以特定的角度拍摄对象，并将 JPG 图像保存在给定的目录中。</p><p id="74ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">图 7 显示了典型的摄像机路径(左图)和拍摄的最终图像(右图)。</em></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi oq"><img src="../Images/80542bb74ca98df0490376a48a50ee39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j_rIGcjlUqfZGbjB_GWx6g.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 7:<strong class="bd ng"> Image Capture Path, and Resulting Images</strong> | Source: Author</figcaption></figure><p id="8df1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于每个对象，我们拍摄 30 幅图像用于训练，10 幅用于验证，同时保持白色背景/中性背景。</p><p id="4763" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="http://stanislaschaillou.com/suggestive-cad/download/Data_Generator.zip" rel="noopener ugc nofollow" target="_blank"> <em class="lf">用于捕捉图像的数据生成器可从以下地址下载。</em> </a></p><p id="4826" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">我们最终获得一个标记图像库，每类 10 个对象，总共有 14 个不同的类。</strong>图 8 中显示了一个子集。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi or"><img src="../Images/19c0fd2bd3b24558398c1418b06befd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0PXIeHQV1HxhrxBfspySyA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 8:<strong class="bd ng"> Subset of Training Set </strong>| Source: Author</figcaption></figure></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="4c1d" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">B.分类</h2><p id="5360" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated"><strong class="kk iu">一旦我们的数据集准备就绪，我们的目标是在大量的 3D 对象图像上训练我们的第一个模型，即分类器</strong>，同时在属于类似类别的其他对象图像集上验证其性能。</p><h2 id="d501" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">迭代次数</h2><p id="80f3" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated">几个参数严重影响训练好的分类器的准确性:</p><ul class=""><li id="00b1" class="oa ob it kk b kl km ko kp kr oc kv od kz oe ld of og oh oi bi translated"><strong class="kk iu">训练和验证集的大小</strong></li><li id="a7a8" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">班级人数</strong></li><li id="0ede" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">每类对象的数量</strong></li><li id="0939" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">每个对象的图像数量</strong></li><li id="3f2e" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">两组中图像的尺寸</strong> s</li><li id="cf60" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">物体周围摄像机的拍摄路径</strong></li></ul><p id="11be" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这一点上，我们在不同的选项之间迭代，目的是增加模型在验证集上的整体准确性。由于资源有限，我们负担不起从头开始培训的费用。<strong class="kk iu">使用一些迁移学习很方便，因为它提供了提高我们模型准确性的可能性，同时绕过了数天的培训</strong>。我们添加了一个<a class="ae le" href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/" rel="noopener ugc nofollow" target="_blank"> VGG16 预训练模型</a>作为模型的第一层，这反过来提高了我们 32%的准确度。</p><p id="1d6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">这个训练过程的一个重要收获是，摄像机路径实际上会显著影响最终的精度。</strong>在我们尝试过的许多版本中，我们在下面的<em class="lf">图 9 </em>中展示了两种不同摄像机路径的性能对比:<strong class="kk iu">圆形</strong>和<strong class="kk iu">球形</strong>。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi os"><img src="../Images/4572aeec557d26054feb65009c48aab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*No-3NIMFRdWxqU3BHk7odA.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 9:<strong class="bd ng"> Training Performance Under Different Image Capture Technics</strong> | Source: Author</figcaption></figure><h2 id="4508" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">结果</h2><p id="ad74" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated">最终，我们满足于使用<em class="lf"> 200*200 </em> px 图像的分类器，具有球形相机路径，每个对象 30 个图像用于训练，10 个用于验证。在 30 个时期之后，<strong class="kk iu">我们最终在验证集上获得了 93%的准确度。</strong>现在似乎很清楚，以下参数对整个模型的性能有很大影响:</p><ul class=""><li id="27cf" class="oa ob it kk b kl km ko kp kr oc kv od kz oe ld of og oh oi bi translated"><strong class="kk iu">增加图像尺寸可以提高精确度。</strong>在我们的机器上，速度和精度之间的平衡在<em class="lf"> 200x200 </em> px 左右找到了平衡点。</li><li id="e287" class="oa ob it kk b kl oj ko ok kr ol kv om kz on ld of og oh oi bi translated"><strong class="kk iu">摄像机捕捉路径也直接影响精度。</strong>通过使用球形捕获路径，而不是圆形路径(<em class="lf">见图 9 </em>)，我们显著提高了模型性能:在更少的历元之后，精确度更高。使用球形路径似乎是一种更全面的捕捉给定形状的方式。</li></ul><p id="e9ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">图 10 显示了分类器对四种不同用户输入的典型结果。</em></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ot"><img src="../Images/82e78d573f67a3df2e7e32f7e75b862c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kGtDorHf33Xa0xVY450qQw.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 10:<strong class="bd ng"> Results of Classifier Model, User Input (left) to Predicted Class (right) | </strong>Source: Author</figcaption></figure><ul class=""><li id="cbe4" class="oa ob it kk b kl km ko kp kr oc kv od kz oe ld of og oh oi bi translated">更有趣的是在给定用户的 3D 建模过程中运行的相同模型的性能。当一个对象被建模时，少到几个表面就足以将分类器推向正确的方向。因此，在建模过程的早期，我们能够识别建模对象的类。作为回报，我们可以从我们的数据库向用户建议模型，如果用户在建议中找到匹配，可能会减少 3D 建模时间。<em class="lf">图 11 </em>显示了在建模过程的五个不同步骤中分类器的结果。</li></ul><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ou"><img src="../Images/c7d874b3e875b0ea8d38d6d7d824c077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*eomsLjmtcX08t9CHhQOCRg.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 11: <strong class="bd ng">Classification During Modeling Process </strong>| Source: Author</figcaption></figure><p id="8490" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://github.com/StanislasChaillou/Independent_Study/blob/master/Week%204/Classification_Model_testing.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lf">更多关于 GitHub。</em></strong>T37】</a></p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h2 id="c840" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">C.相称的</h2><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ov"><img src="../Images/898747caa8eb3b4155707af91beb2e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cZy1KY-0hbcvX8tiCEZTYQ.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 12:<strong class="bd ng"> Matching: Finding Best Matches Among Database Through a Convolutional Model </strong>| Source: Author</figcaption></figure><p id="5834" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二步，<strong class="kk iu">我们的模型试图在庞大的 3D 模型数据库中寻找理想的匹配</strong>。如果分类模型有助于缩小搜索范围，则匹配模型将给定类别的所有 3D 模型从“最相似”到“最不相似”进行排序。</p><p id="4394" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://github.com/StanislasChaillou/Independent_Study/blob/master/Week%204/Matching_Model.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lf">更上 GitHub </em> </strong> </a> <strong class="kk iu"> <em class="lf">。</em>T11】</strong></p><p id="9e1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">我们的匹配模型是另一个卷积神经网络，根据给定类别</strong>的对象的图像进行训练，并根据从不同角度拍摄的视图对相同的对象进行验证。对于测试，我们的输入将是一个用户建模的 3D 对象的图像，输出将是我们的数据库中所有对象的列表，按照相似性的顺序排列。</p><h2 id="f1af" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">结果</h2><p id="e943" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated">当我们为每个单独的类训练匹配模型时，我们现在能够将分类模型和匹配模型嵌套为一个单独的管道。<strong class="kk iu">我们现在可以处理输入图像，首先对其进行分类，最后与数据库中的相似对象进行匹配。</strong>同时，我们的模型输出一些预测置信度，帮助我们衡量原始模型和实际匹配之间的相似程度。</p><p id="a749" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">已经对不同类别的 7 个对象进行了测试，如图 13 所示。</em></p><p id="1746" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://github.com/StanislasChaillou/Independent_Study/blob/master/Week%204/Testing_Matching_Model.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> <em class="lf">更多关于 GitHub。</em> </strong> </a></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ow"><img src="../Images/ef3a08113ce0ab3ea1d262685d3d0778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*j-btrnnLjLARYTpAuht9yQ.gif"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Figure 13:<strong class="bd ng"> Full Pipeline With Functional Classifying &amp; Matching Model </strong>| Source: Author</figcaption></figure></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="e068" class="nh mj it bd mk ni nj nk mn nl nm nn mq jz no ka mt kc np kd mw kf nq kg mz nr bi translated">三。结论</h1><p id="0862" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi ls translated">首先，我们鼓励在我们的行业内进一步发展和改进上述理念。在这方面已经做了很多工作，并为邻近领域带来了解决方案。让这些技术渗透到我们的学科中，将会真正有益于我们日常的建筑实践。<strong class="kk iu"> </strong>此外，像我们这样嵌套 CNN 模型只是一个更大概念的一个可能版本:<strong class="kk iu"> 3D 形状识别和建议</strong>。可以部署其他方法和模型，将公开可用数据的广度&amp;丰富性带到架构师&amp;设计者的能力范围内。</p><p id="4af2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">事实上，除了本文开发的简单管道，还有一个更基本的想法</strong> : <strong class="kk iu">架构形式的资格</strong>。正如在<a class="ae le" rel="noopener" target="_blank" href="/ai-architecture-f9d78c6958e0">之前的一篇文章</a>中所描述的，能够构建框架，包括现有形状的异质性和复杂性，将很快对我们的学科至关重要。随着数字数据量的增加，并汇集在大型公共存储库中，我们对这些共享知识的访问将与我们的查询智能一样好。正如我们在这里展示的，我们可以依靠<em class="lf">在某种程度上</em>依靠机器学习来找到一种共同语言，从而能够比较不同的&amp;复杂形状。</p><h2 id="b90c" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">在一个更基本的层面上，这项工作只是显示了暗示性设计的潜力。通过在创意过程中为设计师带来相关的选择，我们有机会拓宽他们的工作范围。随着这种方法扩展到比简单的离散对象更大的分类法，它最终会将被动的集体知识转化为主动的灵感来源。</h2></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><p id="a98f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae le" href="https://towardsdatascience.com/@sfjchaillou" rel="noopener" target="_blank"> <strong class="kk iu"> <em class="lf">更多文章，点击这里。</em> </strong> </a></p></div><div class="ab cl mb mc hx md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="im in io ip iq"><h1 id="cda5" class="nh mj it bd mk ni nj nk mn nl nm nn mq jz no ka mt kc np kd mw kf nq kg mz nr bi translated">文献学</h1><p id="114b" class="pw-post-body-paragraph ki kj it kk b kl ns ju kn ko nt jx kq kr nu kt ku kv nv kx ky kz nw lb lc ld im bi translated"><strong class="kk iu">【1】</strong><strong class="kk iu">用于 3D 形状识别的多视角卷积神经网络</strong>，苏航、苏卜兰苏·马吉、伊万杰洛斯·卡罗格拉基斯、埃里克·博学-米勒、马萨诸塞大学阿姆赫斯特分校| <a class="ae le" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Su_Multi-View_Convolutional_Neural_ICCV_2015_paper.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="ae07" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">【2】</strong><strong class="kk iu">vox net:用于实时物体识别的 3D 卷积神经网络</strong>，丹尼尔·马图拉纳和塞巴斯蒂安·舍雷尔| <a class="ae le" href="https://www.ri.cmu.edu/pub_files/2015/9/voxnet_maturana_scherer_iros15.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p><p id="853c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">【3】</strong><strong class="kk iu">三维对象的再混合和重采样在设计中使用体积表示和机器学习</strong>，NJ Namju Lee，哈佛 GSD，2017 | <a class="ae le" href="http://njstudio.co.kr/main/project/2017_thesisVoxelHarvardGSD/public/" rel="noopener ugc nofollow" target="_blank">链接</a></p></div></div>    
</body>
</html>