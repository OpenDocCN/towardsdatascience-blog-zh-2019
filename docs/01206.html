<html>
<head>
<title>Review: CRAFT — Cascade Region-proposal-network And FasT r-cnn (Object Detection)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:CRAFT 级联区域-提议-网络和快速 r-cnn(目标检测)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=collection_archive---------19-----------------------#2019-02-24">https://towardsdatascience.com/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858?source=collection_archive---------19-----------------------#2019-02-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9a81" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">更好的对象提议，更准确的对象分类，胜过更快的 R-CNN</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6df9fed8af383f222b856ca5b336466f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0vaNq7_8yL2izL7C"/></div></div></figure><p id="b555" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi ln translated"><span class="l lo lp lq bm lr ls lt lu lv di">在</span>这个故事里，由<strong class="kt ir">中科院</strong>和<strong class="kt ir">清华</strong>的<strong class="kt ir">工艺</strong>进行了回顾。在<a class="ae lw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快速 R-CNN </a>中，区域提案网络(RPN)用于生成提案。这些建议，在投资回报汇集后，将通过网络进行分类。但是发现<a class="ae lw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>有一个核心问题:</p><ul class=""><li id="bf49" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">在提案生成中，仍然有很大比例的背景区域。许多背景样本的存在导致许多假阳性。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mg"><img src="../Images/bc8b2f499e9daa6850fca88911a64887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tqkP5QNPAEgMUY0M5zvbrA.png"/></div></div></figure><p id="020b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在<strong class="kt ir">工艺</strong>中，如上图，在 RPN 之后增加了另一个 CNN，使<strong class="kt ir">产生更少的提案(即这里的 300 个)</strong>。然后，对这 300 个建议进行分类，并输出大约 20 个基元检测结果。对于每个原始结果，<strong class="kt ir">使用一对其余分类来执行精细的对象检测</strong>。发表在<strong class="kt ir"> 2016 CVPR </strong>上，引用<strong class="kt ir"> 50 余次</strong>。(<a class="mh mi ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----2ce987361858--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="24aa" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">概述</h1><ol class=""><li id="230b" class="lx ly iq kt b ku ni kx nj la nk le nl li nm lm nn md me mf bi translated"><strong class="kt ir">级联建议生成</strong></li><li id="8142" class="lx ly iq kt b ku no kx np la nq le nr li ns lm nn md me mf bi translated"><strong class="kt ir">级联对象分类</strong></li><li id="8c9b" class="lx ly iq kt b ku no kx np la nq le nr li ns lm nn md me mf bi translated"><strong class="kt ir">消融研究</strong></li><li id="52c2" class="lx ly iq kt b ku no kx np la nq le nr li ns lm nn md me mf bi translated"><strong class="kt ir">结果</strong></li></ol></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="999e" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated"><strong class="ak"> 1。级联建议生成</strong></h1><h2 id="5669" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">1.1.基线 RPN</h2><ul class=""><li id="ae99" class="lx ly iq kt b ku ni kx nj la nk le nl li nm lm mc md me mf bi translated">一个理想的建议生成器应该在覆盖几乎所有对象实例的同时生成尽可能少的建议。由于 CNN 池操作导致的分辨率损失和滑动窗口的固定纵横比，RPN 在覆盖具有极端尺度或形状的对象方面较弱。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/247defe2b7dc44a61416a02972ab9a30.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*bhfWkIUYeuEd2sMm3pZh-g.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">Recall Rates (%), Overall is 94.87%, Lower than 94.87% is bold in text.</strong></figcaption></figure><ul class=""><li id="d476" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">以上结果是基于使用 PASCAL VOC 2007 train+val 训练的<a class="ae lw" href="https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11" rel="noopener"> VGG_M </a>的基线 RPN，并在测试集上测试。</li><li id="4d28" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">每个对象类别的召回率差异很大。具有极端纵横比和比例的对象很难被检测到，例如船和瓶子。</li></ul><h2 id="b461" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">1.2.提议的级联结构</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/184be03ad45b8b7f830eb2f91dcd7083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*muR3JS9ghzpQiBD8pPNrCQ.png"/></div></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">The additional classification network after RPN is denoted as FRCN Net here</strong></figcaption></figure><ul class=""><li id="150d" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">RPN 之后的附加分类网络。</li><li id="b3a7" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">附加网络是一个 2 类检测网络，在上图中表示为 FRCN 网络。它使用 RPN 的输出作为训练数据。</li><li id="afe3" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">在训练 RPN 网络之后，每个训练图像的 2000 个原始建议被用作 FRCN 网络的训练数据。</li><li id="98fa" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">在训练期间，正负采样分别基于正 0.7 IoU 和负 0.3 IoU 以下。</li><li id="d7c1" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">有两个优点:</li><li id="417c" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">1)首先，附加的 FRCN 网进一步<strong class="kt ir">提高了目标提议</strong>的质量，<strong class="kt ir">缩小了更多的背景区域</strong>，使提议更符合任务要求。</li><li id="3914" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">2)第二，<strong class="kt ir">来自多个来源的建议可以合并</strong>作为 FRCN 网的输入，以便可以使用互补信息。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="40d7" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">2.级联对象分类</h1><h2 id="d04e" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">2.1.基线<a class="ae lw" href="http://Fast R-CNN" rel="noopener ugc nofollow" target="_blank">快速 R-CNN </a></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/d3d02a379c0413bfcc958236285488ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*pqaMXNmFbWyQPeM4KXUlVw.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><a class="ae lw" href="http://Fast R-CNN" rel="noopener ugc nofollow" target="_blank"><strong class="bd ok">Fast R-CNN</strong></a><strong class="bd ok"> Results (Orange: Train, Red: Boat, Blue: Potted Plant)</strong></figcaption></figure><ul class=""><li id="738b" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">它在捕捉类别内差异方面很弱，因为“背景”类通常占据训练样本的很大比例<strong class="kt ir">。</strong></li><li id="d65c" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">如上图所示，<strong class="kt ir">误分类错误是最终检测</strong>的主要问题。</li></ul><h2 id="ce72" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">2.2.提议的级联结构</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/42a5573460a687964bde5fa98a0fe167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tSTdXkdOr72keLvWhaEqJA.png"/></div></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">Cascade Object Classification</strong></figcaption></figure><ul class=""><li id="0a05" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">为了改善由于错误分类导致的太多假阳性的问题，<strong class="kt ir">one-vs-rest 分类器被用作每个对象类别的附加两类交叉熵损失</strong>，如上所示。</li><li id="ecfa" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">每个 one-vs-rest 分类器看到特定于一个特定对象类别的提议(也包含一些假阳性)，使其<strong class="kt ir">专注于捕获类别内差异</strong>。</li><li id="3bdc" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">标准 FRCN 网(FRCN-1)首先使用来自级联建议结构的目标建议进行训练</strong>。</li><li id="632f" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">然后，基于 FRCN-1 的输出训练另一个 FRCN 网络(FRCN-2 ),这是原始检测。</li><li id="7f1f" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">被分类为“背景”的原始检测被丢弃。</li><li id="b8ef" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">使用<em class="oo"> N </em>两类交叉熵损失的总和</strong>，其中<em class="oo"> N </em>等于对象类别的数量。</li><li id="1864" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">FRCN-1 和 FRCN-2 的卷积权重是共享的</strong>,因此全图像特征图只需计算一次。</li><li id="83dc" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">产生 2 个<em class="oo"> N 个</em>分数和 4 个<em class="oo"> N 个</em>边界框回归目标的新层从高斯分布初始化。</li><li id="b92a" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">因此，在测试时间，以 300 个目标提议作为输入，FRCN-1 输出大约 20 个原始检测，每个具有<em class="oo"> N </em>原始分数。</li><li id="3213" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">然后，每个基元检测再次由 FRCN-2 分类，并且输出分数(<em class="oo"> N </em>个类别)以逐个类别的方式乘以基元分数(<em class="oo"> N </em>个类别)，以获得该检测的最终<em class="oo"> N </em>个分数。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="17eb" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">3.消融研究</h1><h2 id="9041" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">3.1.提案生成</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/e4dbcbaf13a2f55799f406c607605f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Y05PbPRYFbCtYee95c5r1Q.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">Recall Rates (%)</strong></figcaption></figure><ul class=""><li id="e5b1" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">使用在 ILSVRC DET 列车+va1 上预训练的 VGG-19 ，并在 val2 上进行测试。</li><li id="1a03" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">提出的 FRCN </strong>，分别基于 0.7 IoU 以上和 0.3 IoU 以下使用正负抽样，<strong class="kt ir">的召回率最高，达到 92.37%，比 RPN 高出 2%以上。</strong></li><li id="09ef" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">使用 300 个建议的 FRCN 优于使用 2000 个建议的选择性搜索。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/d398cb53d5ca4ff94d4a989122aea244.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*6_rWwUCh8bauKVgiN-n3BQ.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">Recall Rates (%) and mAP (%) on PASCAL VOC 2007 Test Set</strong></figcaption></figure><ul class=""><li id="e6b9" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">与自下而上的方法相比，RPN 建议没有很好地本地化(高 IoU 阈值下的低召回率)。</li><li id="f35e" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">使用更大的网络无法帮助(RPN_L)，因为它是由固定锚引起的。</li><li id="7f52" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">“Ours”保持每个图像的固定数量的建议(与 RPN 相同)，而“Ours_S”保持其分数(级联 FRCN 分类器的输出)高于固定阈值的建议。</li><li id="80df" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">级联建议生成器<strong class="kt ir">不仅进一步消除了背景建议，还带来了更好的定位</strong>，两者都有助于检测 AP。</li></ul><h2 id="4076" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">3.2.对象分类</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/be73a10d05e28e27ccfdd7dbbb8a1824.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*TmEiyBjzVQA-eYjXuTDVdQ.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">mAP (%) on PASCAL VOC 2007 Test Set</strong></figcaption></figure><ul class=""><li id="4bb7" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated"><strong class="kt ir">“相同”</strong>:表示没有微调。与没有级联分类结构的图相似。这就像运行 FRCN-1 两次，这是一个迭代的包围盒回归。</li><li id="962f" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">“clf”</strong>:微调额外的一对其余分类权重。地图提高到 66.3%。</li><li id="c801" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">“fc+clf”</strong>:微调最后一个卷积层之后的所有层。mAP 为 68.0%，具有最好的结果。</li><li id="d211" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">“conv+fc+clf”</strong>:就像完全训练新的特征表示，学习另一个分类器。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5cf13df9402139af2131b002b3bc9304.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*CrnPehhsMzm3qFXVAfKanA.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">mAP (%) on PASCAL VOC 2007 Test Set</strong></figcaption></figure><ul class=""><li id="3535" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">如果用 one-vs-rest 代替原来的分类，mAP 变得更差，只有 46.1%。</li><li id="3f3e" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">如果使用级联分类，mAP 提高到 68.0%。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h1 id="089e" class="mq mr iq bd ms mt mu mv mw mx my mz na jw nb jx nc jz nd ka ne kc nf kd ng nh bi translated">4.结果</h1><h2 id="0e24" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">4.1.PASCAL VOC 2007 和 2012</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/bd213d06b85b84ef5892677ea5093513.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*cyDE4Yu3_G7xbjbK0wi_7g.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">mAP (%) on PASCAL VOC 2007 and 2012</strong></figcaption></figure><ul class=""><li id="5cee" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated"><strong class="kt ir"> FRCN </strong> : <a class="ae lw" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快速 R-CNN </a>。</li><li id="df67" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir"> RPN_un </strong> : <a class="ae lw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>在提议网络和分类器网络之间具有非共享的 CNN。</li><li id="2aa0" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir"> RPN </strong> : <a class="ae lw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>。</li><li id="595e" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">工艺</strong>:带级联建议网络，比 VOC 2007 中的 RPN_un 好但比 RPN 差。使用级联分类器网络，它在 VOC 2007 和 VOC 2012 中都比<a class="ae lw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/daa5bc95d2456e2136551df99ed015d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zCI4kQFKh4or_Vaje8dYww.png"/></div></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">CRAFT on PASCAL VOC 2007 Test Set</strong></figcaption></figure></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h2 id="026c" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">4.2.ILSVRC 目标检测任务</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/ee6b91e98aa853a4b95f6cfbe3399a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*YXw1UiLrnSf92I-xnlWbmA.png"/></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">Recall Rate (%) on ILSVRC val2 Set</strong></figcaption></figure><ul class=""><li id="3c57" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated"><strong class="kt ir"> 0.6 NMS </strong>:更严格的 NMS，比基本版好。</li><li id="9927" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">重新评分</strong>:通过考虑级联结构两个阶段的两个评分，对每个提案重新评分也有帮助。</li><li id="4281" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir"> +DeepBox </strong>:融合 DeepBox 提议，将 RPN 提议作为 FRCN 网的融合输入，使召回率提高到 94%以上。比+SS 要好。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/2a90c98074351894f92b2f16e77c6e5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*nfKAQikeihj60q962lgspA.png"/></div></div><figcaption class="og oh gj gh gi oi oj bd b be z dk"><strong class="bd ok">mAP (%) on ILSVRC val2 Set</strong></figcaption></figure><ul class=""><li id="3d8f" class="lx ly iq kt b ku kv kx ky la lz le ma li mb lm mc md me mf bi translated">这里使用<a class="ae lw" href="https://medium.com/coinmonks/paper-review-of-googlenet-inception-v1-winner-of-ilsvlc-2014-image-classification-c2b3565a64e7" rel="noopener"> GoogLeNet </a>模型，带<a class="ae lw" href="https://medium.com/@sh.tsang/review-batch-normalization-inception-v2-bn-inception-the-2nd-to-surpass-human-level-18e2d0f56651" rel="noopener">批量归一化</a>。</li><li id="c1d6" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated">ILSVRC 2013train + 2014train + val1 用作训练集。</li><li id="56d9" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">使用级联建议网络，实现了 47.0%的 mAP</strong>，这已经超过了之前最先进的系统(如 Superpixel Labeling 和<a class="ae lw" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6"> DeepID-Net </a>的集合结果。</li><li id="6d8e" class="lx ly iq kt b ku no kx np la nq le nr li ns lm mc md me mf bi translated"><strong class="kt ir">还具有级联分类器网络，48.5% mAP </strong>，额外的 1.5%绝对增益。</li></ul></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><p id="d7a0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">由于级联网络同时适用于区域建议网络和分类器网络，提高了检测精度。</p></div><div class="ab cl mj mk hu ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="ij ik il im in"><h2 id="c429" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">参考</h2><p id="7ffc" class="pw-post-body-paragraph kr ks iq kt b ku ni jr kw kx nj ju kz la ox lc ld le oy lg lh li oz lk ll lm ij bi translated">【2016 CVPR】【工艺】<a class="ae lw" href="https://arxiv.org/abs/1604.03239" rel="noopener ugc nofollow" target="_blank"> <br/>从图像中工艺物体</a></p><h2 id="72ca" class="nt mr iq bd ms nu nv dn mw nw nx dp na la ny nz nc le oa ob ne li oc od ng oe bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kr ks iq kt b ku ni jr kw kx nj ju kz la ox lc ld le oy lg lh li oz lk ll lm ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(是)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(况)(。</p><p id="8b77" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">物体检测<br/></strong><a class="ae lw" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lw" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lw" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lw" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lw" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lw" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lw" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lw" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lw" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a> yolo 9000[<a class="ae lw" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolov 3</a>][<a class="ae lw" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae lw" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>][<a class="ae lw" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">语义切分<br/></strong><a class="ae lw" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lw" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lw" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lw" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】【parse net<a class="ae lw" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lw" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae lw" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a><a class="ae lw" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a></p><p id="fc65" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">生物医学图像分割<br/></strong><a class="ae lw" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a><a class="ae lw" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a><a class="ae lw" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a><a class="ae lw" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a><a class="ae lw" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a><a class="ae lw" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a></p><p id="3134" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir"> 实例分段 <br/> </strong> <a class="ae lw" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae lw" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae lw" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae lw" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae lw" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae lw" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a>】</p><p id="58de" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p></div></div>    
</body>
</html>