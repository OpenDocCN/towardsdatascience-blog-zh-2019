<html>
<head>
<title>A look into GLTR (using GPT-2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GLTR 研究(使用 GPT 新协议)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-look-into-gltr-using-gpt-2-76d823057421?source=collection_archive---------12-----------------------#2019-04-21">https://towardsdatascience.com/a-look-into-gltr-using-gpt-2-76d823057421?source=collection_archive---------12-----------------------#2019-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/35caaa16105c56513961cf3f722b1264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20gYXbf-ZNSNoULtzuuNTg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Taken from <a class="ae jd" href="http://gltr.io/" rel="noopener ugc nofollow" target="_blank">http://gltr.io/</a></figcaption></figure><div class=""/><p id="b125" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着 BERT 模型[1]的发布，自然语言处理(NLP)领域在过去的一年中取得了巨大的进步，该模型改善了许多问题的技术水平，如文本分类、问题回答等。现在，Open AI [2]发布了名为 GPT-2 [3]的语言模型，据称该模型能够生成无法识别为机器或人类所写的文本样本。</p><p id="2948" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最近，来自麻省理工学院-IBM 沃森人工智能实验室和<a class="ae jd" href="http://nlp.seas.harvard.edu/" rel="noopener ugc nofollow" target="_blank">哈佛大学的合作团队推出了一款名为<strong class="kf jh">G</strong>iant<strong class="kf jh">L</strong>angauge Model<strong class="kf jh">T</strong>est<strong class="kf jh">R</strong>oom(<strong class="kf jh">GLTR</strong>)的文本取证工具。GLTR 基本上使用 GPT-2 语言模型来区分人类生成的文本和机器生成的文本。</a></p><p id="dca2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将使用<strong class="kf jh">GLTR(Python 中的)</strong>来分析不同来源的文本片段，看看它们在文本的平滑度方面有何不同。我将使用 https://github.com/HendrikStrobelt/detecting-fake-text<a class="ae jd" href="https://github.com/HendrikStrobelt/detecting-fake-text" rel="noopener ugc nofollow" target="_blank">的 GLTR 代码。</a></p><p id="c765" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面是这段代码，<strong class="kf jh">将文本作为输入</strong>并使用 GPT-2 模型来<strong class="kf jh">输出包含三样东西的有效载荷</strong>。</p><ol class=""><li id="bfe9" class="lb lc jg kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">给定上下文中每个单词的概率。</li><li id="93ca" class="lb lc jg kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">给定上下文，整个词汇表中每个单词的等级。</li><li id="aa59" class="lb lc jg kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">给定上下文，前 K 个单词及其概率。</li></ol><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="9aaf" class="ly lz jg lu b gy ma mb l mc md"><em class="me">import </em>numpy <em class="me">as </em>np<br/><em class="me">import </em>torch<br/><em class="me">import </em>time<br/><em class="me">import </em>nltk<br/><br/><em class="me">from </em>pytorch_pretrained_bert <em class="me">import </em>(GPT2LMHeadModel, GPT2Tokenizer,<br/>                                     BertTokenizer, BertForMaskedLM)<br/><br/><em class="me">from </em>matplotlib <em class="me">import </em>pyplot <em class="me">as </em>plt<br/><br/><em class="me">class </em>AbstractLanguageChecker():<br/>    <em class="me">"""<br/>    Abstract Class that defines the Backend API of GLTR.<br/><br/>    To extend the GLTR interface, you need to inherit this and<br/>    fill in the defined functions.<br/>    """<br/><br/>    def __init__</em>(<em class="me">self</em>):<br/>        <em class="me">'''<br/>        In the subclass, you need to load all necessary components<br/>        for the other functions.<br/>        Typically, this will comprise a tokenizer and a model.<br/>        '''<br/>        self</em>.device = torch.device(<br/>            "cuda" <em class="me">if </em>torch.cuda.is_available() <em class="me">else </em>"cpu")<br/><br/>    <em class="me">def </em>check_probabilities(<em class="me">self</em>, in_text, topk=40):<br/>        <em class="me">'''<br/>        Function that GLTR interacts with to check the probabilities of words<br/><br/>        Params:<br/>        - in_text: str -- The text that you want to check<br/>        - topk: int -- Your desired truncation of the head of the distribution<br/><br/>        Output:<br/>        - payload: dict -- The wrapper for results in this function, described below<br/><br/>        Payload values<br/>        ==============<br/>        bpe_strings: list of str -- Each individual token in the text<br/>        real_topk: list of tuples -- (ranking, prob) of each token<br/>        pred_topk: list of list of tuple -- (word, prob) for all topk<br/>        '''<br/>        raise </em>NotImplementedError<br/><br/>    <em class="me">def </em>postprocess(<em class="me">self</em>, token):<br/>        <em class="me">"""<br/>        clean up the tokens from any special chars and encode<br/>        leading space by UTF-8 code '\u0120', linebreak with UTF-8 code 266 '\u010A'<br/>        </em><strong class="lu jh"><em class="me">:param</em></strong><em class="me"> token:  str -- raw token text<br/>        </em><strong class="lu jh"><em class="me">:return</em></strong><em class="me">: str -- cleaned and re-encoded token text<br/>        """<br/>        raise </em>NotImplementedError<br/><br/><br/><em class="me">def </em>top_k_logits(logits, k):<br/>    <em class="me">'''<br/>    Filters logits to only the top k choices<br/>    from https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py<br/>    '''<br/>    if </em>k == 0:<br/>        <em class="me">return </em>logits<br/>    values, _ = torch.topk(logits, k)<br/>    min_values = values[:, -1]<br/>    <em class="me">return </em>torch.where(logits &lt; min_values,<br/>                       torch.ones_like(logits, dtype=logits.dtype) * -1e10,<br/>                       logits)<br/><br/><br/><br/><em class="me">class </em>LM(AbstractLanguageChecker):<br/>    <em class="me">def __init__</em>(<em class="me">self</em>, model_name_or_path="gpt2"):<br/>        super(LM, <em class="me">self</em>).__init__()<br/>        <em class="me">self</em>.enc = GPT2Tokenizer.from_pretrained(model_name_or_path)<br/>        <em class="me">self</em>.model = GPT2LMHeadModel.from_pretrained(model_name_or_path)<br/>        <em class="me">self</em>.model.to(<em class="me">self</em>.device)<br/>        <em class="me">self</em>.model.eval()<br/>        <em class="me">self</em>.start_token = '&lt;|endoftext|&gt;'<br/>        print("Loaded GPT-2 model!")<br/><br/>    <em class="me">def </em>check_probabilities(<em class="me">self</em>, in_text, topk=40):<br/>        <em class="me"># Process input<br/>        </em>start_t = torch.full((1, 1),<br/>                             <em class="me">self</em>.enc.encoder[<em class="me">self</em>.start_token],<br/>                             device=<em class="me">self</em>.device,<br/>                             dtype=torch.long)<br/>        context = <em class="me">self</em>.enc.encode(in_text)<br/>        context = torch.tensor(context,<br/>                               device=<em class="me">self</em>.device,<br/>                               dtype=torch.long).unsqueeze(0)<br/>        context = torch.cat([start_t, context], dim=1)<br/>        <em class="me"># Forward through the model<br/>        </em>logits, _ = <em class="me">self</em>.model(context)<br/><br/>        <em class="me"># construct target and pred<br/>        </em>yhat = torch.softmax(logits[0, :-1], dim=-1)<br/>        y = context[0, 1:]<br/>        <em class="me"># Sort the predictions for each timestep<br/>        </em>sorted_preds = np.argsort(-yhat.data.cpu().numpy())<br/>        <em class="me"># [(pos, prob), ...]<br/>        </em>real_topk_pos = list(<br/>            [int(np.where(sorted_preds[i] == y[i].item())[0][0])<br/>             <em class="me">for </em>i <em class="me">in </em>range(y.shape[0])])<br/>        real_topk_probs = yhat[np.arange(<br/>            0, y.shape[0], 1), y].data.cpu().numpy().tolist()<br/>        real_topk_probs = list(map(<em class="me">lambda </em>x: round(x, 5), real_topk_probs))<br/><br/>        real_topk = list(zip(real_topk_pos, real_topk_probs))<br/>        <em class="me"># [str, str, ...]<br/>        </em>bpe_strings = [<em class="me">self</em>.enc.decoder[s.item()] <em class="me">for </em>s <em class="me">in </em>context[0]]<br/><br/>        bpe_strings = [<em class="me">self</em>.postprocess(s) <em class="me">for </em>s <em class="me">in </em>bpe_strings]<br/><br/>        <em class="me"># [[(pos, prob), ...], [(pos, prob), ..], ...]<br/>        </em>pred_topk = [<br/>            list(zip([<em class="me">self</em>.enc.decoder[p] <em class="me">for </em>p <em class="me">in </em>sorted_preds[i][:topk]],<br/>                     list(map(<em class="me">lambda </em>x: round(x, 5),<br/>                              yhat[i][sorted_preds[i][<br/>                                      :topk]].data.cpu().numpy().tolist()))))<br/>            <em class="me">for </em>i <em class="me">in </em>range(y.shape[0])]<br/><br/>        pred_topk = [[(<em class="me">self</em>.postprocess(t[0]), t[1]) <em class="me">for </em>t <em class="me">in </em>pred] <em class="me">for </em>pred <em class="me">in </em>pred_topk]<br/>        payload = {'bpe_strings': bpe_strings,<br/>                   'real_topk': real_topk,<br/>                   'pred_topk': pred_topk}<br/>        <em class="me">if </em>torch.cuda.is_available():<br/>            torch.cuda.empty_cache()<br/><br/>        <em class="me">return </em>payload<br/><br/>    <em class="me">def </em>sample_unconditional(<em class="me">self</em>, length=100, topk=5, temperature=1.0):<br/>        <em class="me">'''<br/>        Sample `length` words from the model.<br/>        Code strongly inspired by<br/>        https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_gpt2.py<br/><br/>        '''<br/>        </em>context = torch.full((1, 1),<br/>                             <em class="me">self</em>.enc.encoder[<em class="me">self</em>.start_token],<br/>                             device=<em class="me">self</em>.device,<br/>                             dtype=torch.long)<br/>        prev = context<br/>        output = context<br/>        past = <em class="me">None<br/>        # Forward through the model<br/>        with </em>torch.no_grad():<br/>            <em class="me">for </em>i <em class="me">in </em>range(length):<br/>                logits, past = <em class="me">self</em>.model(prev, past=past)<br/>                logits = logits[:, -1, :] / temperature<br/>                <em class="me"># Filter predictions to topk and softmax<br/>                </em>probs = torch.softmax(top_k_logits(logits, k=topk),<br/>                                      dim=-1)<br/>                <em class="me"># Sample<br/>                </em>prev = torch.multinomial(probs, num_samples=1)<br/>                <em class="me"># Construct output<br/>                </em>output = torch.cat((output, prev), dim=1)<br/><br/>        output_text = <em class="me">self</em>.enc.decode(output[0].tolist())<br/>        <em class="me">return </em>output_text<br/><br/>    <em class="me">def </em>postprocess(<em class="me">self</em>, token):<br/>        with_space = <em class="me">False<br/>        </em>with_break = <em class="me">False<br/>        if </em>token.startswith('Ġ'):<br/>            with_space = <em class="me">True<br/>            </em>token = token[1:]<br/>            <em class="me"># print(token)<br/>        elif </em>token.startswith('â'):<br/>            token = ' '<br/>        <em class="me">elif </em>token.startswith('Ċ'):<br/>            token = ' '<br/>            with_break = <em class="me">True<br/><br/>        </em>token = '-' <em class="me">if </em>token.startswith('â') <em class="me">else </em>token<br/>        token = '“' <em class="me">if </em>token.startswith('ľ') <em class="me">else </em>token<br/>        token = '”' <em class="me">if </em>token.startswith('Ŀ') <em class="me">else </em>token<br/>        token = "'" <em class="me">if </em>token.startswith('Ļ') <em class="me">else </em>token<br/><br/>        <em class="me">if </em>with_space:<br/>            token = '\u0120' + token<br/>        <em class="me">if </em>with_break:<br/>            token = '\u010A' + token<br/><br/>        <em class="me">return </em>token</span></pre><p id="d9e3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了检查一篇文章的通顺程度，我会画出每个单词的排名。根据 GPT-2 语言模型，如果文本中单词的排名较高，则文本将是不平滑的。以下代码用于为文本创建这些绘图。</p><pre class="lp lq lr ls gt lt lu lv lw aw lx bi"><span id="e400" class="ly lz jg lu b gy ma mb l mc md"><em class="me">def </em>plot_text(vals, what, name):<br/>    <em class="me">if </em>what=="prob":<br/>        ourvals = vals[0]<br/>        x = list(range(1,len(ourvals)+1))<br/>        y = ourvals<br/>        plt.plot(x, y, color='orange')<br/>        plt.ylim(0,1)<br/>        plt.savefig(name + ".png")<br/>        <em class="me"># plt.show()<br/>    elif </em>what=="rank":<br/>        ourvals = vals[1]<br/>        x = list(range(1, len(ourvals) + 1))<br/>        y = ourvals<br/>        plt.plot(x, y, color='orange')<br/>        plt.ylim(-1000, 50000)<br/>        plt.savefig(name + ".png")<br/>        <em class="me"># plt.show()</em></span><span id="e64e" class="ly lz jg lu b gy mf mb l mc md"><em class="me">def </em>main_code(raw_text):<em class="me"><br/></em><br/>    lm = LM()<br/>    start = time.time()<br/>    payload = lm.check_probabilities(raw_text, topk=5)<br/>    <em class="me"># print(payload["pred_topk"])<br/>    </em>real_topK = payload["real_topk"]<br/>    ranks = [i[0] <em class="me">for </em>i <em class="me">in </em>real_topK]<br/>    preds = [i[1] <em class="me">for </em>i <em class="me">in </em>real_topK]<br/>    plot_text([preds, ranks], 'rank', "rank_")<br/>    end = time.time()<br/>    print("{:.2f} Seconds for a check with GPT-2".format(end - start))</span></pre><p id="4565" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们看看不同来源的文本在流畅度方面是否有所不同。我们将检查以下文本。</p><ol class=""><li id="a2a2" class="lb lc jg kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">由 GPT-2 语言模型生成的文本。</li><li id="8815" class="lb lc jg kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">新闻文章中的文字。</li><li id="3ec2" class="lb lc jg kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">来自博客的文本。</li></ol><p id="8be9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是情节:</p><figure class="lp lq lr ls gt is gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/95076a40b999c8336c306ba304ec7d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*B6vrf1tjv2l_OfkxLSWB3w.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Rank plot for GPT-2 Generated Text</figcaption></figure><figure class="lp lq lr ls gt is gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/3f8ceaf95901b74be40d5efae50820e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*U7H6c_TIVnm12u_bhNa-Ng.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Rank plot for News Article</figcaption></figure><figure class="lp lq lr ls gt is gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/e0f840860ed28401fe98bca801b786fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*laNDdcodnc6rdH4BNe5h9A.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Rank plot for Blog Text</figcaption></figure><p id="97fb" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GPT-2 生成的文本的等级图非常平滑，因为它是由 GPT-2 模型本身检查的。博客和新闻文章数据有一些峰值，这表明围绕这些词有一些不一致之处。</p><p id="aecc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">作为一个有趣的练习，你可以分析不同类型的文章，看看在流畅度方面有什么不同。</p><p id="0921" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GLTR 的一个主要用途可能是在 Grammarly 这样的系统中。它可以指出导致不一致的单词，并提示用户进行更改。这至少是一个不错的 GitHub 项目。</p><h2 id="71e8" class="ly lz jg bd mh mi mj dn mk ml mm dp mn ko mo mp mq ks mr ms mt kw mu mv mw mx bi translated"><strong class="ak">参考文献</strong></h2><p id="b620" class="pw-post-body-paragraph kd ke jg kf b kg my ki kj kk mz km kn ko na kq kr ks nb ku kv kw nc ky kz la ij bi translated">[1]<a class="ae jd" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1810.04805</a></p><p id="8082" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2]https://openai.com/<a class="ae jd" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="8994" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]<a class="ae jd" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">https://d4mucfpksywv . cloudfront . net/better-language-models/language _ models _ are _ unsupervised _ multask _ learners . pdf</a></p></div></div>    
</body>
</html>