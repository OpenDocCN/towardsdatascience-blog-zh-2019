<html>
<head>
<title>Build your own dataset using Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Scrapy 构建自己的数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84?source=collection_archive---------17-----------------------#2019-09-25">https://towardsdatascience.com/data-science-pipeline-part-1-obtaining-data-from-web-using-scrapy-fe0ad9963d84?source=collection_archive---------17-----------------------#2019-09-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="66b6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何构建自己的数据集，并将其用于数据科学和分析</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c27a19a382b4c9c7d22f941790888315.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pchqpvJK4Jbn-I12"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@markusspiske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Markus Spiske</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="693e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据科学管道</h1><ul class=""><li id="2d37" class="lo lp iq lq b lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">获取数据(在本文中讨论)</li><li id="672d" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">数据清理</li><li id="015f" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">探索/可视化数据</li><li id="91c0" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">数据建模</li><li id="d26f" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">解释结果</li></ul><p id="df2f" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">几乎每个数据科学家都面临的一个常见问题是收集数据。虽然有很多网站可以找到<a class="ae kv" rel="noopener" target="_blank" href="/top-10-great-sites-with-free-data-sets-581ac8f6334">这篇文章</a>中提到的数据集，但这总是不够。有时，构建自己的数据集是一条可行之路。另一方面，尽管 web 是一个巨大的数据源，我们只需要知道如何有效地挖掘它。在本文中，我将带您了解使用<a class="ae kv" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>从 web 抓取数据的过程，这是一个用 Python 编写的强大的 web 爬行框架。</p><p id="e1fd" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">如果你想直接进入代码，就去看看这个<a class="ae kv" href="https://github.com/sagunsh/sofifa" rel="noopener ugc nofollow" target="_blank"> github repo </a>。</p><h1 id="5f51" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据源</h1><p id="6baa" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">国际足联 20 名球员的排名出来了，作为一个狂热的足球爱好者，我发现这样的数据很吸引人。你可以在<a class="ae kv" href="https://sofifa.com/?col=oa&amp;sort=desc" rel="noopener ugc nofollow" target="_blank"> sofifa </a>上找到这些数据。我们将抓取数据，并使用 Scrapy 将它们保存在 csv 文件中。</p><h1 id="8583" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">安装和设置</h1><p id="e7eb" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">在这篇文章中，我将使用 Python 3 和 Scrapy 1.7。对于 Linux 和 Mac，使用以下命令通过 pip 安装 scrapy 相当简单:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="b8ad" class="ni kx iq ne b gy nj nk l nl nm">pip install Scrapy  # or pip3 install Scrapy</span></pre><p id="4037" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">Windows 用户需要安装 Anaconda。你可以在这里找到安装说明<a class="ae kv" href="https://docs.anaconda.com/anaconda/install/windows/" rel="noopener ugc nofollow" target="_blank"/>。安装 Anaconda 后，打开 Anaconda 提示符并键入以下命令来安装 Scrapy:</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7053" class="ni kx iq ne b gy nj nk l nl nm">conda install -c conda-forge scrapy</span></pre><h1 id="1b88" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">创建项目</h1><p id="2fcc" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">一旦我们的系统中有了 Scrapy，下一步就是创建一个项目。打开终端/anaconda 提示符并键入</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="3bb2" class="ni kx iq ne b gy nj nk l nl nm">scrapy startproject fifa20</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/14d86745051ff8280a3a5d31bb77244c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aKolBG77_kLowPYfcUEy_A.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">starting scrapy project</figcaption></figure><p id="5b5e" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">默认情况下，Scrapy 会在<code class="fe no np nq ne b">fifa20.</code>中创建一堆文件和目录</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/75e65aba1acf2b27bec4cb3579e799c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*oDSZBMpxjQe-LGIVWQ3wGQ.jpeg"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">inside fifa20 project</figcaption></figure><p id="284d" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">在<code class="fe no np nq ne b">fifa20</code>里面，你会发现另一个<code class="fe no np nq ne b">fifa20</code>文件夹，里面包含了我们所有的代码。<code class="fe no np nq ne b">spiders</code>目录包含负责从网络中提取数据的 Scrapy spiders。一般来说，每个蜘蛛负责抓取一个特定的网站。一个 scrapy 项目可以有多个蜘蛛。例如，如果你正在创建一个价格监控系统，你可能想要抓取多个网站，如亚马逊、沃尔玛、新蛋等。每个网站都有一个专门的蜘蛛来处理抓取逻辑。</p><p id="3bbf" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">包含项目的设置，例如运行多少个并发线程，两个请求之间的延迟，使用哪个中间件或管道。</p><p id="9ea6" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">对于这个项目，我们将设置一个真实的用户代理。继续在<code class="fe no np nq ne b">settings.py</code>中更改这些变量。</p><p id="ea60" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">您的<code class="fe no np nq ne b">settings.py</code>文件看起来会像这样</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="a99f" class="ni kx iq ne b gy nj nk l nl nm">BOT_NAME = 'fifa20'</span><span id="385f" class="ni kx iq ne b gy ns nk l nl nm">SPIDER_MODULES = ['fifa20.spiders']<br/>NEWSPIDER_MODULE = 'fifa20.spiders'</span><span id="ee03" class="ni kx iq ne b gy ns nk l nl nm"># Crawl responsibly by identifying yourself (and your website) on the user-agent<br/>USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</span><span id="af2e" class="ni kx iq ne b gy ns nk l nl nm"># Obey robots.txt rules<br/>ROBOTSTXT_OBEY = True</span></pre><p id="3f94" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">查看<a class="ae kv" href="https://doc.scrapy.org/en/latest/topics/architecture.html" rel="noopener ugc nofollow" target="_blank"> Scrapy architecture </a>探索更多关于中间件、物品、管道和其他设置的信息。</p><h1 id="cb23" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">创建蜘蛛</h1><p id="8bd6" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">要创建一个蜘蛛，导航到外部的<code class="fe no np nq ne b">fifa20</code>项目目录并键入</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7fef" class="ni kx iq ne b gy nj nk l nl nm">scrapy genspider sofifa sofifa.com</span></pre><p id="31f6" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">这将在<code class="fe no np nq ne b">spiders</code>目录中创建一个名为 sofifa 的蜘蛛。这是文件的样子</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f241" class="ni kx iq ne b gy nj nk l nl nm"># -*- coding: utf-8 -*-<br/>import scrapy</span><span id="8f91" class="ni kx iq ne b gy ns nk l nl nm">class SofifaSpider(scrapy.Spider):<br/>    name = 'sofifa'<br/>    allowed_domains = ['sofifa.com']<br/>    start_urls = ['<a class="ae kv" href="http://sofifa.com/'" rel="noopener ugc nofollow" target="_blank">http://sofifa.com/'</a>]</span><span id="f9f5" class="ni kx iq ne b gy ns nk l nl nm">    def parse(self, response):<br/>        pass</span></pre><p id="720c" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">让我们试着理解这是怎么回事。</p><ul class=""><li id="871e" class="lo lp iq lq b lr mn lt mp lv nt lx nu lz nv mb mc md me mf bi translated"><code class="fe no np nq ne b">SofifaSpider</code>是 spider 类，它继承了包含我们所有抓取代码的<code class="fe no np nq ne b">scrapy.Spider</code>。</li><li id="0a31" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">命令中使用的蜘蛛名称是<code class="fe no np nq ne b">sofifa</code>。我们稍后将使用它从命令行运行蜘蛛。</li><li id="c716" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">allowed_domains</code>包含蜘蛛爬行的域列表。在这种情况下，蜘蛛只抓取 sofifa.com 域名的网址，而忽略其他网址。这有助于我们限制爬虫仅爬行期望的域。</li><li id="58df" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">start_urls</code>包含一个 url 列表，蜘蛛使用它作为初始 URL。</li><li id="9c89" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">parse</code>方法是一个回调函数，默认情况下在爬取<code class="fe no np nq ne b">start_urls</code>中的 URL 后调用。</li><li id="5b48" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">parse</code>(或任何回调)方法接收包含下载网页的响应对象。</li></ul><p id="4047" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">我们将使用以下网址作为我们的起始网址</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="07e1" class="ni kx iq ne b gy nj nk l nl nm">start_urls = ['<a class="ae kv" href="https://sofifa.com/?col=oa&amp;sort=desc&amp;showCol%5B%5D=ae&amp;showCol%5B%5D=hi&amp;showCol%5B%5D=wi&amp;showCol%5B%5D=pf&amp;showCol%5B%5D=oa&amp;showCol%5B%5D=pt&amp;showCol%5B%5D=bo&amp;showCol%5B%5D=bp&amp;showCol%5B%5D=jt&amp;showCol%5B%5D=vl&amp;showCol%5B%5D=wg&amp;showCol%5B%5D=rc&amp;showCol%5B%5D=wk&amp;showCol%5B%5D=sk&amp;showCol%5B%5D=aw&amp;showCol%5B%5D=dw&amp;showCol%5B%5D=ir" rel="noopener ugc nofollow" target="_blank">https://sofifa.com/?col=oa&amp;sort=desc&amp;showCol%5B%5D=ae&amp;showCol%5B%5D=hi&amp;showCol%5B%5D=wi&amp;showCol%5B%5D=pf&amp;showCol%5B%5D=oa&amp;showCol%5B%5D=pt&amp;showCol%5B%5D=bo&amp;showCol%5B%5D=bp&amp;showCol%5B%5D=jt&amp;showCol%5B%5D=vl&amp;showCol%5B%5D=wg&amp;showCol%5B%5D=rc&amp;showCol%5B%5D=wk&amp;showCol%5B%5D=sk&amp;showCol%5B%5D=aw&amp;showCol%5B%5D=dw&amp;showCol%5B%5D=ir</a>']</span></pre><p id="fd8a" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">不要担心看起来杂乱的 url，它是通过选择我们希望在最终数据集中使用的某些列而生成的。</p><h1 id="a8eb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">使用 Scrapy Shell 剖析 HTML</h1><p id="165d" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">Scrapy shell 是一个有用的命令行工具，可以用来分析网页的结构，找出想要的 xpaths 和选择器。键入以下命令开始</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0a53" class="ni kx iq ne b gy nj nk l nl nm">scrapy shell "<a class="ae kv" href="https://sofifa.com/?col=oa&amp;sort=desc&amp;showCol%5B%5D=ae&amp;showCol%5B%5D=hi&amp;showCol%5B%5D=wi&amp;showCol%5B%5D=pf&amp;showCol%5B%5D=oa&amp;showCol%5B%5D=pt&amp;showCol%5B%5D=bo&amp;showCol%5B%5D=bp&amp;showCol%5B%5D=jt&amp;showCol%5B%5D=vl&amp;showCol%5B%5D=wg&amp;showCol%5B%5D=rc&amp;showCol%5B%5D=wk&amp;showCol%5B%5D=sk&amp;showCol%5B%5D=aw&amp;showCol%5B%5D=dw&amp;showCol%5B%5D=ir" rel="noopener ugc nofollow" target="_blank">https://sofifa.com/?col=oa&amp;sort=desc&amp;showCol%5B%5D=ae&amp;showCol%5B%5D=hi&amp;showCol%5B%5D=wi&amp;showCol%5B%5D=pf&amp;showCol%5B%5D=oa&amp;showCol%5B%5D=pt&amp;showCol%5B%5D=bo&amp;showCol%5B%5D=bp&amp;showCol%5B%5D=jt&amp;showCol%5B%5D=vl&amp;showCol%5B%5D=wg&amp;showCol%5B%5D=rc&amp;showCol%5B%5D=wk&amp;showCol%5B%5D=sk&amp;showCol%5B%5D=aw&amp;showCol%5B%5D=dw&amp;showCol%5B%5D=ir</a>"</span></pre><p id="f298" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">你会看到大量的日志和消息。它们只是您项目的设置。上面的命令为我们做的是下载给定 url 的 HTML 内容，并且可以通过<code class="fe no np nq ne b">response</code>对象访问。要进行检查，请键入</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="7bcc" class="ni kx iq ne b gy nj nk l nl nm">response.url</span></pre><p id="ab47" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">你会看到</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="d041" class="ni kx iq ne b gy nj nk l nl nm">'<a class="ae kv" href="https://sofifa.com/?col=oa&amp;sort=desc&amp;showCol%5B%5D=ae&amp;showCol%5B%5D=hi&amp;showCol%5B%5D=wi&amp;showCol%5B%5D=pf&amp;showCol%5B%5D=oa&amp;showCol%5B%5D=pt&amp;showCol%5B%5D=bo&amp;showCol%5B%5D=bp&amp;showCol%5B%5D=jt&amp;showCol%5B%5D=vl&amp;showCol%5B%5D=wg&amp;showCol%5B%5D=rc&amp;showCol%5B%5D=wk&amp;showCol%5B%5D=sk&amp;showCol%5B%5D=aw&amp;showCol%5B%5D=dw&amp;showCol%5B%5D=ir'" rel="noopener ugc nofollow" target="_blank">https://sofifa.com/?col=oa&amp;sort=desc&amp;showCol%5B%5D=ae&amp;showCol%5B%5D=hi&amp;showCol%5B%5D=wi&amp;showCol%5B%5D=pf&amp;showCol%5B%5D=oa&amp;showCol%5B%5D=pt&amp;showCol%5B%5D=bo&amp;showCol%5B%5D=bp&amp;showCol%5B%5D=jt&amp;showCol%5B%5D=vl&amp;showCol%5B%5D=wg&amp;showCol%5B%5D=rc&amp;showCol%5B%5D=wk&amp;showCol%5B%5D=sk&amp;showCol%5B%5D=aw&amp;showCol%5B%5D=dw&amp;showCol%5B%5D=ir'</a></span></pre><p id="6a8d" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">如果您想在浏览器中查看页面的外观，请键入</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6e0b" class="ni kx iq ne b gy nj nk l nl nm">view(response)</span></pre><p id="25a0" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">您将能够在浏览器中查看该页面。</p><p id="a216" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">我们可以选择在 Scrapy 中使用 css 选择器和/或 xpath。如果您不熟悉 css 和 xpath 的概念，请查看这些教程以了解它是如何工作的。</p><ul class=""><li id="dfc2" class="lo lp iq lq b lr mn lt mp lv nt lx nu lz nv mb mc md me mf bi translated"><a class="ae kv" href="https://docs.scrapy.org/en/xpath-tutorial/topics/xpath-tutorial.html" rel="noopener ugc nofollow" target="_blank">https://docs . scrapy . org/en/XPath-tutorial/topics/XPath-tutorial . html</a></li><li id="f1ef" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="https://docs.scrapy.org/en/xpath-tutorial/topics/selectors.html" rel="noopener ugc nofollow" target="_blank">https://docs . scrapy . org/en/XPath-tutorial/topics/selectors . html</a></li><li id="0d33" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">【https://www.guru99.com/xpath-selenium.html】</li><li id="87b8" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="http://scrapingauthority.com/2016/09/07/xpath-and-css-selectors/" rel="noopener ugc nofollow" target="_blank">http://scraping authority . com/2016/09/07/XPath-and-CSS-selectors/</a></li></ul><p id="d287" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">现在让我们检查网页并获取数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/dd065a01a09286d0ea43ee1a6bea7e79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_w49bD5brlQihrxgYwZ6A.jpeg"/></div></div></figure><p id="5c36" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">我们可以看到所有的数据都在一个表标签里面，每个球员的信息都在<code class="fe no np nq ne b">tbody -&gt; tr</code>里面。遍历所有 tr 标签将有助于我们接近数据。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="8d69" class="ni kx iq ne b gy nj nk l nl nm">for player in response.css('table&gt;tbody&gt;tr'):<br/>    print(player.css('td.col-name&gt;div&gt;a::attr(title)').extract())</span></pre><p id="2dca" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">这将同时打印出国家和玩家的名字。让我们试着理解这个片段。</p><ul class=""><li id="5c6b" class="lo lp iq lq b lr mn lt mp lv nt lx nu lz nv mb mc md me mf bi translated"><code class="fe no np nq ne b">table&gt;tbody&gt;tr</code>选择<code class="fe no np nq ne b">tbody</code>内的<code class="fe no np nq ne b">tr</code>标签，该标签在<code class="fe no np nq ne b">table</code>内</li><li id="60c0" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">response.css('table&gt;tbody&gt;tr')</code>返回选择器列表</li><li id="c3c6" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">player.css('td.col-name&gt;div&gt;a::attr(title)')</code>选择带有类<code class="fe no np nq ne b">col-name</code>的第一个<code class="fe no np nq ne b">td</code>，类<code class="fe no np nq ne b">col-name</code>包含一个<code class="fe no np nq ne b">div</code>，其内部是一个<code class="fe no np nq ne b">a</code>标签。<code class="fe no np nq ne b">::attr(title)</code>提取你在 HTML 中看到的标题属性，分别是国家和玩家名字。</li></ul><p id="f1dc" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">为了简单起见，让我们用一个玩家的数据测试所有东西，并在我们的脚本中使用它。所以在 shell 中，键入</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f16a" class="ni kx iq ne b gy nj nk l nl nm">player = response.css('table&gt;tbody&gt;tr')[0]</span><span id="cf54" class="ni kx iq ne b gy ns nk l nl nm">country = player.css('td.col-name&gt;div&gt;a::attr(title)').extract()[0]<br/>player = player.css('td.col-name&gt;div&gt;a::attr(title)').extract()[1]<br/>positions = player.css('td.col-name span.pos ::text').extract()</span></pre><p id="acdd" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">遍历标签时使用<code class="fe no np nq ne b">&gt;</code>和空格的区别在于</p><ul class=""><li id="bdd9" class="lo lp iq lq b lr mn lt mp lv nt lx nu lz nv mb mc md me mf bi translated"><code class="fe no np nq ne b">&gt;</code>表示类似于<code class="fe no np nq ne b">td.col-name&gt;div&gt;a</code>中的直接子标签</li><li id="35b7" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">空格意味着任何子元素，不一定是直接子元素，比如在<code class="fe no np nq ne b">td.col-name span.pos</code>中，中间包含一个<code class="fe no np nq ne b">a</code>标签。</li></ul><p id="4088" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">我们来看看第一个位置，这是玩家的主要位置。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="dbc7" class="ni kx iq ne b gy nj nk l nl nm">primary_position = player.css('td.col-name span.pos ::text').get()</span></pre><blockquote class="nx ny nz"><p id="a969" class="ml mm oa lq b lr mn jr mo lt mp ju mq ob mr ms mt oc mu mv mw od mx my mz mb ij bi translated">extract()返回一个列表，而 get()或 extract_first()返回第一个元素</p></blockquote><p id="48ef" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">类似地，我们可以得到其他字段</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="2b9d" class="ni kx iq ne b gy nj nk l nl nm">age = player.css('td.col-ae ::text').get()<br/>overall = player.css('td.col-oa ::text').get()<br/>potential = player.css('td.col-oa ::text').get()<br/>height = player.css('td.col-hi ::text').get()<br/>value = player.css('td.col-vl ::text').get()<br/>wage = player.css('td.col-wg ::text').get()</span></pre><p id="4425" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated"><em class="oa">以此类推。</em></p><h1 id="f8bb" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">处理分页</h1><p id="d304" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">滚动到页面底部，我们可以看到下一页链接。每个页面有 60 个玩家，这个网站上大约有 18K 个玩家。因此，我们需要获得分页链接并抓取数据，直到没有下一页的那一页，也就是最后一页。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="fa46" class="ni kx iq ne b gy nj nk l nl nm">response.css('div.pagination&gt;a::attr(href)').get()</span></pre><p id="a289" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">上面一行打印了一个不完整的 url，它是一个相对 url，这在现代网站中很常见。为了完成它，scrapy 提供了一个方法<code class="fe no np nq ne b">urljoin</code>。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="0791" class="ni kx iq ne b gy nj nk l nl nm">response.urljoin(response.css('div.pagination&gt;a::attr(href)').get())</span></pre><p id="a590" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">这将打印完整的 url。现在点击下一页，尝试验证我们的 css 是否适用于所有页面。你会发现两个分页链接:上一页和下一页，我们使用的选择器将获得上一页链接。</p><p id="52b5" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated"><strong class="lq ir">解决方案:</strong>使用 Xpath。到目前为止，我们能够使用类来区分 web 元素，但情况并不总是如此。这两个分页链接之间的唯一区别是文本内容(上一页和下一页)。Xpath 允许我们根据文本内容精确地选择 web 元素。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="f9f9" class="ni kx iq ne b gy nj nk l nl nm">response.xpath('//span[@class="bp3-button-text" and text()="Next"]/parent::a/@href').get()</span></pre><p id="14da" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">让我们来分解一下:</p><ul class=""><li id="e53e" class="lo lp iq lq b lr mn lt mp lv nt lx nu lz nv mb mc md me mf bi translated"><code class="fe no np nq ne b">//span[@class="bp3-button-text" and text()="Next"]</code>将选择等级为<code class="fe no np nq ne b">bp3-button-text</code>的跨度，然后输入文本<strong class="lq ir">。</strong></li><li id="972c" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><code class="fe no np nq ne b">parent::a/@href</code>将遍历树并获取其父<code class="fe no np nq ne b">a</code>标签的 href 属性。</li></ul><p id="f1e1" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">同样，我们可以使用<code class="fe no np nq ne b">response.urljoin</code>来完成 url。所以我们的代码应该是这样的。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="6bb6" class="ni kx iq ne b gy nj nk l nl nm">next_page = response.xpath('//span[@class="bp3-button-text" and text()="Next"]/parent::a/@href').get()<br/>if next_page:<br/>    yield Request(response.urljoin(next_page), callback=self.parse)</span></pre><p id="c8b5" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">这是为了检查<code class="fe no np nq ne b">next_page</code>元素是否存在。然后，它通过加入 url 向该页面发送请求，并再次调用<code class="fe no np nq ne b">parse</code>方法。<code class="fe no np nq ne b">parse</code>是默认的回调函数，所以如果需要的话，您可以删除回调参数。现在，爬行器递归地爬行每个页面，同时产生数据。</p><p id="4c94" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">这是我们最终代码的样子。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oe of l"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">sofifa.py</figcaption></figure><h1 id="00a1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">执行蜘蛛</h1><p id="c1a0" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">Scrapy 有一个内置功能，可以将数据直接转储到 json 或 csv 文件中。记住我们的<code class="fe no np nq ne b">SofifaSpider</code>类中的<code class="fe no np nq ne b">name</code>变量，我们将使用它来调用蜘蛛。</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="adaf" class="ni kx iq ne b gy nj nk l nl nm">scrapy crawl sofifa - fifa20_data.csv</span></pre><p id="ed71" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">如果您想要 json 中的数据</p><pre class="kg kh ki kj gt nd ne nf ng aw nh bi"><span id="ba4c" class="ni kx iq ne b gy nj nk l nl nm">scrapy crawl sofifa - fifa20_data.json</span></pre><p id="4333" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">Scrapy 提供了许多现成的特性，使得编写 scrapers 和收集数据变得很容易。我们在这篇文章中看到了其中的一些。在 40 行代码下，我们设法创建了一个爬虫，它将在不到 30 分钟内抓取超过 18K 的数据。</p><p id="402f" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">您可以查看其他高级设置，如使用代理、添加延迟、更改并发请求等。</p><p id="faa5" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">你可以从这个<a class="ae kv" href="https://github.com/sagunsh/sofifa" rel="noopener ugc nofollow" target="_blank"> github repo </a>下载代码和数据。</p><h1 id="77f5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="83dc" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv na ms mt lx nb mv mw lz nc my mz mb ij bi translated">每个数据科学项目都需要数据，而 web 是一个巨大的数据源。数据收集是任何数据科学项目的第一步，您的分析结果取决于您所拥有的数据的质量，或者简单地说就是垃圾进垃圾出。因此，如果我们花一些时间在无聊的部分，如收集、清理和准备上，而不是仅仅关注使用哪种模型或制作哪种可视化效果，我们可以改善最终结果。</p><p id="30f3" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">接下来，我们将了解如何清理数据并为分析做好准备。</p><p id="de37" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">以下是一些你可能会觉得有用的学习材料:</p><ul class=""><li id="afdf" class="lo lp iq lq b lr mn lt mp lv nt lx nu lz nv mb mc md me mf bi translated">https://docs.scrapy.org/en/latest/intro/tutorial.html<a class="ae kv" href="https://docs.scrapy.org/en/latest/intro/tutorial.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="6576" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">【https://learn.scrapinghub.com/scrapy/ T4】</li><li id="fb0a" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="http://scrapingauthority.com/" rel="noopener ugc nofollow" target="_blank">http://scrapingauthority.com/</a></li><li id="7450" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="https://www.datacamp.com/community/tutorials/making-web-crawlers-scrapy-python" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/making-web-crawlers-scrapy-python</a></li><li id="500c" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="https://www.analyticsvidhya.com/blog/2017/07/web-scraping-in-python-using-scrapy/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/07/we B- scraping-in-python-using-scrapy/</a></li><li id="5d8b" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated"><a class="ae kv" href="https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3" rel="noopener ugc nofollow" target="_blank">https://www . digital ocean . com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3</a></li></ul><p id="8b36" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">如果您在代码中有任何困惑，或者在推导 xpath 和 css 选择器时需要帮助，请告诉我。另外，如果你对如何在接下来的步骤中分析这些数据有任何建议，请在评论中告诉我。你也可以在推特上找到我。</p></div></div>    
</body>
</html>