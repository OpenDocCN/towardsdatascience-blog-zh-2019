<html>
<head>
<title>Dimensionality Reduction toolbox in python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">python 中的降维工具箱</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-toolbox-in-python-9a18995927cd?source=collection_archive---------8-----------------------#2019-03-11">https://towardsdatascience.com/dimensionality-reduction-toolbox-in-python-9a18995927cd?source=collection_archive---------8-----------------------#2019-03-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/e9e909ba3669b16ce0cb885d0af25f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QJVtq9tlbvIG4mSrCIw40Q.jpeg"/></div></figure><p id="2440" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">这篇文章来源于我和我的朋友 Hervé Trinh 的工作。</p><p id="056c" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">近年来，数据量爆炸式增长了 80%以上。这导致了许多机器学习模型的出现，因为用一个重要的数据集来训练这些模型更容易。</p><p id="6c29" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">然而，数据的权重会对算法的执行时间产生显著影响，因为复杂度随着数据的大小而增加。同时，维度会使查看数据库中包含的信息变得复杂。</p><p id="0149" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">降维是使用特征选择或特征提取等策略减少特征集中特征总数的过程。<br/> <br/>例如，包含汽车特征的底座很难查看，因为它们数量众多。人们可以想象将里程和车龄结合起来形成特征磨损，只要它们是相关的。</p><p id="c60e" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">降维算法有很多种，主要有两类，线性方法和非线性方法。</p><p id="2fae" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">我将分享的技术是关于 python 的。确保您的机器上安装了 python。</p><p id="2997" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">首先，我们导入必要的库。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><p id="bceb" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">让我们导入 mnist 数据进行处理。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><p id="cec1" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">从高维到低维的线性数据转换的一种非常流行的技术是主成分分析，也称为 PCA。在接下来的几节中，让我们试着了解更多关于 PCA 的知识，以及如何使用它进行特征提取。</p><h2 id="710b" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">主成分分析</h2><p id="2939" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">主成分分析是一种统计方法，使用线性正交变换过程将可能相关的高维特征集转换为线性不相关的低维特征集。这些转换和新创建的特征也称为主要组件或 PCs。在任何 PCA 变换中，PC 的总数总是小于或等于特征的初始数量。第一主成分试图捕捉原始特征集的最大方差。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><p id="7746" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">前面的代码将减小 mnist 数据库的大小，并以其原始维度重新构建它。我们现在可以绘制小尺寸的数据。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/56338cef7b8c1e047a795fb140899658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B8O4RD6fkAdPAycOBAuWVg.png"/></div></div></figure><h2 id="a0be" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">增量 PCA</h2><p id="2b91" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">增量主成分分析是 ACP 的一种变体，它只保留最重要的奇异向量，以将数据投影到一个空间中以<br/>缩减大小。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/a4cdbcab26e60e2e75a683efe6db230d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s2lHnVrDija8fb00bnzEtQ.png"/></div></div></figure><h2 id="cd32" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">核主成分分析</h2><p id="6f05" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">KPCA 使执行复杂的非线性投影降维成为可能。</p><p id="6242" class="pw-post-body-paragraph ju jv iq jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ij bi translated">例如，下面的代码使用 Scikit-Learn 的 KernelPCA 类对一个 RBF 内核执行 kPCA。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mb"><img src="../Images/04d52616fcb3c74f83d0c18e567acaff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h1UHFuON1iyGvwbO3o87ng.png"/></div></div></figure><h2 id="830d" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">稀疏主成分分析</h2><p id="8452" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">稀疏 PCA 使用 ACP 和 SVD 之间的链接，通过求解低阶矩阵近似问题来提取主要分量。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mc"><img src="../Images/59e431ef2999a05102cc5c0356e36256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGB3PQQBT2ZqRp2ydc4GUg.png"/></div></div></figure><h2 id="6af2" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">奇异值分解</h2><p id="dca9" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">奇异值分解甚至可以应用于矩形矩阵；然而，特征值只为方阵定义。通过 SVD 方法获得的特征值的等价物被称为奇异值，而获得的与特征向量等价的向量被称为奇异向量。然而，由于它们在本质上是矩形的，我们需要为它们的维数分别有左奇异向量和右奇异向量。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/7ff6a9e861600a7aa7280ce8ffafdb4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-HiIftctphMfZrI2frnxA.png"/></div></div></figure><h2 id="1425" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">高斯随机投影</h2><p id="4151" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">在随机投影中，具有非常大的维度(d)的数据被投影<br/>到具有随机矩阵的二维空间(kd)中。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/02e02bae08763d669c56fd28a9a060fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4IP_-6IN1WICJaRtPddkvw.png"/></div></div></figure><h2 id="1589" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">稀疏随机投影</h2><p id="901c" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">分段随机矩阵是使用传统降维方法的密集随机投影矩阵的替代方案。它确保类似的嵌入质量，同时最大限度地提高存储效率，并允许更快地计算投影数据。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi lw"><img src="../Images/41f970b905b3436a175b38090c377359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K7Fewy6BLoy7Jokh_-w9kQ.png"/></div></div></figure><h2 id="0297" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">多维标度[MDS]</h2><p id="e6c9" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">MDS 是一种将样本之间的二相似性可视化的方法。MDS 返回一个最佳解决方案，在一个更小的维度空间中表示数据，如果维度的数量 k 是预先定义的。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/22230fa17764bb81922e4b6914e6441a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QqBwI-6nHBlMhfbK7IrkYw.png"/></div></div></figure><h2 id="d403" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">ISOMAP</h2><p id="4e54" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">它是一种基于谱理论的非线性降维方法，试图在低维中保持大地距离。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/2052a0ac4d7052335d49b6d5f3b76811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*euBKxsiEoZVi2BFDgewerQ.png"/></div></div></figure><h2 id="f2e1" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">迷你批处理词典学习</h2><p id="8169" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">基于字典的学习解决了矩阵分解的问题，这相当于找到一个字典，该字典可以在代码简约的条件下给出好的结果。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/d252ceaed7054fffb5633a151bbe189f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vm8aN7L3a1D-1PfJZRxN6Q.png"/></div></div></figure><h2 id="1b32" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">独立成分分析</h2><p id="360e" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">独立分量分析是一种主要用于信号处理以线性分离混合数据的方法。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi me"><img src="../Images/85935264035a419c771a4301a063fe1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qRB_mHf6kpbOT7u-sseWXw.png"/></div></div></figure><h2 id="5029" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">T-分布随机邻居嵌入[T-SNE]</h2><p id="3d5a" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">T-SNE 减少维数，同时试图保持相似的实例接近，不相似的实例分开。它主要用于可视化，特别是可视化高维空间中的实例集群。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi md"><img src="../Images/46177ef417b73cb6ae9abc6e716e01fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwUdsIcJ3CwjJk1geu_8XQ.png"/></div></div></figure><h2 id="2761" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">局部线性嵌入[LLE]</h2><p id="bb77" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">LLE 的工作方式是，首先测量每个训练实例如何与其最近的邻居(c.n .)线性相关，然后寻找训练集的低维表示，其中这些局部关系得到最佳保留(稍后将有更多详细信息)。这使得它特别擅长展开扭曲的流形，尤其是在没有太多噪声的时候。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi me"><img src="../Images/48307257d63ad5af1259ef3db27fe03f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5iF58biGsWRrBA_AgjeU5A.png"/></div></div></figure><h2 id="07f5" class="ky kz iq bd la lb lc dn ld le lf dp lg kf lh li lj kj lk ll lm kn ln lo lp lq bi translated">自动编码器</h2><p id="af0b" class="pw-post-body-paragraph ju jv iq jw b jx lr jz ka kb ls kd ke kf lt kh ki kj lu kl km kn lv kp kq kr ij bi translated">自动编码器是人工神经网络，能够学习输入数据的有效表示，称为编码，而无需任何监督(即，训练集是无标签的)。</p><figure class="ks kt ku kv gt jr"><div class="bz fp l di"><div class="kw kx l"/></div></figure><figure class="ks kt ku kv gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="lx ly di lz bf ma"><div class="gh gi mf"><img src="../Images/4fca9c2e861ca18608a20619b3388ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_m7lJbs036T-chIuBkXzzg.png"/></div></div></figure></div></div>    
</body>
</html>