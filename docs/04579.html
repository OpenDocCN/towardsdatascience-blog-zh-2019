<html>
<head>
<title>Spectral Clustering Algorithm Implemented From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现谱聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045?source=collection_archive---------1-----------------------#2019-07-14">https://towardsdatascience.com/unsupervised-machine-learning-spectral-clustering-algorithm-implemented-from-scratch-in-python-205c87271045?source=collection_archive---------1-----------------------#2019-07-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b57a2b0dac37d12beb675d0cad50fb94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jG2wj7v8VGuO95pX"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk"><a class="ae jg" href="https://www.pexels.com/photo/back-view-of-a-student-answering-a-problem-on-the-chalkboard-8197497/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/back-view-of-a-student-answering-a-problem-on-the-chalkboard-8197497/</a></figcaption></figure><div class=""/><p id="b540" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谱聚类是一种流行的无监督机器学习算法，其性能往往优于其他方法。此外，谱聚类实现起来非常简单，并且可以通过标准的线性代数方法有效地解决。在谱聚类中，确定哪些点属于哪个聚类的是相似性，而不是绝对位置(即 k 均值)。后者在处理数据形成复杂形状的问题时特别有用。</p><h1 id="1a5c" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">算法</h1><p id="8da2" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">该算法可以分为 4 个基本步骤。</p><ol class=""><li id="1bc2" class="mh mi jj ki b kj kk kn ko kr mj kv mk kz ml ld mm mn mo mp bi translated">构建相似度图</li><li id="2819" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">确定邻接矩阵 W、度矩阵 D 和拉普拉斯矩阵 L</li><li id="4fb4" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">计算矩阵 L 的特征向量</li><li id="76b0" class="mh mi jj ki b kj mq kn mr kr ms kv mt kz mu ld mm mn mo mp bi translated">使用第二小特征向量作为输入，训练 k-means 模型并使用它来对数据进行分类</li></ol><h1 id="b31b" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">密码</h1><p id="b615" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在下一节中，我们将从头开始实现谱聚类。我们将需要以下库。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="014e" class="ne lf jj na b gy nf ng l nh ni">import numpy as np<br/>float_formatter = lambda x: "%.3f" % x<br/>np.set_printoptions(formatter={'float_kind':float_formatter})<br/>from sklearn.datasets.samples_generator import make_circles<br/>from sklearn.cluster import SpectralClustering, KMeans<br/>from sklearn.metrics import pairwise_distances<br/>from matplotlib import pyplot as plt<br/>import networkx as nx<br/>import seaborn as sns<br/>sns.set()</span></pre><p id="3d3a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，数据集由样本(行)及其要素(列)组成。但是，谱聚类算法只能应用于连接节点的图。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/6cd9561b91856dc941e013f20a4e69ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*_jcClq4AWDivNFS-.gif"/></div></figure><p id="9e9d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，我们必须对数据进行转换，以便从由行和列组成的表格转换成图形。假设我们有以下数据集。我们可以清楚地看到，这些数据可以分成三组。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="d981" class="ne lf jj na b gy nf ng l nh ni">X = np.array([<br/>    [1, 3], [2, 1], [1, 1],<br/>    [3, 2], [7, 8], [9, 8],<br/>    [9, 9], [8, 7], [13, 14],<br/>    [14, 14], [15, 16], [14, 15]<br/>])</span><span id="bc31" class="ne lf jj na b gy nk ng l nh ni">plt.scatter(X[:,0], X[:,1], alpha=0.7, edgecolors='b')<br/>plt.xlabel('Weight')<br/>plt.ylabel('Height')</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3cc1b31902040dfb4b3dfdfbed00c15d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*rFfIUG75XCkNo02TYIBdAw.png"/></div></figure><p id="3f62" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们构造相似性矩阵，一个 NxN 矩阵，其中 N 是样本的数量。我们用每一对点之间的欧几里德距离填充单元格。</p><p id="6f8d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们通过复制相似性矩阵的内容来创建邻接矩阵，并且仅在这一次，我们设置阈值，使得如果距离大于预定义的限制，则我们将值设置为 0，否则设置为 1。</p><p id="18ea" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">邻接矩阵可以用来构建一个图。如果邻接矩阵的单元格中有 1，那么我们在列和行的节点之间画一条边。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="5cae" class="ne lf jj na b gy nf ng l nh ni">W = pairwise_distances(X, metric="euclidean")<br/>vectorizer = np.vectorize(lambda x: 1 if x &lt; 5 else 0)<br/>W = np.vectorize(vectorizer)(W)<br/>print(W)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c870261054d0acd13017d80ee8499190.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*iUKncNnmjfVTjmFOCfdEEg.png"/></div></figure><p id="aea9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程的剩余部分，我们将使用<code class="fe nn no np na b">networkx</code>库来可视化图形。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="d7a8" class="ne lf jj na b gy nf ng l nh ni">def draw_graph(G):<br/>    pos = nx.spring_layout(G)<br/>    nx.draw_networkx_nodes(G, pos)<br/>    nx.draw_networkx_labels(G, pos)<br/>    nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)</span></pre><p id="7c5f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，我们随机生成一个图并打印它的邻接矩阵。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="83a2" class="ne lf jj na b gy nf ng l nh ni">G = nx.random_graphs.erdos_renyi_graph(10, 0.5)</span><span id="ef9a" class="ne lf jj na b gy nk ng l nh ni">draw_graph(G)</span><span id="1183" class="ne lf jj na b gy nk ng l nh ni">W = nx.adjacency_matrix(G)<br/>print(W.todense())</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/1f3ff6d945afb696162f31a636c57a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*VT9uFoWk_bv6iJnzmLzZjQ.png"/></div></figure><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ca84565ccb7f5341816e13f0ff696777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*NoW0ZtV-i4Dy68uH4Qw8ag.png"/></div></figure><p id="993f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意这些节点是如何形成单个组件的(即，所有其他节点都可以从一个给定的节点到达)。</p><p id="964c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们建立了邻接矩阵，我们就建立了度矩阵。对于度矩阵的每一行，我们通过对邻接矩阵中相应行的所有元素求和来沿着对角线填充单元。</p><p id="b8de" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我们通过从度矩阵中减去邻接矩阵来计算拉普拉斯矩阵。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="d860" class="ne lf jj na b gy nf ng l nh ni"># degree matrix<br/>D = np.diag(np.sum(np.array(W.todense()), axis=1))<br/>print('degree matrix:')<br/>print(D)</span><span id="feb6" class="ne lf jj na b gy nk ng l nh ni"># laplacian matrix<br/>L = D - W<br/>print('laplacian matrix:')<br/>print(L)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c1765c52a5c0a44418ba8baac6cf5633.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*b8M6kh6Z18AzfFS6Ljy3pA.png"/></div></figure><p id="6919" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我们有了拉普拉斯矩阵，我们就可以利用它的一个特殊属性来分类我们的数据。</p><ul class=""><li id="212e" class="mh mi jj ki b kj kk kn ko kr mj kv mk kz ml ld nt mn mo mp bi translated"><strong class="ki jk"> <em class="nu">若图(W)有 K 个连通分量，则 L 有 K 个特征值为 0 的特征向量。</em> </strong></li></ul><p id="d4ca" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，由于在我们当前的例子中，我们只有一个组件，一个特征值将等于 0。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="e3fe" class="ne lf jj na b gy nf ng l nh ni">e, v = np.linalg.eig(L)</span><span id="85e7" class="ne lf jj na b gy nk ng l nh ni"># eigenvalues<br/>print('eigenvalues:')<br/>print(e)</span><span id="78f1" class="ne lf jj na b gy nk ng l nh ni"># eigenvectors<br/>print('eigenvectors:')<br/>print(v)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a712d66212e4371380f7a14d5a809587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*AirgWZGoNH_95p3_QaElqw.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="2fa9" class="ne lf jj na b gy nf ng l nh ni">fig = plt.figure()</span><span id="6c8d" class="ne lf jj na b gy nk ng l nh ni">ax1 = plt.subplot(121)<br/>plt.plot(e)<br/>ax1.title.set_text('eigenvalues')</span><span id="0007" class="ne lf jj na b gy nk ng l nh ni">i = np.where(e &lt; 10e-6)[0]<br/>ax2 = plt.subplot(122)<br/>plt.plot(v[:, i[0]])</span><span id="3b68" class="ne lf jj na b gy nk ng l nh ni">fig.tight_layout()<br/>plt.show()</span></pre><p id="3d4b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所见，在 10 个特征值中，有一个等于 0。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/e0e0cac6447c00c887ea1b9dbae94d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*2JBk78vlyPZjTdXkvywJKA.png"/></div></figure><p id="dffa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看另一个例子。上图由两部分组成。因此，2 个特征值等于 0。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="ab11" class="ne lf jj na b gy nf ng l nh ni">G = nx.Graph()<br/>G.add_edges_from([<br/>    [1, 2],<br/>    [1, 3],<br/>    [1, 4],<br/>    [2, 3],<br/>    [2, 7],<br/>    [3, 4],<br/>    [4, 7],<br/>    [1, 7],<br/>    [6, 5],<br/>    [5, 8],<br/>    [6, 8],<br/>    [9, 8],<br/>    [9, 6]<br/>])</span><span id="6b11" class="ne lf jj na b gy nk ng l nh ni">draw_graph(G)</span><span id="fbda" class="ne lf jj na b gy nk ng l nh ni">W = nx.adjacency_matrix(G)<br/>print(W.todense())</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/aa27aa59eed8057ff3456d1088a9322c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*zJES4qyJgAukBvDP8_cxig.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="3d6c" class="ne lf jj na b gy nf ng l nh ni"># degree matrix<br/>D = np.diag(np.sum(np.array(W.todense()), axis=1))<br/>print('degree matrix:')<br/>print(D)</span><span id="ff7c" class="ne lf jj na b gy nk ng l nh ni"># laplacian matrix<br/>L = D - W<br/>print('laplacian matrix:')<br/>print(L)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3ac221209bcb15a9e0ddec4fc2a881a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*o0J2i-4Vp_6hoO4JpfzIrA.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="269f" class="ne lf jj na b gy nf ng l nh ni">e, v = np.linalg.eig(L)</span><span id="0c0c" class="ne lf jj na b gy nk ng l nh ni"># eigenvalues<br/>print('eigenvalues:')<br/>print(e)</span><span id="ae82" class="ne lf jj na b gy nk ng l nh ni"># eigenvectors<br/>print('eigenvectors:')<br/>print(v)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/91213ca088e1b2625edb01886562decd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*Dp1cX8WGzcVGzvjqpX3kmg.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="cbdb" class="ne lf jj na b gy nf ng l nh ni">fig = plt.figure(figsize=[18, 6])</span><span id="ac29" class="ne lf jj na b gy nk ng l nh ni">ax1 = plt.subplot(131)<br/>plt.plot(e)<br/>ax1.title.set_text('eigenvalues')</span><span id="b7fa" class="ne lf jj na b gy nk ng l nh ni">i = np.where(e &lt; 10e-6)[0]<br/>ax2 = plt.subplot(132)<br/>plt.plot(v[:, i[0]])<br/>ax2.title.set_text('first eigenvector with eigenvalue of 0')</span><span id="f203" class="ne lf jj na b gy nk ng l nh ni">ax3 = plt.subplot(133)<br/>plt.plot(v[:, i[1]])<br/>ax3.title.set_text('second eigenvector with eigenvalue of 0')</span></pre><p id="a727" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们仔细看看每个特征向量的图，我们可以清楚地看到，前 5 个节点映射到相同的值，其他 5 个节点映射到另一个值。我们可以利用这一事实将节点分为两类。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/31afdedf78bc5a3e41e9a43df7f68af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*USQe4bQPT3l_u142M2Z-vQ.png"/></div></div></figure><p id="97d4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个稍微复杂一点的例子。前面的图是由一个单独的部分组成的。然而，看起来我们有两节课。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="4917" class="ne lf jj na b gy nf ng l nh ni">G = nx.Graph()<br/>G.add_edges_from([<br/>    [1, 2],<br/>    [1, 3],<br/>    [1, 4],<br/>    [2, 3],<br/>    [3, 4],<br/>    [4, 5],<br/>    [1, 5],<br/>    [6, 7],<br/>    [7, 8],<br/>    [6, 8],<br/>    [6, 9],<br/>    [9, 6],<br/>    [7, 10],<br/>    [7, 2]<br/>])</span><span id="84fc" class="ne lf jj na b gy nk ng l nh ni">draw_graph(G)</span><span id="02e9" class="ne lf jj na b gy nk ng l nh ni">W = nx.adjacency_matrix(G)<br/>print(W.todense())</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6ecaa0cc9c6dad3ceaff987c37243b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*GMzFpCuja-h-bYovqSBrdg.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="7509" class="ne lf jj na b gy nf ng l nh ni"># degree matrix<br/>D = np.diag(np.sum(np.array(W.todense()), axis=1))<br/>print('degree matrix:')<br/>print(D)</span><span id="a509" class="ne lf jj na b gy nk ng l nh ni"># laplacian matrix<br/>L = D - W<br/>print('laplacian matrix:')<br/>print(L)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ed922dff0985d2b021da41c0066d1aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*iG2h2bFxAmtYvk8SZmhG9w.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="15b8" class="ne lf jj na b gy nf ng l nh ni">e, v = np.linalg.eig(L)</span><span id="8288" class="ne lf jj na b gy nk ng l nh ni"># eigenvalues<br/>print('eigenvalues:')<br/>print(e)</span><span id="3059" class="ne lf jj na b gy nk ng l nh ni"># eigenvectors<br/>print('eigenvectors:')<br/>print(v)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2530f33b536e8da9757bf8df18b149f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*4yY9uup4NKZ26JA94_sScA.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="bd01" class="ne lf jj na b gy nf ng l nh ni">fig = plt.figure(figsize=[18, 6])</span><span id="465a" class="ne lf jj na b gy nk ng l nh ni">ax1 = plt.subplot(131)<br/>plt.plot(e)<br/>ax1.title.set_text('eigenvalues')</span><span id="a959" class="ne lf jj na b gy nk ng l nh ni">i = np.where(e &lt; 0.5)[0]<br/>ax2 = plt.subplot(132)<br/>plt.plot(v[:, i[0]])</span><span id="7d27" class="ne lf jj na b gy nk ng l nh ni">ax3 = plt.subplot(133)<br/>plt.plot(v[:, i[1]])<br/>ax3.title.set_text('second eigenvector with eigenvalue close to 0')</span></pre><p id="7cbc" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们只有一个分量，所以只有一个特征值等于 0。然而，如果我们看看第二小的特征值，我们仍然可以观察到两类之间的区别。如果我们画一条横线，我们就能正确地对节点进行分类。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/8be9a325f42b2d2bf8ebb9244b18de7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b1rZzMsdWwC4vNoWongJBQ.png"/></div></div></figure><p id="0c74" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看另一个例子。同样，图将由单个组件组成，但这一次，看起来节点应该放在三个容器中的一个。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="77b8" class="ne lf jj na b gy nf ng l nh ni">G = nx.Graph()<br/>G.add_edges_from([<br/>    [1, 2],<br/>    [1, 3],<br/>    [1, 4],<br/>    [2, 3],<br/>    [3, 4],<br/>    [4, 5],<br/>    [1, 5],<br/>    [6, 7],<br/>    [7, 8],<br/>    [6, 8],<br/>    [6, 9],<br/>    [9, 6],<br/>    [7, 10],<br/>    [7, 2],<br/>    [11, 12],<br/>    [12, 13],<br/>    [7, 12],<br/>    [11, 13]<br/>])</span><span id="a7dd" class="ne lf jj na b gy nk ng l nh ni">draw_graph(G)</span><span id="7c45" class="ne lf jj na b gy nk ng l nh ni">W = nx.adjacency_matrix(G)<br/>print(W.todense())</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/ca4d0039407589178ba26729536f3c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*gPm0n0ObKFrQb0P0rXfjWQ.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="297f" class="ne lf jj na b gy nf ng l nh ni"># degree matrix<br/>D = np.diag(np.sum(np.array(W.todense()), axis=1))<br/>print('degree matrix:')<br/>print(D)</span><span id="abc2" class="ne lf jj na b gy nk ng l nh ni"># laplacian matrix<br/>L = D - W<br/>print('laplacian matrix:')<br/>print(L)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4c1a9c9820363c123a9a39efdc634197.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*egr5sRH9VIG9GiXs1Gx5bg.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="5b90" class="ne lf jj na b gy nf ng l nh ni">e, v = np.linalg.eig(L)</span><span id="802c" class="ne lf jj na b gy nk ng l nh ni"># eigenvalues<br/>print('eigenvalues:')<br/>print(e)</span><span id="15db" class="ne lf jj na b gy nk ng l nh ni"># eigenvectors<br/>print('eigenvectors:')<br/>print(v)</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/315a82c0f1d613119c8fb887b249d0fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*AcfEHwDxmPR_ZU2EGkgOpQ.png"/></div></figure><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="a61c" class="ne lf jj na b gy nf ng l nh ni">fig = plt.figure(figsize=[18, 6])</span><span id="f2a5" class="ne lf jj na b gy nk ng l nh ni">ax1 = plt.subplot(221)<br/>plt.plot(e)<br/>ax1.title.set_text('eigenvalues')</span><span id="89b6" class="ne lf jj na b gy nk ng l nh ni">i = np.where(e &lt; 0.5)[0]<br/>ax2 = plt.subplot(222)<br/>plt.plot(v[:, i[0]])</span><span id="771e" class="ne lf jj na b gy nk ng l nh ni">ax3 = plt.subplot(223)<br/>plt.plot(v[:, i[1]])<br/>ax3.title.set_text('second eigenvector with eigenvalue close to 0')</span><span id="7d5b" class="ne lf jj na b gy nk ng l nh ni">ax4 = plt.subplot(224)<br/>plt.plot(v[:, i[2]])<br/>ax4.title.set_text('third eigenvector with eigenvalue close to 0')</span><span id="756b" class="ne lf jj na b gy nk ng l nh ni">fig.tight_layout()</span></pre><p id="ebc6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们只有 1 个分量，所以 1 个特征值等于 0。但是，我们可以再次使用第二小的特征值来计算出哪个节点应该放在哪个类别中。</p><figure class="mv mw mx my gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/1a3041e00e7179d9a1b2db43b12b3bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2krr4xZ5uZIZN-WbLpNkIA.png"/></div></div></figure><p id="2593" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实践中，我们使用 k-means 根据节点在特征向量中的对应值对节点进行分类。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="5fae" class="ne lf jj na b gy nf ng l nh ni">U = np.array(v[:, i[1]])</span><span id="262c" class="ne lf jj na b gy nk ng l nh ni">km = KMeans(init='k-means++', n_clusters=3)</span><span id="5f79" class="ne lf jj na b gy nk ng l nh ni">km.fit(U)</span><span id="aebf" class="ne lf jj na b gy nk ng l nh ni">km.labels_</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3d7f2c28519546addd5344744278af2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*rEGBuZS24N15lwricQbbTw.png"/></div></figure><p id="5201" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们使用 scitkit-learn 的实现来比较 k-means 和谱聚类。假设我们的数据在绘制时采用了以下形状。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="4b61" class="ne lf jj na b gy nf ng l nh ni">X, clusters = make_circles(n_samples=1000, noise=.05, factor=.5, random_state=0)<br/>plt.scatter(X[:,0], X[:,1])</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a797a5f40463fa16df20859e9ab331c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*ifj2n_l1pfe8m1YiQ7JCuw.png"/></div></figure><p id="27b6" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用 k-means 时，我们得到以下结果。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="47ce" class="ne lf jj na b gy nf ng l nh ni">km = KMeans(init='k-means++', n_clusters=2)</span><span id="63d7" class="ne lf jj na b gy nk ng l nh ni">km_clustering = km.fit(X)</span><span id="e572" class="ne lf jj na b gy nk ng l nh ni">plt.scatter(X[:,0], X[:,1], c=km_clustering.labels_, cmap='rainbow', alpha=0.7, edgecolors='b')</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/9932c3082526e8ae7540b693640b0d30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*AJ-6nBPq1DRFoVxAO4Byfw.png"/></div></figure><p id="65ca" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相反，当使用谱聚类时，我们将每个圆放在自己的聚类中。</p><pre class="mv mw mx my gt mz na nb nc aw nd bi"><span id="11c0" class="ne lf jj na b gy nf ng l nh ni">sc = SpectralClustering(n_clusters=2, affinity='nearest_neighbors', random_state=0)</span><span id="9f4d" class="ne lf jj na b gy nk ng l nh ni">sc_clustering = sc.fit(X)</span><span id="dceb" class="ne lf jj na b gy nk ng l nh ni">plt.scatter(X[:,0], X[:,1], c=sc_clustering.labels_, cmap='rainbow', alpha=0.7, edgecolors='b')</span></pre><figure class="mv mw mx my gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f3f9a0d1268228149bf411833860361e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Ti4ELOOjg1Xyc7iutS5pVw.png"/></div></figure><h1 id="994c" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">最后的想法</h1><p id="3470" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">与 k-means 相反，谱聚类考虑了数据点的相对位置。</p></div></div>    
</body>
</html>