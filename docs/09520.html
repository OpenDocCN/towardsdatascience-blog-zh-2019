<html>
<head>
<title>Examining The Weight And Bias of LSTM in Tensorflow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Tensorflow 2 中检查 LSTM 的权重和偏差</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/examining-the-weight-and-bias-of-lstm-in-tensorflow-2-5576049a91fa?source=collection_archive---------8-----------------------#2019-12-15">https://towardsdatascience.com/examining-the-weight-and-bias-of-lstm-in-tensorflow-2-5576049a91fa?source=collection_archive---------8-----------------------#2019-12-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9bb3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">张量流 2 中 LSTM 权重和偏差混淆结构的解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fffd9a357609fef6e11bb5f117843a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4l4afidsqWyPWIq9p4VrzA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">source: <a class="ae ky" href="https://dnacentre.co.uk/wp-content/uploads/2018/04/blog-uk-genetic-engineering.png" rel="noopener ugc nofollow" target="_blank">https://dnacentre.co.uk/wp-content/uploads/2018/04/blog-uk-genetic-engineering.png</a></figcaption></figure><p id="a114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好，首先，什么是 LSTM？LSTM 是长短期记忆的缩写。<strong class="lb iu">它是递归神经网络(RNN)中应用最广泛的一种</strong>。基本上，<strong class="lb iu"> RNN 将时间序列数据作为输入</strong>。时间序列中的每一个数据从最早的时间步长开始依次成为 RNN 的输入。<strong class="lb iu">某个时步输入的 RNN 输出将与下一个时步</strong>的数据一起作为 RNN 的输入。当 RNN 处理完数据的最后一个时间步长时，将输出实际输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/bed7ac92520c6083b771f7a539e36da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F8WphMopgERgNflz8Eu6Uw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">How the RNN process the input</figcaption></figure><p id="5a5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LSTM 的神经元结构是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lw"><img src="../Images/cace770db820b756504598221c2c97d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png"/></div></div></figure><p id="88ba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在时间步长的每个过程中，LSTM 有 4 层神经元。<strong class="lb iu">这 4 层一起形成一个称为门的处理</strong>称为遗忘门- &gt;输入门- &gt;输出门(- &gt;表示序列处理在 LSTM 中发生的顺序)。那就是 LSTM，我不会涉及 LSTM 的细节，因为那会是一篇很长的文章，而且这不是我这次的重点。</p><p id="4543" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">长话短说，为了我最近的实验，我需要取回我的 LSTM 的重量和偏差。我用 TensorFlow 2 建立了一个 LSTM。<strong class="lb iu">在 TensorFlow 2 中，我们可以使用下面的代码</strong>访问 LSTM 权重和偏差的结构。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="31c5" class="mc md it ly b gy me mf l mg mh">import tensorflow as tf</span><span id="dba7" class="mc md it ly b gy mi mf l mg mh">simple_lstm_model = tf.keras.models.Sequential()<br/>simple_lstm_model.add(tf.keras.Input((18,7)))<br/>simple_lstm_model.add(tf.keras.layers.LSTM(2))</span><span id="c8d1" class="mc md it ly b gy mi mf l mg mh">print(simple_lstm_model.layers[0].trainable_weights)</span></pre><p id="9cbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我构建了一个接受 18 x 7 形状输入的 LSTM。18 是数据的总时间步长，7 是参数的总数。<strong class="lb iu">对于每个时间步长，LSTM 将采用 7 个参数</strong>。我声明<strong class="lb iu">这个 LSTM 有两个隐藏状态</strong>。<strong class="lb iu">隐藏状态就像每个时间步长</strong>的 LSTM 输出。这意味着<strong class="lb iu">我们的 LSTM 最终将输出 2 个实数</strong>。这也意味着，<strong class="lb iu">LSTM</strong>中每层神经元的数量是 2。当您运行这个脚本时，您将得到如下输出。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="63d2" class="mc md it ly b gy me mf l mg mh">[&lt;tf.Variable 'lstm/kernel:0' shape=(7, 8) dtype=float32&gt;, &lt;tf.Variable 'lstm/recurrent_kernel:0' shape=(2, 8) dtype=float32&gt;, &lt;tf.Variable 'lstm/bias:0' shape=(8,) dtype=float32&gt;]</span></pre><p id="b157" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，这里我们有 3 组参数，它由 lstm/kernel(形状为 7x8)、lstm/recurrent_kernel(形状为 2x8)和 lstm/bias(形状为 8)组成。最后一部分(lstm/bias)很明显，就是偏见。为什么我们有 8 个？记住，在 LSTM 中我们有 4 层神经元，我声明这个 LSTM 有 2 个隐藏状态，或者说每层有 2 个神经元。每个神经元有 1 个偏差，4 层中的每层有 2 个神经元，所以总数是 8。</p><p id="97df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">容易混淆的部分是 lstm/kernel 和 lstm/recurrent_kernel。内核在这里的意思是重量。lstm/kernel 表示我们的输入相对于 lstm 每个神经元的权重。我声明输入有 18 个时间步长和 7 个参数，所以<strong class="lb iu">每个参数对每个神经元</strong>有 1 个权重，这就是 lstm/kernel 的形状为 7x8 的原因。最后，<strong class="lb iu"> lstm/recurrent_kernel 表示我们的隐藏状态的权重，也称为 lstm 在前一时间步(t-1)相对于 LSTM </strong>中每个神经元的输出。<strong class="lb iu">我声明隐藏状态是 2，LSTM 的总神经元是 8 </strong>，那么隐藏状态对 LSTM 神经元层的权重形状一定是 2×8。</p><p id="5484" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是关于 LSTMin TensorFlow 2 的权重和偏差的解释。您还可以使用 summary()函数看到整个神经网络的结构，如下所示。</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="4ca2" class="mc md it ly b gy me mf l mg mh">import tensorflow as tf</span><span id="93ae" class="mc md it ly b gy mi mf l mg mh">simple_lstm_model = tf.keras.models.Sequential()<br/>simple_lstm_model.add(tf.keras.Input((18,7)))<br/>simple_lstm_model.add(tf.keras.layers.LSTM(2))<br/>simple_lstm_model.add(tf.keras.layers.Dense(5))</span><span id="19cd" class="mc md it ly b gy mi mf l mg mh">simple_lstm_model.summary()</span></pre><p id="88b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出是</p><pre class="kj kk kl km gt lx ly lz ma aw mb bi"><span id="1ab4" class="mc md it ly b gy me mf l mg mh">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>lstm (LSTM)                  (None, 2)                 80        <br/>_________________________________________________________________<br/>dense (Dense)                (None, 5)                 15        <br/>=================================================================<br/>Total params: 95<br/>Trainable params: 95<br/>Non-trainable params: 0</span></pre><p id="967c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">这里我将之前的 LSTM 和一个普通的神经网络层</strong>叠加在一起。这个网络的最终输出有 5 个元素。这个堆叠式神经网络的总权重和偏差为 95。</p><p id="2414" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这些了，下期帖子见。</p><p id="309e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参考:</p><p id="526f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://medium.com/ai-journal/lstm-gru-recurrent-neural-networks-81fe2bcdf1f9" rel="noopener">https://medium . com/ai-journal/lstm-gru-recurrent-neural-networks-81 Fe 2 BCD f1 f 9</a>，2019 年 12 月 15 日获取</p><p id="3935" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>，2019 年 12 月 15 日进入</p></div></div>    
</body>
</html>