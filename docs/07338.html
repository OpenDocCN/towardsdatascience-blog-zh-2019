<html>
<head>
<title>Visualisation of embedding relations (word2vec, BERT)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">嵌入关系的可视化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36?source=collection_archive---------17-----------------------#2019-10-15">https://towardsdatascience.com/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36?source=collection_archive---------17-----------------------#2019-10-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c3b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这个故事中，我们将可视化单词嵌入向量，以理解由嵌入描述的单词之间的关系。这个故事重点讲 word2vec [1]和 BERT [2]。为了理解嵌入，我建议阅读不同的介绍(比如<a class="ae kl" href="http://jalammar.github.io/illustrated-bert/" rel="noopener ugc nofollow" target="_blank">这个</a>)，因为这个故事并不打算描述它们。</p><p id="54f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个故事是我使用 BERT 情境化嵌入向量开发神经机器翻译(NMT)之旅的一部分。欢迎建议！</p><h1 id="0e66" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">单词嵌入</h1><p id="04b7" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">单词嵌入是为单词生成计算机友好的数字向量表示的模型。Word2vec 为每个单词产生 300 个介于 0 和 1 之间的浮点数。对计算机来说可能更容易理解，但对人来说则相反。</p><p id="0cef" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了 5x60 矩阵中 300 数值的灰度像素。看着图像，我们能观察到的东西并不多。</p><p id="0fdc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以认为，第 42 列的较轻值在所有与人类相关的单词中是相似的，而在单词<em class="lp"> apple </em>中是完全不同的。</p><p id="624c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另外，<code class="fe lq lr ls lt b">queen-woman+man = king</code>是计算机可以理解的东西。字典中与<code class="fe lq lr ls lt b">queen-woman+man</code>最接近的单词是<code class="fe lq lr ls lt b">king</code>，这被认为是单词嵌入的重要特征之一。但是<em class="lp">很难看到</em>。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/e5d3666d264288e45db957ad2e641bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AkferepBml9_hx9FeZEu3Q.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Word2vec representation of certain words</figcaption></figure><h1 id="833a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">嵌入投影</h1><p id="8b1d" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">嵌入投影背后的基本思想是将表示向量的维度减少到 2D 或 3D，因此它可以被可视化。这背后的数学称为线性投影:如果我们有一个 n 维向量<strong class="jp ir"> x </strong>，我们想要一个 m 维投影(m &lt; n) <strong class="jp ir"> y </strong>，我们需要找到一个 m*n 矩阵<strong class="jp ir"> A </strong>，秩为 m，并计算<strong class="jp ir"> y </strong> =A <strong class="jp ir"> x </strong>。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mk"><img src="../Images/12573a520a298f03fda6dafea0d71a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tp8hqj1gaYwfJmNz32gCYg.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk"><a class="ae kl" href="https://de.wikipedia.org/wiki/Projektion_(Lineare_Algebra)#/media/Datei:Orthogonal_Decomposition_qtl1.svg" rel="noopener ugc nofollow" target="_blank">Illustration of projection in Wikipedia (cc)</a></figcaption></figure><p id="14fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们关注 2D！如果我们想要一个投影，其中某个单词到(1，0)点，而另一个单词到(0，1)点，我们想要解 I=AX 线性方程，其中<em class="lp"> I </em>是 2 大小相同的矩阵，<em class="lp"> A </em>是投影的矩阵，<em class="lp"> X </em>矩阵包含我们想要的作为列的基础中的向量。</p><p id="1e2b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果 A 矩阵是一个方阵，这就是 X 的逆矩阵，这个问题可以用高斯消去法很容易地解决。这里，我们希望将更大的向量投影到 2D 子空间中，因此，它不是一个正方形矩阵(对于 word2vec 为 2x300，使用 BERT base 为 2x768)。这里我们要的叫做<em class="lp">左逆矩阵</em>。然而，我们将使用伪逆或<a class="ae kl" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" rel="noopener ugc nofollow" target="_blank"> <em class="lp"> Moore-Penrose 逆</em> </a> <em class="lp"> </em>，因为它在 Numpy 中有一个实现，并且生成一个接近于单位(<code class="fe lq lr ls lt b"><a class="ae kl" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.pinv.html" rel="noopener ugc nofollow" target="_blank">numpy.linalg.pinv</a></code>)的矩阵。</p><p id="5e28" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好吧。我们来编码吧！我们有一个单词列表:<code class="fe lq lr ls lt b">[‘man’, ‘woman’, ‘rich’, ‘poor’, ‘queen’, ‘king’, ‘fisherman’, ‘teacher’, ‘actress’, ‘actor’]</code>我们想要一个 2D 子空间，其中男女线是一个轴，贫富线是另一个轴。为此，我们将把<code class="fe lq lr ls lt b">man-woman</code>投影到(1，0)并将<code class="fe lq lr ls lt b">rich-poor</code>投影到(0，1)，在投影之后，我们将移动这些点以适合轴。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ml mm l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Code to project words for 2D visualisation</figcaption></figure><p id="12f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下图显示了投影的结果。观察这张图片<em class="lp">我们可以看到</em>一个<em class="lp">渔夫</em>，一个<em class="lp">国王</em>和一个<em class="lp">演员</em>更像一个<em class="lp">男人</em>，而不是一个<em class="lp">女王</em>或<em class="lp">女演员</em>。同样，我们可以说一个<em class="lp">国王</em>和一个<em class="lp">王后</em>接近<em class="lp">富</em>而一个<em class="lp">老师</em>接近<em class="lp">穷</em>。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi gj"><img src="../Images/8aaa7ad691452bceceb39af87129d998.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qILLqkYzzIo1pCuJ-z-06g.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Projection of the words in a subspace with poor-rich and man-woman axes</figcaption></figure><p id="bee2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">要看更高级的 word 投影，我可以推荐 TensorFlow <a class="ae kl" href="http://projector.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">嵌入式投影仪</a>。</p><h1 id="a19d" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">句子投射</h1><p id="cdb1" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">BERT 提供了依赖于上下文的嵌入。在这里，我们想要研究，我们能从中观想什么。我之前没有看到类似的东西，所以可能是第一次有人这么做。否则，我很高兴在评论中看到其他解决方案！</p><p id="9112" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种可视化背后的思想是，BERT 中的每个句子都以一个<code class="fe lq lr ls lt b">[CLS]</code>标记开始，以一个<code class="fe lq lr ls lt b">[SEP]</code>标记结束，因此，将句子投影为(1，0)是<code class="fe lq lr ls lt b">[CLS]</code>标记，而(0，1)是<code class="fe lq lr ls lt b">[SEP]</code>标记，我们可以在 2D 子空间中说明句子，并且嵌入以所有句子都从(1，0)开始并以(0，1)结束的方式进行归一化。</p><p id="73eb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe lq lr ls lt b">[CLS] This is a sentence example. [SEP]</code></p><p id="a1c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这背后的数学原理和以前一样。我们使用预先训练好的 BERT base <code class="fe lq lr ls lt b">bert_12_768_12_book_corpus_wiki_en_uncased</code>。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ml mm l"/></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Code for the sentence-dependent embedding projection</figcaption></figure><p id="9cc8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了演示令牌的顺序，让我们用箭头将点连接起来！我们可以观察到第一个标记'<em class="lp">，</em>'更靠近起始标记和最后一个标记'<em class="lp">。</em>’离安德令牌更近，但其他的都在相对较小的区域。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mn"><img src="../Images/19d23ae71d943d187cc15e38119abed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDJa0o6mr73-t3zVHDcRlA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Sentence visualisation using contextualised embeddings</figcaption></figure><p id="49b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们把句子中的单词'<em class="lp"> sky </em>改成'<em class="lp"> sea </em>'！点天空和海洋被标记。我想指出的是，主要区域中的点的顺序被改变了(尝试在故事的结尾链接的代码以获得更好的分辨率)。虽然海句子没有十字箭头，但天空句子的顺序看起来完全是随机的。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mo"><img src="../Images/517f9d6cfde704d854921a2d040eaa46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPpft94JRUlRZTfl4GFFRA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Difference between the sky and the sea in the sentence “The … is blue today.”</figcaption></figure><p id="592e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们来调查时态的变化！从这张图片(与其他句子相比)来看，我认为这个模型在句子的时态方面没有太大的差异。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mp"><img src="../Images/22bc49173f1fb5883ee81487cb2be5ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6qzX5W0lK81YEwuRaNCyKw.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Difference between tenses. is / was / will be</figcaption></figure><p id="772d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与时态相比，句末的标点符号对嵌入的影响更大。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/eef38f11f5a1ed55e4092cbe107d704c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-avk1egT5b19SnYliJ-NnQ.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Difference between punctuation characters.</figcaption></figure><p id="68af" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们之前提到的，BERT 嵌入是依赖于上下文的。为了说明这一点，下面的例子展示了包含单词<em class="lp"> Mexico </em>的复杂句子。墨西哥点被标记。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mq"><img src="../Images/fe3581a4bde643f38d2a59cf92ece0d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GTTjgZD28vR9pzqgOAkU1Q.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Sentences with Mexico in it</figcaption></figure><p id="1585" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在前面的例子中，我们可以看到句子“<em class="lp">那也在墨西哥</em>”与我们到目前为止看到的其他句子有些不同。它主要在 2D 空间的负域区域上。换句话说，离<code class="fe lq lr ls lt b">[CLS]</code>令牌还远着呢。</p><p id="d985" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了猜测为什么会发生这种情况，我们必须研究一下 BERT 的训练数据。使用两个无监督的任务来训练 BERT，其中一个是掩蔽语言建模(Masked LM)，另一个是下一句预测(NSP)。在第二个任务中，句子对用一个分隔符训练，像这样:<br/> <code class="fe lq lr ls lt b">[CLS] This is the first sentence. [SEP] This is the second one. [SEP]</code></p><p id="6281" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如我们在这里看到的，第二个句子以一个<code class="fe lq lr ls lt b">[SEP]</code>标记开始，而不是以<code class="fe lq lr ls lt b">[CLS]</code>标记开始。我认为这是观察到的不规则现象背后的原因。换句话说，句子，即对话中的反应，可能离<code class="fe lq lr ls lt b">[CLS]</code>标记更远。让我们看看能否用更多的例子来支持这个论点！</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mr"><img src="../Images/0f1643da54c2ed0a4da2f07c54ded237.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3Ofdocx8T6iMvjsAkb7RA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Possible reaction sentences to see x&lt;0 points</figcaption></figure><p id="09a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们也可以在 y&lt;0 区域捕捉到一些点，但是不太明显。我认为对这些特殊句子的观察显示了进一步工作的潜力。</p><p id="ad09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，让我们检查一些不正确的句子！我们可以看到，这些非句子也试图适应点的规则区域。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi ms"><img src="../Images/afa21f378505b8db5ade216e41e5fcba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWXZE_KVg1Gbb_4jSCsuCA.png"/></div></div><figcaption class="mg mh gj gh gi mi mj bd b be z dk">Incorrect sentences compared to two correct ones</figcaption></figure><h1 id="7f49" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">摘要</h1><p id="6b3b" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">在这个故事中，我们讨论了一种可视化单词嵌入的方法，并以一种新颖的方式讨论了将其用于句子可视化的想法。</p><p id="aecf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有代码都可以在 Google Colab 上找到。</p><h1 id="96bd" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">参考</h1><p id="f91c" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">[1] Mikolov，t .，Chen，k .，Corrado，g .，&amp; Dean，J. (2013 年)。<a class="ae kl" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计。</a> <em class="lp"> arXiv 预印本 arXiv:1301.3781 </em>。</p><p id="762e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2] Devlin，j .，Chang，M. W .，Lee，k .，&amp; Toutanova，K. (2018 年)。<a class="ae kl" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> Bert:用于语言理解的深度双向转换器的预训练。</a> <em class="lp"> arXiv 预印本 arXiv:1810.04805 </em>。</p><h1 id="aa2d" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">用伯特的故事学习 NMT</h1><ol class=""><li id="fa35" class="mt mu iq jp b jq lk ju ll jy mv kc mw kg mx kk my mz na nb bi translated"><a class="ae kl" href="https://medium.com/@neged.ng/bleu-bert-y-comparing-sentence-scores-307e0975994d" rel="noopener"> BLEU-BERT-y:比较句子得分</a></li><li id="faa5" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated"><a class="ae kl" href="https://medium.com/@neged.ng/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36" rel="noopener">嵌入关系的可视化(word2vec，BERT) </a></li><li id="0086" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/machine-translation-a-short-overview-91343ff39c9f">机器翻译:简要概述</a></li><li id="74eb" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/identifying-the-right-meaning-of-the-words-using-bert-817eef2ac1f0">使用 BERT 识别单词的正确含义</a></li><li id="aa3c" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/machine-translation-compare-to-sota-6f71cb2cd784">机器翻译:对比 SOTA </a></li><li id="2d78" class="mt mu iq jp b jq nc ju nd jy ne kc nf kg ng kk my mz na nb bi translated"><a class="ae kl" rel="noopener" target="_blank" href="/simple-bert-using-tensorflow-2-0-132cb19e9b22">使用 TensorFlow 2.0 的简单 BERT</a></li></ol></div></div>    
</body>
</html>