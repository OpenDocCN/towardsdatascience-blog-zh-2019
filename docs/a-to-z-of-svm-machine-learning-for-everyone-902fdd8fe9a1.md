# SVM 的从头到尾——每个人的机器学习

> 原文：<https://towardsdatascience.com/a-to-z-of-svm-machine-learning-for-everyone-902fdd8fe9a1?source=collection_archive---------21----------------------->

## 我们的目标是用简单的理论、简单的数学和简单的代码提供对机器学习算法最简单有效的理解

![](img/8d83e85b82f0ced81233142fd75a66c2.png)

Source: [https://wallpapercave.com/w/wp3205387](https://wallpapercave.com/w/wp3205387)

## 我们将讨论以下主题:

1-概述

2-简介

3-SVM 是如何工作的

4-支持向量和余量

五线性和非线性 SVM

6-SVM 的硬保证金和软保证金

SVM 的 7- I/O

SVM 的 8-超参数

9-SVM 的优势

10-SVM 的挑战

11-SVM 的劣势

12-结论

# 概观

如果你不知道 SVM 是什么，它是用来做什么的，或者它是如何工作的，并且想知道所有这些问题的答案，那么你来对地方了。在这篇文章中，我们将从一个日常生活的例子开始探索 SVM，然后试图以简单的方式理解 SVM，保持我们的讨论简短而准确。那么，让我们开始吧:

SVM 代表 **S** 支持 **V** 向量 **M** 机器。它主要是一种分类算法。在我们继续之前，让我们讨论一下什么是分类

## 分类

如果我们抛硬币，我们要么得到一条尾巴，要么得到一个头，这意味着只有两种可能的结果，或者我们可以说硬币的某一面属于两个类别。分类是将任何事件的结果(如抛硬币是具有两个类别/结果的事件)分类到预定义类别(如抛硬币事件中的头尾)的过程。

# 简介:

SVM 用于将输入(稍后将详细介绍输入)分类为一个预定义的类别(如是/否或头/尾)。

如果 SVM 用于分类两个类，如头/尾，那么这样的分类器是**二元分类器**，否则它变成**多类分类器**

> SVM 既是二元分类器，也是多类分类器

让我们用一个真实的例子来继续这个算法，这样我们可以更好地理解。

假设在一所大学里，学生注册了两门课——统计学和数学。根据学生在这些课程中的表现，教师必须决定某个特定的学生是否可以参加机器学习课程。换句话说，它是否会是“是”或“否”。现在，教师拥有过去学生的数据，并且教师希望根据学生的表现自动授予他们机器学习的权限。

显然，这是一个分类问题，我们需要一个能够解决分类问题的机器学习算法。

我们知道，SVM 是最好的分类算法之一。所以，让我们深入 SVM

# SVM 是如何运作的

我们想对数据做的第一件事是通过绘图来查看它，这样我们就可以对我们可用的数据有一个大致的概念。

![](img/6da146d905b4ff4641172adba4bcb2eb.png)

Source: Kdnuggets

从上图中可以看出，红色圆圈表示课程分数相对较低的学生，而蓝色圆圈表示两个课程都很好的学生。这意味着蓝色圆圈是我们的*是的*类，因为这些学生将有机会进入机器学习类，红色是那些不会进入机器学习类的学生。

我们 SVM 的任务是在这些阶级之间划一条线(是/否)，这样这些阶级就不会相互混淆。这两个圆池(或这两个类)之间可以有无穷多条线。下面显示了两条可能的线:

![](img/c512e7ef27fb612a7161c5e48065ab52.png)

Source: Kdnuggets

我们可以看到，两条线都做了将这些类彼此分离的工作，并且线两边的类没有混合。

SVM 背后的整个理念就是划清界限(将各个阶级区分开来)。

SVM 画的分班线叫做**超平面**。在上图中我们有两个超平面。那么，选哪个呢？

> 正是这个问题的答案将 SVM 与其他分类算法区分开来。SVM 选择类别之间分离最大的线(或超平面)。

或者我们可以说，SVM 选择了与两边的圆(或点)有最大距离的超平面。但是在图 so 中有很多点，计算距离时要考虑哪些点。SVM 认为最接近超平面的点如下图所示:

![](img/62f0a2cf73b64db32b96a803e3ef9647.png)

Hyperplane, Margin & Support Vectors (Source: Kdnuggets)

## SVM 的支持向量和利润

可以看出，这个超平面与两侧最近点的距离最大。这些点有助于确定最佳超平面，因为只有这些点是最近的点。因此，这些最近的点(上图阴影区域中的两个点)是支持最优超平面的点，由于这个原因，这些点被称为**支持向量**。支持向量之间的距离称为**余量**。

> 我们现在可以说SVM 试图最大化边际以便在阶级之间有一个很好的分离。

除了超平面(上图中的黑线)，我们还有另外两条支持向量所在的线。我们的主超平面(黑线)被称为**决策边界**，以区别于其左侧和右侧的其他两个超平面。

# 线性和非线性 SVM

## 线性 SVM

我们上面的数据在两个类之间有一条直线。这意味着数据是线性可分的或由一条直线。在类之间画直线超平面的 SVM 叫做 **LSVM 线性 SVM。**

## 非线性 SVM

但是在现实生活中，数据并不总是线性可分的。在这种情况下，SVM 不会在两个等级之间画一条直线，而是画一条曲线，如下图所示。(注意，我们现在讨论的是非线性 SVM)

![](img/41820b635c9129690b1666d089b8b01c.png)

Non Linear Data (Source: Kdnuggets)

在这种情况下，我们不能完全分开这两个类。有两种可能性:

**→** 要么我们画一条曲线来完全区分阶级

→或者我们考虑到一些错误(错误分类),并将其线性分离

在前一种情况下，我们谈论的是非线性 SVM。*内核技巧*将帮助我们实现那条曲线。因此，核技巧只不过是一种改变超平面形状以避免错误分类的方法。默认内核是*线性*。

# SVM 的硬利润和软利润

如果数据是线性可分的，那么 SVM 可能返回最大的准确性，或者我们可以说 SVM 做了完美的分类。这样的余量被称为**硬余量** *。但有时，我们没有线性可分性，为了在如此混乱的数据中绘制线性，我们必须放宽我们的余量，以允许错误分类。现在这个(放松的)余量被称为**软余量***

# SVM 的输入/输出:

SVM 总是期望数字输入并返回数字输出。因为它是一个监督算法，所以除了输入，它还需要标签。输入(X1，X2)和标签(Y)的示例如下所示

![](img/99ca5376dd2e56bb3ab7fed92508186d.png)

Source: Jason Brownlee Book on ML

# SVM 的超参数

超参数用于调整算法以最大化其准确性。每个机器算法都有特定的超参数。

SVM 也有一些超参数。让我们讨论其中的一些。

## 超参数 C(软边界常数)

在后一种允许一些误差(或误分类)的情况下，我们通过称为“C”的参数来控制误差量。它可以是任何值，如 0.01，甚至 100 或更大。这取决于问题的类型和我们拥有的数据。

c 直接影响超平面。c 与边距的宽度成反比。所以 C 越大，边距越小，反之亦然。C 对利润的影响如下所示:

![](img/3f10ad2cfd98662fc0e1c0d479d10411.png)

C = 0.1 (Large Width) Source: yunhaocsblog

现在让我们将 C 的值增加到 10，看看它对利润的影响。

![](img/467f7a16e5b357eaf19e64c583895dd3.png)

C = 10 (Small Margin)

所以选择 c 并没有严格的规则，它完全取决于问题和手头的数据。

## 超参数γ

我们之前已经讨论过，超平面仅基于支持向量来决定。支持向量之外的点不被赋予任何权重。但是*伽马超参数用于给支持向量之外的点赋予*权重*。*

*gamma 的小值表示支持向量之外的点将在计算余量(或决定超平面)时被赋予权重，反之亦然。*

但是我们为什么要这样做呢？嗯，改变*γ*的值会改变超平面的形状，从而改变新的超平面→新的精度。因此，归根结底，准确性才是最重要的。不管我们如何实现它。

## SVM 的内核技巧

如前所述，内核用于将不可分离的数据转换为可分离的数据。这是通过改变 SVM 模型中内核的名称来实现的。

一些内核有***、*** **多项式、径向** 等。

# SVM 的优势

→能够有效处理少量数据

→当特征数量大于样本数量时可以工作

→在高维空间有效(核技巧)

→内存高效—使用训练数据的子集(仅支持向量)

→可以控制过拟合和欠拟合(使用 C 超参数)

# SVM 的挑战

→选择合适的内核及其参数

→在硬利润/软利润之间选择

→选择正确的 C 值

# SVM 的缺点

→当我们拥有大型数据集时，它的性能不佳

→对噪声数据敏感(可能会使数据过拟合)

# 结论

综上所述，SVM 是一个有监督的机器学习算法，既能分类又能分类，但以分类而闻名。它主要用于文本分类以及许多其他应用。

SVM 的数学和编码以及其他算法都在计划中，将在未来的故事中讨论

谢谢你