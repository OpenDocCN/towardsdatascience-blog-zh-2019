<html>
<head>
<title>Where did the Binary Cross-Entropy Loss Function come from?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">二元交叉熵损失函数从何而来？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715?source=collection_archive---------4-----------------------#2019-11-15">https://towardsdatascience.com/where-did-the-binary-cross-entropy-loss-function-come-from-ac3de349a715?source=collection_archive---------4-----------------------#2019-11-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c6cc" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">博文补充部分"</em> <a class="ae km" href="https://medium.com/@rafayak/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c" rel="noopener"> <em class="kl">无非 NumPy:理解&amp;从零开始用计算图创建二分类神经网络</em></a><em class="kl"/></p></div><div class="ab cl kn ko hu kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="ij ik il im in"><p id="bb47" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">二元分类提出了一个独特的问题，其中:</p><ol class=""><li id="e37b" class="ku kv iq jp b jq jr ju jv jy kw kc kx kg ky kk kz la lb lc bi translated"><em class="kl">每个例子(</em> <strong class="jp ir"> x </strong> <em class="kl">，</em> <strong class="jp ir"> y </strong> <em class="kl">)都属于</em> <strong class="jp ir"> <em class="kl">两个</em> </strong> <em class="kl">互补类</em>、</li><li id="9417" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk kz la lb lc bi translated"><em class="kl">每个示例相互独立(即一个示例的结果不会影响另一个示例的结果)并且</em>，</li><li id="4369" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk kz la lb lc bi translated"><em class="kl">所有生成的示例都来自相同的底层分布/过程(即，如果我们为“猫对非猫”检测创建数据集，那么我们输入神经网络用于训练“猫对非猫”的所有示例都应该来自相同的数据集，而不是来自不同的不相关数据集，例如“狗对非狗”的数据集)。</em></li></ol><p id="5ab1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在统计学和概率论中，属性<em class="kl"> 2 </em>和<em class="kl"> 3 </em>统称为<strong class="jp ir"> <em class="kl"> i.i.d(独立同分布)。</em></strong><strong class="jp ir"><em class="kl">I . I . d .</em></strong>假设有助于使许多计算简单得多。</p><p id="a86e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们只需要预测正类<em class="kl">即</em><strong class="jp ir"><em class="kl">p(y = 1 | x)=p̂</em></strong>因为负类的概率可以从中导出<em class="kl">即</em><strong class="jp ir"><em class="kl">p(y = 0 | x)= 1-p(y = 1 | x)=1-p̂</em></strong>。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi li"><img src="../Images/c86f7984e49d028b9b6d8b9389eca632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*0xl7aqf8-HUvRT4aUIEgBQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 1. Piecewise probability expression</figcaption></figure><p id="c013" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个好的二进制分类器应该产生一个<strong class="jp ir"> <em class="kl">高</em> </strong>值的<strong class="jp ir"><em class="kl"/></strong>当例子有一个正标签(<strong class="jp ir"> <em class="kl"> y=1 </em> </strong> ) <strong class="jp ir">。</strong>另一方面，对于一个负标签的例子(<strong class="jp ir"> y=0 </strong>)，分类器应该产生一个<strong class="jp ir"> <em class="kl">低</em></strong><strong class="jp ir"><em class="kl">p̂</em>的值。</strong> <em class="kl"> </em>换句话说:</p><ul class=""><li id="0f28" class="ku kv iq jp b jq jr ju jv jy kw kc kx kg ky kk lu la lb lc bi translated"><em class="kl">最大化</em> <strong class="jp ir"> <em class="kl"> p̂ </em> </strong>当<strong class="jp ir"> <em class="kl"> y=1 </em> </strong>和</li><li id="a271" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk lu la lb lc bi translated"><em class="kl">最大化</em> <strong class="jp ir"> <em class="kl"> 1-p̂ </em> </strong>当<strong class="jp ir"> <em class="kl"> y=0 时。</em> </strong></li></ul><p id="c133" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看如何将这种直觉结合成一个单一的表达:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/5bac755f86d1ec913e9bbb588edaced0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*aRvSEo6FX6F8DL_ITUD6oQ.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 2. Bernoulli Distribution expression</figcaption></figure><p id="0216" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">原来我们想出来的单行表达式，在上图中，叫做<strong class="jp ir"><em class="kl"/></strong>伯努利分布，计算它为<em class="kl">单个数据点</em>叫做<strong class="jp ir"> <em class="kl">伯努利试验</em> </strong>。我们需要<em class="kl">最大化</em><strong class="jp ir"><em class="kl">伯努利分布</em> </strong> <em class="kl">对于每一次试验</em>，我们该如何做呢？这很简单，回想一下你的高中时代，任何凸函数(u 形函数)的最大值(或最小值)都出现在 1ˢᵗ导数等于零的点上。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/7e69db1ab8cb64a6572e18af81c9feca.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*NKDax2DHx2bTPckxXEVrBw.png"/></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 3. At min/max of a function, the derivative is zero</figcaption></figure><p id="1c98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<strong class="jp ir"> <em class="kl">伯努利分布</em> </strong>表达式及其导数的计算在其当前形式下可能会有点麻烦，更不用说小值的乘法和幂运算可能会在数值上不稳定，并可能导致数值溢出。幸运的是，<strong class="jp ir"><em class="kl"/></strong><em class="kl">(由“</em> log <em class="kl">”表示，而不是“</em>ln<em class="kl">”</em>)可以在这里帮到我们。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi lx"><img src="../Images/d7f9c6673bcf663fa4a08c3a7806c7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kHhZLbka-TVKrRUI-ffOVw.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 4. Taking log of Bernoulli Distribution</figcaption></figure><p id="cbf0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="kl">请注意，在将自然对数应用于伯努利分布后，我们将表达式简化为概率对数的总和</em><strong class="jp ir"><em class="kl"/></strong>。此外，请注意，这个简化的表达式非常类似于二元交叉熵损失函数，但符号相反。为了通过数值方法达到伯努利分布 的<strong class="jp ir"> <em class="kl">对数的最大点(即朝着最优点的方向迭代移动)，我们需要执行<strong class="jp ir"> <em class="kl">【梯度上升】</em> </strong> <em class="kl">，因为对数函数的曲线向上弯曲(</em> <strong class="jp ir"> <em class="kl">图 4 </em> </strong> <em class="kl">)，即凹形</em> <strong class="jp ir"> <em class="kl">。</em> </strong>在神经网络中，我们更喜欢用<strong class="jp ir"> <em class="kl">梯度下降</em> </strong>而不是<strong class="jp ir"> <em class="kl">上升</em> </strong>来寻找最优点。我们这样做是因为神经网络的学习/优化被设定为一个"<em class="kl">损失最小化"</em>问题，所以这是我们将负号添加到伯努利分布</em> </strong> <em class="kl">的<strong class="jp ir"> <em class="kl">对数的地方，结果是二元交叉熵损失函数:</em></strong></em></p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mc"><img src="../Images/1862005022036862bb47dd3d974a15cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HUddTtyusGuUTIZFhbFf1w.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 5. Taking negative of the log of Bernoulli Distribution</figcaption></figure><p id="0bae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意<em class="kl">最大化</em>伯努利分布的<em class="kl">对数与<em class="kl">最小化</em>伯努利分布的<em class="kl">负对数相同。</em> </em></strong> <em class="kl">最小点和最大点出现在同一点，现在我们可以很容易地应用梯度下降，沿着曲线向下移动到最佳点。</em></p><p id="0c8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用一个叫做<strong class="jp ir"> <em class="kl">最大似然估计(MLE) </em> </strong>的概念，我们可以扩展伯努利分布，得出<em class="kl">二元交叉熵成本</em>函数。回想一下，对于单个数据点，我们将最大化一次伯努利试验，<em class="kl">对于多个数据点，我们将最大化多次伯努利试验的乘积。</em></p><p id="ff8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">考虑以下示例，其中我们有两个分类器 A 和 B，它们对三个<strong class="jp ir">I . I . d .</strong>示例进行概率预测:</p><ul class=""><li id="b62a" class="ku kv iq jp b jq jr ju jv jy kw kc kx kg ky kk lu la lb lc bi translated">分类器-A : P(X₁),P(X₂),P(X₃) = 0.7，0.8，0.9</li><li id="2152" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk lu la lb lc bi translated">分类器-B : P(X₁),P(X₂),P(X₃) = 0.8，0.8，0.8</li></ul><p id="74ec" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么对于我们的三个例子 X₁，X₂和 X₃，哪个分类器更有可能是更好的分类器呢？</p><p id="e812" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">根据 MLE，具有最高概率乘积的分类器<strong class="jp ir">可能</strong>是更高级的分类器。让我们检查一下:</p><ul class=""><li id="a91a" class="ku kv iq jp b jq jr ju jv jy kw kc kx kg ky kk lu la lb lc bi translated">分类器-a:p(x₁)×p(x₂)×p(x₃)= 0.7×0.8×0.9 = 0.504</li><li id="4455" class="ku kv iq jp b jq ld ju le jy lf kc lg kg lh kk lu la lb lc bi translated">分类器-b:p(x₁)×p(x₂)×p(x₃)= 0.8×0.8×0.8 = 0.512</li></ul><p id="e6f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以分类器-B 是<strong class="jp ir"> <em class="kl">更可能是</em> </strong>更好的分类器。</p><p id="4440" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将此概念应用于多个独立伯努利试验(<em class="kl">具有多个独立伯努利试验的分布被称为</em> <strong class="jp ir"> <em class="kl">二项式分布</em> </strong>)并最大化数据集/批次中每个<strong class="jp ir"><em class="kl">【m】</em></strong>示例的概率，我们得到:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi md"><img src="../Images/393ba963cf7370335c6af4d2f503342e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5grmYwT6GEPf4OMYUAyXEw.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 6. The Likelihood function</figcaption></figure><p id="8d89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当前形式的<strong class="jp ir"> <em class="kl">似然</em> </strong>函数容易因为多个乘积而出现数值溢出。所以我们将改为取似然函数的<strong class="jp ir"> <em class="kl">自然对数。</em> </strong></p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi me"><img src="../Images/727908c58d3ff0e0a3c07170a3276ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JPpvQKzpayvccD02DK11Q.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 7. Log of the likelihood function</figcaption></figure><p id="5e64" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">回想一下，最大化一个函数与最小化该函数的负值是一样的。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mf"><img src="../Images/c39a035deee06f34213aed0710587120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmhDCT4vOZz4MJLQrRdLSg.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 8. The negative Log-likelihood function of the Bernoulli Trails</figcaption></figure><p id="e7f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于<strong class="jp ir"> <em class="kl">缩放函数不会改变函数的最大值或最小值</em> </strong>点(例如，<strong class="jp ir"> <em class="kl"> y=x </em> </strong>和<strong class="jp ir"> <em class="kl"> y=4x </em> </strong>位于<strong class="jp ir"> <em class="kl"> (0，0) </em> </strong>处)，所以最后，我们将<em class="kl">负对数似然</em>函数除以示例总数(<strong class="jp ir"> <em class="kl">原来是我们一直在用的二元交叉熵(BCE)代价函数。</em></strong></p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="gh gi mg"><img src="../Images/82f915bf78a82237ac16124f845aa4f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9HcMYTPeN1cV56qqxe6dA.png"/></div></div><figcaption class="lq lr gj gh gi ls lt bd b be z dk">Fig 9. Binary Cross-Entropy Function is Negative Log-Likelihood scaled by the reciprocal of the number of examples(m)</figcaption></figure><p id="f627" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，我们假设基础数据服从伯努利分布，这使得我们可以使用最大似然法并得出一个合适的成本函数。 <strong class="jp ir"> <em class="kl">数据的这种假设/知识在贝叶斯统计中称为“先验”。</em> </strong></p><p id="fcc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如有任何问题，请随时在<a class="ae km" href="https://twitter.com/RafayAK" rel="noopener ugc nofollow" target="_blank">Twitter</a><a class="ae km" href="https://twitter.com/RafayAK" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">@</strong>RafayAK</a>上联系我，并查看“<a class="ae km" href="https://medium.com/@rafayak/nothing-but-numpy-understanding-creating-binary-classification-neural-networks-with-e746423c8d5c" rel="noopener">二元分类</a>上的其余帖子。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><a href="https://www.buymeacoffee.com/rafaykhan"><div class="gh gi mh"><img src="../Images/bdb1aed53d63b0bff2c1599be3aa9dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*q8O4kAyuVD-9slT_aAKc1Q.png"/></div></a><figcaption class="lq lr gj gh gi ls lt bd b be z dk">If you enjoyed it!</figcaption></figure></div></div>    
</body>
</html>