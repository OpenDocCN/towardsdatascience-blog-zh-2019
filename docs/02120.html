<html>
<head>
<title>Hitchhiker’s Guide to Residual Networks (ResNet) in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">喀拉斯剩余网络搭便车指南(ResNet)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hitchhikers-guide-to-residual-networks-resnet-in-keras-385ec01ec8ff?source=collection_archive---------2-----------------------#2019-04-08">https://towardsdatascience.com/hitchhikers-guide-to-residual-networks-resnet-in-keras-385ec01ec8ff?source=collection_archive---------2-----------------------#2019-04-08</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><div class=""><h2 id="ae81" class="pw-subtitle-paragraph js iu iv bd b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj dk translated">学习剩余网络的基础，并在 Keras 中建立一个 ResNet</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/cf65c4bc0731269b32241a543de63e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FIgQCF4EF1WmMGkf"/></div></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Photo by <a class="ae la" href="https://unsplash.com/@bethewerewolf?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Andrés Canchón</a> on <a class="ae la" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e97a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">非常深的神经网络很难训练，因为它们更容易消失或爆发梯度。为了解决这个问题，来自一层的激活单元可以被直接馈送到网络的更深层，这被称为<strong class="ld iw">跳过连接</strong>。</p><p id="3868" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这形成了<strong class="ld iw">剩余网络</strong>或<strong class="ld iw">剩余网络</strong>的基础。这篇文章将在 Keras 中实现一个之前介绍剩余网络的基础知识。</p><blockquote class="lx"><p id="6fe7" class="ly lz iv bd ma mb mc md me mf mg lw dk translated">关于机器学习、深度学习和人工智能的实践视频教程，请查看我的<a class="ae la" href="https://www.youtube.com/channel/UC-0lpiwlftqwC7znCcF83qg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> YouTube 频道</a>。</p></blockquote><figure class="mh mi mj mk ml kp"><div class="bz fp l di"><div class="mm mn l"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">With ResNets, we can build very deep neural networks</figcaption></figure><h1 id="68fc" class="mo mp iv bd mq mr ms mt mu mv mw mx my kb mz kc na ke nb kf nc kh nd ki ne nf bi translated">残余块</h1><p id="6c41" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">ResNet 的构造块被称为<strong class="ld iw">剩余块</strong>或<strong class="ld iw">标识块</strong>。当一层的激活被快进到神经网络中更深的层时，残余块就简单了。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/587ce460b00173eb79652c27f4854f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*Qq5cR6Zn2ZGBAQqIsWpsXg.png"/></div><figcaption class="kw kx gj gh gi ky kz bd b be z dk">Example of a residual block</figcaption></figure><p id="cacd" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">正如您在上面的图像中看到的，来自前一层的激活被添加到网络中更深一层的激活中。</p><p id="1368" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">这个简单的调整允许训练更深层次的神经网络。</p><p id="eb77" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">理论上，训练误差应该随着更多层被添加到神经网络而单调减少。然而，在实践中，对于传统的神经网络，它将达到训练误差开始增加的点。</p><p id="3945" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">ResNets 没有这个问题。随着更多的层被添加到网络中，训练误差将不断减小。实际上，ResNets 已经使训练 100 层以上的网络成为可能，甚至达到 1000 层。</p><h1 id="fa83" class="mo mp iv bd mq mr ms mt mu mv mw mx my kb mz kc na ke nb kf nc kh nd ki ne nf bi translated">建立图像分类的资源网</h1><p id="f1ca" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">现在，让我们使用<a class="ae la" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>为图像分类构建一个 50 层的 ResNet。</p><blockquote class="nm nn no"><p id="26ba" class="lb lc np ld b le lf jw lg lh li jz lj nq ll lm ln nr lp lq lr ns lt lu lv lw io bi translated">Keras 是一个高级神经网络 API，用 Python 编写，能够在<a class="ae la" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>、<a class="ae la" href="https://github.com/Microsoft/cntk" rel="noopener ugc nofollow" target="_blank"> CNTK </a>或<a class="ae la" href="https://github.com/Theano/Theano" rel="noopener ugc nofollow" target="_blank"> Theano </a>之上运行。它的开发重点是支持快速实验。</p></blockquote><p id="9fcc" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在这种情况下，我们将使用 TensorFlow 作为后端。当然，在开始之前，可以随意抓取整个<a class="ae la" href="https://github.com/marcopeix/Deep_Learning_AI/blob/master/4.Convolutional%20Neural%20Networks/2.Deep%20Convolutional%20Models/Residual%20Networks.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>并进行所有必要的导入。</p><h2 id="a0f2" class="nt mp iv bd mq nu nv dn mu nw nx dp my lk ny nz na lo oa ob nc ls oc od ne oe bi translated">步骤 1:定义标识块</h2><p id="67d1" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">首先，我们定义身份块，这将使我们的神经网络成为一个<em class="np">剩余网络</em>，因为它表示跳过连接:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi of"><img src="../Images/b442fb3152248cec2b841adf3c82d6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ch0bt5vPFf1nZC6MMRjnYQ.png"/></div></div></figure><h2 id="4897" class="nt mp iv bd mq nu nv dn mu nw nx dp my lk ny nz na lo oa ob nc ls oc od ne oe bi translated">步骤 2:卷积块</h2><p id="99c3" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">然后，我们构建一个卷积模块，如下所示:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi of"><img src="../Images/a432ccce35c9f1080257ed65b33ec45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMutYYp5zvt8X8WqjAFyDw.png"/></div></div></figure><p id="910a" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">注意卷积块是如何结合<em class="np">主</em>路径和<em class="np">快捷方式</em>的。</p><h2 id="be4e" class="nt mp iv bd mq nu nv dn mu nw nx dp my lk ny nz na lo oa ob nc ls oc od ne oe bi translated">步骤 3:构建模型</h2><p id="7516" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">现在，我们将两个模块结合起来构建一个 50 层剩余网络:</p><figure class="kl km kn ko gt kp"><div class="bz fp l di"><div class="og mn l"/></div></figure><h2 id="9d89" class="nt mp iv bd mq nu nv dn mu nw nx dp my lk ny nz na lo oa ob nc ls oc od ne oe bi translated">第四步:培训</h2><p id="aced" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">在训练之前，要认识到我们有一个返回<em class="np">模型</em>的函数。因此，我们需要将它赋给一个变量。然后，Keras 要求我们编译模型:</p><pre class="kl km kn ko gt oh oi oj ok aw ol bi"><span id="2552" class="nt mp iv oi b gy om on l oo op">model = ResNet50(input_shape = (64, 64, 3), classes = 6)<br/>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</span></pre><p id="28fa" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">完成后，我们可以对图像进行标准化，并对其进行一次性编码:</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oq"><img src="../Images/d6847d6ac311219e0e273e623bb1b747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GHuchbEHNWkbxlSBmWaow.png"/></div></div></figure><p id="53dd" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">之后，我们可以拟合模型:</p><pre class="kl km kn ko gt oh oi oj ok aw ol bi"><span id="ecc3" class="nt mp iv oi b gy om on l oo op">model.fit(X_train, Y_train, epochs = 2, batch_size = 32)</span></pre><p id="8fd2" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">看看它的表现如何:</p><pre class="kl km kn ko gt oh oi oj ok aw ol bi"><span id="875c" class="nt mp iv oi b gy om on l oo op">preds = model.evaluate(X_test, Y_test)<br/>print (“Loss = “ + str(preds[0]))<br/>print (“Test Accuracy = “ + str(preds[1]))</span></pre><p id="9f86" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">现在，你会看到我们只有 16%的准确率。这是因为我们只训练了 2 个纪元。你可以在你自己的机器上训练你的模型更长的时间，但是要意识到这需要很长的时间，因为它的网络非常大。</p><h2 id="754c" class="nt mp iv bd mq nu nv dn mu nw nx dp my lk ny nz na lo oa ob nc ls oc od ne oe bi translated">步骤 5:打印模型摘要</h2><p id="0ced" class="pw-post-body-paragraph lb lc iv ld b le ng jw lg lh nh jz lj lk ni lm ln lo nj lq lr ls nk lu lv lw io bi translated">Keras 使得对我们刚刚构建的模型进行总结变得非常容易。只需运行以下代码:</p><pre class="kl km kn ko gt oh oi oj ok aw ol bi"><span id="1747" class="nt mp iv oi b gy om on l oo op">model.summary()</span></pre><p id="872e" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">您将获得网络中每一层的详细总结。</p><p id="00cf" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">您还可以生成网络架构的图片，并将其保存在您的工作目录中:</p><pre class="kl km kn ko gt oh oi oj ok aw ol bi"><span id="3db6" class="nt mp iv oi b gy om on l oo op">plot_model(model, to_file=’ResNet.png’)<br/>SVG(model_to_dot(model).create(prog=’dot’, format=’svg’))</span></pre></div><div class="ab cl or os hz ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="io ip iq ir is"><p id="ec4f" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">太好了！您刚刚学习了剩余网络的基础知识，并使用 Keras 构建了一个剩余网络！同样，随意训练算法更长时间(约 20 个历元)，您应该会看到网络表现非常好。然而，如果你只在 CPU 上训练，这可能需要超过 1 小时。</p><p id="d789" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">在未来的帖子中，我将展示如何在 TensorFlow 中执行神经风格转移，这是一种非常有趣的应用卷积神经网络的方法！</p><p id="ad0b" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">继续学习！</p><p id="13a5" class="pw-post-body-paragraph lb lc iv ld b le lf jw lg lh li jz lj lk ll lm ln lo lp lq lr ls lt lu lv lw io bi translated">参考:<a class="ae la" href="https://www.deeplearning.ai/" rel="noopener ugc nofollow" target="_blank"> deeplearning.ai </a></p></div></div>    
</body>
</html>