<html>
<head>
<title>Learning Parameters, Part 4: Tips For Adjusting Learning Rate, Line Search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习参数，第 4 部分:调整学习速度的技巧，线搜索</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b?source=collection_archive---------20-----------------------#2019-09-27">https://towardsdatascience.com/learning-parameters-part-4-6a18d1d3000b?source=collection_archive---------20-----------------------#2019-09-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="155f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/learning-parameters/latest" rel="noopener">学习参数</a></h2><div class=""/><div class=""><h2 id="9cc4" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">在进入高级优化算法之前，让我们回顾一下梯度下降中的学习率问题。</h2></div><p id="9138" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-3-ee8558f65dd7">第 3 部分</a>中，我们看到了优化器的随机性和小批量版本。在这篇文章中，我们将会看到一些关于如何调整学习率等的普遍遵循的启发法。如果你对这些试探法不感兴趣，可以直接跳到<a class="ae lk" href="https://medium.com/tag/learning-parameters/latest" rel="noopener">学习参数</a>系列的<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-5-65a2f3583f7d">第 5 部分</a>。</p><blockquote class="ll lm ln"><p id="8266" class="ko kp lo kq b kr ks ka kt ku kv kd kw lp ky kz la lq lc ld le lr lg lh li lj ij bi translated">引用说明:本博客中的大部分内容和图表直接取自 IIT 马德拉斯大学教授 Mitesh Khapra 提供的深度学习课程第 5 讲。</p></blockquote><p id="5bd1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有人可能会说，我们可以通过设置较高的学习率来解决在缓坡上导航的问题(即，通过将较小的坡度乘以较大的学习率<strong class="kq ja"> <em class="lo"> η </em> </strong>)来放大较小的坡度)。这个看似微不足道的想法有时在误差函数平缓的情况下确实有效，但当误差面平坦时就不起作用了。这里有一个例子:</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/d55609642b49918959624769367809ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/1*umP0flU2tdvAAfW_qfli_g.gif"/></div></figure><p id="437a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">显然，在斜率较大的区域，已经很大的梯度会进一步扩大，较大的学习率在某种程度上有助于这一原因，但一旦误差表面变平，就没有太大帮助了。假设有一个可以适应梯度的学习率总是好的，这是安全的，我们将在学习参数系列的下一篇文章(第 5 部分)中看到一些这样的算法。</p><h1 id="6af6" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">一些有用的提示</h1><h2 id="b431" class="ms mb iq bd mc mt mu dn mg mv mw dp mk kx mx my mm lb mz na mo lf nb nc mq iw bi translated">初学率小贴士</h2><ul class=""><li id="3890" class="nd ne iq kq b kr nf ku ng kx nh lb ni lf nj lj nk nl nm nn bi translated">调整学习率。在对数标度上尝试不同的值:0.0001、0.001、0.01、0.1、1.0。</li><li id="4066" class="nd ne iq kq b kr no ku np kx nq lb nr lf ns lj nk nl nm nn bi translated">每一个都运行几个时期，找出一个最有效的学习速率。</li><li id="e02e" class="nd ne iq kq b kr no ku np kx nq lb nr lf ns lj nk nl nm nn bi translated">现在围绕这个值进行更精细的搜索。例如，如果最佳学习率是 0.1，那么现在尝试一些值:0.05，0.2，0.3。</li><li id="4efb" class="nd ne iq kq b kr no ku np kx nq lb nr lf ns lj nk nl nm nn bi translated">声明:这些只是启发，没有明确的赢家策略。</li></ul><h2 id="5359" class="ms mb iq bd mc mt mu dn mg mv mw dp mk kx mx my mm lb mz na mo lf nb nc mq iw bi translated">退火学习率提示</h2><p id="18d7" class="pw-post-body-paragraph ko kp iq kq b kr nf ka kt ku ng kd kw kx nt kz la lb nu ld le lf nv lh li lj ij bi translated"><strong class="kq ja">阶跃衰减</strong></p><ul class=""><li id="a6fa" class="nd ne iq kq b kr ks ku kv kx nw lb nx lf ny lj nk nl nm nn bi translated">每 5 个周期后将学习率减半</li><li id="2019" class="nd ne iq kq b kr no ku np kx nq lb nr lf ns lj nk nl nm nn bi translated">如果验证误差大于前一个时期结束时的误差，则在一个时期后将学习率减半</li></ul><p id="cf5c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">指数衰减</strong></p><ul class=""><li id="9cfe" class="nd ne iq kq b kr ks ku kv kx nw lb nx lf ny lj nk nl nm nn bi translated"><em class="lo"> η = η₀⁻ᵏᵗ </em>，其中<em class="lo"> η₀ </em> <strong class="kq ja"> <em class="lo"> </em> </strong>和<em class="lo"> k </em>为超参数，t 为步数</li></ul><p id="4ae0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja"> 1/t 衰变</strong></p><ul class=""><li id="ac25" class="nd ne iq kq b kr ks ku kv kx nw lb nx lf ny lj nk nl nm nn bi translated"><em class="lo"> η = (η₀)/(1+kt)，</em>其中<em class="lo"> η₀ </em> <strong class="kq ja"> <em class="lo"> </em> </strong>和<em class="lo"> k </em>为超参数，t 为步数。</li></ul><h2 id="0151" class="ms mb iq bd mc mt mu dn mg mv mw dp mk kx mx my mm lb mz na mo lf nb nc mq iw bi translated">动量秘诀</h2><p id="097c" class="pw-post-body-paragraph ko kp iq kq b kr nf ka kt ku ng kd kw kx nt kz la lb nu ld le lf nv lh li lj ij bi translated">以下时间表由 Sutskever <em class="lo">等人</em>于 2013 年提出</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/7c78f923943ac86f2aee6f3c87e064f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*0LV_yZM5e516h1YFHaFXDA.png"/></div></figure><p id="ca6f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其中，<em class="lo"> γ_max </em>选自{0.999，0.995，0.99，0.9，0}。</p><h1 id="37d6" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">线搜索</h1><p id="e7f2" class="pw-post-body-paragraph ko kp iq kq b kr nf ka kt ku ng kd kw kx nt kz la lb nu ld le lf nv lh li lj ij bi translated">在实践中，通常进行线搜索以找到相对更好的<em class="lo"> η </em>值。在线搜索中，我们使用不同的学习率(<em class="lo"> η </em>)更新<em class="lo"> w </em>，并在每次迭代中检查更新后的模型的误差。最终，我们保留给出最低损失的<em class="lo"> w </em>的更新值。看一下代码:</p><p id="863d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本质上，在每一步，我们都试图从可用的选项中使用最佳的<em class="lo"> η </em>值。这显然不是最好的主意。我们在每一步都做了更多的计算，但这是寻找最佳学习速率的一种折衷。今天，有更酷的方法可以做到这一点。</p><h2 id="497a" class="ms mb iq bd mc mt mu dn mg mv mw dp mk kx mx my mm lb mz na mo lf nb nc mq iw bi translated">在线搜索正在进行</h2><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div class="gh gi ls"><img src="../Images/5a1cf474579bfe100a291988aee84ce5.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/1*8lDYTCFKGAUHcsJpbzdGsQ.gif"/></div></figure><p id="0d37" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">显然，收敛比普通梯度下降更快(见<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-1-eb3e8bb9ffbb?source=your_stories_page---------------------------">第 1 部分</a>)。我们看到一些振荡，但注意到这些振荡与我们在动量和 NAG 中看到的非常不同(见<a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-2-a190bef2d12">第 2 部分</a>)。</p><p id="4756" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">注:</strong> Leslie N. Smith 在他 2015 年的论文中，<a class="ae lk" href="https://arxiv.org/pdf/1506.01186.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lo">循环学习率用于训练神经网络</em> </a> <em class="lo"> </em>提出了一种比线搜索更聪明的方式。我建议读者参考<a class="ae lk" rel="noopener" target="_blank" href="/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0">这篇由<a class="ae lk" href="https://towardsdatascience.com/@surmenok" rel="noopener" target="_blank"> Pavel Surmenok </a>撰写的文章</a>来了解更多信息。</p><h1 id="342c" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">结论</h1><p id="2ce0" class="pw-post-body-paragraph ko kp iq kq b kr nf ka kt ku ng kd kw kx nt kz la lb nu ld le lf nv lh li lj ij bi translated">在学习参数系列的这一部分中，我们看到了一些启发，可以帮助我们调整学习速度和动力，以便更好地进行培训。我们还研究了线搜索，这是一种曾经流行的方法，用于在梯度更新的每一步找到最佳学习速率。在学习参数系列的下一(最后)部分，我们将仔细研究具有自适应学习速率的梯度下降，特别是以下优化器——AdaGrad、RMSProp 和 Adam。</p><p id="f7ec" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你可以在这里找到下一部分:</p><ul class=""><li id="88c3" class="nd ne iq kq b kr ks ku kv kx nw lb nx lf ny lj nk nl nm nn bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/learning-parameters-part-5-65a2f3583f7d">学习参数，第 5 部分:AdaGrad、RMSProp 和 Adam </a></li></ul><h1 id="56c4" class="ma mb iq bd mc md me mf mg mh mi mj mk kf ml kg mm ki mn kj mo kl mp km mq mr bi translated">承认</h1><p id="bc8e" class="pw-post-body-paragraph ko kp iq kq b kr nf ka kt ku ng kd kw kx nt kz la lb nu ld le lf nv lh li lj ij bi translated">IIT·马德拉斯教授的<a class="ae lk" href="https://www.cse.iitm.ac.in/~miteshk/" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja"/></a>和<a class="ae lk" href="https://www.cse.iitm.ac.in/~miteshk/CS7015.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> CS7015:深度学习</strong> </a> <strong class="kq ja"> </strong>课程如此丰富的内容和创造性的可视化，这要归功于很多。我只是简单地整理了提供的课堂讲稿和视频。</p><figure class="lt lu lv lw gt lx gh gi paragraph-image"><div role="button" tabindex="0" class="ob oc di od bf oe"><div class="gh gi oa"><img src="../Images/76bca3209966c0bfa781d09c81a5c8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fRGHjjqjl5P6IArtZpx_Hg.jpeg"/></div></div></figure></div></div>    
</body>
</html>