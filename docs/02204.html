<html>
<head>
<title>Random Forests for Complete Beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">完全初学者的随机森林</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forests-for-complete-beginners-2014d9ed91c0?source=collection_archive---------10-----------------------#2019-04-11">https://towardsdatascience.com/random-forests-for-complete-beginners-2014d9ed91c0?source=collection_archive---------10-----------------------#2019-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/028532ad9590f216c641df03fcda332e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y6AZMkmO2c70JCf_8oMstg.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="73e3" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">随机森林和决策树的权威指南。</h2></div><p id="5a3b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在我看来，大多数机器学习教程对初学者不够友好。</p><p id="4b98" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">上个月，我为完全初学者 写了一篇<a class="ae lp" href="https://victorzhou.com/blog/intro-to-neural-networks/" rel="noopener ugc nofollow" target="_blank">神经网络简介<strong class="kv jf">。这篇文章将采用同样的策略，也就是说<strong class="kv jf">假设没有机器学习的先验知识</strong>。我们将学习什么是随机森林，以及它们是如何从头开始工作的。</strong></a></p><p id="6898" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">准备好了吗？让我们开始吧。</p><h1 id="be32" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">1.决策树🌲</h1><p id="a3ec" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">随机森林🌲🌲🌲实际上只是一堆决策树🌲捆绑在一起(哦💡这就是为什么它被称为<em class="mn">森林</em>。在进入森林之前，我们需要谈论树木。</p><p id="853a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">请看下面的数据集:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/1422ddf6b05f1ea3f1d0624111cb3a4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*fg52zj3pD8BfykLO1lRvxQ.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The Dataset</figcaption></figure><p id="b2de" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果我告诉你有一个坐标为 1 的新点，你认为它会是什么颜色？</p><p id="b85c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">蓝色，对吗？</p><p id="bea8" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你刚刚在脑子里评估了一个决策树:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mx"><img src="../Images/cd0093cfaed669d628285c8da885a715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFsqlPmG5gr2w8IZGFzPsw.png"/></div></div></figure><p id="d258" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是一个简单的决策树，有一个<strong class="kv jf">决策节点</strong>，由<strong class="kv jf">测试</strong> <em class="mn"> x </em> &lt; 2。如果测试通过(<em class="mn">x</em>T49】2)，我们选择左边的<strong class="kv jf">分支</strong>并选择蓝色。如果测试失败(<em class="mn"> x </em> ≥2)，我们选择右边的<strong class="kv jf">分支</strong>并选择绿色。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi my"><img src="../Images/8e97cb34850938f948d571513323331a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*0ap4TCedV17TPc5-hVGaEw.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The Dataset, split at x=2</figcaption></figure><p id="02c2" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">决策树经常被用来回答这类问题:给定一个<strong class="kv jf">标签为</strong>的数据集，我们应该如何<strong class="kv jf">对</strong>新样本进行分类？</p><blockquote class="mz na nb"><p id="5869" class="kt ku mn kv b kw kx kf ky kz la ki lb nc ld le lf nd lh li lj ne ll lm ln lo im bi translated"><strong class="kv jf">标记</strong>:我们的数据集被标记，因为每个点都有一个类别(颜色):蓝色或绿色。</p><p id="b047" class="kt ku mn kv b kw kx kf ky kz la ki lb nc ld le lf nd lh li lj ne ll lm ln lo im bi translated"><strong class="kv jf">分类</strong>:分类一个新的数据点就是给它分配一个类别(颜色)。</p></blockquote><p id="232c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这里的数据集现在有 3 个类，而不是 2 个:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b6b84eb6d65e99013faac2da56970579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*IsQPPs2x3L2C6vMQweBQig.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The Dataset v2</figcaption></figure><p id="2058" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们的旧决策树不再那么管用了。给定新的点(<em class="mn"> x </em>，<em class="mn"> y </em>)，</p><ul class=""><li id="bc53" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo nl nm nn no bi translated">如果<em class="mn"> x </em> ≥2，我们仍然可以理直气壮地将其归类为绿色。</li><li id="adbb" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">如果<em class="mn"> x </em> &lt; 2，我们不能马上将其归类为蓝色——它也可能是红色。</li></ul><p id="051b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们需要添加另一个<strong class="kv jf">决策节点</strong>到我们的决策树中:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nu"><img src="../Images/defecc0acc7fb2973fd8471f31fa687b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XzuhQdljs1ECrPiatzdJg.png"/></div></div></figure><p id="9069" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">很简单，对吧？这是决策树背后的基本思想。</p><h1 id="4323" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">2.训练决策树</h1><p id="4d6e" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">让我们开始训练决策树吧！我们将再次使用 3 类数据集:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/b6b84eb6d65e99013faac2da56970579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*IsQPPs2x3L2C6vMQweBQig.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">The Dataset v2</figcaption></figure><h1 id="f58c" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">2.1 训练决策树:根节点</h1><p id="784b" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">我们的第一个任务是确定我们的树中的根决策节点。它将测试哪个特性(<em class="mn"> x </em>或<em class="mn"> y </em>)，测试阈值是多少？例如，我们之前树中的根节点使用了<em class="mn"> x </em>特性，测试阈值为 2:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/2001ab039a349ddee7d3c7b37ec49563.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*em8-0bMb_vZNfIrRCzl11Q.png"/></div></div></figure><p id="b5f1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">直觉上，我们想要一个做出“好”分割的决策节点，其中“好”可以粗略地定义为尽可能多地分离不同类的<strong class="kv jf">。上面的根节点做了一个“好”的分割:<em class="mn">所有的</em>果岭在右边，<em class="mn">没有</em>果岭在左边。</strong></p><p id="7b6a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">因此，我们现在的目标是选择一个根节点，给我们一个可能的“最佳”分割。但是我们如何量化拆分有多好呢？很复杂。我写了一整篇博文，讲述了一种使用基尼系数的方法。<strong class="kv jf"> ←我建议在你继续之前现在就阅读它</strong>——我们将在这篇文章的后面使用这些概念。</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="d958" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">欢迎回来！</p><blockquote class="mz na nb"><p id="dabc" class="kt ku mn kv b kw kx kf ky kz la ki lb nc ld le lf nd lh li lj ne ll lm ln lo im bi translated">希望你只是看了<a class="ae lp" href="https://victorzhou.com/blog/gini-impurity/" rel="noopener ugc nofollow" target="_blank">我的基尼不纯帖</a>。如果你没有，这里有一个很短的 TL；DR:我们可以用基尼系数来计算任何分割的基尼系数。<strong class="kv jf">更好的分割有更高的基尼系数。</strong></p></blockquote><p id="0e8b" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">回到确定我们根决策节点的问题。既然我们已经有了评估拆分的方法，我们要做的就是找到可能的最佳拆分！为了简单起见，我们将<strong class="kv jf">尝试每一个可能的分割</strong>并使用最好的分割(基尼系数最高的分割)。<strong class="kv jf">这不是找到最佳分割</strong>的最快方法，但却是最容易理解的。</p><p id="754e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">尝试每一次分裂意味着尝试</p><ul class=""><li id="1775" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo nl nm nn no bi translated">每个特征(<em class="mn"> x </em>或<em class="mn"> y </em>)。</li><li id="b6a5" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">所有“唯一”阈值。我们只需要尝试产生不同分裂的阈值。</li></ul><p id="defc" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">例如，如果我们想使用<em class="mn"> x </em>坐标，我们可以选择以下阈值:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/151aec0dc5682c97621fa7c7167017f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*NkiOZ45MORlN-D0e3YXiKg.png"/></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">x Thresholds</figcaption></figure><p id="b81c" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">让我们对<em class="mn"> x </em> =0.4 的分割进行基尼系数计算。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oe"><img src="../Images/92ef14106ff31a749eade0321da36fb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUUOoY14dFHvrDSZA_KGkA.png"/></div></div></figure><p id="b2ca" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">首先，我们计算整个数据集的基尼系数:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/2539bbe7f7abc508c5f997074c9e2be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZsEJIH1RdGavi6Elokh3Q.png"/></div></div></figure><p id="4f17" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">然后，我们计算两个分支的基尼系数:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/0a5f812043b48c386cfce0236adbc3e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*37Un7IqNEOFTy0mB6O7jxg.png"/></div></figure><p id="8fe7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">最后，我们通过从原始杂质中减去加权分支杂质来计算基尼系数:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oh"><img src="../Images/11f62551fad124bcfd1ae14ca01290b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-r8g3WyIJFJtoz05W456w.png"/></div></div></figure><blockquote class="mz na nb"><p id="7021" class="kt ku mn kv b kw kx kf ky kz la ki lb nc ld le lf nd lh li lj ne ll lm ln lo im bi translated">对刚刚发生的事感到困惑？我告诉过你你应该读一下<a class="ae lp" href="https://victorzhou.com/blog/gini-impurity/" rel="noopener ugc nofollow" target="_blank">我的基尼不洁贴</a>。它可以解释所有这些基尼系数。</p></blockquote><p id="c79a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们可以用同样的方法计算每一种可能分割的基尼系数:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/a77d6a0503c4ef43e629ba49501a5d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PkqBkt_l86pMvuBwQVvcIQ.png"/></div></div></figure><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oj"><img src="../Images/dd186d068a37486ea408242b2d152766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SR_oG3lW6pXmHZ1gAZja1g.png"/></div></div><figcaption class="mt mu gj gh gi mv mw bd b be z dk">All Thresholds</figcaption></figure><p id="e96e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在尝试了<em class="mn"> x </em>和<em class="mn"> y </em>的所有阈值后，我们发现<em class="mn"> x </em> =2 的分割具有最高的 Gini 增益，因此我们将使我们的根决策节点使用阈值为 2 的<em class="mn"> x </em>特性。这是我们目前掌握的情况:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d194b58ab03c1268d068653bc20be4ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*UfXouRE0oUjin8U-5i644w.png"/></div></figure><p id="ba58" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">不断进步！</p><h1 id="d097" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">2.2:训练决策树:第二个节点</h1><p id="f614" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">是时候做出我们的第二个决定了。我们(任意)去左支吧。<strong class="kv jf">我们现在只使用将采取左分支</strong>的数据点(即满足<em class="mn"> x </em> &lt; 2 的数据点)，特别是 3 个蓝色和 3 个红色。</p><p id="99ea" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了建立我们的第二个决策节点，<strong class="kv jf">我们只是做同样的事情！</strong>我们对现有的 6 个数据点尝试了每一种可能的分割，并意识到<em class="mn"> y </em> =2 是最佳分割。我们把它变成一个决策节点，现在有了这个:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ol"><img src="../Images/fa93acb405c2b984e12fe5c28d065c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p63IbWQNBT0pu_AwYCHc5g.png"/></div></div></figure><p id="d4da" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">我们的决策树差不多完成了…</p><h1 id="53a2" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">2.3 训练决策树:何时停止？</h1><p id="5b94" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">让我们继续下去，试着做第三个决策节点。这次我们将使用根节点的右分支。该分支中唯一的数据点是 3 个果岭。</p><p id="9acf" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">同样，我们尝试了所有可能的分裂，但他们都</p><ul class=""><li id="87e3" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo nl nm nn no bi translated">都一样好。</li><li id="6c5e" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">基尼系数为 0(基尼系数已经是 0，不能再低了)。</li></ul><p id="00d7" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在这里添加一个决策节点是没有意义的，因为这样做不会改进我们的决策树。因此，我们将使这个节点成为一个<strong class="kv jf">叶节点</strong>，并给它贴上绿色标签。这意味着<strong class="kv jf">我们将到达该节点的任何数据点归类为绿色</strong>。</p><p id="bda3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">如果我们继续剩下的两个节点，同样的事情会发生:我们将左下方的节点作为蓝叶节点，将右下方的节点作为红叶节点。这给我们带来了最后的结果:</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/bcbcceaaf5477c45c51384ec4fda14d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*miVlzGUdzuncRaUrW9jE7g.png"/></div></div></figure><p id="6edb" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">一旦我们的决策树中所有可能的分支都以叶节点结束，我们就完成了。我们已经训练了一个决策树！</p><h1 id="2a4a" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">3.随机森林🌲🌳🌲🌳🌲</h1><p id="ae8b" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">我们终于准备好讨论随机森林了。记得我之前说的吗？</p><blockquote class="mz na nb"><p id="4fe8" class="kt ku mn kv b kw kx kf ky kz la ki lb nc ld le lf nd lh li lj ne ll lm ln lo im bi translated">随机森林实际上只是捆绑在一起的一堆决策树。</p></blockquote><p id="ed28" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这是真的，但有点简单化了。</p><h1 id="d5c5" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">3.1 装袋</h1><p id="4b34" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">考虑以下算法，在给定一个具有<em class="mn"> n </em>个点的数据集的情况下，训练一组决策树:</p><ol class=""><li id="455c" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo on nm nn no bi translated">样本，<strong class="kv jf">与替换的</strong>，<em class="mn"> n </em>来自数据集中的训练样本。</li><li id="e59a" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo on nm nn no bi translated">在 n 个样本上训练一个决策树。</li><li id="a080" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo on nm nn no bi translated">重复<em class="mn"> t </em>次，换一些<em class="mn"> t </em>。</li></ol><p id="a02a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了使用带有 t <em class="mn"> t </em>树的模型进行预测，我们聚集来自各个决策树的预测，并且</p><ul class=""><li id="62c2" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo nl nm nn no bi translated">如果我们的树产生了类别标签(比如颜色)，就以多数票为准。</li><li id="8ee5" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">如果我们的树产生数值(例如预测温度、价格等)，则取平均值<strong class="kv jf"/>。</li></ul><p id="abb9" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">这种技术叫做<strong class="kv jf">装袋</strong>，或<a class="ae lp" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank">T3】b 装袋<strong class="kv jf">聚集</strong>再聚集</a>。我们进行的替换抽样被称为<a class="ae lp" href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noopener ugc nofollow" target="_blank"> bootstrap </a>样本。</p><figure class="mp mq mr ms gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/b2dc5e027204c99008b019e2f693b743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I70tHSTfgNH93PMGYKAcsA.png"/></div></div></figure><p id="dea1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">袋装决策树非常接近随机森林——它们只是缺少了一样东西…</p><h1 id="61ea" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">3.2 装袋→随机森林</h1><p id="4042" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">袋装决策树只有一个参数:<em class="mn"> t </em>，树的个数。</p><p id="8b7e" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">随机森林还有第二个参数，控制寻找最佳分割点时尝试多少功能。本教程的简单数据集只有两个特征(<em class="mn"> x </em>和<em class="mn"> y </em>)，但大多数数据集都有更多的特征(数百或数千)。</p><p id="83ba" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">假设我们有一个包含 p 个特征的数据集。每次我们做一个新的决策节点时，我们不再尝试所有的功能，而是只尝试功能的一个子集。我们这样做主要是为了注入随机性，使单棵树更加独特，而<strong class="kv jf">减少了树</strong>之间的相关性，从而提高了森林的整体性能。这种技术有时被称为<strong class="kv jf">特征打包</strong>。</p><h1 id="6481" class="lq lr je bd ls lt lu lv lw lx ly lz ma kk mb kl mc kn md ko me kq mf kr mg mh bi translated">4.现在怎么办？</h1><p id="557e" class="pw-post-body-paragraph kt ku je kv b kw mi kf ky kz mj ki lb lc mk le lf lg ml li lj lk mm lm ln lo im bi translated">这是一个初学者对随机森林的介绍！快速回顾一下我们的工作:</p><ul class=""><li id="2d6f" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo nl nm nn no bi translated">介绍了<strong class="kv jf">决策树</strong>，随机森林的积木。</li><li id="9d93" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">学习了如何通过迭代进行可能的最佳分割来训练决策树。</li><li id="65d8" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">定义了基尼系数，这是一个用来量化收入差距有多“好”的指标。</li><li id="bc95" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">看到了<strong class="kv jf">一个随机森林=一堆决策树。</strong></li><li id="d0ca" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">了解<strong class="kv jf">打包</strong>如何结合多棵树的预测。</li><li id="2750" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">了解到<strong class="kv jf">特征装袋</strong>是装袋决策树和随机森林的区别。</li></ul><p id="1b11" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">你可以在这里做一些事情:</p><ul class=""><li id="e145" class="ng nh je kv b kw kx kz la lc ni lg nj lk nk lo nl nm nn no bi translated">在真实数据集上试验 scikit-learn 的<a class="ae lp" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank"> DecisionTreeClassifier </a>和<a class="ae lp" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> RandomForestClassifier </a>类。</li><li id="e31d" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">尝试从头开始编写一个简单的决策树或随机森林实现。我很乐意给予指导或代码审查！只需<a class="ae lp" href="https://twitter.com/victorczhou" rel="noopener ugc nofollow" target="_blank">发微博给我</a>或者<a class="ae lp" href="mailto:vzhou842@gmail.com" rel="noopener ugc nofollow" target="_blank">发邮件给我</a>。</li><li id="b289" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">阅读关于<a class="ae lp" href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting" rel="noopener ugc nofollow" target="_blank">梯度增强决策树</a>的内容，并使用<a class="ae lp" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>，这是一个强大的梯度增强库。</li><li id="a1ec" class="ng nh je kv b kw np kz nq lc nr lg ns lk nt lo nl nm nn no bi translated">阅读关于随机森林的扩展<a class="ae lp" href="https://en.wikipedia.org/wiki/Random_forest#ExtraTrees" rel="noopener ugc nofollow" target="_blank">ext tree</a>，或者玩玩 scikit-learn 的<a class="ae lp" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html" rel="noopener ugc nofollow" target="_blank">ext tree classifier</a>类。</li></ul><p id="0efd" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">本教程到此结束。我喜欢<a class="ae lp" href="https://victorzhou.com/tag/machine-learning" rel="noopener ugc nofollow" target="_blank">写关于机器学习的</a>(但也有其他话题)，所以如果你想获得新帖子的通知，请<a class="ae lp" href="https://victorzhou.com/subscribe/?src=intro-to-random-forests-medium" rel="noopener ugc nofollow" target="_blank"> <strong class="kv jf">订阅</strong> </a> <strong class="kv jf">。</strong></p><p id="8289" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">感谢阅读！</p></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="0c88" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><em class="mn">原载于【victorzhou.com】<a class="ae lp" href="https://victorzhou.com/blog/intro-to-random-forests/" rel="noopener ugc nofollow" target="_blank"><em class="mn"/></a><em class="mn">。</em></em></p></div></div>    
</body>
</html>