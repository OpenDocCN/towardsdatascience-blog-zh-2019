<html>
<head>
<title>Linear Discriminant Analysis (LDA) 101, using R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性判别分析(LDA) 101，使用 R</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-discriminant-analysis-lda-101-using-r-6a97217a55a6?source=collection_archive---------2-----------------------#2019-01-31">https://towardsdatascience.com/linear-discriminant-analysis-lda-101-using-r-6a97217a55a6?source=collection_archive---------2-----------------------#2019-01-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="47ee" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">决策边界、分离、分类等等。让我们潜入 LDA！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f3fcad76985321948a72556cbad827db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7cvaSKXfY8ZZ4-0V9EO1OQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><strong class="bd ky">Marcin Ryczek </strong>—<strong class="bd ky"> A man feeding swans in the snow</strong> (<em class="kz">Aesthetically fitting to the subject</em>)</figcaption></figure></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><p id="2c46" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是我上一篇关于<strong class="lj iu">主成分分析</strong>的文章的后续文章，所以如果你喜欢的话可以看看:</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/principal-component-analysis-pca-101-using-r-361f4c53a9ff"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">主成分分析(PCA) 101，使用 R</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">一次提高一个维度的可预测性和分类能力！使用 2D 图“可视化”30 个维度！</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu ks mg"/></div></div></a></div><p id="3b69" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果没有<strong class="lj iu">就继续读</strong>，我们先处理一个没有 PCA 的案例<strong class="lj iu"/>，然后用<strong class="lj iu"> LDA 跟进 PCA-‘转换’数据</strong>。</p><blockquote class="mv"><p id="aa90" class="mw mx it bd my mz na nb nc nd ne mc dk translated">在构建你的 LDA 模型之前，用 PCA 进行降维会得到(稍微)更好的结果。T11】</p></blockquote><p id="0242" class="pw-post-body-paragraph lh li it lj b lk nf ju lm ln ng jx lp lq nh ls lt lu ni lw lx ly nj ma mb mc im bi translated">如果你喜欢这篇文章并想看更多，请务必关注我的简介。</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="e66c" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">设置</h1><p id="3bc6" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">对于本文，我们将使用来自<a class="ae nk" href="http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29" rel="noopener ugc nofollow" target="_blank"> <em class="oi"> UCI 机器学习报告</em> </a>的乳腺癌威斯康星州数据集作为我们的数据。如果您想继续学习，请继续为自己加载:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="b0a1" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">wdbc </strong>&lt;- read.csv("wdbc.csv", header = F)</span><span id="51d7" class="oo nm it ok b gy ot oq l or os"><strong class="ok iu">features </strong>&lt;- c("radius", "texture", "perimeter", "area", "smoothness", "compactness", "concavity", "concave_points", "symmetry", "fractal_dimension")</span><span id="faa4" class="oo nm it ok b gy ot oq l or os">names(<strong class="ok iu">wdbc</strong>) &lt;- c("<strong class="ok iu">id</strong>", "<strong class="ok iu">diagnosis</strong>", paste0(<strong class="ok iu">features</strong>,"<strong class="ok iu">_mean</strong>"), paste0(<strong class="ok iu">features</strong>,"<strong class="ok iu">_se</strong>"), paste0(<strong class="ok iu">features</strong>,"<strong class="ok iu">_worst</strong>"))</span></pre><p id="f32d" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的代码将简单地加载数据并命名所有 32 个变量。<strong class="lj iu"> ID </strong>、<strong class="lj iu">诊断</strong>和十(30)个不同的特征。来自 UCI:</p><p id="3f5a" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="oi"/><strong class="lj iu"><em class="oi">表示</em></strong><em class="oi"/><strong class="lj iu"><em class="oi">标准误差</em> </strong> <em class="oi">以及</em><strong class="lj iu"><em class="oi"/></strong><em class="oi">最差或者最大(三个最大值的平均值)的这些特征被计算用于每个图像，从而得到</em> <strong class="lj iu"> <em class="oi"> 30 个特征</em> </strong> <em class="oi">。例如，字段 3 是平均半径，字段 13 是半径 SE，字段 23 是最差半径。”</em></p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="b74b" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">为什么是艾达？</h1><p id="715f" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">让我们提醒自己，我们的数据的“点”是什么，<strong class="lj iu">我们试图描述肿瘤的什么性质</strong> <strong class="lj iu">决定了它是否是恶性的。换句话说:“比如说，如果肿瘤有一定的大小、质地和凹陷，那么它很有可能是恶性的。”</strong></p><p id="525d" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这确实是<strong class="lj iu">‘分类’</strong>的基本概念，广泛应用于各种<strong class="lj iu">数据科学</strong>领域，尤其是<strong class="lj iu">机器学习</strong>。</p><p id="6682" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，即使你没有读过我关于主成分分析的文章，我相信你也能体会这个图的简单性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/32a89e169acbd29a35ae01e78eed7555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*afh2Jhhb35FIXoUW0-y_0Q.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">2D PCA-plot showing clustering of “Benign” and “Malignant” tumors across 30 features.</figcaption></figure><p id="9fcd" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们在这里看到的是'<strong class="lj iu"><em class="oi"/></strong>'和'<strong class="lj iu"> <em class="oi">良性</em> </strong>'这两个类别之间的“清晰”<strong class="lj iu">分离</strong>，在一个 30 维数据集中只有<strong class="lj iu"> ~63%方差的图上。</strong></p><p id="7197" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">简单地使用上面图中的二维，我们可能会得到一些很好的估计，但更高维的数据很难掌握(但也说明了更多的差异)，谢天谢地，这就是<strong class="lj iu"> LDA </strong>的作用，它会试图找到我们在分类中最成功的“<strong class="lj iu">截止点</strong>或“<strong class="lj iu">决策界限</strong>，所以现在我们知道<em class="oi">为什么</em>，让我们更好地了解<em class="oi">如何:【T21</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/9cf84faf626ba6aa20277c0be05a5792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*9PMtADVFSt91iumlLt3Jng.png"/></div></figure><p id="5725" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">只考虑<strong class="lj iu">二维</strong>和<strong class="lj iu">两个不同的集群</strong>。LDA 会将这些聚类投影到一维。想象它为每个类别/聚类创建单独的概率密度函数，然后我们尝试<strong class="lj iu">最大化这些之间的差异</strong>(有效地通过<strong class="lj iu">最小化它们之间的‘重叠’区域</strong>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/2853dfde864a614d7a9d6e3b42306030.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXQ5sgMF0XmiY4Jc6gJVwA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">From Sebastian Raschka: <a class="ae nk" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka.com/Articles/2014_python_lda.html</a></figcaption></figure><p id="3d0f" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在上面的例子中，我们有一个沿着 x 轴<strong class="lj iu">的<strong class="lj iu">蓝色</strong>和<strong class="lj iu">绿色</strong>集群的完美分离。这意味着，如果数据的未来点根据提出的<strong class="lj iu">概率密度函数</strong>表现，那么我们应该能够将它们完美地分类为<strong class="lj iu">蓝色</strong>或<strong class="lj iu">绿色</strong>。</strong></p><h2 id="48c5" class="oo nm it bd nn ox oy dn nr oz pa dp nv lq pb pc nx lu pd pe nz ly pf pg ob ph bi translated">好了，到此为止，让我们进入 R 并尝试一下！</h2></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="8ca5" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">原始数据的 LDA(所有 30 个维度)</h1><p id="a342" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">好了，继续节目，让我们从定义数据开始:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="1c6e" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">wdbc.data</strong> &lt;- as.matrix(wdbc[,c(3:32)])<br/>row.names(wdbc.data) &lt;- wdbc$id<br/><strong class="ok iu">wdbc_raw </strong>&lt;- cbind(<strong class="ok iu">wdbc.data</strong>, as.numeric(<strong class="ok iu">wdbc$diagnosis</strong>)-1)<br/>colnames(wdbc_raw)[31] &lt;- "diagnosis"</span></pre><p id="4037" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这只是简单地<strong class="lj iu">将 ID 作为变量</strong>移除，并将我们的数据定义为一个<strong class="lj iu">矩阵</strong>而不是<strong class="lj iu">数据框架</strong>，同时仍然保留 ID，但是在列名<strong class="lj iu">中。</strong></p><p id="6ed6" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们需要定义一个<strong class="lj iu">训练/测试分割</strong>，这样我们就有一些数据可以<strong class="lj iu">测试我们的模型</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="994a" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">smp_size_raw</strong> &lt;- floor(0.75 * nrow(<strong class="ok iu">wdbc_raw</strong>))<br/>train_ind_raw &lt;- sample(nrow(wdbc_raw), size = <strong class="ok iu">smp_size_raw</strong>)<br/><strong class="ok iu">train_raw.df</strong> &lt;- as.data.frame(wdbc_raw[<strong class="ok iu">train_ind_raw</strong>, ])<br/><strong class="ok iu">test_raw.df</strong> &lt;- as.data.frame(wdbc_raw[<strong class="ok iu">-train_ind_raw</strong>, ])</span></pre><p id="cf30" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这将使用<strong class="lj iu"> R </strong>中的<strong class="lj iu"> <em class="oi"> sample() </em> </strong>函数对我们的数据进行<strong class="lj iu"> 75/25 分割</strong>，这非常方便。然后我们将我们的<strong class="lj iu">矩阵</strong>转换成<strong class="lj iu">数据帧</strong>。</p><p id="fd4b" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们的数据已经准备好了，我们可以使用<strong class="lj iu"><em class="oi">【LDA()</em></strong>函数 i <strong class="lj iu"> R </strong>进行我们的分析，它在功能上与<strong class="lj iu"> <em class="oi"> lm() </em> </strong>和<em class="oi">【glm()</em>函数相同:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="19d8" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">f</strong> &lt;- paste(names(<strong class="ok iu">train_raw.df</strong>)[31], "~", paste(names(<strong class="ok iu">train_raw.df</strong>)[-31], collapse=" + "))</span><span id="7166" class="oo nm it ok b gy ot oq l or os"><strong class="ok iu">wdbc_raw.lda</strong> &lt;- <strong class="ok iu">lda</strong>(as.formula(paste(<strong class="ok iu">f</strong>)), data = <strong class="ok iu">train_raw.df</strong>)</span></pre><p id="6add" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这是一个小小的<strong class="lj iu"> lifehack </strong>粘贴所有的变量名，而不是全部手动编写。如果你想看你的<strong class="lj iu"> FDA </strong>的<strong class="lj iu">系数</strong>和<strong class="lj iu">组意味着</strong>的话，你可以调用对象'<em class="oi"> wdbc_raw.lda </em>，但是它相当冗长，所以我不会在本文中发布输出。</p><p id="ade1" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们根据我们的<strong class="lj iu">测试数据</strong>做出一些<strong class="lj iu">预测</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="670c" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">wdbc_raw.lda.predict</strong> &lt;- predict(<strong class="ok iu">wdbc_raw.lda</strong>, newdata = <strong class="ok iu">test_raw.df</strong>)</span></pre><p id="73ab" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你想检查预测，只需调用'<em class="oi">wdbc _ raw . LDA . predict $ class</em>'</p><h2 id="b116" class="oo nm it bd nn ox oy dn nr oz pa dp nv lq pb pc nx lu pd pe nz ly pf pg ob ph bi translated">估价</h2><p id="8248" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">这是令人兴奋的部分，现在我们可以看到<strong class="lj iu">我们的模型表现有多好</strong>！</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="d929" class="oo nm it ok b gy op oq l or os">### CONSTRUCTING ROC AUC PLOT:</span><span id="657b" class="oo nm it ok b gy ot oq l or os"># Get the posteriors as a dataframe.<br/><strong class="ok iu">wdbc_raw.lda.predict.posteriors</strong> &lt;- as.data.frame(<strong class="ok iu">wdbc_raw.lda.predict$posterior</strong>)</span><span id="08b4" class="oo nm it ok b gy ot oq l or os"># Evaluate the model<br/><strong class="ok iu">pred </strong>&lt;- prediction(<strong class="ok iu">wdbc_raw.lda.predict.posteriors</strong>[,2], <strong class="ok iu">test_raw.df$diagnosis</strong>)<br/><strong class="ok iu">roc.perf</strong> = performance(<strong class="ok iu">pred</strong>, measure = "tpr", x.measure = "fpr")<br/><strong class="ok iu">auc.train</strong> &lt;- performance(<strong class="ok iu">pred</strong>, measure = "auc")<br/><strong class="ok iu">auc.train</strong> &lt;- <strong class="ok iu">auc.train</strong>@y.values</span><span id="20b5" class="oo nm it ok b gy ot oq l or os"># Plot<br/>plot(<strong class="ok iu">roc.perf</strong>)<br/>abline(a=0, b= 1)<br/>text(x = .25, y = .65 ,paste("AUC = ", round(<strong class="ok iu">auc.train</strong>[[1]],3), sep = ""))</span></pre><p id="be98" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们开始了，一个美丽的<strong class="lj iu">大鹏图</strong>！在这里，我简单地画出了<strong class="lj iu">兴趣点</strong>，并添加了<strong class="lj iu">图例</strong>来解释它。现在，我绘制为<strong class="lj iu">“最佳”截止点</strong>的点就是我们曲线中具有最低<strong class="lj iu">欧几里德距离</strong>的点到点<em class="oi"> (0，1) </em>的点，该点表示<strong class="lj iu"> 100%真阳性率</strong>和<strong class="lj iu"> 0%假阳性率</strong>，这意味着我们有一个<strong class="lj iu">完美分离</strong> /预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/c72e91a7491fb3ffbf0abad41c231205.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IdKZvQ6uvoTglaxkFuWvZA.png"/></div></div></figure><p id="af26" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj iu"> <em class="oi">那么这是什么意思？这意味着根据我们希望我们的模型如何“表现”,我们可以使用不同的截止值。我们想要<strong class="lj iu"> 100%的真阳性率</strong>以得到一些假阳性为代价吗？或者，我们希望<strong class="lj iu"> 0%的假阳性</strong>以一个爱真阳性率为代价？如果是良性肿瘤，被诊断为恶性肿瘤会更糟吗？如果是恶性肿瘤，被告知你很健康会更糟吗？</em></strong></p><p id="9a64" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们的<strong class="lj iu"/>点的<strong class="lj iu"> TRP 为 96.15% </strong>和<strong class="lj iu"> FPR 为 3.3% </strong>这看起来不错，但我们真的想告诉<strong class="lj iu"> 3.3%的健康人他们患有癌症</strong>和<strong class="lj iu"> 3.85%的病人他们很健康</strong>吗？</p><p id="c6d2" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请记住，您的结果肯定会与我的不同，因为进行训练/测试分割的样本方法是随机的。</p><p id="59c6" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们在 PCA 变换的数据上看一看 LDA，看看我们是否得到一些更好的结果。</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="4f2c" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">基于降维数据(6 维)的 LDA</h1><p id="ba89" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">如果您想继续学习，请阅读我关于 PCA 的文章:</p><div class="md me gp gr mf mg"><a rel="noopener follow" target="_blank" href="/principal-component-analysis-pca-101-using-r-361f4c53a9ff"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">主成分分析(PCA) 101，使用 R</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">一次提高一个维度的可预测性和分类能力！使用 2D 图“可视化”30 个维度！</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">towardsdatascience.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu ks mg"/></div></div></a></div><p id="4ee5" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">好了，我们有了包含<strong class="lj iu"> 6 个组件</strong>的 PCA，让我们创建一个由这些组件和我们的<strong class="lj iu">响应</strong>组成的<strong class="lj iu">新数据集</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="47c8" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">wdbc.pcst </strong>&lt;- <strong class="ok iu">wdbc.pr$x[,1:6]</strong><br/><strong class="ok iu">wdbc.pcst</strong> &lt;- cbind(<strong class="ok iu">wdbc.pcst</strong>, as.numeric(<strong class="ok iu">wdbc$diagnosis</strong>)-1)<br/>colnames(wdbc.pcst)[7] &lt;- "diagnosis"</span></pre><p id="f517" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将使用<strong class="lj iu">完全相同的</strong>方法来制作<strong class="lj iu">训练/测试分割</strong>，所以让我们跳到<strong class="lj iu"> LDA </strong>和<strong class="lj iu">预测</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="572d" class="oo nm it ok b gy op oq l or os"><strong class="ok iu">wdbc.lda</strong> &lt;- <strong class="ok iu">lda</strong>(diagnosis ~ PC1 + PC2 + PC3 + PC4 + PC5 + PC6, data = <strong class="ok iu">train.df</strong>)</span><span id="38ca" class="oo nm it ok b gy ot oq l or os"><strong class="ok iu">wdbc.lda.predict</strong> &lt;- predict(<strong class="ok iu">wdbc.lda</strong>, newdata = <strong class="ok iu">test.df</strong>)</span></pre><p id="0c5e" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，我们可以像以前一样简单地创建我们的<strong class="lj iu"> ROC 图</strong>,看看我们会得到什么样的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/86778e53f6de353d5e905a6e253bd6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zw4mMc8fDQrzOKZne9tjpg.png"/></div></div></figure><p id="7de0" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">很快，我们得到了一些更好的结果，但这可能仍然是纯粹的运气。</p><p id="5c65" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们必须运行一些模拟并比较这两者！</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="90ba" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">比较</h1><p id="2905" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">现在看你的“<em class="oi">运气</em>”你可能会看到<strong class="lj iu"> PCA 转换的 LDA </strong>在<strong class="lj iu"> AUC </strong>方面比<strong class="lj iu">原始 LDA 的<strong class="lj iu"> <em class="oi">稍好</em> </strong>。然而，这可能只是随机发生的..那么让我们借助于<strong class="lj iu"> PCA 变换 LDA </strong>和<strong class="lj iu">原始 LDA </strong>的<strong class="lj iu"> 100.000 模拟</strong>来做一个<em class="oi">快速</em><strong class="lj iu">T-检验</strong>:</strong></p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="de14" class="oo nm it ok b gy op oq l or os">t.test(as.numeric(<strong class="ok iu">AUC_raw</strong>),as.numeric(<strong class="ok iu">AUC_pca</strong>))</span></pre><p id="db91" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj iu"> <em class="oi"> AUC_raw </em> </strong>和<strong class="lj iu"> <em class="oi"> AUC_pca </em> </strong>是简单的数组，带有我运行的每次迭代的结果<strong class="lj iu"> AUC </strong>分数。</p><p id="ee58" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我不会用模拟部分来烦你，因为这是一大块丑陋的代码，所以请相信我！同时查看下面测试结果中的<strong class="lj iu">测向计数</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/148b87cbc75a68e29db4d8f1087c9aaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*iZMAgwIF-_0N6vqsAwlmXg.png"/></div></figure><p id="afac" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">非常低的 p 值，这意味着两者之间存在统计学差异！因此，尽管他们的平均值仅相差<strong class="lj iu"> 0.000137 </strong>到<strong class="lj iu">100.000</strong>T56】尾这是一个<strong class="lj iu">统计上显著的差异</strong>。换句话说:</p><blockquote class="mv"><p id="25f1" class="mw mx it bd my mz na nb nc nd ne mc dk translated"><strong class="ak"> <em class="kz">“在构建你的 LDA 模型之前，用 PCA 进行降维将会得到(稍微)更好的结果！”</em> </strong></p></blockquote><p id="f16a" class="pw-post-body-paragraph lh li it lj b lk nf ju lm ln ng jx lp lq nh ls lt lu ni lw lx ly nj ma mb mc im bi translated">因为每篇文章都需要一个花哨的情节:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/703659dbb417347a54c91c9bbfd10a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsH5JwDrxpdDtIgMtvV1bA.png"/></div></div></figure></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><h1 id="a510" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">结束语</h1><p id="774e" class="pw-post-body-paragraph lh li it lj b lk od ju lm ln oe jx lp lq of ls lt lu og lw lx ly oh ma mb mc im bi translated">如果你想多看多学，一定要<a class="ae nk" href="https://medium.com/@peter.nistrup" rel="noopener"> <strong class="lj iu">跟着我上</strong> </a>🔍<strong class="lj iu"/><a class="ae nk" href="https://twitter.com/peternistrup" rel="noopener ugc nofollow" target="_blank"><strong class="lj iu">推特</strong> </a> <strong class="lj iu"> </strong>🐦</p><div class="md me gp gr mf mg"><a href="https://medium.com/@peter.nistrup" rel="noopener follow" target="_blank"><div class="mh ab fo"><div class="mi ab mj cl cj mk"><h2 class="bd iu gy z fp ml fr fs mm fu fw is bi translated">彼得·尼斯特鲁普-中等</h2><div class="mn l"><h3 class="bd b gy z fp ml fr fs mm fu fw dk translated">阅读彼得·尼斯特拉普在媒介上的作品。数据科学、统计和人工智能...推特:@PeterNistrup，LinkedIn…</h3></div><div class="mo l"><p class="bd b dl z fp ml fr fs mm fu fw dk translated">medium.com</p></div></div><div class="mp l"><div class="pk l mr ms mt mp mu ks mg"/></div></div></a></div></div></div>    
</body>
</html>