<html>
<head>
<title>NLP 101: Word2Vec — Skip-gram and CBOW</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 101: Word2Vec — Skip-gram 和 CBOW</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314?source=collection_archive---------0-----------------------#2019-11-24">https://towardsdatascience.com/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314?source=collection_archive---------0-----------------------#2019-11-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d9aa" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">单词嵌入速成班。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1478334069837b89b163289de065b1a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D9Wj3dHI4VSobKLB"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@sincerelymedia?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sincerely Media</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="fa31" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">单词嵌入是什么意思？</h1><p id="5d95" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">单词嵌入只是单词数字表示的一种花哨说法。一个很好的类比是我们如何使用颜色的 RGB 表示。</p><h1 id="60a8" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么我们需要单词嵌入？</h1><p id="3942" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">作为一个人，从直觉上来说，想要用数字来表示单词或宇宙中的任何其他对象没有多大意义，因为数字是用来量化的，为什么需要量化单词呢？</p><p id="816d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在科学课上，我们说我的车的速度是 45 公里/小时，我们得到了我们开得有多快/多慢的感觉。如果我们说我的朋友以 60 公里/小时的速度开车，我们可以比较一下我们谁开得更快。此外，我们可以计算在某个时间点我们将在哪里，当我们知道我们旅程的距离时，我们将到达我们的目的地等等。类似地，在科学之外，我们用数字来量化质量，当我们报出一件物品的价格时，我们试图量化它的价值，一件衣服的尺寸，我们试图量化它最适合的身体比例。</p><p id="2773" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">所有这些表述都是有意义的，因为通过使用数字，我们可以更容易地根据这些品质进行分析和比较。鞋子和钱包哪个更值钱？尽管这两种物品不同，但有一种方法可以回答这个问题，那就是比较它们的价格。除了量化方面之外，这种表示没有任何其他可获得的东西。</p><p id="74f7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">既然我们知道对象的数字表示通过量化某种质量来帮助分析，问题是我们想要量化什么质量的词？</p><p id="dfa5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">答案是，我们要量化<strong class="lq ir"> <em class="mp">语义</em> </strong>。我们希望用一种人类能够理解的方式来表达单词。不是这个词的确切意思，而是上下文的意思。例如，当我说单词<em class="mp"> see，</em>时，我们确切地知道我在说什么动作——上下文——即使我们可能无法引用它的意思，那种我们可以在字典中找到的，我们头顶的意思。</p><h1 id="58d1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">什么是好质量的单词嵌入，如何生成？</h1><p id="969f" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">最简单的单词嵌入是使用一键向量。如果你的词汇表中有 10，000 个单词，那么你可以将每个单词表示为一个 1x10，000 的向量。</p><p id="bc5f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">举个简单的例子，如果我们的词汇表中有 4 个单词— <em class="mp">芒果、草莓、城市、德里— </em>，那么我们可以将它们表示如下:</p><ul class=""><li id="b65a" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">芒果[1，0，0，0]</li><li id="f595" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">草莓[0，1，0，0]</li><li id="8e3b" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">城市[0，0，1，0]</li><li id="e9f0" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">德里[0，0，0，1]</li></ul><p id="88f6" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">上面的方法有一些问题，首先，我们的向量的大小取决于我们的词汇的大小(这可能是巨大的)。这是一种空间浪费，并且以指数方式增加了算法的复杂性，导致了维度的诅咒。</p><p id="84ec" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">其次，这些嵌入将与它们的应用紧密耦合，使得迁移学习到使用相同大小的不同词汇的模型，从词汇中添加/删除单词几乎是不可能的，因为这将需要再次重新训练整个模型。</p><p id="4feb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">最后，创建嵌入的全部目的是捕捉单词的上下文含义，这是这种表示法所不能做到的。意思或用法相似的词之间没有关联。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="7bf9" class="nj kx iq nf b gy nk nl l nm nn"><strong class="nf ir">## Current situation</strong> <br/>Similarity(Mango, Strawberry) == Similarity(Mango, City) == 0</span><span id="41db" class="nj kx iq nf b gy no nl l nm nn"><strong class="nf ir">## Ideal situation</strong><br/>Similarity(Mango, Strawberry) &gt;&gt; Similarity(Mango, City)</span><span id="a304" class="nj kx iq nf b gy no nl l nm nn">** Note: Similarity(a,b) = <em class="mp">a.b/(||a||*||b||</em>) Cosine similarity</span></pre><h2 id="fc95" class="nj kx iq bd ky np nq dn lc nr ns dp lg lx nt nu li mb nv nw lk mf nx ny lm nz bi translated">连续词袋模型(CBOW)和跳格</h2><p id="8086" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">两者都是通过使用神经网络来学习每个单词的底层单词表示的架构。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/1459be73736a3940816fd67f263fd8e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cuOmGT7NevP9oJFJfVpRKA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Source: <a class="ae kv" href="https://arxiv.org/pdf/1309.4168v1.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ob">Exploiting Similarities among Languages for Machine Translation</em></a> paper.</figcaption></figure><p id="4e1e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在<strong class="lq ir"> CBOW </strong>模型中，上下文(或周围单词)的分布式表示被组合起来<strong class="lq ir">预测中间的单词</strong>。而在<strong class="lq ir">跳格</strong>模型中，输入单词的分布式表示用于<strong class="lq ir">预测上下文</strong>。</p><p id="32d1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">任何神经网络或任何监督训练技术的先决条件是具有标记的训练数据。你如何训练一个神经网络来预测单词嵌入，当你没有任何标记数据，即单词和它们相应的单词嵌入？</p><h2 id="c96c" class="nj kx iq bd ky np nq dn lc nr ns dp lg lx nt nu li mb nv nw lk mf nx ny lm nz bi translated">跳格模型</h2><p id="dfcf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将通过为神经网络创建一个“假”任务来进行训练。我们不会对这个网络的输入和输出感兴趣，相反，我们的目标实际上只是学习隐藏层的权重，这实际上是我们试图学习的“单词向量”。</p><p id="3b92" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">Skip-gram 模型的假任务是，给定一个单词，我们将尝试预测它的相邻单词。我们将通过窗口大小——一个超参数——来定义一个相邻单词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/223c3d8cc50fd73a765f4dfa0d0c1dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkxbwD55_8M3XBRb1bGm7A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The word highlighted in yellow is the source word and the words highlighted in green are its neighboring words.</figcaption></figure><p id="fd60" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">给定句子:<br/> <em class="mp">“我早餐吃橘子</em> <strong class="lq ir"> <em class="mp">果汁</em> </strong> <em class="mp">和鸡蛋。”</em> <br/>且窗口大小为 2，如果目标词是<em class="mp">果汁，</em>其邻词将是<em class="mp">(有、橙、和、蛋)。</em>我们的输入和目标词对将是(果汁，有)，(果汁，橙)，(果汁，和)，(果汁，鸡蛋)。<br/>还要注意，在样本窗口中，单词与源单词的接近度不起作用。所以<em class="mp">有，橙，和，</em>和<em class="mp">蛋</em>在训练时会被同等对待。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/c210dbe7d2a1153bfa6828b4c152d0b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-sJwjMYZ3-Fpr2Zuuz5kmw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Architecture for skip-gram model. Source: <a class="ae kv" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank">McCormickml tutorial</a></figcaption></figure><p id="d57d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">输入向量的维数将是 1xV——其中 V 是词汇表中的<em class="mp">个单词— </em>,即单词的一键表示。单个隐藏层将具有维度 VxE，其中 E 是单词嵌入的大小，并且是超参数。隐藏层的输出将是 1xE 的尺寸，我们将把它输入到一个<a class="ae kv" href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/" rel="noopener ugc nofollow" target="_blank"> softmax 层</a>。输出层的维度将是 1xV，其中向量中的每个值将是目标单词在该位置的概率得分。<br/>根据我们之前的例子，如果我们有一个向量[0.2，0.1，0.3，0.4]，这个词成为<em class="mp">芒果</em>的概率是 0.2，<em class="mp">草莓</em>是 0.1，<em class="mp">城市</em>是 0.3，<em class="mp">德里</em>是 0.4。</p><p id="1d7f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">对应于源单词的训练样本的反向传播在一次反向传递中完成。所以对于<em class="mp">果汁，</em>我们将完成所有 4 个目标词<em class="mp"> ( have，orange，and，eggs)的正向传递。</em>然后我们将计算对应于每个目标单词的误差向量【1xV dimension】。我们现在将有 4 个 1xV 误差向量，并将执行逐元素求和以获得 1xV 向量。隐藏层的权重将基于这个累积的 1xV 误差向量进行更新。</p><h2 id="1372" class="nj kx iq bd ky np nq dn lc nr ns dp lg lx nt nu li mb nv nw lk mf nx ny lm nz bi translated">CBOW</h2><p id="6f91" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">CBOW 中的假任务有点类似于 Skip-gram，在某种意义上，我们仍然采用一对单词，并教导模型它们共同出现，但我们不是添加错误，而是添加相同目标单词的输入单词。</p><p id="e229" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我们的隐藏层和输出层的尺寸将保持不变。只有我们的输入层的维度和隐藏层激活的计算将改变，如果我们对于单个目标单词有 4 个上下文单词，我们将有 4 个 1xV 输入向量。每个都将乘以 VxE 隐藏层，返回 1xE 向量。将对所有 4 个 1xE 向量进行元素平均，以获得最终的激活，然后将该激活馈入 softmax 层。</p><p id="52d1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> Skip-gram </strong>:适用于少量的训练数据，甚至可以很好地表示罕见的单词或短语。<br/> <strong class="lq ir"> CBOW </strong>:训练速度比 skip-gram 快几倍，对常用词的准确率略高。</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="f41d" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在本帖第二部分<a class="ae kv" rel="noopener" target="_blank" href="/nlp-101-negative-sampling-and-glove-936c88f3bc68">NLP 101:阴性采样和手套</a>中，我们讨论了:</p><ul class=""><li id="3b0f" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated"><strong class="lq ir">负采样</strong> —一种在不影响嵌入质量的情况下改善学习的技术</li><li id="e296" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated">另一个单词嵌入叫做<strong class="lq ir"> GloVe </strong>，它是基于计数和基于窗口的混合模型。</li></ul><h1 id="d2c4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><ul class=""><li id="ac00" class="mq mr iq lq b lr ls lu lv lx ol mb om mf on mj mv mw mx my bi translated"><a class="ae kv" href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf" rel="noopener ugc nofollow" target="_blank">讲稿 cs 224d:NLP 第一部分的深度学习</a></li><li id="62a6" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><a class="ae kv" href="https://cs224d.stanford.edu/lecture_notes/LectureNotes2.pdf" rel="noopener ugc nofollow" target="_blank">讲稿 cs 224d:NLP 第二部分的深度学习</a></li><li id="5727" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><a class="ae kv" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank">麦考密克，C. (2016 年 4 月 19 日)。<em class="mp"> Word2Vec 教程——跳格模型</em>。</a></li></ul><h2 id="5915" class="nj kx iq bd ky np nq dn lc nr ns dp lg lx nt nu li mb nv nw lk mf nx ny lm nz bi translated">我认为你会喜欢:D 的其他文章</h2><ul class=""><li id="a1e6" class="mq mr iq lq b lr ls lu lv lx ol mb om mf on mj mv mw mx my bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/back-propagation-721bfcc94e34">是的，你应该听听 Andrej Karpathy，并了解反向传播</a></li><li id="b947" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5">NLP 模型评估—最新基准</a></li><li id="2f59" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/attaining-attention-in-deep-learning-a712f93bdb1e">理解深度学习中的注意力</a></li><li id="8e66" class="mq mr iq lq b lr mz lu na lx nb mb nc mf nd mj mv mw mx my bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/transformers-89034557de14">变形金刚</a>——谷歌的伯特<a class="ae kv" href="https://medium.com/@ria.kulshrestha16/keeping-up-with-the-berts-5b7beb92766" rel="noopener">和 OpenAI 的 GPT </a>等模型的基本模块。</li></ul></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><blockquote class="oo op oq"><p id="1efb" class="lo lp mp lq b lr mk jr lt lu ml ju lw or mm lz ma os mn md me ot mo mh mi mj ij bi translated">我很高兴你坚持到了这篇文章的结尾。<em class="iq">🎉我希望你的阅读体验和我写这篇文章时一样丰富。<em class="iq">💖</em></em></p><p id="69a2" class="lo lp mp lq b lr mk jr lt lu ml ju lw or mm lz ma os mn md me ot mo mh mi mj ij bi translated">请点击这里查看我的其他文章<a class="ae kv" href="https://medium.com/@ria.kulshrestha16" rel="noopener">。</a></p><p id="1512" class="lo lp mp lq b lr mk jr lt lu ml ju lw or mm lz ma os mn md me ot mo mh mi mj ij bi translated">如果你想联系我，我会选择推特。</p></blockquote></div></div>    
</body>
</html>