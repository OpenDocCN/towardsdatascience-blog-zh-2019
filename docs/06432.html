<html>
<head>
<title>Introduction to Natural Language Processing for Noobs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向 Noobs 的自然语言处理介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-natural-language-processing-for-noobs-8f47d0a27fcc?source=collection_archive---------14-----------------------#2019-09-15">https://towardsdatascience.com/introduction-to-natural-language-processing-for-noobs-8f47d0a27fcc?source=collection_archive---------14-----------------------#2019-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/df7c6bdcf2d6febe1bc422bea710f98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n00rMuZbGjQvxZbCbLYgZA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@fabioha?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">fabio</a> on <a class="ae jd" href="https://unsplash.com/search/photos/ai?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><div class=""><h2 id="fe31" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">使用 Kaggle 竞争的 NLP 及其基本流水线概述</h2></div><p id="ce47" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇文章是我试图给出一个可能对新手有帮助的基本概念的概述。为了更好地理解，我将关注一个完整的 Kaggle 竞赛<a class="ae jd" href="https://www.kaggle.com/c/quora-insincere-questions-classification/overview" rel="noopener ugc nofollow" target="_blank"> Quora 虚假问题分类</a>。在这里，我们提供了 131 万个带标签的问题和 37.6 万个问题，我们必须预测这些问题的标签。这个竞赛是一个二元分类问题的例子。我已经包含了可以用来解决这个问题的 python 代码示例。代码使用了<em class="lr"> Keras </em>库，非常容易理解。这场竞赛的基线解决方案是在 Kaggle <a class="ae jd" href="https://www.kaggle.com/vksbhandary/introduction-to-nlp-for-noobs" rel="noopener ugc nofollow" target="_blank">内核</a>上。</p><p id="e964" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以先从基础开始吧。</p><h1 id="0056" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">介绍</h1><p id="c687" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">NLP 是计算机科学的一个分支，它用自然语言处理人和机器之间的交互。它是语言学和计算机科学的交叉，因此它使机器能够理解并以自然语言回复人类的查询。自然语言处理的主要问题是人类语言是不明确的。人类非常聪明，能够理解上下文和单词的意思，但对计算机来说，这个问题完全是另一个层面，因为计算机既不理解概念也不理解上下文。为了让计算机理解概念，它们需要对世界、语言句法和语义有一个基本的理解。</p><h1 id="da65" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">文本表示</h1><p id="778e" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们可以理解和阅读一段文本，但对计算机来说，每段文本都是一系列数字，不代表任何概念。一个简单的例子，字母“A”在英语中有特殊的含义，它被认为是所有字母表中的第一个字母。但是计算机认为它是 65(因为 65 是字母“A”的 ASCII 码)。ASCII 是基于英文字符的传统编码系统。这些字符的集合在 NLP 中通常被称为<em class="lr">标记</em>。</p><blockquote class="mp"><p id="47a8" class="mq mr jg bd ms mt mu mv mw mx my lq dk translated">一个<a class="ae jd" href="https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html" rel="noopener ugc nofollow" target="_blank">标记</a>是某个特定文本中的一个字符序列的实例，这些字符被组合在一起以在自然语言中产生某种意义。</p></blockquote><p id="5cd4" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">在 NLP 管道中表示任何文本的最简单方法是通过一键编码的向量表示法。如果一个句子包含某个单词，那么向量中相应的条目表示为“1”，否则为“0”。例如，让我们考虑下面两句话:</p><ol class=""><li id="8563" class="ne nf jg kx b ky kz lb lc le ng li nh lm ni lq nj nk nl nm bi translated">“自然语言处理是最好的领域”</li><li id="1dbe" class="ne nf jg kx b ky nn lb no le np li nq lm nr lq nj nk nl nm bi translated">“我是自然语言处理的新手”</li></ol><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/8578bb683e3fd2a122c51b66c5249672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yhYuK-VPqehSjwpF7tUS3A.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Table 1</strong>. One hot encoding of all words in sentence 1 and 2</figcaption></figure><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/4396057eff3ad72416eb25a561e64d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*epdExF9CbyJlreexLnQ6aA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Table 2</strong>. Collapsed One hot encoded vector for sentence 1 and 2</figcaption></figure><p id="54fc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一种流行的编码是将分类变量表示为二进制向量。一种单词的热编码用于对文本进行编码，其中除了当前单词之外，每个单词都用零表示。如果我们有一千个唯一单词的语料库，那么每个单词表示将需要大小为 1000 的向量。尽管大小依赖于词汇，但即使在今天，一键编码表示仍在产品中使用。</p><p id="d6d5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">表 1 显示了句子 1 和 2 中所有单词的一次性编码。表 2 所示的表示称为折叠或“二进制”表示。</p><p id="54be" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现代 NLP 系统有一个单词字典，其中每个单词都有一个整数表示。所以那句“自然语言处理是最好的领域！”可以表示为“2 13 6 34 12 22 90”(忽略标点符号，将每个单词都视为小写)。这种表示在内存使用方面更有效，并且还保留了语言语义。</p><h1 id="c6d8" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">基本 NLP 流水线</h1><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nz"><img src="../Images/ca8e5f000820f86baf4e1d32c0a5f1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xl1fqK2iLtZZEQvcX6HthA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Figure 1</strong>: Basic NLP pipeline</figcaption></figure><p id="80fd" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">NLP 中有很多任务可以使用上面显示的管道。大多数 Kaggle 比赛都清理了数据，这在现实生活中不会发生，因为这些数据集是由比赛协调员收集和预处理的。在现实世界的场景中，<strong class="kx jh">我们必须总是假设数据需要一些预处理</strong>。预处理之后，我们需要将文本分解成<em class="lr">记号</em>和<em class="lr">句子</em>。在分解文本之后，我们使用预先训练的嵌入来初始化我们的模型。</p><p id="a431" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了这篇文章的简单，我从我们的管道中跳过了文本分析。文本分析是任何 NLP 管道中非常重要的一部分。根据分析的见解，可以修改管道流程以提高应用程序的性能。</p><h1 id="5868" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated"><strong class="ak">文本预处理</strong></h1><p id="10a3" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">文本数据可能非常混乱。所以大多数时候我们需要对文本进行预处理。预处理可以包括去除最常见的拼写错误、替换文本中的数字、替换俚语词、消除常见的语法错误、匿名化数据、去除停用词等。</p><p id="15c8" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">代码示例:</strong></p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/2acf4ed1a97e1e83ee6c5f6381f44fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwPSS9iCevdWuLbV0otDGw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 1</strong>: Replacing empty records</figcaption></figure><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/b6b482f435f4378700c28e9ff02091a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*526t4rV5L0mtcNzl1_f2Qg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 2</strong>: Declaring function to clear text</figcaption></figure><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/1418c1558e38d5a77f461e5d6f86a20d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbZuPoGVa5IRgmkvyGn6Tw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 3</strong>: Cleaning question text</figcaption></figure><p id="48de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的代码样本中<em class="lr"> train_df、</em> <em class="lr"> test_df </em>是熊猫 dataframe。首先，我们使用<a class="ae jd" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html" rel="noopener ugc nofollow" target="_blank"><em class="lr">fillna()</em></a><em class="lr"/>函数删除数据集中的空记录。然后我们声明<em class="lr"> clean_text </em>函数来分离令牌。在代码示例 3 中，我们通过使用 pandas dataframe 的<em class="lr"> apply() </em>函数，调用<em class="lr"> clean_text </em>函数将其应用于我们的训练和测试数据集。</p><h1 id="7fd7" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated"><strong class="ak">文本规范化</strong></h1><blockquote class="mp"><p id="ea98" class="mq mr jg bd ms mt mu mv mw mx my lq dk translated">文本规范化是将文本转换成单一规范形式的过程。</p></blockquote><p id="804b" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">有许多文本规范化技术，如标记化、词条化、词干化、句子分割、拼写纠正等。其中，标记化是最常用的文本规范化方法。</p><blockquote class="mp"><p id="0846" class="mq mr jg bd ms mt mu mv mw mx my lq dk translated">标记化是将一段文本转换成在自然语言中有意义的一系列单词或特殊字符的过程。</p></blockquote><p id="40ea" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">例如文本“NLP 是最好的！”可以转换成列表“NLP”、“is”、“the”、“best”和“！”(注意特殊字符"！"与“最佳”分开，因为它在英语中有特殊的含义)。像中文这样的一些语言没有用空格分隔的单词，所以这些语言的标记化就更加困难。</p><p id="bd40" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">词汇化是确定两个单词是否有相同词根的任务。例如，单词“got”和“gone”是动词“go”的形式。词干化类似于词汇化，但它只是从单词的末尾剥离。使用词汇化或词干化意味着从数据中丢弃一些信息。拼写校正可以在你的自然语言处理系统中使用，以消除输入错误，提高自然语言处理系统的性能。然而，NLP 系统被假定为对由于未知令牌引起的输入的小变化是鲁棒的。</p><p id="4707" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">代码示例:</strong></p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/8c152aa697f124289fb9c266627f9e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dj4yma7chHlNwaMouQIuag.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 4</strong>: Declaring and fitting tokenizer on training data</figcaption></figure><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/80d71fc84e1313f20db269d1286e807a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9-gX2Mmy7RtJXyDoLVxdA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 5</strong>: Using trained tokenizer to tokenize all questions</figcaption></figure><p id="f1f5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上面的代码示例中，我们通过使用一个<em class="lr">标记器</em>对象的<em class="lr"> fit_on_texts() </em>函数来训练一个标记器。该对象可用于将文本转换为输入数据的整数表示形式。之后，我们可以通过使用<em class="lr"> Keras </em>包中的<em class="lr"> pad_sequences() </em>函数来限制输入序列的大小。</p><h1 id="3c3a" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">单词嵌入</h1><p id="712e" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">单词嵌入是文档词汇在向量空间中的表示。嵌入向量的大小范围从 50 到 600，可以捕获语法和语义信息。如果有足够的计算资源可用，那么甚至可以使用更大的向量大小。单词嵌入的性能可能因为所使用的不同算法或嵌入向量的大小而不同。但是在某些时候，增加嵌入大小并不能提高性能。</p><p id="156b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">语言模型可以代替自然语言处理系统中的单词嵌入。LM 不同于单词嵌入，因为它们可以在语料库上被训练和微调。单词嵌入被视为单个层，不能进一步调整。统计语言模型是单词序列的概率分布。一个这样的例子是 N-gram 模型。N-gram 模型计算单词“w”出现在单词“h”之后的概率。</p><p id="bde6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">代码示例:</strong></p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/a89467fef52133e386c4657fdfe95a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FVAY1PbIzhVENbr9_5-VIA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 6</strong>: Reading embedding file format</figcaption></figure><p id="1a0d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了在代码中使用单词嵌入，我们读取并处理嵌入文件。在每一行中，嵌入文件包含一个单词及其嵌入向量。在代码示例 6 中，我们拆分了这一行，并创建了一个字典来存储每个单词的嵌入向量。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/43c2e6aad0168d3f57b3561bc6cd1e76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3x5gmw1hdcBcrYWfy-gCA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 7</strong>: Extracting embedding vectors and calculating mean and standard deviation</figcaption></figure><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/ccb1d6d7c31abdeddbf4079091ce9e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D9b0NHz1MU7NlBiMxsMPOA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 8</strong>: Recreating embedding matrics</figcaption></figure><p id="88fc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在读取嵌入文件后，我们可以创建一个使用单词和嵌入矩阵的列表。注意，代码示例 8 使用<em class="lr">标记器</em>对象来访问词汇单词。这里，我们使用正态分布重新创建嵌入矩阵。这有助于初始化字典<em class="lr"> embeddings_index 中不存在的单词向量。</em></p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><div class="nt nu nv nw gt oj"><a rel="noopener follow" target="_blank" href="/introduction-to-word-embedding-and-word2vec-652d0c2060fa"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jh gy z fp oo fr fs op fu fw jf bi translated">单词嵌入和 Word2Vec 简介</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">单词嵌入是最流行的文档词汇表示之一。它能够捕捉…的上下文</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ix oj"/></div></div></a></div><p id="1d30" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要详细了解单词嵌入和 word2vec，请阅读上面的文章。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><h1 id="f5ee" class="ls lt jg bd lu lv oy lx ly lz oz mb mc km pa kn me kp pb kq mg ks pc kt mi mj bi translated"><strong class="ak">建筑模型</strong></h1><p id="da11" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">机器学习管道中的模型是一系列数学运算，可以学习估计未知数据的输出。选择您的型号非常重要，因为它可以决定您系统的性能。</p><p id="7894" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">新手的话可以从非常基础的机型入手。但是您应该至少用几个模型进行试验，看看哪个参数设置能得到最好的结果。一旦你有了六个或更多运行良好的模型，你就可以把它们整合成一个巨大的模型。</p><p id="1f75" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最初，您可以选择使用双 LSTM 或 GRU 层和平均池层进行测试。实验之后，您可以通过使模型更深入或使用更先进的模型来对模型进行更改。为了跟踪 NLP 的进展，你可以访问这个<a class="ae jd" href="http://nlpprogress.com" rel="noopener ugc nofollow" target="_blank">站点</a>，看看哪个模型在特定的任务中表现得更好。</p><p id="40e9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下图给出了我们为第一个测试选择的模型的概念表示。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pd"><img src="../Images/f94fe5644de511dcb98967f2a0f6b55b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*effyvxpOullWDnPtUZgh_Q.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Figure 2</strong>: Conceptual representation of our model</figcaption></figure><p id="8cc4" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上图中，您可以看到我们模型的概念性布局。模型中的层代表特定的数学运算。一个模型至少可以有两层，输入层和输出层。Keras 中的<strong class="kx jh">输入层</strong>用于实例化一个张量。模型中的<strong class="kx jh">输出层</strong>是具有多个节点的神经网络。</p><p id="9ff5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">模型中的嵌入层</strong>对输入序列和嵌入矩阵进行点积，将每个词索引转换成对应的嵌入向量。</p><p id="8a28" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">丢弃层</strong>以给定的百分比随机丢弃输入。这一层将输入单位转换为零。这个过程有助于防止过度拟合。</p><p id="82ab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">门控递归单元是一种不存在消失梯度问题的递归层。<strong class="kx jh">双向 GRU </strong>的输出是前向和后向 GRU 的组合</p><p id="7eba" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">汇集操作通过执行数学平均、最大值等来减小输入序列的大小。因此，<strong class="kx jh">平均池层</strong>执行平均池，同时<strong class="kx jh">最大池层</strong>执行最大池。<strong class="kx jh">级联层</strong>组合不同的输入序列。</p><p id="8d23" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">密集层</strong>是一个简单的神经网络，具有固定的节点数和指定的激活函数。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="6233" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了完全理解递归神经网络和 GRU 网络的工作原理，你可以阅读下面的文章。</p><div class="ip iq gp gr ir oj"><a rel="noopener follow" target="_blank" href="/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jh gy z fp oo fr fs op fu fw jf bi translated">递归神经网络图解指南</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">理解直觉</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="pe l ou ov ow os ox ix oj"/></div></div></a></div><div class="ip iq gp gr ir oj"><a rel="noopener follow" target="_blank" href="/understanding-gru-networks-2ef37df6c9be"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jh gy z fp oo fr fs op fu fw jf bi translated">了解 GRU 网络</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">在这篇文章中，我将试图给出一个相当简单和易懂的解释，一个真正迷人的类型…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="pf l ou ov ow os ox ix oj"/></div></div></a></div></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="e776" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">代码示例:</strong></p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/9fdc1507e02792158aedb9f7c5280731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOPTBDNOohEIZnUA4CBYCA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 9</strong>: Model definition code</figcaption></figure><p id="286e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在代码示例 9 中，输入层接受大小等于<em class="lr"> ques_len 的输入。</em>嵌入层以嵌入矩阵及其维数为参数。嵌入层的输出大小为<em class="lr">ques _ len</em>x<em class="lr">embedding _ matrix . shape[1]</em>。<em class="lr">空间丢弃</em>层随机丢弃<em class="lr">嵌入</em>层的输出，分数为 0.2。</p><p id="ba0a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">接下来的两层是双向 GRU，这是一种递归神经网络。<em class="lr">全局平均池 1D </em>和<em class="lr">全局最大池 1D </em>从第二双向 GRU 层的输出中提取平均和最大特征。输出<em class="lr"> avg_pool、</em>和<em class="lr"> max_pool </em>被连接以馈入密集层。最后一层是输出层，给出一个介于 0 和 1 之间的数字。</p><h1 id="71b8" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">初始化模型</h1><blockquote class="mp"><p id="d708" class="mq mr jg bd ms mt mu mv mw mx my lq dk translated">迁移学习是通过从已经学习过的相关任务中迁移知识来提高在新任务中的学习。</p></blockquote><p id="eeba" class="pw-post-body-paragraph kv kw jg kx b ky mz kh la lb na kk ld le nb lg lh li nc lk ll lm nd lo lp lq ij bi translated">在这一阶段，我们试图在我们的自然语言处理系统中利用以前的知识。使用在不同设置中训练的独立模型可以提高我们的模型的性能。这个过程被称为知识转移。在 NLP 系统中初始化神经网络的最流行的方法是单词嵌入。</p><p id="fdc9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">NLP 的最新发展清楚地表明迁移学习是前进的方向。使用预先训练的 LMs，新的最先进的模型变得越来越好。LMs 是在一个巨大的数据集上预先训练的，它们也基于 Transformer 架构。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="c6ef" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想了解更多关于变压器模型的信息，请阅读下面的文章。</p><div class="ip iq gp gr ir oj"><a href="https://arxiv.org/abs/1706.03762" rel="noopener  ugc nofollow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jh gy z fp oo fr fs op fu fw jf bi translated">你需要的只是关注</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">主导序列转导模型是基于复杂的递归或卷积神经网络在一个…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">arxiv.org</p></div></div></div></a></div></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="0feb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在编码样本 9 中，我们使用<em class="lr">嵌入 _ 层</em>初始化我们的模型，它被初始化为<em class="lr">嵌入 _ 矩阵</em>的权重。</p><h1 id="4d3d" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated"><strong class="ak">培训模式</strong></h1><p id="13d4" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在您决定第一次使用哪个模型后，您可以训练您的模型。您应该总是在十分之一的训练数据集上开始测试您的模型，因为这会使测试更快。有些问题是，不同的预处理方法可能会产生不同的性能。</p><figure class="nt nu nv nw gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/f16bdf6eb899e2ea0c7a2f8007dd3788.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3c_Wl4rfwTBcSgjc42mvyg.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><strong class="bd nx">Code sample 10</strong>: Training our model using model.fit() method</figcaption></figure><p id="8b7f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在代码示例 10 中，我们添加了每次保存模型的功能，如果它的验证准确性增加的话。最后，我们调用函数<em class="lr"> fit()，</em>来使用<em class="lr"> train_X，train_y </em>(我们的特征和标签)和一系列其他参数训练模型。如果我们想以更快的速度进行训练，我们可以随时增加批量。<em class="lr">历元的数量</em>表示相同的训练数据将被输入模型的次数。</p><h1 id="06ea" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">测试和评估模型</h1><p id="bd14" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在训练模型之后，我们必须测试我们的模型的准确性，并评估它在看不见的数据上的表现。为此，我们预测结果并与实际标签进行比较。如果模型的性能不符合我们的预期，我们需要在我们的管道中做出改变。在 Kaggle 比赛中，排行榜分数是评估我们模型的一个很好的方式。</p></div></div>    
</body>
</html>