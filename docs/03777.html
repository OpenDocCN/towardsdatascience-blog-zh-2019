<html>
<head>
<title>A Learning Engine for Embodied AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于嵌入人工智能的学习引擎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-learning-engine-for-embodied-ai-7ef54f6574f0?source=collection_archive---------26-----------------------#2019-06-14">https://towardsdatascience.com/a-learning-engine-for-embodied-ai-7ef54f6574f0?source=collection_archive---------26-----------------------#2019-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="76b1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">本文介绍了在游戏模拟器中创建自主代理的深度学习框架</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4fc3ee0caea9a0440e7257d62269313e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Fqof-6lkuIZ7rEbFmrGcQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">An Agent Learning a Match to Sample Task using Neurostudio</figcaption></figure><p id="1693" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于能够创建物理和视觉环境的高保真表示，现代游戏引擎已经成为训练多种具体化人工智能的可行工具。术语“具体化人工智能”在这里用于区分机器学习算法的类型。一个被具体化的人工智能包含了一个环境中的视觉表现，不管是真实的还是虚拟的，并且正在优化一些服从于那个角色的东西。例如，垃圾邮件分类器并不代表具体化的人工智能，因为它在其环境中缺乏任何种类的可见身份。然而，用于动画表情的垃圾邮件分类器可以说是表示具体化的 AI，因为优化器现在具有时空内的物理表示。</p><p id="b6da" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">创建这种具体化人工智能的工具可以被认为是一种新型的软件，或者我在这里称之为“学习引擎”。虽然这些例子已经存在，但大多数用于创建具体化代理的深度学习工具仍处于起步阶段，需要大量的专业知识来部署。在这篇文章中，我介绍了一个名为<a class="ae lu" href="https://unrealengine.com/marketplace/en-US/slug/neurostudio-self-learning-ai" rel="noopener ugc nofollow" target="_blank"> Neurostudio </a>的深度学习框架，它利用 Unreal Game Engine 的可视化脚本语言蓝图，通过新手可以学习的几个简单步骤来创建自主学习代理。</p><p id="4447" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">https://www.youtube.com/watch?v=rgQWJ5bkVkk<a class="ae lu" href="https://www.youtube.com/watch?v=rgQWJ5bkVkk" rel="noopener ugc nofollow" target="_blank"/></p><p id="3135" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为什么首先要使用视频游戏引擎来训练具体化的人工智能？在部署强化学习等下一代人工智能技术时，一个持久的障碍是缺乏合适的培训环境。各种各样的研究已经注意到使用基线现实训练具体化代理人的困难。通常，人工智能的物理约束，如机器人的电子执行器，禁止像深度强化学习这样的技术产生良好结果所需的高容量训练计划。</p><p id="d473" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一种替代方法是使用模拟器来训练代理。从这里开始，智能体可以继续存在于模拟器中(就像电子游戏《NPC》的情况一样)，或者它们的学习可以转移到“实体”现实中，就像机器人的情况一样。在这两种情况下，使用模拟器来训练具体化代理都有几个优点。首先，当事情发生灾难性的错误时，其结果通常没有在物理现实中发生灾难性错误时那么严重。一个疯狂奔跑的机器人的模拟和一个 800 磅重的四肢乱飞的怪物是完全不同的。更重要的是，很多时候人们可能希望代理人永远留在模拟器中，就像电子游戏角色的情况一样。</p><p id="0989" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我在这里描述的系统将深度神经网络与强化学习算法相结合，以实现自我学习的具体化代理。使用这个系统，一个智能体可以简单地通过改变 AI 角色控制器中的奖励函数来执行任何涉及其环境中对象的动作组合。这是强化学习的主要优势——不需要手工制作行为，而是指定一个要奖励的行为，让代理自己学习实现奖励的必要步骤。本质上，这就是如何用食物奖励来教狗表演一个魔术。</p><p id="5f6d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样的方法也可以用来训练 NPC、虚拟助理甚至机器人。通过这个系统可以获得各种各样的有意行为，包括路径寻找、NPC 攻击和防御以及许多类型的人类行为。最先进的实现包括那些用于在国际象棋、围棋和多人战略视频游戏中击败一流人类玩家的实现。</p><p id="c2d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://www.youtube.com/watch?v=SmBxMDiOrvs" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=SmBxMDiOrvs</a></p><p id="7a38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">强化学习已经成为训练具体化代理人的一个有前途的途径，因为这样的代理人几乎总是与一些时间和行动的概念有关。强化学习不同于人工智能的其他分支，因为它专门解决学习的问题，其中代理可以采取一些与时间和环境相关的行动。在这种情况下，时间可以是一系列的游戏移动或训练时期。以这样或那样的方式，有一个时间空间，在其中事情正在发生。人工智能不像监督学习技术那样简单地迭代一组冻结的标记数据。</p><p id="b144" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦一个主体存在于时间和变化的环境中，复杂性就成了压倒一切的问题。在诸如表格 Q 学习的强化学习技术中，算法必须跟踪表格中环境变化、动作和奖励的所有组合。根据环境的复杂性和代理可用的操作，这可能会使这样的表变得非常大。即使几个环境因素相互作用也能迅速导致组合爆炸。</p><p id="e778" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人类也使用强化学习的变体来获得许多技能，你可能会忍不住问，在复杂性成为问题之前，我们自己可以跟踪多少环境变量？从视觉上来说，答案少得惊人——根据物体是运动还是静止，在 4 到 8 个之间。根据我们对强化学习和组合爆炸的了解，这是有道理的——超过八个对象，注意力的空间分辨率就会停滞不前。四个动态对象可以以 24 种不同的方式组合，八个对象以 40，320 种方式组合！除此之外，我们进入了从人类的角度来看几乎毫无意义的数字——12 个物体可以以 4.79 亿种独特的方式组合。例如，在玩视频游戏的任何时候，我们通常会从成千上万的像素中抽象出 4 到 5 个不同的对象来跟踪。</p><p id="0ade" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，我们如何从所有这些像素组合中提取出少量有意义的特征呢？在人类中，进化为我们做了所有艰苦的工作，在生物学中这种天赋被称为潜在抑制。在这个过程中，熟悉的物体会失去它们的显著性，我们会随着时间的推移而忽略它们。遗忘的面纱确实是一个仁慈的面纱。虽然根据计算机的处理能力，计算机可以跟踪比人更多的特征，但组合爆炸甚至会加重现代超级计算机的负担。对计算机科学家来说幸运的是，“神经网络”可以帮助解决这种复杂的问题。</p><p id="f74e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">深度神经网络最常与称为“监督学习”的人工智能领域相关联，这需要有人提供一个标记训练数据的数据库，供算法从中学习模式。深度神经网络的惊人之处在于，它们可以像猫的照片一样获取嘈杂、非线性、非均匀的数据，并将其抽象为对其进行分类至关重要的几个特征。这就是你的垃圾邮件分类器如何在你的电子邮件收件箱中工作，以及网飞如何根据你喜欢或不喜欢的电影为你创建推荐。这种分类器在软件中越来越常见，并由于深度神经网络而受到了鼓舞。神经网络比早期的统计方法(如逻辑回归和贝叶斯分类器)更强大，因为它们擅长发现隐藏在多层复杂性下的模式。分类器是强大的工具，但与强化学习不同，它们通常需要某种标记数据来训练。</p><p id="e4a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2015 年，DeepMind 公司的研究人员突发奇想，利用深度神经网络从雅达利视频游戏屏幕的屏幕像素中提取像素，以便将强化学习算法应用于玩游戏。如上所述，深度神经网络有能力获取非常嘈杂的大型数据集，并检测其中的模式。Atari 视频游戏的屏幕可以被认为是这样一个大而嘈杂的数据集。通过使用 Atari 控制台的屏幕作为神经网络的训练集，他们将所有像素组合的复杂性减少到与玩家可能做出的不同动作相关的少数几个特征，通常只有 4 或 5 个(Volodymyr Mnih，2015)。他们仅仅是幸运的没有一个游戏需要更多的动作来获胜吗？一点也不，因为这些游戏是为人类设计的，它们的活动和环境空间有限。为外星人设计的 Atari 视频游戏能够从 10，000 种不同的动作组合中学习，这完全是另一回事。然而，在大多数游戏功能只是装饰，对人们应该采取的行动没有任何意义的情况下，深度神经网络可以将复杂性降低到强化学习算法可以管理的程度。</p><p id="7a59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">雅达利的情况同样适用于围棋这样的棋类游戏。由于游戏固有的复杂性，这种古老的中国娱乐方式被认为是计算机无法掌握的。正如围棋专家喜欢提醒我们的那样，在一场围棋比赛中，可能的棋盘组合比宇宙有史以来存在的夸克数量还要多。但就像在雅达利电子游戏中一样，围棋游戏中的许多棋盘位置与游戏的任何给定回合都不相关。它们就像屏幕远处角落里的像素，在它表明敌人正向你走来之前并不重要。“深度强化学习”，即深度神经网络和强化学习的结合，被证明在掌握围棋方面与在 Attari 视频游戏中一样有效。2017 年，由 DeepMind 开发的围棋深度强化学习算法 AlphaZero 击败了几位世界领先的人类围棋选手以及最佳围棋人工智能。</p><p id="f5e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当想到像围棋这样的游戏时，人们陷入的一个主要谬误是假设复杂的游戏需要复杂类型的学习。在分形几何中，看似无限变化的令人困惑的图案可以从简单的公式中推导出来。进化已经产生了无数极其复杂的生命形式，它受到一个同样简单的学习规则——错误——的指导。同样的学习方程式可以让你掌握井字游戏，也可以让你掌握像围棋这样的游戏。在这两个游戏中，强化学习可以发现与获胜相关的关键关联。这并不是说没有更复杂的方法来解决学习问题。1997 年在国际象棋比赛中击败加里·卡斯帕罗夫的 IBM 超级计算机 DeepBlue 是一个庞大的程序，其中内置了数千个由国际象棋专家和程序员编写的场景。但是这样复杂的程序，最终远不如像 Q 学习这样的简单算法健壮和强大。首先，他们编织了编码他们的人类的经验偏见。当雅达利深度强化学习算法在 DeepMind 开发出来时，它发现了一种在乒乓游戏中增加分数的方法，这种方法使用了一种人类玩家以前不知道的技巧。如果它完全是根据人类的经验编写的，它很可能永远不会做出这种“外星人”的动作。强化学习的优势在于，独自玩游戏时，它可以尝试数百万个游戏历史上从未有人想过要尝试的动作。</p><p id="c712" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">许多专家评论员在研究国际象棋强化学习算法 AlphaZero 时，看到了更高级版本的 DeepBlue，因此没有意识到他们正在研究一种完全不同的人工智能，一种具有完全不同含义的人工智能。因为强化学习模仿了人类学习的一种方式，所以可以用于掌握围棋的相同算法可以用于掌握煎蛋卷或叠衣服。当你第一次开始学习叠衣服时，你会犯错误，袖子不整齐，折痕不精确。通过重复，或者用计算机科学的话说，迭代，你慢慢地学会了让你达到目标状态，完美折叠衬衫所必需的正确动作。以这种方式，许多人类活动可以被“游戏化”并变成强化学习问题。</p><p id="ed76" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将神经网络与强化学习结合起来，允许人们采用诸如 Q 学习的算法，并扩大代理可以学习的环境和动作空间。极端情况下—代理学习的环境可以是屏幕的实际像素输入。这反映了人类和狗等哺乳动物从视野中学习的方式。</p><p id="1fa4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Neurostudio 学习引擎基于深度强化学习算法，称为深度 Q 学习，简称 DQN。对它的表亲表格式 Q 学习有一个基本的了解，对理解深度 Q 学习会有帮助。在下面的链接中，您可以找到表格 Q 学习的介绍，其中代理解决了一个简单的匹配样本难题:<a class="ae lu" href="https://www.unrealengine.com/marketplace/artificial-intelligence-q-learning" rel="noopener ugc nofollow" target="_blank">https://www . unrealenengine . com/market place/artificial-intelligence-Q-learning</a></p><p id="4e42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Q 学习的工作方法有时被称为逆向归纳法。想象一下，一个徒步旅行者迷失了方向，正试图找到返回营地的路。首先，他们随机选择一个前进的方向。他们爬过一块岩石，观察这个动作是否能让他们更接近目标。从这个新的有利位置，他们决定如何评价他们以前的行为。爬过岩石是一个好的决定还是一个坏的决定？实际上，我们正在从猜测中学习猜测。首先对未来进行猜测(考虑到我的新状态，到达营地有多容易)，然后根据第一次猜测对他们最后一次行动的值进行猜测(我爬过这块岩石，离大本营更近了，所以我的最后一次行动很好)，我们通常称之为试错学习，或联想学习。在表格式的 Q 学习数学形式中，这需要表达式</p><p id="c520" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Q(状态，动作)=奖励(状态，动作)+ Gamma * Max[Q(下一个状态，所有动作)]</p><p id="4a13" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中 R(状态，动作)是由当前动作收集的任何奖励，gamma 是介于 0 和 1 之间的固定折扣率，它控制代理人对当前奖励和未来奖励的重视程度。Max[Q(下一个状态，所有动作)]也称为贝尔曼更新规则，使状态动作对的当前值依赖于可以从该位置采取的下一个最佳未来动作。这是等式的“向前看”部分。以这种方式，一个人相信将来会发生的奖励可以被向后链接到到达那里的步骤。</p><p id="89ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了将 Q 学习与神经网络相结合，神经网络首先预测对应于给定其当前状态的代理可用的每个动作的值。最初，这些只是由神经网络产生的随机噪声。然而，在代理收到奖励后，它可以应用 Q 学习方程，并使用状态动作对的预测值和状态动作对的新值之间的结果误差来形成误差项。这个误差项可以用来训练神经网络。把神经网络想象成一个最小化预测误差的工具，把 Q 学习方程想象成提供这些预测误差的东西。</p><p id="52fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">经过多次训练后，神经网络逐渐提高了对代理在模拟器中可能遇到的每个状态动作对的估计。为了进行战略性的移动，代理只需在给定网络当前状态的情况下向前运行网络，并采取预计具有最高价值的行动。</p><p id="cb48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然我已经详细解释了 DQN 的工作原理，但是使用 Neurostudio 并不一定需要了解学习是如何发生的细节。相反，人们只需要清楚代理人正在从环境中的哪些元素学习，代理人可以采取的行动，以及它可以获得的回报。就像你不需要理解脑科学来教狗如何变戏法一样，你需要理解狗有什么能力，它发现什么是有益的，以及何时给予这些奖励来最大限度地学习。</p><p id="10aa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也就是说，在选择神经网络的参数时，对深度学习原则(如过拟合)的理解可能是有帮助的。训练神经网络既是一门科学，也是一门艺术，因为神经网络的许多参数会影响学习。</p><p id="1f5e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然深度强化学习允许代理在复杂的真实世界环境中学习，但它们通常比表格 Q 学习代理做得更慢。给定影响神经网络的大量参数，也可能更难以“调整”代理的行为。</p><p id="d729" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，使用神经网络允许代理人概括他们的策略——也就是说，他们将把学到的策略应用于各种各样的环境状态，这些环境状态类似于他们最初接受训练的环境状态。这使得他们即使在全新的环境中也能聪明地行动。相比之下，使用表格式 Q 学习，所学习的策略将仅在代理具有过去关联的确切环境状态中被调用。这种将学习推广到新任务和新环境的能力使深度强化学习成为目前可用的最强大的机器学习框架之一，也是学习引擎的自然起点。我期望使用这样的学习引擎来扩展真实和虚拟代理的视野，并迎来一个新的嵌入式人工智能时代。</p></div></div>    
</body>
</html>