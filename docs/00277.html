<html>
<head>
<title>Various Optimization Algorithms For Training Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练神经网络的各种优化算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6?source=collection_archive---------1-----------------------#2019-01-13">https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6?source=collection_archive---------1-----------------------#2019-01-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1fda" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">正确的优化算法可以成倍地减少训练时间。</h2></div><p id="ee87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">许多人可能在训练神经网络时使用优化器，而不知道该方法被称为优化。优化器是用来改变神经网络属性的算法或方法，如权重和学习速率，以减少损失。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/4cfff9aaf96f4a9f28ef311eec7f00fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*65Mxg_Yfq-L7AvaS0K5aGA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Optimizers help to get results faster</figcaption></figure><p id="535a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你应该如何改变你的神经网络的权重或学习速率来减少损失是由你使用的优化器定义的。优化算法或策略负责减少损失，并尽可能提供最准确的结果。</p><p id="5f92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将了解不同类型的优化器及其优势:</p><h1 id="a576" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">梯度下降</strong></h1><p id="f64f" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">梯度下降是最基本但最常用的优化算法。它大量用于线性回归和分类算法。神经网络中的反向传播也使用梯度下降算法。</p><p id="83dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度下降是一种一阶优化算法，它依赖于损失函数的一阶导数。它计算出应该以何种方式改变权重，以使函数达到最小值。通过反向传播，损耗从一层转移到另一层，并且模型的参数(也称为权重)根据损耗进行修改，以便损耗可以最小化。</p><p id="1edf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">算法:<strong class="kh ir"> θ=θ−α⋅∇J(θ) </strong></p><p id="0046" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="024a" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">容易计算。</li><li id="9601" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">容易实现。</li><li id="f38c" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">很好理解。</li></ol><p id="acbd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><ol class=""><li id="a356" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">可能陷入局部最小值。</li><li id="d88c" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">计算整个数据集的梯度后，权重会发生变化。因此，如果数据集过大，可能需要数年时间才能收敛到最小值。</li><li id="4751" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">需要大量内存来计算整个数据集的梯度。</li></ol><h1 id="409b" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">随机梯度下降</strong></h1><p id="59ae" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">这是梯度下降的一个变种。它试图更频繁地更新模型的参数。在这种情况下，在计算每个训练样本的损失之后，改变模型参数。因此，如果数据集包含 1000 行，SGD 将在数据集的一个周期内更新模型参数 1000 次，而不是像梯度下降那样更新一次。</p><p id="126b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">θ=θ−α⋅∇j(θ；x(一)；y(i))，其中{x(i)，y(i)}为训练示例</strong>。</p><p id="5f4b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于模型参数频繁更新，参数在不同强度的损失函数中具有高方差和波动。</p><p id="7ce0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="168f" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">因此，模型参数的频繁更新在更短的时间内收敛。</li><li id="0dab" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">需要较少的存储器，因为不需要存储损失函数值。</li><li id="4f5c" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">可能会得到新的最小值。</li></ol><p id="ae15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><ol class=""><li id="1a6f" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">模型参数的高方差。</li><li id="83bc" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">甚至在达到全局最小值后也可以射击。</li><li id="4a25" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">为了获得与梯度下降相同的收敛性，需要缓慢降低学习率的值。</li></ol><h1 id="b017" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">小批量梯度下降</strong></h1><p id="95e6" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">它是所有梯度下降算法中最好的。这是对 SGD 和标准梯度下降的改进。它会在每次批处理后更新模型参数。因此，数据集被分成不同的批次，在每一批次之后，参数被更新。</p><p id="4035" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">θ=θ−α⋅∇j(θ；B(i))，其中{B(i)}为训练样本</strong>的批次。</p><p id="cba1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优势</strong>:</p><ol class=""><li id="9100" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">经常更新模型参数，并且方差较小。</li><li id="8120" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">需要中等大小的内存。</li></ol><p id="a207" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">所有类型的梯度下降都有一些挑战:</strong></p><ol class=""><li id="aba4" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">选择学习率的最佳值。如果学习率太小，梯度下降可能需要很长时间才能收敛。</li><li id="05fc" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">对所有参数都有一个恒定的学习率。可能有些参数我们不想以同样的速度改变。</li><li id="6c46" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">可能会陷入局部最小值。</li></ol><h1 id="0536" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">气势</strong></h1><p id="5ac7" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">动量是为了减少 SGD 中的高方差和软化收敛而发明的。它加速了向相关方向的收敛，减少了向无关方向的波动。该方法中还使用了一个超参数，称为动量，用“<strong class="kh ir"> γ </strong>表示。</p><p id="3ad8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">V(t)=γV(t1)+α。∇J(θ) </strong></p><p id="b52c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，权重更新为<strong class="kh ir">θ=θV(t)。</strong></p><p id="3e3a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">动量项<strong class="kh ir"> γ </strong>通常设置为 0.9 或类似值。</p><p id="07ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="9b27" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">减少参数的振荡和高方差。</li><li id="422b" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">比梯度下降收敛得更快。</li></ol><p id="92a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><ol class=""><li id="9bff" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">增加了一个需要手动精确选择的超参数。</li></ol><h1 id="8ce7" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">内斯特罗夫加速梯度</h1><p id="1aab" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">动量可能是一个好方法，但是如果动量太高，算法可能错过局部最小值，并且可能继续上升。因此，为了解决这个问题，开发了 NAG 算法。这是一种前瞻方法。我们知道我们将使用<strong class="kh ir">γV(t1)</strong>来修改权重，因此<strong class="kh ir">θγV(t1)</strong>大致告诉我们未来的位置。现在，我们将基于这个未来参数而不是当前参数来计算成本。</p><p id="c5ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">V(t)=γV(t1)+α。∇j(θγv(t1))</strong>，然后使用<strong class="kh ir">θ=θv(t)更新参数。</strong></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi my"><img src="../Images/45be23710e887f6f726c02ed62628655.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*aPPq25NxIQB_lP1fhqRNkg.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">NAG vs momentum at local minima</figcaption></figure><p id="d8e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="cc03" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">不会错过局部最小值。</li><li id="1152" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">如果出现最小值，速度会变慢。</li></ol><p id="9444" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><ol class=""><li id="83b0" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">尽管如此，超参数仍需要手动选择。</li></ol><h1 id="1400" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">阿达格拉德</h1><p id="d5f1" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">所解释的所有优化器的缺点之一是，对于所有参数和每个周期，学习率是恒定的。这个优化器改变了学习率。它改变每个参数的学习率<strong class="kh ir">‘η’</strong>和每个时间步长<strong class="kh ir">‘t’。</strong>这是一种二阶优化算法。它对误差函数的导数起作用。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/6f596eaf3f4542e19e9097c976a7a8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*zCRt57Wf8KYkvYmqWFyqew.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">A derivative of loss function for given parameters at a given time t.</figcaption></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi na"><img src="../Images/d54143d51493057996659cb8d7c0ca8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*QKKVHVeX312PJN7pCPtgGw.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Update parameters for given input i and at time/iteration t</figcaption></figure><p id="db9c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> η </strong>是一个学习率，在给定的时间，基于为给定的参数<strong class="kh ir"> θ(i)计算的先前梯度，对给定的参数<strong class="kh ir"> θ(i) </strong>进行修改。</strong></p><p id="40a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们存储梯度的平方和 w.r.t. <strong class="kh ir"> θ(i) </strong>直到时间步长<strong class="kh ir"> t </strong>，而<strong class="kh ir"> ϵ </strong>是一个避免被零除的平滑项(通常约为 1e 8)。有趣的是，如果没有平方根运算，该算法的性能会差得多。</p><p id="8c16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">它对不太频繁的参数进行大的更新，对频繁的参数进行小的更新。</p><p id="8523" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="ef90" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">每个训练参数的学习率变化。</li><li id="dea6" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">不需要手动调整学习率。</li><li id="9e3c" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">能够在稀疏数据上训练。</li></ol><p id="2ecc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><ol class=""><li id="33ae" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">因为需要计算二阶导数，所以计算成本高。</li><li id="2a2a" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">学习率总是下降，导致训练缓慢。</li></ol><h1 id="f87d" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">阿达德尔塔</h1><p id="4654" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">它是阿达格勒的扩展，倾向于消除它的<em class="nb">衰减学习率</em>问题。<strong class="kh ir"> <em class="nb"> Adadelta </em> </strong>将累积的过去梯度的窗口限制为某个固定大小<strong class="kh ir"> w </strong>，而不是累积所有先前平方的梯度。在此，使用指数移动平均值，而不是所有梯度的总和。</p><p id="b0ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> E[g ](t)=γ。e[g](t1)+(1γ)。g (t) </strong></p><p id="5179" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将<strong class="kh ir"> γ </strong>设置为与动量项相似的值，大约为 0.9。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/fd4981afb174c74b8fd2ab19bde0d98d.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*TVdJhPPIaSNsQe95yKApmA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Update the parameters</figcaption></figure><p id="bc5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="10ee" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">现在学习速度不衰减，训练不停止。</li></ol><p id="368c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><ol class=""><li id="0fd1" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">计算开销很大。</li></ol><h1 id="b5eb" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="a4d6" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated"><a class="ae nd" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank"> Adam </a>(自适应力矩估计)适用于一阶和二阶动量。Adam 背后的直觉是，我们不希望滚动得太快，因为我们可以跳过最小值，我们希望稍微降低速度，以便仔细搜索。除了存储类似于<strong class="kh ir"> AdaDelta </strong>、<strong class="kh ir">T43】Adam、</strong>、<em class="nb">、</em>的过去平方梯度的指数衰减平均值之外，还保存过去梯度的指数衰减平均值<strong class="kh ir"> M(t)。</strong></p><p id="d908" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> M(t)和 V(t) </strong>分别是梯度<em class="nb"> </em>的<strong class="kh ir"> <em class="nb">均值</em> </strong>和<strong class="kh ir"> <em class="nb">无中心方差</em> </strong>的一阶矩值。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f65c55c1edad920a2e4096d1532efb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*uhvftel2AgBioJkgaYdRiA.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">First and second order of momentum</figcaption></figure><p id="4534" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们取<strong class="kh ir"> M(t) </strong>和<strong class="kh ir"> V(t) </strong>的平均值，使得<strong class="kh ir">E[M(t)】</strong>可以等于<strong class="kh ir"> E[g(t)] </strong>其中，<strong class="kh ir">E[f(x)】</strong>是<strong class="kh ir"> f(x) </strong>的期望值。</p><p id="7d93" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要更新参数:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a283bd45f2aa8af615480dc0623b4a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*kl2TXe-A5C7UEQEgCIC_hQ.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Update the parameters</figcaption></figure><p id="fa82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">β1 的值是 0.9，β2 的值是 0.999，而'<strong class="kh ir"> ϵ' </strong>的值是(10 x exp(-8))。</p><p id="cb9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">优点</strong>:</p><ol class=""><li id="77bb" class="mk ml iq kh b ki kj kl km ko mm ks mn kw mo la mp mq mr ms bi translated">方法太快，收敛很快。</li><li id="f864" class="mk ml iq kh b ki mt kl mu ko mv ks mw kw mx la mp mq mr ms bi translated">纠正消失学习率，高方差。</li></ol><p id="df00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">缺点</strong>:</p><p id="2e43" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算成本高。</p><h1 id="08d2" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated">各种优化器之间的比较</h1><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/76eda2406c7c6f92a764646a25a8535f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*_osB82GKHBOT8k1idLqiqA.gif"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Comparison 1</figcaption></figure><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/76eda2406c7c6f92a764646a25a8535f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*_osB82GKHBOT8k1idLqiqA.gif"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">comparison 2</figcaption></figure><h1 id="058f" class="ln lo iq bd lp lq lr ls lt lu lv lw lx jw ly jx lz jz ma ka mb kc mc kd md me bi translated"><strong class="ak">结论</strong></h1><p id="3528" class="pw-post-body-paragraph kf kg iq kh b ki mf jr kk kl mg ju kn ko mh kq kr ks mi ku kv kw mj ky kz la ij bi translated">亚当是最好的优化者。如果一个人想在更短的时间内比亚当更有效地训练神经网络，那么他就是优化者。</p><p id="d402" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于稀疏数据，使用具有动态学习率的优化器。</p><p id="fa7b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果，想使用梯度下降算法比 min-batch 梯度下降是最好的选择。</p><p id="a972" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你们喜欢这篇文章，并且能够对不同优化算法的不同行为有一个很好的直觉。</p></div></div>    
</body>
</html>