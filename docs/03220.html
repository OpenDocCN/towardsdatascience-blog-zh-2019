<html>
<head>
<title>All the algebra you need to know about Linear Regression to be interview-ready</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面试前你需要知道的关于线性回归的所有代数知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-linear-regression-to-be-interview-ready-fc58a00a0b8c?source=collection_archive---------7-----------------------#2019-05-23">https://towardsdatascience.com/all-you-need-to-know-about-linear-regression-to-be-interview-ready-fc58a00a0b8c?source=collection_archive---------7-----------------------#2019-05-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/952214b1d28c4d15653ca9f67ba220b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*03OJTwqgvqGS9nPF"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Photo by <a class="ae jd" href="https://unsplash.com/@johnmoeses?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">John Moeses Bauan</a> on <a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="fd12" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">线性回归是数据科学中最简单也是最重要的算法之一。无论你面试的是数据科学、数据分析、机器学习还是量子研究领域的工作，你最终都可能不得不回答关于 LR 的特定代数问题。以下是你需要知道的，让你对自己的 LR 知识充满信心。</p><p id="49c1" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">注:本文全是理论，无编码。它还假设您已经至少对代数有一点熟悉。</em></p><h1 id="ab0c" class="lc ld jg bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">知道自己的假设</strong></h1><p id="0096" class="pw-post-body-paragraph kd ke jg kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">虽然本文假设您已经对 LR 有所了解，但是让我们提醒一下公式和假设。给定 N 个观察值，一个输出向量 Y(维数为 Nx1)和 p 个输入 x1，X2，…，Xp(每个输入向量的维数为 Nx1)，<strong class="kf jh"> LR 假设回归函数 E(Y|X)在输入</strong>中是线性的。y 因此统计:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/bcaea813a630722b25fea547ff76d90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*kr1FkhHPnJwOIgYA6wCN7Q.png"/></div></figure><p id="d318" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<strong class="kf jh">ε是误差项</strong>。线性假设实际上是 LR<strong class="kf jh">唯一严格必要的假设</strong>在本文后面，我们将看到我们可以添加更多的假设来推断更多的结果。</p><p id="989b" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然上面的公式看起来很简单，但找到系数(β)并不总是很容易——继续向前，我们将把β称为系数的估计值。</p><h1 id="950f" class="lc ld jg bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">了解关键指标的定义</h1><p id="c2ed" class="pw-post-body-paragraph kd ke jg kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">以下是你绝对需要知道(牢记)的 3 个指标:</p><ul class=""><li id="d538" class="mk ml jg kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated"><strong class="kf jh"> RSS </strong>是残差平方和</li></ul><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a79129473b8a43ece756f5e525756e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*VluvChYLI9Qv6xWofwaPpw.png"/></div></figure><p id="75f3" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb"> y_i </em>为观测值<em class="lb"> i </em>的输出，<em class="lb"> ŷ_i </em>为观测值 i <em class="lb"> : </em>的估计输出</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2079e8f9f4b37a01e2662a2ef61c6a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*Mduh39LxsS_VT_HO4DKoeA.png"/></div></figure><ul class=""><li id="2bff" class="mk ml jg kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated">TSS 是平方和的总和</li></ul><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/0ac0929feb96970a2eb4416aa167a2aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*mxsJBISHVlJ8OeTrbyvzdQ.png"/></div></figure><p id="759f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在哪里</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/d9cc1458091b17833a3b8b06283cce4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*sCcADzoMBvmSe2bywwe8Jw.png"/></div></figure><ul class=""><li id="631c" class="mk ml jg kf b kg kh kk kl ko mm ks mn kw mo la mp mq mr ms bi translated"><strong class="kf jh"> R 平方</strong>，它是 X 和 Y 之间线性关系的量度</li></ul><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/7cf5be924fd32b4a19d5a0e7bc677616.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*pHIDS2QVLr5pg789FRLsVw.png"/></div></figure><p id="9919" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">只要你知道这些公式，你就应该能够通过逻辑推理推断出所有其他的结果。</p><h1 id="718e" class="lc ld jg bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">知道答案:如何(何时)计算它</strong></h1><p id="c477" class="pw-post-body-paragraph kd ke jg kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">正如我们之前所说，LR 的关键是找到系数的估计值。我们通过<strong class="kf jh">最小化 RSS 找到这些。为此，让我们定义 X 和 Y 为</strong></p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi my"><img src="../Images/550f378523e36f7ef92caf52d7485686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pptlEJyXbFDmL7TlPRhDLQ.png"/></div></div></figure><p id="891f" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，为了考虑截距β_ 0，我们必须在输入矩阵中添加一列 1。我们的最小化问题等价于求解:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mz"><img src="../Images/d302596871767e0c320bc3a8ffe6fcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ynuur03BGGGX0P-e6pzupA.png"/></div></div></figure><p id="edd6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们可以计算右边项的<strong class="kf jh">梯度</strong>:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi na"><img src="../Images/0672293d55c53ab38e9993abb6a21aa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*vZTonMpA0ouMOlxN_6DKtw.png"/></div></figure><p id="306a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于我们的估计 hat 应该等于 0。</p><p id="a933" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">假设 X </strong> ⊤ <strong class="kf jh"> X 是非奇异的，</strong>这给了我们一个显式解<strong class="kf jh"> : </strong></p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/bcb056b6d94d92ac2a14a3f5fdbb390d.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*wRpfeZTfPdbef653HTlydQ.png"/></div></figure><p id="f64d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个你需要知道的公式，但是你也应该能够证明它，就像上面做的那样。非奇点的假设是这里的关键。我们还推断出 y 估计的公式:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nc"><img src="../Images/4c8a3cb183bd9df7acfaeb9f5216ee73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*RFs1rIOhTjgrd3nKej6MHA.png"/></div></div></figure><p id="8002" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">知道<strong class="kf jh">维 Nx1 </strong> (1 个输入变量)中的显式解也是有用的:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/95bdaeecd997c39a9fe69b39bf36fee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*4Xj2IalzFa036xaTvvbk9A.png"/></div></figure><p id="7001" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb"> x </em>在这里是一个向量(不是矩阵)——当你已经有了更一般的解决方案时，很容易记住:X⊤X 成为输入的方差(在一维中，求一个项的倒数等于除以那个项)，X⊤y 成为协方差项。你也可以通过在一维空间做类似的计算来计算这个解。</p><h1 id="dfc8" class="lc ld jg bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">熟悉假设检验(假设正态误差)</h1><p id="49f4" class="pw-post-body-paragraph kd ke jg kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">在采访中，对 LR 有一些统计概念也很重要。</p><p id="215a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本节假设你已经掌握了统计测试的基础知识(包括<a class="ae jd" href="https://en.wikipedia.org/wiki/T-statistic" rel="noopener ugc nofollow" target="_blank"> t 统计</a>、<a class="ae jd" href="https://en.wikipedia.org/wiki/F-distribution" rel="noopener ugc nofollow" target="_blank"> f 统计</a>和<a class="ae jd" href="https://en.wikipedia.org/wiki/Statistical_hypothesis_testing" rel="noopener ugc nofollow" target="_blank">假设检验</a>)。</p><p id="dd83" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设正常误差，即</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f041db9ba07f1c7265e040a092ea43ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*3vqNfSNUOQKmCzRX4X6oaQ.png"/></div></figure><p id="1306" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">(别忘了ε在这里是矢量！)那么我们的估计解满足:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/f0af2ca1995a171e95eec32f4eefcc22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*xbxzcrRe24EnFO_P8CYPCQ.png"/></div></figure><p id="010a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/41f857d45bd564129cb7b1dc28766902.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*GmfuzN6xm3uoiMnmW-kRhw.png"/></div></figure><p id="eb04" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就把我们带到了</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/20b366cd09e4e16cc08b56e23153e225.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*I16vPbKE-KvpW0HlsK9nhQ.png"/></div></figure><p id="d968" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一发现有助于评估系数β_ j 为零的<strong class="kf jh">零假设</strong>:我们可以计算<strong class="kf jh"> t-score </strong></p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/b45dc5f7d8c2817f565f78690dcbc01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*bvC06jnIStsWv7n0Llu1fg.png"/></div></figure><p id="10f7" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">在零假设</strong>下，t_j 遵循具有 N-p-1 个自由度的 t 分布，而<strong class="kf jh">对于足够大的 N</strong>遵循正态分布。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/21e6027cc3e354c684915f3d375c08f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*TypzkyKIsz4rYm3xFtWZQw.png"/></div></figure><p id="e517" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">计算这个分数可以帮助你评估零假设。例如，<strong class="kf jh">找到高于 1.96 的|t_j|分数将确保系数β_ j 不为空的 5%显著性。</strong></p><p id="f9ec" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您还可以计算给定系数的<strong class="kf jh">置信区间</strong>:近似的<strong class="kf jh">(1–2 * alpha)-置信区间</strong>由下式给出</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/533495cc9758e440ae99c46018b9b53f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_LNBj4bZ_D70UoniLLPng.png"/></div></div></figure><p id="a099" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用标准正态曲线下的面积来计算:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/2f35f4d1de347c5c97fa40714bbb3037.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*UEWMvf3RY-FU2-kPtzTxMQ.png"/></div></figure><p id="6221" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们还可以通过计算 F-stat 来检验假设<strong class="kf jh">,即每个系数都为零</strong></p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/44dafddedfaa57e3b3dcd4a34078d738.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*9aFhz88SvRboLXknZsfQcA.png"/></div></figure><p id="012a" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在无效假设下，遵循 F(p，N-p-1)分布。因此，F 的大值构成了反对零假设的证据。</p><h1 id="8314" class="lc ld jg bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">红利 1:我们为什么要做最小二乘法？</h1><p id="83cf" class="pw-post-body-paragraph kd ke jg kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">当我们寻找β的最优估计时，我们本能地跳到了最小二乘优化，但是为什么呢？首先，我们可以证明(我们不会在这里，查看<a class="ae jd" href="http://www.unm.edu/~lspear/geog525/24linreg2.pdf" rel="noopener ugc nofollow" target="_blank">这篇文章</a>了解更多细节)最小二乘估计量是无偏的，即</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/825791f6b393cf6941d0c01ef363616e.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*pKYC_XODvRTsiU795s20iQ.png"/></div></figure><p id="f7c8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，实际上有一个定理证明最小二乘估计量<strong class="kf jh">具有最小方差</strong>。这就是<a class="ae jd" href="https://en.wikipedia.org/wiki/Gauss–Markov_theorem" rel="noopener ugc nofollow" target="_blank">高斯-马尔可夫定理</a>:它表明<strong class="kf jh">最佳无偏估计量是最小二乘</strong>。</p><h1 id="bd5e" class="lc ld jg bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated"><strong class="ak">加成 2:当</strong> X⊤X <strong class="ak">排名不全怎么办？</strong></h1><p id="6960" class="pw-post-body-paragraph kd ke jg kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">首先，我们来看看这是什么时候发生的。让我们提醒自己，X 的维数是(N，p+1)。那么 X⊤X 就是(p+1，p+1)。我们可以证明<strong class="kf jh"> X⊤X 是全排名的当且仅当 x 是排名 p+1，</strong> <strong class="kf jh">这迫使 N &gt; p </strong>。这里可以看到证明<a class="ae jd" href="https://math.stackexchange.com/questions/691812/proof-of-when-is-a-xtx-invertible" rel="noopener ugc nofollow" target="_blank">。这意味着这些特征是线性独立的(但不一定不相关)。</a></p><p id="09b0" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当情况并非如此时(当我们的特征多于观察值时)，我们可以使用<strong class="kf jh">收缩方法</strong>如岭回归。的确，当我们在 X⊤X 的对角线上增加一项时，问题就变得可行了。对于<strong class="kf jh">岭回归</strong>的例子:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c099af5391d1faf342f2651956a617e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*L9ABwykfGRGkQL8c3bFobA.png"/></div></figure><p id="f768" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一个解决方案:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9f5b200caec793f7510d1a7e40c9e450.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*VvncLQxOX9PsAFOP4JQBCQ.png"/></div></figure><p id="d4ae" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">的确，因为 X⊤X，是<a class="ae jd" href="https://en.wikipedia.org/wiki/Definiteness_of_a_matrix" rel="noopener ugc nofollow" target="_blank">正半定的</a>，它的特征值都是正的，在对角线上加一个正项就使它全秩了。所以这使得问题非奇异。这就是我们必须对 p &gt; &gt; N 的情况应用<strong class="kf jh">正则化技术</strong>的原因之一</p></div></div>    
</body>
</html>