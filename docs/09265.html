<html>
<head>
<title>PyTorch for Deep Learning: A Quick Guide for Starters</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习 PyTorch:初学者快速指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-for-deep-learning-a-quick-guide-for-starters-5b60d2dbb564?source=collection_archive---------2-----------------------#2019-12-08">https://towardsdatascience.com/pytorch-for-deep-learning-a-quick-guide-for-starters-5b60d2dbb564?source=collection_archive---------2-----------------------#2019-12-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="06b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2019 年，ML 框架的战争有两个主要竞争者:<a class="ae kl" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>和<a class="ae kl" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>。由于 PyTorch 易于使用，越来越多的研究人员和学生<a class="ae kl" href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/" rel="noopener ugc nofollow" target="_blank">采用 py torch，而在工业界，Tensorflow 目前仍是首选平台。</a></p><p id="71ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PyTorch 的一些主要优势包括:</p><ul class=""><li id="f532" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated">简单性:它非常 Python 化，可以很容易地与 Python 生态系统的其他部分集成。它易于学习、使用、扩展和调试。</li><li id="f9a3" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated">PyTorch 在可用性方面大放异彩，因为它设计了更好的面向对象的类，这些类封装了所有重要的数据选择以及模型架构的选择。PyTorch 的文档也很出彩，对初学者很有帮助。</li><li id="067f" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">动态图:</strong> PyTorch 实现了动态计算图。这意味着网络可以在运行时改变行为，只需很少或不需要开销。这对于调试和以最小的努力构建复杂的模型非常有帮助。允许 PyTorch 表达式自动区分。</li></ul><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi la"><img src="../Images/5b1205ccceed7f9f333d5aa99adad2f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Mlpusre3Vdllufpx"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><a class="ae kl" href="https://unsplash.com/photos/CbgXKSd3qkQ" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="84d4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PyTorch 在研究中越来越受欢迎。下图显示了在其他深度学习框架中，单词“PyTorch”每月被提及的次数占所有被提及次数的百分比。我们可以看到，2019 年 arXiv 中 PyTorch 的上升趋势非常明显，几乎达到了 50%。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi lq"><img src="../Images/ffc7265a4eb60b89cdb00f0060704e03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*91yyjqA4nrpyl0CTEcgyeg.png"/></div></div><figcaption class="lm ln gj gh gi lo lp bd b be z dk"><a class="ae kl" href="https://arxiv.org/abs/1912.01703" rel="noopener ugc nofollow" target="_blank">arXiv papers mentioning PyTorch is growing</a></figcaption></figure><p id="718e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">动态图形生成</strong>，紧密的<strong class="jp ir"> Python 语言集成</strong>，以及相对简单的 API<strong class="jp ir">使得 PyTorch 成为一个优秀的研究和实验平台。</strong></p><h1 id="e3e1" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">装置</h1><p id="2710" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">PyTorch 提供了一个非常简洁的界面来获得要安装的工具的正确组合。下面是快照选择和相应的命令。Stable 代表 PyTorch 的最新测试和支持版本。这个应该适合很多用户。预览是可用的，如果你想要最新的版本，没有完全测试和支持。您可以从 Anaconda(推荐)和 Pip 安装包中进行选择，并支持各种 CUDA 版本。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi mu"><img src="../Images/d94f17ace7ee2c0a4fce9ab7468b16ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDlRnTbbi8_j82iw0VqxPw.png"/></div></div></figure><h1 id="cad6" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">PyTorch 模块</h1><p id="946e" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">现在我们将讨论 PyTorch 库的关键模块，如<strong class="jp ir">张量</strong>、<strong class="jp ir">亲笔签名</strong>、<strong class="jp ir">优化器</strong>和<strong class="jp ir">神经网络(NN ) </strong>，它们对于创建和训练神经网络是必不可少的。</p><h1 id="017d" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">张量</h1><p id="ba98" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">张量是 PyTorch 的主力。我们可以把<em class="mv">张量</em>想象成多维数组。PyTorch 有一个由<em class="mv"> torch </em>模块提供的大量操作库。PyTorch 张量非常接近非常流行的 NumPy 阵列。事实上，PyTorch 具有与 NumPy 无缝互操作的特性。与 NumPy 数组相比，PyTorch 张量增加了一个优势，即张量和相关操作都可以在 CPU 或 GPU 上运行。PyTorch 提供的第二个重要功能是允许张量跟踪对它们执行的操作，这有助于计算输出相对于任何输入的梯度或导数。</p><h2 id="cad8" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">基本张量运算</h2><p id="8173" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">张量是指将向量和矩阵推广到任意维数。张量的维数与用于引用张量内标量值的索引的数量一致。零阶张量(0D 张量)只是一个数字或者一个<em class="mv">标量</em>。一阶张量(1D 张量)是一组数字或一个矢量。类似地，二阶张量(2D)是一组向量或一个矩阵。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5241677e11388195e1a59d012fca42c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*Q8bknFjSyoN-UIzKc89P9g.png"/></div></figure><p id="1c9e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在让我们在 PyTorch 中创建一个张量。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e990edb5aae0a3e59ac8c3d325993350.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*6tEnEI_G_PaSt5cQ9nRNYA.png"/></div></figure><p id="fe74" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">导入 torch 模块后，我们调用了一个函数<em class="mv"> torch.ones </em>，它创建了一个大小为 9 的(2D)张量，其中填充了值 1.0。</p><p id="aec3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其他方式包括使用<code class="fe nk nl nm nn b"><strong class="jp ir">t<em class="mv">orch.zeros</em></strong></code><em class="mv">；</em>零填充张量，<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">torch.randn</em></strong></code>，<em class="mv">；</em>来自随机均匀分布。</p><h2 id="93aa" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">类型和尺寸</h2><p id="6fe8" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">每个张量都有相关的类型和大小。使用<code class="fe nk nl nm nn b"><strong class="jp ir">torch.Tensor</strong></code>构造函数时，默认的张量类型是<code class="fe nk nl nm nn b"><strong class="jp ir">torch.FloatTensor</strong></code>。但是，您可以将张量转换为不同的类型(<code class="fe nk nl nm nn b"><strong class="jp ir">float</strong></code> <strong class="jp ir">、</strong>、<code class="fe nk nl nm nn b"><strong class="jp ir">long</strong></code>、<strong class="jp ir">、</strong>、<code class="fe nk nl nm nn b"><strong class="jp ir">double</strong></code>等)。)通过在初始化时或稍后使用类型转换方法之一指定它。指定初始化类型有两种方法:要么直接调用特定张量类型的构造函数，如<code class="fe nk nl nm nn b"><strong class="jp ir">FloatTensor</strong></code> <strong class="jp ir"> </strong>或<code class="fe nk nl nm nn b"><strong class="jp ir">LongTensor</strong></code>，要么使用特殊的方法<code class="fe nk nl nm nn b"><strong class="jp ir"> torch.tensor()</strong></code>，并提供<code class="fe nk nl nm nn b"><strong class="jp ir">dtype</strong>.</code></p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi no"><img src="../Images/2a9677b46ea951b3a0669f8a3d8b6036.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*6fPYkffpVkkYYAXoP508sg.png"/></div></figure><h2 id="bcb5" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">一些有用的张量运算:</h2><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi np"><img src="../Images/2e1c2b78823fe4288044450b5d50b1b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/format:webp/1*88Zot00KqxxbwzBXAo8dzw.png"/></div></figure><p id="876a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">找出张量中的最大值项以及包含最大值的索引。这些可以通过<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">max()</em></strong></code>和<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">argmax()</em></strong></code>功能完成。我们还可以使用<code class="fe nk nl nm nn b"><strong class="jp ir">item()</strong></code>从 1D 张量中提取标准的 Python 值。</p><blockquote class="nq nr ns"><p id="569c" class="jn jo mv jp b jq jr js jt ju jv jw jx nt jz ka kb nu kd ke kf nv kh ki kj kk ij bi translated">大多数对张量进行操作并返回张量的函数都会创建一个新的张量来存储结果。如果你需要一个<strong class="jp ir"> <em class="iq">就地</em> </strong>函数，寻找一个附加了下划线(<code class="fe nk nl nm nn b"><em class="iq">_</em></code>)的函数，例如<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="iq">torch.transpose_</em></strong></code>将对一个张量进行就地转置。</p></blockquote><p id="a989" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">torch.from_numpy</em></strong></code> <em class="mv"> &amp; </em> <code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">torch.numpy()</em></strong></code> <em class="mv">，张量和 Numpy 之间的转换非常简单。</em></p><p id="5c25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">另一个常见的操作是<strong class="jp ir"> <em class="mv">重塑</em> </strong>一个张量。这是经常使用的操作之一，也非常有用。我们可以用<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">view()</em></strong></code>或<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">reshape()</em></strong></code>来做这件事:</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/3efa40568c17edda7bd82229eb86e869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*EHUYjQn3UDeTdqvz9XgHTQ.png"/></div></figure><p id="0515" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">Tensor.reshape()</em></strong></code>和<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">Tensor.view()</em></strong></code>虽然不一样。</p><ul class=""><li id="5ad9" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">Tensor.view()</em></strong></code>只作用于连续的张量，并且<strong class="jp ir">永远不会</strong>复制内存。这将在非连续张量上引起误差。但是你可以通过调用<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">contiguous()</em></strong></code>使张量连续，然后你可以调用<code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">view()</em></strong></code>。</li><li id="f5c4" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><code class="fe nk nl nm nn b"><strong class="jp ir"><em class="mv">Tensor.reshape()</em></strong></code>将对任何张量起作用，如果需要的话<strong class="jp ir">可以克隆</strong>。</li></ul><h2 id="58f7" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">张量广播</h2><p id="d456" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">PyTorch 支持类似 NumPy 的广播。<em class="mv">广播</em>可以让你在两个张量之间进行运算。广播语义参见<a class="ae kl" href="https://pytorch.org/docs/stable/notes/broadcasting.html" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><h1 id="ded2" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">张量简而言之:什么，如何在哪里</h1><p id="f9ea" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">唯一定义张量的三个<a class="ae kl" href="https://pytorch.org/docs/stable/tensor_attributes.html" rel="noopener ugc nofollow" target="_blank">属性</a>是:</p><p id="ef73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> dtype: </strong> <strong class="jp ir">张量的每个元素中实际存储的是什么</strong>？这可以是浮点数或整数等。PyTorch 有九种不同的数据类型。</p><p id="6b89" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">布局:我们如何从逻辑上解释这个物理内存。最常见的布局是步进张量。跨距是一个整数列表:第 k 个跨距表示从张量的第 k 维中的一个元素到下一个元素所必需的内存跳跃。</strong></p><p id="826c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">设备:张量的物理内存实际存储在哪里，例如在 CPU 或 GPU 上。<code class="fe nk nl nm nn b"><strong class="jp ir">torch.device</strong></code>包含设备类型(<code class="fe nk nl nm nn b">'<strong class="jp ir">cpu</strong>'</code>或<code class="fe nk nl nm nn b">'<strong class="jp ir">cuda</strong>'</code>)和设备类型的可选设备序号。</strong></p><h1 id="34cc" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">亲笔签名</h1><p id="9606" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">亲笔签名是自动区分系统。自动微分是干什么的？给定一个网络，它会自动计算梯度。当计算向前传球时，自动签名同时执行请求的计算并建立一个表示计算梯度的函数的图形。</p><h2 id="c566" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">这是如何实现的？</h2><p id="c4cb" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">PyTorch 张量可以根据产生它们的操作和父张量来记住它们来自哪里，并且它们可以自动提供这种操作相对于它们的输入的导数链。这是通过<code class="fe nk nl nm nn b"><strong class="jp ir">requires_grad</strong></code> <strong class="jp ir">实现的，如果</strong>设置为真。</p><p id="2ee4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe nk nl nm nn b"><strong class="jp ir">t= torch.tensor([1.0, 0.0], requires_grad=True)</strong></code></p><p id="72ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">计算完梯度后，导数的值被自动填充为张量的<code class="fe nk nl nm nn b"><strong class="jp ir">grad </strong></code>属性。对于具有任意数量张量的函数的任意组合用<code class="fe nk nl nm nn b"><strong class="jp ir">requires_grad= True</strong></code>；PyTorch 将计算整个函数链的导数，并在这些张量的<code class="fe nk nl nm nn b"><strong class="jp ir">grad </strong></code>属性中累加它们的值。</p><h1 id="6745" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">优化者</h1><p id="f1a6" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">优化器用于更新权重和偏差，即模型的内部参数，以减少误差。更多细节请参考我的另一篇<a class="ae kl" rel="noopener" target="_blank" href="/estimators-loss-functions-optimizers-core-of-ml-algorithms-d603f6b0161a">文章</a>。</p><p id="04cd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">PyTorch 有一个<code class="fe nk nl nm nn b"><strong class="jp ir">torch.optim</strong></code>包，里面有各种优化算法，比如 SGD(随机梯度下降)、Adam、RMSprop 等。</p><p id="3865" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看如何创建一个提供的优化器 SGD 或 Adam。</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="ec98" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir">import torch.optim as optim<br/>params = torch.tensor([1.0, 0.0], requires_grad=True)</strong></span><span id="cf0e" class="mw ls iq nn b gy of oc l od oe"><strong class="nn ir">learning_rate = 1e-3</strong></span><span id="1839" class="mw ls iq nn b gy of oc l od oe"><strong class="nn ir">## SGD<br/>optimizer = optim.SGD([params], lr=learning_rate)</strong></span><span id="baf8" class="mw ls iq nn b gy of oc l od oe"><strong class="nn ir">## Adam<br/>optimizer = optim.Adam([params], lr=learning_rate)</strong></span></pre><p id="8e4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果不使用优化器，我们将需要手动更新模型参数，如下所示:</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="da0d" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir">    for params in model.parameters(): <br/>       params -= params.grad * learning_rate</strong></span></pre><p id="1279" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以使用优化器中的<code class="fe nk nl nm nn b"><strong class="jp ir">step()</strong></code>方法向前迈进一步，而不是手动更新每个参数。</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="f91f" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir">optimizer.step()</strong></span></pre><p id="a78e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">调用 step 时，params 的值会更新。优化器查看<code class="fe nk nl nm nn b"><strong class="jp ir">params.grad</strong></code>，并通过从中减去<code class="fe nk nl nm nn b"><strong class="jp ir">learning_rate</strong></code> <strong class="jp ir"> </strong>乘以<code class="fe nk nl nm nn b"><strong class="jp ir">grad</strong></code> <strong class="jp ir"> </strong>来更新<code class="fe nk nl nm nn b"><strong class="jp ir">params </strong></code>，就像我们在没有使用优化器的情况下所做的一样。</p><p id="941b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><code class="fe nk nl nm nn b"><strong class="jp ir">torch.optim</strong></code> <strong class="jp ir"> </strong>模块通过传递一个参数列表，帮助我们抽象出具体的优化方案。由于有多种优化方案可供选择，我们只需要为我们的问题选择一种，然后让底层 PyTorch 库为我们施展魔法。</p><h1 id="f5d6" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">神经网络</h1><p id="14d1" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">在 PyTorch 中,<code class="fe nk nl nm nn b"><strong class="jp ir">torch.nn</strong></code>包定义了一组类似于神经网络层的模块。模块接收输入张量并计算输出张量。<code class="fe nk nl nm nn b"><strong class="jp ir">torch.nn</strong></code>包还定义了一组训练神经网络时常用的有用损失函数。</p><p id="253a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">构建神经网络的步骤是:</p><ul class=""><li id="81cc" class="km kn iq jp b jq jr ju jv jy ko kc kp kg kq kk kr ks kt ku bi translated"><strong class="jp ir">神经网络构建:</strong>创建神经网络层。设置参数(权重、偏差)</li><li id="31af" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">正向传播:</strong>计算预测输出。测量误差。</li><li id="2746" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">反向传播:</strong>找到误差后，我们<strong class="jp ir"> </strong>反向传播我们的误差梯度以更新我们的权重参数。我们通过对误差<strong class="jp ir"> </strong>函数<strong class="jp ir"> </strong>相对于我们的神经网络的参数进行求导来做到这一点。</li><li id="712e" class="km kn iq jp b jq kv ju kw jy kx kc ky kg kz kk kr ks kt ku bi translated"><strong class="jp ir">迭代优化:</strong>我们要尽可能的把误差降到最低。我们通过梯度下降不断迭代更新参数。</li></ul><h1 id="488b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">建立一个神经网络</h1><p id="e916" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">让我们按照上面的步骤，在 PyTorch 中创建一个简单的神经网络。</p><h2 id="5082" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">第一步:<strong class="ak">神经网络构建</strong></h2><p id="77b2" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">我们这里把我们的 NN <code class="fe nk nl nm nn b"><strong class="jp ir">Net</strong></code> <strong class="jp ir"> </strong>叫做<strong class="jp ir">。</strong>我们继承了<code class="fe nk nl nm nn b"><strong class="jp ir">nn.Module</strong></code>。与<code class="fe nk nl nm nn b">super().__init__()</code>结合，这创建了一个跟踪架构的类，并提供了许多有用的方法和属性。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi og"><img src="../Images/8202d5b185027b234ada1bd3c66bdbc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQl5EOIhIynMqiXSgzC0uw.png"/></div></div></figure><p id="f223" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的神经网络<code class="fe nk nl nm nn b"><strong class="jp ir">Net</strong></code>有一个隐含层<code class="fe nk nl nm nn b"><strong class="jp ir">self.hl</strong></code>和一个输出层<code class="fe nk nl nm nn b"> <strong class="jp ir">self.ol</strong></code>。</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="8312" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir">self.hl = nn.Linear(1, 10)</strong></span></pre><p id="ee0f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这一行创建了一个具有 1 个输入和 10 个输出的线性变换模块。它还会自动创建权重和偏差张量。一旦用<code class="fe nk nl nm nn b"><strong class="jp ir">net.hl.weight</strong></code>和<code class="fe nk nl nm nn b"><strong class="jp ir">net.hl.bias</strong></code>创建了网络<code class="fe nk nl nm nn b"><strong class="jp ir">net</strong></code>，就可以访问权重和偏差张量。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi oh"><img src="../Images/3200a506fafe269dcecc530b24ed7bef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08TwDZ74TzzaxLSd3e7bhQ.png"/></div></div></figure><p id="da86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经使用<code class="fe nk nl nm nn b"><strong class="jp ir">self.relu = nn.ReLU()</strong></code>定义了激活。</p><h2 id="4ea3" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated"><strong class="ak">第二步:正向传播</strong></h2><p id="72f9" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">用<code class="fe nk nl nm nn b"><strong class="jp ir">nn.Module</strong></code>创建的 PyTorch 网络必须定义一个<code class="fe nk nl nm nn b"><strong class="jp ir">forward()</strong></code>方法。它接受一个张量<code class="fe nk nl nm nn b"><strong class="jp ir">x</strong></code>并通过您在<code class="fe nk nl nm nn b"><strong class="jp ir">__init__</strong></code>方法中定义的操作传递它。</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="9279" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir">def</strong> forward(self, x):<br/>   hidden = self.hl(x)<br/>   activation = self.relu(hidden)<br/>   output = self.ol(activation)</span></pre><p id="17b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以看到，输入张量经过隐层，然后是激活函数(relu)，最后是输出层。</p><h2 id="ce04" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">步骤 3:反向传播</h2><p id="1d24" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">这里，我们必须计算误差或损失，并反向传播我们的误差梯度，以更新我们的权重参数。</p><p id="538d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">损失函数获取(输出，目标)并计算一个估计<code class="fe nk nl nm nn b"><strong class="jp ir">output </strong></code>离<code class="fe nk nl nm nn b"><strong class="jp ir">target</strong></code>有多远的值。在<code class="fe nk nl nm nn b"><strong class="jp ir">torch.nn</strong></code>包下有几个不同的<a class="ae kl" href="https://pytorch.org/docs/nn.html#loss-functions" rel="noopener ugc nofollow" target="_blank">损失函数</a>。一个简单的损失是<code class="fe nk nl nm nn b"><strong class="jp ir">nn.MSELoss</strong></code>，它计算输入和目标之间的均方误差。</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="6c2e" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir">output = net(input)<br/>loss_fn = nn.MSELoss()<br/>loss = loss_fn(output, target)</strong></span></pre><h2 id="7b27" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">反向投影</h2><p id="3375" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">一个简单的函数调用<code class="fe nk nl nm nn b"><strong class="jp ir">loss.backward()</strong></code> <strong class="jp ir"> </strong>传播错误。不要忘记清除现有的梯度，否则梯度将积累到现有的梯度。调用<code class="fe nk nl nm nn b"><strong class="jp ir">loss.backward()</strong></code>后，查看反向调用前后的隐藏层偏差梯度。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi oi"><img src="../Images/064f72158fb1c0fb53aa42a8afeae1a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m5vGPMvkMFhbjmPFmL9VOw.png"/></div></div></figure><p id="b6c0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以在调用 backward()之后，我们看到渐变是为隐藏层计算的。</p><h2 id="20d7" class="mw ls iq bd lt mx my dn lx mz na dp mb jy nb nc mf kc nd ne mj kg nf ng mn nh bi translated">步骤 4:迭代优化</h2><p id="3748" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">我们已经看到 optimizer 如何帮助我们更新模型的参数。</p><pre class="lb lc ld le gt nx nn ny nz aw oa bi"><span id="d85a" class="mw ls iq nn b gy ob oc l od oe"><strong class="nn ir"><em class="mv"># create your optimizer</em><br/>optimizer = optim.Adam(net.parameters(), lr=1e-2)</strong></span><span id="d7c5" class="mw ls iq nn b gy of oc l od oe"><strong class="nn ir">optimizer.zero_grad()   <em class="mv"># zero the gradient buffers</em></strong></span><span id="e41b" class="mw ls iq nn b gy of oc l od oe"><strong class="nn ir">output = net(input)     # calculate output<br/>loss = loss_fn(output, target) #calculate loss<br/>loss.backward()      # calculate gradient</strong></span><span id="a316" class="mw ls iq nn b gy of oc l od oe"><strong class="nn ir">optimizer.step()     <em class="mv"># update parameters</em></strong></span></pre><p id="f855" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意不要错过<code class="fe nk nl nm nn b"><strong class="jp ir">zero_grad()</strong></code> <strong class="jp ir"> </strong>来电。如果你错过调用它，梯度会在每次调用 backward 时累积，你的梯度下降不会收敛。下面是 Andrej 最近的一条推文，展示了修复这些漏洞的沮丧和时间。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8f9c00cde6e6ff0956c3ea23b1402cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*n_23Pjhm_VgpsADK-51EiQ.png"/></div></figure><p id="6b03" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，我们的基本步骤(1，2，3)已经完成，我们只需要迭代训练我们的神经网络，以找到最小的损失。所以我们运行<code class="fe nk nl nm nn b"><strong class="jp ir">training_loop</strong></code> <strong class="jp ir"> </strong>多次，直到损失最小。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi ok"><img src="../Images/6123d57db114ac1e2e44180de803a58a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S6NSxMPq0cJHblvR0SIu9g.png"/></div></div></figure><p id="5645" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们运行我们的神经网络来训练输入<code class="fe nk nl nm nn b"><strong class="jp ir">x_t</strong></code>和目标<code class="fe nk nl nm nn b"><strong class="jp ir">y_t</strong></code>。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi ol"><img src="../Images/3fce494d5d9fd31df6ba1c5ed3c1e964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tC-oOE9FYFsp_bNC-72P8A.png"/></div></div></figure><p id="a868" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们称<code class="fe nk nl nm nn b"><strong class="jp ir">training_loop </strong></code>为 1500 个时代，可以通过所有其他的论点，如<code class="fe nk nl nm nn b"><strong class="jp ir">optimizer</strong></code>、<code class="fe nk nl nm nn b"><strong class="jp ir">model</strong></code>、<code class="fe nk nl nm nn b"><strong class="jp ir">loss_fn</strong></code>、、<code class="fe nk nl nm nn b"><strong class="jp ir">inputs</strong></code>、<strong class="jp ir">、</strong>和<code class="fe nk nl nm nn b"><strong class="jp ir">target</strong></code>。每 300 个周期后，我们打印损失，我们可以看到每次迭代后损失都在减少。看起来我们最基本的神经网络正在学习。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi om"><img src="../Images/aee42a39d6f4fa8e2335da58117319e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uxgYxXSMgfhXJd-ZUVDOA.png"/></div></div></figure><p id="0ce2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们绘制了模型输出(黑叉)和目标数据(红圈)，模型似乎学得很快。</p><figure class="lb lc ld le gt lf gh gi paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="gh gi on"><img src="../Images/29ae38cee59f6c340d78edcdc85497e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-0Xsf4I7fmdRI5Z_qw8axQ.png"/></div></div></figure><p id="0a0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">到目前为止，我们已经讨论了 PyTorch 的基本或必要元素，以帮助您入门。我们可以看到我们构建的代码是如何模块化的，每个组件都提供了基本块，可以根据我们的要求进一步扩展以创建机器学习解决方案。</p><p id="26d1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为实际问题创建基于机器学习的解决方案涉及到大量的数据准备工作。然而，PyTorch 库提供了许多工具来使数据加载变得容易和更具可读性，如分别处理图像、文本和音频数据的<code class="fe nk nl nm nn b"><strong class="jp ir">torchvision</strong></code>、<code class="fe nk nl nm nn b"><strong class="jp ir">torchtext </strong></code>和<code class="fe nk nl nm nn b"><strong class="jp ir">torchaudio</strong></code>、<strong class="jp ir">、</strong>。</p><p id="3f94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练机器学习模型通常非常困难。当我们遇到一些问题时，总是需要一个工具来帮助可视化我们的模型和理解训练进度。<a class="ae kl" href="https://www.tensorflow.org/tensorboard" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">tensor board</strong></a><strong class="jp ir"/>就是这样一个工具，<strong class="jp ir"> </strong>帮助我们记录来自模型训练的事件，包括各种标量(如精度、损失)、图像、直方图等。自从 PyTorch 1.2.0 发布以来，TensorBoard 现在是 PyTorch 的内置特性。请按照<a class="ae kl" href="https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html" rel="noopener ugc nofollow" target="_blank">本</a>和<a class="ae kl" rel="noopener" target="_blank" href="/https-medium-com-dinber19-take-a-deeper-look-at-your-pytorch-model-with-the-new-tensorboard-built-in-513969cf6a72">本</a>教程安装和使用 Pytorch 中的张量板。</p><p id="a889" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">谢谢你的阅读。下一篇文章再见:)</p><h1 id="751b" class="lr ls iq bd lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">参考资料:</h1><p id="65f5" class="pw-post-body-paragraph jn jo iq jp b jq mp js jt ju mq jw jx jy mr ka kb kc ms ke kf kg mt ki kj kk ij bi translated">[1]<a class="ae kl" href="https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/" rel="noopener ugc nofollow" target="_blank">https://the gradient . pub/state-of-ml-frameworks-2019-py torch-domains-research-tensor flow-domains-industry/</a></p><p id="e6cf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]https://pytorch.org/<a class="ae kl" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"/></p><p id="3c0d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae kl" href="https://www.kdnuggets.com/2018/05/wtf-tensor.html" rel="noopener ugc nofollow" target="_blank">https://www.kdnuggets.com/2018/05/wtf-tensor.html</a></p></div></div>    
</body>
</html>