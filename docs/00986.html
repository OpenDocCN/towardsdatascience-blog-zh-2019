<html>
<head>
<title>Review: DRN — Dilated Residual Networks (Image Classification &amp; Semantic Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:DRN——扩张残差网络(图像分类和语义分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5?source=collection_archive---------10-----------------------#2019-02-15">https://towardsdatascience.com/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5?source=collection_archive---------10-----------------------#2019-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ea6b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用扩展卷积、改进的 ResNet，用于图像分类、图像定位和语义分割</h2></div><p id="cc10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事中，回顾了<strong class="kh ir">普林斯顿大学</strong>和<strong class="kh ir">英特尔实验室</strong>的<strong class="kh ir"> DRN(扩张剩余网络)</strong>。在 2016 年 ICML 发表了用于语义分割的<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5"> DilatedNet </a>之后，作者发明了 DRN，它不仅可以改善语义分割，还可以改善图像分类，而不增加模型的深度或复杂性。发表在<strong class="kh ir"> 2017 CVPR </strong>上，引用<strong class="kh ir"> 100 余次</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d527e1a8fb5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="c512" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="0a74" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la mt mu mv mw bi translated"><strong class="kh ir">扩张卷积</strong></li><li id="1bf1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">扩张的原因</strong></li><li id="5d77" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">扩张的剩余网络(DRN) </strong></li><li id="3ed4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">本地化</strong></li><li id="fc54" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">去网纹</strong></li><li id="f2fe" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">结果</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="6a17" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 1。扩张卷积</strong></h1><ul class=""><li id="df59" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">为简单起见，我仅引用<a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">中的方程式:</a></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="ab gu cl ni"><img src="../Images/b715da21779d01de703dc7f8f9ee0eca.png" data-original-src="https://miro.medium.com/v2/format:webp/1*mlHFvK6H_wMCyURSZNZWGQ.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Standard Convolution (Left), Dilated Convolution (Right)</strong></figcaption></figure><ul class=""><li id="5bee" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">左边的是标准卷积。右边的是扩张的回旋。我们可以看到在求和的时候，就是 s+ <em class="nt"> l </em> t=p 我们在卷积的时候会跳过一些点。</li><li id="c034" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir">当<em class="nt"> l </em> =1 时，为标准卷积。</strong></li><li id="8add" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir">当<em class="nt">l</em>T52】1 时，为扩张卷积。</strong></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi nu"><img src="../Images/5253839abf5eac2581ea4feea8ecac64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*btockft7dtKyzwXqfq70_w.gif"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Standard Convolution (l=1) (Left) Dilated Convolution (l=2) (Right)</strong></figcaption></figure><ul class=""><li id="ba86" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">以上举例说明了<em class="nt"> l </em> =2 时<strong class="kh ir">展开卷积的例子。我们可以看到<strong class="kh ir">感受野比标准感受野大</strong>。</strong></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="ab gu cl ni"><img src="../Images/46022047de14e5688756fdd46897bdfe.png" data-original-src="https://miro.medium.com/v2/format:webp/1*tnDNIyPePgHvb8JIx8SbqA.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">l=1 (left), l=2 (Middle), l=4 (Right)</strong></figcaption></figure><ul class=""><li id="d258" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">上图显示了更多关于感受野的例子。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="ce2f" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 2。扩张卷积的原因</strong></h1><ul class=""><li id="353a" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">研究发现，在网络末端获得的输出特征图很小的情况下，语义切分的准确率会降低。</li><li id="eab0" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">在<a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1"> FCN </a>中，也说明了当需要 32 倍上采样时，我们只能得到一个非常粗略的分割结果。因此，需要更大的输出特征图。</li><li id="3aad" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">一种简单的方法是简单地去除网络中的子采样(步长)步骤，以增加特征图的分辨率。然而，这也减少了感受野，严重减少了上下文的数量。对于更高分辨率来说，感受野的这种减小是不可接受的代价。</li><li id="5ce8" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">由于这个原因，使用扩张的卷积来增加较高层的感受野，补偿由于去除二次采样而引起的感受野的减少。</li><li id="6cf8" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">并且发现使用扩张卷积也有助于本文的图像分类任务。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="eb31" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">3.<strong class="ak">扩张残差网络(DRN) </strong></h1><ul class=""><li id="d011" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">在本文中，使用<em class="nt"> d </em>作为膨胀因子。</li><li id="c0b5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir">当<em class="nt"> d </em> =1 时，为标准卷积。</strong></li><li id="e2d9" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir">当<em class="nt">d</em>T46】1 时，为扩张卷积。</strong></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/02d2afc2e6b06dbb0121b38d2746534c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*-67TMJkhBO3sTtzAg2oUHg.png"/></div></figure><h2 id="ca12" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">原件<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a></h2><ul class=""><li id="2a99" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">在原<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>中，最后 2 组卷积层<em class="nt"> G4 </em>和<em class="nt"> G5 </em>使用 3×3 标准卷积(<em class="nt"> d </em> =1):</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi om"><img src="../Images/1343ea7d7e655241e99e92e04c7d4e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*N0zzVc47DRk4VKepFh-zhA.png"/></div></figure><ul class=""><li id="b5e2" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">由于最大池化，特征地图变得越来越小。</li><li id="75ff" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">输出特征图只有 7×7 的大小。这并不像上一节提到的那样好。</li></ul><h2 id="817a" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">DRN</h2><ul class=""><li id="7b51" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">在 DRN，在<em class="nt"> G4 处，使用 d </em> =2:</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a15c29e7aa940398a9a9172285a54ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*WASw_XlfLpdktRJe-SISOw.png"/></div></figure><ul class=""><li id="8f30" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">在<em class="nt"> G5 </em>处，对于第一次卷积(<em class="nt"> i </em> =1)，仍然使用<em class="nt"> d </em> =2:</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi on"><img src="../Images/0e3b19b7c91918ace578e7b9887a6fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*TzJkoYfCGM-JmsGxXh6jIw.png"/></div></figure><ul class=""><li id="b2d3" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">在<em class="nt"> G5 </em>，对于剩余的卷积(<em class="nt">I&gt;T33】1)，使用<em class="nt"> d </em> =4:</em></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/4b42beb8f5a984ad5ed047102af7fcda.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*F-r-HbochYCcet3RUjgWAg.png"/></div></figure><ul class=""><li id="d639" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">最后，<em class="nt"> G5 </em>在 DRN 的产量是 28×28，比原来的<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>产量大很多。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="16c8" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">4.<strong class="ak">本地化</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi op"><img src="../Images/45396f41106924b7cf3721deb75d2886.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*xXUUqY8JKtqDLlywvDbrwA.png"/></div></figure><ul class=""><li id="746b" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">对于图像分类任务，最后，有一个全局平均池，然后是 1×1 卷积和 softmax。</li><li id="7fe5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">为了进行本地化配置，只需简单地删除普通池。不涉及训练或参数调整。准确的分类 DRN 可以直接用于定位。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="8f64" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">5.<strong class="ak">去网纹</strong></h1><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a8669f1fe435c1d2bf4d37d4dd4cdb19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*eo3TBwf0DDC3GR-yGRiDkQ.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">A Gridding Artifact</strong></figcaption></figure><ul class=""><li id="4aa2" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">如上所示，当特征图的高频成分高于扩展卷积的采样速率时，会出现网格伪像。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oq"><img src="../Images/e30135cef1f2b47626fdfd7b4e7e1fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJ5MxPPS6jG18n_FyzrSVQ.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">DRN-A (Top) DRN-B (Middle) DRN-C (Bottom)</strong></figcaption></figure><ul class=""><li id="9558" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated"><strong class="kh ir"> DRN-A </strong>:只有扩张卷积的，有网格状伪影。</li><li id="f4a6" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir"> DRN-B </strong>:发现第一次最大汇集操作导致高振幅高频率激活。因此，<strong class="kh ir">第一最大池层由 2 个残差块(4 个 3×3 卷积层)代替，以减少网格伪影。</strong>网络末端还增加了 2 个剩余块。</li><li id="fe2e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir"> DRN-C </strong>:在网络的末端，膨胀逐渐降低到<strong class="kh ir">去除混叠伪影</strong>，即<strong class="kh ir">2-膨胀卷积后跟 1-膨胀卷积。</strong>然而，神器仍然在这里，因为它可以通过残余连接传递。因此，<strong class="kh ir">相应的剩余连接被移除。</strong></li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi or"><img src="../Images/93e5b409e78b06631b420046b03bae49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oicmB_oJVvcp8-5ndCHvYA.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Activation Maps of </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd np">ResNet-18</strong></a><strong class="bd np"> and Corresponding DRNs</strong></figcaption></figure><ul class=""><li id="15c2" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">上面显示了一个可视化。</li><li id="7f35" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir"> DRN-A-18 </strong>:卷积扩张，存在网格状伪影。</li><li id="4163" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir"> DRN-B-26 </strong>:用卷积代替最大池，特征图有更少的伪影。</li><li id="df35" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir"> DRN-C-26 </strong>:随着逐渐变小的扩张卷积和去除残留连接，伪影进一步减少。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi os"><img src="../Images/22267ce5631996dcfaa8ccd01a0011f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A2B-yJTcLyFLgIA8EYHIw.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Feature Map Visualization at Different Levels in DRN-C-26 (The highest average activation at each level is shown)</strong></figcaption></figure></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="3690" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">6.结果</h1><h2 id="b548" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">6.1.ImageNet 上的图像分类</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/dea5f25990e3305b97390892901ca3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*bA20J-GosOx9Jo-STsnMbA.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Top-1 &amp; Top-5 Error Rates on ImageNet Validation Set</strong></figcaption></figure><ul class=""><li id="143b" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated"><strong class="kh ir"> DRN-A-18 和 DRN-A-34 在 1-crop top-1 精度上分别领先</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet-18</strong></a><strong class="kh ir">和</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet-34</strong></a>2.43 和 2.92 个百分点。(在<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-34 </a>到 DRN-A-34 的情况下，误差相对减少 10.5%。)</li><li id="d5c0" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir"> DRN-A-50 在单作物 top-1 精度上比</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir">雷斯内特-50 </strong> </a>高出一个百分点以上。</li><li id="1745" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated"><strong class="kh ir">将一个</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">直接转化为一个 DRN-A </strong>，完全不改变模型的深度或容量，<strong class="kh ir">显著提高分类精度。</strong></li><li id="3958" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">每只 DRN-C 的表现都明显优于相应的 DRN-A</li><li id="7f1a" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-26 是从 DRN-A-18 衍生而来的，其精确度与更深的 DRN-A-34 相当。</li><li id="6294" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-42 是从 DRN-A-34 衍生而来的，其精确度与更深的 DRN-A-50 相当。</li><li id="1074" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-42 接近于 ResNet-101 的精确度，尽管后者更深 2.4 倍。</li></ul><h2 id="e993" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">6.2.ImageNet 上的对象定位</h2><ul class=""><li id="9572" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">这里，基于特征图激活值执行弱监督对象定位。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/71c249ecabae15a5daa96fe8a73ca43f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*wsG0bky3O8F5D2Rgbj4cAg.png"/></div></figure><ul class=""><li id="5a1b" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated"><em class="nt"> C </em> =1000，因为它是一个 1000 级的 ImageNet 数据集。</li><li id="6c43" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">以<em class="nt"> C </em>分辨率的响应图<em class="nt"> W </em> × <em class="nt"> H </em>，f( <em class="nt"> c </em>，<em class="nt"> w </em>，<em class="nt"> h </em>)为位置响应(<em class="nt"> w </em>，<em class="nt"> h </em>)，各位置的优势类为<em class="nt"> g </em> ( <em class="nt"> w </em>，<em class="nt"> h </em>)。边界框的集合是<em class="nt"> Bi </em>其中<em class="nt"> t </em>是激活阈值。并且在<em class="nt"> Bi </em>中选择最小包围盒<em class="nt"> bi </em>。</li><li id="76ff" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">对于地面实况框大于 0.5 的 IoU，它被认为是准确的。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/a14bd212518be2862afc872fcd90b284.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*TzDAM-sHBsMmQ2I9pfBp1A.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Top-1 &amp; Top-5 Localization Error Rates on ImageNet Validation Set</strong></figcaption></figure><ul class=""><li id="17b3" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated"><strong class="kh ir"> DRNs 优于相应的</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir">车型</strong>，说明了基本 DRN 建设的好处。</li><li id="540f" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-26 的性能明显优于 DRN-A-50，尽管其深度要低得多。这表明去网格方案对于需要更详细的空间图像分析的应用特别有益。</li><li id="d2c3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-26 也胜过<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">雷斯内特-101 </a>。</li></ul><h2 id="4bc2" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">6.3.城市景观的语义分割</h2><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ow"><img src="../Images/2b12b43c71c3bcb260eae70bc486604b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsDLhrbpZLOGo4gJirG8Nw.png"/></div></div></figure><ul class=""><li id="ab0e" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">对于<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>，它得到了 66.6%的平均 IoU。</li><li id="fc8c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-26 比 ResNet-101 基准高出一个多百分点，尽管深度低了 4 倍。</li><li id="145c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">尽管深度低 2.4 倍，但 DRN-C-42 型号的性能比 ResNet-101 基线高出 4 个百分点。</li><li id="4d56" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">DRN-C-26 和 DRN-C-42 都优于 DRN-A-50，这表明去网格结构对密集预测任务特别有利。</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ox"><img src="../Images/8877d1c5477caf7338acdbd9366a1d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PoMpMTWFNP9bLmhx8fOfGQ.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Cityscape Dataset</strong></figcaption></figure><ul class=""><li id="e486" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">如上所示，DRN-A-50 的预测被网格伪影破坏，即使该模型是在密集像素级监督下训练的。</li><li id="2a64" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la nc mu mv mw bi translated">相比之下，DRN-C-26 的预测不仅更准确，而且更清晰。</li></ul><h2 id="2f19" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">6.4.使用 DRN-D 获得更多结果</h2><ul class=""><li id="5adb" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la nc mu mv mw bi translated">在作者的<a class="ae lk" href="https://github.com/fyu/drn" rel="noopener ugc nofollow" target="_blank"> GitHub </a>中也有一个 DRN-D，它是 DRN-C 的简化版本</li></ul><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/b51c5964a1245fff15eab32ddcb9222b.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*4p4N4DXhVHqM0NrY_4tsSg.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Classification error rate on ImageNet validation set and numbers of parameters.</strong></figcaption></figure><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oz"><img src="../Images/259d103f4c9b88637b7b5aebcb48a17e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OWtoDU-94zj7OQec.png"/></div></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Classification error rate on ImageNet validation set and numbers of parameters</strong></figcaption></figure><p id="8de3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有的 DRN 也可以获得较低的错误率，而有较少的参数数目(较小的模型)。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/9bcfc6d7f7e31b164d211c48f3794d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*uL7N2ZB4MwdvyAzpVSxiEQ.png"/></div><figcaption class="nl nm gj gh gi nn no bd b be z dk"><strong class="bd np">Segmentation mIoU and number of parameters ( *trained with poly learning rate, random scaling and rotations.)</strong></figcaption></figure><ul class=""><li id="178a" class="mm mn iq kh b ki kj kl km ko nq ks nr kw ns la nc mu mv mw bi translated">DRN-D-22 以较少的参数达到 68%的 mIoU，与 DRN-C-26 相同，但高于 DRN-A-50。</li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><p id="a431" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不是逐渐降低内部表示的分辨率直到场景的空间结构不再可辨，而是在最终输出图层中始终保持高空间分辨率。图像分类精度提高，最终 DRN 优于最先进的<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>。</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="8cf4" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">参考</h2><p id="d30e" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko pb kq kr ks pc ku kv kw pd ky kz la ij bi translated">【2017 CVPR】【DRN】<br/><a class="ae lk" href="https://arxiv.org/abs/1705.09914" rel="noopener ugc nofollow" target="_blank">散漫残网</a></p><h2 id="7233" class="oa lv iq bd lw ob oc dn ma od oe dp me ko of og mg ks oh oi mi kw oj ok mk ol bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko pb kq kr ks pc ku kv kw pd ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(要)(到)(这)(些)(人)(,)(我)(们)(就)(不)(想)(要)(到)(这)(些)(人)(里)(来)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(了)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">离子</a><a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网</a><a class="ae lk" href="https://medium.com/datadriveninvestor/review-noc-winner-in-2015-coco-ilsvrc-detection-object-detection-d5cc84e372a" rel="noopener">NoC</a> yolo 9000[<a class="ae lk" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">yolov 3</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610">FPN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">retina net</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44">DCN</a>]</p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lk" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a></p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/> </strong> [ <a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">累计视觉 1 </a> ] [ <a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">累计视觉 2/DCAN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>]</p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 实例分段 <br/> </strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a>】</p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p></div></div>    
</body>
</html>