<html>
<head>
<title>Avoiding the vanishing gradients problem using gradient noise addition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用梯度噪声添加避免消失梯度问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343?source=collection_archive---------24-----------------------#2019-09-05">https://towardsdatascience.com/avoiding-the-vanishing-gradients-problem-96183fd03343?source=collection_archive---------24-----------------------#2019-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq jr js"><p id="372f" class="jt ju jv jw b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr im bi translated">也发表在<a class="ae ks" href="https://afagarap.works/2019/09/05/avoiding-vanishing-gradients.html" rel="noopener ugc nofollow" target="_blank">https://afagarap . works/2019/09/05/avoiding-vanishing-gradients . html</a></p></blockquote><h1 id="c85c" class="kt ku it bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">介绍</h1><p id="3399" class="pw-post-body-paragraph jt ju it jw b jx lr jz ka kb ls kd ke lt lu kh ki lv lw kl km lx ly kp kq kr im bi translated">神经网络是用于逼近函数的计算模型，该函数对数据集特征<strong class="jw iu"> <em class="jv"> x </em> </strong>和标签<strong class="jw iu"> <em class="jv"> y </em> </strong>之间的关系进行建模，即<strong class="jw iu"> <em class="jv"> f(x) ≈ y </em> </strong>。神经网络通过学习最佳参数<strong class="jw iu"> <em class="jv"> θ </em> </strong>来实现这一点，使得预测<strong class="jw iu"><em class="jv">f(x；θ) </em> </strong>和标签<strong class="jw iu"> <em class="jv"> y </em> </strong>最小。它们通常借助于在输出层观察到的误差的反向传播，通过基于梯度的算法进行学习。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/c82c94c5606123df9431234046134f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aMycR8vXJ-xXxnzS-WMXsA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Illustrated using <a class="ae ks" href="http://alexlenail.me/NN-SVG/index.html" rel="noopener ugc nofollow" target="_blank">NN-SVG</a>. A feed-forward neural network with two hidden layers. It learns to approximate the target label <strong class="bd mp">y</strong> by learning the appropriate <strong class="bd mp"><em class="mq">θ </em></strong><em class="mq">parameters with the criteria of minimizing the difference between its output label </em><strong class="bd mp"><em class="mq">f(x;</em></strong><em class="mq"> </em><strong class="bd mp"><em class="mq">θ)</em></strong><em class="mq"> and target label </em><strong class="bd mp"><em class="mq">y</em></strong><em class="mq">.</em></figcaption></figure><p id="d66f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">通过这种学习范式，神经网络在一些任务中产生了有希望的结果，如图像分类(<a class="ae ks" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">Krizhevsky et al .(2012)</a>；【何等(2015) )、图像生成(<a class="ae ks" href="https://arxiv.org/abs/1809.11096" rel="noopener ugc nofollow" target="_blank">布洛克等(2018)</a>；<a class="ae ks" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets" rel="noopener ugc nofollow" target="_blank"> Goodfellow 等人(2014)</a>；<a class="ae ks" href="https://arxiv.org/abs/1511.06434" rel="noopener ugc nofollow" target="_blank">拉德福德等(2015)</a>；<a class="ae ks" href="http://openaccess.thecvf.com/content_iccv_2017/html/Zhu_Unpaired_Image-To-Image_Translation_ICCV_2017_paper.html" rel="noopener ugc nofollow" target="_blank">朱等(2017) </a>、语言建模(<a class="ae ks" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">德夫林等(2018)</a>；<a class="ae ks" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">霍华德和鲁德(2018) </a>、音频合成(<a class="ae ks" href="https://arxiv.org/abs/1902.08710" rel="noopener ugc nofollow" target="_blank">恩格尔等人(2019)</a>；<a class="ae ks" href="https://arxiv.org/abs/1609.03499" rel="noopener ugc nofollow" target="_blank"> Oord 等人(2016) </a>)，以及图像字幕(<a class="ae ks" href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Vinyals_Show_and_Tell_2015_CVPR_paper.html" rel="noopener ugc nofollow" target="_blank"> Vinyals 等人(2015)</a>；<a class="ae ks" href="http://proceedings.mlr.press/v37/xuc15.pdf" rel="noopener ugc nofollow" target="_blank">徐等(2015) </a>等。然而，对于神经网络来说，这种优势并不总是如此。在 2012 年通过赢得 ImageNet 挑战赛而东山再起之前，训练神经网络是出了名的困难。</p><p id="e91d" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">困难是由许多问题造成的，例如，计算能力和数据不足以利用神经网络的全部潜力。在很大程度上，这是因为神经网络对初始权重很敏感(<a class="ae ks" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi" rel="noopener ugc nofollow" target="_blank"> Glorot 和 Bengio，2010 </a>)，并且由于以下任何一个或两个原因，当梯度值降低到无穷小的值时，它们往往会过早地停止学习(<a class="ae ks" href="https://www.bioinf.jku.at/publications/older/ch7.pdf" rel="noopener ugc nofollow" target="_blank"> Hochreiter 等人，2001</a>):(1)它们的激活函数具有小范围的梯度值，以及(2)它们的深度。这种现象被称为<em class="jv">消失梯度</em>问题。</p><p id="ff00" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">上述问题的第一个原因是我们对这篇文章的关注。换句话说，当我们用基于梯度的算法和反向传播来训练深度神经网络时，会出现<em class="jv">消失梯度</em>问题，其中通过每个隐藏层反向传播的梯度减小到无穷小的值，模型学习所需的信息不再存在。</p><p id="ff0c" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">自然，已经提出了许多解决方案来缓解这个问题，例如，使用不同的激活函数(<a class="ae ks" href="https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf" rel="noopener ugc nofollow" target="_blank"> Nair 和 Hinton，2010 </a>)，以及使用剩余连接(<a class="ae ks" href="http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html" rel="noopener ugc nofollow" target="_blank"> He 等人，2016 </a>)。在本文中，我们以激活函数的形式回顾了消失梯度问题的许多提议的解决方案，但是我们将我们的架构限制为前馈神经网络。我们还将研究一种实验方法，这种方法可以帮助避免消失梯度问题，也可以帮助它们更快更好地收敛。</p><h1 id="f290" class="kt ku it bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">批量归一化的梯度噪声添加</h1><p id="aab8" class="pw-post-body-paragraph jt ju it jw b jx lr jz ka kb ls kd ke lt lu kh ki lv lw kl km lx ly kp kq kr im bi translated">已经提出了几个研究工作来解决消失梯度问题，这些工作包括但不限于引入新的激活函数和新的架构。</p><p id="6e8f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">最简单的神经网络结构将由具有逻辑激活函数的隐藏层组成，该逻辑激活函数用基于梯度的学习算法和反向传播来训练。这种架构的问题是其激活函数将隐藏层值压缩为[0，1] ∈ ℝ.用该函数反向传播的梯度具有最大值 0.25(见表 1)，因此这些值变得饱和，直到没有更多有用的信息供学习算法用于更新模型的权重。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mr"><img src="../Images/0aadfe3976ac500561af50e8db80b581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-iyvAfd-ToACFDJjgdW0w.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Table 1. Activation functions for neural nets, together with their respective derivatives, and critical point values.</figcaption></figure><p id="f2d9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">多年来，这个问题的主流解决方案是使用双曲正切，最大梯度值为 1(见表 1)。然而，梯度值仍然会因该函数而饱和，这可以从图 1 中看到。因此，引入了校正线性单位(ReLU)激活功能(<a class="ae ks" href="https://www.cs.toronto.edu/~hinton/absps/reluICML.pdf" rel="noopener ugc nofollow" target="_blank"> Nair 和 Hinton，2010 </a>)。</p><p id="a527" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">ReLU 激活函数具有相同的最大梯度值 1，但其优于逻辑函数和双曲正切函数的优势在于其激活值不会饱和。然而，ReLU 具有其自身的缺点，即，由于其最小梯度值为 0，它触发了“死亡神经元”的问题，即，神经元没有激活值。所以，是的，即使它避免了非负值的饱和，它的负值也会触发死亡神经元现象。</p><p id="4aa1" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">由于这个缺点，ReLU 的变体之一是 Leaky ReLU，它有一个简单的修改，具有略高于零的下限。反过来，这种修改允许模型避免饱和梯度值和“死神经元”问题。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ms"><img src="../Images/81ada51fbfcf49a3242b6a92e0db8a77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QxeovtXvUEphj5utRE0jxQ.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Figure 1. Plotted using <a class="ae ks" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank">matplotlib</a>. Activation function values, their gradients, and their noise-augmented gradient values. For instance, adding Gaussian noise to the gradients of the logistic activation function increases its maximum value, i.e. from 0.25 to approximately 1.047831 (from a Gaussian distribution having a mean value of 0 and a standard deviation value of 0.5).</figcaption></figure><p id="07f5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">但是，尽管进行了这种修改，<a class="ae ks" href="https://arxiv.org/abs/1710.05941v1" rel="noopener ugc nofollow" target="_blank"> Ramachandran 等人(2017) </a>声称已经开发出了一种比 ReLU 更好的功能，即“Swish”激活功能。上述函数可以描述为逻辑加权线性函数，其最大梯度值约为 1.0998(如表 1 所示)，在 CIFAR 数据集(使用 ResNet)、ImageNet 数据集(使用 Inception 和 MobileNet)和机器翻译(使用 12 层变压器模型)上的性能优于 ReLU。</p><p id="da51" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">虽然这些解决方案更侧重于制定新的激活以改善神经网络的学习，但<a class="ae ks" href="https://arxiv.org/abs/1511.06807" rel="noopener ugc nofollow" target="_blank"> Neelakantan 等人(2015) </a>的工作介绍了一种简单而有效的改善神经网络性能的方法。该方法是简单地添加梯度噪声，以改善非常深的神经网络的学习(见等式。1).它不仅提高了神经网络的性能，而且有助于避免过拟合问题。虽然作者没有明确说明他们提出的解决方案旨在缓解消失梯度问题，但可以这样看，因为训练期间计算的梯度是膨胀的，因此有助于避免导致消失梯度问题的饱和梯度值。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/bfd03281b65aee8e93a1c8d498852f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/0*iftr5mI4WSm1Shkd"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Eq. 1. The gradient noise addition approach for improving the learning of deep neural networks.</figcaption></figure><p id="2bc7" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">时间步长<em class="jv"> t </em>处的标准偏差σ然后通过以下等式迭代退火，</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c9835247e41fa22ec770f8df28bf33dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/0*j2kYP_bWGVspIHgW"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Eq. 2. The annealing function helps to shift the gradient values away from zero during early training iterations.</figcaption></figure><p id="9a57" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">在<a class="ae ks" href="https://arxiv.org/abs/1511.06807" rel="noopener ugc nofollow" target="_blank"> Neelakantan et al. (2015) </a>的原始论文中，η参数是从{0.1，1.0}中选取的，而γ参数在他们所有的实验中都被设置为 0.55。</p><p id="350f" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">TensorFlow 2.0 用于实现本文实验的模型及其计算。为了实现退火梯度噪声添加，我们简单地增加使用<code class="fe mv mw mx my b">tf.GradientTape</code>计算的梯度，通过添加来自等式 1 产生的高斯分布的值。1 和 Eq。2.也就是说，使用<code class="fe mv mw mx my b">tf.add</code>，如代码片段 1 的第 7 行所示。</p><figure class="ma mb mc md gt me"><div class="bz fp l di"><div class="mz na l"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Snippet 1. Model optimization with annealing gradient noise addition.</figcaption></figure><p id="24e1" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">最后，这种方法通过使用批量标准化得到进一步增强(<a class="ae ks" href="https://arxiv.org/abs/1502.03167" rel="noopener ugc nofollow" target="_blank">约夫和赛格迪，2015 </a>)。因此，利用这种方法，在训练会话开始期间，层激活将被迫呈现单位高斯分布。</p><h1 id="e83e" class="kt ku it bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">实证结果</h1><p id="5496" class="pw-post-body-paragraph jt ju it jw b jx lr jz ka kb ls kd ke lt lu kh ki lv lw kl km lx ly kp kq kr im bi translated">在接下来的实验中，MNIST 手写数字分类数据集(<a class="ae ks" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"> LeCun，Cortes，and Burges，2010 </a>)用于训练和评估我们的神经网络。每个图像都被整形为 784 维向量，然后通过将每个像素值除以最大像素值(即 255)进行归一化，并添加来自标准偏差为 5e-2 的高斯分布的随机噪声，以提高在数据集上收敛的难度。</p><h2 id="881e" class="nb ku it bd kv nc nd dn kz ne nf dp ld lt ng nh lh lv ni nj ll lx nk nl lp nm bi translated">提高梯度值</h2><p id="bef8" class="pw-post-body-paragraph jt ju it jw b jx lr jz ka kb ls kd ke lt lu kh ki lv lw kl km lx ly kp kq kr im bi translated">在训练过程中，我们可以观察神经网络学习时的梯度分布。在图 2 中，我们以具有逻辑激活函数的神经网络为例。由于逻辑激活函数具有最小的最大梯度值(即 0.25)，我们可以考虑观察其梯度分布中值得注意的变化。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nn"><img src="../Images/ad998b07762f632665f2ac9cb5f81ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e_Iw6AxcCyTwAx2sUKRjig.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Figure 2. Figure from TensorBoard. Gradient distribution over time of neural nets with logistic activation function on the MNIST dataset. Top to bottom: Baseline model, experimental model with GNA, and experimental model with GNA + batch normalization.</figcaption></figure><p id="d8fd" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">从上面的图表中我们可以看出，从基线配置到实验配置，模型的梯度分布急剧变化，即从相当小的值(对于两层神经网络为+/- 4e-3，对于五层神经网络为+/- 5e-6)到相对大的值(对于两层和五层神经网络均为+/- 4)。虽然这并不能保证优越的模型性能，但它确实让我们了解到，将有足够的梯度值在神经网络中传播，从而避免梯度消失。我们转到下一小节来检查模型的分类性能。</p><h2 id="a7f9" class="nb ku it bd kv nc nd dn kz ne nf dp ld lt ng nh lh lv ni nj ll lx nk nl lp nm bi translated">分类性能</h2><p id="4b7e" class="pw-post-body-paragraph jt ju it jw b jx lr jz ka kb ls kd ke lt lu kh ki lv lw kl km lx ly kp kq kr im bi translated">使用具有动量的随机梯度下降(SGD )(学习速率α = 3e-4，动量γ = 9e-1)在扰动的 MNIST 数据集上训练模型 100 个时期，小批量大小为 1024(对于两层神经网络)和小批量大小为 512(对于五层神经网络)。我们的网络包括(1)两个各有 512 个神经元的隐藏层，和(2)五个隐藏层，每个隐藏层有以下神经元:512，512，256，256，128。两种网络架构的权重都是用 Xavier 初始化来初始化的。</p><p id="4177" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">我们可以在图 3-6 中观察到，使用实验方法，梯度噪声添加(GNA)和 GNA 批量归一化(BN)，有助于神经网络在损失和准确性方面更快更好地收敛。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi no"><img src="../Images/8af6208f4fde8286ef72505c0b03a3e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYICNtXB0dWPzrx3l83Frw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Figure 3. Plotted using <a class="ae ks" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank">matplotlib</a>. Training loss over time of the baseline and experimental (with GNA, and GNA + batch normalization) two-layered neural networks on the MNIST dataset.</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi np"><img src="../Images/efe2a381d041d10fd58f71937c099897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vtcpL0uGz51BuzRY76FBig.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Figure 4. Plotted using <a class="ae ks" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank">matplotlib</a>. Training accuracy over time of the baseline and experimental (with GNA, and GNA + batch normalization) two-layered neural networks on the MNIST dataset.</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nq"><img src="../Images/3332c6de97d39be029cfb7feec0bb713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rl7sVPtB7unZyyv-iECCzg.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Table 2. Test accuracy of the baseline and experimental (with GNA, and GNA + batch normalization) two-layered neural networks on the MNIST dataset.</figcaption></figure><p id="3aa9" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">从表 2 中，我们可以看到，使用 GNA 和 GNA+BN 的测试精度值显著提高，尤其是在基于逻辑的神经网络上。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nr"><img src="../Images/3fa340322867b7781dcd46a7e7d6c2ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dEH9nZZGjT2sI0e8QOe7Jw.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Figure 5. Plotted using <a class="ae ks" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank">matplotlib</a>. Training loss over time of the baseline and experimental (with GNA, and GNA + batch normalization) five-layered neural networks on the MNIST dataset.</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ns"><img src="../Images/b68ed428a156438d54878710ef989b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jHALiJAWtZCNN0x1tE10sA.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Figure 6. Plotted using <a class="ae ks" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank">matplotlib</a>. Training accuracy over time of the baseline and experimental (with GNA, and GNA + batch normalization) five-layered neural networks on the MNIST dataset.</figcaption></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nt"><img src="../Images/8ef3693889b7ad74fb2dfe12af1bfd38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vU29lPlif7LYg9gJ3NJtCg.png"/></div></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">Table 3. Test accuracy of the baseline and experimental (with GNA, and GNA + batch normalization) five-layered neural networks on the MNIST dataset.</figcaption></figure><p id="b528" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">从表 2 的结果可以看出，所有的基线双层神经网络都通过 GNA 和 GNA+BN 得到了改善。然而，对于五层神经网络(见表 3)，基于 ReLU 的模型未能提高测试精度。此外，我们可以看到，在这种配置中，基于 TanH 的神经网络比基于 ReLU 的模型具有更好的测试精度。我们可以将这归因于我们使用了<a class="ae ks" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi" rel="noopener ugc nofollow" target="_blank"> Xavier 初始化</a>(其中 TanH 的性能最佳)而不是<a class="ae ks" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html" rel="noopener ugc nofollow" target="_blank"> He 初始化</a>(其中 ReLU 的性能最佳)。</p><p id="4833" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">总的来说，五层神经网络的这些结果支持早先的陈述，即梯度值的膨胀不一定保证优越的性能。</p><p id="d77d" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">但这里有趣的是基线模型与 Swish 激活功能的大幅改进——测试准确率提高高达 54.47%。</p><p id="b1c5" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">尽管在两层神经网络的结果中，基于 Swish 的模型比基于 ReLU 和基于漏 RELU 的模型具有稍低的测试精度，但我们可以看到，对于五层神经网络，基于 Swish 的模型比基于 ReLU 的模型具有更高的测试精度(但比基于漏 ReLU 的模型稍低)。这在某种程度上证实了这样一个事实，即 Swish 在更深的网络上优于 ReLU，<a class="ae ks" href="https://arxiv.org/abs/1710.05941v1" rel="noopener ugc nofollow" target="_blank"> Ramachandran 等人(2017) </a>在其 12 层变压器模型的结果上展示了这一点。</p><h1 id="608e" class="kt ku it bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">结束语</h1><p id="371e" class="pw-post-body-paragraph jt ju it jw b jx lr jz ka kb ls kd ke lt lu kh ki lv lw kl km lx ly kp kq kr im bi translated">在本文中，我们概述了在神经网络中关于一组激活函数(逻辑、双曲正切、校正线性单位、低界校正线性单位和逻辑加权线性单位)传播的梯度值。我们使用了一种结合梯度噪声添加和批量标准化的方法，试图缓解消失梯度的问题。我们看到，在测试准确性方面，模型性能提高了 54.47%。此外，使用实验方法，模型比它们的基线对应物收敛得更快更好。</p><p id="bb24" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">我们在这篇文章中的探索只是冰山一角，因为有很多事情可以讨论关于消失梯度的问题。例如，如果我们尝试 Xavier 和 He 初始化方案，并将它们与基线和实验方案的零初始化和随机初始化进行比较，结果如何？添加梯度噪声有多大帮助？也就是它还能帮助一个 30 层以上的神经网络吗？如果我们也使用层归一化或权重归一化会怎么样？它将如何公平地对待或对待残余网络？</p><p id="dfe6" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">我希望我们在这篇文章中已经覆盖了足够多的内容，让你对渐变消失的问题以及避免这个问题的不同方法有更多的了解。</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="4129" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">完整代码可在<a class="ae ks" href="https://github.com/afagarap/vanishing-gradients" rel="noopener ugc nofollow" target="_blank">这里</a>获得。如果你有任何反馈，你可以通过<a class="ae ks" href="https://twitter.com/afagarap" rel="noopener ugc nofollow" target="_blank">推特</a>联系我。我们也可以通过<a class="ae ks" href="https://www.linkedin.com/in/abienfredagarap/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系！</p></div><div class="ab cl nu nv hx nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="im in io ip iq"><p id="9cdc" class="pw-post-body-paragraph jt ju it jw b jx jy jz ka kb kc kd ke lt kg kh ki lv kk kl km lx ko kp kq kr im bi translated">如果你喜欢读这篇文章，也许你也会发现我关于在 TensorFlow 2.0 中实现自动编码器的博客<a class="ae ks" href="https://medium.com/@afagarap/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7" rel="noopener">很有趣！</a></p><h1 id="2df9" class="kt ku it bd kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">参考</h1><ul class=""><li id="a298" class="ob oc it jw b jx lr kb ls lt od lv oe lx of kr og oh oi oj bi translated">克里日夫斯基、亚历克斯、伊利亚·苏茨基弗和杰弗里·e·辛顿。"使用深度卷积神经网络的图像网络分类."<em class="jv">神经信息处理系统的进展</em>。2012.</li><li id="25ad" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">何，，等，“深度残差学习在图像识别中的应用”<em class="jv">IEEE 计算机视觉和模式识别会议论文集</em>。2016.</li><li id="3c07" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">布洛克，安德鲁，杰夫·多纳休和卡伦·西蒙扬。"高保真自然图像合成的大规模 gan 训练."<em class="jv"> arXiv 预印本 arXiv:1809.11096 </em> (2018)。</li><li id="9a44" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">伊恩·古德菲勒等着《生成性对抗性网络》<em class="jv">神经信息处理系统的进展</em>。2014.</li><li id="d598" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">拉德福德，亚历克，卢克·梅斯，和苏密特·钦塔拉。"深度卷积生成对抗网络的无监督表示学习."<em class="jv"> arXiv 预印本 arXiv:1511.06434 </em> (2015)。</li><li id="4cf7" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">使用循环一致对抗网络的不成对图像到图像翻译。IEEE 计算机视觉国际会议论文集。2017.</li><li id="6777" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">伯特:用于语言理解的深度双向转换器的预训练。arXiv 预印本 arXiv:1810.04805 (2018)。</li><li id="7fd9" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">霍华德杰里米和塞巴斯蒂安.鲁德。“用于文本分类的通用语言模型微调。”arXiv 预印本 arXiv:1801.06146 (2018)。</li><li id="d98c" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">《甘瑟思:对抗性神经音频合成》arXiv 预印本 arXiv:1902.08710 (2019)。</li><li id="87aa" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">《Wavenet:原始音频的生成模型》arXiv 预印本 arXiv:1609.03499 (2016)。</li><li id="40df" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">展示和讲述:一个神经图像字幕生成器。IEEE 计算机视觉和模式识别会议录。2015.</li><li id="2cd9" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">徐，凯尔文，等。“展示、参与和讲述:视觉注意下的神经图像字幕生成”机器学习国际会议。2015.</li><li id="e97d" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">格洛特，泽维尔，和约舒阿·本吉奥。"理解训练深度前馈神经网络的困难."第十三届人工智能与统计国际会议论文集。2010.</li><li id="f201" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">《递归网络中的梯度流:学习长期依赖的困难》(2001).</li><li id="747d" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">奈尔、维诺德和杰弗里·e·辛顿。"校正的线性单位改进了受限的玻尔兹曼机器."第 27 届机器学习国际会议录(ICML-10)。2010.</li><li id="e93e" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">Ramachandran，Prajit，Barret Zoph 和 Quoc V. Le。"嗖嗖:一个自门控激活功能."arXiv 预印本 arXiv:1710.05941 7 (2017)。</li><li id="48d3" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">加入梯度噪音可以改善深度网络的学习。arXiv 预印本 arXiv:1511.06807 (2015)。</li><li id="d8fa" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">约夫、谢尔盖和克里斯蒂安·塞格迪。"批量标准化:通过减少内部协变量转移加速深度网络训练."arXiv 预印本 arXiv:1502.03167 (2015)。</li><li id="e85a" class="ob oc it jw b jx ok kb ol lt om lv on lx oo kr og oh oi oj bi translated">勒昆、扬恩、科琳娜·科尔特斯和 C. J .伯格斯。" MNIST 手写数字数据库."美国电话电报公司实验室[在线]。可用:【http://yann.lecun.com/exdb/mnist】(2010):18。</li></ul></div></div>    
</body>
</html>