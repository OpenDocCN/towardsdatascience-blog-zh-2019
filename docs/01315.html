<html>
<head>
<title>When Multi-Task Learning meet with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当多任务学习遇到 BERT 时</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/when-multi-task-learning-meet-with-bert-d1c49cc40a0c?source=collection_archive---------7-----------------------#2019-03-02">https://towardsdatascience.com/when-multi-task-learning-meet-with-bert-d1c49cc40a0c?source=collection_archive---------7-----------------------#2019-03-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="38f7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">面向自然语言理解的多任务深度神经网络简介</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4c94fe14441b8e9f0e3cf0c4555056cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-q3nP9nUETegGxKH"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="4c03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a> (Devlin et al .，2018)在多个 NLP 问题中得到了 2018 年最先进的结果。它利用 transformer 架构来学习<code class="fe lv lw lx ly b">contextualized word embeddings</code>,以便这些向量在不同的领域问题中代表更好的含义。为了扩展 BERT 的使用范围，刘等人提出了用<code class="fe lv lw lx ly b">Multi-Task Deep Neural Networks</code> ( <code class="fe lv lw lx ly b">MT-DNN</code>)在多个自然语言处理问题中取得最新的结果。<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>在<code class="fe lv lw lx ly b">MT-DNN</code>中帮助构建了一个共享文本表示，同时微调部分利用了多任务学习。</p><p id="8b0a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本故事将讨论用于自然语言理解的<a class="ae ky" href="https://arxiv.org/pdf/1901.11504.pdf" rel="noopener ugc nofollow" target="_blank">多任务深度神经网络</a>(刘等，2019)，并将涵盖以下内容:</p><ul class=""><li id="a85c" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">多任务学习</li><li id="26cb" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">数据</li><li id="daad" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">体系结构</li><li id="b6b6" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">实验</li></ul><h1 id="c352" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">多任务学习</h1><p id="b0d4" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated"><code class="fe lv lw lx ly b">Multi-task learning</code>是迁移学习的一种。当从多种事物中学习知识时，我们不需要从头开始学习一切，但是我们可以应用从其他任务中学到的知识来缩短学习曲线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nk"><img src="../Images/417c546e793fdf430902f0bfe1c03685.png" data-original-src="https://miro.medium.com/v2/0*FLSUhNZ5-WGHzm3M"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2192" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以滑雪和滑雪板为例，如果你已经掌握了滑雪，你不需要花很多时间去学习滑雪板。这是因为两项运动都有一些相同的技巧，你只需要理解不同的部分就可以了。最近，我听朋友说他是滑雪高手。他只花了一个月就掌握了滑雪。</p><p id="df55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到数据科学，研究人员和科学家认为，学习文本表示时可以应用迁移学习。<a class="ae ky" rel="noopener" target="_blank" href="/learning-generic-sentence-representation-by-various-nlp-tasks-df39ce4e81d7"> GenSen </a> (Sandeep 等人，2018)证明了多任务学习改善了句子嵌入。可以从不同任务中学习部分文本表示，并且可以将那些共享参数传播回去以学习更好的权重。</p><h1 id="5681" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">数据</h1><p id="35c0" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">输入是一个单词序列，可以是一个单句，也可以是用分隔符将两个句子组合在一起。与<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>相同，句子将被标记化并转化为初始单词嵌入、片段嵌入和位置嵌入。此后，多双向变换器将用于学习上下文单词嵌入。不同的部分是利用多任务来学习文本表示，并在微调阶段将其应用到单个任务中。</p><h1 id="e50c" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">DNN 山的建筑</h1><p id="d1f3" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">训练模型需要经过两个阶段。第一阶段包括词典编码器和变换编码器的预训练。通过跟随<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">伯特</a>，两个编码器都通过掩蔽语言建模和下一句预测来训练。第二阶段是微调部分。应用基于小批量随机梯度下降(SGD)。</p><p id="9292" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与单一任务学习不同，MT-DNN 将计算不同任务之间的损失，并同时将变化应用到模型中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2f8931ef6abec6387fd5e194a0295f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*Bgqm_CjdBvrD8Y4n9AU_UA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Training Procedure of MT-DNN (Liu et al., 2019)</figcaption></figure><p id="2b07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同任务的损失是不同的。对于分类任务，这是一个二元分类问题，所以使用交叉熵损失。对于文本相似性任务，使用均方误差。对于分级任务，使用负对数似然。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/0a8c378fa81e89308734b30767c11f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*bdf3Po2s1ARBEKbRpEN8Tw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Eq. 6 for classification (Liu et al., 2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2945b742a0f0eb889699426ee134afeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*V8oBjqy4mYpVq4Lp-Yc73A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Eq. 6 for regression (Liu et al., 2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f563adf9322ed23ea070dd265d053e6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*a5c9rBy3OpKdrmbrXPt2Iw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Eq. 6 for ranking (Liu et al., 2019)</figcaption></figure><p id="ae0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从下面的架构图来看，共享层通过<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>将文本转换为上下文嵌入。在共享层之后，它将通过不同的子流程来学习每个特定任务的表示。任务特定层被训练用于特定的任务问题，例如单句分类和成对文本相似性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/c364a9a9494974d639bd3564974787bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9QUIOMfCluQ1u6ptEFy8g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Architecture of MT-DNN (Liu et al., 2019)</figcaption></figure><h1 id="be80" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">实验</h1><p id="5ed0" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated"><code class="fe lv lw lx ly b">MT-DNN</code>基于 BERT 的<a class="ae ky" href="https://github.com/huggingface/pytorch-pretrained-BERT" rel="noopener ugc nofollow" target="_blank"> PyTorch 实现，超参数为:</a></p><ul class=""><li id="fce8" class="lz ma it lb b lc ld lf lg li mb lm mc lq md lu me mf mg mh bi translated">优化器:Adamax</li><li id="25bc" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">学习率:53–5</li><li id="bcf7" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">批量:32</li><li id="217b" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">最大历元:5</li><li id="8a6e" class="lz ma it lb b lc mi lf mj li mk lm ml lq mm lu me mf mg mh bi translated">辍学率:0.1</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/8f04d65942dfe4e0269816432de24b71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TDVCPenf4XmDidzAXJk7hQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">GLUE test set result (Liu et al., 2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2f71517a7de464a09876887889cd8daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*4FVJaRsgGnMnaMuf2v7aIA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">SNLI and SciTail result (Lit et al., 2019)</figcaption></figure><h1 id="0e7e" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">拿走</h1><ul class=""><li id="b0aa" class="lz ma it lb b lc nf lf ng li ns lm nt lq nu lu me mf mg mh bi translated">即使是类似的架构(即<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>)，也可以通过多个 NLP 问题学习更好的文本表示。</li></ul><h1 id="94ec" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="be7f" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">延伸阅读</h1><p id="1470" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">变压器的双向编码器表示(BERT) </a></p><p id="9967" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/learning-generic-sentence-representation-by-various-nlp-tasks-df39ce4e81d7">通用分布式语句表示(GenSen) </a></p><h1 id="a986" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><p id="2a51" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">Devlin J .，Chang M. W .，Lee K .，Toutanova K .，2018 年。<a class="ae ky" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a></p><p id="91c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Sandeep S .、Adam T .、Yoshua B .、Christopher J . p .<a class="ae ky" href="https://arxiv.org/pdf/1804.00079.pdf" rel="noopener ugc nofollow" target="_blank">通过大规模多任务学习学习通用分布式句子表示</a>。2018</p><p id="f80c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">刘小东，何炳春，陈伟志，高建峰 2019。<a class="ae ky" href="https://arxiv.org/pdf/1901.11504.pdf" rel="noopener ugc nofollow" target="_blank">用于自然语言理解的多任务深度神经网络</a></p></div></div>    
</body>
</html>