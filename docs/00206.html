<html>
<head>
<title>An illustrative introduction to Fisher’s Linear Discriminant</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">费希尔线性判别式的说明性介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-illustrative-introduction-to-fishers-linear-discriminant-9484efee15ac?source=collection_archive---------7-----------------------#2019-01-09">https://towardsdatascience.com/an-illustrative-introduction-to-fishers-linear-discriminant-9484efee15ac?source=collection_archive---------7-----------------------#2019-01-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/7c1b05c824ae620029bf57c6ac4ac02e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XfBJ8NEwSB_zW8Up1V71mQ.jpeg"/></div></div></figure><p id="6c8b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了处理两个或更多类别的分类问题，大多数机器学习(ML)算法以相同的方式工作。</p><p id="7076" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">通常，他们对输入数据应用某种转换，结果是将原始输入维度减少到一个较小的数字。目标是将数据投射到一个新的空间。然后，一旦投影，该算法试图通过找到一个线性分离来分类这些点。</p><p id="0e97" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于输入维数较小的问题，任务稍微容易一些。以下面的数据集为例。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi kz"><img src="../Images/2c60e65c6ea4b3d89e21fc662872f843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*foduk2DnEvxttu8tFpvixw.png"/></div></figure><p id="a8b3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">假设我们要对红蓝圈进行正确的分类。</p><p id="825a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">很明显，用简单的线性模型我们不会得到好的结果。没有将输入映射到其正确类别的输入和权重的线性组合。但是，如果我们可以转换数据，这样我们就可以画一条线来区分这两个类，会怎么样呢？</p><p id="1026" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们对两个输入特征向量求平方，就会发生这种情况。现在，一个线性模型将很容易对蓝点和红点进行分类。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/2851e0dcbc3121822cee3350fe97bb2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtB5d3c9ShP4s4ayAP3pBw.png"/></div></div></figure><p id="61d6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然而，有时我们不知道我们应该使用哪种转换。实际上，找到最佳表示法并不是一个简单的问题。我们可以对数据进行多种转换。同样，它们中的每一个都可能产生不同的分类器(就性能而言)。</p><p id="bfc8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这个问题的一个解决方案是学习正确的转换。这被称为<strong class="kd iu">表示学习</strong>，这正是深度学习算法所做的。</p><p id="42f4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">神奇的是，我们不需要“猜测”哪种转换会产生数据的最佳表示。算法会解决的。</p><p id="4fe7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是，请记住，无论是表示学习还是手工转换，模式都是相同的。我们需要以某种方式改变数据，以便它可以很容易地分离。</p><p id="a5b3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">让我们退一步考虑一个更简单的问题。</p><p id="654a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将探索<strong class="kd iu">费希尔的线性判别式</strong> (FLD)如何设法将多维数据分类到多个类别。但是在我们开始之前，请随意打开这个<a class="ae lf" href="https://github.com/sthalles/fishers-linear-discriminant/blob/master/Fishers_Multiclass.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab 笔记本</a>并跟随。</p><h1 id="d235" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">费希尔线性判别式</h1><p id="2fe8" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">看待分类问题的一种方式是通过降维的镜头。</p><p id="ebfc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，考虑两类分类问题的情况(<strong class="kd iu"> K=2 </strong>)。蓝色和红色点在<strong class="kd iu"> R </strong>。一般来说，我们可以取任意一个 D 维输入向量，并将其向下投影到 D '维。这里，<strong class="kd iu"> D </strong>表示原始输入尺寸，而<strong class="kd iu">D’</strong>是投影空间尺寸。在整篇文章中，认为<strong class="kd iu">D’</strong>小于<strong class="kd iu"> D </strong>。</p><p id="2608" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在投影到一维(数轴)的情况下，即<strong class="kd iu">D’= 1</strong>，我们可以选取一个阈值<strong class="kd iu"> t </strong>在新的空间中进行分类。给定一个输入向量<strong class="kd iu"> x </strong>:</p><ul class=""><li id="5ad8" class="mj mk it kd b ke kf ki kj km ml kq mm ku mn ky mo mp mq mr bi translated">如果预测值<strong class="kd iu"> y &gt; = t </strong>则<strong class="kd iu"> x </strong>属于 C1 类(类 1)。</li><li id="61aa" class="mj mk it kd b ke ms ki mt km mu kq mv ku mw ky mo mp mq mr bi translated">否则，它被归类为 C2(2 类)。</li></ul><p id="2e7b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意向量<strong class="kd iu"> y </strong>(预测)<strong class="kd iu"> </strong>等于输入<strong class="kd iu"> x </strong>和权重<strong class="kd iu">w→y</strong>=<strong class="kd iu">w</strong>ᵀ<strong class="kd iu">x</strong>的线性组合</p><p id="2001" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">以下面的数据集为例。我们想把原来的数据维数从<strong class="kd iu"> D=2 </strong>减少到<strong class="kd iu"> D'=1 </strong>。换句话说，我们需要一个将二维向量映射到一维向量的变换 t—t(<strong class="kd iu">v</strong>)=ℝ→ℝ。</p><p id="a7ec" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">首先，让我们计算两个类的平均向量<strong class="kd iu"> m1 </strong>和<strong class="kd iu"> m2 </strong>。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/5fb6cd48633ca18d82809bd55c3e41ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*Ky__i4qZwyy-WUSF8_wU4A.png"/></div></figure><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/fcabeab066ee5e523009c76fe45e3549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6CxIZBax3YI3h1AvvSoFg.png"/></div></div></figure><p id="c1d3" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">注意，<strong class="kd iu"> N1 </strong>和<strong class="kd iu"> N2 </strong>分别表示 C1 和 C2 班级的分数。现在，考虑使用类均值作为分离的度量。换句话说，我们希望将数据投影到加入 2 类平均值的向量<strong class="kd iu"> W </strong>上。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/84c3808aeeadf498722ab743fec22544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*aVKRo9LPaHGROakZuEc4wg.png"/></div></figure><p id="3dce" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">值得注意的是，任何一种向更小维度的投影都可能会丢失一些信息。在这个场景中，请注意，这两个类在它们的原始空间中是明显可分的(通过一条线)。</p><p id="937b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是，在重新投影后，数据会出现某种类别重叠，如图上的黄色椭圆和下面的直方图所示。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/d53c533380caa641141d37625e3cdc90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*ygJYUt-FURz9KawGQYhWfQ.png"/></div></figure><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/cb03841370e8aa2627adcf646a6ce9c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*3FX4Gdm6P6SO628pmgtvqw.png"/></div></figure><p id="fb6e" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这就是费雪线性判别式发挥作用的地方。</p><p id="b09d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">Fisher 提出的想法是最大化一个函数，该函数将在投影的类均值之间给出一个大的分离，同时在每个类内给出一个小的方差，从而最小化类重叠。</p><p id="50f6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">换句话说，FLD 选择了最大化类别分离的投影。为此，它最大化类间方差与类内方差之比。</p><p id="8276" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">简而言之，为了将数据投影到一个更小的维度并避免类重叠，FLD 维护了两个属性。</p><ul class=""><li id="70ca" class="mj mk it kd b ke kf ki kj km ml kq mm ku mn ky mo mp mq mr bi translated">数据集类之间的巨大差异。</li><li id="a388" class="mj mk it kd b ke ms ki mt km mu kq mv ku mw ky mo mp mq mr bi translated">每个数据集类中的微小差异。</li></ul><p id="964d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">请注意，较大的类间方差意味着预计的类平均值应尽可能远离。相反，小的类内方差具有保持投影数据点彼此更接近的效果。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/a1721feab12949ffdbfc8bd2f49a4f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v8kk5kYy2179bxiV3gPPBA.png"/></div></div></figure><p id="7681" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了找到具有以下属性的投影，FLD 用以下标准学习权重向量<strong class="kd iu"> W </strong>。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/ef465e835b2f5d8cb784e578df900a95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M9GOo_qFMXT1YNwXIzb9nA.png"/></div></div></figure><p id="bf92" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果我们替换等式(1)和(2)给出的平均向量<strong class="kd iu"> m1 </strong>和<strong class="kd iu"> m2 </strong>以及方差<strong class="kd iu"> s </strong>，我们得到等式(3)。如果我们对(3)关于<strong class="kd iu"> W </strong>求导(经过一些简化)，我们得到<strong class="kd iu"> W </strong>的学习方程(方程 4)。</p><p id="9978" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">即<strong class="kd iu"> W </strong>(我们期望的变换)<strong class="kd iu"> </strong>正比于<strong class="kd iu">类内协方差</strong>矩阵的逆乘以类均值的差。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/534cc5ce1038ab8536b338a8d0068068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zlaz-JKRa4F7MSNig28A7g.png"/></div></div></figure><p id="ee7b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">正如预期的那样，结果允许用简单的阈值进行完美的分类分离。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/f460c8d3184cd396a0132d3b1e3c68b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*6kKqxE3YVcVWYF3MjwpPRQ.png"/></div></figure><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi na"><img src="../Images/6ee5afda810feac8895a87795ceb9f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*HK16495yDOpGYFsJCErdUQ.png"/></div></figure><h1 id="0d1b" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">多类 Fisher 线性判别式</h1><p id="7fd3" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">我们可以将 FLD 推广到超过 K 个类别的情况。这里，我们需要类别内和类别间协方差矩阵<strong class="kd iu">和<strong class="kd iu">的推广形式。</strong></strong></p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/dc6d401211e414efbcca0e3fb5fbcd9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ihSnoJLR0lh8wqjJ-MKHmw.png"/></div></div></figure><p id="09f7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于类内协方差矩阵<strong class="kd iu"> SW </strong>，对于每个类，取集中输入值与其转置值之间的矩阵乘法之和。等式 5 和 6。</p><p id="7cc0" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了估计类间协方差<strong class="kd iu"> SB </strong>，对于每个类<strong class="kd iu"> k=1，2，3，…，K </strong>，取局部类均值<strong class="kd iu"> mk </strong>和全局类均值<strong class="kd iu">m</strong>的<strong class="kd iu">外积</strong>，然后乘以类<strong class="kd iu"> k </strong>中记录的数量<strong class="kd iu"/>—等式 7。</p><p id="157b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">FLD 准则的最大化通过矩阵的特征分解来解决，即<strong class="kd iu"> SW </strong>的逆和<strong class="kd iu"> SB </strong>之间的乘法。因此，为了找到权重向量<strong class="kd iu"> W </strong>，我们取对应于其最大特征值的<strong class="kd iu">D’</strong>特征向量(等式 8)。</p><p id="9fed" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">换句话说，如果我们想要将我们的数据维数从<strong class="kd iu"> D=784 </strong>减少到<strong class="kd iu"> D'=2 </strong>，那么变换向量<strong class="kd iu"> W </strong>由对应于<strong class="kd iu"> D'=2 </strong>最大特征值的 2 个特征向量组成。这给出了<strong class="kd iu"> W = (N，D’)</strong>的最终形状，其中<strong class="kd iu"> N </strong>是输入记录的数量，而<strong class="kd iu">D’</strong>是减少的特征空间维度。</p><h1 id="a11f" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">构建线性判别式</h1><p id="dcb8" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">到目前为止，我们仅使用 Fisher 线性判别式作为降维方法。为了真正创建判别式，我们可以在 D 维输入向量<strong class="kd iu"> x </strong>上为每个类别<strong class="kd iu"> K </strong>建模一个<strong class="kd iu">多元高斯分布</strong>,如下所示:</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/86f3f6d02e44939611521e899792d233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A4shtkbvUU8RLnXJ3J14lA.png"/></div></div></figure><p id="7470" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这里<strong class="kd iu"> <em class="ne"> μ </em> </strong>(平均值)是一个 D 维向量。<strong class="kd iu">σ</strong>(sigma)是一个<strong class="kd iu"> DxD </strong>矩阵——协方差矩阵。并且|<strong class="kd iu">∑</strong>|是协方差的行列式。行列式是协方差矩阵<strong class="kd iu">σ</strong>拉伸或收缩空间的程度的度量。</p><p id="cceb" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在 Python 中，看起来是这样的。</p><figure class="la lb lc ld gt ju"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="df77" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">高斯分布的参数:<strong class="kd iu"> <em class="ne"> μ </em> </strong>和<strong class="kd iu"><em class="ne"/>σ，</strong>使用投影输入数据<strong class="kd iu">为每个类别<strong class="kd iu"> k=1，2，3，…，K </strong>计算。</strong>我们可以使用每个类别中训练集数据点的分数来推断先验<em class="ne"> P(Ck) </em>类别概率(第 11 行)。</p><p id="a2c7" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">一旦我们有了高斯参数和先验，我们就可以单独地为每个类<strong class="kd iu"> k=1，2，3，…，K </strong>计算类条件密度<em class="ne">P(</em><strong class="kd iu"><em class="ne">)x</em></strong><em class="ne">| Ck】</em>。为此，我们首先将 D 维输入向量<strong class="kd iu"> x </strong>投影到一个新的<strong class="kd iu">D’</strong>空间。请记住<strong class="kd iu">D’&lt;D</strong>。然后，我们对每个投影点评估等式 9。最后，我们可以利用等式 10 得到每类<strong class="kd iu"> k=1，2，3，…，K </strong>的后验类概率<em class="ne">P(Ck |</em><strong class="kd iu"><em class="ne">x</em></strong><em class="ne">)</em>。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/1b3f1d1eb1e2483ecb919b6131987a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eThiSZ1eGLHDSnlblV-W0A.png"/></div></div></figure><p id="f6c2" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">等式 10 在下面的得分函数的第 8 行进行评估。</p><figure class="la lb lc ld gt ju"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="da25" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">然后，我们可以将输入向量<strong class="kd iu"> x </strong>分配给具有最大后验概率的类别<strong class="kd iu"> k </strong>。</p><h1 id="f4f8" class="lg lh it bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated">对 MNIST 进行测试</h1><p id="41e0" class="pw-post-body-paragraph kb kc it kd b ke me kg kh ki mf kk kl km mg ko kp kq mh ks kt ku mi kw kx ky im bi translated">使用 MNIST 作为玩具测试数据集。如果我们选择将原始输入尺寸<strong class="kd iu"> D=784 </strong>减少到<strong class="kd iu"> D'=2 </strong>，我们可以获得大约<em class="ne"> 56% </em>的测试数据精度。然而，如果我们将投影空间尺寸增加到<strong class="kd iu">D’= 3</strong>，我们将达到接近<em class="ne"> 74% </em>的精确度。这两个投影也使结果特征空间更容易可视化。</p><figure class="la lb lc ld gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/8a670f075af461b2e98fc3d91fb407b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxfQmK5xIW-snl26vKPNeQ.png"/></div></div></figure><figure class="la lb lc ld gt ju gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/11d907d68cfcdf094af4b17ea54750b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*n3Le7S8gf9bs9PFEUG0PBQ.png"/></div></figure><p id="0961" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">这篇文章的一些要点。</p><ul class=""><li id="50d8" class="mj mk it kd b ke kf ki kj km ml kq mm ku mn ky mo mp mq mr bi translated">Fisher 的线性判别式，本质上是一种降维的技术，而不是判别式。</li><li id="2851" class="mj mk it kd b ke ms ki mt km mu kq mv ku mw ky mo mp mq mr bi translated">对于二元分类，我们可以找到一个最优阈值<strong class="kd iu"> t </strong>并据此对数据进行分类。</li><li id="ce22" class="mj mk it kd b ke ms ki mt km mu kq mv ku mw ky mo mp mq mr bi translated">对于多类数据，我们可以(1)使用高斯分布对类条件分布建模。(2)求先验类概率<em class="ne">P(Ck)</em>,( 3)用<strong class="kd iu">贝叶斯</strong>求后验类概率<em class="ne"> p(Ck|x) </em>。</li><li id="67ca" class="mj mk it kd b ke ms ki mt km mu kq mv ku mw ky mo mp mq mr bi translated">为了找到投影输入数据的最佳方向，Fisher 需要监督数据。</li><li id="4bc7" class="mj mk it kd b ke ms ki mt km mu kq mv ku mw ky mo mp mq mr bi translated">给定一个维度为<strong class="kd iu"> D </strong>的数据集，我们最多可以将它向下投影到<strong class="kd iu"/><strong class="kd iu">D’</strong>等于<strong class="kd iu"> D-1 </strong>维度。</li></ul><p id="7df8" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">本文基于<a class="ae lf" href="http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf" rel="noopener ugc nofollow" target="_blank">模式识别与机器学习</a>的<strong class="kd iu">第 4.1 章</strong>。克里斯托弗·毕晓普的书。</p><p id="cd97" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated"><strong class="kd iu">感谢阅读。</strong></p></div></div>    
</body>
</html>