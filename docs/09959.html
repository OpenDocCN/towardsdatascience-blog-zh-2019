<html>
<head>
<title>NLP 102: Negative Sampling and GloVe</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 102:阴性取样和手套</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68?source=collection_archive---------3-----------------------#2019-12-30">https://towardsdatascience.com/nlp-101-negative-sampling-and-glove-936c88f3bc68?source=collection_archive---------3-----------------------#2019-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bdfe" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让 Word2Vec 变得越来越好。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/c95a72973e0bf421b40afacf7eb37ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NZ749mTZSGrMJy3j"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@sortino?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Joshua Sortino</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="801d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从语料库中生成高质量单词嵌入的一种方法是使用<a class="ae kv" rel="noopener" target="_blank" href="/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314"> Word2Vec — CBOW 或 Skip-gram 模型</a>。这两种模式有一些共同点:</p><ul class=""><li id="18b2" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">训练样本由一对基于出现的接近度选择的单词组成。</li><li id="5cbb" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">网络的最后一层是 softmax 函数。</li></ul><h2 id="d3c9" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">CBoW/Skip-gram 的问题</h2><p id="3886" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">首先，对于每个训练样本，<strong class="ky ir">只有对应于目标词的权重可能得到显著更新</strong>。在训练神经网络模型时，在每次反向传播过程中，我们都尝试更新隐藏层中的所有权重。对应于非目标单词的权重将接收到微小的变化或根本没有变化，即在每一遍中，我们仅进行非常稀疏的更新。</p><p id="32f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其次，对于每一个训练样本，<strong class="ky ir">使用 softmax 计算最终概率是一个非常昂贵的操作</strong>，因为它涉及对我们词汇表中所有单词的分数求和以进行标准化。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/c99d33526c752b62d0b8861ac78d4c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/0*ua9uy-jZPw0uHD62"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The softmax function.</figcaption></figure><p id="64e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，对于每个训练样本，我们执行一个昂贵的操作来计算单词的概率，这些单词的权重甚至可能不会更新，或者更新得如此之少，以至于不值得额外的开销。</p><p id="320b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了克服这两个问题，我们尝试减少为每个训练样本更新的权重数量，而不是强行创建训练样本。</p><h1 id="1cd9" class="nf mh iq bd mi ng nh ni ml nj nk nl mo jw nm jx mr jz nn ka mu kc no kd mx np bi translated">负采样</h1><p id="7392" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">负采样<strong class="ky ir">允许我们只修改一小部分权重，而不是每个训练样本的所有权重</strong>。我们通过稍微修改我们的问题来做到这一点。我们不是试图预测词汇表中所有单词的邻近单词的概率，而是试图预测我们的训练样本单词是否是邻近单词的概率。参考我们之前的例子(<em class="nq">桔子</em>，<em class="nq">果汁</em>)，我们并不试图预测果汁成为邻近词的概率，即 P( <em class="nq">果汁</em> | <em class="nq">桔子</em>)，我们试图通过计算 P(1| &lt; <em class="nq">桔子</em>，<em class="nq">果汁</em>)来预测(<em class="nq">桔子</em>，<em class="nq">果汁</em>)是否是邻近词</p><p id="2254" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们现在已经把它变成了 10，000 个二进制分类问题，而不是一个巨大的 soft max——在 10，000 个类别中分类。</p><p id="52a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们通过随机选择少量“负面”词<strong class="ky ir"><em class="nq"/></strong>(一个超参数，假设为 5)来更新权重，从而进一步简化问题。(在这种情况下,“否定”单词是我们希望网络输出 0 的单词)。</p><p id="bc68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的训练样本(<em class="nq">橘子</em>，<em class="nq">果汁</em>，我们将取五个词，说<em class="nq">苹果</em>，<em class="nq">晚餐</em>，<em class="nq">狗</em>，<em class="nq">椅子，房子</em>作为负样本。对于这个特定的迭代，我们将只计算<em class="nq">果汁、苹果、晚餐、狗、椅子、房子的概率。</em>因此，损失将只为它们传播回来，因此只有对应于它们的权重将被更新。</p><h2 id="89d9" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">目标函数</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/9025a08ca93ba5bd024ca0d3bfc39f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HqQmyokPsmwYuna7LRVyMA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Overall Objective function in Skip-gram and Negative Sampling. Here sigmoid = 1/(1+exp(x)), t is the time step and theta are the various variables at that time step, all the U and V vectors.</figcaption></figure><p id="e56d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第一项试图最大化位于上下文窗口中的实际单词的出现概率，</strong>即它们共现。而第二项，<strong class="ky ir">试图迭代一些不在窗口中的随机单词<em class="nq"> j </em>，并最小化它们同现的概率</strong>。</p><p id="3dfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们根据随机单词出现的频率对它们进行采样。P(w) = U(w)的 3/4 次方，其中 U(w)是一个<a class="ae kv" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank">单图</a>分布。3/4 次幂使得不太频繁的单词被更频繁地采样，如果没有它，采样诸如“The”、“is”等频繁单词的概率将比诸如“斑马”、“大象”等单词高得多。</p><p id="3353" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们上面提到的例子中，我们试图最大化概率 P(1| &lt;<em class="nq">桔子</em>，<em class="nq">果汁</em> &gt;)和最大化(因为在我们的目标函数中它前面有一个负号，所以当我们将选择最大值时，我们将鼓励它们不要发生)我们的负样本的概率 P(1| &lt; <em class="nq">桔子，苹果</em> &gt;)，P(1| &lt; <em class="nq">桔子，晚餐</em> &gt;)，P(1</p><p id="1731" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于大型数据集，选择一个小的值<em class="nq"> k </em>，大约在 2 到 5 之间。而对于较小的数据集，相对较大的值是优选的，大约 5 到 20。</p><h1 id="7119" class="nf mh iq bd mi ng nh ni ml nj nk nl mo jw nm jx mr jz nn ka mu kc no kd mx np bi translated">子采样</h1><p id="6912" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">语料库中单词的分布是不均匀的。有些词比另一些词出现得更频繁。诸如“the”“is”“are”等词出现得如此频繁，以至于在训练模型时省略一些实例不会影响其最终嵌入。此外，它们的大多数出现并没有告诉我们它的上下文含义。</p><p id="62ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在二次抽样中，我们通过限制样本出现的频率来限制一个词的样本数量。对于频繁出现的单词，我们会删除一些作为相邻单词和输入单词的实例。</p><h1 id="3885" class="nf mh iq bd mi ng nh ni ml nj nk nl mo jw nm jx mr jz nn ka mu kc no kd mx np bi translated">性能考虑因素</h1><p id="b400" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在多 CPU 机器上使用并行训练可以显著减少 word2Vec 的训练时间。超参数的选择对性能(速度和精度)至关重要，但不同的应用会有所不同。要做出的主要选择是:</p><ul class=""><li id="4e52" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nq">架构</em> </strong> : skip-gram(较慢，对不常用词更好)vs CBOW(快)。</li><li id="7a22" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nq">训练算法</em> </strong>:分级 softmax(对不常用词更好)vs 负采样(对常用词更好，对低维向量更好)。</li><li id="7c13" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"><em class="nq"/></strong>常用词的子采样:可以提高大型数据集的精度和速度(有用值在 1e-3 到 1e-5 范围内)。</li><li id="d538" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nq">词向量的维度</em> </strong>:通常越多越好，但也不尽然。</li><li id="0df5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir"> <em class="nq">上下文(窗口)大小</em> </strong>:对于 skip-gram 通常在 10 左右，对于 CBOW 在 5 左右。</li></ul><p id="ffcf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">几乎所有使用的目标函数都是凸的，所以初始化很重要。在实践中，使用小随机数来初始化单词嵌入会产生好的结果。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="962c" class="nf mh iq bd mi ng nz ni ml nj oa nl mo jw ob jx mr jz oc ka mu kc od kd mx np bi translated">基于计数的方法</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b2fc889c55332959b498951c40ed7faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*PNlYsZX3dMxpEYbZVfgXJA.png"/></div></figure><p id="7561" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 word2vec 中，我们本质上要做的是捕获单词的共现，但是一次一个滑动窗口。如果我们在一个矩阵中一次性捕捉到 co 出现的频率，会怎么样？这将在计算上比训练神经网络更简单和容易。使用这种方法，问题有点类似于我们面对的一次性表示(在<a class="ae kv" rel="noopener" target="_blank" href="/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314">以前的帖子</a>中讨论过)。虽然我们在一定程度上解决了捕获单词间相似性的问题，但其他维度问题仍然存在。</p><p id="0073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总而言之，通过使用<strong class="ky ir">基于窗口的方法</strong>来学习我们的嵌入，我们<strong class="ky ir">能够捕获复杂的模式</strong>而不仅仅是单词相似度，但是我们<strong class="ky ir">对统计数据</strong>的使用效率很低。通过使用纯粹基于计数的方法，我们使统计数据得到了有效的利用，但是当我们将这些嵌入应用到外部任务时，这些复杂的潜在模式非常有用。</p><h1 id="090c" class="nf mh iq bd mi ng nh ni ml nj nk nl mo jw nm jx mr jz nn ka mu kc no kd mx np bi translated">手套</h1><p id="02f2" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">它是基于计数和基于窗口的模型的混合<strong class="ky ir">。GloVe 的优势在于，与 Word2vec 不同，GloVe 不仅仅依赖于局部统计(单词的局部上下文信息，基于窗口的模型)，而是融入了全局统计(单词共现，基于计数的模型)来获取单词向量。</strong></p><h2 id="be63" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">目标函数</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/cd2ee5022aefcaec70f617e32045c262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-eyUyHtE4aKSQkPZO-iCw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Objective function for the GloVe model.</figcaption></figure><p id="0937" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">对于每一对可能同时出现的单词<em class="nq"> i </em>和<em class="nq"> j </em>，我们试图最小化它们单词嵌入的内积与<em class="nq"> i </em>和<em class="nq"> j </em>的对数之差。术语</strong>f(P<em class="nq">ij)</em>允许我们降低一些非常频繁的共现的权重，并限制非常频繁的单词的重要性。</p><p id="c9c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里的<em class="nq"> u </em>和<em class="nq"> v </em>向量是可以互换的。我们在这里使用两组不同的向量，因为它在优化时提供了更多的稳定性。如果我们只使用一组向量，我们将执行一个与它本身的内积，我们将有一个表现不太好的目标函数。在我们的最终评估中，我们将通过执行元素相加来简单地将两者结合起来。</p><h2 id="fb50" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated">优势</h2><ul class=""><li id="b40d" class="ls lt iq ky b kz mz lc na lf og lj oh ln oi lr lx ly lz ma bi translated">快速训练:与基于窗口的方法不同，在基于窗口的方法中，我们过去常常一次优化一个窗口(可能多次对相同的同现进行训练)，在 GloVe 中，我们一次只优化一个计数。</li><li id="1760" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">可扩展:在庞大的语料库中，罕见的单词不会经常出现，但 GloVe 允许我们捕捉语义，而不管单词出现的次数。</li><li id="5dd5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">有效使用统计数据:这有助于模型在小语料库和小向量上表现良好。</li></ul><h1 id="4908" class="nf mh iq bd mi ng nh ni ml nj nk nl mo jw nm jx mr jz nn ka mu kc no kd mx np bi translated">突出</h1><h2 id="42d4" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">最近的邻居</strong></h2><p id="a966" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">两个单词向量之间的欧几里德距离(或余弦相似度)提供了一种测量相应单词的语言或语义相似度的有效方法。</p><h2 id="ecd3" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">线性子结构</strong></h2><p id="7777" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">这种计算两个词之间相似性的简单性可能是有问题的，因为两个给定的词几乎总是表现出比单个数字更复杂的关系。比如<em class="nq">男</em>可能被认为和<em class="nq">女</em>相似，两个词都是描述人类的；另一方面，在现实世界中，我们知道不应该认为男人=女人。这种差异不仅是生物学上的，也是社会学上的。{性别平等仍然是许多人渴望的理想，却没有人去实现}。为了以定量的方式捕捉区分<em class="nq">男人</em>和<em class="nq">女人</em>的细微差别，模型有必要将一个以上的数字与单词对相关联。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="69bf" class="nf mh iq bd mi ng nz ni ml nj oa nl mo jw ob jx mr jz oc ka mu kc od kd mx np bi translated">参考</h1><ul class=""><li id="8a6b" class="ls lt iq ky b kz mz lc na lf og lj oh ln oi lr lx ly lz ma bi translated"><a class="ae kv" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">单词和短语的分布式表示及其组合性</a></li><li id="d65c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank"> Chris McCormick Word2Vec 教程第 2 部分—负采样</a></li><li id="0aab" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">斯坦福手套博客</a></li><li id="823e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">谷歌:Word2Vec </li></ul><h1 id="6f13" class="nf mh iq bd mi ng nh ni ml nj nk nl mo jw nm jx mr jz nn ka mu kc no kd mx np bi translated">我认为你会喜欢:D 的其他文章</h1><ul class=""><li id="c09a" class="ls lt iq ky b kz mz lc na lf og lj oh ln oi lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/back-propagation-721bfcc94e34">是的，你应该听听安德烈·卡帕西的话，了解一下反向传播</a></li><li id="366b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/evaluation-of-an-nlp-model-latest-benchmarks-90fd8ce6fae5">NLP 模型评估—最新基准</a></li><li id="a03b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/attaining-attention-in-deep-learning-a712f93bdb1e">在深度学习中获得注意力</a></li></ul></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><blockquote class="oj ok ol"><p id="6513" class="kw kx nq ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">我很高兴你坚持到了这篇文章的结尾。<em class="iq">🎉我希望你的阅读体验和我写这篇文章时一样丰富。<em class="iq">💖</em></em></p><p id="3e34" class="kw kx nq ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">请在这里查看我的其他文章。</p><p id="d87c" class="kw kx nq ky b kz la jr lb lc ld ju le om lg lh li on lk ll lm oo lo lp lq lr ij bi translated">如果你想联系我，我会选择推特。</p></blockquote></div></div>    
</body>
</html>