<html>
<head>
<title>Everything you need to know about “Activation Functions” in Deep learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于深度学习模型中的“激活函数”，你需要知道的一切</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253?source=collection_archive---------0-----------------------#2019-12-30">https://towardsdatascience.com/everything-you-need-to-know-about-activation-functions-in-deep-learning-models-84ba9f82c253?source=collection_archive---------0-----------------------#2019-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="c654" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章是你的<strong class="jp ir">一站式解决方案，可以解决你脑海中出现的与深度学习模型中使用的激活函数相关的每个可能的问题</strong>。<strong class="jp ir">这些基本上是我关于激活函数的笔记，以及我对这个话题的所有知识都集中在一个地方</strong>。所以，不做任何不必要的介绍，让我们直接进入正题。</p><h1 id="524f" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">内容</h1><ol class=""><li id="30f3" class="lj lk iq jp b jq ll ju lm jy ln kc lo kg lp kk lq lr ls lt bi translated"><strong class="jp ir">什么是激活功能</strong>，它在<strong class="jp ir">网络</strong>中做什么？</li><li id="5479" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated"><strong class="jp ir">为什么</strong>需要它而<strong class="jp ir">为什么不用线性函数代替</strong>？</li><li id="5864" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">激活功能的理想特征是什么？</li><li id="f3ee" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">使用各种非线性激活</li><li id="2766" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated"><strong class="jp ir">值得注意的</strong>来自<strong class="jp ir">最新研究</strong>的非线性激活</li><li id="19f3" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated"><strong class="jp ir">如何(和哪个)在深度神经网络中使用</strong>它们</li></ol><h1 id="b52c" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">什么是激活函数？</h1><p id="e740" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">简单地说，激活函数是添加到人工神经网络中的函数，以便帮助<strong class="jp ir">网络学习数据</strong>中的复杂模式。当与我们大脑中基于神经元的模型进行比较时，激活功能最终决定向下一个神经元发出什么信号。这也正是激活函数在人工神经网络中的作用。<strong class="jp ir">它接收前一个单元的输出信号，并将其转换成某种形式，可以作为下一个单元的输入</strong>。下图总结了这种比较。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/e696ee95151c90793bd7bb4dec6182fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BMSfafFNEpqGFCNU4smPkg.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Source: cs231n by Stanford</figcaption></figure><h1 id="a569" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">为什么需要它？</h1><p id="71a1" class="pw-post-body-paragraph jn jo iq jp b jq ll js jt ju lm jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">在网络中具有非线性激活函数有多种原因。</p><ol class=""><li id="cca4" class="lj lk iq jp b jq jr ju jv jy ms kc mt kg mu kk lq lr ls lt bi translated">除了前面讨论的生物相似性，它们还有助于根据我们的要求，将神经元的输出值限制在一定的范围内。这一点很重要，因为激活函数的输入是<strong class="jp ir"> W*x + b </strong>，其中<strong class="jp ir"> W </strong>是单元的重量，而<strong class="jp ir"> x </strong>是输入，然后有偏置<strong class="jp ir"> b </strong>加到其上。如果不受某个限制，这个值可以非常大，特别是在具有数百万个参数的非常深的神经网络的情况下。这将导致计算问题。例如，有一些激活函数(如 softmax)为不同的输入值(0 或 1)输出特定的值。</li><li id="bf4b" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated">激活函数中最重要的特征是其将非线性加入神经网络的能力。为了理解这一点，让我们考虑如下图所示的多维数据:</li></ol><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4d12652b8dd5f0ca7eda9fd949007185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*Ll3CR4irXqQioc4jTsQHzQ.png"/></div></figure><p id="b7c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用三个特征(图中的体重、收缩压和年龄)的线性分类器可以给我们一条穿过三维空间的线，但它永远无法准确地学习使一个人成为吸烟者或不吸烟者的模式(手头的分类问题)，因为定义这种分类的模式仅仅是<strong class="jp ir">非线性的。人工神经网络出现了。如果我们使用具有单个细胞但没有激活功能的人工神经网络会怎样。因此，我们的输出基本上是<strong class="jp ir"> W*x + b. </strong>，但这是不好的，因为<strong class="jp ir"> W*x 也有 1 度</strong>，因此是线性的，而<strong class="jp ir">这基本上等同于线性分类器。</strong></strong></p><p id="9387" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们堆叠多层。让我们将 nᵗʰ图层表示为 fₙ(x).的函数所以我们有:</p><p id="2da7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> o(x) = fₙ(fₙ₋₁(….f₁(x)</strong></p><p id="daa8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，这也不够复杂，尤其是对于具有非常高的模式的问题，例如在计算机视觉或自然语言处理中面临的问题。</p><p id="e3ce" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了使模型获得学习非线性模式的能力(也称为更高程度的复杂性)，在它们之间添加了特定的非线性层(激活函数)。</p><h1 id="07f2" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">激活功能的理想特征</h1><ol class=""><li id="647d" class="lj lk iq jp b jq ll ju lm jy ln kc lo kg lp kk lq lr ls lt bi translated"><strong class="jp ir">消失梯度问题:</strong>使用过程梯度下降来训练神经网络。梯度下降包括反向传播步骤，该步骤基本上是链式法则，以获得权重的变化，从而减少每个时期后的损失。考虑一个两层网络，第一层表示为 f₁(x，第二层表示为 f₂(x).整个网络是 o(x) = f₂(f₁(x)).如果我们在反向传递过程中计算权重，我们得到 o`(x) = f₂(x)*f₁`(x).这里 f₁(x)本身是由<em class="mw">动作</em> (W₁*x₁ + b₁)组成的复合函数，其中<em class="mw">动作</em>是层 1 之后的激活函数。再次应用链式法则，我们清楚地看到 f₁`(x) = <em class="mw">行为</em> (W₁*x₁ + b₁)*x₁，这意味着它也直接依赖于激活值。现在想象这样一个链式法则在反向传播时经过多层。如果<em class="mw"> Act </em>()的值在 0 和 1 之间，那么几个这样的值将相乘以计算初始层的梯度。这降低了初始层的梯度值，并且这些层不能正确地学习。换句话说，由于网络的深度和激活将值移至零，它们的梯度趋于消失。这被称为<strong class="jp ir">消失梯度问题</strong>。所以我们希望我们的激活函数不会将梯度移向零。</li><li id="8b28" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated"><strong class="jp ir">以零为中心:</strong>激活函数的输出应该在零处对称，这样梯度就不会移动到特定的方向。</li><li id="c5cc" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated"><strong class="jp ir">计算开销</strong>:激活函数在每一层之后应用，在深度网络中需要计算上百万次。因此，它们的计算成本应该很低。</li><li id="6920" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk lq lr ls lt bi translated"><strong class="jp ir">可微分:</strong>如上所述，使用梯度下降过程训练神经网络，因此模型中的层需要可微分或至少部分可微分。<strong class="jp ir">这是一个功能作为激活功能层工作的必要条件。</strong></li></ol><h1 id="8ea4" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">使用各种非线性激活</h1><ul class=""><li id="5325" class="lj lk iq jp b jq ll ju lm jy ln kc lo kg lp kk mx lr ls lt bi translated"><strong class="jp ir">乙状结肠:</strong>乙状结肠的定义是:</li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi my"><img src="../Images/92069ff40994e8f2fc6477b644e64c04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hYXbd20tIReMJ3T5D4OZLg.png"/></div></div></figure><p id="2712" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">此激活功能仅出于历史原因，从未在实际模型中使用过。</strong>计算量很大，会导致消失梯度问题，并且不是以零为中心的。这种方法一般用于二分类问题。</p><ul class=""><li id="0c9d" class="lj lk iq jp b jq jr ju jv jy ms kc mt kg mu kk mx lr ls lt bi translated"><strong class="jp ir">软最大值</strong>:软最大值是乙状结肠的一种更一般化的形式。用于<strong class="jp ir">多类分类问题</strong>。与 sigmoid 类似，它产生 0–1 范围内的值，因此被用作分类模型中的最后一层。</li><li id="8dea" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk mx lr ls lt bi translated"><strong class="jp ir">Tanh:</strong>Tanh 定义为:</li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/3820537b9423fbe8adb472855b7f1c0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*IUc4KQ4L6ZWMfvzkNpNlag.png"/></div></figure><p id="32f5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你把它比作 sigmoid，它只解决了一个以零为中心的问题。</p><ul class=""><li id="02c8" class="lj lk iq jp b jq jr ju jv jy ms kc mt kg mu kk mx lr ls lt bi translated"><strong class="jp ir"> ReLU </strong> : ReLU <strong class="jp ir">(整流线性单元)</strong>定义为<strong class="jp ir"> f(x) = max(0，x): </strong></li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a4d7c2e5068b7ccd27b579a56645f09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*nLGwhQGJRDOnQvluaX-WiQ.png"/></div></figure><p id="d1d8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个广泛使用的激活函数，尤其是对于卷积神经网络。它易于计算，不会饱和，也不会导致梯度消失的问题。它只有一个问题，那就是不以零为中心。它受到<strong class="jp ir">“垂死的热路”</strong>问题的困扰。因为所有负输入的输出都是零。导致一些节点完全死亡，什么也学不到。</p><p id="1ad6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">ReLU 的另一个问题是激活爆炸，因为它的上限是 inf。这有时会导致节点不可用。</p><ul class=""><li id="97ed" class="lj lk iq jp b jq jr ju jv jy ms kc mt kg mu kk mx lr ls lt bi translated"><strong class="jp ir">泄漏 ReLU 和参数 ReLU </strong>:定义为<strong class="jp ir"> f(x) = max(αx，x) </strong></li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2f0e9f654a9fe9032327ada91ff65580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*scA-bQ597yTXJ-417RETAQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">the figure is for α = 0.1</figcaption></figure><p id="9442" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里α是一个超参数，一般设置为<strong class="jp ir"> 0.01 </strong>。显然，漏 ReLU 在一定程度上解决了<strong class="jp ir">“将死 ReLU”</strong>的问题。注意，如果我们将α设置为 1，那么泄漏 ReLU 将变成线性函数 f(x) = x，并且将是无用的。因此，<strong class="jp ir"> α的值永远不会接近 1。</strong>如果我们分别为每个神经元设置<strong class="jp ir"> α </strong>作为超参数，我们得到<strong class="jp ir">参数 ReLU </strong>或<strong class="jp ir"> PReLU </strong>。</p><ul class=""><li id="46ef" class="lj lk iq jp b jq jr ju jv jy ms kc mt kg mu kk mx lr ls lt bi translated"><strong class="jp ir"> ReLU6 </strong>:基本上是正向的 ReLU 限制，定义为<strong class="jp ir"> f(x) = min(max(0，x)，6) </strong></li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/e8ebda887fa5b0f29aa7673c84beda1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*7BV9JmXkw9AG6S0dZfyrUw.png"/></div></figure><p id="6a40" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这有助于停止激活爆炸，从而停止梯度爆炸(去 inf ),以及正常 ReLUs 发生的另一个小问题。</p><p id="18f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一个人想到的想法是，为什么不将 ReLU6 和 LeakyReLU 结合起来，以解决我们在以前的激活功能中遇到的所有已知问题。流行的 DL 框架没有提供这种激活功能的实现，但我认为这是一个好主意。</p><h1 id="8f39" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">值得注意的</strong>非线性激活来自<strong class="ak">最新研究</strong></h1><ul class=""><li id="c2ae" class="lj lk iq jp b jq ll ju lm jy ln kc lo kg lp kk mx lr ls lt bi translated"><strong class="jp ir"> Swish </strong>:这是 Ramachandran 等人在 2017 年提出的，定义为<strong class="jp ir"> f(x) = x*sigmoid(x) </strong>。</li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/331c0f8b70470e26fbda7cf2c485129f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*tcMipRvB37hTLEM0baYsqg.png"/></div></figure><p id="5b8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">与 ReLU 相比，它的性能稍好，因为它的图形与 ReLU 非常相似。但是，因为它不会像 ReLU 在 x = 0 时那样在某一点突然改变，所以在训练时更容易收敛。</p><p id="7aad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，Swish 的缺点是计算量很大。为了解决这个问题，我们来到了 Swish 的下一个版本。</p><ul class=""><li id="cda1" class="lj lk iq jp b jq jr ju jv jy ms kc mt kg mu kk mx lr ls lt bi translated"><strong class="jp ir">硬切换或 H 切换</strong>:这被定义为:</li></ul><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/43fd760c3efb81240e05a7988a942170.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*Mk7jTfv13SiXDyAl3ncYUQ.png"/></div></figure><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/e3b83877318b7216072f1b5fa714c7ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*S6ejo08UC5Ms3JK5qmascg.png"/></div></figure><p id="f04d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最好的部分是，它几乎类似于 swish，但它的计算成本更低，因为它用 ReLU(线性类型)代替了 sigmoid(指数函数)。</p><h1 id="eb93" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">如何在深度神经网络中使用</strong><strong class="ak"/>？</h1><ul class=""><li id="423e" class="lj lk iq jp b jq ll ju lm jy ln kc lo kg lp kk mx lr ls lt bi translated"><strong class="jp ir"> Tanh 和 sigmoid 导致巨大的消失梯度问题</strong>。因此，不应使用它们。</li><li id="3792" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk mx lr ls lt bi translated"><strong class="jp ir">从你网络中的 ReLU 开始</strong>。激活层是在重量层(类似 CNN、RNN、LSTM 或线性致密层)之后添加的，如上文所述。如果你认为模型已经停止学习，那么你可以用一个 LeakyReLU 来代替它，以避免垂死的 ReLU 问题。然而，泄漏的 ReLU 会稍微增加计算时间。</li><li id="9a7a" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk mx lr ls lt bi translated"><strong class="jp ir">如果您的网络中也有批量定额层，则在激活功能生成 CNN-批量定额- <em class="mw">行为</em> </strong> <em class="mw">之前添加。虽然批处理规范和激活函数的顺序是一个有争议的话题，有些人说顺序无关紧要，但我使用上面提到的顺序只是为了遵循最初的批处理规范论文。</em></li><li id="121e" class="lj lk iq jp b jq lu ju lv jy lw kc lx kg ly kk mx lr ls lt bi translated">激活函数在其默认的超参数中工作得最好，这些超参数在流行的框架中使用，如 Tensorflow 和 Pytorch。然而，可以调整<strong class="jp ir"> LeakyReLU 中的负斜率，将其设置为 0.02 </strong>以加快学习。</li></ul><p id="93c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那是所有的乡亲们😃</p><p id="f6ed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我已经尝试解决每一个与激活功能相关的问题，但是，如果我错过了什么，请在下面评论。</p><p id="ece5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">可以在我的</strong> <a class="ae ng" href="https://github.com/vandit15" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir"> Github </strong> </a> <strong class="jp ir">上看更多深度学习相关的东西，关注我的</strong><a class="ae ng" href="https://www.linkedin.com/in/vandit-jain15/" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir">Linkedin</strong></a><strong class="jp ir">。</strong></p><p id="1232" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">我之前的一些文章:</strong></p><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-auto-deeplab-googles-latest-on-segmentation-181425d17cd5"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">关于 Auto-Deeplab 你需要知道的一切:谷歌关于细分的最新消息</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">搜索图像分割模型</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny mm nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/everything-you-need-to-know-about-mobilenetv3-and-its-comparison-with-previous-versions-a5d5e5a6eeaa"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">关于 MobileNetV3 及其与以前版本的比较，您需要了解的一切</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">论文综述:寻找 MobilenetV3，ICCV 19</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nz l nv nw nx nt ny mm nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/self-supervised-gans-using-auxiliary-rotation-loss-60d8a929b556"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">使用辅助旋转损耗的自监督 GANs</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">弥合有监督和无监督图像生成之间的差距</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="oa l nv nw nx nt ny mm nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">使用专门为其制作的损失来处理类不平衡数据</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">通过添加大约 10 行代码，在严重的类不平衡数据上获得超过 4%的准确性提升。</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="ob l nv nw nx nt ny mm nk"/></div></div></a></div></div></div>    
</body>
</html>