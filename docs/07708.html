<html>
<head>
<title>Unsupervised Learning: Image Compression In 10 Lines of R Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习:10 行 R 代码中的图像压缩</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/image-compression-in-10-lines-of-r-code-7d7a8578d3bb?source=collection_archive---------27-----------------------#2019-10-25">https://towardsdatascience.com/image-compression-in-10-lines-of-r-code-7d7a8578d3bb?source=collection_archive---------27-----------------------#2019-10-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b497" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习:无监督学习</h2><div class=""/><div class=""><h2 id="24ca" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用主成分分析压缩图像的酷方法</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/1a0d6cb641dbfbe8f4ac5ed460b69340.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DXMVuzJ1H8_iQaoaLDy2Cg.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@gaetanocessati?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Gaetano Cessati</a> on <a class="ae lh" href="https://unsplash.com/s/photos/image-compress?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c153" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">主成分分析(PCA)是一种强大的机器学习工具。作为一种无监督的学习技术，它擅长于降维和特征提取。</p><blockquote class="me"><p id="6eb8" class="mf mg it bd mh mi mj mk ml mm mn md dk translated">但是，你知道我们可以用 PCA 来压缩图像吗？</p></blockquote><p id="c2ba" class="pw-post-body-paragraph li lj it lk b ll mo kd ln lo mp kg lq lr mq lt lu lv mr lx ly lz ms mb mc md im bi translated">在这篇文章中，我将介绍这个过程，并解释 PCA 如何用 10 行 R 代码压缩图像，最后描述了简单的数学运算。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mt"><img src="../Images/18f7d21fa72db8955e97d998bb385093.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOeT2oOoms15Sf7uh9pBfQ.jpeg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Photo by <a class="ae lh" href="https://unsplash.com/@pietrozj?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Pietro Jeng</a> on <a class="ae lh" href="https://unsplash.com/s/photos/animal?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="9e8d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> #安装包并加载库</strong></p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="ae5c" class="ng nh it nc b gy ni nj l nk nl">#install.packages(“tidyverse”)<br/>#install.packages(“gbm”)<br/>#install.packages(“e1071”)<br/>#install.packages(“imager”)<br/>library(tidyverse)<br/>library(tree) <br/>library(randomForest) <br/>library(gbm) <br/>library(ROCR) <br/>library(e1071) <br/>library(imager)</span><span id="007c" class="ng nh it nc b gy nm nj l nk nl"># load the dataset. This is a 100*100*1000 array of data. An array is a generalization of a matrix to more than 2 dimensions. The first two dimensions index the pixels in a 100*100 black and white image of a face; the last dimension is the index for one of 1000 face images. The dataset can be accessed at: <a class="ae lh" href="https://cyberextruder.com/face-matching-data-set-download/" rel="noopener ugc nofollow" target="_blank">https://cyberextruder.com/face-matching-data-set-download/</a>.</span><span id="0c98" class="ng nh it nc b gy nm nj l nk nl">load(“faces_array.RData”)</span><span id="7ac2" class="ng nh it nc b gy nm nj l nk nl">#PAC requires a single matrix. so, we need to transform the 100*100 matrix into a single vector (10,000). </span><span id="e224" class="ng nh it nc b gy nm nj l nk nl">face_mat &lt;- sapply(1:1000, function(i) as.numeric(faces_array[, , i])) %&gt;% t</span><span id="cf0e" class="ng nh it nc b gy nm nj l nk nl"># To visualize the image, we need a matrix. so, let's convert 10000 dimensional vector to a matrix<br/>plot_face &lt;- function(image_vector) { <br/> plot(as.cimg(t(matrix(image_vector, ncol=100))), axes=FALSE, asp=1)<br/> }</span><span id="07b1" class="ng nh it nc b gy nm nj l nk nl">plot_face(face_mat[, sample(1000, 1)])</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/3fb5b5c65c19cc7cc65a210b6e7fb9d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I1VM6sXqp8Da8-enWOzejA.png"/></div></div></figure><p id="54a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们试图获得数据集的基本信息，并构造一个新的函数进行分析。</p><p id="54b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">#查平均脸</strong></p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="875e" class="ng nh it nc b gy ni nj l nk nl">face_average = colMeans(face_mat)</span><span id="2983" class="ng nh it nc b gy nm nj l nk nl">plot_face(face_average)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/20e6052a31dda2618e0881e88917bd5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*02l-4gu9wqb1X8Ayq80pkg.png"/></div></figure><p id="9dba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很大程度上，我们可以把“平均脸”理解为其他图像的基线。通过在平均脸上加上或减去数值，我们可以得到其他的脸。</p><p id="9e10" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">#以上代码不算 10 行的限制。# </strong></p><p id="2b71" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> #这是我们的 10 行代码# </strong></p><pre class="ks kt ku kv gt nb nc nd ne aw nf bi"><span id="197f" class="ng nh it nc b gy ni nj l nk nl"># generate PCA results;<br/># scale=TRUE and center=TRUE --&gt; mean 0 and variance 1<br/>pr.out = prcomp(face_mat,center=TRUE, scale=FALSE)</span><span id="1dc3" class="ng nh it nc b gy nm nj l nk nl"># pr.out$sdev: the standard deviations of the principal components; <br/># (pr.out$sdev)²: variance of the principal components<br/>pr.var=(pr.out$sdev)² </span><span id="b673" class="ng nh it nc b gy nm nj l nk nl"># pve: variance explained by the principal component<br/>pve = pr.var/sum(pr.var) </span><span id="91f8" class="ng nh it nc b gy nm nj l nk nl"># cumulative explained variance<br/>cumulative_pve &lt;-cumsum(pve)</span><span id="02f3" class="ng nh it nc b gy nm nj l nk nl"><strong class="nc jd">#see the math explanation attached in the end<br/></strong>U = pr.out$rotation<br/>Z = t(pr.out$x)</span><span id="23ba" class="ng nh it nc b gy nm nj l nk nl"># Let's compress the 232nd face of the dataset and add the average face back and create four other images adopting the first 10,50,100, and 300 columns.<br/>par(mfrow=c(1,5))<br/>plot_face(face_mat[232,])<br/>for (i in c(10,50,100,300))<br/> {<br/> plot_face((U[,1:i]%*%Z[1:i,])[,232]+face_average) <br/> }</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi np"><img src="../Images/870b5a7f6e988eb9299f79a5d96c7e3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ZcUjuUmwFVfehltg5DMbQ.png"/></div></div></figure><p id="a132" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们做到了！成绩还不算太差。最左边是原始图像，后面是四幅压缩图像。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><p id="68b8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">简单的数学解释。</p><p id="0042" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">PCA 与矩阵的奇异值分解(SVD)密切相关。所以，<strong class="lk jd">x</strong>= ud(v^t)=<strong class="lk jd">z</strong>*(v^t)，</p><p id="52a5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">其中<strong class="lk jd"> x </strong>是<strong class="lk jd"> </strong>的 1000*10000 矩阵，</p><ul class=""><li id="7917" class="nq nr it lk b ll lm lo lp lr ns lv nt lz nu md nv nw nx ny bi translated">v:特征向量的矩阵(由 prcomp 返回的旋转)</li><li id="3765" class="nq nr it lk b ll nz lo oa lr ob lv oc lz od md nv nw nx ny bi translated">d:主成分的标准偏差(由 prcomp 返回的 sdev)</li><li id="16be" class="nq nr it lk b ll nz lo oa lr ob lv oc lz od md nv nw nx ny bi translated">所以，<strong class="lk jd"> z </strong> = UD(旋转空间中主分量的坐标(prcomp$x))。</li></ul><p id="7379" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">换句话说，我们可以使用 V 的前 k 列和 z 的前 k 列来压缩图像:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/93d4992c6f956d4d432c10326ad4239a.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*36q7XXR8-8ZEIUG1wGOwVw.png"/></div></figure><p id="393a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数学结束。</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="28eb" class="ng nh it bd of og oh dn oi oj ok dp ol lr om on oo lv op oq or lz os ot ou iz bi translated">喜欢读这本书吗？</h2><blockquote class="ov ow ox"><p id="9182" class="li lj oy lk b ll lm kd ln lo lp kg lq oz ls lt lu pa lw lx ly pb ma mb mc md im bi translated">请在<a class="ae lh" href="https://www.linkedin.com/in/leihuaye/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/leihua_ye" rel="noopener ugc nofollow" target="_blank"> Twitter </a>找到我。</p><p id="0cb1" class="li lj oy lk b ll lm kd ln lo lp kg lq oz ls lt lu pa lw lx ly pb ma mb mc md im bi translated">查看我关于人工智能和机器学习的其他帖子。</p></blockquote><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/classifying-rare-events-using-five-machine-learning-techniques-fab464573233"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd jd gy z fp pk fr fs pl fu fw jc bi translated">使用 5 种机器学习算法对罕见事件进行分类</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">哪一种最适合不平衡数据？有什么权衡吗？</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt lb pf"/></div></div></a></div></div></div>    
</body>
</html>