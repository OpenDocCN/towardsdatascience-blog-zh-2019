# 日常生活和微观预测

> 原文：<https://towardsdatascience.com/everyday-life-and-microprediction-f76c1721872a?source=collection_archive---------29----------------------->

![](img/ff6b93febe62b9dac3c10caa2c819f9d.png)

Photo by [@chengfengrecord](https://unsplash.com/@chengfengrecord)

## 具有人工智能的 B2C 预测和安全工具

作为个体，我们有权利使用人工智能来尝试预测日常生活中的行为吗？举个例子，你应该能够预测雇佣特定保姆的风险吗？2018 年，Predictim 发布了一项服务，承诺通过扫描他们在网络、社交媒体和在线犯罪数据库上的存在来审查可能的保姆。可以说是个人使用的 AI 安全。

> Predictim 提供了一种新的创新方法，使用人工智能和替代数据来即时审查人们。

可以说，Predictim 没有获得[任何形式的巨额投资](https://www.crunchbase.com/organization/predictim#section-overview)(种子期 10 万美元)。然后在 2018 年末，[推特和脸书宣布 Predictim 已经被禁止](https://nakedsecurity.sophos.com/2018/11/28/social-media-scraping-app-predictim-banned-by-facebook-and-twitter/)。乔尔·西蒙诺夫和萨尔·帕萨似乎没有采取进一步的行动来创办一家新公司。事实上，在社交媒体跟踪的*好名字或坏名字*中，我现在看到 [Joel Simonoff 是 NASA 的机器学习研究员](https://www.linkedin.com/in/joelsimonoff/)。

## 什么是好的，什么是不好的？

然而，初创公司的兴衰提出了一个有趣的问题，即我们如何使用技术来预测并根据这些预测采取行动。在 Predictim 中，有一个明显的内在种族主义的例子。人工智能对尊重和态度的扫描，对家庭领域的伦理如此缺乏理解，似乎是一种明显的侵犯。什么是好的，什么是不好的？

我们似乎仍然认为，公司收集我们在家里的行为的预测信息是没问题的，只要他们用雄辩的声音说话，或者如果你在网上找到了你想要的答案。我们的 fitbit(测量健康信息)、手机(情境感知、位置信息)和社交媒体(心理特征、偏好)也是如此。

这些具有更大框架的预测工具肯定会有与保姆应用一样糟糕或更糟糕的偏差，但我们也许可以在另一个时间讨论这个问题。

## 微观预测

你可能听说过**微观管理**:这是一种管理风格，管理者通过这种方式密切观察和/或控制和/或提醒他/她的下属或员工的工作。然而，它可以在工作环境之外的[关系中引用](https://www.psychologytoday.com/us/blog/invisible-chains/201509/micromanaging-every-move-inside-controlling-relationship)。

大声说出微观预测，我或多或少把它作为一个*的讨论点*。预测并不是什么新鲜事，在某种程度上，你可以说它是让我们与众不同的一部分:

> [**“象征性抽象思维**](https://singularityhub.com/2017/12/28/what-is-it-that-makes-humans-unique/) **:** 很简单，这是我们思考不在物理上存在的物体、原理、观念的能力。它赋予我们复杂语言的能力。这是由较低的喉头(比所有其他动物允许更广泛的声音)和复杂语言的大脑结构支持的

多么美妙的象征性抽象思维，然而它回避了这样一个问题，即预测什么或不预测什么是否有一个限度。甚至可能会有关于如何预测的问题，或者更有可能是法律，因为伦理考虑会转化为监管。事实上，预言在各种科学中的支配地位可以追溯到很久以前，许多提到的[王子](https://en.wikipedia.org/wiki/The_Prince)被*尼可罗·马基亚维利*作为早期的例子估计早在1513 年就已经被分发。使用定量信息并收集这些信息以更好地做出决策的想法。

在这种预测背景下保护权利的最近例子是[欧盟通用数据保护条例](https://eugdpr.org/) (GDPR)和 FDA 考虑[监管**医疗保健**](https://www.technologyreview.com/f/613264/the-fda-wants-to-regulate-machine-learning-in-healthcare/)中的机器学习。

**机器学习** (ML)是对算法和统计模型的科学研究，计算机系统使用这些算法和统计模型来有效地执行特定任务，而不使用明确的指令，而是依靠模式和推理。

没有明确的指导方针，似乎对什么可以做什么不可以做有一个模糊的认识。人类学家 Marilyn Strathern 写了一篇更早的文章，名为《T2:未来的亲属关系和文化研究》(1995 年)。其中她谈到了我们对什么是人工或非人工的概念，提到了它是如何随着时间的推移而变化的。这种人工或技巧并不是一成不变的，被认为是自然的变化。

## 为什么要做微预测？

微观管理通常被认为有负面的[含义](https://en.wikipedia.org/wiki/Connotation)，主要是因为它显示了工作场所缺乏自由。微预测作为一个话题可能需要进一步讨论，我还没有定义它，因为我还不知道如何以一种好的方式使用它。然而，为了激发更多的兴趣，我想用 Predictim 关于伦理学的评论来结束我的文章:

> “我们非常重视道德和偏见，”Predictim 的首席执行官萨尔·帕萨(Sal Parsa)在电话中小心翼翼地告诉我。“事实上，在过去的 18 个月里，我们训练了我们的产品、我们的机器、我们的算法，以确保它是合乎道德的，没有偏见。我们把敏感的属性，受保护的职业，性别，种族，从我们的训练集中去掉。我们持续审计我们的模型。除此之外，我们还增加了人工审核流程。”
> - [Brian Merchant 于 2018 年 6 月 12 日在 Gizmodo 撰文](https://gizmodo.com/predictim-claims-its-ai-can-flag-risky-babysitters-so-1830913997?fbclid=IwAR0W84ixJjXVApa7NWgJGjUM9zjThoaPv5KvjkPOJTnKZEZh5xhM6lrQJC0)

*这听起来耳熟吗？如果是，谁有权侵犯或使用用户数据？*

我不确定我是否会支持根据这一标准对保姆进行扫描，但我会支持保险公司或信用评分吗？

基于狭义的人工智能技术结合社交媒体或其他来源等非传统数据的信用评分可能是一个有趣的领域，可以继续监测，因为新公司正在涌现，并以可能令人不安的新方式应用机器学习。

偏见:对一个人或一群人的倾向或偏见，尤其是以一种被认为不公平的方式。在人类学领域，我们讨论**种族中心主义**:根据源于自身文化标准和习俗的先入之见来评价其他文化。

我们如何使用技术或谴责技术的使用是令人着迷的。

我一定会进一步探讨这个问题。

**这是#500daysofAI** 的第 22 天，希望你喜欢。

–

> 什么是#500daysofAI？
> 
> 我在挑战自己，用#500daysofAI 写下并思考未来 500 天的人工智能话题。这是我发明的一个挑战，让自己一直思考这个话题，分享我的想法。
> 
> 这是受电影《夏日 500 天》的启发，主角试图找出爱情失败的原因，并在此过程中重新发现他生活中真正的激情。