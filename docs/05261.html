<html>
<head>
<title>Training Neural Network using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 PyTorch 训练神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-a-neural-network-using-pytorch-72ab708da210?source=collection_archive---------6-----------------------#2019-08-06">https://towardsdatascience.com/training-a-neural-network-using-pytorch-72ab708da210?source=collection_archive---------6-----------------------#2019-08-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><blockquote class="jq"><p id="dfe1" class="jr js it bd jt ju jv jw jx jy jz ka dk translated">“一知半解是一件危险的事情；<strong class="ak">(亚历山大·波普)</strong>深饮或尝不出皮耶里的春天</p></blockquote><figure class="kc kd ke kf kg kh gh gi paragraph-image"><div class="gh gi kb"><img src="../Images/66ab69744dfef72bba147783bf96337b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*zG1ldy1i6-OUIjURp6wQww.png"/></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">Human brain vs Neural network (image source <a class="ae ko" href="https://www.quora.com/What-is-the-difference-between-artificial-intelligence-and-neural-networks" rel="noopener ugc nofollow" target="_blank">here</a>)</figcaption></figure><p id="4231" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">所以在<a class="ae ko" rel="noopener" target="_blank" href="/building-neural-network-using-pytorch-84f6e75f9a">之前的</a>文章中，我们已经建立了一个非常简单和“幼稚”的神经网络，它不知道将输入映射到输出的函数。为了使它更加智能，我们将通过向它显示“真实数据”的例子来训练网络，然后调整网络参数(权重和偏差)。简而言之，我们在调整模型参数(权重和偏差)的同时，通过迭代训练数据集来提高精确度。</p><p id="4abd" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">为了找到这些参数，我们需要知道我们的网络对真实输出的预测有多差。为此，我们将计算<code class="fe lm ln lo lp b">cost</code>，也称为<code class="fe lm ln lo lp b">loss function</code>。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="9310" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated"><strong class="ak">成本</strong></h1><p id="7d2a" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated"><code class="fe lm ln lo lp b">Cost</code>或<code class="fe lm ln lo lp b">loss function</code>是我们预测误差的度量。通过相对于网络参数最小化<code class="fe lm ln lo lp b">loss</code>，我们可以找到<code class="fe lm ln lo lp b">loss</code>最小的状态，并且网络能够以高精度预测正确的标签。我们使用一个叫做<code class="fe lm ln lo lp b">gradient descent</code>的过程找到这个<code class="fe lm ln lo lp b">minimum loss</code>。在处检查不同种类的<code class="fe lm ln lo lp b">cost</code>功能<a class="ae ko" href="https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications" rel="noopener ugc nofollow" target="_blank"/></p><h1 id="674b" class="lx ly it bd lz ma na mc md me nb mg mh mi nc mk ml mm nd mo mp mq ne ms mt mu bi translated">梯度下降</h1><p id="fe70" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated">梯度下降需要成本函数。我们需要这个<code class="fe lm ln lo lp b">cost</code>函数，因为我们需要将其最小化，以便获得高预测精度。GD 的全部意义在于最小化<code class="fe lm ln lo lp b">cost</code>函数。该算法的目标是到达最低值<code class="fe lm ln lo lp b">error value</code>的过程。为了在成本函数中得到最低的<code class="fe lm ln lo lp b">error value</code>(相对于一个权重)，我们需要调整模型的参数。那么，我们需要调整参数到什么程度呢？我们可以使用<code class="fe lm ln lo lp b">calculus</code>找到它。使用<code class="fe lm ln lo lp b">calculus</code>我们知道函数的<code class="fe lm ln lo lp b">slope</code>是关于值的函数的<code class="fe lm ln lo lp b">derivative</code>。<code class="fe lm ln lo lp b">gradient</code>是损失函数的斜率，指向变化最快的方向。</p><h1 id="e72a" class="lx ly it bd lz ma na mc md me nb mg mh mi nc mk ml mm nd mo mp mq ne ms mt mu bi translated">在上反向传播<strong class="ak"/></h1><p id="50cf" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated"><code class="fe lm ln lo lp b">Gradient Descent</code>对于单层网络来说实施起来很简单，但是对于多层网络来说，实施起来更复杂和更深入。多层网络的训练是通过<strong class="kr iu">反向传播</strong>完成的，这实际上是微积分中链式法则的应用。如果我们把一个两层网络转换成一个图形表示，这是最容易理解的。</p><figure class="ng nh ni nj gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nf"><img src="../Images/c2776ecea057fe7e54446df7bfee0303.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PhOCrD3sPgPKRIaTv4-Gg.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">image source: udacity course material</figcaption></figure><h2 id="a046" class="no ly it bd lz np nq dn md nr ns dp mh la nt nu ml le nv nw mp li nx ny mt nz bi translated">向前传球</h2><p id="7f49" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated">在向前传递中，数据和操作是自下而上的。</p><p id="8399" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated"><strong class="kr iu">步骤 1: </strong>我们通过线性变换<strong class="kr iu"> </strong> <code class="fe lm ln lo lp b"><strong class="kr iu">𝐿1</strong></code>将输入<code class="fe lm ln lo lp b"><strong class="kr iu">𝑥</strong></code>传递给权重<code class="fe lm ln lo lp b"><strong class="kr iu">𝑊1</strong></code>和偏差<code class="fe lm ln lo lp b"><strong class="kr iu">𝑏1</strong></code></p><p id="39e8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated"><strong class="kr iu">第二步:</strong>然后输出经过 sigmoid 运算<strong class="kr iu"> </strong> <code class="fe lm ln lo lp b"><strong class="kr iu">𝑆</strong></code>和另一个线性变换<code class="fe lm ln lo lp b">𝐿2</code></p><p id="77ca" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">第三步:最后我们计算ℓ.的损失</p><p id="5928" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">我们用这个损失来衡量网络的预测有多糟糕。目标是调整<code class="fe lm ln lo lp b">weights</code>和<code class="fe lm ln lo lp b">biases</code>以最小化损失。</p><h2 id="10a6" class="no ly it bd lz np nq dn md nr ns dp mh la nt nu ml le nv nw mp li nx ny mt nz bi translated">偶数道次</h2><p id="104a" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated">为了用梯度下降来训练权重，我们通过网络反向传播损失的梯度。</p><p id="5d8f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">每个操作在输入和输出之间都有一些梯度。</p><p id="415b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">当我们向后发送梯度时，我们将引入的梯度与操作的梯度相乘。</p><p id="b3a4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">从数学上来说，这实际上就是用链式法则计算损耗相对于重量的梯度。</p><figure class="ng nh ni nj gt kh gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/66faea3056ec92cd8b1621f0e98051dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*6Hrmr-W_1ZzERp0kdkkfqw.png"/></div></figure><p id="e667" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">我们使用这个梯度和一些学习速率α来更新我们的权重。</p><figure class="ng nh ni nj gt kh gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/6deb4d91ae44c687252981dab24e2e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*xpLgyetn9FGjJuJRn9xbTw.png"/></div></figure><p id="fff8" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">学习率<code class="fe lm ln lo lp b">α</code>被设置为使得权重更新步长足够小，使得迭代方法稳定在最小值。</p><p id="0f8d" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">点击了解更多关于 backprop <a class="ae ko" href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/" rel="noopener ugc nofollow" target="_blank">的信息</a></p><h1 id="7d9f" class="lx ly it bd lz ma na mc md me nb mg mh mi nc mk ml mm nd mo mp mq ne ms mt mu bi translated">PyTorch 的损失</h1><p id="f5ad" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated">PyTorch 提供了损失，如<a class="ae ko" href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">交叉熵损失</a> <code class="fe lm ln lo lp b">nn.CrossEntropyLoss</code>。对于像 MNIST 这样的分类问题，我们使用 softmax 函数来预测分类概率。</p><p id="579b" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">为了计算<code class="fe lm ln lo lp b">loss</code>,我们首先定义<code class="fe lm ln lo lp b">criterion</code>,然后传入网络的<code class="fe lm ln lo lp b">output</code>,并修正标签。</p><blockquote class="oc od oe"><p id="9d19" class="kp kq of kr b ks kt ku kv kw kx ky kz og lb lc ld oh lf lg lh oi lj lk ll ka im bi translated"><code class="fe lm ln lo lp b">nn.CrossEntropyLoss</code>标准将<code class="fe lm ln lo lp b">nn.LogSoftmax()</code>和<code class="fe lm ln lo lp b">nn.NLLLoss()</code>组合在一个类中。</p><p id="5424" class="kp kq of kr b ks kt ku kv kw kx ky kz og lb lc ld oh lf lg lh oi lj lk ll ka im bi translated">输入应该包含每个类的分数。</p></blockquote><p id="1cf2" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">也就是说，我们需要将网络的原始输出传递给损耗，而不是 softmax 函数的输出。这种原始输出通常被称为<code class="fe lm ln lo lp b"><em class="of">logits</em></code> <em class="of"> </em>或<code class="fe lm ln lo lp b"><em class="of">scores</em></code> <em class="of"> </em>我们使用<code class="fe lm ln lo lp b">logits</code>是因为<code class="fe lm ln lo lp b">softmax</code>给出的概率通常非常接近于<code class="fe lm ln lo lp b">zero</code>或<code class="fe lm ln lo lp b">one</code>，但浮点数不能准确地表示接近 0 或 1 的值(<a class="ae ko" href="https://docs.python.org/3/tutorial/floatingpoint.html" rel="noopener ugc nofollow" target="_blank">在此阅读更多信息</a>)。通常最好避免用概率进行计算，通常我们使用对数概率。</p><h1 id="1c4b" class="lx ly it bd lz ma na mc md me nb mg mh mi nc mk ml mm nd mo mp mq ne ms mt mu bi translated">亲笔签名</h1><p id="f628" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated">Torch 提供了一个名为<code class="fe lm ln lo lp b">autograd</code>的模块来自动计算张量的<strong class="kr iu"><em class="of"/></strong><em class="of"/>。这是一种计算导数的引擎。它记录在<strong class="kr iu">梯度激活张量</strong>上执行的所有操作的图形，并创建一个<strong class="kr iu">非循环</strong>图形，称为<strong class="kr iu">动态计算图形</strong>。该图的<em class="of">叶</em>是输入张量，而<em class="of">根</em>是输出张量。</p><p id="288f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated"><code class="fe lm ln lo lp b">Autograd</code>的工作原理是跟踪在张量上执行的操作，然后通过这些操作向后<em class="of">前进</em>，计算沿途的梯度。</p><p id="f385" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">为了确保 PyTorch 跟踪张量上的操作并计算梯度，我们需要设置<code class="fe lm ln lo lp b">requires_grad = True</code>。我们可以用<code class="fe lm ln lo lp b">torch.no_grad()</code>关闭代码块的渐变。</p></div><div class="ab cl lq lr hx ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="im in io ip iq"><h1 id="6c6c" class="lx ly it bd lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu bi translated"><strong class="ak">训练网络</strong></h1><p id="a1c4" class="pw-post-body-paragraph kp kq it kr b ks mv ku kv kw mw ky kz la mx lc ld le my lg lh li mz lk ll ka im bi translated">最后，我们需要一个<code class="fe lm ln lo lp b">optimizer</code>来更新梯度权重。我们从 PyTorch 的<code class="fe lm ln lo lp b"><a class="ae ko" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank">optim</a></code> <a class="ae ko" href="https://pytorch.org/docs/stable/optim.html" rel="noopener ugc nofollow" target="_blank">包</a>中得到这些。例如，我们可以通过<code class="fe lm ln lo lp b">optim.SGD</code>使用随机梯度下降。</p><p id="1ffc" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated"><strong class="kr iu">训练神经网络的过程:</strong></p><ul class=""><li id="9dcd" class="oj ok it kr b ks kt kw kx la ol le om li on ka oo op oq or bi translated">通过网络向前传递</li><li id="950a" class="oj ok it kr b ks os kw ot la ou le ov li ow ka oo op oq or bi translated">使用网络输出来计算损耗</li><li id="5994" class="oj ok it kr b ks os kw ot la ou le ov li ow ka oo op oq or bi translated">用<code class="fe lm ln lo lp b">loss.backward()</code>对网络进行反向遍历，以计算梯度</li><li id="c0d6" class="oj ok it kr b ks os kw ot la ou le ov li ow ka oo op oq or bi translated">使用优化器更新权重</li></ul><p id="1cb4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">我们将准备用于训练的数据:</p><pre class="ng nh ni nj gt ox lp oy oz aw pa bi"><span id="95bc" class="no ly it lp b gy pb pc l pd pe">import torch<br/>from torch import nn<br/>import torch.nn.functional as F<br/>from torchvision import datasets, transforms</span><span id="a753" class="no ly it lp b gy pf pc l pd pe"># Define a transform to normalize the data<br/>transform = transforms.Compose([transforms.ToTensor(),<br/>                                transforms.Normalize((0.5,), (0.5,)),<br/>                              ])</span><span id="f2ed" class="no ly it lp b gy pf pc l pd pe"># Download and load the training data<br/>trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)<br/>trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)</span></pre><p id="11fe" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">用真实数据训练:</p><p id="3ecf" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">在一些术语中，一次遍历整个数据集被称为<code class="fe lm ln lo lp b"><em class="of">epoch</em></code>。所以这里我们将循环通过<code class="fe lm ln lo lp b">trainloader</code>来获得我们的训练批次。对于每一批，我们将进行一次训练，计算损失，进行一次反向训练，并更新权重。</p><pre class="ng nh ni nj gt ox lp oy oz aw pa bi"><span id="4398" class="no ly it lp b gy pb pc l pd pe">model = nn.Sequential(nn.Linear(784, 128),<br/>                      nn.ReLU(),<br/>                      nn.Linear(128, 64),<br/>                      nn.ReLU(),<br/>                      nn.Linear(64, 10),<br/>                      nn.LogSoftmax(dim=1))</span><span id="eacd" class="no ly it lp b gy pf pc l pd pe"># Define the loss<br/>criterion = nn.NLLLoss()</span><span id="f9a5" class="no ly it lp b gy pf pc l pd pe"># Optimizers require the parameters to optimize and a learning rate<br/>optimizer = optim.SGD(model.parameters(), lr=0.003)</span><span id="e193" class="no ly it lp b gy pf pc l pd pe">epochs = 5<br/>for e in range(epochs):<br/>    running_loss = 0<br/>    for images, labels in trainloader:<br/>        # Flatten MNIST images into a 784 long vector<br/>        images = images.view(images.shape[0], -1)<br/>    <br/>        # Training pass<br/>        optimizer.zero_grad()<br/>        <br/>        output = model(images)<br/>        loss = criterion(output, labels)<br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>        running_loss += loss.item()<br/>    else:<br/>        print(f"Training loss: {running_loss/len(trainloader)}")</span></pre><blockquote class="oc od oe"><p id="87ac" class="kp kq of kr b ks kt ku kv kw kx ky kz og lb lc ld oh lf lg lh oi lj lk ll ka im bi translated">N <!-- -->注:<code class="fe lm ln lo lp b">optimizer.zero_grad()</code>:当我们用相同的参数进行多次反向传递时，梯度就会累积。这意味着我们需要在每次训练过程中将梯度归零，否则我们将保留以前训练批次的梯度。</p></blockquote><p id="25b5" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">看吧！训练损失在每一个时期都在下降。</p><p id="7489" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">随着网络的训练，我们可以检查它的预测。</p><figure class="ng nh ni nj gt kh gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi pg"><img src="../Images/9f00424f239672f3b3d54215f0944b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*cXYvLc9YMFHN9P_ApHMwHA.png"/></div></div><figcaption class="kk kl gj gh gi km kn bd b be z dk">Prediction result after training</figcaption></figure><p id="7e7f" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">现在我们的网络是辉煌的！它能准确预测我们图像中的数字。是不是很酷？</p><p id="c5f4" class="pw-post-body-paragraph kp kq it kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll ka im bi translated">声明:本文基于我在 facebook-udacity 奖学金挑战项目中的学习。</p></div></div>    
</body>
</html>