<html>
<head>
<title>What To Do When Your Classification Data is Imbalanced</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当您的分类数据不平衡时该怎么办</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-to-do-when-your-classification-dataset-is-imbalanced-6af031b12a36?source=collection_archive---------8-----------------------#2019-09-05">https://towardsdatascience.com/what-to-do-when-your-classification-dataset-is-imbalanced-6af031b12a36?source=collection_archive---------8-----------------------#2019-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3758" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这篇文章中，我们将看看不同的方法和工具，可以用来解决机器学习中出现的一个常见问题，即倾斜数据集的问题。</h2></div><p id="11bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">建立一个好的机器学习模型的关键是它被训练的数据。因此，训练数据必须是干净和平衡的。在完善训练数据上花费的时间越多，在模型上花费的精力就越少。因此，让我们看看如何着手获得一个平衡的数据集。在本文中，我们将讨论，</p><ul class=""><li id="2576" class="le lf it kk b kl km ko kp kr lg kv lh kz li ld lj lk ll lm bi translated">不平衡的数据集意味着什么？</li><li id="a9fc" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">为什么数据集不平衡很重要？</li><li id="724d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">处理不平衡数据集的不同方法。</li><li id="4f8d" class="le lf it kk b kl ln ko lo kr lp kv lq kz lr ld lj lk ll lm bi translated">处理不平衡数据集的不同工具。</li></ul><h2 id="966a" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">不平衡的数据集意味着什么？</h2><p id="4caf" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">让我们看看当试图解决一个分类问题时，偏斜意味着什么。当数据集中的大多数数据项表示属于一个类的项目时，我们说数据集是偏斜的或不平衡的。为了更好地理解，让我们考虑一个二元分类问题，癌症检测。假设我们的数据集中有 5000 个实例，但只有 500 个阳性实例，即实际存在癌症的实例。然后我们有一个不平衡的数据集。这种情况在现实生活中的数据集上更常见，因为在所有发生的检查中发现癌症或在所有每天发生的交易中发现欺诈交易的几率相对较低。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/bae07a13710a6c67ed5d71602d5ca0c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iFjJSIoheEUgDnDJFKlS6Q.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">In this cancer dataset, there are only 57 positive instances whereas there are 212 negative instances, making it a perfect example of class imbalance.</figcaption></figure><h2 id="47b6" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">为什么数据集有偏差很重要？</h2><p id="270c" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">当您的数据集没有平等地表示所有数据类时，模型可能会过度适应在您的数据集中表示更多的类，从而忽略了少数类的存在。它甚至可能给你一个很好的准确性，但在现实生活中却悲惨地失败了。在我们的示例中，一个每次都持续预测没有癌症的模型也将具有良好的准确性，因为癌症本身的发生在输入中是罕见的。但是，当一个实际的癌症病例被分类时，它就会失败，达不到它最初的目的。</p><h2 id="6537" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">处理不平衡数据集的不同方法</h2><p id="ca10" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">一种广泛采用的处理高度不平衡数据集的技术称为重采样。重采样是在数据被分成训练集、测试集和验证集之后进行的。仅在训练集上进行重采样，否则性能度量可能会出现偏差。重采样有两种类型:过采样和欠采样。</p><p id="e3d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">欠采样包括从多数类中移除样本，过采样包括从少数类中添加更多样本。过采样的最简单实现是从少数类中复制随机记录，这可能会导致过拟合。在欠采样中，最简单的技术是从多数类中移除随机记录，这会导致信息丢失。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ng"><img src="../Images/0ecc99bbaa60b5fbb68c11e01b8e6fb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*49LgGsY4l09sNwcR.png"/></div></div><figcaption class="nc nd gj gh gi ne nf bd b be z dk">Under- and Over-Sampling</figcaption></figure><p id="9e2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一种类似于上采样的技术是创建合成样本。将合成样本添加到训练数据中也仅在训练测试拆分之后完成。</p><h2 id="9a3f" class="ls lt it bd lu lv lw dn lx ly lz dp ma kr mb mc md kv me mf mg kz mh mi mj mk bi translated">处理不平衡数据集的不同工具</h2><p id="4e8d" class="pw-post-body-paragraph ki kj it kk b kl ml ju kn ko mm jx kq kr mn kt ku kv mo kx ky kz mp lb lc ld im bi translated">Scikit Learn 的 sklearn.utils.resample 包允许您对数据进行重采样。它将数组作为输入，并以一致的方式对它们进行重新采样。</p><p id="3868" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，让我们尝试对这个数据集进行过采样。</p><pre class="mr ms mt mu gt nh ni nj nk aw nl bi"><span id="7322" class="ls lt it ni b gy nm nn l no np">X = df.drop(‘diagnosis’,axis=1)<br/>y = df[‘diagnosis’]</span><span id="e8b3" class="ls lt it ni b gy nq nn l no np">from sklearn.model_selection import train_test_split<br/>from sklearn.utils import resample</span><span id="a7af" class="ls lt it ni b gy nq nn l no np">#split data into test and training sets<br/>X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)</span><span id="0ec1" class="ls lt it ni b gy nq nn l no np">#combine them back for resampling<br/>train_data = pd.concat([X_train, y_train], axis=1)</span><span id="6497" class="ls lt it ni b gy nq nn l no np"># separate minority and majority classes<br/>negative = train_data[train_data.diagnosis==0]<br/>positive = train_data[train_data.diagnosis==1]</span><span id="f095" class="ls lt it ni b gy nq nn l no np"># upsample minority<br/>pos_upsampled = resample(positive,<br/> replace=True, # sample with replacement<br/> n_samples=len(negative), # match number in majority class<br/> random_state=27) # reproducible results</span><span id="9b26" class="ls lt it ni b gy nq nn l no np"># combine majority and upsampled minority<br/>upsampled = pd.concat([negative, pos_upsampled])</span><span id="48de" class="ls lt it ni b gy nq nn l no np"># check new class counts<br/>upsampled.diagnosis.value_counts()</span><span id="6b6c" class="ls lt it ni b gy nq nn l no np">1    139<br/>0    139<br/>Name: diagnosis, dtype: int64</span></pre><p id="62e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，在我们的训练数据中，这两个类的实例数量相同。</p><p id="cabe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来让我们看看欠采样。</p><pre class="mr ms mt mu gt nh ni nj nk aw nl bi"><span id="ca52" class="ls lt it ni b gy nm nn l no np"># downsample majority<br/>neg_downsampled = resample(negative,<br/> replace=True, # sample with replacement<br/> n_samples=len(positive), # match number in minority class<br/> random_state=27) # reproducible results</span><span id="9d78" class="ls lt it ni b gy nq nn l no np"># combine minority and downsampled majority<br/>downsampled = pd.concat([positive, neg_downsampled])</span><span id="c199" class="ls lt it ni b gy nq nn l no np"># check new class counts<br/>downsampled.diagnosis.value_counts()</span><span id="c72e" class="ls lt it ni b gy nq nn l no np">1    41<br/>0    41<br/>Name: diagnosis, dtype: int64</span></pre><p id="4151" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">两个类都有 41 个实例。</p><p id="ae1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">imblearn 库有一个名为 imblearn.over_sampling 的类。使用 SMOTE 执行过采样的 SMOTE。这是 SMOTE 或合成少数过采样技术的实现。让我们看看下面的实现。</p><pre class="mr ms mt mu gt nh ni nj nk aw nl bi"><span id="6ec3" class="ls lt it ni b gy nm nn l no np">from imblearn.over_sampling import SMOTE</span><span id="ec8a" class="ls lt it ni b gy nq nn l no np"># Separate input features and target<br/>X = df.drop(‘diagnosis’,axis=1)<br/>y = df[‘diagnosis’]</span><span id="ee43" class="ls lt it ni b gy nq nn l no np"># setting up testing and training sets<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)</span><span id="2615" class="ls lt it ni b gy nq nn l no np">sm = SMOTE(random_state=27, ratio=1.0)<br/>X_train, y_train = sm.fit_sample(X_train, y_train)</span><span id="d1e6" class="ls lt it ni b gy nq nn l no np">X_train.shape, y_train.shape</span><span id="3705" class="ls lt it ni b gy nq nn l no np">((314, 5), (314,)) #We now have 314 data items in our training set<br/>y_train = pd.DataFrame(y_train, columns = ['diagnosis'])<br/>y_train.diagnosis.value_counts()<br/>1    157<br/>0    157<br/>Name: diagnosis, dtype: int64</span></pre><p id="5200" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SMOTE 为这两个类创建了足够的合成数据，每个类有 157 个数据项。</p><p id="24f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们刚刚讨论了最流行的重采样方法。Imblearn 在 imblearn.under_sampling 和 imblearn.over_sampling 类下定义了许多其他欠采样和过采样方法，以及在 imblearn.combine 类下组合这两种方法的方法。</p><p id="0238" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以在 https://unbalanced-learn . readthedocs . io/en/stable/API . html # module-imb learn . over _ sampling 上了解更多信息</p></div></div>    
</body>
</html>