<html>
<head>
<title>Data Augmentation for Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于语音识别的数据扩充</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-augmentation-for-speech-recognition-e7c607482e78?source=collection_archive---------7-----------------------#2019-05-01">https://towardsdatascience.com/data-augmentation-for-speech-recognition-e7c607482e78?source=collection_archive---------7-----------------------#2019-05-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="84f2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">自动语音识别(ASR)</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2964ffffbd1cb9bbc1a35b0a236f2758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iD9ioERFLy_BIX4f"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="d95d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个故事发表在<a class="ae kv" href="https://dev.to/makcedward/data-augmentation-for-speech-recognition-bfc" rel="noopener ugc nofollow" target="_blank"> Dev.to </a>和 Medium 上。</p><p id="81bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">语音识别的目的是将音频转换成文本。这项技术广泛应用于我们的生活中。<a class="ae kv" href="https://en.wikipedia.org/wiki/Google_Assistant" rel="noopener ugc nofollow" target="_blank">谷歌助手</a>和<a class="ae kv" href="https://en.wikipedia.org/wiki/Amazon_Alexa" rel="noopener ugc nofollow" target="_blank">亚马逊 Alexa </a>就是将我们的声音作为输入并转换成文本以理解我们意图的一些例子。</p><p id="c9dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与其他自然语言处理问题一样，关键挑战之一是缺乏足够数量的训练数据。它导致过多或难以处理看不见的数据。谷歌大脑(Google Brain)和人工智能(AI)团队通过引入几种用于语音识别的数据增强方法来解决这个问题。本故事将讨论<a class="ae kv" href="https://arxiv.org/pdf/1904.08779.pdf" rel="noopener ugc nofollow" target="_blank"> SpecAugment:一种用于自动语音识别的简单数据增强方法</a> (Park 等人，2019 年)，并将涵盖以下内容:</p><ul class=""><li id="26cb" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">数据</li><li id="daad" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">体系结构</li><li id="b6b6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">实验</li></ul><h1 id="7941" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">数据</h1><p id="b7ed" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">为了处理数据，波形音频转换成声谱图，并馈入神经网络产生输出。执行数据扩充的传统方式通常应用于波形。Park 等人采用了另一种方法，即操纵声谱图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/bc55c141a897d36032e93a4498065c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*DMFTlxD5-3rhmcks.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Waveform audio to spectrogram (<a class="ae kv" href="https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html" rel="noopener ugc nofollow" target="_blank">Google Brain</a>)</figcaption></figure><p id="0040" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定一个声谱图，你可以把它看作一幅图像，其中 x 轴是时间，而 y 轴是频率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/b06b142e122fd434e3ee0723fc7485a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*d3wpu740AzHNtsof.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Spectrogram representation (<a class="ae kv" href="https://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html" rel="noopener ugc nofollow" target="_blank">librosa</a>)</figcaption></figure><p id="d9c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">直观地说，它提高了训练速度，因为没有波形数据到频谱图数据之间的数据转换，而是增加了频谱图数据。</p><p id="0ca5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Park 等人推出了用于语音识别中数据增强的<code class="fe nf ng nh ni b">SpecAugment</code>。有三种基本方法来扩充数据，即时间弯曲、频率屏蔽和时间屏蔽。在他们的实验中，他们将这些方法结合在一起，并引入了 4 种不同的组合，即 LibriSpeech basic (LB)、LibriSpeech double (LD)、Switchboard mild (SM)和 Switchboard strong (SS)。</p><h2 id="70da" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated"><strong class="ak"> <em class="nv">时间扭曲</em> </strong></h2><p id="7723" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">将选择一个随机点，并向左或向右弯曲一段距离 W，该距离从 0 到沿该线的时间弯曲参数 W 的均匀分布中选择。</p><h2 id="3eec" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated">频率掩蔽</h2><p id="773f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">a 频道[f0，F0+f]被屏蔽。F 选自 0 至频率掩模参数 F 的均匀分布，f0 选自(0，νF ),其中ν为频率通道数。</p><h2 id="86f6" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated">时间掩蔽</h2><p id="3ead" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">t 个连续的时间步长[t0，t0+t]被屏蔽。T 选自 0 至时间屏蔽参数 T 的均匀分布，t0 选自[0，τT]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5914e197178bc2c87e8a47183c5d6eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*xq6oahbJzFI9HdwY1fdWJw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">From top to bottom, the figures depict the log mel spectrogram of the base input with no augmentation, time warp, frequency masking and time masking applied. (Park et al., 2019)</figcaption></figure><h2 id="3360" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated">基本增强政策的组合</h2><p id="9747" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">通过结合频率屏蔽和时间屏蔽的增强策略，引入了 4 种新的增强策略。而符号表示:</p><ul class=""><li id="5700" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">w:时间弯曲参数</li><li id="a27b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">f:频率屏蔽参数</li><li id="fcf3" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">mF:应用的频率屏蔽数量</li><li id="9343" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">t:时间屏蔽参数</li><li id="1167" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">mT:应用时间屏蔽的次数</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/e4c06248ab8203bda7d80b5f23c8fd05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*xfOBzbams8Z27HLVvTRvFA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Configuration for LB, LD, SM and SS (Park et al., 2019)</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/2ae5e64b8b4dc2b2183787d7446e17de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*PFcBVKIa4zWtcnwBjvy2NA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">From top to bottom, the figures depict the log mel spectrogram of the base input with policies None, LB and LD applied. (Park et al., 2019)</figcaption></figure><h1 id="a52f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">体系结构</h1><h2 id="61fa" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated">听、听、拼(LAS)网络架构</h2><p id="dd59" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">Park 等人使用 LAS 网络架构来验证使用和不使用数据增强的性能。它包括两层卷积神经网络(CNN)，注意力和堆叠双向 LSTMs。由于本白皮书的目标是数据扩充，并且利用模型来查看模型的影响，因此您可以从<a class="ae kv" href="https://arxiv.org/pdf/1508.01211.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>深入研究 LAS。</p><h2 id="a64b" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated">学习费率表</h2><p id="ce53" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">学习率时间表成为决定模型性能的关键因素。类似于<a class="ae kv" rel="noopener" target="_blank" href="/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89">倾斜三角形学习率(STLR) </a>，应用非静态学习率。学习速率将呈指数衰减，直到它达到其最大值的 1/100，并在该点之后保持恒定。一些参数表示为:</p><ul class=""><li id="8453" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">sr:加速步骤(从零学习率开始)完成</li><li id="ab75" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">si:指数衰减的步骤开始</li><li id="4f92" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">sf:指数衰减的步骤停止。</li></ul><p id="ec0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一种学习速率调度是统一标签平滑。正确的类别标签被赋予置信度 0.9，而其他标签的置信度相应地增加。参数表示为:</p><ul class=""><li id="75aa" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">噪声:变权噪声</li></ul><p id="e87c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在后面的实验中，定义了三个标准的学习速率表:</p><ol class=""><li id="4839" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nz ly lz ma bi translated">B(asic): (sr，snoise，si，sf ) = (0.5k，10k，20k，80k)</li><li id="6d8b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nz ly lz ma bi translated">d(double):(Sr，snoise，si，sf ) = (1k，20k，40k，160k)</li><li id="cf2e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nz ly lz ma bi translated">L(ong): (sr，snoise，si，sf ) = (1k，20k，140k，320k)</li></ol><h2 id="4597" class="nj mh iq bd mi nk nl dn mm nm nn dp mq lf no np ms lj nq nr mu ln ns nt mw nu bi translated">语言模型</h2><p id="51c7" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">LM 用于进一步提高模型性能。一般来说，LM 被设计成在给定前一个记号的结果的情况下预测下一个记号。一旦预测到新标记，在预测下一个标记时，它将被视为“前一个标记”。这种方法在很多现代的 NLP 模型中都有应用，比如<a class="ae kv" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">伯特</a>和<a class="ae kv" rel="noopener" target="_blank" href="/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655"> GPT-2 </a>。</p><h1 id="4bfe" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">实验</h1><p id="7519" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">模型性能通过<a class="ae kv" href="https://en.wikipedia.org/wiki/Word_error_rate" rel="noopener ugc nofollow" target="_blank">单词错误率</a> (WER)来衡量。</p><p id="6505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从下图中，“Sch”表示学习率计划，而“Pol”表示扩充政策。我们可以看到，具有 6 个 LSTM 层和 1280°嵌入向量的 LAS 表现出最好的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/1c794856907a9ba53b0f68be6d950719.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*gAVZttVQlypTtT6Cbncqzw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Evaluation of LibriSpeech (Park et al., 2019)</figcaption></figure><p id="fc5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过将 LAS-6–1280 与 SpecAugment 一起使用，与其他模型和没有数据扩充的 LAS 相比，可获得最佳结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2fc33ede01839fcc30472ac88dde6051.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*74sHvUtwGvuN04nCZ9tfRg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Comparing SpecAugment method in LibriSpeech 960h (Park et al., 2019)</figcaption></figure><p id="7524" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在配电盘 300h 中，LAS-4–1024 被用作基准。我们可以看到 SpecAugment 确实有助于进一步提高模型性能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/500e1300f54bad2de71749ab3cd643b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*TeKA8tmEsyNxBu-E0m9FbA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Comparing SpecAugment method in Switchboard 300h (Park et al., 2019)</figcaption></figure><h1 id="c77b" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">拿走</h1><ul class=""><li id="b0aa" class="ls lt iq ky b kz my lc mz lf od lj oe ln of lr lx ly lz ma bi translated">时间扭曲并没有显著提高模型性能。如果资源有限，这种方法将被放弃。</li><li id="b8b4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">标签平滑导致训练不稳定。</li><li id="09b5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">数据扩充将过拟合问题转化为欠拟合问题。从下图中，您可以注意到，没有增强(无)的模型在训练集中表现接近完美，而在其他数据集中没有类似的结果。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b873eb4656dc8b934977f8fd73404450.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*210d_HNZ72-BAA0VUmCxyA.png"/></div></figure><ul class=""><li id="edfa" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">为了便于语音识别的数据扩充，<a class="ae kv" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> nlpaug </a>现在支持 SpecAugment 方法。</li></ul><h1 id="94ec" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。欢迎在<a class="ae kv" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与<a class="ae kv" href="https://makcedward.github.io/" rel="noopener ugc nofollow" target="_blank"> me </a>联系，或者在<a class="ae kv" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae kv" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="be7f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">延伸阅读</h1><ul class=""><li id="b84c" class="ls lt iq ky b kz my lc mz lf od lj oe ln of lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-in-nlp-2801a34dfc28">自然语言处理中的数据扩充</a></li><li id="e4a8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-library-for-text-9661736b13ff">文本的数据扩充</a></li><li id="4780" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-for-audio-76912b01fdf6">音频数据增强</a></li><li id="7aa6" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html" rel="noopener ugc nofollow" target="_blank">谷歌正式发布 SpecAugment】</a></li><li id="a631" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89">倾斜三角形学习率(STLR) </a></li><li id="4813" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">变压器的双向编码器表示</a></li><li id="9e8d" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/too-powerful-nlp-model-generative-pre-training-2-4cc6afb6655">创成式预培训 2 </a></li></ul><h1 id="1470" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><ul class=""><li id="f312" class="ls lt iq ky b kz my lc mz lf od lj oe ln of lr lx ly lz ma bi translated">D.朴正熙、陈伟雄、张宇人、赵超群、左宗棠、朱布克及黎庆伟。<a class="ae kv" href="https://arxiv.org/pdf/1904.08779.pdf" rel="noopener ugc nofollow" target="_blank"> SpecAugment:一种用于自动语音识别的简单数据扩充方法</a>。2019</li><li id="6cc7" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">W.Chan，N. Jaitly，Q. V. Le 和 O. Vinyals .<a class="ae kv" href="https://arxiv.org/pdf/1508.01211.pdf" rel="noopener ugc nofollow" target="_blank">听，听，拼</a>。2015</li></ul></div></div>    
</body>
</html>