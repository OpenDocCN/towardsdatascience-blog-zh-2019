<html>
<head>
<title>Mathematical Intuition behind Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降背后的数学直觉</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mathematical-intuition-behind-gradient-descent-f1b959a59e6d?source=collection_archive---------9-----------------------#2019-05-08">https://towardsdatascience.com/mathematical-intuition-behind-gradient-descent-f1b959a59e6d?source=collection_archive---------9-----------------------#2019-05-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="620c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">梯度下降中更新规则的数学推导——机器学习和深度学习中最流行的优化算法</h2></div><blockquote class="ki"><p id="312c" class="kj kk it bd kl km kn ko kp kq kr ks dk translated">梯度下降是一种寻找函数最小值的迭代优化算法，最常用于机器学习和深度学习。</p></blockquote><h1 id="df9f" class="kt ku it bd kv kw kx ky kz la lb lc ld jz le ka lf kc lg kd lh kf li kg lj lk bi translated">介绍</h1><p id="0530" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">如果你在生活中曾见过或听说过“梯度下降”这个术语，你肯定会遇到下面这个等式:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/ea1f8d336ecee4bb4a64f3b35c87e394.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*oPyD4XpMj-R5SBIfAjqRUw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Gradient Descent - parameter update step</figcaption></figure><p id="9623" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">和下图:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/2d8e6922cdf31447554b5094ec41fd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*4wYeb3VpTIBb6wW8wYvOmQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Cost vs Weight in Gradient Descent</figcaption></figure><p id="5cce" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">在上式中，L 是损失函数(或成本函数)，而<em class="my"> θ </em>是成本函数所依赖的任何参数。在神经网络(或深度学习)的情况下，这些是权重(<em class="my"> W </em>)和偏差(<em class="my"> b </em>)。</p><p id="080a" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">目标是找到损失函数的全局最小值。这些参数在训练算法的每次迭代期间被更新，直到我们达到损失函数的最小值。</p><p id="d66d" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">在深度学习(或神经网络)的背景下，我们根据权重和偏差写出上述等式，如下:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ce393e429af5580a6500994e4190e9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*r5E1SAvrEwHag-PCsO2qLg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">weight and bias update in gradient descent</figcaption></figure><p id="04b0" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">这是梯度下降优化算法中的基本步骤，在训练的每次迭代期间执行。<br/>我们来数学推导一下这个方程(不要慌！你只需要高中基础微积分就可以做到这一点)。这样做之后，无论何时你遇到参数更新步骤，你都会知道它的起源，并且感觉更有力量！</p><h1 id="b45d" class="kt ku it bd kv kw kx ky kz la lb lc ld jz na ka lf kc nb kd lh kf nc kg lj lk bi translated">通用学习算法</h1><p id="9d25" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">让我们首先快速回顾一下一般的学习算法:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/69e28ccacddbcb984f2b281d7203c756.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*SLM-Ph9MF8QEly9bq5omTQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">general learning algorithm</figcaption></figure><p id="6472" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">这里的“直到满意”是一个主观条件，可以是许多停止标准中的一个，如达到阈值损失值或重复某个固定次数等。</p><h2 id="1930" class="nd ku it bd kv ne nf dn kz ng nh dp ld lu ni nj lf ly nk nl lh mc nm nn lj no bi translated">参数更新步骤</h2><p id="2e5b" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">注意，更新步骤包括添加一些变化δ<em class="my">w</em>，δ<em class="my">b</em>到<em class="my"> w </em>，<em class="my"> b </em>。我们很快就会发现那些确实是损失的<strong class="ln iu">负偏导数</strong> w.r.t <em class="my"> w </em>，<em class="my"> b </em>分别是<strong class="ln iu">-</strong><strong class="ln iu">𝛿l/𝛿<em class="my">w</em></strong>和<strong class="ln iu"> - 𝛿L/𝛿 <em class="my"> b </em> </strong>其中 L = f( <em class="my"> w </em>，<em class="my"> b </em>)。</p><p id="094e" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">让我们用数学公式来表达这些:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi np"><img src="../Images/7820c06515e9880b9e5fbc0622d86a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*3eWyIAmQT7aewSJcFQXs2Q.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/46076105504cd2b2ed7e9683d7d6cb14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brFPpfB9vxpq-fmLNjiAaA.png"/></div></div></figure><p id="b972" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">注意，我们还没有引入学习率α。我们先来了解一下学习率的需求。</p><h1 id="1433" class="kt ku it bd kv kw kx ky kz la lb lc ld jz na ka lf kc nb kd lh kf nc kg lj lk bi translated">需要学习率</h1><p id="f44c" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">我们知道<em class="my"> θ </em>是向量，δ<em class="my">θ</em>也是向量。<br/>让我们考虑这两个向量的和:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/a9940e877f4616b425a94a45064df1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*bCkMDYm2QaHGXUdh-q71CA.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">vector sum of <em class="nw">θ</em> and Δ<em class="nw">θ</em></figcaption></figure><p id="22b1" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">很明显，与单个矢量相比，两个矢量的合成和相当大。这是因为我们迈出了一大步δ<em class="my">θ</em>。我们需要在这个方向上迈出一小步，这样向量和就很小。这也很重要，因为如果我们对参数<em class="my"> θ </em>进行如此大的更新(δ<em class="my">θ</em>，我们可能会错过损失函数 l 的全局最小值。因此，我们引入学习率来限制我们对参数<em class="my"> θ </em>进行更新的大小。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d9b6f9dcc1842ec34756c5c6f8a74a8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*8i0Mg-lNHkw6KJwEByeNjw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">vector sum of <em class="nw">θ</em> and αΔ<em class="nw">θ</em></figcaption></figure><p id="b942" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">注意如何借助学习率α &lt; 1, we limit the amount of update we make to <em class="my"> θ </em>。</p><p id="272a" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">现在让我们找出δ<em class="my">θ</em>的正确值，它将减少损失值。</p><p id="47b9" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">在我们继续之前，让我介绍一下著名的泰勒级数，我们将用它来求δw，δb，从而得到δ<em class="my">θ</em></p><h1 id="3875" class="kt ku it bd kv kw kx ky kz la lb lc ld jz na ka lf kc nb kd lh kf nc kg lj lk bi translated">泰勒级数及其在梯度下降中的应用</h1><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ny"><img src="../Images/7a2d5049e1a214b128096f84034df29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9TjxcPbQLcT0KED2CCa0A.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Taylor series</figcaption></figure><p id="e287" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated"><a class="ae nz" href="https://en.wikipedia.org/wiki/Taylor_series" rel="noopener ugc nofollow" target="_blank">泰勒级数</a>用于找出距离 x 点δx 处的函数值，已知该函数在该点的导数。</p><p id="8093" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">让我们用泰勒级数来求δw 的值。<br/>在这种情况下，函数 f 将是损失函数 L，我们对 L 进行级数展开(<em class="my">w</em>+α*δ<em class="my">w</em>)。<br/>我们必须找到一个δ<em class="my">w</em>的值，使得 L(<em class="my">w</em>+α*δ<em class="my">w</em>)&lt;L(<em class="my">w</em>)。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oa"><img src="../Images/9b32135a51a05a39755d5a82d6939b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MunoEfU1pXFW8xZD931DPA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Taylor series for loss function in terms of w</figcaption></figure><p id="9fc2" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">在这一步，我们可以推断，我们需要第二项为负，新的损失小于旧的损失。</p><p id="21c0" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">但是 Loss <em class="my"> L(θ) </em>是一个多元函数。它不仅是重量<em class="my"> w </em>的函数，也是偏差<em class="my"> b </em>的函数。我们将它们表示为一个向量<em class="my"> θ </em> = [ <em class="my"> w </em>，<em class="my"> b </em>。<br/>所以我们需要写下泰勒级数的向量形式来求δ<em class="my">θ。</em></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ob"><img src="../Images/50452ed892af775d3bed738a4cfdcfbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZO3P44DWJi7bBQh0vObxw.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">vector form of Taylor series for parameter vector θ</figcaption></figure><p id="642c" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">这里∇ <em class="my"> L(θ) </em>代表损耗 w.r.t <em class="my"> θ的一阶梯度。<br/> </em>梯度无非是函数 w.r.t 对其每个参数的偏导数的向量。<br/>类似地∇将是一个向量的二阶偏导数等等。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi oc"><img src="../Images/be8274092757473ff935b589fd6cc241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkZYSUIQ0L-X6Wcx0RtABA.png"/></div></div></figure><p id="9d0a" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">实际上，学习率α非常小(0.001，0.00001 等。)，所以α，α，…将非常小，它们对损耗 L 的贡献可以忽略不计。因此它们可以从等式中忽略。最后的等式将变成</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi od"><img src="../Images/25e1ddcbe591a087796bf03a93fe5a71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvKOJuKsscx1ZMSd1KrNxA.png"/></div></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">updated equation for loss</figcaption></figure><h1 id="11a9" class="kt ku it bd kv kw kx ky kz la lb lc ld jz na ka lf kc nb kd lh kf nc kg lj lk bi translated">寻找δθ的最佳值</h1><p id="a8c7" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">由于我们希望更新后的损耗<em class="my">L(θ+α</em>δ<em class="my">θ)</em>小于之前的损耗<em class="my"> L(θ) </em>，并且由于损耗是一个正的量，上述等式中的第二项必须是负的。<br/>所以我们需要这样的δ值<em class="my"> θ </em>所以<em class="my">T63】使得第二项的点积为负，即我们需要</em></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/b88d9a1af929eaa569ab62c107abc4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*HMBfzzVRZfElQYlvY3v_qg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">condition for new loss to be negative</figcaption></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/f6c37aff6307a09fb4f71be58a4ef215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KY_Aib-qHNJkn9PAd2bXg.png"/></div></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi of"><img src="../Images/032b8565ed90023082f6887b985681a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*hGZzevPIoQyMPXWYpTbpiA.png"/></div></figure><p id="c8b9" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">我们知道这一点</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7de54309cc2d142bd699d75534384609.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*cmxQ62gYlnqMyZ-Bmg2n6g.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">cosine of angle between 2 vectors is their dot product divided by product of their magnitudes</figcaption></figure><p id="f0a7" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">我们还知道 cos <em class="my"> β </em>位于-1 和 1 之间，即-1 ⩽ cos <em class="my"> β </em> ⩽ +1。<br/>因此，</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2e468c7f721a2cdc99017b290bc30d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*cLcoP1JBAeJuhREoDOeXKQ.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/a4b93dcbedf82bf7bec5150815502118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*8aqwYlg2B_QStKGl4lcPgw.png"/></div></figure><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/56e3d43a470cad7fad9aa40ff26489d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*NY-KuYxRkZllsJ8YOInrdw.png"/></div></figure><p id="50ab" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">现在我们希望点积尽可能的负(这样损耗可以尽可能的低)<br/>但是从上面的不等式可以看出，它能获得的最大负值是-k。</p><p id="a9c8" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">现在，为了使点积为-k，cos <em class="my"> β </em>必须等于-1。<br/> cos <em class="my"> β </em> = -1 对应<em class="my"> β </em> = 180</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/4182cb47e0a22bc2c4e4938e1d8eee1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rW9SDAzPRXWNTpeflGqyuQ.jpeg"/></div></figure><p id="f555" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">由于这两个矢量方向相反，从矢量的性质我们知道</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi ol"><img src="../Images/8d7b232fc69da36eb187d6b8adc262c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*DCmPnfc_4_jS9B_R4XKS_Q.png"/></div></div></figure><h2 id="f8b7" class="nd ku it bd kv ne nf dn kz ng nh dp ld lu ni nj lf ly nk nl lh mc nm nn lj no bi translated">最终参数更新步骤</h2><p id="1a6b" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">现在，如果我们在<a class="ae nz" href="https://medium.com/p/f1b959a59e6d#b45d" rel="noopener">通用学习算法</a>的参数更新步骤中替换这些值，它变成</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3f8bfa93d4dbcfc834e966e77d602c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*09hZiNDautMrYSCQxRYRog.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">updated parameter update step in learning algorithm</figcaption></figure><p id="2af6" class="pw-post-body-paragraph ll lm it ln b lo ms ju lq lr mt jx lt lu mu lw lx ly mv ma mb mc mw me mf ks im bi translated">现在，我的朋友，这个等式与我们开始推导的完全相似。<br/>每次使用此规则更新参数(w 和 b)时，训练集的损失将会减少，直到不能再减少，即当斜率(或偏导数)变为 0 时。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi on"><img src="../Images/638bfc0d908df2163c79654652ad1340.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*B2bICFV7iI2bK2m2gUeVFg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Result in decrease of loss due to iterative update steps</figcaption></figure><h1 id="f972" class="kt ku it bd kv kw kx ky kz la lb lc ld jz na ka lf kc nb kd lh kf nc kg lj lk bi translated">多重权重和偏差的梯度下降规则(矢量化符号)</h1><p id="f591" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">我们已经导出了单个权重和偏差的更新规则。<br/>在现实中，深度神经网络具有许多权重和偏差，它们被表示为矩阵(或张量)，因此我们的更新规则也应该被修改，以同时更新网络的所有权重和偏差。</p><blockquote class="oo op oq"><p id="5b33" class="ll lm my ln b lo ms ju lq lr mt jx lt or mu lw lx os mv ma mb ot mw me mf ks im bi translated">注意，大多数深度学习文本使用符号δ而不是∇来表示方程中的梯度。</p></blockquote><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/83cac715a8ba14b2e29e85cc73b5117b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*q1i3E9T4t9cTTuWYacni8w.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">gradient descent update rule for deep neural network</figcaption></figure><h1 id="c807" class="kt ku it bd kv kw kx ky kz la lb lc ld jz na ka lf kc nb kd lh kf nc kg lj lk bi translated">结尾部分</h1><p id="8171" class="pw-post-body-paragraph ll lm it ln b lo lp ju lq lr ls jx lt lu lv lw lx ly lz ma mb mc md me mf ks im bi translated">这一个就到此为止。希望看完这些，你对梯度下降算法有了更直观的认识。</p></div></div>    
</body>
</html>