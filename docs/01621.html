<html>
<head>
<title>Review: V-Net — Volumetric Convolution (Biomedical Image Segmentation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:V-Net —体积卷积(生物医学图像分割)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974?source=collection_archive---------5-----------------------#2019-03-16">https://towardsdatascience.com/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974?source=collection_archive---------5-----------------------#2019-03-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7f1f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于体积医学图像分割的全卷积网络</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/7a29c02445f9d5295233444df8778229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*rsPDzIw12bKwbwAK01wKBA.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Slices from MRI volumes depicting prostate (PROMISE 2012 challenge dataset)</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ks"><img src="../Images/b8d8efb4377c55e91420f1cc63a4928b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*4woVLluGFnlr3ch0TLilsw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Qualitative results (PROMISE 2012 challenge dataset)</strong></figcaption></figure><p id="add2" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi lp translated"><span class="l lq lr ls bm lt lu lv lw lx di">在</span>这个故事中，<strong class="kv ir"> V-Net </strong>被简要回顾。临床实践中使用的大多数医学数据由 3D 体积组成，例如描绘前列腺的 MRI 体积，而大多数方法仅能够处理 2D 图像。提出了一种基于体完全卷积神经网络的三维图像分割方法。</p><p id="232b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">前列腺 MRI 体积分割是一项具有挑战性的任务，由于广泛的外观，以及不同的扫描方法。强度分布的变形和变化也会发生。</p><p id="fdaa" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">注释医学卷并不容易。注释需要专家，这产生了高成本。自动分段有助于降低成本。</strong></p><p id="b82c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">然而，前列腺分割是一项重要的任务，在诊断期间和治疗计划期间都具有临床相关性，在诊断期间需要评估前列腺的体积，在治疗计划期间需要精确估计解剖边界。这是一篇<strong class="kv ir"> 2016 3DV </strong>论文，引用<strong class="kv ir"> 600 多篇</strong>。(<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----aa15dbaea974--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><ol class=""><li id="7223" class="mh mi iq kv b kw kx kz la lc mj lg mk lk ml lo mm mn mo mp bi translated"><strong class="kv ir">虚拟网络架构</strong></li><li id="6afa" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo mm mn mo mp bi translated"><strong class="kv ir">骰子损失</strong></li><li id="9983" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo mm mn mo mp bi translated"><strong class="kv ir">结果</strong></li></ol></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="d638" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak"> 1。虚拟网络架构</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/bbf26135c1806409f3869289e6d537aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rcT-PbkROWrSg0PRqO-KAA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">V-Net Architecture</strong></figcaption></figure><ul class=""><li id="23f9" class="mh mi iq kv b kw kx kz la lc mj lg mk lk ml lo ns mn mo mp bi translated">v 网如上图。网络的<strong class="kv ir">左边部分</strong>由<strong class="kv ir">压缩路径</strong>组成，而<strong class="kv ir">右边部分对信号</strong>进行解压缩，直到达到其原始大小。</li><li id="db08" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">如你所见，它类似于<a class="ae nt" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760"> U-Net </a>，但有一些不同。</li></ul><h2 id="0fef" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">1.1.左边的</h2><ul class=""><li id="150d" class="mh mi iq kv b kw og kz oh lc oi lg oj lk ok lo ns mn mo mp bi translated">网络的左侧分为以不同分辨率运行的不同阶段。每一级包括一到三个卷积层。</li><li id="5739" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">在每个阶段，学习一个剩余函数。每一级的输入在卷积层中使用，通过非线性处理，并加到该级的最后一个卷积层的输出上，以便能够学习剩余函数。这种架构与<a class="ae nt" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760"> U-Net </a>等无残差学习网络相比，保证了收敛性。</li><li id="c406" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">在每个阶段执行的<strong class="kv ir">卷积</strong>使用大小为<strong class="kv ir"> 5×5×5 体素</strong>的<strong class="kv ir">体积核</strong>。(体素代表 3D 空间中规则网格上的值。术语体素通常用于 3D 空间，就像点云中的体素化一样。)</li><li id="5af6" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">沿着压缩路径，<strong class="kv ir">分辨率通过与步长为 2 </strong>的 2×2×2 体素宽内核进行卷积而降低。因此，生成的要素地图的大小减半，其目的<strong class="kv ir">与汇集图层</strong>相似。并且<strong class="kv ir">特征通道的数量在 V-Net 的压缩路径的每个阶段</strong>加倍。</li><li id="007a" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">由于反向传播不需要将池层的输出映射回它们的输入的开关，所以用卷积运算代替池运算有助于在训练期间具有更小的内存占用。</li><li id="55a1" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">下采样有助于增加感受野。</li><li id="e62f" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated"><strong class="kv ir"> PReLU </strong>用作非线性激活功能。(预走建议在<a class="ae nt" href="https://medium.com/coinmonks/review-prelu-net-the-first-to-surpass-human-level-performance-in-ilsvrc-2015-image-f619dddd5617" rel="noopener">预走网</a>中)。)</li></ul><h2 id="6cfc" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">1.2.对吧</h2><ul class=""><li id="e3f4" class="mh mi iq kv b kw og kz oh lc oi lg oj lk ok lo ns mn mo mp bi translated">网络提取特征，并且<strong class="kv ir">扩展较低分辨率特征图的空间支持</strong>,以便收集和组合必要的信息来输出双通道体积分割。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ol"><img src="../Images/7d8dec45c5fc6963355fa7ad12d34676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bl-7kOoc3sONYgm5InOUQ.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Convolution for Downsampling (Left), Deconvolution for Upsampling (Right)</strong></figcaption></figure><ul class=""><li id="df80" class="mh mi iq kv b kw kx kz la lc mj lg mk lk ml lo ns mn mo mp bi translated">在每个阶段，使用<strong class="kv ir">去卷积</strong>操作，以便<strong class="kv ir">增加输入的大小，随后是一到三个卷积层，<strong class="kv ir">涉及在前一层中使用的 5×5×5 核</strong>的一半数量。</strong></li><li id="afa5" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated"><strong class="kv ir">剩余函数</strong>被学习，类似于网络的左侧部分。</li><li id="cf66" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">这两个特征映射由最后一个卷积层的<strong class="kv ir">计算，具有<strong class="kv ir"> 1×1×1 核大小</strong>并产生与输入卷大小相同的<strong class="kv ir">输出。</strong></strong></li><li id="1ef3" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">这两个输出特征图是<strong class="kv ir">通过应用软最大体素法</strong>对前景和背景区域的概率分割。</li></ul><h2 id="7aa4" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">1.3.水平连接</h2><ul class=""><li id="d556" class="mh mi iq kv b kw og kz oh lc oi lg oj lk ok lo ns mn mo mp bi translated">与<a class="ae nt" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760"> U-Net </a>类似，<strong class="kv ir">位置信息在压缩路径中丢失(左)。</strong></li><li id="d8cd" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">因此，从 CNN 左边部分的早期阶段提取的特征通过水平连接被转发到右边部分。</li><li id="4ec1" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">这可以帮助<strong class="kv ir">向正确的部分提供位置信息，并提高最终轮廓预测的质量。</strong></li><li id="985c" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">并且这些连接改善了模型的收敛时间。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="589d" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">2.骰子损失</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c1537e09979f716d7ddafd1d5bbd8e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*2gfJ5OX7hzkIdCaAWKr-QA.png"/></div></figure><ul class=""><li id="5ed3" class="mh mi iq kv b kw kx kz la lc mj lg mk lk ml lo ns mn mo mp bi translated">上面是两个二进制卷之间的<strong class="kv ir">骰子系数<em class="on"> D </em>。(范围在 0 和 1 之间)</strong></li><li id="02d3" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">用<em class="on"> N </em>体素，<em class="on"> pi </em>:预测体素，<em class="on"> gi </em>:地面真实体素。</li><li id="8c87" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">如上所述，在 softmax 之后的网络末端，我们得到的输出是每个体素属于前景和背景的概率。</li><li id="cd88" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">和骰子可以被区分，产生梯度:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/f5a8791304a4666d0aeb1031858aa1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*7gZbfxfFl-wZgvcZ_7rqYQ.png"/></div></figure><ul class=""><li id="c4d2" class="mh mi iq kv b kw kx kz la lc mj lg mk lk ml lo ns mn mo mp bi translated">使用骰子损失，不需要对不同类别的样本进行加权来建立前景和背景体素之间的正确平衡。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="1b40" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">3.结果</h1><h2 id="2996" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">3.1.培养</h2><ul class=""><li id="8840" class="mh mi iq kv b kw og kz oh lc oi lg oj lk ok lo ns mn mo mp bi translated">网络处理的所有体积都具有 128×128×64 体素的<strong class="kv ir">固定尺寸和 1<strong class="kv ir">×1<strong class="kv ir">×1.5 毫米的空间分辨率。</strong></strong></strong></li><li id="e783" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">数据集很小，因为需要一个或多个专家来手动追踪可靠的基本事实注释，并且存在与他们的获取相关联的成本。</li><li id="ec9c" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated"><strong class="kv ir">只有 50 个 MRI 体积用于训练</strong>，相对手动地面实况标注从<strong class="kv ir"> PROMISE 2012 challenge 数据集</strong>获得。</li><li id="ab62" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">该数据集包含在<strong class="kv ir">不同医院</strong>采集的医疗数据，使用<strong class="kv ir">不同设备</strong>和<strong class="kv ir">不同采集协议</strong>，以表示临床可变性。</li><li id="84b5" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">因此，需要增加数据。对于每次迭代，使用通过 2×2×2 控制点网格和 B 样条插值获得的<strong class="kv ir">密集变形场</strong>的训练图像的随机变形版本。</li><li id="5866" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">此外，<strong class="kv ir">通过使用直方图匹配使每次迭代中使用的训练体积的强度分布适应属于数据集的其他随机选择的扫描的强度分布，来改变数据的强度分布</strong>。</li><li id="0b4e" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">小批量，只有 2 卷，由于内存要求高。</li></ul><h2 id="b276" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">3.2.测试</h2><ul class=""><li id="7d04" class="mh mi iq kv b kw og kz oh lc oi lg oj lk ok lo ns mn mo mp bi translated">处理不可见的 30 个 MRI 体积。</li><li id="a99d" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">softmax 之后的体素属于前景的概率高于属于背景的概率(&gt; 0.5)，被认为是解剖结构的一部分。</li><li id="70e0" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">测量了 Dice 系数和 Hausdorff 距离。</li><li id="5850" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">Hausdorff 距离是用来衡量形状相似性的。Hausdorff 距离是获得两个形状之间的最大距离。(如果感兴趣，关于豪斯多夫距离的非常简要的介绍在<a class="ae nt" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener"> CUMedVision2 / DCAN </a>。)</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi op"><img src="../Images/20d9e14c0185d0548098be6a8c60809e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxx5hzmHVTOhUGkMHMaouA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Quantitative Comparison on the PROMISE 2012 Challenge Dataset</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi oq"><img src="../Images/5abffb0c0bddb005210489fd65940ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U3jpuXVM_gS9ApoZuPJMDw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Dice Loss (Green) Logistic Loss (Yellow)</strong></figcaption></figure><ul class=""><li id="da3f" class="mh mi iq kv b kw kx kz la lc mj lg mk lk ml lo ns mn mo mp bi translated">如上所示，使用骰子损失的 V-Net 优于使用逻辑损失的 V-Net。</li><li id="723d" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">并且 V-Net 优于大多数现有技术，但不仅仅是 Imorphics。</li><li id="1a62" class="mh mi iq kv b kw mq kz mr lc ms lg mt lk mu lo ns mn mo mp bi translated">作者在未来的工作中提到，他们将致力于在其他模态(如超声)中分割包含多个区域的体积，并通过在多个 GPU 上分割网络来实现更高的分辨率。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="6403" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">参考</h2><p id="d519" class="pw-post-body-paragraph kt ku iq kv b kw og jr ky kz oh ju lb lc or le lf lg os li lj lk ot lm ln lo ij bi translated">【2016 3DV】【V-Net】<br/><a class="ae nt" href="https://arxiv.org/abs/1606.04797" rel="noopener ugc nofollow" target="_blank">V-Net:用于体医学图像分割的全卷积神经网络</a></p><h2 id="3937" class="nu mw iq bd mx nv nw dn nb nx ny dp nf lc nz oa nh lg ob oc nj lk od oe nl of bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kt ku iq kv b kw og jr ky kz oh ju lb lc or le lf lg os li lj lk ot lm ln lo ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(情)(况)(,)(还)(是)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">物体检测<br/></strong><a class="ae nt" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae nt" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae nt" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae nt" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae nt" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae nt" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae nt" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a><a class="ae nt" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">ION</a><a class="ae nt" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath Net</a>【T21 [ <a class="ae nt" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae nt" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae nt" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae nt" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae nt" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae nt" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">语义切分<br/></strong><a class="ae nt" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae nt" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae nt" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a>】<a class="ae nt" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae nt" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae nt" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae nt" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae nt" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae nt" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">生物医学图像分割<br/></strong>[<a class="ae nt" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae nt" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae nt" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae nt" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae nt" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae nt" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a></p><p id="3134" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated"><strong class="kv ir">实例分割 <br/> </strong> <a class="ae nt" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS </a> <a class="ae nt" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">DeepMask </a> <a class="ae nt" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">SharpMask </a> <a class="ae nt" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">MultiPathNet </a> <a class="ae nt" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC </a>】 <a class="ae nt" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">InstanceFCN </a> <a class="ae nt" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS </a></p><p id="58de" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p><p id="f29d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">(T38) 人类姿势估计 (T39) <br/> [(T41) 汤普森 NIPS'14 [T42)]</p></div></div>    
</body>
</html>