<html>
<head>
<title>Mixed precision training using fastai</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 fastai 的混合精确训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mixed-precision-training-using-fastai-435145d3178b?source=collection_archive---------7-----------------------#2019-04-13">https://towardsdatascience.com/mixed-precision-training-using-fastai-435145d3178b?source=collection_archive---------7-----------------------#2019-04-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/12c17a5cfd079204598451b3daf42e9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Us28ILk64clAO21F8_R9w.jpeg"/></div></div></figure><h2 id="2f7e" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">介绍</h2><p id="b4e7" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">图像分类是深度学习的 Hello World。对我来说，这个项目是用胸部 x 光检测肺炎(T2)。由于这是一个相对较小的数据集，我可以在大约 50 分钟内训练我的模型。但是如果我告诉你，只要增加一行代码</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/227550b294f9fe5639d303940054658c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jva1ZNsGpdaHBSbGTYGcQ.png"/></div></div></figure><p id="2971" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">我们可以减少训练时间(理论上减少 50%)，而不会显著降低精确度。但是首先…</p><h2 id="081a" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">为什么这很重要？</strong></h2><p id="66e3" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">我使用的数据集包括大约 4500 张图片。它花了 50 分钟的唯一原因是因为图像是高清的。</p><p id="ffa0" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">然而，如果我们将同一个项目扩展到现实世界的应用程序，可能会有更多的图像。看看<a class="ae lp" href="https://stanfordmlgroup.github.io/competitions/chexpert/" rel="noopener ugc nofollow" target="_blank">斯坦福的 Chexpert 数据集</a>。</p><p id="c469" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">此外，我训练了一个模型来预测一种疾病，但在现实中，我们将不得不预测比两类疾病更多的疾病。在这种情况下，减少培训时间确实是一个优势。这将降低资源成本，帮助我们更快地进行实验。那么我们该怎么做呢？</p><p id="6086" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">我们变得不那么精确。</p><blockquote class="ma mb mc"><p id="ca25" class="ku kv md kw b kx lv kz la lb lw ld le me lx lg lh mf ly lj lk mg lz lm ln lo ij bi translated">事实证明，有时，在深度学习中使事情不那么精确会使它更好地概括。</p><p id="5869" class="ku kv md kw b kx lv kz la lb lw ld le me lx lg lh mf ly lj lk mg lz lm ln lo ij bi translated">杰瑞米·霍华德</p></blockquote><h2 id="2663" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">神经网络中的精度</h2><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/1586a22097c68cc1468f04bd410c91e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*VAVfRwcQmq923ts1Kx-Q1Q.png"/></div></figure><p id="c583" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">在神经网络中，所有的浮点数，即我们的输入、权重和激活都是用 32 位存储的。使用 32 位给了我们很高的精度。但是更高的精度也意味着需要更多的计算时间和更多的内存来存储这些变量。如果我们只用 16 位呢？</p><h2 id="c6d8" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">半精度</strong></h2><p id="8c20" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">减少内存使用的一种方法是以一半精度(16 位)执行所有操作。</p><p id="774e" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">根据定义，这将占用 RAM 中一半的空间，并且理论上可以让您的批处理大小翻倍。增加的批量意味着并行执行更多的操作，从而减少培训时间。然而，这也带来了一些问题。</p><h2 id="e1ff" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">半精度的问题</h2><ol class=""><li id="45bc" class="mi mj iq kw b kx ky lb lc kh mk kl ml kp mm lo mn mo mp mq bi translated"><strong class="kw ir">不精确的重量更新:</strong></li></ol><p id="5781" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">我们将模型的权重更新如下:</p><pre class="lr ls lt lu gt mr ms mt mu aw mv bi"><span id="4453" class="jy jz iq ms b gy mw mx l my mz">w = w - learning_rate * w.gradient</span></pre><p id="9b02" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">以半精度执行该运算的问题是,<code class="fe na nb nc ms b">w.grad</code>通常非常小，我们的<code class="fe na nb nc ms b">learning_rate</code>也是如此，这会使等式的第二项非常小，以至于根本不会发生更新。</p><p id="ae07" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">2.<strong class="kw ir">梯度下溢:</strong></p><p id="5d44" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">类似于不精确的权重更新，如果我们的梯度太小(低于可以使用 16 位表示的值)，它们将被转换为 0。</p><p id="0cd0" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">3.<strong class="kw ir">激活爆炸:</strong></p><p id="3363" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">一系列矩阵乘法(前向传递)很容易导致神经网络的激活(输出)变得如此之大，以至于它们达到 NaN(或无穷大)</p><p id="2518" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">解决这些问题的办法就是使用<strong class="kw ir"> <em class="md">混合精度训练</em> </strong>。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/02fd71630345788e9fdabc40210c3439.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HTfvmT3VMEEyYSdA-SCHw.png"/></div></div><figcaption class="ne nf gj gh gi ng nh bd b be z dk">source: <a class="ae lp" href="https://arxiv.org/pdf/1710.03740.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1710.03740.pdf</a></figcaption></figure><h2 id="2b2e" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">混合精确训练</h2><p id="abea" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated"><a class="ae lp" href="https://www.kaggle.com/dipam7/mixed-precision-on-pneumonia-using-fastai" rel="noopener ugc nofollow" target="_blank">全朱庇特笔记本。</a></p><p id="d74c" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">顾名思义，我们做任何事情都不会有一半的精度。我们在 FP16 中执行一些操作，而在 FP32 中执行其他操作。更具体地说，我们以 32 位精度进行权重更新。这解决了问题#1。</p><p id="8094" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">为了克服梯度下溢，我们使用称为<strong class="kw ir">梯度缩放</strong>的东西。我们将损失函数乘以一个比例因子。我们这样做是为了避免梯度下降到 FP16 可以表示的范围之下，从而避免它被 0 取代。我们还要确保缩放因子不会大到导致我们的激活溢出。</p><p id="77ae" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">使用这些思想，我们避免了半精度的问题，并有效地训练我们的网络。</p><h2 id="0c61" class="jy jz iq bd ka kb kc dn kd ke kf dp kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">最后的想法</h2><p id="2058" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le kh lf lg lh kl li lj lk kp ll lm ln lo ij bi translated">使用混合精度训练的想法只出现了几年，并不是所有的 GPU 都支持它。但是这是一个值得了解的想法，并且在将来会被更多的使用。</p><p id="281a" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">结果表明，该方法在不影响精度的情况下，减少了训练时间。</p><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi lq"><img src="../Images/c0b8c1e81223e5a834467c933e0f72cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n-cGQBiBwKQ1kO9bvjxI-g.png"/></div></div></figure><figure class="lr ls lt lu gt jr gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/469aac5ddb8ae6c5d44ba221e1fb7389.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*KMVQp4rwTKNpKrn9RmrHBA.png"/></div></figure><p id="0e86" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">这就是本文的全部内容。</p><p id="258b" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">如果你想了解更多关于深度学习的知识，可以看看我在这方面的系列文章:</p><div class="nj nk gp gr nl nm"><a href="https://medium.com/@dipam44/deep-learning-series-30ad108fbe2b" rel="noopener follow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">深度学习系列</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">我所有关于深度学习的文章的系统列表</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">medium.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa jw nm"/></div></div></a></div><p id="2026" class="pw-post-body-paragraph ku kv iq kw b kx lv kz la lb lw ld le kh lx lg lh kl ly lj lk kp lz lm ln lo ij bi translated">~快乐学习。</p></div></div>    
</body>
</html>