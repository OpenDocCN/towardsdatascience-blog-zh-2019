<html>
<head>
<title>Different types of regularization On Neuronal Network with Pytorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pytorch 神经网络的不同类型正则化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/different-types-of-regularization-on-neuronal-network-with-pytorch-a9d6faf4793e?source=collection_archive---------14-----------------------#2019-05-06">https://towardsdatascience.com/different-types-of-regularization-on-neuronal-network-with-pytorch-a9d6faf4793e?source=collection_archive---------14-----------------------#2019-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec34" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">py torch 中正则化工具的实现</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/c9de22b67b7f214b4f6622c865f2c8f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JG9jwKZh-uHTJG_bdtgpvg.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Image by <a class="ae kz" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3098693" rel="noopener ugc nofollow" target="_blank">Gerd Altmann</a> from <a class="ae kz" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3098693" rel="noopener ugc nofollow" target="_blank">Pixabay</a></figcaption></figure><p id="8b03" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我在做一些关于神经元网络框架中的<em class="lw">稀疏性</em>的研究，我看到了文章“<a class="ae kz" href="https://www.sciencedirect.com/science/article/pii/S0925231217302990." rel="noopener ugc nofollow" target="_blank"> <em class="lw">深度神经网络的组稀疏正则化</em> </a> <em class="lw">”。本文</em>解决了同时优化(I)神经网络的权重，(ii)每个隐藏层的神经元数量，以及(iii)活动输入特征子集(即特征选择)的挑战性任务。他们基本上提出了在神经元网络的正则化框架中整合<em class="lw">套索惩罚</em>。关于<em class="lw">正规化的定义和重要性，有大量的帖子和广泛的研究。</em>比如这些帖子<em class="lw"/><a class="ae kz" rel="noopener" target="_blank" href="/l1-and-l2-regularization-methods-ce25e7fc831c"><em class="lw">【1】</em></a><em class="lw"/><em class="lw"/><a class="ae kz" rel="noopener" target="_blank" href="/over-fitting-and-regularization-64d16100f45c"><em class="lw">【2】</em></a><em class="lw"/>都是对<em class="lw">正规化</em>的快速介绍。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="ef90" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了重现深度神经网络<em class="lw"/><em class="lw"/>中的实验，我<em class="lw"> </em>决定使用众所周知的深度学习框架<a class="ae kz" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>来构建我的项目。据 Medium 称，这是第<a class="ae kz" href="https://medium.com/the-mission/8-best-deep-learning-frameworks-for-data-science-enthusiasts-d72714157761" rel="noopener">第四</a>最佳深度学习框架。它在学术层面越来越受欢迎，因为它易于模块化，比大多数竞争对手更快地测试新想法。</p><p id="c215" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我意识到没有这样的工具可以做任何类型的正则化，除非 L2 正则化。在 pyTorch 中，L2 是在<em class="lw">优化器</em>的“<em class="lw">权重衰减</em>选项中实现的，不同于<a class="ae kz" href="https://github.com/Lasagne/Lasagne/tree/master/lasagne" rel="noopener ugc nofollow" target="_blank"> <em class="lw">千层面</em> </a> <em class="lw">(另一个深度学习框架)，</em>在其内置实现中提供了 L1 和 L2 正则化。您可以找到许多<a class="ae kz" href="https://discuss.pytorch.org/t/simple-l2-regularization/139/2" rel="noopener ugc nofollow" target="_blank"> <em class="lw">建议的解决方案</em> </a>解释如何使用 pyTorch 框架实现 L1 正则化，但是没有简单的实现工具来插入您的实验。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="2e91" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了弥补这种缺失，我决定使用 pyTorch 框架构建一些<em class="lw"/>“随时可用”的正则化对象。实现可以在<a class="ae kz" href="https://github.com/dizam92/pyTorchReg/blob/master/src/regularization/regularizer.py" rel="noopener ugc nofollow" target="_blank"> <em class="lw">这里找到</em> </a>。我实现了<a class="ae kz" href="https://github.com/dizam92/pyTorchReg/blob/5ad3ca0a0cc7561741871a4135c2c08bc0bbef6c/src/regularization/regularizer.py#L21" rel="noopener ugc nofollow" target="_blank"> L1 </a>正则化、经典<a class="ae kz" href="https://github.com/dizam92/pyTorchReg/blob/5ad3ca0a0cc7561741871a4135c2c08bc0bbef6c/src/regularization/regularizer.py#L44" rel="noopener ugc nofollow" target="_blank"> L2 </a>正则化、<a class="ae kz" href="https://github.com/dizam92/pyTorchReg/blob/5ad3ca0a0cc7561741871a4135c2c08bc0bbef6c/src/regularization/regularizer.py#L67" rel="noopener ugc nofollow" target="_blank"> ElasticNet </a>正则化(L1 + L2)、<a class="ae kz" href="https://github.com/dizam92/pyTorchReg/blob/5ad3ca0a0cc7561741871a4135c2c08bc0bbef6c/src/regularization/regularizer.py#L124" rel="noopener ugc nofollow" target="_blank"> GroupLasso </a>正则化和一个更具限制性的惩罚<a class="ae kz" href="https://github.com/dizam92/pyTorchReg/blob/5ad3ca0a0cc7561741871a4135c2c08bc0bbef6c/src/regularization/regularizer.py#L99" rel="noopener ugc nofollow" target="_blank"> SparseGroupLasso </a>，这是在<em class="lw">组稀疏正则化中为深度神经网络引入的。</em></p><p id="cd1d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">类对象被构建为将 pyTorch <em class="lw">模型作为参数。</em>我们可以对特定的<em class="lw">参数</em>权重或模型中每一层的所有权重应用正则化惩罚。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi me"><img src="../Images/ff2b81981316f085e48e0f380055ae0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lB2rOFbuy6pJOZxdrYmCog.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Base model of regularizer object</figcaption></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mf"><img src="../Images/5533dd90d68ea4b5836f1aa1e9ca50cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gvR6A1x89cEwav_dZUqUhw.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Example of implementation of the L1 regularizer</figcaption></figure><p id="62bc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我将这些实现应用于完全连接的神经元网络。这是最简单的模型架构，用于查看不同类型的正则化对模型性能的影响。</p><p id="607b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">像最初的文章一样，我在代表图像分类问题的<a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html" rel="noopener ugc nofollow" target="_blank">数字</a>数据集上测试了实现。它由从几十个不同的人那里收集的 1797 个 8 × 8 的手写数字灰度图像组成。我使用了一个简单的全连接神经网络，它有两个隐藏层，分别有 40 和 20 个神经元。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mg"><img src="../Images/243d1352276d3aaaa2d1b97e0df9d492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzelW6dOP90dnT_LSgChGQ.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Results Performances with different type of regularization applied to the simple FC model</figcaption></figure><p id="5d53" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从结果可以看出，<strong class="lc iu"> L2 正则化</strong>模型和<strong class="lc iu">权重衰减正则化</strong>模型在测试集上具有相同的性能。这很好，也是我们所期望的，因为<strong class="lc iu">重量衰减</strong>是 pyTorch 已经提出的 L2 实现。较稀疏的方法(L1 正则化和 GL 正则化模型)也表现得相当好，但它们并不比<strong class="lc iu">权重衰减正则化</strong>模型更好。我们现在可以看看模型的真正稀疏性。为了对此进行研究，我们将绝对值低于 10e-3 的权重设置为 0。然后我们分析了每个模型中神经元的所有稀疏百分比。稀疏度百分比的计算方法是，取所有权重总和为 0 的神经元的数量，并将该数量除以每层的大小。正如所料，<strong class="lc iu"> L1 正则化</strong>模型比<strong class="lc iu"> L2 正则化</strong>模型具有更稀疏的密度。但是在这种情况下，性能仍然很好。有时候，太多的稀疏对模型也不好。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mh"><img src="../Images/d25bfe8630826381b27ae1b8552c9f63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xIHWju0DmLkQpKiARTAGWQ.png"/></div></div></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><p id="2598" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">总之，我提供了一个不同类型正则化选项的内置和自适应实现。我在 Digits 数据集上的一个更简单的全连接模型上测试了所有正则化，以查看不同正则化之间的性能。我们可以看到稀疏性在这些情况下仍然可以很好地执行任务。下一步是看看我们如何将<a class="ae kz" href="https://github.com/dizam92/pyTorchReg" rel="noopener ugc nofollow" target="_blank"> pyTorchReg </a>模块扩展到其他模型架构。</p></div></div>    
</body>
</html>