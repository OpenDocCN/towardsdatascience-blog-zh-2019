<html>
<head>
<title>Optimizing Neural Networks — Where to Start?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">优化神经网络——从哪里开始？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/optimizing-neural-networks-where-to-start-5a2ed38c8345?source=collection_archive---------4-----------------------#2019-01-10">https://towardsdatascience.com/optimizing-neural-networks-where-to-start-5a2ed38c8345?source=collection_archive---------4-----------------------#2019-01-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="66d7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">通过使用 Google Colab 中的 Keras 构建和调整神经网络来发展直觉</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3964b24a46619de1daad262dbd2b60c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkGVVr21QaFs6BOTJb-6tA.jpeg"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/photos/f_0t4fYEauU?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Adi Goldstein</a> on <a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bfab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di"> T </span>神经网络要调优的参数和超参数(以下都称为参数)非常多，那么从哪里开始呢？</p><p id="0b25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在吴恩达教授的深度学习专业化课程中，他给出了以下指导方针:</p><ul class=""><li id="e4ce" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">从<em class="mk">学习率</em>开始；</li><li id="32f6" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">然后试<em class="mk">隐藏单元数量</em> s、<em class="mk">小批量尺寸</em> e 和<em class="mk">动量项</em>；</li><li id="2dde" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">最后，调整<em class="mk">层数</em>和<em class="mk">学习率衰减</em>。</li></ul><p id="dc63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些都是很好的建议。但是，为了使它们成为我们技能的一部分，我们需要直觉:)为了实现这一点，我用 Python 构建了一个可定制的神经网络类，并进行了一系列实验来验证这些想法。让我们看看！</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="b42a" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">设置环境</h1><p id="3ce4" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">我们将在这个项目中使用<a class="ae kv" rel="noopener" target="_blank" href="/getting-started-with-tensorflow-in-google-colaboratory-9a97458e1014"> Google Colab </a>，所以大部分库都已经安装好了。因为我们将训练神经网络，所以使用 GPU 来加速训练是很重要的。</p><p id="7900" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要启用 GPU，只需进入下拉菜单中的“运行时”并选择“更改运行时类型”。然后，您可以将鼠标悬停在右上角的“已连接”上进行验证:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a1faf9b77ef44a4bc1cd2ed6a4392dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*42vjVUAdlvXOrVEUtGv-5Q.png"/></div></figure><h1 id="8967" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">获取数据</h1><p id="dc81" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">在本项目中，我们将使用皮马印第安人糖尿病数据集，原因如下:</p><ul class=""><li id="5328" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">数据集具有挑战性，最高精度结果只有 77%左右，这给了我们做大量模型调整的机会；</li><li id="7dbd" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">数据集很小，只有 768 行和 9 列。这使得我们可以更快地训练，从而可以进行 10 重交叉验证，以更好地表示模型性能。</li></ul><p id="42c7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然我们可以手动下载数据集，但为了重现性，还是从 Kaggle 下载吧。因为我们需要使用 Kaggle 的 API，所以我们将首先通过访问 Kaggle 上的“我的帐户”页面来创建 API 令牌。这会将一个<code class="fe oa ob oc od b">kaggle.json</code>文件下载到您的计算机上。</p><p id="f0bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们需要将这个凭证文件上传到 Colab:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="bac8" class="oi my iq od b gy oj ok l ol om">from google.colab import files<br/>files.upload()</span></pre><p id="0e6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以安装 Kaggle API 并将凭证文件保存在。kaggle”目录。</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="8d45" class="oi my iq od b gy oj ok l ol om">!pip install -U -q kaggle<br/>!mkdir -p ~/.kaggle<br/>!cp kaggle.json ~/.kaggle/</span></pre><p id="8084" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以下载数据集了:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="bdc9" class="oi my iq od b gy oj ok l ol om">!kaggle datasets download -d uciml/pima-indians-diabetes-database</span></pre><p id="d644" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该数据集将被下载到您当前的工作目录，即 Colab 中的“content”文件夹。由于每次重启 Colab 会话时文件都会被删除，因此将文件保存在 Google Drive 中是个好主意。您只需要使用下面的代码安装驱动器并保存在那里:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="968b" class="oi my iq od b gy oj ok l ol om">from google.colab import drive<br/>drive.mount('/content/gdrive')</span></pre><p id="fed4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦安装完毕，你就可以通过“/content/gdrive”路径直接从 Google Drive 加载数据。当您需要保存绘图文件时，安装 Google Drive 也会派上用场。</p><h1 id="19ce" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">带有 XGBoost 的基线模型</h1><p id="6073" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">XGBoost 因其高准确性和高效率而被称为 go-to 算法。让我们试一试！</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="45ba" class="oi my iq od b gy oj ok l ol om">t1 = time()<br/>clf = xgb.XGBClassifier()<br/>cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)<br/>scores = cross_val_score(clf, X, y, cv=cv)<br/>t2 = time()<br/>t = t2 - t1</span><span id="6275" class="oi my iq od b gy on ok l ol om">print("Mean Accuracy: {:.2%}, Standard Deviation: {:.2%}".format(scores.mean(), scores.std()))<br/>print("Time taken: {:.2f} seconds".format(t))</span></pre><p id="7e29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们得到了 74.88%的准确率，只用了 0.35 秒！如果我们将特征标准化并再次测试，我们将得到 76.31%的结果！这个结果已经非常接近这个数据集上的最新精度。</p><h1 id="22d8" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">创建模型</h1><p id="f3bf" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">为了能够测试不同的模型，我们需要动态创建模型的能力。同时，我们还需要测试模型并提供结果。这两种需求都让我想到了面向对象编程。然后我创建了下面的测试类。我将在另一篇文章中解释这一部分和下一部分的技术细节。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><h1 id="6504" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">自动化测试</h1><p id="3ec3" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">因为我们需要测试许多不同的参数组合，并且需要保存结果，所以自动化测试过程很重要。同样，让我展示而不是讲述，因为细节将在后面的帖子中解释:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="oo op l"/></div></figure><h1 id="0b97" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">基线神经网络模型</h1><p id="c9dc" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">让我们从具有以下默认参数的基线模型开始:</p><ul class=""><li id="4586" class="mb mc iq ky b kz la lc ld lf md lj me ln mf lr mg mh mi mj bi translated">输入尺寸=8</li><li id="decb" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">层数=2</li><li id="648e" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">数量单位=8</li><li id="7791" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">激活='relu '</li><li id="c27a" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">activation_out='sigmoid '</li><li id="6bf8" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">损失= '二元交叉熵'</li><li id="50e4" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">初始值设定项='random_uniform '</li><li id="ad5b" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">优化器='adam '</li><li id="916c" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">学习率=0.001</li><li id="7693" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">度量=['准确性']</li><li id="86c6" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">纪元=10</li><li id="351b" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">batch_size=4</li><li id="be64" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">one_hot=False</li></ul><p id="1bc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们跑:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="bbab" class="oi my iq od b gy oj ok l ol om">param_dict_defaults, param_dict = get_defaults(), get_defaults()<br/>accuracy_baseline = run_test(X=X, y=y, param_dict=param_dict_defaults)</span></pre><p id="eef3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们会得到:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="4c00" class="oi my iq od b gy oj ok l ol om">Finished cross-valiation. Took 1.5 mintues. Mean Accuracy: 71.61%, Standard Deviation: 2.92%</span></pre><p id="87ac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还不错，但肯定离 77.7%的顶级成绩差远了。</p><h1 id="b931" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">不同参数的重要性</h1><p id="e545" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">为了理解不同参数对模型调整的影响，让我们一次调整一个参数，同时保持其他参数不变(因此不同于 sklearn 中的 GridSearchCV 等穷举搜索)。运行测试将为我们提供以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/6cbee35752924b00b0e4ce89d71f0121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6xGhj7_q0d7cZlbaq8lhxQ.png"/></div></div></figure><p id="0029" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，有趣的是，上面的参数调优指南中没有提到的一些参数可能是重要的因素，例如优化器和时期。</p><p id="6c1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二，学习率确实是最有影响力的参数之一。</p><p id="7dd8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三，对于这个特定的实验(包括参数选择)，似乎层数比隐藏单元的数量更重要。这与上述准则相违背。</p><p id="5e20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面是调整趋势，可用于查找要调整的范围。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/8382ccd972bc78f9585affcd965708d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3oaMzV1US0rD2wkWyTs7eQ.png"/></div></div></figure><p id="94f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">重要的是要注意，这里的测试只是为了提供一些直觉，不应该作为正式的规则。这是由于至少两个原因——一，各种参数及其候选值不一定具有可比性；第二，神经网络中存在天生的随机性，因此，如上图所示的结果可能会改变。</p><p id="20ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然参数值之间的相互作用很可能很重要，即 40 个历元在与非 0.001(例如 0.1)的学习率配对时可能会产生更差的准确性，但我们仍将在此尝试一种简单的方法——组合独立调整的最佳参数值并训练一个模型，这为我们提供了:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="4394" class="oi my iq od b gy oj ok l ol om">Finished cross-valiation. Took 49.3 mintues. Mean Accuracy: 78.00%, Standard Deviation: 4.59%</span></pre><p id="9613" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哇，这是一个残酷的 50 分钟！虽然我们不能抱怨结果，因为这是最先进的！看起来天真的方法确实有效。</p><h1 id="18bd" class="mx my iq bd mz na nv nc nd ne nw ng nh jw nx jx nj jz ny ka nl kc nz kd nn no bi translated">参数调谐</h1><p id="7eb7" class="pw-post-body-paragraph kw kx iq ky b kz np jr lb lc nq ju le lf nr lh li lj ns ll lm ln nt lp lq lr ij bi translated">现在我们已经看到了参数的相对重要性，是时候调整模型了。因为学习速度是最重要的，所以让我们先解决它。我们将使用下面的代码来生成 6 个介于 0.0001 和 0.01 之间的随机学习率值，因为根据上面的优化趋势可视化，这是最有希望的区域。</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="3dde" class="oi my iq od b gy oj ok l ol om">bases = np.repeat(10, 3)<br/>exponents_1 = -(np.random.rand(3) + 3) <br/>exponents_2 = -(np.random.rand(3) + 2) </span><span id="c4cf" class="oi my iq od b gy on ok l ol om">learning_rate = np.power(bases, exponents_1).tolist() + np.power(bases, exponents_2).tolist()</span></pre><p id="5ed9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">运行测试后，我们得到了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/d71fdd186f59628b303bceb45bc054e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*ooPdExfeBLxWijlgswOG2Q.png"/></div></figure><p id="223e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这使我们认为 0.0006716184352348816 是最佳学习率。让我们利用这一点，继续用 6 个选项来调整批量大小，因为我们肯定要相信 ng 教授的指导方针，即批量大小是第二重要的参数:)</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="e00c" class="oi my iq od b gy oj ok l ol om">batch_size = [2 ** e for e in range(6)]</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/11c58f66904de75021fedb5c4b0463ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*Y9aqPp_b7Kpc7gTiTe9G6w.png"/></div></figure><p id="be41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管批量大小为 2 的结果更准确，但是时间成本远远超过了收益，所以我们将采用批量大小为 16 的结果。</p><p id="f2c1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在更新了参数字典中的批处理大小值之后，我们现在可以继续调整时期数了。由于训练和测试的时间会随着时期数的增加而增加，所以最好在以后的阶段调整这个参数，以避免长时间的运行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/3e8b196b36a439d12c4511550874f8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*DmDpVz6NLrlB1mbyMOqh-A.png"/></div></figure><p id="7894" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给了我们最好的 200 个历元。接下来，让我们构建具有标准化功能的最终模型:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="af22" class="oi my iq od b gy oj ok l ol om">run_test(X=X_std, y=y, param_dict=param_dict)</span></pre><p id="481d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给了我们:</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="0995" class="oi my iq od b gy oj ok l ol om">Finished cross-valiation. Took 8.3 mintues.<br/>Mean Accuracy: 78.53%, Standard Deviation: 3.64%</span></pre><p id="8ac8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">绝对伟大的结果！所用时间不算太差，虽然比 XGBoost 多了 1422 倍😂</p><p id="ae1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们不调整参数，只标准化特性会怎么样呢？</p><pre class="kg kh ki kj gt oe od of og aw oh bi"><span id="7e41" class="oi my iq od b gy oj ok l ol om">Finished cross-valiation. Took 1.7 mintues. Mean Accuracy: 76.95%, Standard Deviation: 2.88%</span></pre><p id="6049" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，参数调整的效果似乎有点微不足道，但标准化，即使特征具有零均值和单位方差，对于神经网络模型调整来说是巨大的。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="3568" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">摘要</h1><ul class=""><li id="2963" class="mb mc iq ky b kz np lc nq lf ov lj ow ln ox lr mg mh mi mj bi translated">学习率是需要调整的最重要的参数，因为它可以产生很大的性能改进，同时不会对训练时间产生负面影响。</li><li id="028c" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">较小的批量可能会提供更好的结果，但也更耗时！同样，针对更多纪元的训练通常有助于提高准确度，但时间成本也很高。</li><li id="968d" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">优化器可能是一个需要优化的重要参数。</li><li id="3e58" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">更深更广的神经网络可能并不总是有用的。</li><li id="5450" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">特征标准化可以极大地提高模型性能，与参数调整相比，这是一项轻而易举的任务。</li><li id="8ef1" class="mb mc iq ky b kz ml lc mm lf mn lj mo ln mp lr mg mh mi mj bi translated">神经网络很棒，但不是万能的。如上所述，训练和调整神经网络模型的时间比非神经网络要多几千倍甚至几百万倍！神经网络最适合计算机视觉和自然语言处理等用例。</li></ul><p id="d2fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在 GitHub 上的<a class="ae kv" href="https://github.com/georgeliu1998/keras_model_tuning" rel="noopener ugc nofollow" target="_blank">我的项目报告</a>中找到完整的代码。一定要试一试，看看你能得到什么结果！</p><p id="0aaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢您的阅读！有什么我可以改进的吗？请在下面告诉我。我们都通过相互学习变得更好！</p></div></div>    
</body>
</html>