<html>
<head>
<title>Implement Grid World with Q-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Q-Learning 实现网格世界</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-grid-world-with-q-learning-51151747b455?source=collection_archive---------3-----------------------#2019-05-12">https://towardsdatascience.com/implement-grid-world-with-q-learning-51151747b455?source=collection_archive---------3-----------------------#2019-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cae3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">强化学习在网格游戏中的应用</h2></div><p id="e6da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae lb" href="https://medium.com/@zhangyue9306/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff" rel="noopener">之前的故事</a>中，我们谈到了如何使用值迭代实现一个确定性的网格世界游戏。这一次，让我们进入一种更普遍的强化学习形式——Q 学习。</p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lj"><img src="../Images/96e44da652658af1a6e3cc3c70a9d92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ENYgRWnBAr8yLfXo.jpeg"/></div></div></figure><h1 id="2062" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">提高一个档次</h1><p id="4adb" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">鉴于<em class="ms"> V(s) </em>是从状态到该状态的估计值的映射，Q 函数— <em class="ms"> Q(s，a) </em>只是与 V 函数不同的一个分量。当你处于特定的状态时，不要认为你得到了一个值，向前想一步，你处于一种状态，通过采取特定的行动，你得到了相应的值。本质上，这两种功能没有区别，只是通过将状态与行动绑定在一起方便了我们的生活。例如，回想一下使用值迭代的 grid world 的结果，我们得到了每个状态的估计值，但是为了拥有我们的策略<em class="ms"> π(s，a) </em>，这是从状态到动作的映射，我们需要更进一步，选择可以达到下一个状态的最大值的动作。然而，在 Q 函数中，状态和动作首先是成对的，这意味着当一个人拥有最佳 Q 函数时，他就拥有该状态的最佳动作。</p><p id="73cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了 Q-function，我们还将为我们的游戏增添更多乐趣:</p><ul class=""><li id="b224" class="mt mu iq kh b ki kj kl km ko mv ks mw kw mx la my mz na nb bi translated">代理操作是不确定的</li><li id="6c4b" class="mt mu iq kh b ki nc kl nd ko ne ks nf kw ng la my mz na nb bi translated">报酬随比率γ衰减</li></ul><p id="4a67" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">非确定性意味着代理将不能去它想要去的地方。当它采取一个动作时，它将有可能在不同的动作中崩溃。</p><p id="f174" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">衰变率γ在 0 和 1 之间。它表示代理人对未来奖励的关心程度，1 表示奖励永不衰减，代理人同样关心未来的所有奖励，0 表示代理人只关心当前状态的奖励。这个因素有助于调整代理人的长期愿景——想象一下像围棋这样的战略游戏，有时在当前状态下看似愚蠢的行动在长期利益和胜利方面是值得的。</p><p id="d7cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">就是这样！让我们着手实施。[ <a class="ae lb" href="https://github.com/MJeremy2017/RL/blob/master/GridWorld/gridWorld_Q.py" rel="noopener ugc nofollow" target="_blank">完整代码</a></p><h1 id="7396" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">电路板设置</h1><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="ab gu cl nh"><img src="../Images/b4835cbc77f3cae8414b692f55417ab6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*S3oszDubmqzfuu2vRnn3yw.png"/></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Board</figcaption></figure><p id="c725" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">董事会设置与之前讨论的基本相同，唯一的区别是代理采取行动。当它采取行动时，它将有 0.8 的概率进入期望的状态，并有相等的概率处于垂直状态。也就是说，如果代理人选择向上，那么它有 0.8 的概率向上，0.1 的概率向左和向右。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="0d37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在确定代理的下一个位置时，我们将采取返回<code class="fe no np nq nr b">chooseActionProb()</code>的操作，并利用我们已经定义的<code class="fe no np nq nr b">nxtPosition()</code>函数。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="cb54" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe no np nq nr b">nxtPosition()</code>函数接受一个动作，验证该动作的合法性并返回该动作的状态。</p><h1 id="3b19" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">代理人</h1><p id="4c48" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">让我们跳到主课程——如何通过迭代计算和更新 Q 值。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi ns"><img src="../Images/d63d3b23c912123c472a47bda9ef5ff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bmQXi79_h7oK-Lf_6QniMQ.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">Q-value update</figcaption></figure><p id="b8c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，在每一步，代理采取行动<code class="fe no np nq nr b">a</code>，收集相应的奖励<code class="fe no np nq nr b">r</code>，并从状态<code class="fe no np nq nr b">s</code>移动到<code class="fe no np nq nr b">s'</code>。所以每一步都要考虑一整对<code class="fe no np nq nr b">(s, a, s',r)</code>。</p><p id="ce5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其次，我们给出当前<em class="ms"> Q </em>值的估计，它等于当前奖励加上下一状态的最大<em class="ms"> Q </em>值乘以一个衰减率γ。值得注意的一点是，我们将所有中间奖励设置为 0，因此代理在结束状态之前无法收集任何非零奖励，无论是 1 还是-1。(这不是强制性的，你可以尝试其他奖励，看看代理如何行动)</p><p id="eb7e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们通过将α乘以一个<strong class="kh ir">时间差</strong>(新的估计值和当前值之间的差)来更新当前<em class="ms"> Q </em>值的估计值。</p><h2 id="f7dd" class="nt lw iq bd lx nu nv dn mb nw nx dp mf ko ny nz mh ks oa ob mj kw oc od ml oe bi translated">q 值初始化</h2><p id="c5d1" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">整个更新与值迭代非常相似，尽管<em class="ms"> Q </em> value 认为动作和状态是一对。当初始化<em class="ms"> Q </em>值时，我们需要将每个状态和每个动作设置为 0，并将它们存储在字典中作为<code class="fe no np nq nr b">Q_value[state][action]=0</code>。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="bf2b" class="nt lw iq bd lx nu nv dn mb nw nx dp mf ko ny nz mh ks oa ob mj kw oc od ml oe bi translated">行动</h2><p id="0603" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">就采取行动而言，仍将基于我们在<a class="ae lb" href="https://medium.com/@zhangyue9306/reinforcement-learning-implement-grid-world-from-scratch-c5963765ebff" rel="noopener">勘探&amp;开采</a>中讨论的勘探率。当代理利用状态时，它将根据当前估计的<em class="ms"> Q </em>值采取最大化<em class="ms"> Q </em>值的行动。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nm nn l"/></div></figure><h2 id="5fd5" class="nt lw iq bd lx nu nv dn mb nw nx dp mf ko ny nz mh ks oa ob mj kw oc od ml oe bi translated">更新 Q 值</h2><p id="f2bb" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">类似于数值迭代，<em class="ms"> Q </em>数值更新也是以相反的方式进行，每次更新将在游戏结束时进行。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="nm nn l"/></div></figure><p id="c11c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在游戏结束时，我们显式地将最后一个状态的所有行为设置为当前奖励，即 1 或-1，但这部分是可选的，它有助于更快地收敛。以下部分与值迭代相同，只是我们在这里加了一个<code class="fe no np nq nr b">decay_gamma</code>(注:<code class="fe no np nq nr b">self.decay_gamma * reward</code>应该是<code class="fe no np nq nr b">self.decay_gamma * reward + 0</code>作为我们设置为 0 的当前状态的奖励)。</p><h1 id="a2cd" class="lv lw iq bd lx ly lz ma mb mc md me mf jw mg jx mh jz mi ka mj kc mk kd ml mm bi translated">玩游戏</h1><p id="af11" class="pw-post-body-paragraph kf kg iq kh b ki mn jr kk kl mo ju kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">我们开球吧！玩了 50 轮之后，我们有了下面的状态-动作对的更新。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi of"><img src="../Images/7bb003b61d7f6309141374ba3ce48e9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ce_Wqb32URcswrSjpFlebg.png"/></div></div><figcaption class="ni nj gj gh gi nk nl bd b be z dk">result after 50 rounds</figcaption></figure><p id="fa0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们从<code class="fe no np nq nr b">(2, 0)</code>开始，最大行动应该是值为<code class="fe no np nq nr b">0.209</code>的<code class="fe no np nq nr b">up</code>，然后到达<code class="fe no np nq nr b">(1, 0)</code>，从那里最佳行动仍然是值为<code class="fe no np nq nr b">0.339</code>的<code class="fe no np nq nr b">up</code>，以此类推……最后我们得到我们的策略<code class="fe no np nq nr b">up -&gt; up -&gt; right -&gt; right -&gt; right</code>。我们可以看到，通过传播和更新，我们的代理足够聪明，可以在每个状态下产生最佳行动。Q-learning 的一个好处是，与基本值迭代相比，我们可以直接获得每个状态下的最佳行动。</p><p id="238a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请在这里查看完整代码<a class="ae lb" href="https://github.com/MJeremy2017/RL/blob/master/GridWorld/gridWorld_Q.py" rel="noopener ugc nofollow" target="_blank"/>，如果您发现任何警告，欢迎评论或投稿！</p></div></div>    
</body>
</html>