<html>
<head>
<title>Molecular Properties: A Journey through Multiple Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分子性质:多元线性回归之旅</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/molecular-properties-a-journey-through-multiple-linear-regression-1a1043c7de25?source=collection_archive---------28-----------------------#2019-12-18">https://towardsdatascience.com/molecular-properties-a-journey-through-multiple-linear-regression-1a1043c7de25?source=collection_archive---------28-----------------------#2019-12-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="68a6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">这是我对分子性质的<a class="ae ki" href="https://www.kaggle.com/c/champs-scalar-coupling" rel="noopener ugc nofollow" target="_blank"> Kaggle 竞赛的版本。目标是使用各种可用的特征来预测标量耦合常数。</a></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi kj"><img src="../Images/87c9491532025b43ce1389d90f95b817.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*q2zDtg9EHHL4fHUkrnc6fQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk"><a class="ae ki" href="https://www.kaggle.com/c/champs-scalar-coupling" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="33ae" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">自从完成我的数据科学训练营以来，我还没有做过包含我所学内容的个人项目。在看到 Kaggle 比赛和我对生物的热爱后，我决定尝试一下。不幸的是，当时我对自己在时限内完成比赛的能力没有信心，所以我只是下载并开始一点一点地工作，试图更好地理解这个过程。我在这里发表了多篇关于这些过程的文章。这些文章可以在下面找到:</p><blockquote class="lr"><p id="68bc" class="ls lt it bd lu lv lw lx ly lz ma lq dk translated"><a class="ae ki" href="https://medium.com/@imamun/steps-before-machine-learning-417a2812a1cc" rel="noopener">机器学习前的步骤</a></p><p id="3178" class="ls lt it bd lu lv lw lx ly lz ma lq dk translated"><a class="ae ki" href="https://medium.com/@imamun/lgbm-and-feature-extraction-ae87fe83ea77" rel="noopener"> LGBM 和特征提取</a></p><p id="fa1d" class="ls lt it bd lu lv lw lx ly lz ma lq dk translated"><a class="ae ki" href="https://medium.com/@imamun/linear-regression-of-selected-features-132cc6c4b600" rel="noopener">所选特征的线性回归</a></p><p id="0430" class="ls lt it bd lu lv lw lx ly lz ma lq dk translated"><a class="ae ki" rel="noopener" target="_blank" href="/picking-the-proper-mla-for-linear-regression-4221ea5a8f2e">为线性回归选择合适的 MLA</a></p></blockquote><p id="cefb" class="pw-post-body-paragraph kv kw it kx b ky mb ju la lb mc jx ld le md lg lh li me lk ll lm mf lo lp lq im bi translated">以上四篇文章概括了我解决这个问题的每一种不同的方法。在上一篇文章之后，我决定休息一会儿，然后回来从一个新的角度看这个项目。我还使用了其他一些不同的方法，但我将只关注<a class="ae ki" href="https://github.com/imamun93/Molecular_Properties/blob/master/KaggleSet.ipynb" rel="noopener ugc nofollow" target="_blank">最终笔记本</a>并在此介绍整个过程。为了理解这个过程，我建议你通读上面的文章。它们都是短文，通读它们不应该超过半小时。</p><p id="b6d3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在让我们开始加载我将使用的所有库:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="eb04" class="ml mm it mh b gy mn mo l mp mq"><strong class="mh iu">import</strong> <strong class="mh iu">pandas</strong> <strong class="mh iu">as</strong> <strong class="mh iu">pd</strong><br/><strong class="mh iu">import</strong> <strong class="mh iu">numpy</strong> <strong class="mh iu">as</strong> <strong class="mh iu">np</strong><br/><strong class="mh iu">import</strong> <strong class="mh iu">matplotlib.pyplot</strong> <strong class="mh iu">as</strong> <strong class="mh iu">plt</strong><br/>%matplotlib inline<br/><strong class="mh iu">import</strong> <strong class="mh iu">networkx</strong> <strong class="mh iu">as</strong> <strong class="mh iu">nx</strong><br/><strong class="mh iu">import</strong> <strong class="mh iu">seaborn</strong> <strong class="mh iu">as</strong> <strong class="mh iu">sns</strong><br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn</strong> <strong class="mh iu">import</strong> preprocessing, tree<br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn.ensemble</strong> <strong class="mh iu">import</strong> RandomForestRegressor<br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn.model_selection</strong> <strong class="mh iu">import</strong> cross_validate, cross_val_score, GridSearchCV<br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn.linear_model</strong> <strong class="mh iu">import</strong> LinearRegression, Ridge, Lasso<br/><strong class="mh iu">import</strong> <strong class="mh iu">lightgbm</strong><br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn.model_selection</strong> <strong class="mh iu">import</strong> KFold<br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn</strong> <strong class="mh iu">import</strong> linear_model<br/><strong class="mh iu">from</strong> <strong class="mh iu">sklearn.model_selection</strong> <strong class="mh iu">import</strong> train_test_split<br/><strong class="mh iu">from</strong> <strong class="mh iu">keras.models</strong> <strong class="mh iu">import</strong> Sequential<br/><strong class="mh iu">from</strong> <strong class="mh iu">keras.layers</strong> <strong class="mh iu">import</strong> Dense<br/><strong class="mh iu">from</strong> <strong class="mh iu">keras</strong> <strong class="mh iu">import</strong> optimizers<br/><strong class="mh iu">import</strong> <strong class="mh iu">keras</strong><br/><strong class="mh iu">import</strong> <strong class="mh iu">warnings</strong><br/>warnings.filterwarnings('ignore')</span></pre><p id="b80f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后让我们加载库，看看一些基本的统计数据:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="a012" class="ml mm it mh b gy mn mo l mp mq">df= pd.read_csv('molecule_complete.csv')<br/><br/>df.columns #the names of all the columns</span><span id="f521" class="ml mm it mh b gy mr mo l mp mq">Index(['molecule_name', 'atom_index_0', 'atom_index_1', 'type',<br/>       'scalar_coupling_constant', 'potential_energy', 'X', 'Y', 'Z',<br/>       'XX_atom1', 'YX_atom1', 'ZX_atom1', 'XY_atom1', 'YY_atom1', 'ZY_atom1',<br/>       'XZ_atom1', 'YZ_atom1', 'ZZ_atom1', 'XX', 'YX', 'ZX', 'XY', 'YY', 'ZY',<br/>       'XZ', 'YZ', 'ZZ', 'mulliken_charge_atom1', 'mulliken_charge',<br/>       'type_scc', 'fc', 'sd', 'pso', 'dso', 'atom_atom1_structure',<br/>       'x_atom1_structure', 'y_atom1_structure', 'z_atom1_structure', 'atom',<br/>       'x', 'y', 'z'],<br/>      dtype='object')</span><span id="f2e9" class="ml mm it mh b gy mr mo l mp mq">df.select_dtypes(include=[object]) #this shows the categorical variables</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/93fadefaa2cc41a9cb160e645b58355b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpC0GpN4ycmWSupAt9504g.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The categorical variables among the features</figcaption></figure><p id="418a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在来看一些图片。这要感谢 Kaggle 的竞争对手安德鲁:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="2c40" class="ml mm it mh b gy mn mo l mp mq">fig, ax = plt.subplots(figsize = (20, 12)) <strong class="mh iu">for</strong> i, t <strong class="mh iu">in</strong> enumerate(df[‘type’].unique()): df_type = df.loc[df[‘type’] == t] G = nx.from_pandas_edgelist(df_type, ‘atom_index_0’, ‘atom_index_1’, [‘scalar_coupling_constant’]) plt.subplot(2, 4, i + 1); nx.draw(G, with_labels=<strong class="mh iu">True</strong>); plt.title(f’Graph for type <strong class="mh iu">{t}</strong>’)</span></pre><p id="c0f3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这有助于创建与标量耦合常数相关的所有分类变量的网络图。我还为<em class="mx"> fc、muliken_charge、pso、sd </em>和<em class="mx"> dso </em>创建了更多的关系图，因为它们也被确定为我之前工作中的重要特性。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi my"><img src="../Images/85bdb628259cf75aeb5a9a62d0a6885f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMxZZgoF1-WwdxSgFOjDdw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This is just for the Scalar Constant. The rest of the graph can be found in the final notebook linked above</figcaption></figure><p id="77ac" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这可能有点难以解释。然而，我们看到一些类型更多地聚集在一起，而其他类型如 2JHH 与标量形成了相当独特的关系。使用 Tableau Visual 有一个稍微好一点的方法来可视化每种类型对目标原子的影响:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mz"><img src="../Images/e9e5cae437b786231d3947d1e19ea7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bFxhsob6Pr7LyPd8LVkahw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This shows how much weight each atom type has on different atoms.</figcaption></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi na"><img src="../Images/c6121c9678f8c0e1096eebcea150a3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FE4F9REoUF4Bxb-xaHu1mQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">While this shows how many times each type shows up altogether.</figcaption></figure><p id="eaca" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">结合上面两张图片，我们可以理解为什么 2JHH 具有如此独特的图式，而像 1JHC 和 3JHC 这样的原子是相似的，并且更加聚集在一起。现在让我们使用类型和标量常数创建一个标量常数库和一个 violin 图:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="001c" class="ml mm it mh b gy mn mo l mp mq">#this was done for all important features from the LGBM feature extraction model, but I will only show scalar here. <br/>fig, ax = plt.subplots(figsize = (18, 6))<br/>plt.subplot(1, 2, 1);<br/>plt.hist(df['scalar_coupling_constant'], bins=20);<br/>plt.title('Basic scalar_coupling_constant histogram');<br/>plt.subplot(1, 2, 2);<br/>sns.violinplot(x='type', y='scalar_coupling_constant', data=df);<br/>plt.title('Violinplot of scalar_coupling_constant by type');</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nb"><img src="../Images/81b6053e5b903653ef2d2b83a62c303f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P5pnz_Fpy65CnSSYwZ7EBg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The bin shows most scalar numbers are rather small. The violin plot however shows another great visual on how each type plays a critical role on our target variables</figcaption></figure><p id="c0dc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在此之后，我使用 LabelEncoder 将所有分类变量转换为虚拟变量，创建一个训练和测试集，最后运行 sci-kit learn 的 train-test-split。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="533b" class="ml mm it mh b gy mn mo l mp mq"><strong class="mh iu">for</strong> f <strong class="mh iu">in</strong> ['type', 'type_scc', 'atom_atom1_structure', 'atom']:<br/>    lbl = preprocessing.LabelEncoder()<br/>    lbl.fit(list(df[f].values))<br/>    df[f] = lbl.transform(list(df[f].values))</span><span id="7c15" class="ml mm it mh b gy mr mo l mp mq">train= df.drop(['molecule_name', 'scalar_coupling_constant'], axis=1)<br/>test= df['scalar_coupling_constant']</span><span id="d62d" class="ml mm it mh b gy mr mo l mp mq">feature_train, feature_test, target_train, target_test= train_test_split(train, test, test_size=0.12)</span></pre><p id="510d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这给了我以下信息:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="8b53" class="ml mm it mh b gy mn mo l mp mq">total feature training features:  4099169<br/>total feature testing features:  558978<br/>total target training features:  4099169<br/>total target testing features:  558978</span></pre><p id="4502" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在我以前的模型中，我没有使用任何分类变量。因此，我决定再次运行 LGBM 模型，但包括虚拟变量:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="ad3b" class="ml mm it mh b gy mn mo l mp mq">train_data = lightgbm.Dataset(feature_train, label=target_train)<br/>test_data = lightgbm.Dataset(feature_test, label=target_test)</span><span id="1f9b" class="ml mm it mh b gy mr mo l mp mq">#Create some parameters:<br/><br/>n_fold = 7 folds = KFold(n_splits=n_fold, shuffle=<strong class="mh iu">True</strong>, random_state=4) </span><span id="b1aa" class="ml mm it mh b gy mr mo l mp mq">#The formatting below is a bit messy but please be aware of proper indentation. Copy paste into medium is messing up proper indentation</span><span id="cbe4" class="ml mm it mh b gy mr mo l mp mq">parameters = {'num_leaves': 250,<br/>'min_child_samples': 75,<br/>'objective': 'regression',<br/>'max_depth': 24,<br/>'learning_rate': 0.001,<br/>"boosting_type": "gbdt",<br/>"subsample_freq": 3,<br/>"subsample": 0.9,<br/>"bagging_seed": 15,<br/>"metric": 'mae',<br/>"verbosity": -1,<br/>'reg_alpha': 0.01,<br/>'reg_lambda': 0.3,<br/>'colsample_bytree': 1.0}</span><span id="9709" class="ml mm it mh b gy mr mo l mp mq">#Now to run the model:<br/><br/>model = lightgbm.train(parameters,                        train_data, valid_sets=test_data,                        num_boost_round=5000,                        early_stopping_rounds=100)  #I should have used 10,000 but 5000 was still great</span><span id="edf7" class="ml mm it mh b gy mr mo l mp mq">#Time to visualize the new feature importance change:<br/>ax = lightgbm.plot_importance(model, max_num_features=40, figsize=(15,15))<br/>plt.show()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nc"><img src="../Images/d1b9cec9ea74da522edde97e38401b4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cc57BFzimGGOuZMUrVNWKw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">We see virtually no change compare to the previous LGBM model I made</figcaption></figure><p id="8d4c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我又做了一个视觉效果，它关注的是准确性而不是平均绝对误差，它给出了一个稍微不同的特征重要性，但它只影响了前 5 个特征的顺序。然后我意识到，由于目标变量包含的数字大多很小，但并不是所有的特性都是如此，这就对数字较大的特性产生了某种形式的偏差。然后我决定将数据标准化:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="5f3d" class="ml mm it mh b gy mn mo l mp mq">normal_feature=df.drop(['molecule_name', 'scalar_coupling_constant'], axis=1) </span><span id="2a52" class="ml mm it mh b gy mr mo l mp mq">target= df['scalar_coupling_constant']</span><span id="db0d" class="ml mm it mh b gy mr mo l mp mq">x= normal_feature.values<br/>min_max_scaler = preprocessing.MinMaxScaler()</span><span id="ffd0" class="ml mm it mh b gy mr mo l mp mq">x_scaled = min_max_scaler.fit_transform(x) #this gave an array instead of a dataframe. Now to convert this array into a dataframe and then properly change all column names back to the original</span><span id="9556" class="ml mm it mh b gy mr mo l mp mq">nf = pd.DataFrame(x_scaled)</span><span id="f75a" class="ml mm it mh b gy mr mo l mp mq">#This will reassign the names. It would have been easier to create a loop function but since I did it once already, I just copy pasted and added the additional dummy variables in</span><span id="2023" class="ml mm it mh b gy mr mo l mp mq">normfeat= nf.rename(columns={0: 'atom_index_0', 1:'atom_index_1', 2: 'type', 3:'potential_energy', 4: 'X', 5: 'Y', 6: 'Z',                        7: 'XX_atom1', 8: 'YX_atom1', 9: 'ZX_atom1', 10: 'XY_atom1', 11: 'YY_atom1',                         12: 'ZY_atom1',13: 'XZ_atom1', 14: 'YZ_atom1', 15: 'ZZ_atom1',        16: 'XX', 17: 'YX', 18: 'ZX', 19: 'XY', 20: 'YY', 21:'ZY', 22:'XZ', 23:'YZ', 24:'ZZ',        25: 'mulliken_charge_atom1', 26: 'mulliken_charge', 27: 'type_scc',                               28: 'fc', 29: 'sd', 30:'pso',31: 'dso', 32: 'atom_atom1_structure',        33:'x_atom1_structure', 34:'y_atom1_structure', 35:'z_atom1_structure', 36: 'atom', 37:'x', 38:'y', 39:'z'                       })</span></pre><p id="4749" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在创建一个新的训练-测试-分割，因为这些本质上是新的变量。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="311b" class="ml mm it mh b gy mn mo l mp mq">features= normfeat <br/>target= df[[‘scalar_coupling_constant’]] </span><span id="95eb" class="ml mm it mh b gy mr mo l mp mq">feature_train, feature_test, target_train, target_test= train_test_split(features, target, test_size=0.1)</span><span id="1fd6" class="ml mm it mh b gy mr mo l mp mq">#Then to convert it all to numpy array for easier calculations:<br/><br/>feature_array= feature_train.to_numpy() <br/>target_array= target_train.to_numpy()</span></pre><p id="6ad0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">做完这些，我决定进入真正的机器建模。我没有做另一个 LGBM 模型，因为我不认为它会改变更多的功能，即使在正常化后，但你可以尝试一下，让我知道。我从基本的线性回归模型开始，使用负均方误差进行测量。这意味着数字越小，结果越好:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="6b15" class="ml mm it mh b gy mn mo l mp mq">linear= LinearRegression()<br/>mse= cross_val_score(linear, feature_array, target_array, scoring='neg_mean_squared_error', cv= 20)<br/>mean_mse= np.mean(mse)</span><span id="a1cb" class="ml mm it mh b gy mr mo l mp mq">mse= -6.683008610663098e-09</span></pre><p id="996c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我专门为目标测试变量创建了一个新的数据框架，用于将来比较 ridge 和 lasso 模型:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="3d17" class="ml mm it mh b gy mn mo l mp mq">target_test= target_test.reset_index()</span><span id="ec46" class="ml mm it mh b gy mr mo l mp mq">target_test=target_test.drop(['index'], axis=1)<br/>#Only run the above codes once or it will give you error</span><span id="d076" class="ml mm it mh b gy mr mo l mp mq">target_test=target_test.rename(columns={'scalar_coupling_constant': 'Actual Test Scalar'})</span></pre><p id="d044" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在创建一个新的山脊模型，并根据标准化的特征阵列对其进行测试。我选择 GridSearchCV 用于山脊和套索，因为根据我的研究，它是最佳的。它使用交叉验证，而不是创建一个新的训练测试拆分案例。这最小化了数据泄漏的可能性，因为该算法不会看到实际的测试集。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="c960" class="ml mm it mh b gy mn mo l mp mq">ridge= Ridge()<br/>ridgereg= GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv= 15)<br/>ridgereg.fit(feature_array, target_array)<br/>print('ridge param: ', ridgereg.best_params_)<br/>print('ridge score: ', ridgereg.best_score_)<br/>ridgereg</span><span id="dcf2" class="ml mm it mh b gy mr mo l mp mq">#The answers:<br/>ridge param:  {'alpha': 1e-25}<br/>ridge score:  -6.683015092064162e-09</span><span id="8f30" class="ml mm it mh b gy mr mo l mp mq">#Again, sorry about the formatting.</span><span id="f36e" class="ml mm it mh b gy mr mo l mp mq">GridSearchCV(cv=15, error_score='raise-deprecating',<br/>             estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None, normalize=False, random_state=None, solver='auto', tol=0.001),<br/>             iid='warn', n_jobs=None,<br/>             param_grid={'alpha': [1e-25, 1e-20, 1e-15, 1e-10, 1e-05, 0.01, 1,<br/>                                   10]},<br/>             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,<br/>             scoring='neg_mean_squared_error', verbose=0)</span><span id="b876" class="ml mm it mh b gy mr mo l mp mq"><br/># Finally to fit the best ridge parameters to our training set:</span><span id="4167" class="ml mm it mh b gy mr mo l mp mq">ridgereg= GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv= 20) ridgereg.fit(feature_array, target_array)</span><span id="4004" class="ml mm it mh b gy mr mo l mp mq">#To check the actual score:<br/>ridgereg.score(feature_test, target_test)</span><span id="9620" class="ml mm it mh b gy mr mo l mp mq">-6.836977895342504e-09 #this is a very small mean squared error.</span></pre><p id="f3da" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在对未知的测试用例进行一些预测:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="d656" class="ml mm it mh b gy mn mo l mp mq">ridgepredict=ridgereg.predict(feature_test)<br/>actualtest=np.array(target_test)<br/><br/>plt.rcParams["figure.figsize"] = (8, 8) <br/>fig, ax = plt.subplots() <br/>ax.scatter(actualtest, ridgepredict) <br/>ax.set(title="Ridge Actual vs Predict") <br/>ax.set(xlabel="Actual", ylabel="Predict");</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nd"><img src="../Images/aa6595ae8b2476f5741dffc32af6f319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E-QuhOSyfodw6pBCHmof2Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">This shows how well my model is predicting against the actual test.</figcaption></figure><p id="fb90" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这让我担心过度拟合，我不明白为什么数据之间有差距。所以我把山脊线的分数保存到一个数据框中，然后转移到 lasso 上。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="18be" class="ml mm it mh b gy mn mo l mp mq">#To save the ridge data into a dataframe<br/>ridgedata= pd.concat([target_test, pd.DataFrame(ridgepredict)], axis=1)<br/>ridgecomparison= ridgedata.rename(columns={0:'Predicted Ridge Scalar'})</span></pre><p id="d287" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">套索的计算方法与山脊相同:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="5b8f" class="ml mm it mh b gy mn mo l mp mq">lasso= Lasso()<br/>lassoreg= GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv= 20)<br/>lassoreg.fit(feature_array, target_array)</span><span id="7aa3" class="ml mm it mh b gy mr mo l mp mq">lassoreg.score(feature_test, target_test)</span><span id="f1e3" class="ml mm it mh b gy mr mo l mp mq">-1.3148967532843286e-05 #the score is worse than Ridge. This showed that Ridge is better for this problem than Lasso</span><span id="7d2e" class="ml mm it mh b gy mr mo l mp mq">#To create the Lasso graph:<br/>lassopredict=lassoreg.predict(feature_test)<br/>plt.rcParams["figure.figsize"] = (8, 8)<br/>fig, ax = plt.subplots()<br/>ax.scatter(actualtest, lassopredict)<br/>ax.set(title="Lasso Actual vs Predict")<br/>ax.set(xlabel="Actual", ylabel="Predict");</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ne"><img src="../Images/ef8fe2ba73df1d2f65705350aac3f6c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UtXSB2mpBa3NwJaZcr5CxA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The same issue persist.</figcaption></figure><p id="b87b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">此时，我将 lasso 分数保存到一个数据帧中，并决定使用 Tableau 作为视觉媒介，直接将结果与实际测试分数进行比较:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="e20b" class="ml mm it mh b gy mn mo l mp mq">lassodata= pd.concat([target_test, pd.DataFrame(lassopredict)], axis=1) <br/>lassocomparison= lassodata.rename(columns={0:'Predicted Lasso Scalar'})</span><span id="953e" class="ml mm it mh b gy mr mo l mp mq">#Combining the test dataframe with the Ridge and Lasso and then saving it as a CSV to check with Tableau</span><span id="7037" class="ml mm it mh b gy mr mo l mp mq">accuracy_check= pd.concat([ridgecomparison, lassocomparison], axis=1)<br/>accuracy_check= accuracy_check.loc[:,~accuracy_check.columns.duplicated()] #this drops any duplicated columns</span><span id="9803" class="ml mm it mh b gy mr mo l mp mq">accuracy_check.to_csv('accuracy_check.csv', index=<strong class="mh iu">False</strong>)</span><span id="f5bf" class="ml mm it mh b gy mr mo l mp mq"># You can run the following to see if it saved properly:<br/>acc_chk= pd.read_csv('accuracy_check.csv')<br/>acc_chk.head()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nf"><img src="../Images/fad7b2e5b59d870a0101c0485ff3084c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PFne30gQLQvdP6Axm6okkQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The Tableau visual (sorry, there is no codes for this)</figcaption></figure><p id="25bf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这表明了我的山脊和套索模型的准确性。由于使用 tableau 检查三个连续列的准确性有点棘手，所以我对所有列使用 COUNT 函数作为一种过滤形式，然后创建一个 SUM 线性图，其中实际的测试标量是主要部分。大小显示了大多数数字的集中，它在更大的项目计划中并没有真正发挥任何作用。然而，很容易画出一条清晰的最佳拟合线，表明我使用的算法可以用来预测分子标量常数电荷。这也有助于解释一些差距。大多数标量一开始都是很小的数字，边界之外的几千个可能造成了线性图形中的缺口。然而，我们可以清楚地看到一条最佳拟合线。</p><p id="c02b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我还想尝试一种算法。一个神经网络。在不同的笔记本上尝试了几个不同的版本后，我决定用 Relu 和 adadelta 作为优化器。我选择 adadelta 是因为来自<a class="ae ki" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank"> Keras 文档</a>的解释:</p><blockquote class="ng nh ni"><p id="58f8" class="kv kw mx kx b ky kz ju la lb lc jx ld nj lf lg lh nk lj lk ll nl ln lo lp lq im bi translated">学习率基于梯度更新的移动窗口，而不是累积所有过去的梯度。这样，即使已经进行了多次更新，Adadelta 也能继续学习。</p></blockquote><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="0889" class="ml mm it mh b gy mn mo l mp mq">earlystop=keras.callbacks.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=<strong class="mh iu">None</strong>, restore_best_weights=<strong class="mh iu">False</strong>)</span><span id="75a6" class="ml mm it mh b gy mr mo l mp mq">terminate= keras.callbacks.callbacks.TerminateOnNaN()</span><span id="81fc" class="ml mm it mh b gy mr mo l mp mq">adadelta= optimizers.Adadelta(learning_rate=1.0, rho=0.95)</span></pre><p id="34fd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在设置了基本的边界之后，我创建了一个带有早停和早停的模型。它们的描述和用法可以在 keras 文档中找到。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="55d6" class="ml mm it mh b gy mn mo l mp mq">m4 = Sequential() m4.add(Dense(64, input_dim=40, activation='relu')) <br/>m4.add(Dense(32, activation='relu')) <br/>m4.add(Dense(16, activation='relu')) <br/>m4.add(Dense(8, activation='relu')) <br/>m4.add(Dense(4, activation='relu')) <br/>m4.add(Dense(1, activation='linear')) <br/>m4.summary()</span><span id="d750" class="ml mm it mh b gy mr mo l mp mq">Model: "sequential_4" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= dense_19 (Dense)             (None, 64)                2624       _________________________________________________________________ dense_20 (Dense)             (None, 32)                2080       _________________________________________________________________ dense_21 (Dense)             (None, 16)                528        _________________________________________________________________ dense_22 (Dense)             (None, 8)                 136        _________________________________________________________________ dense_23 (Dense)             (None, 4)                 36         _________________________________________________________________ dense_24 (Dense)             (None, 1)                 5          ================================================================= Total params: 5,409 <br/>Trainable params: 5,409 <br/>Non-trainable params: 0</span><span id="47e0" class="ml mm it mh b gy mr mo l mp mq"><em class="mx">#early callback</em><br/>m4.compile(optimizer=adadelta, loss='mean_squared_error', metrics=['mae', 'acc'])<br/>his4=m4.fit(feature_array, target_array, validation_split=0.3, verbose=1, callbacks=[earlystop, terminate], <br/>            epochs=300, batch_size=5000)</span></pre><p id="2b25" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">由于提前回调和终止，模型结束得相当快。它显示出高 mae 和低精度，因为收敛发生得相对较早。但我知道这是一个错误，因为下图。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="5922" class="ml mm it mh b gy mn mo l mp mq"><em class="mx">#with early and termination callback established</em> <br/>acc = his4.history['acc'] <br/>val_acc = his4.history['val_acc'] <br/>loss = his4.history['loss'] <br/>val_loss = his4.history['val_loss'] <br/>epochs = range(len(acc)) <br/>plt.plot(epochs, acc, 'r', label='Training acc') <br/>plt.plot(epochs, val_acc, 'b', label='Validation acc') plt.title('Training and validation accuracy') <br/>plt.ylabel('accuracy')   <br/>plt.xlabel('epoch') <br/>plt.legend() plt.figure() <br/>plt.plot(epochs, loss, 'r', label='Training loss') <br/>plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') <br/>plt.ylabel('loss')   <br/>plt.xlabel('epoch') <br/>plt.legend() <br/>plt.show()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nm"><img src="../Images/476769c1457f7391e05962d1c67ce2d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gCWmWi7v16XSn1S8Es_etw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The convergence occurs due to the Loss but not accuracy. The accuracy is far too low to be valid.</figcaption></figure><p id="2ecb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然后我运行了一个新的模型，但是这次我没有使用提前回调或者终止。我想穿越所有的时代:</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="37ec" class="ml mm it mh b gy mn mo l mp mq">m4.compile(optimizer=adadelta, loss='mean_squared_error', metrics=['mae', 'acc'])</span><span id="8438" class="ml mm it mh b gy mr mo l mp mq">his4=m4.fit(feature_array, target_array, validation_split=0.3, verbose=1,<br/>            epochs=300, batch_size=5000)</span><span id="fc15" class="ml mm it mh b gy mr mo l mp mq">#The graph</span><span id="9f30" class="ml mm it mh b gy mr mo l mp mq">acc = his4.history['acc'] <br/>val_acc = his4.history['val_acc'] <br/>loss = his4.history['loss'] <br/>val_loss = his4.history['val_loss'] <br/>epochs = range(len(acc)) <br/>plt.plot(epochs, acc, 'r', label='Training acc') <br/>plt.plot(epochs, val_acc, 'b', label='Validation acc') plt.title('Training and validation accuracy') <br/>plt.ylabel('accuracy')   <br/>plt.xlabel('epoch') <br/>plt.legend() plt.figure() <br/>plt.plot(epochs, loss, 'r', label='Training loss') <br/>plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') <br/>plt.ylabel('loss')   <br/>plt.xlabel('epoch') <br/>plt.legend() <br/>plt.show()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nn"><img src="../Images/892c6fde756bcd19695d4e304fddec65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GtTY-6eLs1v3k2LygpJ7mA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">With no early callback</figcaption></figure><p id="0bd1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在我看到精度和损耗都收敛了。然而，准确性似乎收敛在一个非常低的点，并不是这个模型的一个合适的度量。由于 adadelta 使用衰减作为均值点，我也决定研究平均绝对误差而不是精度。由于精度如此之小，并且没有适当的梯度，因此无法使用。</p><pre class="kk kl km kn gt mg mh mi mj aw mk bi"><span id="c43b" class="ml mm it mh b gy mn mo l mp mq">acc = his4.history['mae'] <br/>val_acc = his4.history['val_mae'] <br/>loss = his4.history['loss'] <br/>val_loss = his4.history['val_mae'] <br/>epochs = range(len(mae)) <br/>plt.plot(epochs, mae, 'r', label='Training mae') <br/>plt.plot(epochs, val_mae, 'b', label='Validation mae') plt.title('Training and validation mae') <br/>plt.ylabel('mae')   <br/>plt.xlabel('epoch') <br/>plt.legend() plt.figure() <br/>plt.plot(epochs, loss, 'r', label='Training loss') <br/>plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') <br/>plt.ylabel('loss')   <br/>plt.xlabel('epoch') <br/>plt.legend() <br/>plt.show()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi no"><img src="../Images/6627bc956eebebc15988304c5ea0810e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sRSUZMNHHvbjkvJRrNFPVg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Mean Absolute Error</figcaption></figure><p id="0dcf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这显示了更好的收敛和梯度下降。如果我运行一个更高的纪元计数，mae 很可能会下降更多。但此时，mae 位于 0.6403，亏损位于 0.6871。虽然它没有山脊模型 mae 小，但它仍然是一个下降模型。但是，就定量分析而言，岭模型仍然是用于线性回归问题的最佳模型，该问题包含多个要素，每个要素都有数百万个数据集。此外，每个特性的值都很稀疏，有些特性甚至可能不起任何作用。也有可能一些特征对目标变量产生相同的影响，导致多重共线性和权重重要性的差异，从而导致模型中的偏差和过度拟合。考虑到所有这些因素，我认为使用岭模型来计算平均误差而不是精确度对于这个挑战来说是完美的。我这个项目的 github url 将在下面链接，所以你可以看看，也许可以改进我的模型。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><div class="kk kl km kn gt nw"><a href="https://github.com/imamun93/Molecular_Properties" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">imamun 93/分子特性</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">来自 Kaggle 竞争预测分子性质的数据。所有的数据都可以在那里找到。由于 git 大小的限制…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">github.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok kp nw"/></div></div></a></div><div class="ol om gp gr on nw"><a href="https://www.kaggle.com/c/champs-scalar-coupling" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">预测分子性质</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">你能测量一对原子之间的磁相互作用吗？</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">www.kaggle.com</p></div></div><div class="of l"><div class="oo l oh oi oj of ok kp nw"/></div></div></a></div><div class="ol om gp gr on nw"><a href="https://keras.io/optimizers/" rel="noopener  ugc nofollow" target="_blank"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">优化器- Keras 文档</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">优化器是编译 Keras 模型所需的两个参数之一:from Keras import optimizer model =…</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">keras.io</p></div></div></div></a></div></div></div>    
</body>
</html>