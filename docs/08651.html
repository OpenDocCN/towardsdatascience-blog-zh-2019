<html>
<head>
<title>Finding Shortest Path using Q-Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Q-学习算法寻找最短路径</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/finding-shortest-path-using-q-learning-algorithm-1c1f39e89505?source=collection_archive---------7-----------------------#2019-11-21">https://towardsdatascience.com/finding-shortest-path-using-q-learning-algorithm-1c1f39e89505?source=collection_archive---------7-----------------------#2019-11-21</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><figure class="gm go js jt ju jv gi gj paragraph-image"><div class="gi gj jr"><img src="../Images/6dae7ba8e46a86ea654e7986d917c703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*j8vHYP3fzEuwpre88xRtAg.png"/></div></figure><p id="f003" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">无向图中的最短路径。你也可以在这里阅读<a class="ae kx" href="https://medium.com/p/finding-shortest-path-using-q-learning-algorithm-1c1f39e89505?source=email-8ba179e84084--writer.postDistributed&amp;sk=bae9863c16d3c4ff585abe35cd173cc6" rel="noopener"><em class="kw"/></a><em class="kw">。</em></p><p id="3db3" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated"><strong class="ka iv">图</strong>是用于模拟对象之间成对关系的数学结构。图是由边连接的顶点组成的。在无向图中，我会找到两个顶点之间的最短路径。</p><p id="2afa" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated"><strong class="ka iv"> Q-learning </strong>是一种无模型<em class="kw">强化学习</em>算法。Q-learning 的目标是学习一个策略，它告诉代理在什么情况下采取什么行动。它不需要一个环境模型，它可以处理随机转移和奖励的问题，而不需要适应。</p></div><div class="ab cl ky kz hy la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="in io ip iq ir"><p id="643a" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">首先，我导入必要的模块。我将使用<em class="kw"> networkx </em>库来定义一个图形</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj lf"><img src="../Images/c5fb1c1cdd598af1e49ae4608858ad84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j_x3rVdPMxCW18Do9rUSVA.jpeg"/></div></div></figure><p id="5af8" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">让我们定义并形象化这个图表</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj lo"><img src="../Images/0ca625b2dd962871606b312edae64c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqYcT5ryCkrsYlER7N2NeA.jpeg"/></div></div></figure><p id="4d2f" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">Q-learning 算法包括一个代理、一组状态和每个状态的一组动作。它在某种程度上使用 Q 值和随机性来决定采取何种行动。q 值根据所采取行动的回报和可能的未来回报进行初始化和更新。</p><p id="b63b" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">数学上，</p><p id="982c" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">q 值可通过下式计算:</p><p id="686b" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">我想找到从 0 到 10 的最短路径。我需要吸引走到涉及 10 的边，因此我给这些行动很高的奖励。在 networkx 库中，G[node]给出了与该节点形成一条边的所有节点。</p><ul class=""><li id="d806" class="lp lq iu ka b kb kc kf kg kj lr kn ls kr lt kv lu lv lw lx bi translated">这里我初始化奖励和 Q 矩阵:</li><li id="b999" class="lp lq iu ka b kb ly kf lz kj ma kn mb kr mc kv lu lv lw lx bi translated">除了到达节点 10 的动作，我把所有的奖励都设为 0。这些动作是从 8 到 10 或者从 9 到 10。像奖励一样，Q 值在矩阵中初始化。为了消除不可能的动作，他们的 Q 值被设置为-100。例如，在图中，不可能直接从 2 到 10，因此其 Q 值设置为-100。可能的操作被初始化为 0。让我们来看看 R 和 Q 矩阵:</li><li id="1f54" class="lp lq iu ka b kb ly kf lz kj ma kn mb kr mc kv lu lv lw lx bi translated">现在我将定义一个函数，它接受一个起始节点并返回下一个节点。它也接受随机探索的探索率。否则，它根据可能动作的最高 Q 值来选择动作。</li><li id="a95f" class="lp lq iu ka b kb ly kf lz kj ma kn mb kr mc kv lu lv lw lx bi translated">之后，我们需要一个函数来更新所采取行动的 Q 值</li><li id="1598" class="lp lq iu ka b kb ly kf lz kj ma kn mb kr mc kv lu lv lw lx bi translated">现在是时候提高 Q 值了，从随机节点开始，走 50000 步</li></ul><p id="dae6" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">让我们检查一下最终的 Q 值</p><p id="8c0a" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">现在，我们可以通过在决定行动时从 Q 矩阵中选择最高 Q 值，找到 0 到 10 之间的最短路径:</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj md"><img src="../Images/d6f04ab5a6b03c80d41fb4d1f1e2318d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RD0iLJWG7dwUKr5c-nY-uQ.jpeg"/></div></div></figure><p id="7777" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated">参考</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj me"><img src="../Images/ef1adfefa3aac2fd5c15d419be2a95be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*polx1yv9JHIFdO-QXqEO7Q.jpeg"/></div></div></figure><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj mf"><img src="../Images/a9fa55337c88c419c4bed8ca26c53c4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwfUe_aXpji8-FYI3Z3-UQ.jpeg"/></div></div></figure><p id="822b" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi translated"><a class="ae kx" href="https://en.wikipedia.org/wiki/Graph_theory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Graph_theory</a><br/><a class="ae kx" href="https://everything.explained.today/Q-learning/" rel="noopener ugc nofollow" target="_blank">https://everything.explained.today/Q-learning/</a><br/><a class="ae kx" href="http://firsttimeprogrammer.blogspot.com/2016/09/getting-ai-smarter-with-q-learning.html" rel="noopener ugc nofollow" target="_blank">http://first time programmer . blogspot . com/2016/09/getting-ai-smarter-with-q-learning . htm</a><a class="ae kx" href="https://www.poilabs.com/" rel="noopener ugc nofollow" target="_blank">l</a></p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj mg"><img src="../Images/b781f8737e8918ef1b57567bb67124a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmdpTygXzj-M8vFb97KA2A.jpeg"/></div></div></figure><p id="471c" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi">After, we need a function for updating Q-value of the action taken</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj mh"><img src="../Images/0053288b6ac90769b81d8db74e294e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EGPwlfbfi5jcfYvYnS29Wg.jpeg"/></div></div></figure><p id="7329" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi">Now it is time to improve Q-values by starting at random nodes and making 50000 walk</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj mi"><img src="../Images/12f30fe6159365326e64b7ec9526e25a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B4DoqdlneOW6T6bLgsp6cg.jpeg"/></div></div></figure><p id="2815" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi">Let’s check the final Q-values</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj mj"><img src="../Images/ec8a2717a58e850f1b6ad7196da49115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eINCF8-2Cg8QsNuAT0zPyw.jpeg"/></div></div></figure><p id="40c7" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi">Now we can find shortest path between 0 and 10, by choosing highest Q-value from Q matrix when deciding an action:</p><figure class="lg lh li lj gu jv gi gj paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gi gj mk"><img src="../Images/d9f6d2365c919557c5495bebd6cf7d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7W4TFp1cHl0QXlZ3lFPN7A.jpeg"/></div></div></figure><p id="1c95" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi">References</p><p id="d23f" class="pw-post-body-paragraph jy jz iu ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv in bi"><a class="ae kx" href="https://en.wikipedia.org/wiki/Graph_theory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Graph_theory</a> <br/><a class="ae kx" href="https://everything.explained.today/Q-learning/" rel="noopener ugc nofollow" target="_blank">https://everything.explained.today/Q-learning/</a> <br/><a class="ae kx" href="http://firsttimeprogrammer.blogspot.com/2016/09/getting-ai-smarter-with-q-learning.html" rel="noopener ugc nofollow" target="_blank">http://firsttimeprogrammer.blogspot.com/2016/09/getting-ai-smarter-with-q-learning.htm</a><a class="ae kx" href="https://www.poilabs.com/" rel="noopener ugc nofollow" target="_blank">l</a></p></div></div>    
</body>
</html>