<html>
<head>
<title>LOGISTIC REGRESSION CLASSIFIER</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-classifier-2d4845916b35?source=collection_archive---------24-----------------------#2019-03-04">https://towardsdatascience.com/logistic-regression-classifier-2d4845916b35?source=collection_archive---------24-----------------------#2019-03-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d14f1771c24098bed90309a1cf7101ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YfinX3cWjhKmXW-gbUyu9Q.jpeg"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Longleat Hedge Maze</figcaption></figure><div class=""/><p id="73c9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">(下篇)</em> </strong></p><p id="2589" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">一步一步完成指南(概念性)</em> </strong></p><p id="67b6" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在“逻辑回归”系列文章的第一篇文章<a class="ae lb" href="https://medium.com/@caglarsubas/logistic-regression-classifier-8583e0c3cf9" rel="noopener">中，我们回答了以下问题:</a></p><ul class=""><li id="8c9c" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated">它使用和产生什么类型的数据(输入和输出)？</li><li id="25bb" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">有哪些实验可以拿它在手？</li><li id="fc41" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">它使用哪种决策/激活功能？</li><li id="4c78" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">学习的目标如何写成一个等式？</li></ul><p id="2ab8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是系列文章的第二篇，我们将继续“优化目标”主题，并尝试完成下表中列出的所有剩余主题。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi lq"><img src="../Images/5f5274b9f932cf10578613e0290587c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*yOhIZHJ4wQhQYKA_JPw4WQ.png"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Index of the Article Series</figcaption></figure><h1 id="5694" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">E.优化目标</h1><h1 id="5c6b" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">E.1 .获得梯度方程(微分)</h1><h2 id="5daa" class="mt lw jf bd lx mu mv dn mb mw mx dp mf kn my mz mj kr na nb mn kv nc nd mr ne bi translated">E.1.1 .硬币实验，“平均学习”</h2><p id="6607" class="pw-post-body-paragraph kc kd jf ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">我们可以加入似然函数:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/79a563b4fc8b289189560fc7853ba24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*zgE-ISTzZzYLJazK6PFkJw.png"/></div></div></figure><p id="b13f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">其中<em class="la"> s </em>为硬币实验成功(正面)发生的次数，<em class="la"> n-s </em>为失败(反面)发生的次数，如下所示:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a638e43c48cae2f1ca025b0fe4353321.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*S2HI65HJ489mFyDK01XWTg.png"/></div></figure><p id="9d4e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">取联合似然函数的对数，我们得到对数似然:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ef11745f531bcbbadcafce6153cd2a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*lDYSibNMYha1ZcAkNdgEJQ.png"/></div></figure><p id="13e9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">它可以写成总和的形式:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6a84eaefd7027c112f2abc121cf331b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*PD6NdjCffjai4MdL9bAXLQ.png"/></div></figure><p id="2fe0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">取该函数相对于<em class="la"> p </em>的导数，并使其等于零，这将为我们带来最大化对数似然的<em class="la"> p </em>的最佳值。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/52630f411f0f2b8384bd316fda372f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/format:webp/1*u4151j5ZwAgFCIGhgB11xw.png"/></div></figure><p id="40e3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">分配求和运算:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ff6054b8954ea763d69cb5ee67310548.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*eEueI2mB7QgPRLSEiOlt8g.png"/></div></figure><p id="3e94" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">乘以<em class="la"> p(1-p) </em>我们得到:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3381ddb1b954556123aff9678b36a4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*D94H3gd8C2MsYPYWZOMdsA.png"/></div></figure><p id="c1d9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">分布后，我们看到两个结果项相互抵消:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/fc6725dc5b8bb745339e87e91acd99ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*MWMbMFfh38NYFWr4Op4bjA.png"/></div></div></figure><p id="bd99" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">留给我们的是:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/23ec45c80f3a4e4b2b9211af63720e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*_YbKuLXGKkmi8y1-UbKdFw.png"/></div></figure><p id="c163" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">求解为<em class="la"> p </em>:</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/9d4e9af62766581ded3967eccb06c522.png" data-original-src="https://miro.medium.com/v2/resize:fit:232/format:webp/1*9Q2WnaFjSyGSG65d3ZlaWw.png"/></div></figure><p id="01f5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">看看我们如何证明在没有输入(解释变量)存在的情况下，'<em class="la">平均值</em>'将带来最佳的估计性能(最大似然)。</p><h2 id="89a9" class="mt lw jf bd lx mu mv dn mb mw mx dp mf kn my mz mj kr na nb mn kv nc nd mr ne bi translated">E.1.2 .信用评分实验，“随机学习”</h2><p id="294d" class="pw-post-body-paragraph kc kd jf ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">像在一个真正的机器学习实验中一样考虑 xᵢ的情况怎么样？计算输入向量中每个观测值的“梯度/斜率”需要很长时间，并且当输入集合非常大时，通常是“内存不足”类型的操作。使用“随机”过程通过从数据集中随机选择特征向量并仅计算其梯度/斜率来弥补这种低效率。该过程重复选择随机数据点并比较它们的斜率，直到出现斜率收敛。这个过程以“随机梯度下降/上升——SGD/SGA”优化而闻名。SGD 算法的符号旅行如下图 1 所示。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/17b48f8241d5f488edfa79d5686ac940.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*jeMxJLZz-o5xniDMKqcTAg.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="nv">Figure-1: Gradient Descent Illustration</em></figcaption></figure><p id="7388" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">要应用 SGD/SGA，我们需要回到“信用评分”实验，该实验具有下面给出的目标(联合似然)函数，并首先获得其梯度方程。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi np"><img src="../Images/fe63f21e9d4b0d98022224c692a7df97.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*9q9Nrz3bs6HL-6GASoptaw.png"/></div></figure><p id="b69d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以下操作是寻找“信用评分”实验的目标函数的“梯度方程”的连续步骤。</p><p id="38e1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"><em class="la"/></strong>:联合对数似然函数</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nw"><img src="../Images/c3f4dacf03daa86755f2a8b2bee11794.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*ooGr0VIdjZN5xi5nGxzT6A.png"/></div></div></figure><p id="aab5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la"> Step-2 </em> </strong>:取 log 将<em class="la"> ∏ </em>转换为<em class="la"> ∑ </em></p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/cad7a122a3d47781e77715a65546cedf.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*8qYDfJkrVhqIKhKOPDUsWg.png"/></div></figure><p id="19f7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">第三步</em> </strong>:将<em class="la"> P(yᵢ|xᵢ) </em>替换为<em class="la"> 1/(1+e⁻ᶠ) </em></p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/3b53a70c96de7566c1e23ea1178fed05.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*2t4_z7qGymNz9jEDYQ8o7Q.png"/></div></figure><p id="8a27" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">第四步</em> </strong>:结合 wrt <em class="la"> yᵢ </em></p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/ea6a12949fe2352280249808943a67ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*AGIfSP96n5axzL84rM1Fhw.png"/></div></figure><p id="7ea7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">第五步</em> </strong>:合并方括号内的两个对数项</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ee4d329b90ab35759188785084499924.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*8BTzBNXy51h1Z06Wi5-yaw.png"/></div></figure><p id="9ba1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">第六步</em> </strong>:取消对数和指数函数</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/f766d72f21ae5f97fe5ce5989196f71e.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*VBDDuPvxN5ezS-ExhDk6aQ.png"/></div></figure><p id="0087" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la"> Step-7 </em> </strong>:我们要取偏导数的最终方程</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/15a40898c7b58b0c04098288ce81df2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*9SBS-UtVPRZwkQhd5dFHoA.png"/></div></figure><p id="99a2" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> <em class="la">第 8 步</em> </strong>:偏导数</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7e4cd01c3ce410de88b54da61243634e.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*mFpRQJkNO3S4DYS3lzq3aA.png"/></div></figure><ul class=""><li id="43b8" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated"><em class="la">偏-1 </em>:</li></ul><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4d171db74645f1745a1df7f6bef8efca.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*422oAoZT_8WyfjojjY1FEQ.png"/></div></figure><ul class=""><li id="78ff" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated"><em class="la">部分-2 </em>:</li></ul><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f195eafa0758f0892e538293f51d5496.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*2_DizIez1FdeAfgD0thfoA.png"/></div></figure><ul class=""><li id="069f" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated"><em class="la">组合分音</em>:</li></ul><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5c2cd9a8c19838e1e056b9ac6e61b97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*INVxIQPpvRq6iGx_Bcys5g.png"/></div></figure><ul class=""><li id="9d9a" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated"><em class="la">用相应的条件概率(后验)项替换指数项</em>:</li></ul><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/6e6dfd4079d27201297135dbf6e6b1e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*kCo_VQqhPlq-_LQsayEWTA.png"/></div></figure><ul class=""><li id="acb6" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated"><em class="la">以 xᵢ为普通</em>:</li></ul><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/986777b20ab778b1a87d3ec7e9ae597f.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*IpVBe6suX_QjyrcscKbyGA.png"/></div></figure><p id="7edb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">给出对数似然梯度的最终形式(</em> <strong class="ke jg"> <em class="la">梯度方程</em> </strong> <em class="la"> ) </em>。</p><p id="1480" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">总之，我们的目标是找到参数<em class="la"> β </em>的最佳值，使“信用评分”实验的对数似然函数最大化。上述步骤为我们提供了对数似然的微分版本，预计它将在“梯度/斜率”为零的点处收敛到局部最大值/最小值。</p><p id="b691" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，在常规的优化过程中[9]算法将尝试计算每个数据点的<em class="la"> βᵢ </em>向量<em class="la"> xᵢ </em>向量，这是一个“<em class="la"> j×1 </em>维度的特征向量。由于计算所有系数在计算上是低效的，SGD/SGA 来到舞台上，牵着我们的手，走在目标/损失曲线[10]的底部或顶部。</p><h1 id="f256" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">E.2 .寻找最大值-最小值</h1><p id="4c49" class="pw-post-body-paragraph kc kd jf ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">由于最终的“梯度方程”是“超越”形式，即包含非代数函数，如对数和指数，它不能直接求解(不存在封闭形式的解)。所以我们需要<strong class="ke jg">迭代</strong>逼近技术，例如；</p><ul class=""><li id="1b37" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated">梯度下降</li><li id="26c7" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">牛顿-拉夫森方法</li></ul><p id="2891" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在机器学习中，我们通常在试图逼近目标函数的全局最大值或最小值时使用梯度下降技术，因为它相对于牛顿-拉夫逊法具有具体的优势。</p><p id="4ecd" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Newton-Raphson 方法是一种求根算法[11],它利用函数的二阶导数知识(Hessian 矩阵)使函数最大化。当二阶导数[12]已知且易于计算时(如在逻辑回归中)，这可能会更快。然而，二阶导数的解析表达式通常是复杂或难以处理的，需要大量的计算。</p><p id="7e34" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">另一方面，梯度下降仅使用一阶导数的知识来最大化/最小化函数。它简单地跟随从当前点到期望的山或洞的最陡下降。这就像在损失函数图中滚动一个球(如图 1 所示)，直到它停下来。由于梯度下降使用一阶导数，它被配置为寻找局部最大值/最小值，但我们需要得到全局最大值/最小值。为了解决这个问题，我们使用“随机”方法，随机计算损失曲线/超平面的不同点的梯度，并将所有局部最小值/最大值相互比较以获得全局值。这个过程如图 2 所示，其中 y 轴代表样本内误差(Eᵢₙ)'来自训练中的预测，x 轴代表在每一步 t 变化的权重/系数(βₜ)</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/946d0ed125a9a9b9e902b59dca8a2d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*qCD14ZqTkzkVcqc4aPjKGg.jpeg"/></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="nv">Figure-2: Stochastic Gradient Descent Illustration</em></figcaption></figure><p id="611a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">标准梯度下降算法定义如下，其中<em class="la"> η </em>是“<em class="la">学习率</em>，<em class="la">δ</em>代表梯度方程。</p><figure class="lr ls lt lu gt is gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/53cf24d58c6dae5e589789e66642882c.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*cEEU2ZzERg_Z1v_IOBd08g.png"/></div></figure><p id="98a8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在上面的算法中，在当前点，人们采取与函数的梯度(或近似梯度)的负值成比例的步骤(学习速率)。相反，如果采取与梯度的正值成比例的步长，则接近该函数的局部最大值。这个过程被称为梯度上升。</p><p id="d822" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">更正式地说，我们应该注意到，当 Eᵢₙ(||βₜ-βₜ-₁||在我们做出步骤后开始不显示有意义的变化时，上述算法将收敛(中断)。</p><p id="85cf" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了获得更多关于梯度下降算法的技术知识，最好阅读<a class="ae lb" rel="noopener" target="_blank" href="/gradient-descent-demystified-bc30b26e432a"> Avinash Kadimisetty </a>在《走向数据科学》中的帖子，并在 youtube 上观看<em class="la"> 3BLUE1BROWNSERIES </em>频道的<a class="ae lb" href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;t=463s" rel="noopener ugc nofollow" target="_blank">视频</a>，这在我看来绝对是最有信息量的分享！除此之外，如果你想了解牛顿-拉夫森方法及其在逻辑回归中的应用，请观看 Youtube 上<em class="la">百货</em>频道精心准备的<a class="ae lb" href="https://www.youtube.com/channel/UC5_6ZD6s8klmMu9TXEB_1IA" rel="noopener ugc nofollow" target="_blank">视频</a>。所有这些分享都列在文章末尾的参考文献列表中。</p><h1 id="12e5" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">F.进一步阅读</h1><ul class=""><li id="de97" class="lc ld jf ke b kf nf kj ng kn ok kr ol kv om kz lh li lj lk bi translated"><strong class="ke jg"> Sigmoid vs. ReLu </strong>:使用‘Sigmoid’作为激活函数，在训练模型时会带来一些弊端。例如，它的一阶导数不是单调的，如下所示。ReLu 是目前神经网络分类器最常用的激活函数。详细说明可以从<a class="ae lb" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6">这里</a>找到。</li></ul><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/5322fd93bcf07db675f811b59e6f2c70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A3A_rt4YmumHusvTvVTxw.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk">Figure-3: Derivative of Sigmoid Function</figcaption></figure><figure class="lr ls lt lu gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oo"><img src="../Images/b39eaeadf45176e5af69452ba7bb0df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*29VH_NiSdoLJ1jUMLrURCA.png"/></div></div><figcaption class="iz ja gj gh gi jb jc bd b be z dk"><em class="nv">Figure-4: Sigmoid and ReLu Comparison</em></figcaption></figure><ul class=""><li id="2af9" class="lc ld jf ke b kf kg kj kk kn le kr lf kv lg kz lh li lj lk bi translated">逻辑回归 vs .朴素贝叶斯:这实际上是理解“判别型”和“生成型”模型之间的区别。这里有一个简短而优雅的帖子。</li></ul><h1 id="3454" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">G.附录</h1><h1 id="8702" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">G.1 .脚注</h1><p id="584d" class="pw-post-body-paragraph kc kd jf ke b kf nf kh ki kj ng kl km kn nh kp kq kr ni kt ku kv nj kx ky kz ij bi translated">[1]互补子群被称为“生成模型”，其成员包括“朴素贝叶斯”和“费希尔线性判别式”。</p><p id="d62c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[2]因为它产生的曲线是连续的。但是“希格诺函数——符号(x)”不是，因为它是离散的。</p><p id="3cf0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[3]这导致了拟合过程中的“偏差”。噪声是数据产生过程的自然过程。因此，即使我们使用我们所有的数据集(使用平均假设集<em class="la"> g_bar(x) </em>，对于未知的目标函数，总有一个“近似”极限。</p><p id="c02e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[4]由该比率产生的值将用于建立“评分等级”,这是“信用评分模型”建立过程的最后一部分。</p><p id="dafd" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[5]寻找问题的答案:“特征中一个单位的变化对目标的影响有多大？”</p><p id="0e29" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[6]例如随机梯度下降或二次规划</p><p id="57e1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[7]朴素贝叶斯、集合树、SVM、神经网络等。</p><p id="fc26" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[8]或者最小化“对数似然函数”的负值将是一个棘手的动作，这取决于我们拥有的优化工具。如果目标是最小化，目标函数可以称为“损失/成本函数”。</p><p id="9b24" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[9]例如，一种“梯度下降”算法，它不是“随机地”工作，而是顺序地试图计算目标/损失函数曲线/超平面上的所有斜率。</p><p id="582b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[10]或者通常是如图 4 所示的超平面</p><p id="b13b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[11]它被称为“求根方法”,因为它试图通过用线性函数 g 逼近 f’然后显式求解该函数的根来找到满足 f’(x)= 0 的点 x。g 的根不是</p><p id="6e80" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[12]帮助我们了解目标函数曲面的“凹度”。</p><h1 id="c2c2" class="lv lw jf bd lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms bi translated">G.2 .参考</h1><ul class=""><li id="a5b1" class="lc ld jf ke b kf nf kj ng kn ok kr ol kv om kz lh li lj lk bi translated"><a class="ae lb" href="https://www.cs.cmu.edu/~mgormley/courses/10701-f16/schedule.html" rel="noopener ugc nofollow" target="_blank">https://www . cs . CMU . edu/~ mgormley/courses/10701-F16/schedule . html</a>(第五讲)</li><li id="3645" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/archive/cs/cs 109/cs 109.1166/pdf/40% 20 logistic regression . pdf</a></li><li id="953c" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://www.youtube.com/watch?v=mbyG85GZ0PI&amp;t=2s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=mbyG85GZ0PI&amp;t = 2s</a></li><li id="c82d" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="http://kronosapiens.github.io/blog/2017/03/28/objective-functions-in-machine-learning.html" rel="noopener ugc nofollow" target="_blank">http://kronosapiens . github . io/blog/2017/03/28/objective-functions-in-machine-learning . html</a></li><li id="1a31" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/gradient-descent-demystified-bc30b26e432a">https://towards data science . com/gradient-descent-de mysticed-BC 30 b 26 e 432 a</a></li><li id="a77f" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">https://youtu.be/YMJtsYIp4kg<a class="ae lb" href="https://youtu.be/YMJtsYIp4kg" rel="noopener ugc nofollow" target="_blank"/></li><li id="be1d" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated">【https://newonlinecourses.science.psu.edu/stat414/node/191/ T4】</li><li id="e39b" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://datascience.stackexchange.com/questions/25444/advantages-of-monotonic-activation-functions-over-non-monotonic-functions-in-neu" rel="noopener ugc nofollow" target="_blank">https://data science . stack exchange . com/questions/25444/advantages-of-monotonic-activation-functions-over-non-monotonic-functions-in-neu</a></li><li id="7653" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://stats.stackexchange.com/questions/253632/why-is-newtons-method-not-widely-used-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/253632/why-is-newtons-method-not-wide-used-in-machine-learning</a></li><li id="2e67" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://stackoverflow.com/questions/12066761/what-is-the-difference-between-gradient-descent-and-newtons-gradient-descent" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/12066761/梯度下降和牛顿梯度下降的区别是什么</a></li><li id="d8c8" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="http://www.wikizero.biz/index.php?q=aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvTGluZWFyX2NsYXNzaWZpZXI" rel="noopener ugc nofollow" target="_blank">http://www.wikizero.biz/index.php?q = ahr 0 CHM 6 ly 9 lbi 53 wtpcgvkaweub 3 jnl 3 DP a2 kvtgluzwfyx 2 nsyxnzawzpxi</a></li><li id="85e4" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Linear_classifier" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Linear_classifier</a></li><li id="0852" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Logistic_regression" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Logistic_regression</a></li><li id="96c7" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;t=463s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;t = 463s</a></li><li id="6d79" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/activation-functions-neural-networks-1cbd9f8d91d6">https://towards data science . com/activation-functions-neural-networks-1 CBD 9 F8 d 91d 6</a></li><li id="2824" class="lc ld jf ke b kf ll kj lm kn ln kr lo kv lp kz lh li lj lk bi translated"><a class="ae lb" href="https://sebastianraschka.com/faq/docs/naive-bayes-vs-logistic-regression.html" rel="noopener ugc nofollow" target="_blank">https://sebastianraschka . com/FAQ/docs/naive-Bayes-vs-logistic-regression . html</a></li></ul></div></div>    
</body>
</html>