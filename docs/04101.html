<html>
<head>
<title>Deep Learning and Medical Image Analysis for Malaria Detection with fastai</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">fastai 用于疟疾检测的深度学习和医学图像分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-and-medical-image-analysis-for-malaria-detection-with-fastai-c8f08560262f?source=collection_archive---------24-----------------------#2019-06-27">https://towardsdatascience.com/deep-learning-and-medical-image-analysis-for-malaria-detection-with-fastai-c8f08560262f?source=collection_archive---------24-----------------------#2019-06-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="519c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习使用高级深度学习环境对血液涂片图像进行分类</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f571f87ccb38aff49f8378fc35d90975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-iNYbQM1uUrvKipml0ZpQ.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Jimmy Chan/Pexels free images</figcaption></figure><p id="80dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在国家医学图书馆(NLM)的一部分 Lister Hill 国家生物医学通信中心(LHNCBC)提供了一个健康和受感染的血液涂片疟疾图像的<a class="ae lu" href="https://ceb.nlm.nih.gov/repositories/malaria-datasets/" rel="noopener ugc nofollow" target="_blank">注释数据集</a>之后，各种帖子和论文已经发表，展示了如何使用卷积神经网络的图像分类来学习和分类这些图像。</p><p id="ac2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该数据集中的图像看起来像我们下面收集的图像:寄生的涂片将显示一些彩色点，而未感染的涂片将倾向于均匀着色。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/9f4c62d7d51083d0afe494ccabbed543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*7sQD2kxeqjf5OXqK5nG6iA.png"/></div></figure><p id="6a2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对这些涂片进行分类应该不是一件非常困难的事情。下面列出的帖子中描述的结果表明，96%到 97%的分类准确率是可行的。</p><p id="897b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我们将展示如何使用<strong class="la iu"> fast.ai </strong> CNN 图书馆来学习分类这些疟疾涂片。Fast.ai <strong class="la iu"> </strong>是一个库，基于 PyTorch 构建，使得编写机器学习应用程序变得更快更简单。Fast.ai 还提供了一门<a class="ae lu" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank">在线课程</a>，涵盖了 fast.ai 和深度学习的一般使用。与较低级别的“高级”库相比，如<em class="lw"> Keras </em>、<em class="lw"> TensorFlow </em>。无论是 Keras 还是 pure <em class="lw"> PyTorch </em>，fast.ai 都极大地减少了制作最先进的神经网络应用程序所需的样板代码数量。</p><h1 id="ea34" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">本公告基于以下材料:</h1><ol class=""><li id="a4c0" class="mp mq it la b lb mr le ms lh mt ll mu lp mv lt mw mx my mz bi translated"><a class="ae lu" href="https://www.pyimagesearch.com/2018/12/03/deep-learning-and-medical-image-analysis-with-keras/" rel="noopener ugc nofollow" target="_blank"> PyImagesearch::深度学习和医学图像分析与 Keras </a> —以疟疾图像为例，作者 Adrian Rosebrock，2018 年 12 月 3 日；</li><li id="b698" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><a class="ae lu" href="https://ceb.nlm.nih.gov/repositories/malaria-datasets/" rel="noopener ugc nofollow" target="_blank">来自 NIH </a>的疟疾数据集——来自疟疾筛查者研究活动的薄血涂片图像的分割细胞库；</li><li id="fb2a" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/detecting-malaria-with-deep-learning-9e45c1e34b60"> TowardsDataScience::利用深度学习检测疟疾——人工智能造福社会——医疗保健案例研究</a>——Dipanjan(DJ)Sarkar 对上述内容的评论；</li><li id="5328" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt mw mx my mz bi translated"><a class="ae lu" href="https://peerj.com/articles/4568/" rel="noopener ugc nofollow" target="_blank">PeerJ::Sivaramakrishnan Raja Raman 等人的预训练卷积神经网络作为特征提取器，用于改进薄血涂片图像中的疟原虫检测</a> —上述帖子的作者基于其工作的原始科学论文。</li></ol><p id="1c3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经改编了这个材料，以便在 2019 年 5 月与 PyTorch/fast.ai 一起使用。<a class="ae lu" href="https://drive.google.com/open?id=1a1_jjRcq6xRzQXvmwhJuhq1A97jnp9xO" rel="noopener ugc nofollow" target="_blank">被评论的代码作为谷歌合作笔记本</a>免费提供。</p><p id="0e20" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们去看代码吧…</p><h1 id="ea23" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">初始化</h1><p id="00ce" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">每次运行此笔记本时，请执行以下部分中的操作一次…</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="da6b" class="nn ly it nj b gy no np l nq nr">%reload_ext autoreload<br/>%autoreload 2<br/>%matplotlib inline</span></pre><h1 id="0472" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">在 Google Colab 上测试您的虚拟机…</h1><p id="463f" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">只是为了确定，看看哪一个 CUDA 驱动和哪一个 GPU Colab 已经为你提供了。GPU 通常是:</p><ul class=""><li id="db28" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated">一个 11 GB 内存的<strong class="la iu"> K80 </strong>或者(如果你真的幸运的话)</li><li id="b885" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nv mx my mz bi translated">一个 14 GB 内存的特斯拉 T4</li></ul><p id="bf71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果谷歌的服务器很拥挤，你最终只能访问 GPU 的一部分。如果您的 GPU 与另一台<em class="lw"> Colab </em>笔记本共享，您将看到可供您使用的内存量减少。</p><p id="4129" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">小贴士:避开美国西海岸的高峰期。我住在 GMT-3，我们比美国东海岸早两个小时，所以我总是试图在早上进行繁重的处理。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="3aff" class="nn ly it nj b gy no np l nq nr">!/opt/bin/nvidia-smi<br/>!nvcc --version</span></pre><p id="2c9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我开始运行这里描述的实验时，我很幸运:我有一个 15079 MB RAM 的完整 T4！我的输出如下所示:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="ed84" class="nn ly it nj b gy no np l nq nr">Thu May  2 07:36:26 2019       <br/>+-----------------------------------------------------------------------------+<br/>| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |<br/>|-------------------------------+----------------------+----------------------+<br/>| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br/>| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br/>|===============================+======================+======================|<br/>|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |<br/>| N/A   63C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |<br/>+-------------------------------+----------------------+----------------------+<br/>                                                                               <br/>+-----------------------------------------------------------------------------+<br/>| Processes:                                                       GPU Memory |<br/>|  GPU       PID   Type   Process name                             Usage      |<br/>|=============================================================================|<br/>|  No running processes found                                                 |<br/>+-----------------------------------------------------------------------------+<br/>nvcc: NVIDIA (R) Cuda compiler driver<br/>Copyright (c) 2005-2018 NVIDIA Corporation<br/>Built on Sat_Aug_25_21:08:01_CDT_2018<br/>Cuda compilation tools, release 10.0, V10.0.130</span></pre><h1 id="e45e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">图书馆进口</h1><p id="17e5" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">在这里，我们导入所有必需的包。我们将使用<a class="ae lu" href="http://www.fast.ai/2018/10/02/fastai-ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai V1 库</a>，它位于<a class="ae lu" href="https://hackernoon.com/pytorch-1-0-468332ba5163" rel="noopener ugc nofollow" target="_blank"> Pytorch 1.0 </a>之上。fast.ai 库提供了许多有用的功能，使我们能够快速轻松地建立神经网络并训练我们的模型。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f2e4" class="nn ly it nj b gy no np l nq nr">from fastai.vision import *<br/>from fastai.metrics import error_rate<br/>from fastai.callbacks import SaveModelCallback</span><span id="4e5d" class="nn ly it nj b gy nw np l nq nr"># Imports for diverse utilities<br/>from shutil import copyfile<br/>import matplotlib.pyplot as plt<br/>import operator<br/>from PIL import Image<br/>from sys import intern   # For the symbol definitions</span></pre><h1 id="faa1" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">实用功能:导出和恢复功能</h1><p id="3e65" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">导出用于部署的网络并创建副本</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="82ef" class="nn ly it nj b gy no np l nq nr">def exportStageTo(learn, path):<br/>    learn.export()<br/>    # Faça backup diferenciado<br/>    copyfile(path/'export.pkl', path/'export-malaria.pkl')<br/>    <br/>#exportStage1(learn, path)</span></pre><p id="e046" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">恢复部署模型，例如为了继续微调</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="7709" class="nn ly it nj b gy no np l nq nr">def restoreStageFrom(path):<br/>  # Restore a backup<br/>  copyfile(path/'export-malaria.pkl', path/'export.pkl')<br/>  return load_learner(path)<br/>  <br/>#learn = restoreStage1From(path)</span></pre><h1 id="5989" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">下载疟疾数据</h1><p id="89e5" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我上面列出的作者使用了 NIH 疟疾数据集。让我们做同样的事情:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="7e51" class="nn ly it nj b gy no np l nq nr">!wget  --backups=1 -q <a class="ae lu" href="https://ceb.nlm.nih.gov/proj/malaria/cell_images.zip" rel="noopener ugc nofollow" target="_blank">https://ceb.nlm.nih.gov/proj/malaria/cell_images.zip</a><br/>!wget  --backups=1 -q <a class="ae lu" href="https://ceb.nlm.nih.gov/proj/malaria/malaria_cell_classification_code.zip" rel="noopener ugc nofollow" target="_blank">https://ceb.nlm.nih.gov/proj/malaria#/malaria_cell_classification_code.zip</a><br/># List what you've downloaded:<br/>!ls -al</span></pre><p id="834a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lw"> wget </em>的<em class="lw"> backups=1 </em>参数将允许您在下载失败的情况下多次重复命令行，而无需创建大量新版本的文件。</p><p id="a9d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后一行应该产生以下输出:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="6174" class="nn ly it nj b gy no np l nq nr">total 345208<br/>drwxr-xr-x 1 root root      4096 May  2 07:45 .<br/>drwxr-xr-x 1 root root      4096 May  2 07:35 ..<br/>-rw-r--r-- 1 root root 353452851 Apr  6  2018 cell_images.zip<br/>drwxr-xr-x 1 root root      4096 Apr 29 16:32 .config<br/>-rw-r--r-- 1 root root     12581 Apr  6  2018 malaria_cell_classification_code.zip<br/>drwxr-xr-x 1 root root      4096 Apr 29 16:32 sample_data</span></pre><p id="e3b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在解压缩 NIH 疟疾细胞图像数据集:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="731f" class="nn ly it nj b gy no np l nq nr">!unzip cell_images.zip</span></pre><p id="f698" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将产生一个非常大的详细输出，如下所示:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="0b48" class="nn ly it nj b gy no np l nq nr">Archive:  cell_images.zip<br/>   creating: cell_images/<br/>   creating: cell_images/Parasitized/<br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_162.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_163.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_164.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_165.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_166.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_167.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_168.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_169.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_170.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144104_cell_171.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_138.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_139.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_140.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_141.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_142.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_143.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144348_cell_144.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144823_cell_157.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144823_cell_158.png  <br/> extracting: cell_images/Parasitized/C100P61ThinF_IMG_20150918_144823_cell_159.png<br/>....<br/>....<br/>....<br/>...and so on...</span></pre><h1 id="421b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">准备您的数据</h1><p id="0bed" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">将<strong class="la iu"> cell_images </strong>文件夹的名称改为<strong class="la iu"> train </strong>，然后将<em class="lw"> mv </em>放在一个新的<em class="lw"> data </em>文件夹上面，这样<em class="lw"> fast.ai </em>就可以用它来<em class="lw">自动</em>生成<em class="lw"> train，验证</em>和<em class="lw">测试</em>集合，而不用再大惊小怪了…</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="02b7" class="nn ly it nj b gy no np l nq nr">!mv cell_images train<br/>!mkdir data<br/>!mv train data</span></pre><h1 id="55e8" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">看看你的文件夹</h1><p id="4799" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">如果还没有安装<em class="lw"> tree </em>命令，使用<em class="lw">来安装 tree </em>:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="44be" class="nn ly it nj b gy no np l nq nr">!apt install tree</span></pre><p id="5aab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在运行它:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f1b2" class="nn ly it nj b gy no np l nq nr">!tree ./data --dirsfirst --filelimit 10</span></pre><p id="8e92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将显示文件树的结构:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="55cc" class="nn ly it nj b gy no np l nq nr">./data<br/>└── train<br/>    ├── Parasitized [13780 exceeds filelimit, not opening dir]<br/>    └── Uninfected [13780 exceeds filelimit, not opening dir]</span><span id="0259" class="nn ly it nj b gy nw np l nq nr">3 directories, 0 files</span></pre><p id="623d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不要忘记设置一个<em class="lw">文件限制</em>，否则你会有大量的输出…</p><h1 id="6505" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">初始化一些变量</h1><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="1051" class="nn ly it nj b gy no np l nq nr">bs = 256        # Batch size, 256 for small images on a T4 GPU...<br/>size = 128      # Image size, 128x128 is a bit smaller than most <br/>                # of the images...<br/>path = Path("./data")   # The path to the 'train' folder you created...</span></pre><h1 id="0607" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">创建您的培训和验证数据集</h1><p id="c001" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">在使用<em class="lw"> Keras </em>的<em class="lw"> PyImagesearch </em>的原始资料中，有一个很长的例程从数据中创建训练、验证和测试文件夹。有了<em class="lw"> fast.ai </em>就没必要了:如果你只有一个‘train’文件夹，你可以在创建<strong class="la iu"> DataBunch </strong>时通过简单地传递几个参数来自动分割它。我们将把数据分成一个<em class="lw">训练集</em> (80%)和一个<em class="lw">验证集</em> (20%)。这是通过<em class="lw">imagedatabunch . from _ folder()</em>构造函数方法中的<strong class="la iu"> valid_pct = 0.2 </strong>参数完成的:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="be10" class="nn ly it nj b gy no np l nq nr"># Limit your augmentations: it's medical data! <br/># You do not want to phantasize data...<br/># Warping, for example, will let your images badly distorted, <br/># so don't do it!<br/># This dataset is big, so don't rotate the images either. <br/># Lets stick to flipping...<br/>tfms = get_transforms(max_rotate=None, max_warp=None, max_zoom=1.0)<br/># Create the DataBunch!<br/># Remember that you'll have images that are bigger than 128x128 <br/># and images that are smaller, o squish them all in order to <br/># occupy exactly 128x128 pixels...<br/>data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH, <strong class="nj iu">valid_pct = 0.2</strong>, bs=bs)<br/>#<br/>print('Transforms = ', len(tfms))<br/># Save the DataBunch in case the training goes south... <br/># so you won't have to regenerate it..<br/># Remember: this DataBunch is tied to the batch size you selected. <br/>data.save('imageDataBunch-bs-'+str(bs)+'-size-'+str(size)+'.pkl')<br/># Show the statistics of the Bunch...<br/>print(data.classes)<br/>data</span></pre><p id="ad67" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lw"> print() </em>将输出转换和类:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="3cdb" class="nn ly it nj b gy no np l nq nr">Transforms =  2<br/>['Parasitized', 'Uninfected']</span></pre><p id="2e59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后一行，<em class="lw"> data </em>将简单地输出<em class="lw"> ImageDataBunch </em>实例的返回值:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="846f" class="nn ly it nj b gy no np l nq nr">ImageDataBunch;</span><span id="2fec" class="nn ly it nj b gy nw np l nq nr">Train: LabelList (22047 items)<br/>x: ImageList<br/>Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)<br/>y: CategoryList<br/>Uninfected,Uninfected,Uninfected,Uninfected,Uninfected<br/>Path: data;</span><span id="e0be" class="nn ly it nj b gy nw np l nq nr">Valid: LabelList (5511 items)<br/>x: ImageList<br/>Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)<br/>y: CategoryList<br/>Parasitized,Uninfected,Parasitized,Uninfected,Uninfected<br/>Path: data;</span><span id="7444" class="nn ly it nj b gy nw np l nq nr">Test: None</span></pre><h1 id="6c5e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">查看您的数据集群，看看增加是否可以接受…</h1><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="cd40" class="nn ly it nj b gy no np l nq nr">data.show_batch(rows=5, figsize=(15,15))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/fd9c2d0040b94cec28447cce19057d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F75bw80VnwYUdkMxefOBKg.png"/></div></div></figure><h1 id="e8ab" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">培训:resnet34</h1><p id="1861" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">如果你不知道用什么，从 34 层的剩余网络开始是一个好的选择。不要太小也不要太大…在上面列出的教程中，作者使用了:</p><ul class=""><li id="05e2" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated">一个自定义的小 ResNet (PyImagesearch)</li><li id="7ade" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nv mx my mz bi translated">VGG19(面向数据科学)</li></ul><p id="051c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将采用现成的 fast.ai 残差网络(ResNets)。让我们创建我们的第一个网络:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="42c9" class="nn ly it nj b gy no np l nq nr">learn = cnn_learner(data, models.resnet34, metrics=error_rate)<br/>learn.model</span></pre><p id="7773" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后一行将以文本流的形式输出网络的架构。它看起来会像这样:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f0f8" class="nn ly it nj b gy no np l nq nr">Sequential(<br/>  (0): Sequential(<br/>    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU(inplace)<br/>    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>    (4): Sequential(<br/>      (0): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>      (1): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>      (2): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>....and so on....</span></pre><p id="8cc5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">即使像 ResNet34 这样的“小”网络也仍然非常大。不要费心去理解输出。你可以稍后阅读更多关于剩余网络的内容。有许多关于 ResNets 的介绍性帖子。</p><h1 id="0fb7" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">培训策略</h1><p id="3da2" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">fast.ai 的一大区别就来了:易用的 HYPOs(超参数优化策略)。超参数优化是 CNN 的一个有点神秘的分支学科。这是因为 CNN 有如此多的参数，试图通过设置一些非标准值来选择我们将改变的参数，以便为我们的网络提供更好的性能，这是一个非常复杂的问题，也是一项研究<em class="lw">本身</em>。fast.ai 库提供了一些非常先进但易于使用的 HYPOs，它们对快速实现更好的 CNN 有很大的帮助。</p><p id="698e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将采用 Leslie N. Smith 开发的<em class="lw"> fit1cycle </em>方法，详情如下:</p><ul class=""><li id="5e09" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated">https://docs.fast.ai/callbacks.one_cycle.html<a class="ae lu" href="https://docs.fast.ai/callbacks.one_cycle.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="90f7" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nv mx my mz bi translated">神经网络超参数的训练方法:第一部分——学习速率、批量大小、动量和权重衰减——https://arxiv.org/abs/1803.09820<a class="ae lu" href="https://arxiv.org/abs/1803.09820" rel="noopener ugc nofollow" target="_blank"/></li><li id="bb5f" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nv mx my mz bi translated">超级收敛:使用大学习率快速训练残差网络—<a class="ae lu" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1708.07120</a></li><li id="a072" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nv mx my mz bi translated">有一篇来自<a class="ae lu" href="https://towardsdatascience.com/@nachiket.tanksale" rel="noopener" target="_blank"> Nachiket Tanksale </a>的非常有趣的文章，名为<a class="ae lu" rel="noopener" target="_blank" href="/finding-good-learning-rate-and-the-one-cycle-policy-7159fe1db5d6">寻找好的学习率和单周期政策</a>，其中讨论了周期学习率和动量。</li></ul><p id="fb70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于这种方法很快，我们将在第一个迁移学习阶段仅使用 10 个时期。如果性能变得更好，我们也将在每个时期保存网络:<a class="ae lu" href="https://docs.fast.ai/callbacks.html#SaveModelCallback" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/callbacks.html#SaveModelCallback</a></p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="21e6" class="nn ly it nj b gy no np l nq nr">learn.fit_one_cycle(10, callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy', name='malaria-1')])<br/># Save it!<br/>learn.save('malaria-stage-1')<br/># Deploy it!<br/>exportStageTo(learn, path)</span></pre><p id="a4f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将生成一个如下所示的表作为输出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/379ba743d99ef7b7762dd619fc1fc249.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*V2M8XE5nkZC5WbfZm4Sfcg.png"/></div></figure><p id="d589" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上表显示，验证集的准确率为 96.4%，而且这仅仅是在迁移学习的情况下！<em class="lw"> error_rate </em> fast.ai 显示您将始终看到与训练集相关联的那个。作为比较，Adrian Rosebrock 在 PyImagesearch 帖子中使用他的自定义 ResNet 实现了 97%的搜索率。</p><h1 id="a9de" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">ResNet34 的结果</h1><p id="5ef3" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">让我们看看我们得到了什么额外的结果。我们将首先查看哪些是模型相互混淆的实例。我们将尝试看看模型预测的结果是否合理。在这种情况下，错误看起来是合理的(没有一个错误看起来明显幼稚)。这表明我们的分类器工作正常。</p><p id="dafc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，我们将绘制混淆矩阵。这在 fast.ai 中也很简单。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="bc89" class="nn ly it nj b gy no np l nq nr">interp = ClassificationInterpretation.from_learner(learn)</span><span id="46cc" class="nn ly it nj b gy nw np l nq nr">losses,idxs = interp.top_losses()</span><span id="7cd7" class="nn ly it nj b gy nw np l nq nr">len(data.valid_ds)==len(losses)==len(idxs)</span></pre><p id="3885" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看你的<strong class="la iu"> 9 个最差结果</strong>(首先不使用热图):</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="82dc" class="nn ly it nj b gy no np l nq nr">interp.plot_top_losses(9, figsize=(20,11), heatmap=False)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/61d3d0b49eb1fd8223bdbdd3f9650906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4dTxgo304db05SfOUG7pbg.png"/></div></div></figure><p id="5d97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，进行同样的操作，但使用热图突出显示导致错误分类的原因:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="4da8" class="nn ly it nj b gy no np l nq nr">interp.plot_top_losses(9, figsize=(20,11), heatmap=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/6558f98b7f9a36735a080ceea68f3231.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*86BQ1CXGmfC_Id83KJHTHQ.png"/></div></div></figure><h1 id="7632" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">显示混淆矩阵</h1><p id="32eb" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">fast.ai 的<em class="lw">ClassificationInterpretation</em>类有一个高级实例方法，允许快速轻松地绘制混淆矩阵，以更好的方式向您显示 CNN 的表现有多好。只有两个类并没有多大意义，但我们还是要这么做:它生成了漂亮的图片…你可以设置结果图的大小和分辨率。我们将 5x5 英寸设置为 100 dpi。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="94d4" class="nn ly it nj b gy no np l nq nr">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/26d9c18ed5bd0fed73743c9cc6021fe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*5KgobsrBdMwcFCCrHOCdPA.png"/></div></figure><h1 id="4cf3" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">展示你的学习曲线:</h1><p id="889a" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">观察学习和验证曲线是很有趣的。它将向我们显示网络是否以稳定的方式学习，或者它是否振荡(这可能表明质量差的数据)，以及我们是否有一个好的结果，或者我们是否过度拟合或欠拟合我们的网络。</p><p id="1176" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再说一次 fast.ai 有高级方法可以帮助我们。每个 fast.ai <em class="lw"> cnn_learner </em>都有一个自动创建的<em class="lw">记录器</em>实例。记录器记录训练期间的历元、损失、最优和度量数据。<em class="lw">plot _ loss()</em>方法将创建一个带有训练和验证曲线的图形:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="57f1" class="nn ly it nj b gy no np l nq nr">learn.recorder.plot_losses()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a04ba67e3eb612029dfb9c7ca1e53906.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*GH9LHYXgPbIi7f1ekKEC7w.png"/></div></figure><p id="6078" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个结果看起来真的太好了，网络微调没有意义。如果我们仔细观察，我们会发现在大约 500 个批次时，验证损失比训练损失更严重，这表明网络可能在这一点上开始过度拟合。这表明我们已经接受了足够的培训，至少对于这个 ResNet 模型是如此。</p><p id="b88f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们观察到的过度拟合可能表明我们采用的网络模型对于数据的复杂性来说是过度的，这意味着我们正在训练一个学习单个实例而不是数据集泛化的网络。测试这一假设的一个非常简单实用的方法是尝试用一个更简单的网络学习数据集，看看会发生什么。</p><p id="a15f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们使用一个更小的网络，再试一次…</p><h1 id="34d5" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">ResNet18</h1><p id="0fcd" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">这个网络简单多了。让我们看看它是否有效。</p><p id="b520" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">ResNet18 要小得多，所以我们会有更多的 GPU RAM。我们将再次创建<em class="lw">数据群发</em>，这一次批量更大…</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="ee8a" class="nn ly it nj b gy no np l nq nr"># Limit your augmentations: it's medical data! <br/># You do not want to phantasize data...<br/># Warping, for example, will let your images badly distorted, <br/># so don't do it!<br/># This dataset is big, so don't rotate the images either. <br/># Lets stick to flipping...<br/>tfms = get_transforms(max_rotate=None, max_warp=None, max_zoom=1.0)<br/># Create the DataBunch!<br/># Remember that you'll have images that are bigger than 128x128 <br/># and images that are smaller, so squish them to occupy <br/># exactly 128x128 pixels...<br/>data = ImageDataBunch.from_folder(path, ds_tfms=tfms, size=size, resize_method=ResizeMethod.SQUISH, valid_pct = 0.2, <strong class="nj iu">bs=512</strong>)<br/>#<br/>print('Transforms = ', len(tfms))<br/># Save the DataBunch in case the training goes south... so you won't have to regenerate it..<br/># Remember: this DataBunch is tied to the batch size you selected. <br/>data.save('imageDataBunch-bs-'+str(bs)+'-size-'+str(size)+'.pkl')<br/># Show the statistics of the Bunch...<br/>print(data.classes)<br/>data</span></pre><p id="1167" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，我们坚持使用我们的<em class="lw"> valid_pct = 0.2 </em>:我们仍然让 fast.ai 随机选择数据集的 20%作为验证集。</p><p id="cd4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的代码将输出如下内容:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="06cb" class="nn ly it nj b gy no np l nq nr">Transforms =  2<br/>['Parasitized', 'Uninfected']</span></pre><p id="5c8a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并且:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="84dc" class="nn ly it nj b gy no np l nq nr">ImageDataBunch;</span><span id="2360" class="nn ly it nj b gy nw np l nq nr">Train: LabelList (22047 items)<br/>x: ImageList<br/>Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)<br/>y: CategoryList<br/>Uninfected,Uninfected,Uninfected,Uninfected,Uninfected<br/>Path: data;</span><span id="8881" class="nn ly it nj b gy nw np l nq nr">Valid: LabelList (5511 items)<br/>x: ImageList<br/>Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128),Image (3, 128, 128)<br/>y: CategoryList<br/>Parasitized,Uninfected,Parasitized,Uninfected,Parasitized<br/>Path: data;</span><span id="a3d1" class="nn ly it nj b gy nw np l nq nr">Test: None</span></pre><p id="644d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，创建学习者:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="bce9" class="nn ly it nj b gy no np l nq nr">learn18 = cnn_learner(data, models.resnet18, metrics=error_rate)</span></pre><p id="e876" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您的 Colab 环境没有 ResNet18 的预训练数据，fast.ai 会自动下载它:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="e5f4" class="nn ly it nj b gy no np l nq nr">Downloading: "https://download.pytorch.org/models/resnet18-5c106cde.pth" to /root/.torch/models/resnet18-5c106cde.pth<br/>46827520it [00:01, 28999302.58it/s]</span></pre><p id="da9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看模型:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="2049" class="nn ly it nj b gy no np l nq nr">learn18.model</span></pre><p id="f06d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将列出你的网的结构。它比 ResNet34 小得多，但仍然有很多层。输出将如下所示:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="f947" class="nn ly it nj b gy no np l nq nr">Sequential(<br/>  (0): Sequential(<br/>    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)<br/>    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>    (2): ReLU(inplace)<br/>    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)<br/>    (4): Sequential(<br/>      (0): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>      (1): BasicBlock(<br/>        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>      )<br/>    )<br/>    (5): Sequential(<br/>      (0): BasicBlock(<br/>        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)<br/>        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (relu): ReLU(inplace)<br/>        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)<br/>        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        (downsample): Sequential(<br/>          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)<br/>          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)<br/>        )<br/>      )<br/>...and so on...</span></pre><h1 id="268b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">让我们训练它</h1><p id="5628" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">我们将再次使用<em class="lw"> fit_one_cycle </em> HYPO 训练策略。将训练限制在 10 个时期，以观察这个较小网络的行为:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="a57e" class="nn ly it nj b gy no np l nq nr">learn18.fit_one_cycle(10, callbacks=[SaveModelCallback(learn, every='epoch', monitor='accuracy', name='malaria18-1')])<br/># Save the network<br/>learn18.save('malaria18-stage-1')<br/># Deploy it also<br/>exportStageTo(learn18, path)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/2633fee189f206b6dc51b866660fe5e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*lGKefFZiGOZD5t6JUXJq1w.png"/></div></figure><p id="69ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该表显示网络学习到大约 96.1%的准确度，并且建议网络不应该被进一步训练:时段#8 和#9 之间的损失显示出 0.005 的减少，但是准确度保持不变，这表明网络已经开始过拟合。</p><p id="2a4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们生成一个<em class="lw">分类解释</em>并查看混淆矩阵和损失曲线。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="871d" class="nn ly it nj b gy no np l nq nr">interp = ClassificationInterpretation.from_learner(learn18)<br/>losses,idxs = interp.top_losses()</span><span id="0190" class="nn ly it nj b gy nw np l nq nr">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/3d359280aecca75a8e085921a0d598b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*RxT4Wu713McPhn2k4uWC6A.png"/></div></figure><p id="f1c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个混淆矩阵比我们为 ResNet34 生成的稍微差一点，但只是非常差一点。ResNet18 是否不太适合这个问题？</p><p id="1fb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来看看损失:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/56030c7cc784eb93c12ae69bd97f3d18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*3ci3kP6j7r3sFhRwHU1LgA.png"/></div></figure><p id="2d14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该图显示，ResNet18 在大约 290 批次后开始有点过度拟合。记住我们的<em class="lw"> bs </em>在这里是 512，在 ResNet34 是 256。</p><p id="c83c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看我们能否更好地微调网络。</p><h1 id="d10e" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">微调一下！</h1><p id="254d" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">这里我们将介绍另一个 fast.ai HYPO:自动选择的可变学习率。我们将让 fast.ai 为每个历元和每个层选择使用哪个学习速率，提供一个我们认为足够的学习速率范围。我们将训练 30 个纪元的网络。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="ea2c" class="nn ly it nj b gy no np l nq nr"># Unfreeze the network<br/>learn18.unfreeze()<br/># Learning rates range: max_lr=slice(1e-4,1e-5)<br/>learn18.fit_one_cycle(30, <strong class="nj iu">max_lr=slice(1e-4,1e-5)</strong>, <br/>                    callbacks=[SaveModelCallback(learn, <br/>                    every='epoch', monitor='accuracy', <br/>                    name='malaria18')])<br/># Save as stage 2...<br/>learn18.save('malaria18-stage-2')<br/># Deploy<br/>exportStageTo(learn18, path)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4e24bdfec375f474ab98942e8d9fd3fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/1*xmksjqCKgGfPo2fsxgIwVg.png"/></div></figure><p id="7022" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> 97%的准确率！</strong>这正是 Adrian Rosebrock 在<a class="ae lu" href="https://www.pyimagesearch.com/2018/12/03/deep-learning-and-medical-image-analysis-with-keras/" rel="noopener ugc nofollow" target="_blank"> PyImagesearch 帖子</a>中使用他的定制 Keras ResNet 实现所实现的，该帖子在上述三篇参考文献中提供了最佳的准确性结果。</p><p id="0be4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，<em class="lw">验证失败</em>在过去的时代变得<strong class="la iu">更糟</strong>。这表明我们从大约第 20 纪元开始就已经<em class="lw">过度拟合</em>。如果你想部署这个网络，我建议你从 epoch 20 加载结果并生成一个部署网络。它似乎没有变得更好，没有这个网络。</p><h1 id="90ca" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">看看结果</h1><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="dc45" class="nn ly it nj b gy no np l nq nr">interp = ClassificationInterpretation.from_learner(learn18)<br/>losses,idxs = interp.top_losses()</span><span id="9f43" class="nn ly it nj b gy nw np l nq nr">interp.plot_confusion_matrix(figsize=(5,5), dpi=100)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3e1a3f4d954bb89c3e118e1c0d9b66d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*syB4EcPfIaL5MQ4Myv_4Sw.png"/></div></figure><p id="e840" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这比我们以前吃过的要好。让我们看看损失曲线:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/4723ac7888be7564cddb7623224e86af.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*5fLMVahtSe7bEfFmNRBy-w.png"/></div></figure><p id="ed5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们看到网络似乎在 500 批次后开始过度拟合，这将证实我们从上面的结果表中推断的怀疑。如果你看上面的曲线，你会看到<strong class="la iu">验证损失</strong>在训练的最后三分之一开始增加，表明这部分训练只是过度拟合了网络。</p><h1 id="6816" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">如果培训中途中断，我该怎么办？</h1><p id="6810" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">如果你的训练被打断了，你会怎么做？这可能是因为你在 Google Colab 笔记本上达到了连续 12 小时的“免费”操作时间，或者因为你的计算机由于某种原因停止了。我住在巴西，电力短缺是常事…</p><p id="55f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lw"> fit_one_cycle </em>方法适用于变化的自适应学习速率，遵循速率先增大后减小的曲线。如果您在第 10 个时期(比如说 20 个时期)中断训练，然后重新开始 9 个时期以上的训练，<strong class="la iu">您将不会得到与不间断训练 20 个时期相同的结果</strong>。您必须能够记录您停止的位置，然后从该点重新开始训练周期，并使用该周期部分的正确超参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/8d5fa3658ed88e31179157e295612d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SfQdflj95ziJHcMiUfCNbA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">A fit_one_cycle training session divided into three subsessions. Image by PPW@GitHub</figcaption></figure><p id="47e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你要做的第一件事就是保存你的网络:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="dcb2" class="nn ly it nj b gy no np l nq nr">learn.fit_one_cycle(20, max_lr=slice(1e-5,1e-6), <br/>       callbacks=[SaveModelCallback(learn, every='epoch',  <br/>                  monitor='accuracy', name=<strong class="nj iu"><em class="lw">&lt;callback_save_file&gt;</em></strong>)])</span></pre><p id="953c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将使您的网络在每个时期都被保存，您提供的名称后跟<em class="lw">_ #时期</em>。所以在纪元#3，文件<em class="lw"> saved_net_3.pth </em>将被写入。您可以在完成以下操作后加载此文件:</p><ul class=""><li id="4a36" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated">重新创建了<em class="lw">数据束</em>和</li><li id="cb98" class="mp mq it la b lb na le nb lh nc ll nd lp ne lt nv mx my mz bi translated">用它重新实例化了网络。</li></ul><p id="f042" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">重新加载<em class="lw">后。pth </em>文件，你可以重新开始你的训练，只是你要告诉<em class="lw"> fit_one_cycle </em>考虑 20 个历元，但是要从历元#4 开始训练。</p><p id="e5e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要了解这是如何做到的，请看这里:</p><ul class=""><li id="70d6" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated"><a class="ae lu" href="https://github.com/PPPW/deep-learning-random-explore/tree/master/divide_1cycle?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">将一个长周期政策分成几个小周期——PPW 的 GitHub</a></li></ul><p id="a704" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你是怎么做到的？</p><p id="42cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">fast.ai 中的<em class="lw"> fit_one_cycle </em>方法已经开发出来，允许您告诉它从周期的哪个部分恢复中断的训练。恢复培训的代码如下所示:</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="9f1f" class="nn ly it nj b gy no np l nq nr"># Create a new net if training was interrupted and you had to <br/># restart your Colab session</span><span id="050b" class="nn ly it nj b gy nw np l nq nr">learn = cnn_learner(data, models.&lt;your_model_here&gt;, <br/>                    metrics=[accuracy, error_rate])</span><span id="3b39" class="nn ly it nj b gy nw np l nq nr"># If you're resuming, only indicating the epoch from which to <br/># resume, indicated by <strong class="nj iu"><em class="lw">start_epoch=&lt;epoch#&gt;</em></strong> will load the last <br/># saved .pth, it is not necessary to explicitly reload the last <br/># epoch, you only should <strong class="nj iu">NOT</strong> change the name given in <br/># name=&lt;callback_save_file&gt;:<br/># when resuming fast.ai will try to reload <br/># <strong class="nj iu"><em class="lw">&lt;callback_save_file&gt;_&lt;previous_epoch&gt;.pth</em></strong><br/># Unfreeze the network<br/>learn50.unfreeze()</span><span id="1b4d" class="nn ly it nj b gy nw np l nq nr"># Use start_epoch=&lt;some_epoch&gt; to resume training...<br/>learn.fit_one_cycle(20, max_lr=slice(1e-5,1e-6), <br/>                    <strong class="nj iu"><em class="lw">start_epoch=&lt;next_epoch#&gt;</em></strong>,<br/>                    callbacks=[SaveModelCallback(learn, <br/>                    every='epoch', monitor='accuracy', <br/>                    <strong class="nj iu"><em class="lw">name=&lt;callback_save_file&gt;</em></strong>)])</span></pre><p id="1ba5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">fast.ai 会告诉你“<em class="lw">已加载&lt;回调 _ 保存 _ 文件&gt; _ &lt;上一个 _ 纪元# &gt; </em>”，恢复训练。</p><p id="9ef1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以在此查看<em class="lw"> fit_one_cycle </em>方法支持的所有参数:</p><ul class=""><li id="db1b" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated"><a class="ae lu" href="https://docs.fast.ai/train.html#fit_one_cycle" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/train.html#fit_one_cycle</a></li></ul><h1 id="0a11" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">我们学到了什么？</h1><p id="fb92" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">与其他方法相比，作者分别使用纯<em class="lw"> Keras </em>和<em class="lw"> TensorFlow。Keras </em>为了解决这个问题，借助<em class="lw"> fast.ai </em>我们能够使用更少的代码解决同样的疟疾血液涂片分类问题，同时使用高级超参数优化策略，使我们能够更快地进行训练。同时，一组高级功能也允许我们以表格和图形的形式轻松检查结果。</p><p id="ebc8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">利用 fast.ai 提供的现成残差网络模型，在 ImageNet 上进行预训练，我们获得的精度结果比上面三篇文章中的两篇(包括发表在 PeerJ 上的科学论文)高出 1%，相当于上面文章的最佳表现。这表明 fast.ai 是更传统的 CNN 框架的一个非常有前途的替代方案，特别是如果手头的任务是一个“标准”的深度学习任务，如图像分类、对象检测或语义分割，可以通过微调现成的预训练网络模型来解决。</p><h1 id="58f1" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">想看看更有野心的例子吗？</h1><p id="dfdf" class="pw-post-body-paragraph ky kz it la b lb mr ju ld le ms jx lg lh nf lj lk ll ng ln lo lp nh lr ls lt im bi translated">用 fast.ai 看我们的皮肤癌检测教程:</p><ul class=""><li id="37f5" class="mp mq it la b lb lc le lf lh ns ll nt lp nu lt nv mx my mz bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/deep-learning-for-diagnosis-of-skin-images-with-fastai-792160ab5495"><strong class="la iu">towards data science</strong>:<em class="lw">利用 fastai 进行皮肤图像诊断的深度学习</em>——<em class="lw">学习从皮肤镜图像中识别皮肤癌和其他病症</em> </a>作者 Aldo von Wangenheim</li></ul><p id="e5b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本例中，我们将向您展示如何处理更大的数据集并调整最佳学习率。</p></div></div>    
</body>
</html>