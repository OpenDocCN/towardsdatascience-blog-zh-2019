<html>
<head>
<title>Machine Learning Techniques applied to Stock Price Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习技术在股票价格预测中的应用</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-learning-techniques-applied-to-stock-price-prediction-6c1994da8001?source=collection_archive---------0-----------------------#2019-01-28">https://towardsdatascience.com/machine-learning-techniques-applied-to-stock-price-prediction-6c1994da8001?source=collection_archive---------0-----------------------#2019-01-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/96557411c22f533272793c6930ba7842.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*qh1SN05owrtvrQ2bhkDdWg.jpeg"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Image generated using Neural Style Transfer.</figcaption></figure><p id="79ad" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">机器学习有很多应用，其中之一就是预测时间序列。可以说，预测最有趣(或者最有利可图)的时间序列之一是股票价格。</p><p id="9891" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最近我读了一篇将机器学习技术应用于股票价格预测的博文。这里可以看<a class="ae kz" href="https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/" rel="noopener ugc nofollow" target="_blank">。这是一篇写得很好的文章，探讨了各种技术。然而，我觉得这个问题可以用更严谨的学术方法来处理。例如，在文章中，方法“移动平均”、“自动 ARIMA”和“先知”的预测范围为<strong class="kd iu"> 1 年</strong>，而“线性回归”、“k-最近邻”和“长短期记忆(LSTM)”的预测范围为<strong class="kd iu"> 1 天</strong>。文章的结尾写道:“LSTM 轻而易举地超越了我们迄今为止看到的任何算法。”但是很明显，我们不是在这里比较苹果和苹果。</a></p><p id="f6e9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">所以，这是我对这个问题的看法。</p><h1 id="eed9" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">问题陈述</h1><p id="c30e" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">我们的目标是预测 Vanguard Total Stock Market ETF(VTI)的每日调整收盘价，使用前 N 天的数据(即预测范围=1)。我们将使用 VTI 从 2015 年 11 月 25 日到 2018 年 11 月 23 日的三年历史价格，可以从<a class="ae kz" href="https://finance.yahoo.com/quote/VTI/" rel="noopener ugc nofollow" target="_blank">雅虎财经</a>轻松下载。下载后，数据集如下所示:</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi md"><img src="../Images/f39d639dbcd90399df7fe689bbcc66fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qpTXdsdWGAkUE3Nyt42WCg.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Downloaded dataset for VTI.</figcaption></figure><p id="e212" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将把这个数据集分成 60%训练、20%验证和 20%测试。将使用训练集来训练模型，将使用验证集来调整模型超参数，并且最终将使用测试集来报告模型的性能。下图显示了调整后的收盘价，分为相应的训练集、验证集和测试集。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/47a702a25629a6beddf2b0aa4393e8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TG5OG81pOLWCOXK3A72NWg.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Split the dataset into 60% train, 20% validation, and 20% test.</figcaption></figure><p id="ea11" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">为了评估我们方法的有效性，我们将使用均方根误差(RMSE)和平均绝对百分比误差(MAPE)指标。对于这两个指标，值越低，预测越好。</p><h1 id="b0cb" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">最后一个值</h1><p id="cb25" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">在最后一个值方法中，我们将简单地将预测值设置为最后一个观察值。在我们的上下文中，这意味着我们将当前调整后的收盘价设置为前一天的调整后收盘价。这是最具成本效益的预测模型，通常用作比较更复杂模型的基准。这里没有要优化的超参数。</p><p id="02a6" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了使用最后一个值方法的预测。如果仔细观察，您会发现每天的预测值(红叉)只是前一天的值(绿叉)。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mm"><img src="../Images/c060493223187ad1be058650742e86ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxJBJC1o3-8nALJgbCCmCw.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predictions using the last value method.</figcaption></figure><h1 id="b560" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">移动平均数</h1><p id="3abc" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">在移动平均法中，预测值将是前 N 个值的平均值。在我们的上下文中，这意味着我们将当前调整后的收盘价设置为前 N 天调整后的收盘价的平均值。超参数 N 需要调整。</p><p id="72bc" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了不同 N 值的验证集上实际值和预测值之间的 RMSE。我们将使用 N=2，因为它给出了最低的 RMSE。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mn"><img src="../Images/52336376dffeb136b3c221a88a4c011f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7rpbtf9SJef4w3jU6MXByw.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">RMSE between actual and predicted values on the validation set, for various N.</figcaption></figure><p id="9f50" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了使用移动平均法的预测。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mo"><img src="../Images/a3e178a100e7453f39353935a1aecc58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGUo8infdJRj33EXvEgi2A.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predictions using the moving average method.</figcaption></figure><p id="537d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以查看 Jupyter 笔记本的移动平均法<a class="ae kz" href="https://github.com/NGYB/Stocks/blob/master/StockPricePrediction/StockPricePrediction_v3_mov_avg.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="b505" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">线性回归</h1><p id="bd20" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">线性回归是一种对因变量和一个或多个自变量之间的关系进行建模的线性方法。我们在这里使用线性回归的方法是，我们将对前 N 个值拟合一个线性回归模型，并使用该模型来预测当天的值。下图是一个 N=5 的例子。实际调整后的收盘价显示为深蓝色十字，我们希望预测第 6 天的价值(黄色方块)。我们将通过前 5 个实际值拟合一条线性回归线(浅蓝色线)，并用它来做第 6 天的预测(浅蓝色圆圈)。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mp"><img src="../Images/212c6e93390dd3185acc5320a47d4dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ler0pvLAL-gTSu567kjTjA.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predicting the next value using linear regression with N=5.</figcaption></figure><p id="13e9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我们用来训练模型和进行预测的代码。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="8ad3" class="mv lb it mr b gy mw mx l my mz">import numpy as np<br/><br/>from sklearn.linear_model import LinearRegression</span><span id="d38e" class="mv lb it mr b gy na mx l my mz">def get_preds_lin_reg(df, target_col, N, pred_min, offset):<br/>    """<br/>    Given a dataframe, get prediction at each timestep<br/>    Inputs<br/>        df         : dataframe with the values you want to predict     <br/>        target_col : name of the column you want to predict<br/>        N          : use previous N values to do prediction<br/>        pred_min   : all predictions should be &gt;= pred_min<br/>        offset     : for df we only do predictions for df[offset:]<br/>    Outputs<br/>        pred_list  : the predictions for target_col<br/>    """<br/>    # Create linear regression object<br/>    regr = LinearRegression(fit_intercept=True)</span><span id="94d4" class="mv lb it mr b gy na mx l my mz">    pred_list = []</span><span id="e41b" class="mv lb it mr b gy na mx l my mz">    for i in range(offset, len(df['adj_close'])):<br/>        X_train = np.array(range(len(df['adj_close'][i-N:i]))) <br/>        y_train = np.array(df['adj_close'][i-N:i]) <br/>        X_train = X_train.reshape(-1, 1)     <br/>        y_train = y_train.reshape(-1, 1)<br/>        regr.fit(X_train, y_train)            # Train the model<br/>        pred = regr.predict(N)<br/>    <br/>        pred_list.append(pred[0][0])  <br/>    <br/>    # If the values are &lt; pred_min, set it to be pred_min<br/>    pred_list = np.array(pred_list)<br/>    pred_list[pred_list &lt; pred_min] = pred_min<br/>        <br/>    return pred_list</span></pre><p id="9690" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了不同 N 值的验证集上实际值和预测值之间的 RMSE。我们将使用 N=5，因为它给出了最低的 RMSE。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mn"><img src="../Images/23599d1fa6ba07e71869073ebc70368e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xaf-iINyMZgFpRxTtKO17Q.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">RMSE between actual and predicted values on the validation set, for various N.</figcaption></figure><p id="9b01" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了使用线性回归方法的预测。可以观察到这种方法不能捕捉方向的变化(即下降趋势到上升趋势，反之亦然)非常好。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nb"><img src="../Images/0c44ffcdb7d4387218b42e4cfd02c855.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nq9qU2Vbm82pffTkSaybPg.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predictions using the linear regression method.</figcaption></figure><p id="693a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以查看 Jupyter 笔记本上的线性回归<a class="ae kz" href="https://github.com/NGYB/Stocks/blob/master/StockPricePrediction/StockPricePrediction_v2_lin_reg.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="7b8f" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">极端梯度增强(XGBoost)</h1><p id="bbcc" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">梯度推进是以迭代的方式将弱学习者转换为强学习者的过程。XGBoost 这个名字指的是推动提升树算法的计算资源极限的工程目标。自 2014 年推出以来，XGBoost 已被证明是一种非常强大的机器学习技术，通常是许多机器学习竞赛中的首选算法。</p><p id="49c4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将在训练集上训练 XGBoost 模型，使用验证集调整它的超参数，最后在测试集上应用 XGBoost 模型并报告结果。可以使用的明显特征是最近 N 天的调整收盘价，以及最近 N 天的成交量。除了这些功能，我们还可以做一些功能工程。我们将构建的附加功能有:</p><ul class=""><li id="ee4e" class="nc nd it kd b ke kf ki kj km ne kq nf ku ng ky nh ni nj nk bi translated">最近 N 天每天的最高价和最低价之差</li><li id="d804" class="nc nd it kd b ke nl ki nm km nn kq no ku np ky nh ni nj nk bi translated">最近 N 天每天的开盘和收盘之间的差异</li></ul><p id="13de" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在构建这个模型的过程中，我学到了一个有趣的经验，即特征缩放对于模型的正常工作非常重要。我的第一个模型根本没有实现任何缩放，验证集上的预测显示在下面的图中。这里发生的情况是，模型对 89 到 125 之间的调整后收盘价值进行训练，因此模型只能输出该范围内的预测。当模型试图预测验证集时，如果看到超出此范围的值，它就不能很好地进行归纳。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nq"><img src="../Images/5339296c601e513b9a6b3870c0fadfaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qd52wdIdgNCl3A2mzKfR_Q.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predictions are highly inaccurate if feature and target scaling are not done properly.</figcaption></figure><p id="6708" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我接下来尝试的是调整训练集，使其均值为 0，方差为 1，并对验证集应用相同的转换。但是很明显这并不奏效，因为这里我们使用了从训练集计算的平均值和方差来转换验证集。由于来自验证集的值比来自训练集的值大得多，因此缩放后，这些值仍然会更大。结果是预测看起来仍然如上，只是 y 轴上的值现在被缩放了。</p><p id="2bf5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">最后，我所做的是调整训练集，使其均值为 0，方差为 1，并以此来训练模型。随后，当我对验证集进行预测时，对于每个样本的每个特征组，我将调整它们，使均值为 0，方差为 1。例如，如果我们在 T 日进行预测，我将采用最近 N 天(T-N 日至 T-1 日)调整后的收盘价，并将其调整为均值为 0，方差为 1。对于量的特性也是如此，我将取最近 N 天的量，并将它们缩放到均值为 0，方差为 1。对我们上面构建的附加特性重复同样的操作。然后，我们使用这些缩放后的特征进行预测。预测值也将被缩放，我们使用它们相应的平均值和方差对它们进行逆变换。我发现这种扩展方式提供了最好的性能，正如我们将在下面看到的。</p><p id="c809" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我们用来训练模型和进行预测的代码。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="644f" class="mv lb it mr b gy mw mx l my mz">import math<br/>import numpy as np</span><span id="47db" class="mv lb it mr b gy na mx l my mz">from sklearn.metrics import mean_squared_error<br/>from xgboost import XGBRegressor</span><span id="0b1d" class="mv lb it mr b gy na mx l my mz">def get_mape(y_true, y_pred): <br/>    """<br/>    Compute mean absolute percentage error (MAPE)<br/>    """<br/>    y_true, y_pred = np.array(y_true), np.array(y_pred)<br/>    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</span><span id="881e" class="mv lb it mr b gy na mx l my mz">def train_pred_eval_model(X_train_scaled, \<br/>                          y_train_scaled, \<br/>                          X_test_scaled, \<br/>                          y_test, \<br/>                          col_mean, \<br/>                          col_std, \<br/>                          seed=100, \<br/>                          n_estimators=100, \<br/>                          max_depth=3, \<br/>                          learning_rate=0.1, \<br/>                          min_child_weight=1, \<br/>                          subsample=1, \<br/>                          colsample_bytree=1, \<br/>                          colsample_bylevel=1, \<br/>                          gamma=0):<br/>    '''<br/>    Train model, do prediction, scale back to original range and do <br/>    evaluation<br/>    Use XGBoost here.<br/>    Inputs<br/>        X_train_scaled     : features for training. Scaled to have <br/>                             mean 0 and variance 1<br/>        y_train_scaled     : target for training. Scaled to have <br/>                             mean 0 and variance 1<br/>        X_test_scaled      : features for test. Each sample is <br/>                             scaled to mean 0 and variance 1<br/>        y_test             : target for test. Actual values, not <br/>                             scaled<br/>        col_mean           : means used to scale each sample of <br/>                             X_test_scaled. Same length as <br/>                             X_test_scaled and y_test<br/>        col_std            : standard deviations used to scale each <br/>                             sample of X_test_scaled. Same length as <br/>                             X_test_scaled and y_test<br/>        seed               : model seed<br/>        n_estimators       : number of boosted trees to fit<br/>        max_depth          : maximum tree depth for base learners<br/>        learning_rate      : boosting learning rate (xgb’s “eta”)<br/>        min_child_weight   : minimum sum of instance weight(hessian) <br/>                             needed in a child<br/>        subsample          : subsample ratio of the training <br/>                             instance<br/>        colsample_bytree   : subsample ratio of columns when <br/>                             constructing each tree<br/>        colsample_bylevel  : subsample ratio of columns for each <br/>                             split, in each level<br/>        gamma              : minimum loss reduction required to make <br/>                             a further partition on a leaf node of <br/>                             the tree<br/>    Outputs<br/>        rmse               : root mean square error of y_test and <br/>                             est<br/>        mape               : mean absolute percentage error of <br/>                             y_test and est<br/>        est                : predicted values. Same length as y_test<br/>    '''</span><span id="8652" class="mv lb it mr b gy na mx l my mz">    model = XGBRegressor(seed=model_seed,<br/>                         n_estimators=n_estimators,<br/>                         max_depth=max_depth,<br/>                         learning_rate=learning_rate,<br/>                         min_child_weight=min_child_weight,<br/>                         subsample=subsample,<br/>                         colsample_bytree=colsample_bytree,<br/>                         colsample_bylevel=colsample_bylevel,<br/>                         gamma=gamma)<br/>    <br/>    # Train the model<br/>    model.fit(X_train_scaled, y_train_scaled)<br/>    <br/>    # Get predicted labels and scale back to original range<br/>    est_scaled = model.predict(X_test_scaled)<br/>    est = est_scaled * col_std + col_mean</span><span id="594b" class="mv lb it mr b gy na mx l my mz">    # Calculate RMSE<br/>    rmse = math.sqrt(mean_squared_error(y_test, est))<br/>    mape = get_mape(y_test, est)<br/>    <br/>    return rmse, mape, est</span></pre><p id="2e34" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了不同 N 值的验证集上实际值和预测值之间的 RMSE。我们将使用 N=3，因为它给出了最低的 RMSE。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5e6305346487986151ae55fc2bf1db99.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*MkdGA3L13xuJ_7EGLWbMiQ.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Tuning N using RMSE and MAPE.</figcaption></figure><p id="1295" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">优化前后的超参数和性能如下所示。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/e5cadfa985049f63db447e45481254f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*w5TnnpOx6WIjWl8GF7dtBQ.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Tuning XGBoost hyperparameters using RMSE and MAPE.</figcaption></figure><p id="8990" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了使用 XGBoost 方法的预测。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mo"><img src="../Images/5b83dfab746967f05e384b16a0ab058b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJ3Kajro5aUcyaOppWIGiQ.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predictions using the XGBoost method.</figcaption></figure><p id="e443" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以在这里查看 XGBoost <a class="ae kz" href="https://github.com/NGYB/Stocks/blob/master/StockPricePrediction/StockPricePrediction_v1c_xgboost.ipynb" rel="noopener ugc nofollow" target="_blank">的 Jupyter 笔记本。</a></p><h1 id="9a6f" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">长短期记忆(LSTM)</h1><p id="ba46" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">LSTM 是一种深度学习技术，被开发用来解决长序列中遇到的消失梯度问题。LSTM 有三个门:更新门、遗忘门和输出门。更新和忽略门确定存储单元的每个元件是否被更新。输出门决定了作为激活输出到下一层的信息量。</p><p id="1e1d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我们将要使用的 LSTM 建筑。我们将使用两层 LSTM 模块，中间有一个脱扣层以避免过度拟合。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nt"><img src="../Images/3e3a1591468b00a0e49607dd70bc7e10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vADTmGi9sNuzv0Vr9aOEw.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">LSTM network architecture.</figcaption></figure><p id="9ac9" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我们用来训练模型和进行预测的代码。</p><pre class="me mf mg mh gt mq mr ms mt aw mu bi"><span id="6159" class="mv lb it mr b gy mw mx l my mz">import math<br/>import numpy as np</span><span id="d1fb" class="mv lb it mr b gy na mx l my mz">from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, LSTM</span><span id="1f57" class="mv lb it mr b gy na mx l my mz">def train_pred_eval_model(x_train_scaled, \<br/>                          y_train_scaled, \<br/>                          x_test_scaled, \<br/>                          y_test, \<br/>                          mu_test_list, \<br/>                          std_test_list, \<br/>                          lstm_units=50, \<br/>                          dropout_prob=0.5, \<br/>                          optimizer='adam', \<br/>                          epochs=1, \<br/>                          batch_size=1):<br/>    '''<br/>    Train model, do prediction, scale back to original range and do <br/>    evaluation<br/>    Use LSTM here.<br/>    Returns rmse, mape and predicted values<br/>    Inputs<br/>        x_train_scaled  : e.g. x_train_scaled.shape=(451, 9, 1). <br/>                          Here we are using the past 9 values to  <br/>                          predict the next value<br/>        y_train_scaled  : e.g. y_train_scaled.shape=(451, 1)<br/>        x_test_scaled   : use this to do predictions <br/>        y_test          : actual value of the predictions<br/>        mu_test_list    : list of the means. Same length as <br/>                          x_test_scaled and y_test<br/>        std_test_list   : list of the std devs. Same length as <br/>                          x_test_scaled and y_test<br/>        lstm_units      : dimensionality of the output space<br/>        dropout_prob    : fraction of the units to drop for the <br/>                          linear transformation of the inputs<br/>        optimizer       : optimizer for model.compile()<br/>        epochs          : epochs for model.fit()<br/>        batch_size      : batch size for model.fit()<br/>    Outputs<br/>        rmse            : root mean square error<br/>        mape            : mean absolute percentage error<br/>        est             : predictions<br/>    '''<br/>    # Create the LSTM network<br/>    model = Sequential()<br/>    model.add(LSTM(units=lstm_units, <br/>                   return_sequences=True, <br/>                   input_shape=(x_train_scaled.shape[1],1)))</span><span id="edb2" class="mv lb it mr b gy na mx l my mz">    # Add dropput with a probability of 0.5<br/>    model.add(Dropout(dropout_prob)) </span><span id="66e1" class="mv lb it mr b gy na mx l my mz">    model.add(LSTM(units=lstm_units))</span><span id="8189" class="mv lb it mr b gy na mx l my mz">    # Add dropput with a probability of 0.5<br/>    model.add(Dropout(dropout_prob)) </span><span id="41ed" class="mv lb it mr b gy na mx l my mz">    model.add(Dense(1))</span><span id="0cdb" class="mv lb it mr b gy na mx l my mz">    # Compile and fit the LSTM network<br/>    model.compile(loss='mean_squared_error', optimizer=optimizer)<br/>    model.fit(x_train_scaled, y_train_scaled, epochs=epochs,   <br/>              batch_size=batch_size, verbose=0)<br/>    <br/>    # Do prediction<br/>    est_scaled = model.predict(x_test_scaled)<br/>    est = (est_scaled * np.array(std_test_list).reshape(-1,1)) + <br/>          np.array(mu_test_list).reshape(-1,1)<br/>    <br/>    # Calculate RMSE and MAPE<br/>    rmse = math.sqrt(mean_squared_error(y_test, est))<br/>    mape = get_mape(y_test, est)<br/>    <br/>    return rmse, mape, est</span></pre><p id="c2d1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">我们将使用与 XGBoost 中相同的方法来缩放数据集。LSTM 网络在调整验证集之前和之后的超参数和性能如下所示。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/0a3e6834c3ef3886a49af3fa78e6680e.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*AB1lE3lHQT3U-hhaKDtpiw.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Tuning LSTM hyperparameters using RMSE and MAPE.</figcaption></figure><p id="15c4" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下图显示了使用 LSTM 的预测。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mo"><img src="../Images/af4a80b12aaac926f815551ba849c81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OLoZ0DSW75Jwg_sP_Kz9FA.png"/></div></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Predictions using the LSTM method.</figcaption></figure><p id="ef8b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">你可以在这里查看 LSTM 的 Jupyter 笔记本。</p><h1 id="67ad" class="la lb it bd lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">调查结果和未来工作</h1><p id="d19b" class="pw-post-body-paragraph kb kc it kd b ke ly kg kh ki lz kk kl km ma ko kp kq mb ks kt ku mc kw kx ky im bi translated">下面，我们在同一个图中绘制了我们之前探索的所有方法的预测。很明显，使用线性回归的预测提供了最差的性能。除此之外，从视觉上很难判断哪种方法提供了最好的预测。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nv"><img src="../Images/cf12c50e87c4d6463100b561c42ee56e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PGDW9TYLQ9Anoqf3YBS34g.png"/></div></div></figure><p id="8c1a" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是我们探讨的各种方法在 RMSE 和 MAPE 的对比。我们看到最后一个值方法给出了最低的 RMSE 和 MAPE，其次是 XGBoost，然后是 LSTM。有趣的是，简单的最后值方法优于所有其他更复杂的方法，但这很可能是因为我们的预测范围只有 1。对于更长的预测范围，我相信其他方法可以比最后值方法更好地捕捉趋势和季节性。</p><figure class="me mf mg mh gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/65e8c18728dd2451668e5a5b8dfafd5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*-cMeTeIF3FmLxuX5oSzWEw.png"/></div><figcaption class="jx jy gj gh gi jz ka bd b be z dk">Comparison of various methods using RMSE and MAPE.</figcaption></figure><p id="5183" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">作为未来的工作，探索更长的预测范围将是有趣的，例如 1 个月或 1 年。探索其他预测技术也将是有趣的，如自回归综合移动平均(ARIMA)和三重指数平滑(即。Holt-Winters 方法)，并看看它们与上面的机器学习方法相比如何。</p><p id="6c18" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你有兴趣进一步探索，看看这篇<a class="ae kz" href="https://bit.ly/3rPQfJg" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><div class="nx ny gp gr nz oa"><a href="https://ngyibin.medium.com/subscribe" rel="noopener follow" target="_blank"><div class="ob ab fo"><div class="oc ab od cl cj oe"><h2 class="bd iu gy z fp of fr fs og fu fw is bi translated">中等</h2><div class="oh l"><h3 class="bd b gy z fp of fr fs og fu fw dk translated">编辑描述</h3></div><div class="oi l"><p class="bd b dl z fp of fr fs og fu fw dk translated">ngyibin.medium.com</p></div></div></div></a></div></div></div>    
</body>
</html>