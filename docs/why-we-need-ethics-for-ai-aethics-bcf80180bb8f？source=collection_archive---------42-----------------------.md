# 为什么我们需要人工智能伦理学

> 原文：<https://towardsdatascience.com/why-we-need-ethics-for-ai-aethics-bcf80180bb8f?source=collection_archive---------42----------------------->

## 我们需要特别考虑分类、机器学习和人工智能对决策过程的影响。

![](img/c341b10e24cce77a2664a763d41ead7e.png)

Photo by [timJ](https://unsplash.com/@the_roaming_platypus?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

2017 年 [Kate Crawford](https://medium.com/u/efbec1a070e4?source=post_page-----bcf80180bb8f--------------------------------) 出席 NIPS，*神经信息处理系统会议。*她令人信服地表明，基于现有数据训练的分析模型会表现出不受欢迎的行为，比如歧视行为。

作为例子，她提到了著名的 ProBublica 新闻文章。这篇文章研究了警方识别出[黑人更有可能犯罪的算法。在文章中，ProBublica 给出了分数不公平的例子。黑人女性 Brisha Borden 的累犯分数高于白人 Vernon Prater——后者已经是一名累犯，并将继续被判犯有新的罪行。](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)

分析模型中更多不良行为的例子是可用的；著名的“[谷歌大猩猩](https://www.theverge.com/2015/7/1/8880363/google-apologizes-photos-app-tags-two-black-people-gorillas)案是另一个例子，或者说[女性在患乳腺癌](https://www.trouw.nl/nieuws/vrouwen-die-borstkanker-hebben-gehad-kunnen-nu-eindelijk-een-hypotheek-krijgen~b0442313/?referer=https%3A%2F%2Fwww.google.com%2F)后被拒绝贷款。

凯特·克劳福德在她的演讲中深入探讨了偏见的概念。当我们说一个模型或算法有偏差时，我们指的是什么？她区分了分配性伤害和代表性伤害，并出色地展示了机器学习中偏见的细微差别——如果你没有看过这个演讲，我建议你去看看。

她在演讲结束时评论说，偏见可能是一般分类的副作用。分类是一种降低维度复杂度的技术，这些不期望的行为是分类的人工产物。我们目前正在进行人类历史上最大的分类实验。

但是，当我们说模型“弄错了”或模型表现出“不受欢迎的行为”时，我们实际上是什么意思呢如果某个亚群在数据中确实过多或过少会怎样。我们应该竭尽全力调整算法中的预测以适应某种平等吗？这不是一个技术问题，这是一个*伦理规范*问题*。*

简单地将所有模型故障归咎于基础数据或操作人员“正确”调整模型的失败是太容易了这个问题更深入——当我们谈论不受欢迎的行为时，我们指的是什么？通过算法进行的歧视又意味着什么？

为了回答这些问题，我们需要了解对错，即*伦理、*以及我们对该做什么有具体的概念，即*规范*。

例如，假设基础数据正确地确定了一些群体与较高的风险相关联。在荷兰的案例中，先前被诊断患有乳腺癌的妇女获得了更高的保费，或者被拒绝抵押贷款和贷款。我们直觉地认为这是错误的，即使基础数据(可能)是正确的，假设它们对贷款发行人来说是更高的风险。然而，指责人工智能是错误的——它只是从所有历史案例中学习。请注意，一旦识别出不良行为的“问题”，从技术上抑制某些模型行为就很简单了。

规范性问题更难:我们应该让贷款发放者承担向这些女性发放抵押贷款的风险吗？如果你的反应是‘是’，那么我同意你的观点，但是问问你自己这样想的后果是什么？谁来决定谁来承担责任？女人呢？贷款发行人？还是政府？人们总是被拒绝某些服务——毫无疑问，从伦理角度来看，一些原因是可疑的。

这里的‘歧视’是特定人群在特定时间划定的一个*伦理规范*边界。我们需要考虑的是以*伦理*的形式大规模采用人工智能的伦理和影响。

同样的问题也出现在企业中，数据科学家被要求创建一个模型来预测一些指标:销售、流失、加价。但是根据您的目标，许多模型和模型变体都是可能的。

躲在犯罪统计数字的背后比参与一场关于应该做什么的辩论要容易得多。分析模型简单地给出了'*现状*和'*未来*'状态，中间没有伦理修正。数据科学并没有告诉我们什么是好的，什么应该是好的，也没有告诉我们什么是对的，或者我们应该做什么，T21。

更糟糕的是，我们知道科学——以及科学方法——有严重的缺陷。仅仅从技术角度考虑建模中的所有问题是错误的。我们科学研究结构中的缺陷可能会导致错误发表的论文；也就是说，那些声称找到了问题答案的论文实际上是伪造的。

考虑一下研究中的偏见。这种偏见倾向于解决科学家认为存在真正积极关系的问题，这意味着对那些我们认为不存在这种关系的问题做的研究较少。

请原谅我:假设任意的统计能力为 80%。在一组 1000 个实验中，比如说，其中 100 个具有真正的正相关，我们应该正确地识别 100 个中的 80 个具有真正的正相关。

但是，我们也会将剩下的 900 个中的 20%识别为误报；总共 180 个。如果所有的阳性结果都被公布，我们的比率是 80(真阳性)比 180(假阳性)。发表的研究中只有 30%是“真实的”

当使用技术发现作为技术决策的基础时，所有这些都应该让科学家们处于优势地位。正如山姆·哈里斯喜欢说的，坏科学的解药不是更多的科学。我们不能躲在“更多科学”的背后，来找出如何处理人工智能中的伦理问题。

我们能克服这些技术问题的唯一方法是不要用我们创造它们时所用的那种思维方式。我们需要特别考虑与人工智能在公共领域的应用相关的伦理问题，在公共领域，我们应用人工智能来做出自动决策。我们需要有非技术思维来克服这个技术问题。

因此我们需要*伦理学*——人工智能的伦理学。