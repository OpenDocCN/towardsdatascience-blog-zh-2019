<html>
<head>
<title>How neuroscientists analyze data from transparent fish brains : part 2, clustering neural data.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经科学家如何分析来自透明鱼脑的数据:第 2 部分，聚类神经数据。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-neuroscientists-analyze-data-from-transparent-fish-brains-part-2-clustering-neural-data-c1b974a146e2?source=collection_archive---------22-----------------------#2019-02-15">https://towardsdatascience.com/how-neuroscientists-analyze-data-from-transparent-fish-brains-part-2-clustering-neural-data-c1b974a146e2?source=collection_archive---------22-----------------------#2019-02-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f3fb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PCA 和 K-means 将具有相似活动的神经元分组</h2></div><p id="1d6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第一篇文章中(你可以在这里阅读<a class="ae le" rel="noopener" target="_blank" href="/how-neuroscientists-analyze-data-from-transparent-fish-brains-part-1-pre-processing-63a09436ea93"/>)，我描述了神经科学家如何将来自透明鱼脑的大脑图像预处理成可利用的数据集:一个 2D 矩阵[神经元 x 时间],代表每个时间点每个神经元的活动。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/d6f5836b474ac4f967d178fe0c91b170.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*_Eeaxk1M0mFI5qsj_LZGKQ.png"/></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Two non-overlapping populations of amygdala neurons in the mouse brain. Picture from Mark Cembrowski, from the Cell Picture Show 2018 (have a look <a class="ae le" href="https://www.cell.com/pictureshow/neural-mapping" rel="noopener ugc nofollow" target="_blank">here </a>for many gorgeous photos !)</figcaption></figure><p id="9262" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们将看到这些数据如何被用来回答这个问题:<strong class="kk iu">我能把具有相似活动特征的神经元归为一组吗？</strong>对于神经科学家来说，找到具有相同活动的神经元群是一个极其重要的问题，因为同步神经元群可能是大脑区域电路中的“构建模块”的基础。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="f025" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一步是<strong class="kk iu">归一化</strong>每个神经元的活动，以减去背景活动。确定称为 F0 的<strong class="kk iu">基线</strong>活动值:例如，它可以是记录的前 3 分钟期间的平均活动。归一化的方法是去除 F0，然后除以 F0: (F — F0)/F0。因此，最终结果用δF/F0(简称“DFF”)表示:这个四维值代表活性相对于基线的增加或减少，用百分比表示。举个例子，</p><ul class=""><li id="c1a9" class="ly lz it kk b kl km ko kp kr ma kv mb kz mc ld md me mf mg bi translated">当 F=F0 时，则δF/F0 = 0(与基线相比没有增加)</li><li id="9f8c" class="ly lz it kk b kl mh ko mi kr mj kv mk kz ml ld md me mf mg bi translated">当 F=2*F0 时，则δF/F0 = 1(与基线相比增加 100%)。</li></ul><p id="07e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mm">(注:F 代表“荧光”，因为神经活动是通过神经元活跃时发出的荧光强度间接测量的。)</em></p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="028e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据标准化后，就可以进行聚类了。目标是将神经元分组在一起，因此——对于记录在 T 个时间戳中的 N 个神经元——在 T 维空间中将有 N 个数据点。为了可视化，让我们在 3 个时间戳期间在 3D 空间中绘制 6 个神经元的数据集:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mn"><img src="../Images/7531f912fa5d4ac20c4b7720b1fc695b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uljXP2ehxRb_eg73ROiIAA.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">An example data set for visualisation : 6 neurons recorded over 3 time frames = 6 data points in a 3D space.</figcaption></figure><p id="bbff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实上，神经元不仅仅在 3 个时间步长内被记录…实际数字可以是大约 50000 帧(<em class="mm">例如:20Hz 下 45 分钟的记录= &gt; 54000 个记录帧</em>)，所以这个图应该有 50000 个维度…</p><p id="985c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，我听到你说“<em class="mm"> 50000 个维度，太多了！我真的要把它们都留着吗？反正我肯定所有维度都没那么有用</em>”。你可能是对的:想象一下，有一段时间，大脑什么也没发生？(在健康的大脑中，这在生理上不太可能，但让我们假设一下)。或者在某个时候，所有的神经元突然变得超级活跃:这可能是可能的，但是<strong class="kk iu">如果我们的目标是将神经元分成几个集群，这将不会提供太多信息</strong>…所以，在集群之前，我们可以<strong class="kk iu">用 PCA </strong>(主成分分析)减少我们数据集的非常高维度。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="d2e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">降维在很多帖子里都有描述，这里我就长话短说。让我们看一个简单的 2D 数据集，如图 a):十几个人的年龄和身高。我们可以在 b)中看到，数据遵循一个<strong class="kk iu">总体“趋势”</strong>，用箭头表示。如果我们将数据点投影到那个箭头上(如 c 中所示)，我们仍然可以很好地了解数据的分布。这个“趋势”就是第一主成分(PC)。在图 d)中，我们可以选择仅在一维而不是二维中表示我们的数据。我们减少了维度的数量，但我们仍然可以捕获关于数据集的重要信息。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi ms"><img src="../Images/9088c172c1e7b2baf0d469d382c84933.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCJNcBKHSk-MRHt7eWW7YQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Little sketch of dimensionality reduction with PCA.</figcaption></figure><p id="3e17" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">PCA 的目标是<strong class="kk iu">基于数据集的“原始”成分(这里是年龄和身高)计算主成分</strong>。借助上面的示意图，您可能已经注意到箭头捕捉到了数据集方差最大化的方向<strong class="kk iu"/>，或者更简单地说，在原始空间(a 中)最远的 2 个点在 PCs 空间(d 中)仍然最远。因此，PCA 背后的基本原理是找到数据集中方差最大的维度。</p><p id="f7fe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mm">(备注:这是与 tSNE 完全不同的基本原理——</em><a class="ae le" href="https://lvdmaaten.github.io/tsne/" rel="noopener ugc nofollow" target="_blank"><em class="mm">t 分布随机邻居嵌入</em></a><em class="mm">——这也是一种在神经科学中广泛用于高维数据集可视化的降维技术。这将在下一篇文章中描述！</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mt"><img src="../Images/0069da68e033f3763eebcb5d01f4bc5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0kawboR9pBHo-X2YWVFNBg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Diagonalization of the covariance matrix</figcaption></figure><p id="2348" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一种方法是计算数据集的<strong class="kk iu">协方差矩阵</strong>并将其对角化。因此，你找到了一个新的空间，其中协方差矩阵是对角的:特征向量<strong class="kk iu">是主分量，特征值<strong class="kk iu">量化了每个主分量所解释的方差的百分比。对于 PCA，主成分是正交的。</strong></strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mu"><img src="../Images/26f0876799db560c9a4e1c128893ae4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AUFgEM8eJDoydhgK4gfeDQ.png"/></div></div></figure><p id="6e27" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于主成分是由原始成分组合而成的，它们不一定“意味着”与数据相关的任何东西。在我的例子中，沿着第一个 PC 的值将是年龄平方的平方根加上身高平方…这个组合允许我们只保留一维。</p><p id="14fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的神经数据集中，第一维是神经元的数量(几百个)。使用 PCA，我们可以将第二维从大约 50000(时间帧)减少到 10(主成分)。</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="570f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦对数据进行了归一化处理，降低了第二个维度，我们就可以使用一个简单的、众所周知的算法进行聚类:<a class="ae le" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu"> K-means </strong> </a>。同样，许多 TDS 帖子已经致力于这一算法(你可以在这里 观看动画<a class="ae le" rel="noopener" target="_blank" href="/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">)。因此，我将简单地评论一下毕晓普书中著名的方案。在下面的 2D 数据集中，我想分成 2 组:</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mv"><img src="../Images/2dbd8c8cc5614491732db8963d99d31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84K_O46dviyZ9Jciho8Cnw.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">From C. Bishop’s book “<a class="ae le" href="https://www.springer.com/fr/book/9780387310732" rel="noopener ugc nofollow" target="_blank">Pattern recognition and machine learning</a>”, page 426</figcaption></figure><ul class=""><li id="2b92" class="ly lz it kk b kl km ko kp kr ma kv mb kz mc ld md me mf mg bi translated"><strong class="kk iu">初始化</strong> (a):随机选择聚类的中心(红色和蓝色十字)。</li><li id="6471" class="ly lz it kk b kl mh ko mi kr mj kv mk kz ml ld md me mf mg bi translated"><strong class="kk iu">期望</strong> (b):每个数据点被分配到最近的聚类中心。</li><li id="67fd" class="ly lz it kk b kl mh ko mi kr mj kv mk kz ml ld md me mf mg bi translated"><strong class="kk iu">最大化</strong> (c):重新计算聚类中心(它们是彩色点的质心)。</li><li id="ca87" class="ly lz it kk b kl mh ko mi kr mj kv mk kz ml ld md me mf mg bi translated">(d-e-f):重复期望最大化步骤，直到收敛(即聚类中心不再移动)。</li></ul><p id="28aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(<em class="mm">备注:如这里</em> <a class="ae le" rel="noopener" target="_blank" href="/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68">所讨论的</a> <em class="mm">，聚类数 K 必须由用户选择。对于简单的数据集，这可以通过视觉检查来完成。但是对于高维数据集，选择正确的 K 可能很棘手。有几种方法可以帮助你找到一个合适的 K，如</em> <a class="ae le" rel="noopener" target="_blank" href="/want-clusters-how-many-will-you-have-8737f4ba9bf2">这篇<em class="mm"> </em> </a> <em class="mm">帖子中所述。</em>)</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><p id="6902" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 PCA 和 K-means，神经科学家可以将具有相似活动的神经元分组在一起。让我们看一个来自挪威<a class="ae le" href="https://yaksilab.com/" rel="noopener ugc nofollow" target="_blank"> Yaksi 实验室</a>的<a class="ae le" href="https://www.cell.com/current-biology/fulltext/S0960-9822(14)00016-5#secsectitle0025" rel="noopener ugc nofollow" target="_blank"><em class="mm"/></a>研究的例子。研究人员调查了斑马鱼的嗅觉以及大脑如何处理气味。他们对缰核<em class="mm"> </em>(一个被认为整合来自感觉区域的信息的大脑区域)进行了成像，并检测了神经元<a class="ae le" rel="noopener" target="_blank" href="/how-neuroscientists-analyze-data-from-transparent-fish-brains-part-1-pre-processing-63a09436ea93">，如我在第一篇帖子</a>中所述。</p><p id="b240" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实验的第一部分，研究人员没有做任何具体的事情，但一些神经元无论如何都是活跃的。这叫做“<strong class="kk iu">自发活动</strong>”。科学家们执行了 PCA 和 K-means(要求 K=6)，并根据神经元在时间上的活动将它们分成 6 个簇。我们首先可以看到的是，自发活动并不是随机的，具有相似活动的<strong class="kk iu">神经元在空间上是接近的</strong>！下图显示了按簇分组的神经元的活动概况，以及每个神经元在缰核中的位置:用相同颜色表示的神经元属于同一个簇。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mw"><img src="../Images/8f3472a14d4277357507816573bd3b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MEKsBDt3KDH6QfIrkbdfJg.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Spontaneous activity of habenular neurons. Left) neural activity in time (the traces are grouped in 6 clusters). Each row is a neuron, the color represents the DFF, introduced before. Right) spatial map showing the position of each neuron in the habenula (color-coded by cluster number). Figure found <a class="ae le" href="https://www.cell.com/current-biology/fulltext/S0960-9822(14)00016-5#figures" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><p id="9279" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在实验的第二部分，研究人员通过水向鱼传递一种气味(传递的时间由下图中的箭头表示)。他们再次执行了 PCA 和 K=6 的 K-means，并注意到气味传递期间的<strong class="kk iu">簇与自发活动期间的簇非常相似。</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="mo mp di mq bf mr"><div class="gh gi mx"><img src="../Images/7e374f14d8c7145b35a3de0fa0291704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*acSsdFz2nHl9rhaR8dIxTQ.png"/></div></div><figcaption class="ln lo gj gh gi lp lq bd b be z dk">Same legend than above, but during odor delivery. Figure found <a class="ae le" href="https://www.cell.com/current-biology/fulltext/S0960-9822(14)00016-5#figures" rel="noopener ugc nofollow" target="_blank">here</a>.</figcaption></figure><p id="88d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，自发同步的神经元在气味处理过程中也很可能是同步的！这就像一些神经元已经“预先连线”在一起，使它们更有可能以类似的方式对气味做出反应。</p><p id="5237" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大脑中的自发活动是如何组织的？这对信息处理意味着什么？这些问题非常好，激励了许多神经科学家！</p></div></div>    
</body>
</html>