<html>
<head>
<title>Embeddings-free Deep Learning NLP model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无嵌入深度学习 NLP 模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/embeddings-free-deep-learning-nlp-model-ce067c7a7c93?source=collection_archive---------20-----------------------#2019-02-12">https://towardsdatascience.com/embeddings-free-deep-learning-nlp-model-ce067c7a7c93?source=collection_archive---------20-----------------------#2019-02-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2d21" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">什么是单词嵌入</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/41e0ca662690863b2f0553fc7ed52444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Usjaiw5J5eqnI4IS"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="45b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">单词嵌入</a>(例如 word2vec，GloVe)在几年前被引入，并从根本上改变了 NLP 任务。有了嵌入，我们不需要在大多数 NLP 任务中导致非常高维特征的一键编码。我们可以用 300 个维度来代表超过 100 万个单词。</p><p id="7df5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不同种类的嵌入，如<a class="ae ky" rel="noopener" target="_blank" href="/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10">字符嵌入</a>，句子嵌入(如<a class="ae ky" rel="noopener" target="_blank" href="/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c">跳读</a>和<a class="ae ky" rel="noopener" target="_blank" href="/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8">推断</a>)，上下文嵌入(如<a class="ae ky" rel="noopener" target="_blank" href="/elmo-helps-to-further-improve-your-word-embeddings-c6ed2c9df95f"> ELMo </a>和<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb"> BERT </a>)是近年来发展起来的。我们一直都需要嵌入层吗？有时这可能不可行，因此 Ravi S .和 Kozareva Z .引入了无嵌入深度学习模型。</p><h1 id="c718" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">设备上</h1><p id="749c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们可以在云中或内部轻松分配 1 GB 内存和 16 个 CPU 来部署 sexy 模型。我们不需要牺牲模型准确性来减少模型占用空间，也不需要在大多数企业级基础架构中产生大量的大型模型。</p><p id="daed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时，我们别无选择，只能将模型部署到设备，而不是利用云基础架构。原因可以是</p><ul class=""><li id="7a0d" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Sensitive Data</code>:数据可能无法从设备发送到云端</li><li id="b073" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Network</code>:高速网络可能无法覆盖。</li></ul><p id="aba1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，如果我们将模型部署到智能 Swatch 或物联网设备等设备，则需要一个非常小的模型。没有人想在你的 Android Wear 操作系统中加载 1 GB 的型号。将模型部署到设备时面临的挑战:</p><ul class=""><li id="88d3" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Small memory footprint</code></li><li id="2daa" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Limited storage</code></li><li id="39a6" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Low computational capacity</code></li></ul><h1 id="8825" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">自治神经网络</h1><p id="682c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">因为目标是将模型部署到小型设备。它不能是高资源需求的。因此，SGNN 的目标是:</p><ul class=""><li id="01ac" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Tiny memory footprint</code>:加载预训练嵌入时没有初始化。</li><li id="29bb" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated"><code class="fe nb nc nd ne b">On-the-fry</code>:将输入文本实时转换为低维特征。</li></ul><h2 id="30b7" class="nk lw it bd lx nl nm dn mb nn no dp mf li np nq mh lm nr ns mj lq nt nu ml nv bi translated">投影神经网络</h2><p id="20f5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Ravi S .和 Kozareva Z .利用投影神经网络模型架构来减少内存和计算消耗，而不是使用具有高占用空间的原始神经网络。</p><p id="0607" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其思想是在训练阶段训练两个神经网络(即训练器网络和投影网络)。全网优化教练机网络损耗、投影网络损耗和投影损耗。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4b0eb5b519eb52736735dd4136ffdc2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*fMK83_b2jkWCXW7bh39v_g.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Projection Neural Network Architecture (Ravi S. 2017)</figcaption></figure><h2 id="4f12" class="nk lw it bd lx nl nm dn mb nn no dp mf li np nq mh lm nr ns mj lq nt nu ml nv bi translated">模型架构</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/f5cc5fe82fd32765b89062009ebabc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*BZKAy2sbfk0QiChSdS93Hw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">SGNN Model Architecture (Ravi S. and Kozareva Z., 2018)</figcaption></figure><ul class=""><li id="959c" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><code class="fe nb nc nd ne b">On-the-fry Computation</code>:将(函数 F 和 P)文本实时转换为中间结果和项目层，无需查找预定义的矢量。</li><li id="5c79" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Hash Function Projection</code>:通过修改版本的<a class="ae ky" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" rel="noopener ugc nofollow" target="_blank">位置敏感散列</a> (LSH)将高维特征减少到低维特征，这允许将相似的输入投射到相同的桶。</li><li id="b22f" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated"><code class="fe nb nc nd ne b">Model Optimization</code>:在投影层使用二进制特征(0 或 1)实现非常低的内存占用。</li></ul><h1 id="c84a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实验</h1><p id="738e" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Ravi S .和 Kozareva Z .通过<code class="fe nb nc nd ne b">Switchboard Dialog Act Corpus</code> (SwDA)和<code class="fe nb nc nd ne b">ICSI Meeting Recorder Dialog Act Corpus</code> (MRDA)对 SGNN 进行了评估，结果非常好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/8234c284cb0bbfa3c6c3d8384557d5fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*WaFhM-XRt_sjmbfh-BG4QQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">SwDA Data Result (Ravi S. and Kozareva Z., 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b0a55acc381d45be28b73aace41fa673.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*mz9yIjgOFN3YnMW9zSa5vQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">MRDA Data Result (Ravi S. and Kozareva Z., 2018)</figcaption></figure><h1 id="3214" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">拿走</h1><ul class=""><li id="d00c" class="ms mt it lb b lc mn lf mo li oa lm ob lq oc lu mx my mz na bi translated">越来越多复杂的模型架构被发布，以在不同的学科中实现最先进的结果。然而，它可能不适合小型设备，如物联网设备或移动设备，因为这些设备中资源有限。</li><li id="ca4a" class="ms mt it lb b lc nf lf ng li nh lm ni lq nj lu mx my mz na bi translated">当交付一个令人惊叹的模型时，准确性并不是唯一的关注点。在某些情况下，必须考虑速度和模型复杂性。我们可能需要牺牲精确性来得到一个轻量级的模型。</li></ul><h1 id="488e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="fd76" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="250b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">拉维..2017.<a class="ae ky" href="https://arxiv.org/pdf/1708.00630.pdf" rel="noopener ugc nofollow" target="_blank"> ProjectionNet:使用神经投射学习高效的设备上深度网络</a>。</p><p id="13a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拉维 s 和科扎列娃 z，2018。<a class="ae ky" href="https://aclweb.org/anthology/D18-1105" rel="noopener ugc nofollow" target="_blank">用于设备上短文本分类的自治神经网络</a>。</p></div></div>    
</body>
</html>