# å¦‚ä½•é€šè¿‡æ·±åº¦å­¦ä¹ å’ŒåµŒå…¥å±‚è·å¾—è¡¨æ ¼æ•°æ®çš„æœ€æ–°ç»“æœ

> åŸæ–‡ï¼š<https://towardsdatascience.com/how-to-gain-state-of-the-art-result-on-tabular-data-with-deep-learning-and-embedding-layers-d1eb6b83c52c?source=collection_archive---------8----------------------->

## å¡æ ¼å°”è“çš®ä¹¦æ¨åœŸæœºç«èµ›çš„å¦ä¸€ç§æ–¹æ³•

![](img/e7b230bb97e49c0703c19e52c9400492.png)

Embeddings can be use other than word representations

# åŠ¨æœº

T åŸºäº ree çš„æ¨¡å‹ï¼Œå¦‚ Random Forest å’Œ XGBoostï¼Œåœ¨è§£å†³è¡¨æ ¼(ç»“æ„åŒ–)æ•°æ®é—®é¢˜ä¸­éå¸¸æµè¡Œï¼Œå¹¶åœ¨æœ€è¿‘çš„ Kaggle ç«èµ›ä¸­è·å¾—äº†å¾ˆå¤šå…³æ³¨ã€‚è¿™æ˜¯æœ‰å…¶å……åˆ†ç†ç”±çš„ã€‚ç„¶è€Œï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘æƒ³ä»‹ç»ä¸€ç§ä¸åŒäº fast.ai çš„**è¡¨æ ¼**æ¨¡å—çš„æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨äº†:

> **æ·±åº¦å­¦ä¹ å’ŒåµŒå…¥å±‚ã€‚**

è¿™æœ‰ç‚¹è¿èƒŒè¡Œä¸šå…±è¯†ï¼Œå³æ·±åº¦å­¦ä¹ æ›´å¤šåœ°ç”¨äºå›¾åƒã€éŸ³é¢‘æˆ– NLP ç­‰éç»“æ„åŒ–æ•°æ®ï¼Œé€šå¸¸ä¸é€‚åˆå¤„ç†è¡¨æ ¼æ•°æ®ã€‚ç„¶è€Œï¼Œåˆ†ç±»æ•°æ®çš„åµŒå…¥å±‚çš„å¼•å…¥æ”¹å˜äº†è¿™ç§è§‚ç‚¹ï¼Œæˆ‘ä»¬å°†å°è¯•åœ¨[è“çš®ä¹¦æ¨åœŸæœºç«èµ›](https://www.kaggle.com/c/bluebook-for-bulldozers/overview)ä¸Šä½¿ç”¨ [fast.ai](http://fast.ai) çš„è¡¨æ ¼æ¨¡å—ï¼Œå¹¶çœ‹çœ‹è¿™ç§æ–¹æ³•èƒ½èµ°å¤šè¿œã€‚

ä½ å¯ä»¥æ‰¾åˆ° Kaggle ç¬”è®°æœ¬ğŸ“” [*æ­¤å¤„*](https://www.kaggle.com/lymenlee/blue-book-bulldozer-fast-ai-deep-learning) *ã€‚*

# åŠ è½½æ•°æ®

![](img/fbb45b05f67f22344a3fa065f8eab646.png)

é¦–å…ˆï¼Œè®©æˆ‘ä»¬å¯¼å…¥å¿…è¦çš„æ¨¡å—ã€‚è¿™é‡Œæœ€æ ¸å¿ƒçš„ä¸€ä¸ªæ˜¯`**fastai.tabular**`:

```
from fastai import *
from fastai.tabular import *
```

ç„¶åæˆ‘ä»¬å°†æ•°æ®è¯»å…¥ç†ŠçŒ«æ•°æ®å¸§ã€‚æ‚¨å¯ä»¥åœ¨æœ¬æ–‡é¡¶éƒ¨çš„ Kaggle ç¬”è®°æœ¬é“¾æ¥ä¸­æ‰¾åˆ°å…·ä½“çš„ä»£ç ï¼Œä½†åœ¨è¿™é‡Œï¼Œæˆ‘å°†åªå±•ç¤ºå¿…è¦çš„ä»£ç ç‰‡æ®µï¼Œä»¥å°½å¯èƒ½ä¿æŒç®€æ´ã€‚æˆ‘ä»¬å°† CSV æ–‡ä»¶è¯»å…¥`train_df`ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬ä¸»è¦å·¥ä½œçš„æ•°æ®å¸§ã€‚æˆ‘ä»¬è¿˜å°†åœ¨`test_df`ä¸­è¯»åˆ°æµ‹è¯•é›†ã€‚

è®©æˆ‘ä»¬ç®€å•çœ‹ä¸€ä¸‹æˆ‘ä»¬æ­£åœ¨å¤„ç†çš„æ•°æ®:

```
len(train_df), len(test_df)
(401125, 12457)
```

![](img/f27e770da4e99719e010ec37025c0a97.png)

# å¯¹è®­ç»ƒé›†æ’åº

è¿™æ˜¯ä¸ºäº†åˆ›å»ºä¸€ä¸ªå¥½çš„éªŒè¯é›†ã€‚ä¸€ä¸ªå¥½çš„éªŒè¯é›†å¯¹äºä¸€ä¸ªæˆåŠŸçš„æ¨¡å‹çš„é‡è¦æ€§æ€ä¹ˆå¼ºè°ƒéƒ½ä¸ä¸ºè¿‡ã€‚å› ä¸ºæˆ‘ä»¬è¦é¢„æµ‹æœªæ¥çš„é”€å”®ä»·æ ¼æ•°æ®ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªéªŒè¯é›†ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½åœ¨è®­ç»ƒé›†çš„â€œæœªæ¥â€æ”¶é›†ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦é¦–å…ˆå¯¹è®­ç»ƒé›†è¿›è¡Œæ’åºï¼Œç„¶åå°†â€œæœªæ¥â€éƒ¨åˆ†æ‹†åˆ†ä¸ºéªŒè¯é›†ã€‚

```
train_df = train_df.sort_values(by='saledate', ascending=False)
train_df = train_df.reset_index(drop=True)
```

# æ•°æ®é¢„å¤„ç†

æ¯”èµ›çš„è¯„ä¼°æ–¹æ³•ä½¿ç”¨ RMSLE(å‡æ–¹æ ¹å¯¹æ•°è¯¯å·®)ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬å–é¢„æµ‹çš„å¯¹æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨è€ RMSE ä½œä¸ºæˆ‘ä»¬çš„æŸå¤±å‡½æ•°ã€‚åªæ˜¯è¿™æ ·æ›´å®¹æ˜“ã€‚

```
train_df.SalePrice = np.log(train_df.SalePrice)
```

å¯¹äº**ç‰¹å¾å·¥ç¨‹**ï¼Œç”±äºæˆ‘ä»¬å°†ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¥è§£å†³é—®é¢˜ï¼Œå¹¶ä¸”å®ƒéå¸¸æ“…é•¿ç‰¹å¾æå–ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†åªåœ¨`saledate`è¿›è¡Œã€‚è¿™æ˜¯ä½¿ç”¨æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå®ƒéœ€è¦æ›´å°‘çš„åŠŸèƒ½å·¥ç¨‹å’Œæ›´å°‘çš„é¢†åŸŸçŸ¥è¯†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ fast.ai çš„`add_datepart`å‡½æ•°æ¥æ·»åŠ æ›´å¤šä¸é”€å”®æ—¥æœŸç›¸å…³çš„åŠŸèƒ½ã€‚

```
# The only feature engineering we do is add some meta-data from the sale date column, using 'add_datepart' function in fast.ai
add_datepart(train_df, "saledate", drop=False)
add_datepart(test_df, "saledate", drop=False)
```

`add_datepart`æ‰€åšçš„æ˜¯ï¼Œå®ƒæ¥å—`saledate`åˆ—ï¼Œå¹¶æ·»åŠ äº†ä¸€å †å…¶ä»–åˆ—ï¼Œå¦‚`day of week`ã€`day of month`ï¼Œæ— è®ºæ˜¯æœˆã€å­£ã€å¹´çš„å¼€å§‹è¿˜æ˜¯ç»“æŸï¼Œç­‰ç­‰ã€‚è¿™äº›æ·»åŠ çš„åŠŸèƒ½å°†æä¾›å¯¹æ—¥æœŸçš„æ›´å¤šæ´å¯Ÿï¼Œå¹¶ä¸ç”¨æˆ·è´­ä¹°è¡Œä¸ºç›¸å…³ã€‚ä¾‹å¦‚ï¼Œåœ¨å¹´åº•ï¼Œå…¬å¸é€šå¸¸ä¼šå¼€å±•ä¿ƒé”€æ´»åŠ¨ï¼Œä»·æ ¼é€šå¸¸ä¼šé™ä½ã€‚

è®©æˆ‘ä»¬æ£€æŸ¥æ˜¯å¦æ‰€æœ‰è¿™äº›ä¸æ—¥æœŸç›¸å…³çš„ç‰¹å¾éƒ½è¢«æ·»åŠ åˆ°æˆ‘ä»¬çš„æ•°æ®æ¡†æ¶ä¸­:

```
# check and see whether all date related meta data is added.
def display_all(df):
    with pd.option_context("display.max_rows", 1000, "display.max_columns", 1000): 
        display(df)

display_all(train_df.tail(10).T)
```

![](img/bd72a53e99ed90d42ba3e27982352b89.png)

ä»–ä»¬ç¡®å®è¢«åŠ äº†è¿›å»ã€‚å¾ˆå¥½ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦åšä¸€äº›æ•°æ®é¢„å¤„ç†ï¼Œå› ä¸ºè¿™ä¸ªæ•°æ®å¸§æœ‰ç›¸å½“å¤šçš„ç¼ºå¤±æ•°æ®ï¼Œæˆ‘ä»¬è¿˜æƒ³å¯¹åˆ—è¿›è¡Œåˆ†ç±»å’Œè§„èŒƒåŒ–ã€‚æœ‰äº† fast.ai åº“ï¼Œè¿™å°±ç›¸å½“ç®€å•äº†ã€‚æˆ‘ä»¬åªéœ€åœ¨ Python åˆ—è¡¨ä¸­æŒ‡å®šæˆ‘ä»¬æƒ³è¦çš„é¢„å¤„ç†æ–¹æ³•ï¼Œå°±åƒè¿™æ ·:

```
# Defining pre-processing we want for our fast.ai DataBunch
procs=[FillMissing, Categorify, Normalize]
```

è¿™ä¸ªå˜é‡`procs`ç¨åå°†è¢«ç”¨äºåˆ›å»ºç”¨äºè®­ç»ƒçš„ fast.ai æ•°æ®æŸã€‚

# æ„å»ºæ¨¡å‹

L è®©æˆ‘ä»¬çœ‹çœ‹æ¯ä¸€åˆ—çš„æ•°æ®ç±»å‹ï¼Œä»¥å†³å®šå“ªäº›æ˜¯åˆ†ç±»çš„ï¼Œå“ªäº›æ˜¯è¿ç»­çš„:

```
train_df.dtypes
g = train_df.columns.to_series().groupby(train_df.dtypes).groups
g
```

ç»“æœå¦‚ä¸‹:

![](img/9d389c96ca9fc738851406e66900d86d.png)

ç„¶åï¼Œæˆ‘ä»¬å°†æ‰€æœ‰åˆ†ç±»åˆ—æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨`cat_vars`ä¸­ï¼Œæ‰€æœ‰è¿ç»­åˆ—æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨`cont_vars`ä¸­ã€‚è¿™ä¸¤ä¸ªå˜é‡ä¹Ÿå°†ç”¨äºæ„é€  fast.ai DataBunchã€‚

```
# prepare categorical and continous data columns for building Tabular DataBunch.
cat_vars = ['SalesID', 'YearMade', 'MachineID', 'ModelID', 'datasource', 'auctioneerID', 'UsageBand', 'fiModelDesc', 'fiBaseModel', 'fiSecondaryDesc', 'fiModelSeries', 'fiModelDescriptor', 'ProductSize', 
            'fiProductClassDesc', 'state', 'ProductGroup', 'ProductGroupDesc', 'Drive_System', 'Enclosure', 'Forks', 'Pad_Type', 'Ride_Control', 'Stick', 'Transmission', 'Turbocharged', 'Blade_Extension', 
            'Blade_Width', 'Enclosure_Type', 'Engine_Horsepower', 'Hydraulics', 'Pushblock', 'Ripper', 'Scarifier', 'Tip_Control', 'Tire_Size', 'Coupler', 'Coupler_System', 'Grouser_Tracks', 'Hydraulics_Flow', 
            'Track_Type', 'Undercarriage_Pad_Width', 'Stick_Length', 'Thumb', 'Pattern_Changer', 'Grouser_Type', 'Backhoe_Mounting', 'Blade_Type', 'Travel_Controls', 'Differential_Type', 'Steering_Controls', 
            'saleYear', 'saleMonth', 'saleWeek', 'saleDay', 'saleDayofweek', 'saleDayofyear', 'saleIs_month_end', 'saleIs_month_start', 'saleIs_quarter_end', 'saleIs_quarter_start', 'saleIs_year_end', 
            'saleIs_year_start'
           ]cont_vars = ['MachineHoursCurrentMeter', 'saleElapsed']
```

æˆ‘ä»¬å°†åˆ›å»ºå¦ä¸€ä¸ªæ•°æ®å¸§`df`æ¥é¦ˆå…¥æ•°æ®é›†ä¸­ã€‚æˆ‘ä»¬è¿˜å°†å› å˜é‡æŒ‡å®šä¸º`dep_var`ã€‚

```
# rearrange training set before feed into the databunch
dep_var = 'SalePrice'
df = train_df[cat_vars + cont_vars + [dep_var,'saledate']].copy()
```

ç°åœ¨æ˜¯æ—¶å€™åˆ›å»ºæˆ‘ä»¬çš„éªŒè¯é›†äº†ã€‚æˆ‘ä»¬é€šè¿‡ä»è®­ç»ƒé›†ä¸­åˆ‡æ‰ä¸€å—æœ€è¿‘çš„æ¡ç›®æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚è¡—åŒºåº”è¯¥æœ‰å¤šå¤§ï¼Ÿå—¯ï¼Œå’Œæµ‹è¯•é›†ä¸€æ ·å¤§ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ä»£ç :

```
# Look at the time period of test set, make sure it's more recent
test_df['saledate'].min(), test_df['saledate'].max()# Calculate where we should cut the validation set. We pick the most recent 'n' records in training set where n is the number of entries in test set. 
cut = train_df['saledate'][(train_df['saledate'] == train_df['saledate'][len(test_df)])].index.max()
cut12621# specify the valid_idx variable as the cut out range.
valid_idx = range(cut)
```

æˆ‘ä»¬é¦–å…ˆæŸ¥çœ‹æµ‹è¯•é›†çš„æ—¶é—´æ®µï¼Œå¹¶ç¡®ä¿å®ƒæ¯”æˆ‘ä»¬æ‰€æœ‰çš„è®­ç»ƒé›†æ›´è¿‘ã€‚ç„¶åæˆ‘ä»¬è®¡ç®—éœ€è¦å‰ªä¸‹å¤šå°‘è®°å½•ã€‚

æœ€åï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ fast.ai çš„ datablock API æ„å»ºæˆ‘ä»¬çš„ DataBunch è¿›è¡Œè®­ç»ƒ:

```
# Use fast.ai datablock api to put our training data into the DataBunch, getting ready for training
data = (TabularList.from_df(df, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)
                   .split_by_idx(valid_idx)
                   .label_from_df(cols=dep_var, label_cls=FloatList)
                   .databunch())
```

# æ„å»ºæ¨¡å‹

æˆ‘ä»¬å°†ä»åˆšåˆšåˆ›å»ºçš„æ•°æ®é›†ä¸­å¯åŠ¨ä¸€ä¸ª fast.ai `tabular.learner`ã€‚æˆ‘ä»¬å¸Œæœ›å°†é¢„æµ‹çš„ä»·æ ¼èŒƒå›´é™åˆ¶åœ¨å†å²é”€å”®ä»·æ ¼èŒƒå›´å†…ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è®¡ç®—`y_range`ã€‚è¯·æ³¨æ„ï¼Œæˆ‘ä»¬å°†`SalePrice`çš„æœ€å¤§å€¼ä¹˜ä»¥ 1.2ï¼Œå› æ­¤å½“æˆ‘ä»¬åº”ç”¨ sigmoid æ—¶ï¼Œä¸Šé™ä¹Ÿå°†è¢«è¦†ç›–ã€‚è¿™æ˜¯ä»æ¨¡å‹ä¸­æŒ¤å‡ºæ›´å¤šæ€§èƒ½çš„ä¸€ä¸ªå°æŠ€å·§ã€‚

```
max_y = np.max(train_df['SalePrice'])*1.2
y_range = torch.tensor([0, max_y], device=defaults.device)
y_rangetensor([ 0.0000, 14.2363], device='cuda:0')
```

ç°åœ¨æˆ‘ä»¬å¯ä»¥åˆ›å»ºæˆ‘ä»¬çš„å­¦ä¹ è€…:

```
# Create our tabular learner. The dense layer is 1000 and 500 two layer NN. We used dropout, hai 
learn = tabular_learner(data, layers=[1000,500], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=rmse)
```

å…³äº fast.ai `tabular_learner`æœ€é‡è¦çš„ä¸€ç‚¹æ˜¯ä¸ºåˆ†ç±»æ•°æ®ä½¿ç”¨åµŒå…¥å±‚ã€‚è¿™æ˜¯ä½¿æ·±åº¦å­¦ä¹ åœ¨å¤„ç†è¡¨æ ¼æ•°æ®æ–¹é¢å…·æœ‰ç«äº‰åŠ›çš„'**ç§˜æ–¹**'ã€‚ç”±äºæ¯ä¸ªåˆ†ç±»å˜é‡éƒ½æœ‰ä¸€ä¸ªåµŒå…¥å±‚ï¼Œæˆ‘ä»¬ä¸ºåˆ†ç±»å˜é‡å¼•å…¥äº†è‰¯å¥½çš„äº¤äº’ï¼Œå¹¶åˆ©ç”¨äº†æ·±åº¦å­¦ä¹ çš„æœ€å¤§ä¼˜åŠ¿:è‡ªåŠ¨ç‰¹å¾æå–ã€‚ä¸ºäº†æ›´å¥½çš„æ­£åˆ™åŒ–ï¼Œæˆ‘ä»¬è¿˜å¯¹å¯†é›†å±‚å’ŒåµŒå…¥å±‚ä½¿ç”¨äº† Drop Outã€‚å­¦ä¹ è€…çš„æŒ‡æ ‡æ˜¯ RMSEï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è®°å½•äº†é”€å”®ä»·æ ¼ã€‚æˆ‘ä»¬æ¥çœ‹çœ‹æ¨¡å‹ã€‚

```
TabularModel(
  (embeds): ModuleList(
    (0): Embedding(388505, 600)
    (1): Embedding(72, 18)
    (2): Embedding(331868, 600)
    (3): Embedding(5155, 192)
   ...
    (60): Embedding(3, 3)
    (61): Embedding(2, 2)
    (62): Embedding(3, 3)
  )
  (emb_drop): Dropout(p=0.04, inplace=False)
  (bn_cont): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Linear(in_features=2102, out_features=1000, bias=True)
    (1): ReLU(inplace=True)
    (2): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Dropout(p=0.001, inplace=False)
    (4): Linear(in_features=1000, out_features=500, bias=True)
    (5): ReLU(inplace=True)
    (6): BatchNorm1d(500, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): Dropout(p=0.01, inplace=False)
    (8): Linear(in_features=500, out_features=1, bias=True)
  )
)
```

ä»ä¸Šé¢å¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬æœ‰åˆ†ç±»åˆ—çš„åµŒå…¥å±‚ï¼Œç„¶åæ˜¯åˆ é™¤å±‚ã€‚å¯¹äºè¿ç»­åˆ—ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ‰¹å¤„ç†èŒƒæ•°å±‚ï¼Œç„¶åæˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›åˆ—(åˆ†ç±»åµŒå…¥+è¿ç»­å˜é‡)è¿æ¥åœ¨ä¸€èµ·ï¼Œå¹¶å°†å…¶æ”¾å…¥ä¸¤ä¸ªå®Œå…¨è¿æ¥çš„å±‚ä¸­ï¼Œè¿™ä¸¤ä¸ªå±‚åˆ†åˆ«æœ‰ 1000 å’Œ 500 ä¸ªèŠ‚ç‚¹ï¼Œä¸­é—´æœ‰ Reluã€batch norm å’Œ Dropoutã€‚å¾ˆæ ‡å‡†çš„ä¸œè¥¿ã€‚

ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨¡å‹ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨ fast.ai çš„å­¦ä¹ ç‡æŸ¥æ‰¾å™¨æ¥æŸ¥æ‰¾ä¸€ä¸ªå¥½çš„å­¦ä¹ ç‡:

```
learn.lr_find()
learn.recorder.plot()
```

![](img/ef55d784d921d1c0f800cb89d30c4493.png)

æˆ‘ä»¬å°†æŒ‘é€‰å­¦ä¹ ç‡æ›²çº¿æ–œç‡æœ€å¤§çš„ä¸€ç«¯çš„å­¦ä¹ ç‡:`le-02`

è®©æˆ‘ä»¬ä½¿ç”¨ fast.ai çš„å•å‘¨æœŸè®­ç»ƒæ–¹æ³•è¿›è¡Œä¸€äº›è®­ç»ƒã€‚æ³¨æ„ï¼Œæˆ‘ä»¬ä¸ºæ­£åˆ™åŒ–æ·»åŠ äº†ä¸€äº›æƒé‡è¡°å‡(0.2)ã€‚

```
learn.fit_one_cycle(2, 1e-2, wd=0.2)
```

![](img/a5c80cf11c352746d44e47ef31f0c30b.png)

æˆ‘ä»¬å¯ä»¥ç”¨è¾ƒå°çš„å­¦ä¹ ç‡è®­ç»ƒæ›´å¤šçš„å‘¨æœŸ:

```
learn.fit_one_cycle(5, 3e-4, wd=0.2)
```

![](img/882813d008b5a95a58e68eaae4b5131c.png)

åœ¨æˆ‘ä»¬çš„éªŒè¯é›†ä¸Šï¼Œæˆ‘ä»¬å·²ç»è¾¾åˆ°äº† 0.223**çš„åˆ†æ•°ã€‚ç”±äºç«èµ›ä¸æ¥å—æäº¤ææ–™ï¼Œæˆ‘ä»¬åªèƒ½é€šè¿‡æŸ¥çœ‹æ’è¡Œæ¦œæ¥å¤§è‡´äº†è§£è¯¥æ¨¡å‹çš„è¡¨ç°:**

![](img/d8e8dd01a68bdfb8b75006ea713489d6.png)

æ’åç¬¬ä¸€çš„æ˜¯ **0.229** ã€‚å¯¹æ¯”è¿™æ¬¾è½¦å‹çš„ **0.223** ã€‚æˆ‘ä»¬ä¸çŸ¥é“å®ƒåœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Œä½†æ€»çš„æ¥è¯´ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å¾—åˆ°çš„ç»“æœä¸€ç‚¹ä¹Ÿä¸å·®ã€‚

# å…³äºåµŒå…¥å±‚çš„æ›´å¤šå†…å®¹

W è®©ä¸€åˆ‡ç‚¹å‡»è¿™é‡Œçš„æ˜¯åµŒå…¥å±‚ã€‚åµŒå…¥åªæ˜¯ä¸€ä¸ªæŠŠæŸç‰©æ˜ å°„åˆ°ä¸€ä¸ªå‘é‡çš„èŠ±å“¨è¯´æ³•ã€‚å°±åƒ NLP ä¸­è¶Šæ¥è¶Šæµè¡Œçš„å•è¯åµŒå…¥ä¸€æ ·ï¼Œå®ƒæ„å‘³ç€ä½¿ç”¨ä¸€ä¸ªå‘é‡(å¤§å°æ˜¯ä»»æ„çš„ï¼Œå–å†³äºä»»åŠ¡)æ¥è¡¨ç¤ºå•è¯ï¼Œè¿™äº›å‘é‡æ˜¯æƒé‡ï¼Œå¯ä»¥é€šè¿‡åå‘æ”¯æŒæ¥è®­ç»ƒã€‚

![](img/1371423a5a805016b3a69a605435bfcc.png)

ç±»ä¼¼åœ°ï¼Œå¯¹äºæˆ‘ä»¬çš„ä¾‹å­ï¼Œæˆ‘ä»¬åœ¨åˆ†ç±»å˜é‡ä¸Šä½¿ç”¨äº†åµŒå…¥ã€‚æ¯ä¸€åˆ—éƒ½æœ‰ä¸€ä¸ªå¯ä»¥è®­ç»ƒçš„åµŒå…¥çŸ©é˜µã€‚æ¯ä¸ªå”¯ä¸€çš„åˆ—å€¼éƒ½æœ‰ä¸€ä¸ªæ˜ å°„åˆ°å®ƒçš„ç‰¹å®šå‘é‡ã€‚è¿™æ–¹é¢çš„ç¾å¦™ä¹‹å¤„åœ¨äº:**é€šè¿‡åµŒå…¥ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥å¼€å‘å˜é‡**çš„â€œè¯­ä¹‰â€ï¼Œè¿™ç§â€œè¯­ä¹‰â€ä»¥æƒé‡çš„å½¢å¼å½±å“æˆ‘ä»¬çš„é”€å”®ä»·æ ¼ï¼Œå¯ä»¥é€šè¿‡æˆ‘ä»¬çš„æ·±åº¦ç¥ç»ç½‘ç»œæå–å’Œè®­ç»ƒã€‚è¯¥æ¨¡å‹å°†å…·æœ‰â€œT2â€æ·±åº¦ï¼Œå®ƒéœ€è¦å¾ˆå¥½åœ°é€‚åº”å¤§æ•°æ®é›†ã€‚

ä½†æ˜¯ä¸è¦æŠŠæˆ‘çš„è¯å½“çœŸï¼Œè¿˜æ˜¯åªçœ‹æˆ‘è¿™ä¸ªä¸èµ·çœ¼çš„å°é¡¹ç›®çš„æˆæœå§ã€‚åœ¨ä¸€ä¸ªæ›´è£è€€çš„ä¾‹å­ä¸­ï¼Œæœ‰[è¿™ç¯‡è®ºæ–‡](https://arxiv.org/abs/1604.06737)ï¼Œä½œè€…æ˜¯åœ¨ä¸€ä¸ªåä¸º [Rossman](https://www.kaggle.com/c/rossmann-store-sales/overview) (é¢„æµ‹æœªæ¥é”€å”®)çš„ Kaggle ç«èµ›ä¸­è·å¾—ç¬¬ä¸‰åçš„äººã€‚åœ¨æ’è¡Œæ¦œä¸Šçš„é¡¶çº§å›¢é˜Ÿä¸­ï¼Œå…¶ä»–äººéƒ½ä½¿ç”¨äº†æŸç§é‡å‹åŠŸèƒ½å·¥ç¨‹ï¼Œä½†é€šè¿‡ä½¿ç”¨åµŒå…¥å±‚ï¼Œä»–ä»¬ä»¥è¾ƒå°‘çš„åŠŸèƒ½å·¥ç¨‹è·å¾—äº†ç¬¬ä¸‰åã€‚

æ›´æœ‰è¶£çš„æ˜¯ï¼Œæœ‰äº†åµŒå…¥å±‚ï¼Œä½ å®é™…ä¸Šå¯ä»¥åœ¨åµŒå…¥çŸ©é˜µç©ºé—´ä¸­å¯è§†åŒ–å˜é‡æŠ•å½±ã€‚ä»¥ç½—æ–¯æ›¼é¡¹ç›®ä¸ºä¾‹ã€‚ä»–ä»¬å¯¹å¾·å›½å„å·çš„åµŒå…¥çŸ©é˜µè¿›è¡Œäº†äºŒç»´æŠ•å½±ã€‚

> å¦‚æœä½ åœ¨åµŒå…¥ç©ºé—´ä¸Šåœˆå‡ºä¸€äº›å·ï¼Œåœ¨å®é™…åœ°å›¾ä¸Šåœˆå‡ºç›¸åŒçš„å·ã€‚ä½ ä¼šå‘ç°å®ƒä»¬æƒŠäººçš„ç›¸ä¼¼ã€‚åµŒå…¥å±‚å®é™…ä¸Šå‘ç°äº†åœ°ç†ã€‚

è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰ç”¨ï¼Ÿåœ¨ Medium ä¸Šå…³æ³¨æˆ‘([æç«‹ä¼Ÿ](https://medium.com/u/72c98619a048?source=post_page-----dbe7106145f5----------------------))æˆ–è€…ä½ å¯ä»¥åœ¨ Twitter [@lymenlee](https://twitter.com/lymenlee) æˆ–è€…æˆ‘çš„åšå®¢ç½‘ç«™[wayofnumbers.com](https://wayofnumbers.com/)ä¸Šæ‰¾åˆ°æˆ‘ã€‚ä½ ä¹Ÿå¯ä»¥çœ‹çœ‹æˆ‘ä¸‹é¢æœ€å—æ¬¢è¿çš„æ–‡ç« ï¼

[](/this-is-cs50-a-pleasant-way-to-kick-off-your-data-science-education-d6075a6e761a) [## â€œè¿™æ˜¯ CS50â€:å¼€å§‹æ•°æ®ç§‘å­¦æ•™è‚²çš„æ„‰å¿«æ–¹å¼

### ä¸ºä»€ä¹ˆ CS50 ç‰¹åˆ«é€‚åˆå·©å›ºä½ çš„è½¯ä»¶å·¥ç¨‹åŸºç¡€

towardsdatascience.com](/this-is-cs50-a-pleasant-way-to-kick-off-your-data-science-education-d6075a6e761a) [](/two-sides-of-the-same-coin-fast-ai-vs-deeplearning-ai-b67e9ec32133) [## ä¸€æšç¡¬å¸çš„ä¸¤é¢:æ°ç‘ç±³Â·éœåå¾·çš„ fast.ai vs å´æ©è¾¾çš„ deeplearning.ai

### å¦‚ä½•ä¸é€šè¿‡åŒæ—¶å‚åŠ  fast.ai å’Œ deeplearning.ai è¯¾ç¨‹æ¥â€œè¿‡åº¦é€‚åº”â€ä½ çš„äººå·¥æ™ºèƒ½å­¦ä¹ 

towardsdatascience.com](/two-sides-of-the-same-coin-fast-ai-vs-deeplearning-ai-b67e9ec32133) [](/what-you-need-to-know-about-netflixs-jupyter-killer-polynote-dbe7106145f5) [## ä½ éœ€è¦äº†è§£ç½‘é£çš„â€œæœ±åº‡ç‰¹é»‘ä»”â€:å†°ç©´ğŸ“–

### æ˜¯æ—¶å€™è®© Jupyter ç¬”è®°æœ¬æœ‰ä¸ªæœ‰ä»·å€¼çš„ç«äº‰å¯¹æ‰‹äº†

towardsdatascience.com](/what-you-need-to-know-about-netflixs-jupyter-killer-polynote-dbe7106145f5)