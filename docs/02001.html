<html>
<head>
<title>Understanding Linear Regression and The Need For Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解线性回归和梯度下降的必要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-linear-regression-and-the-need-for-gradient-descent-2cc0f25763d5?source=collection_archive---------21-----------------------#2019-04-02">https://towardsdatascience.com/understanding-linear-regression-and-the-need-for-gradient-descent-2cc0f25763d5?source=collection_archive---------21-----------------------#2019-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="61a3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从头开始实现线性回归</h2></div><p id="ac24" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我将从头开始用 python 实现一元线性回归，以更好地理解算法和梯度下降的需要。在我们开始之前，这里有一个线性回归算法的快速概述。</p><h2 id="c7ae" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">线性回归:概述</h2><p id="74ba" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">假设我们有所有朋友的身高和体重。假设一个人的体重只是身高的函数，没有其他因素，如果我们知道一个新人的身高，我们如何预测他的体重呢？这就是我们使用一元线性回归的地方。当要预测的值或目标变量是连续的时，我们采用线性回归。线性回归是一种机器学习算法，它试图将给定的一组点拟合到一条直线上，y = ax + b。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mc"><img src="../Images/436e2850bc009ee7db3b718035e455f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y0X6mxbAT7WLn-c5.png"/></div></div></figure><p id="da68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的示例中，因变量 y 将是身高，自变量 x 将是体重，线性回归算法的工作是找到<strong class="kk iu"> a </strong>和<strong class="kk iu"> b </strong>的值，这样<strong class="kk iu"> ax + b </strong>将绘制一条符合所有(体重、身高)坐标的直线，即给出相应的体重。现在我们来谈谈它是如何做到这一点的。</p><h2 id="8ecd" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">该算法</h2><p id="521f" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">线性回归算法通过最小化预测点和实际点之间距离的均方误差(MSE)来找到<strong class="kk iu"> a </strong>和<strong class="kk iu"> b </strong>的正确值。</p><p id="0282" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这本质上是一个优化问题，其中需要确定<strong class="kk iu"> a </strong>和<strong class="kk iu"> b </strong>的值，这是通过找到某个损失函数的最小值来确定的。该算法从<strong class="kk iu"> a </strong>和<strong class="kk iu"> b </strong>的一些初始值开始，并计算 y。然后找到实际点(y_actual)和由预测 y 值(y_pred)给出的点之间的距离。该算法计算距离的平方和及其平均值。这个平均值或 MSE 是需要通过优化问题最小化的损失函数。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="ab gu cl mo"><img src="../Images/43ab7446f36f40276108cb81df96399e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*A71zTD6_QqUzLhMKj1Rgiw.png"/></div><figcaption class="mp mq gj gh gi mr ms bd b be z dk">Finding the distance between the predicted points and the actual points</figcaption></figure><h2 id="c380" class="le lf it bd lg lh li dn lj lk ll dp lm kr ln lo lp kv lq lr ls kz lt lu lv lw bi translated">Python 实现</h2><p id="f498" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">现在我们知道了算法是如何工作的，让我们试着用 python 来实现它。</p><p id="1c4f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们导入一些我们需要的库，matplotlib 来绘制我们的线，numpy 来处理我们的数据。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="869c" class="le lf it mu b gy my mz l na nb">import matplotlib.pyplot as plt<br/>import numpy as np</span></pre><p id="4319" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们为 x 和 y 取一些值，</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="f0ba" class="le lf it mu b gy my mz l na nb">x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) <br/>y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])</span></pre><p id="2c9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">x 和 y 都具有(10)的形状，绘制时看起来如下图所示，</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="1946" class="le lf it mu b gy my mz l na nb">plt.scatter(x, y)</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/06160ab98fb82f730bb28adf6e21a21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*ldAvyYVDqPOlNxP6TKmiGg.png"/></div></figure><p id="55c8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们初始化 a 和 b，并定义计算 y_pred 和 mse 的方程。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="f7a5" class="le lf it mu b gy my mz l na nb">a = b = 0</span><span id="3583" class="le lf it mu b gy nd mz l na nb">y_ = a * x + b</span><span id="d019" class="le lf it mu b gy nd mz l na nb">mse = np.mean((y — y_)**2)</span><span id="fd41" class="le lf it mu b gy nd mz l na nb">print(mse)</span><span id="1aeb" class="le lf it mu b gy nd mz l na nb">&gt;&gt;54.1</span></pre><p id="b17a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于给定的 x 和 y 值，mse 的初始值为 54.1。我们现在的目标是使其尽可能接近零。</p><p id="e9af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们选择一个方向来遍历并更新 a 和 b 的值，直到我们的 mse 为零。我要把 a 和 b 都增加随机值，每一步 0.01，看看会得到什么。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="2247" class="le lf it mu b gy my mz l na nb">while mse &gt; 0:<br/> <br/>  a = a+0.01<br/>  b = b+0.01<br/>  y_ = a * x + b<br/> <br/>  if np.mean((y — y_)**2) &lt; mse :<br/>   mse = np.mean((y — y_)**2)<br/> <br/>  else :<br/>   print (a,b,mse)<br/>   break</span><span id="0761" class="le lf it mu b gy nd mz l na nb">&gt;&gt;&gt;1.1900000000000008 1.1900000000000008 0.5634000000000003</span></pre><p id="8bdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们的 a 和 b 都是 1.19 时，我们的 mse 值下降到 0.563。</p><p id="9a14" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画出结果线，看看它与我们的实际点是如何吻合的。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="4e1d" class="le lf it mu b gy my mz l na nb">y_pred = a * x + b</span><span id="8f6c" class="le lf it mu b gy nd mz l na nb">plt.figure(figsize = (12, 10))</span><span id="e44d" class="le lf it mu b gy nd mz l na nb">plt.plot(x, y_pred, label = ‘Predicted’)<br/>plt.scatter(x, y, label = ‘Actual’)</span><span id="7252" class="le lf it mu b gy nd mz l na nb">plt.show()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi ne"><img src="../Images/8aded0ada9163ab6e4df48391aa185f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v84umh4LkIjz9FDtcEtoWw.png"/></div></div></figure><p id="8f22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的路线似乎符合要点，但并不那么好。让我们将这个结果与成熟的线性回归模型(如 sklearn 模型)产生的结果进行比较。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="39f9" class="le lf it mu b gy my mz l na nb">x = x.reshape(-1,1)<br/>y = y.reshape(-1,1)</span><span id="49ae" class="le lf it mu b gy nd mz l na nb">from sklearn.linear_model import LinearRegression</span><span id="5d4b" class="le lf it mu b gy nd mz l na nb">reg_model = LinearRegression().fit(x, y)</span><span id="be6e" class="le lf it mu b gy nd mz l na nb">y_pred = reg_model.predict(x)</span><span id="a04f" class="le lf it mu b gy nd mz l na nb">plt.figure(figsize = (12, 10))</span><span id="c255" class="le lf it mu b gy nd mz l na nb">plt.plot(x, y_pred, label = ‘Predicted’)<br/>plt.scatter(x, y, label = ‘Actual’)</span><span id="a9a7" class="le lf it mu b gy nd mz l na nb">plt.show()</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/a6fc3f45ec6f401b39e9b7ba227d3d60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*E6VUQy8dVIh0uZOYFDcqsg.png"/></div></figure><p id="e3fb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">sklearn 回归器生成的直线似乎完全符合这些点。让我们看看 a 和 b 的值是多少。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="793a" class="le lf it mu b gy my mz l na nb">reg_model.intercept_</span><span id="b53c" class="le lf it mu b gy nd mz l na nb">&gt;&gt; array([1.23636364])</span><span id="b568" class="le lf it mu b gy nd mz l na nb">reg_model.coef_</span><span id="b4c5" class="le lf it mu b gy nd mz l na nb">&gt;&gt; array([[1.16969697]])</span></pre><p id="e172" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接近但不相同。显然，他们采取了不同的路线来最小化他们的 mse，并以不同的值结束。</p><p id="3360" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们用一组不同的 x 和 y 再试一次。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="1e0c" class="le lf it mu b gy my mz l na nb">x = np.linspace(-1, 1, 101)<br/>y = 4 * x + np.random.randn(101) * 0.44<br/>print (x.shape , y.shape)</span><span id="a207" class="le lf it mu b gy nd mz l na nb">&gt;&gt; (101,) (101,)</span></pre><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5d076e1fb37d74e83de19066765452ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*sjxQgpIxNxbUUlVSaDsnyg.png"/></div></figure><p id="90a8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在的初始 mse 是 5.79788117428826。让我们再运行一次同样的代码，看看我们得到的 a 和 b 的值是多少，在我们的探索中得到这个 5.7 到 0。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="4451" class="le lf it mu b gy my mz l na nb">while mse &gt; 0:<br/> <br/>  a = a+0.01<br/>  b = b+0.01<br/>  y_ = a * x + b<br/> <br/>  if np.mean((y — y_)**2) &lt; mse :<br/>   mse = np.mean((y — y_)**2)<br/> <br/>  else :<br/>   print (a,b,mse)<br/>   break</span><span id="f4a3" class="le lf it mu b gy nd mz l na nb">&gt;&gt;&gt;1.0300000000000007 1.0300000000000007 4.4175244648874</span></pre><p id="7aab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">很明显，我们选择的路线是条死胡同。当我们尝试将 a 和 b 都增加 0.01 时，我们的 mse 停留在 4.4，离 0 很远。没关系，让我们试试不同的路线。</p><pre class="md me mf mg gt mt mu mv mw aw mx bi"><span id="2638" class="le lf it mu b gy my mz l na nb">while mse &gt; 0:<br/> <br/> a=a+0.1<br/> b=b-0.01<br/> y_ = a * x + b<br/> <br/> if np.mean((y — y_)**2) &lt; mse :<br/> mse = np.mean((y — y_)**2)<br/> <br/> else :<br/> print (a,b,mse)<br/> break</span><span id="eb7a" class="le lf it mu b gy nd mz l na nb">&gt;&gt; 4.000000000000002 -0.4000000000000002 0.34416766289398254</span></pre><p id="a4f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那似乎更好。让我们试着画出来，看看这条线有多吻合。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi nh"><img src="../Images/2ab1493e01d88f20001f19f7fc576822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IoImT06yu03EXCiQnTNMcA.png"/></div></div></figure><p id="dd28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">再次，接近，但可以更好。很明显，需要改变的是我们上下遍历数轴寻找 a 和 b 的正确值的方式，但是我们如何知道是减少还是增加，如果是，增加多少？有一百万条不同的路径可供我们选择，但没有一条路径适用于每一个数据集。</p><p id="bd74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个问题的答案是<strong class="kk iu">梯度下降</strong>。我们通过在损失负梯度的最陡下降方向上迭代行进来最小化损失。</p><p id="8d55" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">称为学习率的模型超参数决定步长。学习率 x 梯度给出了我们更新 a 和 b 的值。我们将在以后的帖子中更深入地研究梯度下降算法。</p></div></div>    
</body>
</html>