# 处理 ML 模型中高度相关的列

> 原文：<https://towardsdatascience.com/background-d5f101e00afc?source=collection_archive---------11----------------------->

## 解决一个常见问题的简单尝试

![](img/1928b8854bb25d6a623f4882e1c49cdd.png)

# 背景

作为数据科学训练营项目的一部分，我正在处理一个信用卡默认数据集。我们不得不运行各种 ML 算法来尝试和预测某人是否会违约，主要使用 [F1 分数](https://en.wikipedia.org/wiki/F1_score)作为衡量标准。我们尝试在岭和套索逻辑回归、K 近邻和决策树上进行网格搜索，看看哪个模型效果最好。

# 问题！

正如上面的小标题所暗示的，我遇到了一些问题。以下是数据帧的列列表:

![](img/6fff40c83ed7ef9eda028c186da913af.png)

请注意，有一些看起来非常相似的列，pay _ 0–6、bill _ AMT 1–6 和 pay _ AMT 1–6。这些数字代表他们所在的月份。当我查看相关性时，我发现了这个:

![](img/bb9dcaa2e676c80d7d01d0120635a684.png)![](img/94b116fb470b1821923041e3fbf24db5.png)![](img/875100df6dc77df8f50e1774fbed08c9.png)

如您所见，pay_ columns 和 bill_amt 列彼此之间有很强的相关性。pay_amt 这几个并不是那么的相互关联。高相关性表明许多列包含冗余信息，即来自一列的信息包含在其他列中。

# 量化问题

我的第一个问题是:我如何量化这种程度的冗余？公认的是，相关平方是另一个列描述一个列的良好程度的良好近似值。如果我把所有的相关矩阵平方并相加，会发生什么？

每个组有 6 列，因此如果所有列都是独立的(因此相关性为 0)，平方和将为 6，因为每列与自身的相关性为 1。如果它们都是相同的，那么所有的相关性都是 1，和是 36。第一种情况下每列的平均值为 1，第二种情况下为 6。在第一种情况下，有 6 列“有价值”的信息，而在第二种情况下，6/6=1 列“有价值”的信息，这对我来说很直观。

找到上面 3 个例子的总和，每次除以 6，我们得到:

![](img/a79fc887486390bc7949de2a5dc7f7c0.png)

为了得到每组中的非冗余列数，我们将该组中的总列数除以这个数。这表明 pay_ group 实际上有 6/3.21= 1.87 列“有价值”的信息，bill_amt 组有 6/4.94= 1.21 列有价值，pay_amt 组有 6/1.19=5.05 列有价值。这很直观，因为 bill_amt 列几乎完全相同，而 pay_amt 列则不同。

我还问自己，如果这些组相互关联，例如“账单金额”和“支付金额”，会发生什么，然后我得出了一个优雅的解决方案。

# 解决办法

我的解决方案是为每一列得出一个冗余分数，我称之为 C，它是该列与数据中所有其他列之间相关性的平方和。以下是按 C 度量的顶部和底部列:

![](img/88d53bfbb2d7cf83f7a3bb6b17909791.png)

Unsurprisingly, bill_amt and pay_ columns are at the top

![](img/36a701f5ba469204b80fe22f4477a10f.png)

注意，正如前面所讨论的，C 从来不会低于 1，对于 bill_amt 列，C 接近于 6(非常多余！).

# 应用解决方案:岭回归

如果你在做岭回归，有一个与列的平方系数成比例的惩罚。例如，如果你的模型试图预测 X，使用 A，你说 X=2A，惩罚将是(2 )λ，也就是 4λ。λ是进行岭回归前的超参数集。现在如果你创建一个列 B=A，你的模型将是 X= A+B，这将有一个(2*1 )λ，或 2λ的惩罚，是之前的一半。这意味着岭回归对冗余列的惩罚不足。

这个问题可以通过将列中的值除以 sqrt(C)来解决。在上面的例子中，A 和 B 之间的相关性是 1，所以每一列的 C 将是 2。如果我们将每一列除以√2，我们将得到等式 X= √2 A + √2 B，给我们一个(2*√2 )λ的惩罚，或如前所述的 4λ。这意味着我们的模型不会因为加入新列而有偏差。

声明:记得在使用定标器后进行划分，而不是之前！

# 应用解决方案:K-最近邻

K-最近邻(KNN)算法试图通过查看相似的数据点来猜测目标变量。相似数据点的数量看起来是“K ”,它通过最小化“距离”来确定相似性。

测量距离的一种常用方法是欧几里德距离——两点之间的直线距离。计算方法是将每列中的差值平方，将它们相加，然后对总和求平方根。例如，如果 A 列中的距离为 3，B 列中的距离为 4，则欧几里德距离为 sqrt(3 + 4 )= sqrt(25)= 5。

在上面的例子中，如果我们定义一个新的列 X=A，列 X 中的距离将是 3，新的欧几里德距离将是 sqrt(2*3 + 4 )= sqrt(34)。这造成了对冗余信息的不适当的偏见。

这也可以通过将两列除以 sqrt(C)来解决。在上面的例子中，A 和 X 之间的相关性是 1，所以如果 B 独立于 A，它们的 C 值将是 2。所以除以 sqrt(2)后，每列的距离都是 3/sqrt(2)。新的欧氏距离将是 sqrt(2 *(3/√2)+4)= sqrt(2 * 4.5+4)= sqrt(25)，和之前一样！

注意:如果你使用 p 阶的闵可夫斯基距离，你用(C )^(1/p).)除列在欧几里得的例子中，如果你在测量曼哈顿距离，p=1，那么你用 C 除列。

声明:记得在使用定标器后进行划分，而不是之前！

# 应用解决方案:随机森林

随机森林是随机生成的决策树的集合，这些决策树对机器学习问题的解决方案进行“投票”。通过引导选择随机数据，并在每个节点选择随机“特征”，即随机列。

如果列中有冗余信息，那么随机选择会使我们偏向冗余信息。这实际上可以很容易地解决，当选择它们时，给每个列一个 1/C 的权重。

从 A 和 B 中选一个的时候，几率是 50/50。如果我们加上 X=A，那么我们有 2/3 的机会从 A 获得信息。因为 A 和 X 的相关性为 1，所以它们的 C 值都为 2。除以 C，我们得到 A 的权重为 0.5，X 的权重为 0.5，B 的权重为 1。这意味着我们有 25%的机会选择 A，25%的机会选择 X，50%的机会选择 B，回到 50/50。

# 结论

我觉得我上面讨论的是对机器学习模型中相关列出现的一些问题的平滑解决方案。我试图在谷歌上寻找尝试类似事情的人，但我什么也找不到。如果有缺陷，或者你知道有人以前这样做过，请通知我，这样我可以做必要的更正。