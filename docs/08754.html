<html>
<head>
<title>Qrash Course II: From Q-Learning to Gradient Policy &amp; Actor-Critic in 12 Minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Qrash 课程 II:12 分钟内从 Q-学习到梯度政策&amp;行动者-批评家</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c?source=collection_archive---------3-----------------------#2019-11-24">https://towardsdatascience.com/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c?source=collection_archive---------3-----------------------#2019-11-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2c4a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">朝着真正理解强化学习的基础又迈进了一步</h2></div><p id="a543" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">本帖讨论的所有算法的实现都可以在</em> <a class="ae lf" href="https://github.com/shakedzy/notebooks/tree/master/gradient_policy_and_actor_critic" rel="noopener ugc nofollow" target="_blank"> <em class="le">我的 GitHub 页面</em> </a> <em class="le">找到。</em></p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="1d1b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">使用此</strong> <a class="ae lf" rel="noopener" target="_blank" href="/qrash-course-ii-from-q-learning-to-gradient-policy-actor-critic-in-12-minutes-8e8b47129c8c?sk=77e4a62c7dd982d32d1034c151552271"> <strong class="kk iu">好友链接</strong> </a>即使不是普通会员也可以阅读这篇博文</p></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi ln"><img src="../Images/1ed3969dec86f345e22e24d0d883ef91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOIFd0MGo1ZbZ8XNoHYl2w.jpeg"/></div></div></figure><p id="14b5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="le">Qrash 课程系列:</em> </strong></p><ol class=""><li id="d81c" class="lz ma it kk b kl km ko kp kr mb kv mc kz md ld me mf mg mh bi translated"><em class="le">第一部分:</em> <a class="ae lf" href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" rel="noopener"> <em class="le">强化学习和 Q-Learning 简介</em> </a></li><li id="04fc" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><em class="le">第二部分:政策梯度和行动者-批评家</em></li></ol></div><div class="ab cl lg lh hx li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="im in io ip iq"><p id="ce3a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之前——也是第一个——<a class="ae lf" href="https://medium.com/@shakedzy/qrash-course-deep-q-networks-from-the-ground-up-1bbda41d3677" rel="noopener">Qrash 课程帖子</a>带我们从对强化学习一无所知一路走到完全理解 RL 最基础的算法之一:<em class="le"> Q 学习</em>，以及它的深度学习版本<em class="le"> Deep Q-Network </em>。继续我们的旅程，再介绍两个算法:<em class="le">渐变策略</em>和<em class="le">演员-评论家</em>。这两个人，加上 DQN，可能是现代深度强化学习最基本的组成部分。</p><h1 id="fb18" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">为什么 Q-Learning 不够用？</h1><p id="3501" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我们可能应该问自己的第一个问题是<em class="le">为什么</em>我们应该从 Q-Learning <em class="le">前进？它在哪里失败或表现不佳？这个算法确实有一些缺陷，理解它们很重要:</em></p><ol class=""><li id="495c" class="lz ma it kk b kl km ko kp kr mb kv mc kz md ld me mf mg mh bi translated"><strong class="kk iu">在大量操作中表现不佳:</strong>让我们假设一个操作数量很大的环境。非常大，比如几千，甚至更多。使用<em class="le">ε</em>-贪婪策略探索所有可能的动作可能会花费太长时间，并且该算法很容易收敛到局部最大值，而不是真正的最佳可能解决方案。不太好。</li><li id="5911" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="kk iu">动作集合必须是有限的:</strong>让我们把可能动作的数量拉伸到它的绝对极限:无限。这实际上很常见——想象一下一辆自动驾驶汽车，它的动作是转动方向盘的程度。这是一个连续的数字范围，因此是无限的。很明显为什么 Q 学习算法不能处理这种情况。</li><li id="b0ac" class="lz ma it kk b kl mi ko mj kr mk kv ml kz mm ld me mf mg mh bi translated"><strong class="kk iu">糟糕的探索策略:</strong>我们来举个例子——假设一个环境中有 10 种不同的行动是可能的，一个 Q-学习代理使用一个<em class="le">ε</em>-贪婪策略，其中<em class="le"> ε </em> = 0.1。现在让我们假设行动#1 是在这个给定时刻具有最高 Q 值的行动。这意味着代理人有 91%的机会选择行动#1 (90%的机会选择贪婪行动+ 1%的机会随机选择这个行动)，而所有其他行动只有 1%的机会被选中。现在考虑这些 q 值:<strong class="kk iu"> <em class="le"> Q(s，a ) = 3.01，Q(s，a ) = 3.00，…，Q(s，a ⁰) = 0.03。</em> </strong>尽管行动#1 和#2 的 Q 值非常接近，但选择#1 而不是#2 的机会要大得多，正如刚才解释的那样。更糟糕的是，选择行动#2 和#10 的几率是完全一样的，尽管行动#2 实际上是百倍(！！)更好。现在结合第 1 节中的问题，您可以看到为什么在这种情况下这可能不是最好的算法。</li></ol><p id="920e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们应该如何处理这些情况？让我们来认识一下我们的救世主吧！</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="c167" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">政策梯度</h1><p id="9e9f" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">Q 学习算法通过试图找到每个状态的<em class="le">动作值函数—</em>Q 值函数来学习。它的整个学习过程是基于计算出每个可能动作的<em class="le">质量</em>的思想，并根据这些知识进行选择。因此，Q-Learning 试图对所有可能的动作有完整和公正的了解——这也是它最大的缺点，因为这需要对每个可能的状态和动作进行足够次数的尝试。策略梯度算法以更稳健的方式学习，通过<em class="le">而不是</em>尝试评估每个动作的<em class="le">值</em>——而是简单地评估它应该偏好哪个动作。</p><p id="bc6d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我更好地说明一下:每个模型最终都有一些它试图优化的学习参数(这些通常是神经网络的权重)。让我们将这些参数表示为<strong class="kk iu"> θ </strong>。在 DQN，学习周期是:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/63a5d663de8d8b28a7e5d56274597abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*LUW5PJXAKrEjFZP7jnGO6g.png"/></div></figure><p id="916e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着训练过程优化了网络的学习参数，然后用于计算 Q 值。然后，我们使用这些 Q 值来决定我们的策略π(其中<em class="le">策略</em>就是每个行动被选择的概率)，DQN 的<em class="le">ε-贪婪</em>策略取决于预测的 Q 值，因为它给具有最高 Q 值的行动最高的概率:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nn"><img src="../Images/e63e2da8cf35f1a57e930462944b03ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p7--1Q709AkC5F0ooqMnaA.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">Policy of a Deep Q-Learning algorithm</figcaption></figure><p id="1a38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">策略梯度算法的学习周期更短，跳过了 Q 值部分:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/96715e337f2667a66638eb894dc28538.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*tGs2FREYZuPWJCd94jFDUw.png"/></div></figure><p id="9d32" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着网络将直接输出选择每个动作的概率，跳过额外的计算。这将允许该算法更加鲁棒。</p><p id="1896" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在剩下一个非常重要的问题— <em class="le">我们如何优化网络的权重？</em>在 DQN，事情很简单:我们知道如何计算 Q 值，因此我们可以根据它们优化算法。但是我们现在做什么？我们的主要目标是确保模型在每个学习阶段后变得更好。让我们试着把这写成在每个时间步<em class="le"> t </em>的权重的一个非常通用的更新规则:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi nt"><img src="../Images/d86c69e8f3c8a38d2094f336b738cad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXBZgFZcTRBof8qd4Gblgg.png"/></div></div></figure><p id="d273" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们将<em class="le"> J </em>定义为一些<em class="le">性能度量</em>，我们稍后会讲到，而<em class="le"> α </em>是学习率。我想花一点时间来澄清时间步长<em class="le"> t </em>和<em class="le"> s </em>之间的区别，时间步长 t 表示一集的<em class="le">步</em>和<em class="le">s</em>步<em class="le">状态</em>。<em class="le">状态</em>由环境定义，例如——所有可能的棋盘。<em class="le">步骤</em>由代理定义，并标记它所经历的状态序列。这意味着步骤<em class="le"> t=3 </em>和<em class="le"> t=7 </em>可能是相同的状态<em class="le"> s </em>，因为一些环境允许代理多次返回完全相同的状态。另一方面，每集每一步只出现一次。步骤是按时间顺序排列的<em class="le">时间顺序</em>。</p><p id="7443" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们回到我们的更新规则，我们将它定义为依赖于新的<em class="le"> J </em>参数的<em class="le">梯度</em>。虽然它可能对神经网络训练的反向传播<em class="le"> </em>阶段很熟悉，但这里有一个非常重要的区别:在梯度策略中，我们希望<em class="le">提高</em>的性能，因此我们希望<em class="le">最大化</em>对<em class="le"> J </em>的导数，而不是最小化它。这被称为梯度<em class="le">上升— </em>与反向传播中执行的梯度<em class="le">下降</em>相反。其实也是差不多的逻辑，只是反过来而已。</p><h2 id="5415" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">设计我们的第一个梯度策略算法</h2><p id="a478" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在这一点上，很明显我们如何定义<em class="le"> J </em>将会有所不同，但是这也完全取决于<em class="le"> us </em>来决定什么是正确的定义。让我们尝试找到一些简单的东西——一些取决于 Q 值的东西(即使算法从来不会计算它)。回想一下，Q 值<em class="le"> Q(s，a) </em>是从状态<em class="le"> s </em>开始并在执行动作<em class="le">a</em>之后，代理人应获得的所有奖励的度量，所有奖励的总和似乎是一个相当好的表现度量——毕竟，代理人的唯一目的是增加其收集的总奖励。因此，我们可以使用整个剧集的总累积奖励作为我们的绩效衡量标准:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi og"><img src="../Images/4387c1ee228b68bf2462a44d8957f0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*08ua8e5OCgiquMbkr2huQQ.png"/></div></figure><p id="0959" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是初始状态 s⁰的所有 q 值的总和，乘以选择每个动作的概率。这正是我们期待的一整集的总体回报。现在，某一状态的所有 Q 值乘以它们的概率的总和有了一个名字:它被称为状态的值函数，并被表示为<em class="le"> V(s) </em>。一个状态的价值函数是从状态<em class="le"> s </em>到这一集结束的预期回报的度量，而不知道将选择哪个行动。所以我们基本上将初始状态的价值函数定义为我们的性能度量:<em class="le"> J = V(s⁰) </em>。</p><p id="7713" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了<em class="le"> J </em>，我们需要计算它的梯度。这涉及到一些复杂的数学问题，因为<em class="le"> J </em>依赖于行动选择的概率<em class="le">p(a)</em>哪个<em class="le"> </em>是从策略<em class="le"> π — </em>中派生出来的，但是π依赖于<em class="le"> J </em>，因为这是我们如何定义它的。你可以在萨顿·巴尔托的《强化学习导论》中找到完整的数学推导。)，第 325–327 页(这里有一个<a class="ae lf" href="http://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf" rel="noopener ugc nofollow" target="_blank">免费网络版</a>)。我在这里跳过它，写下解决方案:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oh"><img src="../Images/30f87663ff3e34cdff179784aa87e92e.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*Ozv4ooZUbzNFtbwmvrgB4g.png"/></div></div></figure><p id="d93d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="le"> G_t </em>是从步骤<em class="le"> t </em>到该集结束的总累积奖励。这意味着我们的<strong class="kk iu"> θ </strong>的更新规则是:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7915f51e7a58a4eeb11aa6c1d020a45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*whicjIA00GcXBqUju_caKA.png"/></div></figure><p id="1ae3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对一项政策进行对数运算可能看起来令人生畏或者很奇怪——但这只是我们需要应用的另一个数学函数，仅此而已。</p><h2 id="b5fb" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">勘探和开发</h2><p id="cad9" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">我想再次强调策略梯度算法的一个重要特性——它直接学习策略<em class="le"/>。这意味着网络的输入是当前状态<em class="le"> s </em>，输出是选择每个动作的概率:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/a89f3661055778492ce3eaf502e06780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*ljaxm3mHlCI4SO7n_oX8Aw.jpeg"/></div></figure><p id="0161" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么我们实际上如何选择一个动作呢？我们简单地对动作进行加权采样。这也解决了我们探索的需要——每个行动都有被选中的机会，但不是均等的，因为最好的行动最有可能被选中。</p><h2 id="4568" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">培养</h2><p id="0a75" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">与 Q-Learning 不同，Policy Gradient 算法是一种<em class="le"> on-policy </em>算法，这意味着它只使用当前活动策略进行的状态-动作转换进行学习。从技术上来说，这意味着没有像 DQN 那样的经验回放记忆。一旦模型被训练，它的<strong class="kk iu"> θ </strong>参数会改变，因此它的策略也会改变。这意味着在这次训练之前收集的所有经验必须被丢弃，并且不能再用于训练。所以我们为训练收集的每一条数据都被使用一次，而且只有一次。</p><h2 id="dfea" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">梯度政策在起作用</h2><p id="7bfa" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">虽然在这一点上一般的想法有望被理解，但是举例总是更好。使用强化学习解决的一个非常受欢迎的任务是车杆问题:一辆可以向左或向右移动的车，需要确保站在它上面的杆不会倒下。这是我对这个挑战的解决方案。你也试着去实现它。</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/b11505cdd3e0d2449aedd75957467190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*7HKopFrjZOSjTyImMflUZQ.gif"/></div><figcaption class="no np gj gh gi nq nr bd b be z dk">Cart-Pole</figcaption></figure><h1 id="8e27" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">演员兼评论家</h1><p id="97cf" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">让我们花一分钟的时间来认识到目前为止我们已经开发了什么:我们现在有一个代理，它学习一个策略，而不需要学习每个动作的实际值。它真正需要的唯一东西是一些<em class="le">性能</em>指标，它将试图最大化——在我们的例子中，我们选择了一整集的总预期回报。</p><p id="0d75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种方法的好处很简单——不需要访问和尝试每一个可能的状态-动作对，因为代理对它应该做什么产生了某种“直觉”。但是由于代理对其性能度量的绝对依赖性，这带来了一个缺点。让我们考虑一下我们的例子，我们选择了一集的总体回报作为表现:考虑一集采取了一百步。每一步产生+10 的奖励，但是第 47 步产生-100 的奖励。我们选择的绩效衡量标准无法区分这个陷阱，因为它只知道总体回报。这意味着当我们的代理到达第 47 步时，它可能永远不会尝试另一个动作，因为它不会学习任何特定于状态和动作的知识。</p><p id="b043" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们如何解决这个问题？我们希望有一个智能体，一方面，以类似于梯度策略方法的方式学习策略，尽管另一方面，我们理解特定于状态和特定于动作的知识的重要性，就像 Q 学习方法一样。解决办法？将两者结合在一起。</p><p id="1f67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种组合方法被称为行动者-批评家，由两个<em class="le">子代理</em>一起学习组成:一个学习应该采取行动的政策(因此被称为行动者)，另一个学习每个状态和行动的 Q 值(因此被称为批评家)。然后，我们将<strong class="kk iu"> θ </strong>的更新规则改为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/051b828b351dbb36864ca02cc839ca90.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*VovvAQ2gkAPG6htW1NYWxA.png"/></div></figure><p id="5451" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们只是用 Q 值替换了总报酬<em class="le"> G </em>，但是 Q 值现在也是一个学习参数——由第二个子代理学习。这产生了稍微复杂一点的架构:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi om"><img src="../Images/910fa7d942bd521dbd4f9ccf72fa1e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BU1Z5TJxHBkZf3x8DRKJow.png"/></div></div><figcaption class="no np gj gh gi nq nr bd b be z dk">A schematic view of an Actor-Critic architecture. Dashed lines represent flows only relevant during training, so during inference (prediction) phase, only the Actor is being used</figcaption></figure><p id="31b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，就像策略梯度一样，行动者-批评者也是基于策略的模型——这再次意味着在每次训练之后，所有以前的训练数据都将被丢弃。</p><p id="c11e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅此而已。你猜对了！你现在可以尝试自己实现一个演员评论家，或者看看我的实现解决<a class="ae lf" href="https://gym.openai.com/envs/Acrobot-v1/" rel="noopener ugc nofollow" target="_blank"> Acrobot </a>挑战。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="nk nl l"/></div></figure><h1 id="3fe0" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">A2C、A3C 和 DDPG</h1><p id="0be9" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">这里我们已经做得差不多了，但是让我们再往前走一步，了解一些对我们的演员评论家代理非常有用的优化。</p><h2 id="eddb" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">A2C</h2><p id="0f76" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">回想一下，我们现在在更新规则中使用 Q 值，以便允许代理拥有一些特定于状态-动作的知识。现在让我问你一个问题:<em class="le"> Q(s，a)=100 </em>是不是一个好的 Q 值？答案是— <em class="le">我们不知道。</em>我们不知道，因为我们没有关于<em class="le">其他</em>可能行动的 Q 值的信息。如果所有其他动作产生的 Q 值为 1，那么 100 真的非常好。但是如果所有其他的都产生 10，000，那么 100 实际上是相当糟糕的。这不仅让你困惑，也让模型困惑。如果知道某个动作<em class="le">与休息</em>相比有多好，或者换句话说，采取某个特定动作的<em class="le">优势</em>是什么，将会很有帮助。因此，我们可以用一个动作的优势替换演员-评论家更新规则中的 Q 值，其定义为:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi on"><img src="../Images/655509bbc1f7a5158a5e66375a49f218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eBhXWBYWFowshm0czLR7OQ.png"/></div></div></figure><p id="6032" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<em class="le"> V(s) </em>是状态的价值函数，我们之前已经讨论过了。学习一个动作的优点要容易得多——积极的<em class="le"> A(s，a) </em>是好的，消极的是不好的。这种演员-评论家模型的变体被称为优势演员-评论家，缩写为 AAC，或者更常见的是:A2C(这只是一种有趣的写法，有两个 a 和一个 C)。</p><p id="3221" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会注意到 A2C 版本似乎让学习变得更复杂了，因为现在代理需要学习<em class="le"> Q(s，a) </em>和<em class="le"> V(s) </em>。但事实并非如此。如果 Q 值是从状态<em class="le"> s </em>和动作<em class="le"> a </em>收到的奖励，然后一直持续到该集结束，我们可以这样写:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oo"><img src="../Images/c9ada405615a0d2775f0bab8c9cd1f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lNfvOU1Bd7gLvmrXsPh29w.png"/></div></div></figure><p id="f316" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">意思是——一个 Q 值实际上是直接回报<em class="le"> r </em>和下一个状态的值<em class="le">s’</em>。这意味着我们可以将优势写成:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi op"><img src="../Images/eb4a2a8588b2d5e904a8dad6b58d4566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OnNzydQhRzq1cC8R-9haLQ.png"/></div></div></figure><p id="031a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所以我们实际上只需要学习值函数，并使用它两次——对于状态<em class="le"> s </em>和下一个状态<em class="le">s’</em>。这个小小的调整实际上使得 A2C 比原来的演员兼评论家更容易实现。你可以在 GitHub 上查看我对 A2C 的实现。</p><h2 id="d198" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">A3C</h2><p id="9639" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">既然我们可以拥有多个代理，为什么只拥有一个代理？这是 A3C 模型背后的核心思想，它代表异步优势 Actor-criterion。想法很简单:有许多不同的代理，每个代理都在自己的环境副本中玩，但它们都共享相同的策略和参数。每个代理都在自己的时间上更新共享策略(与其他代理异步)，这使得学习过程更快、更健壮。</p><h2 id="c617" class="nu mo it bd mp nv nw dn mt nx ny dp mx kr nz oa mz kv ob oc nb kz od oe nd of bi translated">DDPG</h2><p id="02fb" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">DDPG(深度确定性策略梯度)算法的设计考虑了连续动作空间的问题。Actor-criterion 模型的训练阶段非常嘈杂，因为它基于自己的预测进行学习。为了解决这个问题，DDPG 借用了我们钟爱的 DQN 的一些元素:首先，它使用了 Experience Replay 内存，这让它成为了一个脱离政策的模型。其次，它使用了与 Double DQN 模型相同的降噪方法——它使用了 Actor 和 criterion 的两个副本，一个副本经过训练，另一个副本以如下方式缓慢更新:</p><figure class="lo lp lq lr gt ls gh gi paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="gh gi oq"><img src="../Images/39852b9a58bf97d692184701de5037f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2MzQYZddgddBsPSrv_DGA.png"/></div></div></figure><p id="fbda" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，<em class="le">线上的</em>网络是训练有素的那一个，而&lt;的&lt; 1 个。详见<a class="ae lf" href="https://github.com/shakedzy/notebooks/blob/master/gradient_policy_and_actor_critic/Continuous%20Mountain%20Car%20with%20DDPG.ipynb" rel="noopener ugc nofollow" target="_blank">我对 DDPG </a>的实现。</p><figure class="lo lp lq lr gt ls"><div class="bz fp l di"><div class="or nl l"/></div></figure><h1 id="be69" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">结束语</h1><p id="4089" class="pw-post-body-paragraph ki kj it kk b kl nf ju kn ko ng jx kq kr nh kt ku kv ni kx ky kz nj lb lc ld im bi translated">在本教程中，我们扩展了增强学习的基本构造块的知识，在算法库中添加了梯度策略和 Actor-criterion。现代强化学习的规模要大得多，但最终它都依赖于一些类似的理念——现在你已经理解了。好样的。</p></div></div>    
</body>
</html>