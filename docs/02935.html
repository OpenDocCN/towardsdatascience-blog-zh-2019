<html>
<head>
<title>Regularization and Geometry</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化和几何</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-and-geometry-c69a2365de19?source=collection_archive---------21-----------------------#2019-05-12">https://towardsdatascience.com/regularization-and-geometry-c69a2365de19?source=collection_archive---------21-----------------------#2019-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/7c0df50e88a97fc382f2e7c58db595f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOEqrZoL1--WYv0h7WxiKQ.jpeg"/></div></div></figure><div class=""/><h1 id="f224" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">一、偏倚-方差权衡</strong></h1><p id="773c" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">当我们执行统计建模时，目标不是选择适合所有训练数据点的模型并获得训练数据的最小误差。目标是让模型能够很好地概括新的和看不见的数据。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/f5c329a7c7fed0d03e8137602f582716.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*EaLwMYXiXdB8iFQQigZPzw.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Bias and variance contributing to generalization error [1]</figcaption></figure><p id="af1c" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">随着越来越多的参数被添加到模型中，模型的复杂性增加，即，它可以拟合训练数据中的更多噪声，并导致过度拟合。这意味着我们在增加模型的方差，减少模型的偏差。从上图可以看出，如果我们不断提高模型的复杂度，泛化误差最终会超过最优点并继续增加。</p><p id="dfb0" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">总之，如果我们的模型复杂性超过了最佳点，我们就有陷入过度拟合区域的风险；而如果复杂度没有达到最佳点，我们就处于不适合区域。</p><p id="d5c5" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">正则化是一种有助于防止统计模型陷入过度拟合区域的技术。它通过阻碍模型的复杂性来做到这一点。</p><h1 id="66f7" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">二。正规化</h1><p id="6b07" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated"><strong class="ky jc">线性回归</strong></p><p id="381f" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">这是一个简单的线性回归方程。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/e9f3bd6e5f65ebc19bf5c4f255e7752a.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*TNJyNxF8HfDlv-C3uYHr0w.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Linear Regression</figcaption></figure><p id="5b45" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">我们的目标是找到最小化残差平方和(RSS)的β集。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/16bc2a9861f78ea1652b9e0d6b31f93a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*_Uj9KITIze418HxgpwTIFg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Residual Sum of Squares</figcaption></figure><p id="a3e6" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky jc"> L2 正规化</strong></p><p id="fe80" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">利用 L2 正则化的回归模型也称为岭回归。就线性回归而言，不仅仅是最小化 RSS。我们通过对测试版的约束为游戏增加了另一个元素。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/eca3fbd8faf8c163c63bb610ba59d0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*zIh6Z-PUZbJZiFbPqfmyqg.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">L1 Regularization</figcaption></figure><p id="7cf0" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">“s”可以理解为 betas 版的预算。如果你想让第一个测试变大，第二个测试必须变小，反之亦然。这变成了一场竞争，因为我们有更多的测试版，不重要的将被迫变小。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ml"><img src="../Images/3690c9ecdb37aafc8d965d392d927063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WXCYBidYnW4D8VI1A7FIzA.jpeg"/></div></div></figure><p id="9952" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">由于平方项，RSS 方程实际上给出了椭圆的几何形状。黑点是使用正规方程的贝塔集，即没有正则化。红圈包含了我们“买得起”的所有测试集。红点是岭回归的最终 betas，在我们的预算内最接近黑点。红点将总是在约束的边界上。只有当法线方程的解，即没有正则化的线性回归的解，已经满足约束时，才能在约束内获得红点。</p><p id="395b" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky jc"> L1 正规化</strong></p><p id="8319" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">利用 L1 正则化的回归模型也被称为 Lasso ( <em class="mm">最小绝对收缩和选择算子</em>)回归。套索的目标类似于山脊，只是约束变为:</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d69fa318e59a1d9fc131e7840ec7fdb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*q90E85w9ZmT_GbuY5dB25g.png"/></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">L2 Regularization</figcaption></figure><p id="4aed" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">套索还为我们提供了一个美丽的几何图形，它具有独特的性质。</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ml"><img src="../Images/3e449e703a2512a6aa84ab194813722f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WHp1RrtEUhMh-aMsd0wajQ.jpeg"/></div></div></figure><p id="a26c" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">通过 L2 正则化，我们可以“承受”的贝塔集位于一个菱形内。红点在菱形的角上，将其中一个 betas 设置为 0。</p><p id="69fc" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">如果红点在菱形的边缘，即椭圆接触到菱形的边缘，会怎么样呢？因此，两个 betas 都不会是 0。然而，二维空间中的菱形是一种特殊情况，如果红点在边缘上，则两个β都不为 0。</p><p id="7a7f" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">让我们考虑一下我们有 3 个 betas 的情况。约束的几何图形变为:</p><figure class="lv lw lx ly gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/66d578ec38cbc4f9c8791f54ef4ad509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jdVlHC6plWGdJd6iN8Q4YA.jpeg"/></div></div><figcaption class="lz ma gj gh gi mb mc bd b be z dk">Lasso with 3 betas</figcaption></figure><p id="4630" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">上图由 6 个顶点和 8 条边组成。如果红点位于边缘，一个β将被设置为 0。如果它位于一个顶点上，两个β将被设置为 0。随着尺寸的增加，顶点和边的数量也增加，使得椭圆更有可能在这些位置之一与菱形接触。也就是说，套索往往在更高的维度上工作得更好。</p><p id="0395" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated"><strong class="ky jc">L1 和 L2 正规化的差异</strong></p><p id="d041" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">岭回归是线性回归的一种扩展，它将贝塔系数设置得很小，从而减少了不相关特征的影响。这样，统计模型将不适合训练数据中的所有噪声，并且落入过拟合区域。</p><p id="3d68" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">套索回归带来了一些独特的属性表，因为它的美丽的几何图形。一些测试将被设置为 0，给我们一个稀疏的输出。我们也可以使用套索进行特征选择。虽然诸如<a class="ae mp" href="https://www.statisticssolutions.com/best-subsets-regression/" rel="noopener ugc nofollow" target="_blank">最佳子集</a>、<a class="ae mp" href="https://gerardnico.com/data_mining/stepwise_regression" rel="noopener ugc nofollow" target="_blank">向前逐步</a>或<a class="ae mp" href="https://gerardnico.com/data_mining/stepwise_regression" rel="noopener ugc nofollow" target="_blank">向后逐步</a>的特征选择技术可能是时间低效的，但是 Lasso 将更快地收敛到最终解。</p><h1 id="ee57" class="jy jz jb bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak">三世。结论</strong></h1><p id="21cc" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">具有高复杂性的统计模型可能易于过度拟合。在本文中，我介绍了两种正则化技术来阻止模型拟合训练数据中的所有噪声。而且，我借助几何解释了它们的性质和区别。</p><p id="22e1" class="pw-post-body-paragraph kw kx jb ky b kz md lb lc ld me lf lg lh mf lj lk ll mg ln lo lp mh lr ls lt ij bi translated">我的文章到此结束！祝大家有美好的一天:)</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h2 id="48c8" class="mx jz jb bd ka my mz dn ke na nb dp ki lh nc nd km ll ne nf kq lp ng nh ku ni bi translated"><strong class="ak">图像:</strong></h2><p id="29e0" class="pw-post-body-paragraph kw kx jb ky b kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt ij bi translated">[1]丹尼尔·桑德斯，<a class="ae mp" href="https://djsaunde.wordpress.com/2017/07/17/the-bias-variance-tradeoff/" rel="noopener ugc nofollow" target="_blank">偏差-方差权衡</a> (2017)</p></div></div>    
</body>
</html>