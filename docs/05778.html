<html>
<head>
<title>Deep Learning from Scratch and Using Tensorflow in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始深度学习并在 Python 中使用 Tensorflow</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-from-scratch-and-using-tensorflow-in-python-34aad75f939?source=collection_archive---------11-----------------------#2019-08-23">https://towardsdatascience.com/deep-learning-from-scratch-and-using-tensorflow-in-python-34aad75f939?source=collection_archive---------11-----------------------#2019-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="d4ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">深度学习是目前在现实世界、数据科学应用中使用的最流行的模型之一。在从图像到文本到语音/音乐的各个领域，这都是一个有效的模式。随着其使用的增加，快速和可扩展地实现深度学习的能力变得至关重要。Tensorflow 等深度学习平台的兴起，帮助开发者以更简单的方式实现他们需要的东西。</p><p id="b606" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在本文中，我们将了解深度学习是如何工作的，并熟悉它的术语——如反向传播和批量大小。我们将为 Python 中预定义的输入和输出实现一个简单的深度学习模型——从理论到零实现——然后使用 Keras 和 Tensorflow 等深度学习平台做同样的事情。我们使用 Keras 和 tensor flow 1 . x 版和 2.0 版编写了这个简单的深度学习模型，具有三个不同的复杂程度和易于编码的程度。</p><h2 id="699d" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated"><strong class="ak">深度学习从无到有的实现</strong></h2><p id="70ab" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">考虑一个简单的多层感知器，它有四个输入神经元，一个隐藏层有三个神经元，一个输出层有一个神经元。我们有三个数据样本用于表示为 X 的输入，三个数据样本用于表示为 yt 的期望输出。因此，每个输入数据样本有四个特征。</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="9144" class="ko kp it lr b gy lv lw l lx ly"># Inputs and outputs of the neural net:<br/>import numpy as np</span><span id="5ff9" class="ko kp it lr b gy lz lw l lx ly">X=np.array([[1.0, 0.0, 1.0, 0.0],[1.0, 0.0, 1.0, 1.0],[0.0, 1.0, 0.0, 1.0]])<br/>yt=np.array([[1.0],[1.0],[0.0]])</span></pre><figure class="lm ln lo lp gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi ma"><img src="../Images/b5eaa4c96736e7c4d5556b4cd1435f04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKx5RnCsEKHFNxwO1LEPeg.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Neural Network with four input neurons, one hidden layer with three neurons and an output layer with one neuron</figcaption></figure><p id="3f0f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图中的<strong class="js iu"> <em class="mm"> x </em> </strong> <em class="mm"> (m) </em>为<strong class="js iu"> X </strong>，<strong class="js iu"> <em class="mm"> h </em> </strong> (m)为输入<strong class="js iu"><em class="mm">X</em></strong><em class="mm">(m)<strong class="js iu">W</strong><em class="mm">I</em>和<strong class="js iu"> W </strong> <em class="mm"> h </em></em></p><p id="5bb8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">神经网络(NN)的目标是获得权重和偏差，以便对于给定的输入，NN 提供期望的输出。但是，我们事先不知道适当的权重和偏差，所以我们更新权重和偏差，使得 NN、<em class="mm"> yp(m) </em>和期望的输出、<em class="mm"> yt(m) </em>之间的误差最小。这个迭代最小化过程被称为 NN 训练。</p><p id="4cac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设隐藏层和输出层的激活函数都是 sigmoid 函数。因此，</p><figure class="lm ln lo lp gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mn"><img src="../Images/6a37a674452c4d1f9063d10c68b7d8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vhaR3D5glDOH-6aPSiUKDw.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The size of weights, biases and the relationships between input and outputs of the neural net</figcaption></figure><p id="c4e2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其中激活函数是 sigmoid，<em class="mm"> m </em>是第<em class="mm"> m </em>个数据样本，<em class="mm"> yp(m) </em>是 NN 输出。</p><p id="b84e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">误差函数测量神经网络的输出与期望输出之间的差异，可以用数学方法表示为:</p><figure class="lm ln lo lp gt mb gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/33fcc8837ce3d086909040404c549624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*q-KfJ8oPioHZzBFmGlLVsw.png"/></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">The Error defined for the neural net which is squared error</figcaption></figure><p id="6e0c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上述 NN 的伪代码总结如下:</p><figure class="lm ln lo lp gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mp"><img src="../Images/a01cc273a74d238d4b3b185b2fe15f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6zmwq6fP1kq546z1oBFdQ.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">pseudocode for the neural net training</figcaption></figure><p id="2440" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从我们的伪代码中，我们认识到应该计算误差(E)相对于参数(权重和偏差)的偏导数。使用微积分中的<a class="ae mq" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链式法则</a>,我们可以写出:</p><figure class="lm ln lo lp gt mb gh gi paragraph-image"><div role="button" tabindex="0" class="mc md di me bf mf"><div class="gh gi mr"><img src="../Images/4f6492f571bd669b61f60b68eb5821f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JkSu4tDUcxhHNIgfZVMUBA.png"/></div></div><figcaption class="mi mj gj gh gi mk ml bd b be z dk">Derivative of Error function with respect to the weights</figcaption></figure><p id="824d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里我们有两个选项来更新反向路径中的权重和偏差(反向路径意味着更新权重和偏差，使得误差最小化):</p><ol class=""><li id="071a" class="ms mt it js b jt ju jx jy kb mu kf mv kj mw kn mx my mz na bi translated">使用训练数据的所有<em class="mm"> N </em>个样本</li><li id="5bb0" class="ms mt it js b jt nb jx nc kb nd kf ne kj nf kn mx my mz na bi translated">使用一个样本(或几个样本)</li></ol><p id="7070" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于第一个，我们说批量大小是<em class="mm"> N </em>。对于第二种情况，如果使用一个样本来更新参数，我们说批量大小为 1。因此，批量意味着有多少数据样本被用于更新权重和偏差。</p><p id="a3b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">你可以找到上述神经网络的实现，其中误差相对于参数的梯度被象征性地计算，具有不同的批量大小<a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/NN_from_scratch_different_batch.ipynb" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><p id="59a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">从上面的例子可以看出，从头开始创建一个简单的深度学习模型涉及到非常复杂的方法。在下一节中，我们将看到深度学习框架如何帮助我们的模型实现可扩展性和更大的易用性。</p><h2 id="53a4" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated"><strong class="ak">使用 Keras、Tensorflow 1.x 和 2.0 的深度学习实现</strong></h2><p id="80bd" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">在上一节中，我们使用链式法则计算了误差 w.r.t .参数的梯度。我们亲眼看到这不是一个简单或可扩展的方法。此外，请记住，我们在每次迭代中计算偏导数，因此不需要符号梯度，尽管它的值很重要。这就是深度学习框架如<a class="ae mq" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>和<a class="ae mq" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>可以发挥作用的地方。深度学习框架使用自动 Diff 方法进行部分梯度的数值计算。如果你不熟悉 AutoDiff，StackExchange 有一个很好的例子<a class="ae mq" href="https://stats.stackexchange.com/questions/224140/step-by-step-example-of-reverse-mode-automatic-differentiation" rel="noopener ugc nofollow" target="_blank">来介绍一下。</a></p><p id="6df4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自动 Diff 将复杂的表达式分解成一组原始表达式，即最多由单个函数调用组成的表达式。由于每个单独表达式的微分规则都是已知的，所以可以用有效的方式计算出最终结果。</p><p id="7cdd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们在 Keras、Tensorflow 1.x 和 Tensorflow 2.0 中实现了具有三个不同级别的 NN 模型:</p><p id="2596" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 1-高级(Keras 和 Tensorflow 2.0): </strong> <a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/TF_v2_HighLevel_batchsize1_train_on_batch.py" rel="noopener ugc nofollow" target="_blank">批量为 1 的高级 tensor flow 2.0</a></p><p id="b086" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 2-中级(Tensorflow 1.x 和 2.0): </strong> <a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/TF_v1_MediumLevel_batchsize1.py" rel="noopener ugc nofollow" target="_blank">中级 Tensorflow 1.x 批量 1 </a>，<a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/TF_v1_MediumLevel_batchsizeN.py" rel="noopener ugc nofollow" target="_blank">中级 Tensorflow 1.x 批量 N </a>，<a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/TF_v2_MediumLevel_batchsize1.py" rel="noopener ugc nofollow" target="_blank">中级 Tensorflow 2.0 批量 1 </a>，<a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/TF_v2_MediumLevel_batchsizeN.py" rel="noopener ugc nofollow" target="_blank">中级 Tensorflow v 2.0 批量 N </a></p><p id="5d06" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> 3-低级(Tensorflow 1.x): </strong> <a class="ae mq" href="https://github.com/miladtoutounchian/Deep-Learning-/blob/master/TF_v1_LowLevel_batchsizeN.py" rel="noopener ugc nofollow" target="_blank">批量为 N 的低级 tensor flow 1 . x</a></p><h2 id="2a15" class="ko kp it bd kq kr ks dn kt ku kv dp kw kb kx ky kz kf la lb lc kj ld le lf lg bi translated">代码片段:</h2><p id="9eb3" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">对于高层，我们使用 Keras 和 Tensorflow v 2.0 通过<em class="mm"> model.train_on_batch </em>完成了实现:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="9aaf" class="ko kp it lr b gy lv lw l lx ly"># High-Level implementation of the neural net in Tensorflow:<br/>model.compile(loss=mse, optimizer=optimizer)<br/><strong class="lr iu">for</strong> _ <strong class="lr iu">in</strong> range(2000):<br/>    <strong class="lr iu">for</strong> step, (x, y) <strong class="lr iu">in</strong> enumerate(zip(X_data, y_data)):<br/>        model.train_on_batch(np.array([x]), np.array([y]))</span></pre><p id="e666" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在使用 Tensorflow 1.x 的中级中，我们定义了:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="4061" class="ko kp it lr b gy lv lw l lx ly">E = tf.reduce_sum(tf.pow(ypred - Y, 2))<br/>optimizer = tf.train.GradientDescentOptimizer(0.1)<br/>grads = optimizer.compute_gradients(E, [W_h, b_h, W_o, b_o])<br/>updates = optimizer.apply_gradients(grads)</span></pre><p id="4f60" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这确保了在循环的<em class="mm">中，更新变量将被更新。对于中级，梯度和它们的更新在 for_loop 外部定义，而在 for_loop 更新内部迭代更新。在中级使用 Tensorflow v 2.x 中，我们使用了:</em></p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="e799" class="ko kp it lr b gy lv lw l lx ly"># Medium-Level implementation of the neural net in Tensorflow</span><span id="e81e" class="ko kp it lr b gy lz lw l lx ly"><strong class="lr iu"># </strong>In for_loop</span><span id="7b9d" class="ko kp it lr b gy lz lw l lx ly"><strong class="lr iu">with</strong> tf.GradientTape() <strong class="lr iu">as</strong> tape:<br/>   x = tf.convert_to_tensor(np.array([x]), dtype=tf.float64)<br/>   y = tf.convert_to_tensor(np.array([y]), dtype=tf.float64)<br/>   ypred = model(x)<br/>   loss = mse(y, ypred)<br/>gradients = tape.gradient(loss, model.trainable_weights)<br/>optimizer.apply_gradients(zip(gradients, model.trainable_weights))</span></pre><p id="2159" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在低级实现中，每个权重和偏差被单独更新。在使用 Tensorflow v 1.x 的低级中，我们定义了:</p><pre class="lm ln lo lp gt lq lr ls lt aw lu bi"><span id="ece4" class="ko kp it lr b gy lv lw l lx ly"># Low-Level implementation of the neural net in Tensorflow:<br/>E = tf.reduce_sum(tf.pow(ypred - Y, 2))<br/>dE_dW_h = tf.gradients(E, [W_h])[0]<br/>dE_db_h = tf.gradients(E, [b_h])[0]<br/>dE_dW_o = tf.gradients(E, [W_o])[0]<br/>dE_db_o = tf.gradients(E, [b_o])[0]<br/># In for_loop:<br/>evaluated_dE_dW_h = sess.run(dE_dW_h,<br/>                                     feed_dict={W_h: W_h_i, b_h: b_h_i, W_o: W_o_i, b_o: b_o_i, X: X_data.T, Y: y_data.T})<br/>        W_h_i = W_h_i - 0.1 * evaluated_dE_dW_h<br/>        evaluated_dE_db_h = sess.run(dE_db_h,<br/>                                     feed_dict={W_h: W_h_i, b_h: b_h_i, W_o: W_o_i, b_o: b_o_i, X: X_data.T, Y: y_data.T})<br/>        b_h_i = b_h_i - 0.1 * evaluated_dE_db_h<br/>        evaluated_dE_dW_o = sess.run(dE_dW_o,<br/>                                     feed_dict={W_h: W_h_i, b_h: b_h_i, W_o: W_o_i, b_o: b_o_i, X: X_data.T, Y: y_data.T})<br/>        W_o_i = W_o_i - 0.1 * evaluated_dE_dW_o<br/>        evaluated_dE_db_o = sess.run(dE_db_o,<br/>                                     feed_dict={W_h: W_h_i, b_h: b_h_i, W_o: W_o_i, b_o: b_o_i, X: X_data.T, Y: y_data.T})<br/>        b_o_i = b_o_i - 0.1 * evaluated_dE_db_o</span></pre><p id="6677" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如你在上面的低级实现中看到的，开发者对数字操作和计算的每一步都有更多的控制。</p><h1 id="f55b" class="ng kp it bd kq nh ni nj kt nk nl nm kw nn no np kz nq nr ns lc nt nu nv lf nw bi translated"><strong class="ak">结论</strong></h1><p id="664e" class="pw-post-body-paragraph jq jr it js b jt lh jv jw jx li jz ka kb lj kd ke kf lk kh ki kj ll kl km kn im bi translated">我们现在已经表明，通过使用用于权重和偏差更新的符号梯度计算，从零开始实现即使是简单的深度学习模型也不是一种容易或可扩展的方法。由于使用了 AutoDiff，使用深度学习框架加速了这一过程，AutoDiff 基本上是用于更新权重和偏差的稳定的数值梯度计算。</p></div></div>    
</body>
</html>