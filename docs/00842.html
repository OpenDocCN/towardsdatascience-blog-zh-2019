<html>
<head>
<title>Conventional Methods for Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">传统的文本分类方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061?source=collection_archive---------13-----------------------#2019-02-08">https://towardsdatascience.com/nlp-learning-series-part-2-conventional-methods-for-text-classification-40f2839dd061?source=collection_archive---------13-----------------------#2019-02-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1e43" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/nlp-learning-series" rel="noopener" target="_blank"> NLP 学习系列</a>(第二部分)</h2><div class=""/><div class=""><h2 id="60c4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">教机器学习文本</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d06ca77beeb3ad3d1298b3ae21b22732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZSySg_VluEb68aE8"/></div></div></figure><p id="ccaf" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这是 NLP 文本分类系列的第二篇文章。给你一个回顾，最近我在 Kaggle 上开始了一个 NLP 文本分类比赛，叫做 Quora 问题不真诚挑战。我想通过一系列关于文本分类的博客文章来分享这些知识。<a class="ae lz" href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/" rel="noopener ugc nofollow" target="_blank">的第一篇帖子</a>谈到了各种与深度学习模型和<strong class="lf jd">一起工作的<strong class="lf jd">预处理技术，增加了嵌入覆盖率</strong>。在这篇文章中，我将尝试带你了解一些基本的传统模型，如 TFIDF、计数矢量器、哈希等。并尝试评估它们的性能以创建一个基线。我们将在<a class="ae lz" rel="noopener" target="_blank" href="/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566">第三篇文章</a>中更深入地研究<strong class="lf jd">深度学习模型</strong>，这些模型将专注于解决文本分类问题的不同架构。我们将尝试使用我们在本次比赛中未能使用的各种其他模型，如系列文章第四篇中的<strong class="lf jd"> ULMFit 迁移学习</strong>方法。</strong></p><p id="2e16" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">作为旁注</strong>:如果你想了解更多关于 NLP 的知识，我想<strong class="lf jd">推荐<a class="ae lz" href="https://www.coursera.org/specializations/aml?siteID=lVarvwc5BD0-AqkGMb7JzoCMW0Np1uLfCA&amp;utm_content=2&amp;utm_medium=partners&amp;utm_source=linkshare&amp;utm_campaign=lVarvwc5BD0" rel="noopener ugc nofollow" target="_blank">高级机器学习专精</a>中关于<a class="ae lz" href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>的这门超棒的课程</strong>。您可以免费开始 7 天的免费试用。本课程涵盖了自然语言处理中从基础到高级的各种任务:情感分析、摘要、对话状态跟踪等等。您可以免费开始 7 天的免费试用。普通</p><p id="d294" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我可能需要一点时间来写完整个系列。在那之前，你也可以看看我的其他帖子:【Kagglers 正在使用什么进行文本分类，它谈论了 NLP 中使用的各种深度学习模型和<a class="ae lz" href="https://mlwhiz.com/blog/2019/01/06/pytorch_keras_conversion/" rel="noopener ugc nofollow" target="_blank">如何从 Keras 切换到 Pytorch </a>。</p><p id="7693" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">所以我们再次从第一步开始:预处理。</p><h1 id="aa71" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">文本数据的基本预处理技术(续)</h1><p id="9e9f" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">所以在上一篇文章中，我们讨论了各种用于深度学习的文本预处理方法。传统方法的大部分预处理保持不变。<strong class="lf jd">我们仍然会删除特殊字符、标点符号和缩写</strong>。但是当涉及到传统方法时，我们可能也想做词干化/词汇化。让我们来谈谈它们。</p><blockquote class="mx my mz"><p id="243d" class="ld le na lf b lg lh kd li lj lk kg ll nb ln lo lp nc lr ls lt nd lv lw lx ly im bi translated">出于语法原因，文档将使用不同形式的单词，如 organize、organize 和 organizing。此外，还有一系列具有相似含义的衍生相关词，如民主、民主和民主化。</p></blockquote><p id="d1a0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于我们将在特征创建步骤中为单词创建特征，因此有必要将单词简化为一个共同的标准，以便“组织”、“组织”和“组织”可以由单个单词“组织”来引用</p><h1 id="5fb7" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">a)词干</h1><p id="39ac" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">词干提取是使用粗略的启发式规则将单词转换成其基本形式的过程。例如，一个规则可以是从任何单词的末尾去掉“s”，这样“cats”就变成了“cat”。或者另一个规则是用“我”代替“ies ”,这样“ponies”就变成了“poni”。这里要注意的一个要点是，当我们对这个单词进行词干处理时，我们可能会得到一个像“poni”这样的无意义的单词。但是它仍然适用于我们的用例，因为我们计算特定单词的出现次数，而不是像传统方法那样关注这些单词的含义。出于完全相同的原因，它对深度学习不起作用。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3f89d919c414229926b1c92343a224e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/0*LooZiyuyn4yhTC5k.png"/></div></figure><p id="88ed" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">通过在 python 中使用这个函数，我们可以非常简单地做到这一点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h1 id="f8db" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">b)词汇化</h1><p id="5bc8" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">词汇化与词干化非常相似，但它的目的是仅当字典中存在基本形式时才删除词尾。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="6836" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一旦我们完成了对文本的处理，我们的文本必然会经历以下这些步骤。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><h1 id="010d" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">文本表示</h1><p id="0f03" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">在传统的机器学习方法中，我们应该为文本创建特征。为了实现这一目标，有很多的表现形式。让我们一个一个来说。</p><h1 id="14cb" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">a)单词包—计数矢量器功能</h1><p id="cf3c" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">假设我们有一系列的句子(文档)</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="80d3" class="nm mb it ni b gy nn no l np nq">X = [ 'This is good', 'This is bad', 'This is awesome' ]</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/ef9e0302ba4b01040e22bfb62965d5d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zi35X0t8SwIx8mas.png"/></div></div></figure><p id="2e81" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">单词包将创建一个所有句子中最常用单词的字典。对于上面的例子，字典应该是这样的:</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="cf52" class="nm mb it ni b gy nn no l np nq">word_index {'this':0,'is':1,'good':2,'bad':3,'awesome':4}</span></pre><p id="190c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后用上面的字典对句子进行编码。</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="9777" class="nm mb it ni b gy nn no l np nq">This is good - [1,1,1,0,0] <br/>This is bad - [1,1,0,1,0] <br/>This is awesome - [1,1,0,0,1]</span></pre><p id="634f" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在 Python 中，我们可以通过使用 Python 中的 CountVectorizer 类非常简单地做到这一点。不用太担心名字重，它只是做了我上面解释的事情。它有许多参数，其中最重要的是:</p><ul class=""><li id="3d29" class="ns nt it lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated"><strong class="lf jd"> ngram_range: </strong>我在代码(1，3)中指定。这意味着在创建要素时，将考虑单字、双字和三元字。</li><li id="2a85" class="ns nt it lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated"><strong class="lf jd"> min_df: </strong>一个 ngram 应该出现在语料库中作为特征使用的最小次数。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="08b6" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">然后，我们可以将这些特征用于任何机器学习分类模型，如逻辑回归、朴素贝叶斯、SVM 或 LightGBM。例如:</p><pre class="ks kt ku kv gt nh ni nj nk aw nl bi"><span id="87a2" class="nm mb it ni b gy nn no l np nq"># Fitting a simple Logistic Regression on CV Feats <br/>clf = LogisticRegression(C=1.0) <br/>clf.fit(xtrain_cntv,y_train)</span></pre><p id="9c01" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个内核链接，我在 Quora 数据集上测试了这些特性。如果你喜欢，请不要忘记投赞成票。</p><h1 id="29a1" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">b) TFIDF 功能</h1><p id="c4b0" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">TFIDF 是一种从句子中寻找特征的简单技术。在计数特征中，我们计算文档中出现的所有单词/ngrams，而在 TFIDF 中，我们只计算重要单词的特征。我们如何做到这一点？如果你想一个语料库中的文档，我们将考虑关于该文档中任何单词的两件事:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi og"><img src="../Images/e4a9955c7df973c0df8179a34d3960d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b1c9_SLu2iKji23S.png"/></div></div></figure><ul class=""><li id="3aa7" class="ns nt it lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated"><strong class="lf jd">词频:</strong>这个词在文档中有多重要？</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oh"><img src="../Images/4359c4ec275c4e4846f43076f8e8e849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9V9xptg-1WJfwz097aLrug.png"/></div></div></figure><ul class=""><li id="58a5" class="ns nt it lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated"><strong class="lf jd">逆文档频率:</strong>术语在整个语料库中的重要性如何？</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/5d608eaf7eab72d85e0692782c405673.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpIDshym1LBXizdrpkJDtQ.png"/></div></div></figure><p id="bebc" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">TFIDF 就是这两个分数的乘积。</p><p id="f02c" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">直观上，一个人可以理解，如果一个单词在文档中出现多次，它就是重要的。但是这就产生了一个问题。像“a”、“the”这样的词在句子中多次出现。他们的 TF 分会一直很高。我们通过使用逆文档频率来解决这个问题，如果这个词很罕见，那么它就很高，如果这个词在整个语料库中很常见，那么它就很低。</p><p id="def0" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本质上，我们希望在文档中找到不常见的重要单词。</p><p id="2fc2" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在 Python 中，我们可以通过使用 Python 中的 TFIDFVectorizer 类非常简单地做到这一点。它有许多参数，其中最重要的是:</p><ul class=""><li id="364d" class="ns nt it lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated"><strong class="lf jd"> ngram_range: </strong>我在代码(1，3)中指定。这意味着在创建要素时，将考虑单字、双字和三元字。</li><li id="707c" class="ns nt it lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated"><strong class="lf jd"> min_df: </strong>一个 ngram 应该出现在语料库中作为特征使用的最小次数。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="d3de" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">同样，我们可以将这些特征用于任何机器学习分类模型，如逻辑回归、朴素贝叶斯、SVM 或 LightGBM。<a class="ae lz" href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个内核的链接，我在 Quora 数据集上尝试了这些特性。如果你喜欢，请不要忘记投赞成票。</p><h1 id="b8a0" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">c)散列特征</h1><p id="2542" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">通常在一个文档语料库中会有很多元语法。我们的 tfidf 矢量器生成的特征数量超过了 2，000，000 个。这可能会在非常大的数据集上导致一个问题，因为我们必须在内存中保存一个非常大的词汇字典。解决这个问题的一个方法是使用哈希技巧。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oj"><img src="../Images/632271a7da445dfa85eadb1331d799a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7Pd_iPjy2psZnc-g.png"/></div></div></figure><p id="b609" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">人们可以将散列看作是将任何 ngram 映射到一个数字范围(例如 0 到 1024 之间)的单个函数。现在我们不必把语法存储在字典里。我们可以使用函数来获取任何单词的索引，而不是从字典中获取索引。</p><p id="883e" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">由于可能有超过 1024 个 n gram，不同的 n gram 可能映射到同一个数字，这称为冲突。我们提供的散列函数的范围越大，冲突的机会就越少。</p><p id="e376" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">在 Python 中，我们可以通过使用 Python 中的 HashingVectorizer 类非常简单地做到这一点。它有许多参数，其中最重要的是:</p><ul class=""><li id="c09e" class="ns nt it lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated"><strong class="lf jd"> ngram_range: </strong>我在代码(1，3)中指定。这意味着在创建要素时，将考虑单字、双字和三元字。</li><li id="c8e8" class="ns nt it lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated"><strong class="lf jd"> n_features: </strong>你要考虑的特性的数量。上面我给的范围。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="5815" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">这里的是一个内核链接，我在 Quora 数据集上测试了这些特性。如果你喜欢，请不要忘记投赞成票。</p><h1 id="0bdc" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">d) Word2vec 功能</h1><p id="7508" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">在之前的帖子中，我们已经谈了一些关于 word2vec 的内容。我们也可以使用单词到 vec 特征来创建句子级别的专长。我们想为句子创建一个<code class="fe ok ol om ni b">d</code>维向量。为此，我们将简单地对一个句子中所有单词的单词嵌入进行平均。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/f6d903bb8d2ff8161a153d007edf6b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UUJxq22ya4Rh5_Du.png"/></div></div></figure><p id="42e7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们可以在 Python 中使用以下函数来实现这一点。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nf ng l"/></div></figure><p id="6175" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/" rel="noopener ugc nofollow" target="_blank">这里的</a>是一个内核的链接，我在 Quora 数据集上测试了这些特性。如果你喜欢，请不要忘记投赞成票。</p><h1 id="81d2" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">结果</h1><p id="334c" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">以下是不同方法在 Kaggle 数据集上的结果。我做了一份五层简历。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/4ef39f357051cef518d67a54947ce6fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ryAIE0riMM3IKY3q.png"/></div></div></figure><p id="0578" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" href="https://www.kaggle.com/mlwhiz/conventional-methods-for-quora-classification/" rel="noopener ugc nofollow" target="_blank">这里的</a>是代码。如果你喜欢，请不要忘记投赞成票。还要注意，我没有对模型进行调优，所以这些结果只是粗略的。您可以尝试通过使用 hyperopt 执行超参数调整<a class="ae lz" href="https://mlwhiz.com/blog/2017/12/28/hyperopt_tuning_ml_model/" rel="noopener ugc nofollow" target="_blank">或只是老式的网格搜索来获得更多性能，之后模型的性能可能会发生显著变化。</a></p><h1 id="d491" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">结论</h1><p id="7d1a" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">虽然深度学习在 NLP 分类任务中表现得更好，但了解这些问题在过去是如何解决的仍然是有意义的，这样我们就可以了解问题的本质。我试图提供一个关于传统方法的视角，人们应该在转向深度学习方法之前也用它们进行实验来创建基线。如果你想进一步了解 NLP  <a class="ae lz" href="https://click.linksynergy.com/link?id=lVarvwc5BD0&amp;offerid=467035.11503135394&amp;type=2&amp;murl=https%3A%2F%2Fwww.coursera.org%2Flearn%2Flanguage-processing" rel="noopener ugc nofollow" target="_blank">，这里</a>是一门很棒的课程。您可以免费开始 7 天的免费试用。如果你认为我可以为这个流程添加一些东西，请在评论中提及。</p><h1 id="a59c" class="ma mb it bd mc md me mf mg mh mi mj mk ki ml kj mm kl mn km mo ko mp kp mq mr bi translated">尾注和参考文献</h1><p id="a4d6" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated">这篇文章是许多优秀的 Kagglers 们努力的结果，我将在这一部分尝试引用他们。如果我漏掉了某个人，请理解我不是故意的。</p><ul class=""><li id="e6a6" class="ns nt it lf b lg lh lj lk lm nu lq nv lu nw ly nx ny nz oa bi translated"><a class="ae lz" href="https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">在 Kaggle 上逼近(几乎)任何 NLP 问题</strong> </a></li><li id="55c6" class="ns nt it lf b lg ob lj oc lm od lq oe lu of ly nx ny nz oa bi translated"><a class="ae lz" href="https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings" rel="noopener ugc nofollow" target="_blank"> <strong class="lf jd">如何:使用嵌入时的预处理</strong> </a></li></ul></div><div class="ab cl op oq hx or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="im in io ip iq"><p id="0d65" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><a class="ae lz" rel="noopener" target="_blank" href="/nlp-learning-series-part-3-attention-cnn-and-what-not-for-text-classification-4313930ed566"> <em class="na">这里的</em> </a> <em class="na">是下一个帖子。</em></p></div></div>    
</body>
</html>