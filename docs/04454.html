<html>
<head>
<title>A Running Speed Benchmark for Kernel &amp; Non-Kernel Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">内核和非内核算法的运行速度基准</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-running-speed-benchmark-for-kernel-non-kernel-algorithms-b7db5bfabcef?source=collection_archive---------28-----------------------#2019-07-09">https://towardsdatascience.com/a-running-speed-benchmark-for-kernel-non-kernel-algorithms-b7db5bfabcef?source=collection_archive---------28-----------------------#2019-07-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="73a1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">非内核算法总是比基于内核的算法快吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f90d16cbc6ffe2c29cc5159c0a309944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uPJ6a5BmGM02zDmi.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The Kernel method, Wikipedia, <a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0" rel="noopener ugc nofollow" target="_blank">CC BY-SA 4.0</a>, <a class="ae ky" href="https://en.wikipedia.org/wiki/Kernel_method" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Kernel_method</a></figcaption></figure><p id="0c0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多年来，我知道线性 SVM 比内核版本快，这是常识，而不是我测试过的东西。不久前，我想知道是否所有基于内核的算法都比它们的非内核版本慢。下面的比较用作几个众所周知的算法的运行时间基准。</p><blockquote class="lv lw lx"><p id="19d7" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">内核方法因使用了<a class="ae ky" href="https://en.wikipedia.org/wiki/Positive-definite_kernel" rel="noopener ugc nofollow" target="_blank">内核函数</a>而得名，这使得它们能够在高维、<em class="it">隐式</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Feature_space" rel="noopener ugc nofollow" target="_blank">特征空间</a>中操作，而无需计算该空间中数据的坐标，而是简单地计算特征空间中所有数据对的图像之间的<a class="ae ky" href="https://en.wikipedia.org/wiki/Inner_product" rel="noopener ugc nofollow" target="_blank">内积</a>。该操作通常比坐标的显式计算在计算上更便宜。这种方法被称为“<strong class="lb iu">内核技巧</strong>”——来自维基百科</p></blockquote><p id="1575" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">想直接跑这个基准的可以在我的 <a class="ae ky" href="https://github.com/orico/KernelAlgorithms/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Github </strong> </a>里找<strong class="lb iu">笔记本。用于该基准的数据集是乳腺癌数据集，它很小，但众所周知。你可以用其他数据集做实验，看看实证结果是否保持不变，我怀疑会的。</strong></p><h2 id="9b8f" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">我们将比较以下算法:</h2><ol class=""><li id="57c6" class="mv mw it lb b lc mx lf my li mz lm na lq nb lu nc nd ne nf bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" rel="noopener ugc nofollow" target="_blank">线性 SVM </a> vs <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">线性核 SVM </a></li><li id="8c9f" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> PCA </a> vs 线性<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html" rel="noopener ugc nofollow" target="_blank">核 PCA </a></li><li id="73b0" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> K-means </a> vs <a class="ae ky" href="https://tslearn.readthedocs.io/en/latest/gen_modules/clustering/GlobalAlignmentKernelKMeans/tslearn.clustering.GlobalAlignmentKernelKMeans.fit_predict.html" rel="noopener ugc nofollow" target="_blank">内核 K-means </a></li><li id="ed02" class="mv mw it lb b lc ng lf nh li ni lm nj lq nk lu nc nd ne nf bi translated"><a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html" rel="noopener ugc nofollow" target="_blank"> PDF </a> vs <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html" rel="noopener ugc nofollow" target="_blank"> KDE </a>(高斯)vs <a class="ae ky" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gaussian_kde.html" rel="noopener ugc nofollow" target="_blank">高斯 KDE </a></li></ol><p id="1c67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种比较是双重的，一次是“适合”，另一次是“预测”或“转换”。请注意，我找不到概率密度函数(PDF)的等效转换方法。支持向量机(SVM)、主成分分析(PCA)、核密度估计(KDE)和 K-means 是众所周知的，并且具有相关的功能，对于核-K-means，我必须查看一个名为<a class="ae ky" href="https://tslearn.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Tslearn </a>的外部包。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/6a3d967489f4f78da65a9806de70f3aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*QLKxgdlomCQ--NkT1NyDXw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Table 1: Running time for ‘fit’</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/8ca52bc5f5747983f8d7d07b857047a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*DROs_MDXOxAbrkxWNUMHtA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Table 2: Running time for ‘predict’ or ‘transform’</figcaption></figure><h2 id="83d2" class="mc md it bd me mf mg dn mh mi mj dp mk li ml mm mn lm mo mp mq lq mr ms mt mu bi translated">结果和讨论</h2><p id="d42b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">这两种类型之间似乎有很大的差异，对于“fit”(表 1)，值得注意的是线性 SVM 比它的内核对应物快两倍，PCA 快大约 30 倍，K-means 快大约 40 倍。对于“预测”(表 2)或“转换”，我们看到的是相同的画面，但比例不同。速度差异可能有多种原因。如果你感兴趣的话，值得检查一下每个算法的实现和时间复杂度，这里没有涉及。</p><p id="d27d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除非你真的需要某个算法的内核版本，否则应该首先考虑非内核版本。值得注意的是，最快的版本存在于 Sklearn 中，这意味着您不需要查看其他包。</p></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="398e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Ori Cohen 博士拥有计算机科学博士学位，专注于机器学习。他领导着 Zencity.io 的研究团队，试图积极影响市民的生活。</p></div></div>    
</body>
</html>