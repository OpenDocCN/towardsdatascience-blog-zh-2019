<html>
<head>
<title>Linear Regression with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch 线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817?source=collection_archive---------2-----------------------#2019-04-20">https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817?source=collection_archive---------2-----------------------#2019-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="69c4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">线性回归是一种方法，它试图通过最小化距离来找到因变量和自变量<strong class="jp ir">之间的线性关系，如下所示。</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/aaebc4e188a24bea502d3438296d7450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4JzgiTIUfFePLBj4A_JPw.jpeg"/></div></div><figcaption class="kx ky gj gh gi kz la bd b be z dk">Taken from <a class="ae lb" href="https://www.youtube.com/watch?v=zPG4NjIkCjc" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=zPG4NjIkCjc</a></figcaption></figure><p id="9d5b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将展示如何使用 PyTorch 实现一个简单的线性回归模型<strong class="jp ir">。</strong></p><p id="4ed1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们考虑一个非常基本的线性方程，即<strong class="jp ir"> y=2x+1 </strong>。这里，‘x’是自变量，y 是因变量。我们将使用此等式创建一个虚拟数据集，用于训练此线性回归模型。下面是创建数据集的代码。</p><pre class="km kn ko kp gt lc ld le lf aw lg bi"><span id="ace7" class="lh li iq ld b gy lj lk l ll lm">import numpy as np</span><span id="9577" class="lh li iq ld b gy ln lk l ll lm"><em class="lo"># create dummy data for training<br/></em>x_values = [i <strong class="ld ir">for </strong>i <strong class="ld ir">in </strong>range(11)]<br/>x_train = np.array(x_values, dtype=np.float32)<br/>x_train = x_train.reshape(-1, 1)<br/><br/>y_values = [2*i + 1 <strong class="ld ir">for </strong>i <strong class="ld ir">in </strong>x_values]<br/>y_train = np.array(y_values, dtype=np.float32)<br/>y_train = y_train.reshape(-1, 1)</span></pre><p id="2fc9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们创建了数据集，我们就可以开始为我们的模型编写代码。首先要定义模型架构。我们使用下面这段代码来实现。</p><pre class="km kn ko kp gt lc ld le lf aw lg bi"><span id="88c8" class="lh li iq ld b gy lj lk l ll lm">import torch<br/>from torch.autograd import Variable</span><span id="e35c" class="lh li iq ld b gy ln lk l ll lm"><strong class="ld ir">class </strong>linearRegression(torch.nn.Module):<br/>    <strong class="ld ir">def </strong>__init__(self, inputSize, outputSize):<br/>        super(linearRegression, self).__init__()<br/>        self.linear = torch.nn.Linear(inputSize, outputSize)<br/><br/>    <strong class="ld ir">def </strong>forward(self, x):<br/>        out = self.linear(x)<br/>        <strong class="ld ir">return </strong>out</span></pre><p id="c3a6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们为线性回归定义了一个类，它继承了<strong class="jp ir"> torch.nn.Module </strong>，后者是包含所有必需函数的基本神经网络模块。我们的线性回归模型只包含一个简单的线性函数。</p><p id="7528" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，我们使用下面的代码实例化这个模型。</p><pre class="km kn ko kp gt lc ld le lf aw lg bi"><span id="1746" class="lh li iq ld b gy lj lk l ll lm">inputDim = 1        <em class="lo"># takes variable 'x' <br/></em>outputDim = 1       <em class="lo"># takes variable 'y'<br/></em>learningRate = 0.01 <br/>epochs = 100<br/><br/>model = linearRegression(inputDim, outputDim)<br/><em class="lo">##### For GPU #######<br/></em><strong class="ld ir">if </strong>torch.cuda.is_available():<br/>    model.cuda()</span></pre><p id="df4c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">之后，我们初始化损失(<strong class="jp ir">均方误差</strong>)和优化(<strong class="jp ir">随机梯度下降</strong>)函数，我们将在该模型的训练中使用这些函数。</p><pre class="km kn ko kp gt lc ld le lf aw lg bi"><span id="62b5" class="lh li iq ld b gy lj lk l ll lm">criterion = torch.nn.MSELoss() <br/>optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)</span></pre><p id="581c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">完成所有初始化后，我们现在可以开始训练我们的模型。下面是训练模型的代码。</p><pre class="km kn ko kp gt lc ld le lf aw lg bi"><span id="9f18" class="lh li iq ld b gy lj lk l ll lm"><strong class="ld ir">for </strong>epoch <strong class="ld ir">in </strong>range(epochs):<br/>    <em class="lo"># Converting inputs and labels to Variable<br/>    </em><strong class="ld ir">if </strong>torch.cuda.is_available():<br/>        inputs = Variable(torch.from_numpy(x_train).cuda())<br/>        labels = Variable(torch.from_numpy(y_train).cuda())<br/>    <strong class="ld ir">else</strong>:<br/>        inputs = Variable(torch.from_numpy(x_train))<br/>        labels = Variable(torch.from_numpy(y_train))<br/><br/>    <em class="lo"># Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients<br/>    </em>optimizer.zero_grad()<br/><br/>    <em class="lo"># get output from the model, given the inputs<br/>    </em>outputs = model(inputs)<br/><br/>    <em class="lo"># get loss for the predicted output<br/>    </em>loss = criterion(outputs, labels)<br/>    print(loss)<br/>    <em class="lo"># get gradients w.r.t to parameters<br/>    </em>loss.backward()<br/><br/>    <em class="lo"># update parameters<br/>    </em>optimizer.step()<br/><br/>    print(<strong class="ld ir">'epoch {}, loss {}'</strong>.format(epoch, loss.item()))</span></pre><p id="ab6b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们的线性回归模型已经训练好了，让我们来测试一下。由于这是一个非常简单的模型，我们将在现有的数据集上进行测试，并绘制原始输出与预测输出的对比图。</p><pre class="km kn ko kp gt lc ld le lf aw lg bi"><span id="fcb2" class="lh li iq ld b gy lj lk l ll lm"><strong class="ld ir">with </strong>torch.no_grad(): <em class="lo"># we don't need gradients in the testing phase<br/>    </em><strong class="ld ir">if </strong>torch.cuda.is_available():<br/>        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()<br/>    <strong class="ld ir">else</strong>:<br/>        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()<br/>    print(predicted)<br/><br/>plt.clf()<br/>plt.plot(x_train, y_train, <strong class="ld ir">'go'</strong>, label=<strong class="ld ir">'True data'</strong>, alpha=0.5)<br/>plt.plot(x_train, predicted, <strong class="ld ir">'--'</strong>, label=<strong class="ld ir">'Predictions'</strong>, alpha=0.5)<br/>plt.legend(loc=<strong class="ld ir">'best'</strong>)<br/>plt.show()</span></pre><p id="9907" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这绘制了下图。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lp"><img src="../Images/9a535184a83921b578d97893ef150bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*JdLWW-oqZ_yAVTg0qJACNA.png"/></div></figure><p id="600f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">看起来我们的模型已经正确地计算出了自变量和因变量之间的线性关系。</p><p id="7a45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果你已经理解了这一点，你应该试着为一个稍微复杂一点的<strong class="jp ir">有多个独立变量的线性方程训练一个线性回归模型。</strong></p></div></div>    
</body>
</html>