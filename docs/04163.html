<html>
<head>
<title>Semantic similarity classifier and clustering sentences based on semantic similarity.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语义相似度分类器和基于语义相似度的句子聚类。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-similarity-classifier-and-clustering-sentences-based-on-semantic-similarity-a5a564e22304?source=collection_archive---------8-----------------------#2019-06-30">https://towardsdatascience.com/semantic-similarity-classifier-and-clustering-sentences-based-on-semantic-similarity-a5a564e22304?source=collection_archive---------8-----------------------#2019-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/24a9259f078d96b3efe6bc0389719e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*RsF6MMkuv0eECd_6m0-otw.png"/></div></figure><p id="ef1a" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">最近，我们一直在做一些实验，通过利用预先训练的模型来聚集语义相似的消息，这样我们就可以在不使用标记数据的情况下获得一些东西。这里的任务是给定一个句子列表，我们对它们进行聚类，使得语义相似的句子在同一个聚类中，并且聚类的数量不是预先确定的。</p><p id="0c95" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">语义相似度分类器的任务是:对给定的两个句子/消息/段落进行语义等价的分类。</p><h2 id="d353" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">第一步:通过嵌入来表示每个句子/信息/段落。对于这个任务，我们使用了 infersent，它工作得很好。</h2><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="6d42" class="kv kw it lt b gy lx ly l lz ma"><em class="mb">InferSent</em> is a <em class="mb">sentence embeddings</em> method that provides semantic representations for English sentences. It is trained on natural language inference data and generalizes well to many different tasks.</span></pre><div class="mc md gp gr me mf"><a href="https://github.com/facebookresearch/InferSent" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">Facebook 研究/推断</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">推断句嵌入。在 GitHub 上创建一个帐户，为 Facebook research/INF sent 开发做贡献。</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">github.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt jv mf"/></div></div></a></div><p id="79a9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">代码如下:</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="836e" class="kv kw it lt b gy lx ly l lz ma"><em class="mb"># Load infersent model </em><br/>model_version = 2<br/>MODEL_PATH = "infersent_sentence_encoder/infersent<strong class="lt iu">%s</strong>.pkl" % model_version<br/>params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,<br/>                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}<br/>model = InferSent(params_model)<br/>model.load_state_dict(torch.load(MODEL_PATH))</span><span id="d4d4" class="kv kw it lt b gy mu ly l lz ma"><em class="mb"># If infersent1 -&gt; use GloVe embeddings. If infersent2 -&gt; use InferSent embeddings.</em><br/>W2V_PATH = 'infersent_sentence_encoder/GloVe/glove.840B.300d.txt' <strong class="lt iu">if</strong> model_version == 1 <strong class="lt iu">else</strong> 'infersent_sentence_encoder/fastText/crawl-300d-2M.vec'<br/>model.set_w2v_path(W2V_PATH)</span><span id="b85d" class="kv kw it lt b gy mu ly l lz ma">#load data<br/>ds = pd.read_msgpack('./ds.mp')<br/>sentences = ds['text']</span><span id="18f3" class="kv kw it lt b gy mu ly l lz ma"># generate infersent sentence embeddings<br/>model.build_vocab(sentences, tokenize=<strong class="lt iu">True</strong>)<br/>embs = model.encode(sentences, tokenize=<strong class="lt iu">True</strong>)</span></pre><h2 id="2a5f" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">步骤 2:寻找语义相似的句子/信息/段落的候选</h2><p id="9936" class="pw-post-body-paragraph jx jy it jz b ka mv kc kd ke mw kg kh ki mx kk kl km my ko kp kq mz ks kt ku im bi translated">这里的想法是索引每个句子/消息/段落的表示(嵌入),并基于距离阈值为每个句子挑选 k (=10)个 NN(最近邻)候选。我们发现 nmslib 非常快速高效。</p><pre class="lo lp lq lr gt ls lt lu lv aw lw bi"><span id="6eda" class="kv kw it lt b gy lx ly l lz ma"><strong class="lt iu">import</strong> <strong class="lt iu">nmslib</strong><br/><br/>NTHREADS = 8<br/><strong class="lt iu">def</strong> create_index(a):<br/>    index = nmslib.init(space='angulardist')<br/>    index.addDataPointBatch(a)<br/>    index.createIndex()<br/>    <strong class="lt iu">return</strong> index</span><span id="5413" class="kv kw it lt b gy mu ly l lz ma"><strong class="lt iu">def</strong> get_knns(index, vecs, k=3):<br/>    <strong class="lt iu">return</strong> zip(*index.knnQueryBatch(vecs, k=k,num_threads=NTHREADS))</span><span id="80fa" class="kv kw it lt b gy mu ly l lz ma">nn_wvs = create_index(embs)</span><span id="de69" class="kv kw it lt b gy mu ly l lz ma">to_frame = <strong class="lt iu">lambda</strong> x: pd.DataFrame(np.array(x)[:,1:])</span><span id="09fe" class="kv kw it lt b gy mu ly l lz ma">idxs, dists = map(to_frame, get_knns(nn_wvs, embs, k=10))</span><span id="60b2" class="kv kw it lt b gy mu ly l lz ma">catted = pd.concat([idxs.stack().to_frame('idx'), dists.stack().to_frame('dist')], axis=1).reset_index().drop('level_1',1).rename(columns={'level_0': 'v1', 'idx': 'v2'})</span></pre><h2 id="32d0" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">第三步:获得候选对在语义相似度分类器上的预测概率。(关于语义相似性分类器的细节将在以后的博客文章中介绍)</h2><p id="1603" class="pw-post-body-paragraph jx jy it jz b ka mv kc kd ke mw kg kh ki mx kk kl km my ko kp kq mz ks kt ku im bi translated">把第二步想成候选生成(侧重于召回)，第三步想成侧重于精度。在所有被认为是潜在重复的候选者中，我们给每一对分配概率。</p><h2 id="c9a5" class="kv kw it bd kx ky kz dn la lb lc dp ld ki le lf lg km lh li lj kq lk ll lm ln bi translated">步骤 4:聚集聚类以合并聚类</h2><p id="f618" class="pw-post-body-paragraph jx jy it jz b ka mv kc kd ke mw kg kh ki mx kk kl km my ko kp kq mz ks kt ku im bi translated">基于在步骤 3 中被认为是重复的候选者，我们使用 scikit 中的凝聚聚类实现来合并聚类。在凝聚聚类中，所有观察值都从它们自己的聚类开始，并且使用指定的合并标准合并聚类，直到收敛，此时不再发生合并。</p><div class="mc md gp gr me mf"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">sk learn . cluster . agglomerate clustering-sci kit-learn 0 . 21 . 2 文档</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">连通性矩阵。为每个样本定义遵循给定数据结构的相邻样本。这可以…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">scikit-learn.org</p></div></div><div class="mo l"><div class="na l mq mr ms mo mt jv mf"/></div></div></a></div><p id="a7c6" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这在实践中工作得相当好，并且形成的集群具有良好的语义等价性。</p></div></div>    
</body>
</html>