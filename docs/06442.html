<html>
<head>
<title>Interactive Q learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交互式 Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interactive-q-learning-9d9203fdad70?source=collection_archive---------24-----------------------#2019-09-15">https://towardsdatascience.com/interactive-q-learning-9d9203fdad70?source=collection_archive---------24-----------------------#2019-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b577" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解 Q 表的最佳方式…</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/3b1529ad2059d8a066f77056e747b08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*gRqnRGbup0fUzU97Km0Hvg.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Give me maximum reward :)</figcaption></figure><p id="5e0b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">去玩</strong> @ <a class="ae lr" href="http://mohitmayank.com/interactive_q_learning" rel="noopener ugc nofollow" target="_blank">互动 Q 学习</a></p><p id="5a96" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">代号</strong> @ <a class="ae lr" href="https://github.com/imohitmayank/interactive_q_learning" rel="noopener ugc nofollow" target="_blank">莫希特的 Github </a></p><h2 id="313a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">介绍</h2><p id="b5b6" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在经历了解 Q 学习的过程中，我总是对网格世界(由盒子组成的 2D 世界，代理从一个盒子移动到另一个盒子并收集奖励)着迷。几乎所有强化学习的课程都从 Q 表的基本介绍开始，最直观的 Q 表例子是网格世界。也就是说，很多球场只是画出了它们的静态世界，并没有给观众提供任何游戏素材。为了解决这个问题，我想到了创建一个交互式网格世界，用户可以定义世界、状态和代理。这将有助于用户复制课程的网格世界，理解它实际上是如何工作的，甚至会问——当你改变固定变量时会发生什么？我们开始吧！</p><h2 id="65b1" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">交互式网格世界</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/45cb83a5f08c484dda9526ed771f1c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INSxrgijm3xUgavs-eFbBg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Interactive grid world</figcaption></figure><p id="ca06" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">交互式网格世界分为两个主要区域，</p><ol class=""><li id="9033" class="mr ms iq kx b ky kz lb lc le mt li mu lm mv lq mw mx my mz bi translated"><strong class="kx ir">游乐场</strong>:由盒子或状态组成，动作发生在那里。</li><li id="91ef" class="mr ms iq kx b ky na lb nb le nc li nd lm ne lq mw mx my mz bi translated"><strong class="kx ir">设置:</strong>由多级设置组成，您可以通过这些设置来设计和控制游乐场。</li></ol><p id="687d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们通过浏览可用的设置来理解网格世界的动态。设置区域可以进一步分为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/65f5cd432ab3c884cdc598d8fa02fc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*o7BBwWBkuXEVPiuTxJxukg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The 4 subsections of settings.</figcaption></figure><ul class=""><li id="49d0" class="mr ms iq kx b ky kz lb lc le mt li mu lm mv lq ng mx my mz bi translated"><strong class="kx ir"> Gridworld 级别设置:</strong>由改变整体世界格式的设置组成，包含-</li></ul><p id="cd3d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> —大小</strong>:选择世界的大小。“大”意味着更多的州。</p><p id="c1b6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> —速度</strong>:选择加工速度。当我们想快速得到最终结果时，“快”是好的，但是“慢”是直观形象化的最佳选择。</p><ul class=""><li id="e61d" class="mr ms iq kx b ky kz lb lc le mt li mu lm mv lq ng mx my mz bi translated"><strong class="kx ir">状态级别设置:</strong>帮助设计单个状态及其行为，包含-</li></ul><p id="8959" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> —奖励值</strong>:分配给点击状态的奖励。</p><p id="d33d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> —状态类型</strong>:状态的类型。“终结”——基本上是游戏结束状态，代理在进入终结状态时完成当前剧集。‘墙’——一个没有奖励的固定状态，一个代理人不能跨越。‘正常’—默认状态类型。</p><p id="9cc6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> —应用于所有</strong>:将当前奖励值和状态类型应用于网格世界中所有盒子的快捷按钮。</p><ul class=""><li id="2c35" class="mr ms iq kx b ky kz lb lc le mt li mu lm mv lq ng mx my mz bi translated"><strong class="kx ir">代理级别设置:</strong>定义了代理的学习行为，包含-</li></ul><p id="f2fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir">折扣</strong>:适用于未来奖励的折扣。默认值为 0.9。</p><p id="36bc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir">确定性</strong>:定义代理人动作的确定性概率。1 表示从一个盒子的“右”将总是导致右边的盒子。而 0.7 意味着只有 70%的机会发生这种情况，10%的机会去任何相邻的州。(对于数学爱好者来说，还有 3 个相邻的状态，因此 10 * 3 = 30 %完成了 100%)</p><p id="65d8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir"> E-greedy </strong>:定义代理的利用/探索性质。0 表示代理是完全贪婪的，并且总是选择可用的最佳动作。1 表示代理是完全随机的，可以从任何可用的操作中进行选择。为了了解更多关于ε贪婪的信息，我建议浏览一下我之前的帖子，<a class="ae lr" href="http://mohitmayank.com/reinforcement-learning-with-multi-arm-bandit/" rel="noopener ugc nofollow" target="_blank">用多臂强盗</a>进行强化学习。</p><ul class=""><li id="0e2a" class="mr ms iq kx b ky kz lb lc le mt li mu lm mv lq ng mx my mz bi translated"><strong class="kx ir">执行类型设置:</strong>控制世界的流动，包含-</li></ul><p id="e4fe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir">运行/停止</strong>:让代理在 gridworld 里面玩。切换开关。</p><p id="1572" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir">复位</strong>:回到初始设置和动态。</p><p id="b20c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir">显示/隐藏策略</strong>:切换策略方向箭头的可见性。</p><p id="f950" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">— <strong class="kx ir">显示/隐藏高亮显示</strong>:切换当前状态高亮显示的可见性。</p><h2 id="9b25" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">怎么玩？</h2><p id="2736" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">让我们举一些例子来更好地理解这一点。</p><p id="3225" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">示例 1: </strong>第一个示例可以是我之前关于用 Q 表进行强化学习的帖子中的“啤酒游戏”。我建议通读这篇文章，对 Q 表有一个基本的了解。世界看起来是这样的，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/8056a499186158e40770b7fdbe14e50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ar3k67CSv0GrDo9z1IuqvQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The beer game</figcaption></figure><p id="3d06" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们希望代理人学会总是带着啤酒去那个州，而不是带着洞去那个州。让我们在交互式网格世界中重现这一场景，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/542c7d7257088bf2c65d6c6c20219725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*fr3fzda26xqdXQtRgBq4lw.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Solving example 1 on interactive grid world</figcaption></figure><p id="ab79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，网格世界看起来像这样，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9a9a2e3c8b09edc59304f21012cb2772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ipcUtAPHQOB6QN1LnruFCA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Agent’s expected rewards and policy after convergence for example 1.</figcaption></figure><p id="9ba8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">例子 2 </strong>:我们从<a class="ae lr" href="https://www.udemy.com/artificial-intelligence-az/" rel="noopener ugc nofollow" target="_blank"> Udemy 的人工智能课程</a>中举一个例子。世界看起来是这样的，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2fb897c32ead39a4c0a5dd0c60d08fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*91Plm2qb9BKWy6PnpCpkHw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Green flags have reward of 1 and the state with fire has negative reward, say -1.</figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e411257b4a89de3cf9a16b91a645edf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*8DYL0yHVz2LXCex_N4FLUg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">The policy learned after convergence.</figcaption></figure><p id="cad0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图中的预期状态奖励(V)以及方向(政策)是在经过多次迭代训练后获得的。让我们试着复制这个世界，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/30648fe420c787f63b0e6137dc00e70a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6o8fN-TY9_VzOamPQvm3VQ.gif"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Solving example 2 on interactive grid world</figcaption></figure><p id="cb94" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，网格世界看起来像这样，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/f9d25350649091f5005d2ba48a68051a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3kZG2wy-t1RORZc3Mkb2Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Agent’s expected rewards and policy after convergence</figcaption></figure><p id="d0ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将这个与课程幻灯片中显示的进行比较，它是完全相同的！</p><h2 id="c6a0" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">结论</h2><p id="83a6" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">这个项目还远没有结束！有很多东西我想补充，但我想它们会随着时间的推移而出现。当前版本可能有一些错误，因此如果出现任何不想要的行为，请求助于最后的选项——刷新页面并在<a class="ae lr" href="https://github.com/imohitmayank/interactive_q_learning" rel="noopener ugc nofollow" target="_blank"> GitHub </a>:)报告问题。除此之外，请继续尝试。</p><h2 id="64b2" class="ls lt iq bd lu lv lw dn lx ly lz dp ma le mb mc md li me mf mg lm mh mi mj mk bi translated">参考</h2><p id="ab2c" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">[1] <a class="ae lr" href="https://www.udemy.com/artificial-intelligence-az/" rel="noopener ugc nofollow" target="_blank"> Udemy 的人工智能 A-Z:学习如何构建人工智能</a></p><p id="b97f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] <a class="ae lr" href="http://ai.berkeley.edu/reinforcement.html" rel="noopener ugc nofollow" target="_blank">加州大学柏克莱分校 CS188 人工智能简介</a></p><p id="cb74" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[3] <a class="ae lr" href="https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html" rel="noopener ugc nofollow" target="_blank">由<a class="ae lr" href="https://twitter.com/karpathy" rel="noopener ugc nofollow" target="_blank"> <strong class="kx ir">安德烈·卡帕西</strong> </a>加固. js </a></p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="3915" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">干杯！</p></div></div>    
</body>
</html>