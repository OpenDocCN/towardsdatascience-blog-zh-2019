<html>
<head>
<title>Review: SRDenseNet — DenseNet for SR (Super Resolution)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾:SRDenseNet —用于 SR 的 DenseNet(超分辨率)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8?source=collection_archive---------14-----------------------#2019-02-27">https://towardsdatascience.com/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8?source=collection_archive---------14-----------------------#2019-02-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ffd5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">带跳跃连接的 DenseNet 块优于 SRCNN、VDSR 和 DRCN</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/e64d428bcf33468b1166b053d2350ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*aVnKhQXhUlfRyPwAe4bf0w.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">SRDenseNet has much better quality</strong></figcaption></figure><p id="7abb" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi lo translated"><span class="l lp lq lr bm ls lt lu lv lw di">在</span>这个故事里，<strong class="ku ir">帝国视觉科技</strong>的<strong class="ku ir"> SRDenseNet </strong>在<strong class="ku ir">福洲中国</strong>进行了回顾。在 SRDenseNet 中，<a class="ae lx" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>建议的密集块用于提取高层特征。此外，在密集块之间增加了跳跃连接。瓶颈层和解卷积层用于在重构高分辨率(HR)图像之前进行放大。发表在<strong class="ku ir"> 2017 ICCV </strong>上，引用<strong class="ku ir"> 70 余次</strong>。(<a class="ly lz ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----cbee599de7e8--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><ol class=""><li id="e315" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln mm mn mo mp bi translated"><strong class="ku ir">密集块</strong></li><li id="fb7f" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln mm mn mo mp bi translated"><strong class="ku ir"> SRDenseNet 变体</strong></li><li id="2716" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln mm mn mo mp bi translated"><strong class="ku ir">反褶积、瓶颈层和重建层</strong></li><li id="d432" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln mm mn mo mp bi translated"><strong class="ku ir">消融研究</strong></li><li id="fc42" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln mm mn mo mp bi translated"><strong class="ku ir">结果</strong></li></ol></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="abaa" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak"> 1。密集块</strong></h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi nn"><img src="../Images/3d02a417d3a78dce16e26192a9ae870f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5FKv33lZNfdxAU01ZoQe-g.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Dense Block (The paths at the bottom are copied from previous layers to deeper layers.</strong></figcaption></figure><h2 id="21f0" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">1.1.串联而不是求和</h2><ul class=""><li id="5d91" class="mh mi iq ku b kv oe ky of lb og lf oh lj oi ln oj mn mo mp bi translated">与<a class="ae lx" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>，<strong class="ku ir">不同的是，特征图在</strong><a class="ae lx" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="ku ir">dense net</strong></a><strong class="ku ir">中串接，而不是直接求和。</strong></li><li id="8344" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">因此，第<em class="ok"> i </em>层接收所有前面层的特征图作为输入:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/02e42c40bb997bc58a2706a054cdb73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*yHHRvkmOaSNKosYc2nY3Zw.png"/></div></figure><ul class=""><li id="8f87" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated">其中[ <em class="ok"> X </em> 1，<em class="ok"> X </em> 2，…，<em class="ok">Xi</em>1]表示在前面的卷积层 1，2，…，<em class="ok">I</em>1 中生成的特征图的连接。</li><li id="42bb" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">这种<a class="ae lx" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>结构缓解了消失梯度问题。</li><li id="7f8b" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">重用已经学习过的特征地图迫使当前层学习补充信息，从而避免学习冗余特征。</li><li id="98a5" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">此外，在所提出的网络中，每一层都有一条通向损耗的短路径，从而导致隐含的深度监督。</li></ul><h2 id="a87e" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">1.2.增长率</h2><ul class=""><li id="dc9a" class="mh mi iq ku b kv oe ky of lb og lf oh lj oi ln oj mn mo mp bi translated">本次工作在一个致密区块中有<strong class="ku ir"> 8 个褶积层。</strong></li><li id="01ab" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">当密集块中每个卷积层产生<em class="ok"> k 个</em>特征图作为输出时，一个</strong> <a class="ae lx" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> <strong class="ku ir">密集块</strong> </a> <strong class="ku ir">产生的特征图总数为<em class="ok"> k </em> ×8 </strong>，其中<em class="ok"> k </em>称为<strong class="ku ir">增长率</strong>。</li><li id="0ef4" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">增长率<em class="ok"> k </em>调节每层对最终重建贡献多少新信息。</strong></li><li id="dcac" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">在上图中，<strong class="ku ir">每个块由 8 个卷积层</strong>组成。为了防止网络增长过宽，<strong class="ku ir">增长率设置为 16 </strong>和<strong class="ku ir">每个块的输出有 128 个特征图</strong>。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="d7c6" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak"> 2。SRDenseNet 变体</strong></h1><h2 id="7b11" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">2.1.SRDenseNet_H</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi om"><img src="../Images/f3d2288a9d04e69aa286b509787e74d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bv7mKkPVMIG094TalHBhpg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">SRDenseNet_H</strong></figcaption></figure><ul class=""><li id="c367" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated">这是基本的 SRDenseNet。</li><li id="7a97" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">8 个密集块用于提取高级特征。</li></ul><h2 id="5511" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">2.2.SRDenseNet_HL</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi on"><img src="../Images/21b6363ffcc0bf5c932697561de8617b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PKkHYbAZvW7uMS_XA41QVg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">SRDenseNet_HL</strong></figcaption></figure><ul class=""><li id="0059" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated"><strong class="ku ir">跳过连接</strong>用于连接低级和高级特征。</li><li id="e49f" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">然后，连接的要素地图将用作反卷积图层的输入。</li></ul><h2 id="c719" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">2.3.SRDenseNet_All</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi oo"><img src="../Images/aa7395e6a131f6cd3eb1fb7562f9a1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4nxUqDBCZRt01FzNvw3PNA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">SRDenseNet_All</strong></figcaption></figure><ul class=""><li id="a7a3" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated"><strong class="ku ir">密集跳过连接</strong>用于组合所有卷积层产生的特征图，用于 SR 重建。</li><li id="a92b" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">反褶积层前还增加了</strong>瓶颈层。</li><li id="9670" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">SRDenseNet_All 有<strong class="ku ir"> 69 个重量层</strong>和 68 个激活层。</li><li id="dc1e" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">感受野的大小与深度成正比，可以利用 LR 图像中的大量上下文信息来推断 HR 图像中的高频信息</strong>。</li><li id="3615" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">由于使用了许多 ReLU 层，可以在非常深的网络中利用高度非线性</strong>来模拟 LR 图像和 HR 图像之间的复杂映射函数。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="952b" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated"><strong class="ak"> 3。反卷积、瓶颈和重建层</strong></h1><h2 id="9046" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">3.1.瓶颈层</h2><ul class=""><li id="2eae" class="mh mi iq ku b kv oe ky of lb og lf oh lj oi ln oj mn mo mp bi translated">网络中的所有要素地图连接在 SRDenseNet_All 中，为后续反卷积图层生成许多输入。</li><li id="663b" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">具有 1×1 内核的卷积层被用作瓶颈层</strong>以减少输入特征图的数量。</li><li id="d670" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">使用 1×1 瓶颈层，特征地图的数量减少到 256 个。</li><li id="0c96" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">之后，去卷积层将 256 个特征图从 LR 空间变换到 HR 空间。</li></ul><h2 id="3c51" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">3.2.去卷积层</h2><ul class=""><li id="4465" class="mh mi iq ku b kv oe ky of lb og lf oh lj oi ln oj mn mo mp bi translated">在<a class="ae lx" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>和<a class="ae lx" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>中，双三次插值用于在卷积之前将低分辨率(LR)图像提升到 HR 空间。</li><li id="d353" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">所有卷积都在 HR 空间中进行，这增加了 SR 的计算复杂度。</li><li id="dbec" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">此外，插值方法没有为解决随机共振问题带来新的信息。</li><li id="08e1" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">因此，在卷积之后，使用去卷积层来学习放大滤波器。有两个好处。</li><li id="5ed3" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">首先，<strong class="ku ir">它加速了 SR 重建过程。</strong>在网络末端增加反褶积层后，整个计算过程在 LR 空间进行。如果放大因子是<em class="ok"> r </em>，那么它将减少因子<em class="ok"> r </em>的计算成本。</li><li id="880e" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">此外，<strong class="ku ir">来自 LR 图像的大量上下文信息用于推断高频细节</strong>。</li><li id="eec2" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">在这项工作中，<strong class="ku ir">两个连续的 3×3 内核的反褶积层</strong>和<strong class="ku ir"> 256 个特征图</strong>用于向上扩展。</li></ul><h2 id="6c82" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">3.3.重建层</h2><ul class=""><li id="4424" class="mh mi iq ku b kv oe ky of lb og lf oh lj oi ln oj mn mo mp bi translated">重建层<strong class="ku ir">是一个 3×3 核</strong>和<strong class="ku ir">一路输出</strong>的卷积层。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="3e18" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">4.消融研究</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi op"><img src="../Images/64bf894995aaa02f4c1f7359624106ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Sd_Px57cjpcElF0ijs1rg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">PSNR/SSIM on Urban100</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/5995b8e62942967e9bc81a82647dd3ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*0sUr7ll6zxdVAP5KrjRTkw.png"/></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">PSNR/SSIM on 4 Datasets</strong></figcaption></figure><ul class=""><li id="2768" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated">从 ImageNet 中随机选择 50，000 幅图像用于训练。</li><li id="e6fb" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">在 HR 空间中裁剪尺寸为 100×100 的非重叠子图像。LR 图像是通过使用比例因子为 4 倍的双三次曲线对 HR 图像进行下采样而获得的。只有 Y 通道用于训练。</li><li id="1d90" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">ReLU 用于所有权重层，并使用 Adam 优化器。</li><li id="06e3" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">使用 32 的小批量。</li><li id="7bd2" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">在测试过程中，数据集<strong class="ku ir"> Set5，Set14 </strong>，<strong class="ku ir"> B100，</strong>来自 Berkeley 分割数据集，由<strong class="ku ir"> 100 幅自然图像</strong>，<strong class="ku ir"> Urban100 </strong>组成，其中包含<strong class="ku ir"> 100 幅挑战图像</strong>。</li><li id="1406" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">测试了 LR 和 HR 图像之间的 4 倍比例因子。</li><li id="9bf5" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">PSNR 和 SSIM 是在图像的 Y 通道上计算的。</li><li id="6470" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">用的是英伟达 Titan X GPU。</li><li id="827b" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">SRDenseNet_All 在 SRDenseNet 变体中具有最高的 PSNR 和 SSIM。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h1 id="6522" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">5.结果</h1><h2 id="de2b" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated"><strong class="ak"> 5.1。定量结果</strong></h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi or"><img src="../Images/adb1fe1f3af421bf8d65a5f791cb8f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M5idNK-cIR1cQfbvMfarBw.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">PSNR/SSIM on 4 Datasets</strong></figcaption></figure><ul class=""><li id="221b" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated">对于<a class="ae lx" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>，使用最佳 9–5–5 图像模型。</li><li id="3266" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">至于 A+方法，它没有预测图像边界。为了公平比较，HR 图像的边界被裁剪，以便所有结果具有相同的区域。</li><li id="4913" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">就 PSNR 而言，所提出的方法在不同的数据集上比最新的结果实现了 0.2dB-0.8dB 的改善。</li><li id="aa09" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">平均起来，<strong class="ku ir">比具有 3 层的</strong><a class="ae lx" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"><strong class="ku ir">SRCNN</strong></a><strong class="ku ir"/>提高了大约 1.0 dB，比具有 20 层的<a class="ae lx" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"><strong class="ku ir">【VDSR】</strong></a><strong class="ku ir"/>提高了大约 0.5 dB。</li><li id="c9d5" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">所有方法中最显著的改进，包括<a class="ae lx" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener"> SRCNN </a>、<a class="ae lx" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f"> VDSR </a>和<a class="ae lx" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener"> DRCN </a>，都是在极具挑战性的数据集 Urban100 上获得的。</li></ul><h2 id="bf84" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">5.2.定性结果</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi os"><img src="../Images/0e2606023804137a56c0f45d006c4849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S5UWxZdm3Kj7eAyLppMZqg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Urban100 img096</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi os"><img src="../Images/b972e45391292180ff180e3aa34b2b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1ivvP_Wc752N2xVjBzeHg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Urban100 img099</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi os"><img src="../Images/01b64ae1105a1e0bcc174d3ce3841b25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VX7C1OTkRyvw0IlFKAFx9w.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Urban100 img004</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ot"><img src="../Images/2dd1322c6150959abb347e9c2a2a92fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hBEsszMN2o9XoKKvSX3pZA.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">B100 148026</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ou"><img src="../Images/dfd7ef6d7002006920f5b177b25039cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9Ry6k9-patibRNSAbgPxg.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">B100 253027</strong></figcaption></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="no np di nq bf nr"><div class="gh gi ov"><img src="../Images/133168605ae9300585b43e5d1818256b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkJGW_KXRRGVrjAOj5y36w.png"/></div></div><figcaption class="kn ko gj gh gi kp kq bd b be z dk"><strong class="bd kr">Set14 ppt3</strong></figcaption></figure><ul class=""><li id="22b4" class="mh mi iq ku b kv kw ky kz lb mj lf mk lj ml ln oj mn mo mp bi translated">对于 Urban100 上的上述图像，SRDenseNet 可以很好地重建线条和轮廓，而其他方法会产生模糊的结果。</li><li id="43fc" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated">对于 B100 和 Set14 上的上述图像，SRDenseNet 可以重建纹理图案并避免失真。</li><li id="7fca" class="mh mi iq ku b kv mq ky mr lb ms lf mt lj mu ln oj mn mo mp bi translated"><strong class="ku ir">在 Titan X GPU 上实现超分辨<strong class="ku ir"> B100 </strong>的平均速度为 36ms </strong>，达到 4 倍缩放因子的实时 SR。</li></ul></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><p id="9354" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">最后，作者提到，目前的研究趋势是研究 SR 问题的感知损失，如 SRGAN，它“伪造”纹理，使其具有更好的人眼感知质量，尽管 PSNR 较低。他们也会对此进行调查。</p></div><div class="ab cl ma mb hu mc" role="separator"><span class="md bw bk me mf mg"/><span class="md bw bk me mf mg"/><span class="md bw bk me mf"/></div><div class="ij ik il im in"><h2 id="eafa" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">参考</h2><p id="3f59" class="pw-post-body-paragraph ks kt iq ku b kv oe jr kx ky of ju la lb ow ld le lf ox lh li lj oy ll lm ln ij bi translated">【2017 ICCV】【SRDenseNet】<br/><a class="ae lx" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Tong_Image_Super-Resolution_Using_ICCV_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">使用密集跳跃连接的图像超分辨率</a></p><h2 id="391e" class="ns mw iq bd mx nt nu dn nb nv nw dp nf lb nx ny nh lf nz oa nj lj ob oc nl od bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph ks kt iq ku b kv oe jr kx ky of ju la lb ow ld le lf ox lh li lj oy ll lm ln ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(他)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(呢)(?)(她)(们)(都)(不)(在)(这)(些)(情)(况)(下)(,)(她)(们)(还)(是)(不)(在)(这)(些)(事)(上)(有)(什)(么)(情)(况)(吗)(?)(她)(们)(都)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(,)(她)(们)(们)(还)(不)(在)(这)(些)(事)(上)(有)(什)(么)(好)(的)(情)(情)(情)(况)(。</p><p id="8b77" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">物体检测<br/></strong><a class="ae lx" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lx" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lx" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a>】<a class="ae lx" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae lx" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766">ION</a><a class="ae lx" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">multipath Net</a>【T21 [ <a class="ae lx" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae lx" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">语义切分<br/></strong><a class="ae lx" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lx" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】【parse net<a class="ae lx" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lx" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSP net</a><a class="ae lx" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a><a class="ae lx" rel="noopener" target="_blank" href="/review-drn-dilated-residual-networks-image-classification-semantic-segmentation-d527e1a8fb5">DRN</a></p><p id="fc65" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir">生物医学图像分割<br/></strong><a class="ae lx" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a><a class="ae lx" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a><a class="ae lx" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a><a class="ae lx" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a><a class="ae lx" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a><a class="ae lx" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a></p><p id="3134" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ir"> 实例分段 <br/> </strong> <a class="ae lx" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339"> DeepMask </a> <a class="ae lx" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61"> SharpMask </a> <a class="ae lx" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413"> MultiPathNet </a> <a class="ae lx" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34"> MNC </a> <a class="ae lx" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92"> InstanceFCN </a> <a class="ae lx" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2"> FCIS </a>】</p><p id="58de" class="pw-post-body-paragraph ks kt iq ku b kv kw jr kx ky kz ju la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">)(我)(们)(都)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(,)(我)(们)(还)(是)(不)(知)(道)(。</p></div></div>    
</body>
</html>