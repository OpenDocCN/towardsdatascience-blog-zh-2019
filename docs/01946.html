<html>
<head>
<title>Review: DPN — Dual Path Networks (Image Classification)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:Dual 双路径网络(图像分类)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/review-dpn-dual-path-networks-image-classification-d0135dce8817?source=collection_archive---------11-----------------------#2019-03-31">https://towardsdatascience.com/review-dpn-dual-path-networks-image-classification-d0135dce8817?source=collection_archive---------11-----------------------#2019-03-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fb44" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">优于 ResNet、DenseNet、PolyNet、ResNeXt，ILSVRC 2017 对象本地化挑战赛冠军</h2></div><p id="fd76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>这个故事里，<strong class="kh ir">【双路径网络】</strong>被简要回顾。这是新加坡国立大学、北京理工大学、国防科技大学、奇虎 360 AI 研究所合作的作品。<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>支持功能重用，而<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>支持新功能探索。DPN 从 ResNet 和 DenseNet 这两者中挑选优势。最后，它在图像分类任务上优于<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>、<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>。DPN <strong class="kh ir">赢得了 ILSVRC 2017 本地化挑战赛</strong>。凭借更好的主干，它还可以为对象检测和语义分割任务获得最先进的结果。并作为<strong class="kh ir"> 2017 NIPS </strong>论文发表，引用<strong class="kh ir"> 100 余篇</strong>。(<a class="ll lm ep" href="https://medium.com/u/aff72a0c1243?source=post_page-----d0135dce8817--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="3809" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">概述</h1><ol class=""><li id="d5ec" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la mt mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="kh ir">ResNet</strong></a><strong class="kh ir"/><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense net</strong></a><strong class="kh ir">和 DPN </strong></li><li id="6058" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la mt mu mv mw bi translated"><strong class="kh ir">与最先进方法的比较</strong></li></ol></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="afef" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated"><strong class="ak"> 1。</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">雷斯内特</a> <strong class="ak">，</strong> <a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803">丹森内特</a> <strong class="ak">和 DPN </strong></h1><h2 id="692b" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">1.1.<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a></h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d64dd72d0ff0e21bc0080e330fdec705.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*46EXakKKGFcmhRa7GO2Vxw.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="bd oa">DenseNet</strong></a></figcaption></figure><ul class=""><li id="da1a" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">作者尝试将<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>表示为高阶递归神经网络(HORNN)进行解释。</li><li id="f73f" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">当<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>表示为 HORNN 时，<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>可以表示如上图。</li><li id="4c55" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">绿色箭头表示共享权重卷积。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi of"><img src="../Images/720dbc8e7dabc7b7e946201ade08b19d.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*KDoXGdxMbnPD9CxkyFUM0A.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="bd oa">DenseNet</strong></a></figcaption></figure><h2 id="954a" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">1.2.ResNet </h2><ul class=""><li id="0aca" class="mm mn iq kh b ki mo kl mp ko mq ks mr kw ms la oe mu mv mw bi translated">添加了一个新路径来临时保存绿色箭头的输出以供重用。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi og"><img src="../Images/838b28a430b48caa19cf4d1b3760cf0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*Im4dWD9WKs1e72oQAiTGZw.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd oa">ResNet</strong></a><strong class="bd oa"> (Left) </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="bd oa">DenseNet</strong></a><strong class="bd oa"> (Right)</strong></figcaption></figure><ul class=""><li id="3cd5" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">虚线矩形实际上是剩余路径。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oh"><img src="../Images/4c3ea0153a6f8c56bd0f60e08400b51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5a8Pi8zR7sn12VpHp2nNiw.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"><strong class="bd oa">ResNet</strong></a><strong class="bd oa"> (Left) </strong><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="bd oa">DenseNet</strong></a><strong class="bd oa"> (Right)</strong></figcaption></figure><ul class=""><li id="7113" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">剩余网络本质上是密集连接的网络，但是具有共享连接。</li><li id="8e01" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> <strong class="kh ir"> ResNet </strong> </a> <strong class="kh ir">:特征细化(特征复用)。</strong></li><li id="e0a3" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"><strong class="kh ir">dense net</strong></a><strong class="kh ir">:不断探索新功能。</strong></li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/750b8c48939172f220d37a0ac88caac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*aVQI395bmAVnWxjxGq9c8Q.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Managing a Company</strong></figcaption></figure><ul class=""><li id="568c" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">就像管理公司一样:</li><li id="8c79" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><strong class="kh ir">员工需要不断提高技能(特征提炼)。</strong></li><li id="e69c" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><strong class="kh ir">还需要招聘大一新生到公司(特色探索)。</strong></li><li id="0c4e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">论文中有大量段落和方程供<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet </a>和<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet </a>解释。如果感兴趣，请阅读该文件。</li></ul><h2 id="6fe3" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">1.3.DPN</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi on"><img src="../Images/f5002dbaaac2d5e37eb3b45398092646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Kessu9zI3XWDEG52KqnoA.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">DPN</strong></figcaption></figure><ul class=""><li id="db80" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">为了兼具两者的优点，网络变成了如上左图。</li><li id="a310" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">将两列合并为一列，DPN 如上图所示。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oo"><img src="../Images/aeb72597a08b49a95dc5c559f0a1c3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dfbnh1gmYYXtCCnzJFNqBg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Detailed Architecture and Complexity Comparison</strong></figcaption></figure><ul class=""><li id="5453" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">与<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>相比，DPN 被有意设计成具有相当小的模型尺寸和较少的 FLOPs。</li><li id="a53e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><strong class="kh ir"> DPN-92 </strong>比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kh ir">ResNeXt-101</strong></a><strong class="kh ir">【32×4d】</strong>成本约为<strong class="kh ir">成本约为 15%，而<strong class="kh ir"> DPN-98 </strong>比</strong><a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kh ir">ResNeXt-101</strong></a><strong class="kh ir">【64×4d】</strong>成本约为 26%。</li><li id="badc" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">在 224×224 输入的情况下，<strong class="kh ir"> DPN-92 </strong>比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kh ir">ResNeXt-101</strong></a><strong class="kh ir">【32×4d】</strong><strong class="kh ir">DPN-98</strong>比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"><strong class="kh ir">ResNeXt-101</strong></a><strong class="kh ir">消耗约<strong class="kh ir">25% FLOPs(64</strong></strong></li></ul></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h1 id="1a1f" class="lu lv iq bd lw lx ly lz ma mb mc md me jw mf jx mg jz mh ka mi kc mj kd mk ml bi translated">2.<strong class="ak">与最先进方法的比较</strong></h1><h2 id="2fce" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">2.1.图像分类</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi op"><img src="../Images/335794c240c29078f6d57697663d7bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaE9fFxjjk8cRxaX_LE4Pg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">ImageNet-1k Dataset Validation Set (+: Mean-Max Pooling)</strong></figcaption></figure><ul class=""><li id="ea6a" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">与<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt-101 </a> (32×4d)相比，深度仅为 92 的浅 DPN 将 top-1 错误率降低了 0.5%的绝对值，与<a class="ae lk" rel="noopener" target="_blank" href="/review-densenet-image-classification-b6631a8ef803"> DenseNet-161 </a>相比，降低了 1.5%的绝对值，但提供的 FLOPs 要少得多。</li><li id="e9f1" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">更深的 DPN (DPN-98)超过了最好的残差网络— <a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt-101 </a> (64×4d)，并且仍然享有少 25%的 FLOPs 和小得多的模型尺寸(236 MB 对 320 MB)。</li><li id="8926" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">DPN-131 显示出优于最佳单一型号的精确度——非常深的<a class="ae lk" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>，具有小得多的型号尺寸(304 MB v.s. 365 MB)。</li><li id="09a5" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>采用<a class="ae lk" rel="noopener" target="_blank" href="/review-stochastic-depth-image-classification-a4e225807f4a">随机深度(SD) </a>等众多招数进行训练，DPN-131 可以使用标准训练策略进行训练。而且 DPN-131 的实际训练速度比<a class="ae lk" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea">波利尼特</a>快 2 倍左右。</li></ul><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi oq"><img src="../Images/bda7c26f795d18139e80784924966f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_XzIkydrDC6zxxN_d42bA.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Comparison of total actual cost between different models during training.</strong></figcaption></figure><ul class=""><li id="ef39" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">实际成本如上所述进行比较。</li><li id="1a1e" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">DPN-98 比性能最好的<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>快 15%,使用的内存少 9%,测试错误率也低得多。</li><li id="6227" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">与性能最好的<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt </a>相比，更深的 DPN-131 只多花费了大约 19%的训练时间，但却达到了最先进的单一模型性能。</li><li id="0673" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a> (537 层)【23】的训练速度，基于使用 MXNet 的重新实现，约为每秒 31 个样本，表明 DPN-131 在训练期间运行速度比<a class="ae lk" rel="noopener" target="_blank" href="/review-polynet-2nd-runner-up-in-ilsvrc-2016-image-classification-8a1a941ce9ea"> PolyNet </a>快约 2 倍。</li></ul><h2 id="4acb" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">4.2.场景分类</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi or"><img src="../Images/d09b279e59962cbc75f022ed5c5cb52e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*I-Y6qxizZtLsvpvn4le8Sw.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Places365-Standard dataset Validation Accuracy</strong></figcaption></figure><ul class=""><li id="c79b" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">Places365-Standard 数据集是一个高分辨率的场景理解数据集，包含 365 个场景类别的超过 180 万幅图像。</li><li id="b2cc" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">DPN-92 需要的参数少得多(138 MB 对 163 MB)，这再次证明了它的高参数效率和高泛化能力。</li></ul><h2 id="8c4b" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">4.3.目标检测</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi os"><img src="../Images/8d354a79e6916395cf200eb6b5bee20e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOi6K04FLpguDm1RPGhBmg.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">PASCAL VOC 2007 test set</strong></figcaption></figure><ul class=""><li id="92d7" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">该模型在 VOC 2007 trainval 和 VOC 2012 trainval 的联合集上进行训练，并在 VOC 2007 测试集上进行评估，使用<a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">更快的 R-CNN </a>框架。</li><li id="5688" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">DPN 获得了 82.5%的地图，这是一个很大的进步，比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8"> ResNet-101 </a>提高了 6.1%，比<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac"> ResNeXt-101 </a> (32×4d)提高了 2.4%。</li></ul><h2 id="11c5" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">4.4.语义分割</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi ot"><img src="../Images/f1c4beec68615a2e0ad6f0caa1e21d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yNJlsw3tMtTmjjsXp4Bevw.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">PASCAL VOC 2012 test set</strong></figcaption></figure><ul class=""><li id="0f2a" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated">细分框架基于<a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d"> DeepLabv2 </a>。conv4 和 conv5 中的 3×3 卷积层替换为 atrous 卷积，并且在 conv5 的最终要素图中使用了阿特鲁空间金字塔池(ASPP)。</li><li id="7168" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">DPN-92 具有最高的整体 mIoU 精度，将整体 mIoU 提高了绝对值 1.7%。</li><li id="ffd4" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">考虑到<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac">的 ResNeXt-101 </a> (32×4d)与<a class="ae lk" rel="noopener" target="_blank" href="/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">的 ResNet-101 </a>相比，整体 mIoU 仅提高绝对值 0.5%，建议的 DPN-92 与<a class="ae lk" rel="noopener" target="_blank" href="/review-resnext-1st-runner-up-of-ilsvrc-2016-image-classification-15d7f17b42ac">的 ResNeXt-101 </a> (32×4d)相比，提高了 3 倍以上。</li></ul><h2 id="528d" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">4.5.ILSVRC 2017 对象本地化</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/bd507161eabf342829cf3405e37ea0f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*tIyvGlREfAoW7C2I6scaPg.png"/></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Visualization</strong></figcaption></figure><ul class=""><li id="2a57" class="mm mn iq kh b ki kj kl km ko ob ks oc kw od la oe mu mv mw bi translated"><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">以更快的 R-CNN </a>为框架。</li><li id="3e74" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">当使用额外的训练数据时，DPN 在分类和本地化任务中也获得了胜利。</li><li id="8828" class="mm mn iq kh b ki mx kl my ko mz ks na kw nb la oe mu mv mw bi translated">排行榜:<a class="ae lk" href="http://image-net.org/challenges/LSVRC/2017/results" rel="noopener ugc nofollow" target="_blank">http://image-net.org/challenges/LSVRC/2017/results</a></li></ul><h2 id="332a" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">4.5.ILSVRC 2017 对象检测</h2><figure class="np nq nr ns gt nt gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi ov"><img src="../Images/d7164026faf46a1eca8668205475a4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yzwCxgEMqDFhsV093TgVcQ.png"/></div></div><figcaption class="nw nx gj gh gi ny nz bd b be z dk"><strong class="bd oa">Visualization</strong></figcaption></figure></div><div class="ab cl ln lo hu lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="ij ik il im in"><h2 id="694c" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">参考</h2><p id="d867" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ow kq kr ks ox ku kv kw oy ky kz la ij bi translated">【2017 NIPS】【DPN】<br/><a class="ae lk" href="https://arxiv.org/abs/1707.01629" rel="noopener ugc nofollow" target="_blank">双路径网络</a></p><h2 id="ab86" class="nc lv iq bd lw nd ne dn ma nf ng dp me ko nh ni mg ks nj nk mi kw nl nm mk nn bi translated">我以前的评论</h2><p id="4da9" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko ow kq kr ks ox ku kv kw oy ky kz la ij bi translated">)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(不)(想)(到)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(就)(想)(到)(了)(这)(些)(人)(们)(,)(我)(们)(们)(都)(不)(想)(要)(到)(这)(些)(人)(,)(但)(我)(们)(还)(没)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(想)(到)(这)(些)(事)(,)(我)(们)(还)(没)(想)(到)(这)(里)(去)(。 )(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(有)(什)(么)(情)(况)(呢)(?)(我)(们)(都)(不)(想)(到)(这)(些)(人)(,)(但)(是)(这)(些)(人)(还)(是)(这)(些)(人)(,)(还)(是)(这)(些)(人)(,)(还)(没)(有)(什)(么)(好)(的)(情)(情)(况)(,)(还)(是)(这)(些)(人)(,)(我)(们)(还)(没)(想)(到)(这)(里)(来)(。</p><p id="8b77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">物体检测<br/></strong><a class="ae lk" href="https://medium.com/coinmonks/review-of-overfeat-winner-of-ilsvrc-2013-localization-task-object-detection-a6f8b9044754" rel="noopener">过食</a><a class="ae lk" href="https://medium.com/coinmonks/review-r-cnn-object-detection-b476aba290d1" rel="noopener">R-CNN</a><a class="ae lk" href="https://medium.com/coinmonks/review-fast-r-cnn-object-detection-a82e172e87ba" rel="noopener">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-faster-r-cnn-object-detection-f5685cb30202">快 R-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-mr-cnn-s-cnn-multi-region-semantic-aware-cnns-object-detection-3bd4e5648fde">MR-CNN&amp;S-CNN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deepid-net-def-pooling-layer-object-detection-f72486f1a0f6">DeepID-Net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-craft-cascade-region-proposal-network-and-fast-r-cnn-object-detection-2ce987361858">CRAFT</a><a class="ae lk" rel="noopener" target="_blank" href="/review-r-fcn-positive-sensitive-score-maps-object-detection-91cd2389345c">R-FCN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-ion-inside-outside-net-2nd-runner-up-in-2015-coco-detection-object-detection-da19993f4766"> [ </a><a class="ae lk" rel="noopener" target="_blank" href="/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5"> DSSD </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">约洛夫 1 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-yolov2-yolo9000-you-only-look-once-object-detection-7883d2b02a65">约洛夫 2 /约洛 9000 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-yolov3-you-only-look-once-object-detection-eab75d7a1ba6">约洛夫 3 </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-fpn-feature-pyramid-network-object-detection-262fc7482610"> FPN </a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-retinanet-focal-loss-object-detection-38fba6afabe4">视网膜网</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44"> DCN </a> ]</p><p id="6582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">语义切分<br/></strong><a class="ae lk" rel="noopener" target="_blank" href="/review-fcn-semantic-segmentation-eb8c9b50d2d1">FCN</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deconvnet-unpooling-layer-semantic-segmentation-55cf8a6e380e">de convnet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv1-deeplabv2-atrous-convolution-semantic-segmentation-b51c5fbde92d">deeplab v1&amp;deeplab v2</a><a class="ae lk" rel="noopener" target="_blank" href="/review-crf-rnn-conditional-random-fields-as-recurrent-neural-networks-semantic-segmentation-a11eb6e40c8c">CRF-RNN</a>】<a class="ae lk" rel="noopener" target="_blank" href="/review-segnet-semantic-segmentation-e66f2e30fb96">SegNet</a>】<a class="ae lk" href="https://medium.com/datadriveninvestor/review-parsenet-looking-wider-to-see-better-semantic-segmentation-aa6b6a380990" rel="noopener">parse net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-dilated-convolution-semantic-segmentation-9d5a5bd768f5">dilated net</a><a class="ae lk" rel="noopener" target="_blank" href="/review-pspnet-winner-in-ilsvrc-2016-semantic-segmentation-scene-parsing-e089e5df177d">PSPNet</a><a class="ae lk" rel="noopener" target="_blank" href="/review-deeplabv3-atrous-convolution-semantic-segmentation-6d818bfd1d74">deeplab v3</a>]</p><p id="fc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">生物医学图像分割<br/></strong>[<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision1-fully-convolutional-network-biomedical-image-segmentation-5434280d6e6" rel="noopener">cumed vision 1</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cumedvision2-dcan-winner-of-2015-miccai-gland-segmentation-challenge-contest-biomedical-878b5a443560" rel="noopener">cumed vision 2/DCAN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-u-net-biomedical-image-segmentation-d02bf06ca760">U-Net</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-cfs-fcn-biomedical-image-segmentation-ae4c9c75bea6" rel="noopener">CFS-FCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-u-net-resnet-the-importance-of-long-short-skip-connections-biomedical-image-ccbf8061ff43" rel="noopener">U-Net+ResNet</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-multichannel-segment-colon-histology-images-biomedical-image-segmentation-d7e57902fbfc">多通道</a>[<a class="ae lk" rel="noopener" target="_blank" href="/review-v-net-volumetric-convolution-biomedical-image-segmentation-aa15dbaea974">V-Net</a>]</p><p id="3134" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">实例分割<br/></strong>[<a class="ae lk" href="https://medium.com/datadriveninvestor/review-sds-simultaneous-detection-and-segmentation-instance-segmentation-80b2a8ce842b" rel="noopener">SDS</a>[<a class="ae lk" rel="noopener" target="_blank" href="/review-hypercolumn-instance-segmentation-367180495979">超列</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-deepmask-instance-segmentation-30327a072339">深度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-sharpmask-instance-segmentation-6509f7401a61">锐度掩码</a> ] [ <a class="ae lk" rel="noopener" target="_blank" href="/review-multipath-mpn-1st-runner-up-in-2015-coco-detection-segmentation-object-detection-ea9741e7c413">多路径网络</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-mnc-multi-task-network-cascade-winner-in-2015-coco-segmentation-instance-segmentation-42a9334e6a34">MNC</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-instancefcn-instance-sensitive-score-maps-instance-segmentation-dbfe67d4ee92">Instance fcn</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-fcis-winner-in-2016-coco-segmentation-instance-segmentation-ee2d61f465e2">FCIS</a></p><p id="58de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超分辨率<br/></strong>[<a class="ae lk" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">Sr CNN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-fsrcnn-super-resolution-80ca2ee14da4">fsr CNN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-vdsr-super-resolution-f8050d49362f">VDSR</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-espcn-real-time-sr-super-resolution-8dceca249350" rel="noopener">ESPCN</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-red-net-residual-encoder-decoder-network-denoising-super-resolution-cb6364ae161e" rel="noopener">红网</a>][<a class="ae lk" href="https://medium.com/datadriveninvestor/review-drcn-deeply-recursive-convolutional-network-super-resolution-f0a380f79b20" rel="noopener">DRCN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-drrn-deep-recursive-residual-network-super-resolution-dca4a35ce994">DRRN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-lapsrn-ms-lapsrn-laplacian-pyramid-super-resolution-network-super-resolution-c5fe2b65f5e8">LapSRN&amp;MS-LapSRN</a>][<a class="ae lk" rel="noopener" target="_blank" href="/review-srdensenet-densenet-for-sr-super-resolution-cbee599de7e8">srdensenenet</a></p><p id="f29d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">人体姿态估计</strong><br/><a class="ae lk" rel="noopener" target="_blank" href="/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36">深度姿态</a><a class="ae lk" rel="noopener" target="_blank" href="/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c">汤普逊·尼普斯 14 </a></p></div></div>    
</body>
</html>