<html>
<head>
<title>Vanilla Deep Q Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">普通深度 Q 网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb?source=collection_archive---------3-----------------------#2019-07-15">https://towardsdatascience.com/dqn-part-1-vanilla-deep-q-networks-6eb4a00febfb?source=collection_archive---------3-----------------------#2019-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="1a56" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/Dqn-family" rel="noopener" target="_blank"> DQN 家族</a></h2><div class=""/><div class=""><h2 id="ddff" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">深度 Q 学习解释</h2></div><h1 id="5bfd" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">介绍</h1><p id="9257" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">该职位的结构如下:</p><p id="49ac" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">我们将简要介绍一般的策略迭代和时间差分方法。然后，我们将 Q 学习理解为一个一般的策略迭代。最后，我们将理解并实现 Deepmind 的论文“<a class="ae mh" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank">用深度强化学习玩雅达利(Mnih et al. 2013) </a>中提出的 DQN。</p><h2 id="4f04" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">通用策略迭代(GPI)</h2><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0b319926aaf6e6a27c6daf39699a2f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*kIF1yo8z8GOxDO2w7livQw.png"/></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">General policy iteration</figcaption></figure><p id="f280" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">我们把<strong class="li ja">一般策略迭代<em class="nf"> </em> </strong>称为策略评估和策略迭代的交替。我们从某个任意初始化的策略开始，评估该策略(表示为<strong class="li ja"> E </strong>)，从评估中导出一个新策略(表示为<strong class="li ja"> I </strong>)，并重复这个过程，直到我们达到一个最佳策略。通过这个迭代过程，我们得到{V_π}和{π}的单调递增(<em class="nf">改进</em>)序列。</p><p id="f538" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">我们如何保证这一点？我们将看看<em class="nf">政策改进定理</em>:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/06ce83eaeb23fdda134beddfe529f7f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gNjjknJWEeJnuKTt8piPiQ.png"/></div></div></figure><p id="3f0f" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">该定理指出，遵循政策π'的价值比遵循政策π'的价值更大。即政策π'优于π。我们现在将证明政策改进定理。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nl"><img src="../Images/2cdf174417e3016c719a5b29c241cdf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q51CzxGSU39-BHUkKfdUvw.png"/></div></div></figure><p id="77a1" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">因此，通过政策评估和迭代的迭代相互作用，我们可以最终达到我们的最优政策和价值函数。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h2 id="5d55" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">时间差分法</h2><p id="deb0" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">时间差分法是蒙特卡罗方法和动态规划方法的结合。回忆每种方法:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi nt"><img src="../Images/786effee8c6c8a56e74628fd732640eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpSgodwUFdqVTw2i0tuItg.png"/></div></div></figure><ul class=""><li id="3dab" class="nu nv iq li b lj mc lm md lp nw lt nx lx ny mb nz oa ob oc bi translated">蒙特卡罗方法使用估计值(1)进行更新。因为我们不知道真实的期望值，所以我们从环境中采样 G_t。</li><li id="f7a6" class="nu nv iq li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated">动态编程(DP)方法使用(3)进行更新。我们说 DP 方法是 bootstrap，因为我们使用 v_pi(s_{t+1})的当前估计来执行更新。</li></ul><p id="6d37" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">在 TD 方法中，我们将蒙特卡罗的<em class="nf">采样</em>与动态规划的<em class="nf">自举</em>结合起来。我们对期望值进行采样，如(1)所示，并使用下一个状态值的当前估计值来更新原始状态的值，如(3)所示。TD 目标<code class="fe oi oj ok ol b">\delta</code>可以有多种形式，但以下是最基本的形式:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi om"><img src="../Images/fd15a02452b73427d9370e35af1f7077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*POb8POz5y7vQ05mCFNc6Fw.png"/></div></div></figure></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h2 id="a19e" class="mi kp iq bd kq mj mk dn ku ml mm dp ky lp mn mo la lt mp mq lc lx mr ms le iw bi translated">q 学习和 GPI</h2><p id="8387" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在 Q 学习中，我们直接逼近我们的最优动作值函数。在 GPI 意义上，我们从 Q 函数中导出策略，并通过 TD 方法执行策略评估，以获得下一个 Q 函数。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi on"><img src="../Images/3e42a12a1214c338577813ae6b810541.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L3ZiapIvf-JQBXFgJ_xOvQ.png"/></div></div></figure><p id="165d" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">现在让我们的 Q 函数用θ参数化，在我们的例子中，就是神经网络。根据 GPI 公式，我们希望将当前 Q 值与目标 Q 值之间的差异降至最低。为此，我们希望获得两者之间的均方误差:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oo"><img src="../Images/3fe04c3df8a89e727a7bb1decef54b6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wCuiE2XNc7yM_PjD9qLRGg.png"/></div></div></figure><p id="cb02" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">然后执行梯度下降以最小化两者之间的误差。</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d970a088e9b38a6634f4188926b2c56c.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*2P1UknhVIIoOE7blUx32Jw.png"/></div></figure></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="d933" class="ko kp iq bd kq kr oq kt ku kv or kx ky kf os kg la ki ot kj lc kl ou km le lf bi translated">深度 Q 网络</h1><p id="b143" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">发表在(Mnih 等人，2013 年)的 Deep Q learning 利用深度学习的进步从<em class="nf">高维度感官输入中学习策略。</em>具体来说，它使用卷积网络从 Atari 2600 游戏中学习原始像素，而不是低维特征向量。下图展示了 DQN 的建筑:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ov"><img src="../Images/7320420dece9ebc907adb6771aba2e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HZwudtDQwNDUIPjFOhvnqQ.png"/></div></div><figcaption class="nb nc gj gh gi nd ne bd b be z dk">Source: <a class="ae mh" href="https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/DQNBreakoutBlocks.png" rel="noopener ugc nofollow" target="_blank">https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/DQNBreakoutBlocks.png</a></figcaption></figure><p id="e6ca" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">为了使计算更加可行，原始的 4 帧游戏图像(RGB 像素)被缩小到 4 帧(84 x 84)图像，从而得到一个(84 x 84 x 4)张量。然后我们把它输入到一个卷积神经网络，它输出一个包含每个动作的 Q 值的向量。从那里，我们使用一个探索方案(通常是 epsilon-greedy ),并在具有最高 Q 值的动作和随机动作之间进行概率选择。</p><p id="5550" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">在更高的层面上，深度 Q 学习是这样工作的:</p><ol class=""><li id="e318" class="nu nv iq li b lj mc lm md lp nw lt nx lx ny mb ow oa ob oc bi translated">使用当前策略在重放缓冲区中收集和存储样本</li><li id="b36c" class="nu nv iq li b lj od lm oe lp of lt og lx oh mb ow oa ob oc bi translated">从重放缓冲器中随机抽样批次的体验(称为<strong class="li ja">体验重放</strong>)</li><li id="c5b7" class="nu nv iq li b lj od lm oe lp of lt og lx oh mb ow oa ob oc bi translated">使用采样的经验来更新 Q 网络</li><li id="ff3c" class="nu nv iq li b lj od lm oe lp of lt og lx oh mb ow oa ob oc bi translated">重复 1-3</li></ol><p id="3827" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">我们将进一步了解步骤(2)和(3)，这将让我们直接进入实现阶段。</p><h1 id="33fe" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">体验回放</h1><p id="7f80" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为什么我们要随机抽样经验，而不是仅仅使用过去的连续经验？连续的经历彼此(在时间上)高度相关。在统计学习和优化任务中，我们希望我们的数据<strong class="li ja">独立分布</strong>。也就是说，我们不希望我们提供的数据以任何方式相互关联。经验的随机抽样打破了这种行为的时间相关性，并将其分布/平均到许多以前的状态。通过这样做，我们避免了模型中的显著振荡或发散——相关数据可能产生的问题。</p><h1 id="a6f0" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">更新 Q 网络</h1><p id="40fb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为了更新 Q 网络，我们希望最小化目标 Q 值(根据贝尔曼方程)和当前 Q 输出之间的均方误差:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ox"><img src="../Images/9ced3b21eff07b8012c6d1a3609ddcef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ollSneCLG6-U8lLU0cWJ-A.png"/></div></div></figure><p id="3e48" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">在哪里</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oy"><img src="../Images/e8a5d9fd6d40f7befeaa655ad9178564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXjgKhy-TNkduzdZd5P1RQ.png"/></div></div></figure><p id="94f5" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">最理想的情况是，我们希望误差减少，这意味着我们当前政策的输出越来越接近真实的 Q 值。因此，利用如上定义的损失函数，我们根据以下等式对损失函数执行梯度步骤:</p><figure class="mu mv mw mx gt my gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi oz"><img src="../Images/de37462e877b988a0ef30e9d1c40992a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gK422bgcgvrdD5kB8l4PIg.png"/></div></div></figure></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><h1 id="cc8b" class="ko kp iq bd kq kr oq kt ku kv or kx ky kf os kg la ki ot kj lc kl ou km le lf bi translated">实施指南</h1><p id="167a" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们将从构建配备有卷积神经网络的深度 Q 网络开始:</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="2952" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">在<code class="fe oi oj ok ol b">forward</code>函数中，我们输入像素图像，并通过我们的模型输出对应于每个动作的 Q 值向量。</p><p id="5006" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">然后，我们将构建我们的重放缓冲区，在那里我们可以存储体验—(状态、动作、奖励、下一个状态、bool(is_done))转换—以及用于学习的随机体验样本:</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="c542" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">接下来，我们编写一个函数来计算每个梯度步长的值损失。这看起来像:</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="1960" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">最后，我们将把它们都放在我们的 DQN 代理中:</p><figure class="mu mv mw mx gt my"><div class="bz fp l di"><div class="pa pb l"/></div></figure><p id="9706" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">香草 DQN 的实现到此结束。您可以在我的 GitHub 资源库中找到完整的可运行实现:</p><div class="pc pd gp gr pe pf"><a href="https://github.com/cyoon1729/deep-Q-networks" rel="noopener  ugc nofollow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd ja gy z fp pk fr fs pl fu fw iz bi translated">cy oon 1729/深度 Q 网络</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">Q-learning 家族(PyTorch)算法的模块化实现。实现包括:DQN，DDQN，决斗…</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">github.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt mz pf"/></div></div></a></div></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="a41d" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">在本系列的后面部分，我们将探索 DQN 的许多变体，它们在许多方面改进了原来的版本。</p><p id="fd32" class="pw-post-body-paragraph lg lh iq li b lj mc ka ll lm md kd lo lp me lr ls lt mf lv lw lx mg lz ma mb ij bi translated">感谢阅读！</p><h1 id="5a5d" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">参考资料:</h1><ul class=""><li id="40bf" class="nu nv iq li b lj lk lm ln lp pu lt pv lx pw mb nz oa ob oc bi translated"><a class="ae mh" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank">用深度强化学习玩雅达利(Mnih et al. 2013) </a></li><li id="1301" class="nu nv iq li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated">强化学习:导论(萨顿和巴尔托)</li></ul><h1 id="3c9a" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">下一篇文章:</h1><p id="9823" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><a class="ae mh" href="https://towardsdatascience.com/tagged/Dqn-family" rel="noopener" target="_blank">我的系列</a>将从香草深度 Q 学习(这篇文章)开始，直到 Deepmind 的彩虹 DQN，当前的艺术状态。查看<a class="ae mh" rel="noopener" target="_blank" href="/double-deep-q-networks-905dd8325412">我的下一篇文章</a>关于用双 Q 学习减少高估偏差！</p><ol class=""><li id="6d07" class="nu nv iq li b lj mc lm md lp nw lt nx lx ny mb ow oa ob oc bi translated">深度 Q 网络</li><li id="9282" class="nu nv iq li b lj od lm oe lp of lt og lx oh mb ow oa ob oc bi translated"><a class="ae mh" rel="noopener" target="_blank" href="/double-deep-q-networks-905dd8325412">双深 Q 网络</a></li></ol></div></div>    
</body>
</html>