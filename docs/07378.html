<html>
<head>
<title>Demystifying Convolutional Neural Networks using ScoreCam</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 ScoreCam 解密卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-convolutional-neural-networks-using-scorecam-344a0456c48e?source=collection_archive---------11-----------------------#2019-10-16">https://towardsdatascience.com/demystifying-convolutional-neural-networks-using-scorecam-344a0456c48e?source=collection_archive---------11-----------------------#2019-10-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/17dbca927300074c7482fc6df2dda339.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*crHGH8nfsMSafpep"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">(<a class="ae kf" href="https://unsplash.com/photos/hKVg7ldM5VU" rel="noopener ugc nofollow" target="_blank">source</a>)</figcaption></figure><p id="6eea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最近，越来越多的注意力集中在机器学习模型的可解释性上，主要是深度学习模型，因为它们具有黑箱性质。一种重要的深度学习架构是卷积神经网络(CNN ),它在计算机视觉领域取得了突破，包括图像分类、对象检测、语义分割、实例分割、图像字幕等。在 CNN 的改进和发展方面的进展是指数级的，并且架构已经被极大地简化，但是预测结果不能被分解成直观的和完全可理解的部分。</p><p id="421a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">理解和解释模型对于建立人们对我们系统的信心至关重要。CNN 解决的最常见的问题是图像分类，通常称为显著图或属性图的视觉解释方法用于找到影响模型预测的最有影响力的特征。有几种用于生成显著图的技术，其中最常见的是基于梯度的可视化，其反向传播关于输入层的目标类分数的偏导数。然而，基于梯度的地图的问题是它们通常质量低并且具有随机噪声。其他方法包括基于扰动的方法，该方法通过向输入添加小噪声(扰动)并观察预测的变化来工作。另一个是类激活图(CAM ),我最近在我的<a class="ae kf" rel="noopener" target="_blank" href="/demystifying-convolutional-neural-networks-using-class-activation-maps-fe94eda4cef1">帖子</a>中解释了它以及它们的扩展，比如 Grad-Cam 和 Grad-Cam++。它们通常通过激活图的线性加权组合来生成显著图，以突出图像空间中的重要区域。然而，它们也遭受与图像中的目标对象无关的随机噪声的问题，并且权重不能很好地捕捉每个激活图的重要性。</p><h1 id="ffa9" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">计分摄像机:</h1><p id="8feb" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在这篇文章中，我将尝试解释最近的一篇论文 Score-Cam，它是对前面提到的方法的改进，并试图使 CNN 的可解释性与其前辈一样。</p><p id="2b6c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Score-Cam 建立在类激活映射的基础上，并声称可以解决以前报告的无关噪声问题，并生成更清晰和有意义的解释。在 Score-Cam 中，研究人员遵循基于扰动的方法的思想，该方法掩盖了原始输入中的部分区域，并观察目标分数的变化。所获得的激活掩模被视为输入图像的一种掩模，其掩蔽输入图像的部分，并使模型在部分掩蔽的图像上进行预测。然后利用目标类的分数来表示激活图的重要性。</p><p id="230e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用基于扰动的方法的屏蔽激活示例</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/951cd2cb5b72118fdf40ec15fbe8e17a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*zO4CSNDqH4AK8E6LBrqoYA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mm">Figure 1. Input Image</strong></figcaption></figure><div class="mi mj mk ml gt ab cb"><figure class="mn ju mo mp mq mr ms paragraph-image"><img src="../Images/4c92725782b44cdfd01e46308cbfb117.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*xUa5fM9LFNDoFFMmCCKcRA.png"/></figure><figure class="mn ju mo mp mq mr ms paragraph-image"><img src="../Images/9a92aa03a8dbd8f68b690d760f057ec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*img6aqRbonBpshHymEp7bw.png"/></figure><figure class="mn ju mo mp mq mr ms paragraph-image"><img src="../Images/75ed641a6f101606d7147f7705a1bc21.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*vzziW-PIBdiITJLHqElxOA.png"/><figcaption class="kb kc gj gh gi kd ke bd b be z dk mt di mu mv">Figure 2. Masks applied on Input Image.</figcaption></figure></div><p id="2a8f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与著名的 Grad-Cam 不同，Score-Cam 没有利用梯度，因为研究人员认为传播的梯度非常不稳定，并在基于梯度的显著图中产生随机噪声。图 3 中显示了梯度的不稳定特性，其中当输入图像稍微改变时，梯度会急剧改变，即使这种改变不会被人眼察觉，也不会改变预测结果。这就是为什么有理由怀疑基于梯度的方法减少冗余噪声的有效性。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/f6b3f3893312c51ae5d246b9997c8d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DfU6x-dRwH-5HhFOdcmy1Q.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 3. The partial derivative changes sharply for indistinguishable changes in the input image.</figcaption></figure><h1 id="7a72" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">方法:</h1><p id="0664" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">各种以前的作品，如凸轮，GradCam 等。已经断言了这样一个事实，即 CNN 中更深的层捕获更高级别的视觉信息。此外，卷积要素自然会保留在完全连接的图层中丢失的空间信息，因此通常期望最后一个卷积图层在高级语义和详细空间信息之间提供最佳折衷，并且这些图层中的神经元会在输入图像中寻找特定于类的语义信息。因此，在 Score-Cam 中，我们使用最后一层来获得包含平衡表示的激活图。</p><p id="7f96" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，GradCam 和 GradCam++使用流经 CNN 最后一层的梯度信息来表示每个激活图的重要性。在 Score-Cam 中，我们使用特定目标类的得分权重。因此，Score-Cam 可以摆脱对梯度的依赖，并作为更通用的框架工作，因为它只需要访问模型的类激活图和输出分数。</p><p id="19fa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Score-Cam 的流水线如图 4 所示，显示了实现最终显著图的所有步骤。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/77a2d376ec89feb21018d19ccfa1dc33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2NGWNzZC__E274CCL8Fl_g.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Figure 4. The pipeline of the Score-Cam.</figcaption></figure><p id="c031" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了使用 Score-Cam 获得类别区分显著图，该过程被分成以下步骤:</p><ol class=""><li id="592d" class="my mz it ki b kj kk kn ko kr na kv nb kz nc ld nd ne nf ng bi translated">第一步是将图像传递给 CNN 模型并执行 forward_pass。在正向传递之后，从网络中的最后一个卷积层提取激活。</li><li id="d78a" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">从具有 1xmxn 形状的最后一层获得的每个激活图然后使用双线性插值被上采样到与输入图像相同的尺寸。</li><li id="90d7" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">在对激活图进行上采样之后，所得到的激活图被归一化为[0，1]范围内的每个像素，以保持像素之间的相对强度。使用图 5 中的<strong class="ki iu">所示的公式实现标准化。</strong></li></ol><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/434b19f072b000f1aac9ef5610f7ea28.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*clIeZuCqizNVfvetzkK12Q.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mm">Figure 5. </strong>Normalizing each Activation Map</figcaption></figure><p id="e74d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.在激活图的归一化完成之后，通过将每个归一化的激活图(<strong class="ki iu">1×W×H)</strong>乘以原始输入图像(<strong class="ki iu">3×W×H</strong>)以获得形状为<strong class="ki iu">3×W×H .</strong>的掩蔽图像 M，激活图的高亮区域被投影到输入空间上</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nn"><img src="../Images/dafa75f0bca3e974d77a7b076d7306e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*gihJvpV2l77hc6z5_v2vEA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mm">Figure 6.</strong> Element-Wise Multiplication of all the activation maps with the Input Image.</figcaption></figure><p id="4561" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.由此获得的掩蔽图像<strong class="ki iu"> M </strong>然后被传递到具有 SoftMax 输出的卷积神经网络。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi no"><img src="../Images/38f1495996845de6106fb7021a86b8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*EJTVu6FY3afgIqZu8q21xw.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mm">Figure 7.</strong> The Output Score Sk is obtained by the Softmax Operation.</figcaption></figure><p id="45ea" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.在得到每个类的分数后，我们提取目标类的分数来表示第 k 个激活图的重要性。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi np"><img src="../Images/617bc06a983374996fa9e6c349b40145.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*MQ1_MQWVjiaB37TZm2uVLQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mm">Figure 8</strong>. The score of Target Class used as weights.</figcaption></figure><p id="ccd2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7.然后，我们计算目标类得分和每个激活图之间的线性组合的所有激活图的总和。这导致单个激活映射具有与输入类相同的大小。</p><p id="7b58" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">8.最后，我们将逐像素 ReLU 应用于最终的激活图。我们应用 ReLU 是因为我们只对对感兴趣的类有积极影响的特性感兴趣。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c6c35f4e1cc2bf038ca33fd7d9a93d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*Loi5PhEBtKmMPkBd7_34Mg.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><strong class="bd mm">Figure 9.</strong></figcaption></figure><h1 id="5ef9" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">分数 CAM 在 KERAS 中的实现；</h1><p id="72ca" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">我们将遵循之前在图和方法中描述的相同管道。</p><ol class=""><li id="616f" class="my mz it ki b kj kk kn ko kr na kv nb kz nc ld nd ne nf ng bi translated">我们使用在 Imagenet 上预先训练的 VGG16 作为整个管道的模型。</li></ol><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1bd61c204fadff8d05310e9a8665afd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*OvrB9tzdsOhSmtQxaV2l6g.png"/></div></figure><p id="e51c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.我们加载上面图 1 的<strong class="ki iu"> </strong>中所示的输入图像，对其进行预处理，使其适合传递到 VGG16 模型中。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/863eee0c75f89955221d5d819bbe6ffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*IvOZee75SlKGwmyCRd5GYg.png"/></div></figure><p id="a9be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.之后，我们通过模型传递图像，并获得每个类别的预测分数。我们从预测中提取目标类的索引。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nt"><img src="../Images/799ccfe596ef323d75fcfefbb0a5a712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OtLy3XUELdTGRcrJwOEfjg.png"/></div></div></figure><p id="0cd2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">4.然后，我们从大小为(<strong class="ki iu">1×14×14×512</strong>)的最终卷积层中提取激活。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nu"><img src="../Images/30caea2730d28f01a280426a3ae6e9f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63usFiXtGHR2CTVSp4_myg.png"/></div></div></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/7764f4258fce2e0f1a97742c1587b011.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*aEFLwGOisex81LqgCnDcMA.png"/></div></figure><p id="311c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">5.然后，我们对所有激活图进行上采样，以匹配原始输入图像的大小。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nw"><img src="../Images/fb788ee9b222ea2772f2f94d3c1ee8a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DeFFYeyIjh5rgzP00QSa6w.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Function to Upsample Activations</figcaption></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nx"><img src="../Images/650caf2eab93e640b8923a70535b5d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bAosNkNXzNzcJI_mlHs_QA.png"/></div></div></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ny"><img src="../Images/389e2df3835a6ed7328d213a9090add8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*utC6oq3au8s6NRlasCvhrw.png"/></div></div></figure><p id="abdf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">6.然后，我们使用图 5 中的公式对 512 个激活图进行标准化。</p><blockquote class="nz oa ob"><p id="ce0a" class="kg kh oc ki b kj kk kl km kn ko kp kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated"><strong class="ki iu">注意:我们在分母中添加了一个非常小的项，即 1e-5，以防止被零除的误差导致 nan 值。</strong></p></blockquote><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi og"><img src="../Images/8074c292f6de95567cd7b9b090915fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aLiVWkx3hLaS9Sa-8lKjtw.png"/></div></div></figure><p id="23b7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">7.在步骤 6 之后，我们通过在掩模和输入图像之间执行元素乘法来投影在原始图像上产生的掩模，如图 6 中的<strong class="ki iu">所示。</strong></p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/23ca1414658275369bb234d5dff1f4d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2havVGL1XoMJp169eE-R5A.png"/></div></div></figure><p id="b51d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">8.这样获得的掩蔽图像然后通过 VGG16 模型向前传播，并且获得 softmax 分数。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oi"><img src="../Images/ca5678e2fa6c3db8f16fc68a64b9e2f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5y8K0uz_FdKSHg5F6Wb12g.png"/></div></div></figure><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f4817d496a918db48ee8d452127756e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*nKO-VEBvzQqv_p9GeCOjRg.png"/></div></figure><p id="4139" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">9.在获得所有类的分数后，我们只需提取与我们的目标类相对应的分数。在我们的输入图像的例子中，我们有两个类，但是为了演示的目的，我们将把狗作为我们的目标类。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9b1b4a199ee791222ec3efe603ddbc54.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*mJNRkq2JnUF3ztiGtK1ZpQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Here 242 corresponds to the index of the class dog.</figcaption></figure><p id="5a0e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">10.现在，我们已经得到了生成类别区分显著图所需的全部内容，即归一化的激活图和用作权重的目标类别的分数。现在，我们执行目标类 softmax 分数和标准化激活图的逐元素乘法。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ol"><img src="../Images/13b977739704b6e85a82fe7a73e68223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLnwWUho4m7kZ1FLwTfILQ.png"/></div></div></figure><p id="ae92" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">11.在最后一步完成并且我们有了结果之后，我们对所有的激活图执行求和(512)，并且我们组合所有的图以产生形状为<strong class="ki iu">1×224×224×3</strong>的单个激活图</p><p id="1420" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">12.为了获得最终的显著图，我们对上一步中获得的激活图执行逐像素 ReLU。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi om"><img src="../Images/dfb48ae31f02ba97aba1e65dd56ba72e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*VDBeTYauXnP0Tp-6lWJ3hA.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Pixel-Wise ReLU</figcaption></figure><p id="3b9f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得的最终类别判别图如下面的图 9 所示。</p><figure class="mi mj mk ml gt ju gh gi paragraph-image"><div class="gh gi on"><img src="../Images/7e5297e47023cf4e5aa37da62f4e8dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*L6LAPRlD9q3MooByNJcmxQ.png"/></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Score-Map Saliency Map</figcaption></figure><p id="4523" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所看到的，分数凸轮是阶级歧视，以及有更少的噪音相比，其前身。</p><h1 id="891f" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Score-Cam 的优势:</h1><ol class=""><li id="c86a" class="my mz it ki b kj mc kn md kr oo kv op kz oq ld nd ne nf ng bi translated">像 Grad-Cam 和 Grad-Cam++这样的 Score-Cam 可以用在任何卷积神经网络架构中，并且不需要像 Cam 那样重新训练模型来产生显著图。</li><li id="035b" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">Score-Cam 具有类别区分性，可以去除不相关的噪声，从而生成有意义的显著图。</li><li id="ac5f" class="my mz it ki b kj nh kn ni kr nj kv nk kz nl ld nd ne nf ng bi translated">使用 Softmax 分数作为权重，并消除对不稳定梯度的依赖。</li></ol><h1 id="6bd6" class="le lf it bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论:</h1><p id="25fc" class="pw-post-body-paragraph kg kh it ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">在这篇文章中，我们讨论了最近发布的一篇关于基于分数的类激活映射(Score-Cam)的新架构的论文，该架构提供了一种解决模型可解释性问题的新方法。该论文介绍了一种架构，其中去除了无用的噪声，并且仅产生重要的显著图。此外，它消除了对目标类别梯度的依赖，并产生了一种更通用的方法来产生显著图。本文从两种产生显著图的方法中得到启发，主要是扰动图和类激活图，并提出了一种结合两者优点的方法。</p><p id="d0d3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解更多关于 Score-Cam 的信息，请阅读以下链接提供的论文【https://arxiv.org/abs/1910.01279<a class="ae kf" href="https://arxiv.org/abs/1910.01279" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="1866" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望你喜欢这篇文章，如果有进一步的讨论、疑问或相关内容，你可以通过<a class="ae kf" href="https://www.linkedin.com/in/divyanshu-mishra-ai/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>联系我，或者通过<a class="ae kf" href="https://twitter.com/Perceptron97" rel="noopener ugc nofollow" target="_blank"> Twitter 关注我。</a></p></div></div>    
</body>
</html>