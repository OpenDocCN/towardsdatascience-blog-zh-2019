# 强化学习从零开始，无需复杂的虚拟环境包

> 原文：<https://towardsdatascience.com/reinforcement-learning-from-scratch-without-complex-virtual-environment-packages-631909a290c7?source=collection_archive---------17----------------------->

这个项目是作为在 Python 笔记本中独立学习强化学习(RL)的一种手段而创建的。无需与复杂的虚拟环境交互即可应用 RL。通过完全定义概率环境，我们能够简化学习过程，并清楚地展示改变参数对结果的影响。这在任何机器学习任务中都是有价值的，但在强化学习中尤其如此，在强化学习中，如果没有清晰明确的示例，很难理解影响变化的参数。

![](img/8b69708023676dce508de7946dc05d9e.png)

Model-Based vs Model-Free Learning

# 介绍

目的是在扔或移动到更好的位置之间找到最佳动作，以便将纸放入垃圾箱(垃圾桶)。在这个问题中，我们可以从房间的任何位置投掷，但它的概率与当前距离垃圾箱的距离和纸张投掷的方向有关。因此，可以采取的行动是将纸向任何 360 度方向扔，或者移动到一个新的位置，尝试增加扔进垃圾箱的概率。

在前面的第 1 部分中，我们介绍了这样一个问题，即箱子的位置是已知的，可以用值迭代法直接求解。

**在第 2 部分中，我们现在展示了如果使用无模型方法(如 Q 学习、蒙特卡罗等)隐藏概率，如何类似地使用 RL 来寻找最优策略。**

 [## RL 从头开始第 2 部分:了解 RL 参数| Kaggle

### 编辑描述

www.kaggle.com](https://www.kaggle.com/osbornep/rl-from-scratch-part-2-understanding-rl-paramters) 

**此外，我们以此为契机，引入一种新的可视化方法来检查参数。**当我们改变强化学习中的参数时，我们通过多次尝试(或片段)来观察结果，我们希望显示稳定性和收敛性。这是一个二维比较(x =事件，y =输出)，当我们想要观察结果时，如果我们改变一个参数，这就变成了三维比较(x =事件，y =输出，z =参数)。最简单和最常用的解决方案是为每个参数选择生成多个图。另一种更复杂的视觉选择是三维绘图。

**相反，我们介绍了一种新的交互式动画方法，通过这种方法可以显示参数随时间的变化。**

这种可视化的目的是改善你比较参数选择的方式。然而，由于参数选择通常是一个需要快速执行的过程，我们承认这一要求必须易于实现。最终的参数选择可以被适当地格式化，但是决策过程中的视觉美感可能不太严格。

因此，尽管这是一个我们在之前已经[介绍过的方法，但我们后来进一步将它正式定义为一个无需大量了解 Plot.ly 库就可以轻松下载和使用的包(Github 正在发布)。](/creating-interactive-animation-for-parameter-optimisation-using-plot-ly-8136b2997db)

我们将在本笔记本中演示如何使用绘图功能。下面显示了一个可以使用的交互式动画的快速示例，我们可以看到前面讨论过的尺寸。

**最后，模型可解释性的最新研究强调了清晰一致地用图解法概述方法的需求。**

[来自谷歌](https://arxiv.org/abs/1810.03993)的研究人员引入了**模型卡**作为一种在训练算法中提供透明度的手段。

*经过训练的机器学习模型越来越多地用于执行执法、医学、教育和就业等领域的高影响力任务。为了阐明机器学习模型的预期用例，并尽量减少它们在不太适合的环境中的使用，我们建议发布的模型应附有详细描述其性能特征的文档。*

![](img/7d23c88e79d5bee55054ddc22a4e3a05.png)

Model Cards Spec

此外，我的一位同事正在介绍一个框架，用于将图表方法正式化，以便熟练的研究人员共享他们的工作；称为[拨](https://arxiv.org/abs/1812.11142)。这项研究试图识别人工智能系统的重复出现的原语和构建模块，并提出一种示意性符号来尝试和促进关于人工智能系统研究的改进的交流。

*目前，还没有一致的模型来可视化或形式化地表示人工智能系统的架构。这种表示的缺乏给现有模型和系统的描述带来了可解释性、正确性和完整性的挑战。DIAL(图形化的人工智能语言)是为了成为人工智能系统的“工程示意图”而创建的。它在这里被作为一个面向 AI 系统的公共图形语言的社区对话的起点。*

**因此，我们创建了一个视觉效果来展示和总结整个工作流程，以生成最终输出:**

![](img/bc07da00024720f7cb35c2e2afdda3d4.png)

Complete RL Process Diagram

# 预处理:介绍概率环境

根据之前定义的环境(参见[元笔记本](https://www.kaggle.com/osbornep/meta-environment-defined-only-for-forking))，我们还发现了通过价值迭代计算出的最优策略。

最佳策略可以从数据文件中导入，并且是固定的，假定条柱在(0，0)处，概率的计算如下面的函数所示。

## RL 环境定义

一个**策略**是当前为所有给定状态推荐的动作。**状态(** s **和**s′**)**是房间中的位置，而**动作(** a **)** 要么在 8 个方向(北、东北、东、…西北)中的一个方向上移动，要么从正北向任何 360 度方向(0、1、2、3、4、…、359、360 度)投掷(参见[导航方位](https://en.wikipedia.org/wiki/Bearing_(navigation))。

在我们的概率环境中，我们已经定义了**移动具有被正确跟随的保证结果**(即，你不会错过步伐)，但是**向一个方向投掷不能保证进入箱子**。投掷成功进入箱子的概率与当前位置到箱子的距离和从真实方向投掷的方向有关。

这定义了我们的概率环境和转移函数:

P(s，s′)= P(s _ { t+1 } = s′| s _ t = s，a_t=a)

是在动作 a([https://en.wikipedia.org/wiki/Reinforcement_learning](https://en.wikipedia.org/wiki/Reinforcement_learning))下从状态 s 转移到状态 s’的概率。

(注:bin 是垃圾桶/垃圾箱/垃圾桶/垃圾筐/垃圾桶/垃圾桶/垃圾桶/废纸篓的英文说法)

![](img/f2a72315d50aae4cd812bd04317e6138.png)

# 概率函数

该函数定义了从任何给定状态成功投掷的概率，并通过以下公式计算:

首先，如果位置与箱相同(即人已经直接在箱内)，则概率固定为 100%。

接下来，我们必须在两种情况下重新定义投掷方向，以适应 360 度与 0 度相同的事实。例如，如果我们在箱子的西南方，并投掷 350 度，这将与-10 度相同，然后将正确地与从人到箱子的小于 90 度的方位相关联。

然后计算欧几里得距离，接着是人可能离箱子的最大距离。

然后，我们按照前面的图计算从人到容器的方位，并计算限制在+/- 45 度窗口内的分数。最接近真实方位的投掷得分较高，而较远的投掷得分较低，任何大于 45 度(或小于-45 度)的投掷都是负的，然后被设置为零概率。

最后，给定当前位置，总概率与距离和方向都相关。

# 初始化状态-动作对

在应用算法之前，我们将每个状态-动作值初始化到一个表中。首先，我们为所有投掷动作形成这个，然后是所有移动动作。

我们可以向任何方向投掷，因此每度有 360 个动作，从北 0 顺时针到 359 度。

虽然运动看起来很简单，因为有 8 种可能的动作(北、东北、东等)，但不像从任何位置向任何方向投掷，有些运动是不可能的。例如，如果我们在房间的边缘，我们不能移动超出边界，这需要考虑。虽然这可以编码得更好，但我已经用 if/elif 语句手动完成了，如果位置和移动是不可能的，就跳过这一行。

![](img/2037772ffba3480cff3a28b692c83506.png)

Initialised Q Table

# 定义无模型强化学习方法

我们介绍了三种无模型方法，它们被认为是最容易应用的，并比较了它们的优缺点。然而，在我们这样做之前，我们考虑无模型方法和我们以前使用的基于价值迭代模型的方法之间的差异。简而言之，基于模型的方法使用概率环境的知识作为指导，并相应地计划最佳行动。在无模型方法中，算法不知道这个概率，它只是尝试动作并观察结果。

在本例中，我们已经计算了概率，并将使用这些概率来查找操作的结果，但它们不会直接用于算法的学习中。

此外，在基于模型的方法中，我们在大的“扫描”中更新所有动作，其中所有状态的值在一次通过中被更新。在无模型方法中，我们使用情节，其中仅更新被访问的状态。**片段**是从开始状态到结束状态的路径；在我们的例子中，终端状态是当算法抛出试卷时，结果可能是成功或失败。

![](img/8b69708023676dce508de7946dc05d9e.png)

# 形成情节并定义行动选择过程

如果我们定义起始位置，一集就是从那个位置一直到扔纸为止所采取的动作。如果它到达垃圾桶，那么我们有一个正的目标奖励+1。然而，如果我们错过了 bin，那么我们的目标奖励为-1。

## 动作选择

我们可以继续这个选择过程，但是这是一个非常低效的选择行动的方法。当我们实施我们的学习过程时，我们将开始学习哪些行动会导致积极的目标，所以如果我们继续随机选择，我们就浪费了所有的努力。

因此，我们引入一种考虑到这一点的方法，称为**ε贪婪**。

试验的比例 1−ϵ *选择最佳杠杆，比例* ϵ *随机(等概率)选择一个杠杆。典型的参数值可能是*ε= 0.1*，但这可能会根据环境和偏好而有很大变化。* ( [维基](https://en.wikipedia.org/wiki/Multi-armed_bandit))

换句话说，我们用概率ϵϵ随机选择一个动作，否则将选择最佳动作。如果我们有多个“最佳行动”,我们会从列表中随机选择。

那么，我们为什么不每次都选择最佳行动呢？如果我们有一个有效的行动，但不一定是最好的，这可能会导致一个问题。这在其他机器学习问题中经常被认为是局部最小值/最大值。如果我们一直使用一个似乎有效的行动，我们可能会错过尝试更好行动的机会，因为我们从未尝试过，这可能会导致结果的不稳定。

下面的动画演示了我们降低ε时的结果。ε值较高时，我们会随机选择行动，因此可能会选择不好的行动。随着我们减少ε，我们会越来越贪婪地选择行动来改善结果，同时仍然确保我们可以探索新的行动来最小化我们处于局部最大值而不是全局最大值的风险。

**我们因此选择一个小的ε值** ϵ=0.1

# RL 算法简介

我们已经介绍了情节和如何选择动作，但我们还没有演示算法如何使用它来学习最佳动作。因此，我们将正式定义我们的第一个 RL 算法，*时间差异 0* 。

## 时间差—零

时间差异λ是取决于λ的选择的一系列算法。最简单的方法是将其设置为零，此时更新规则如下:

**定义:TD(0)更新规则:** [Wiki](https://en.wikipedia.org/wiki/Temporal_difference_learning)

![](img/9cbe8c8ef82c77140cac80cd489af983.png)

其中:

*   V(s)是状态 s 的值，
*   α是**学习率参数**，
*   r 是奖励，
*   γ是**贴现因子参数**，
*   v(s′)是下一个状态的值。

*那么这个等式是什么意思呢？*简而言之，我们基于当前状态的值是什么和采取行动到情节中定义的下一个状态的结果的组合，更新我们对当前状态的质量的知识，表示为 V(s)。

例如，假设我们开始学习过程，我们的第一个动作是从状态[-5，-5]投掷，并且它成功地击中了垃圾箱，那么我们因为达到目标而获得了+1 的正奖励。因此，我们有以下更新:

![](img/3965a2a53f36c29f5df1049733f02716.png)

这似乎是一个微不足道的计算，但重要的是要记住，成功是不能保证的。因此，如果我们考虑所有可能的行动，第一次投掷的结果意味着我们相信这次投掷行动是当前的最佳选择。这个投掷动作的值为 0.5，相比之下，所有其他尚未测试的动作的值为 0。

因此，根据ϵ−greedy 的遴选程序，我们将再次尝试。然而，这一次，纸张没有进入垃圾箱，而是未命中，因此我们得到了负的终端奖励 1 1:

![](img/a0a4aba645b9bdf250b87fc9a26b72d3.png)

所以我们看到，这个状态的值现在稍微减少了，因为第二次投掷。

**强化学习的核心概念是，我们通过重复采样来测试动作；我们需要重复样本的数量，直到结果收敛到真实概率结果的估计。**

例如，如果我们考虑扔硬币两次，我们很可能两个结果都是正面，但如果我们扔 100 次，我们可能会看到正面和反面各占一半。在我们的例子中，如果从状态[-5，-5]投掷是一个好的动作，那么总的来说，重复尝试应该会产生积极的结果。起初这可能很难理解，但简单来说，我们正在通过反复试验来测试动作，并让我们的算法完成所有工作，所以我们不必这样做。

**注:目前，我们将把起始状态固定为[-5，-5]，参数固定为** ϵ=0.1 **、** α=0.5 **和** γ=0.5 **，直到我们稍后演示参数变化。**

**在 100 集之后，我们看到固定起始点周围的状态已经更新，但是如果我们将下面的热图与前面的线图并排比较，我们会发现在 100 集之后，它还没有完全收敛，并且仍在更新。**

![](img/af1c987fe1e5bdb52bed6af6fd8b8460.png)

Initial TD(0) Output

**因此，我们将剧集数量从 100 集大幅增加到 1000 集**

由于我们开始发现这需要越来越长的时间，一个好主意是引入一种方法来跟踪循环的进程。为了做到这一点，我应用了这篇文章中介绍的方法。

![](img/f8bd411ea1eb74074adf5febc2baf6af.png)

Increase to 1,000 episodes

**不同的奖励**

我们注意到，这样的结果表明状态的值非常负，并且它们是发散的(即不稳定)。

我们可以采取一些措施来改善这一点，首先，我们将对其他行为进行奖励。目前，我们仅有的奖励是当算法投出正球并获得+1 或负球获得-1 时。

这是强化学习过程的一部分，它让我们控制算法优化的内容。例如，假设我们想阻止算法投掷，我们可以为每个移动动作引入一个小的正奖励(比如 0.1)，如下所示。

![](img/297c61a90be1a405e0879e20f0af29ac.png)

Add reward for moving: r_move = 0.1

虽然这一开始看起来更糟，但状态振荡的值表明它试图找到一个值，但我们对参数的选择导致它发散。但是，我们至少可以看到它越来越接近收敛。

我们可以开始改变参数，但部分问题是我们总结了大量动作(360°投掷方向和 8 个移动方向)的状态值。因此，与其将此归纳为一个值，不如单独考虑每个状态-动作对的质量。

为此，我们可以介绍我们的第二种无模型方法: **Q-learning** 。

# q 学习

与 TD(0)非常相似，Q-learning 在我们采取每一个动作的同时进行学习，而是通过搜索可能的后续动作来学习得更快。

**定义:Q-学习更新规则:** [Wiki](https://en.wikipedia.org/wiki/Q-learning)

![](img/0e8b1b947ce09913c80a08b4a2ab17eb.png)

其中:

*   Q(s_t，a_t)是状态-动作对 s 的值，
*   α是**学习率参数**，
*   r 是奖励，
*   γ是**贴现因子参数**，
*   Q(s_t+1，a)是下一个状态中动作对的值。

和以前一样，我们将参数固定为ϵ=0.1，α=0.5，γ=0.5。

![](img/67b8533bc980a0a24f3c80bd3714e98b.png)

Q-Learning Initial Run

# 变化参数

我们有三个要改变的主要参数，学习率αα，折扣因子γγ和我们的ϵ−greedyϵ−greedy 动作选择值。

下面的解释直接来自维基百科[并且已经详细介绍了ϵϵ参数。](https://en.wikipedia.org/wiki/Q-learning)

**探索与利用**

*学习速率或步长决定了新获得的信息覆盖旧信息的程度。因子 0 使代理什么也学不到(专门利用先前的知识)，而因子 1 使代理只考虑最近的信息(忽略先前的知识来探索可能性)。在完全确定的环境中，* αt=1 *的学习率是最佳的。当问题是随机的时，算法在某些技术条件下收敛于学习率，这要求它下降到零。在实践中，经常使用恒定的学习速率，例如对于所有的* t *，αt=0.1 *。[3]**

**折扣系数**

*贴现因子* γγ *决定了未来奖励的重要性。因子 0 会使代理人只考虑当前的奖励而“近视”(或短视)，即* rt *(在上面的更新规则中)，而接近 1 的因子会使其争取长期的高奖励。如果折扣因子达到或超过 1，行动值可能会出现分歧。对于* γ=1 *，没有终端状态，或者如果代理从未达到一个终端状态，所有环境历史变得无限长，并且具有附加的、未折扣的回报的效用通常变得无限长。[4]即使贴现因子仅略低于 1，当用人工神经网络近似价值函数时，Q 函数学习也会导致误差传播和不稳定性。[5]在这种情况下，从一个较低的折扣系数开始，并将其提高到最终值，可以加速学习。【6】*

**那么这些意味着什么，更重要的是，我们选择参数的目的是什么？**

总的目标是，我们试图为任何给定的状态找到最佳的行动，同时以合理的努力次数(以情节次数、所需的计算或时间来衡量)实现这一目标。对**学习率**的一个很好的解释是，高值意味着我们更重视在每个动作中获得的信息，因此学习更快，但可能会发现很难完全收敛，而小值将需要更长时间，但会稳定地收敛。

一个很好的类比是把它想象成我们只用一根球杆打高尔夫球；高α值对应于使用大击球杆，而小α值类似于使用小击球杆。击球大的球杆最初会让我们更接近果岭，但一旦我们接近，就很难准确地击球入洞。然而，一个小击球的俱乐部将需要更多的尝试来达到果岭，但一旦它做到了，我们就有更多的控制，可以更容易地到达球洞。

![](img/ad9b3c2767d7d2c33b409d98d53fca8f.png)

我们已经在我们早期的应用中观察到大α参数的影响，其中值在每一集之间振荡。因此，我们需要使用较小的值，但这带来了收敛所需的集数方面的挑战。我们已经需要数以千计的剧集来聚合一个固定的开始状态，并且我们有 100 集来考虑整个环境。

**这是我们必须考虑的权衡，最佳决策可能是在需要时一次只学习一个固定的起始状态，而不是试图找到所有状态的最佳策略。**

当我们在下面的动画中观察变化 alpha 的趋势时，我们看到，如果我们固定开始状态，我们能够使用一个小的 alpha 值，而不需要不可能的大量剧集。如果我们要考虑所有的状态，我们可能需要使用稍微大一点的 alpha 值来及时得到合适的结果。

**因此，我们回到考虑[-5，-5]的单一固定起始状态，并选择α值** α=0.1 **。**

**有了这些设定，我们再来评估** γ **的选择。根据维基百科的解释，我们知道这个值对应于我们是否认为未来的回报重要。当我们考虑这一点时，提醒我们自己 Q-learning 的更新规则是有帮助的。**

在 Q-learning 更新规则中，我们看到γ缩放了从下一个状态获得最佳动作的 Q 值。这与作为同一等级的一部分的行为本身的奖励有关，因此，如果我们通过使用较小的 gamma 值来减少这一点，那么奖励就具有更大的权重。相反，如果我们取一个高的伽马值，我们认为从下一个状态获得的信息更重要。

**因此，我们将理想地选择一个增加未来奖励价值的值，这样我们的决策将导致最佳的 bin，并选择值** γ=0.9 **。**

## 阿尔法分析

阿尔法= 0.9

![](img/18228d65979502d19c4d2171ea539fc0.png)

阿尔法= 0.1

![](img/eabe548ceaeb3d7cf4db1f388c5d617e.png)

## 伽马分析

伽马= 0.9

![](img/ee90f775bfb59f1c618a425c9a8e3604.png)![](img/606c5c9a1fd7b7ec96a91042b0c1f875.png)

# 最终参数输出

```
- alpha = 0.1
- gamma = 0.9
- epsilon = 0.1
- 10,000 episodes
- Fixed start state: [-5,-5]
```

输出:

```
The optimal action from the start state is to MOVE in direction:  2
```

![](img/7bc492e690f429e9b029edd6e71d8512.png)

# 结论

我们看到起始状态的最终输出是向东移动。如前所述，我们可能要考虑改变奖励，这样移动会稍微受到阻碍，因为我们的算法似乎是在收集移动的奖励，而不是达到最终目标。

剧集覆盖的所有州的结果都集中在 10，000 集内，尽管看起来许多还没有被完全探索并且不是最佳的。但是如果我们只关心起始状态，那么这些就无关紧要了。

我希望这篇笔记/文章对演示每个参数对学习的影响和一个独立例子中 RL 的整个过程是有用的。

谢谢