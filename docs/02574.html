<html>
<head>
<title>Principal Component Analysis the Machine Learning Perspective (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析:机器学习的视角(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-the-machine-learning-perspective-part-2-a2630fa3b89e?source=collection_archive---------7-----------------------#2019-04-27">https://towardsdatascience.com/principal-component-analysis-the-machine-learning-perspective-part-2-a2630fa3b89e?source=collection_archive---------7-----------------------#2019-04-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/26ce6a0a4f4abbcff919bc820b882bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJvDXx39tybuKH1yQhE9uA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2017/sixwaysandco.jpg" rel="noopener ugc nofollow" target="_blank">https://3c1703fe8d.site.internapcdn.net/newman/gfx/news/hires/2017/sixwaysandco.jpg</a></figcaption></figure><p id="3de2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我之前的文章中，我从统计学的角度回顾了主成分分析。在本文中，我将回顾同一主题的机器学习方面。请注意，我将涉及的范围是基本的，不包括 PCA 的变体，如概率 PCA 和核 PCA。就像我之前的文章一样，内容相当专业，所以熟悉以下一些内容会让这篇文章更容易理解:<a class="ae kf" href="https://www.amazon.com/Elementary-Algebra-Classics-Advanced-Mathematics/dp/013468947X" rel="noopener ugc nofollow" target="_blank">线性代数(矩阵、特征向量/特征值、对角化、正交性)和统计学/机器学习(标准化、方差、协方差、独立性、线性回归</a>)。</p><p id="ad46" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将从寻找二维数据空间的一维投影开始。以下是我们的数据集。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi le"><img src="../Images/1c8f9be39172b82839093d9ad35a8931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pHH60XQXXbVJFYryLxrtAQ.png"/></div></div></figure><p id="e422" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们有无限多可能的一维投影:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lj"><img src="../Images/151d9ea9752f99118652b2ca901baa1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1zD8nVkQbxoZnyyooClKQ.png"/></div></div></figure><p id="ede3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们所有的可能性中，我们将关注<strong class="ki iu">正交投影</strong>。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lk"><img src="../Images/dbba9a93133edc47d3411f56c6d5882b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cSFhA_dbq-1dmvH9Ms3f-g.png"/></div></div></figure><p id="56a6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在所有可能的投影中，正交投影是我们感兴趣的原因，因为它具有最接近的矢量特性。根据<strong class="ki iu">最近向量性质，在 W 空间的所有向量中，</strong>最接近<strong class="ki iu"> u </strong>的向量就是<strong class="ki iu"> u </strong>在 W 上的正交投影，换句话说，我们希望得到最接近原始数据集的投影，以保持降维后尽可能多的信息。这是最近向量性质的证明:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ll"><img src="../Images/0bf152a77e1057af2f9094305c14c3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FpuQNpWbzfdhUjKQM5koA.png"/></div></div></figure><p id="73e0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们知道了投影的方向，让我们用数学公式来表示投影。为了做到这一点，我们将从正交投影的定义开始。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lm"><img src="../Images/e82cb6d46880604ad05354f27c3f0875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwv40Dha4pObSBybQMio5w.png"/></div></div></figure><p id="d8dd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的等式是我们的数据需要经过正交化的线性变换。最后一个表达式显示了我们最终想要的:一个新的、更低的维度上的正交投影数据。因此，<strong class="ki iu">我们希望找到一个线性算子，它用最小正交投影将我们的数据映射到一个子空间上</strong>。</p><p id="b344" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">重要的是要看到，正交投影是原始 d 维空间的 k 维子空间，用原始 d 维坐标表示。比如 k = 1，q 的 q 转置为(d x k * k x d = d x d)。看起来降维还没做，但那是因为我们参考了原始坐标的投影。如下图所示，一维投影数据是指二维坐标。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ln"><img src="../Images/89e80a9924f1d6c4d34b2f9413c906e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XxgarHMYKJY0YX53xDb9OA.png"/></div></div></figure><p id="0a36" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到这一点，我们可以将问题表述为寻找最佳线性变换 Pi(将我们的数据变换为投影到较低维度的线性算子),以最小化重建误差:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lo"><img src="../Images/878c62e184a26715a1566a54868e0dd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W9AsNXnfaTb2g5XVFX0fEA.png"/></div></div></figure><p id="4aed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不要混淆 PCA 和线性回归。PCA 是最小化<em class="lp">正交投影</em>的距离，而线性回归是最小化<em class="lp">在 y 轴上的距离</em>。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lq"><img src="../Images/c85fe28f3c6eaac851addb9579bef8b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjxDJn0hPJwKPebIDhVGuA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="https://wikidocs.net/4870" rel="noopener ugc nofollow" target="_blank">https://wikidocs.net/4870</a></figcaption></figure><p id="6fe1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<strong class="ki iu"> k </strong>维子空间中，有<strong class="ki iu"> k </strong>个正交基向量。基向量不必是正交的，但是子空间中的每个基向量可以使用<a class="ae kf" href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process" rel="noopener ugc nofollow" target="_blank"> Gram-Schmidt 过程</a>用正交基来代替，并且我们可以容易地将基向量的长度改变为 1。因此，我们对这个优化问题的约束将是基向量的长度必须是 1。我们可以重申我们的问题:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lr"><img src="../Images/88fdffbf0955a9bea364c7b631e466ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ddc1TkdOo6FvpVxP8AfrA.png"/></div></div></figure><h1 id="0630" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">将最小化转换为最大化(K = 1 的情况)</h1><p id="18c3" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我们将从最容易处理的情况开始，即当投影维数 k = 1 时。处理 k = 1 情况的好处是，我们可以消除 Pi 或基向量 q 的内部求和，因为这里只有一个向量。</p><p id="e39e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以通过做一些线性代数，将这个问题转化为最大化问题，而不是致力于最小化问题。我们将从最初开始的正交投影表达式开始，而不是从上一段中的表达式开始。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mv"><img src="../Images/709df923e1d79beaf27156c437cb6f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4cuyWRwDDFKnYZDGWKRKIQ.png"/></div></div></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mw"><img src="../Images/dd18b866fda9870722271fb32c32dd08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4Xr5XmqcQSrhJvYaR9CYw.png"/></div></div></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mx"><img src="../Images/05dd835caf8a058a18a85576d1ce52c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qK3iXvplfjVODvYHUOCMRA.png"/></div></div></figure><p id="d986" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一次求和不依赖于基向量<strong class="ki iu"> q </strong>，所以不影响我们的最小化问题。去掉常数后，现在我们试图最小化负面表达，就变成了最大化问题。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/7bcc2912232fd9bbd514ead0c4be6612.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpYDiphb7ROo174LB_YLaQ.png"/></div></div></figure><p id="fa3b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">做更多的代数，</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ca"><img src="../Images/5b332d502c04a0d65329c87d14c2c37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMaOYYZvd1cCJaN-lXyPkQ.png"/></div></div></figure><p id="c390" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在问题变成了，</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mz"><img src="../Images/e7cb4ac0f9c7876bfba16893a8951dc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lGMdRbcDx5xCZzCdaHVZIg.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="http://www.cs.columbia.edu/~verma/teaching.html" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~verma/teaching.html</a></figcaption></figure><h1 id="084a" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">最大化问题是什么意思？</h1><p id="650d" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我们开始的最小化问题是最小化从数据集到投影的正交距离。对于最大化问题，我们看到是<strong class="ki iu">最大化方差</strong>。现在我将展示为什么问题的最大化版本是投影数据集方差的最大化。为了证明它，让我们从方差的方程开始。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi na"><img src="../Images/3f4ed27c9125c3ebc82191fafdbefa41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E4JcZI6LBs4p2rx855xKUQ.png"/></div></div></figure><p id="58a5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，上面的等式是一个标量乘以一个矢量本身的点积。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nb"><img src="../Images/026eef83aa8baba60f51060a179a6658.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YI9Ln9kdOfG7_SKGxEyQ2g.png"/></div></div></figure><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nc"><img src="../Images/0022bebb33818eca8d51092fa1b67df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbA9cHAOGMeJu-rWCRJ0xQ.png"/></div></div></figure><h1 id="ad19" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">为什么我们把最小化问题转化为最大化问题？</h1><p id="f2d8" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">那什么是 X q 的转置呢？和原来的 X 有什么不同？</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nd"><img src="../Images/3c62d49e5f32d992d1a425da25425e19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R29Y7ZKIU9U93FkVtEmzMw.png"/></div></div></figure><p id="cb30" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，列向量表示 k 维的新子空间内的距离。</p><p id="338c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从最小化和最大化的角度来看，看到同样的问题是很有趣的，因为它们都实现了降低维度的目标，但实现的方式不同。<em class="lp">最小化将是最小化残差，残差是数据点和投影</em>之间的正交距离。另一方面，<strong class="ki iu"> <em class="lp">最大化问题是最大化正交投影数据集的方差</em> </strong>。我们可以直观地看一下最小化和最大化:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ne"><img src="../Images/85fe0cd8b77d7d265d2de36ab5fb664c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Otq6U_8n9sE9p_Sa8v1ZVQ.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/" rel="noopener ugc nofollow" target="_blank">http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/</a></figcaption></figure><h1 id="7022" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">一般 K 情况</h1><p id="921f" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">现在我们将把 k = 1 的表达式转换成一般的 k 种情况的表达式。的原始最小化表达式</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nf"><img src="../Images/033b9bb5fcbab22020df759af0c9e604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ly-IkFc9OUTU4B1L01pBA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="http://www.cs.columbia.edu/~verma/teaching.html" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~verma/teaching.html</a></figcaption></figure><p id="b7c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相当于</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/015b6ff5ca047b12a8a1c9fbd7ff50c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDvh_0eu04WKHZO9KSDMTw.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="http://www.cs.columbia.edu/~verma/teaching.html" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~verma/teaching.html</a></figcaption></figure><p id="98c4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们有不止一个 q 并且 q 不再是一个向量而是一个矩阵时。</p><p id="98b2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">原因是因为外积之和(即变换运算符)相当于矩阵乘法，因为它从一个向量增长为一个矩阵，如下所示:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nh"><img src="../Images/6c02556081e24ca33a6844242ac1efca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EsWsgkBt7laFngHQ4Xkiww.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="http://mlwiki.org/index.php/Matrix-Matrix_Multiplication" rel="noopener ugc nofollow" target="_blank">http://mlwiki.org/index.php/Matrix-Matrix_Multiplication</a></figcaption></figure><p id="e72f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了将最大化问题转化为一般的 k 情况，我们需要决定我们想要从矩阵中最大化什么。让我们从轨迹的定义开始。一个<em class="lp"> n </em> -by- <em class="lp"> n </em>方阵<em class="lp"> A </em>的<strong class="ki iu">迹</strong>定义为<a class="ae kf" href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)" rel="noopener ugc nofollow" target="_blank">A<em class="lp">A</em></a><em class="lp">的主对角线(从左上到右下的对角线)上的元素之和。</em>由于我们的矩阵 Q(Q 的转置)是对称的，所以上面提到的对称矩阵的相同定理将被应用于它可以被分解为 t 的 P D 转置，并且我们还将使用 trace 的一个性质 trace(AB) = trace (BA)。</p><p id="0f44" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果 A 是可对角化的矩阵，那么 A 的迹等于 A 的特征值之和，证明如下:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ni"><img src="../Images/32e4517464db7cc678277c87b1e00338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0bc2ctK_N3heQlS2ooH3tw.png"/></div></div></figure><p id="3c8c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，为了最大化方差，我们可以最大化矩阵的迹，它是 d 的对角线元素之和。</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nj"><img src="../Images/cd3b7258b6655872e645f603d306a0d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbzXnpLKuYRBtdxz_GFHBA.png"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk"><a class="ae kf" href="http://www.cs.columbia.edu/~verma/teaching.html" rel="noopener ugc nofollow" target="_blank">http://www.cs.columbia.edu/~verma/teaching.html</a></figcaption></figure><p id="be8f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也可以像这样将迹的概念引入最小化问题:</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nk"><img src="../Images/fbde808ee47e250fdd98fd719f961da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vn5p9cIbuP8MviO4ku12Ew.png"/></div></div></figure><p id="ead2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，最大化矩阵的轨迹，</p><figure class="lf lg lh li gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi nl"><img src="../Images/4fe2fe89383fc5f912374e1e00021a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8hV8qJd79mdPrHiu6VnvA.png"/></div></div></figure><p id="95a2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">等同于最大化协方差矩阵以及与 X 的 X 转置相关联的特征值。注意，X 的 X 转置的维数是 d×d，但是其<strong class="ki iu"><em class="lp"/></strong>正被最大化的矩阵具有 k×k 的维数。追踪操作的输出是特征值之和的 k×k 矩阵， 但是<strong class="ki iu"> <em class="lp"> argmax </em> </strong>运算的输出是(d×k)Q 矩阵，其中每一列是 X 的 X 转置的特征向量。 </p><h1 id="db84" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">投影数据</h1><figure class="lf lg lh li gt ju gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e3a80783ceaee223a87bb63711f62485.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*HWJw2Hqct4xA7afFh9LJ-g.png"/></div></figure><p id="c556" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们只研究了新维度的基本向量。然而，我们真正想要的是原始数据在新维度上的投影。PCA 的最后一步是我们需要将 Q 的 Q 变换与原始数据矩阵相乘，以获得投影矩阵。我们从(d×k)Q 矩阵和 Q 的 Q 转置得到 d×d 维的结果。乘以(d×n)X 矩阵，投影矩阵为 d×n。</p><h1 id="2d51" class="ls lt it bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">结论</h1><p id="1460" class="pw-post-body-paragraph kg kh it ki b kj mq kl km kn mr kp kq kr ms kt ku kv mt kx ky kz mu lb lc ld im bi translated">我们从 d×d 协方差矩阵开始，通过最小化重构误差得到前 k 个特征向量，这相当于最大化矩阵的迹。所以我们成功降维了。回到统计学的角度，我们为什么要最大化方差的问题已经得到了回答，因为我们要最小化重建误差，这与最大化方差有相同的结果。</p></div></div>    
</body>
</html>