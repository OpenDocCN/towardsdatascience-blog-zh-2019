# “可解释的人工智能”:谁为我们做决定？

> 原文：<https://towardsdatascience.com/explainable-ai-who-takes-the-decisions-for-us-97b1d33edd91?source=collection_archive---------22----------------------->

![](img/ce38d373ac77bf1c8967b4c3a2413f5e.png)

作者:詹法纳，皮卡罗齐，迪塞科

2016 年 3 月，谷歌开发的围棋软件 AlphaGo 击败了韩国冠军 Lee Sedol，取得了历史性的成绩(这是计算机首次击败围棋九段棋手)，在人工智能(AI)领域的从业者中引起了很大的兴趣和反响。

学术界和大众媒体已经说了很多来解释这一事件的规模，围棋游戏是出了名的“计算复杂”，对计算机来说，比国际象棋复杂得多，它很难用技术来解决，这些技术在 90 年代末深蓝(IBM)与国际象棋冠军卡斯帕罗夫的历史性胜利中取得了成功。

但对我们来说，这是 AlphaGo 与 Lee Sedol 的第二场比赛中的第 37 步，这是计算机获胜的决定性一步，围棋冠军范辉随后评论道:*“这不是人类的举动，我从未见过人类下这样的棋”。没有人能够理解或解释这一制胜之举背后的策略，给它一个合理的解释。*

柏林大学机器学习教授克劳斯-罗伯特·穆勒致力于改善乳腺癌诊断的人工智能技术。穆勒教授还开发了一个名为“分层相关性传播”(LRP)的程序，该程序分析并解释了各种人工智能系统做出的决定。作为一项测试，穆勒将 LRP 应用于两个软件程序，这两个程序专门用于识别庞大图像库中的马图像。这两个应用程序都在图像识别方面产生了很高的性能，令人惊讶的结果是，LRP 已经表明，虽然其中一个应用程序专注于一匹马的显著特征，但另一个应用程序识别出了这些马，因为一组公共的版权像素存在于所使用的档案中包含的所有马的图像中，所以纯属偶然。结果是难以区分的，但是如果没有 LRP，我们永远不会知道这两个项目中的一个是通过应用完全没有根据的标准来工作的。

围棋第三十七步棋和马图像识别的例子看起来可能相当琐碎，但允许我们将一个正变得至关重要的话题具体化。最后一代人工智能代理是“黑匣子”，对我们的理解是不透明的，通常不允许我们了解它们如何做出某个决定或某个选择。当它处理识别马匹或玩围棋时，这不是非常重要，但如果人工智能应用于不同的学科(医学、法律或金融领域)，就对人的实际影响而言，它就变得必不可少；我们能接受一个癌症的诊断，却没有得到一个最低限度的解释，说明是什么因素导致了这个结论吗？或者在抵押贷款时遭到拒绝，却不知道哪个参数是关键因素？

在进一步发展这个主题之前，让我们后退一步，试着更好地理解像 *AI* 或*机器学习*这样的词的含义，这些词在公共语言中强有力地出现，但可能充满歧义和误解。

现代意义上的“人工智能”研究的诞生(不用追溯到希腊神话中的巨人青铜机器人*塔罗斯*的诞生)可以追溯到 1950 年 a .图灵的基础工作“*计算机器和智能”*，其中图灵测试被定义为机器与人类相比的智能的条件和衡量标准。

当时，这些研究导致了第一个“专家系统”的诞生，如著名的 ELIZA 软件，它能够与人进行基本的对话。这种方法是符号推理，即将一个知识领域转化为符号，并为机器提供一个推理引擎，该引擎具有处理符号以得出结论的所有规则。

尽管有一些有希望的结果，但最初的热情之后是对专家系统进一步发展的可能性的深刻幻灭，对知识的完整描述的“蛮力”方法和对所有选择树的算法探索被证明是不切实际的，并导致了所谓的“人工智能冬天”时期，冬天的特点是投资削减。转折点发生在 90 年代末和新千年初之间，在这期间，大量数据和巨大计算能力的同时可用性导致了基于机器学习技术的人工智能的现代意义。

基本上，我们不是试图描述知识，教人工智能如何操纵符号来解决问题，而是改变了我们的方法，专注于机器的学习能力，利用对巨大数据库的训练:如果我想教机器如何识别一只猫， 我不会向机器提供猫是什么的抽象描述，但我可能会提供大量代表猫的图像，人工智能系统将被构造为识别另一只不在它被训练的图像数据库中的猫(机器学习)。

所有正在成为我们日常生活一部分的人工智能系统，如手机上的 Siri，作为家庭助手的 Google Home，玩 Jeopardy 或提供医疗建议的 Watson，都是这个新的人工智能系统家族的一部分，它诞生于 2000 年初出现的新一轮机器学习和后续深度学习(基于神经网络的机器学习的进一步发展)的研究

但是，将这些代理视为“智能”系统是正确的吗？普通的叙述和耸人听闻的地方倾向于这样定义他们，但问题并不那么简单。Watson 和 Siri 专门解决特定问题，它们应该在“弱人工智能”范围内得到适当考虑，即在没有适当的进一步编程和没有人类特有的认知功能:意向性和意识的情况下，代理解决特定领域的特定问题，而没有扩展到其他领域的可能性。所有这些系统都被认为是属于“弱人工智能”，因此在“类似于”人类智能这个词的特定含义中，它们并不智能。与“弱人工智能”相反，我们应该考虑“强人工智能”，即试图实现一个智能代理，与人类在相同情况下表现出的行为没有区别。

“强 AI”一词是在塞尔 1960 年的工作和著名的名为“中国房间”的心智实验的背景下诞生的。假设我们创造了一个智能体，它会表现得好像懂中文一样；这意味着代理运行一个程序，该程序允许处理接收到的输入(中文字符),从而提供中文答案，并通过一个假设的图灵测试，该测试与理解中文并回答相同问题的人没有区别。我们会说这个人工智能真的懂中文吗？

关于这个话题的辩论是广泛的，没有穷尽的，但最明显的答案似乎是最受认可的:人工智能操纵符号，表现得好像它理解中文，但没有“真正”的理解，语法与语义不一致，意向性和意识对人工智能来说仍然是非常遥远的目标。

前景开始变得更加清晰:没有人类意义上的智能机器，但弱人工智能的爆发产生了大量人工智能，这些人工智能正在改变人类的习惯，并渗透到金融、医药等关键领域，帮助我们做出决策。

所以，让我们回到最初的问题:我们能理解基于神经网络和深度学习的新人工智能代理的决策背后的基本原理吗？

答案通常是否定的，与基于符号推理的第一代系统不同，当前的机器学习不允许我们理解人工智能体如何执行分配的任务:智能体能够识别猫，但我们不知道它遵循什么特征和模型来做这件事，这不是算法，智能体是一个黑盒，对我们的理解是“不透明的”。

从我们简短的题外话中，我们现在可以更好地理解人工智能系统在最多样化的领域(从医学到金融，从自动驾驶系统到军事防御)的大规模传播中出现的困难和严峻问题。

我们完全知道指导这些代理人设计的原则，但同时我们也意识到不可能“逆向”理解导致系统本身从最不相关的情况到最激烈的军事冲突的决定的原因和理由。

所谓的“可解释的人工智能”(XAI)产生于对人工智能系统提出的建议、解决方案或决定之后的问题给出答案的意识和需求:代理人是基于什么决定以这种方式而不是另一种方式进行的？没有任何答案，就不可能有对 AI 的任何信任。

已经提到的 LRP 系统或由 DARPA(美国国防高级研究计划局)进行的研究活动就是例子，其中的目标是实现人工智能系统的可解释性，而不影响学习阶段的性能。

关于数据处理和安全的新法律法规，如 2018 年 5 月 25 日开始的欧洲 GDPR，进一步放大了“可解释的人工智能”的重要性。GDPR 使不透明人工智能系统的应用变得极其困难，因为 GDPR 要求自动化系统的每个决定都具有可追溯性(第 22 条)，重申每个主体都有权获得关于智能系统所做的对其有影响的决定的解释。

不同的国家也在相应地变化。举个例子，我们来想想 2018 年 4 月的文件“ *AI 在英国:准备好了，愿意了，有能力了？”英国上议院研究人工智能领域的机遇和开放问题，特别关注讨论的“可解释性”。在这种情况下，如果没有深入的多学科方法和进一步采用人工智能的强大障碍，加速研究以解决可能难以弥合的法律空白可能是合适和有用的。*

**参考文献:**

**喷宁，d**(2017)**。** *可解释的人工智能(XAI)【在线】。*(网址:[https://www.darpa.mil/attachments/XAIProgramUpdate.pdf)](https://www.darpa.mil/attachments/XAIProgramUpdate.pdf))(上次访问时间 2018 年 5 月)。

**上议院** (2018)。*英国的 AI:准备好了，愿意了，有能力了？【在线】。*(网址:[https://publications . parliament . uk/pa/LD 201719/LD select/ldai/100/100 . pdf)](https://publications.parliament.uk/pa/ld201719/ldselect/ldai/100/100.pdf))(上次访问时间 2018 年 5 月)。

塞尔，j . r .(1980)。头脑、大脑和程序。行为和脑科学 3(3):417–457

**图灵，m . a .**(1950)。*计算机器和智能，*心智，49:433–460