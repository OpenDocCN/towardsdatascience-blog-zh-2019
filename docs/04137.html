<html>
<head>
<title>A Gentle Introduction to Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863?source=collection_archive---------4-----------------------#2019-06-29">https://towardsdatascience.com/a-gentle-introduction-to-natural-language-processing-e716ed3c0863?source=collection_archive---------4-----------------------#2019-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5c25" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">介绍自然语言处理和对文本数据的情感分析。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ebded4b8abcabc0f3892c045e78bf18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PGB0w1JZslqA-hM0xGrmJw.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://s3.amazonaws.com/codecademy-content/courses/NLP/Natural_Language_Processing_Overview.gif" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="2af6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人类通过某种形式的语言进行交流，无论是文本还是语音。现在要让计算机和人类互动，计算机需要理解人类使用的自然语言。自然语言处理就是让计算机学习、处理和操作自然语言。</p><p id="8571" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇博客中，我们将看看在自然语言处理任务中使用的一些常见做法。并且在电影评论上建立简单的情感分析模型，以预测给定评论是正面的还是负面的。</p><h1 id="31dd" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是自然语言处理(NLP)？</h1><p id="db3c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">NLP 是人工智能的一个分支，它处理分析、理解和生成人类自然使用的语言，以便使用自然人类语言而不是计算机语言在书面和口头上下文中与计算机交互。</p><h1 id="106a" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">自然语言处理的应用</h1><ul class=""><li id="0e4b" class="ms mt it lb b lc mn lf mo li mu lm mv lq mw lu mx my mz na bi translated">机器翻译(谷歌翻译)</li><li id="4c76" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">自然语言生成</li><li id="799d" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">网络搜索</li><li id="d2bb" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">垃圾邮件过滤器</li><li id="b606" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">情感分析</li><li id="80c5" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">聊天机器人</li></ul><p id="97f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">…以及更多</p><h1 id="2725" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据清理:</h1><p id="acde" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在数据清理过程中，我们从原始数据中移除特殊字符、符号、标点符号、HTML 标签&lt;&gt;等，这些数据不包含模型要学习的信息，这些只是我们数据中的噪声。</p><p id="ca50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程还取决于问题陈述，比如从原始文本中删除什么。例如，如果问题包含来自经济或商业世界的文本，那么像$或其他货币符号这样的符号可能包含一些我们不想丢失的隐藏信息。但大多数情况下我们会移除它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="c7c7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据预处理:</h1><blockquote class="ni nj nk"><p id="54b7" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">数据预处理是一种数据挖掘技术，包括将原始数据转换成可理解的格式。</p></blockquote><h2 id="39bc" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">小写:</h2><p id="29e3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">让所有文本都变成小写是最简单也是最有效的文本预处理形式之一。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/e3098a35b09acf77c6b6b756b328043f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PeAYrvLkXGU587TLDfgq1w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc nh l"/></div></figure><h2 id="4d86" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">符号化:</h2><p id="c8dc" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">记号化是将文本文档分解成称为记号的单个单词的过程。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="od nh l"/></div></figure><p id="f23f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，句子被分解成单词(记号)。自然语言工具包(NLTK)是一个流行的开源库，广泛用于 NLP 任务。对于这个博客，我们将使用 nltk 进行所有的文本预处理步骤。</p><blockquote class="ni nj nk"><p id="e1bc" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">您可以使用 pip 下载 nltk 库:</p></blockquote><pre class="kj kk kl km gt oe of og oh aw oi bi"><span id="a9ff" class="np lw it of b gy oj ok l ol om"><em class="nl">!pip install nltk</em></span></pre><h2 id="f622" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">停止单词删除:</h2><p id="fe9c" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">停用词是在文本文档中不提供太多信息的常用词。像‘the’，‘is’，‘a’这样的词价值较小，会给文本数据增加干扰。</p><p id="f889" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLTK 中有一个内置的停用词列表，我们可以用它从文本文档中删除停用词。然而，这不是每个问题的标准停用词表，我们也可以根据领域定义自己的停用词表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="on nh l"/></div></figure><p id="c084" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">NLTK 有一个预定义的停用词列表。我们可以从这个列表中添加或删除停用词，或者根据具体任务来使用它们。</p><h2 id="7635" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">词干:</h2><p id="d5d3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">词干化是将一个单词缩减为其词干/词根的过程。它将单词(如“help”、“helping”、“helped”、“helped”)的词形变化减少到词根形式(如“help”)。它从单词中去掉词缀，只留下词干。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/23b5b036a00bb05b59014d182cd02961.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZLdN6Z_fn873jbD8TmJ96Q.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://www.thinkinfi.com/2018/09/difference-between-stemming-and.html" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op nh l"/></div></figure><p id="f446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词干可能是也可能不是语言中的有效词。例如，movi 是 movie 的词根，emot 是 emotion 的词根。</p><h2 id="8174" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">词汇化:</h2><p id="2241" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">词汇化与词干化的作用相同，将单词转换为其词根形式，但有一点不同，即在这种情况下，词根属于语言中的有效单词。例如，在词干的情况下，单词 caring 将映射到“care”而不是“car”。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq nh l"/></div></figure><p id="51cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">WordNet 是英语中有效单词的数据库。NLTK 的 WordNetLemmatizer()使用来自 WordNet 的有效单词。</p><h2 id="e4b9" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">N-grams:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e087418899729886720925628ee42e1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*tufb2Ea4ZBVydTuvrIUT5A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://stackoverflow.com/questions/18193253/what-exactly-is-an-n-gram" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="6879" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">N-grams 是多个单词一起使用的组合，N=1 的 N-grams 称为 unigrams。类似地，也可以使用二元模型(N=2)、三元模型(N=3)等等。</p><p id="5614" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们希望保留文档中的序列信息时，可以使用 n 元语法，比如给定的单词后面可能跟什么单词。单字不包含任何序列信息，因为每个单词都是独立的。</p><h1 id="46e1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">文本数据矢量化:</h1><blockquote class="ni nj nk"><p id="4573" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">将文本转换为数字的过程称为文本数据矢量化。现在，在文本预处理之后，我们需要用数字表示文本数据，也就是说，用数字对数据进行编码，以便算法进一步使用。</p></blockquote><h2 id="e3b4" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">包话(鞠躬):</h2><p id="d3e7" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">这是最简单的文本矢量化技术之一。BOW 背后的直觉是，如果两个句子包含一组相似的单词，就说它们是相似的。</p><p id="85b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑这两句话:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/797bbfb111a002f187acf54681524d86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*hjC8eHW5JrisXCjQZm6WSA.jpeg"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://www.slideshare.net/ds_mi/50-shades-of-text-leveraging-natural-language-processing-nlp-alessandro-panebianco" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><blockquote class="ni nj nk"><p id="e536" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">在 NLP 任务中，每个文本句子被称为一个文档，这些文档的集合被称为文本语料库。</p></blockquote><p id="6379" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BOW 在语料库(数据中所有标记的集合)中构建了一个包含 d 个唯一单词的字典。例如，上图中的语料库由 S1 和 S2 的单词组合而成。</p><p id="ed3a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以想象创建一个表，其中的列是语料库中唯一的单词集，每行对应一个句子(文档)。如果这个单词出现在句子中，我们把它的值设置为 1，否则我们把它设置为 0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/afa71be8f0c753caa292245d093b9fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oByoDP8SXq4PX62ej2v9yg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://www.slideshare.net/ds_mi/50-shades-of-text-leveraging-natural-language-processing-nlp-alessandro-panebianco" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="8802" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将创建一个矩阵<strong class="lb iu"> dxn </strong>，其中<strong class="lb iu"> d </strong>是语料库中唯一标记的总数，而<strong class="lb iu"> n </strong>等于文档的数量。在上面的例子中，矩阵的形状为<strong class="lb iu"> 11x2。</strong></p><h2 id="79e2" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">TF-IDF:</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/d5554ab72d79b5886333af9304b612bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BHpIxB_7S-W1tiFNPNndXg.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://www.slideshare.net/ds_mi/50-shades-of-text-leveraging-natural-language-processing-nlp-alessandro-panebianco" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><p id="24b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">它代表词频(TF)-逆文档频率。</p><p id="d8fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">词频:</strong></p><p id="d123" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词频定义了在文档中找到某个单词的概率。现在假设我们想找出在文档<strong class="lb iu"> dj </strong>中找到单词<strong class="lb iu"> wi </strong>的概率是多少。</p><blockquote class="ov"><p id="d712" class="ow ox it bd oy oz pa pb pc pd pe lu dk translated">词频(<strong class="ak"> wi </strong>，<strong class="ak"> dj </strong> ) =</p><p id="7c9c" class="ow ox it bd oy oz pa pb pc pd pe lu dk translated"><strong class="ak"> wi </strong>出现在<strong class="ak"> dj 中的次数/ </strong>在<strong class="ak"> dj 中的总字数</strong></p></blockquote><p id="0603" class="pw-post-body-paragraph kz la it lb b lc pf ju le lf pg jx lh li ph lk ll lm pi lo lp lq pj ls lt lu im bi translated"><strong class="lb iu">逆文档频率:</strong></p><p id="19c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">IDF 背后的直觉是，如果一个单词出现在所有文档中，它就没有多大用处。它定义了单词在整个语料库中的独特性。</p><blockquote class="ov"><p id="f61d" class="ow ox it bd oy oz pa pb pc pd pe lu dk translated">IDF(wi，Dc) = log(N/ni)</p></blockquote><p id="e418" class="pw-post-body-paragraph kz la it lb b lc pf ju le lf pg jx lh li ph lk ll lm pi lo lp lq pj ls lt lu im bi translated">这里，<strong class="lb iu"> Dc </strong> =语料库中的所有文档，</p><p id="482e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> N </strong> =文件总数，</p><p id="3821" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> ni </strong> =包含 word ( <strong class="lb iu"> wi </strong>)的文档。</p><blockquote class="ni nj nk"><p id="7c9e" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">如果<strong class="lb iu"> wi </strong>在语料库中更频繁，则 IDF 值减少。</p><p id="f7ab" class="kz la nl lb b lc ld ju le lf lg jx lh nm lj lk ll nn ln lo lp no lr ls lt lu im bi translated">如果<strong class="lb iu"> wi </strong>不频繁，这意味着<strong class="lb iu"> ni </strong>减少，因此 IDF 值增加。</p></blockquote><blockquote class="ov"><p id="f9a2" class="ow ox it bd oy oz pk pl pm pn po lu dk translated">TF( <strong class="ak">作业指导书</strong>、<strong class="ak">DJ</strong>)<strong class="ak">*</strong>IDF(<strong class="ak">作业指导书</strong>、<strong class="ak"> Dc </strong>)</p></blockquote><p id="6acd" class="pw-post-body-paragraph kz la it lb b lc pf ju le lf pg jx lh li ph lk ll lm pi lo lp lq pj ls lt lu im bi translated">TF-IDF 是 TF 和 IDF 值的乘积。它给予在文档中出现较多而在语料库中出现较少的单词更大的权重。</p></div><div class="ab cl pp pq hx pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="im in io ip iq"><h1 id="98d2" class="lv lw it bd lx ly pw ma mb mc px me mf jz py ka mh kc pz kd mj kf qa kg ml mm bi translated">情感分析:IMDB 电影评论</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qb"><img src="../Images/54b3a3e070d55488f214ca62bdc14db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jp-t7Y4H0D6GtYgO"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image <a class="ae ky" href="https://en.wikipedia.org/wiki/IMDb" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure><h2 id="6215" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">关于</h2><p id="5b4f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">该数据集包含来自 IMDB 网站的 50，000 条评论，正面和负面评论的数量相等。任务是预测给定评论(文本)的极性(积极或消极)。</p><p id="d5c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我使用<a class="ae ky" href="https://deepnote.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Deepnote </strong> </a>在 IMDB 数据集上做数据分析，它的设置简单快捷，并且提供了很好的协作工具。我最喜欢的是从多个数据源即插即用的能力。如果你是新手，正在开始你的数据科学之旅，我强烈建议你去看看。这里是这个项目的<a class="ae ky" href="https://deepnote.com/@ronakv/Sentiment-Analysis-9cb468b0-9200-400f-9896-e4e9d46dbc48" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h2 id="1f0a" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">1.数据的加载和浏览</h2><p id="aaa6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">IMDB 数据集可以从<a class="ae ky" href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="7ad6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">数据集概述:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qc nh l"/></div></figure><p id="1081" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正面评价标为 1，负面标为 0。</p><p id="854c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">样本正面评论:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qd nh l"/></div></figure><p id="eaf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">样品差评:</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qd nh l"/></div></figure><h2 id="1f61" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">2.数据预处理</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qe nh l"/></div></figure><p id="4f36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们在一个方法中完成了数据清理和预处理的所有步骤，如上所述。我们使用词干化而不是词干化，因为在测试两者的结果时，词干化给出的结果比词干化稍好。</p><p id="640d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">词干化或词干化或两者的使用取决于问题，所以我们应该尝试看看哪种方法对给定的任务最有效。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qf nh l"/></div></figure><p id="537b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过对所有评论应用 data_preprocessing()，在 dataframe 中添加一个新列 preprocessed _ review。</p><h2 id="2a5c" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">3.向量化文本(评论)</h2><p id="6d9d" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">将数据集分为训练和测试(70–30):</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qg nh l"/></div></figure><p id="5f56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们正在使用 sklearn 的 train_test_split 将数据拆分为训练和测试。这里我们使用参数分层，在训练和测试中有相等比例的类。</p><p id="8458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">低头</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qh nh l"/></div></figure><p id="fde2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们使用了 min _ df = 10<em class="nl">T5，因为我们只需要那些在整个语料库中出现至少 10 次的单词。</em></p><p id="89d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> TF-IDF </strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qh nh l"/></div></figure><h2 id="2a2f" class="np lw it bd lx nq nr dn mb ns nt dp mf li nu nv mh lm nw nx mj lq ny nz ml oa bi translated">4.构建 ML 分类器</h2><p id="0a39" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">带评论 BOW 编码的朴素贝叶斯</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qi nh l"/></div></figure><p id="9702" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">带 BOW 的朴素贝叶斯给出了 84.6%的准确率。用 TF-IDF 试试吧。</p><p id="4a6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">带有 TF-IDF 编码评论的朴素贝叶斯</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qj nh l"/></div></figure><p id="eb1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF 给出了比 BOW 稍好的结果(85.3%)。现在让我们用一个简单的线性模型，逻辑回归，来试试 TF-IDF。</p><p id="5e4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">对 TF-IDF 编码的评论进行逻辑回归</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="qk nh l"/></div></figure><p id="4521" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用 TFIDF 编码的评论的逻辑回归给出了比朴素贝叶斯更好的结果，准确率为 88.0%。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ql nh l"/></div></figure><p id="9f99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制混淆矩阵为我们提供了关于有多少数据点被模型正确和错误分类的信息。</p><p id="7bbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 7500 个负面评论中，6515 个被正确分类为负面，985 个被错误分类为正面。在 7500 个正面评论中，6696 个被正确分类为正面，804 个被错误分类为负面。</p><h1 id="ca53" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><p id="aeca" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们已经学习了一些基本的 NLP 任务，并为电影评论的情感分析建立了简单的 ML 模型。通过深度学习模型尝试单词嵌入，可以实现进一步的改进。</p></div><div class="ab cl pp pq hx pr" role="separator"><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu pv"/><span class="ps bw bk pt pu"/></div><div class="im in io ip iq"><p id="a418" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。完整的代码可以在<a class="ae ky" href="https://github.com/ronakvijay/IMDB_Sentiment_Analysis/blob/master/Sentiment_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="1490" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考资料:</h1><div class="qm qn gp gr qo qp"><a href="https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/" rel="noopener  ugc nofollow" target="_blank"><div class="qq ab fo"><div class="qr ab qs cl cj qt"><h2 class="bd iu gy z fp qu fr fs qv fu fw is bi translated">处理文本数据(使用 Python)的终极指南——面向数据科学家和工程师</h2><div class="qw l"><h3 class="bd b gy z fp qu fr fs qv fu fw dk translated">引言实现任何水平的人工智能所需的最大突破之一是拥有…</h3></div><div class="qx l"><p class="bd b dl z fp qu fr fs qv fu fw dk translated">www.analyticsvidhya.com</p></div></div><div class="qy l"><div class="qz l ra rb rc qy rd ks qp"/></div></div></a></div><div class="qm qn gp gr qo qp"><a href="https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html" rel="noopener  ugc nofollow" target="_blank"><div class="qq ab fo"><div class="qr ab qs cl cj qt"><h2 class="bd iu gy z fp qu fr fs qv fu fw is bi translated">关于自然语言处理和机器学习的文本预处理</h2><div class="qw l"><h3 class="bd b gy z fp qu fr fs qv fu fw dk translated">数据科学家卡维塔·加内桑。根据最近的一些谈话，我意识到文本预处理是一个严重的…</h3></div><div class="qx l"><p class="bd b dl z fp qu fr fs qv fu fw dk translated">www.kdnuggets.com</p></div></div><div class="qy l"><div class="re l ra rb rc qy rd ks qp"/></div></div></a></div></div></div>    
</body>
</html>