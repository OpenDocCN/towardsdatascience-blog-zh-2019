<html>
<head>
<title>Data Scientist’s toolkit — How to gather data from different sources</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学家的工具包—如何从不同来源收集数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-scientists-toolkit-how-to-gather-data-from-different-sources-1b92067556b3?source=collection_archive---------17-----------------------#2019-07-18">https://towardsdatascience.com/data-scientists-toolkit-how-to-gather-data-from-different-sources-1b92067556b3?source=collection_archive---------17-----------------------#2019-07-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db23" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Master all — csv、tsv、zip、txt、api、json、sql …</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2f403c08d875f4a6d013d051f86a23cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XyaZKCvbitsxaL1Q"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@jakobowens1?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jakob Owens</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dcf4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不久前！</p><p id="c011" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您还记得将外部硬盘中的数据发送给您进行分析或建模的时间吗？</p><p id="64fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，作为一名数据科学家，你不局限于这些方法。存储数据、共享数据以及获取数据、扩充数据的不同来源有多种方式。</p><p id="d600" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面，我列出了几种收集数据的方法供你分析</p><h2 id="5a44" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">目录:</h2><ol class=""><li id="8ce1" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated">CSV 文件</li><li id="ad15" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">平面文件(制表符、空格或任何其他分隔符)</li><li id="32e9" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">文本文件(在单个文件中—一次读取所有数据)</li><li id="4be2" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">压缩文件</li><li id="0f8f" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">多个文本文件(数据被分割到多个文本文件中)</li><li id="251b" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">从互联网下载文件(服务器上托管的文件)</li><li id="dec6" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">网页(抓取)</li><li id="f195" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">API(JSON)</li><li id="6470" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">文本文件(逐行读取数据)</li><li id="6a5e" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated">关系数据库管理系统(SQL 表)</li></ol></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="95aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在 Python 中，文件被描述为文本<strong class="lb iu">或二进制<strong class="lb iu">文件</strong>，两者之间的区别很重要</strong></p><p id="d709" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">文本文件</strong>由一系列行组成。每一行都以一个称为 EOL 或行尾字符的特殊字符结束。有几种类型，但最常见的是<code class="fe nl nm nn no b">\n</code>或<code class="fe nl nm nn no b">,</code></p><p id="3d5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">二进制文件</strong>类型基本上是除文本文件之外的任何类型的文件。由于其性质，二进制文件只能由知道或理解文件结构的应用程序来处理</p><blockquote class="np nq nr"><p id="73c3" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 1。CSV 文件</em> </strong></p></blockquote><p id="f18b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">存储和共享数据集最常见的格式是逗号分隔格式或 csv 文件。<code class="fe nl nm nn no b">pandas.read_csv()</code>是最有用和最强大的方法，我强烈推荐你阅读它的<a class="ae ky" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html" rel="noopener ugc nofollow" target="_blank">文档</a>。通过使用适当类型的<code class="fe nl nm nn no b">sep</code>,您可以在 dataframe 中加载多种类型的数据</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="6d9a" class="lv lw it no b gy oa ob l oc od">import pandas</span><span id="3626" class="lv lw it no b gy oe ob l oc od">df = pd.read_csv('data.csv', sep =',')</span></pre><blockquote class="np nq nr"><p id="beeb" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu">T23】2。平锉平锉</strong></p></blockquote><p id="6e29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但有时您可能会收到制表符分隔、固定宽度格式或分号分隔等文件。<code class="fe nl nm nn no b">Pandas</code>提供多种方法来恰当地读取此类数据。但是，当您指定正确的分隔符时，甚至<code class="fe nl nm nn no b">read_csv</code>也能很好地工作</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="f58e" class="lv lw it no b gy oa ob l oc od">import pandas</span><span id="2915" class="lv lw it no b gy oe ob l oc od"># Tab separated file<br/>df = pd.read_csv('data.tsv', sep='\t')<br/>OR <br/># columns are separated by space <br/>df = pd.read_csv('data.txt', sep = ' ')</span></pre><blockquote class="np nq nr"><p id="eb36" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 3。文本文件</em> </strong></p></blockquote><p id="5de6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到使用<code class="fe nl nm nn no b">pandas.read_csv()</code>方法可以读取 txt 文件，但是让我们看看读取 txt 文件的另一种方式是<strong class="lb iu">上下文管理器</strong>。它们在分配和管理资源、关闭所有打开的文件方面非常有效</p><p id="5ce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上下文管理器最广泛使用的例子是<code class="fe nl nm nn no b">with</code>语句</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="40dd" class="lv lw it no b gy oa ob l oc od">file_name = 'data.txt'</span><span id="9baf" class="lv lw it no b gy oe ob l oc od">with open(file_name, mode = 'r') as file:<br/>    df = file</span></pre><blockquote class="np nq nr"><p id="da59" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 4。Zip 文件</em> </strong></p></blockquote><p id="b638" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有时，您可能会得到一个 csv 文件，其中可能包含您需要的 csv 文件，以节省大小。提取。你需要使用' zip file '库。如果你使用的是 windows，这样做比从 windows 中提取要好得多</p><p id="7091" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nl nm nn no b">zipfile</code>也是一个上下文管理器，将支持我们前面看到的<code class="fe nl nm nn no b">with</code>语句</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="9ee5" class="lv lw it no b gy oa ob l oc od">from zipfile import ZipFile</span><span id="0fd5" class="lv lw it no b gy oe ob l oc od">file_name = 'my_zip_file.zip'<br/>with open(file_name, mode = 'r') as f:<br/>    f.extractall()<br/># Extractall method will extract all the contents of zipfile in the same folder</span><span id="1fce" class="lv lw it no b gy oe ob l oc od"># now we can load the extracted csv file in our dataframe<br/>import pandas</span><span id="dea5" class="lv lw it no b gy oe ob l oc od">df = pd.read_csv('my_csv_file.csv')</span></pre><blockquote class="np nq nr"><p id="fac9" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 5。多个文本文件</em> </strong></p></blockquote><p id="ac4e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能会遇到这样的情况，您的数据以多个文本文件的形式提供给您。例如，你被提供了 1000 个电影标题的评论，而不是把所有的放入一个，他们把每个单独的评论作为一个文件，并且提供了 1000 个这样的文本文件</p><p id="4bab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用一个库<code class="fe nl nm nn no b">glob</code>——它使得打开具有相似路径结构的文件变得简单</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="3b97" class="lv lw it no b gy oa ob l oc od">import pandas<br/>import glob</span><span id="f151" class="lv lw it no b gy oe ob l oc od">folder_name = 'my_folder'</span><span id="c1a2" class="lv lw it no b gy oe ob l oc od">df_list = []<br/>for review in glob.glob('my_folder/*.txt'):<br/>    with open(review, mode = 'r') as file:<br/>        movie = {}<br/>        movie['title'] = file.readline()<br/>        movie['review'] = file.read()<br/>df_list.append(movie)</span><span id="317a" class="lv lw it no b gy oe ob l oc od">df = pd.DataFrame(df_list)</span></pre><ul class=""><li id="62e5" class="mo mp it lb b lc ld lf lg li of lm og lq oh lu oi mw mx my bi translated">是 glob 语句中的通配符。它允许 python 扫描所有的。给定路径中的 txt 文件</li></ul><blockquote class="np nq nr"><p id="a65c" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 6。从网上下载文件</em> </strong></p></blockquote><p id="93af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你必须下载保存在服务器上的文件。你必须使用一个库— <code class="fe nl nm nn no b">requests</code></p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="1ec9" class="lv lw it no b gy oa ob l oc od">import requests<br/>import os</span><span id="c3f6" class="lv lw it no b gy oe ob l oc od">sample_url = '192.145.232.xx/2019/2/class_notes/gather_data.txt'</span><span id="a3a5" class="lv lw it no b gy oe ob l oc od">folder_name = 'my_folder'</span><span id="68f5" class="lv lw it no b gy oe ob l oc od">if not os.path.exists(folder_name):<br/>    os.makedirs(folder_name)</span><span id="725f" class="lv lw it no b gy oe ob l oc od">response = requests.get(url)<br/>file_name = 'gather_data.txt'<br/>file_loc = os.path.join(folder_name, file_name)<br/>with open(file_loc, mode='wb') as outfile:<br/>    outfile.write(response.content)</span></pre><blockquote class="np nq nr"><p id="68a6" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 7。</em>网页(网页抓取)</strong></p></blockquote><p id="d9c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网络抓取是一种使用代码从网页中提取数据的奇特方式。存储在网页上的数据被称为 HTML，即超文本标记语言。它是由这些叫做标签的东西组成的，这些东西赋予了网页结构。<code class="fe nl nm nn no b">&lt;title&gt;,</code> <code class="fe nl nm nn no b">&lt;div&gt;,</code> <code class="fe nl nm nn no b">&lt;h1&gt;</code> …..等等</p><p id="9daa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为 HTML 代码只是文本，所以可以使用解析器提取其中的标签和内容。为此，我们将使用一个库— <code class="fe nl nm nn no b">BeautifulSoup</code></p><p id="3ccd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们想从 IMDb 和烂番茄两者中提取关于《复仇者联盟》最终结局(2019)的信息</p><p id="4075" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:Inspect element 是您寻找相关标签和属性以提取数据的最好朋友</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="8dda" class="lv lw it no b gy oa ob l oc od">from bs4 import BeautifulSoup as bs<br/>import requests<br/>import pandas</span><span id="ad27" class="lv lw it no b gy oe ob l oc od">imdb_url = '<a class="ae ky" href="https://www.imdb.com/title/tt4154796/" rel="noopener ugc nofollow" target="_blank">https://www.imdb.com/title/tt4154796/</a>'<br/>response = requests.get(imdb_url)<br/>soup = bs(response.content, features = b'lxml)<br/>movie = {}<br/>movie['title'] = soup.find('div', attrs = {'class': 'title_wrapper'}).find('h1').text<br/>movie['imdb_rating'] = soup.find('span', attrs = {'itemprop': 'ratingValue'}).text</span><span id="754c" class="lv lw it no b gy oe ob l oc od">rt_url = '<a class="ae ky" href="https://www.rottentomatoes.com/m/avengers_endgame" rel="noopener ugc nofollow" target="_blank">https://www.rottentomatoes.com/m/avengers_endgame</a>'<br/>response = requests.get(rt_url)<br/>soup = bs(response.content, features = 'lxml')<br/>movie['tomatometer'] = soup.find('span', attrs = {'class' : 'mop-ratings-wrap__percentage'}).text<br/>movie['audience_score'] = soup.find('div', attrs = {'class' : 'audience-score'}).find('span', attrs={'class' : 'mop-ratings-wrap__percentage'}).text</span><span id="1b26" class="lv lw it no b gy oe ob l oc od">df = pd.DataFrame(movie)</span></pre><p id="2f59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方式，我们从两个不同的来源收集了关于《复仇者联盟》结局的信息</p><blockquote class="np nq nr"><p id="6904" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 8。API</em></strong>(应用程序编程接口)</p></blockquote><p id="c392" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你当然可以从网页中提取信息，但是更好的获取信息的方式是通过 API。</p><p id="fc75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">维基百科有几个公开开放的 API，其中流行的一个是 Mediawiki。我们将使用 python 库<code class="fe nl nm nn no b">wptools</code></p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="d2dc" class="lv lw it no b gy oa ob l oc od">import wptools<br/>import pandas</span><span id="3491" class="lv lw it no b gy oe ob l oc od"># For a wikipedia URL '<a class="ae ky" href="https://en.wikipedia.org/wiki/Avengers:_Endgame" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Avengers:_Endgame</a>' we only need to pass the string after /wiki/</span><span id="28d3" class="lv lw it no b gy oe ob l oc od">wiki_page = wptools.page('<a class="ae ky" href="https://en.wikipedia.org/wiki/Avengers:_Endgame" rel="noopener ugc nofollow" target="_blank">Avengers:_Endgame</a>).get()</span><span id="06f2" class="lv lw it no b gy oe ob l oc od"># Now this wiki_page has fetched extracts, images, infobox data, wiki data etc<br/># By using wikipage.data() method we can extract all the information</span><span id="4e6a" class="lv lw it no b gy oe ob l oc od">wiki_page.data['image'] # this will return 3 images back</span><span id="20d4" class="lv lw it no b gy oe ob l oc od">first_image = wiki_page.data['image'][0]<br/>print (first_image['url'])<br/>'https://upload.wikimedia.org/wikipedia/en/0/0d/Avengers_Endgame_poster.jpg'</span><span id="c72f" class="lv lw it no b gy oe ob l oc od"># Now you can save this poster link or use the requests method as seen above to save poster of the movie</span></pre><blockquote class="np nq nr"><p id="9bae" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 9。文本文件</em> </strong>(逐行读取数据)</p></blockquote><p id="df73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您已经使用 twitter api — tweepy 下载了 tweepy 数据</p><p id="5776" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你已经将所有的推文保存在一个文本文件中——tweets _ JSON . txt</p><p id="ef98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你现在的目标是从这些推文中提取有用的信息，将其保存在数据框架中，并用于进一步的分析。这是您在第 3 步和第 8 步中学到的技能的组合</p><p id="9974" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第 1 部分—让我向您展示如何使用 tweepy 获取 twitter 数据。</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="d182" class="lv lw it no b gy oa ob l oc od">import tweepy<br/>import json</span><span id="e8f2" class="lv lw it no b gy oe ob l oc od">consumer_key = 'xxx'<br/>consumer_secret = 'xxx'<br/>access_token = 'xxx'<br/>access_secret = 'xxx'</span><span id="3d3c" class="lv lw it no b gy oe ob l oc od">auth = OAuthHandler(consumer_key, consumer_secret)<br/>auth.set_access_token(access_token, access_secret)</span><span id="6e8b" class="lv lw it no b gy oe ob l oc od">api = tweepy.API(auth, wait_on_rate_limit = True)</span><span id="fecf" class="lv lw it no b gy oe ob l oc od">tweet_ids = [] #This will be the list of tweets for which you need the information for</span><span id="f0f4" class="lv lw it no b gy oe ob l oc od">file_name = 'tweets_json.txt'<br/>with open(file_name, mode='w') as file:<br/>    for tweet_id in tweet_id:<br/>        tweet = api.get_status(tweet_id, tweet_mode = 'extended')<br/>        json.dump(tweet._json, file)<br/>        file.write('/n')</span></pre><p id="d036" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第 2 部分—阅读 tweets_json.txt 以提取有用的信息</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="d83b" class="lv lw it no b gy oa ob l oc od">import pandas<br/>import json</span><span id="ebcb" class="lv lw it no b gy oe ob l oc od">df_list = []<br/>with open('tweets_json.txt', mode = 'r') as file:<br/>    for line in file:<br/>        tweet = {}<br/>        json_data = json.loads(line)<br/>        tweet['tweet_id'] = json_data['id']<br/>        tweet['retweet_count'] = json_data['retweet_count']<br/>        tweet['favorite_count'] = json_data['favorite_count']<br/>        df_list.append(tweet)<br/>tweet_df = pd.DataFrame(df_list)</span></pre><blockquote class="np nq nr"><p id="335c" class="kz la ns lb b lc ld ju le lf lg jx lh nt lj lk ll nu ln lo lp nv lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it"> 10。RDBMS</em>T3【SQL 数据库】</strong></p></blockquote><p id="3cd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据库是“保存在计算机中的一组结构化数据，尤其是可以通过各种方式访问的数据。”它便于数据的存储、检索、修改和删除</p><p id="5350" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在数据争论的背景下，数据库和 SQL 开始用于存储数据或收集数据:</p><ul class=""><li id="3c8c" class="mo mp it lb b lc ld lf lg li of lm og lq oh lu oi mw mx my bi translated"><strong class="lb iu">连接数据库，将数据</strong>导入熊猫数据框架</li><li id="3060" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu oi mw mx my bi translated"><strong class="lb iu">连接数据库并将数据从熊猫数据帧存储到数据库</strong></li></ul><ol class=""><li id="3487" class="mo mp it lb b lc ld lf lg li of lm og lq oh lu mv mw mx my bi translated">用 python 连接到数据库—</li></ol><p id="e53e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用 SQLAlchemy 连接到 SQLite 数据库，这是一个用于 python 的数据库工具包</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="8ac8" class="lv lw it no b gy oa ob l oc od">import sqlalchemy<br/>import pandas</span><span id="a59b" class="lv lw it no b gy oe ob l oc od">engine = sqlalchemy.create_engine('sqlite:///movies.db')</span></pre><p id="b3da" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns"> movies.db 不会显示在 jupyter 笔记本仪表盘上</em></p><p id="7cfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.在数据库中存储熊猫数据帧</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="466a" class="lv lw it no b gy oa ob l oc od"># Store dataframe 'df' in a table called movie_tbl in the database</span><span id="8fcd" class="lv lw it no b gy oe ob l oc od">df.to_sql('movie_tbl', engine, index=False)</span></pre><p id="367d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ns"> movies.db 现在将显示在 jupyter 笔记本仪表盘中</em></p><p id="a5f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.读取熊猫数据框架中的数据库表</p><pre class="kj kk kl km gt nw no nx ny aw nz bi"><span id="5090" class="lv lw it no b gy oa ob l oc od">df = pd.read_sql('SELECT * FROM movie_tbl', engine)</span></pre><h2 id="96fc" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结束语:</h2><ul class=""><li id="1828" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu oi mw mx my bi translated">我试图给出各种事物的基本概念，避免写下太多的细节</li><li id="4092" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu oi mw mx my bi translated">请随意写下您的想法/建议/反馈</li></ul></div></div>    
</body>
</html>