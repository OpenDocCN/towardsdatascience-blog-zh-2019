# 你教过你的机器了吗？

> 原文：<https://towardsdatascience.com/have-you-taught-your-machine-yet-45540b7e646b?source=collection_archive---------19----------------------->

## 谷歌的可教机器如何在你的小浏览器中学习

![](img/35d940da44e652225029020f7059b6c6.png)

A visual representation of what a CNN actually sees — [Source](https://github.com/utkuozbulak/pytorch-cnn-visualizations)

# 机器可以看见

自 2015 年，当一个 [Resnet](https://arxiv.org/abs/1512.03385) 首次超过人类对图像进行分类的准确度阈值时，深度学习席卷了整个世界。阅读详细介绍这些成就的研究通常会给人一种印象，即表现良好的深度学习模型需要一个包含一百万张标记图像的数据集和一群 GPU 来训练它们。但是如果你只有几百张图片和一台带 CPU 的笔记本电脑会怎么样呢？有了[谷歌的可教机器](https://teachablemachine.withgoogle.com/)，你可以在几分钟内见证机器的视觉效果。

这里有一款[型号](https://teachablemachine.withgoogle.com/models/u87itobc/)(点击试试！)可以对物体是一块布还是一本书进行分类。它是由作者在他的浏览器上使用 4 个对象训练的——一本黑色的书、一本橙色的书、一块白布和一块黄色的布。我们在不同的方向上拍摄每个物体的大约 150 张图像，并在不到一分钟的时间内用总共大约 600 张图像训练该模型。你在这个模型上踢轮胎了吗？为什么不试着给这个模型看一本黄皮书来欺骗它，看看它是否成立？即使你成功地搞乱了这个模型，一台机器能够在如此短的时间内用如此少的数据学会如此准确地对物体进行分类也是非常了不起的。同样不可思议的是，你只需点击几下鼠标，就能体验到这种模式的强大——一切都在你的浏览器中进行——没有服务器，没有 GPU。那么，什么是真正的引擎盖下？

# 迁移学习

可教机器使用迁移学习；一种方法，使用来自另一个模型的可转移知识，并使用手头任务可用的数据对其进行提炼。这有点像在新的令人兴奋的工作场所使用你在以前的工作中学到的技能。人和系统有不同的名字，问题可能看起来有点不同，但是你的核心分析和编程技能仍然是有用的。但是如果你的工作转换是更基本的，你可能会考虑回到学校去提高你的机器学习技能。也许你想参加高级语音识别课程，了解如何检测声音中的欺骗。你的核心数学技能仍然是相关的，但你必须学习一些相当复杂的东西才能让你的下一份工作顺利进行。

在神经网络模型的情况下，第一种方法使用基本模型，该模型在一些常规任务中表现非常好，例如将所有类型的图像分类到一千个不同的类别中。我们只需去掉最后一个分类层，适合我们自己的分类层，并用新图像只训练这一层。在这种情况下，我们的基本模型只是通过创建一个已经捕获了一般相关特征的图像的表示来充当“固定特征提取器”。

第二种方法更进一步，允许我们的基本模型的权重在训练过程中改变。这被称为“微调”模型。

# 具体细节

谷歌的可教机器使用前一种方法，将基本模型视为固定的特征提取器。我们将确切地看到他们的[实现](https://github.com/googlecreativelab/teachablemachine-community/blob/master/libraries/image/src/teachable-mobilenet.ts)如何使用迁移学习模型。

![](img/cbed035b2c15493fc5746bdcbe34ba73.png)

[MobileNet](https://arxiv.org/pdf/1704.04861.pdf) paper and ImageNet Thumbnails

1.拿训练好的[MobileNet](https://arxiv.org/pdf/1704.04861.pdf)【1】模型，一个 28 层的 CNN 模型来分类图像。

2.截断此模型的 softmax 层，并将此模型的输出设置为图像的倒数第二个张量表示

3.创建一个只有两层的小模型——一层是密集层(比如 100 个单元),另一层是最终的 softmax 层，其单元数与我们想要的类数一样多。我们的第一个密集层必须接受与 MobileNet 输出相同的输入。

4.捕捉图像并使用 MobileNet 将每个图像转换为其张量。这些转换后的数据就是我们训练模型所需要的。

5.使用这些训练数据和我们定义的模型，单独训练我们的 2 层模型几个时期。

6.通过附加 MobileNet 和我们的 2 层模型创建一个联合模型。

7.给定一张新图像，通过这个联合模型运行它以生成一个预测。

由于 MobileNet 本身非常小(约 2MB)，整个模型可以放入您的浏览器中，并且没有图像需要离开浏览器进行训练。Tensorflow.js 进一步允许我们直接用 javascript 编写所有这些模型，从而赋予整个架构一些奇妙的属性。

1)训练是保护隐私的(图像不必离开你的浏览器)

2)推断可以很快，因为它不需要服务器——这是 MobileNet 通过浏览器摄像头检测几个人姿势的快照

![](img/f1d0d6be7293cb8677f0b6fe334c6f2c.png)

Multi-pose detection on a webcam — Authors webcam using image from [Source](https://takahashikarate.com/)

3)用户不需要安装任何库——打开一个网页，你就可以立即体验这个模型

4)结合上面的迁移学习方法，您可以在几分钟内训练出一个新模型

# 该不该回大学？

我们上面详述的方法显然非常有效。但是微调呢？这对训练这些内部神经元有帮助吗？

本文【2】中探讨的视觉十项全能基准旨在回答不同迁移学习方法在性能上的比较。使用一个基础模型作为在 ImageNet 1K 数据集上训练的 resnet28，本文评估了不同迁移学习方法在各种目标数据集(如 CIFAR100、VGG 弗劳尔斯等)上的性能。，这是每一列。我们刚刚讨论的两种不同的迁移学习方法在红框中突出显示为“微调”模型和“特征提取”模型。数字代表图像分类的最高精度。

![](img/045d7efdf6856d28312d821f67490022.png)

Learning multiple visual domains with residual adapters — [Source](https://arxiv.org/pdf/1705.08045.pdf)

在他们的实验中，微调似乎始终优于固定特征提取器。这很可能是因为模型学习了一些非常 Imagenet 特定的特征，例如试图区分几个狗品种。将这些神经元中的一部分重新用于目标数据集，看起来确实有积极的效果。然而，训练一个 Resnet28 是耗时且计算量大的。该白皮书还找到了一个更好的解决方案，可以充分利用这两个领域的优势。

所以回到大学确实看起来很有用，但是很贵。权衡归结为你可能获得多少，以及是否值得花费。

参考

[1] A.G. Howard，M. Zhu，B. Chen，D. Kalenichenko，W. Wang，T. Weyand，M. Andreetto，H. Adam， [MobileNets:用于移动视觉应用的高效卷积神经网络](https://arxiv.org/pdf/1704.04861.pdf) (2017)

[2] S.A. Rebuffi，H. Bilen，A. Vedaldi [用剩余适配器学习多个视觉域](https://arxiv.org/pdf/1705.08045.pdf) (2017)