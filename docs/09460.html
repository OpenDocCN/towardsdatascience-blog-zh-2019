<html>
<head>
<title>Pre-processing a Wikipedia dump for NLP model training — a write-up</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为 NLP 模型训练预处理 Wikipedia 转储—书面报告</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67?source=collection_archive---------18-----------------------#2019-12-13">https://towardsdatascience.com/pre-processing-a-wikipedia-dump-for-nlp-model-training-a-write-up-3b9176fdf67?source=collection_archive---------18-----------------------#2019-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b6d7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">下载、提取、清理和预处理 NLP 模型的维基百科转储(例如，像 BERT、RoBERTa 等的变压器)。)培训</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a80e4be1c427039e2d822ee81b3eee40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BTQ9yiSy_SLfqVmlwe7y9w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk"><a class="ae ky" href="https://www.wikiwand.com/en/Bert_(Sesame_Street)" rel="noopener ugc nofollow" target="_blank">Wikipedia entry for Bert</a> (from Sesame Street)</figcaption></figure><p id="a2e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://dumps.wikimedia.org/" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">Wikipedia dumps</strong></a><strong class="lb iu"/>在现代 NLP 研究中被频繁用于模型训练，尤其是与<strong class="lb iu">变形金刚</strong>如 BERT、RoBERTa、XLNet、XLM 等。因此，对于任何有志于掌握这些模型的 NLP 研究人员来说，这篇文章展示了下载、提取、清理和预处理维基百科转储所涉及的一切(和代码)。</p><h1 id="141c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">📥下载维基百科转储</h1><p id="fa7b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">维基百科转储以多种语言的多种格式免费提供。对于英语维基百科，最新转储的所有可用格式的完整列表可以在<a class="ae ky" href="https://dumps.wikimedia.org/enwiki/latest/" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="b4fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们主要对文本数据感兴趣，所以为了本文的目的，我们将使用下面的代码下载这样一个压缩 XML 格式的转储(只包含页面和文章):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Simple bash script to download the latest Wikipedia dump in the chosen language</figcaption></figure><p id="c238" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，要下载最新的英语维基百科转储，只需在终端中运行以下命令:<code class="fe mu mv mw mx b">./download_wiki_dump.sh en</code></p><h1 id="5add" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">🗜️提取和清理维基百科转储</h1><p id="7000" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们刚刚下载的维基百科转储还不能进行预处理(句子标记和每行一句)。首先，我们需要提取并清理转储，这可以通过使用下面的代码使用<a class="ae ky" href="https://github.com/attardi/wikiextractor/" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> WikiExtractor </strong>，</a>轻松完成:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ms mt l"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Simple bash script to extract and clean a Wikipedia dump</figcaption></figure><p id="e3d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，要提取并清理我们刚刚下载的 Wikipedia 转储，只需在您的终端中运行以下命令:<code class="fe mu mv mw mx b">./extract_and_clean_wiki_dump.sh<strong class="lb iu"> </strong>enwiki-latest-pages-articles.xml.bz2</code></p><h1 id="0fa4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">⚙️预处理维基百科转储</h1><p id="ffda" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">既然我们已经成功地下载、提取和清理了维基百科转储，我们可以开始预处理它了。实际上，这意味着对文章进行句子标记，以及将它们每行一句地写到一个文本文件中，这可以使用微软速度惊人的<a class="ae ky" href="https://github.com/microsoft/BlingFire" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">bling fire tokenizer</strong></a>来完成，使用下面的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="60fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，要预处理我们刚刚提取和清理的 Wikipedia 转储，只需在您的终端中运行以下命令:<code class="fe mu mv mw mx b">python3 preprocess_wiki_dump.py<strong class="lb iu"> </strong>enwiki-latest-pages-articles.txt</code></p></div><div class="ab cl my mz hx na" role="separator"><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd ne"/><span class="nb bw bk nc nd"/></div><div class="im in io ip iq"><p id="3e08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">就这样，大功告成！</strong>🙌现在，您可以使用自己新创建的维基百科语料库，亲自尝试 NLP 中最新最棒的内容。🤗</p></div></div>    
</body>
</html>