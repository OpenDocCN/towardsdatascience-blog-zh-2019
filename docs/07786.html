<html>
<head>
<title>Backpropagation super simplified!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播超级简化！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-super-simplified-2b8631c0683d?source=collection_archive---------17-----------------------#2019-10-28">https://towardsdatascience.com/backpropagation-super-simplified-2b8631c0683d?source=collection_archive---------17-----------------------#2019-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3aed439eeeae4f20e1b1a1956ce13377.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kS_ojcRVyYNsxsRP"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@averey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Robina Weermeijer</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="1f8b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不会说反向传播是一个非常简单的算法。如果你不知道微积分，线性代数，矩阵乘法，这可能是非常令人生畏的。即使你知道一些或全部，要掌握它也需要一点脑力锻炼。</p><p id="0889" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我这么说并不是要打击你的积极性，让你避免学习它(是的，你可以避免它，但仍然可以继续你的深度学习之旅)。它有点复杂，但我不会说它超级难，而是非常直观和容易掌握。你会惊奇地发现，与它所解决的问题相比，它是多么简单。它实际上是深层神经网络的主干。所需的概念非常容易学习，而<a class="ae jg" href="https://www.khanacademy.org/" rel="noopener ugc nofollow" target="_blank">可汗学院</a>是实现这一目的的绝佳资源。我已经列出了所需数学概念的 URL，你可以在阅读帖子之前回顾一下。</p><p id="b004" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">1.<a class="ae jg" href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/v/chain-rule-introduction" rel="noopener ugc nofollow" target="_blank">链式法则</a></p><p id="6352" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">2.<a class="ae jg" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient" rel="noopener ugc nofollow" target="_blank">梯度下降</a></p><p id="4d51" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3.<a class="ae jg" href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations" rel="noopener ugc nofollow" target="_blank">矩阵</a></p><p id="41ac" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于反向传播算法的背景知识已经讲得够多了，我们现在开始吧。当我刚开始学习反向传播算法时，我发现节点的表示和权重非常令人困惑，而不是算法本身。所以，我会尽量让它变得简单。让我们从一个非常简单的神经网络开始。</p><h1 id="abf9" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">简单神经网络</h1><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mc"><img src="../Images/d3afc8e5527eeb10a80e262707c40ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ZqMi-HdD35Rat7HA"/></div></div></figure><h2 id="280b" class="mh lf jj bd lg mi mj dn lk mk ml dp lo kr mm mn ls kv mo mp lw kz mq mr ma ms bi translated">正向输送</h2><p id="b634" class="pw-post-body-paragraph kg kh jj ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">在这个神经网络中，我们将计算每一层的值。</p><p id="3648" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">输入:</strong></p><p id="4be1" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my"> x = z = a </em></p><p id="8a23" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="my"> H1 图层:</em> </strong></p><p id="416f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my"> z = w x + b = w a + b </em></p><p id="2add" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my"> a = f(z ) = f(w a + b ) </em></p><p id="62af" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="my"> H2 层:</em> </strong></p><p id="8671" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my"> z = w a + b </em></p><p id="1e9d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my"> a = f(z ) = f(w a + b ) </em></p><p id="fc09" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk">T29】输出:T31】</strong></p><p id="cccf" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my"> z⁴ = w a </em></p><p id="c65a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my">o =f(z⁴)= f(w a)= f(w(f(w(f(w(w a+b)+b))))</em></p><p id="b022" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这被称为前馈传播。我们必须一次又一次地这样做，通过改变我们的权重和偏好来接近我们想要的输出。</p><h2 id="8955" class="mh lf jj bd lg mi mj dn lk mk ml dp lo kr mm mn ls kv mo mp lw kz mq mr ma ms bi translated">梯度下降和反向传播</h2><p id="453c" class="pw-post-body-paragraph kg kh jj ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">但是，问题是如何改变权重。这就是这个算法的配方。为此，我们需要使用梯度下降，并向后传播(因此得名反向传播算法)，并根据需要改变权重。</p><p id="b75d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，我们的期望输出是<em class="my"> y </em>，当前输出是<em class="my"> o. </em>为了评估我们预测中的差异，我们引入了<strong class="ki jk">成本函数</strong>，也称为 l <strong class="ki jk"> oss 函数</strong>。它可以像<strong class="ki jk"> MSE </strong>(均方误差)一样简单，也可以像<strong class="ki jk">交叉熵</strong>函数一样简单。在这里，我们称之为 C，所以，我们的成本函数将是<em class="my"> C(o，y) </em>。</p><p id="190a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们有了函数形式的差异。我们可以引入微积分来玩玩。我们需要一些函数来帮助我们减少预测值和实际产量之间的差异，我们已经以成本函数的形式量化了这种差异。这就是我们需要梯度下降算法的地方，梯度下降算法是通过不断向最陡下降的方向移动来最小化一个函数，最陡下降的方向等于梯度的负值。</p><p id="a266" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们更深入地研究这种梯度下降。一个函数的梯度意味着，一个函数相对于一个特定量的变化而变化多少。它是通过对特定量取函数的偏导数来计算的。在这种情况下，我们希望看到我们的成本函数相对于我们的权重和偏差的变化有多少变化，从而给出如何改变权重以在我们的预测中获得最小可能误差的想法。</p><p id="2245" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回到我们的例子:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e8eb2e6c99365765ec3451e50acadb5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*upO3OU-L-s7JLtk-TBsVxg.png"/></div></figure><p id="3457" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由于，<em class="my"> C </em>是<em class="my"> o 中的函数，</em>又是 w_1 中的函数，我们可以利用链式法则得到上面的偏导数。</p><p id="9fa5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="my">记住，</em><strong class="ki jk"><em class="my">o =f(z⁴)= f(w a)= f(w(f(w(w(w a+b)+b))))</em></strong></p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/bb3b0174c7d559ae189f31053ae1e4fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*vd6aIqBmbkLZ4nlTLWPl1g.png"/></div></figure><p id="8032" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(同理，我们可以计算出<em class="my"> C </em> w.r.t 其他权重的梯度。)</p><p id="3827" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，我们需要注意几件事:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/a98c54bcff470515573a922775084744.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*N5FfLsoE3EX9kzlTZ-BD_w.png"/></div></figure><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a1ae8fbd30f83f0b01bb7ead257f850f.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*e--RjRcbg7uPhp4aa-xN8g.png"/></div></figure><p id="f3d0" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">术语<strong class="ki jk"> <em class="my"> ∂C/∂w^n </em> </strong>也被称为<strong class="ki jk">局部梯度</strong>。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/b99aac6e3d6c1e0c69be1974924ac5c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*wujLN0qH6xg2aEZ7FBNIDA.png"/></div></figure><p id="e5bd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">类似地，对于偏差:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/dbd6b6b66338a96740152b8f31a7b62b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*P7xFFME9bnTs0SVWj0ZYXg.png"/></div></figure><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3ae18c2f3b21c52d7dcbd8630af14181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*njM8uw_1VCyDxjFK5EDqxw.png"/></div></figure><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/649de7c1d920e266f64860f6ea2d87e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*TexYj07Nsg1cPZixpCZjUw.png"/></div></figure><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ccc617632133520ba4a05bcbd709f67e.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*QuJf3ehawhA6HPc54d81GQ.png"/></div></figure><p id="802c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好了，现在是时候收获我们辛勤工作的果实了。我们已经准备好必须改变权重的量，即 w_n。因此，要修改我们的权重和偏差，我们需要:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e6a8978ae0a34355a63434b7d4212b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*9yxZfRknWZPxifV-mjOAXw.png"/></div></figure><p id="bb35" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里，我们介绍了<strong class="ki jk"> <em class="my"> ϵ </em> </strong>对渐变下降的影响。现在，该模型使用新的权重和偏差进行前向传播，然后一次又一次地反向传播。它继续下去，直到根据我们的约束最小化我们的成本函数到尽可能低的值。</p><h1 id="89b3" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">小型复杂神经网络</h1><p id="faba" class="pw-post-body-paragraph kg kh jj ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">此刻，你一定觉得很不完整，因为你可能会说这个模型非常简单，一旦层中有很多节点，事情就会变得非常复杂。</p><p id="118e" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不完全是。事情依然如故，强硬程度依然如故。你现在要做的就是让自己熟悉编写复杂的指数和我们良好的 ole sigma(≘)符号。记住这一点，我们将再次检查表达式，但对于下面的神经网络，你会同意是相当复杂的。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mc"><img src="../Images/1aa1a21d09e6fbd26fcbfb33758ff151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c-dj-q7RX8hL0ilR"/></div></div></figure><figure class="md me mf mg gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/a175d6143e74ee472d148779b39d4eca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xjbXbzNmjj8izOO35lBAsA.png"/></div></div></figure><p id="71bd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这里要小心，这不是直觉的东西，你必须习惯它。再次概括一下，我们有:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c8b6af1a83270b66293a1113f5b0e544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*ZmgcVk934sByF-nUvDhpbw.png"/></div></figure><p id="d4fa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">记住这一点，我们将尝试重新编写我们之前的表达式，但我将从第一层中选取一个特定的权重进行演示，然后我们将对其进行推广。</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/69f2425e37d7bc717438932e8bfdc891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*op3NkZE2gcgk8I7S4DD30g.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Value of z for node 3 in layer 2</figcaption></figure><p id="1205" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对任意层中任意节点的 z 值进行概化，我们得到:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/46289952dc5b9df6db81abfd2c511f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*W_nV7Lu6iqKLibCO29Fg4A.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Generalized for of the value of z for any node in layer L</figcaption></figure><p id="5ea5" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，相应的激活函数将是:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/c1585012fe0f809a6c3b4754ffbd0253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Kpae4Y_pXP2herAaFqEBXQ.png"/></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Generalized value of a for node j of layer L.</figcaption></figure><p id="0c0c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki jk"> <em class="my">输出层:</em> </strong></p><p id="a8b9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们的例子中，<strong class="ki jk">oT7<em class="my">是输出:</em></strong></p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/21aa9da7a32fe55af22536f848d038c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*-AOswh8HzsJ8umvq-rr8mg.png"/></div></figure><p id="e862" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，对我们的成本函数<em class="my"> C(o，y) </em>应用梯度下降，我们得到:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1ad1b6b92d388c434ebb1d3e75d90492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*9FBq4z3hI6suxxZJFehR0A.png"/></div></figure><p id="edd9" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">看看上面的表达式，这基本上是这个算法最复杂的部分。试着理解它是什么。我们将以类似的方式概括成本相对于所有其他权重的梯度。前面的表达式什么也没做，只是在向后传播时考虑了特定层的所有节点。我们如何做到这一点，非常简单，就是在我们试图改变的权重之前，将所有层的所有节点的所有计算相加。</p><p id="6d7d" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">概括起来，我们可以写:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/eff855f3b3f1dee2a24eb2c3d85a2ed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*dMiJ48CbyhJkb7tjoFONOw.png"/></div></figure><p id="da75" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，权重更新以与我们之前完全相同的方式完成:</p><figure class="md me mf mg gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/e6a8978ae0a34355a63434b7d4212b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*9yxZfRknWZPxifV-mjOAXw.png"/></div></figure><p id="b612" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">拍拍自己，你做到了深度学习最重要的算法之一——反向传播算法的终点。如果你理解了这个算法，你现在可以更深入地挖掘，可以很容易地得到像<a class="ae jg" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失渐变</a>和更多的概念。</p><p id="bc3f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对这篇文章有任何问题或编辑，请在评论区留下回复，我会回复的。</p><p id="5f24" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另外，请继续关注许多这样的深度学习算法和编码练习。</p></div></div>    
</body>
</html>