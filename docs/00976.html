<html>
<head>
<title>TF-IDF from scratch in python on a real-world dataset.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF-IDF 在真实数据集上用 python 从头开始。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089?source=collection_archive---------0-----------------------#2019-02-15">https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089?source=collection_archive---------0-----------------------#2019-02-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><h1 id="5293" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">目录:</h1><ul class=""><li id="dd1e" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">什么是 TF-IDF？</li><li id="de3f" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">预处理数据。</li><li id="f5b0" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">标题和正文的权重。</li><li id="96f8" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">使用 TF-IDF <strong class="kn ir">匹配分数的文档检索。</strong></li><li id="e46a" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">利用 TF-IDF <strong class="kn ir">余弦相似度进行文档检索。</strong></li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/bcf1eb96ce4102e1ef8ef29181e674ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WapTyrci2fL-8_um"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Photo by <a class="ae ly" href="https://unsplash.com/@sanwaldeen_sink?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sanwal Deen</a> on <a class="ae ly" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="2c56" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">简介:TF-IDF</h1><p id="419f" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">TF-IDF 代表<strong class="kn ir">“词频—逆文档频率”</strong>。这是一种量化一组文档中的单词的技术。我们通常为每个单词计算一个分数，以表示它在文档和语料库中的重要性。该方法是信息检索和文本挖掘中广泛使用的技术。</p><p id="380c" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">如果我给你一个句子，比如“这栋楼好高”。我们知道单词和句子的语义，就很容易理解这个句子。但是任何程序(比如:python)如何解读这句话呢？任何编程语言都更容易理解数值形式的文本数据。因此，出于这个原因，我们需要对所有文本进行矢量化，以便更好地表示。</p><p id="fe02" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">通过对文档进行矢量化，我们可以进一步执行多项任务，例如查找相关文档、排序、聚类等。当您执行 google 搜索时，使用的正是这种技术(现在它们被更新为更新的 transformer 技术)。网页称为文档，用来搜索的文本称为查询。搜索引擎维护所有文档的固定表示。当您使用查询进行搜索时，搜索引擎将找到该查询与所有文档的相关性，按照相关性的顺序对它们进行排序，并向您显示前 k 个文档。所有这些过程都是使用查询和文档的矢量化形式来完成的。</p><p id="e276" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">现在回到我们的 TF-IDF，</p><p id="0b95" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">TF-IDF =术语频率(TF) *逆文档频率(IDF)</p><h2 id="8dfd" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">术语</h2><ul class=""><li id="82b9" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">t —术语(单词)</li><li id="0be6" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">d-文档(一组单词)</li><li id="82fc" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">N —语料库的计数</li><li id="5d5b" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">语料库—整个文档集</li></ul><h2 id="d856" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated"><strong class="ak">词频</strong></h2><p id="0713" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">这衡量一个单词在文档中的出现频率。这在很大程度上取决于文档的长度和单词的一般性，例如，一个非常常见的单词如“was”<strong class="kn ir"/>可以在一个文档中出现多次。但是，如果我们取两个分别具有 100 个单词和 10，000 个单词的文档，则在 10，000 个单词的文档中，常见单词“was”出现的概率较高。但是我们不能说较长的文件比较短的文件更重要。正是因为这个原因，我们对频率值进行了归一化，我们用频率除以文档中的总字数。</p><p id="ea6a" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">回想一下，我们需要最终对文档进行矢量化。当我们计划对文档进行矢量化时，我们不能只考虑特定文档中出现的单词。如果我们这样做，那么两个文档的向量长度将会不同，并且计算相似性是不可行的。因此，我们所做的是将<strong class="kn ir"> vocab </strong>上的文档矢量化。Vocab 是语料库中所有可能世界的列表。</p><p id="cfd8" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们需要所有词汇的字数和文档的长度来计算 TF。如果某个特定文档中不存在该术语，则该特定文档的特定 TF 值将为 0。在一个极端的情况下，如果文档中的所有单词都相同，那么 TF 将为 1。归一化的 TF 值的最终值将在[0 到 1]的范围内。0，1 包括在内。</p><p id="d84a" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">TF 对于每个文档和单词都是独立的，因此我们可以将 TF 表述如下:</p><blockquote class="nf"><p id="bca7" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">tf(t，d)= d 中 t 的计数/d 中的字数</p></blockquote><p id="49df" class="pw-post-body-paragraph lz ma iq kn b ko np mb mc kq nq md me ks nr mg mh ku ns mj mk kw nt mm mn ky ij bi translated">如果我们已经计算了 TF 值，并且如果这产生了文档的矢量化形式，为什么不仅仅使用 TF 来寻找文档之间的相关性呢？我们为什么需要英特尔信息技术峰会？</p><p id="b7c7" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">让我解释一下，像“是”、“是”这样最常见的词会有很高的值，赋予这些词很高的重要性。但是用这些词来计算相关性会产生不好的结果。这类常用词被称为停用词。虽然我们将在稍后的预处理步骤中删除停用词，但是更理想的是在文档中找到该词的存在，并以某种方式减少它们的权重。</p><h2 id="3573" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated"><strong class="ak">文档频率</strong></h2><p id="43ab" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">这个<strong class="kn ir"> </strong>衡量文档在一整套语料库中的重要性。这与 TF 非常相似，但唯一的区别是 TF 是文档 d 中术语 t 的频率计数器，而 DF 是术语 t 在文档集 n 中出现<strong class="kn ir">次</strong>的计数。换句话说，DF 是该单词出现在其中的文档的数量。如果该术语在文档中至少出现一次，我们就认为出现了一次，我们不需要知道该术语出现的次数。</p><blockquote class="nf"><p id="6f64" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">df(t)= t 在 N 个文档中的出现次数</p></blockquote><p id="b7fc" class="pw-post-body-paragraph lz ma iq kn b ko np mb mc kq nq md me ks nr mg mh ku ns mj mk kw nt mm mn ky ij bi translated">为了保持这也在一个范围内，我们通过除以文档总数来标准化。我们的主要目标是知道一个术语的信息量，DF 是它的精确逆。这就是我们反转 DF 的原因</p><h2 id="9091" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated"><strong class="ak">逆文档频率</strong></h2><p id="596f" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">IDF 是衡量术语 t 的信息量的文档频率的倒数。当我们计算 IDF 时，对于最常出现的单词(如停用词)来说，IDF 将非常低(因为它们出现在几乎所有的文档中，并且 N/df 将给予该单词非常低的值)。这最终给出了我们想要的相对权重。</p><blockquote class="nf"><p id="2caa" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">idf(t) = N/df</p></blockquote><p id="d492" class="pw-post-body-paragraph lz ma iq kn b ko np mb mc kq nq md me ks nr mg mh ku ns mj mk kw nt mm mn ky ij bi translated">现在 IDF 有一些其他问题，当我们有一个大的语料库规模，比如 N=10000，IDF 值会爆炸。因此，为了抑制这种影响，我们采用了 IDF 的日志。</p><p id="9241" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">在查询时，当单词不在 vocab 中时，它将被忽略。但是在少数情况下，我们使用固定的 vocab，并且 vocab 中的一些单词可能在文档中不存在，在这种情况下，df 将是 0。因为我们不能被 0 整除，所以我们通过在分母上加 1 来平滑这个值。</p><blockquote class="nf"><p id="6e72" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">idf(t) = log(N/(df + 1))</p></blockquote><p id="8313" class="pw-post-body-paragraph lz ma iq kn b ko np mb mc kq nq md me ks nr mg mh ku ns mj mk kw nt mm mn ky ij bi translated">最后，通过取 TF 和 IDF 的乘积值，我们得到 TF-IDF 分数。TF-IDF 有许多不同的版本，但是现在，让我们集中在这个基本版本上。</p><blockquote class="nf"><p id="7089" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">tf-idf(t，d) = tf(t，d) * log(N/(df + 1))</p></blockquote><h1 id="d46f" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk bi translated">关于我:</h1><p id="6a4e" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">我是 NLP 和 DL 领域的高级数据科学家和 AI 研究员。<br/>联系我:<a class="ae ly" href="https://twitter.com/PWilliamScott" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae ly" href="https://www.linkedin.com/in/williamscottp/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</p><h1 id="7ae0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">在真实数据集上实现</h1><p id="4aea" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">现在我们已经了解了什么是 TF-IDF，让我们来计算数据集的相似性得分。</p><p id="b3d2" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们将要使用的数据集是几个故事的存档，这个数据集有许多不同格式的文档。下载数据集，打开你们的笔记本，我指的是 Jupyter 笔记本😜。</p><p id="7f7f" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">数据集链接:<a class="ae ly" href="http://archives.textfiles.com/stories.zip" rel="noopener ugc nofollow" target="_blank">http://archives.textfiles.com/stories.zip</a></p><h2 id="4889" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">步骤 1:分析数据集</h2><p id="3160" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">任何机器学习任务的第一步都是分析数据。因此，如果我们查看数据集，乍一看，我们会看到所有包含英文单词的文档。每个文档都有不同的名称，其中有两个文件夹。</p><p id="e7be" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">现在一个重要的任务是识别正文中的标题，如果我们分析文档，有不同的标题排列模式。但是大多数标题都是居中对齐的。现在我们需要想办法提取标题。但是在我们全力以赴开始编码之前，让我们稍微深入分析一下数据集。</p><p id="eee9" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">花几分钟时间自己分析数据集。尝试探索…</p><p id="e268" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">经过进一步检查，我们可以注意到每个文件夹(包括根目录)中都有一个 index.html，其中包含所有的文档名及其标题。因此，让我们认为自己是幸运的，因为标题是给我们的，而不是穷尽地从每个文件中提取标题。</p><h2 id="4d7b" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">步骤 2:提取标题和正文:</h2><p id="f402" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">没有具体的方法来做到这一点，这完全取决于手头的问题陈述和我们对数据集所做的分析。</p><p id="0711" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">因为我们已经发现标题和文档名在 index.html 中，所以我们需要提取这些名称和标题。我们很幸运，index.html 有标签，我们可以使用模式提取我们需要的内容。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e6f1813e7951c417ac063b31c98c77e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*FzVgRqUI93BYWTeYpH_mjA.png"/></div></figure><p id="48d6" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">在我们开始提取标题和文件名之前，因为我们有不同的文件夹，首先让我们抓取文件夹，以便稍后一次性读取所有 index.html 文件。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="cbbb" class="mt jo iq nz b gy od oe l of og">[x[0] for x in os.walk(str(os.getcwd())+’/stories/’)]</span></pre><p id="7072" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">os.walk 提供了目录中的文件，os.getcwd 提供了当前目录和标题，我们将在当前目录+ stories 文件夹中进行搜索，因为我们的数据文件位于 stories 文件夹中。</p><blockquote class="nf"><p id="5199" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">总是<strong class="ak">假设</strong>你正在处理一个巨大的数据集，这有助于自动化代码。</p></blockquote><p id="eb4d" class="pw-post-body-paragraph lz ma iq kn b ko np mb mc kq nq md me ks nr mg mh ku ns mj mk kw nt mm mn ky ij bi translated">现在我们可以发现，<strong class="kn ir">文件夹</strong>给了根文件夹额外的<strong class="kn ir"> / </strong>，所以我们要删除它。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="bb5b" class="mt jo iq nz b gy od oe l of og">folders[0] = folders[0][:len(folders[0])-1]</span></pre><p id="af13" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">上面的代码删除了文件夹中第 0 个索引的最后一个字符，这是根文件夹</p><p id="3a85" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">现在，让我们遍历所有的 index.html，提取它们的标题。要做到这一点，我们需要找到一种模式来去掉标题。由于这是在 html 中，我们的工作会简单一点。</p><p id="f09f" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">让我想想…</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d5453b3a82854efbf7e4139b033221e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*CPUina17fnWY7likaWrK2Q.png"/></div></figure><p id="6223" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们可以清楚地观察到，每个文件名都被括在(<strong class="kn ir"> &gt; &lt;)和(<strong class="kn ir"> " </strong> ) <strong class="kn ir"> </strong>之间，每个标题都在<strong class="kn ir"/>(<strong class="kn ir">&lt;BR&gt;&lt;TD&gt;</strong>)和(<strong class="kn ir"> \n </strong>)之间</strong></p><p id="645d" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们将使用简单的正则表达式来检索名称和标题。以下代码给出了与该模式匹配的所有值的列表。因此 names 和 titles 变量包含了所有名字和头衔的列表。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="df0e" class="mt jo iq nz b gy od oe l of og">names = re.findall(‘&gt;&lt;A HREF=”(.*)”&gt;’, text)<br/>titles = re.findall(‘&lt;BR&gt;&lt;TD&gt; (.*)\n’, text)</span></pre><p id="b994" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">现在我们已经有了从索引中检索值的代码，我们只需要迭代所有的文件夹，并从所有 index.html 文件中获取标题和文件名</p><blockquote class="oi oj ok"><p id="4a82" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-从索引文件中读取文件</p><p id="630d" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-提取标题和名称</p><p id="df49" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-迭代到下一个文件夹</p></blockquote><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="0ada" class="mt jo iq nz b gy od oe l of og">dataset = []</span><span id="8cf5" class="mt jo iq nz b gy op oe l of og">for i in folders:<br/>    file = open(i+"/index.html", 'r')<br/>    text = file.read().strip()<br/>    file.close()</span><span id="978d" class="mt jo iq nz b gy op oe l of og">    file_name = re.findall('&gt;&lt;A HREF="(.*)"&gt;', text)<br/>    file_title = re.findall('&lt;BR&gt;&lt;TD&gt; (.*)\n', text)<br/>    <br/>    for j in range(len(file_name)):<br/>        dataset.append((str(i) + str(file_name[j]), file_title[j]))</span></pre><p id="7c48" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">这将准备数据集的索引，数据集是文件位置及其标题的元组。有一个小问题，根文件夹 index.html 也有文件夹及其链接，我们需要删除这些。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oq"><img src="../Images/154fc5179c942b97c7da4bba0676c80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Le7cOSkXe93zFPFTmC56KQ.png"/></div></div></figure><p id="9ba6" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">只需使用条件检查器来删除它。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="c8af" class="mt jo iq nz b gy od oe l of og">if c == False:<br/>    file_name = file_name[2:]<br/>    c = True</span></pre><h1 id="d5e0" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">第三步:预处理</h1><p id="9bca" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">预处理是我们处理任何文本模型的主要步骤之一。在此阶段，我们必须查看数据的分布情况，需要什么技术以及应该清理多深。</p><p id="9f10" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">这一步从来没有一个统一的规则，完全取决于问题陈述。几个强制性的预处理是:转换成小写，删除标点符号，删除停用词和词干。在我们的问题陈述中，基本的预处理步骤似乎就足够了。</p><h2 id="0451" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">小写字母</h2><p id="3170" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">在文本处理过程中，每个句子被拆分成单词，每个单词被视为预处理后的一个标记。编程语言认为文本数据是敏感的，这意味着<strong class="kn ir">与<strong class="kn ir">不同。</strong>我们人类知道这两者属于同一个标记，但是由于字符编码，它们被认为是不同的标记。转换成小写是一个非常强制性的预处理步骤。因为我们的所有数据都在列表中，所以 numpy 有一个方法可以立刻将列表转换成小写。</strong></p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="20bb" class="mt jo iq nz b gy od oe l of og">np.char.lower(data)</span></pre><h2 id="18b7" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">停止言语</h2><p id="d45e" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">停用词是最常见的不会给文档向量带来任何附加值的词。事实上，删除这些将增加计算和空间效率。nltk 库有一个下载停用词的方法，所以我们可以直接使用 nltk 库，遍历所有的词并删除停用词，而不是自己显式地提到所有的停用词。有许多有效的方法可以做到这一点，但我只给出一个简单的方法。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi or"><img src="../Images/80a3eae49f4d75c17fa29a50e604859b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*d3iD7b4YXHa0Ybjlgnrh6A.png"/></div></figure><p id="580c" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们将遍历所有的停用词，如果是停用词，就不把它们追加到列表中</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="f954" class="mt jo iq nz b gy od oe l of og">new_text = ""<br/>for word in words:<br/>    if word not in stop_words:<br/>        new_text = new_text + " " + word</span></pre><h2 id="8516" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">标点</h2><p id="91cd" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">标点符号是语料库文档中不必要的符号集。我们应该对我们所做的事情小心一点，可能会有一些问题，比如预处理后 us-us“United Stated”被转换为“us”。连字符和通常应小心处理。但是对于这个问题陈述，我们将删除它们</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="84db" class="mt jo iq nz b gy od oe l of og">symbols = "!\"#$%&amp;()*+-./:;&lt;=&gt;?@[\]^_`{|}~\n"<br/>for i in symbols:<br/>    data = np.char.replace(data, i, ' ')</span></pre><p id="f3a6" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们将所有的符号存储在一个变量中，并迭代该变量，删除整个数据集中的特定符号。我们在这里使用 numpy，因为我们的数据存储在一个列表列表中，numpy 是我们的最佳选择。</p><h2 id="ec40" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">撇号</h2><p id="a4c9" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">注意标点符号中没有撇号。因为当我们首先删除标点符号时，它会将 don't 转换为 dont，并且它是一个不会被删除的停用词。我们要做的是，首先删除停用词，然后是符号，最后重复停用词删除，因为少数词可能仍然有撇号，而不是停用词。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="7c47" class="mt jo iq nz b gy od oe l of og">return np.char.replace(data, "'", "")</span></pre><h2 id="49b5" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">单个字符</h2><p id="3d84" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">单个字符在了解文档的重要性方面没有太大用处，并且少数最后的单个字符可能是不相关的符号，所以删除单个字符总是好的。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="0ff9" class="mt jo iq nz b gy od oe l of og">new_text = ""<br/>for w in words:<br/>    if len(w) &gt; 1:<br/>       new_text = new_text + " " + w</span></pre><p id="98dc" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们只需要迭代所有的单词，如果长度不大于 1，就不追加单词。</p><h2 id="b460" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">堵塞物</h2><p id="2135" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">这是预处理的最后也是最重要的部分。词干处理将单词转换成词干。</p><p id="4660" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">例如，<strong class="kn ir"> playing 和 played </strong>是同一类型的词，基本上表示一个动作<strong class="kn ir"> play。</strong>斯特梅尔正是这样做的，它将单词简化为词干。我们将使用一个名为 porter-stemmer 的库，它是一个基于规则的词干分析器。波特-斯特梅尔识别并移除单词的后缀或词缀。词干分析器给出的单词不需要有几次是有意义的，但是它将被识别为模型的单个标记。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi os"><img src="../Images/ab50366e97c54871687e3a6f30a9c053.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*gwPsPUq2yS9hlql5qBoTkQ.png"/></div></figure><h2 id="dade" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">引理满足</h2><p id="2182" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">引理化是一种将单词简化为单词的根同义词的方法。与词干提取不同，词汇匹配确保缩减后的单词再次成为词典中的单词(同一种语言中的单词)。WordNetLemmatizer 可用于对任何单词进行词汇化。</p><h2 id="97fd" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">词干化与词汇化</h2><p id="5ad9" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">词干-不必是字典中的单词，根据一些规则删除前缀和词缀</p><p id="0a56" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">词汇化——将成为词典中的一个词。简化为词根同义词。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3eedc955aaa5d933e9d465f25aa60dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*uVgEZI7UFLMjHqemI_MzGA.png"/></div></figure><p id="820b" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">一个更有效的方法是先用引理，然后用词干，但是对于一些问题语句，单独用词干也可以，这里我们不使用引理。</p><h2 id="5026" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">转换数字</h2><p id="5fd5" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">当用户给出诸如<strong class="kn ir"> 100 美元</strong>或<strong class="kn ir">100 美元的查询时。</strong>对于用户来说，这两个搜索词是相同的。但是我们的 IR 模型将它们分开处理，因为我们将 100 美元、100 元作为不同的代币存储。因此，为了使我们的红外模式更好一点，我们需要将 100 转换为 100。为了实现这一点，我们将使用一个名为<strong class="kn ir"> num2word </strong>的库。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/9e9b94492e47379ba2e91dfa24698d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*VPLWTb0cbjaMsj40r87q5w.png"/></div></figure><p id="0759" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">如果我们看一下上面的输出，它给出了一些符号和句子，如“一百个<strong class="kn ir"> <em class="ol">和</em> </strong>两个”，但是该死的我们刚刚清理了我们的数据，那么我们该如何处理呢？不用担心，我们将在将数字转换为单词后再次运行标点和停止单词。</p><h2 id="fdeb" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">预处理</h2><p id="8414" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">最后，我们将把所有这些预处理方法放入另一个方法中，我们称之为预处理方法。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="e828" class="mt jo iq nz b gy od oe l of og">def preprocess(data):<br/>    data = convert_lower_case(data)<br/>    data = remove_punctuation(data)<br/>    data = remove_apostrophe(data)<br/>    data = remove_single_characters(data)<br/>    data = convert_numbers(data)<br/>    data = remove_stop_words(data)<br/>    data = stemming(data)<br/>    data = remove_punctuation(data)<br/>    data = convert_numbers(data)</span></pre><p id="c6f9" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">如果仔细观察，一些预处理方法会再次重复。正如所讨论的，这只是帮助清理数据的一点点深度。现在我们需要阅读文档，并将它们的标题和正文分开存储，因为我们以后会用到它们。在我们的问题陈述中，我们有非常不同类型的文档，由于编码兼容性，这可能会导致阅读文档时出现一些错误。要解决这个问题，只需在 open()方法中使用 encoding="utf8 "，errors='ignore'。</p><h2 id="f1ad" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">步骤 3:计算 TF-IDF</h2><p id="daa5" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">回想一下，我们需要给标题和正文不同的权重。现在我们该如何处理这个问题呢？在这种情况下，TF-IDF 的计算将如何进行？</p><p id="965c" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">给标题和正文不同的权重是一种非常常见的方法。我们只需要把文档看作 body + title，使用它我们可以找到 vocab。并且我们需要给标题中的单词不同的权重，给正文中的单词不同的权重。为了更好地解释这一点，让我们考虑一个例子。</p><p id="e726" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">title = "这是一篇小说论文"</p><p id="d552" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">body = "本文由许多论文的综述组成"</p><p id="fae9" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">现在，我们需要计算身体和标题的 TF-IDF。暂时让我们只考虑单词<strong class="kn ir"> paper </strong>，而忘记删除停用词。</p><p id="9495" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">题目中 word <strong class="kn ir"> paper </strong>的 TF 是多少？1/4?</p><p id="776e" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">不，是 3/13。怎么会？word paper 在标题和正文中出现 3 次，标题和正文的总字数为 13。正如我之前提到的，我们只是<strong class="kn ir">认为</strong>标题中的单词具有不同的权重，但是在计算 TF-IDF 时，我们仍然考虑整个文档。</p><p id="530a" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">那么<strong class="kn ir">论文</strong>无论是标题还是正文的 TF 都是一样的？对，是一样的！这只是我们要给出的重量差。如果这个词同时出现在标题和正文中，那么 TF-IDF 值不会有任何减少。如果这个单词只出现在标题中，那么这个特定单词的正文的权重就不会加到这个单词的 TF 中，反之亦然。</p><blockquote class="oi oj ok"><p id="17c0" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">文档=正文+标题</p><p id="effb" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">TF-IDF(文档)= TF-IDF(标题)* alpha + TF-IDF(正文)* (1-alpha)</p></blockquote><h2 id="7710" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">计算 DF</h2><p id="21d1" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">让我们聪明点，事先算算 DF。我们需要遍历所有文档中的所有单词，并存储每个单词的文档 id。为此，我们将使用字典，因为我们可以将单词用作键，将一组文档用作值。我提到 set 是因为，即使我们试图多次添加文档，set 也不会只接受重复的值。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="aadf" class="mt jo iq nz b gy od oe l of og">DF = {}<br/>for i in range(len(processed_text)):<br/>    tokens = processed_text[i]<br/>    for w in tokens:<br/>        try:<br/>            DF[w].add(i)<br/>        except:<br/>            DF[w] = {i}</span></pre><p id="6a03" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">如果单词还没有集合，我们将创建一个集合，否则将它添加到集合中。try 块检查这种情况。这里 processed_text 是文档的主体，我们将对标题重复同样的操作，因为我们需要考虑整个文档的 DF。</p><blockquote class="oi oj ok"><p id="713b" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">len(DF)将给出独特的单词</p></blockquote><p id="18fe" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">DF 将单词作为键，文档 id 列表作为值。但是对于 DF，我们实际上不需要文档列表，我们只需要计数。所以我们要用它的计数来替换这个列表。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/fe323a8f1821a5c9cca680eab87d1cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*izuc05xNS0ii3tID-9_TpA.png"/></div></figure><p id="e910" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">这就是我们需要的所有单词的数量。为了在我们的词汇表中找到全部的唯一单词，我们需要使用 DF 的所有关键字。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/c43ac51b847ccfc803424c14e83a83e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*Zu8CxAxcU4qhI9X0M4KuHg.png"/></div></figure><h2 id="a97b" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">计算 TF-IDF</h2><p id="2560" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">回想一下，我们需要为标题和正文保持不同的权重。为了计算正文或标题的 TF-IDF，我们需要同时考虑标题和正文。为了使我们的工作简单一点，让我们使用一个字典，将<em class="ol">(文档，令牌)</em>对作为键，将任何 TF-IDF 分数作为值。我们只需要迭代所有文档，我们可以使用 Coutner，它可以给出令牌的频率，计算 tf 和 idf，最后作为(doc，token)对存储在 tf_idf 中。tf_idf 字典是针对正文的，我们将使用相同的逻辑为标题中的单词构建一个字典 tf_idf_title。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="b61c" class="mt jo iq nz b gy od oe l of og">tf_idf = {}<br/>for i in range(N):<br/>    tokens = processed_text[i]<br/>    counter = Counter(tokens + processed_title[i])<br/>    for token in np.unique(tokens):<br/>        tf = counter[token]/words_count<br/>        df = doc_freq(token)<br/>        idf = np.log(N/(df+1))<br/>        tf_idf[doc, token] = tf*idf</span></pre><p id="8a76" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">来计算不同的权重。首先，我们需要维护一个值 alpha，它是主体的权重，那么显然 1-alpha 将是标题的权重。现在让我们深入研究一下数学，我们讨论过，如果一个单词同时出现在正文和标题中，那么这个单词的 TF-IDF 值将是相同的。我们将维护两个不同的 tf-idf 字典，一个用于正文，一个用于标题。</p><p id="cc17" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我们要做的是聪明一点，我们会为身体计算 TF-IDF；全身 TF-IDF 值乘以α；迭代标题中的标记；替换(文档，令牌)对的正文 TF-IDF 值中的标题 TF-IDF 值。花些时间来处理这个:P</p><blockquote class="oi oj ok"><p id="2ae8" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">流量:</p><p id="cd53" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-计算所有文档正文的 TF-IDF</p><p id="7e41" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-计算所有文档标题的 TF-IDF</p><p id="3318" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-将主体 TF-IDF 乘以α</p><p id="98d0" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">-Iterate Title IF-IDF for every(doc，token)</p><p id="5f06" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">—如果标记在正文中，请将正文(文档，标记)值替换为标题(文档，标记)中的值</p></blockquote><p id="a539" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">我知道这一开始并不容易理解，但还是让我解释一下为什么上面的流程有效，因为我们知道如果标记在两个地方，那么主体和标题的 tf-idf 将是相同的，我们对主体和标题使用的权重总和为 1</p><p id="5975" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">TF-IDF = body _ TF-IDF * body _ weight+title _ TF-IDF * title _ weight</p><p id="7119" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">body_weight + title_weight = 1</p><p id="4796" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">当一个令牌同时出现在这两个地方时，那么最终的 TF-IDF 将与获取 body 或 title tf_idf 相同。这正是我们在上面的流程中所做的。最后，我们有一个字典 tf_idf，它的值是(doc，token)对。</p><h1 id="2212" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">步骤 4:使用匹配分数排名</h1><p id="5dfa" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">匹配分数是计算相似性的最简单的方法，在这种方法中，我们为每个文档添加查询中标记的 tf_idf 值。例如，对于查询“hello world ”,我们需要检查每个文档中是否存在这些单词，如果存在，那么 tf_idf 值将被添加到特定 doc_id 的匹配分数中。最后，我们将排序并取前 k 个文档。</p><p id="75ce" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">上面提到的是理论概念，但由于我们使用字典来保存数据集，我们要做的是迭代字典中的所有值，并检查该值是否存在于令牌中。因为我们的字典是一个(document，token)键，所以当我们在查询中找到一个 token 时，我们会将文档 id 和 tf-idf 值一起添加到另一个字典中。最后，我们将只取前 k 个文档。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="e5ef" class="mt jo iq nz b gy od oe l of og">def matching_score(query):<br/>    query_weights = {}<br/>    for key in tf_idf: <br/>        if key[1] in tokens:<br/>            query_weights[key[0]] += tf_idf[key]</span></pre><p id="a740" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">密钥[0]是文档 id，密钥[1]是令牌。</p><h1 id="78f1" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">步骤 5:使用余弦相似度排序</h1><p id="5a4b" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">当我们有一个完美工作的<strong class="kn ir">匹配分数</strong>时，为什么我们还需要余弦相似度？虽然<strong class="kn ir">匹配分数</strong>给出了相关的文档，但是当我们给出长的查询时，它很失败，它将不能正确地对它们进行排序。余弦类似地做的是，它将所有文档标记为 tf-idf 令牌的向量，并测量余弦空间中的相似性(向量之间的角度。少数情况下，查询长度会很短，但它可能与文档密切相关。在这种情况下，余弦相似性是找到相关性的最佳方法。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5b0372cb240164221fb8f7bdb56df51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*OGD_U_lnYFDdlQRXuOZ9vQ.png"/></div></figure><p id="1363" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">观察上面的图，蓝色向量是文档，红色向量是查询，我们可以清楚地看到，尽管文档 d1 的曼哈顿距离(绿线)非常高，但查询仍然接近文档 d1。在这种情况下，余弦相似度会更好，因为它考虑了这两个向量之间的角度。但是匹配分数将返回文档 d3，但是这不是非常密切相关的。</p><blockquote class="oi oj ok"><p id="efff" class="lz ma ol kn b ko mo mb mc kq mp md me om mq mg mh on mr mj mk oo ms mm mn ky ij bi translated">匹配分数计算曼哈顿距离(从尖端开始的直线)<br/>余弦分数考虑向量的角度。</p></blockquote><h2 id="743a" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">…向量化…</h2><p id="1951" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">要计算上述任何一项，最简单的方法是将所有内容转换为矢量，然后计算余弦相似度。因此，让我们将查询和文档转换成向量。我们将使用 total_vocab 变量来为每个标记生成一个索引，该变量包含所有唯一标记的列表，我们将使用 numpy of shape (docs，total_vocab)来存储文档向量。</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="1206" class="mt jo iq nz b gy od oe l of og"># Document Vectorization<br/>D = np.zeros((N, total_vocab_size))<br/>for i in tf_idf:<br/>    ind = total_vocab.index(i[1])<br/>    D[i[0]][ind] = tf_idf[i]</span></pre><p id="3b90" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">对于 vector，我们需要计算 TF-IDF 值，我们可以从查询本身计算 TF，并且我们可以利用我们为文档频率创建的 DF。最后，我们将在一个(1，vocab_size) numpy 数组中存储 tf-idf 值，令牌的索引将从 total_voab 列表中决定</p><pre class="lj lk ll lm gt ny nz oa ob aw oc bi"><span id="a05c" class="mt jo iq nz b gy od oe l of og">Q = np.zeros((len(total_vocab)))<br/>counter = Counter(tokens)<br/>words_count = len(tokens)<br/>query_weights = {}<br/>for token in np.unique(tokens):<br/>    tf = counter[token]/words_count<br/>    df = doc_freq(token)<br/>    idf = math.log((N+1)/(df+1))</span></pre><p id="d01f" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">现在，我们要做的就是计算所有文档的余弦相似度，并返回最大的 k 个文档。余弦相似度定义如下。</p><blockquote class="nf"><p id="622f" class="ng nh iq bd ni nj nk nl nm nn no ky dk translated">np.dot(a，b)/(norm(a)*norm(b))</p></blockquote><h1 id="a8ee" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy nu ka kb kc nv ke kf kg nw ki kj kk bi translated">分析</h1><p id="7357" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">我从 doc_id <strong class="kn ir"> 200 </strong>(对我来说)中取了文本，在匹配得分和余弦相似度两方面粘贴了一些长查询和短查询的内容。</p><h2 id="68d3" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">简短查询</h2><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oy"><img src="../Images/d5940c58e464351bfd1ec76d16764797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BXE8ZNAswuZOzaFAq-364w.png"/></div></div></figure><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oz"><img src="../Images/aa74851ce9ce1e247ac4bed429650ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJq-umHQSMp9byAM-QeGFg.png"/></div></div></figure><h2 id="a1c6" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">长查询</h2><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi pa"><img src="../Images/6949e4302cbe0bd3be203440a524d6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vV5IQY96PJnXvr594yFQzA.png"/></div></div></figure><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi pb"><img src="../Images/f217b15397c15b204b2a176267e99cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JLbgOAL7nfweeEBcCUvaqA.png"/></div></div></figure><p id="c48c" class="pw-post-body-paragraph lz ma iq kn b ko mo mb mc kq mp md me ks mq mg mh ku mr mj mk kw ms mm mn ky ij bi translated">正如我们可以从上面的文档中看到的，余弦方法总是比匹配方法得到更高的评价，这是因为余弦相似性学习了更多的上下文。</p><h2 id="b2c0" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">关于我:</h2><p id="671a" class="pw-post-body-paragraph lz ma iq kn b ko kp mb mc kq kr md me ks mf mg mh ku mi mj mk kw ml mm mn ky ij bi translated">我是 NLP 和 DL 领域的高级数据科学家和 AI 研究员。<br/>愿意联系:<a class="ae ly" href="https://twitter.com/PWilliamScott" rel="noopener ugc nofollow" target="_blank"> Twitter </a>，<a class="ae ly" href="https://www.linkedin.com/in/williamscottp/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</p><h2 id="030d" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">自己试试。<a class="ae ly" href="https://github.com/williamscott701/Information-Retrieval/blob/master/2.%20TF-IDF%20Ranking%20-%20Cosine%20Similarity%2C%20Matching%20Score/TF-IDF.ipynb" rel="noopener ugc nofollow" target="_blank">点击此处</a>进行 git 回购。</h2><h2 id="a0ad" class="mt jo iq bd jp mu mv dn jt mw mx dp jx ks my mz kb ku na nb kf kw nc nd kj ne bi translated">使用的库</h2><ul class=""><li id="675c" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">nltk，numpy，re，mat，num2words</li></ul><h1 id="979e" class="jn jo iq bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">信息检索系列:</h1><ul class=""><li id="e57d" class="kl km iq kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><a class="ae ly" href="https://medium.com/@williamscott701/introduction-to-information-retrieval-series-436082826197" rel="noopener"> 1。简介</a></li><li id="9d9e" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated"><a class="ae ly" href="https://medium.com/@williamscott701/information-retrieval-unigram-postings-and-positional-postings-a28b907c4e8" rel="noopener"> 2。单字索引&amp;位置索引</a></li><li id="6370" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">3.TF-IDF</li><li id="9430" class="kl km iq kn b ko ld kq le ks lf ku lg kw lh ky kz la lb lc bi translated">更多即将推出…</li></ul></div></div>    
</body>
</html>