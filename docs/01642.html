<html>
<head>
<title>Dataset deduplication using spark’s MLlib</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 spark 的 MLlib 进行数据集重复数据删除</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deduplication-using-sparks-mllib-4a08f65e5ab9?source=collection_archive---------10-----------------------#2019-03-17">https://towardsdatascience.com/deduplication-using-sparks-mllib-4a08f65e5ab9?source=collection_archive---------10-----------------------#2019-03-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="7267" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于拥有大量数据的公司来说，重复数据删除过程始终非常重要。首先，重复数据删除最大限度地减少了存储业务数据所需的空间，并将为我们的管道带来更低的基础设施成本和更好的性能。另一方面，通过持续集成和持续交付<strong class="jp ir"> (CI/CD) </strong>，减少重复的数量将降低管道的复杂性，并增加业务时间。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/a7725d77bdecc27fda84945cf513dfd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1vvpEyp4MmWCImZZlVexsw.jpeg"/></div></figure><p id="11a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有时，重复数据删除过程由简单的文本到文本匹配组成，您可以简单地选择<strong class="jp ir"> CRC32 校验和</strong>或<strong class="jp ir"> MD5 </strong>匹配。然而，在一些情况下，数据集的行仅仅因为一些列上的一些小的文本差异而不同，即使它们表示相同的实体<strong class="jp ir">。</strong>因此，<strong class="jp ir"> </strong>本文展示了一个<strong class="jp ir">实体识别和链接过程</strong>使用了两种不同的 spark 方法，对报废电子商务网站收集的特定产品数据集进行处理。</p><p id="7548" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面描述完整代码和过程可以在这里找到:</p><div class="kt ku gp gr kv kw"><a href="https://github.com/ronald-smith-angel/dataset_deduplication_sparkml" rel="noopener  ugc nofollow" target="_blank"><div class="kx ab fo"><div class="ky ab kz cl cj la"><h2 class="bd ir gy z fp lb fr fs lc fu fw ip bi translated">Ronald-Smith-angel/dataset _ 重复数据删除 _sparkml</h2><div class="ld l"><h3 class="bd b gy z fp lb fr fs lc fu fw dk translated">使用 spark ML 库和 Scala-Ronald-Smith-angel/dataset _ de duplication _ spark ML 进行数据集重复数据删除</h3></div><div class="le l"><p class="bd b dl z fp lb fr fs lc fu fw dk translated">github.com</p></div></div><div class="lf l"><div class="lg l lh li lj lf lk kr kw"/></div></div></a></div><p id="f468" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一般的过程可以在这个特质上找到(<strong class="jp ir"> …是的，我用 scala 做数据科学！！！</strong>):</p><pre class="km kn ko kp gt ll lm ln lo aw lp bi"><span id="3e7d" class="lq lr iq lm b gy ls lt l lu lv">package com.sample.utils<br/><br/>import org.apache.spark.sql.DataFrame<br/><br/>trait OperationsHelper {<br/>  def ds: DataFrame<br/><br/>  def preparedDataSet()(df: DataFrame): DataFrame<br/><br/>  def deduplicateDataSet()(df: DataFrame): DataFrame<br/><br/>  def resultsDataFrame()(df: DataFrame): DataFrame<br/>}</span></pre><p id="9876" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">正如你将看到的，这个助手背后的想法将是有一个函数管道，从这里可以轻松地调用链数据帧<a class="ae lw" href="https://medium.com/@mrpowers/chaining-custom-dataframe-transformations-in-spark-a39e315f903c" rel="noopener"> <strong class="jp ir">转换</strong> </a> <strong class="jp ir"> </strong>。</p><h2 id="d56e" class="lq lr iq bd lx ly lz dn ma mb mc dp md jy me mf mg kc mh mi mj kg mk ml mm mn bi translated">预处理产品数据</h2><p id="fa2b" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">数据科学界广泛使用降维技术来获得更小的特征集，以便在训练和评估模型时进行分析并获得更好的性能。PCA 方法允许降维，同时保留那些描述大量信息的特征。因此<strong class="jp ir">，</strong>该预处理阶段遵循以下步骤:</p><ul class=""><li id="aa09" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated"><strong class="jp ir"> <em class="nc">数据清洗:</em> </strong>清洗数据要有一个通用的刻度。对于产品的情况，数据集由一个简单的文本清理组成，包括大小写、空白、编码和符号。</li><li id="9bf4" class="mt mu iq jp b jq nd ju ne jy nf kc ng kg nh kk my mz na nb bi translated"><strong class="jp ir">特征选择:</strong>使用 PCA 技术选择一组特征。<em class="nc">(“标题块”、“内容块”、“颜色”、“产品类型”)</em></li></ul><p id="6519" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上述特性中的内容包含候选重复产品的大部分差异。</p><h2 id="0413" class="lq lr iq bd lx ly lz dn ma mb mc dp md jy me mf mg kc mh mi mj kg mk ml mm mn bi translated">1 —方法 A:区分位置的散列法(LSH)</h2><p id="7a7e" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">位置敏感散列是一种用于实体解析的技术，然后将找到代表相同实体的记录。spark MLlib 有一个自定义的<a class="ae lw" href="https://spark.apache.org/docs/2.1.0/ml-features.html#locality-sensitive-hashing" rel="noopener ugc nofollow" target="_blank"> LSH 实现</a>，这里使用它来查找如下重复项:</p><ul class=""><li id="363c" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">首先，使用所选特征的串联来生成散列(上面的 PC)。对于真实世界的例子，可以生成每个特征的散列。但是，对于这个示例，为了更快地获得结果，使用了一个简单的串联列。</li><li id="8eea" class="mt mu iq jp b jq nd ju ne jy nf kc ng kg nh kk my mz na nb bi translated">然后，该列用于生成 LSH 向量，如下所示:</li></ul><p id="7769" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">—<strong class="jp ir">分词器</strong>使用<a class="ae lw" href="https://kb.yoast.com/kb/list-stop-words/" rel="noopener ugc nofollow" target="_blank"> <strong class="jp ir">单词停止器</strong> </a>为记录生成单词列表。</p><p id="e490" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">—<strong class="jp ir">计数矢量模型</strong>为 LSH 算法创建带有哈希和桶(类似哈希)的矢量。</p><pre class="km kn ko kp gt ll lm ln lo aw lp bi"><span id="075f" class="lq lr iq lm b gy ls lt l lu lv"><strong class="lm ir">val </strong>pccTokenizer = <strong class="lm ir">new </strong>Tokenizer()<br/>  .setInputCol(OperationsHelperLSH.<em class="nc">ConcatComments</em>)<br/>  .setOutputCol(OperationsHelperLSH.<em class="nc">ColumnWordsArray</em>)<br/><strong class="lm ir">val </strong>wordsArrayDF = pccTokenizer.transform(df)<br/><br/><strong class="lm ir">val </strong>remover = <strong class="lm ir">new </strong>StopWordsRemover()<br/>  .setCaseSensitive(<strong class="lm ir">false</strong>)<br/>  .setStopWords(OperationsHelperLSH.<em class="nc">stopWords</em>)<br/>  .setInputCol(OperationsHelperLSH.<em class="nc">ColumnWordsArray</em>)<br/>  .setOutputCol(OperationsHelperLSH.<em class="nc">ColumnFilteredWordsArray</em>)<br/><br/><strong class="lm ir">val </strong>wordsFiltered = remover.transform(wordsArrayDF)<br/><br/><strong class="lm ir">val </strong>validateEmptyVector = <em class="nc">udf</em>({ v: Vector =&gt; v.numNonzeros &gt; 0 }, DataTypes.<em class="nc">BooleanType</em>)</span><span id="6315" class="lq lr iq lm b gy ni lt l lu lv"><strong class="lm ir">val </strong>vectorModeler: CountVectorizerModel = <strong class="lm ir">new </strong>CountVectorizer()<br/>  .setInputCol(OperationsHelperLSH.<em class="nc">ColumnFilteredWordsArray</em>)<br/>  .setOutputCol(OperationsHelperLSH.<em class="nc">ColumnFeaturesArray</em>)<br/>  .setVocabSize(<em class="nc">VocabularySHLSize</em>)<br/>  .setMinDF(10)<br/>  .fit(wordsFiltered)<br/><br/><strong class="lm ir">val </strong>vectorizedProductsDF = vectorModeler.transform(wordsFiltered)<br/>  .filter(validateEmptyVector(<em class="nc">col</em>(OperationsHelperLSH.<em class="nc">ColumnFeaturesArray</em>)))<br/>  .select(<em class="nc">col</em>(OperationsHelperWindowStrategy.<em class="nc">ConcatComments</em>),<br/>    <em class="nc">col</em>(OperationsHelperLSH.<em class="nc">ColumnUniqueId</em>),<br/>    <em class="nc">col</em>(OperationsHelperLSH.<em class="nc">ColumnFilteredWordsArray</em>),<br/>    <em class="nc">col</em>(OperationsHelperLSH.<em class="nc">ColumnFeaturesArray</em>))<br/><br/>(vectorizedProductsDF, vectorModeler)</span></pre><p id="0519" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">class:<strong class="jp ir"><em class="nc">com . sample . products . operations helper LSH . Scala</em></strong></p><ul class=""><li id="cb17" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">为了完成训练步骤，使用 MinHashLSHModel 来训练产品数据，生成类似产品的最终桶。</li><li id="d0af" class="mt mu iq jp b jq nd ju ne jy nf kc ng kg nh kk my mz na nb bi translated">最后，使用 KNN 相似的哈希可以找到一个类别。</li></ul><pre class="km kn ko kp gt ll lm ln lo aw lp bi"><span id="3d19" class="lq lr iq lm b gy ls lt l lu lv"><em class="nc">  /**<br/>    * Uses the dataset to train the model.<br/>    *<br/>    */<br/>  </em><strong class="lm ir">def </strong>deduplicateDataSet(df: DataFrame): (DataFrame, MinHashLSHModel) = {<br/><br/>    <strong class="lm ir">val </strong>minLshConfig = <strong class="lm ir">new </strong>MinHashLSH().setNumHashTables(hashesNumber)<br/>      .setInputCol(OperationsHelperLSH.<em class="nc">ColumnFeaturesArray</em>)<br/>      .setOutputCol(OperationsHelperLSH.<em class="nc">hashValuesColumn</em>)<br/><br/>    <strong class="lm ir">val </strong>lshModel = minLshConfig.fit(df)<br/><br/>    (lshModel.transform(df), lshModel)<br/>  }<br/><br/><br/>  <em class="nc">/**<br/>    * Applies KNN to find similar records.<br/>    *</em><strong class="lm ir"><em class="nc"><br/>    </em></strong><em class="nc">*/<br/>  </em><strong class="lm ir">def </strong>filterResults(df: DataFrame,<br/>                    vectorModeler: CountVectorizerModel,<br/>                    lshModel: MinHashLSHModel,<br/>                    categoryQuery: (String, String)<br/>                   ): DataFrame = {<br/>    <strong class="lm ir">val </strong>key = Vectors.<em class="nc">sparse</em>(<em class="nc">VocabularySHLSize</em>,<br/>      <em class="nc">Seq</em>((vectorModeler.vocabulary.indexOf(categoryQuery._1), 1.0),<br/>        (vectorModeler.vocabulary.indexOf(categoryQuery._2), 1.0)))<br/>    <br/><strong class="lm ir">lshModel</strong>.approxNearestNeighbors(df, key, nearNeighboursNumber).toDF()</span><span id="a692" class="lq lr iq lm b gy ni lt l lu lv">}<br/></span></pre><p id="25a9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行一个例子:转到测试<code class="fe nj nk nl lm b">com.sample.processor.products.ProcessorProductsLshTest</code>，你会看到一个完整的流程在运行。</p><p id="363d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输入参数:</strong></p><p id="49e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nc">类别</em> → color = 'negro '和<em class="nc">产品类型</em> = 'tdi '。</p><p id="98e2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nc">近邻</em></p><p id="d8f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><em class="nc">散列数</em> → 3 <strong class="jp ir"> <em class="nc">(散列越多精度越高，但计算开销越大)。</em> </strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nm"><img src="../Images/60b196cf167c85399e384f245914870f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qotx2xy4kEiGqi3Aumqb4g.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk">3 products with almost the same text for selected features.</figcaption></figure><h2 id="3cb3" class="lq lr iq bd lx ly lz dn ma mb mc dp md jy me mf mg kc mh mi mj kg mk ml mm mn bi translated"><strong class="ak">结果分析:</strong></h2><p id="6467" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated"><strong class="jp ir">优点:</strong></p><ul class=""><li id="2e43" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated"><em class="nc">准确:</em>如果使用了一组完整的字段(表示字符串)，正确的哈希值和邻居值可以检测到几乎所有的重复值。</li><li id="c6b4" class="mt mu iq jp b jq nd ju ne jy nf kc ng kg nh kk my mz na nb bi translated">更快:与其他 ML 策略如<strong class="jp ir">项频反</strong>等相比。</li></ul><p id="435a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">缺点:</strong></p><ul class=""><li id="ed1d" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">需要一个资源好的集群。</li><li id="94e4" class="mt mu iq jp b jq nd ju ne jy nf kc ng kg nh kk my mz na nb bi translated">需要一个数据清理的过程。</li></ul><h2 id="a113" class="lq lr iq bd lx ly lz dn ma mb mc dp md jy me mf mg kc mh mi mj kg mk ml mm mn bi translated">2 —方法 B:使用 Levenshtein +火花窗口的模糊匹配:</h2><p id="8538" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated"><a class="ae lw" href="https://medium.com/@mrpowers/fuzzy-matching-in-spark-with-soundex-and-levenshtein-distance-6749f5af8f28" rel="noopener"> Levenshtein </a>是一种用于字符串模糊匹配的算法。基本上，这个方法测量两个字符串之间的差异。此外，spark 窗口函数以简洁的方式允许数据集分析功能，避免了多个<em class="nc"> groupBy </em>和<em class="nc"> Join </em>操作。因此，该方法定义了一个 2 级窗口来分组相似的数据，然后将 Levenshtein 应用于相同窗口中的值以发现重复项。该过程描述如下:</p><ul class=""><li id="e954" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">首先选择一组描述为<strong class="jp ir">非模糊</strong>的记录<strong class="jp ir">。</strong>该列表包含代表<strong class="jp ir">类别</strong>的<strong class="jp ir"> </strong>列<strong class="jp ir"> </strong>，并且在 PCA 过程中大多数时候是没有错误的:<em class="nc">(“产品类型”、“城市”、“国家”、“地区”、“年份”)。</em></li></ul><p id="9770" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此窗口表示用于分析的常规窗口哈希。</p><ul class=""><li id="01b7" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">其次，应用第二个窗口来发现非常相似的记录。该列表表示既不是<strong class="jp ir">模糊</strong>列表(PCA) <strong class="jp ir">中的</strong>零件，也不是<strong class="jp ir">非模糊</strong>列表中的零件的<strong class="jp ir">记录:<em class="nc">(“门”、“燃料”、“制造”、“里程”、“型号”、“颜色”、“价格”)</em></strong></li></ul><p id="f202" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">注意:</strong>“日期”字段有助于仅订购和获取最新的。</p><ul class=""><li id="3467" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">然后，对于每个组，将<a class="ae lw" href="https://medium.com/@mrpowers/fuzzy-matching-in-spark-with-soundex-and-levenshtein-distance-6749f5af8f28" rel="noopener"> <strong class="jp ir"> levenshtein </strong> </a>(仅在第二个窗口中的字符串差异)应用于来自 PCA 结果的串接的最模糊字段:<strong class="jp ir"> <em class="nc">(“标题块”、“内容块”)。</em> </strong></li></ul><p id="1502" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如您所见，使用了列的<strong class="jp ir"> MD5 </strong>表示，而不是每个字符串，以获得更好的性能:</p><p id="0b4f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> keyhash: </strong> MD5 为类别列集合。下图显示了同一类别的许多产品。</p><p id="7711" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> hashDiff </strong>:表示<em class="nc">非模糊</em>集合的 MD5 hash。下图显示了属于同一类别但具有<strong class="jp ir">不同</strong>描述(&gt; levenshteinThreshold)的产品，以及具有相同<em class="nc"> hashDiff </em> <strong class="jp ir">且 levenshtein(&lt;levenshteinThreshold)差异较小的产品。</strong></p><ul class=""><li id="ac9e" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">最后，hashes(两者)和 rank 相同的值只改变<strong class="jp ir"> <em class="nc"> row_num </em> </strong>。过滤<strong class="jp ir"> <em class="nc"> row_num == 1 </em> </strong>有可能得到去重数据集。</li></ul><pre class="km kn ko kp gt ll lm ln lo aw lp bi"><span id="e11c" class="lq lr iq lm b gy ls lt l lu lv"><em class="nc">/**<br/>  * Applies windows functions and Levenshtein to group similar  categories.<br/>  *</em><strong class="lm ir"><em class="nc"><br/>  </em></strong><em class="nc">*/<br/></em><strong class="lm ir">override def </strong>deduplicateDataSet()(df: DataFrame): DataFrame = {<br/>  df<br/>    .withColumn(OperationsHelperWindowStrategy.<em class="nc">ColumnRank</em>, <em class="nc">dense_rank</em>().over(<em class="nc">windowProductKeyHash</em>))<br/>    .withColumn(OperationsHelperWindowStrategy.<em class="nc">ColumnHashWithDiff</em>,<br/>      <em class="nc">concat</em>(<em class="nc">col</em>(OperationsHelperWindowStrategy.<em class="nc">ColumnCategoryFieldsHash</em>),<br/>        <em class="nc">when</em>(<em class="nc">levenshtein</em>(<br/>          <em class="nc">first</em>(OperationsHelperWindowStrategy.<em class="nc">ConcatComments</em>).over(<em class="nc">windowProductsCategoryRank</em>),<br/>          <em class="nc">col</em>(OperationsHelperWindowStrategy.<em class="nc">ConcatComments</em>)) &gt;= levenshteinThreshold, <em class="nc">lit</em>("1"))<br/>          .otherwise(<em class="nc">lit</em>(""))))<br/>    .withColumn(OperationsHelperWindowStrategy.<em class="nc">ColumnRowNum</em>, <em class="nc">row_number</em>().over(<em class="nc">windowProductsCategoryRank</em>))<br/>}</span></pre><p id="881e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">class:<strong class="jp ir"><em class="nc">com . sample . products . operationshelperwindowstrategy . Scala</em></strong></p><p id="8c4e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">运行一个例子:转到测试<code class="fe nj nk nl lm b">com.sample.processor.products.ProcessorProductsWindowsTest</code>，你会看到一个完整的流程在运行。</p><p id="85e4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">输入参数:</strong> levenshteinThreshold → 6</p><p id="f64b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结果:</strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="nn no di np bf nq"><div class="gh gi nv"><img src="../Images/4b0dac276416e31eccb13d586b22c407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rb5afiU6CVxMDmAijDb4SQ.png"/></div></div><figcaption class="nr ns gj gh gi nt nu bd b be z dk">2 groups example with almost exact values.</figcaption></figure><p id="435c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">过滤 rn == 1 后，会对结果进行重复数据删除。这将删除样本数据集中的<strong class="jp ir"> &gt; 1/3 </strong>。</p><h2 id="536a" class="lq lr iq bd lx ly lz dn ma mb mc dp md jy me mf mg kc mh mi mj kg mk ml mm mn bi translated">结果分析:</h2><p id="79db" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated"><strong class="jp ir">优点:</strong></p><ul class=""><li id="d780" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">更多的控制在火花分割和功能。</li></ul><p id="594b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">缺点:</strong></p><ul class=""><li id="d1eb" class="mt mu iq jp b jq jr ju jv jy mv kc mw kg mx kk my mz na nb bi translated">可能会有更多的假阳性。</li></ul><h1 id="11d3" class="nw lr iq bd lx nx ny nz ma oa ob oc md od oe of mg og oh oi mj oj ok ol mm om bi translated"><strong class="ak">最终结论</strong></h1><p id="42c5" class="pw-post-body-paragraph jn jo iq jp b jq mo js jt ju mp jw jx jy mq ka kb kc mr ke kf kg ms ki kj kk ij bi translated">重复数据删除流程始终取决于公司需求和要分析的数据量。本文描述了两种不同的策略。因此，带有窗口函数的 Levenshtein 对于小维数问题已经足够好了；否则，LSH 永远是最好的选择</p></div></div>    
</body>
</html>