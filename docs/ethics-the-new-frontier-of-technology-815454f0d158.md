# 伦理学，技术的新前沿

> 原文：<https://towardsdatascience.com/ethics-the-new-frontier-of-technology-815454f0d158?source=collection_archive---------25----------------------->

## 内部人工智能

## [可信人工智能之路](http://go.sas.com/9bnhdb)

# 我们在创造怪物吗？

随着人工智能(AI)和机器学习(ML)应用融入我们生活的越来越多的方面，越来越多的声音表达了对道德影响的担忧，算法偏见(“[算法，中立的幻觉](/algorithms-the-illusion-of-neutrality-8438f9ca8471?source=friends_link&sk=7a6c8948fb22dd92db75218baf6e4ac6)”)和黑盒模型(“ [X-AI，黑盒和水晶球](/x-ai-black-boxes-and-crystal-balls-fd27a00752ec?source=friends_link&sk=134d1a420e0368e64577b3b7a6ae89db)”)缺乏透明度和可解释性的担忧。

我们正在建造超出我们智力理解能力的系统。谁能认真地假装他们理解自动驾驶汽车使用的上亿行代码？

人工智能正在朝着更加自主和类似人类的认知活动快速发展，如自然语言处理和计算机视觉。算法需要越来越少的监督才能发挥作用。在某些情况下，他们甚至开始重写自己的代码。这些“通用算法”会进化，就像生物体自然进化一样。难怪一些学术研究实验室现在正在寻找理解算法的方法，把它们当作野生动物看待，观察它们在世界上的行为。

这是否意味着我们正在创造怪物？

![](img/b5380ef2758f3ac74f2aa975f8bdd367.png)

Photo by [freestocks](https://unsplash.com/@freestocks?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

弗兰肯斯坦的生物逃离我们控制的神话，以及对机器人杀手消灭人类的恐惧，都在助长炒作。在这个话题上，不乏制造恐慌和试图吸引我们注意力的媒体文章。然而，我认为现实完全不同。正如过去的许多技术突破一样，技术进步比社会进步更快。随着意识的建立，实践将会改变，行为准则、最佳实践和安全措施将会发展，人工智能的使用将会变得更加(自我)规范。

算法没有伦理、道德、价值观或意识形态，但人有。关于人工智能伦理的问题是关于制造和使用人工智能的人的伦理问题。

# 承担责任

事实上，[组织已经在为更加道德和负责任地使用人工智能做准备。由福布斯 Insights 在全球 300 多名 C 级高管中进行的一项由 SAS、埃森哲和英特尔最近进行的研究明确确立了可信人工智能的重要性。文化挑战，特别是缺乏信任，被认为是阻止更广泛和更快采用人工智能的主要障碍。](https://www.sas.com/en_us/news/press-releases/2018/september/artificial-intelligence-survey-ax-san-diego.html)

大多数组织似乎正在采取行动，实施治理和监督系统来监控 AI 应用程序的输出。例如，超过一半的人至少每周审查一次人工智能系统的输出。在最成功和最成熟的组织中，这个数字上升到了 74%。

![](img/81d470a9165ff08fe45515a9e3b5daa7.png)

[Organizations are gearing up for more ethical and responsible use of AI](https://www.sas.com/en_us/news/press-releases/2018/september/artificial-intelligence-survey-ax-san-diego.html)

伦理问题现在是创新者、技术专家和商业领袖最关心的问题。许多公司引入了伦理委员会来审查人工智能的使用，并为其员工提供伦理培训。

一些工程师有理由担心他们正在开发的技术的可能用途。最近，来自非营利人工智能组织 OpenAI 的研究人员创建了一个文本生成系统，可以对提示做出长达一页的响应，模仿从幻想散文到虚假名人新闻故事和家庭作业的一切。OpenAI 通常向公众发布其项目。然而，在这种情况下，由于担心可能的恶意使用，它决定不公开这项技术。

![](img/a4b19f162787007e41f7d13fea233727.png)

Photo by [Lianhao Qu](https://unsplash.com/@lianhao?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

业界已经出台了许多举措和框架来解决这个问题。例如，F.A.T.E .(公平、问责、透明、可解释)社区创建了一套原则，以帮助组织在使用人工智能时引入道德规范，应对偏见的挑战，并一致地解释模型结果。像谷歌这样的大玩家已经决定公开宣传他们使用人工智能的道德原则。2018 年 5 月，谷歌首席执行官桑德尔·皮帅宣布了一系列原则，包括不在与武器或监控相关的应用中使用人工智能，并建立“对社会有益”的人工智能应用，避免产生或加强偏见，并对人们负责。为更大利益而工作的值得称赞的努力也非常适合一家在人工智能领域投入大量资金的公司的公关活动。但趋势很明显，不容忽视。

# 数据科学应该是一个受监管的职业吗？

今天，数据科学家通常专注于数据分析和算法的基础科学。他们喜欢尝试新的语言和新的技术，尝试新事物，并且“处于边缘”。但是让人工智能走出实验室进入现实世界需要的不仅仅是科学。数学实际上是容易的部分。其他人也需要参与进来:

*   **商业领袖**必须设定愿景，并用商业术语定义期望的结果。他们还必须参与探索数据和见解的迭代过程，在此过程中细化业务目标。
*   **领域专家**理解数据，能够指导数据科学家做出正确的假设，选择正确的数据集，提出正确的问题，并解释结果。
*   **IT 团队**必须提供一个可控的环境来管理数据科学家开发的模型。他们有责任获取模型并将它们嵌入到应用程序中以供使用。根据业务目标和技术约束，模型必须部署在需要的地方，例如，云中、数据库中、边缘或数据流中。这个过程需要工程和数据科学团队之间的紧密协作。

所有这些人都有责任确保人工智能应用程序是值得信赖和符合道德的。然而，当每个人都有责任时，也许没有人负责。我认为，数据科学家也许应该承担更广泛的责任，监督从数据到发现和部署的端到端分析生命周期。在与工程团队、领域专家和商业领袖的合作中，他们应该充当良好实践的守护者吗？

这就引出了一个问题:数据科学是否应该成为一个受监管的职业。我们是否应该建立一套职业行为准则，对数据科学家进行严格评估，并规定责任和义务？

![](img/49d7c0157731042e140c2c44f2cd70c4.png)

Photo by [MD Duran](https://unsplash.com/@mdesign85?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

如果我们考虑到当前数据科学家人才的短缺，为人工智能的道德发展和使用建立标准甚至更为关键。许多新的数据科学家正在进入市场，但缺乏处理工作道德方面的意识和经验。当然，[这种短缺不会通过 40 美元的在线课程](https://thenextweb.com/podium/2019/04/07/we-cant-fix-the-data-scientist-talent-shortage-with-40-online-courses/amp/)来解决，这个角色完全职业化还需要时间。

# 人工智能监管环境的演变

然而，也有一种观点认为，这种监护应该是一个新的 C 级角色的责任。正如欧盟在某些情况下要求设立数据保护官(DPO)一样，我不会对未来几年出现新的角色感到惊讶，或者对现有角色(如首席分析官)承担新的法律责任。这将为人工智能技术的开发和使用提供必要的监督和问责。

人工智能显然已经提上了商业领袖和政策制定者的议事日程。许多政府和私人组织已经开始发布指导方针和最佳实践，为人工智能的使用提供保障。这需要一些时间，但适当的监管将随之而来，与欧盟 GDPR 非常相似。

GDPR 已经建立了强有力的规则来保护个人数据和个人数据隐私的基本权利。有很多关于 GDPR 是否包括解释权的讨论。根据 GDPR，数据主体有权:

*   了解自动决策；
*   质疑自动化系统的输出；和
*   如果决策对数据和逻辑有重大影响，则获取关于数据和逻辑的有意义的信息

这只是一个开始！更多的是在布鲁塞尔做饭…

![](img/cc82a2c3e3b7346191400b4826726eb0.png)

Photo by [Guillaume Périgois](https://unsplash.com/@guillaumeperigois?utm_source=medium&utm_medium=referral) on [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral)

欧盟委员会已经成立了欧洲人工智能高级小组，由来自学术界、民间社会和工业界的代表组成。这个多学科小组的目标是:

*   作为数字单一市场的一部分，为人工智能开发一个[全欧洲战略，通过协调投资、规则和方法的一致性来提高欧洲在世界舞台上的竞争力；和](https://ec.europa.eu/digital-single-market/en/artificial-intelligence)
*   为可信和道德的人工智能制定[指南和最佳实践](https://ec.europa.eu/digital-single-market/en/news/draft-ethics-guidelines-trustworthy-ai)，这可能会导致另一个类似于 GDPR 的法规。

因此，法律框架正忙于赶上人工智能应用的创新和新实践。然而，越来越明显的是，生产值得信赖和负责任的人工智能的能力是交付道德和商业价值的必要条件。一枚硬币的两面！

更多信息，请阅读我关于这个主题的另外两篇博客:

*   [算法，中立的假象](/algorithms-the-illusion-of-neutrality-8438f9ca8471?source=friends_link&sk=7a6c8948fb22dd92db75218baf6e4ac6)
*   [X-AI，水晶球和黑盒](/x-ai-black-boxes-and-crystal-balls-fd27a00752ec?source=friends_link&sk=134d1a420e0368e64577b3b7a6ae89db)