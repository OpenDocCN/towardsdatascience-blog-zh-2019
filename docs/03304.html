<html>
<head>
<title>Get Started With TensorFlow 2.0 and Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">开始使用 TensorFlow 2.0 和线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-started-with-tensorflow-2-0-and-linear-regression-29b5dbd65977?source=collection_archive---------3-----------------------#2019-05-27">https://towardsdatascience.com/get-started-with-tensorflow-2-0-and-linear-regression-29b5dbd65977?source=collection_archive---------3-----------------------#2019-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="870a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">🤖<a class="ae ep" href="https://equipintelligence.medium.com/list/deep-learning-techniques-methods-and-how-tos-01015cf5f917" rel="noopener">深度学习</a></h2><div class=""/><div class=""><h2 id="62d0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用新的 TF 2.0 APIs 的线性回归模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/1b2232b285aa9fd173368f073337f804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YSx9-ZV70HySyA7j"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Photo by <a class="ae le" href="https://unsplash.com/@joshuaearle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Joshua Earle</a> on <a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5fe6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">TensorFlow 2.0 是<a class="mb mc ep" href="https://medium.com/u/b1d410cb9700?source=post_page-----29b5dbd65977--------------------------------" rel="noopener" target="_blank"> TensorFlow </a>家族的重大突破。这是全新的和翻新的，也<em class="md">不那么令人毛骨悚然</em>！我们将在<a class="ae le" href="https://www.tensorflow.org/alpha" rel="noopener ugc nofollow" target="_blank"> TensorFlow 2.0 </a>中创建一个简单的<a class="ae le" href="https://ml-cheatsheet.readthedocs.io/en/latest/linear_regression.html" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型来探索一些新的变化。所以，打开你的代码编辑器，让我们开始吧！</p><p id="3572" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，打开<a class="ae le" href="https://colab.research.google.com/drive/1HL1HDCDhh1FiV-NckjfTYPd50K7H7wzi#scrollTo=uyKcSFJrbhRf&amp;forceEdit=true&amp;offline=true&amp;sandboxMode=true" rel="noopener ugc nofollow" target="_blank">本笔记本</a>进行互动学习体验。</p><p id="ee23" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">注意！TensorFlow 2.0 现已在稳定频道上线！</strong></p><p id="692f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要查看一些基本概念及其更简单的解释，请参见，</p><ul class=""><li id="263d" class="me mf iq lh b li lj ll lm lo mg ls mh lw mi ma mj mk ml mm bi translated"><a class="ae le" href="https://developers.google.com/machine-learning/glossary/" rel="noopener ugc nofollow" target="_blank">机器学习词汇表|谷歌</a></li><li id="f941" class="me mf iq lh b li mn ll mo lo mp ls mq lw mr ma mj mk ml mm bi translated"><a class="ae le" href="https://ml-cheatsheet.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">毫升备忘单</a></li></ul><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="7c84" class="mu mv iq bd mw mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated">让我们先从数学开始。</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nm mt l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">First impressions for Calculus</figcaption></figure><p id="2d8e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们首先会得到一些关于线性回归的信息。在线性回归中，我们倾向于为您的数据找到最佳拟合线。</p><p id="f782" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该线可在其<a class="ae le" href="https://www.khanacademy.org/math/algebra/two-var-linear-equations/slope-intercept-form/a/introduction-to-slope-intercept-form" rel="noopener ugc nofollow" target="_blank">斜率截距表</a>中定义为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5a0f126ecf4c683f940a4d10bc72326d.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*0KOryCobBFnjusemwUXrQA.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">The hypothesis function</figcaption></figure><blockquote class="no np nq"><p id="5d11" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated"><em class="iq"> m </em>和<em class="iq"> c </em>分别是斜率和 y 截距。其中<em class="iq"> W </em>和<em class="iq"> b </em>分别是 2ⁿᵈ方程的权重和偏差。</p></blockquote><p id="62e9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了优化我们的参数 w 和 b，我们需要一个损失函数。这就是<a class="ae le" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mse-l2" rel="noopener ugc nofollow" target="_blank">均方误差(L1 / MSE ) </a>出现的原因。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bb926b40329280abfa282f73da026aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*FqTjSPCb83Zc0GCOXzKDsw.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Mean Squared Error</figcaption></figure><blockquote class="no np nq"><p id="cab5" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated">其中<em class="iq"> N </em>是批次/数据集中的样本数，<em class="iq"> y </em>是预测结果，而<em class="iq"> y⁻ ( y-hat ) </em>是目标结果。</p></blockquote><p id="55de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，我们需要均方误差函数的导数，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/4564652d195ba0dc804c693886c228ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZtVJ69VV6v6iP_zPuTuL4w.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">The derivative of the Mean Squared Error function</figcaption></figure><blockquote class="no np nq"><p id="29dc" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated">其中<em class="iq"> N </em>是批次/数据集中的样本数，y 是预测结果，而<em class="iq"> y⁻ ( y-hat ) </em>是目标结果。</p></blockquote><p id="0a0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，进入<a class="ae le" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">梯度下降</a>，通过它我们将更新我们的参数θ。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5b4aca1e523b9a117528ab1dc95063ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*tNUcghMdvFu0kP8hWwSnKQ.png"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Gradient Descent update rule</figcaption></figure><blockquote class="no np nq"><p id="d323" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated">其中<em class="iq"> θ </em>是我们的参数，<em class="iq"> α </em>是学习率或步长，<em class="iq">损失</em>是我们的损失函数。</p></blockquote><p id="55cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们通过获得 w 和 b 相对于损失函数(MSE)的<a class="ae le" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives" rel="noopener ugc nofollow" target="_blank">偏导数</a>来优化 w 和 b。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/2e6ee46b5741b746ece215cc938c0d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YH2wm5kwEXr-ryJ_qx5-WQ.png"/></div></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">Partial derivatives w.r.t loss function</figcaption></figure><blockquote class="no np nq"><p id="98a6" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated">其中<em class="iq"> w </em>和<em class="iq"> b </em>是优化的参数，<em class="iq"> h </em>是假设函数，<em class="iq"> loss </em>是损失函数，<em class="iq">MSE’</em>是均方误差损失函数的导数。</p></blockquote><h1 id="597c" class="mu mv iq bd mw mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated">钻研代码。获取一些数据！</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ny mt l"/></div><figcaption class="la lb gj gh gi lc ld bd b be z dk">TensorFlow 2.0!</figcaption></figure><p id="6bba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将使用 Kaggle.com 大学研究生招生的数据。它包含 6 个连续特征和 1 个二元特征，总共有 7 个特征。标签或预期结果是学生的入学机会。这是我们的目标变量。</p><p id="c74c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将下载数据集并将其解析成我们真正喜欢的东西——训练和验证数据集！</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="dcfd" class="mu mv iq bd mw mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated">在 TF 2.0 中创建模型</h1><p id="123d" class="pw-post-body-paragraph lf lg iq lh b li nz ka lk ll oa kd ln lo ob lq lr ls oc lu lv lw od ly lz ma ij bi translated">我们使用 TensorFlow 的低级 API 定义了 3 种方法，用于:</p><ol class=""><li id="8a46" class="me mf iq lh b li lj ll lm lo mg ls mh lw mi ma oe mk ml mm bi translated"><em class="md">均方误差函数</em></li><li id="3ea1" class="me mf iq lh b li mn ll mo lo mp ls mq lw mr ma oe mk ml mm bi translated"><em class="md">均方误差函数的导数</em></li><li id="d2bd" class="me mf iq lh b li mn ll mo lo mp ls mq lw mr ma oe mk ml mm bi translated"><em class="md">假设函数/回归函数</em></li></ol><p id="9fa2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们之前在原始数学中讨论过。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><p id="5669" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，我们初始化一些用于训练的超参数，并创建<code class="fe of og oh oi b"><a class="ae le" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset" rel="noopener ugc nofollow" target="_blank">tf.data.Dataset</a></code>对象来高效地存储和批处理我们的数据。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><blockquote class="no np nq"><p id="3563" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated"><strong class="lh ja">注意到一个 TF 2.0 的变化？对于 TF 1.x 的早期版本，我们使用了<code class="fe of og oh oi b">tf.data.Dataset.make_one_shot_iterator()</code>方法来创建迭代器。这已经改变了，现在我们用<code class="fe of og oh oi b"><a class="ae le" href="https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#__iter__" rel="noopener ugc nofollow" target="_blank">tf.data.Dataset.__iter__()</a></code></strong></p></blockquote><p id="8023" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，我们用<code class="fe of og oh oi b">batch_size</code>的<a class="ae le" href="https://developers.google.com/machine-learning/glossary/#batch_size" rel="noopener ugc nofollow" target="_blank">批量</a>来训练<code class="fe of og oh oi b">num_epochs</code>时期的模型，这使得<code class="fe of og oh oi b">num_samples/batch_size</code>的<a class="ae le" href="https://developers.google.com/machine-learning/glossary/#learning_rate" rel="noopener ugc nofollow" target="_blank">步长</a>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><blockquote class="no np nq"><p id="d663" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated">变化:我们不需要通过一个<code class="fe of og oh oi b">tf.Session()</code>对象运行 ops 和 tensors。TensorFlow 2.0 默认启用<a class="ae le" href="https://www.tensorflow.org/tutorials/eager/eager_basics" rel="noopener ugc nofollow" target="_blank">急切执行</a>。为了得到一个<code class="fe of og oh oi b">tf.Tensor</code>的值，我们只使用了<code class="fe of og oh oi b">tf.Tensor.numpy()</code>方法。</p></blockquote><p id="48b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，我们可以使用<code class="fe of og oh oi b">matplotlib.pyplt</code>得到历元损失图，</p><pre class="kp kq kr ks gt oj oi ok ol aw om bi"><span id="2176" class="on mv iq oi b gy oo op l oq or">import matplotlib.pyplot as plt</span><span id="8ccf" class="on mv iq oi b gy os op l oq or">plt.plot( epochs_plot , loss_plot ) <br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/fdcadb6a8a3aeeb288e20180ee86bca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*hQyzUnBXdksPfthZx4UZZw.png"/></div></figure><p id="2c81" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在为了评估我们模型的准确性，我们测量了平均绝对误差。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><blockquote class="no np nq"><p id="c871" class="lf lg md lh b li lj ka lk ll lm kd ln nr lp lq lr ns lt lu lv nt lx ly lz ma ij bi translated">变化:<code class="fe of og oh oi b">tf.metrics</code>现在返回一个 op(运算)而不是张量。不过那很好！</p></blockquote><h1 id="a25a" class="mu mv iq bd mw mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated">等等，还有呢！</h1><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="7c3c" class="mu mv iq bd mw mx my mz na nb nc nd ne kf nf kg ng ki nh kj ni kl nj km nk nl bi translated">仅此而已。</h1><p id="3fb6" class="pw-post-body-paragraph lf lg iq lh b li nz ka lk ll oa kd ln lo ob lq lr ls oc lu lv lw od ly lz ma ij bi translated">ope 这是对 TensorFlow 2.0 和线性回归的一个很好的介绍。谢谢大家，机器学习快乐！</p></div></div>    
</body>
</html>