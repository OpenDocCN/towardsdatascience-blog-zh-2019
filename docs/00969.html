<html>
<head>
<title>Backpropagation for people who are afraid of math</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对害怕数学的人进行反向传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-for-people-who-are-afraid-of-math-936a2cbebed7?source=collection_archive---------6-----------------------#2019-02-14">https://towardsdatascience.com/backpropagation-for-people-who-are-afraid-of-math-936a2cbebed7?source=collection_archive---------6-----------------------#2019-02-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="21b1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">反向传播是机器学习中最重要的概念之一。有许多在线资源解释了这种算法背后的直觉(在我看来，其中最好的是斯坦福<a class="ae kl" href="http://cs231n.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> cs231n 视频讲座</a>中的反向传播讲座)。另一个非常好的来源，是<a class="ae kl" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">这个</a>，但是从直觉到实践，可能(委婉地说)很有挑战性。</p><p id="17a1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我承认，花了更多的时间，试图让我的层和重量的所有尺寸都适合，不断忘记什么是什么，什么连接在哪里，我坐下来，画了一些图表来说明整个过程。就当是可视化伪代码吧。</p><h1 id="5a4a" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">挑战:从直觉到实践</h1><p id="c9e1" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">所以，假设你对算法应该做什么有很好的直觉理解，但在让它工作时有困难，这篇文章是为你写的！</p><p id="34bf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在，要明确的是，这篇文章不会试图解释直觉部分。正如我之前所写的，有很多好的、可靠的资源可以做到这一点。这是一个简单的(算法允许的最简单的)..)帮助您让代码工作的实用指南。事实上，如果你没有任何直觉地遵循这些步骤，你的代码可能会工作。尽管如此，我还是强烈建议您阅读一些关于<a class="ae kl" href="https://en.wikipedia.org/wiki/Perceptron" rel="noopener ugc nofollow" target="_blank">感知器</a>的内容(注意，感知器使用不可微分的步进激活函数，因此您不能真正使用反向传播，但它的结构和权重更新方法确实为更复杂的神经网络奠定了基础)。这是最基本的神经网络，只有一层。在进入更复杂的网络之前，了解如何更新简单网络的权重确实很有帮助。</p><p id="3ce5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练网络时，我们重复以下步骤(对于<em class="lp"> n </em> epoches):</p><ol class=""><li id="fa35" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">执行向前传球。</li><li id="0f39" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">通过反向传播计算 W(每个权重的增量)。</li><li id="2b6f" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">更新权重。</li></ol><p id="8ad4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这篇文章中，我将重点关注第二步。</p><p id="32c1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">准备好了吗？让我们跳进来吧！</p><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="mj mk l"/></div><figcaption class="ml mm gj gh gi mn mo bd b be z dk">-propagating…</figcaption></figure><h1 id="f100" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">破译方程式</h1><p id="c853" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">下图是任意网络的示意图。因为反向传播的过程基本上每一步都是相同的(取决于您使用的<a class="ae kl" href="https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0" rel="noopener">激活功能</a>)。我们只看最后几层(你可以把 L95 层当作输入层，如果这样让你觉得更安全的话。就计算而言没什么区别)。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mp"><img src="../Images/b1dc4783ed7a1b6ce32c15c4f37317e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YYCa1g9X_j11o4JWoQV1cg.png"/></div></div></figure><p id="e7d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如您所见，我们的任意网络在第 95 层有 5 个神经元，然后在第 96 层有 2 个神经元，然后在输出层(L100)有 4、3、3 和 1 个神经元。所有层都由权重连接，由线标记(传统上)。<br/>注意每个节点被分成<em class="lp"> Z </em>和<em class="lp"> O </em>。对于每个节点，<em class="lp"> Z </em>值是通过将前一层的<em class="lp"> O </em>值乘以连接它们与该节点的权重来计算的。<em class="lp"> O </em>通过在<em class="lp"> Z </em>上应用非线性激活函数获得。现在，下面你会发现可怕的梯度计算。不要害怕！我们将复习每个表达。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mw"><img src="../Images/ce3f02f2109214b105917b3cfdf70a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFEmXNTGeNcN995ANFl6zg.png"/></div></div></figure><p id="713e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当试图更新和优化网络的权重时，我们试图找到-</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mx"><img src="../Images/4b1398eb65ca60f531741f16b6c3fb5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:118/format:webp/1*HcPLH3PcK4Y1WUORSQ3Lpg.png"/></div></div></figure><p id="4064" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">损失对重量的导数(“重量的变化如何影响损失”)，并使用链式法则，将该任务分成三部分:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi my"><img src="../Images/73155d28f9aa2eb1a4c2c8ae4b5cb58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*iyR3Q3jzpot_3GrIwqeAyQ.png"/></div></figure><p id="5783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">损失对下一层的导数。这是从输出层“向上游传递”的损失。或者换句话说，“下一层的变化如何影响损失”。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi my"><img src="../Images/663572765544990b30310a2995a88beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*sug_AV0S74zKFWYij6m7fg.png"/></div></figure><p id="8221" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一层相对于当前层的导数(可以解释为“当前层的变化如何影响下一层”)，它只是连接到下一层的权重乘以其激活函数的导数。和</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mx"><img src="../Images/8b15069b0a562a323f34b9082443ce62.png" data-original-src="https://miro.medium.com/v2/resize:fit:118/format:webp/1*K_SPSZeYuy4EsUj2LplWvA.png"/></div></div></figure><p id="54b4" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">-(“权重的变化如何影响当前层”)，它是前一层的<em class="lp"> O </em>值乘以当前层的激活函数导数。</p><p id="d370" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了使事情更清楚，我写了实际的计算，用颜色编码到我们的网络中，用于我们网络的最后两层，L100 和 L99。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mz"><img src="../Images/36e3a5a7fa8dd5209b91d2e9f0af4f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGPTU4aCUiLY2f8_L6nnVw.png"/></div></div></figure><p id="8a6d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，与每个计算相关的导数项出现在它的下方。与损耗相关的两个导数(以红色显示)非常重要，因为它们用于前面各层的计算。这在下图中可以清楚地看到:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi na"><img src="../Images/a79267a08fc9c39a61980cee354d05fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jV2brg3c6K7RvB_BK_VnUw.png"/></div></div></figure><p id="5568" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意∂Loss <strong class="jp ir">是如何向下传播</strong>的。看着这个模式，你应该开始明白如何在代码中实现它。还要注意，我强调了最后两层，它们构成了前面提到的感知器。</p><p id="408e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">注意，我没有提到在这个图中将整个表达式乘以学习率(α)，因为它看起来太拥挤了，并且遮蔽了带回家的消息，这是链规则的应用。您应该尝试不同的α值，以获得最佳性能。无论如何，α确实出现在下一张图中。</p><h1 id="2ea2" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">通过一批实例反向传播</h1><p id="d551" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">需要注意的重要一点是，我们看到的示意图中的每一层实际上都是一个矢量，代表为一个<strong class="jp ir">单个</strong>实例所做的计算。通常我们会将一批实例输入到网络中。看了下一张图后，这一点会更清楚，图中显示了对一批<em class="lp"> n </em>实例的计算。请注意，这是完全相同的网络(L95 层有 5 个神经元，L96 层有 2 个神经元，依此类推……)，只是我们现在看到的是<em class="lp"> n </em>个实例，而不是一个。</p><p id="3bc8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对我来说，当实现反向传播时，最具挑战性的部分是获得不同层的大小、权重和梯度矩阵。这幅插图旨在整理事物。</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nb"><img src="../Images/c7567e03a3aa387a1e8090dca9216109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4pXNLsJ0iaA2gk502jxdBA.png"/></div></div></figure><p id="43a0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在顶部，你会看到示意网络。<em class="lp"> n </em>的实际大小没有影响(用于计算。显然，这在更大的范围内会有所不同…)，因为你会注意到，当我们在反向传播时执行矩阵乘法时，我们总是对<em class="lp"> n </em>求和。也就是说，矩阵相乘时<em class="lp"> n </em>的长度是“丢失”的。这正是我们想要的，对我们批次中所有实例的损失求和。</p><p id="1101" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">图表的其余部分分为两个部分:</p><ul class=""><li id="2a32" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk nc lw lx ly bi translated"><strong class="jp ir">向前传球。</strong></li><li id="005a" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk nc lw lx ly bi translated"><strong class="jp ir">反向传播。</strong></li></ul><p id="9bc0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">向前传球</strong>对你们大多数人来说应该很明显。如果不是，我建议在继续学习之前先阅读一下矩阵乘法。我要指出的一件事是，每个权重矩阵取一个大小为<em class="lp"> (n，k) </em>的层，输出一个大小为<em class="lp"> (n，j) </em>的层。这种权重矩阵的大小为<em class="lp"> (k，j) </em>。你可能会注意到这张图缺少了偏差单位。这是因为我想让它尽可能的清晰，并关注不同的矩阵大小如何适应反向传播过程。下面有一小段是关于添加偏置单元的。</p><p id="d75a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">反向传播</strong>部分是一个“位”更棘手… :) <br/>该部分图表分为<strong class="jp ir">三个子部分</strong>:</p><h2 id="1b67" class="nd kn iq bd ko ne nf dn ks ng nh dp kw jy ni nj la kc nk nl le kg nm nn li no bi translated"><strong class="ak"> 1。变量</strong></h2><p id="4da0" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这里我列出了计算的不同元素，最重要的是它们的形状。关于这一部分的几点说明:</p><ol class=""><li id="966d" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated"><em class="lp"> Z </em>指激活前层的值。</li><li id="49a5" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><em class="lp"> O </em>指激活后的层值。</li><li id="dbed" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><em class="lp"> σ </em>指激活功能。</li><li id="e99e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated"><em class="lp"> g' </em>指激活函数的导数。</li></ol><p id="9f62" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">请注意，本节对构成的变量进行了分组</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi my"><img src="../Images/663572765544990b30310a2995a88beb.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*sug_AV0S74zKFWYij6m7fg.png"/></div></figure><p id="a12c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">和</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi my"><img src="../Images/73155d28f9aa2eb1a4c2c8ae4b5cb58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*iyR3Q3jzpot_3GrIwqeAyQ.png"/></div></figure><p id="13e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在顶部(以红色突出显示)，以及那些构成</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mx"><img src="../Images/8b15069b0a562a323f34b9082443ce62.png" data-original-src="https://miro.medium.com/v2/resize:fit:118/format:webp/1*K_SPSZeYuy4EsUj2LplWvA.png"/></div></div></figure><p id="3b87" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在底部(用蓝色突出显示)。</p><h2 id="bbaa" class="nd kn iq bd ko ne nf dn ks ng nh dp kw jy ni nj la kc nk nl le kg nm nn li no bi translated"><strong class="ak"> 2。计算</strong></h2><p id="cd45" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">这是所有戏剧发生的地方。这里其实没有什么新东西。这些都是在前面的图表中看到的完全相同的计算，但是矩阵大小写得很清楚。此外，何时使用元素乘法，何时使用矩阵乘法都有明确的说明(矩阵乘法用@表示，因为这是 Numpy.dot 的简写。您将在下面的代码部分中看到它的作用)，以及何时需要转置矩阵。图表和下面的代码假设了一个平方损失函数。其导数定义为<code class="fe np nq nr ns b">output - labels</code>。</p><h2 id="a5d2" class="nd kn iq bd ko ne nf dn ks ng nh dp kw jy ni nj la kc nk nl le kg nm nn li no bi translated">3.重量更新</h2><p id="d188" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">剩下的就是通过向每个权重矩阵添加 w 来更新我们的权重。请注意，我们只在完成反向传播后才执行更新。</p><h1 id="be5d" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">密码</h1><p id="85d8" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">关于此部分的一些注意事项:</p><ol class=""><li id="284d" class="lq lr iq jp b jq jr ju jv jy ls kc lt kg lu kk lv lw lx ly bi translated">出于可读性的目的，这里给出的代码是伪代码。</li><li id="4cc6" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">由于这篇文章旨在用作实践指南，我鼓励你仔细阅读图表，并在查看代码示例之前，尝试编写自己的<strong class="jp ir">T4 实现<strong class="jp ir">。这些图包含了你自己成功构建它所需要的所有信息。查看图表，了解渐变是如何从一层传递到下一层的。确保你明白什么是乘什么，什么轴是求和的。看看我们如何在每次迭代中获得适合我们层的权重的形状 W 矩阵。</strong></strong></li><li id="8476" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">网上有很多解决方案。我强烈推荐这本书，因为它很容易理解。我自己的实现很大程度上基于它。</li><li id="c95f" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk lv lw lx ly bi translated">代码假设使用 sigmoid 激活函数。由于 sigmoid 函数(<code class="fe np nq nr ns b">σ(z) *(1-σ(z))</code>)的导数只需要<em class="lp"> O </em>的值(当然是<code class="fe np nq nr ns b">σ(z)</code> <em class="lp"> ) </em>，所以我们不需要激活(<em class="lp"> Z </em>)前神经元的值。对于使用不同激活功能的实施，在进行正向传递时，您需要保存<em class="lp"> Z </em>值。</li></ol><h2 id="b73e" class="nd kn iq bd ko ne nf dn ks ng nh dp kw jy ni nj la kc nk nl le kg nm nn li no bi translated">使用循环</h2><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nt mk l"/></div></figure><h2 id="f434" class="nd kn iq bd ko ne nf dn ks ng nh dp kw jy ni nj la kc nk nl le kg nm nn li no bi translated">使用递归</h2><figure class="me mf mg mh gt mi"><div class="bz fp l di"><div class="nt mk l"/></div></figure><h1 id="eb4f" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">添加偏置单元</h1><p id="3f6a" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">您可能已经注意到，之前的图表缺少偏置单位。我选择从这些图中去掉偏差，因为我想让它们尽可能简单和直观，但是你绝对应该考虑添加它！<br/>您可以为每个层“手动”添加一个偏差，然后计算该偏差的损失导数:</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/04258d8f7e3eb94acfeca5b3ca017784.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*ZaEcaXt9D16CJ0tPt4yuVA.png"/></div></figure><p id="8386" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们已经知道如何计算</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4e071b7264e5ee3ac8c11fe47ce8d718.png" data-original-src="https://miro.medium.com/v2/resize:fit:78/format:webp/1*zczgCjNGP25aT4jpKQvIxA.png"/></div></figure><p id="33c6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">，以及</p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d7049fae64d4533e5577edb3476d7385.png" data-original-src="https://miro.medium.com/v2/resize:fit:108/format:webp/1*v8qM0LBr8Dg9neIe-bkDJA.png"/></div></figure><p id="48c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">只是当前层的激活函数导数。</p><p id="7ac7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您还可以将偏差添加到权重矩阵中。这基本上意味着在每一层添加一个偏向神经元的向量(一个 1 的向量)，并相应地初始化权重矩阵形状(就像在简单的线性回归中一样)。不过要记住的一件事是，偏置单元本身永远不应该在正向传递中更新，因为它们连接到下一个层的<strong class="jp ir">神经元，而不是前一个</strong>层的<strong class="jp ir">神经元(见图)。</strong></p><figure class="me mf mg mh gt mi gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nx"><img src="../Images/0cc53c8ce2deecb595d4cdb7f3bcbfa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9oE40gugpQELmTZ9hoEwaA.png"/></div></div></figure><p id="ee15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一种方法是避免更新这些神经元，但这可能会变得很棘手，尤其是在向后传递中(完全公开，这就是我所做的，我不建议这样做……)。一个更简单的解决方案是正常进行正向和反向传递，但在每次层更新后将偏置神经元重新初始化为 1。</p><h1 id="0dea" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">一些有用的提示</h1><ul class=""><li id="157e" class="lq lr iq jp b jq lk ju ll jy ny kc nz kg oa kk nc lw lx ly bi translated">反向传播时<strong class="jp ir">不要</strong>更新权重！！请记住，下一次迭代(前一层)将需要这些(未更新的)权重来计算损失。您可以保存 w，并在反向传播部分的末尾更新权重(就像我在代码示例中所做的那样)，或者不断更新前面两层的权重，在我看来，这是令人困惑的，而且过于复杂。</li><li id="cc8e" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk nc lw lx ly bi translated">如果层不是由非线性函数激活的(例如输出层)，则梯度仅为 1。</li><li id="8e0d" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk nc lw lx ly bi translated">你的程序没有崩溃，并不意味着它能工作。确保你的网络收敛，减少损失。</li><li id="ba12" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk nc lw lx ly bi translated">事实上，你的网络收敛，你的损失减少，并不意味着它的工作最佳。将您的结果与其他实现进行比较。研究学习速度和网络结构。</li><li id="d2f8" class="lq lr iq jp b jq lz ju ma jy mb kc mc kg md kk nc lw lx ly bi translated">尝试不同的权重初始化方法。这会对性能产生巨大的影响。</li></ul><h1 id="c807" class="km kn iq bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">摘要</h1><p id="c7fb" class="pw-post-body-paragraph jn jo iq jp b jq lk js jt ju ll jw jx jy lm ka kb kc ln ke kf kg lo ki kj kk ij bi translated">反向传播可能是一个棘手的问题，但是如果你希望很好地理解神经网络是如何工作的，你应该避免在自己实现一个简单的网络之前跳入更高级的解决方案，如<a class="ae kl" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>或<a class="ae kl" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank"> Pytorch </a>。这是所有深度学习的基础，也是成功处理更复杂网络的关键。也很好玩(起作用的时候)。</p><p id="4ecf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">祝你好运！</p></div></div>    
</body>
</html>