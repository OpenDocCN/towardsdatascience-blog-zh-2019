<html>
<head>
<title>Why default CNN are broken in Keras and how to fix them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras 中默认 CNN 为什么坏了，如何修复</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-default-cnn-are-broken-in-keras-and-how-to-fix-them-ce295e5e5f2?source=collection_archive---------5-----------------------#2019-07-06">https://towardsdatascience.com/why-default-cnn-are-broken-in-keras-and-how-to-fix-them-ce295e5e5f2?source=collection_archive---------5-----------------------#2019-07-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2439" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">深入了解 CNN 初始化… </em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/f8e7feb566e241becf5cb5c4eab2f6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0GH2vPt4eGTdZEzf"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/@ninoliverpool?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Nino Yang</a> on <a class="ae kz" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="71f5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">上周，我使用在 CIFAR10 数据集上训练的 VGG16 模型进行了一些实验。我需要从头开始训练模型，所以没有在 ImageNet 上使用预训练版本。</p><p id="0b52" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所以我开始了 50 个纪元的训练，去喝杯咖啡，然后回到这些学习曲线上:</p><div class="kk kl km kn gt ab cb"><figure class="lw ko lx ly lz ma mb paragraph-image"><img src="../Images/724da66c575a8dcafc099dccfb2ff10b.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*jiWpNqIwW8UxIRYRhXS5TQ.png"/></figure><figure class="lw ko mc ly lz ma mb paragraph-image"><img src="../Images/45c6685350b4deb6501e17f4056f7e71.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*OJUxl2Qnjj7sZpOeNyPJgg.png"/></figure></div><p id="a4f9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">模特什么都没学到！</strong></p><p id="ccfc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我已经看到网络收敛非常缓慢，振荡，过度拟合，发散，但这是我第一次看到网络的这种行为什么也没做。因此，我挖了一点，看看发生了什么事。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="ff32" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">实验</h1><p id="40e4" class="pw-post-body-paragraph la lb it lc b ld nc ju lf lg nd jx li lj ne ll lm ln nf lp lq lr ng lt lu lv im bi translated">这就是我如何创建我的模型。它遵循原始 VGG16 架构，但大部分全连接层被移除，因此几乎只保留卷积。</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="9974" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在让我们多理解一点，是什么导致了我在这篇文章开始时向你展示的这种训练曲线。</p><p id="93e0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">当模型的学习出现问题时，检查梯度的表现通常是个好主意。我们可以用以下公式得到每一层的平均值和标准差:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="96e6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">通过绘制它们，我们有:</p><div class="kk kl km kn gt ab cb"><figure class="lw ko nj ly lz ma mb paragraph-image"><img src="../Images/fa21f3dc727adc7008f8b95ceabd9229.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*vzG9ASldvPDp6CZuJhFLTQ.png"/></figure><figure class="lw ko nj ly lz ma mb paragraph-image"><img src="../Images/1c3badde0477a24becf6417c9082dee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*n9zFq4nCDJoIgJobAMcPHw.png"/><figcaption class="kv kw gj gh gi kx ky bd b be z dk nk di nl nm">Stats of gradients of VGG16 initialized with Glorot uniform</figcaption></figure></div><p id="f188" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">哇…在我的模型中根本没有渐变，也许我们应该检查激活是如何沿着层发展的。我们可以通过下式得到它们的平均值和标准偏差:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="c19d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，如果我们把它们画出来:</p><div class="kk kl km kn gt ab cb"><figure class="lw ko nn ly lz ma mb paragraph-image"><img src="../Images/c75ef423bbb8e493f697663f1018e634.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*8s_ABoP34Y_YK1Qv4upXbQ.png"/></figure><figure class="lw ko no ly lz ma mb paragraph-image"><img src="../Images/b6ed93167eff6fd50e8322e562e8a5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*hxSykdfRagCN6ArgQ2M_FA.png"/><figcaption class="kv kw gj gh gi kx ky bd b be z dk np di nq nm">Stats of activations of VGG16 initialized with Glorot uniform</figcaption></figure></div><p id="8d93" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">那里正在发生什么！</strong></p><p id="848d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">提醒您，每个卷积层的梯度计算如下:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1b3eabef269e91e62b72ee2bf8f20005.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*lNXfU28asSTCk6d5RS4vlw.png"/></div></figure><p id="3c9c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">其中，δx 和δy 用于表示梯度，∂L/∂x 和∂L/∂y.梯度是使用反向传播算法和链式法则计算的，这意味着我们从最后一层开始，反向传播到更早的层。但是，如果我们的最后一层激活趋向于 0，会发生什么呢？正如我们在这里所得到的，梯度在任何地方都等于 0，因此不能反向传播，导致网络不能学习任何东西。</p><p id="cc95" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因为我的网络非常空(没有批量标准化，没有丢失，没有数据扩充，..)，我猜这个问题应该来自一个糟糕的初始化，所以我读了明凯的论文<strong class="lc iu">【1】</strong>，我将简要描述它说了什么。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="1026" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">初始化方法</h1><p id="de07" class="pw-post-body-paragraph la lb it lc b ld nc ju lf lg nd jx li lj ne ll lm ln nf lp lq lr ng lt lu lv im bi translated">初始化一直是深度学习的一个重要研究领域，尤其是随着架构和非线性的不断发展。一个好的初始化其实是我们可以训练深度神经网络的原因。</p><p id="90c5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里是明凯论文的主要观点，它们显示了初始化应该具有的条件，以便具有带有 ReLU 激活功能的正确初始化的 CNN。需要一点数学知识，但是不要担心，你应该能够掌握大纲。</p><p id="9ff0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们考虑卷积层的输出为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ea5d588f6d968ef04032ee967ad20b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*FSJgl0Di3gbNw6gUrld-_w.png"/></div></figure><p id="41d8" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，如果偏差被初始化为 0，并且在权重<em class="ns"> w </em>和元素<em class="ns"> x </em>都相互独立并且<br/>共享相同分布的假设下，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5ccf6334564376877b97c0e86ab72739.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*AeW7goFoVokQ6jKPRsXcmA.png"/></div></figure><p id="b6f5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">用<em class="ns"> n </em>，一层的权值个数(<em class="ns">即</em> <em class="ns"> n=k c </em>)。由下面的<em class="ns">独立</em>属性的乘积的方差:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/018184155c35928b9a6202a50324269f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*mBt-kZTfRK7yzu-32eVwnQ.png"/></div></figure><p id="17f1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它变成了:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nw"><img src="../Images/e67e70982fd0e43eadd7627e3a272f9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PX_G90BlveWLA6CCiylpkQ.png"/></div></div></figure><p id="f601" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后，如果我们让权重<em class="ns"> w </em>的平均值为 0，则得出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/48008b24f9e8ca330d1aca77c3372c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*TI2iRjeoAsMAaDwwrSc_3A.png"/></div></figure><p id="7c52" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由<em class="ns">柯尼希-惠更斯</em>财产:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/5a0eb36c1b87655404e4dddd51aef9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*XZf1WK79xSd1RrUzuO43gQ.png"/></div></figure><p id="8abd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它最终给出:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/0808e81784399e6da01be3a67fe71c4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*oxS0v04HP8u-zzKQZdxl1A.png"/></div></figure><p id="77d9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">但是，由于我们使用的是 ReLU 激活函数，我们有:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/47fbb21d4af6bb60bcd39b79c6f4c069.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*yqrD3GwntDQhwVf72jCxIw.png"/></div></figure><p id="8a46" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">因此:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/09d531c45ff3f273f36aef8e399cdf60.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*uzXKuwK_CAevlufZLc77FQ.png"/></div></figure><p id="343b" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这是单个卷积层输出的方差，但如果我们想考虑所有的卷积层，我们必须考虑所有卷积层的乘积，即:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ab5648a4f18d70be7accd3b31ac8a29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:898/format:webp/1*AYEebFXL5FU1wSyVhp-e-Q.png"/></div></figure><p id="279c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由于我们有一个产品，现在很容易看出，如果每层的方差不接近 1，那么网络可能会迅速退化。事实上，如果它小于 1，它将迅速向 0 消失，如果它大于 1，那么激活值将无限增长，甚至可以成为一个很高的数字，你的计算机不能代表它(NaN)。所以，为了有一个行为良好的 ReLU CNN，必须认真遵守以下条件:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi od"><img src="../Images/fd995ad478899400f0775fbedb575c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*l0wuB6YcQ0sdhSnUfuGaCw.png"/></div></figure><p id="cb3d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">作者比较了当你训练一个深度 CNN 初始化为当时的标准初始化(Xavier/Glorot)<strong class="lc iu">【2】</strong>和用他们的解决方案初始化时会发生什么。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oe"><img src="../Images/bbe6d0f7d8ca0e0ade425b686b013d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*apYdMu8RzYDojyMYKm4rTA.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Comparison of the training of a 22-layer ReLU CNN initialized with Glorot (blue) or Kaiming (red). The one initialized with Glorot doesn’t learn anything</figcaption></figure><p id="ee9a" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个图表看起来熟悉吗？正是我一开始所见证和展示给你的！用 Xavier/Glorot 初始化训练的网络不学习任何东西。</p><p id="0bc6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><strong class="lc iu">现在猜猜 Keras 里默认初始化的是哪一个？</strong></p><p id="6014" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">没错！默认情况下，在 Keras 中，卷积层的初始化遵循 Glorot 均匀分布:</p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="78a6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果我们现在把初始化改为明凯统一的，会发生什么？</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="c8c5" class="of ml it bd mm og oh dn mq oi oj dp mu lj ok ol mw ln om on my lr oo op na oq bi translated">使用明凯初始化</h2><p id="fe9d" class="pw-post-body-paragraph la lb it lc b ld nc ju lf lg nd jx li lj ne ll lm ln nf lp lq lr ng lt lu lv im bi translated">让我们重新创建我们的 VGG16 模型，但这一次我们改变了初始化为<em class="ns">贺 _ 制服。</em></p><p id="fcde" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，让我们在训练模型之前检查激活和渐变。</p><div class="kk kl km kn gt ab cb"><figure class="lw ko nj ly lz ma mb paragraph-image"><img src="../Images/630162376e53298d299f650c6ad043a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*13z-1AJBCU3ydfsPxm6XGg.png"/></figure><figure class="lw ko nj ly lz ma mb paragraph-image"><img src="../Images/f056a520b5725e61bf1e4b3d041faacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Zz1NLJ5mn0T5RaN2mMekqw.png"/></figure></div><p id="7cee" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，随着明凯的初始化，我们的激活平均值约为 0.5，标准差约为 0.8</p><div class="kk kl km kn gt ab cb"><figure class="lw ko or ly lz ma mb paragraph-image"><img src="../Images/f1c53eed888fcfb870c87e2b0bb55871.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*8AeJ97Ax1ZSxhIqeYB0GhA.png"/></figure><figure class="lw ko os ly lz ma mb paragraph-image"><img src="../Images/d0456af7bd018db9a461f1a53b72a530.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*LIO2Gv0-Tqw2kv6OhTLuUA.png"/></figure></div><p id="0879" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以看到，现在我们有了一些梯度，如果我们想让我们的网络学习一些东西，这是一件好事。</p><p id="5b9c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在，如果我们训练我们的新模型，我们得到这些曲线:</p><div class="kk kl km kn gt ab cb"><figure class="lw ko nj ly lz ma mb paragraph-image"><img src="../Images/6a05b4584769a9384b956bce274368cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Ht-nLAHfSJHlUAG62FoMMQ.png"/></figure><figure class="lw ko nj ly lz ma mb paragraph-image"><img src="../Images/111e0bc610f8fff272afd23406a11764.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*trONminKDgHkbD7Uw7nDcQ.png"/></figure></div><p id="f2cc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们现在可能需要添加一些正则化，但是，嘿，这仍然比以前好，对不对？</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="a362" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结论</h1><p id="30da" class="pw-post-body-paragraph la lb it lc b ld nc ju lf lg nd jx li lj ne ll lm ln nf lp lq lr ng lt lu lv im bi translated">在这篇文章中，我们展示了初始化可能是你的模型中非常重要的一部分，但却经常被忽视。此外，它还表明，即使是像 Keras 这样优秀的图书馆，也不能认为它们是理所当然的。</p><h2 id="9b5e" class="of ml it bd mm og oh dn mq oi oj dp mu lj ok ol mw ln om on my lr oo op na oq bi translated">我希望这篇博文对你有所帮助！您可能再也不会忘记正确初始化您的网络了！如有不清楚的地方，请随时给我反馈或问我问题。</h2></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h2 id="068f" class="of ml it bd mm og oh dn mq oi oj dp mu lj ok ol mw ln om on my lr oo op na oq bi translated">参考资料和进一步阅读</h2><p id="4dd1" class="pw-post-body-paragraph la lb it lc b ld nc ju lf lg nd jx li lj ne ll lm ln nf lp lq lr ng lt lu lv im bi translated">[1]: <a class="ae kz" href="https://arxiv.org/pdf/1502.01852.pdf" rel="noopener ugc nofollow" target="_blank">《明凯》何初始化论文</a></p><p id="c9b6" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[2]: <a class="ae kz" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" rel="noopener ugc nofollow" target="_blank">泽维尔·格罗特初始化论文</a></p><p id="fddb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[3]: <a class="ae kz" href="https://www.youtube.com/watch?v=s2coXdufOzE" rel="noopener ugc nofollow" target="_blank">吴恩达初始课</a></p></div></div>    
</body>
</html>