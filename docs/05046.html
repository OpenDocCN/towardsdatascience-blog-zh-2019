<html>
<head>
<title>Exercise Classification with Machine Learning (Part I)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于机器学习的运动分类(上)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exercise-classification-with-machine-learning-part-i-7cc336ef2e01?source=collection_archive---------11-----------------------#2019-07-29">https://towardsdatascience.com/exercise-classification-with-machine-learning-part-i-7cc336ef2e01?source=collection_archive---------11-----------------------#2019-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="f153" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这篇分为两部分的文章中，我们将深入探讨一个具体问题:对人们进行各种锻炼的视频进行分类。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b5183159add0f5b9b37377a05c26ac6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sziFPGMLEtLTdBDNTNVfeg.jpeg"/></div></div></figure><p id="e209" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">第一篇文章将关注一种更为<strong class="js iu">的算法</strong>方法，使用<em class="la">k-最近邻</em>对未知视频进行分类，在<a class="ae lb" href="https://medium.com/@trevor.j.phillips/exercise-classification-with-machine-learning-part-ii-d60d1928f31d" rel="noopener">第二篇文章</a>中，我们将关注一种专门的<strong class="js iu">机器学习</strong> (ML)方法。</p><p id="8d9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将要讨论的所有内容的代码都可以在 GitHub 的<a class="ae lb" href="https://github.com/trevphil/TechniqueAnalysis" rel="noopener ugc nofollow" target="_blank"> <strong class="js iu">这个</strong> </a>资源库中找到。算法方法(第一部分)是用 Swift 编写的，可以作为一个<a class="ae lb" href="https://cocoapods.org/pods/TechniqueAnalysis" rel="noopener ugc nofollow" target="_blank"> CocoaPod </a>获得。ML 方法(<a class="ae lb" href="https://medium.com/@trevor.j.phillips/exercise-classification-with-machine-learning-part-ii-d60d1928f31d" rel="noopener">第二部分</a>)是用 Python/TensorFlow 编写的，可以作为 GitHub 资源库的一部分找到。</p><h1 id="3033" class="lc ld it bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">背景</h1><p id="bbba" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">我们想要构建一个系统，它将一个人进行锻炼的视频作为输入，并输出一个描述该视频的<strong class="js iu">类标签</strong>。理想情况下，视频可以是任何长度，任何帧速率，任何摄像机角度。类别标签的子集可能如下所示:</p><ul class=""><li id="2efa" class="mf mg it js b jt ju jx jy kb mh kf mi kj mj kn mk ml mm mn bi translated"><code class="fe mo mp mq mr b">back squats — <strong class="js iu">correct</strong> form</code></li><li id="4e22" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><code class="fe mo mp mq mr b">back squats — <strong class="js iu">incorrect</strong> form</code></li><li id="3ec6" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><code class="fe mo mp mq mr b">push-ups — <strong class="js iu">correct</strong> form</code></li><li id="8edb" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><code class="fe mo mp mq mr b">push-ups — <strong class="js iu">incorrect</strong> form</code></li><li id="0411" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated">诸如此类…</li></ul><p id="61ba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如何构建这样的系统？最棘手的部分之一是我们需要识别帧之间的关系——也就是说，人在每帧的<em class="la">位置之间的关系。这意味着我们正在处理时域中的数据结构，我们称之为<strong class="js iu">时间序列</strong>。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mx"><img src="../Images/ab91c9286f1c4a302c3b86dd4f646c25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7LYkPTe5eNFRclCc3TSnw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Timeseries in 2 or more dimensions (at least one dimension is always time!)</figcaption></figure><p id="1020" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">给定一些从输入视频构建的“未知”时间序列，我们想要根据我们预测的练习给时间序列分配一个<strong class="js iu">标签</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nc"><img src="../Images/a30ddecd5461037cba512e7c95b691d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aqUfi5St7H7HvEZU3ebrkQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">In this example, “A”, “B”, and “C” are potential labels for the unknown timeseries</figcaption></figure><h2 id="12cc" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">如何制作时间系列？</h2><p id="8e4f" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">我提到过我们将从一个输入视频中构建时间序列，但是我们怎么做呢？让我们假设每个视频都包含一个人进行某种锻炼的镜头。我的策略是:</p><ol class=""><li id="a75e" class="mf mg it js b jt ju jx jy kb mh kf mi kj mj kn np ml mm mn bi translated">标准化每个视频的长度</li><li id="b638" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn np ml mm mn bi translated">对于每一帧，使用 14 个关键身体点来确定人的“姿势”</li><li id="47a2" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn np ml mm mn bi translated">创建表示姿势随时间变化的时间序列数据结构</li><li id="9302" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn np ml mm mn bi translated">过滤掉数据中的噪音</li></ol><h2 id="aa9d" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">标准化视频长度</h2><p id="3ce1" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">处理一个 5 分钟的视频是不现实的，所以为了加强数据的一致性，我们将标准化每个视频的长度。我从每个视频的中间对一个 5 秒的片段进行子采样，假设中间发生的事情是整个视频的“代表”。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/02f822b3d2a89b6f5a4a616a562ec6ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rEmeDFbWvH62bsnwiZS4lA.png"/></div></div></figure><p id="a88b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于 5 秒以下的视频，我们使用整个东西。</p><h2 id="fb90" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">确定姿势</h2><p id="2d1d" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">给定子剪辑的一帧(图像)，我们想要确定图像中人的姿态。已经可以估计身体点(称为<em class="la">姿势估计</em>)，例如使用项目<a class="ae lb" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> OpenPose </a>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/3a9436d6d7771773a8f80a28e2c7aa79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*qpiOCY8nHzhZr6dcJjbtwg.gif"/></div><figcaption class="my mz gj gh gi na nb bd b be z dk"><a class="ae lb" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></figcaption></figure><p id="70b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我从<a class="ae lb" href="https://github.com/tucan9389/PoseEstimation-CoreML/tree/master/models" rel="noopener ugc nofollow" target="_blank">这里</a>拿了一个预先训练好的卷积神经网络(CNN)并用它来估计每个视频帧的姿势。使用这个 ML 模型，我们为 14 个身体部位中的每一个都获得了一个(x，y)坐标，以及每个身体部位的位置精度的置信水平[0，1]。你可以在<a class="ae lb" href="https://github.com/trevphil/TechniqueAnalysis" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>上看到实现细节。</p><h2 id="834d" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">把它放在一起</h2><p id="3e3e" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">现在，从视频构建时间序列的一切都已就绪。为了把它放在一起，我们简单地连接每个视频帧的姿态数据。由此产生的时间序列有 4 个维度:</p><ul class=""><li id="9447" class="mf mg it js b jt ju jx jy kb mh kf mi kj mj kn mk ml mm mn bi translated"><strong class="js iu">时间</strong>:由视频帧索引</li><li id="1163" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><strong class="js iu">身体部位</strong>:共 14 个身体部位</li><li id="9633" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><strong class="js iu"> x 位置</strong>:身体部位的 x 坐标，从[0，1]开始归一化</li><li id="ec3a" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><strong class="js iu"> y 位置</strong>:身体部位的 y 坐标，从[0，1]开始归一化</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/c6e9d2c95db2f52ea14a46fd8d1e4046.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*mSNPvlSeDZQrEoR11VOw4A.png"/></div></figure><p id="2956" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">由于有 14 个身体部位和每个部位的(x，y)坐标，我们可以将时间序列数据结构想象成 28 个随时间变化的波(14 x2 = 28)。</p><p id="c737" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">(x，y)坐标通过将每个身体部位位置分别除以帧的宽度或高度来归一化。框架的左下角作为我们坐标系的原点。</p><h2 id="b1f1" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">平滑噪声数据</h2><p id="183c" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">在这一点上，我们有一个时间序列，但当我们试图分类和分配一个类标签时，它包含的噪声可能会导致不准确的预测。为了平滑噪声数据，我使用了一种叫做<strong class="js iu">黄土</strong> ( <em class="la">局部加权散点平滑</em>)的过滤器。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/f5460aa1d26c0d8ab8b002775c31d30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ZvyFDS_RY7d5_a1EkIFNjw.gif"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk"><a class="ae lb" href="https://ggbaker.selfip.net/data-science/media/loess.gif" rel="noopener ugc nofollow" target="_blank">https://ggbaker.selfip.net/data-science/media/loess.gif</a></figcaption></figure><p id="7f51" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">总的想法是，对于每个“原始”数据点，我们通过取相邻点的<em class="la">加权平均</em>来得到更好的估计。离我们考虑的点越近的邻居权重越高，因此对平均值的影响也越大。</p><p id="c489" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">黄土的好处是(例如，与卡尔曼滤波器相比)只需要考虑一个参数。此参数控制相邻点对加权平均值的影响程度。它越大，邻居的影响就越大，因此产生的曲线就越平滑。</p><p id="3e62" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是我们的时间序列数据的一个例子，在之前的<em class="la">和在</em>黄土过滤之后的<em class="la">:</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/b9834f51e3c49044d70b8f27125bf996.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nd7s6vtjNxiuiAIPhk2w0w.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Timeseries before and after LOESS filtering</figcaption></figure><h1 id="3f4f" class="lc ld it bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">时间序列分析</h1><p id="6ee7" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">既然我们有了将输入视频预处理成可用数据的方法，我们需要分析产生的时间序列并对视频进行分类。</p><p id="d1f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">分析可以用很多方法来完成，但在这篇文章中，我们将重点关注一种叫做<em class="la">k-最近邻</em>的算法。基本上，我们会将<strong class="js iu">未知</strong>时间序列与大量<strong class="js iu">已知</strong>时间序列进行比较，找出最接近的匹配(<code class="fe mo mp mq mr b">k=1</code>)。比较是使用距离函数来完成的，我们将在后面讨论。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/ae8420b6d88adbb0dfc2f91ee14c016f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QvSJGRJDCRacqIJodH9MvQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">The green item is “unknown,” and we’ll assign it the label of the closest “known” item</figcaption></figure><p id="669d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们找到未知视频的最近邻居，我们预测未知视频与已知项目具有<em class="la">相同的类别标签</em>，因为根据我们的距离函数它们是最近的。</p><h2 id="68e4" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">距离函数是什么？</h2><p id="31cd" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">在二维空间中寻找两点之间的距离很容易:</p><pre class="kp kq kr ks gt nw mr nx ny aw nz bi"><span id="28a0" class="nd ld it mr b gy oa ob l oc od">dist = sqrt( (x2 — x1)^2 + (y2 — y1)^2 )</span></pre><p id="3300" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是两波之间的距离呢？还是两个时间序列之间的距离？没那么容易…</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/effa25dcde7b1d66ca1c3f5a9fb6c64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IGpR1oU7NLVvpIB7baiUtw.png"/></div></div></figure><p id="b220" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">更复杂的是，想象以下场景:</p><blockquote class="of og oh"><p id="b4dc" class="jq jr la js b jt ju jv jw jx jy jz ka oi kc kd ke oj kg kh ki ok kk kl km kn im bi translated">我们有一个“已知”标记的时间序列，来自一个人做俯卧撑的视频。他每 2 秒做<strong class="js iu"> 1 个俯卧撑</strong>，并在视频开始 1 秒做<strong class="js iu">。现在，我们想从其他人做俯卧撑的视频中标记一个“未知”的时间序列。他每 3 秒做<strong class="js iu"> 1 个俯卧撑</strong>，并在视频</strong>的 2 秒后开始做<strong class="js iu">。</strong></p></blockquote><p id="f3d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">即使这些时间序列来自相同的练习(俯卧撑)，当我们试图比较“未知”时间序列和“已知”时间序列时，会出现两个问题:不同的<strong class="js iu">频率</strong>和不同的<strong class="js iu">相移</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/99d6ab7f01d7f2f01383881638a6517b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DGYh1gUIFB9PTXAvGVfe-Q.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Differing frequency (e.g. push-up rate)</figcaption></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi om"><img src="../Images/641887941d19915314f533d19154dad3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*ojruc8cQdf8Ge2SgmSkJcw.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Differing phase shift (e.g. push-up starting time)</figcaption></figure><h2 id="4261" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">比较时间序列</h2><p id="108f" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">为了帮助解决不同频率和相移的问题，我们将采用一种叫做<em class="la">动态时间扭曲</em> (DTW)的算法。这是使用动态编程的非线性对齐策略。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/c8d66d6d5bee8b857c95afa92eb549d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjOGmnwXazWGdm56JrgjZQ.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk"><a class="ae lb" href="https://www.cs.unm.edu/~mueen/DTW.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.unm.edu/~mueen/DTW.pdf</a></figcaption></figure><p id="0780" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与许多动态规划算法一样，在 DTW 中，我们填充一个矩阵，其中每个单元的值是相对于相邻单元的函数。</p><p id="db6a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设我们正在比较长度为<code class="fe mo mp mq mr b">M</code>的时间序列<code class="fe mo mp mq mr b">s</code>和长度为<code class="fe mo mp mq mr b">N</code>的时间序列<code class="fe mo mp mq mr b">q</code>。矩阵的每一行对应于<code class="fe mo mp mq mr b">s</code>的一个时间点，每一列对应于<code class="fe mo mp mq mr b">q</code>的一个时间点。这样矩阵就是<code class="fe mo mp mq mr b">MxN</code>。第<code class="fe mo mp mq mr b">m</code> ( <code class="fe mo mp mq mr b">0 &lt;= m &lt; M</code>)行和第<code class="fe mo mp mq mr b">n</code> ( <code class="fe mo mp mq mr b">0 &lt;= n &lt; N</code>)列的单元成本如下:</p><pre class="kp kq kr ks gt nw mr nx ny aw nz bi"><span id="dce4" class="nd ld it mr b gy oa ob l oc od">cost[m, n] = <br/>    distance(s[m], q[n]) +<br/>    min(cost[m-1, n], cost[m, n-1], cost[m-1, n-1])</span></pre><p id="209f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">将<code class="fe mo mp mq mr b">s[m]</code>想象为<em class="la">未知</em>视频中的人在<code class="fe mo mp mq mr b">m</code>时刻的姿势，将<code class="fe mo mp mq mr b">q[n]</code>想象为<em class="la">已知</em>视频中的人在<code class="fe mo mp mq mr b">n</code>时刻的姿势。它们之间的距离<code class="fe mo mp mq mr b">distance(s[m], q[n])</code>是各个身体部位坐标之间的 2D 距离的<strong class="js iu">和</strong>。</p><blockquote class="of og oh"><p id="fbfc" class="jq jr la js b jt ju jv jw jx jy jz ka oi kc kd ke oj kg kh ki ok kk kl km kn im bi translated">我还对身体各部分之间的距离进行了加权求和，给与特定锻炼相关的身体部分分配了更多的权重。例如，在分析<strong class="js iu">下蹲</strong>时<strong class="js iu">脚</strong>是不相关的(因为它们不移动)，所以它们的权重较小。但是对于<strong class="js iu">跳</strong>来说，脚动的多，重量也大。这项技术稍微提高了算法的准确性。</p></blockquote><p id="63b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一旦我们填满了 DTW 成本矩阵，我们计算的最后一个单元实际上是两个时间序列之间的最小“距离”。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/0f98060e67d27f0d18450154f1cd5b99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2q5IZuygWQ5NQTGWKCWD7Q.png"/></div></div><figcaption class="my mz gj gh gi na nb bd b be z dk">Visualization of DTW (image has been modified from <a class="ae lb" href="https://www.cs.unm.edu/~mueen/DTW.pdf" rel="noopener ugc nofollow" target="_blank">this presentation</a>)</figcaption></figure><p id="33c3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以通过使用一种叫做“扭曲窗口”的东西来提高 DTW 的效率简而言之，这意味着我们不计算整个成本矩阵，而是计算矩阵的一个较小的对角线部分。如果您有兴趣了解更多信息，请查阅论文<a class="ae lb" href="http://alumni.cs.ucr.edu/~xxi/495.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="la">快速时间序列分类使用数量缩减</em> </a>。</p><h1 id="b7cc" class="lc ld it bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">结果</h1><p id="b52f" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated">现在我们已经建立了一个系统，它将一个未知的视频作为输入，将其转换为时间序列，并使用 DTW 将其与来自一组已标记视频的时间序列进行比较。最匹配的已知时间序列的标签将用于对未知视频进行分类。</p><p id="4e8d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了评估这个系统，我使用了大约 100 个视频和 3 个练习的数据集:负重深蹲、引体向上和俯卧撑。每个视频中的练习要么是正确的<em class="la"/>要么是不正确的<em class="la"/>(使用不适当的技术)。从人的正面、侧面和背面记录视频。对于侧面角度，我通过在 y 轴上翻转原始图像生成了第二个视频。<strong class="js iu"> 80% </strong>的视频被用作“标记”数据，剩余的<strong class="js iu"> 20% </strong>被保留用于测试算法的准确性。</p><p id="9b8c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">测试结果表明，该算法非常擅长区分练习之间的<em class="la">(<strong class="js iu">90–100%准确度</strong>)，但不太擅长对</em>练习中的变化<em class="la">进行分类(正确与不正确的技术)。这些变化可能很微妙，很难跟踪，在这方面我能达到的最大精度是<strong class="js iu"> ~65% </strong>。</em></p><h2 id="8f1c" class="nd ld it bd le ne nf dn li ng nh dp lm kb ni nj lq kf nk nl lu kj nm nn ly no bi translated">丰富</h2><p id="ffa4" class="pw-post-body-paragraph jq jr it js b jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj me kl km kn im bi translated"><em class="la">k-最近邻</em>算法的一个主要缺点是推理时间——即对未知视频进行分类的时间——随着标记数据集的大小成比例增长，因为我们将未知时间序列与每个标记项目进行比较。</p><p id="bfe4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<a class="ae lb" href="https://medium.com/@trevor.j.phillips/exercise-classification-with-machine-learning-part-ii-d60d1928f31d" rel="noopener">第二部分</a>中，我们将探索一种使用 ML 模型对视频进行分类的端到端方法，从而使推理时间<strong class="js iu">保持恒定</strong>。</p><h1 id="7de8" class="lc ld it bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">资源</h1><ul class=""><li id="36f2" class="mf mg it js b jt ma jx mb kb op kf oq kj or kn mk ml mm mn bi translated">主<a class="ae lb" href="https://github.com/trevphil/TechniqueAnalysis" rel="noopener ugc nofollow" target="_blank"> GitHub 库</a>带有视频处理和 DTW</li><li id="ab05" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><a class="ae lb" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">打开姿势</a></li><li id="fd5c" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><a class="ae lb" href="https://github.com/tucan9389/PoseEstimation-CoreML" rel="noopener ugc nofollow" target="_blank">iOS 上的姿态估计</a></li><li id="0de0" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><a class="ae lb" href="https://ggbaker.selfip.net/data-science/content/filtering.html" rel="noopener ugc nofollow" target="_blank">局部加权散点图平滑</a>(黄土)</li><li id="a827" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Dynamic_time_warping" rel="noopener ugc nofollow" target="_blank">动态时间扭曲</a> (DTW)</li><li id="e8ef" class="mf mg it js b jt ms jx mt kb mu kf mv kj mw kn mk ml mm mn bi translated"><a class="ae lb" href="http://alumni.cs.ucr.edu/~xxi/495.pdf" rel="noopener ugc nofollow" target="_blank"> DTW《翘曲的窗户》</a></li></ul></div></div>    
</body>
</html>