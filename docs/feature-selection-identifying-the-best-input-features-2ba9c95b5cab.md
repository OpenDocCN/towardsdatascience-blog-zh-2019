# 特征选择:识别最佳输入特征

> 原文：<https://towardsdatascience.com/feature-selection-identifying-the-best-input-features-2ba9c95b5cab?source=collection_archive---------8----------------------->

*在本文中，我们将了解什么是特征选择，特征选择和降维的区别。特征重要性如何帮助？了解不同的技术，如过滤方法、包装器和嵌入方法，以便用 Python 代码识别最佳特性。*

![](img/d1b73f9b4bfbc344b2959ef21ac432f1.png)

# 什么是特征选择？

特征选择也被称为**属性选择**或**变量选择**，是**特征工程**的一部分。它是在数据集中选择最相关的属性或特征子集进行预测建模的过程。

[](https://www.datadriveninvestor.com/2019/02/08/machine-learning-in-finance/) [## 金融中的机器学习|数据驱动的投资者

### 在我们讲述一些机器学习金融应用之前，我们先来了解一下什么是机器学习。机器…

www.datadriveninvestor.com](https://www.datadriveninvestor.com/2019/02/08/machine-learning-in-finance/) 

选定的功能有助于预测模型识别隐藏的业务洞察力。

*如果我们需要预测 IT 人员的工资，那么根据我们的共同理解，我们需要工作经验、技能、工作地点和当前职位。这些是有助于薪资预测的几个关键特征。如果数据集包含人的身高，我们知道该特征与工资预测无关，因此不应作为特征选择的一部分。*

**特征选择是决定包含哪些相关原始特征和排除哪些不相关特征进行预测建模的过程。**

# 特征选择和降维的区别

特征选择和降维的目标是减少数据集中属性或特征的数量。

特征选择和降维的关键区别在于，在**特征选择中，我们不改变原始特征，然而，在降维中，我们从原始特征中创建新特征**。这种使用降维的特征转换通常是不可逆的。

特性选择基于某些统计方法，比如我们将在本文中讨论的过滤器、包装器和嵌入式方法。

对于降维，我们使用像主成分分析(PCA)这样的技术

# 需要特征选择

*   **帮助更快地训练模型:**我们已经减少了相关特征的数量，因此训练要快得多。
*   **提高模型解释能力并简化模型** —它通过仅包含最相关的特征来降低模型的复杂性，因此易于解释。这对于解释预测模型非常有帮助
*   **提高模型的准确性:**我们只包括与我们的预测相关的特征，这些特征提高了模型的准确性。不相关的特征会引入噪声并降低模型的准确性
*   **减少过度拟合:**过度拟合是指预测模型不能很好地概括测试数据或基于训练的未知数据。为了减少过度拟合，我们需要去除数据集中的噪声，并包括对预测影响最大的特征。噪声来自数据集中不相关的特征。当预测模型作为训练的一部分已经学习了噪声时，那么它将不会很好地对看不见的数据进行概括。

# 不同的特征选择方法

*   过滤器
*   包装材料
*   嵌入式方法

# 特征选择的过滤方法

过滤方法基于某个单变量度量对每个特征进行排名，然后选择排名最高的特征。一些单变量指标是

*   **方差:**去除常数和准常数特征
*   **卡方:**用于分类。这是一种独立性的统计测试，用于确定两个变量的相关性。
*   **相关系数:**删除重复特征
*   **信息增益或互信息**:评估自变量在预测目标变量时的相关性。换句话说，它决定了独立特征预测目标变量的能力。

## **过滤方法的优点**

*   过滤方法是模型不可知的
*   完全依赖数据集中的特征
*   计算速度非常快
*   基于不同的统计方法

## **过滤方法的缺点**

*   过滤方法着眼于单个特征，以确定其相对重要性。一个特性本身可能没什么用，但是当它与其他特性结合起来时，可能是一个重要的影响因素。过滤方法可能会遗漏这些特征。

## ***选择最佳特征的过滤标准***

使用选择独立特征

*   与目标变量高度相关
*   与其他独立变量相关性低
*   自变量的较高信息增益或互信息

# 特征选择的包装方法

包装器方法搜索输入要素的最佳子集来预测目标变量。它选择提供模型最佳精度的特征。包装器方法使用基于先前模型的推理来决定是否需要添加或删除新特性。

包装方法有

*   **穷举搜索:**评估输入特征的所有可能组合，以找到为所选模型提供最佳准确度的输入特征子集。当输入特征的数量变大时，计算开销非常大；
*   **正向选择:**从一个空特征集开始，并保持一次添加一个输入特征，并评估模型的准确性。这个过程一直持续到我们用预定数量的特征达到一定的精度；
*   **逆向选择:**从所有特征开始，然后保持一次移除一个特征，以评估模型的准确性。保留产生最佳准确度的特征集。

***总是在测试数据集上评估模型的准确性。***

## 优势

*   对每个输入要素之间的要素依赖性进行建模
*   取决于所选的型号
*   基于特征子集选择精度最高的模型

## 缺点:

*   计算非常昂贵，因为训练发生在每个输入特征集组合上
*   不依赖模型

# 特征选择的嵌入式方法

嵌入式方法使用过滤器和包装器特征选择方法的质量。特征选择嵌入在机器学习算法中。

过滤方法不包括学习，只涉及特征选择。包装器方法使用机器学习算法来评估特征子集，而不包含关于分类或回归函数的特定结构的知识，因此可以与任何学习机结合

嵌入式特征选择算法包括

*   **决策树**
*   **正则化——L1(拉索)和 L2(山脊)正则化**

通过拟合模型，使用这些机器学习技术。这些方法为我们提供了更好的准确性的特征重要性。

点击阅读更多关于 L1 和 L2 合法化的信息

***在下一篇文章中，我们将使用过滤方法*** 在 python 中实现一些特征选择方法

## 参考资料:

[http://ijcsit . com/docs/Volume % 202/vol 2 issue 3/ijcsit 2011020322 . pdf](http://ijcsit.com/docs/Volume%202/vol2issue3/ijcsit2011020322.pdf)

[https://arxiv.org/pdf/1907.07384.pdf](https://arxiv.org/pdf/1907.07384.pdf)

[http://people.cs.pitt.edu/~iyad/DR.pdf](http://people.cs.pitt.edu/~iyad/DR.pdf)

[https://link . springer . com/chapter/10.1007% 2f 978-3-540-35488-8 _ 6](https://link.springer.com/chapter/10.1007%2F978-3-540-35488-8_6)