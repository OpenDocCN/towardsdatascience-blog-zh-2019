<html>
<head>
<title>Do Conv-nets Dream of Psychedelic Sheep?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Conv 网会梦到迷幻的绵羊吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-conv-nets-dream-of-psychedelic-sheep-40f4d35fa146?source=collection_archive---------33-----------------------#2019-05-14">https://towardsdatascience.com/do-conv-nets-dream-of-psychedelic-sheep-40f4d35fa146?source=collection_archive---------33-----------------------#2019-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div class="gh gi jq"><img src="../Images/85d58a2ac12bfb39af4bc30e2b3dba82.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/0*oIH-91B9U7InOO6t"/></div></figure><p id="2d9b" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在连续的抽象层次上深度做梦。从上到下:输入图像，con v2–3x 3 _ reduce，inception_4c-1×1。使用 <a class="ae kw" href="https://deepdreamgenerator.com/" rel="noopener ugc nofollow" target="_blank"> <em class="kv">制作的 deepdreamgenerator </em> </a> <em class="kv">和公共领域图片来自</em> <a class="ae kw" href="https://www.flickr.com/photos/yellowstonenps/11296378705/in/album-72157647227252459/" rel="noopener ugc nofollow" target="_blank"> <em class="kv">黄石国家公园 NPS </em> </a></p><h1 id="3a36" class="kx ky it bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">深度学习的普遍存在</h1><p id="d12d" class="pw-post-body-paragraph jx jy it jz b ka lv kc kd ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku im bi translated">深度学习的现代成功部分归功于乔治·西本科、库尔特·霍尼克和其他人开发的<a class="ae kw" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>。该定理本质上陈述了具有至少一个隐藏层和非线性激活函数的神经网络通常可以逼近任意连续函数。在过去的十年里，为训练深度网络而重新使用<a class="ae kw" href="https://www.exxactcorp.com/NVIDIA-Tesla-GPU-Solutions" rel="noopener ugc nofollow" target="_blank">强大的 GPU</a>释放了通用逼近定理的潜力，催生了许多新的商业和研究领域。在深度学习模型中，许多隐藏的层可以像通天塔一样一层一层堆叠起来，内部表示可以用来表示复杂的抽象和功能层次。这些表征可以是模型的一部分，从看似无关紧要的事物中预测任何事物，如<a class="ae kw" href="https://aiweirdness.com/post/184136874667/physics-acronyms-there-was-an-attempt" rel="noopener ugc nofollow" target="_blank">无意义的天体物理学首字母缩写词</a>、流式视频偏好和约会匹配；潜在的严重后果:信用风险评级、医疗诊断和约会匹配。</p><p id="f247" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">鉴于它们的普遍性，理解深度神经网络模型如何形成输入数据的内部表示并做出决策比以往任何时候都更重要。在实践中，通过随机梯度下降的变体的反向传播确实可以很好地适应训练数据，但是不能保证全局收敛，并且输入数据和学习过程经常以令人惊讶的方式相互作用。</p><p id="984e" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><a class="ae kw" href="https://www.youtube.com/watch?v=tlOIHko8ySg" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=tlOIHko8ySg</a></p><h1 id="a40d" class="kx ky it bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">深层网络的不稳定性</h1><p id="b8ae" class="pw-post-body-paragraph jx jy it jz b ka lv kc kd ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku im bi translated">深度网络令人惊讶的学习方式导致许多人将训练和使用神经网络的整个过程称为“黑箱”，特别是在流行媒体中。当模特<a class="ae kw" href="https://www.technologyreview.com/s/612876/this-is-how-ai-bias-really-happensand-why-its-so-hard-to-fix/" rel="noopener ugc nofollow" target="_blank">表现不好</a>时，这些描述会导致糟糕的表现和公众的挫折感。</p><p id="fae1" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">如果你从事深度学习，尝试理解你的模型在想什么是值得的。理解模型并清楚地预测它们的行为不仅有助于提高模型的性能，而且社会<a class="ae kw" href="https://blog.exxactcorp.com/a-tutorial-introduction-to-privacy-centric-deep-learning/" rel="noopener ugc nofollow" target="_blank">对隐私</a>的关注以及欧洲<a class="ae kw" href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation" rel="noopener ugc nofollow" target="_blank"> GDPR </a>等法规也恰当地<a class="ae kw" href="https://www.kdnuggets.com/2018/03/gdpr-machine-learning-illegal.html" rel="noopener ugc nofollow" target="_blank">鼓励了面向公众的机器学习中增强的解释能力</a>。</p><h1 id="6fe2" class="kx ky it bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">简单的模型，简单的可视化</h1><p id="319f" class="pw-post-body-paragraph jx jy it jz b ka lv kc kd ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku im bi translated">曾几何时，神经网络的参数从数百到数千，直接检查神经元激活是可行的，并可能富有成效。神经网络活动的一个经典的直接可视化是辛顿图(还有谁？)，它通过正方形网格的大小和阴影直接显示积极和消极激活。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/93499c68342a189e59037998bb3bb463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*5vo58u7mYEJUHSho"/></div></figure><p id="55de" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">在某些情况下，可视化单个神经元的活动(使用一些其他技巧)仍然是有见地的，例如 OpenAI 的<a class="ae kw" href="https://openai.com/blog/unsupervised-sentiment-neuron/" rel="noopener ugc nofollow" target="_blank">无监督情绪神经元</a>工作或 Andrej Karpathy 等人的<a class="ae kw" href="https://arxiv.org/abs/1506.02078" rel="noopener ugc nofollow" target="_blank">可视化递归神经网络</a>(与<a class="ae kw" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">相关的博客文章</a>，大约占页面的 3/4)。然而，即使在这种情况下，找到一个有意义的可解释的神经元也是一次钓鱼探险，大多数神经元仍然是相当难以理解的。</p><h1 id="8496" class="kx ky it bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">在大型模型中构思见解</h1><p id="c710" class="pw-post-body-paragraph jx jy it jz b ka lv kc kd ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku im bi translated">你可能已经看到了谷歌的迷幻概念/深梦技术的一些迭代。深度做梦是减少神经网络黑盒神秘性的早期尝试。通过使用网络来不断增强与给定神经元的激活相关的特征，可以生成图像来获得关于在神经网络结构中如何形成表示和决策的一些想法。这可以被拟人化为网络看着一张图片，问“图片内容让我想起了什么，我怎样才能让它看起来更像这样？”运用深度梦境技术是这篇文章的标题图像的来源。在不同的层应用这种技术会产生不同层次的抽象。在标题图像中，一只大角羊在 con v2–3x 3 _ reduce 层中做梦，这在网络中很早就出现了，产生了像笔触一样的纹理。这些是你在不同方向的边缘、点和曲线检测器，倾向于被深度 conv 网络的前几层中的卷积核学习。在 inception_4c-1×1 层中稍微深入一点，我们看到与用于训练 inception 模型的类的类型相关联的特征集合怪异地覆盖在图像上。</p><p id="04f9" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">深度做梦当然获得了足够的关注，并被用作制作美丽图像的创造性工具，但深度做梦本身并没有提供多少可操作的见解。这可能是从手臂必须从哑铃中长出来的启示中得来的。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/40c5d5293f6584c8bb7dc70a942897c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/0*tAynN20Nyohu8cIs"/></div></figure><p id="e1fe" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kv">谷歌的 4.0 版让 CC 长出了粗壮的手臂。</em></p><p id="1996" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">神经网络计算机视觉中应用的其他可视化技术包括 Jason Yosinski 等人的特征可视化工具箱(<a class="ae kw" href="https://www.youtube.com/watch?v=AgkfIQ4IGaM" rel="noopener ugc nofollow" target="_blank">演示视频</a>)，如果你准备用<a class="ae kw" href="https://www.exxactcorp.com/Caffe" rel="noopener ugc nofollow" target="_blank"> Caffe </a>建立一个 Python 环境，你可以<a class="ae kw" href="https://github.com/yosinski/deep-visualization-toolbox" rel="noopener ugc nofollow" target="_blank">亲自尝试一下</a>。作者展示了几个主要属于“有趣”类别的观察结果，如对面孔、文本和褶皱织物做出反应的神经元。从“有趣的”实现到可操作的见解的点连接是可解释性开始回报时间和工具投资的地方。事实上，它本身已经成为机器学习的一个子领域，满足了对拥有可解释机器学习技能的研究人员和工程师的需求。这对你的个人或组织工具箱是一个有价值的补充。</p><h1 id="31c8" class="kx ky it bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">从可视化到可操作的效用</h1><p id="4186" class="pw-post-body-paragraph jx jy it jz b ka lv kc kd ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku im bi translated">也许对深度学习可解释性最有凝聚力的研究来自克里斯·奥拉、谷歌大脑和 OpenAI 的汇合。追溯到几年前，主要发表在蒸馏上，这项工作建立在深度梦和特征可视化的概念上，以开发基于<a class="ae kw" href="https://distill.pub/2017/feature-visualization/" rel="noopener ugc nofollow" target="_blank">优化的可视化</a>，进一步开发以理解具有<a class="ae kw" href="https://distill.pub/2018/building-blocks/" rel="noopener ugc nofollow" target="_blank">空间激活</a>的属性，并且随着<a class="ae kw" href="https://distill.pub/2019/activation-atlas/" rel="noopener ugc nofollow" target="_blank">激活图集</a>的最近出版，作者展示了对对抗示例和奇怪特征相关性的漏洞的见解。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mg"><img src="../Images/37b77ad0ed7c9cbd695678d8281857f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U5P8NYN3O6qfb_aP"/></div></div></figure><p id="3145" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kv">激活图谱是一系列研究中的最新进展，从简单的:单个神经元激活的特征可视化，到复杂的:从代表网络可能遇到的所有可能图像的流形中进行子采样的激活图谱。图片修改下一张</em> <a class="ae kw" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> <em class="kv"> CC 由 4.0 </em> </a> <em class="kv">授权来自</em> <a class="ae kw" href="https://distill.pub/2019/activation-atlas/" rel="noopener ugc nofollow" target="_blank"> <em class="kv">山卡特、克里斯奥拉等人 2019 </em> </a></p><p id="4a38" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">激活地图集与相关的<a class="ae kw" href="https://distill.pub/2018/building-blocks/" rel="noopener ugc nofollow" target="_blank">空间激活</a>部分相似，但有一个关键区别。虽然空间激活表示单个图像的模型特征提取，但是激活图谱形成所有可能图像的特征提取图。例如，不是在小狗的图像上排列松软的耳朵、可爱的噪音和毛绒绒的爪子的特征可视化，而是激活地图集将各种各样的动物鼻子排列在一起，这些鼻子最终将融入其他相关特征，如皮毛、耳朵和尾巴。</p><p id="9bc4" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated">这种神经网络特征可视化的整体视图允许以有趣的方式调查和预测模型行为。通过观察区分“水肺潜水员”和“浮潜”的特征与看起来像属于机车引擎的特征的接近程度，作者猜测他们可以从火车图像中引入一个补丁，以欺骗网络将浮潜者识别为水肺潜水员。该团队调查的其他对抗性例子包括通过添加面条将煎锅伪装成炒锅，以及通过添加棒球将灰鲸伪装成鲨鱼。</p><figure class="mb mc md me gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ml"><img src="../Images/5f2d7329cbeb69fc7c990d4724ac2ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fh47TZLRGlr2cpaM"/></div></div></figure><p id="4830" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kv">做煎锅和炒锅只需要几根面条。图片使用下一张</em> <a class="ae kw" href="https://creativecommons.org/licenses/by/4.0/" rel="noopener ugc nofollow" target="_blank"> <em class="kv"> CC 通过 4.0 </em> </a> <em class="kv">许可来自</em> <a class="ae kw" href="https://distill.pub/2019/activation-atlas/" rel="noopener ugc nofollow" target="_blank"> <em class="kv">山卡特、克里斯奥拉等人 2019 </em> </a></p><h1 id="e732" class="kx ky it bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">向前发展的可解释价值</h1><p id="54b9" class="pw-post-body-paragraph jx jy it jz b ka lv kc kd ke lw kg kh ki lx kk kl km ly ko kp kq lz ks kt ku im bi translated">总之，自《深度梦》以来，可解释性领域已经发展了很多，并开始提供可操作的见解。用激活地图集生成的对抗性例子的类型依赖于全网络特征可视化和人类直觉的巧妙结合。随着深度学习网络继续部署到新的领域，并在现有领域中获得更大的采用，它们将被认为更负责任，更可预测地可靠。在这种情况下，对可解释性工具和从业者产生的需求和价值只会增加。</p></div><div class="ab cl mm mn hx mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="im in io ip iq"><p id="bfe3" class="pw-post-body-paragraph jx jy it jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku im bi translated"><em class="kv">原载于 2019 年 5 月 14 日</em><a class="ae kw" href="https://blog.exxactcorp.com/conv-nets-visualization-interpretation" rel="noopener ugc nofollow" target="_blank"><em class="kv">https://blog.exxactcorp.com</em></a><em class="kv">。</em></p></div></div>    
</body>
</html>