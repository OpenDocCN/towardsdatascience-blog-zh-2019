<html>
<head>
<title>Difference between Local Response Normalization and Batch Normalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">局部响应标准化和批量标准化的区别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac?source=collection_archive---------2-----------------------#2019-06-19">https://towardsdatascience.com/difference-between-local-response-normalization-and-batch-normalization-272308c034ac?source=collection_archive---------2-----------------------#2019-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ed12" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">深度神经网络中使用的不同规范化技术的简短教程。</h2></div><h1 id="7363" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">为什么要正常化？</h1><p id="853d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">归一化对于深度神经网络已经变得很重要，深度神经网络补偿某些激活函数的无界性质，例如 ReLU、eLU 等。使用这些激活函数，输出层不会被限制在一个有限的范围内(例如[-1，1]代表<em class="lt"> tanh </em>)，而是可以增长到训练允许的高度。为了限制无限制激活增加输出图层值，在激活函数之前使用了归一化。深度神经网络中使用了两种常见的规范化技术，并且经常被初学者误解。在本教程中，将讨论这两种规范化技术的详细解释，突出它们的主要区别。</p><h1 id="c864" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">局部响应标准化</h1><p id="dd08" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">本地响应规范化(LRN)首先在 AlexNet 架构中引入，其中使用的激活函数是<em class="lt"> ReLU </em>，而不是当时更常见的<em class="lt"> tanh </em>和<em class="lt"> sigmoid </em>。除了上述原因，使用 LRN 的原因是为了促进<em class="lt">侧抑制。</em>它是神经生物学中的一个概念，指一个神经元降低其邻居活动的能力[1]。在 DNNs 中，这种横向抑制的目的是执行局部对比度增强，使得局部最大像素值被用作下一层的激励。</p><p id="d69c" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">LRN 是一个<strong class="kz ir">不可训练层</strong>，它在局部邻域内对特征图中的像素值进行平方归一化。基于所定义的邻域，有两种类型的 LRN，如下图所示。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/841308453b0785ec9799847d4205e583.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFl0tPjwvc49HirAJZPhEA.png"/></div></div></figure><p id="7506" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated"><strong class="kz ir">通道间 LRN: </strong>这是 AlexNet paper 最初使用的。定义的邻域是穿过通道的<strong class="kz ir">。对于每个(x，y)位置，归一化是在深度维度上进行的，并由以下公式给出</strong></p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ml"><img src="../Images/60f1903efb0d8a9c3fc90a923060a552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXGTZuvplcGpyE8DuP4B2w.png"/></div></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">LRN used in AlexNet [2]</figcaption></figure><p id="9315" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">其中<em class="lt"> i </em>表示滤波器 I 的输出，<em class="lt"> a(x，y)，b(x，y) </em>分别为归一化前后<em class="lt"> (x，y) </em>位置的像素值，N 为通道总数。常数<em class="lt"> (k，α，β，n) </em>是超参数。<em class="lt"> k </em>用于避免任何奇点(被零除)，<em class="lt"> α </em>用作归一化常数，而<em class="lt"> β </em>是对比常数。常数<em class="lt"> n </em>用于定义邻域长度，即在执行归一化时需要考虑多少个连续的像素值。(<em class="lt"> k，α，β，n)=(0，1，1，N) </em>的情况为标准归一化)。在上图中，当 N=4 时，N 等于 2。</p><p id="7ea1" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">让我们看一个通道间 LRN 的例子。请看下图</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mq"><img src="../Images/d6013abfb9fff7e388b0c03a42825fd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmnOhSTIzn04sC0w1d3FPg.png"/></div></div></figure><p id="8ceb" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">不同的颜色表示不同的通道，因此 N=4。假设超参数为(<em class="lt"> k，α，β，n)=(0，1，1，2)。</em><em class="lt">n = 2</em>的值意味着在计算位置<em class="lt"> (i，x，y) </em>的归一化值时，我们考虑上一个和下一个滤波器在相同位置的值，即<em class="lt"> (i-1，x，y) </em>和<em class="lt"> (i+1，x，y) </em>。对于<em class="lt"> (i，x，y)=(0，0，0) </em>我们有<em class="lt">值(I，x，y)=1 </em>，<em class="lt">值(i-1，x，y) </em>不存在，<em class="lt">值(i+，x，y)=1 </em>。因此<em class="lt"> normalized_value(i，x，y) = 1/( + ) = 0.5 </em>，可以在上图的下半部分看到。其余的标准化值以类似的方式计算。</p><p id="04ac" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated"><strong class="kz ir">信道内 LRN: </strong>在信道内 LRN 中，邻域仅在同一信道内扩展，如上图所示。该公式由下式给出</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mr"><img src="../Images/aebb13be8d2cf0e653f22138dde8eb6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-19IMI2wJVDtaz4Uf4dRog.png"/></div></div></figure><p id="8aa1" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">其中(W，H)是特征图的宽度和高度(例如上图中的(W，H) = (8，8))。通道间和通道内 LRN 的唯一区别是归一化的邻域。在通道内 LRN 中，在所考虑的像素周围定义了 2D 邻域(与通道间的 1D 邻域相反)。例如，下图显示了在 n=2(即大小为(n+1)x(n+1)的 2D 邻域以(x，y)为中心)的 5×5 特征图上的通道内归一化。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ms"><img src="../Images/41ba8f9cf68b94b30f1ef4380af53ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vFC3KU-wQPG1ZnXFYC_xPQ.png"/></div></div></figure><h1 id="54b4" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">批量标准化:</h1><p id="99a9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">批量标准化(BN)是一个<strong class="kz ir">可训练层</strong>，通常用于解决<strong class="kz ir"> <em class="lt">内部协变量移位(ICF)</em></strong><em class="lt">【1】的问题。</em> ICF 的产生是由于隐藏神经元分布的改变/激活。考虑下面这个二元分类的例子，我们需要对玫瑰和非玫瑰进行分类</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/2729c780b9a0030b3265aa9aaefe34bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gUOjaIspVsz-PVxLDLQGQA.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Roses vs No-roses classification. The feature map plotted on the right have different distributions for two different batch sampled from the dataset [1]</figcaption></figure><p id="6b9a" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">假设我们已经训练了一个神经网络，现在我们从数据集中选择两个明显不同的批次进行推断(如上所示)。如果我们对这两个批次进行正向传递，并绘制隐藏层(网络深处)的特征空间，我们将看到分布的显著变化，如上图右侧所示。这称为输入神经元的<strong class="kz ir"> <em class="lt">协变移位</em> </strong>。这在训练时有什么影响？在训练期间，如果我们选择属于不同分布的批次，那么它会减慢训练，因为对于给定的批次，它会尝试学习某个分布，而该分布对于下一批次是不同的。因此，它不断地在分布之间来回跳跃，直到它收敛。这个<strong class="kz ir"> <em class="lt">协变偏移</em> </strong>可以通过确保一个批次内的成员不属于相同/相似的分布来减轻。这可以通过随机选择批量图像来完成。隐藏神经元也存在类似的协变量移位。即使这些批次是随机选择的，隐藏的神经元也可能最终具有某种分布，从而减慢训练速度。隐藏层的这种协变量移位称为内部协变量移位。问题是我们不能像对输入神经元那样直接控制隐藏神经元的分布，因为它会随着训练更新训练参数而不断变化。批处理规范化有助于缓解这个问题。</p><p id="7d8d" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在批量标准化中，隐藏神经元的输出在被馈送到激活函数之前以下面的方式被处理。</p><ol class=""><li id="2acc" class="mu mv iq kz b la lu ld lv lg mw lk mx lo my ls mz na nb nc bi translated">将整批<em class="lt"> B </em>标准化为零均值和单位方差</li></ol><ul class=""><li id="699a" class="mu mv iq kz b la lu ld lv lg mw lk mx lo my ls nd na nb nc bi translated">计算整个小批量产量的平均值:<em class="lt"> u_B </em></li><li id="d32e" class="mu mv iq kz b la ne ld nf lg ng lk nh lo ni ls nd na nb nc bi translated">计算整个小批量产量的方差:s <em class="lt"> igma_B </em></li><li id="8259" class="mu mv iq kz b la ne ld nf lg ng lk nh lo ni ls nd na nb nc bi translated">通过减去平均值并除以方差来标准化小批量</li></ul><p id="f34e" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">2.引入两个可训练参数(<em class="lt"> Gamma: </em> scale_variable 和<em class="lt"> Beta: </em> shift_variable)来缩放和移动标准化小批量输出</p><p id="45e8" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">3.将该缩放和移位的标准化小批量馈送到激活功能。</p><p id="ff55" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">BN 算法可以在下图中看到。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c976c9a44dd5443d52f1512422b15927.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*Hiq-rLFGDpESpr8QNsJ1jg.png"/></div><figcaption class="mm mn gj gh gi mo mp bd b be z dk">Batch Normalization Algorithm [2]</figcaption></figure><p id="f73b" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">在一批所有激活中对每个像素进行归一化。考虑下图。让我们假设我们有一个 3 号的小批量。隐藏层产生大小为(C，H，W) = (4，4，4)的激活。由于批量大小是 3，我们将有 3 个这样的激活。现在，对于激活中的每个像素(即，对于每个 4x4x4=64 像素)，我们将通过找到所有激活中该像素位置的平均值和方差来对其进行归一化，如下图的左部所示。一旦找到平均值和方差，我们将从每次激活中减去平均值，然后除以方差。下图的右半部分描述了这一点。减法和除法是逐点进行的。(如果你习惯 MATLAB，除法就是点分<strong class="kz ir">。/ </strong>)。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nk"><img src="../Images/432707f9e6d93e2acb761bb89ce42ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PgUwNzUYs2_Sp5nrPfSZ5g.jpeg"/></div></div></figure><p id="cc2e" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">步骤 2(即缩放和移位)的原因是让训练决定我们是否需要归一化。在某些情况下，不进行规范化可能会产生更好的结果。因此，BN 不是预先选择是否包括归一化层，而是让训练来决定。当<em class="lt"> Gamma = sigma_B </em>和<em class="lt"> Beta = u_B </em>时，不进行归一化，恢复原始激活。吴恩达在 BN 上的一个非常好的视频教程可以在这里找到</p><h1 id="c20d" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">比较:</h1><p id="2b82" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">LRN 有多个方向来跨(通道间或通道内)执行归一化，另一方面，BN 只有一种执行方式(针对所有激活中的每个像素位置)。下表比较了这两种标准化技术。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi nm"><img src="../Images/9d2df25a8d828b432cc3bf386ce70992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J7rxGz1f_2YWjdcsvqNCNA.png"/></div></div></figure><p id="4185" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated"><strong class="kz ir">参考文献:</strong></p><p id="ff58" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">[1]<a class="ae nl" href="https://www.learnopencv.com/batch-normalization-in-deep-networks/" rel="noopener ugc nofollow" target="_blank">https://www . learnopencv . com/batch-normalization-in-deep-networks/</a></p><p id="b89c" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">[2]约菲、谢尔盖和克里斯蒂安·塞格迪。"批量标准化:通过减少内部协变量转移加速深度网络训练."<em class="lt"> arXiv 预印本 arXiv:1502.03167 </em> (2015)。</p><h1 id="88f4" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">奖金:</h1><p id="f2a1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">可以在下面的链接中找到这个主题和机器学习中许多其他重要主题的紧凑备忘单</p><div class="nn no gp gr np nq"><a href="https://medium.com/swlh/cheat-sheets-for-machine-learning-interview-topics-51c2bc2bab4f" rel="noopener follow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd ir gy z fp nv fr fs nw fu fw ip bi translated">机器学习面试主题的备忘单</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">ML 面试的视觉备忘单(www.cheatsheets.aqeel-anwar.com)</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">medium.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe mj nq"/></div></div></a></div></div><div class="ab cl of og hu oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="ij ik il im in"><p id="09b7" class="pw-post-body-paragraph kx ky iq kz b la lu jr lc ld lv ju lf lg lw li lj lk lx lm ln lo ly lq lr ls ij bi translated">如果这篇文章对你有帮助，欢迎鼓掌、分享和回复。如果想了解更多关于机器学习和数据科学的知识，请关注我@<a class="om on ep" href="https://medium.com/u/a7cc4f201fb5?source=post_page-----272308c034ac--------------------------------" rel="noopener" target="_blank"><strong class="kz ir">Aqeel an war</strong></a><strong class="kz ir">或者在</strong><a class="ae nl" href="https://www.linkedin.com/in/aqeelanwarmalik/" rel="noopener ugc nofollow" target="_blank"><strong class="kz ir"><em class="lt">LinkedIn</em></strong></a><strong class="kz ir"><em class="lt">上与我联系。</em> </strong></p></div></div>    
</body>
</html>