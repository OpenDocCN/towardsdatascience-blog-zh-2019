<html>
<head>
<title>A mental model for applied NLP engineers: Tf-IDF to Language models in one diagram</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向应用型自然语言处理工程师的心智模型:一张图中语言模型的 Tf-IDF</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/all-you-need-to-know-about-nlp-based-predictive-modeling-in-one-diagram-7be3b547454a?source=collection_archive---------20-----------------------#2019-09-14">https://towardsdatascience.com/all-you-need-to-know-about-nlp-based-predictive-modeling-in-one-diagram-7be3b547454a?source=collection_archive---------20-----------------------#2019-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2592" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理清 NLP 预测建模场景</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/42da1ab54c50ca1db8e13d7966012510.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3eYG7-TliMY5pyrkARENcw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image credit: <a class="ae ky" href="https://www.um.edu.mt/" rel="noopener ugc nofollow" target="_blank">https://www.um.edu.mt</a></figcaption></figure><h2 id="858a" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">秘诀:5 分钟后你将成为职业选手，</strong></h2><ol class=""><li id="c830" class="lv lw it lx b ly lz ma mb li mc lm md lq me mf mg mh mi mj bi translated">At 从自然语言数据集设计特征以构建经典的 ML 模型</li><li id="ca5a" class="lv lw it lx b ly mk ma ml li mm lm mn lq mo mf mg mh mi mj bi translated">在构建嵌入时，从复杂的自然语言数据集中学习酷的表示，以构建 DNN 模型。</li></ol><p id="d084" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">NLP 作为一个领域正在以曲速前进。神经模型对 NLP 的干预的好处是，它在许多任务中推动了极限，但坏处是，它使试图进入该领域的新人的学习场景变得复杂。我觉得有义务通过分享我用来推理基于 NLP 的预测任务的<strong class="lx iu">心智模型</strong>来带来一些清晰。正如标题中所承诺的，我已经试图在一个单一的图表中传达我想要传达的一切，请将这个心智模型视为指南针，而不是地图。<em class="ne">话虽如此，请注意这篇文章并不适合绝对的初学者。</em></p><p id="01bf" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">在进入这篇文章的实质之前，请允许我设置一下背景，澄清一些术语的细微差别和覆盖范围。</p><h2 id="7a20" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">术语</strong></h2><p id="5a0b" class="pw-post-body-paragraph mp mq it lx b ly lz ju ms ma mb jx mu li nf mw mx lm ng mz na lq nh nc nd mf im bi translated">心智模型中将使用的术语的快速注释。</p><p id="d4f5" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">为了更容易理解，我特意使用了<strong class="lx iu"/>而不是<strong class="lx iu">文本或者“字符”或者“短语”</strong>作为下面心智模型中的战术单位，因为 a .语言是组合的，单词是最低的战术单位，可以有各种组合(与字符或其他任何东西相比); B:大多数 NLP 文献在讨论 NLP 问题和解决方案时都使用<strong class="lx iu">“单词”</strong>作为默认选择。单词 N-grams 可以表示一个单词(当 N=1 时为 1-gram 或 unigram)、一个单词对(当 N=2 时为 bigram)等等。这同样适用于 char N-grams。</p><p id="4cdc" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated"><strong class="lx iu"> <em class="ne">嵌入:</em> </strong></p><p id="49b9" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">虽然我们知道在机器学习中“嵌入”是一种奇特的说法，即一种获取原始文本数据并将其“嵌入”到向量空间中的技术。因为当在向量空间中呈现训练数据时，机器学习算法是“最快乐的”。原因并不奇怪:我们可以做向量数学或者计算向量之间的距离等等。<strong class="lx iu"> <em class="ne">(承蒙:来自 Quora.com 的斯里达尔·马哈德万教授)</em> </strong></p><p id="4e57" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">但是从另一个层面来说，学习嵌入只不过是学习底层文本语料库的表示。单词嵌入是一种<strong class="lx iu">上下文无关的嵌入或表示</strong>(这里的上下文是指语用上下文，而不是语义上下文)，像 ELMo、BERT 这样的语言模型学习一种<strong class="lx iu">上下文相关的嵌入或表示</strong>。查看这个 G <a class="ae ky" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank"> oogle BERT 帖子</a>获得更多更深入的见解。</p><p id="a3b2" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated"><strong class="lx iu"> <em class="ne">语义语境</em> </strong></p><blockquote class="ni"><p id="9616" class="nj nk it bd nl nm nn no np nq nr mf dk translated">“一个词由它的朋友来描述”——J . r .弗斯</p></blockquote><p id="a1ba" class="pw-post-body-paragraph mp mq it lx b ly ns ju ms ma nt jx mu li nu mw mx lm nv mz na lq nw nc nd mf im bi translated"><strong class="lx iu">语义上下文</strong>是一个记录良好的概念，但这里有一个快速回顾。语言是象征性的:单词<strong class="lx iu">汉堡包</strong>和<strong class="lx iu">奶昔</strong>在语义上<strong class="lx iu">与<strong class="lx iu">“食物”</strong>相关，但我们需要一种技术将这些知识输入机器。分布假说粗略地指出，出现在相似语境中的词往往具有相似的含义。因此，可以认为汉堡包和奶昔通常出现在文本语料库中的食物上下文中。这被称为语义上下文，算法需要找出语义相关性的概念。</strong></p><p id="7549" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated"><strong class="lx iu"> <em class="ne">语用语境(或用法语境)</em> </strong></p><p id="dadb" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">语义语境忽略了所有的语用语境。我这么说是什么意思？让我们来考虑这个例子:<em class="ne">他去监狱</em> <strong class="lx iu"> <em class="ne">牢房</em> </strong> <em class="ne">用他的</em> <strong class="lx iu"> <em class="ne">牢房</em> </strong> <em class="ne">手机去</em> <strong class="lx iu"> <em class="ne">牢房</em> </strong> <em class="ne">提取犯人的血样</em> <strong class="lx iu"> <em class="ne">(礼貌用语:</em></strong><a class="ae ky" href="https://www.quora.com/profile/Ajit-Rajasekharan" rel="noopener ugc nofollow" target="_blank"><strong class="lx iu"><em class="ne">Ajit Rajasekharan<em class="ne">考虑语义和语用上下文的嵌入将为单元的三个向量生成不同的向量。例如，第一个牢房(监狱牢房案件)更接近监禁、犯罪等词。而第二个“手机”(手机壳)更接近于 iPhone、android、galaxy 等词..</em></em></strong></a></p><h2 id="df15" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">覆盖范围</strong></h2><p id="d4d7" class="pw-post-body-paragraph mp mq it lx b ly lz ju ms ma mb jx mu li nf mw mx lm ng mz na lq nh nc nd mf im bi translated">这个心智模型涵盖了所有基于 NLP 的预测任务，这些任务可以被构造为<strong class="lx iu"> N:1 </strong>监督学习任务，即输入是任意长度(N)的文本序列，输出是标签或分数。</p><p id="104e" class="pw-post-body-paragraph mp mq it lx b ly mr ju ms ma mt jx mu li mv mw mx lm my mz na lq nb nc nd mf im bi translated">这个心智模型<strong class="lx iu">不包括基于</strong> NLP 的任务，这些任务采取<strong class="lx iu"> N: N 或 N: M </strong>监督学习任务的形式，其中输入和输出分别是任意长度(N)的文本序列或任意长度 N 和 M 的输入和输出文本序列。例如，N: N 的最佳示例是<strong class="lx iu">序列标记(NER，词性)，自动语音识别</strong>，N: M 的典型示例是<strong class="lx iu">机器翻译和文本摘要</strong>。</p><h2 id="040b" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">故意遗漏</h2><p id="eee7" class="pw-post-body-paragraph mp mq it lx b ly lz ju ms ma mb jx mu li nf mw mx lm ng mz na lq nh nc nd mf im bi translated">如果你仔细观察，我故意省略了数据清理和准备，因为它们是非常领域和任务特定的。因此，当您应用这个心智模型时，请确保您在正确的时间注入了适当的数据清理和准备技术。</p><h1 id="2e80" class="nx la it bd lb ny nz oa le ob oc od lh jz oe ka ll kc of kd lp kf og kg lt oh bi translated">心理模型</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/dae0e9072bb1473de2431b3a670129b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fuKYhDkyPBY4f559OeftRA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by the author</figcaption></figure><h2 id="c3a6" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">一些经验数据</h2><p id="0bd3" class="pw-post-body-paragraph mp mq it lx b ly lz ju ms ma mb jx mu li nf mw mx lm ng mz na lq nh nc nd mf im bi translated">您可以看到，对于较小的数据集，经典 ML 工作得很好，但对于较大的数据集，它们就逐渐消失了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/46f69566fced55897f8988351bc75276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XV-KSH9XXAQhyqJ-MI3YPA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by the author (Screenshot from the research paper)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/a9d79743af7c406fa1fe26cb7f8f36ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FZD4VAxKe-lYC0G_Os9C1g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Image by the author (Screenshot from the research paper)</figcaption></figure><h2 id="88ae" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">一些代码来玩</h2><ol class=""><li id="e1fd" class="lv lw it lx b ly lz ma mb li mc lm md lq me mf mg mh mi mj bi translated"><a class="ae ky" href="https://www.kaggle.com/prithiviraj/for-beginners-tfidf-logistic-regression#_=_" rel="noopener ugc nofollow" target="_blank"> TFiDF +逻辑回归，中等规模数据集</a></li><li id="d317" class="lv lw it lx b ly mk ma ml li mm lm mn lq mo mf mg mh mi mj bi translated">定制单词嵌入+ <a class="ae ky" href="https://nbviewer.jupyter.org/github/PrithivirajDamodaran/NLP-Experiments/blob/master/Seq%20Models/Sequence_based_Text_Classification_.ipynb" rel="noopener ugc nofollow" target="_blank"> BiLSTM，小型数据集</a></li><li id="3efa" class="lv lw it lx b ly mk ma ml li mm lm mn lq mo mf mg mh mi mj bi translated"><a class="ae ky" href="https://www.kaggle.com/prithiviraj/cooking-dl-notebook" rel="noopener ugc nofollow" target="_blank">定制单词嵌入+ CNN + BiLSTM，大型数据集</a></li></ol><h2 id="1da3" class="kz la it bd lb lc ld dn le lf lg dp lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated"><strong class="ak">进一步阅读</strong></h2><ol class=""><li id="41c5" class="lv lw it lx b ly lz ma mb li mc lm md lq me mf mg mh mi mj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/canning-the-cant-fun-with-homonyms-and-word-vectors-179ab58c76d2">更多关于语义、语用和句法的语境。</a></li><li id="86b4" class="lv lw it lx b ly mk ma ml li mm lm mn lq mo mf mg mh mi mj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/gmail-style-smart-compose-using-char-n-gram-language-models-a73c09550447">什么是语言模型？</a></li></ol></div></div>    
</body>
</html>