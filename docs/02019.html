<html>
<head>
<title>Some examples of applying BERT in specific domain</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在特定领域应用 BERT 的几个例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-apply-bert-in-scientific-domain-2d9db0480bd9?source=collection_archive---------5-----------------------#2019-04-03">https://towardsdatascience.com/how-to-apply-bert-in-scientific-domain-2d9db0480bd9?source=collection_archive---------5-----------------------#2019-04-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9715" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在特定领域中应用 BERT</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/06dcffb2e76e21481a151ab5409f8d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h_pcxOdgwaFn2oSO"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@hikendal?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Kendal James</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="dcb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2018 年发布了几个新的预训练的情境化嵌入。新的最先进的结果每个月都在变化。伯特是著名的模特之一。在这个故事中，我们将扩展 BERT 来看看我们如何在不同领域问题上应用 BERT。</p><p id="1c06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">艾伦人工智能研究所(AI2)对 BERT 进行了进一步研究，并发布了基于 BERT 的 SciBERT，以解决科学数据上的性能问题。它使用来自 BERT 的预训练模型，并通过使用科学出版物(其中 18%的论文来自计算机科学领域，82%来自广泛的生物医学领域)来微调上下文化嵌入。另一方面，Lee 等人从事生物医学领域的工作。他们还注意到，通用的预训练 NLP 模型在特定领域数据中可能不会很好地工作。因此，他们将 BERT 微调为 BioBERT，在生物医学的 NER、关系抽取和问答 NLP 任务中有 0.51% ~ 9.61%的绝对提升。</p><p id="6f83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事将讨论关于<a class="ae ky" href="https://arxiv.org/pdf/1903.10676.pdf" rel="noopener ugc nofollow" target="_blank"> SCIBERT:科学文本的预训练语境化嵌入</a> (Beltagy 等人，2019)，<!-- --> BioBERT:生物医学文本挖掘的预训练生物医学语言表示模型<!-- --> (Lee 等人，2019)。将涵盖以下内容:</p><ul class=""><li id="26cb" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">数据</li><li id="daad" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">体系结构</li><li id="b6b6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实验</li></ul><h1 id="7941" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据</h1><p id="570c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">SciBERT 和 BioBERT 还为预训练引入了特定领域的数据。Beltag 等人使用从语义学者中随机挑选 1.14M 的论文来微调 BERT 并构建 SciBERT。语料库包括 18%的计算机科学领域的论文和 82%的广义生物医学领域的论文。另一方面，Lee 等人使用 BERT 的原始训练数据来微调 BioBERT 模型，这些数据包括英语维基百科和图书语料库以及领域特定数据，即 PubMed 摘要和 PMC 全文文章。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/9904094687e9a743860a35bc517bc7ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ocMEJtFMDYPG_3AiFsMng.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Training data among models</figcaption></figure><p id="2f5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些变化被应用在科技文章中以获得成功。ScispaCy 作为 spaCy 的科学专用版本，用于将文档拆分成句子。之后，Beltagy 等人使用<a class="ae ky" href="https://aclweb.org/anthology/D18-2012" rel="noopener ugc nofollow" target="_blank"> SentencePiece </a>库为 SciBERT 构建新的单词表，而不是使用 BERT 的词汇表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/998316c598807658a098714ff7b1e130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MhNie2aPLIid3hq_uh0qqg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Different tokenization method among models.</figcaption></figure><h1 id="3f41" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">体系结构</h1><p id="e03a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">SciBERT 和 BioBERT 都遵循 BERT 模型架构，该架构是多双向转换器，并通过预测屏蔽令牌和下一句来学习文本表示。标记序列将被转换成标记嵌入、分段嵌入和位置嵌入。标记嵌入指的是语境化的单词嵌入，片段嵌入只包括 2 个嵌入，它们或者是 0 或者是 1 来表示第一个句子和第二个句子，位置嵌入存储相对于序列的标记位置。你可以访问这个<a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">故事</a>来了解更多关于伯特的事情。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/9d3a7b8db35d78dd2aee440eb41a56c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*q2OjkfFaUb8L_g6K6PGlWQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">BioBERT Architecture (Lee et al., 2019)</figcaption></figure><h1 id="4bfe" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">实验</h1><h2 id="a086" class="nj mk it bd ml nk nl dn mp nm nn dp mt li no np mv lm nq nr mx lq ns nt mz nu bi translated">科学伯特(SciBERT)</h2><p id="4b33" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">命名实体识别(NER)和参与者干预比较结果提取(PICO)都是序列标记。依存句法分析(DEP)是预测句子中标记之间的依存关系。分类(CLS)和关系分类(REL)是分类任务。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/fedbbc22b5fc915c1abc9785e5a7c054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*2pnsByvB2pDy5dmIgHUxxQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Evaluation dataset (Beltagy et al., 2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/2a29a646f5473f27e4f209875ae94ade.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQMB5JSBrQM9Rey2ZVu_iQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Evaluation result among different dataset (Beltagy et al., 2019)</figcaption></figure><h2 id="7d91" class="nj mk it bd ml nk nl dn mp nm nn dp mt li no np mv lm nq nr mx lq ns nt mz nu bi translated">生物医学伯特</h2><p id="00bf" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">从下表中，你可以注意到 BioBERT 在特定领域数据集上的表现优于 BERT。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/7739e4bc1240db2e54fd806a318759ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SWk4NQh2p9rl0chBQd9dg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Comparison between BERT and BioBERT on relation extraction dataset (Lee et al., 2019)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c3b122368bc931930940d43082d1d70d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*2vBK0HV-QqrGS_tbEM5tXA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Comparison between BERT and BioBERT on question answering dataset (Lee et al., 2019)</figcaption></figure><h1 id="c77b" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">拿走</h1><ul class=""><li id="b0aa" class="lv lw it lb b lc nb lf nc li nz lm oa lq ob lu ma mb mc md bi translated">通用的预训练模型在特定领域可能无法达到最先进的结果。因此，需要微调步长来提高目标数据集的性能。</li><li id="1a19" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在伯特和伯特的基础模型之后，Transformer(多重自我关注)变得越来越有名。我还注意到，BERT 的基本模型保持了最先进的性能。</li></ul><h1 id="94ec" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。欢迎在<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，或者在<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener"> Medium </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="be7f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">延伸阅读</h1><ul class=""><li id="2d9f" class="lv lw it lb b lc nb lf nc li nz lm oa lq ob lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb">变压器的双向编码器表示(BERT) </a></li><li id="0972" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" href="https://github.com/allenai/scibert" rel="noopener ugc nofollow" target="_blank">西伯特 GIT 回购</a></li><li id="908a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">BioBERT GIT 回购<a class="ae ky" href="https://github.com/naver/biobert-pretrained" rel="noopener ugc nofollow" target="_blank">【1】</a>/<a class="ae ky" href="https://github.com/dmis-lab/biobert" rel="noopener ugc nofollow" target="_blank">【2】</a></li></ul><h1 id="1470" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><ul class=""><li id="7470" class="lv lw it lb b lc nb lf nc li nz lm oa lq ob lu ma mb mc md bi translated">贝尔塔吉、科汉和罗。<a class="ae ky" href="https://arxiv.org/pdf/1903.10676.pdf" rel="noopener ugc nofollow" target="_blank"> SCIBERT:科学文本的预训练语境化嵌入</a>。2019</li><li id="3e67" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">J.李，尹文伟，金圣贤，金圣贤，苏春河和姜俊杰。<a class="ae ky" href="https://arxiv.org/pdf/1901.08746.pdf" rel="noopener ugc nofollow" target="_blank"> BioBERT:用于生物医学文本挖掘的预训练生物医学语言表示模型</a>。2019</li></ul></div></div>    
</body>
</html>