<html>
<head>
<title>Ensemble Learning case study: Running XGBoost on Google Colab free GPU</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习案例研究:在 Google Colab 免费 GPU 上运行 XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-xgboost-on-google-colab-free-gpu-a-case-study-841c90fef101?source=collection_archive---------13-----------------------#2019-10-22">https://towardsdatascience.com/running-xgboost-on-google-colab-free-gpu-a-case-study-841c90fef101?source=collection_archive---------13-----------------------#2019-10-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="4be0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">本文是一个案例研究的第二部分，我们正在探索 1994 年人口普查收入数据集。在<a class="ae ko" rel="noopener" target="_blank" href="/ensemble-learning-and-model-interpretability-a-case-study-95141d75a96c">的第一部分</a>中，我们深入研究了数据集，比较了一些集成方法的性能，然后探索了一些有助于模型可解释性的工具。</p><p id="958e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在第二部分中，我们将探索一种叫做<strong class="js iu">梯度增强</strong>的技术和<strong class="js iu">谷歌合作实验室</strong>，这是一个免费的 Jupyter 笔记本环境，不需要设置，完全在云中运行。</p><h1 id="aee3" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">什么是梯度增强和 XGBoost？</h1><p id="8db1" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">梯度推进是一种集成方法，很像我们上次讨论的打包和粘贴。然而，Boosting 与前面提到的方法的不同之处在于它如何进行这种模型组合。它使用技术将几个<em class="ls">弱学习者</em>组合成一个<em class="ls">强学习者。</em></p><p id="6bcd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">其他集成方法依赖于同时构建几个孤立的基础学习器，然后用于进行预测，而 Boosting 算法依赖于顺序过程，其中每个模型试图纠正以前模型的错误。这导致了一个主要的效率缺陷，因为模型是一次创建一个，并且依赖于先前迭代获得的结果</p><p id="1bd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">梯度推进采用梯度下降算法来最小化顺序模型中的误差。因此，本质上，它是一个优化问题，目标是最小化误差(损失)函数。</p><p id="3bd5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XGBoost(极限梯度增强)反过来也是梯度增强算法的优化实现。它的特点(除了其他增强):并行树构建，缓存感知访问，稀疏感知，正则化(L1 和 L2)，加权分位数草图。该库已经成为近年来赢得许多数据科学竞赛的首选解决方案之一。</p><h1 id="0f92" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">一些 XGBoost 超参数</h1><p id="4acd" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">原作者将 XGBoost 超参数分为 3 类:</p><ul class=""><li id="7da2" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn ly lz ma mb bi translated"><strong class="js iu">通用参数</strong>:控制算法整体功能的超参数；</li><li id="1242" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">助推器参数:</strong>在算法的每一步控制单个助推器(树或回归)的超参数；</li><li id="8cf9" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">学习任务参数:</strong>配置要执行的优化的超参数；</li></ul><p id="e02c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XGBoost 最大的优势之一是可定制的数量。要查看可用超参数的完整列表(它非常广泛)，我强烈建议您查看项目文档页面。</p><p id="f27e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XGBoost 提供了一个 scikit-learn 兼容的 API，一些参数的名称略有不同，但它们的工作方式与常规库 API 相同。</p><p id="f11f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们将简要回顾一些将在案例研究中调整的超参数，以便更好地理解它们:</p><ul class=""><li id="a241" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn ly lz ma mb bi translated"><strong class="js iu"> booster: </strong>允许我们选择要使用的每个 booster:GB tree，用于基于树的模型，或者 gblinear，用于线性模型。</li><li id="65a2" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">目标</strong>:是学习任务超参数的一部分，它规定了优化过程中要使用的学习任务(回归、分类、排序等)和函数。</li><li id="4873" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> tree_method: </strong>我们将使用选项“gpu_exact”在 gpu 上运行</li><li id="1436" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> eval_metric: </strong>用于评估训练数据性能的指标。我们可以以 python 列表的格式传递多个指标，因此，我们将在案例研究中使用“error”(二进制分类错误)和“auc”。</li><li id="9442" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> learning_rate (eta): </strong>在每一步树提升之后，对新增加的权重进行缩放。这种技术被称为“收缩”，负责减少每棵树的影响，为未来的树留下空间来改进模型。通常，发现低于 0.1 的 learning_rates 产生更好的泛化误差。</li><li id="536e" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> gamma: </strong>当产生的分裂产生损失函数的正减少时，树节点被分裂。gamma 参数指定执行这种分割所需的最小缩减。它的值取决于所使用的损失函数。</li><li id="df69" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> max_depth: </strong>限制树木允许生长的最大深度。较大的树容易过度生长。</li><li id="0e16" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> colsample_bytree: </strong>设置拟合每棵树时随机使用的要素(数据集列)的分数。</li><li id="d94a" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">子样本:</strong>设置拟合每棵树时随机使用的观察值(数据集行)的分数。</li><li id="383e" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> reg_alpha: </strong>控制 L1 正则化。由于 L1 正则化倾向于将权重拉至零，因此当使用线性增强器或高维数据时，更推荐使用 L1 正则化。</li><li id="087c" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated">reg_lambda: 控制 L2 正则化。由于 L2 正则化鼓励较低的权重(但不一定是 0)，它可以用于树助推器。</li><li id="04a4" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu"> n_estimators: </strong>要拟合的树的数量</li></ul><p id="3ac3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">正如您所看到的，有多个超参数需要优化，上面给出的这些参数甚至还没有接近全部参数。</p><h1 id="96ed" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">评估 XGBoost 性能</h1><p id="6b6a" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">首先，我们将安装一个<em class="ls">现成的</em> XGBoost 分类器，以获得对模型性能的基本了解，这意味着我们不会调整模型的大多数参数。</p><p id="b34d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">XGBoost 库实现了两个用于模型训练的主要 API:默认的<strong class="js iu">学习 API </strong>、<strong class="js iu"> </strong>，对模型进行更精细的控制；以及<strong class="js iu"> Scikit-Learn API </strong>，这是一个 Scikit-Learn 包装器，使我们能够将 XGBoost 模型与 scikit-learn 对象结合使用，如<strong class="js iu">管道</strong>和<strong class="js iu">随机搜索 CV </strong>。</p><p id="a7da" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们现在将主要关注于<strong class="js iu"> Scikit-Learn API </strong>。这个 API 提供了一种方法来评估随着新树的增加模型的性能变化。为此，我们必须向<em class="ls"> fit </em>方法提供以下附加参数:<strong class="js iu"> eval_set </strong>，要使用的评估集(通常是训练集和测试集)，以及<strong class="js iu"> eval_metric </strong>，执行评估时要使用的度量。如果提供了这两个参数，评估结果将由拟合模型的<strong class="js iu"> eval_results </strong>属性<strong class="js iu"> </strong>提供。</p><p id="34de" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是说够了，让我们最后展示一些代码！由于本文是案例研究的第 2 部分，我们将不讨论预处理管道步骤的开发，因为这已经在本系列的第 1 部分中讨论过了。</p><p id="50a4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，我们加载数据集，将其转换为<strong class="js iu"> X </strong>特征矩阵和<strong class="js iu"> y </strong>目标向量，执行预处理步骤，最后，将数据分成训练集和测试集。下面的代码突出显示了这个过程:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="52e3" class="mq kq it mm b gy mr ms l mt mu"># load the dataset<br/>income = pd.read_csv("income.csv")</span><span id="7153" class="mq kq it mm b gy mv ms l mt mu"># Create the X feature matrix and the y target vector<br/>X = income.drop(labels=["high_income", 'fnlwgt'], axis=1)<br/>y = income["high_income"]</span><span id="7f80" class="mq kq it mm b gy mv ms l mt mu"># the only step necessary to be done outside of pipeline<br/># convert the target column to categorical<br/>col = pd.Categorical(y)<br/>y = pd.Series(col.codes)</span><span id="de74" class="mq kq it mm b gy mv ms l mt mu"># validate the preprocessing pipeline by passing data through it<br/>clean_X = preprocessing_pipeline.fit_transform(X)<br/>clean_X_df = pd.DataFrame(clean_X, columns=X.columns)</span><span id="fe0f" class="mq kq it mm b gy mv ms l mt mu"># split the clean_X into train and test sets<br/>X_train, X_test, y_train, y_test = train_test_split(clean_X, y, test_size=0.20, random_state=seed, shuffle=True, stratify=y)</span></pre><p id="01eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样，我们终于可以训练我们的第一个 XGBoost 分类器了！为了更好地理解模型的演变，我们将把<strong class="js iu"> n_estimators </strong>超参数设置为 500。下面的代码片段突出显示了模型训练:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="9f7b" class="mq kq it mm b gy mr ms l mt mu">import time<br/>from xgboost import XGBClassifier</span><span id="3ddb" class="mq kq it mm b gy mv ms l mt mu"># create a default XGBoost classifier<br/>model = XGBClassifier(n_estimators=500, random_state=seed)</span><span id="3475" class="mq kq it mm b gy mv ms l mt mu"># define the eval set and metric<br/>eval_set = [(X_train, y_train), (X_test, y_test)]<br/>eval_metric = ["auc","error"]</span><span id="ec95" class="mq kq it mm b gy mv ms l mt mu"># fit the model<br/>%time model.fit(X_train, y_train, eval_metric=eval_metric, eval_set=eval_set, verbose=False)</span></pre><p id="68eb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">jupyter magic 命令返回 Python 语句或表达式的执行时间。在这种情况下，训练花费了<strong class="js iu"> 11.2 秒</strong>完成，对于一个开始来说还不错。</p><p id="1125" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们可以通过进行一些预测来衡量模型性能:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="216f" class="mq kq it mm b gy mr ms l mt mu"># final model assessment<br/>pred_test = model.predict(X_test)<br/>pred_train = model.predict(X_train)</span><span id="b224" class="mq kq it mm b gy mv ms l mt mu">print('Train Accuracy: ', accuracy_score(y_train, pred_train))<br/>print('Test Accuraccy: ', accuracy_score(y_test, pred_test))</span><span id="bd9d" class="mq kq it mm b gy mv ms l mt mu">print('Classification Report:')<br/>print(classification_report(y_test,pred_test))</span></pre><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/a6b18cf7d3cb29807cbb331bd1a5f166.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*jxozMBGJXgSt9vVnsAr9WQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 1: </strong>Default XGBoost performance</figcaption></figure><p id="32be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">默认模型已经给了我们比上一篇文章中的 tunned random forest 更好的测试精度！让我们看看随着新估计量的增加，模型的表现如何:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="9528" class="mq kq it mm b gy mr ms l mt mu"># retrieve performance metrics<br/>results = model.evals_result()<br/>epochs = len(results['validation_0']['error'])<br/>x_axis = range(0, epochs)</span><span id="cfb1" class="mq kq it mm b gy mv ms l mt mu">fig, ax = plt.subplots(1, 2, figsize=(15,5))</span><span id="c401" class="mq kq it mm b gy mv ms l mt mu"># plot auc<br/>ax[0].plot(x_axis, results['validation_0']['auc'], label='Train')<br/>ax[0].plot(x_axis, results['validation_1']['auc'], label='Test')<br/>ax[0].legend()<br/>ax[0].set_title('XGBoost AUC-ROC')<br/>ax[0].set_ylabel('AUC-ROC')<br/>ax[0].set_xlabel('N estimators')</span><span id="c6df" class="mq kq it mm b gy mv ms l mt mu"># plot classification error<br/>ax[1].plot(x_axis, results['validation_0']['error'], label='Train')<br/>ax[1].plot(x_axis, results['validation_1']['error'], label='Test')<br/>ax[1].legend()<br/>ax[1].set_title('XGBoost Classification Error')<br/>ax[1].set_ylabel('Classification Error')<br/>ax[1].set_xlabel('N estimators')</span><span id="1f9d" class="mq kq it mm b gy mv ms l mt mu">plt.show()<br/>plt.tight_layout()</span></pre><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi nf"><img src="../Images/8231e082af8c9fb3536cebe2519d00c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bi_o_h9M9l_wKF2HAuaHYg.png"/></div></div></figure><p id="e9b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">嗯，看起来我们的模型运行良好，直到大约 300 个估计器，然后训练集的误差继续下降，而测试集的误差或多或少保持稳定。那可能是过度适应的迹象！避免这种情况的一种方法是使用<strong class="js iu"> early_stopping_rounds </strong>参数，一旦在指定的轮数后没有观察到改进，就停止执行。</p><h1 id="ba41" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">使用随机搜索调整超参数</h1><p id="bb9b" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">因此，看起来我们的默认模型通过其<em class="ls">开箱即用的</em>配置表现得相当好。然而，我们仍然可以通过调整一些超参数来从中获取更多信息，但是我们应该如何做呢？</p><p id="79ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">想象一下，如果我们想要调整 7 个超参数，并为每个参数测试 3 个不同的值。手动调谐是不可能的，因为有大量不同的可能组合。如果我们随后决定使用简单的网格搜索来完成这项工作，我们将使用 37 = 2187 个不同的模型来结束 um！在这种情况下，如果我们将拟合默认模型所需的时间(11.2 秒)视为一般经验法则，那么训练将需要大约 408 分钟或 6.8 小时！</p><p id="63b5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，正如我们上次所做的，我们将使用随机搜索来调整参数，它可能不会找到最佳解决方案，但会在合理的时间内找到“足够好”的解决方案。</p><p id="8767" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">使用 scikit-learn 的<strong class="js iu">randomzedsearccv</strong>，我们配置了搜索空间，以包括<strong class="js iu"> learning_rate </strong>、<strong class="js iu"> colsample_bytree </strong>、<strong class="js iu">子样本</strong>、<strong class="js iu"> max_depth </strong>、<strong class="js iu"> n_estimators </strong>、<strong class="js iu"> reg_lambda、</strong>和<strong class="js iu"> gamma </strong>超参数。我们还将迭代次数限制为 50 次，这样实验不会运行太长时间。配置和安装该模型的过程如下所示:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="9a34" class="mq kq it mm b gy mr ms l mt mu"># create a default XGBoost classifier<br/>model = XGBClassifier(<br/>    random_state=seed, <br/>    eval_metric=["error", "auc"]<br/>)</span><span id="9597" class="mq kq it mm b gy mv ms l mt mu"># Create the grid search parameter grid and scoring funcitons<br/>param_grid = {<br/>    "learning_rate": [0.1, 0.01],<br/>    "colsample_bytree": [0.6, 0.8, 1.0],<br/>    "subsample": [0.6, 0.8, 1.0],<br/>    "max_depth": [2, 3, 4],<br/>    "n_estimators": [100, 200, 300, 400],<br/>    "reg_lambda": [1, 1.5, 2],<br/>    "gamma": [0, 0.1, 0.3],<br/>}</span><span id="d2dd" class="mq kq it mm b gy mv ms l mt mu">scoring = {<br/>    'AUC': 'roc_auc', <br/>    'Accuracy': make_scorer(accuracy_score)<br/>}</span><span id="aac0" class="mq kq it mm b gy mv ms l mt mu"># create the Kfold object<br/>num_folds = 10<br/>kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)</span><span id="2889" class="mq kq it mm b gy mv ms l mt mu"># create the grid search object<br/>n_iter=50</span><span id="c438" class="mq kq it mm b gy mv ms l mt mu">grid = RandomizedSearchCV(<br/>    estimator=model, <br/>    param_distributions=param_grid,<br/>    cv=kfold,<br/>    scoring=scoring,<br/>    n_jobs=-1,<br/>    n_iter=n_iter,<br/>    refit="AUC",<br/>)</span><span id="167e" class="mq kq it mm b gy mv ms l mt mu"># fit grid search<br/>%time best_model = grid.fit(X_train,y_train)</span></pre><p id="ddac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们再次使用<strong class="js iu"> %time </strong>命令来测量训练过程的执行速度。这一次大约花了<strong class="js iu"> 20 分 19 秒</strong>在执行 50 轮后返回最佳模型。如果我们使用更大的搜索空间和更多的迭代次数，你现在可以知道这个过程是如何变得非常耗时的。</p><p id="da47" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然后，我们可以检查最佳模型 AUC 分数和超参数:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="c57d" class="mq kq it mm b gy mr ms l mt mu">print(f'Best score: {best_model.best_score_}')<br/>print(f'Best model: {best_model.best_params_}')</span></pre><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/af8b13a39ed75ed02678dc95e6697f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*AuuFLoq6dxmZuGN_btmxhg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 3:</strong> XGBoost AUC score and hyperparameters after running on CPU</figcaption></figure><p id="22b2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，我们可以进行一些预测来评估模型的整体性能:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="3c95" class="mq kq it mm b gy mr ms l mt mu">pred_test = best_model.predict(X_test)<br/>pred_train = best_model.predict(X_train)</span><span id="64a9" class="mq kq it mm b gy mv ms l mt mu">print('Train Accuracy: ', accuracy_score(y_train, pred_train))<br/>print('Test Accuraccy: ', accuracy_score(y_test, pred_test))</span><span id="b7fb" class="mq kq it mm b gy mv ms l mt mu">print('\nConfusion Matrix:')<br/>print(confusion_matrix(y_test,pred_test))<br/>print('\nClassification Report:')<br/>print(classification_report(y_test,pred_test))</span></pre><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/2b5e176607816de63eb9eff91a5ec378.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*bT_pwxPY2OHJA9jdmPF7Qg.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 4: </strong>Tunned XGBoost performance</figcaption></figure><p id="39e4" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">看起来我们在训练和测试的准确性上都有了一点点的提高，但是说实话并没有那么多。</p><h1 id="f3fe" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">利用 GPU 的力量</h1><p id="e7de" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">我们终于到达了你可能正在等待的部分！我们将使用 Google 联合实验室的免费 GPU 访问，有望加快拟合 XGBoost 模型的过程。</p><p id="fef8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">但是你如何配置你的工作空间呢？这是一个相当简单的两步过程:首先，你需要进入<strong class="js iu">编辑&gt;笔记本偏好设置，</strong>然后，在<strong class="js iu">硬件加速器</strong>下拉菜单中选择<strong class="js iu"> GPU </strong>并按<strong class="js iu">保存</strong>。就是这样！您的运行时将重新启动，然后就可以运行了。</p><p id="b74e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 XGBoost 方面，唯一改变的是包含了<strong class="js iu"> tree_method </strong>超参数。在创建模型时，随机搜索的其他内容保持不变！模型创建的变化可以在下面的代码片段中看到:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="59c8" class="mq kq it mm b gy mr ms l mt mu">model = XGBClassifier(<br/>    tree_method = "gpu_hist", <br/>    random_state=seed, <br/>    eval_metric=["error", "auc"]<br/>)</span></pre><p id="de0d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在用这种配置重新训练模型后，我们证实完成整个过程大约需要<strong class="js iu"> 18 分钟</strong>。这可能看起来不是一个很大的进步，但它比之前的运行快了大约<strong class="js iu">10%</strong><strong class="js iu"/>！使用 GPU 的优势在较大的数据集上更容易看到，其中并行化开销将更多地由并行化速度的提高来补偿。</p><p id="b93f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">之后，我们重复这个过程来检查最佳的模型参数和性能。结果如下图所示:</p><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d36755808b51628c6c7c3c079b4b9fca.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*6no-1EhvvdGe1eyNPkbA0w.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 5:</strong> XGBoost AUC score and hyperparameters after running on GPU</figcaption></figure><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9e2cf4831940285224d699fc432d52da.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*pxQbdlKJmCTVQ6NKhRULzQ.png"/></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 6: </strong>GPU<strong class="bd ne"> </strong>tunned XGBoost performance</figcaption></figure><p id="93c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可以看到培训和测试准确性的又一次轻微提高，这总是很好的，不是吗！？</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="5bd5" class="kp kq it bd kr ks nv ku kv kw nw ky kz la nx lc ld le ny lg lh li nz lk ll lm bi translated">模型的可解释性如何？</h1><p id="c765" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">因此，为了帮助更好地理解 XGBoost 模型预测，我们可以使用本系列最后一部分中介绍的任何技术:检查并绘制拟合模型的<strong class="js iu"> feature_importances_ </strong>属性；使用 ELI5 特征权重表和预测说明；最后，使用 SHAP 图。</p><p id="b1b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，XGBoost 库还有另一个锦囊妙计，它提供了一个内置的绘图 API，用于生成 boosting 中使用的各个树的特性重要性和表示图！</p><p id="aa2e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">为了绘制全局特征重要性，我们可以使用<strong class="js iu"> plot_importances </strong>方法。可以指定三个度量之一来计算单个特征分数:</p><ul class=""><li id="b9fd" class="lt lu it js b jt ju jx jy kb lv kf lw kj lx kn ly lz ma mb bi translated"><strong class="js iu">特征权重:</strong>根据特征在树中出现的次数计算得分</li><li id="8b53" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">增益:</strong>根据使用该特性的拆分的平均增益计算得分</li><li id="1e32" class="lt lu it js b jt mc jx md kb me kf mf kj mg kn ly lz ma mb bi translated"><strong class="js iu">覆盖率:</strong>根据使用该特性的分割的平均覆盖率(受分割影响的样本数)计算得分</li></ul><p id="c21c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面的代码显示了这三个示例:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="250a" class="mq kq it mm b gy mr ms l mt mu"># store the winning model in a new variable<br/>xgc = best_model_gpu.best_estimator_<br/># saving the feature names to the model<br/>xgc.get_booster().feature_names = X.columns.to_list()</span><span id="653d" class="mq kq it mm b gy mv ms l mt mu"># Create the feature importances plot<br/>fig, ax = plt.subplots(1, 3, figsize=(15,5))</span><span id="6c24" class="mq kq it mm b gy mv ms l mt mu"># plot importances with feature weight<br/>xgb.plot_importance(<br/>    booster=xgc, <br/>    importance_type='weight',<br/>    title='Feature Weight',<br/>    show_values=False,<br/>    height=0.5,<br/>    ax=ax[0],<br/>)</span><span id="e620" class="mq kq it mm b gy mv ms l mt mu"># plot importances with split mean gain<br/>xgb.plot_importance(<br/>    booster=xgc,<br/>    importance_type='gain',<br/>    title='Split Mean Gain',<br/>    show_values=False,<br/>    height=0.5,<br/>    ax=ax[1]<br/>)</span><span id="bd66" class="mq kq it mm b gy mv ms l mt mu"># plot importances with sample coverage<br/>xgb.plot_importance(<br/>    xgc,<br/>    importance_type='cover',<br/>    title='Sample Coverage',<br/>    show_values=False,<br/>    height=0.5,<br/>    ax=ax[2]<br/>)</span><span id="54b0" class="mq kq it mm b gy mv ms l mt mu">plt.tight_layout()<br/>plt.show()</span></pre><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi oa"><img src="../Images/97573c63e7e1c4b127d59634d4525d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qyLwUjHnhOWlfFzli5Dcg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 7: </strong>XGBoost feature importances</figcaption></figure><p id="a88e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最后，为了绘制特定的树，我们可以使用如下所示的<strong class="js iu"> plot_tree </strong>方法:</p><pre class="mh mi mj mk gt ml mm mn mo aw mp bi"><span id="fb9b" class="mq kq it mm b gy mr ms l mt mu"># Create the feature importances plot<br/>fig, ax = plt.subplots(figsize=(20,20))</span><span id="81ef" class="mq kq it mm b gy mv ms l mt mu"># plot a decision tree from the booster<br/>xgb.plot_tree(booster=xgc, num_trees=0, ax=ax, rankdir='LR')</span><span id="110b" class="mq kq it mm b gy mv ms l mt mu">plt.tight_layout()<br/>plt.show()</span></pre><figure class="mh mi mj mk gt mx gh gi paragraph-image"><div role="button" tabindex="0" class="ng nh di ni bf nj"><div class="gh gi ob"><img src="../Images/84efe8669923f881c61bb0d5fc1694af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AcGZd7_hqmPWdxEgqKpcrg.png"/></div></div><figcaption class="na nb gj gh gi nc nd bd b be z dk"><strong class="bd ne">Figure 8: </strong>XGBoost tree</figcaption></figure></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="6e3e" class="kp kq it bd kr ks nv ku kv kw nw ky kz la nx lc ld le ny lg lh li nz lk ll lm bi translated">结论</h1><p id="c6ba" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">至此，我们结束了这个关于整体学习的两部分系列，我希望它是令人愉快的，也许还有点用处！</p><p id="07aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢阅读！请在下面留下你的想法。</p></div></div>    
</body>
</html>