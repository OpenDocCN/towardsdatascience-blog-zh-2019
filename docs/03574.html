<html>
<head>
<title>Introduction to Hidden Markov Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">隐马尔可夫模型简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781?source=collection_archive---------2-----------------------#2019-06-07">https://towardsdatascience.com/introduction-to-hidden-markov-models-cd2c93e6b781?source=collection_archive---------2-----------------------#2019-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d58d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们介绍马尔可夫链和隐马尔可夫模型。</h2></div><p id="5712" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">马尔可夫链</strong></p><p id="8e9b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们先简单介绍一下马尔可夫链，一种随机过程。我们从链条的几个“状态”开始，{ <em class="le"> S </em> ₁,…，<em class="le">s</em>ₖ}；例如，如果我们的链代表每天的天气，我们可以有{雪，雨，阳光}。一个过程(<em class="le"> X </em> ₜ)ₜ应该是一个马尔可夫链)的性质是:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/aa76da317576c671df79cc24c556650d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*h4v6Up-zgrgod1cgnCvsPQ.png"/></div></figure><p id="329e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，处于状态<em class="le"> j </em>的概率只取决于前一个状态，而不取决于之前发生的事情。</p><p id="097a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">马尔可夫链通常用带有转移概率的图来描述，即从状态<em class="le"> i </em>移动到状态<em class="le"> j </em>的概率，用<em class="le"> p </em> ᵢ,ⱼ.来表示让我们看看下面的例子:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ln"><img src="../Images/31d4efb2a4f8795d383a3c5d5e72a944.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*teqRljsSOQzjilyaKkze4A.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Markov chain example</figcaption></figure><p id="b7a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该链有三种状态；例如，雪和雨之间的转移概率是 0.3，也就是说，如果昨天下雪，今天有 30%的可能性会下雨。转移概率可以总结为一个矩阵:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/ba11994cfcdeb3ff78030b7770314e44.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*xA_DqKa1QCBIi3DRppSmFA.png"/></div></figure><p id="1e36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意每一行的总和等于 1(想想为什么)。这样的矩阵称为随机矩阵。(<em class="le"> i </em>，<em class="le"> j </em>)定义为<em class="le">p</em>ᵢ,ⱼ——在<em class="le"> i </em>和<em class="le"> j </em>之间的转移概率。</p><p id="8b18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实:如果我们取矩阵的幂，<em class="le"> P </em> ᵏ，(<em class="le"> i </em>，<em class="le"> j </em>)条目代表在<em class="le"> k </em>步从状态<em class="le"> i </em>到达状态<em class="le"> j </em>的概率。</p><p id="8761" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在许多情况下，我们被给定一个初始概率向量<em class="le"> q </em> =( <em class="le"> q </em> ₁,…，<em class="le"> q </em> ₖ)在时间<em class="le"> t </em> =0 时处于每个状态。因此，在时间<em class="le"> t </em>处于状态<em class="le"> i </em>的概率将等于向量<em class="le"> P </em> ᵏ <em class="le"> q </em>的第<em class="le"> i- </em>个条目。</p><p id="6929" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果今天下雪、下雨和阳光的概率是 0，0.2，0.8，那么 100 天后下雨的概率计算如下:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/a9df60f2f09091b5e07bd46457c793c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*tmykVqKLS3h74xkB7RUn0w.png"/></div></figure><p id="216e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第二项等于≈ 0.44。</p><p id="4d21" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">隐马尔可夫模型</strong></p><p id="8da8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在隐马尔可夫模型(HMM)中，我们有一个看不见的马尔可夫链(我们无法观察到)，每个状态从<em class="le"> k </em>个观察值中随机产生一个，这对我们来说是可见的。</p><p id="1d8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个例子。假设我们有上面的马尔可夫链，有三个状态(雪、雨和阳光)，<em class="le"> P </em> -转移概率矩阵和<em class="le">q</em>-初始概率。这是看不见的马尔可夫链——假设我们在家，看不见天气。然而，我们可以感觉到我们房间内的温度，假设有两种可能的观察结果:热和冷，其中:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/7d48b0a276e43bb7a0929c2a016e5d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*KBDkLmHfGC4faLmULPhEww.png"/></div></figure><p id="f86e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">基本示例</strong></p><p id="9b58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作为第一个例子，我们应用 HMM 来计算我们连续两天感到寒冷的概率。这两天，底层马尔可夫状态有 3*3=9 个选项。让我们举例说明这 9 个选项之一的概率计算:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi lz"><img src="../Images/62671b37c137c8fbf0b660334079c88c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oXOptScDLnLVxBs3IFVWmw.png"/></div></div></figure><p id="1131" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将所有选项相加得出期望的概率。</p><p id="dcf0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">寻找隐藏状态—维特比算法</strong></p><p id="4a42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在某些情况下，我们被给定一系列的观察值，并想找到最有可能对应的隐藏状态。</p><p id="a889" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">强力解决方案需要指数级的时间(就像上面的计算)；一种更有效的方法叫做<strong class="kk iu">维特比算法</strong>；它的主要思想是这样的:给我们一个观察序列<em class="le"> o </em> ₁,…，<em class="le"> o </em> ₜ <em class="le"> </em>。对于每个状态<em class="le"> i </em>和<em class="le"> t </em> =1，…，<em class="le"> T </em>，我们定义</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi ma"><img src="../Images/39d98b8a6f090683440f7f27efd6fa8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*9Lw_Y0hYczsljnJOxtII3A.png"/></div></div></figure><p id="da2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也就是，给定我们的观察，在状态<em class="le"> i </em>在时间<em class="le"> t </em>结束的路径的最大概率。这里的主要观察是，根据马尔可夫性质，如果在时间<em class="le"> t </em>以<em class="le"> i </em>结束的最可能路径等于在时间<em class="le">t</em>1 的某个<em class="le"> i </em>，则<em class="le"> i </em> *是在时间<em class="le">t</em>1 结束的最可能路径的最后状态的值。这给了我们下面的前向递归:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/9d957ab0fd1c4a12dd582fb9a6886b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*93VPJU9s5Mw1_HxE8gv2Vw.png"/></div></figure><p id="3871" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，<em class="le"> α </em> ⱼ( <em class="le"> o </em> ₜ)表示当隐马尔可夫状态为<em class="le"> j </em>时拥有<em class="le"> o </em> ₜ的概率。</p><p id="6dd0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里有一个例子。让我们生成一个 14 天的序列，其中 1 表示高温，0 表示低温。我们会用算法找到这两周最有可能的天气预报。</p><pre class="lg lh li lj gt mc md me mf aw mg bi"><span id="8b37" class="mh mi it md b gy mj mk l ml mm">import numpy as np<br/>import pandas as pd<br/><br/>obs_map = {'Cold':0, 'Hot':1}<br/>obs = np.array([1,1,0,1,0,0,1,0,1,1,0,0,0,1])<br/><br/>inv_obs_map = dict((v,k) for k, v in obs_map.items())<br/>obs_seq = [inv_obs_map[v] for v in list(obs)]<br/><br/>print("Simulated Observations:\n",pd.DataFrame(np.column_stack([obs, obs_seq]),columns=['Obs_code', 'Obs_seq']) )<br/><br/>pi = [0.6,0.4] # initial probabilities vector<br/>states = ['Cold', 'Hot']<br/>hidden_states = ['Snow', 'Rain', 'Sunshine']<br/>pi = [0, 0.2, 0.8]<br/>state_space = pd.Series(pi, index=hidden_states, name='states')<br/>a_df = pd.DataFrame(columns=hidden_states, index=hidden_states)<br/>a_df.loc[hidden_states[0]] = [0.3, 0.3, 0.4]<br/>a_df.loc[hidden_states[1]] = [0.1, 0.45, 0.45]<br/>a_df.loc[hidden_states[2]] = [0.2, 0.3, 0.5]<br/>print("\n HMM matrix:\n", a_df)<br/>a = a_df.values<br/><br/>observable_states = states<br/>b_df = pd.DataFrame(columns=observable_states, index=hidden_states)<br/>b_df.loc[hidden_states[0]] = [1,0]<br/>b_df.loc[hidden_states[1]] = [0.8,0.2]<br/>b_df.loc[hidden_states[2]] = [0.3,0.7]<br/>print("\n Observable layer  matrix:\n",b_df)<br/>b = b_df.values</span></pre><p id="82b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到:</p><pre class="lg lh li lj gt mc md me mf aw mg bi"><span id="35f1" class="mh mi it md b gy mj mk l ml mm">Simulated Observations:<br/>    Obs_code Obs_seq<br/>0         1     Hot<br/>1         1     Hot<br/>2         0    Cold<br/>3         1     Hot<br/>4         0    Cold<br/>5         0    Cold<br/>6         1     Hot<br/>7         0    Cold<br/>8         1     Hot<br/>9         1     Hot<br/>10        0    Cold<br/>11        0    Cold<br/>12        0    Cold<br/>13        1     Hot<br/><br/> HMM matrix:<br/>          Snow  Rain Sunshine<br/>Snow      0.3   0.3      0.4<br/>Rain      0.1  0.45     0.45<br/>Sunshine  0.2   0.3      0.5<br/><br/> Observable layer  matrix:<br/>          Cold  Hot<br/>Snow        1    0<br/>Rain      0.8  0.2<br/>Sunshine  0.3  0.7</span></pre><p id="d8ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们使用算法:</p><pre class="lg lh li lj gt mc md me mf aw mg bi"><span id="315d" class="mh mi it md b gy mj mk l ml mm">path, delta, phi = viterbi(pi, a, b, obs)<br/>state_map = {0:'Snow', 1:'Rain', 2:'Sunshine'}<br/>state_path = [state_map[v] for v in path]<br/>pd.DataFrame().assign(Observation=obs_seq).assign(Best_Path=state_path)</span></pre><p id="19ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到:</p><pre class="lg lh li lj gt mc md me mf aw mg bi"><span id="93f1" class="mh mi it md b gy mj mk l ml mm">Start Walk Forward<br/><br/>s=0 and t=1: phi[0, 1] = 2.0<br/>s=1 and t=1: phi[1, 1] = 2.0<br/>s=2 and t=1: phi[2, 1] = 2.0<br/>s=0 and t=2: phi[0, 2] = 2.0<br/>s=1 and t=2: phi[1, 2] = 2.0<br/>s=2 and t=2: phi[2, 2] = 2.0<br/>s=0 and t=3: phi[0, 3] = 0.0<br/>s=1 and t=3: phi[1, 3] = 1.0<br/>s=2 and t=3: phi[2, 3] = 1.0<br/>s=0 and t=4: phi[0, 4] = 2.0<br/>s=1 and t=4: phi[1, 4] = 2.0<br/>s=2 and t=4: phi[2, 4] = 2.0<br/>s=0 and t=5: phi[0, 5] = 0.0<br/>s=1 and t=5: phi[1, 5] = 1.0<br/>s=2 and t=5: phi[2, 5] = 1.0<br/>s=0 and t=6: phi[0, 6] = 0.0<br/>s=1 and t=6: phi[1, 6] = 1.0<br/>s=2 and t=6: phi[2, 6] = 1.0<br/>s=0 and t=7: phi[0, 7] = 2.0<br/>s=1 and t=7: phi[1, 7] = 2.0<br/>s=2 and t=7: phi[2, 7] = 2.0<br/>s=0 and t=8: phi[0, 8] = 0.0<br/>s=1 and t=8: phi[1, 8] = 1.0<br/>s=2 and t=8: phi[2, 8] = 1.0<br/>s=0 and t=9: phi[0, 9] = 2.0<br/>s=1 and t=9: phi[1, 9] = 2.0<br/>s=2 and t=9: phi[2, 9] = 2.0<br/>s=0 and t=10: phi[0, 10] = 2.0<br/>s=1 and t=10: phi[1, 10] = 2.0<br/>s=2 and t=10: phi[2, 10] = 2.0<br/>s=0 and t=11: phi[0, 11] = 0.0<br/>s=1 and t=11: phi[1, 11] = 1.0<br/>s=2 and t=11: phi[2, 11] = 1.0<br/>s=0 and t=12: phi[0, 12] = 0.0<br/>s=1 and t=12: phi[1, 12] = 1.0<br/>s=2 and t=12: phi[2, 12] = 1.0<br/>s=0 and t=13: phi[0, 13] = 0.0<br/>s=1 and t=13: phi[1, 13] = 1.0<br/>s=2 and t=13: phi[2, 13] = 1.0<br/>--------------------------------------------------<br/>Start Backtrace<br/><br/>path[12] = 1<br/>path[11] = 1<br/>path[10] = 1<br/>path[9] = 2<br/>path[8] = 2<br/>path[7] = 1<br/>path[6] = 2<br/>path[5] = 1<br/>path[4] = 1<br/>path[3] = 2<br/>path[2] = 1<br/>path[1] = 2<br/>path[0] = 2</span></pre><p id="4cb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这导致输出:</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/9ceba0892f96ede2fd7c8539e800dafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*TYCY4fjXa6rHwr_9a4yftw.png"/></div></figure><p id="df06" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们基于[2]使用了以下实现:</p><pre class="lg lh li lj gt mc md me mf aw mg bi"><span id="7777" class="mh mi it md b gy mj mk l ml mm">def viterbi(pi, a, b, obs):<br/>    <br/>    nStates = np.shape(b)[0]<br/>    T = np.shape(obs)[0]<br/>    <br/>    # init blank path<br/>    path = path = np.zeros(T,dtype=int)<br/>    # delta --&gt; highest probability of any path that reaches state i<br/>    delta = np.zeros((nStates, T))<br/>    # phi --&gt; argmax by time step for each state<br/>    phi = np.zeros((nStates, T))<br/>    <br/>    # init delta and phi <br/>    delta[:, 0] = pi * b[:, obs[0]]<br/>    phi[:, 0] = 0<br/><br/>    print('\nStart Walk Forward\n')    <br/>    # the forward algorithm extension<br/>    for t in range(1, T):<br/>        for s in range(nStates):<br/>            delta[s, t] = np.max(delta[:, t-1] * a[:, s]) * b[s, obs[t]] <br/>            phi[s, t] = np.argmax(delta[:, t-1] * a[:, s])<br/>            print('s={s} and t={t}: phi[{s}, {t}] = {phi}'.format(s=s, t=t, phi=phi[s, t]))<br/>    <br/>    # find optimal path<br/>    print('-'*50)<br/>    print('Start Backtrace\n')<br/>    path[T-1] = np.argmax(delta[:, T-1])<br/>    for t in range(T-2, -1, -1):<br/>        path[t] = phi[path[t+1], [t+1]]<br/>        print('path[{}] = {}'.format(t, path[t]))<br/>        <br/>    return path, delta, phi</span></pre><p id="be62" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">学习和鲍姆-韦尔奇算法</strong></p><p id="5628" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与上述方法类似的方法可以用于 HMM 模型的参数学习。我们有一些数据集，我们想找到最适合 HMM 模型的参数。<strong class="kk iu"> Baum-Welch </strong>算法是一个迭代过程，它找到观察值 P 的概率的(局部)最大值(<em class="le"> O </em> |M)，其中 M 表示模型(带有我们想要拟合的参数)。由于我们通过模型知道 P(M| <em class="le"> O </em>),我们可以使用贝叶斯方法找到 P(M| <em class="le"> O </em>)并收敛到一个最优值。</p><p id="877d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">HMM 有各种各样的应用，从字符识别到金融预测(检测市场机制)。</p><p id="f5bf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">参考文献</strong></p><p id="2936" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]<a class="ae mo" href="https://cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_hmm.pdf" rel="noopener ugc nofollow" target="_blank">https://CSE . buffalo . edu/~ jcorso/t/CSE 555/files/lecture _ hmm . pdf</a></p><p id="c205" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]<a class="ae mo" href="http://www.blackarbs.com/blog/introduction-hidden-markov-models-python-networkx-sklearn/2/9/2017" rel="noopener ugc nofollow" target="_blank">http://www . blackarbs . com/blog/introduction-hidden-Markov-models-python-networkx-sk learn/2/9/2017</a></p></div></div>    
</body>
</html>