<html>
<head>
<title>Learning general sentence representation by various NLP tasks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过各种自然语言处理任务学习一般句子表征</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learning-generic-sentence-representation-by-various-nlp-tasks-df39ce4e81d7?source=collection_archive---------13-----------------------#2019-02-16">https://towardsdatascience.com/learning-generic-sentence-representation-by-various-nlp-tasks-df39ce4e81d7?source=collection_archive---------13-----------------------#2019-02-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8388" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通用句导论</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b0ac318f77c3f0515773a2dcab35fbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PcCY5Nm-vMBcp9WU"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@makcedward?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Edward Ma</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="c6bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多论文介绍了计算更好的文本表示的不同方法。从<a class="ae ky" href="https://medium.com/@makcedward/besides-word-embedding-why-you-need-to-know-character-embedding-6096a34a3b10" rel="noopener">字嵌入</a>、<a class="ae ky" rel="noopener" target="_blank" href="/3-silver-bullets-of-word-embedding-in-nlp-10fa8f50cc5a">词嵌入</a>到<a class="ae ky" rel="noopener" target="_blank" href="/learning-sentence-embeddings-by-natural-language-inference-a50b4661a0b8">句嵌入</a>，我介绍了不同的处理方法。但是大部分都是单个数据集或者问题域的训练。Subramania 等人探索了一种简单、有效的句子嵌入多任务学习方法。假设是好的句子表征可以从大量弱相关的任务中学到。</p><p id="7e0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看完这篇文章，你会明白:</p><ul class=""><li id="0484" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">通用语句表示设计</li><li id="7fdc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">体系结构</li><li id="716b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">履行</li><li id="56b5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">拿走</li></ul><h1 id="8c94" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">通用语句表示设计</h1><p id="c694" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">GensSen 由 Subramania 等人于 2018 年推出..他们探索了多任务方法，即在训练期间切换 5 个 NLP 任务来学习下游任务的通用句子表示。选定的任务包括:</p><ul class=""><li id="0b76" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">神经机器翻译(NMT):将英语翻译成法语和德语</li><li id="55d2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/transforming-text-to-sentence-embeddings-layer-via-some-thoughts-b77bed60822c">跳过思维向量</a>:预测上一句和下一句</li><li id="1dda" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">内容分析:识别句子结构</li><li id="2361" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">自然语言推理 (NLI):分类蕴涵</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/35aad1916381bfb2d58a894640e1d4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*CKp2ujsscQKgQofVGHLSRA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">An approximate number of sentence pairs for each task. (Subramania et al., 2018)</figcaption></figure><p id="74a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下表中，我们注意到“我们的模型”(Subramania 等人提出的方法)在添加更多任务时传输性能得到了提高。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/062d20403021635d4a442bb53b2dbf82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w0VhOOMCzS97k6_LoPxjvg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Evaluation of sentences representation on 10 tasks. (Subramania et al., 2018)</figcaption></figure><p id="7796" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了句子表征，我们还注意到单词嵌入的结果比下表中的其他结果要好</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/b9b59cff1cec7219ceff49d7d1265e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60OJMIe6Ld3ZwBSOeTOqcw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Evaluation of word embeddings. (Subramania et al., 2018)</figcaption></figure><h1 id="7b39" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">体系结构</h1><p id="1149" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">所有 5 个任务都是序列到序列的问题，但是解决不同的 NLP 问题。序列到序列架构是编码器-解码器模型之一，而输入和输出都是顺序的。编码器将句子转换成固定长度的向量，而解码器试图通过链式规则产生条件概率。</p><p id="cf70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">编码器采用双向选通递归单元(GRU ),解码器采用单向递归单元。GRU 有助于消除消失梯度的影响，具有更快的计算速度。</p><p id="26f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于我们需要学习一个通用的句子表示，我们必须在任务之间共享编码器，这样它就可以从不同的任务中学习一些东西。编码的句子表示将被发送到不同的解码器(每个任务的委托解码器)以解码向量并分类/预测结果。在训练模型之后，那些解码器将被扔掉，我们只需要在你的特定 NLP 问题中共享编码器。</p><p id="9f9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于作者建议使用不同的方法来训练编码器，我们必须解决什么时候应该切换任务？作者采用了一种简单的方法，即在每次参数更新后，选取一个任务进行训练。</p><h1 id="8214" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">拿走</h1><ul class=""><li id="7e4f" class="lv lw it lb b lc nb lf nc li nj lm nk lq nl lu ma mb mc md bi translated">多任务学习有助于概括词向量，从而可以很容易地转移到其他问题领域并获得更好的结果。</li></ul><h1 id="908f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以通过<a class="ae ky" href="http://medium.com/@makcedward/" rel="noopener">媒体博客</a>、<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae ky" href="https://github.com/makcedward" rel="noopener ugc nofollow" target="_blank"> Github </a>联系我。</p><h1 id="8faf" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="0ffd" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">Sandeep S .、Adam T .、Yoshua B .、Christopher J . p .，<a class="ae ky" href="https://arxiv.org/pdf/1804.00079.pdf" rel="noopener ugc nofollow" target="_blank">通过大规模多任务学习学习通用分布式句子表示</a>。2018</p><p id="e057" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/Maluuba/gensen" rel="noopener ugc nofollow" target="_blank">https://github.com/Maluuba/gensen</a></p></div></div>    
</body>
</html>