<html>
<head>
<title>Handling Class imbalanced data using a loss specifically made for it</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用专门为其制作的损失来处理类不平衡数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab?source=collection_archive---------4-----------------------#2019-09-04">https://towardsdatascience.com/handling-class-imbalanced-data-using-a-loss-specifically-made-for-it-6e58fd65ffab?source=collection_archive---------4-----------------------#2019-09-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0dd2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">这篇文章是对 Google 在 CVPR 19 年会上发表的一篇名为<a class="ae kf" href="http://openaccess.thecvf.com/content_CVPR_2019/html/Cui_Class-Balanced_Loss_Based_on_Effective_Number_of_Samples_CVPR_2019_paper.html" rel="noopener ugc nofollow" target="_blank">基于有效样本数的等级平衡损失</a>的论文的评论。</h2></div><p id="c094" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">TL；DR——它针对最常用的损失(softmax 交叉熵、焦点损失等)提出了一种分类加权方案。)快速提高准确性，尤其是在处理类别高度不平衡的数据时。</strong></p><p id="8f4b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">链接到本文的实现(使用 py torch)——<a class="ae kf" href="https://github.com/vandit15/Class-balanced-loss-pytorch" rel="noopener ugc nofollow" target="_blank">GitHub</a></p></div><div class="ab cl lc ld hu le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="ij ik il im in"><h1 id="3ab0" class="lj lk iq bd ll lm ln lo lp lq lr ls lt jw lu jx lv jz lw ka lx kc ly kd lz ma bi translated">有效样本数</h1><p id="4a41" class="pw-post-body-paragraph kg kh iq ki b kj mb jr kl km mc ju ko kp md kr ks kt me kv kw kx mf kz la lb ij bi translated">在处理长尾数据集(大部分样本属于很少的几个类，而许多其他类的支持度很低)时，决定如何对不同类的损失进行加权可能会很棘手。通常，权重被设置为类支持的倒数或类支持的平方根的倒数。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/18e09ac9a3a0a87daeb460704371445f.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*duHv3RJF8TnY2OD1W70Jww.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Traditional re-weighting vs proposed re-weighting</figcaption></figure><p id="bd87" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">然而，如上图所示，这是过冲的，因为<em class="ms">随着样本数量的增加，新数据点的额外好处会减少。</em>新添加的样本很有可能是现有样本的近似副本，主要是在大量数据扩充(如重新缩放、随机裁剪、翻转等)时。)在训练神经网络时使用。通过样本的有效数量重新加权给出了更好的结果。</p><p id="98a8" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">样本的有效数量可以想象为<em class="ms">N 个样本将覆盖的实际体积，其中总体积 N 由总样本数表示。</em></p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/14a284bd7c7119831e248ac243602f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*wn9J0lrdefhXI7Ae46jXIg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Effective number of samples</figcaption></figure><p id="cf12" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">形式上，我们把它写成:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/abccc8e8215f1eadb0632f8cca60ea60.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*8HXd5UkBDPWfno6miBrzdQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Effective number of samples</figcaption></figure><p id="3a7d" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这里，我们假设新样本只会以两种方式与先前采样的数据量进行交互:要么完全覆盖，要么完全在外(如上图所示)。有了这个假设，利用归纳法就可以很容易地证明上面的表达式(证明参考论文)。</p><p id="06e0" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们也可以这样写:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/2ff907e2ffc77867f007bbaac35d22ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*hyBq5SZO0x8kycTJV0pydg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Contribution of every sample</figcaption></figure><p id="0a30" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这意味着第 j 个样本对样本的有效数量贡献了 Beta^(j-1。</p><p id="71d3" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">上式的另一个含义是，如果β= 0，则 En = 1。还有，En → n 为β→1。后者可以很容易地用洛必达法则来证明。这意味着当 N 很大时，有效样本数与样本数 N 相同。在这种情况下，唯一原型数 N 很大，每个样本都是唯一的。然而，如果 N=1，这意味着所有数据都可以用一个原型来表示。</p><h1 id="8c73" class="lj lk iq bd ll lm mv lo lp lq mw ls lt jw mx jx lv jz my ka lx kc mz kd lz ma bi translated">类别平衡损失</h1><p id="c1ed" class="pw-post-body-paragraph kg kh iq ki b kj mb jr kl km mc ju ko kp md kr ks kt me kv kw kx mf kz la lb ij bi translated">在没有额外信息的情况下，我们无法为每个类别设置单独的β值，因此，使用整个数据，我们将它设置为特定值(通常设置为 0.9、0.99、0.999、0.9999 中的一个)。</p><p id="05ac" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">因此，类平衡损失可以写成:</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi na"><img src="../Images/32c6f14c5b41647a6bffc9bd780ed857.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*VapvZdPvkOyh6Q1tgu7esg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">CB Loss</figcaption></figure><p id="6bb0" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这里，<strong class="ki ir"> L(p，y) </strong>可以是任意损失函数。</p><h2 id="dd9d" class="nb lk iq bd ll nc nd dn lp ne nf dp lt kp ng nh lv kt ni nj lx kx nk nl lz nm bi translated">类别平衡焦点损失</h2><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/82b89e944f3d07c7dd6aa788704ca241.png" data-original-src="https://miro.medium.com/v2/resize:fit:904/format:webp/1*dvWivF7b1MhqQ1sGZS8ioQ.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">Class-Balanced Focal Loss</figcaption></figure><p id="b8fe" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">焦点丢失的原始版本有一个 alpha 平衡变体。相反，我们将使用每个类别的有效样本数对其进行重新加权。</p><p id="41d5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">类似地，这样的重新加权项也可以应用于其他著名的损失(sigmoid 交叉熵、softmax 交叉熵等)。)</p><h1 id="471d" class="lj lk iq bd ll lm mv lo lp lq mw ls lt jw mx jx lv jz my ka lx kc mz kd lz ma bi translated">履行</h1><p id="f3c9" class="pw-post-body-paragraph kg kh iq ki b kj mb jr kl km mc ju ko kp md kr ks kt me kv kw kx mf kz la lb ij bi translated">在开始实施之前，在使用基于 sigmoid 的损失进行训练时需要注意一点——使用 b = -log(C-1)初始化最后一层的偏差，其中 C 是类的数量而不是 0。这是因为设置 b=0 会在训练开始时导致巨大的损失，因为每个类的输出概率接近 0.5。因此，我们可以假设类 prior 是 1/C，并相应地设置 b 的值。</p><h2 id="ac8a" class="nb lk iq bd ll nc nd dn lp ne nf dp lt kp ng nh lv kt ni nj lx kx nk nl lz nm bi translated">类别的权重计算</h2><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi no"><img src="../Images/d8963aa71068f3c23bc366acea9cb667.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*_lID8VIE2fzWnqTWNQl9Pw.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">calculating normalised weights</figcaption></figure><p id="1949" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">上面几行代码是一个简单的实现，用于获取权重并将其归一化。</p><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6218cb9ef77da7476fd8195ca08a04d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*lkP7T0dF4bMGfDwbI66Cxg.png"/></div><figcaption class="mo mp gj gh gi mq mr bd b be z dk">getting PyTorch tensor for one-hot labels</figcaption></figure><p id="981c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这里，我们得到了权重的一个热点值，这样它们可以分别与每个类的损失值相乘。</p><h1 id="68e7" class="lj lk iq bd ll lm mv lo lp lq mw ls lt jw mx jx lv jz my ka lx kc mz kd lz ma bi translated">实验</h1><figure class="mh mi mj mk gt ml gh gi paragraph-image"><div role="button" tabindex="0" class="nr ns di nt bf nu"><div class="gh gi nq"><img src="../Images/d98a0f54df877e2db329549459715892.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wW7NG9xb2hPERvzaIP8wvQ.png"/></div></div></figure><p id="f5d7" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">类平衡提供了显著的好处，尤其是当数据集高度不平衡时(不平衡= 200，100)。</p><h1 id="fe6c" class="lj lk iq bd ll lm mv lo lp lq mw ls lt jw mx jx lv jz my ka lx kc mz kd lz ma bi translated">结论</h1><p id="0f79" class="pw-post-body-paragraph kg kh iq ki b kj mb jr kl km mc ju ko kp md kr ks kt me kv kw kx mf kz la lb ij bi translated">使用有效样本数的概念，我们可以解决数据重叠的问题。由于我们不对数据集本身做出任何假设，因此重新加权条款通常适用于多个数据集和多个损失函数。因此，类不平衡的问题可以通过更合适的结构来解决，这一点很重要，因为大多数真实世界的数据集都存在大量的数据不平衡。</p><h1 id="b016" class="lj lk iq bd ll lm mv lo lp lq mw ls lt jw mx jx lv jz my ka lx kc mz kd lz ma bi translated">参考</h1><p id="0a03" class="pw-post-body-paragraph kg kh iq ki b kj mb jr kl km mc ju ko kp md kr ks kt me kv kw kx mf kz la lb ij bi translated">[1]基于有效样本数的类平衡损失:【https://arxiv.org/abs/1901.05555 T2】</p></div></div>    
</body>
</html>