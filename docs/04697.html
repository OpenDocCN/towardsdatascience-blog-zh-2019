<html>
<head>
<title>Creating Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-machine-learning-models-b48bb72a791f?source=collection_archive---------22-----------------------#2019-07-17">https://towardsdatascience.com/creating-machine-learning-models-b48bb72a791f?source=collection_archive---------22-----------------------#2019-07-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4388" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用、测试和比较多种机器学习模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/f8eb3d60ade31bb6992e3f37d866d9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*GQH0g0nxFE8tB_Eni8LFIA.jpeg"/></div><figcaption class="kq kr gj gh gi ks kt bd b be z dk">Source: Pixabay</figcaption></figure><p id="6608" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们已经成功地完成了每个数据科学项目中最困难的部分——数据清理和争论，我们将进入有趣的部分，建模！</p><p id="bc78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">虽然本文可以自成体系，但最好先经过<a class="ae lq" rel="noopener" target="_blank" href="/exploratory-data-analysis-feature-engineering-and-modelling-using-supermarket-sales-data-part-1-228140f89298"> <strong class="kw iu">第 1 部分</strong> </a>和<a class="ae lq" rel="noopener" target="_blank" href="/feature-engineering-and-data-preparation-using-supermarket-sales-data-part-2-171b7a7a7eb7"> <strong class="kw iu">第 2 部分</strong> </a>。</p><blockquote class="lr ls lt"><p id="6345" class="ku kv lu kw b kx ky ju kz la lb jx lc lv le lf lg lw li lj lk lx lm ln lo lp im bi translated">机器学习中的建模是一个迭代阶段，在这个阶段，数据科学家不断地训练和测试机器学习模型，以发现最适合给定任务的模型。</p></blockquote><p id="b73c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">存在不同的机器学习模型，选择使用哪一个通常取决于手头的问题。没有机器学习模型对所有类型的问题都是最好的。因此，在这个阶段，您的工作是测试多个模型并微调参数，以挤出每一点准确性。</p><p id="f59d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">注</strong>:虽然我们通常追求性能更高的模型，但通常更明智、更好的选择是性能几乎和复杂模型一样好的更简单的模型。</p><p id="ce78" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为您的机器学习竞赛节省<em class="lu"> .0001 </em>增加的性能，因为在现实世界的大部分时间里，这些微小的差异并不重要，因为我们需要更简单的可解释模型。</p><p id="5cd6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">有鉴于此，我们开始工作吧。从一个简单的基础模型开始总是好的，这样你就可以得到一个基线来衡量性能。</p><p id="9efe" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">由于我们的数据集很小，我们将使用交叉验证，对于性能指标，我们将使用平均绝对误差。交叉验证有助于衡量我们模型的真实性能。</p><p id="8be0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们创建了一个交叉验证函数，如下所示。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><h1 id="b54a" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">对于回归任务，从简单的线性回归开始</h1><p id="3f48" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">我们从简单的线性回归开始建模，交叉验证并打印分数。</p><h2 id="21d2" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调谐什么</h2><p id="6695" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">线性回归很简单，因此没有太多要调整的参数。所以我们只需使用默认参数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="d2e8" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.42615411746170206</span></pre><h1 id="c41b" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">使用决策树</h1><p id="46a7" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">接下来，我们使用另一种简单但有效的算法，称为决策树</p><h2 id="f887" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调谐什么</h2><p id="61c2" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">决策树包含许多超参数，但我们可以快速调整的最重要的超参数是:</p><ol class=""><li id="e7ce" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu">最大深度</strong>:最大深度参数表示树的深度。树越深，模型就越复杂，它就能获取更多关于数据的信息。如果您的模型太简单，请尝试增加 max_depth，反之亦然。</li><li id="f814" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_samples_leaf </strong>:这是一个叶节点所需的最小样本数。较高的叶片数导致更简单的模型，并可能有助于过拟合，而较小的叶片数可能导致欠拟合。</li><li id="ef8f" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_samples_split: </strong>这是分割内部节点所需的最小样本数。该参数增加导致更简单/受约束的模型。</li><li id="4b19" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_features </strong>:这是模型在分割一个节点之前考虑的特征数量。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="97ee" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4280371717119684</span></pre><h1 id="5605" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">使用 K-最近邻</h1><p id="7dc4" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">接下来，我们尝试一种流行的基于距离的算法，称为 k-最近邻算法</p><h2 id="043c" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="874b" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">KNN 包含几个超参数。最重要的一项调整是:</p><ol class=""><li id="ed74" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated">n_neighbors :这是投票时使用的邻居数量。数字越大，它就越精确(有时是这样)，但代价是速度。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="18ea" class="mx mb it nk b gy no np l nq nr"><strong class="nk iu">MAE Sccore:  0.431624658307474</strong></span></pre><p id="5adc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">查看简单模型的 MAE(平均绝对误差)分数，我们看到决策树是迄今为止最好的。这将是我们的基本模型。任何表现更好的模型都将成为下一个要超越的底线。</p><p id="a165" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，我们将从简单模型转向高级模型。许多高性能的机器学习模型通常基于集成。集成是一种结合多种机器学习算法的方法，以获得比任何单一算法都更好的预测性能。</p><p id="a9e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面我们将使用一些流行的合奏方法并比较它们的性能。</p><h1 id="db98" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">打包算法</h1><h1 id="3164" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">随机森林</h1><p id="2f42" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">随机森林算法是 bagging 的一种流行而有效的实现，它从一个引导样本构建多棵树。它有很高的预测能力，开箱即用。</p><h2 id="e599" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="2c30" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">随机森林包含许多超参数，但最重要的优化参数是:</p><ol class=""><li id="c542" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated">n_estimators :这是要创建的决策树的数量。正如我们可能猜测的那样，更多的树会产生更好的模型，但代价是更长的训练时间。</li><li id="b27d" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu">最大深度</strong>:这是单棵树的最大深度。数字越大，树越简单，因此随机森林集合也越简单。</li><li id="3f66" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_features: </strong>定义了允许分割的最大特征数。</li><li id="4b4c" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_samples_split: </strong>这是分割前叶节点所需的最小样本数。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="d0c8" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.41512597643232896</span></pre><h1 id="84c3" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">额外的树</h1><p id="77bd" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">额外树算法是 bagging 的另一个流行的实现，它类似于随机森林，但构建其基础树的方式不同，并且通常更快。它还具有很高的预测能力，有时甚至超过随机森林。</p><h2 id="c9cb" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="297f" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">额外的树包含与随机森林相似的超参数。要调整的重要参数有:</p><ol class=""><li id="d405" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> n_estimators </strong>:这是要创建的决策树的数量。正如我们可能猜测的那样，更多的树会产生更好的模型，但代价是更长的训练时间。</li><li id="14be" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu">最大深度</strong>:这是单棵树的最大深度。数字越大，树越简单，因此随机森林集合也越简单。</li><li id="372c" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_features: </strong>定义了允许分割的最大特征数。</li><li id="5eea" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_samples_split: </strong>这是分割前叶节点所需的最小样本数。</li></ol><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="66fa" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4131678014407999</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><h1 id="4790" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">BAGGING 元估计量</h1><p id="d8b0" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">sklearn 中的 bagging 估计器允许您从任何选择的模型中创建 Bagging 系综。也就是说，你可以从线性回归这样的单一模型中创建 bagging 系综，甚至是像随机森林或额外树这样的系综。</p><h2 id="0ebd" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="18b6" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这里要调整的重要超参数是:</p><ol class=""><li id="1150" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> base_estimator: </strong>这是在执行装袋时使用的估计值。bagging 元估计器从指定的基本估计器构建多个模型。</li><li id="6e9a" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> n_estimators </strong>:这是要创建的基本估计数。较高数量的估计器将导致更好的模型，但是也以更长的训练时间为代价。</li><li id="bbfa" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_samples </strong>:指定训练每个基本估计器的最大样本数。</li><li id="2280" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_features </strong>:这是训练每个基本估计器时从数据集中提取的最大特征数</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="5810" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4045400489482442</span></pre><p id="797e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">从误差指标来看，我们的下一个最佳模型是 bagging 回归器，它基于额外的树回归器。这成为我们新的基线模型。</p><p id="3e55" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">接下来，我们来尝试一些 boosting 算法。</p><h1 id="6534" class="ma mb it bd mc md me mf mg mh mi mj mk jz ml ka mm kc mn kd mo kf mp kg mq mr bi translated">助推算法</h1><p id="04f3" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">Boosting 是另一种流行且有效的集合技术。在 Boosting 中，多个模型被顺序训练。目标是训练比他们的前辈做得更好的模型。这意味着我们必须考虑以前的模型表现不佳的领域，并在这些领域进行改进。</p><h2 id="aa75" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">adaboost 算法</h2><p id="13dd" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">我们从称为 AdaBoost 的升压的普通实现开始。这是一种古老但广泛使用的 boosting 技术，在回归任务中表现良好，尽管众所周知它会过度拟合。</p><h2 id="279e" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="6f59" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这里要调整的重要超参数是:</p><ol class=""><li id="711d" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> n_estimators </strong>:这是要使用的基本估计数。通常，值越高，性能越好。</li><li id="d111" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> learning_rate </strong>:该参数控制训练权重的更新。它通常与 n 估计器一起调整。一个流行的经验法则是“当你将 n_estimator 增加 10 倍时，你也将学习速率降低 10 倍。</li><li id="1162" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> base_estimator </strong>:指定要提升的基本模型。</li><li id="ddbf" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_depth </strong>:指定基本估计值的深度。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="ef69" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4243339946387083</span></pre><h2 id="dc23" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">梯度推进</h2><p id="6170" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">另一种流行的提升算法是梯度提升。</p><h2 id="f7e4" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="3f73" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这里要调整的重要超参数是:</p><ol class=""><li id="66a4" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> n_estimators </strong>:这是要使用的基本估值器的数量。通常，值越高，性能越好。</li><li id="52cb" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_depth </strong>:指定基本估算器的深度。</li><li id="0c7a" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_samples_split </strong>:指定分割一个内部节点所需的最小样本数。</li><li id="940f" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_features </strong>:训练每个基本估计器时从数据集中提取的最大特征数</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="b542" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4063934439500354</span></pre><h2 id="113e" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">XGBOOST</h2><p id="c877" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">“当有疑问时，使用 XGBoost”是近年来 kaggle 平台上最流行的名言之一。</p><p id="9426" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">XGBoost 高效、快速，开箱即用。这是梯度增强的高级实现，确实非常有效。</p><h2 id="5abb" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="3015" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这里要调整的重要超参数是:</p><ol class=""><li id="cf58" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> eta </strong>:只是学习率的不同叫法。它有助于模型权重更新。</li><li id="5c65" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_child_weight:这个</strong>指定一个子节点中所需的所有特性的最小权重和。它主要用于控制过度拟合。</li><li id="a166" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_depth: </strong>该<strong class="kw iu"> </strong>用于定义每个树节点的最大深度。深度越大，模型越复杂，反之亦然。</li><li id="61c0" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_leaf_nodes: </strong>指定一棵树的最大叶子数。</li><li id="63d8" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated">Gamma:Gamma 值指定进行分割所需的最小损失减少量。只有当产生的分裂给出损失函数的正减少时，节点才被分裂。这些值可能因损失函数而异，应该进行调整。</li><li id="7987" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu">子样本</strong>:指定为每棵树随机选择的特征部分。较低的值使算法更简单，并防止过度拟合。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="7ad2" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4099352064907469</span></pre><h2 id="fb1c" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">LIGHTGBM</h2><p id="c0b1" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">LightGBM 是微软的 boosting 算法。这是最快的助推算法之一；高效的开箱即用，目前是大型结构化数据的首选。</p><h2 id="59b5" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="774d" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这里要调整的重要超参数是:</p><ol class=""><li id="eed9" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> num_iterations </strong>:指定要执行的增强迭代次数。</li><li id="0b1f" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu">叶子数量</strong>:指定一棵树要形成的叶子数量。<strong class="kw iu">注意</strong> : <em class="lu">在轻型 GBM 中，由于分裂发生在叶方向而不是深度方向，因此 num_leaves 必须小于 2^(max_depth，否则可能导致过拟合。</em></li><li id="c110" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> min_data_in_leaf </strong>:它是处理过拟合最重要的参数之一。</li><li id="befa" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> max_depth </strong>:指定一棵树可以生长的最大深度。此参数的值非常高会导致过度拟合。</li><li id="5f4d" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> bagging_fraction </strong>:用于指定每次迭代使用的数据的分数。该参数通常用于加速训练。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="6507" class="mx mb it nk b gy no np l nq nr">MAE Sccore:  0.4084352064907469</span></pre><h2 id="1b38" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">CATBOOST</h2><p id="348c" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">Catboost 是最近出现的 boosting 算法。它快速、高效且容易处理分类特征(它最好的特征之一)。</p><p id="399d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">CatBoost 可以自动处理分类变量，不需要像其他机器学习算法那样进行大量的数据预处理</p><h2 id="1adb" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated">我们能调什么</h2><p id="93ad" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">这里要调整的重要超参数是:</p><ol class=""><li id="613d" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated"><strong class="kw iu"> loss_function: </strong>指定用于训练的度量。</li><li id="f619" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu">迭代次数:</strong>可以构建的最大树数。</li><li id="98fb" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu"> learning_rate: </strong>指定用于减少梯度步长的学习率。</li><li id="1a80" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated"><strong class="kw iu">深度:</strong>指定要创建的树的深度。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ly lz l"/></div></figure><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="bffd" class="mx mb it nk b gy no np l nq nr">MAE Score: 0.4123613834195344</span></pre><h2 id="96f9" class="mx mb it bd mc my mz dn mg na nb dp mk ld nc nd mm lh ne nf mo ll ng nh mq ni bi translated"><strong class="ak">最终想法</strong></h2><p id="7c47" class="pw-post-body-paragraph ku kv it kw b kx ms ju kz la mt jx lc ld mu lf lg lh mv lj lk ll mw ln lo lp im bi translated">在这篇文章中，我们学习了如何获取预处理过的数据集，并尝试不同的机器学习模型。虽然本文基于回归任务，但是同样的步骤也可以应用于分类任务。这篇文章的一些要点是:</p><ol class=""><li id="c426" class="ns nt it kw b kx ky la lb ld nu lh nv ll nw lp nx ny nz oa bi translated">选择一个好的验证策略。(小型数据集的交叉验证，大型数据集的拆分)</li><li id="d1b5" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated">从简单开始，得到一个基线模型。</li><li id="28c1" class="ns nt it kw b kx ob la oc ld od lh oe ll of lp nx ny nz oa bi translated">通过调整和测量性能来不断改进模型。</li></ol><p id="2c75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">手动调整最佳模型后，更高级的技术是使用自动超参数搜索算法，如网格或随机搜索。</p><blockquote class="og"><p id="d88a" class="oh oi it bd oj ok ol om on oo op lp dk translated">记住，尽可能让你的模型简单易懂。</p></blockquote><p id="45e6" class="pw-post-body-paragraph ku kv it kw b kx oq ju kz la or jx lc ld os lf lg lh ot lj lk ll ou ln lo lp im bi translated">如果你从这一系列的帖子中学到了什么，别忘了鼓掌和分享。</p><p id="c117" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://github.com/risenW/medium_tutorial_notebooks/blob/master/supermarket_regression.ipynb" rel="noopener ugc nofollow" target="_blank">在 GitHub 上链接到带有代码和解释的完整笔记本。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/0864bceae82b36631903d1763ce643e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*I3Z1IbG2vimlzJpLDYsdxQ.jpeg"/></div></figure><p id="5cc5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<a class="ae lq" href="https://twitter.com/risingodegua" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> Twitter </strong> </a>上和我联系。</p><p id="ad03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<a class="ae lq" href="https://www.linkedin.com/in/risingdeveloper" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> LinkedIn </strong> </a>上与我联系。</p></div></div>    
</body>
</html>