<html>
<head>
<title>Crawlab — The Ultimate Live Dashboard For Web Crawler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">crawlab——网络爬虫的终极实时仪表板</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/crawlab-the-ultimate-live-dashboard-for-web-crawler-6c2d55c18509?source=collection_archive---------14-----------------------#2019-09-14">https://towardsdatascience.com/crawlab-the-ultimate-live-dashboard-for-web-crawler-6c2d55c18509?source=collection_archive---------14-----------------------#2019-09-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="75f2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">来监控你所有的爬虫！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ff23e0b2aefb938c9e2931d847a4ee56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AtfLyr8TCCPeoMUsSYap8Q.jpeg"/></div></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="kr ks l"/></div></figure><p id="713b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">最近，我发现了一个非常有趣而又强大的项目。虽然这个项目才开始 6 个月，但是已经有大约 24k 个赞了。该项目于 2019 年 3 月刚刚启动，从以下几点来看，它似乎很有前途。</p><ol class=""><li id="3330" class="lq lr iq kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">能够为网络爬虫监控不同种类的语言。比如 Python，NodeJS，Go，Java，PHP 以及各种网络爬虫框架包括 Scrapy，Puppeteer，Selenium。</li><li id="4931" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">包括一个漂亮的实时仪表板。</li><li id="007b" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">能够可视化的数据抓取，他们可以通过点击一个按钮下载。</li><li id="5263" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">你只需输入 XPath 和 URL 就可以创建一个爬虫，即所谓的“可配置爬虫”(不幸的是，最新版本 v0.3.0 已经暂时禁用了这个功能，参考可以在这里找到<a class="ae lp" href="https://tikazyq.github.io/crawlab-docs/Usage/Spider/ConfigurableSpider.html?fbclid=IwAR1T1zIvJE8tVInO6pByx4GcJrkNW4cltJBnW8-scs8caLs-fIkAwsT2p4I" rel="noopener ugc nofollow" target="_blank"/></li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="094f" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">Pyspider vs Crawlab</h2><p id="6f92" class="pw-post-body-paragraph kt ku iq kv b kw ne jr ky kz nf ju lb lc ng le lf lg nh li lj lk ni lm ln lo ij bi translated">以前我分享过 Pyspider 是最棒的监控工具之一，如果你没有读过，你可以点击下面的链接来阅读。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/pyspider-a-practical-usage-on-competitor-monitoring-metrics-c934d55f9c9a"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">为什么 Pyspider 可能是初学者最好的刮擦仪表板之一</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">py spider——竞争对手监控指标的实际应用</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa kp nm"/></div></div></a></div><p id="9b2d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">嗯，它们确实有一些相似之处，例如，它们都是爬虫程序的伟大仪表板，它们可以被调度，有一个令人印象深刻的仪表板来可视化…但是，如果你想知道显著的区别，在这里你去:</p><ol class=""><li id="7e56" class="lq lr iq kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">Pyspider 在可视化抓取网站的旅程方面更胜一筹。</li><li id="9fae" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">如果你想集成不同的语言或网络爬虫框架，Crawlab 更好。</li><li id="ef36" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">Crawlab 是用 Golang 写的，一般效率更高，速度更快。</li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="cd37" class="ob mm iq bd mn oc od oe mq of og oh mt jw oi jx mw jz oj ka mz kc ok kd nc ol bi translated">案例研究—将 Scrapy spider 集成到 Crawlab</h1><h2 id="3cf9" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 1 部分—安装 Crawlab</h2><p id="576f" class="pw-post-body-paragraph kt ku iq kv b kw ne jr ky kz nf ju lb lc ng le lf lg nh li lj lk ni lm ln lo ij bi translated">先决条件—在您的笔记本电脑上安装 Docker。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="b469" class="ml mm iq on b gy or os l ot ou">version: '3.3'<br/>services:<br/>  master: <br/>    image: tikazyq/crawlab:latest<br/>    container_name: master<br/>    environment:<br/>      CRAWLAB_API_ADDRESS: "localhost:8000"<br/>      CRAWLAB_SERVER_MASTER: "Y"<br/>      CRAWLAB_MONGO_HOST: "mongo"<br/>      CRAWLAB_REDIS_ADDRESS: "redis"<br/>    ports:    <br/>      - "8080:8080" # frontend<br/>      - "8000:8000" # backend<br/>    depends_on:<br/>      - mongo<br/>      - redis<br/>  mongo:<br/>    image: mongo:latest<br/>    restart: always<br/>    ports:<br/>      - "27017:27017"<br/>  redis:<br/>    image: redis:latest<br/>    restart: always<br/>    ports:<br/>      - "6379:6379"</span></pre><p id="a439" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">复制上面的代码，保存为<strong class="kv ir"><em class="ov">docker-compose . yml</em></strong>。然后在同一个目录中，在您的终端中键入命令<strong class="kv ir"> docker-compose up </strong>。docker 映像将被下载到您的本地。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="233c" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 2 部分—启动 Crawlab 并登录</h2><p id="3484" class="pw-post-body-paragraph kt ku iq kv b kw ne jr ky kz nf ju lb lc ng le lf lg nh li lj lk ni lm ln lo ij bi translated">在您的浏览器上导航到 localhost:8080，您将能够看到如下所示的登录页面。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/ede2f34b33f7d40dd20df86480db7398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xp9dbVvQDOe3IW3JTAM5bA.png"/></div></div></figure><p id="77c7" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">默认用户名:admin</p><p id="7c84" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">默认密码:admin</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="df8c" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 3 部分—上传 Scrapy 项目</h2><p id="6654" class="pw-post-body-paragraph kt ku iq kv b kw ne jr ky kz nf ju lb lc ng le lf lg nh li lj lk ni lm ln lo ij bi translated">转到这个<a class="ae lp" href="http://localhost:8080/#/spiders" rel="noopener ugc nofollow" target="_blank"> URL </a>，然后点击添加蜘蛛按钮，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/b46f5036861332e6ca0ad49923b21a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ctl0EDBiViGH6R8P2WE3xw.png"/></div></div></figure><p id="8d3c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">我正在为<a class="ae lp" href="https://www.gadgetsnow.com/" rel="noopener ugc nofollow" target="_blank"> gadgets now 网站</a>使用我的爬虫。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/858257b301d346a3fd5a94607329c47d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_EytEWkS1h7mEhE82e3zQ.png"/></div></div></figure><p id="a01b" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">上面的快照是我的 scrapy spider 目录，向下一级到包含 scrapy.cfg 的目录(用红框突出显示)，然后压缩这 3 个项目。最后，上传 zip 文件。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="2b74" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 4 部分—获取 MongoDB 的 IP 地址</h2><p id="0d9a" class="pw-post-body-paragraph kt ku iq kv b kw ne jr ky kz nf ju lb lc ng le lf lg nh li lj lk ni lm ln lo ij bi translated">检索该 docker 图像的 docker ID:<code class="fe oz pa pb on b">mongo:latest</code>。您可以使用下面的命令查看 docker id。</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="927a" class="ml mm iq on b gy or os l ot ou">docker ps</span></pre><p id="cf8e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">然后按照下面的命令输入 docker id:</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="3404" class="ml mm iq on b gy or os l ot ou">docker inspect -f '{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' &lt;input your docker id here&gt;</span></pre><p id="625c" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">接下来，您将在 docker 容器中获得您的 MongoDB 的 IP 地址。在我的例子中，IP 地址是 172.18.0.2。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="7476" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 5 部分—输入 IP 地址并修改 pipelines.py</h2><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="5a22" class="ml mm iq on b gy or os l ot ou">import os</span><span id="5316" class="ml mm iq on b gy pc os l ot ou">from pymongo import MongoClient</span><span id="2083" class="ml mm iq on b gy pc os l ot ou">MONGO_HOST ='172.18.0.2'<br/>MONGO_PORT = 27017 <br/>MONGO_DB = 'crawlab_test'</span><span id="c337" class="ml mm iq on b gy pc os l ot ou">class GadgetsnowPipeline(object):<br/>    mongo = MongoClient(host=MONGO_HOST, port=MONGO_PORT)<br/>    db = mongo[MONGO_DB]<br/>    col_name = os.environ.get('CRAWLAB_COLLECTION') <br/>    if not col_name:<br/>        col_name = 'test'<br/>    col = db[col_name]</span><span id="c790" class="ml mm iq on b gy pc os l ot ou">def process_item(self, item, spider):<br/>        item['task_id'] = os.environ.get('CRAWLAB_TASK_ID')<br/>        self.col.save(item)<br/>        return item</span></pre><p id="5b6d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这是修改后的 pipelines.py 的 Python 脚本。我想强调以下几点:</p><ol class=""><li id="9f02" class="lq lr iq kv b kw kx kz la lc ls lg lt lk lu lo lv lw lx ly bi translated">输入我们之前获得的 MongoDB 的 IP 地址:MONGO_HOST = ' 172.18.0.2 '。</li><li id="13b7" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">复制上面的<code class="fe oz pa pb on b">process_item</code>函数，并将其替换到您原来的 pipelines.py 文件中。</li><li id="53b6" class="lq lr iq kv b kw lz kz ma lc mb lg mc lk md lo lv lw lx ly bi translated">MONGO_DB 的值可以是 MongoDB 中您想要的任何数据库名称，对于我的例子，我将其设置为<code class="fe oz pa pb on b">crawlab_test</code>。</li></ol></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="8d1e" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 6 部分—在 items.py 中添加两个新字段</h2><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="4306" class="ml mm iq on b gy or os l ot ou">task_id = scrapy.Field()<br/>_id = scrapy.Field()</span></pre><p id="d5ea" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">这两个字段需要添加到<code class="fe oz pa pb on b">items.py</code>中。</p><p id="f89d" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">首先，<code class="fe oz pa pb on b">task_id</code>是您已经执行的每个任务的标识符，您可以在 spider - &gt; spider_name - &gt;任何任务- &gt;概览选项卡中查看它。</p><p id="d09e" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">其次，<code class="fe oz pa pb on b">_id</code>是 MongoDB 中每个对象的惟一标识符。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="d2e1" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 7 部分—运行您的蜘蛛</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/62d0d6de0de7fd89a82cf441c006cdb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p-i2Xpv8HYkV2xBtlH9J0Q.png"/></div></div></figure><p id="3585" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">点击你新上传的蜘蛛，然后输入执行命令。因为我的 Scrapy 爬虫的名字是 gdgnow，所以我的命令应该是:</p><pre class="kg kh ki kj gt om on oo op aw oq bi"><span id="aadd" class="ml mm iq on b gy or os l ot ou">scrapy crawl gdgnow</span></pre><p id="8e00" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">然后，先点击<strong class="kv ir">保存</strong>按钮，再点击<strong class="kv ir">运行</strong>按钮开始刮削。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h2 id="c271" class="ml mm iq bd mn mo mp dn mq mr ms dp mt lc mu mv mw lg mx my mz lk na nb nc nd bi translated">第 9 部分——可视化结果</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/7480ace50a5110ff293411443811aeff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWb0cyZigfciv6xsJzTcXg.png"/></div></div></figure><p id="52bc" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">以我的爬虫为例，上面的快照显示了我的爬虫的输出，最重要的是，您可以通过点击<strong class="kv ir">下载 CSV </strong>按钮下载 CSV 格式的文件。</p><p id="2316" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">对于<code class="fe oz pa pb on b">item_desc</code>字段，它显示的是<code class="fe oz pa pb on b">undefined</code>，因为我的<code class="fe oz pa pb on b">item_desc</code>是 JSON 格式的，但是 Crawlab 还不支持输出 JSON 字段。如果您希望在输出数据中包含 JSON 字段，目前唯一的选择是登录包含 MongoDB 的 docker，crawlab 将数据传输到这个 docker 并从中提取数据。</p><p id="6d25" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">展示相当惊人，向 Crawlab 的所有开发者致敬！</p><h1 id="ecaf" class="ob mm iq bd mn oc pf oe mq of pg oh mt jw ph jx mw jz pi ka mz kc pj kd nc ol bi translated">最后的想法</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/b76ab02b2090d06b88db8739884a6d99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GE8YO2AZ0g3W4t7FK4l6kA.jpeg"/></div></div></figure><p id="7704" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">非常感谢你耐心阅读到最后。Crawlab 仍处于早期阶段，但它是一个非常有前途的爬虫框架，特别是在监视多个网络爬虫方面。</p><p id="fbfd" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">由于这只是对 Crawlab 的一个简单介绍，我还没有包括 Crawlab 的所有功能，例如，cron job，如何集成其他网络爬虫的框架等等。如果你真的想让我分享更多关于 Crawlab 的内容，请在下面的<strong class="kv ir">评论，我会为此再写一篇文章！</strong></p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="088f" class="ob mm iq bd mn oc od oe mq of og oh mt jw oi jx mw jz oj ka mz kc ok kd nc ol bi translated">关于作者</h1><p id="e7d3" class="pw-post-body-paragraph kt ku iq kv b kw ne jr ky kz nf ju lb lc ng le lf lg nh li lj lk ni lm ln lo ij bi translated"><a class="ae lp" href="https://www.linkedin.com/in/lowweihong/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">低伟鸿</a>是 Shopee 的数据科学家。他的经验更多地涉及抓取网站，创建数据管道，以及实施机器学习模型来解决业务问题。</p><p id="1186" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">他提供爬行服务，可以为你提供你需要的准确和干净的数据。你可以访问<a class="ae lp" href="https://www.thedataknight.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kv ir">这个网站</strong> </a>查看他的作品集，也可以联系他获取<strong class="kv ir">抓取服务</strong>。</p><p id="fd00" class="pw-post-body-paragraph kt ku iq kv b kw kx jr ky kz la ju lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">你可以在<a class="ae lp" href="https://www.linkedin.com/in/lowweihong/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lp" href="https://medium.com/@lowweihong?source=post_page---------------------------" rel="noopener"> Medium </a>上和他联系。</p></div></div>    
</body>
</html>