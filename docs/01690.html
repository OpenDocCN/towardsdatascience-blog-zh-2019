<html>
<head>
<title>Natural Homotopies of Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络的自然同伦</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-and-doughnuts-c2f0f7b7c598?source=collection_archive---------9-----------------------#2019-03-19">https://towardsdatascience.com/deep-learning-and-doughnuts-c2f0f7b7c598?source=collection_archive---------9-----------------------#2019-03-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/84b02b56b071f14cfd150e713267c671.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S-BqYCwp3sU1PugYHP4BFQ.png"/></div></div></figure><div class=""/><h1 id="3c27" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">流形学习</h1><p id="8565" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在流形假设下，现实世界的高维数据集中在一个非线性的低维流形附近<strong class="lb jf">【2】</strong>。换句话说，数据大致位于一个比输入空间维度低得多的流形上，一个可以被检索/学习的流形<strong class="lb jf">【8】</strong></p><p id="939c" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">为了应对维数灾难，流形假设是至关重要的:如果我们期望机器学习算法学习在高维空间中有有趣变化的函数，许多机器学习模型问题似乎是没有希望的<strong class="lb jf">【6】</strong></p><p id="b63d" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">幸运的是，经验证明，人工神经网络<em class="mc">由于其分级、分层的结构</em><strong class="lb jf">【3】，能够捕捉普通数据的几何规律。[3] </strong>展示了证明处理位于低维流形上或附近的数据的能力的实验。</p><p id="bf2e" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">然而，ANN 层如何识别原始数据空间到合适的低维流形之间的映射(表示)？</p><h1 id="f278" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">同胚线性嵌入</h1><p id="8cdb" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">根据<strong class="lb jf">【10】提供的定义，</strong>一个<em class="mc">同胚，也叫连续变换，是两个几何图形或拓扑空间中的点之间在两个方向上连续的等价关系和一一对应关系。</em></p><ul class=""><li id="d418" class="md me je lb b lc lx lg ly lk mf lo mg ls mh lw mi mj mk ml bi translated">同样保持距离的同胚叫做等距。</li><li id="b9bf" class="md me je lb b lc mm lg mn lk mo lo mp ls mq lw mi mj mk ml bi translated">仿射变换是另一种常见的几何同胚。</li></ul><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/35e3ba77072e8f04fc3c55c53e85bb18.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/1*Zt6gwCRtO4VXaKfGQvIHog.gif"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">A continuous deformation between a coffee <a class="ae na" href="https://en.wikipedia.org/wiki/Mug" rel="noopener ugc nofollow" target="_blank">mug</a> and a doughnut (<a class="ae na" href="https://en.wikipedia.org/wiki/Torus" rel="noopener ugc nofollow" target="_blank">torus</a>) illustrating that they are homeomorphic. But there need not be a <a class="ae na" href="https://en.wikipedia.org/wiki/Homotopy" rel="noopener ugc nofollow" target="_blank">continuous deformation</a> for two spaces to be homeomorphic — only a continuous mapping with a continuous inverse function <strong class="bd nb">[4]</strong></figcaption></figure><p id="6c66" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【1】</strong>看 tanh 图层。tanh 层<em class="mc"> tanh(Wx+b) </em>包括:</p><ol class=""><li id="3d5c" class="md me je lb b lc lx lg ly lk mf lo mg ls mh lw nc mj mk ml bi translated"><em class="mc">通过“权重”矩阵 W 进行线性变换</em></li><li id="80d9" class="md me je lb b lc mm lg mn lk mo lo mp ls mq lw nc mj mk ml bi translated"><em class="mc">甲由矢乙翻译</em></li><li id="9d56" class="md me je lb b lc mm lg mn lk mo lo mp ls mq lw nc mj mk ml bi translated"><em class="mc">tanh 的逐点应用</em></li></ol><p id="843f" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">虽然流形学习方法明确地学习低维空间，但是神经网络层是到不一定是低维空间的非线性映射。实际情况就是这样:我们来看具有 N 个输入和 N 个输出的双曲正切层。</p><p id="ab57" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">在这样的 tanh-layers 中，<em class="mc">每一层都拉伸和挤压空间，但它从不切割、破坏或折叠空间。直观上，我们可以看到它保留了拓扑性质[..如果权矩阵 W 是非奇异的，则具有 N 个输入和 N 个输出的 Tanh 层是同胚的。(尽管需要注意领域和范围)</em><strong class="lb jf">【1】</strong>。</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8525050751331bd42eadd9de339f43f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/1*Zqo5ioW-IBzP3veenPc6dA.gif"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">A four-hidden-layers tanh ANN discriminates between two slightly entangled spirals by generating a new data representation where the two classes are linearly separable <strong class="bd nb">[1]</strong></figcaption></figure><p id="40be" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">同胚和可逆的概念与可解释性深深交织在一起:<em class="mc">理解特征空间中的变换如何与相应的输入相关是迈向可解释深度网络的重要一步，可逆深度网络可以在这种分析中发挥重要作用，因为例如，人们可以潜在地从特征空间回溯属性到输入空间</em><strong class="lb jf">【11】</strong></p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3d27cfe1c892e3283da9b85b88e3b215.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*OodwDz6L8QnNzZyaD9ldWQ.png"/></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">An example of problems that arise in mapping manifolds not diffeomorphic to each other. The “holes” in the first manifold prevent a smooth mapping to the second <strong class="bd nb">[12]</strong>. It is a good idea to characterize the learnability of different neural architectures by computable measures of data complexity such as persistent homology <strong class="bd nb">[13]</strong></figcaption></figure><p id="21a8" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">不幸的是，并不总是可能找到同胚映射。<em class="mc">如果数据集中在具有非平凡拓扑的低维流形附近，则不存在到斑点状流形(先验质量集中的区域)的连续可逆映射</em><strong class="lb jf">【12】</strong></p><p id="7dcf" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">让我们回到描述 ANN 层中发生的事情的目标。通过构造同伦，我们可以分析激活函数中非线性程度的增加如何改变 ANN 层将数据映射到不同空间的方式。</p><h1 id="2868" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">自然同伦</h1><p id="7cff" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">两个映射 f0 和 f1 是同伦的，f0 ≃ f1，如果存在一个映射，同伦 F : X × I → Y 使得 f0(x) = F(x，0)和 f1(x) = F(x，1)对于所有的 x∈x<strong class="lb jf">【9】</strong></p><p id="64e1" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【6】</strong>通过将单层感知器中的节点传递函数从线性映射转换为 s 形映射来构造同伦:</p><blockquote class="nf ng nh"><p id="6578" class="kz la mc lb b lc lx le lf lg ly li lj ni lz lm ln nj ma lq lr nk mb lu lv lw im bi translated">通过使用将线性网络变形为非线性网络的自然同伦，我们能够探索通常用于分析线性映射的几何表示如何受到网络非线性的影响。具体地，输入数据子空间被网络转换成曲线嵌入数据流形“<strong class="lb jf"><em class="je">【6】</em></strong></p></blockquote><figure class="ms mt mu mv gt iv"><div class="bz fp l di"><div class="nl nm l"/></div></figure><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/697a3d86dc2b85d689eada46c08efb72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QPcWVqbCnY_jLcDxfPDP2A.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">The data manifold for L=3, s=2 and three weights at 𝜏=1 <strong class="bd nb">[6]</strong></figcaption></figure><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/a546cf967576cc9bd6fc8661c38d9711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8fM8XacEvx1aA2LqMh-wBw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">An intuition of how curvature relates to the existence of multiple projections of y on Z <strong class="bd nb">[6]</strong></figcaption></figure><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/b0f19cdab9eaf87f89489bbff4029c29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SGDYdOglaQSkkYel4jqgLw.png"/></div></div><figcaption class="mw mx gj gh gi my mz bd b be z dk">An example data manifold Z with boundaries Pa,b = Z ± (1/ |k|max)<strong class="bd nb">n </strong>where <strong class="bd nb">n </strong>is the normal to the surface. For all desired vectors y in the region between Pa and Pb, there exists only one solution. It is important to remark that the mapping is not homeomorphic: the mapping is not invertible and Z folds on itself, infinitely <strong class="bd nb">[6]</strong></figcaption></figure><h1 id="92c2" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">结论</h1><p id="0d23" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在流形假设下，学习相当于发现一个非线性的、低维的流形。在这篇简短的博客中，我试图提供一个简短的、直观的、当然不完全全面的直觉，来说明人工神经网络如何将原始数据空间映射到一个合适的低维流形。</p><p id="4f49" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">对于不同的人工神经网络体系结构和分类问题，在层级别可视化映射(表示)的一个很好的工具是可用的<a class="ae na" href="https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" rel="noopener ugc nofollow" target="_blank">这里</a><strong class="lb jf">【15】</strong>它是令人敬畏的。</p><p id="d899" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">免责声明:本博客中的观点是我的，可能的错误和误解也是我的</p><h1 id="bd5b" class="kb kc je bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考</h1><p id="1810" class="pw-post-body-paragraph kz la je lb b lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="lb jf">【1】</strong><a class="ae na" href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/</a></p><p id="a7e1" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【2】</strong>劳伦斯·凯顿。<a class="ae na" href="http://www.lcayton.com/resexam.pdf" rel="noopener ugc nofollow" target="_blank">流形学习算法。</a>“加州大学圣地亚哥分校。代表 12.1–17(2005 年):1。()</p><p id="3edd" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【3】</strong>巴斯里，罗嫩，大卫雅各布。"<a class="ae na" href="https://openreview.net/pdf?id=BJ3filKll" rel="noopener ugc nofollow" target="_blank">使用深度网络有效表示低维流形。</a><em class="mc">arXiv 预印本 arXiv:1602.04723 </em> (2016)。</p><p id="ba01" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【4】</strong><a class="ae na" href="https://en.wikipedia.org/wiki/Homeomorphism" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Homeomorphism</a></p><p id="459e" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">库切、弗兰斯·m 和弗吉尼亚·l·斯通尼克。“关于线性和非线性单层网络之间的自然同伦。”神经网络汇刊 7.2(1996):307–317。</p><p id="62dd" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【6】</strong>Coetzee，Frans Martin，“<a class="ae na" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.59.2508&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">神经网络和其他非线性方程组的分析和求解的同伦方法。</a><em class="mc">博士论文，卡耐基·梅隆大学，5 月</em> (1995)。</p><p id="9228" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【7】</strong>阿迪卡里，玛希玛·兰詹。<a class="ae na" href="https://www.springer.com/cda/content/document/cda_downloaddocument/9788132228417-c2.pdf?SGWID=0-0-45-1588753-p179972415" rel="noopener ugc nofollow" target="_blank">基本代数拓扑及其应用。斯普林格</a>，2016。</p><p id="087d" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【8】</strong>Pierre Geurts，Gilles Louppe，Louis Wehenkel，<a class="ae na" href="http://www.montefiore.ulg.ac.be/~geurts/Cours/AML/aml2017_2018.html" rel="noopener ugc nofollow" target="_blank">迁移学习及相关协议</a>，讲义，2018</p><p id="349a" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【9】</strong>杰斯珀·莫勒，<a class="ae na" href="http://web.math.ku.dk/~moller/e01/algtopI/comments.pdf" rel="noopener ugc nofollow" target="_blank">初学者同伦理论</a>，课堂讲稿</p><p id="12c8" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf"><strong class="lb jf"/><a class="ae na" href="http://mathworld.wolfram.com/Homeomorphism.html" rel="noopener ugc nofollow" target="_blank">http://mathworld.wolfram.com/Homeomorphism.html</a></strong></p><p id="202d" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">雅各布森、约恩-亨里克、阿诺德·斯默德斯和爱德华·奥雅伦。i-revnet:深度可逆网络。“arXiv 预印本 arXiv:1802.07088 (2018)。</p><p id="fd28" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【12】</strong>法洛西，卢卡等.<a class="ae na" href="https://arxiv.org/pdf/1807.04689.pdf" rel="noopener ugc nofollow" target="_blank">同胚变分自动编码的探索。</a>“arXiv 预印本 arXiv:1807.04689 (2018)。</p><p id="d0e4" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">古斯、威廉·h 和鲁斯兰·萨拉胡季诺夫。<a class="ae na" href="https://arxiv.org/pdf/1802.04443.pdf" rel="noopener ugc nofollow" target="_blank">关于用代数拓扑来表征神经网络的能力。</a>“arXiv 预印本 arXiv:1802.04443 (2018)。</p><p id="8d52" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【14】</strong>古德菲勒、伊恩、约舒阿·本吉奥和亚伦·库维尔。<em class="mc">深度学习</em>。麻省理工学院出版社，2016 年。</p><p id="9cb8" class="pw-post-body-paragraph kz la je lb b lc lx le lf lg ly li lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><strong class="lb jf">【15】</strong>h<a class="ae na" href="https://cs.stanford.edu/people/karpathy/convnetjs//demo/classify2d.html" rel="noopener ugc nofollow" target="_blank">ttps://cs . Stanford . edu/people/karpathy/convnetjs//demo/classify 2d . html</a></p></div></div>    
</body>
</html>