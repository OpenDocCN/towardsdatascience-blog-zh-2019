<html>
<head>
<title>Feature Selection Why &amp; How Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择为什么和如何解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-why-how-explained-part-2-352d9130c2e1?source=collection_archive---------16-----------------------#2019-06-11">https://towardsdatascience.com/feature-selection-why-how-explained-part-2-352d9130c2e1?source=collection_archive---------16-----------------------#2019-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="52f2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Python 中特征选择算法的实现</h2></div><p id="baf1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated"><span class="l lc ld le bm lf lg lh li lj di">在</span>的上一篇文章中，我解释了在模型构建中包含不相关或相关特征的问题。在本文中，我将向您展示选择算法的几个简洁的实现，它们可以很容易地集成到您的项目管道中。</p><p id="730a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在深入详细的实现之前，让我们先来看一下我创建的数据集。数据集有 20 个要素，其中 5 个对输出有贡献，2 个是相关的。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><h1 id="e63f" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">1.包装特征选择</h1><p id="993d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">检测能力:★★★☆速度:★☆☆☆</p><p id="d582" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">包装算法很简单。通过训练和验证模型来检查特征的有效性。这意味着包装器在处理大型数据集时非常慢。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/c5003066384f72e0016f052bcd650cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*GU-kVZf7mLXli-sMuPS2SA.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">wrappers are iterative/recursive in nature</figcaption></figure><p id="70d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管添加/删除特性的具体标准可能不同，但核心思想是相同的。因此，我将重点介绍一种叫做反向选择的特殊方法。</p><h2 id="d021" class="mv ls iq bd lt mw mx dn lx my mz dp mb ko na nb md ks nc nd mf kw ne nf mh ng bi translated">— 1.1.包装算法:向后选择</h2><p id="c710" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">提供具有特征系数(例如回归)或重要性因子(例如树)的模型，算法从所有特征开始，贪婪地消除最不重要的特征。一旦删除了所有特征，该算法就返回给出最佳性能的子集。</p><h2 id="40e4" class="mv ls iq bd lt mw mx dn lx my mz dp mb ko na nb md ks nc nd mf kw ne nf mh ng bi translated">— 1.2.Python 中的向后选择:</h2><p id="731d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">Scikit-Learn 提供了一个很好的实现，称为<a class="ae nh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank"> RFECV </a>(递归特征消除和交叉验证选择)，一种基于验证分数的算法来消除不相关的特征。</p><p id="7043" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae nh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank"> RFE 需要几个参数，你必须提供这两个</a>:</p><blockquote class="ni nj nk"><p id="013d" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated"><strong class="kh ir">估计器</strong> : <em class="iq">对象</em></p><p id="205d" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated">使用<code class="fe np nq nr ns b"><em class="iq">fit</em></code>方法的监督学习估计器，通过<code class="fe np nq nr ns b"><em class="iq">coef_</em></code>或<code class="fe np nq nr ns b"><em class="iq">feature_importances_</em></code>提供关于特征重要性的信息</p><p id="ec51" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated"><strong class="kh ir">评分:<em class="iq">字符串，可调用或无，可选，(默认=无)</em> </strong></p></blockquote><p id="dd1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Scikit-Learn 中有许多估值器满足标准。在这个例子中，使用了一个简单的 SVM 分类器。评分函数是评估模型性能(如准确性)的度量。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/7327e64d1d2cafdeb60294675557ba56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*4WhSUx6HfzvIyWGUUan5nw.png"/></div></figure><p id="6884" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">RFECV 选择了 7 个特征，包括所有相关特征和冗余特征。这表明 RFECV 不擅长移除多重共线性(因为共线性要素的系数往往变化很大)。因此，最好先识别多重共线性(例如使用条件号识别<a class="ae nh" href="https://medium.com/@zhangzix/feature-selection-why-how-explained-part-1-c2f638d24cdb" rel="noopener">)。</a></p><h1 id="3c5f" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">2.过滤特征选择</h1><p id="2998" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">检测能力:★★☆☆☆速度:★★★</p><p id="4e05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在使用数百万个数据点的真实应用程序中，包装器变得不切实际。过滤算法通过统计测试(例如相关系数)而不是训练实际模型来估计特征的有效性，从而解决问题。</p><p id="7dc5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然有很多先进的过滤方法，如 mRMR，但我更喜欢这种简单的两步方法。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/ca4dcea4c06627e66a92b18b65914cce.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*4Y0izzK1TD5UAqdrUUzioQ.png"/></div></figure><h2 id="3bab" class="mv ls iq bd lt mw mx dn lx my mz dp mb ko na nb md ks nc nd mf kw ne nf mh ng bi translated">— 2.1.移除相关要素:条件编号</h2><p id="4d50" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">正如第一篇文章中介绍的，共线性或多重共线性是有害的。下面是我自己开发的一个算法，可以消除两者。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/e8f83b34e82d1317d069e3d1db670100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKIhNKUgwrkbXtYuHDG1DA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk"><a class="ae nh" href="https://pareonline.net/getvn.asp?v=13&amp;n=5" rel="noopener ugc nofollow" target="_blank">“Revisiting the Collinear Data Problem: An Assesment of Estimator ‘Ill-Conditioning’ in Linear Regression”</a></figcaption></figure><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><h2 id="c9bf" class="mv ls iq bd lt mw mx dn lx my mz dp mb ko na nb md ks nc nd mf kw ne nf mh ng bi translated">— 2.2.移除不相关的特征:单变量选择</h2><p id="8e09" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">单变量选择对特征和输出执行统计测试，并且仅保留具有高分的特征。常见的测试包括卡方检验、方差分析或互信息。</p><p id="ee1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，Scikit-Learn 提供了一个很好的函数，称为 GenericUnivariateSelect。<a class="ae nh" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect" rel="noopener ugc nofollow" target="_blank">该功能需要 3 个参数</a></p><blockquote class="ni nj nk"><p id="b75b" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated"><strong class="kh ir"> score_func </strong> : <em class="iq">可调用</em></p><p id="bdb7" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated">函数采用两个数组 X 和 y，并返回一对数组(分数、pvalues)。对于“百分位数”或“kbest”模式，它可以返回单个数组分数。</p><p id="489b" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated"><strong class="kh ir">模式</strong> : <em class="iq"> { '百分位'，' k_best '，' fpr '，' fdr '，' fwe'} </em></p><p id="1db2" class="kf kg nl kh b ki kj jr kk kl km ju kn nm kp kq kr nn kt ku kv no kx ky kz la ij bi translated"><strong class="kh ir"> param </strong> : <em class="iq"> float 或 int 取决于特征选择模式</em></p></blockquote><p id="c595" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">选择您想要的统计方法(如卡方检验、F 值)作为评分函数。我将使用卡方，因为 y 是绝对的。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi oa"><img src="../Images/8a9149f72a61edc3ae993df278d6d29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AS6q7vXS9uJUAssnMEeLJQ.png"/></div></div></figure><p id="43d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该算法会丢弃 P 值大于 0.5 的要素。使用不太小的阈值是一个很好的实践，这样我们就不会丢失有用的特性(我们在这里丢失了一个有用的特性)。</p><h1 id="0346" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated"><strong class="ak"> 3。嵌入式特征选择</strong></h1><p id="8c90" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">检测能力:★★★☆☆速度:★★★☆☆</p><p id="7076" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">嵌入式特征选择在训练期间智能地丢弃不必要的特征。常见的算法包括 Lasso 回归、决策树和自动编码神经网络。我将在这里解释套索回归。</p><h2 id="4d0b" class="mv ls iq bd lt mw mx dn lx my mz dp mb ko na nb md ks nc nd mf kw ne nf mh ng bi translated">— 3.1.嵌入式算法:Lasso 回归</h2><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi ob"><img src="../Images/7ae830efb3a7fa58184db8e68e77ec40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V-DqQO7E-QkNwHIQiHyHVA.png"/></div></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">Lasso adds a penalty term to the loss function</figcaption></figure><p id="faff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">套索回归因使用大的要素权重/过多的要素而对模型不利。因此，模型倾向于使用尽可能少的参数，从而产生稀疏的结果(例如，一些特征的权重为 0)。</p><h2 id="757c" class="mv ls iq bd lt mw mx dn lx my mz dp mb ko na nb md ks nc nd mf kw ne nf mh ng bi translated">— 3.2.Python 中的套索回归</h2><p id="fc7a" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">像往常一样，你可以在 Scikit-Learn 中找到 lasso 回归。在初始化模型时，您必须提供的唯一参数是 Alpha，它决定了惩罚的强度。更大的 alpha 导致更稀疏的结果，alpha=0 实质上意味着没有套索惩罚。</p><figure class="lk ll lm ln gt lo gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/80303f648dabb82e60301347909065ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*UWS6pVj_Ex7YNao2iJd97g.png"/></div><figcaption class="mr ms gj gh gi mt mu bd b be z dk">an example of how alpha affects feature selection</figcaption></figure><p id="02ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如您所见，选择强度由 Alpha 决定。问题是我们不知道应该用什么α。这个问题可以通过选择在交叉验证中表现最好的值来解决。</p><figure class="lk ll lm ln gt lo"><div class="bz fp l di"><div class="lp lq l"/></div></figure><p id="5256" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型最终使用 5 个参数，这意味着所有冗余和相关的特征都被排除在模型之外。</p><h1 id="bd79" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">摘要</h1><p id="4edc" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">每种类型的选择算法都有其优点和缺点，并且没有使用什么方法的经验法则。在项目环境中考虑权衡是很重要的。</p><ol class=""><li id="7ce1" class="od oe iq kh b ki kj kl km ko of ks og kw oh la oi oj ok ol bi translated">包装器提供了最高的选择质量，因为它实际上在不同的特征子集上训练模型。高质量的代价是培训时间。</li><li id="f876" class="od oe iq kh b ki om kl on ko oo ks op kw oq la oi oj ok ol bi translated">过滤器的选择基于统计数据，速度非常快。但是，它可能无法检测到最佳的特征子集。</li><li id="57bc" class="od oe iq kh b ki om kl on ko oo ks op kw oq la oi oj ok ol bi translated">中间嵌入选择，以公允的价格提供合理的选择力。</li></ol></div></div>    
</body>
</html>