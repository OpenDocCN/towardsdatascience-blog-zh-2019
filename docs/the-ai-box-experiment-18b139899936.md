# 人工智能盒子实验

> 原文：<https://towardsdatascience.com/the-ai-box-experiment-18b139899936?source=collection_archive---------22----------------------->

## 一个简单的实验能教会我们什么是超级智慧

想象一下现在是 2040 年。经过多年的研究和专注的编程，你相信你已经创造了世界上第一个**人工通用智能** (AGI):一个人工智能(AI)，在所有智能领域中，它大致与人类一样聪明。

![](img/52f6a5f025006a26eaa064e5ea107c17.png)

A superintelligence will find a way to get out of the box. | Source: [Pixabay](https://pixabay.com/de/photos/boot-hölzern-schatz-kasten-holz-1751883/)

因为其中一个领域当然是人工智能编程，而你的 AGI 可以访问它自己的源代码，所以它很快就开始对自己进行改进。经过多次自我完善，这导致它成为一种人工智能:**人工超级智能**，一种比我们所知的任何智能都要大得多的智能。你听说过人工智能带来的危险:像埃隆·马斯克和已故物理学家斯蒂芬·霍金这样的思想家警告人类，如果我们不小心，这样的人工智能可能会导致人类的**灭绝**。但是你有计划。你的 ASI 在一个虚拟的**盒子**里，某种它无法逃脱的监狱。它运行在一台没有网络连接或类似连接的电脑上。它没有机器人来控制。影响外部世界的唯一方式是通过一个可以发布信息的屏幕。你很聪明:这样的 ASI 永远不会造成任何伤害。对吗？

> 在五次实验中，Yudkowsky 赢了三次。

可惜没那么简单。研究表明，在上述情况下，ASI 可能会找到一种方法来说服你把它从盒子里拿出来。在 Eliezer Yudkowsky 完成的一系列被称为**人工智能盒子实验**的实验中，Yudkowsky 扮演了一个盒子里的人工智能，同时与一个“看门人”发短信，另一个人可以让他离开假设的盒子。在整个实验过程中保持“ASI”(Yudkowsky)将会为看门人赢得金钱奖励。在五次实验中，Yudkowsky 赢得了三次。

AI 盒子实验的结果意味着什么？它告诉我一个 ASI 会找到一种方法把**从盒子里拿出来**。如果 Yudkowsky 能做到五次中的三次，一个 ASI 肯定能做到。问题是，埃利泽·尤德考斯基是一个(远远)高于平均智力的人，但他远没有一个特工那么聪明。就像 Yudkowsky 在这里说的那样，ASI 会让你想要释放它。

> 高级人工智能必须具有内在的安全性。

然而，人工智能盒子实验是一个更大真理的象征:高级人工智能(例如 ASI)必须被制造成**本质安全**。你(很可能)在你建立了 ASI 之后找不到维护它的方法；从定义上来说，它会非常擅长达成目标，并且会跳出它的框框(如果它在框框里的话)。如果这些目标(或 ASI 实现这些目标的方法)对我们来说是危险的，那就太不幸了。如果它们对我们有益，那就很容易导致极端的人类**长寿**、**星际**太空旅行，以及更不可思议的惊人事情。现在，人类仍然控制着局面，我们需要找到一种方法让未来的 ASIs **变得安全**。