<html>
<head>
<title>Food for Thought — Paper Tuesday</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">思考的食粮——纸星期二</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/food-for-thought-paper-tuesday-d163f8339d26?source=collection_archive---------31-----------------------#2019-12-18">https://towardsdatascience.com/food-for-thought-paper-tuesday-d163f8339d26?source=collection_archive---------31-----------------------#2019-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="935f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">调和过度参数化与偏差-方差权衡</h2></div><p id="f2d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每周二，我都会强调我在研究或工作中遇到的一篇有趣的论文。希望我的评论能帮助你在 2 分钟内获得论文中最多汁的部分！</p><h1 id="aeac" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">基本思想</h1><p id="0ed7" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">数据过量可能是任何机器学习课程中最基本的方法之一。这个想法很简单——太适合训练数据(通常有很多参数)的模型在测试集上的表现往往很差。过度拟合通常发生在模型被大量参数化时，如具有太多分裂的决策树或未规范化的回归模型。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi gj"><img src="../Images/29538a6102a99229f80dc181e4e5c21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XGs7oF4W8f-rAEYZ.jpg"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">Picture from <a class="ae mn" href="https://stats.stackexchange.com/questions/292283/general-question-regarding-over-fitting-vs-complexity-of-models" rel="noopener ugc nofollow" target="_blank">StackOverflow</a></figcaption></figure><p id="99f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我想强调一篇相当令人兴奋的论文。Mikhai Belkin，Daniel Hsu，马思远和 Soumik Mandal 在他们 2019 年的论文<em class="mo">中证明了现代机器学习实践和偏差-方差权衡的调和</em>表明，如果模型足够复杂(他们称之为双下降曲线)，测试误差最终会再次下降。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mp"><img src="../Images/76380c9c6ae123712a1bf6a84e3180c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8oA-qwGhpHjONEOSOojeRQ.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">from the paper: <a class="ae mn" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">Belkin et al 2019</a></figcaption></figure><p id="a2e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">论文网址:【https://arxiv.org/pdf/1812.11118.pdf T2】</p><h1 id="398b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">结果</h1><p id="6c27" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">研究人员在几个著名的数据集(MNIST，CIFAR10，SVHN)上，在广泛的机器学习模型(神经网络和基于树的模型，RFF)上测试了这一想法，并获得了一致的结果。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mq"><img src="../Images/8b42483a2ce57dd2d7bc9c4084211d78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8X51WmhWmiKlCUDLDlachA.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">from the paper: <a class="ae mn" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">Belkin et al 2019</a></figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi mr"><img src="../Images/c56fda572a6ae7ef0cdcb49d9d317843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b5GX10EC3xoSj9KpJi7rKw.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">from the paper: <a class="ae mn" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">Belkin et al 2019</a></figcaption></figure><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="gh gi ms"><img src="../Images/71515b806ed4bd24f035285b56432980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-C9nU9DsFBUk8UpxFJI5A.png"/></div></div><figcaption class="mj mk gj gh gi ml mm bd b be z dk">from the paper: <a class="ae mn" href="https://arxiv.org/abs/1812.11118" rel="noopener ugc nofollow" target="_blank">Belkin et al 2019</a></figcaption></figure><h1 id="ce5a" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">一些想法</h1><p id="c214" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">巧合的是，我正在为我的本科论文研究神经网络的过度参数化，我看到了几篇论文从数学上证明了如何更容易优化更大的网络(<a class="ae mn" href="https://viterbi-web.usc.edu/~soltanol/onehidden.pdf" rel="noopener ugc nofollow" target="_blank"> Oymak 和 Soltanolkotabi 2019 </a>)。在其他模型中看到类似的现象确实令人兴奋，更重要的是，过度参数化可以改善测试误差。也许这是升级工作站的完美借口，因为拥有一百万棵树的随机森林会给你更好的结果:)</p></div></div>    
</body>
</html>