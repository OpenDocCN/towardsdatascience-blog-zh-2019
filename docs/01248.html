<html>
<head>
<title>Demystifying Support Vector Machines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘支持向量机</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-support-vector-machines-8453b39f7368?source=collection_archive---------4-----------------------#2019-02-26">https://towardsdatascience.com/demystifying-support-vector-machines-8453b39f7368?source=collection_archive---------4-----------------------#2019-02-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="baa2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">SVM 的几何研究</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6d3462d386f6786de4689bd13464d295.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpucaK_2N7gPBWDbH7-5Gw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="172c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是我的第二篇<strong class="kx ir">揭秘</strong>系列文章。你可以在这里查看第一篇文章—</p><div class="lr ls gp gr lt lu"><a rel="noopener follow" target="_blank" href="/demystifying-logistic-regression-ee24c1104d45"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd ir gy z fp lz fr fs ma fu fw ip bi translated">揭秘逻辑回归</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">逻辑回归的几何研究</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi kp lu"/></div></div></a></div><p id="a174" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在本文中，我将重点介绍支持向量机或 SVM。它是最受欢迎的机器学习算法之一，在近十年的时间里(20 世纪 90 年代初至 21 世纪初)，它一直享有第一的地位。然而，它仍然是一个非常基本和重要的算法，你绝对应该拥有它。让我们从 SVM 开始。</p><p id="f124" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我将在本文中涉及的主题如下</p><ul class=""><li id="3ac2" class="mj mk iq kx b ky kz lb lc le ml li mm lm mn lq mo mp mq mr bi translated">SVM 简介</li><li id="c748" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">几何直觉</li><li id="b7b3" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">为什么我们对支持向量平面使用+1 和-1</li><li id="259e" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">损失函数</li><li id="5b7d" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">SVM 的双重形式</li><li id="321f" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">内核及其类型</li><li id="facd" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">努 SVM</li></ul><h1 id="13bb" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">支持向量机</h1><p id="319e" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">支持向量机(SVM)是一种机器学习算法，可用于分类和回归问题。但多用于分类问题。在这个算法中，我们将每个数据项绘制为 n 维空间中的一个点(其中 n 是您拥有的特征的数量)。然后，我们通过找到最能区分这两类的超平面来执行分类。</p><blockquote class="nu nv nw"><p id="229d" class="kv kw nx kx b ky kz jr la lb lc ju ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated">如果你有一个 n 维空间，那么超平面的维数将是 n-1。</p><p id="8412" class="kv kw nx kx b ky kz jr la lb lc ju ld ny lf lg lh nz lj lk ll oa ln lo lp lq ij bi translated">例如，如果您有一个 2d 空间，那么它将是一条线，如果您有一个 3d 空间，那么它将是一个平面，以此类推。</p></blockquote><h1 id="6200" class="mx my iq bd mz na nb nc nd ne nf ng nh jw ni jx nj jz nk ka nl kc nm kd nn no bi translated">几何直觉</h1><p id="2d1b" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">SVM 背后的主要思想是找到一个平面，该平面最好地分开正负点，并且正负平面之间的距离是最大的。</p><blockquote class="ob"><p id="1cd7" class="oc od iq bd oe of og oh oi oj ok lq dk translated">选择具有较大裕度的决策边界的基本原理是，它减少了泛化误差，而具有较小裕度的决策边界通常会导致过度拟合。</p></blockquote><p id="6268" class="pw-post-body-paragraph kv kw iq kx b ky ol jr la lb om ju ld le on lg lh li oo lk ll lm op lo lp lq ij bi translated">从上图中你可能已经注意到的另一件重要的事情是<strong class="kx ir">支持向量</strong>基本上是位于正负超平面上的点。</p><p id="3cf5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们仔细看看我们实际上是如何最大限度地提高利润的。</p><p id="9ef2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们考虑下面的等式—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/dc90aff3b05ffc44696f3ae9640aae8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*esPexQ7A4rNMGQ0_DvsGcg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="5f37" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些只是正负超平面的方程。我只在这个方程中加了 b，这是 y 轴截距。如果你想知道为什么我把正超平面和负超平面的值分别取为+1 和-1，那么就暂时保持这个想法，因为我将在本文的后面详细解释这样做的原因。</p><p id="22e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">减去等式(1)和(2)后，我们得到—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/cc8cc32c1fb61b6f64d474a31812a73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*dLqdzJIwd4Wdi_m9F5yeyQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="eb8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以用向量 w 的长度来归一化这个方程，向量 w 的定义如下</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/0a261323987dd83955e74f086496eaa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*4GCcngaZ6py0FLiQqtuF7A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="02a7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们最后的等式是—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/c1733164f93e9849147d7d865e87d9ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*hnyLNyjR-vcSQOYG1MogEw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="34d4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个方程的左边基本上就是正负超平面之间的距离，实际上就是我们想要最大化的余量。</p><p id="0ff4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们在 SVM 的优化函数是—</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="cd83" class="oz my iq ov b gy pa pb l pc pd">argmax( 2 / ||W||) for all i<br/>such that Yi(W^T * Xi+b) &gt;= 1</span></pre><p id="f104" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里 Yi(W^T * Xi+b) &gt;= 1 意味着这些点是完全线性可分的。所以所有的正点都在平面的正侧，所有的负点都在负侧</p><p id="495a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种方法的问题是，在现实生活中，你几乎找不到完全线性分离的数据集。我们采用的方法被称为<strong class="kx ir">硬边际 SVM </strong>，在现实生活中很少使用。所以为了在现实世界的应用中使用 SVM，一个被修改的版本被创造出来，叫做<strong class="kx ir">软边际 SVM </strong>。</p><h2 id="503e" class="oz my iq bd mz pe pf dn nd pg ph dp nh le pi pj nj li pk pl nl lm pm pn nn po bi translated">软利润 SVM</h2><p id="d172" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">让我们首先来看看软利润 SVM 的等式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/b41730f1163a0bf837cc5d624f231a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*s9zWVuFoU3fXgJi5vzb7KA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="fcad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你不能理解这个等式，不要太担心，我会解释每一项。您可能对||W||/2 这个术语很熟悉。之前我们想最大化 2/||W||这个词，但是现在我们把它反过来了，所以我们把 argmax 改成了 argmin。</p><p id="17ee" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可能已经猜到了'<strong class="kx ir"> n' </strong>表示数据点的数量。所以这个等式中新增加的两项是—</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="53a0" class="oz my iq ov b gy pa pb l pc pd">C         = It is a hyperparameter<br/>ζ (Zeta)  = It denotes the distance of misclassified points </span></pre><h2 id="47e9" class="oz my iq bd mz pe pf dn nd pg ph dp nh le pi pj nj li pk pl nl lm pm pn nn po bi translated">了解泽塔</h2><p id="737e" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">为了更好地理解术语 zeta，让我们看下面的例子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pq"><img src="../Images/6babe6a0bd47011a256be1790b9545d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFsj982lwxRxJsc4BZ-k8A.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="cb71" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里,★代表正点，⬤代表负点。</p><p id="ac55" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正如我在硬边界 SVM 的例子中所说的，我们很难找到一个完全线性可分的数据集，这里我们有一个点 x1，它是一个正点，但它不在正平面上。</p><p id="583c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以在这种特殊情况下，点 x1 和平面𝚷之间的距离是 0.5(朝向负平面)。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="2e81" class="oz my iq ov b gy pa pb l pc pd">For point x1 - <br/>Y(W^T * X + b)  = -0.5<br/>Since class label(Y) is +1 and the distance is -0.5, since it is towards the negative plane</span></pre><p id="c081" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们可以将上面的等式改写如下—</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="9700" class="oz my iq ov b gy pa pb l pc pd">Y(W^T * X + b)  = 1 - 1.5  <br/>So in general form we can write it as <br/>Y(W^T * X + b)  = 1 - ζ</span></pre><blockquote class="ob"><p id="2056" class="oc od iq bd oe of pr ps pt pu pv lq dk translated">所以基本上ζ代表的是误分类点离实际平面的距离。可以观察到，x1 到正平面𝚷 +的距离为 1.5，这正是本例中ζ的值。</p></blockquote><h2 id="fc26" class="oz my iq bd mz pe pw dn nd pg px dp nh le py pj nj li pz pl nl lm qa pn nn po bi translated">理解 C</h2><p id="7cee" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">正如我上面提到的，C 是一个超参数，它可以被有效地调整，以避免过度拟合和欠拟合。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="91e0" class="oz my iq ov b gy pa pb l pc pd">As C increases the tendency of the model to overfit increases<br/>As C decreases the tendency of the model to underfit increases</span></pre></div><div class="ab cl qb qc hu qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="ij ik il im in"><h1 id="b754" class="mx my iq bd mz na qi nc nd ne qj ng nh jw qk jx nj jz ql ka nl kc qm kd nn no bi translated">为什么我们对支持向量平面使用+1 和-1</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qn"><img src="../Images/5293b03d7c7f126e74e9852b4c364a29.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*NWJFojvOVMuy2_XWPqaaog.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="6976" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们没有必要总是选择+1 和-1。所以我们在这里选择 k 的任意值。唯一的限制是它应该大于 0。</p><blockquote class="ob"><p id="c2fb" class="oc od iq bd oe of og oh oi oj ok lq dk translated">我们不能为我们的平面选择不同的值，也就是说，我们不能取+k1 和-k2，因为我们希望我们的正平面和负平面离我们的平面𝚷的距离相等</p></blockquote><p id="2c11" class="pw-post-body-paragraph kv kw iq kx b ky ol jr la lb om ju ld le on lg lh li oo lk ll lm op lo lp lq ij bi translated">现在我们更新的利润是-</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="3f06" class="oz my iq ov b gy pa pb l pc pd">2*k / ||W||<br/>for k = 5 we get</span><span id="e2b6" class="oz my iq ov b gy qo pb l pc pd">10/||W||</span></pre><p id="f3af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，现在我们将使用 10/||W||而不是 2/||W||这是唯一的区别，因为 k 在这里是一个常数，所以我们选择什么值并不重要，因为它不会影响我们的优化问题。</p><p id="0054" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们用+1 和-1 来简化数学计算。</p></div><div class="ab cl qb qc hu qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="ij ik il im in"><h1 id="3104" class="mx my iq bd mz na qi nc nd ne qj ng nh jw qk jx nj jz ql ka nl kc qm kd nn no bi translated">损失函数</h1><p id="282e" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">SVM 使用的损失函数是铰链损失。简而言之，我们可以将铰链损耗理解为一个函数，其值在某个点(比如“z ”)之前不为零，在该点之后等于零。</p><p id="0295" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们研究了软利润 SVM 的方程式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/b41730f1163a0bf837cc5d624f231a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*s9zWVuFoU3fXgJi5vzb7KA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="8575" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里包含ζ和 C 的第二项是<strong class="kx ir">损失项</strong>。现在我们来看看这个术语是怎么来的。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="4929" class="oz my iq ov b gy pa pb l pc pd">Let Y(W^T * X + b) = Z   -- (i)<br/>// Here we are just substituting the value of Y(W^T * X + b) so that it is more readable</span><span id="bd9a" class="oz my iq ov b gy qo pb l pc pd">So from (i) we can say that <br/>If Z &gt; = 1  then the point is correctly classified and<br/>If Z &lt; 1    then the point is misclassified </span></pre><p id="5cfe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你没有理解上面的替换，那么让我进一步澄清一下。</p><p id="6919" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设你有 2 个点 x1 和 x2，其中 x1 为正，x2 为负。现在，对于位于负平面上的点 x2 ,( w^t * x+b)的值将为负，其 y 值将为-1。</p><p id="0744" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以，y*(w^t * x+b)=-1 *(ve 值)= +ve 值</p><p id="6a7f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">类似地，对于正的点 x1，(W^T * X + b)将是正的，并且它的 y 值也将是正的。所以，y*(w^t * x+b)=+1 *(ve 值)= +ve 值。</p><p id="b0b7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，如果你有另一个点 x3，它是正的，但是在负平面上，那么(W^T * X + b)将是负的，但是类别标签 y 仍然是正的。</p><p id="9b58" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以，y*(w^t * x+b)=+1 *(ve 值)= -ve 值</p><p id="9e07" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以这里的关键是，Y*(W^T * X + b)只有在点被正确分类的情况下才会是正的，我们刚刚把 Y*(W^T * X + b)替换为 z</p><p id="ec1b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在你对 Z 的概念已经很熟悉了(希望如此)，让我们看看我们的损失函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qp"><img src="../Images/d02d00ac705c886db29c7d4a405bdfb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*mZElY0cUtO868nzrX3O4Gg.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="f8d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们的损失函数相当简单，如果你不能理解它是如何工作的，那么我将为你分解它。</p><p id="22c6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述-</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="70c8" class="oz my iq ov b gy pa pb l pc pd">If Z &gt;= 1   then the point is correctly classified and<br/>If Z &lt; 1    then the point is misclassified</span></pre><p id="7602" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们将在这里考虑两种情况。</p><p id="dce8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">案例 1</strong>——(Z≥1)</p><p id="cfc2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果 Z ≥1，则 1-Z 将小于 0，因此 Max(0，1-Z ) = 0</p><p id="7076" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">直觉上，如果 Z≥1，则意味着我们已经正确地对该点进行了分类，因此我们的损失为 0。</p><p id="0203" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">案例二</strong> — ( Z &lt; 1)</p><p id="97ae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果 Z &lt;1 then 1-Z will be greater than 0 so Max(0, 1-Z ) = 1-Z</p><p id="1080" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">最后一步</strong></p><p id="fc24" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们已经知道-</p><p id="0290" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Y(W^T * X + b) = 1 — ζ(参见<strong class="kx ir">了解泽塔</strong>小节)</p><p id="312b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以我们可以改写为- <br/> 1 - Y(W^T * X + b) = ζ <br/>而 Y(W^T * X + b) = Z <br/>所以 1-Z = ζ</p><p id="5ad0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从上面的例子我们可以看出，我们想要最小化的项是 1-Z。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/b41730f1163a0bf837cc5d624f231a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*s9zWVuFoU3fXgJi5vzb7KA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 1 (Image by Author)</figcaption></figure><p id="e2be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这正是我们在这里所写的。我们只是用ζ代替了 1-Z</p></div><div class="ab cl qb qc hu qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="ij ik il im in"><h1 id="316d" class="mx my iq bd mz na qi nc nd ne qj ng nh jw qk jx nj jz ql ka nl kc qm kd nn no bi translated">SVM 的双重形式</h1><p id="45a9" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">我们推导出的上面的等式 1 是 SVM 的原始形式。然而，为了利用内核的能力，我们使用双重形式的支持向量机。让我们看看 SVM 的双重形式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qq"><img src="../Images/d25aa4dd77f77bcfb6c7e2a9d9f0e322.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*jjud0afekZ3f0Huk8twHOw.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 2 (Image by Author)</figcaption></figure><p id="3770" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用双重形式的 SVM 的原因是，它允许我们利用内核的力量，这是 SVM 的一个关键特性，如果你不熟悉内核，那么不要太担心，我将在下一节解释内核。但是现在只要理解我们使用 SVM 的双重形式是为了利用内核的力量。</p><blockquote class="ob"><p id="83b0" class="oc od iq bd oe of og oh oi oj ok lq dk translated">从数学上证明了方程 2 等价于方程 1。</p></blockquote><p id="7741" class="pw-post-body-paragraph kv kw iq kx b ky ol jr la lb om ju ld le on lg lh li oo lk ll lm op lo lp lq ij bi translated">关于我们实际上如何得到这种对偶形式的数学证明超出了本文的范围，因为它在数学上有点复杂。如果你想了解背后的数学原理，你可以看看下面的视频</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="qr qs l"/></div></figure><p id="f117" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个视频中，帕特里克·亨利·温斯顿教授提供了一个出色的数学解释，我强烈建议你看看这个视频，以便更好地理解支持向量机的概念。</p><blockquote class="ob"><p id="2402" class="oc od iq bd oe of og oh oi oj ok lq dk translated">这里要注意的最重要的一点是，αi 的值只对支持向量是非零的。所以我们基本上只关心支持向量。</p></blockquote><p id="c9fe" class="pw-post-body-paragraph kv kw iq kx b ky ol jr la lb om ju ld le on lg lh li oo lk ll lm op lo lp lq ij bi translated">我们可以将等式 2 更新如下—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/7e977dba7c9bbda929d4427ec23bc6b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*FCoKr-wAwxrVFPZvte1ung.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Equation 3 (Image by Author)</figcaption></figure><p id="b4ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">之前我们用的是 xi^t Xj，也就是说，我们取 Xi 和 XJ 的点积，相当于余弦相似函数。所以我们可以用 Xi 和 Xj 的其他函数来代替这个余弦相似函数。这就是所谓的内核技巧。现在我们来了解一下内核到底是个什么东西。</p></div><div class="ab cl qb qc hu qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="ij ik il im in"><h1 id="6091" class="mx my iq bd mz na qi nc nd ne qj ng nh jw qk jx nj jz ql ka nl kc qm kd nn no bi translated">内核及其类型</h1><p id="1dc1" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">在上面的等式 3 中，我们可以用任何核函数代替 K。现在你一定想知道那会改变什么。为什么我们使用哪个函数有关系呢？所以让我们试着回答这些问题。</p><p id="0989" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设您有一个不可线性分离的数据集。</p><p id="da61" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，你如何使用 SVM 来分离这些数据？我们不可能找到一个平面来分开这两类人。</p><p id="45d5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输入内核…</p><blockquote class="ob"><p id="2146" class="oc od iq bd oe of og oh oi oj ok lq dk translated">核函数的主要用途是允许我们将数据集投影到更高的维度上，在那里我们可以拟合一个平面来分离我们的数据集。</p></blockquote><p id="cf3b" class="pw-post-body-paragraph kv kw iq kx b ky ol jr la lb om ju ld le on lg lh li oo lk ll lm op lo lp lq ij bi translated">因此，我们可以将上述数据集投影到一个更高的维度上，然后我们可以找到一个平面来分隔这两个类。这正是 SVM 在 90 年代初超受欢迎的原因。</p><h2 id="322a" class="oz my iq bd mz pe pf dn nd pg ph dp nh le pi pj nj li pk pl nl lm pm pn nn po bi translated">内核的类型</h2><p id="e623" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">两种最受欢迎的内核是—</p><ol class=""><li id="801f" class="mj mk iq kx b ky kz lb lc le ml li mm lm mn lq qt mp mq mr bi translated">多项式核</li><li id="5133" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq qt mp mq mr bi translated">径向基函数(RBF)核</li></ol><p id="881e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">多项式核— </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/feb1225a8108108e1ceae9589aa653fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*_8IKttDldgd8dUVhbXQAnA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="3124" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以对于二次核，我们会得到这样的结果—</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qu"><img src="../Images/a32695fdbec0a3e5f93556932fce18ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*sVv9vS9LBhjQVH3mqhiNeQ.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="ddc2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> RBF 核— </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qv"><img src="../Images/1956846e40d514648b7aeb4afbea0509.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*wDaPjQjMUYkPKmK7LN57DQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Image by Author</figcaption></figure><p id="4a2d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里 d 是 x1 和 x2 之间的距离，即 d = | | x1-x2 | |𝜎是一个超参数。</p></div><div class="ab cl qb qc hu qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="ij ik il im in"><h1 id="298a" class="mx my iq bd mz na qi nc nd ne qj ng nh jw qk jx nj jz ql ka nl kc qm kd nn no bi translated">新 SVM —</h1><p id="61d1" class="pw-post-body-paragraph kv kw iq kx b ky np jr la lb nq ju ld le nr lg lh li ns lk ll lm nt lo lp lq ij bi translated">nu 是一个超参数，我们可以用它来定义可接受的误差百分比。</p><pre class="kg kh ki kj gt ou ov ow ox aw oy bi"><span id="4281" class="oz my iq ov b gy pa pb l pc pd">0 &lt;= nu &lt;= 1  // The value of nu is between 0 and 1</span><span id="9d24" class="oz my iq ov b gy qo pb l pc pd">Let's understand it with an example.<br/>Suppose nu = 0.01 and N (Number of data points)= 100,000</span><span id="ed3e" class="oz my iq ov b gy qo pb l pc pd">* Percentage of errors &lt;= 1%<br/>* Number of Support Vectors &gt;= 1% of N i.e. 1000</span></pre><p id="93d7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以在 nu 超参数的帮助下，我们可以做两件事</p><ul class=""><li id="b456" class="mj mk iq kx b ky kz lb lc le ml li mm lm mn lq mo mp mq mr bi translated">我们可以控制模型的误差百分比。</li><li id="7284" class="mj mk iq kx b ky ms lb mt le mu li mv lm mw lq mo mp mq mr bi translated">我们不能控制，但我们可以确定支持向量的数量。</li></ul></div><div class="ab cl qb qc hu qd" role="separator"><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg qh"/><span class="qe bw bk qf qg"/></div><div class="ij ik il im in"><p id="d115" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">至此，我们已经到了这篇文章的结尾。非常感谢你的阅读。</p><p id="9609" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你愿意，你可以鼓掌。它是免费的。</p><p id="e1a8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的<a class="ae qw" href="https://www.linkedin.com/in/dhairya-kumar/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>，<a class="ae qw" href="https://twitter.com/DhairyaKumar16" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae qw" href="https://github.com/Dhairya10" rel="noopener ugc nofollow" target="_blank"> Github </a> <br/>你可以查看我的<a class="ae qw" href="https://alpha-dev.in/" rel="noopener ugc nofollow" target="_blank">网站</a>了解更多关于我和我的工作。</p></div></div>    
</body>
</html>