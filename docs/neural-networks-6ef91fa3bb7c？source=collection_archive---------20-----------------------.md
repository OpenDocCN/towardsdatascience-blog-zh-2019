# 深入探究人工神经网络与生物神经网络的功能

> 原文：<https://towardsdatascience.com/neural-networks-6ef91fa3bb7c?source=collection_archive---------20----------------------->

## 通信、噪声处理和架构变化

![](img/fa275ada676c0f5a1a1b6eac395a307a.png)

# 亮点:

生物神经网络由振荡器组成——这赋予了它们过滤输入和与噪音共振的能力。这也让他们有能力保留隐藏的点火模式。

*人工神经网络与时间无关，不能过滤其输入。他们在训练后保持固定和明显的(但黑匣子)发射模式。*

*不同类型的噪声与人工神经网络不同元素的相互作用仍然是一个开放的研究课题。我们知道噪声可以用来调整它们的性能或帮助避免梯度下降中的局部最小值。*

大脑重组其连接的能力被称为突触可塑性，它是通过长时程增强来实现的。人工神经网络在训练期间使用梯度下降来确定它们的连通性。梯度是使用反向传播计算的，我们可以将其与赫比长时程增强进行比较。

**简介**

头脑是大脑做的事情。那么，*大脑到底在做什么，我们能造出什么来做类似的事情呢？*这些东西到底是怎么做的？**

在本文中，我们深入研究生物和人工神经网络的功能来回答这些问题。我们研究神经元和节点如何通信，它们如何处理噪声，以及网络在学习时如何变化——最终引导我们做出适当的选择和行为。

如果您对这些主题不熟悉，请点击此处了解这些网络的结构组件介绍:

**生物神经网络**

最终，任何网络的输出都取决于其节点的激活模式。在基本水平上，我们可以说一个神经元拥有一个电位，如果达到某个电阈值，就会触发。神经元通过突触进行交流，突触是细胞之间的间隙，信号在这里变成化学物质。一些化学物质会告诉突触后神经元触发，一些会告诉它不要触发。后面的神经元是否会放电，最终取决于它的电位。

**人工神经网络**

另一方面，一个节点持有一个称为“激活”的数值——这个数值越大，这个神经元对下一个神经元的影响越强。通常，我们使用一个 sigmoid 函数来保持激活值在 0 和 1 之间，并且我们有一个偏置项来左移或右移激活函数。通常，节点被组织成层，其中一层中的所有节点都连接到前一层中的每个节点。“权重”决定了连接的强度，就像化学物质一样，它们可以向下面的节点发送积极(着火)或消极(不着火)的信息。

**通信:** *阈值和定时*

我们将会非常仔细地观察随着学习的进行，权重和突触是如何变化的，但是首先，一个有效的连接看起来像什么？基本上，人工神经网络权重不必担心基于它们的时序被忽略，而突触传递则需要。正如我们将看到的，其原因对信息处理有深远的影响。

直到 20 世纪末，神经元被认为只是简单地将它们从周围所有神经元接收到的电输入相加，如果总和超过-70 毫伏，就会触发。今天，我们知道神经元作为一个弱混沌振荡器持有能量。这意味着它们有一个轨道吸引子，所以不是每次振荡都走完全相同的路径，而是走近似相同的路径。更具体地说，神经元是非谐振子，这意味着它们振荡的速度不是恒定的。这些也称为松弛振子，因为它们的特点是能量的松弛积累，周期性地转化为能量的突然损失(类似于动作电位)。

让我们以 gy rgy buzáki 的家用放松振荡器为例，这样我们就可以把它分解开来:滴水的水龙头。一个松弛的振荡器有三个阶段:兴奋状态，活跃状态和不应期。第一个对应于一个积累一滴水的水龙头，而如果你点击它或产生某种输入，水就会下降(但如果你等待更多的水积累，水滴可能会更大)。然后，在活动阶段，液滴会自己落下。随着水的下降，在不应期之后，当没有水或水很少时，系统不会受到干扰。

神经元就像水龙头，在激活和不应期对突触活动免疫。这意味着松弛振荡器在信息传输和接收周期之间交替，其中这些周期的长度由振荡器的频率决定。

这种相位重置特性赋予振荡器网络学习和存储模式的能力。人工神经网络也存储连接模式，但它们完全脱离时间(除了我们试图优化它们运行多长时间)。这里，所有节点接收来自前一层中所有节点的信息，我们能做的最好的事情是调整权重和偏置项，以保持激活为 0。神经元结合高通(电压相关)和低通(时间相关)滤波来执行基于频率的输入选择。在现实中，神经元的每个部分(即树突、轴突等。)可以起到这种有滤波能力的谐振振荡器的作用。

为什么这如此重要？除了由输入选择引起的明显的信息处理差异之外，这是网络如何处理噪声的决定性因素。

**噪声:** *偏差-方差权衡和随机共振*

过度拟合是人工神经网络的致命弱点，噪声可以帮助我们解决这个问题。当机器学习模型具有低偏差(它捕捉变量之间的关系)和高方差(它在数据集之间具有高度可变的拟合质量)时，就会发生过度拟合。这是有问题的，因为它使我们的模型在新数据上工作不佳或不可靠。减少过度拟合的过程称为正则化，在该过程中，偏差上升，方差下降。

在训练过程中添加噪声是调整人工神经网络的一个很好的方法。通常怀疑是高斯噪声(即随机噪声)，如果应用于输入，这也被认为是一种数据增强技术。我们还可以将噪声添加到权重、梯度(我们将在后面进一步探讨)和激活函数中。这些不同元素中的噪声提高性能的机制还没有被精确地确定下来，但是我们可以从大脑中寻找灵感。

在神经科学中，我们可以从随机共振的角度来看待噪音。在这里，神经元的振荡性质清楚地表明，噪声并不总是干扰信号。相反，它能够通过在适当的时候推动神经元超过激活阈值来放大隐藏或“习得”的信号。这就是众所周知的共振，当能量以其自然频率输入系统时就会发生共振。*大脑的节奏*将这比作站在钢琴旁边——当力施加到钢琴附近的地板上时，钢琴的弦会振动。好的乐器能做到这一点是因为它们能放大声音，或者就大脑信号而言，它们能产生共鸣。如果不是通过共振，人工智能中的信号如何或是否被噪声放大还有待观察。现在，让我们仔细看看什么是梯度。

**架构变化:** *梯度下降/反向传播和长时程增强*

神经网络在学习时会改变它们的连接。这导致更有效和更准确地处理它们的输入，将我们带到正确的输出，就像一幅图像是猫还是狗。人工神经网络对固定量的输入——训练数据——进行这种操作，而大脑则连续进行这种操作，但它们是如何做到的呢？

在人工神经网络中，连接不会像在大脑中一样出现或消失，它们只会变强或变弱。最终，目标是最小化网络输出中的误差，这可以使用成本函数来测量。这个成本函数就是我们的模型预测的答案和实际答案之间的误差平方和的平均值。这将我们带到梯度下降——我们称之为下降，因为目标是降低误差。梯度这个词指的是我们如何寻找这个最小误差——使用导数来寻找最陡下降的方向。我们可以将此想象为一个在山谷中滚动的球，它将向下运动，直到最低点，此时表面是平坦的(即导数为零)。在 2D 图中，如果斜率为负，我们将向右移动，如果斜率为正，我们将向左移动。这带来了一些挑战，主要是速度和找到真正的最小值，而不是在山谷中的两个山峰之间卡住，而附近还有一个表面。在一些网络中，梯度噪声不仅有助于防止过度拟合，还降低了训练损失，而在其他网络中，它有助于避免局部极小值。

梯度告诉我们如何最小化成本，反向传播是如何计算成本的。想象一个神经网络，其中输出层有 2 个神经元，一个将图像分类为树懒，另一个将图像分类为熊猫。激活程度较高的神经元将是网络预测的结果——那么当这一预测错误时会发生什么呢？我们不能直接改变激活，但我们可以改变前一层神经元的权重。

假设一个图像的正确答案是 panda，我们的 panda-node 只有 0.2 激活，而它应该是 1。我们可以通过增加积极联系的权重，减少消极联系的权重来提高这个数字。由于来自前一层的具有较大激活值的节点具有更强的效果，因此改变这些节点的权重将对损失函数产生更大的影响(无论权重是正还是负)。这可以与神经科学中的 Hebbian 学习相比较，在 Hebbian 学习中，一起放电的神经元之间的联系得到了最大程度的加强。

同时，我们可以尝试给与我们的 panda-node 有大量连接的节点一个更高的激活值，但是正如我们已经建立的，我们不能直接改变激活值。然而，为了实现这一点，我们可以调整从倒数第二层到该节点的权重。事实上，我们有，这就是为什么我们称之为反向传播。现在，回到我们的输出，懒惰节点将对每层权重应如何变化有自己的看法，因此反向传播取平均值，以最小化两者的损失(在现实生活中，这是对训练数据的子集进行的，也称为小批量，导致随机梯度下降)。

另一方面，大脑改变其连接的能力被称为突触可塑性，其中一种机制被称为长时程增强(LTP)。在这里，突触基于最近的活动而被加强/削弱，这被视为学习和记忆的基础。有几种具有不同属性的 LTP，包括 Hebbian-LTP，其中“一起放电的神经元连接在一起”，就像在人工神经网络中一样。然而，也有非 Hebbian-LTP，其中突触前和突触后神经元不需要一起放电，以及反 Hebbian-LTP，其中突触后神经元必须超极化。

到目前为止，对 LTP 最了解的例子是成人 CA1 海马区的 NMDA 受体依赖性 LTP。以此为例，我们可以探究一些最常见的属性。首先，这种 LTP 是输入特异性的，这意味着一个突触的 LTP 诱导不会改变其他突触。其次是关联性。这意味着当一个途径的活性不足以诱导 LTP 时，另一个途径的同时强活性能够在两个途径中诱导 LTP。最后，我们有持久性，这表明这种增强是长期的，持续几分钟到几个月。到目前为止，我们已经确定的这种现象背后的主要机制是树突棘的修改，树突棘是信号交换的微小突起。

随着我们越来越了解 LTP、神经元和突触，我们也就越来越了解大脑。随着人工神经网络的出现，我们正在接近我们建造像我们自己一样复杂的东西的潜力。

*最初于 11 月 1 日由* [*安巴尔·克莱恩波特*](https://www.linkedin.com/in/ambarkleinbort/) *发表，并在《分析》杂志 Vidhya* 上发表

**参考文献:**

*   [https://neuro physics . ucsd . edu/courses/physics _ 171/Buzsaki % 20G。% 20 rhythms % 20 of % 20 the % 20 brain . pdf](https://neurophysics.ucsd.edu/courses/physics_171/Buzsaki%20G.%20Rhythms%20of%20the%20brain.pdf)
*   [https://www . cell . com/neuron/pdf/s 0896-6273(16)30957-6 . pdf](https://www.cell.com/neuron/pdf/S0896-6273(16)30957-6.pdf)
*   http://proceedings.mlr.press/v97/zhou19d/zhou19d.pdf
*   【https://openreview.net/pdf?id=rkjZ2Pcxe 
*   [https://www.youtube.com/watch?v=aircAruvnKk&t = 203s](https://www.youtube.com/watch?v=aircAruvnKk&t=203s)
*   [https://www.youtube.com/watch?v=qPix_X-9t7E](https://www.youtube.com/watch?v=qPix_X-9t7E)