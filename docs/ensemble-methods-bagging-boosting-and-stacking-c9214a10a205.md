# 整体方法:装袋、助推和堆叠

> 原文：<https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205?source=collection_archive---------0----------------------->

## 理解集成学习的关键概念。

![](img/5f74dd38f97656568bb8192964db7544.png)

Credit: [Nietjuh](https://pixabay.com/fr/users/nietjuh-2218222/) on [Pixabay](https://pixabay.com/)

*本帖与* [*巴蒂斯特·罗卡*](https://medium.com/u/20ad1309823a?source=post_page-----c9214a10a205--------------------------------) *共同撰写。*

# 介绍

“团结就是力量”。这句老话很好地表达了机器学习中非常强大的“集成方法”的基本思想。粗略地说，通常信任许多机器学习竞赛(包括 Kaggle 的竞赛)的顶级排名的集成学习方法是基于这样的假设，即将多个模型组合在一起通常可以产生更强大的模型。

这篇文章的目的是介绍集成学习的各种概念。我们将为读者提供一些必要的答案，以便更好地理解和使用相关方法，并能够在需要时设计合适的解决方案。我们将讨论一些众所周知的概念，如 boosting，bagging，random forest，boosting，stacking 和其他许多集成学习的基础。为了使所有这些方法之间的联系尽可能清晰，我们将尝试在一个更广泛的逻辑框架中呈现它们，我们希望这将更容易理解和记忆。

## 概述

在这篇文章的第一部分，我们将提出弱学习者和强学习者的概念，我们将介绍三种主要的集成学习方法:打包、提升和堆叠。然后，在第二部分，我们将重点放在装袋，我们将讨论的概念，如自举，装袋和随机森林。在第三部分中，我们将介绍 boosting，特别是它的两个最流行的变体:自适应 boosting (adaboost)和梯度 boosting。最后，在第四部分，我们将给出堆栈的概述。

# 什么是系综方法？

集成学习是一种机器学习范式，其中多个模型(通常称为“弱学习者”)被训练来解决同一问题，并被组合以获得更好的结果。主要假设是，当弱模型被正确组合时，我们可以获得更准确和/或更稳健的模型。

## 单一弱学习者

在机器学习中，无论我们面对的是分类还是回归问题，模型的选择对于有机会获得好的结果都是极其重要的。这种选择取决于问题的许多变量:数据的数量、空间的维度、分布假设…

低偏差和低方差是模型的两个最基本的预期特征，尽管它们通常以相反的方向变化。事实上，为了能够“解决”一个问题，我们希望我们的模型有足够的自由度来解决我们正在处理的数据的潜在复杂性，但我们也希望它没有太多的自由度来避免高方差和更健壮。这就是众所周知的**偏差-方差权衡**。

![](img/2f33904d2774db2369e20588deb214e4.png)

Illustration of the bias-variance tradeoff.

在集成学习理论中，我们称**弱学习者**(或**基础模型**)模型，这些模型可以作为构建模块，通过组合其中的几个来设计更复杂的模型。大多数时候，这些基本模型本身表现不佳，要么是因为它们有较高的偏差(例如，低自由度模型)，要么是因为它们有太多的方差而不稳定(例如，高自由度模型)。然后，集成方法的思想是试图通过将这些弱学习者中的几个组合在一起来减少偏差和/或方差，以便创建实现更好性能的**强学习者**(或**集成模型**)。

## 结合弱学习者

为了建立集成学习方法，我们首先需要选择要聚合的基本模型。大多数时候(包括在众所周知的 bagging 和 boosting 方法中),使用单个基础学习算法，因此我们有以不同方式训练的同质弱学习器。于是我们得到的系综模型被称为是“齐次的”。然而，也存在一些使用不同类型的基本学习算法的方法:一些异类弱学习器然后被组合成“异类集成模型”。

重要的一点是，我们对弱学习者的选择应该与我们聚合这些模型的方式一致。如果我们选择具有低偏差但高方差的基础模型，则应该使用倾向于减少方差的聚合方法，而如果我们选择具有低方差但高偏差的基础模型，则应该使用倾向于减少偏差的聚合方法。

这就给我们带来了如何组合这些模型的问题。我们可以提到三种主要的元算法，旨在组合弱学习者:

*   **bagging** ，通常考虑同质的弱学习者，并行地相互独立地学习它们，并按照某种确定性的平均过程组合它们
*   **boosting** ，通常考虑同质的弱学习者，以一种适应性很强的方式顺序学习它们(基础模型取决于前面的模型)，并按照确定性策略组合它们
*   **堆叠**，通常考虑异类弱学习者，并行学习它们，并通过训练元模型来组合它们，以基于不同的弱模型预测输出预测

非常粗略地说，我们可以说 bagging 将主要集中于获得方差小于其分量的集合模型，而 boosting 和 stacking 将主要试图产生比其分量偏差更小的强模型(即使方差也可以被减小)。

在接下来的部分中，我们将在给出堆栈的简要概述之前，详细介绍 bagging 和 boosting(这比 stacking 使用得更广泛一些，并且允许我们讨论集成学习的一些关键概念)。

![](img/5ba0b2483eb85852d06a00242691b8a4.png)

Weak learners can be combined to get a model with better performances. The way to combine base models should be adapted to their types. Low bias and high variance weak models should be combined in a way that makes the strong model more robust whereas low variance and high bias base models better be combined in a way that makes the ensemble model less biased.

# 专注于装袋

在**平行方法**中，我们让不同的学习者相互独立地适应，因此，可以同时训练他们。最著名的方法是“bagging”(代表“bootstrap aggregating”)，旨在产生一个比组成它的单个模型更健壮的**集合模型。**

## 拔靴带

让我们从定义自举开始。这种统计技术包括通过随机抽取替换的 B 观测值，从大小为 N 的初始数据集生成大小为 B 的样本(称为 bootstrap 样本)。

![](img/c013b9fd6e477f97fce34a1fcd9c8b53.png)

Illustration of the bootstrapping process.

在某些假设下，这些样本具有非常好的统计特性:在第一近似值中，它们可以被视为直接来自真实的底层(通常未知)数据分布，并且彼此独立。因此，它们可以被视为真实数据分布的代表性独立样本(几乎同分布样本)。为了使这种近似有效，必须验证假设是双重的。首先，初始数据集的大小 N 应该足够大，以捕捉基本分布的大部分复杂性，以便从数据集采样是从真实分布采样的良好近似(**代表性**)。第二，与引导样本的大小 B 相比，数据集的大小 N 应该足够大，以使样本不太相关(**独立性**)。请注意，在下文中，我们有时会提到 bootstrap 样本的这些属性(代表性和独立性):读者应该始终记住**这只是一个近似值**。

例如，Bootstrap 样本通常用于评估统计估计量的方差或置信区间。根据定义，统计估计量是一些观察值的函数，因此是一个具有来自这些观察值的方差的随机变量。为了估计这种估计量的方差，我们需要对从感兴趣的分布中抽取的几个独立样本进行评估。在大多数情况下，考虑真正独立的样本需要的数据比实际可用的数据多得多。然后，我们可以使用 bootstrapping 来生成几个 bootstrap 样本，这些样本可以被视为“几乎具有代表性”和“几乎独立”(几乎同分布样本)。这些 bootstrap 样本将允许我们通过评估每个样本的值来近似估计量的方差。

![](img/ee7ae20a594f0e62b859ba8671a2ae3d.png)

Bootstrapping is often used to evaluate variance or confidence interval of some statistical estimators.

## 制袋材料

当训练模型时，无论我们是在处理分类问题还是回归问题，我们都会获得一个接受输入、返回输出并根据训练数据集定义的函数。由于训练数据集的理论方差(我们提醒数据集是来自真实的未知基础分布的观察样本)，拟合的模型也易变:**如果观察到另一个数据集，我们将获得不同的模型**。

bagging 的想法很简单:我们希望拟合几个独立的模型，并“平均”它们的预测，以获得一个方差较低的模型。然而，在实践中，我们无法拟合完全独立的模型，因为这需要太多的数据。因此，我们依靠 bootstrap 样本的良好“近似属性”(代表性和独立性)来拟合几乎独立的模型。

首先，我们创建多个 bootstrap 样本，这样每个新的 bootstrap 样本将作为另一个(几乎)独立于真实分布的数据集。然后，我们可以**为这些样本中的每一个拟合一个弱学习者，并最终聚合它们，这样我们就可以“平均”它们的输出**，从而获得一个方差小于其分量的集成模型。粗略地说，由于 bootstrap 样本近似独立且同分布(i.i.d .)，所以学习的基础模型也是如此。然后，“平均”弱学习者的输出不会改变预期的答案，但会减少其方差(就像平均 i.i.d .随机变量保持期望值但减少方差一样)。

因此，假设我们有 L 个大小为 B 的 bootstrap 样本(L 个独立数据集的近似值)

![](img/6eb085fe48342536532efdf24034b23b.png)

我们可以拟合 L 个几乎独立的弱学习者(每个数据集一个)

![](img/5458cb8c4af537dd98eb6a48041e8d05.png)

然后将它们聚合到某种平均过程中，以便获得具有较低方差的集合模型。例如，我们可以这样定义我们的强模型

![](img/a13041b700332d4f8622e0a1270ab929.png)

有几种可能的方法来集合并行拟合的多个模型。对于回归问题，单个模型的输出可以被平均以获得集合模型的输出。对于分类问题，每个模型输出的类可以被视为一个投票，获得大多数投票的类由集成模型返回(这被称为**硬投票**)。仍然对于分类问题，我们也可以考虑所有模型返回的每个类别的概率，平均这些概率并保留平均概率最高的类别(这被称为**软投票**)。如果可以使用任何相关的权重，平均值或投票可以是简单的，也可以是加权的。

最后，我们可以提到，装袋的一大优势是**它可以并行**。由于不同的模型相互独立地拟合，如果需要，可以使用密集的并行化技术。

![](img/8174761d6bec9625a71b56b7c6d0011e.png)

Bagging consists in fitting several base models on different bootstrap samples and build an ensemble model that “average” the results of these weak learners.

## 随机森林

学习树是集成方法非常流行的基础模型。由多棵树组成的强学习者可以称为“森林”。组成一个森林的树木可以选择浅的(很少的深度)或深的(很多深度，如果没有完全生长的话)。浅树的方差较小，但偏差较大，因此对于我们将在下文描述的顺序方法来说，浅树是更好的选择。另一方面，深树具有低偏差但高方差，因此是主要集中在减少方差的 bagging 方法的相关选择。

**随机森林**方法是一种装袋方法，其中**深树**(安装在引导样本上)被组合以产生具有较低方差的输出。然而，随机森林还使用另一种技巧来使多个拟合的树彼此之间的相关性降低:在生长每棵树时，我们不是只对数据集中的观察值进行采样来生成引导样本，而是对特征进行采样，并且只保留它们的随机子集来构建树。

对特征进行采样实际上具有这样的效果，即所有树并不查看完全相同的信息来做出决策，因此，它降低了不同返回输出之间的相关性。采样优于特征的另一个优点是**它使决策过程对缺失数据更加稳健**:具有缺失数据的观测值(来自或不来自训练数据集)仍然可以基于只考虑数据未缺失的特征的树进行回归或分类。因此，随机森林算法结合了 bagging 和随机特征子空间选择的概念来创建更鲁棒的模型。

![](img/bb9eece2f0798e83ade126944dec0092.png)

Random forest method is a bagging method with trees as weak learners. Each tree is fitted on a bootstrap sample considering only a subset of variables randomly chosen.

# 专注于提升

在**顺序方法**中，不同的组合弱模型不再相互独立地拟合。想法是迭代地拟合模型**，使得在给定步骤的模型训练依赖于在先前步骤拟合的模型。“Boosting”是这些方法中最著名的，它产生的集成模型通常比组成它的弱学习者更少偏差。**

## **助推**

**Boosting 方法和 bagging 方法的工作原理是一样的:我们建立一系列模型，这些模型被聚合起来以获得表现更好的强学习者。然而，与主要旨在减少方差的 bagging 不同，boosting 是一种以非常自适应的方式依次拟合多个弱学习者的技术:序列中的每个模型都被拟合，从而给予数据集中被序列中的先前模型处理得不好的观察值更多的重要性。直观地说，每个新模型**都将其努力集中在最困难的观察上**以适应到目前为止，因此我们在过程结束时获得了一个具有较低偏差的强学习者(即使我们可以注意到增强也可以具有减少方差的效果)。像 bagging 一样，Boosting 可以用于回归以及分类问题。**

**由于**主要致力于减少偏差**，通常考虑用于提升的基础模型是具有低方差但高偏差的模型。例如，如果我们想使用树作为我们的基础模型，我们将在大多数情况下选择只有几个深度的浅决策树。促使使用低方差但高偏差模型作为弱学习器进行增强的另一个重要原因是，这些模型一般来说拟合的计算成本较低(参数化时自由度较少)。事实上，由于适合不同模型的计算**不能并行进行**(不像 bagging)，因此依次适合几个复杂的模型可能会变得过于昂贵。**

**一旦选择了弱学习者，我们仍然需要定义它们将如何被顺序拟合(当拟合当前模型时，我们考虑来自先前模型的什么信息？)以及它们将如何聚合(我们如何将当前模型聚合到以前的模型？).我们将在以下两个小节中讨论这些问题，尤其是描述两个重要的提升算法:adaboost 和梯度提升。**

**简而言之，这两种元算法在顺序过程中如何创建和聚集弱学习者方面有所不同。自适应增强更新附加到每个训练数据集观察值的权重，而梯度增强更新这些观察值。这种主要差异来自两种方法试图解决寻找最佳模型的优化问题的方式，该模型可以写成弱学习器的加权和。**

**![](img/f36d745cbda4a538e192619c471b857a.png)**

**Boosting consists in, iteratively, fitting a weak learner, aggregate it to the ensemble model and “update” the training dataset to better take into account the strengths and weakness of the current ensemble model when fitting the next base model.**

## **适应性增压**

**在适应性增强(通常称为“adaboost”)中，我们试图将我们的集成模型定义为 L 个弱学习器的加权和**

**![](img/bbabc6e8b977d35b9ee6544a1d252e5c.png)**

**找到这种形式的最佳集合模型是一个困难的优化问题。然后，我们不是试图一次解决它(找到所有系数和给出最佳总体加性模型的弱学习器)，而是利用一个更易处理的**迭代优化过程**，即使它可能导致次优解。更具体地，我们逐个添加弱学习器，在每次迭代中寻找最佳可能对(系数、弱学习器)以添加到当前集成模型。换句话说，我们递归地定义(s_l)是这样的**

**![](img/abbca0f857c9389becd4390af7bfb1a5.png)**

**其中，选择 c_l 和 w_l，使得 s_l 是最符合训练数据的模型，因此，这是对 s_(l-1)的最佳可能改进。然后我们可以表示**

**![](img/0b716ff42b62e3c8d3847ec4e44f9b12.png)**

**其中 E(。)是给定模型的拟合误差，e(。,.)是损失/误差函数。因此，我们通过“局部地”优化构建并将弱学习者逐个添加到强模型来逼近最优，而不是在总和中对所有 L 个模型进行“全局”优化。**

**更具体地，当考虑二进制分类时，我们可以表明 adaboost 算法可以被重写为如下进行的过程。首先，它**更新数据集中的观测值权重**，并训练新的弱学习器，特别关注被当前集成模型错误分类的观测值。第二，它**根据表示该弱模型性能的更新系数，将弱学习器添加到加权和**中:弱学习器表现越好，它对强学习器的贡献越大。**

**因此，假设我们面临一个二进制分类问题，在我们的数据集中有 N 个观察值，我们希望对给定的弱模型族使用 adaboost 算法。在算法的最开始(序列的第一个模型)，所有的观察值具有相同的权重 1/N。然后，我们重复 L 次(对于序列中的 L 个学习者)以下步骤:**

*   **用当前观测值权重拟合最佳弱模型**
*   **计算更新系数的值，该更新系数是弱学习者的某种标量评估度量，其指示该弱学习者应该在多大程度上被考虑到集成模型中**
*   **通过添加新的弱学习器乘以其更新系数来更新强学习器**
*   **计算新的观察值权重，该权重表示我们希望在下一次迭代中关注哪些观察值(聚集模型错误预测的观察值的权重增加，而正确预测的观察值的权重减少)**

**重复这些步骤，然后我们按顺序建立我们的 L 模型，并将它们聚合成一个简单的线性组合，通过系数加权来表示每个学习者的表现。注意，存在初始 adaboost 算法的变体，例如 LogitBoost(分类)或 L2Boost(回归),其主要区别在于它们对损失函数的选择。**

**![](img/a6b63f61bb5e2916deba09c5d70b0a86.png)**

**Adaboost updates weights of the observations at each iteration. Weights of well classified observations decrease relatively to weights of misclassified observations. Models that perform better have higher weights in the final ensemble model.**

## **梯度推进**

**在梯度提升中，我们试图建立的集成模型也是弱学习者的加权和**

**![](img/bbabc6e8b977d35b9ee6544a1d252e5c.png)**

**就像我们提到的 adaboost 一样，在这种形式下寻找最优模型太难了，需要迭代的方法。自适应增强的主要区别在于顺序优化过程的定义。事实上，梯度推进**将问题转化为梯度下降问题**:在每次迭代中，我们将弱学习器拟合到当前拟合误差相对于当前整体模型的梯度的相反值。让我们试着澄清这最后一点。首先，系综模型上的理论梯度下降过程可以写成**

**![](img/d3974684d062fb7f76a2ea767eca9a3f.png)**

**其中 E(。)是给定模型的拟合误差，c_l 是对应于步长的系数**

**![](img/0151de243dfeeea47f67db8ce1c918d1.png)**

**与步骤 l-1 中关于集合模型的拟合误差的梯度相反。这个(相当抽象)与梯度相反的函数实际上只能对训练数据集中的观测值进行评估(我们知道其输入和输出):这些评估被称为附属于每个观测值的**伪残差**。此外，即使我们知道这些伪残差的观测值，我们也不想向我们的集合模型添加任何类型的函数:我们只想添加弱模型的新实例。因此，自然要做的事情是**将弱学习者拟合到为每个观察计算的伪残差**。最后，按照一维优化过程(线搜索以获得最佳步长 c_l)计算系数 c_l。**

**因此，假设我们想要对给定的弱模型族使用梯度推进技术。在算法的最开始(序列的第一个模型)，伪残差被设置为等于观察值。然后，我们重复 L 次(对于序列的 L 个模型)以下步骤:**

*   **将最佳可能的弱学习器拟合到伪残差(近似与当前强学习器的梯度相反)**
*   **计算最佳步长的值，该值定义了我们在新的弱学习器的方向上更新集成模型的程度**
*   **通过添加新的弱学习器乘以步长(形成梯度下降的步长)来更新集成模型**
*   **计算新的伪残差，该伪残差指示对于每个观测，我们接下来将在哪个方向上更新集合模型预测**

**重复这些步骤，然后我们按顺序建立我们的 L 模型，并按照梯度下降法聚合它们。请注意，自适应提升试图在每次迭代中精确地解决“局部”优化问题(找到最佳弱学习器及其系数以添加到强模型)，而梯度提升则使用梯度下降方法，并且可以更容易地适应大量损失函数。因此，**梯度增强可以被认为是 adaboost 对任意可微损失函数**的推广。**

**![](img/13cfcc891658cc5220645968f5299da4.png)**

**Gradient boosting updates values of the observations at each iteration. Weak learners are trained to fit the pseudo-residuals that indicate in which direction to correct the current ensemble model predictions to lower the error.**

# **堆叠概述**

**堆垛与装袋和助推主要有两点不同。第一次堆叠通常考虑**异质弱学习者**(组合不同的学习算法)，而 bagging 和 boosting 主要考虑同质弱学习者。第二，堆叠学习使用元模型来组合基本模型，而打包和提升遵循确定性算法来组合弱学习器。**

## **堆垛**

**正如我们已经提到的，堆叠的想法是学习几个不同的弱学习器，**通过训练一个元模型**来组合它们，以基于这些弱模型返回的多个预测来输出预测。因此，我们需要定义两件事情来构建我们的堆叠模型:我们想要适合的 L 个学习者和组合它们的元模型。**

**例如，对于分类问题，我们可以选择 KNN 分类器、逻辑回归和 SVM 作为弱学习器，并决定学习神经网络作为元模型。然后，神经网络将把我们三个弱学习器的输出作为输入，并基于它学习返回最终预测。**

**所以，假设我们要拟合一个由 L 个弱学习者组成的堆叠系综。然后，我们必须遵循以下步骤:**

*   **将训练数据分成两部分**
*   **选择 L 个弱学习者，并使它们适合第一层的数据**
*   **对于 L 个弱学习者中的每一个，对第二个文件夹中的观察值进行预测**
*   **使用弱学习者做出的预测作为输入，在第二层上拟合元模型**

**在前面的步骤中，我们将数据集分成两部分，因为已经用于弱学习者训练的数据预测**与元模型**的训练不相关。因此，将数据集分成两部分的一个明显的缺点是，我们只有一半的数据来训练基本模型，另一半的数据来训练元模型。然而，为了克服这种限制，我们可以遵循某种“k-fold 交叉训练”方法(类似于在 k-fold 交叉验证中所做的),使得所有的观察都可以用于训练元模型:对于任何观察，弱学习者的预测都是利用在 k-1 fold 上训练的这些弱学习者的实例来完成的，这些弱学习者不包含所考虑的观察。换句话说，它包括在 k-1 个折叠上的训练，以便在剩余的折叠上进行预测，并且迭代地获得对任何折叠中的观察结果的预测。这样做，我们可以为数据集的每个观察值生成相关的预测，然后在所有这些预测上训练我们的元模型。**

**![](img/1ab8335a8ccf7e951fe9cae6ee2cabce.png)**

**Stacking consists in training a meta-model to produce outputs based on the outputs returned by some lower layer weak learners.**

## **多层堆叠**

**堆叠的一个可能扩展是多级堆叠。它包括用多层进行**堆叠。作为一个例子，让我们考虑一个 3 层堆叠。在第一级(层)中，我们拟合已经选择的 L 个弱学习者。然后，在第二个层次，不是在弱模型预测上拟合单个元模型(如前一小节所述)，而是拟合 M 个这样的元模型。最后，在第三级，我们拟合最后一个元模型，该元模型将前一级的 M 个元模型返回的预测作为输入。****

**从实践的角度来看，注意，对于多层堆叠集成模型的不同层的每个元模型，我们必须选择几乎可以是我们想要的任何东西的学习算法(甚至是已经在较低层使用的算法)。我们还可以提到**增加级别可能是数据昂贵的**(如果不使用类似 k-folds 的技术，则需要更多数据)**或者时间昂贵的**(如果使用类似 k-folds 的技术，则需要拟合大量模型)。**

**![](img/5e1f3399c363d26200594f74b383ce6f.png)**

**Multi-level stacking considers several layers of stacking: some meta-models are trained on outputs returned by lower layer meta-models and so on. Here we have represented a 3-layers stacking model.**

# **外卖食品**

**这篇文章的主要观点如下:**

*   **集成学习是一种机器学习范式，其中多个模型(通常称为弱学习器或基础模型)被训练来解决同一问题，并被组合以获得更好的性能**
*   **主要假设是，如果我们以正确的方式组合弱学习者，我们可以获得更准确和/或更鲁棒的模型**
*   **在 bagging 方法中，相同基础模型的几个实例在不同的引导样本上并行(彼此独立地)训练，然后在某种“平均”过程中聚集**
*   **在 bagging 方法中对(几乎)独立同分布拟合模型进行的这种平均操作主要允许我们获得一个方差低于其分量的集合模型:这就是为什么具有低偏差但高方差的基础模型非常适合 bagging 的原因**
*   **在 boosting 方法中，相同基础模型的几个实例被顺序训练，使得在每次迭代中，训练当前弱学习器的方式取决于先前的弱学习器，更具体地说，取决于它们如何对数据进行操作**
*   **boosting 方法中使用的这种迭代学习策略适应于先前模型的弱点来训练当前模型，主要允许我们获得比其分量具有更低偏差的集成模型:这就是为什么具有低方差但高偏差的弱学习器很适合 boosting 的原因**
*   **在堆叠方法中，不同的弱学习器彼此独立地被拟合，并且元模型在其上被训练以基于由基本模型返回的输出来预测输出**

**在这篇文章中，我们已经给出了集成学习的基本概述，尤其是该领域的一些主要概念:自举、bagging、随机森林、boosting (adaboost，gradient boosting)和堆叠。在被搁置的概念中，我们可以提到例如用于 bagging 的 Out-Of-Bag 评估技术，或者非常流行的“XGBoost”(代表 eXtrem Gradient Boosting)，这是一个实现梯度增强方法以及大量其他技巧的库，这些技巧使学习更加高效(并且易于处理大数据集)。**

**最后，我们想通过提醒集成学习是关于组合一些基本模型以便获得具有更好性能/特性的集成模型来结束。因此，即使装袋、助推和堆叠是最常用的集合方法，也有可能出现变体，并且可以设计成更好地适应某些特定问题。这主要需要两件事:充分理解我们所面临的问题…并且有创造力！**

**感谢阅读！**

**我们与巴蒂斯特·罗卡的最后一篇文章:**

**[](/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28) [## 机器学习中不平衡数据集的处理

### 面对不平衡的班级问题，应该做什么，不应该做什么？

towardsdatascience.com](/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28) [](/brief-introduction-to-markov-chains-2c8cab9c98ab) [## 马尔可夫链简介

### 定义、属性和 PageRank 示例。

towardsdatascience.com](/brief-introduction-to-markov-chains-2c8cab9c98ab)**