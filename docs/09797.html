<html>
<head>
<title>The Video Search Engine — My Journey Into Computer Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视频搜索引擎——我的计算机视觉之旅</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-video-search-engine-my-journey-into-computer-vision-9789824e76bb?source=collection_archive---------21-----------------------#2019-12-23">https://towardsdatascience.com/the-video-search-engine-my-journey-into-computer-vision-9789824e76bb?source=collection_archive---------21-----------------------#2019-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="89c6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">制作视频很容易，但是谁有时间看完呢？我提出一个视频搜索引擎来找到相关的时刻(原型包括在内)。</h2></div><p id="a9d0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创作视频内容是我一生的爱好。我记得在中学制作停止动画电影，毕业到 30 分钟。高中和大学的短片。由于我的孩子们，我的“电影”现在更加面向家庭，但我总是在思考新的项目。</p><figure class="le lf lg lh gt li"><div class="bz fp l di"><div class="lj lk l"/></div></figure><p id="92dd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我反思我喜欢做的项目时，我不断回到同一个问题:记录镜头远比理解它容易。想想你手机的相机胶卷。它可能充满了数百个，如果不是数千个，未经编辑的视频，太长，不值得一看。</p><p id="3287" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">创作和消费视频之间的不平衡是“<a class="ae ll" href="https://www.npr.org/2019/08/09/749938354/episode-932-deep-learning-with-the-elephants" rel="noopener ugc nofollow" target="_blank">典型的现代问题</a>”的一部分，这是由廉价的录制设备甚至更便宜的数字存储设备造成的。快速总结一下，问题是我们可以把 15 个摄像头从 15 个不同的角度对准同一个体育赛事两个小时，产生 30 个小时的未剪辑视频。但是没有人有 30 个小时来观看这些内容。</p><blockquote class="lm"><p id="9c29" class="ln lo it bd lp lq lr ls lt lu lv ld dk translated">我们需要的是一种从视频内容中提取有趣瞬间的方法。</p></blockquote><p id="0f52" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">让我们考虑一个更实际的例子。你当地的零售商可能每个月会录制数千小时的视频来识别商店扒手。然而，尽管有明显的商业价值，这个视频是未经编辑的，冗长的，非常不值得一看。需要的是<strong class="kk iu">一个视频</strong>的“搜索引擎”。以图像或视频“关键时刻”作为搜索词输入，输出相关视频片段作为搜索结果的系统。</p><p id="66d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个问题让我想起了 2004 年夏天在我们国家的一个顶级 R&amp;D 实验室——IBM t . j .沃森研究中心的实习。在那里，我看到了计算机视觉项目的早期应用，例如在进出停车场的汽车上绘制边界框。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/279efb0fc27d006e6c4016c69050745b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*yL994F3BlDpPrDw5.gif"/></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Bounding boxes identifying cars and persons</figcaption></figure><p id="cb51" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在 IBM 工作已经 15 年了，但我从未忘记那种神奇的感觉——看着屏幕上闪烁着汽车和人周围的盒子。当时，这样的输出是机器学习和计算能力的突破。我现在可以探索是否可以建立通用的视频搜索引擎。</p><h2 id="4554" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">探索:视频搜索引擎</h2><p id="f22d" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我的目标是建立一个上述视频搜索引擎的原型。该系统将视频记录乒乓球比赛，当球在比赛时提取视频剪辑，并只向用户显示相关的剪辑。第一个关键结果是使用便携式设备视频记录乒乓球比赛，并将视频发送到云存储进行分析。第二个关键结果是训练一个目标检测模型，该模型发现一个乒乓球在运动。第三个关键结果是将提取的视频剪辑输出到 web UI。</p><p id="0df8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我对这个项目施加了各种限制，让它保持在晚上和周末的范围内。例如，我不打算在 Raspberry Pi 上构建一个完全包含的对象检测系统。相反，我将使用 AWS 来检索存储的视频剪辑，通过对象检测器处理它们，并将结果返回给 web UI。视频的实时处理也超出了这个项目的范围。也就是说，这些限制为这个项目带来了令人兴奋的未来机遇。</p><h2 id="d7e5" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">在便携式设备上录制视频</h2><p id="ad9d" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">到目前为止，我已经完成了第一个关键结果的 70%,通过使用 Raspberry Pi 录制视频内容，并将视频发送到 AWS S3 以供未来分析。</p><p id="c607" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从一开始，我就想象树莓 Pi(带有 Pi 相机模块)将是探索便携式视频捕捉的理想选择。我了解到<a class="ae ll" href="https://www.pyimagesearch.com/2019/04/15/live-video-streaming-over-network-with-opencv-and-imagezmq/" rel="noopener ugc nofollow" target="_blank">有很多选择</a>，包括 IP 摄像头、网络摄像头等等。但是事后来看，我很高兴我选择了 Raspberry Pi，因为它的外形、记录良好的代码和热情的社区。</p><p id="bc36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我启动了 Raspberry Pi，我就配置了一个 SSH 环境，这样我就可以从我的笔记本电脑上执行代码来捕捉图像和视频。</p><p id="4527" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后我不得不把视频发给 S3 AWS。我用 Python 做了一个简单的设计:(1)从 Pi 摄像机打开一个视频流，(2)每两秒钟向 S3 发送一帧。</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="a5ae" class="mi mj it nh b gy nl nm l nn no">import imutils<br/>from imutils.video import VideoStream<br/>from imutils.video import FPS<br/>import boto3<br/>import cv2<br/>import datetime<br/>import time<br/>from decimal import Decimal<br/>import uuid<br/>import json<br/>import pytz<br/>from pytz import timezone<br/>import os</span><span id="cf60" class="mi mj it nh b gy np nm l nn no"># init interface to AWS S3 via Python<br/>s3_client = boto3.client('s3',<br/>      aws_access_key_id=os.environ["AWS_ACCESS_KEY_ID"],<br/>      aws_secret_access_key=os.environ["AWS_SECRET_ACCESS_KEY"])<br/><br/># init where to send the captured frames<br/>s3_bucket = "img-from-raspi-for-web"<br/>s3_key_frames_root = "frames/"</span><span id="ebc5" class="mi mj it nh b gy np nm l nn no"># init video stream from Raspberry Pi Camera<br/>print("[INFO] starting video stream...")<br/>vs = VideoStream(src=0).start()<br/># vs = VideoStream(usePiCamera=True).start()<br/>time.sleep(2.0) # warm up the sensor</span><span id="bdf7" class="mi mj it nh b gy np nm l nn no">def convert_ts(ts, selected_timezone):<br/>   <em class="nq"># Converts a timestamp to the configured timezone. Returns a localized datetime object<br/></em>   tz = timezone(selected_timezone)<br/>   utc = pytz.utc<br/>   utc_dt = utc.localize(datetime.datetime.utcfromtimestamp(ts))<br/>   localized_dt = utc_dt.astimezone(tz)<br/>   return localized_dt<br/><br/># loop over frames from the video file stream<br/>while True:<br/>   # grab the frame from the video stream and resize it<br/>   frame = vs.read()<br/>   approx_capture_ts = time.time()<br/>   if frame is None:<br/>      continue # useful to skip blank frames from sensor<br/><br/>   frame = imutils.resize(frame, width=500)<br/>   ret, jpeg = cv2.imencode('.jpg', frame)<br/>   photo = jpeg.tobytes() <br/>   # bytes are used in future section to call Amazon Rekognition API<br/>    <br/>   now_ts = time.time()<br/>   now = convert_ts(now_ts, "US/Eastern")<br/>   year = now.strftime("%Y")<br/>   mon = now.strftime("%m")<br/>   day = now.strftime("%d")<br/>   hour = now.strftime("%H")<br/><br/>   # build s3_key using UUID and other unique identifiers<br/>   s3_key = (s3_key_frames_root + '{}/{}/{}/{}/{}.jpg').format(year, mon, day, hour, frame_id)<br/><br/>   # Store frame image in S3<br/>   s3_client.put_object(<br/>      Bucket=s3_bucket,<br/>      Key=s3_key,<br/>      Body=photo<br/>   )<br/>   time.sleep(2.0) # essentially, a frame rate of 0.5</span></pre><p id="9c41" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图像开始出现在我的 S3 桶中，所以我开始为这个项目设计数据库。</p><p id="49d9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我的设计将每幅图像、其时间戳和每幅图像的预测结果存储在 NoSQL 表中。稍后，我将查询该数据库中的预测，并获取相应的时间戳，以将视频剪辑成相关的剪辑。</p><p id="f99c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，我为预测设置了一个存根，依靠<a class="ae ll" href="https://aws.amazon.com/blogs/aws/launch-welcoming-amazon-rekognition-video-service/" rel="noopener ugc nofollow" target="_blank"> AWS Rekognition </a> API 来检测对象。下面是我如何将数据保存到 DynamoDB 表中的:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="8321" class="mi mj it nh b gy nl nm l nn no">def detect_labels_local_file(photo):<br/>    response = rekog_client.detect_labels(Image={'Bytes': photo},<br/>            MaxLabels=5,<br/>            MinConfidence=60.0)<br/><br/>    for label in response['Labels']:<br/>        print (label['Name'] + ' : ' + str(label['Confidence']))<br/><br/>    return response</span><span id="dcfd" class="mi mj it nh b gy np nm l nn no">### In the while loop ###<br/>    rekog_response = detect_labels_local_file(photo)</span><span id="9e4d" class="mi mj it nh b gy np nm l nn no">    # Persist frame data in dynamodb<br/>    ddb_item = {<br/>       'frame_id': frame_id,<br/>       'processed_timestamp': now_ts, # beware of putting floats into DDB. Had to use json.loads as a workaround<br/>       'approx_capture_timestamp': approx_capture_ts,<br/>       'rekog_labels': rekog_response['Labels'],<br/>       'rekog_orientation_correction':<br/>          rekog_response['OrientationCorrection']<br/>          if 'OrientationCorrection' in rekog_response else 'ROTATE_0',<br/>       'processed_year_month': year + mon,  # To be used as a Hash Key for DynamoDB GSI<br/>       's3_bucket': s3_bucket,<br/>       's3_key': s3_key<br/>    }<br/><br/>    ddb_data = json.loads(json.dumps(ddb_item), parse_float=Decimal)<br/><br/>    ddb_table.put_item(Item=ddb_data)</span></pre><p id="38bb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">成功！我有一个 NoSQL 表，它引用了 S3 图像、时间戳及其相应的预测:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nr"><img src="../Images/2756f610420901dc5cf065dc9240a2e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BJFEu4mUPIUtH2UmEQRaVw.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Items in DynamoDB table, showing Rekognition API results for captured frames from Pi Camera</figcaption></figure><h2 id="de8a" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">构建乒乓球检测的机器学习模型</h2><p id="9a23" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在摄像头对准我的情况下，AWS Rekognition 以 99.13%的置信度检测到了一个‘人’。但是，Rekognition 能否检测到正在运动的乒乓球来帮助我实现第二个关键结果呢？</p><p id="6244" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遗憾的是，没有。在测试了许多乒乓球图像后，我发现 Rekognition 在检测场景方面的表现令人钦佩——例如标记属于“乒乓球”的图像。但是在寻找乒乓球方面，Rekognition 表现不佳。在大多数情况下，它根本没有将球区分为可识别的物体。当它找到球时，它把它标为“月亮”😎</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nw"><img src="../Images/32a9c323f3f4d2e82f6e46f25156f382.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXmn80y37N_obCS-CRC0Zw.png"/></div></div><figcaption class="me mf gj gh gi mg mh bd b be z dk">Rekognition results for an image of a ping pong match</figcaption></figure><p id="4105" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用 Rekognition API 很方便，但是对于我的项目来说有所限制。幸运的是，如果你想定制自己的模型，亚马逊提供了<a class="ae ll" href="https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection.html" rel="noopener ugc nofollow" target="_blank"> SageMaker 对象检测 API </a>。</p><p id="998e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我从一场乒乓球比赛的视频开始。以下是一些示例框架:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nx"><img src="../Images/bae161567f693383315a7ceebcccc169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AvyC4JQioXNGMykZ.jpg"/></div></div></figure><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi ny"><img src="../Images/fd34d1fc04a768aea272cb3741ed347b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OlWUvIaECEuBINeI.png"/></div></div></figure><h2 id="e21e" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">准备和标记数据</h2><p id="cdb9" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我的第一个任务是用乒乓球标记视频帧，以建立一个训练、验证和测试数据集。<a class="ae ll" href="https://en.wikipedia.org/wiki/FFmpeg" rel="noopener ugc nofollow" target="_blank"> FFmpeg 库</a>有助于将视频转换成我可以标记的图像:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="8495" class="mi mj it nh b gy nl nm l nn no"># from Terminal after installing the ffmpeg library</span><span id="1ba6" class="mi mj it nh b gy np nm l nn no"># Get one frame to identify the area I care about<br/>ffmpeg -i tt-video-1080p.mp4 -ss 00:00:59.000 -vframes 1 thumb.jpg<br/><br/># I found an area I care about in a fixed-position video feed using a photo-editing program: a 512x512 image with a top-left corner at x:710, y:183y<br/>ffmpeg -i tt-video-1080p.mp4 -filter:v "crop=512:512:710:183” cropped.mp4</span><span id="b7dd" class="mi mj it nh b gy np nm l nn no"># output the video as a series of jpg images @ 10 frames per second<br/>ffmpeg -i cropped.mp4 -vf fps=10 thumb%04d.jpg -hide_banner</span></pre><p id="fa6b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的片段在我的机器上生成了成千上万的图像。下一步是给乒乓球添加“边界框”。</p><p id="5630" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有许多<a class="ae ll" href="https://thehive.ai/hive-data?gclid=EAIaIQobChMIwrbin5m45gIV4oNaBR39agSjEAAYAyAAEgLNqfD_BwE" rel="noopener ugc nofollow" target="_blank">服务</a>为你执行这项艰巨的任务，但我选择亲自标记这些图像，以便更深入地理解和欣赏计算机视觉。我求助于 RectLabel，这是一个图像注释工具，用于标记图像以进行边界框对象检测和分割:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi nz"><img src="../Images/6c6aab09b2aaeecf160aee60e1d37282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KyFlTfv107m3pwepzEIVvA.gif"/></div></div></figure><p id="9b77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我在这项任务上花了大约四个小时，平均每分钟 8.3 个标签，得到了 2000 个带标签的图像。这是令人麻木的工作。</p><p id="a4e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我的标记工作进行到一半的时候，我想知道在给定 JPEG 压缩伪像和乒乓球上的运动模糊的情况下，紧密还是松散的边界框会更好地平衡模型精度和模型泛化。在打电话给一个朋友、<a class="ae ll" href="https://www.linkedin.com/in/paulblankley/" rel="noopener ugc nofollow" target="_blank">保罗·布兰克利</a>和<a class="ae ll" href="https://www.quora.com/How-should-I-label-image-data-for-machine-learning" rel="noopener ugc nofollow" target="_blank">咨询网络</a>之后，我了解到“边界框通常紧紧围绕着图像中的每个[对象]”，因为:</p><blockquote class="oa ob oc"><p id="1259" class="ki kj nq kk b kl km ju kn ko kp jx kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated">如果没有精确绘制的边界框，整个算法都会受到影响，从而导致无法准确识别[对象]。这就是为什么质量检查和确保高度重视每个边界框的准确性，从而产生一个强大的人工智能引擎。</p></blockquote><p id="b0d5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我不得不再次做这个项目，我会使用无损图像格式(*。png)并绘制更紧密的边界框来改善我的训练数据。然而我认识到<strong class="kk iu">这种优化不是免费的</strong>。当我开始用更紧的边界框标记图像时，我的平均标记速度下降了大约 50%。</p><p id="ed48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦我完成了对图像的标记，RectLabel 就将数据输出到一个 JSON 文件中，该文件适合计算机视觉任务。以下是输出示例:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="c8e7" class="mi mj it nh b gy nl nm l nn no">{"images":[<br/>    {"id":1,"file_name":"thumb0462.png","width":0,"height":0},<br/>    {"id":2,"file_name":"thumb0463.png","width":0,"height":0},<br/>#    ...<br/>    {"id":4582,"file_name":"thumb6492.png","width":0,"height":0}],</span><span id="7ec7" class="mi mj it nh b gy np nm l nn no">"annotations":[<br/>    {"area":198,"iscrowd":0,"id":1,"image_id":1,"category_id":1,"segmentation":[[59,152,76,152,76,142,59,142]],"bbox":[59,142,18,11]},<br/>    {"area":221,"iscrowd":0,"id":2,"image_id":2,"category_id":1,"segmentation":[[83,155,99,155,99,143,83,143]],"bbox":[83,143,17,13]},<br/>#    ...       {"area":361,"iscrowd":0,"id":4,"image_id":4582,"category_id":1,"segmentation":[[132,123,150,123,150,105,132,105]],"bbox":[132,105,19,19]},</span><span id="aee3" class="mi mj it nh b gy np nm l nn no">"categories":[{"name":"pp_ball","id":1}]<br/>}</span></pre><p id="4305" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我创建了一个函数，按照 Amazon SageMaker 输入通道的预期，将注释分成训练和验证文件夹。如果你正在遵循我的准则，请注意来自<a class="ae ll" href="https://stackoverflow.com/questions/58592206/understanding-the-output-of-sagemaker-object-detection-prediction" rel="noopener ugc nofollow" target="_blank"> Ryo Kawamura </a>的重要提示:</p><blockquote class="oa ob oc"><p id="ed3c" class="ki kj nq kk b kl km ju kn ko kp jx kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated">虽然 COCO JSON 文件中的“category_id”从 1 开始，但 Amazon SageMaker JSON 文件中的“class_id”从 0 开始。</p></blockquote><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="9f60" class="mi mj it nh b gy nl nm l nn no">import json<br/>import os</span><span id="7995" class="mi mj it nh b gy np nm l nn no">def fixCategoryId(category_id):<br/>    return category_id - 1;</span><span id="8896" class="mi mj it nh b gy np nm l nn no">with open(file_name) as f:<br/>    js = json.load(f)<br/>    images = js['images']<br/>    categories = js['categories']<br/>    annotations = js['annotations']<br/>    for i in images:<br/>        jsonFile = i['file_name']<br/>        jsonFile = jsonFile.split('.')[0] + '.json'</span><span id="e643" class="mi mj it nh b gy np nm l nn no">line = {}<br/>        line['file'] = i['file_name']<br/>        line['image_size'] = [{<br/>            'width': int(i['width']),<br/>            'height': int(i['height']),<br/>            'depth': 3<br/>        }]<br/>        line['annotations'] = []<br/>        line['categories'] = []<br/>        for j in annotations:<br/>            if j['image_id'] == i['id'] and len(j['bbox']) &gt; 0:<br/>                line['annotations'].append({<br/>                    'class_id': fixCategoryId(int(j['category_id'])),<br/>                    'top': int(j['bbox'][1]),<br/>                    'left': int(j['bbox'][0]),<br/>                    'width': int(j['bbox'][2]),<br/>                    'height': int(j['bbox'][3])<br/>                })<br/>                class_name = ''<br/>                for k in categories:<br/>                    if int(j['category_id']) == k['id']:<br/>                        class_name = str(k['name'])<br/>                assert class_name is not ''<br/>                line['categories'].append({<br/>                    'class_id': fixCategoryId(int(j['category_id'])),<br/>                    'name': class_name<br/>                })<br/>        if line['annotations']:<br/>            with open(os.path.join('generated', jsonFile), 'w') as p:<br/>                json.dump(line, p)</span><span id="6675" class="mi mj it nh b gy np nm l nn no">jsons = os.listdir('generated')<br/>print ('There are {} images that have annotation files'.format(len(jsons)))</span></pre><p id="5838" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我按照 SageMaker 端点:/train、/validation、/train_annotation 和/validation_annotation 的要求，将文件移动到一个包含四个文件夹的亚马逊 S3 桶中。我在训练和验证文件上使用了 70%的分割，并混洗了数据:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="31d4" class="mi mj it nh b gy nl nm l nn no">import shutil<br/>import random</span><span id="210d" class="mi mj it nh b gy np nm l nn no">num_annotated_files = len(jsons)<br/>train_split_pct = 0.70<br/>num_train_jsons = int(num_annotated_files * train_split_pct)<br/>random.shuffle(jsons) # randomize/shuffle the JSONs to reduce reliance on *sequenced* frames<br/>train_jsons = jsons[:num_train_jsons]<br/>val_jsons = jsons[num_train_jsons:]<br/><br/>#Moving training files to the training folders<br/>for i in train_jsons:<br/>    image_file = './images/'+i.split('.')[0]+'.png'<br/>    shutil.move(image_file, './train/')<br/>    shutil.move('./generated/'+i, './train_annotation/')<br/><br/>#Moving validation files to the validation folders<br/>for i in val_jsons:<br/>    image_file = './images/'+i.split('.')[0]+'.png'<br/>    shutil.move(image_file, './validation/')<br/>    shutil.move('./generated/'+i, './validation_annotation/')<br/><br/><br/>### Upload to S3<br/>import sagemaker<br/>from sagemaker import get_execution_role<br/><br/>role = sagemaker.get_execution_role()<br/>sess = sagemaker.Session()<br/><br/>from sagemaker.amazon.amazon_estimator import get_image_uri<br/>training_image = get_image_uri(sess.boto_region_name, 'object-detection', repo_version="latest")<br/><br/>bucket = 'pp-object-detection' # custom bucket name.<br/># bucket = sess.default_bucket()<br/>prefix = 'rect-label-test'<br/><br/>train_channel = prefix + '/train'<br/>validation_channel = prefix + '/validation'<br/>train_annotation_channel = prefix + '/train_annotation'<br/>validation_annotation_channel = prefix + '/validation_annotation'<br/><br/>sess.upload_data(path='train', bucket=bucket, key_prefix=train_channel)<br/>sess.upload_data(path='validation', bucket=bucket, key_prefix=validation_channel)<br/>sess.upload_data(path='train_annotation', bucket=bucket, key_prefix=train_annotation_channel)<br/>sess.upload_data(path='validation_annotation', bucket=bucket, key_prefix=validation_annotation_channel)<br/><br/>s3_train_data = 's3://{}/{}'.format(bucket, train_channel)<br/>s3_validation_data = 's3://{}/{}'.format(bucket, validation_channel)<br/>s3_train_annotation = 's3://{}/{}'.format(bucket, train_annotation_channel)<br/>s3_validation_annotation = 's3://{}/{}'.format(bucket, validation_annotation_channel)</span></pre><h2 id="79bb" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">训练模型</h2><p id="da1b" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">在下一步中，我创建了一个具有某些超参数的 SageMaker 对象检测器，例如使用“resnet-50”算法的和一个类(我的乒乓球)以及大小为 512x512 像素的图像。</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="5fc8" class="mi mj it nh b gy nl nm l nn no">s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)</span><span id="7e96" class="mi mj it nh b gy np nm l nn no">od_model = sagemaker.estimator.Estimator(training_image, role, train_instance_count=1, train_instance_type='ml.p3.2xlarge', train_volume_size = 50, train_max_run = 360000, input_mode = 'File', output_path=s3_output_location, sagemaker_session=sess)</span><span id="194d" class="mi mj it nh b gy np nm l nn no">od_model.set_hyperparameters(base_network='resnet-50',<br/>                             use_pretrained_model=0,<br/>                             num_classes=1,<br/>                             mini_batch_size=15,<br/>                             epochs=30,<br/>                             learning_rate=0.001,<br/>                             lr_scheduler_step='10',<br/>                             lr_scheduler_factor=0.1,<br/>                             optimizer='sgd',<br/>                             momentum=0.9,<br/>                             weight_decay=0.0005,<br/>                             overlap_threshold=0.5,<br/>                             nms_threshold=0.45,<br/>                             image_shape=512,<br/>                             label_width=600,<br/>                             num_training_samples=num_train_jsons)</span></pre><p id="80ff" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，我为对象检测器设置训练/验证位置，称为。拟合函数，并将模型部署到一个端点:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="de33" class="mi mj it nh b gy nl nm l nn no">train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', content_type='image/png', s3_data_type='S3Prefix')</span><span id="3b9e" class="mi mj it nh b gy np nm l nn no">validation_data = sagemaker.session.s3_input(s3_validation_data, distribution='FullyReplicated', content_type='image/png', s3_data_type='S3Prefix')</span><span id="483d" class="mi mj it nh b gy np nm l nn no">train_annotation = sagemaker.session.s3_input(s3_train_annotation, distribution='FullyReplicated', content_type='image/png', s3_data_type='S3Prefix')</span><span id="0ab4" class="mi mj it nh b gy np nm l nn no">validation_annotation = sagemaker.session.s3_input(s3_validation_annotation, distribution='FullyReplicated', content_type='image/png', s3_data_type='S3Prefix')</span><span id="4a93" class="mi mj it nh b gy np nm l nn no">data_channels = {'train': train_data, 'validation': validation_data, 'train_annotation': train_annotation, 'validation_annotation':validation_annotation}</span><span id="c760" class="mi mj it nh b gy np nm l nn no">od_model.fit(inputs=data_channels, logs=True)</span><span id="1572" class="mi mj it nh b gy np nm l nn no">object_detector = od_model.deploy(initial_instance_count = 1,<br/>                             instance_type = 'ml.m4.xlarge')</span></pre><p id="0709" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴于我只有 2000 张图片，我的亚马逊盒子(ml.p3.2xlarge)花了大约 10 分钟来训练模型。部署端点通常需要更长的时间，并且测试模型的预期是令人痛苦的！</p><p id="c313" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">终于，真相大白的时刻到了。我通过传递一个它从未见过的 PNG 文件来调用我的模型:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="513a" class="mi mj it nh b gy nl nm l nn no">file_with_path = 'test/thumb0695.png'<br/>with open(file_with_path, 'rb') as image:<br/>            f = image.read()<br/>            b = bytearray(f)<br/>            ne = open('n.txt', 'wb')<br/>            ne.write(b)</span><span id="1e9e" class="mi mj it nh b gy np nm l nn no">results = object_detector.predict(b)<br/>        detections = json.loads(results)<br/>        print(detections)</span></pre><p id="21c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我得到了这样的输出:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="6565" class="mi mj it nh b gy nl nm l nn no">[1.0, 0.469, 0.566, 0.537, 0.605, 0.595]</span></pre><p id="f73f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是如何根据 AWS SageMaker 解释这个输出:</p><blockquote class="oa ob oc"><p id="4267" class="ki kj nq kk b kl km ju kn ko kp jx kq od ks kt ku oe kw kx ky of la lb lc ld im bi translated">这些对象数组中的每一个都由六个数字组成。第一个数字是预测的类标签。第二个数字是检测的相关置信度得分。最后四个数字代表边界框坐标[xmin，ymin，xmax，ymax]。这些输出边界框角索引由整体图像尺寸归一化。请注意，这种编码不同于输入使用的编码。json 格式。例如，在检测结果的第一个条目中，0.3088374733924866 是边界框的左坐标(左上角的 x 坐标)作为整体图像宽度的比率，0.07030484080314636 是边界框的上坐标(左上角的 y 坐标)作为整体图像高度的比率，0.7110607028007507 是右坐标(右坐标</p></blockquote><p id="7a75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">凉爽的😏</p><h2 id="ff2d" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">可视化结果</h2><p id="2522" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">坦白地说，我需要一些更实际的东西来欣赏结果。所以我用这个函数来可视化每个预测:</p><pre class="le lf lg lh gt ng nh ni nj aw nk bi"><span id="ddb6" class="mi mj it nh b gy nl nm l nn no">def visualize_detection(img_file, dets, classes=[], thresh=0.6):<br/>    import random<br/>    import matplotlib.pyplot as plt<br/>    import matplotlib.image as mpimg<br/>    <br/>    img = mpimg.imread(img_file)<br/>    plt.imshow(img)<br/>    height = img.shape[0]<br/>    width = img.shape[1]<br/>    colors = dict()<br/>    for det in dets:<br/>        (klass, score, x0, y0, x1, y1) = det<br/>        if score &lt; thresh:<br/>            continue<br/>        cls_id = int(klass)<br/>        if cls_id not in colors:<br/>            colors[cls_id] = (random.random(), random.random(), random.random())<br/>        xmin = int(x0 * width)<br/>        ymin = int(y0 * height)<br/>        xmax = int(x1 * width)<br/>        ymax = int(y1 * height)<br/>        rect = plt.Rectangle((xmin, ymin), xmax - xmin,<br/>                             ymax - ymin, fill=False,<br/>                             edgecolor=colors[cls_id],<br/>                             linewidth=3.5)<br/>        plt.gca().add_patch(rect)<br/>        class_name = str(cls_id)<br/>        if classes and len(classes) &gt; cls_id:<br/>            class_name = classes[cls_id]<br/>        plt.gca().text(xmin, ymin - 2,<br/>                       '{:s} {:.3f}'.format(class_name, score),<br/>                       bbox=dict(facecolor=colors[cls_id], alpha=0.5),fontsize=12, color='white')<br/>    plt.show()</span><span id="e347" class="mi mj it nh b gy np nm l nn no"># then I used the function like this:<br/>object_categories = ['pp_ball']<br/>threshold = 0.40<br/>visualize_detection(file_name_of_image, detections['prediction'], object_categories, threshold)</span></pre><p id="3220" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我看到这个输出时，我兴奋而欣慰地发抖:</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7691e5ec220a45afaf9e3e29277c42b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*XhPzDaBan6w9I38r.png"/></div></figure><p id="8824" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我花了一个小时用它从未见过的各种测试图像敲打模型。有些是伟大的预言。其他的则很傻，比如这个模型把球员制服上的白点当成了乒乓球🙄</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="ns nt di nu bf nv"><div class="gh gi oh"><img src="../Images/38764790b249eaa19ffc9c40e2f803d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hd1qSa5i_ocFP6D42SoOAQ.png"/></div></div></figure><p id="a292" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，我能够通过将置信度阈值提高到 0.40 来消除大多数误报。</p><h2 id="3771" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">未来方向</h2><p id="cb02" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我对目前的结果很满意，但是未来的工作需要评估和优化我的模型。例如，我打算计算<a class="ae ll" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">平均精度(mAP) </a>作为性能指标。那个地图度量将帮助我评估不同的优化，例如添加更多的训练图像，尝试迁移学习，以及尝试其他深度学习拓扑。我将把这些任务留给我的 2020 年路线图(和未来的帖子)。</p><p id="16a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我也很高兴能够在 2020 年实现我的第三个关键成果——通过网络用户界面向用户展示相关的视频剪辑。当关键结果完成后，我将在真实环境中测试整个设置:</p><ul class=""><li id="731b" class="oi oj it kk b kl km ko kp kr ok kv ol kz om ld on oo op oq bi translated">用我的树莓皮视频记录一场现场乒乓球比赛</li><li id="00a1" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">将视频导出到 S3 的图像帧中</li><li id="e727" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">使用对象检测器模型来识别球何时在比赛中</li><li id="a366" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">当球在比赛时存储时间戳</li><li id="af9f" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">向用户提供网络用户界面</li><li id="77d1" class="oi oj it kk b kl or ko os kr ot kv ou kz ov ld on oo op oq bi translated">允许用户将视频过滤到球正在运动的瞬间</li></ul><p id="ee71" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请继续关注这个方向的更多学习和发展。</p><h2 id="6816" class="mi mj it bd mk ml mm dn mn mo mp dp mq kr mr ms mt kv mu mv mw kz mx my mz na bi translated">结束语</h2><p id="3c20" class="pw-post-body-paragraph ki kj it kk b kl nb ju kn ko nc jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">数据科学家普遍认为</p><blockquote class="lm"><p id="6488" class="ln lo it bd lp lq lr ls lt lu lv ld dk translated">算法便宜；</p><p id="749a" class="ln lo it bd lp lq lr ls lt lu lv ld dk translated">数据为王。</p></blockquote><p id="ebc2" class="pw-post-body-paragraph ki kj it kk b kl lw ju kn ko lx jx kq kr ly kt ku kv lz kx ky kz ma lb lc ld im bi translated">这个项目让我深深体会到这一真理。事实上，在 AWS SageMaker 上,<a class="ae ll" href="https://docs.aws.amazon.com/sagemaker/latest/dg/object-detection-api-config.html" rel="noopener ugc nofollow" target="_blank">改变深度学习拓扑结构的能力</a>是微不足道的。然而结果并没有明显的变化。我还以最小的努力在另一个模型中利用了<a class="ae ll" href="https://docs.aws.amazon.com/sagemaker/latest/dg/algo-object-detection-tech-notes.html" rel="noopener ugc nofollow" target="_blank">迁移学习</a>。同样，结果也好不到哪里去。然后，我想起了为我的项目收集和标记图像的艰苦工作…</p><p id="c8cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我将与模型相关的工作和与数据相关的工作在努力程度和跨项目适用性方面进行比较时，我感到困惑。例如，在深度学习拓扑结构之间切换相对容易，许多项目可以在各种计算机视觉任务中利用这些拓扑结构。相比之下，我的图像标签工作需要大量的努力，可能只会对我的项目有益。</p><p id="d97b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">面对这一现实，我对通用视频搜索引擎的可行性感到有点悲观，这种搜索引擎可以从用户任意输入的(1)视频和(2)图像作为搜索词中产生结果。诚然，一个特定用途的视频搜索引擎指日可待。但重要的、不平凡的工作还在后面，以探索一个模型如何能够根据几个图像示例进行归纳，以检测用户想要找到的任何内容。</p><p id="8844" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这方面，这是一个有趣的学习年！</p></div><div class="ab cl ow ox hx oy" role="separator"><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb pc"/><span class="oz bw bk pa pb"/></div><div class="im in io ip iq"><p id="2bf1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">非常感谢<a class="ae ll" href="https://www.linkedin.com/in/sandeeparneja/" rel="noopener ugc nofollow" target="_blank"> Sandeep Arneja </a>、<a class="ae ll" href="https://www.linkedin.com/in/paulblankley/" rel="noopener ugc nofollow" target="_blank"> Paul Blankley </a>、<a class="ae ll" href="https://www.linkedin.com/in/ryo-kawamura-91a11a52/?locale=en_US" rel="noopener ugc nofollow" target="_blank"> Ryo Kawamura </a>和<a class="ae ll" href="https://angel.co/thea-zimnicki" rel="noopener ugc nofollow" target="_blank"> Thea Zimnicki </a>对这个项目的反馈和贡献。</p></div></div>    
</body>
</html>