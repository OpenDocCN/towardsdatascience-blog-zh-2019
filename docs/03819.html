<html>
<head>
<title>In 10 minutes: Web Scraping with Beautiful Soup and Selenium for Data Professionals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">10 分钟内:为数据专业人员提供美味汤和硒的网络刮擦</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/in-10-minutes-web-scraping-with-beautiful-soup-and-selenium-for-data-professionals-8de169d36319?source=collection_archive---------1-----------------------#2019-06-17">https://towardsdatascience.com/in-10-minutes-web-scraping-with-beautiful-soup-and-selenium-for-data-professionals-8de169d36319?source=collection_archive---------1-----------------------#2019-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cd4f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">分析权威指南</h2><div class=""/><div class=""><h2 id="0f0b" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用 BS4 和 Selenium 快速从维基百科和电子商务中提取关键信息</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/70ae4ea1e9e5fdcbe927dc3de5686a35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9PbNnwHdp8ocBn7b.jpg"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">WebScraping — Free Image</figcaption></figure><h1 id="7f9c" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">介绍</h1><p id="505e" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated"><strong class="mb jd">网络抓取</strong>是从网站和在线内容中提取有价值信息的过程。这是一种提取信息和接收数据集以供进一步分析的免费方法。在这个信息实际上高度相关的时代，我相信网络抓取提取替代数据的需求是巨大的，尤其是对我这个数据专业人员来说。</p><p id="6b5d" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">本书的目标</strong>是让你了解使用快速和肮脏的 Python 代码搜集任何公开可用信息的几种方法。只需花 10 分钟来阅读这篇文章——或者更好的是，投稿。然后你可以快速浏览一下你的第一个网络抓取工具的代码。</p><p id="c2cd" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">在这篇文章</strong>中，我们将学习如何从维基百科和电子商务(Lazada)中抓取数据。我们将清理、处理数据，并将数据保存到<em class="na">中。csv </em>文件。我们会用美汤和硒作为我们主要的网页抓取库。</p><h1 id="3e77" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">什么是美汤和硒</h1><h2 id="00a6" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">美味的汤</h2><p id="b225" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">Beautiful Soup 将 HTML 解析成机器可读的树格式，以便快速提取 DOM 元素。它允许提取特定段落和具有特定 HTML ID/Class/XPATH 的表格元素。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/76edecd824aeeeb9ed8c49cb45910b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*QCcUijJQWnu1oW_b2QeunQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Parsing of DOM elements compared to Tree Dir Folder</figcaption></figure><p id="977a" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">每当我需要一个快速和肮脏的方法来提取网上信息。我将永远把 BS 作为我的第一步。通常我会在不到 10 分钟的时间内提取 15 行代码。</p><div class="nn no gp gr np nq"><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jd gy z fp nv fr fs nw fu fw jc bi translated">美丽的汤文档-美丽的汤 4.4.0 文档</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">美汤 4 是通过 PyPi 发布的，如果不能用系统打包器安装，可以安装…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">www.crummy.com</p></div></div><div class="nz l"><div class="oa l ob oc od nz oe lb nq"/></div></div></a></div><h2 id="90e0" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">硒</h2><p id="b2ce" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">Selenium 是一款旨在实现 Web 浏览器自动化的工具。质量保证(QA)工程师通常使用它来自动测试 Selenium 浏览器应用程序。</p><p id="6fd1" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">此外，由于这些自动化功能，它对 web scrape 非常有用:</p><ol class=""><li id="4231" class="of og it mb b mc mv mf mw mi oh mm oi mq oj mu ok ol om on bi translated">单击特定的表单按钮</li><li id="0b8b" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated">在文本字段中输入信息</li><li id="a157" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated">提取浏览器 HTML 代码的 DOM 元素</li></ol><div class="nn no gp gr np nq"><a href="https://www.seleniumhq.org/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jd gy z fp nv fr fs nw fu fw jc bi translated">Selenium - Web 浏览器自动化</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">Selenium 得到了一些最大的浏览器供应商的支持，这些供应商已经(或正在)采取措施使 Selenium 成为一个…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">www.seleniumhq.org</p></div></div><div class="nz l"><div class="ot l ob oc od nz oe lb nq"/></div></div></a></div><h1 id="e650" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">编写你的第一个网络抓取工具</h1><p id="a197" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">(Github 在本文末尾)</p><h1 id="cada" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">美味的汤</h1><h2 id="8538" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">问题陈述</h2><p id="5031" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">假设你是联合国大使，打算访问世界各地的城市，讨论关于气候变化的京都议定书的状况。你需要计划你的旅行，但是你不知道每个国家的首都。因此，你谷歌了一下，在维基百科上找到了这个链接。</p><div class="nn no gp gr np nq"><a href="https://en.wikipedia.org/wiki/List_of_national_capitals" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jd gy z fp nv fr fs nw fu fw jc bi translated">国家首都列表-维基百科</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">这是一个国家首都的列表，包括领土和属地的首都，非主权国家包括…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">en.wikipedia.org</p></div></div></div></a></div><p id="16e4" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">在这个链接中，有一个表格将每个国家映射到首都。你会发现这很好，但你不会止步于此。作为一名数据科学家和联合国大使，您希望从维基百科中提取表格，并将其转储到您的数据应用程序中。你接受了挑战，用 Python 和 BeautifulSoup 编写了一些脚本。</p><h2 id="625d" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">步伐</h2><p id="260a" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">我们将利用以下步骤:</p><ol class=""><li id="8282" class="of og it mb b mc mv mf mw mi oh mm oi mq oj mu ok ol om on bi translated"><strong class="mb jd"> Pip 安装</strong><a class="ae ou" href="https://pypi.org/project/beautifulsoup4/" rel="noopener ugc nofollow" target="_blank"><strong class="mb jd">beautiful soup</strong></a><strong class="mb jd">4、Pip 安装请求</strong>。请求将从 URL 获取 HTML 元素，这将成为 BS 解析的输入。</li><li id="1414" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated"><strong class="mb jd">检查表格引用的是哪个 DOM 元素</strong>。右击鼠标并点击<em class="na">检查元件</em>。Chrome 浏览器的快捷键是 CTRL+I (inspect)。</li><li id="7e2f" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated"><strong class="mb jd">点击左上角的检查按钮</strong>突出显示您想要提取的元素。现在您知道了元素是 HTML 文档中的一个表格元素。</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/28fbf0c8c2104eceebde32e5cdb1ac68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fq8oC46rEnTeuU-D8JH6hA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">National Capitals Elements Wikipedia</figcaption></figure><p id="4f6e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">4.<strong class="mb jd">在您的请求中添加标题和 URL</strong>。这将在维基百科链接中创建一个请求。标题对于欺骗您的请求很有用，这样它看起来就像来自合法的浏览器。</p><p id="cbfd" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">对于维基百科来说，这可能无关紧要，因为所有的信息都是开源的，可以公开获得。但对于其他一些网站，如金融交易网站(SGX)，它可能会阻止没有合法标题的请求。</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="79d3" class="nb li it ox b gy pb pc l pd pe">headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36'}<br/>url = "https://en.wikipedia.org/wiki/List_of_national_capitals"<br/>r = requests.get(url, headers=headers)</span></pre><p id="5e6f" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">5.<strong class="mb jd">启动 BS 和列表元素</strong>提取表格中的所有行</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="3aa8" class="nb li it ox b gy pb pc l pd pe">soup = BeautifulSoup(r.content, "html.parser")<br/>table = soup.find_all('table')[1]<br/>rows = table.find_all('tr')<br/>row_list = list()</span></pre><p id="ee4e" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">6.<strong class="mb jd">遍历表</strong>中的所有行，遍历每个单元格，将其添加到 rows 和 row_list 中</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="d60c" class="nb li it ox b gy pb pc l pd pe">for tr in rows:<br/>    td = tr.find_all('td')<br/>    row = [i.text for i in td]<br/>    row_list.append(row)</span></pre><p id="f84c" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">7.<strong class="mb jd">创建熊猫数据框架并将数据导出到 csv </strong>。</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="59a8" class="nb li it ox b gy pb pc l pd pe">df_bs = pd.DataFrame(row_list,columns=['City','Country','Notes'])<br/>df_bs.set_index('Country',inplace=True)<br/>df_bs.to_csv('beautifulsoup.csv')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/c9f332ba5096ad04778f5e887f625124.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbnksmi5MPTS3q2F-ZMZ_A.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Result of web scraping in csv</figcaption></figure><blockquote class="pg"><p id="48cf" class="ph pi it bd pj pk pl pm pn po pp mu dk translated">恭喜你！只需 7 个步骤和 15 行代码，你就成为了一名专业的 web scraper</p></blockquote><h2 id="e4a7" class="nb li it bd lj nc pq dn ln ne pr dp lr mi ps nh lt mm pt nj lv mq pu nl lx iz bi translated">美丽的汤的局限性</h2><p id="bd63" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">到目前为止，BS 为我们成功地进行了网络搜集。但是我发现根据问题的不同有一些限制:</p><ol class=""><li id="1db8" class="of og it mb b mc mv mf mw mi oh mm oi mq oj mu ok ol om on bi translated">请求过早地接受 html 响应，而不等待 Javascript 的异步调用来呈现浏览器。这意味着它不能获得 Javascript 异步调用(AJAX 等)生成的最新 DOM 元素。</li><li id="da3a" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated">在线零售商，如亚马逊或 Lazada 在网站上安装了反机器人软件，可能会阻止你的爬虫。这些零售商将关闭任何来自 Beautiful Soup 的请求，因为它知道这些请求不是来自合法的浏览器。</li></ol><blockquote class="pv pw px"><p id="561e" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated"><strong class="mb jd">注</strong></p><p id="750d" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated">如果我们在 Lazada 和 Amazon 等电子商务网站上运行 Beautiful Soup，我们会遇到这种连接错误，这是由他们的反垃圾软件阻止机器人发出 http 请求造成的。</p><p id="a007" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated">https connection pool(host = '<a class="ae ou" href="http://www.amazon.com'" rel="noopener ugc nofollow" target="_blank">www . Amazon . com '</a>，port=443):超过 url 的最大重试次数:/(由 SSLError 引起(SSLError(1，'[SSL:CERTIFICATE _ VERIFY _ FAILED]证书验证失败(_ssl.c:833)')，))</p></blockquote><p id="fab9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">解决这个问题的一个方法是使用客户端浏览器，并自动化我们的浏览行为。我们可以通过使用硒来实现这一点。</p><p id="cb09" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">硒万岁！！</p><h1 id="6e0d" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">硒</h1><h2 id="ac83" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">问题陈述</h2><p id="ff8d" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">假设您正在创建价格波动模型来分析 Lazada 和 Amazon 等电子商务提供商。如果没有网络抓取工具，你需要雇人手动浏览大量的产品页面，并把价格一个一个地复制粘贴到 Excelsheet 中。这个过程是非常重复的，特别是如果你想每天/每小时收集数据点。这也将是一个非常耗时的过程，因为它涉及许多手动点击和浏览来复制信息。</p><blockquote class="pv pw px"><p id="7c39" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated">如果我告诉你，你可以自动化这个过程:</p><p id="862d" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated">通过让 Selenium 为您进行产品探索和点击。</p><p id="43c8" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated">通过让 Selenium 打开你的谷歌 Chrome 浏览器来模仿合法用户的浏览行为。</p><p id="b63f" class="lz ma na mb b mc mv kd me mf mw kg mh py mx mk ml pz my mo mp qa mz ms mt mu im bi translated">让 Selenium 将所有信息抽取到列表和 csv 文件中。</p></blockquote><p id="d2f9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">你很幸运，因为你所需要做的就是写一个简单的 Selenium 脚本，然后你就可以在睡个好觉的同时运行 web 抓取程序了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/07a53f119bfe8054a24bc0df7d9a8465.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UX4Tsq9FQ6ZMkWNj2YFpA.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Extracting Lazada Information and Products are time consuming and repetitive</figcaption></figure><h2 id="dfa5" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">安装</h2><ol class=""><li id="2472" class="of og it mb b mc md mf mg mi qb mm qc mq qd mu ok ol om on bi translated"><strong class="mb jd">皮普安装硒</strong>。</li><li id="ef00" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated"><strong class="mb jd">安装硒浏览器</strong>。请参考此链接确定您最喜欢的浏览器(Chrome、Firefox、IE 等)。将它放在与项目相同的目录中。如果你不确定使用哪一个，可以从下面的 Github 链接下载。</li><li id="ab47" class="of og it mb b mc oo mf op mi oq mm or mq os mu ok ol om on bi translated"><strong class="mb jd">包括这些导入</strong></li></ol><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="e9e0" class="nb li it ox b gy pb pc l pd pe">from selenium import webdriver<br/>from selenium.webdriver.common.by import By<br/>from selenium.webdriver.support.ui import WebDriverWait<br/>from selenium.webdriver.support import expected_conditions as EC<br/>from selenium.common.exceptions import TimeoutException</span></pre><p id="f495" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">4.<strong class="mb jd">通过插入可执行路径和 url 驱动 Selenium Chrome 浏览器</strong>。在我的例子中，我使用相对路径来查找位于与我的脚本相同的目录中的 chromedriver.exe。</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="ffb2" class="nb li it ox b gy pb pc l pd pe">driver = webdriver.Chrome(executable_path='chromedriver')<br/>driver.get('https://www.lazada.sg/#')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/2e634f50426bf291714e0d129c17efc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*hk0veJ7ap8P97jmJ2Hd7WQ.gif"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Selenium Running Chrome and Extract Lazada and Redmart Data</figcaption></figure><p id="eb42" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">5.<strong class="mb jd">等待页面加载并找到元素</strong>。这就是 Selenium 与 Requests 和 BS 的不同之处。您可以指示页面等待，直到呈现出某个 DOM 元素。之后，它会继续运行它的网络抓取逻辑。</p><p id="5644" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">您可以停止等待，直到满足预期条件(EC)以通过 ID<em class="na">“Level _ 1 _ Category _ No1”进行查找。</em>如果已经过了 30 秒没有找到该元素，则通过<em class="na"> TimeoutException </em>关闭浏览器。</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="4fae" class="nb li it ox b gy pb pc l pd pe">timeout = 30<br/>try:<br/>    WebDriverWait(driver, timeout).until(EC.visibility_of_element_located((By.ID, "Level_1_Category_No1")))<br/>except TimeoutException:<br/>    driver.quit()</span></pre><p id="c22f" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">恭喜你。我们已经设置了 Selenium 来使用我们的 Chrome 浏览器。现在我们已经准备好自动化信息提取了。</p><h2 id="c4e3" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">信息提取</h2><p id="950d" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">让我们从 Lazada 网站中识别几个属性，并提取它们的 DOM 元素。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/61133eeefb82c87c0a8c6bb19211a7b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dynGsoeFwBS8Ix9H9lqYIg.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">Extracting the DOM Elements via ID, Class, and XPATH Attributes</figcaption></figure><ol class=""><li id="fcbe" class="of og it mb b mc mv mf mw mi oh mm oi mq oj mu ok ol om on bi translated"><strong class="mb jd"> find_element by ID </strong>返回相关的类别列表。</li></ol><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="3d10" class="nb li it ox b gy pb pc l pd pe">category_element = driver.find_element(By.ID,'Level_1_Category_No1').text;<br/>#result -- Electronic Devices as the first category listing</span></pre><p id="b01a" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">2.<strong class="mb jd">获取无序列表 xpath </strong> (ul)并提取每个列表项的值(li)。您可以检查元素，右键单击并选择 copy &gt; XPATH 来轻松地生成相关的 XPATH。请随意打开以下链接了解更多详情。</p><div class="nn no gp gr np nq"><a href="https://www.softwaretestinghelp.com/locate-elements-in-chrome-ie-selenium-tutorial-7/" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jd gy z fp nv fr fs nw fu fw jc bi translated">如何在 Chrome 和 IE 浏览器中定位元素以构建 Selenium 脚本- Selenium 教程…</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">这是我们 Selenium 在线培训系列的教程#7。如果您想查看本系列的所有 Selenium 教程…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">www.softwaretestinghelp.com</p></div></div><div class="nz l"><div class="qf l ob oc od nz oe lb nq"/></div></div></a></div><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="ae57" class="nb li it ox b gy pb pc l pd pe">list_category_elements = driver.find_element(By.XPATH,'//*[@id="J_icms-5000498-1511516689962"]/div/ul')<br/>links = list_category_elements.find_elements(By.CLASS_NAME,"lzd-site-menu-root-item")<br/>for i in range(len(links)):<br/>    print("element in list ",links[i].text)<br/>#result {Electronic Devices, Electronic Accessories, etc}</span></pre><h2 id="99f0" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">点击和操作</h2><ol class=""><li id="b7ab" class="of og it mb b mc md mf mg mi qb mm qc mq qd mu ok ol om on bi translated"><strong class="mb jd">自动行动</strong>。假设你想从 Lazada 主页浏览 Redmart，你可以模仿点击<em class="na"> ActionChains 对象。</em></li></ol><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="d757" class="nb li it ox b gy pb pc l pd pe">element = driver.find_elements_by_class_name('J_ChannelsLink')[1]<br/>webdriver.ActionChains(driver).move_to_element(element).click(element).perform()</span></pre><h2 id="d705" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">从 Redmart 提取所有产品列表</h2><ol class=""><li id="8cc7" class="of og it mb b mc md mf mg mi qb mm qc mq qd mu ok ol om on bi translated"><strong class="mb jd">创建产品标题列表</strong>。我们可以提取并打印它们如下</li></ol><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="69b1" class="nb li it ox b gy pb pc l pd pe">product_titles = driver.find_elements_by_class_name('title')<br/>for title in product_titles:<br/>    print(title.text)</span></pre><div class="ks kt ku kv gt ab cb"><figure class="qg kw qh qi qj qk ql paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/cd1e7e81ff0821e9654976dc6b8b9c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*3Fqiq3r4T8xsy_WU87jmMQ.png"/></div></figure><figure class="qg kw qm qi qj qk ql paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><img src="../Images/8ae595588cb166d5f1d8dca8e446f8ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*w-6W7hJadl3SwgD147ZhoQ.png"/></div><figcaption class="ld le gj gh gi lf lg bd b be z dk qn di qo qp">Redmart Best Seller Title Extractions</figcaption></figure></div><p id="b958" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">2.<strong class="mb jd">提取产品名称、包装尺寸、价格和等级</strong>。我们将打开几个列表来包含每一项，并将它们转储到一个数据帧中。</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="f4c9" class="nb li it ox b gy pb pc l pd pe">product_containers = driver.find_elements_by_class_name('product_container')<br/><br/>for container in product_containers:    product_titles.append(container.find_element_by_class_name('title').text)<br/>pack_sizes.append(container.find_element_by_class_name('pack_size').text)    product_prices.append(container.find_element_by_class_name('product_price').text)<br/>rating_counts.append(container.find_element_by_class_name('ratings_count').text)<br/><br/>data = {'product_title': product_titles, 'pack_size': pack_sizes,'product_price': product_prices, 'rating_count': rating_counts}</span></pre><p id="224c" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">3.<strong class="mb jd">将信息</strong>转储到熊猫数据帧和 csv 中</p><pre class="ks kt ku kv gt ow ox oy oz aw pa bi"><span id="23aa" class="nb li it ox b gy pb pc l pd pe">df_product = pd.DataFrame.from_dict(data)<br/>df_product.to_csv('product_info.csv')</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qq"><img src="../Images/a32565037916ee9e390e0b06674a1da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VWv2mdDvTu-lUPll2_EGw.png"/></div></div><figcaption class="ld le gj gh gi lf lg bd b be z dk">CSV Dump for each of the product in Best Seller Redmart</figcaption></figure><blockquote class="pg"><p id="73ee" class="ph pi it bd pj pk pl pm pn po pp mu dk translated">恭喜你。你已经有效地扩展了你的技能，提取任何网上找到的信息！</p></blockquote><h1 id="4728" class="lh li it bd lj lk ll lm ln lo lp lq lr ki qr kj lt kl qs km lv ko qt kp lx ly bi translated">目的、Github 代码和您的贡献</h1><p id="8af5" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">这个概念证明(POC)的目的是作为我自己的项目的一部分创建的。这个应用程序的目标是使用 web 抓取工具来提取任何公共可用的信息，而不需要太多的成本和人力。</p><p id="eeac" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">在这个 POC 中，我使用 Python 作为脚本语言，<a class="ae ou" href="https://www.mathworks.com/matlabcentral/answers/155126-how-does-the-vision-cascadeobjectdetector-detect-left-and-right-eyes-separately-it-is-constantly-de" rel="noopener ugc nofollow" target="_blank"><em class="na">B</em></a><em class="na">eautiful Soup 和 Selenium library </em> <strong class="mb jd"> </strong>提取必要的信息。</p><p id="db47" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">Github Python 代码位于下面。</p><div class="nn no gp gr np nq"><a href="https://github.com/VincentTatan/Web-Scraping" rel="noopener  ugc nofollow" target="_blank"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jd gy z fp nv fr fs nw fu fw jc bi translated">Vincent tatan/网页抓取</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">美汤硒网刮。为 VincentTatan/Web 抓取开发做出贡献，创建一个…</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">github.com</p></div></div><div class="nz l"><div class="qu l ob oc od nz oe lb nq"/></div></div></a></div><p id="35d3" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">您可以随意克隆这个库，并在有时间的时候贡献自己的一份力量。</p><h1 id="2c17" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">美汤和股票投资</h1><p id="f33c" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">代替今天关于 python 和 web 抓取的主题。你也可以看看我的另一篇关于为有抱负的投资者搜集信息的文章。您应该尝试通过这个演示来指导您编写快速而肮脏的 Python 代码，以便收集、分析和可视化股票。</p><div class="nn no gp gr np nq"><a rel="noopener follow" target="_blank" href="/value-investing-dashboard-with-python-beautiful-soup-and-dash-python-43002f6a97ca"><div class="nr ab fo"><div class="ns ab nt cl cj nu"><h2 class="bd jd gy z fp nv fr fs nw fu fw jc bi translated">价值投资仪表盘，配有 Python Beautiful Soup 和 Dash Python</h2><div class="nx l"><h3 class="bd b gy z fp nv fr fs nw fu fw dk translated">价值投资的 Web 抓取与快速 Dash 可视化概述</h3></div><div class="ny l"><p class="bd b dl z fp nv fr fs nw fu fw dk translated">towardsdatascience.com</p></div></div><div class="nz l"><div class="qv l ob oc od nz oe lb nq"/></div></div></a></div><p id="b1a0" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">希望从这篇相关的出版物中，您可以学习如何收集关键信息并开发有用的应用程序。如果你喜欢，请阅读并联系我。</p><h2 id="ff1b" class="nb li it bd lj nc nd dn ln ne nf dp lr mi ng nh lt mm ni nj lv mq nk nl lx iz bi translated">最后…</h2><p id="4a40" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">咻…就是这样，关于我的想法，我把它写成了文字。我真的希望这对你们来说是一个伟大的阅读。因此，我希望我的想法可以成为你发展和创新的灵感来源。</p><p id="82f1" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">请<strong class="mb jd">在下面评论</strong>出来建议和反馈。</p><p id="1da9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">快乐编码:)</p><h1 id="f668" class="lh li it bd lj lk ll lm ln lo lp lq lr ki ls kj lt kl lu km lv ko lw kp lx ly bi translated">关于作者</h1><p id="5193" class="pw-post-body-paragraph lz ma it mb b mc md kd me mf mg kg mh mi mj mk ml mm mn mo mp mq mr ms mt mu im bi translated">Vincent Tatan 是一名数据和技术爱好者，拥有在 Visa Inc .和 Lazada 实施微服务架构、数据工程和分析管道项目<a class="ae ou" href="https://bit.ly/2I8jkWV." rel="noopener ugc nofollow" target="_blank">的相关工作经验。</a></p><p id="cee0" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">Vincent 是土生土长的印度尼西亚人，在解决问题方面成绩斐然，擅长全栈开发、数据分析和战略规划。</p><p id="d165" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">他一直积极咨询 SMU BI &amp; Analytics Club，指导来自不同背景的有抱负的数据科学家和工程师，并为企业开发他们的产品开放他的专业知识。</p><p id="066a" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">请通过<a class="ae ou" href="http://www.linkedin.com/in/vincenttatan/" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd"> LinkedIn </strong> </a> <strong class="mb jd">，</strong><a class="ae ou" href="https://medium.com/@vincentkernn" rel="noopener"><strong class="mb jd">Medium</strong></a><strong class="mb jd">或</strong> <a class="ae ou" href="https://www.youtube.com/user/vincelance1/videos" rel="noopener ugc nofollow" target="_blank"> <strong class="mb jd"> Youtube 频道</strong> </a>联系文森特</p><p id="ac66" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">免责声明</strong></p><p id="ace1" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated"><strong class="mb jd">本免责声明告知读者，文中表达的观点、想法和意见仅属于作者，不一定属于作者的雇主、组织、委员会或其他团体或个人</strong>。<strong class="mb jd">参考文献是从列表中挑选的，与其他作品的任何相似之处纯属巧合</strong></p><p id="4af9" class="pw-post-body-paragraph lz ma it mb b mc mv kd me mf mw kg mh mi mx mk ml mm my mo mp mq mz ms mt mu im bi translated">这篇文章纯粹是作者的个人项目，与任何其他隐藏的目的无关。</p></div></div>    
</body>
</html>