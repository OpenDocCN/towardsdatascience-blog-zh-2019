<html>
<head>
<title>Why Stochastic Gradient Descent Works?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么随机梯度下降有效？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-towardsdatascience-com-why-stochastic-gradient-descent-works-9af5b9de09b8?source=collection_archive---------6-----------------------#2019-09-14">https://towardsdatascience.com/https-towardsdatascience-com-why-stochastic-gradient-descent-works-9af5b9de09b8?source=collection_archive---------6-----------------------#2019-09-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/a7bab889f628e0483c9194b3c62d5ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*j0n9QJjIR0KIaWtKXsixtg.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Crazy paths often lead to the right destination!</figcaption></figure><p id="3c62" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">优化代价函数是机器学习中最重要的概念之一。梯度下降是最常见的优化算法，也是我们如何训练 ML 模型的基础。但对于大型数据集来说，这可能会非常慢。这就是为什么我们使用这种算法的一种变体，称为随机梯度下降，以使我们的模型学习得更快。但是是什么让它更快呢？这是有代价的吗？</p><p id="3571" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="kw">嗯……在深入 SGD 之前，这里有一个香草渐变下降的快速提醒……</em></strong></p><p id="c5e4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们首先随机初始化模型的权重。使用这些权重，我们计算训练集中所有数据点的成本<strong class="ka ir">。然后计算代价相对于权重的梯度，最后更新权重。这个过程一直持续到我们达到最小值。</strong></p><p id="f153" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更新步骤是这样的…</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi kx"><img src="../Images/651e15f76ab179ffbcf6023b6cfb6e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*SRdydR97i52LmZtuRL8Rhg.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">J is the cost over all the training data points</figcaption></figure><p id="b570" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">现在，如果我们的训练集中的数据点数量变大，会发生什么？说<strong class="ka ir"><em class="kw">m = 1000 万。</em> </strong>在这种情况下，我们必须将所有<strong class="ka ir"> <em class="kw"> m </em> </strong>示例的成本相加，以执行一个更新步骤！</p><p id="208f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> <em class="kw">新币来救我们了……</em></strong></p><blockquote class="lc ld le"><p id="15ee" class="jy jz kw ka b kb kc kd ke kf kg kh ki lf kk kl km lg ko kp kq lh ks kt ku kv ij bi translated">我们不是计算所有数据点的成本，而是计算单个数据点和相应梯度的成本。然后我们更新权重。</p></blockquote><p id="8b87" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">更新步骤如下…</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi li"><img src="../Images/4a32b8e8cd6904eff356d8442e284e1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*nT67GfNMNEjFBncJcAtQew.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">J_i is the cost of ith training example</figcaption></figure><p id="76a2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以很容易地看到，在这种情况下，更新步骤执行得非常快，这就是为什么我们可以在很短的时间内达到最小值。</p><h2 id="9381" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kj ls lt lu kn lv lw lx kr ly lz ma mb bi translated">但是 SGD 为什么会起作用？</h2><p id="517d" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">关键的概念是，我们不需要检查所有的训练示例来了解斜率下降的方向。通过一次只分析一个例子并跟随它的斜率，我们可以到达一个非常接近实际最小值的点。这里有一个直觉…</p><blockquote class="lc ld le"><p id="9190" class="jy jz kw ka b kb kc kd ke kf kg kh ki lf kk kl km lg ko kp kq lh ks kt ku kv ij bi translated">假设你做了一个 app，想通过 100 个客户的反馈来改进它。你可以用两种方法来做这件事。在第一种方式中，你可以将应用程序交给第一个客户，然后将他的反馈交给第二个客户，然后是第三个，以此类推。从他们那里收集反馈后，你可以改进你的应用程序。但是第二种方式，你可以在得到第一个客户的反馈后，马上对 app 进行改进。然后你把它给第二个，在给第三个之前你又提高了一次。请注意，通过这种方式，您可以以更快的速度改进您的应用程序，并且可以更早地达到最佳状态。</p></blockquote><p id="7868" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">希望你能分辨出第一个过程是普通梯度下降，第二个是 SGD。</p><h2 id="b789" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kj ls lt lu kn lv lw lx kr ly lz ma mb bi translated">但是 SGD 也有一些缺点…</h2><p id="730d" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">SGD 比原始梯度下降法快得多，但是 SGD 的收敛路径比原始梯度下降法更嘈杂。这是因为在每一步中，它不是计算实际的梯度，而是一个近似值。所以我们看到成本有很大的波动。但是，这仍然是一个更好的选择。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="gh gi mh"><img src="../Images/711a55f31ce1352b18be8b274181a06c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OETN2wimt58AnHVelhdLpw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Convergence paths are shown on a contour plot</figcaption></figure><p id="add4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们可以在上面的等高线图中看到 SGD 的噪声。需要注意的是，vanilla GD 更新次数较少，但是每次更新实际上都是在一个完整的时期之后进行的。SGD 需要很多更新步骤，但它将需要较少的历元数，也就是说，在这种情况下，我们迭代所有示例的次数将较少，因此这是一个更快的过程。</p><p id="0e01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">从图中可以看出，梯度下降还有第三种变化，称为小批量梯度下降。这是一个利用 SGD 的灵活性和 GD 的准确性的过程。在这种情况下，我们一次获取固定数量(称为<strong class="ka ir">批量</strong>)的训练样本，并计算成本和相应的梯度。然后，我们更新重量，并为下一批继续相同的过程。如果<strong class="ka ir">批量= 1 </strong>，那么它变成 SGD，如果<strong class="ka ir">批量= m </strong>，那么它变成正常 GD。</p><figure class="ky kz la lb gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/2c9e4f3dcce9d6378680e44ead3971fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*teNn1LJVlmUSxZNvqNQquQ.png"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">J_b is the cost of bth batch</figcaption></figure><h2 id="d758" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kj ls lt lu kn lv lw lx kr ly lz ma mb bi translated">从头开始实施</h2><p id="388d" class="pw-post-body-paragraph jy jz iq ka b kb mc kd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv ij bi translated">这里有一个 python 实现的小批量渐变下降。您可以轻松地使 batch_size = 1 来实现 SGD。在这段代码中，我使用 SGD 优化了一个简单的二元分类问题的逻辑回归的成本函数。</p><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="94ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里找到完整的代码<a class="ae mp" href="https://github.com/Suji04/ML_from_Scratch/blob/master/sgd.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="dfa0" class="lj lk iq bd ll lm ln dn lo lp lq dp lr kj ls lt lu kn lv lw lx kr ly lz ma mb bi translated">还好奇？看一个我最近做的视频…</h2><figure class="ky kz la lb gt jr"><div class="bz fp l di"><div class="mq mo l"/></div></figure><p id="bceb" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我希望你喜欢阅读。下次见…学习愉快！</p></div></div>    
</body>
</html>