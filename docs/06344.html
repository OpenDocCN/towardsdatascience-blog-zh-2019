<html>
<head>
<title>Temporal-Difference (TD) Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时差学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/temporal-difference-learning-47b4a7205ca8?source=collection_archive---------6-----------------------#2019-09-12">https://towardsdatascience.com/temporal-difference-learning-47b4a7205ca8?source=collection_archive---------6-----------------------#2019-09-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/38be61735beaba37a5ffc2031679d874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLZg3cqxslMXmqTJdEKW_A.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="d61b" class="pw-subtitle-paragraph kb jd je bd b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks dk translated">使用时间差异(TD)学习的强化学习</h2></div><p id="df03" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">在本文中，我将介绍时差学习方法。时域差分法是蒙特卡罗法和动态规划法的结合。</p><p id="9fa1" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">下面是蒙特卡罗(MC)方法的主要特征:</p><ol class=""><li id="f8ce" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo lu lv lw lx bi translated">没有模型(代理不知道状态 MDP 转换)</li><li id="adf3" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">代理人<strong class="kv jf">从<strong class="kv jf">采样的</strong>经验中学习</strong>(类似于 MC)</li><li id="70c7" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">像 DP 一样，TD 方法部分基于其他<strong class="kv jf">学习估计</strong>来更新估计，而不等待最终结果(它们<strong class="kv jf">像 DP 一样引导</strong>)。</li><li id="9747" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">它可以从<strong class="kv jf">不完整事件</strong>中学习，因此该方法也可以用于连续问题</li><li id="0727" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo lu lv lw lx bi translated">TD 将猜测更新为猜测，并根据实际经验修改猜测</li></ol><p id="6db3" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">为了更好地理解这一点，考虑一个现实生活中的类比；如果蒙特卡洛学习就像一次年度考试，学生在年底完成这一集。类似地，我们有 TD 学习，它可以被认为是每周或每月的考试(学生可以在每个小间隔后根据这个分数(收到的奖励)调整他们的表现，最终分数是所有每周测试的累积(总奖励))。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/a60d05caccd42db30df214b5b4e43a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y2HmwT9CyhYxhb9W.png"/></div></div></figure><h2 id="391a" class="mi mj je bd mk ml mm dn mn mo mp dp mq lc mr ms mt lg mu mv mw lk mx my mz na bi translated">TD(0)</h2><p id="1037" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">TD(0)是 TD 学习的最简单形式。在这种形式的 TD 学习中，在每一步之后，值函数用下一个状态的值来更新，并且沿途获得奖励。这种观察到的回报是保持学习基础的关键因素，并且算法在足够数量的采样后收敛(在无穷大的极限内)。下面是 TD(0)的备份图，以及我们的 gem 收集和检查示例的 TD(0)示例。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/c3ffa350f756346db9fa85a511ac9ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m6nlYyR4MzyT2ztG.png"/></div></div></figure><p id="8c4a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">TD(0)可以用下图中的等式来表示。等式 1 通常在文献中显示，但我发现按照等式 2 编写的等式更直观。我们用α作为学习因子，γ作为折现因子。这里，状态 S 的值在下一个时间步长(t+1)中基于在时间步长 t 之后观察到的回报 r t+1 被更新，其中 S 的期望值在时间步长 t+1 中。因此，S 在时间步长 t 处的自举使用时间步长 t+1 的估计，而 r t+1 是观察到的回报(使算法落地的真实事物)。TD 目标和 TD 误差是等式的两个重要组成部分，用于 RL 的许多其他领域。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/871f8b3934fb4a716dde9f176b4f132d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RWSZAoXxeZMDLMFF.png"/></div></div></figure><h2 id="9ad7" class="mi mj je bd mk ml mm dn mn mo mp dp mq lc mr ms mt lg mu mv mw lk mx my mz na bi translated">萨尔萨</h2><p id="1b69" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">用于<strong class="kv jf">控制或改进</strong>的 TD 算法之一是 SARSA。SARSA 的名字来源于代理从一个状态-动作值对向另一个状态-动作值对迈出一步，并在此过程中收集奖励 R(因此是 S t，at，R t+1，S t+1 &amp;一个 t+1 元组，它创建了术语<strong class="kv jf"> S，A，R，S，A </strong>)。SARSA 是一种<strong class="kv jf">政策上的</strong>方法。SARSA 使用动作值函数 Q 并遵循策略π。<strong class="kv jf">GPI</strong>(blog-2 中描述的广义策略迭代)用于基于策略π采取行动(<strong class="kv jf">ε-贪婪</strong>以确保探索以及贪婪地改进策略)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/634e2e170acb4094767a1c3c8a34a7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qpVNqnaLaSC6gcvm.png"/></div></div></figure><p id="8e47" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">SARSA 可以用下图所示的等式来表示。等式 1 通常在文献中显示，但我发现按照等式 2 编写的等式更直观。我们用α作为学习因子，γ作为折现因子。还显示了 TD 目标和 TD 误差的动作值版本。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nj"><img src="../Images/d96b4625076c87a330e0f5f849864e50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4O-RaUBcM5ktj5SjxWff7g.png"/></div></div></figure><h2 id="1b8b" class="mi mj je bd mk ml mm dn mn mo mp dp mq lc mr ms mt lg mu mv mw lk mx my mz na bi translated">q 学习</h2><p id="b021" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">Q-learning 是一种<strong class="kv jf">非策略</strong>算法。在非策略学习中，我们评估目标策略(π)，同时遵循另一个称为<strong class="kv jf">行为策略</strong> (μ)的策略(这就像机器人遵循视频或基于<strong class="kv jf">另一个代理</strong>获得的经验的代理学习)。DQN(深度 Q-learning)登上了《自然》杂志的头版，它是一种基于 Q-Learning 的算法(几乎没有额外的技巧)，在雅达利游戏中超越了人类水平的专业知识(我将在未来的帖子中详细介绍 DQN)。在 Q-learning 中，目标策略是<strong class="kv jf">贪婪策略</strong>，行为策略是<strong class="kv jf">ε-贪婪策略</strong>(这确保了探索)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nk"><img src="../Images/816c069424d7b5a723d9d15b3808eea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iQWonv3XRRn4y4FS.png"/></div></div></figure><p id="c65a" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">参考下图，以两种不同方式编写的 Q 学习算法。看看目标和行为策略动作在等式中是如何表示的。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/6a4c6156482372ddfb17410e098da014.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0TvpfmcRZXxwmwhA.png"/></div></div></figure><h2 id="515c" class="mi mj je bd mk ml mm dn mn mo mp dp mq lc mr ms mt lg mu mv mw lk mx my mz na bi translated"><strong class="ak">预期萨莎</strong></h2><p id="6c6d" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">预期 SARSA 就像 Q-learning 一样，除了它使用<strong class="kv jf">期望值</strong>而不是下一个状态-动作对的最大值，并考虑了每个动作在当前策略下的可能性。给定下一个状态，Q-learning 算法确定性地将<strong class="kv jf">向同一方向移动</strong>，而 SARSA 按照<strong class="kv jf">期望</strong>跟随，因此，它被称为期望 SARSA。它的备份图如下所示。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/e0dc84f6d3a8ffb06cfd3b8bb5a0c6be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XM4CqHXgtH0Zo_9I.png"/></div></div></figure><p id="1c43" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">参考下图，了解以两种不同方式编写的预期 SARSA 算法。与 Q-learning 的区别就凸显出来了。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nl"><img src="../Images/a74d1513df2523ab4efdb1aa06ebc129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RocrMGXIbLQN5_Y0.png"/></div></div></figure><p id="3748" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">TD 方法具有以下<strong class="kv jf">优势</strong>:</p><ul class=""><li id="469d" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo nn lv lw lx bi translated">TD 可以在线或离线学习每一步<strong class="kv jf"/></li><li id="201e" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo nn lv lw lx bi translated">TD 可以从<strong class="kv jf">不完整序列</strong>中学习</li><li id="a8fe" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo nn lv lw lx bi translated">TD 可以在<strong class="kv jf">非终止环境中工作</strong>(续)</li><li id="1261" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo nn lv lw lx bi translated">与 MC 相比，TD 具有更低的方差,因为它依赖于一个随机动作、转换、奖励</li><li id="5587" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo nn lv lw lx bi translated">通常比 MC 更有效</li><li id="bfab" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo nn lv lw lx bi translated">TD 利用了<strong class="kv jf">马尔可夫特性</strong>，因此在马尔可夫环境中更加有效</li></ul><p id="6260" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated">但是它也有下面的<strong class="kv jf">限制</strong>:</p><ul class=""><li id="611a" class="lp lq je kv b kw kx kz la lc lr lg ls lk lt lo nn lv lw lx bi translated">TD 是一个有偏差的估计值</li><li id="6645" class="lp lq je kv b kw ly kz lz lc ma lg mb lk mc lo nn lv lw lx bi translated">TD 对初始值更加敏感</li></ul><h2 id="e9f8" class="mi mj je bd mk ml mm dn mn mo mp dp mq lc mr ms mt lg mu mv mw lk mx my mz na bi translated">最后一个音符</h2><p id="e30b" class="pw-post-body-paragraph kt ku je kv b kw nb kf ky kz nc ki lb lc nd le lf lg ne li lj lk nf lm ln lo im bi translated">这里我们介绍了单步 TD 方法，但也有多步 TD 方法以及 TD 和 MC 的组合，如 TD(λ)算法。TD 是强化学习中的突破性创新，每个从业者都需要将它放在他们的工具包中。</p><h2 id="a706" class="mi mj je bd mk ml mm dn mn mo mp dp mq lc mr ms mt lg mu mv mw lk mx my mz na bi translated">感谢阅读。可以联系我@ <a class="ae no" href="http://www.linkedin.com/in/baijayantaroy" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</h2></div><div class="ab cl np nq hx nr" role="separator"><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu nv"/><span class="ns bw bk nt nu"/></div><div class="im in io ip iq"><p id="510d" class="pw-post-body-paragraph kt ku je kv b kw kx kf ky kz la ki lb lc ld le lf lg lh li lj lk ll lm ln lo im bi translated"><em class="nw">只需每月 5 美元，就可以无限制地获取最鼓舞人心的内容……点击下面的链接，成为 Medium 会员，支持我的写作。谢谢大家！<br/></em><a class="ae no" href="https://baijayanta.medium.com/membership" rel="noopener"><strong class="kv jf"><em class="nw">https://baijayanta.medium.com/membership</em></strong></a></p></div></div>    
</body>
</html>