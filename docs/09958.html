<html>
<head>
<title>Multi-Armed Bandits and Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多臂土匪与强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da?source=collection_archive---------2-----------------------#2019-12-30">https://towardsdatascience.com/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da?source=collection_archive---------2-----------------------#2019-12-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="35c5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用 Python 例子温和地介绍了经典问题</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d6aed4f48ca57e6abffaac4e7af85316.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2orlZg7aPX5dhdBF"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@carltraw?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Carl Raw</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="1e5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">多臂土匪问题是一些最简单的强化学习(RL)问题来解决。我们有一个代理，我们允许它选择行动，每个行动都有一个回报，这个回报是根据一个给定的、潜在的概率分布返回的。这个游戏有很多集(在这种情况下是单个动作)，目标是最大化你的奖励。</p><p id="5b5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个简单的图片是考虑在<em class="lv">k</em>-许多<a class="ae ky" href="https://en.wikipedia.org/wiki/Slot_machine" rel="noopener ugc nofollow" target="_blank"> <em class="lv">独臂强盗</em> </a>(即老虎机)或一个带<em class="lv"> k </em>手臂的大老虎机之间进行选择。你拉的每一只手臂都有不同的奖励。你有 1000 个 25 美分的硬币，所以你需要制定一些策略来获得最大的收益。</p><p id="8b1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解决这个问题的一个方法是依次选择每一个，记录你得到了多少，然后继续回到付出最多的那个。这是可能的，但是，如前所述，每个 bandit 都有一个与之相关的潜在概率分布，这意味着在找到正确的样本之前，您可能需要更多的样本。但是，你每花一点时间去找出最好的强盗，都会让你失去最大的回报。这种基本的平衡行为被称为<strong class="lb iu">探索-利用困境</strong>。这个基本问题的形式出现在 AI 和 RL 之外的领域，例如在<a class="ae ky" href="https://joshkaufman.net/explore-exploit/" rel="noopener ugc nofollow" target="_blank">选择职业</a>、<a class="ae ky" href="https://www.aaai.org/ocs/index.php/IJCAI/IJCAI15/paper/viewFile/10972/10798" rel="noopener ugc nofollow" target="_blank">金融</a>、<a class="ae ky" href="https://econ.arizona.edu/sites/econ/files/wilsonetal_jepgeneral2014_print.pdf" rel="noopener ugc nofollow" target="_blank">人类心理学</a>，甚至<a class="ae ky" href="http://www.pnas.org/content/106/52/22387" rel="noopener ugc nofollow" target="_blank">医学伦理</a>(尽管，我认为我最喜欢的提议用例与建议有关，由于其丰富性，它在二战期间被给予纳粹德国，<a class="ae ky" href="https://people.eecs.berkeley.edu/~russell/classes/cs294/s11/readings/Gittins:1979.pdf" rel="noopener ugc nofollow" target="_blank">“作为智力破坏的最终形式。”</a>)。</p><h1 id="7279" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">TL；速度三角形定位法(dead reckoning)</h1><p id="7c2a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们按照萨顿和巴尔托的书的框架介绍了多臂强盗问题，并开发了一个解决这些问题的框架和例子。在本帖中，我们将关注ϵ−greedy、ϵ−decay 和乐观策略。一如既往，你可以在这里找到<a class="ae ky" href="https://www.datahubbs.com/multi_armed_bandits_reinforcement_learning_1/" rel="noopener ugc nofollow" target="_blank">的原帖</a>(适当支持 LaTeX)。</p><h1 id="7c7d" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">问题设置</h1><p id="a541" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">首先，让我们用一点技术细节来描述这个问题。我们希望做的是开发一个估计值<em class="lv"> Qt (a) </em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/9c0653d3a572bf77f14437c5941e0930.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*n6jigGk161weIu8xHYQT_A.png"/></div></figure><p id="cf5c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> Qt(a) </em>是当在步骤<em class="lv"> n </em>采取动作<em class="lv"> An </em>时，估计的期望回报(<em class="lv"> Rn </em>)。我们将反复构建一个模型，这个模型将向每个行为的真实价值靠拢。我们将对所有潜在的概率分布使用高斯(正态)分布，以便平均值对应于真实值(毕竟，给定足够的样本，我们会期望我们的奖励收敛于所选行动的平均值)。</p><p id="1398" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最简单的方法是采取<strong class="lb iu">贪婪的行动</strong>或者采取我们认为会在每个时间点最大化我们的回报的行动。另一种写法是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6ff40df9f2431bc7c7c872e84b1cf497.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*S3nJWVy8oQE0gTDnh2D0jw.png"/></div></figure><p id="48ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将这种最大期望或贪婪行为表示为<em class="lv"> A*n </em>。这是我们前面提到的探索-利用困境的<em class="lv">利用</em>方面，如果目标是最大化我们的回报，这就很有意义。当然，只有当我们对每个行动的预期回报有了一个很好的感觉时(除非我们相当幸运)，重复这样做才会有好的效果。因此，我们需要找出一种算法来探索足够的搜索空间，以便我们可以采取最佳行动。</p><p id="91f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始之前，还有最后一个概念要介绍。在典型的 RL 应用中，我们可能需要数十万次迭代，甚至数百万次以上。运行此类模拟并跟踪所有数据来计算平均回报，很快就会变得非常计算密集型。为了避免这种情况，我们可以使用一个方便的公式，这样我们只需要跟踪两个值:平均值和所走的步数。如果我们需要计算步骤<em class="lv"> n，m_n </em>的平均值，我们可以用前面的平均值<em class="lv">m _ n-1</em>和<em class="lv"> n </em>来计算，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/a19a93ddd2b6caf4049bef0b7e06c8b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*IGzEQ2Ze5N6ijgApE-Tynw.png"/></div></figure><p id="1a71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这些，我们就可以开始制定解决 k 线问题的策略了。</p><h1 id="392e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">ϵ-Greedy 方法</h1><p id="f35c" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们简单地讨论了一个纯粹贪婪的方法，我指出它本身不会很好地工作。考虑如果你实现一个纯贪婪的方法，你在<em class="lv"> n=1 </em>采取一个行动<em class="lv"> A_n=a_1 </em>并得到一个奖励。那么，这就成为了你的最高奖励(假设它是正的)，你只需重复<em class="lv"> a_1 ∀ n </em>(对所有步骤<em class="lv"> n </em>采取行动<em class="lv"> a_1 </em>)。为了鼓励一点探索，我们可以使用ϵ-greedy，这意味着我们探索另一个ϵ.概率的选项这给算法带来了一点噪音，以确保你继续尝试其他值，否则，你会继续利用你的最大回报。</p><p id="f1d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们转向 Python 来实现我们的<em class="lv"> k- </em>武装强盗。</p><h1 id="5cdc" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">建造一个贪婪的 k 臂强盗</h1><p id="3e85" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将定义一个名为<code class="fe mw mx my mz b">eps_bandit</code>的类来运行我们的实验。该类将臂数、<code class="fe mw mx my mz b">k</code>、ε值<code class="fe mw mx my mz b">eps</code>、迭代次数<code class="fe mw mx my mz b">iter</code>作为输入。我们还将定义一个术语<code class="fe mw mx my mz b">mu</code>，我们可以用它来调整每支队伍的平均奖励。</p><p id="4079" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先是模块:</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="1bfb" class="ne lx it mz b gy nf ng l nh ni"># import modules <br/>import numpy as np <br/>import matplotlib.pyplot as plt <br/>import pandas as pd <br/>%matplotlib inline</span></pre><p id="9224" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你在 Jupyter 之外建造这个，忽略<code class="fe mw mx my mz b">%matplotlib inline</code>。</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="1633" class="ne lx it mz b gy nf ng l nh ni">class eps_bandit:<br/>    '''<br/>    epsilon-greedy k-bandit problem<br/>    <br/>    Inputs<br/>    =====================================================<br/>    k: number of arms (int)<br/>    eps: probability of random action 0 &lt; eps &lt; 1 (float)<br/>    iters: number of steps (int)<br/>    mu: set the average rewards for each of the k-arms.<br/>        Set to "random" for the rewards to be selected from<br/>        a normal distribution with mean = 0. <br/>        Set to "sequence" for the means to be ordered from <br/>        0 to k-1.<br/>        Pass a list or array of length = k for user-defined<br/>        values.<br/>    '''<br/>    <br/>    def __init__(self, k, eps, iters, mu='random'):<br/>        # Number of arms<br/>        self.k = k<br/>        # Search probability<br/>        self.eps = eps<br/>        # Number of iterations<br/>        self.iters = iters<br/>        # Step count<br/>        self.n = 0<br/>        # Step count for each arm<br/>        self.k_n = np.zeros(k)<br/>        # Total mean reward<br/>        self.mean_reward = 0<br/>        self.reward = np.zeros(iters)<br/>        # Mean reward for each arm<br/>        self.k_reward = np.zeros(k)<br/>        <br/>        if type(mu) == list or type(mu).__module__ == np.__name__:<br/>            # User-defined averages            <br/>            self.mu = np.array(mu)<br/>        elif mu == 'random':<br/>            # Draw means from probability distribution<br/>            self.mu = np.random.normal(0, 1, k)<br/>        elif mu == 'sequence':<br/>            # Increase the mean for each arm by one<br/>            self.mu = np.linspace(0, k-1, k)<br/>        <br/>    def pull(self):<br/>        # Generate random number<br/>        p = np.random.rand()<br/>        if self.eps == 0 and self.n == 0:<br/>            a = np.random.choice(self.k)<br/>        elif p &lt; self.eps:<br/>            # Randomly select an action<br/>            a = np.random.choice(self.k)<br/>        else:<br/>            # Take greedy action<br/>            a = np.argmax(self.k_reward)<br/>            <br/>        reward = np.random.normal(self.mu[a], 1)<br/>        <br/>        # Update counts<br/>        self.n += 1<br/>        self.k_n[a] += 1<br/>        <br/>        # Update total<br/>        self.mean_reward = self.mean_reward + (<br/>            reward - self.mean_reward) / self.n<br/>        <br/>        # Update results for a_k<br/>        self.k_reward[a] = self.k_reward[a] + (<br/>            reward - self.k_reward[a]) / self.k_n[a]<br/>        <br/>    def run(self):<br/>        for i in range(self.iters):<br/>            self.pull()<br/>            self.reward[i] = self.mean_reward<br/>            <br/>    def reset(self):<br/>        # Resets results while keeping settings<br/>        self.n = 0<br/>        self.k_n = np.zeros(k)<br/>        self.mean_reward = 0<br/>        self.reward = np.zeros(iters)<br/>        self.k_reward = np.zeros(k)</span></pre><p id="2440" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多不同的方法来定义这个类。我这样做是为了一旦我们初始化了我们的问题，我们只需调用<code class="fe mw mx my mz b">run()</code>方法就可以检查输出。默认情况下，每只手臂的平均奖励来自 0 左右的正态分布。设置<code class="fe mw mx my mz b">mu="sequence"</code>将使奖励范围从 0 到<em class="lv"> k-1 </em>，以便在评估结果时了解哪些行动提供了最佳奖励，以及采取了哪些行动。最后，您还可以通过将值传递给<code class="fe mw mx my mz b">mu</code>来设置自己的平均奖励。</p><p id="4d21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们用不同的ϵ\epsilonϵ.值来做一些比较对于其中的每一个，我们将设置 k=10，每集运行 1000 步，运行 1000 集。每集之后，我们将重置盗匪，并复制不同盗匪的平均值，以保持事情的一致性。</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="92b2" class="ne lx it mz b gy nf ng l nh ni">k = 10<br/>iters = 1000<br/><br/>eps_0_rewards = np.zeros(iters)<br/>eps_01_rewards = np.zeros(iters)<br/>eps_1_rewards = np.zeros(iters)<br/><br/>episodes = 1000<br/># Run experiments<br/>for i in range(episodes):<br/>    # Initialize bandits<br/>    eps_0 = eps_bandit(k, 0, iters)<br/>    eps_01 = eps_bandit(k, 0.01, iters, eps_0.mu.copy())<br/>    eps_1 = eps_bandit(k, 0.1, iters, eps_0.mu.copy())<br/>    <br/>    # Run experiments<br/>    eps_0.run()<br/>    eps_01.run()<br/>    eps_1.run()<br/>    <br/>    # Update long-term averages<br/>    eps_0_rewards = eps_0_rewards + (<br/>        eps_0.reward - eps_0_rewards) / (i + 1)<br/>    eps_01_rewards = eps_01_rewards + (<br/>        eps_01.reward - eps_01_rewards) / (i + 1)<br/>    eps_1_rewards = eps_1_rewards + (<br/>        eps_1.reward - eps_1_rewards) / (i + 1)<br/>    <br/>plt.figure(figsize=(12,8))<br/>plt.plot(eps_0_rewards, label="$\epsilon=0$ (greedy)")<br/>plt.plot(eps_01_rewards, label="$\epsilon=0.01$")<br/>plt.plot(eps_1_rewards, label="$\epsilon=0.1$")<br/>plt.legend(bbox_to_anchor=(1.3, 0.5))<br/>plt.xlabel("Iterations")<br/>plt.ylabel("Average Reward")<br/>plt.title("Average $\epsilon-greedy$ Rewards after " + str(episodes) <br/>    + " Episodes")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/3d46d020f6c7f1624d730db906f3e88b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*446lsgnmAFdGHjwKHU5l1g.png"/></div></div></figure><p id="3154" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从结果来看，贪婪函数在其他两个函数中表现一致，ϵ=0.01 在这两个函数之间，ϵ=0.1 在这三个函数中表现最好。下面，我们可以看到使用<code class="fe mw mx my mz b">sequence</code>参数的效果更加清晰，并且可以感受到每集采取最佳行动的频率，因为每集的平均值保持一致。</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="eba6" class="ne lx it mz b gy nf ng l nh ni">k = 10<br/>iters = 1000<br/><br/>eps_0_rewards = np.zeros(iters)<br/>eps_01_rewards = np.zeros(iters)<br/>eps_1_rewards = np.zeros(iters)<br/>eps_0_selection = np.zeros(k)<br/>eps_01_selection = np.zeros(k)<br/>eps_1_selection = np.zeros(k)<br/><br/>episodes = 1000<br/># Run experiments<br/>for i in range(episodes):<br/>    # Initialize bandits<br/>    eps_0 = eps_bandit(k, 0, iters, mu='sequence')<br/>    eps_01 = eps_bandit(k, 0.01, iters, eps_0.mu.copy())<br/>    eps_1 = eps_bandit(k, 0.1, iters, eps_0.mu.copy())<br/>    <br/>    # Run experiments<br/>    eps_0.run()<br/>    eps_01.run()<br/>    eps_1.run()<br/>    <br/>    # Update long-term averages<br/>    eps_0_rewards = eps_0_rewards + (<br/>        eps_0.reward - eps_0_rewards) / (i + 1)<br/>    eps_01_rewards = eps_01_rewards + (<br/>        eps_01.reward - eps_01_rewards) / (i + 1)<br/>    eps_1_rewards = eps_1_rewards + (<br/>        eps_1.reward - eps_1_rewards) / (i + 1)<br/>    <br/>    # Average actions per episode<br/>    eps_0_selection = eps_0_selection + (<br/>        eps_0.k_n - eps_0_selection) / (i + 1)<br/>    eps_01_selection = eps_01_selection + (<br/>        eps_01.k_n - eps_01_selection) / (i + 1)<br/>    eps_1_selection = eps_1_selection + (<br/>        eps_1.k_n - eps_1_selection) / (i + 1)<br/>    <br/>plt.figure(figsize=(12,8))<br/>plt.plot(eps_0_rewards, label="$\epsilon=0$ (greedy)")<br/>plt.plot(eps_01_rewards, label="$\epsilon=0.01$")<br/>plt.plot(eps_1_rewards, label="$\epsilon=0.1$")<br/>for i in range(k):<br/>    plt.hlines(eps_0.mu[i], xmin=0,<br/>              xmax=iters, alpha=0.5,<br/>              linestyle="--")<br/>plt.legend(bbox_to_anchor=(1.3, 0.5))<br/>plt.xlabel("Iterations")<br/>plt.ylabel("Average Reward")<br/>plt.title("Average $\epsilon-greedy$ Rewards after " + <br/>     str(episodes) + " Episodes")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/fb91a2e6a6a5962572b97ac76b50494e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0iTGXd9y9eueRo4o-0FZjw.png"/></div></div></figure><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="2fdd" class="ne lx it mz b gy nf ng l nh ni">bins = np.linspace(0, k-1, k)<br/><br/>plt.figure(figsize=(12,8))<br/>plt.bar(bins, eps_0_selection, <br/>        width = 0.33, color='b', <br/>        label="$\epsilon=0$")<br/>plt.bar(bins+0.33, eps_01_selection,<br/>        width=0.33, color='g', <br/>        label="$\epsilon=0.01$")<br/>plt.bar(bins+0.66, eps_1_selection, <br/>        width=0.33, color='r',<br/>        label="$\epsilon=0.1$")<br/>plt.legend(bbox_to_anchor=(1.2, 0.5))<br/>plt.xlim([0,k])<br/>plt.title("Actions Selected by Each Algorithm")<br/>plt.xlabel("Action")<br/>plt.ylabel("Number of Actions Taken")<br/>plt.show()<br/><br/>opt_per = np.array([eps_0_selection, eps_01_selection,<br/>                   eps_1_selection]) / iters * 100<br/>df = pd.DataFrame(opt_per, index=['$\epsilon=0$', <br/>    '$\epsilon=0.01$', '$\epsilon=0.1$'],<br/>                 columns=["a = " + str(x) for x in range(0, k)])<br/>print("Percentage of actions selected:")<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/c90db461cdd26e4035662d2e4484c57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dDbOWLoW0gBJZe1WeIHpeg.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/57d5902eb83c6b05a19cef13f761dcdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*8Fk8ilCWEIEntae95zSNrA.png"/></div></figure><p id="5ed9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">查看算法的平均选择，我们看到为什么较大的ϵ值表现良好，它采取最佳选择 80%的时间。</p><p id="9e7f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">试验ϵ和 k 的不同值，看看这些结果如何变化。例如，缩小搜索空间可能有利于较小的ϵ值，因为勘探的益处较少，反之亦然。此外，增加迭代次数将开始有利于较低的ϵ值，因为它将具有较少的随机噪声。</p><h1 id="6ece" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">ϵ-Decay 战略</h1><p id="fcfe" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">ϵ-greedy 策略有一个明显的弱点，那就是无论他们看到多少例子，他们都会继续包含随机噪声。对这些人来说，最好确定一个最佳解决方案，并继续利用它。为此，我们可以引入ϵ-decay，它减少了每一步探索的概率。其工作原理是将ϵ定义为步数的函数<em class="lv"> n </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/24dcabfd91f75c53719d6b74439e9036.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*OHcZPVCqG3dqIVFtpv7B-Q.png"/></div></figure><p id="2cb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中β &lt;1 is introduced as a scaling factor to reduce the scaling rate so that the algorithm has sufficient opportunity to explore. In this case, we also include +1 in the denominator to prevent infinities from appearing. Given this, we can make a few small changes to our previous class of bandits to define an  【T0】  class that works on the same principles.</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="6f8a" class="ne lx it mz b gy nf ng l nh ni">class eps_decay_bandit:<br/>    '''<br/>    epsilon-decay k-bandit problem<br/>    <br/>    Inputs<br/>    =====================================================<br/>    k: number of arms (int)<br/>    iters: number of steps (int)<br/>    mu: set the average rewards for each of the k-arms.<br/>        Set to "random" for the rewards to be selected from<br/>        a normal distribution with mean = 0. <br/>        Set to "sequence" for the means to be ordered from <br/>        0 to k-1.<br/>        Pass a list or array of length = k for user-defined<br/>        values.<br/>    '''<br/>    <br/>    def __init__(self, k, iters, mu='random'):<br/>        # Number of arms<br/>        self.k = k<br/>        # Number of iterations<br/>        self.iters = iters<br/>        # Step count<br/>        self.n = 0<br/>        # Step count for each arm<br/>        self.k_n = np.zeros(k)<br/>        # Total mean reward<br/>        self.mean_reward = 0<br/>        self.reward = np.zeros(iters)<br/>        # Mean reward for each arm<br/>        self.k_reward = np.zeros(k)<br/>        <br/>        if type(mu) == list or type(mu).__module__ == np.__name__:<br/>            # User-defined averages            <br/>            self.mu = np.array(mu)<br/>        elif mu == 'random':<br/>            # Draw means from probability distribution<br/>            self.mu = np.random.normal(0, 1, k)<br/>        elif mu == 'sequence':<br/>            # Increase the mean for each arm by one<br/>            self.mu = np.linspace(0, k-1, k)<br/>        <br/>    def pull(self):<br/>        # Generate random number<br/>        p = np.random.rand()<br/>        if p &lt; 1 / (1 + self.n / self.k):<br/>            # Randomly select an action<br/>            a = np.random.choice(self.k)<br/>        else:<br/>            # Take greedy action<br/>            a = np.argmax(self.k_reward)<br/>            <br/>        reward = np.random.normal(self.mu[a], 1)<br/>        <br/>        # Update counts<br/>        self.n += 1<br/>        self.k_n[a] += 1<br/>        <br/>        # Update total<br/>        self.mean_reward = self.mean_reward + (<br/>            reward - self.mean_reward) / self.n<br/>        <br/>        # Update results for a_k<br/>        self.k_reward[a] = self.k_reward[a] + (<br/>            reward - self.k_reward[a]) / self.k_n[a]<br/>        <br/>    def run(self):<br/>        for i in range(self.iters):<br/>            self.pull()<br/>            self.reward[i] = self.mean_reward<br/>            <br/>    def reset(self):<br/>        # Resets results while keeping settings<br/>        self.n = 0<br/>        self.k_n = np.zeros(k)<br/>        self.mean_reward = 0<br/>        self.reward = np.zeros(iters)<br/>        self.k_reward = np.zeros(k)</span></pre><p id="9f8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Now running the code:</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="5826" class="ne lx it mz b gy nf ng l nh ni">k = 10<br/>iters = 1000</span><span id="3801" class="ne lx it mz b gy no ng l nh ni">eps_decay_rewards = np.zeros(iters)<br/>eps_1_rewards = np.zeros(iters)</span><span id="899b" class="ne lx it mz b gy no ng l nh ni">episodes = 1000<br/># Run experiments<br/>for i in range(episodes):<br/>    # Initialize bandits<br/>    eps_decay = eps_decay_bandit(k, iters)<br/>    eps_1 = eps_bandit(k, 0.1, iters, eps_decay.mu.copy())<br/>    <br/>    # Run experiments<br/>    eps_decay.run()<br/>    eps_1.run()<br/>    <br/>    # Update long-term averages<br/>    eps_decay_rewards = eps_decay_rewards + (<br/>        eps_decay.reward - eps_decay_rewards) / (i + 1)<br/>    eps_1_rewards = eps_1_rewards + (<br/>        eps_1.reward - eps_1_rewards) / (i + 1)<br/>    <br/>plt.figure(figsize=(12,8))<br/>plt.plot(eps_decay_rewards, label="$\epsilon-decay$")<br/>plt.plot(eps_1_rewards, label="$\epsilon=0.1$")<br/>plt.legend(bbox_to_anchor=(1.2, 0.5))<br/>plt.xlabel("Iterations")<br/>plt.ylabel("Average Reward")<br/>plt.title("Average $\epsilon-decay$ and" + <br/>    "$\epsilon-greedy$ Rewards after " <br/>    + str(episodes) + " Episodes")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/5eeb8c6a10e2733d7f97bbc1cdd25666.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3N-zPhsRkHBq1X8O_eoptw.png"/></div></div></figure><p id="c0ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">The ϵ-decay strategy outperforms our previous best algorithm as it sticks to the optimal action once it is found.</p><p id="d361" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">There’s one last method to balance the explore-exploit dilemma in <em class="lv"> k-bandit </em>问题，<strong class="lb iu">乐观初始值</strong>。</p><h1 id="7780" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">乐观初始值</h1><p id="85dc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">这种方法与我们之前探索的例子有很大不同，因为它没有引入随机噪声来寻找最佳动作，<em class="lv"> A*_n </em>。相反，我们高估了所有行为的回报，并不断选择最大值。在这种情况下，算法会在早期探索，寻求最大化其回报，同时附加信息允许值收敛到其真实均值。这种方法确实需要一些额外的背景知识，因为我们至少需要一些回报的概念，这样我们才能高估它们。</p><p id="d814" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个实现，我们不需要新的类。相反，我们可以简单地使用我们的<code class="fe mw mx my mz b">eps_bandit</code>类，设置ϵ=0，并为估计值提供高的初始值。此外，我喜欢将每个臂的拉计数初始化为 1，而不是 0，以鼓励稍微慢一些的收敛，并确保良好的探索。</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="24a0" class="ne lx it mz b gy nf ng l nh ni">k = 10<br/>iters = 1000</span><span id="63b6" class="ne lx it mz b gy no ng l nh ni">oiv_rewards = np.zeros(iters)<br/>eps_decay_rewards = np.zeros(iters)<br/>eps_1_rewards = np.zeros(iters)</span><span id="7ed0" class="ne lx it mz b gy no ng l nh ni"># Select initial values<br/>oiv_init = np.repeat(5., k)</span><span id="119f" class="ne lx it mz b gy no ng l nh ni">episodes = 1000<br/># Run experiments<br/>for i in range(episodes):<br/>    # Initialize bandits<br/>    oiv_bandit = eps_bandit(k, 0, iters)<br/>    oiv_bandit.k_reward = oiv_init.copy()<br/>    oiv_bandit.k_n = np.ones(k)<br/>    eps_decay = eps_decay_bandit(k, iters, oiv_bandit.mu.copy())<br/>    eps_1 = eps_bandit(k, 0.1, iters, oiv_bandit.mu.copy())<br/>    <br/>    # Run experiments<br/>    oiv_bandit.run()<br/>    eps_decay.run()<br/>    eps_1.run()<br/>    <br/>    # Update long-term averages<br/>    oiv_rewards = oiv_rewards + (<br/>        oiv_bandit.reward - oiv_rewards) / (i + 1)<br/>    eps_decay_rewards = eps_decay_rewards + (<br/>        eps_decay.reward - eps_decay_rewards) / (i + 1)<br/>    eps_1_rewards = eps_1_rewards + (<br/>        eps_1.reward - eps_1_rewards) / (i + 1)<br/>    <br/>plt.figure(figsize=(12,8))<br/>plt.plot(oiv_rewards, label="Optimistic")<br/>plt.plot(eps_decay_rewards, label="$\epsilon-decay$")<br/>plt.plot(eps_1_rewards, label="$\epsilon=0.1$")<br/>plt.legend(bbox_to_anchor=(1.2, 0.5))<br/>plt.xlabel("Iterations")<br/>plt.ylabel("Average Reward")<br/>plt.title("Average Bandit Strategy Rewards after " + <br/>    str(episodes) + " Episodes")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/40fdf267273db992cf0c7b18748df010.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OajF_mI_jp_HYk7Rr6tswA.png"/></div></div></figure><p id="a67f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，在这种情况下，乐观初始值方法优于我们的ϵ−greedy 和ϵ−decay 算法。我们也可以看到，在最后一集里，算法对每个手臂的估计。</p><pre class="kj kk kl km gt na mz nb nc aw nd bi"><span id="77f4" class="ne lx it mz b gy nf ng l nh ni">df = pd.DataFrame({"number of selections": oiv_bandit.k_n - 1,<br/>                  "actual reward": oiv_bandit.mu,<br/>                  "estimated reward": oiv_bandit.k_reward})<br/>df = df.applymap(lambda x: np.round(x, 2))<br/>df['number of selections'] = df['number of selections'].astype('int')<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/673510dc6745f148f9008185cc56f53b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*vAWTWuEz3pCsYVziEuG06w.png"/></div></figure><p id="5bba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了 977 次拉动之外，所有情况下的估计都与实际回报相差甚远。这突出了我们将在更普遍的强化学习中做的许多事情。我们不一定关心获得我们正在互动的环境的准确描述。相反，我们打算<a class="ae ky" href="https://www.cs.cmu.edu/afs/cs/project/jair/pub/volume4/kaelbling96a-html/node23.html" rel="noopener ugc nofollow" target="_blank">学习在这些情况下的最佳行为</a>，并寻求相应的行为。这将开启一场关于无模型学习和基于模型学习的讨论，我们将不得不推迟到下一次。</p><p id="1b01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有<a class="ae ky" href="https://www.datahubbs.com/multi-armed-bandits-reinforcement-learning-2/" rel="noopener ugc nofollow" target="_blank">其他土匪方法</a>我们将探讨，如梯度土匪，上限置信(UCB)方法，和非平稳问题。此外，还有许多其他类似<a class="ae ky" href="http://www.sciencedirect.com/science/article/pii/S0022000012000281?via%3Dihub" rel="noopener ugc nofollow" target="_blank">决斗土匪</a>、<a class="ae ky" href="https://arxiv.org/abs/1401.8257" rel="noopener ugc nofollow" target="_blank">集群土匪</a>、<a class="ae ky" href="https://arxiv.org/abs/1502.03473" rel="noopener ugc nofollow" target="_blank">协同过滤土匪</a>、<a class="ae ky" href="http://www.biorxiv.org/content/biorxiv/early/2017/04/28/106286.full.pdf" rel="noopener ugc nofollow" target="_blank">空间相关土匪</a>、<a class="ae ky" href="https://arxiv.org/abs/1604.07706" rel="noopener ugc nofollow" target="_blank">分布式土匪</a>、<a class="ae ky" href="http://mercurio.srv.dsi.unimi.it/~cesabian/Pubblicazioni/J18.pdf" rel="noopener ugc nofollow" target="_blank">对抗性土匪</a>、<a class="ae ky" href="http://proceedings.mlr.press/v32/agarwalb14.pdf" rel="noopener ugc nofollow" target="_blank">上下文土匪</a>的土匪，这些都是值得探索的。开始怀疑我们是否没有遭到<a class="ae ky" href="https://mlwave.com/multi-armed-bandits-algorithms-made-easy/" rel="noopener ugc nofollow" target="_blank">智力破坏……</a></p></div></div>    
</body>
</html>