<html>
<head>
<title>Ridge Regression Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">岭回归 Python 示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-regression-python-example-f015345d936b?source=collection_archive---------4-----------------------#2019-08-19">https://towardsdatascience.com/ridge-regression-python-example-f015345d936b?source=collection_archive---------4-----------------------#2019-08-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ce54071948412e94961dc08abe4631e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f0zdeA7OSKGJgUI1"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Photo by <a class="ae jg" href="https://unsplash.com/@jeswinthomas?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jeswin Thomas</a> on <a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><div class=""/><p id="bf65" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">过拟合，即模型对训练样本表现良好但未能推广的过程，是机器学习的主要挑战之一。在前面的文章中，我们将介绍如何使用正则化来帮助防止过度拟合。具体来说，我们将讨论岭回归，线性回归的远亲，以及如何用它来确定最佳拟合线。</p><p id="c032" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们开始描述岭回归之前，理解机器学习环境中的<strong class="ki jk">方差</strong>和<strong class="ki jk">偏差</strong>是很重要的。</p><h1 id="7f56" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">偏见</h1><p id="0e4b" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">术语<em class="mh">偏差</em>不是 y 轴截距，而是模型无法得出近似于样本的图的程度。例如，前进线有很高的偏差，因为它不能捕捉数据中潜在的趋势。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/47a0abde3fc9b8c6dec429fee6c18e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*8y5ytC9JGaEbVCg6HUEXPw.png"/></div></figure><p id="498f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，前进线具有相对较低的偏差。如果我们要测量均方误差，它将比前面的例子低得多。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/4302b2526d7cf9c86839b02df4c11129.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*L2YsroxCKMvBwBNN0KldGA.png"/></div></figure><h1 id="ef13" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">差异</h1><p id="e28a" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">与统计定义相反，<em class="mh">方差</em>不是指数据相对于平均值的分布。相反，它描述了数据集之间拟合的差异。换句话说，它测量模型的准确性在呈现不同的数据集时如何变化。例如，上图中的曲线在其他数据集上的表现完全不同。因此，我们说它有很高的方差。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mo"><img src="../Images/4d9edd83993b1699f095442df2813108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*22kNnpHphAc7_ECxrkAHQw.png"/></div></div></figure><p id="8d0c" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，直线具有相对较低的方差，因为不同数据集的均方误差相似。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mp"><img src="../Images/2a126ad14b565e932b1991e789422edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1rzNJjLnAE_hh4pbx-ICg.png"/></div></div></figure><p id="182a" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">岭回归几乎等同于线性回归，只是我们引入了少量的偏差。作为对所述偏差的回报，我们得到方差的显著下降。换句话说，通过以稍微差一点的拟合开始，岭回归对于不完全遵循与模型被训练的数据相同的模式的数据表现得更好。</p><p id="7013" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">添加偏差，通常被称为正则化。顾名思义，正则化用于开发一个模型，该模型擅长预测遵循<strong class="ki jk">正则</strong>模式而非特定模式的数据目标。换个方式说，正则化的目的是防止过拟合。当我们使用比数据建模所需次数更高的多项式时，往往会发生过度拟合。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mq"><img src="../Images/8e471ec460d546bf6f4106df59c6eac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rwrr7mbJ6F8yNw0bqsadPQ.png"/></div></div></figure><p id="dfbe" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个问题，我们在损失函数中引入了一个正则项。在岭回归中，损失函数是线性最小二乘函数，正则化由<a class="ae jg" href="https://en.wikipedia.org/wiki/Matrix_norm" rel="noopener ugc nofollow" target="_blank"> l2 范数</a>给出。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mr"><img src="../Images/1202232d8d438363bd1512724b40c6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CoFy4DZw6arLiurhl-Nhig.png"/></div></div></figure><p id="18d4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为我们试图最小化损失函数，并且<strong class="ki jk"> w </strong>包含在残差平方和中，所以模型将被迫在最小化残差平方和和最小化系数之间找到平衡。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/21deaac91443b7b920eda7864fdfe6af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*6gpCn7hOEF5DzXLKKPL3iQ.png"/></div></figure><p id="df9b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于高次多项式，如果基础数据可以用低次多项式近似，则高阶变量的系数将趋向于 0。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/21ce21b39d10466159333ae18c3d6e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y1-z4noGOokapNTMON_vPw.png"/></div></div></figure><p id="d2be" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们将超参数α设置为某个较大的数，在试图找到成本函数的最小值时，模型会将系数设置为 0。换句话说，回归线的斜率为 0。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mu"><img src="../Images/72a2db531eddd6f74669e5158caea100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7jcxopdIHhE5whTk4REKQ.png"/></div></div></figure><h1 id="26e3" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">算法</h1><p id="a80a" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">给定增加的正则项，找到系数并不困难。我们采用成本函数，执行一点代数运算，对<strong class="ki jk"> w </strong>(系数向量)取偏导数，使其等于 0，然后求解<strong class="ki jk"> w </strong>。</p><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mv"><img src="../Images/4980cc7c0f5fa1105c3aae691661e1e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TzfATjieFsWp9yGTTiqZqA.png"/></div></div></figure><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/9e986e08ee8a59273c63319447426e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*lUPZa9qN8kJkVY9o74q7OQ.png"/></div></figure><h1 id="bae3" class="le lf jj bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">Python 代码</h1><p id="d585" class="pw-post-body-paragraph kg kh jj ki b kj mc kl km kn md kp kq kr me kt ku kv mf kx ky kz mg lb lc ld im bi translated">让我们看看如何使用 Python 从头开始实现岭回归。首先，我们导入以下库。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="1799" class="nc lf jj my b gy nd ne l nf ng">from sklearn.datasets import make_regression<br/>from matplotlib import pyplot as plt<br/>import numpy as np<br/>from sklearn.linear_model import Ridge</span></pre><p id="41aa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用<code class="fe nh ni nj my b">scikit-learn</code>库来生成非常适合回归的样本数据。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="0598" class="nc lf jj my b gy nd ne l nf ng">X, y, coefficients = make_regression(<br/>    n_samples=50,<br/>    n_features=1,<br/>    n_informative=1,<br/>    n_targets=1,<br/>    noise=5,<br/>    coef=True,<br/>    random_state=1<br/>)</span></pre><p id="45d4" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们定义超参数<strong class="ki jk">α。</strong>α决定正则化强度。α值越大，正则化越强。换句话说，当α是一个非常大的数时，模型的偏差会很大。alpha 值为 1 时，模型的行为与线性回归相同。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="50ec" class="nc lf jj my b gy nd ne l nf ng">alpha = 1</span></pre><p id="72a7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们创建单位矩阵。为了让我们之前看到的等式遵守矩阵运算的规则，单位矩阵必须与矩阵 X 的转置点 X 大小相同。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="ac60" class="nc lf jj my b gy nd ne l nf ng">n, m = X.shape<br/>I = np.identity(m)</span></pre><p id="b7b7" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们用上面讨论的等式来求解 w。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="bee5" class="nc lf jj my b gy nd ne l nf ng">w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X) + alpha * I), X.T), y)</span></pre><p id="ba60" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将 w 与生成数据时使用的实际系数进行比较，我们可以看到它们并不完全相等，但很接近。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="0957" class="nc lf jj my b gy nd ne l nf ng">w</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/13a9fcdcfd2dc487ce7af3ee55abb1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*-ksV-ETXi0JmzKT05xtOWg.png"/></div></figure><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="ebcc" class="nc lf jj my b gy nd ne l nf ng">coefficients</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7d67e96cb74e8a9e8b1b93b526589101.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/1*5ppdVJU111nmfHPvacU4Xg.png"/></div></figure><p id="a4aa" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来看看回归线是如何拟合数据的。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="1f02" class="nc lf jj my b gy nd ne l nf ng">plt.scatter(X, y)<br/>plt.plot(X, w*X, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/ab8899945059d61f33bb6bae08c32b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*Cis_LtZMihCLh65M9g5Ikw.png"/></div></figure><p id="865f" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们使用岭回归的<code class="fe nh ni nj my b">scikit-learn</code>实现做同样的事情。首先，我们创建并训练一个<code class="fe nh ni nj my b">Ridge</code>类的实例。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="fe55" class="nc lf jj my b gy nd ne l nf ng">rr = Ridge(alpha=1)</span><span id="7b36" class="nc lf jj my b gy nn ne l nf ng">rr.fit(X, y)</span><span id="e87a" class="nc lf jj my b gy nn ne l nf ng">w = rr.coef_</span></pre><p id="8a30" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到了与用线性代数解出的相同的值。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="4a1a" class="nc lf jj my b gy nd ne l nf ng">w</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/931f160574bfa0646902fccb471dde4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*5iiAO_dWZuasp9ZXZArljw.png"/></div></figure><p id="e9e3" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回归线与上面的一条相同。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="d19d" class="nc lf jj my b gy nd ne l nf ng">plt.scatter(X, y)<br/>plt.plot(X, w*X, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bc920a7639342324f4a771e440d4c6ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*t1FP7FwEgh73BqIKdaryZg.png"/></div></figure><p id="3a62" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，让我们可视化正则化参数α的效果。首先，我们将其设置为 10。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="5091" class="nc lf jj my b gy nd ne l nf ng">rr = Ridge(alpha=10)</span><span id="55b7" class="nc lf jj my b gy nn ne l nf ng">rr.fit(X, y)</span><span id="33f5" class="nc lf jj my b gy nn ne l nf ng">w = rr.coef_[0]</span><span id="2f2e" class="nc lf jj my b gy nn ne l nf ng">plt.scatter(X, y)<br/>plt.plot(X, w*X, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/a8e8e3d0cd42101e911dd4628db19673.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*q9op_DxJF4B2H7Ab_iRRWQ.png"/></div></figure><p id="7dfd" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如我们所见，回归线不再是完美的拟合。换句话说，与 alpha 为 1 的模型相比，该模型具有更高的偏差。为了强调，让我们试试 100 的 alpha 值。</p><pre class="mj mk ml mm gt mx my mz na aw nb bi"><span id="2da3" class="nc lf jj my b gy nd ne l nf ng">rr = Ridge(alpha=100)</span><span id="5c74" class="nc lf jj my b gy nn ne l nf ng">rr.fit(X, y)</span><span id="d037" class="nc lf jj my b gy nn ne l nf ng">w = rr.coef_[0]</span><span id="006d" class="nc lf jj my b gy nn ne l nf ng">plt.scatter(X, y)<br/>plt.plot(X, w*X, c='red')</span></pre><figure class="mj mk ml mm gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/c75005fe2cf78d9d028d330534fe6af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*LXUNn2uIlIS4YKxxs_goGA.png"/></div></figure><p id="b76b" class="pw-post-body-paragraph kg kh jj ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当 alpha 趋向于正无穷大时，回归线将趋向于平均值 0，因为这将最小化不同数据集之间的方差。</p></div></div>    
</body>
</html>