# CNN 与用于图像处理的全连接网络

> 原文：<https://towardsdatascience.com/cnn-vs-fully-connected-network-for-image-processing-8c5b95d4e42f?source=collection_archive---------12----------------------->

# 介绍

本文的目的是提供一个理论视角来理解为什么(单层)细胞神经网络在图像处理方面比全连接网络更好。将使用线性代数(矩阵乘法、特征值和/或 PCA)和 sigmoid/tanh 函数的性质，尝试在全连接网络(逻辑回归)和 CNN 之间进行一对一(*几乎*)比较。最后，为了预测的目的，将检查滤波器大小和滤波图像中保留的信息量之间的权衡。为简单起见，我们假设如下:

1.  全连接网络没有隐藏层(逻辑回归)
2.  原始图像被标准化为像素值在 0 和 1 之间，或者被缩放为均值= 0，方差= 1
3.  Sigmoid/tanh 激活用于输入和卷积图像之间，尽管该参数适用于其他非线性激活函数，如 ReLU。避免使用 ReLU，因为如果图像经过缩放(平均值= 0，方差= 1)而不是归一化，它会破坏分析的严谨性
4.  通道数=图像深度= 1 对于本文的大部分内容，将简要讨论通道数较高的模型
5.  这个问题涉及到分类任务。因此，C > 1
6.  除了激活之外没有非线性，也没有不可微性(如池化、除 1 之外的步幅、填充等。)
7.  负对数似然损失函数用于训练两个网络

## 符号和记号

使用的符号有:

1.  x:二维输入图像的矩阵
2.  用于将
    原始图像映射到全连接网络中的输出的 W₁、b₁:权重矩阵和偏差项
    过滤后的图像映射到 CNN 中的输出
3.  p:输出概率
4.  X₁:滤波图像
5.  x₁:滤波激活图像

关于符号需要注意的两个约定是:

1.  尺寸写在{}之间
2.  不同的维度由 x 分隔。例如:{n x C}表示二维“数组”

# 模型定义

## 全连接网络

![](img/4b0fb201c7c0ba43b9f988df0c183091.png)

FC1: Pre-ouptut layer

![](img/692983e10bce8cd5cbea4e9f7013805e.png)

FC2: Estimated probability

## 卷积神经网络

![](img/1e15b17c74230425a2bdafceab67304f.png)

C1: Filtered image

![](img/2aa6038d8101dad044aee997a107fdab.png)

C2: Filtered-activated image

![](img/2f01df26235ed4f51fdf0448b7b638a7.png)

Activation functions

![](img/f3943124d0502ef0df36d6b7a879a98c.png)

C3: Pre-output layer

![](img/692983e10bce8cd5cbea4e9f7013805e.png)

C4: Estimated probability

# 数学

## 将 CNN 简化为一个全连接网络

假设滤波器是平方的，kₓ = 1，K(a，b) = 1。因此，X₁ = x。现在将使用归一化 x 的优点和 sigmoid/tanh 的便利性质。讨论如下:

## sigmoid/tanh 的必需属性

![](img/2fc763f167c081dfe85b610dea5b23b3.png)

Sigmoid activation as a function of input. Courtesy: ResearchGate article [1]

我们观察到，当输入量很小时，函数是线性的。由于输入图像被归一化或缩放，所有值 x 将位于 0 附近的小区域中，使得|x|

![](img/d219c2250339bc91ec64aa9c28b529a7.png)

这表明过滤激活图像中的信息量非常接近原始图像中的信息量。过滤激活图像的所有像素都连接到输出层(完全连接)。

让我们假设我们学习了输入层完全连接到输出层的全连接网络的最优权重 W₁、b₁。我们可以直接获得给定 CNN 的权重，如 W₁(CNN) = W₁/k 重排为矩阵，b₁(CNN) = b₁.因此，对于 kₓ = 1 且 K(1，1) = 1 的方形滤波器，全连接网络和 CNN 将表现(*几乎*)相同。

由于双曲正切函数是一个重新标度的 sigmoid 函数，因此可以认为同样的性质也适用于双曲正切函数。这也可以在下图中观察到:

![](img/561458c4507b631bac133ff3b64fb35c.png)

tanh activation as a function of input. Courtesy: Wolfram MathWorld [2]

## 滤波器——最差情况

让我们考虑一个正方形图像上的正方形滤波器，其中 kₓ = nₓ，对于所有 a，b，K(a，b) = 1。首先，该滤波器将每个图像映射到一个值(滤波后的图像)，然后该值映射到 c 个输出。因此，过滤后的图像包含的信息(信息瓶颈)比输出图层少-任何少于 C 个像素的过滤后图像都将成为瓶颈。第二，该过滤器将每个图像映射到等于图像值总和的单个像素中。这显然包含非常少的关于原始图像的信息。让我们考虑 MNIST 的例子来理解为什么:考虑具有真实标签‘2’和‘5’的图像。这些图像的值的总和不会相差太多，然而网络应该使用该信息学习一个*清晰的*边界。

## 放松最坏情况第 1 部分:滤波器权重

让我们考虑正方形图像上的正方形滤波器，其中 kₓ = nₓ，但是 k 中不是所有的值都相等。这允许 k 中的变化，使得重要性给予某些像素或区域(将所有其他权重设置为常数，并且仅改变这些权重)。通过改变 K，我们可以发现图像中有助于分类的区域。例如，在 MNIST，假设所有的数字都按照一个通用的模板居中并且写得很好，即使只有一个值被映射到 C 输出，这也可以在类之间创建合理的分离。考虑这种情况类似于判别分析，其中单个值(判别函数)可以分离两个或多个类。

## 放松最坏情况第 2 部分:滤波器宽度

让我们考虑正方形图像上的正方形滤波器，对于所有的 a，b，K(a，b) = 1，但是 kₓ ≠ nₓ.例如，让我们考虑 kₓ = nₓ-1.原始和过滤后的图像如下所示:

![](img/c1d3fcfaac0ae32f57aeaf657cca6d47.png)

Original image

![](img/4903ea8082464fcfbf3a62776ffe2b70.png)

Filtered image

请注意，过滤后的图像总和仅在第一行、第一列、最后一行和最后一列中包含一次元素。所有其他元素出现两次。*假设*滤波图像中的值很小，因为原始图像被归一化或缩放，对于小值 k，激活的滤波图像可以近似为滤波图像的 k 倍。在诸如矩阵乘法(具有权重矩阵)的线性运算下，当 k 为非零时，k*x₁的信息量与 x₁的信息量相同(此处为真，因为 sigmoid/tanh 的斜率在原点附近为非零)。因此，过滤激活图像包含(*大约*)与过滤图像相同数量的*信息*(为了便于理解，写得非常松散，因为【费希尔】‘信息’是得分函数的方差，它与 RV 的方差有关。这种说法的更好版本是:“缩放/归一化的输入图像和缩放/归一化的滤波图像将具有大约相同的信息量”)。

假设原始图像具有非冗余像素和非冗余像素排列，通过应用(nₓ-1，nₓ-1)滤波器，图像的列间距从(nₓnₓ)减少到(2，2)。这导致信息丢失，但对于 K(a，b) = 1，它保证比(nₓ，nₓ)滤波器保留更多的信息。随着过滤器宽度的减小，过滤(因此，过滤激活)图像中保留的信息量增加。它在 kₓ = 1 时达到最大值。

在诸如 MNIST 的实际情况下，边缘附近的大多数像素是冗余的。因此，通过在没有数字信息的边缘附近应用大小为~宽度的块的过滤器，几乎可以保留所有信息。

## 把东西放在一起

CNN 的一个特殊属性是在图像的所有区域应用相同的滤波器。这就是所谓的重量共享。模型中的参数总数= (kₓ * kₓ) + (nₓ-kₓ+1)*(nₓ-kₓ+1)*C.

1.  较大的过滤器导致较小的过滤激活图像，这导致通过全连接层传递到输出层的信息量较少。这导致低信噪比、较高的偏置，但由于全连接层中的参数数量减少，因此降低了过拟合。这是一个高偏差、低方差的例子。
2.  较小的过滤器导致较大的过滤激活图像，这导致更大量的信息通过全连接层传递到输出层。这导致高信噪比、较低的偏置，但可能导致过拟合，因为全连接层中的参数数量增加了。这是一个低偏差、高方差的例子。

已知 K(a，b) = 1，kₓ=1 表现(*几乎*)以及全连通网络。通过反向传播(链式法则)和 SGD 调整 kₓ ≠ 1 的 K(a，b ),保证模型在训练集上表现更好。当用一组不同的超参数(kₓ).)训练时，它也趋向于具有比全连接网络更好的偏差-方差特性

# 总结

kₓ = 1 且 K(1，1) = 1 的 CNN 可以匹配全连接网络的性能。对于 kₓ = nₓ和 K(a，b) = 1，滤波激活图像的表示能力最小。因此，通过调整超参数 kₓ，我们可以控制滤波激活图像中保留的信息量。此外，通过将 K 调整为不同于 1 的值，我们可以聚焦于图像的不同部分。通过调整超参数 kₓ和学习参数 k，CNN 保证具有更好的偏差-方差特性，其下限性能等于全连接网络的性能。这可以通过具有多个通道来进一步改善。

扩展上述讨论，可以认为，如果 CNN 具有相同数量的具有相同/相似结构(每层中的神经元数量)的隐藏层，则 CNN 将优于全连接网络。

然而，这种比较就像把苹果和橘子相比较。适当的比较是将全连接神经网络与具有单个卷积+全连接层的 CNN 进行比较。将具有 1 个隐藏层的全连接神经网络与具有单个卷积+全连接层的 CNN 进行比较是更公平的。

实践中的 MNIST 数据集:逻辑回归模型学习每个数字的模板。这实现了很好的准确性，但是并不好，因为模板可能不能很好地概括。具有完全连接的网络的 CNN 学习适当的内核，并且过滤的图像较少基于模板。与 CNN 相比，具有 1 个隐藏层的全连接网络显示出较少的基于模板的迹象。

# 参考

sigmoid:[https://www . research gate . net/figure/Logistic-curve-From-formula-2-and-figure-1-we-can-see-that-obtain-of-regression _ fig 1 _ 301570543](https://www.researchgate.net/figure/Logistic-curve-From-formula-2-and-figure-1-we-can-see-that-regardless-of-regression_fig1_301570543)

谭:【http://mathworld.wolfram.com/HyperbolicTangent.html】T4