# 人工智能公平性——不同影响消除器的解释

> 原文：<https://towardsdatascience.com/ai-fairness-explanation-of-disparate-impact-remover-ce0da59451f1?source=collection_archive---------8----------------------->

## 人工智能公平性介绍

AI 公平性是机器学习从业者的重要课题。我们必须意识到，当用户与我们的模型交互时，可能会有正面和负面的影响。尽管我们对成功的度量倾向于性能度量(例如，准确性)，但是那些与我们的模型交互的人也可能考虑其他值。使用人工智能的工具正在被开发出来:批准或拒绝贷款；决定是否应该考虑对某人进行面试；确定某人是否适合接受治疗。这些结果对个人都有很大的影响。这就是为什么公平是一个如此重要的考虑因素。

为了确保公平，我们必须分析并解决训练数据中可能存在的任何偏差。机器学习发现并归纳数据中的模式，因此可以复制偏差。当大规模实现这些模型时，可能会导致大量有偏见的决策，伤害大量用户。

## 引入偏差

数据收集、处理和标注是我们在数据中引入偏差的常见活动。

**数据收集**

*   由于收集数据时使用的技术或人员，会引入偏差，例如，该工具仅在特定语言中可用
*   这可能是采样策略的结果，例如，收集的子群体代表性不足

**加工和贴标**

*   丢弃数据，例如子群体可能更常见地具有缺失值，并且通过丢弃那些样本，导致代表性不足
*   人类的标签，或决策者，可能有利于特权群体或加强刻板印象

## 差别性影响

不同的影响是评价公平性的一个尺度。它比较了两个群体中获得正产出的个体比例:一个非特权群体和一个特权群体。

![](img/f348a620f2eb8c548cd51adb4524c9d2.png)

计算方法是获得积极结果的非特权群体的比例除以获得积极结果的特权群体的比例。

行业标准是五分之四规则:如果非特权组收到的积极结果少于特权组的 80%,这就是不同影响违规。然而，你可以决定增加你的业务。

## 预处理缓解

一些人经常建议的减轻偏见的一种方法是简单地删除应该保护的特征。例如，如果你担心一个模型是性别歧视的，而你的数据集中有性别，那么就从传递给机器学习算法的特征中删除它。不幸的是，**这很少能解决问题**。

> 特权群体经历的机会可能没有呈现给非特权群体；每个组的成员可能无法访问相同的资源，无论是财务资源还是其他资源。这意味着它们的环境不同，因此，它们对于机器学习模型的特征也不同，不一定具有可比性。这是系统性偏见的结果。

让我们以一个玩具为例，一个非特权群体为蓝色，一个特权群体为橙色。由于他们无法控制的环境，蓝色倾向于降低我们感兴趣的特征的价值。

我们可以为两组中的每一组绘制特征分布图，并直观地看到这种差异。

![](img/ad146bfc276bd3e7d0726437f3692188.png)

如果您要随机选取一个数据点，您可以使用它的特征值来预测您从哪个组中选择。

例如，如果您选择一个特征值为 6 的数据点，您很可能会认为相应的个体属于橙色组。相反，对于 5，你会认为他们属于蓝色。

特征不一定是预测预期结果的有用属性。但是，如果训练数据的标注倾向于橙色组，则要素的权重会更高，因为它可用于推断分组。

举个例子，一个人的名字不一定会影响他们的工作能力，因此，也不应该影响他们是否被录用。然而，如果招聘人员无意识地有偏见，他们可能会从名字中推断出候选人的性别或种族，并将此作为他们决策的一部分。

## 不同冲击消除器

不同影响移除器是一种预处理技术，用于编辑将用作特征的值，以增加组之间的公平性。如上图所示，一个特征可以很好地表明一个数据点可能属于哪个组。异类影响移除器旨在移除这种区分组成员的能力。

M. Feldman、S. A. Friedler、J. Moeller、C. Scheidegger 和 S. Venkatasubramanian 在论文[中介绍了该技术。](https://arxiv.org/abs/1412.3756)

该算法要求用户指定一个`repair_level`，这表示您希望组的分布重叠多少。让我们来探讨一下 1.0 和 0.8 两种不同修复级别的影响。

**修复值= 1.0**

![](img/7c87d2324d77e4ca19acd81ae936148a.png)

该图显示了使用修复级别为 1.0 的`DisparateImpactRemover`后，非特权组蓝色和特权组橙色的特征的修复值。

您不再能够选择一个点并推断它属于哪个组。这将确保机器学习模型不会发现群体偏见。

**修复值= 0.8**

![](img/f349e9434dde296e1f05a837cc532942.png)

该图显示了使用修复级别为 0.8 的`DisparateImpactRemover`后，非特权组蓝色和特权组橙色的特征的修复值。

分布并不完全重叠，但是您仍然很难区分成员资格，这使得模型很难做到这一点。

## 组内排名

当特征显示两个群体之间的差异时，我们假设他们有不同的机会和经历。然而，在群体中，我们假设他们的经历是相似的。因此，我们希望一个人在他们的组中的排名在修复后得到保留。不同的冲击消除器保持组内的等级排序；如果一个人有蓝色组的最高分，那么在修复后，他仍然有蓝色组的最高分。

## 构建机器学习模型

一旦实现了不同的影响消除器，就可以使用修复的数据来构建机器学习模型。不同的影响指标将验证模型是否无偏(或在可接受的阈值内)。

偏差缓解可能会导致较低的性能指标(例如准确性)，但这并不一定意味着最终模型会不准确。

> 这对人工智能从业者来说是一个挑战:当你知道你有偏见的数据时，你会意识到你正在建立的模型不一定反映现实，也不一定反映你希望坚持的价值观。

## 示例笔记本

作为我对`DisparateImpactRemover`调查的一部分，我使用玩具数据集创建了一个示例笔记本。它演示了以下内容:

*   计算不同的影响(使用 Python 和 AIF360)
*   构建简单的逻辑回归模型
*   创造一个`BinaryLabelDataset`
*   用两种不同的维修级别执行`DisparateImpactRemover`
*   验证组内排名的保留

这个可以在 GitHub [这里](https://nbviewer.jupyter.org/github/srnghn/bias-mitigation-examples/blob/master/Bias%20Mitigation%20with%20Disparate%20Impact%20Remover.ipynb)找到。我们用来实现这个算法的库是 [AI Fairness 360](http://aif360.mybluemix.net/) 。

## 最后的评论

公平的概念是难以置信的微妙，没有任何算法方法来减轻偏见是完美的。然而，通过考虑我们用户的价值，并实施这些技术，我们正朝着一个更公平的世界的正确方向前进。