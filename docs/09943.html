<html>
<head>
<title>VGGNet vs ResNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VGGNet vs ResNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/vggnet-vs-resnet-924e9573ca5c?source=collection_archive---------7-----------------------#2019-12-29">https://towardsdatascience.com/vggnet-vs-resnet-924e9573ca5c?source=collection_archive---------7-----------------------#2019-12-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e911" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">消失梯度问题的清晰答案！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b91746c49a41a55a9c943161ba10beac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r1PYzTHvVMEP6bgwFBDuaw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@benchaccounting?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Bench Accounting</a> on <a class="ae ky" href="https://unsplash.com/s/photos/office-chair?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e0ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">"你能解释一下 VGGNet 和 ResNet 的区别吗？"是人工智能和机器学习领域的一个流行的面试问题。虽然答案就在网上，但我还没能找到一个简明扼要的答案。我们将从什么是 VGGNet 开始，它遇到了什么问题，以及 ResNet 是如何解决这个问题的。</p><h1 id="18fa" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">VGGNet</h1><p id="5b25" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">VGG 代表<em class="lv">视觉几何小组</em>(一组牛津的研究人员开发了这种架构)。VGG 架构由块组成，其中每个块由 2D 卷积和最大池层组成。VGGNet 有两种风格，VGG16 和 VGG19，其中 16 和 19 分别是它们各自的层数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/71f746e5cd8725eee8cf0c1271fdac62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rlOkcko3TpS3q-1FE24f0A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 1 VGGNet architecture</figcaption></figure><p id="811f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在卷积神经网络(CNN)中，随着层数的增加，模型适应更复杂函数的能力也会增加。因此，层数越多越好(不要与人工神经网络混淆，后者不一定随着隐藏层数的增加而提供明显更好的性能)。所以现在你可以争论为什么不用 VGG20，或者 VGG50 或者 VGG100 等等。</p><p id="4fd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">嗯，有一个问题。</p><p id="0d7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用反向传播算法更新神经网络的权重。反向传播算法以减少模型损失的方式对每个权重进行小的改变。这是怎么发生的？它更新每个权重，以便在损失减少的方向上迈出一步。这个方向不过是这个重量的梯度(相对于损失)。</p><p id="de42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用链式法则，我们可以找到每个重量的梯度。它等于(局部梯度)x(从前方流出的梯度)，如图 2 所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mu"><img src="../Images/30ae1b16d0f15175bba6e125c476a5c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6m9C54ptfTIYl6_wZR3w7w.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 2 Flow of Gradients through a Neuron</figcaption></figure><p id="9ecf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题来了。由于这个梯度不断流向初始层，这个值不断乘以每个局部梯度。因此，梯度变得越来越小，使得对初始层的更新非常小，大大增加了训练时间。</p><p id="a853" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果局部梯度不知何故变成了 1，我们就能解决这个问题。</p><p id="04a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧啊。输入 ResNet。</p><h1 id="de94" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">雷斯内特</h1><p id="430f" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">局部梯度怎么会是 1，也就是说，哪个函数的导数总是 1？身份功能！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/ab549791c98e18074ba96a0b541aae0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SxvRfC0c_cVAEABNXNiLZw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 3 Mathematics behind solving the Vanishing Gradient problem</figcaption></figure><p id="9843" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，当这个梯度反向传播时，它的值不会减少，因为局部梯度是 1。</p><p id="bbfe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">ResNet 架构，如下所示，现在应该可以很好地理解它是如何不允许渐变消失问题发生的。ResNet 代表残余网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/838cec3fe9cd0456074893deaeda8878.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jceFx9w3QJ_wZ9QqEySk9A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Fig. 4 ResNet architecture</figcaption></figure><p id="d176" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些<em class="lv">跳跃连接</em>充当坡度<em class="lv">高速公路</em>，让坡度畅通无阻。现在你可以理解为什么 ResNet 有 ResNet50、ResNet101 和 ResNet152 这样的风格了。</p><p id="02fc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这篇文章对你有所帮助。</p></div><div class="ab cl mx my hx mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="im in io ip iq"><p id="7682" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一篇值得一读的文章:<a class="ae ky" rel="noopener" target="_blank" href="/exploring-image-data-augmentation-with-keras-and-tensorflow-a8162d89b844">用 Keras 增强图像数据！</a></p><h2 id="d881" class="ne lx it bd ly nf ng dn mc nh ni dp mg li nj nk mi lm nl nm mk lq nn no mm np bi translated">参考资料:</h2><p id="c98d" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] <a class="ae ky" href="http://cs231n.github.io/" rel="noopener ugc nofollow" target="_blank">用于视觉识别的 CS231n 卷积神经网络</a>作者 Andrej Karpathy。</p><p id="134d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] K. Simonyan 和 A. Zisserman。用于大规模图像识别的非常深的卷积网络。2015 年在 ICLR。</p><p id="a657" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] K. He，X. Zhang，S. Ren 和 J. Sun，“用于图像识别的深度残差学习”，2016 年 IEEE 计算机视觉和模式识别会议(CVPR)，拉斯维加斯，NV，2016 年，第 770-778 页。</p><p id="03dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] <a class="ae ky" href="http://www.draw.io" rel="noopener ugc nofollow" target="_blank"> draw.io </a>供图。</p></div></div>    
</body>
</html>