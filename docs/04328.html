<html>
<head>
<title>Attention Model for News Recommendation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新闻推荐的注意力模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/attention-model-for-news-recommendation-cold-start-problem-dba18da8bf54?source=collection_archive---------17-----------------------#2019-07-05">https://towardsdatascience.com/attention-model-for-news-recommendation-cold-start-problem-dba18da8bf54?source=collection_archive---------17-----------------------#2019-07-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9d74" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用注意机制解决冷启动问题</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1fe7d253c4fc68d7ce21dfddc8bd3776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pSD7zBp78N2lwolb"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@mrsamwheeler?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Sam Wheeler</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="e1b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然奇异值分解为推荐系统提供了一个令人满意的解决方案，但是当新项目没有积累足够的数据时，它就不那么有效了。新闻推荐更具挑战性，因为它提出了三个额外的挑战:</p><ol class=""><li id="924a" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">新闻文章对时间高度敏感</li><li id="bc3e" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">用户对话题敏感，有不同的兴趣</li><li id="37f2" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">新闻语言是高度浓缩的，由每天创造的大量新实体组成</li></ol><p id="5b17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将向您展示如何利用注意力机制来解决推荐系统中的冷启动问题。</p><h1 id="b944" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">注意机制</h1><p id="9319" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">注意机制有着悠久的应用历史，最近被引入来解决自然语言处理中的问题。注意机制使模型能够根据上下文对输入施加不同的权重。例如，在神经机器翻译(NMT)中，注意机制可以用来克服双向信息流。借助注意机制，NMT 模型可以通过“看”原文中的不同位置来生成单词。</p><p id="4c10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">新闻推荐系统的情况类似。推荐引擎要学会“看”相关的部分，忽略阅读历史中不相关的部分。</p><h1 id="7c14" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">深度知识感知网络</h1><p id="38ac" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在 HK01 中，我们的数据团队在新闻推荐系统上投入了巨大的努力。我们采用最先进的算法来改进原始的奇异值分解协同过滤算法。具体来说，我们使用带有三重丢失的自动编码器和 StarSpace 来学习文章嵌入。</p><div class="nd ne gp gr nf ng"><a href="https://medium.com/@LouisKitLungLaw/compute-document-similarity-using-autoencoder-with-triplet-loss-eb7eb132eb38" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">使用具有三元组丢失自动编码器计算文档相似性</h2><div class="nn l"><h3 class="bd b gy z fp nl fr fs nm fu fw dk translated">使用具有三重损失的去噪自动编码器生成嵌入，然后计算嵌入的余弦相似性</h3></div><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nq l nr ns nt np nu kp ng"/></div></div></a></div><div class="nd ne gp gr nf ng"><a href="https://medium.com/@LouisKitLungLaw/documents-embeddings-using-facebook-s-starspace-67b8d1feba32" rel="noopener follow" target="_blank"><div class="nh ab fo"><div class="ni ab nj cl cj nk"><h2 class="bd ir gy z fp nl fr fs nm fu fw ip bi translated">使用脸书空间学习文档嵌入</h2><div class="no l"><p class="bd b dl z fp nl fr fs nm fu fw dk translated">medium.com</p></div></div><div class="np l"><div class="nv l nr ns nt np nu kp ng"/></div></div></a></div><p id="566e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于问题 1 和 2，我们利用微软[1]提出的<strong class="ky ir">深度知识感知网络(DKN) </strong>，来解决冷启动问题。我们用自己的文章嵌入代替知识嵌入，并保持注意力网络来学习用户兴趣和文章嵌入之间的相互作用。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/5033d7b6ac13ec5b92f0b663bb54288f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a-7JrprV2XZTBGIGCSVNHg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Architecture of DKN</figcaption></figure><p id="ce9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DKN 由两个网络组成。DKN 的整体建筑展示在右手边。为了预测点击概率，算法<strong class="ky ir">学习聚合阅读历史序列，形成用户嵌入</strong>。用户嵌入被视为阅读历史中文章嵌入的加权和。那么，问题来了:<strong class="ky ir">如何求权重？</strong></p><p id="4901" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每篇文章嵌入的权重由注意力网络获得。注意力网络对候选新闻、我们要预测的新闻和用户点击的新闻之间的交互进行建模。由于用户可能有各种各样的兴趣，并且不存在能够匹配每一个兴趣的单一新闻，注意力网络的作用是将候选新闻匹配到任何一个。</p><p id="ae8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在获得权重后，该模型通过加权和生成用户嵌入，并将用户和候选新闻嵌入传递给前馈神经网络。</p><p id="4bda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DKN 基本上是一个基于项目的算法。它不需要用户-项目交互数据来进行新项目推荐。而且，它可以在不干预的情况下处理多个利益。</p><h1 id="8d3f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结果</h1><p id="689c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们比较原始模型，奇异值分解的正则化版本，与奇异值分解和 DKN 的集合。通过仔细调整，集合可以实现比原始模型 10%的增量。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="9728" class="mg mh iq bd mi mj oe ml mm mn of mp mq jw og jx ms jz oh ka mu kc oi kd mw mx bi translated">参考</h1><p id="1d1f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">[1]:周，，等.“深度兴趣网络点击率预测研究”第 24 届 ACM SIGKDD 知识发现国际会议论文集&amp;数据挖掘。ACM，2018。</p></div></div>    
</body>
</html>