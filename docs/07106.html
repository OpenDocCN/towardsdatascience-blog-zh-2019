<html>
<head>
<title>The End To All Blurry Pictures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有模糊图片的终结</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-end-to-all-blurry-pictures-f27e49f23588?source=collection_archive---------5-----------------------#2019-10-08">https://towardsdatascience.com/the-end-to-all-blurry-pictures-f27e49f23588?source=collection_archive---------5-----------------------#2019-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fa3f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用 GANs 的图像超分辨率综述和解释。</h2></div><p id="d94f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你能数一数这张图片里有多少只企鹅吗？</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/60b2a02b754a51f10607d445826d0342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oV2OaeUxaWAg2DEgU9zXQQ.png"/></div></div></figure><p id="6347" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在呢？</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/dead150837e42f8993f56098a527bbe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XlJOKIkOXozcUG71mog6tQ.png"/></div></div></figure><p id="df6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">嗯，我把它放大了，但是还是很模糊…</p><p id="9c91" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么，现在呢？</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/92f4d97f44c3b1e6ec1ef72d91cd4b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6vXU30ptdu65I3RY.jpg"/></div></div></figure><p id="7885" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">啊！我们来数数企鹅和它的一个完美的例子<strong class="kk iu"> <em class="lq">图像超分辨率</em> </strong>在行动！</p><p id="b341" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图像超分辨率是从低分辨率图片创建更好看、更高分辨率的图像。任何在 Powerpoint 演示文稿中放入图像的人都知道，缩小图像实际上会让它看起来更糟<em class="lq"/>。这是因为<strong class="kk iu">双三次插值</strong>用于放大图像:一种采用<strong class="kk iu">像素平均值来填充图像放大时产生的间隙</strong>的技术。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lr"><img src="../Images/edf29507be0003152c83d280f03eaec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*80iQakHe9dtyOvNnbLTMUw.png"/></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">How interpolation-based resizing/enlargement works. <a class="ae lw" href="https://www.cambridgeincolour.com/tutorials/image-interpolation.htm" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="6b73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这样做的问题是没有新的数据被创建——这意味着你的图像在分辨率上没有变好，只是变大了。</p><p id="ff42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无论你只是创建一个 powerpoint，分析乳房 x 光片，还是进行<strong class="kk iu">远程面部识别</strong> <em class="lq"> — </em> <strong class="kk iu">超</strong>分辨率<strong class="kk iu">超</strong>都很重要。</p><p id="f794" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么我们如何做到<strong class="kk iu">超分辨率</strong>？好吧，我们转向——人工智能！我们已经看到了大量使用<em class="lq">卷积神经网络</em>的图像超分辨率的最新进展(点击查看我关于它们的文章<a class="ae lw" rel="noopener" target="_blank" href="/classifying-skin-lesions-with-convolutional-neural-networks-fc1302c60d54">，如果你想了解它们是如何用于超分辨率的，请点击</a><a class="ae lw" href="https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c" rel="noopener">查看</a>)。但是，这仍然存在一些问题——图像看起来不如我们喜欢的清晰和详细，为了解决这个问题，制作了<strong class="kk iu"> SRGANs </strong>(超分辨率生成对抗网络<em class="lq">——试着说一口气</em>)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi lx"><img src="../Images/eb46ee135449790fd1bea10ef4985263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0djnkeSMfL1pzTYW7pDWMg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Comparison of different SR methods. Zoom in to notice how crisp SRGAN is!</figcaption></figure><p id="0c5c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在进入超分辨率方面之前，这里有一个 GANs 的快速概述</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h2 id="5295" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">生成对抗网络概述</h2><blockquote class="my mz na"><p id="6d73" class="ki kj lq kk b kl km ju kn ko kp jx kq nb ks kt ku nc kw kx ky nd la lb lc ld im bi translated">"给定一个输入数据集，我们能生成看起来应该在该数据集中的新数据吗？"</p></blockquote><p id="8647" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想想一个<strong class="kk iu">伪造者</strong>在博物馆用赝品调换真画，而<strong class="kk iu">馆长</strong>的工作就是辨别真品和赝品。当它们都刚刚开始时，伪造者会犯很多错误，而馆长也不善于辨别真伪。</p><p id="1136" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着时间的推移，伪造者可以尝试不同的技术，并在制造假货方面变得更好，而馆长也找到了帮助他辨别假货的策略；<strong class="kk iu">他们都在通过彼此提高。</strong></p><p id="9095" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">伪造者的目标是<strong class="kk iu">创造出看起来真实的艺术品</strong>，而策展人的目标是<strong class="kk iu">能够始终发现假画</strong>。</p><p id="984c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这两个角色相互竞争，代表了一个 GAN 中的两个网络。作为<strong class="kk iu">生成网络</strong>的伪造者创建新图像，而作为<strong class="kk iu">辨别网络</strong>的策展人评估来自生成器的图像看起来是否真实。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ne"><img src="../Images/eb56f3eab460cc54754aeeb2549c813f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8LHjU2ZBvzRuqjPK.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Outline of a traditional GAN being trained to generate handwritten digits. <a class="ae lw" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwim5dTu2e_kAhVHsZ4KHSsGDJEQjRx6BAgBEAQ&amp;url=https%3A%2F%2Fskymind.ai%2Fwiki%2Fgenerative-adversarial-network-gan&amp;psig=AOvVaw3iuMEvLAung_3xMKVx8tHp&amp;ust=1569628979213810" rel="noopener ugc nofollow" target="_blank">Source</a>.</figcaption></figure><p id="487b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">发生器接收随机像素数据(噪声)，将其转换为<strong class="kk iu">假输出</strong>，在超分辨率的情况下，它会将随机噪声转换为<strong class="kk iu">更高分辨率的图像</strong>。</p><p id="d67e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">鉴别器在训练集和发生器图像上接受训练，<strong class="kk iu">学习区分它们</strong>。</p><p id="722a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么<em class="lq">为什么</em>用 GAN 更好呢？</p><p id="f1d3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有很多其他方法可以实现超分辨率，例如 SRResNet 和 SRCNN，但是，这些方法都有一个问题:视觉质量差，即使网络看起来运行良好。</p><p id="2aad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们使用传统的损失函数来衡量我们的鉴别器有多准确时，它衡量的是<strong class="kk iu">在数学上</strong>有多接近(欧几里德距离)，而不是<strong class="kk iu">在视觉上</strong>生成的图像与真实图像有多接近，这导致了一个区域中颜色的平滑平均，正如您在下面的 SRResNet 图像中所看到的。</p><div class="lf lg lh li gt ab cb"><figure class="nf lj ng nh ni nj nk paragraph-image"><img src="../Images/14003db93efdb571b9369e87bef3e2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*3MWu_rFPgTh75qO98W8Tsg.png"/></figure><figure class="nf lj nl nh ni nj nk paragraph-image"><img src="../Images/0ec7e98dd1ce4c22cc3eb07127e6c7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*P1_8umDaoUt39B-TmbbJOw.png"/><figcaption class="ls lt gj gh gi lu lv bd b be z dk nm di nn no">SRGAN (left), SRResNet (right) — Notice the smoothness and lack of details in the SRResNet image.</figcaption></figure></div><p id="fbe3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了抵消这一点，创造了一个<strong class="kk iu">感知损失</strong>函数——来测量视觉清晰度。这个损失是两个不同损失的总和→ <strong class="kk iu">内容损失</strong>和<strong class="kk iu">对抗损失</strong>。</p><h2 id="ec44" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">对抗性损失</h2><p id="a6ff" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">使用 GAN 的一个巨大好处是，你可以利用对抗性损失来激励输出看起来自然。发生这种情况是因为 GANs 的基本性质:<strong class="kk iu">寻找看起来不属于</strong>的数据。</p><p id="5351" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对抗性损失是鉴别器网络评估<strong class="kk iu">发电机运行情况</strong>的一部分。如果鉴别器认为图像使我的发生器看起来合法，它将返回一个比它认为图像看起来完全是假的更低的损失。</p><h2 id="0a03" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated"><strong class="ak"> <em class="nu">内容损失</em> </strong></h2><p id="c9f9" class="pw-post-body-paragraph ki kj it kk b kl np ju kn ko nq jx kq kr nr kt ku kv ns kx ky kz nt lb lc ld im bi translated">内容损失通过将生成的图像和原始图像通过 CNN 特征图并计算输出的损失来比较图像中的细微细节。</p><p id="2d69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们来分解一下。</p><p id="dc39" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们训练一个卷积神经网络时——它的层执行<strong class="kk iu"> <em class="lq">特征提取</em></strong>——这是一种奇特的说法，它在图像中寻找模式和形状。随着我们越来越深入地研究网络，我们会发现越来越复杂的特征。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nv"><img src="../Images/33f3be5ad76e410fa2e000333c22f4dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kUrDSPW6mmbKcQ5L.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Visualization of feature maps.</figcaption></figure><p id="bbc0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">好的，酷—让我们分析一下这些特征地图中到底发生了什么。在 Conv1 层中，保存了大量来自图像的原始信息。这是因为 CNN 中的初始(conv)层通常充当边缘检测器。</p><p id="c00f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后来在网络中，更高层次的信息被编码，我们在 conv 2-4 中看到，图像开始变得更加抽象。即使看起来更深的层比初始层编码的信息更少(因为它们看起来模糊)，但它们实际上只是改变了它们包含的信息类型——从几何信息到语义信息。</p><p id="e27e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了更好地理解这一点:这里有一些在<a class="ae lw" href="https://neurohive.io/en/popular-networks/vgg16/" rel="noopener ugc nofollow" target="_blank"> VGG16 网络</a>中通过图像的实际过滤器的可视化效果。<em class="lq">(更具体地说，滤镜激活最多的图像，</em> <a class="ae lw" href="https://github.com/fg91/visualizing-cnn-feature-maps" rel="noopener ugc nofollow" target="_blank"> <em class="lq">代码执行此操作</em> </a> <em class="lq"> ) </em></p><h2 id="38ee" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">第 7 层</h2><div class="lf lg lh li gt ab cb"><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/09c9b53d7922d1d5667fc32d9aa93a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*PPqeZsFHEFb1s4sE3P-5MA.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/fbb8d861271642c5d659067c41edfe14.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*VjHZaZ_Wp8zZ5T7tDrpUMA.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/4841865df3ff958c5a6ce6cc43433426.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vsuEc1q2qEzwPtqdX-F61Q.jpeg"/></div></figure></div><h2 id="24e9" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">第 14 层</h2><div class="lf lg lh li gt ab cb"><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/7a35f3ab11d5e68427426a30db3bfadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*DGcrM2YSUpUStYgsnEqC_Q.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/edc063ddec6379cd70f16cebff495f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Q9UVKrvA4NN9LAh2KGxGnw.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/c9469f1194d1533dea6fb51d317c9944.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*0E1kc2xTO8rhb9_O4kY-yA.jpeg"/></div></figure></div><h2 id="65a6" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">第 20 层</h2><div class="lf lg lh li gt ab cb"><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/9b8a3ac54b681469b7fb7a0426b70d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pGdWcGmRAm5yr1DgboPM0w.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/d23def8e44e9f4e184b44e4f902e6c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*tCBxPkCAoUGawidAssR2IA.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/fe4fcc3cfce19a0a6108ab0bbde579d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*vOCzOGKa0YtnJP1tbzGY-A.jpeg"/></div></figure></div><h2 id="cb54" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">第 30 层</h2><div class="lf lg lh li gt ab cb"><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/51b992530b01c213ea12e61e319e82b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*icRrZaNaq3wmET28bfnH1A.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/088bd56be8153cc3ce40b8356c162f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Q2Xtw19jhbspfaawoarc8g.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/d3975de069d716ffe09c4d002a407ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*1nOLN4nbQvquF-QoxkSdIw.jpeg"/></div></figure></div><h2 id="1acf" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">第 40 层</h2><div class="lf lg lh li gt ab cb"><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/d9848ff7f06c156aa7de2fc2d764c13d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*rQCfj9rms0G_AqGf6pC3YA.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/c0ee0b85cb38fdf0277f487b99f76602.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*gJhNbA_zahIt-9sRbkDqwQ.jpeg"/></div></figure><figure class="nf lj nw nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><img src="../Images/7495491a95c0343b2618f7fecbec6609.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*FC_Nbqa5Qw-oX6O7l1zotQ.jpeg"/></div></figure></div><p id="659b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lq">哇哦。</em></p><p id="6058" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些不仅绝对令人着迷，而且当我们深入网络时，它们还能给我们一种直觉，告诉我们过滤器在寻找什么。在最后一行图像中，我们可以很容易地将拱门、鸟和链条识别为每个过滤器正在寻找的对象。</p><p id="1e56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回到<em class="lq">内容损失，</em>我们通过特征图传递从生成器和参考(原始)图像重建的图像，并比较两个图像中的细微纹理差异，<strong class="kk iu">惩罚纹理看起来平滑的图像</strong>。</p><p id="b475" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">知觉损失的概念也用在了<strong class="kk iu">神经风格转移</strong>中，你可以在我关于它的<a class="ae lw" rel="noopener" target="_blank" href="/making-art-with-your-webcam-ac6d0f5504f4">文章</a>中了解更多！</p><p id="e9cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">…就是这样！这是一个<strong class="kk iu">感知损失函数</strong>的基本思想。</p><p id="47e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着这项技术的进步和不断完善，我们可以从这样的酷结果开始:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/88b70f21ee3fc587bcb49c675efd2522.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/0*3XFtTjhUurataWTt.jpg"/></div></figure><p id="cf19" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对此:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/1329cad4dc68154c603b9488df5a4feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*h431s4kqFRDMqcQtjVtMnw.gif"/></div></figure><p id="14b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这项技术的进步可能会导致超远距离航空/卫星成像、医学图像增强、数字全息摄影，以及更多真正具有高影响力的应用，更不用说让企鹅看起来清晰明了。</p><h2 id="8963" class="mf mg it bd mh mi mj dn mk ml mm dp mn kr mo mp mq kv mr ms mt kz mu mv mw mx bi translated">关键要点:</h2><ul class=""><li id="1dd0" class="nz oa it kk b kl np ko nq kr ob kv oc kz od ld oe of og oh bi translated">GANs 有两个神经网络<strong class="kk iu">相互竞争</strong>来完全生成<strong class="kk iu">新</strong> <strong class="kk iu">图像</strong> <strong class="kk iu">看起来</strong> <strong class="kk iu">真实。</strong></li><li id="7a42" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated">SRGAN 图像比 SRCNN 图像看起来更好，因为它们包含了<strong class="kk iu">更多的细节:</strong>使用感知损失函数的副产品。</li><li id="976e" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated">感知损失的计算方法是将内容损失(结构相似性)与对抗性损失(该图像看起来有多合法)相加</li></ul></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="772e" class="on mg it bd mh oo op oq mk or os ot mn jz ou ka mq kc ov kd mt kf ow kg mw ox bi translated"><strong class="ak">如果你喜欢我的文章或者学到了新东西，请务必:</strong></h1><ul class=""><li id="7ad1" class="nz oa it kk b kl np ko nq kr ob kv oc kz od ld oe of og oh bi translated">在<a class="ae lw" href="https://www.linkedin.com/in/aryan-misra/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系。</li><li id="720b" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated">给我发一些反馈和评论(aryanmisra4@gmail.com)。</li><li id="3aa0" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated">检查一下<a class="ae lw" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank"> SRGAN 纸</a>。</li><li id="8818" class="nz oa it kk b kl oi ko oj kr ok kv ol kz om ld oe of og oh bi translated">此外，请查看 SRGAN 的 Tensorflow 2.0 实现的代码，它在#PoweredByTF2.0 devpost 挑战赛中获得了两项大奖！(<a class="ae lw" href="https://github.com/aryanmisra/NeuraScale/" rel="noopener ugc nofollow" target="_blank"> github </a>，<a class="ae lw" href="https://devpost.com/software/neurascale" rel="noopener ugc nofollow" target="_blank"> devpost </a>)</li></ul></div></div>    
</body>
</html>