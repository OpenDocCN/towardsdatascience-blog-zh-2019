<html>
<head>
<title>An “Equation-to-Code” Machine Learning Project Walk-Through — Part 2 Non-Linear Separable Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“等式到代码”机器学习项目演练—第 2 部分非线性可分问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac?source=collection_archive---------15-----------------------#2019-06-13">https://towardsdatascience.com/an-equation-to-code-machine-learning-project-walk-through-in-python-part-2-non-linear-d193c3c23bac?source=collection_archive---------15-----------------------#2019-06-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9372" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数学方程式背后的详细解释，为您的机器学习或深度学习之旅奠定实用的数学基础</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/621c0dfd0f6f25886c72ee0e85ee57bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qkQch4Kcr6hrK-52"/></div></div></figure><p id="727e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">大家好！这是“等式到代码”演练的第 3 部分。这次</p><p id="c957" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在<a class="ae lq" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7?source=your_stories_page---------------------------">第一部分</a>中，我们谈到了如何利用线性回归解决<strong class="kw iu">线性可分问题</strong>。我们学习了向量表示、标准化、添加偏差、sigmoid 函数、对数似然函数和更新参数。</p><p id="6a05" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这次我们要解决一个<strong class="kw iu">非线性可分问题</strong>。如果你没有看过第一部分，这完全没问题。第 2 部分是独立的。但是如果你想更好地理解第 2 部分，最好先读第 1 部分。</p><div class="lr ls gp gr lt lu"><a rel="noopener follow" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd iu gy z fp lz fr fs ma fu fw is bi translated">Python 中的“等式到代码”机器学习项目演练—第 1 部分线性可分…</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">数学方程式背后的详细解释，为你的机器学习或学习建立实用的数学基础</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi ks lu"/></div></div></a></div><p id="6b03" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下面是<a class="ae lq" href="https://gist.github.com/BrambleXu/52b0aaf10987015a078d36c97729dace" rel="noopener ugc nofollow" target="_blank">数据</a>和<a class="ae lq" href="https://gist.github.com/BrambleXu/2640af09b1f43b93c2d951ba91ca3d5c" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><p id="ae6b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">内容结构如下。<code class="fe mj mk ml mm b">*</code>表示如果您已经完成第 1 部分，可以跳过这一步。</p><ol class=""><li id="5549" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">看数据</li><li id="7803" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated"><strong class="kw iu">非线性可分问题</strong></li><li id="000d" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">标准化*</li><li id="73b3" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated"><strong class="kw iu">添加偏差和多项式项</strong></li><li id="39e9" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">Sigmoid 函数*</li><li id="fb0f" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">似然函数*</li><li id="0b0e" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">更新参数θ*</li><li id="7306" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">绘制直线</li><li id="3283" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated"><strong class="kw iu">精度</strong></li><li id="d52a" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">摘要</li></ol><h1 id="6dd1" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">1 看数据</h1><p id="e791" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">下面是数据，<a class="ae lq" href="https://gist.github.com/BrambleXu/a64df128d6c0c26143f82f7b6e889983" rel="noopener ugc nofollow" target="_blank"> non_linear_data.csv </a></p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="5f7b" class="oc nc it mm b gy od oe l of og">x1,x2,y<br/>0.54508775,2.34541183,0<br/>0.32769134,13.43066561,0<br/>4.42748117,14.74150395,0<br/>2.98189041,-1.81818172,1<br/>4.02286274,8.90695686,1<br/>2.26722613,-6.61287392,1<br/>-2.66447221,5.05453871,1<br/>-1.03482441,-1.95643469,1<br/>4.06331548,1.70892541,1<br/>2.89053966,6.07174283,0<br/>2.26929206,10.59789814,0<br/>4.68096051,13.01153161,1<br/>1.27884366,-9.83826738,1<br/>-0.1485496,12.99605136,0<br/>-0.65113893,10.59417745,0<br/>3.69145079,3.25209182,1<br/>-0.63429623,11.6135625,0<br/>0.17589959,5.84139826,0<br/>0.98204409,-9.41271559,1<br/>-0.11094911,6.27900499,0</span></pre><p id="df2b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">首先，我们需要绘制这些数据，看看它是什么样子的。我们创建一个 Python 文件，并将其命名为 non_logistic_regression.py。</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="95a4" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="9364" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("non_linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="0da8" class="oc nc it mm b gy oh oe l of og"># plot data points<br/>plt.plot(train_x[train_y == 1, 0], train_x[train_y == 1, 1], 'o')<br/>plt.plot(train_x[train_y == 0, 0], train_x[train_y == 0, 1], 'x')<br/>plt.show()</span></pre><p id="e63d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">运行上面的脚本后，您应该会看到下图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7ade453276139aebde2a779b35800e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*pGNzrEkJtDTPhCkHDiZecQ.png"/></div></figure><p id="d59e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">似乎我们不能用一条直线来分离 X 和 o。我们把这样的问题称为非线性可分问题，其中数据不是线性可分的。</p><h1 id="906f" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">2 非线性可分问题</h1><p id="b844" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">在第 1 部分中，我们使用<a class="ae lq" href="https://en.wikipedia.org/wiki/Linear_function_(calculus)?oldformat=true#Properties" rel="noopener ugc nofollow" target="_blank">线性函数</a>来解决线性可分问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8a7f6b94f05f89b47964230a43fb407f.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*vw5nuLbIaL11kdYkCRBHqQ.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">linear function</figcaption></figure><p id="2a7d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是对于非线性可分问题，线性函数过于简单，难以处理。所以我们引入了多项式逻辑回归，它在逻辑回归中增加了一个多项式项。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/1fe7abc3aac2d5f70ec337406cae4fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*j2qhn6jx1xoLRa0DaHK-dg.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">general form</figcaption></figure><p id="3881" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们用θ来表示参数。左边的θ标记表示函数 f(x)有参数θ。右边的θ表示有两个参数。最后一项是多项式项，它使模型推广到非线性可分数据。</p><p id="5607" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">注意我们在<a class="ae lq" href="https://gist.github.com/BrambleXu/a64df128d6c0c26143f82f7b6e889983" rel="noopener ugc nofollow" target="_blank"> non_linear_data.csv </a>中有 x1 和 x2 两个特征。我们选择 x1 作为多项式项。所以功能应该变成低于形式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3f19fd852c6c8878549a3d6a524e359b.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*SEYB_TDpXr5UFEc8ubfyVQ.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">a specific form fit to our data</figcaption></figure><p id="a771" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们初始化 4 个参数</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="d931" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="c601" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="oq">delimiter</em>=',', <em class="oq">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="89da" class="oc nc it mm b gy oh oe l of og"># initialize parameter<br/><strong class="mm iu">theta = np.random.randn(4)</strong></span></pre><h1 id="1468" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">3 标准化</h1><p id="c71b" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">为了使训练快速收敛，我们使用<a class="ae lq" href="https://stats.stackexchange.com/a/10298/116970" rel="noopener ugc nofollow" target="_blank">标准化</a>，也叫<strong class="kw iu"> z </strong> - <strong class="kw iu">评分。</strong>我们是按列来做的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5c3f709c7b442c54fde9bf1fc63508c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*kFMKX970oeEgG6cM7pRkLg.png"/></div></figure><ul class=""><li id="596e" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp os mt mu mv bi translated">𝜇在每一栏都很刻薄</li><li id="d12e" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">𝜎是每列的标准偏差</li></ul><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="9a4b" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="5318" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("linear_data.csv", <em class="oq">delimiter</em>=',', <em class="oq">skiprows</em>=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="9351" class="oc nc it mm b gy oh oe l of og"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="be84" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)</strong></span><span id="9d09" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu">def standardizer(x):<br/>    return (x - mu) / sigma</strong></span><span id="d15b" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu">std_x = standardizer(train_x)</strong></span></pre><h1 id="20fb" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">4 添加偏差和多项式项</h1><p id="96ae" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">我们需要添加一个偏差和多项式项来构建数据矩阵。我们添加一个常数 x0=1，以便对齐矢量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/3f19fd852c6c8878549a3d6a524e359b.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*SEYB_TDpXr5UFEc8ubfyVQ.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">a specific form fit to our data</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/1c7d99d806b8250ca87ea521d4b67c99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzwKswA_ZY7yDKxb_khmWQ.png"/></div></div><figcaption class="ok ol gj gh gi om on bd b be z dk">vector representation</figcaption></figure><p id="45e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您可以在第 1 部分找到更多的矢量表示细节:<a class="ae lq" rel="noopener" target="_blank" href="/an-equation-to-code-machine-learning-project-walk-through-in-python-part-1-linear-separable-fd0e19ed2d7"> 3 矢量表示</a>。</p><p id="3c58" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">为了使计算更简单，我们把 x 转换成矩阵。</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="d4a1" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="0057" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="d672" class="oc nc it mm b gy oh oe l of og"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="29c2" class="oc nc it mm b gy oh oe l of og"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)<br/>def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="3341" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># add x0 and x1^2 to get matrix<br/>def to_matrix(x):<br/>    x0 = np.ones([x.shape[0], 1]) <br/>    x3 = x[:, 0, np.newaxis] ** 2<br/>    return np.hstack([x0, x, x3])</strong></span><span id="f766" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu">mat_x = to_matrix(std_x) </strong></span><span id="9abe" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># dot product<br/>def f(x):<br/>    return np.dot(x, theta)</strong></span></pre><p id="3b3f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们用 x3 来表示<code class="fe mj mk ml mm b">x1*x1</code>。</p><p id="a030" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe mj mk ml mm b">std_x</code>的尺寸为<code class="fe mj mk ml mm b">(20, 2)</code>。在<code class="fe mj mk ml mm b">to_matrix(std_x)</code>之后，<code class="fe mj mk ml mm b">mat_x</code>的尺寸为<code class="fe mj mk ml mm b">(20, 4)</code>。至于点积部分，结果的维度是<code class="fe mj mk ml mm b">(4,)</code>。所以点生成的结果应该是<code class="fe mj mk ml mm b">(20, 4) x (4,) -&gt; (20,)</code>，这是一个包含 20 个样本预测的一维数组。</p><h1 id="8e13" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">5 Sigmoid 函数</h1><p id="9977" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">下面是矢量表示</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/10ce40c08977faa5b95da2864ca87a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*IEr3mjvw_3--VCyHE-XWfA.png"/></div></figure><p id="d1a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">然后我们将基于它建立一个更强大的预测函数，sigmoid 函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/6b8e85b53ae5c43c8ad4dff2f1299236.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*zjFf9ZzshxTJlC7edROoAw.png"/></div></figure><p id="f380" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们用 z 来表示线性函数，并将其传递给 sigmoid 函数。sigmoid 函数将给出每个数据样本的概率。我们的数据中有两个类，一个是<code class="fe mj mk ml mm b">1</code>，另一个是<code class="fe mj mk ml mm b">0</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/598ca2a33e63ef98b71a31237199f730.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*4h5rdzqS4bctbo7ixZpHfA.png"/></div></figure><p id="2f91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到模型基于线性函数部分预测样本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/76e6302b06c1921c101316eb5574518b.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*uXvvRDXyzKCP7Oh0uRArMw.png"/></div></figure><p id="c092" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以写下面的代码</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="e14b" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="994e" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="0f3c" class="oc nc it mm b gy oh oe l of og"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="1026" class="oc nc it mm b gy oh oe l of og"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)<br/>def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="bb72" class="oc nc it mm b gy oh oe l of og"># add x0 and x1^2 to get matrix<br/>def to_matrix(x):<br/>    x0 = np.ones([x.shape[0], 1]) <br/>    x3 = x[:, 0, np.newaxis] ** 2<br/>    return np.hstack([x0, x, x3])<br/>mat_x = to_matrix(std_x)</span><span id="9e6b" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># change dot production to sigmoid function<br/>def f(x):<br/>    return 1 / (1 + np.exp(-np.dot(x, theta)))</strong></span></pre><h1 id="87e4" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">6 似然函数</h1><blockquote class="oy oz pa"><p id="9328" class="ku kv oq kw b kx ky ju kz la lb jx lc pb le lf lg pc li lj lk pd lm ln lo lp im bi translated">如果您对方程的解释不感兴趣，或者您已经在第 1 部分中阅读过，那么您可以跳过这一步</p></blockquote><p id="1358" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，我们准备了数据、模型(sigmoid ),还需要什么？是的，一个目标函数。<strong class="kw iu">目标函数可以指导我们如何以正确的方式更新参数。对于 sigmoid(逻辑回归),我们通常使用<a class="ae lq" href="https://www.wikiwand.com/en/Likelihood_function#/Log-likelihood" rel="noopener ugc nofollow" target="_blank">对数似然</a>作为目标函数</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/3de81529a87d49a1798074dc8c709b82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*B4kyEas04xPWZVarH56faA.png"/></div></figure><p id="81e0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">等等，等等…这些东西到底是怎么回事！</p><p id="ae28" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">不要慌。冷静点。</strong></p><p id="c1ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们把它拆开。</p><ul class=""><li id="f32d" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp os mt mu mv bi translated">1-&gt;2(如何从第 1 行到第 2 行):<code class="fe mj mk ml mm b">log(ab) = log a + log b</code></li><li id="b683" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">2-&gt;3: <code class="fe mj mk ml mm b">log(a)^b = b * log a</code></li><li id="40db" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">3-&gt;4:由于我们只有两个类，y=0 和 y=1，所以我们可以使用下面的等式:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/7a8e5fda818e6cb17e20c614240fd8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*6I_D7MH3ArrhI4vkdLE_GA.png"/></div><figcaption class="ok ol gj gh gi om on bd b be z dk">3-&gt;4</figcaption></figure><ul class=""><li id="29b8" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp os mt mu mv bi translated">4-&gt;5:我们使用下面的变换使等式更具可读性</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/d710cf2c40d17501bede7769043057d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*bLJ9l-I2i2znZDigyve00w.png"/></div></figure><p id="5423" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以我们得到了最后一部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/203b7f77765d76082d62d215395437a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*SLJUZ9eJ5tcKzNFdPQTiiA.png"/></div></figure><p id="9e09" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">别忘了我们为什么开始这个。<strong class="kw iu">目标函数可以指导我们如何以正确的方式更新参数。</strong></p><p id="2d80" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们需要用这个来计算损耗，以更新参数。更具体地说，我们需要计算对数似然函数的<strong class="kw iu">导数</strong>。这里我直接给出最后的更新方程式。(如果你对如何得到这个方程感兴趣，这个<a class="ae lq" href="https://www.youtube.com/watch?v=SB2vz57eKgc" rel="noopener ugc nofollow" target="_blank">视频</a>应该会有帮助)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/92da534e72a3ead43346ba4017d5b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4SKBVfcX3OQ2uLqgnTgzTw.png"/></div></figure><p id="e778" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">第六步，最重要的方程就是这个。如果你不明白如何做到这一点，这是完全可以的。我们需要做的就是把它写成真正的代码。</strong></p><h1 id="7d02" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">7 更新参数θ</h1><blockquote class="oy oz pa"><p id="cff5" class="ku kv oq kw b kx ky ju kz la lb jx lc pb le lf lg pc li lj lk pd lm ln lo lp im bi translated">如果您已经阅读了第 1 部分，可以跳过这一步</p></blockquote><p id="ea91" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这一步非常重要。<strong class="kw iu">不要慌</strong>。我们会破解它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/92da534e72a3ead43346ba4017d5b243.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4SKBVfcX3OQ2uLqgnTgzTw.png"/></div></figure><p id="cf39" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">θj 是第 j 个参数。</p><ul class=""><li id="85a4" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp os mt mu mv bi translated">η是学习率，我们设为 0.001 (1e-3)。</li><li id="9822" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">n 是数据样本的数量，在我们的例子中，我们有 20 个。</li><li id="9b50" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">I 是第 I 个数据样本</li></ul><p id="16ab" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我们有三个参数，所以可以写成三个方程。我们用 x3 来代表<code class="fe mj mk ml mm b">x1*x1</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/3ff5eab945f0274f43a9ca12ea2cc1ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*RcwwcaDapXVRJlDdqdW3bQ.png"/></div></figure><p id="e9bc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe mj mk ml mm b">:=</code>符号就像<code class="fe mj mk ml mm b">=</code>。你可以在这里找到解释<a class="ae lq" href="https://math.stackexchange.com/questions/25214/what-does-mean" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="ac73" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最难的部分是σ(求和符号)，所以为了更好地理解，我扩展了σ。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/dff28b0186a5db02adafa21f12189b0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gp23l1Y_A2Iuq0elke-Dnw.png"/></div></div></figure><p id="a0db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">仔细看。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/a84dd60eb4bfff985c4de97f899c8ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7P4diqGPQXHNNfMzkd0UKg.png"/></div></div></figure><p id="62f3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我给等式中的三个部分涂上颜色，因为我们可以用矩阵来表示它们。看第一行红色和蓝色的部分，我们更新了θ0。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/db5e3e16ff2281e3fa518c2470f6e0b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*447tLj7UZp5FrtdzajAQMQ.png"/></div></div></figure><p id="336a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们把红色部分和蓝色部分写成列向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/5eba89b5540ee2ca291c4f4d071858f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*k6oWPBRvYCehNdWiuKUOGA.png"/></div></figure><p id="14d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">因为我们有 20 个数据样本，所以<code class="fe mj mk ml mm b">f</code>的维数是<code class="fe mj mk ml mm b">(20,1)</code>。<code class="fe mj mk ml mm b">x0</code>的尺寸为<code class="fe mj mk ml mm b">(20,1)</code>。我们可以用转置写矩阵乘法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/0cf3d8cd3910b07662571919f704f771.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CeoMiq-8V77id3NmvaTQiQ.png"/></div></div></figure><p id="0e83" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">所以维度应该是<code class="fe mj mk ml mm b">(1, 20) x (20, 1) -&gt; (1,)</code>。我们得到一个标度来更新θ0。</p><p id="8b46" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe mj mk ml mm b">x1</code>和<code class="fe mj mk ml mm b">x2</code>也是列向量。我们可以把它们写成一个<strong class="kw iu"> X </strong>矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/e695e2e56a9bf00c1577f7db717cedb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*4o5B_hL60FuSYASKMSTtfg.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/904ef77c5a1584b4b1981e5547f327b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*pSaLb6se_HSrnIo2s7ixBw.png"/></div></figure><p id="f344" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">θ是一个行向量</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/2d7bc9e4e313e6fbedfa95c3719c02ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*bhHuRyZEylhCtSjpDKM3nw.png"/></div></figure><p id="6dad" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">回到等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/a84dd60eb4bfff985c4de97f899c8ed8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7P4diqGPQXHNNfMzkd0UKg.png"/></div></div></figure><p id="b671" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/5544dd9d089a5276415b6f09de77c8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*2IkwPx9uJtUTfi1APcbZaw.png"/></div></figure><p id="3327" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">把它写成一个等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/cba3402100ab21cdce3de0cf1ab08c27.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*zppuuAQ87FhuDO5hR8hCKA.png"/></div></figure><p id="c4ef" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">类似 Numpy 数组的版本可能容易理解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/8548f80491498880aa47207a696be49e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JXq9DyKsVVusNtH_cd3TvQ.png"/></div></div></figure><p id="069b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们做一点计算，以确保尺寸是正确的。</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="c770" class="oc nc it mm b gy od oe l of og">θ: (1, 4) <br/>f^T: (1, 20) <br/>x: (20, 4)</span><span id="6d4c" class="oc nc it mm b gy oh oe l of og">dot production: (1, 20) x (20, 4) -&gt; (1, 4)</span></pre><p id="8736" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">一切看起来都那么正确。让我们写代码。其实就两行。</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="f101" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="7092" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="b20b" class="oc nc it mm b gy oh oe l of og"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="4ce7" class="oc nc it mm b gy oh oe l of og"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)<br/>def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="5b76" class="oc nc it mm b gy oh oe l of og"># add x0 and x1^2 to get matrix<br/>def to_matrix(x):<br/>    x0 = np.ones([x.shape[0], 1]) <br/>    x3 = x[:, 0, np.newaxis] ** 2<br/>    return np.hstack([x0, x, x3])<br/>mat_x = to_matrix(std_x)</span><span id="9120" class="oc nc it mm b gy oh oe l of og"># sigmoid function<br/>def f(x):<br/>    return 1 / (1 + np.exp(-np.dot(x, theta)))</span><span id="49f7" class="oc nc it mm b gy oh oe l of og"># update times<br/>epoch = 2000</span><span id="79b6" class="oc nc it mm b gy oh oe l of og"># learning rate<br/>ETA = 1e-3</span><span id="2bf5" class="oc nc it mm b gy oh oe l of og"># update parameter<br/><strong class="mm iu">for _ in range(epoch):<br/></strong>    """<br/>    f(mat_x) - train_y: (20,)<br/>    mat_x: (20, 4)<br/>    theta: (4,)<br/>    <br/>    dot production: (20,) x (20, 4) -&gt; (4,)<br/>    """<br/><strong class="mm iu">    theta = theta - ETA * np.dot(f(mat_x) - train_y, mat_x)</strong></span></pre><p id="bc42" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">奇怪的事？还记得我们在代码前写了什么吗？</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="4288" class="oc nc it mm b gy od oe l of og">dot production: (1, 20) x (20, 4) -&gt; (1, 4)</span><span id="a98b" class="oc nc it mm b gy oh oe l of og">The dimension changes make sense here.</span></pre><p id="fdb2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是为什么我们写代码的时候要用<code class="fe mj mk ml mm b">(20,) x (20, 4) -&gt; (4,)</code>？</p><p id="729f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">实际上，这不是真正的数学符号，这是 Numpy 符号。而且如果你用的是 TensorFlow 或者 PyTroch 的话，应该很熟悉。</p><p id="fc92" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><code class="fe mj mk ml mm b">(20,)</code>表示这是一个包含 20 个数字的一维数组。它可以是行向量，也可以是列向量，因为它只有一维。如果我们将其设置为二维数组，像<code class="fe mj mk ml mm b">(20, 1)</code>或<code class="fe mj mk ml mm b">(1, 20)</code>，我们可以很容易地确定<code class="fe mj mk ml mm b">(20, 1)</code>是列向量而<code class="fe mj mk ml mm b">(1, 20)</code>是行向量。</p><p id="5372" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">但是为什么不显式设置维度来消除歧义呢？</strong></p><p id="3602" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好吧。相信我，我第一次看到这个的时候就有接缝问题。但是经过一些编码实践，我想我知道原因了。</p><p id="58ae" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">因为这样可以节省我们的时间！</strong></p><p id="e937" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们以<code class="fe mj mk ml mm b">(20,) x (20, 4) -&gt; (4,)</code>为例。如果我们想得到<code class="fe mj mk ml mm b">(1, 20) x (20, 4) -&gt; (1, 4)</code>，我们需要对<code class="fe mj mk ml mm b">(20,) x (20, 4) -&gt; (4,)</code>做什么？</p><ul class=""><li id="3158" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp os mt mu mv bi translated">将(20，)转换为(1，20)</li><li id="9023" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">计算(1，20) x (20，4) -&gt; (1，4)</li><li id="acc9" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated">因为(1，4)是一个二维列向量，我们需要将其转换为一维数组。(1,4) -&gt; (4,)</li></ul><p id="291c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">老实说，这很令人沮丧。为什么我们不能一步到位？</p><p id="67be" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对，所以我们才能写<code class="fe mj mk ml mm b">(20,) x (20, 4) -&gt; (4,)</code>。</p><p id="d7df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好了，我们来看看<a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" rel="noopener ugc nofollow" target="_blank"> numpy.dot() </a> doc 是怎么说的。</p><blockquote class="oy oz pa"><p id="cc32" class="ku kv oq kw b kx ky ju kz la lb jx lc pb le lf lg pc li lj lk pd lm ln lo lp im bi translated"><a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dot.html" rel="noopener ugc nofollow" target="_blank"> numpy.dot() </a>:如果<em class="it"> a </em>是一个 N 维数组，<em class="it"> b </em>是一个 1 维数组，那么它就是<em class="it"> a </em>和<em class="it"> b </em>最后一个轴上的和积。</p></blockquote><p id="a467" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嗯，事实上我不明白。但是<a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul" rel="noopener ugc nofollow" target="_blank"> np.matmul() </a>描述了与(20，1)或(1，20)的整形类似的计算，以执行标准的 2d 矩阵乘积。也许我们能得到一些灵感。</p><blockquote class="oy oz pa"><p id="ff5f" class="ku kv oq kw b kx ky ju kz la lb jx lc pb le lf lg pc li lj lk pd lm ln lo lp im bi translated"><a class="ae lq" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.matmul.html#numpy.matmul" rel="noopener ugc nofollow" target="_blank"> np.matmul() </a>:如果第一个参数是一维的，那么通过在它的维数前加上 1，它被提升为一个矩阵。在矩阵乘法之后，前置的 1 被移除。</p></blockquote><p id="bfc7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">哈，这就是缺失的部分！所以在我们的例子中，<code class="fe mj mk ml mm b">(20,)</code>变成了<code class="fe mj mk ml mm b">(1, 20)</code>，因为<code class="fe mj mk ml mm b">(20,4)</code>的第一维度是 20。还有<code class="fe mj mk ml mm b">(1, 20) * (20, 4) -&gt; (1, 4)</code>。然后前置 1 被删除，所以我们得到<code class="fe mj mk ml mm b">(4,)</code>。一步到位。</p><h1 id="05ae" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">8 画这条线</h1><p id="7a6e" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">在更新参数 2000 次后，我们应该绘制结果来查看我们的模型的性能。</p><p id="d94b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将一些数据点做为 x1，根据我们所学的参数计算 x2。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/e520e2e547555aba419cf39cfaa30146.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*-6sHwqo2sVVvDRSwUTLgaQ.png"/></div></figure><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="4771" class="oc nc it mm b gy od oe l of og"># plot line<br/>x1 = np.linspace(-2, 2, 100)<br/><strong class="mm iu">x2 = - (theta[0] + x1 * theta[1] + theta[3] * x1**2) / theta[2]</strong></span><span id="de5a" class="oc nc it mm b gy oh oe l of og">plt.plot(std_x[train_y == 1, 0], std_x[train_y == 1, 1], 'o') # train data of class 1<br/>plt.plot(std_x[train_y == 0, 0], std_x[train_y == 0, 1], 'x') # train data of class 0<br/><strong class="mm iu">plt.plot(x1, x2, linestyle='dashed') # plot the line we learned<br/></strong>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/ad808ef85ce8daef4628a584fb6490a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*79l7lRTc3HE59otGsds9sA.png"/></div></figure><h1 id="3ca9" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">9 准确性</h1><p id="8649" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">在第 2 部分中，我们使用准确性来评估我们的模型性能如何。</p><pre class="kj kk kl km gt ny mm nz oa aw ob bi"><span id="0c2c" class="oc nc it mm b gy od oe l of og">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="2ac3" class="oc nc it mm b gy oh oe l of og"># read data<br/>data = np.loadtxt("linear_data.csv", delimiter=',', skiprows=1)<br/>train_x = data[:, 0:2]<br/>train_y = data[:, 2]</span><span id="e506" class="oc nc it mm b gy oh oe l of og"># initialize parameter<br/>theta = np.random.randn(4)</span><span id="8a50" class="oc nc it mm b gy oh oe l of og"># standardization<br/>mu = train_x.mean(axis=0)<br/>sigma = train_x.std(axis=0)<br/>def standardizer(x):<br/>    return (x - mu) / sigma<br/>std_x = standardizer(train_x)</span><span id="c2b9" class="oc nc it mm b gy oh oe l of og"># add x0 and x1^2 to get matrix<br/>def to_matrix(x):<br/>    x0 = np.ones([x.shape[0], 1]) <br/>    x3 = x[:, 0, np.newaxis] ** 2<br/>    return np.hstack([x0, x, x3])<br/>mat_x = to_matrix(std_x)</span><span id="1287" class="oc nc it mm b gy oh oe l of og"># sigmoid function<br/>def f(x):<br/>    return 1 / (1 + np.exp(-np.dot(x, theta)))</span><span id="4724" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># classify sample to 0 or 1<br/>def classify(x): <br/>    return (f(x) &gt;= 0.5).astype(np.int)</strong></span><span id="2eb1" class="oc nc it mm b gy oh oe l of og"># update times<br/>epoch = 2000</span><span id="a8b6" class="oc nc it mm b gy oh oe l of og"># learning rate<br/>ETA = 1e-3</span><span id="10d8" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># accuracy log<br/>accuracies = []</strong></span><span id="35b0" class="oc nc it mm b gy oh oe l of og"># update parameter<br/>for _ in range(epoch):<strong class="mm iu"><br/></strong>    theta = theta - ETA * np.dot(f(mat_x) - train_y, mat_x)<strong class="mm iu"><br/></strong>    <strong class="mm iu">result = classify(mat_x) == train_y <br/>    accuracy = sum(result) / len(result) <br/>    accuracies.append(accuracy)</strong></span><span id="7453" class="oc nc it mm b gy oh oe l of og"><strong class="mm iu"># plot accuracy line<br/>x = np.arange(len(accuracies))<br/>plt.plot(x, accuracies)<br/>plt.show()</strong></span></pre><ul class=""><li id="a7eb" class="mn mo it kw b kx ky la lb ld mp lh mq ll mr lp os mt mu mv bi translated"><code class="fe mj mk ml mm b">classify(x)</code>:如果概率大于 0.5，我们认为是真的</li><li id="ffb1" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated"><code class="fe mj mk ml mm b">result</code>:包含列表形式的预测，[真，假，…]</li><li id="4dab" class="mn mo it kw b kx mw la mx ld my lh mz ll na lp os mt mu mv bi translated"><code class="fe mj mk ml mm b">accuracy = sum(result) / len(result)</code>:计算当前历元中预测的正确样本数。</li></ul><p id="e92a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，我们绘制了精度线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi px"><img src="../Images/30b99968cfabb804ac95fff22487d0b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*hOeQ3N-MoSvMYwWJ24VPyw.png"/></div></figure><p id="4cd9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到这条线在 1000 个周期后变得稳定。</p><h1 id="d646" class="nb nc it bd nd ne nf ng nh ni nj nk nl jz nm ka nn kc no kd np kf nq kg nr ns bi translated">10 摘要</h1><p id="2346" class="pw-post-body-paragraph ku kv it kw b kx nt ju kz la nu jx lc ld nv lf lg lh nw lj lk ll nx ln lo lp im bi translated">如果你已经看过第 1 部分，你会发现第 2 部分很容易理解。你可以在下面找到完整的代码。留下评论让我知道我的文章是否易懂。请继续关注我的下一篇关于随机梯度下降的文章。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="py pz l"/></div></figure><blockquote class="oy oz pa"><p id="1b3f" class="ku kv oq kw b kx ky ju kz la lb jx lc pb le lf lg pc li lj lk pd lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">查看我的其他帖子</em> </strong> <a class="ae lq" href="https://medium.com/@bramblexu" rel="noopener"> <strong class="kw iu"> <em class="it">中</em> </strong> </a> <strong class="kw iu"> <em class="it">同</em> </strong> <a class="ae lq" href="https://bramblexu.com/posts/eb7bd472/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu"> <em class="it">一个分类查看</em> </strong> </a> <strong class="kw iu"> <em class="it">！<br/>GitHub:</em></strong><a class="ae lq" href="https://github.com/BrambleXu" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="it">bramble Xu</em></strong></a><strong class="kw iu"><em class="it"><br/>LinkedIn:</em></strong><a class="ae lq" href="https://www.linkedin.com/in/xu-liang-99356891/" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="it">徐亮</em> </strong> </a> <strong class="kw iu"> <em class="it"> <br/>博客:</em></strong><a class="ae lq" href="https://bramblexu.com" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="it">bramble Xu</em></strong></a></p></blockquote></div></div>    
</body>
</html>