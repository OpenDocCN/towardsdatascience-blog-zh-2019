<html>
<head>
<title>Extreme Rare Event Classification using Autoencoders in Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Keras 中的自动编码器进行极端罕见事件分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098?source=collection_archive---------1-----------------------#2019-05-03">https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098?source=collection_archive---------1-----------------------#2019-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1c3e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这篇文章中，我们将学习如何实现一个自动编码器来构建一个罕见事件分类器。我们将在这里使用来自<a class="ae kf" href="https://arxiv.org/abs/1809.10717" rel="noopener ugc nofollow" target="_blank">的真实世界罕见事件数据集</a> [1]。</h2></div><p id="6213" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">&lt;<download the="" free="" book="" class="ae kf" href="https://www.understandingdeeplearning.com/" rel="noopener ugc nofollow" target="_blank">了解深度学习，了解更多&gt; &gt;</download></p><h1 id="3402" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">背景</h1><h2 id="09f7" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated"><strong class="ak">什么是极端罕见事件？</strong></h2><p id="c7bc" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">在一个罕见的问题中，我们有一个不平衡的数据集。也就是说，我们的阳性标记样本比阴性标记样本少。在典型的罕见事件问题中，正面标记的数据约占总数的 5-10%。在一个极端罕见的事件问题中，我们只有不到 1%的正面标记数据。例如，在这里使用的数据集中，它约为 0.6%。</p><p id="f599" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这种极端罕见的事件问题在现实世界中很常见，例如，制造过程中的纸张断裂和机器故障，在线行业中的点击或购买。</p><p id="b082" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对这些罕见事件进行分类相当具有挑战性。最近，深度学习已经相当广泛地用于分类。然而，<strong class="ki ir">少量的正标签样本阻碍了深度学习应用</strong>。无论数据有多大，深度学习的使用都会受到积极标记样本数量的限制。</p><h2 id="dbcf" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated">为什么我们还要费心去使用深度学习呢？</h2><p id="b5c9" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">这是一个合理的问题。为什么我们不应该考虑使用另一种机器学习方法呢？</p><p id="2458" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">答案是主观的。我们总是可以采用机器学习的方法。为了使其工作，我们可以从负标签数据中进行欠采样，以得到接近平衡的数据集。由于我们有大约 0.6%的正标签数据，欠采样将导致大约是原始数据大小的 1%的数据集。机器学习方法，例如 SVM 或随机森林，仍然可以在这种规模的数据集上工作。然而，它的准确性会有局限性。我们不会利用剩余的大约 99%的数据中的信息。</p><p id="323e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如果数据足够，深度学习方法可能更有能力。通过使用不同的架构，它还允许模型改进的灵活性。因此，我们将尝试使用深度学习方法。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><p id="4ef1" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这篇文章中，<strong class="ki ir">我们将学习如何使用一个简单的密集层自动编码器来构建一个稀有事件分类器</strong>。这篇文章的目的是演示极端罕见事件分类的自动编码器的实现。我们将把探索自动编码器的不同架构和配置的任务留给用户。如果你发现什么有趣的东西，请在评论中分享。</p><h1 id="d466" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">分类自动编码器</h1><p id="cf8d" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">用于分类的自动编码器方法类似于<strong class="ki ir">异常检测</strong>。在异常检测中，我们学习正常过程的模式。任何不遵循这种模式的都被归类为异常。对于罕见事件的二进制分类，我们可以使用类似的使用自动编码器的方法(从<a class="ae kf" href="https://www.datascience.com/blog/fraud-detection-with-tensorflow" rel="noopener ugc nofollow" target="_blank">这里的</a> [2】)得到)。</p><h2 id="037c" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated">快速修订:什么是自动编码器？</h2><ul class=""><li id="3d81" class="ms mt iq ki b kj mg km mh kp mu kt mv kx mw lb mx my mz na bi translated">自动编码器由两个模块组成:编码器和解码器。</li><li id="3bfc" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">编码器了解进程的基本特征。这些特征通常尺寸减小。</li><li id="3f24" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">解码器可以从这些底层特征中重建原始数据。</li></ul><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ng"><img src="../Images/b84f16073553116c5023c2ca2a34f62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_OcBPkRTpKJo0iz5IaNbQ.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure 1. Illustration of an autoencoder. [<a class="ae kf" href="http://i-systems.github.io/HSE545/machine%20learning%20all/Workshop/CAE/06_CAE_Autoencoder.html" rel="noopener ugc nofollow" target="_blank">Source</a>: Autoencoder by Prof. Seungchul Lee<br/>iSystems Design Lab]</figcaption></figure><h2 id="df71" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated">如何使用自动编码器稀有事件分类？</h2><ul class=""><li id="26a5" class="ms mt iq ki b kj mg km mh kp mu kt mv kx mw lb mx my mz na bi translated">我们将数据分为两部分:正标和负标。</li><li id="2166" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">负标记的数据被视为过程的<em class="nw">正常</em>状态。<em class="nw">正常的</em>状态是当过程无事件时。</li><li id="ec95" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">我们将忽略带正标签的数据，只对带负标签的数据训练自动编码器。</li><li id="82d8" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">该自动编码器现在已经学习了<em class="nw">正常</em>过程的特征。</li><li id="4683" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">一个训练有素的自动编码器将预测来自过程的<em class="nw">正常</em>状态的任何新数据(因为它将具有相同的模式或分布)。</li><li id="ecd3" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">因此，重建误差会很小。</li><li id="7274" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">然而，如果我们试图从一个罕见的事件中重建一个数据，自动编码器将会很困难。</li><li id="03ed" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">这将使罕见事件期间的重建误差较高。</li><li id="98e8" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">我们可以捕捉如此高的重建误差，并将其标记为罕见事件预测。</li><li id="f09b" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">该过程类似于异常检测方法。</li></ul><h1 id="e253" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">履行</h1><h2 id="280f" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated"><strong class="ak">数据和问题</strong></h2><p id="6b87" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">这是一个来自纸浆造纸厂的二进制标签数据。纸张断裂是造纸过程中的严重问题。一个单独的纸页断裂会造成几千美元的损失，并且工厂每天至少会看到一个或多个断裂。这导致每年数百万美元的损失和工作危害。</p><p id="3614" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">由于过程的性质，检测中断事件具有挑战性。如[1]中所述，即使断裂减少 5%,也会给工厂带来巨大的利益。</p><p id="e69f" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们拥有的数据包含 15 天内收集的大约 18k 行。列<code class="fe nx ny nz oa b">y</code>包含二进制标签，1 表示分页符。其余列是预测值。大约有 124 个阳性标记样品(~0.6%)。</p><p id="4750" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在这里下载数据<a class="ae kf" href="https://docs.google.com/forms/d/e/1FAIpQLSdyUk3lfDl7I5KYK_pw285LCApc-_RcoC0Tf9cnDnZ_TWzPAw/viewform" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="5873" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated"><strong class="ak">代码</strong></h2><p id="b42b" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">导入所需的库。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="3346" class="lu ld iq oa b gy of og l oh oi">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span><span id="2a8a" class="lu ld iq oa b gy oj og l oh oi">import pandas as pd<br/>import numpy as np<br/>from pylab import rcParams</span><span id="bdc5" class="lu ld iq oa b gy oj og l oh oi">import tensorflow as tf<br/>from keras.models import Model, load_model<br/>from keras.layers import Input, Dense<br/>from keras.callbacks import ModelCheckpoint, TensorBoard<br/>from keras import regularizers</span><span id="af16" class="lu ld iq oa b gy oj og l oh oi">from sklearn.preprocessing import StandardScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import confusion_matrix, precision_recall_curve<br/>from sklearn.metrics import recall_score, classification_report, auc, roc_curve<br/>from sklearn.metrics import precision_recall_fscore_support, f1_score</span><span id="e7da" class="lu ld iq oa b gy oj og l oh oi">from numpy.random import seed<br/>seed(1)<br/>from tensorflow import set_random_seed<br/>set_random_seed(2)</span><span id="035b" class="lu ld iq oa b gy oj og l oh oi">SEED = 123 #used to help randomly select the data points<br/>DATA_SPLIT_PCT = 0.2</span><span id="08b0" class="lu ld iq oa b gy oj og l oh oi">rcParams['figure.figsize'] = 8, 6<br/>LABELS = ["Normal","Break"]</span></pre><p id="4d48" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">请注意，我们设置随机种子是为了结果的可重复性。</p><p id="192c" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">数据预处理</strong></p><p id="6829" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，我们阅读和准备数据。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="9aa4" class="lu ld iq oa b gy of og l oh oi">df = pd.read_csv("data/processminer-rare-event-mts - data.csv")</span></pre><p id="ef7b" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">这个罕见事件问题的目的是在板块断裂发生之前预测它。我们会尽量提前 4 分钟预测课间休息。为了构建这个模型，我们将把标签上移 2 行(对应于 4 分钟)。我们可以这样做<code class="fe nx ny nz oa b">df.y=df.y.shift(-2)</code>。然而，在这个问题中，我们希望进行这样的移位:如果第<em class="nw"> n </em>行被正标记，</p><ul class=""><li id="bea8" class="ms mt iq ki b kj kk km kn kp ok kt ol kx om lb mx my mz na bi translated">使 row ( <em class="nw"> n </em> -2)和(<em class="nw"> n </em> -1)等于 1。这将帮助分类器学习<strong class="ki ir">到</strong> 4 分钟前的预测。</li><li id="b2d9" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated">删除第<em class="nw">行第</em>行。因为我们不希望分类器学习预测已经发生的中断。</li></ul><p id="ef48" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们将为这种曲线移动开发以下 UDF。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="66c1" class="lu ld iq oa b gy of og l oh oi">sign = <strong class="oa ir">lambda</strong> x: (1, -1)[x &lt; 0]<br/><br/><strong class="oa ir">def</strong> curve_shift(df, shift_by):<br/>    <em class="nw">'''</em><br/><em class="nw">    This function will shift the binary labels in a dataframe.</em><br/><em class="nw">    The curve shift will be with respect to the 1s. </em><br/><em class="nw">    For example, if shift is -2, the following process</em><br/><em class="nw">    will happen: if row n is labeled as 1, then</em><br/><em class="nw">    - Make row (n+shift_by):(n+shift_by-1) = 1.</em><br/><em class="nw">    - Remove row n.</em><br/><em class="nw">    i.e. the labels will be shifted up to 2 rows up.</em><br/><em class="nw">    </em><br/><em class="nw">    Inputs:</em><br/><em class="nw">    df       A pandas dataframe with a binary labeled column. </em><br/><em class="nw">             This labeled column should be named as 'y'.</em><br/><em class="nw">    shift_by An integer denoting the number of rows to shift.</em><br/><em class="nw">    </em><br/><em class="nw">    Output</em><br/><em class="nw">    df       A dataframe with the binary labels shifted by shift.</em><br/><em class="nw">    '''</em><br/><br/>    vector = df['y'].copy()<br/>    <strong class="oa ir">for</strong> s <strong class="oa ir">in</strong> range(abs(shift_by)):<br/>        tmp = vector.shift(sign(shift_by))<br/>        tmp = tmp.fillna(0)<br/>        vector += tmp<br/>    labelcol = 'y'<br/>    <em class="nw"># Add vector to the df</em><br/>    df.insert(loc=0, column=labelcol+'tmp', value=vector)<br/>    <em class="nw"># Remove the rows with labelcol == 1.</em><br/>    df = df.drop(df[df[labelcol] == 1].index)<br/>    <em class="nw"># Drop labelcol and rename the tmp col as labelcol</em><br/>    df = df.drop(labelcol, axis=1)<br/>    df = df.rename(columns={labelcol+'tmp': labelcol})<br/>    <em class="nw"># Make the labelcol binary</em><br/>    df.loc[df[labelcol] &gt; 0, labelcol] = 1<br/><br/>    <strong class="oa ir">return</strong> df</span></pre><p id="d806" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在继续之前，为了简单起见，我们将删除时间和分类列。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="8ebb" class="lu ld iq oa b gy of og l oh oi"><em class="nw"># Remove time column, and the categorical columns</em><br/>df = df.drop(['time', 'x28', 'x61'], axis=1)</span></pre><p id="db59" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，我们将数据分为训练集、有效集和测试集。然后，我们将采用只有 0 的数据子集来训练自动编码器。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="7a77" class="lu ld iq oa b gy of og l oh oi">df_train, df_test = train_test_split(df, test_size=DATA_SPLIT_PCT, random_state=SEED)<br/>df_train, df_valid = train_test_split(df_train, test_size=DATA_SPLIT_PCT, random_state=SEED)</span><span id="0b3a" class="lu ld iq oa b gy oj og l oh oi">df_train_0 = df_train.loc[df['y'] == 0]<br/>df_train_1 = df_train.loc[df['y'] == 1]<br/>df_train_0_x = df_train_0.drop(['y'], axis=1)<br/>df_train_1_x = df_train_1.drop(['y'], axis=1)</span><span id="ab5d" class="lu ld iq oa b gy oj og l oh oi">df_valid_0 = df_valid.loc[df['y'] == 0]<br/>df_valid_1 = df_valid.loc[df['y'] == 1]<br/>df_valid_0_x = df_valid_0.drop(['y'], axis=1)<br/>df_valid_1_x = df_valid_1.drop(['y'], axis=1)</span><span id="3306" class="lu ld iq oa b gy oj og l oh oi">df_test_0 = df_test.loc[df['y'] == 0]<br/>df_test_1 = df_test.loc[df['y'] == 1]<br/>df_test_0_x = df_test_0.drop(['y'], axis=1)<br/>df_test_1_x = df_test_1.drop(['y'], axis=1)</span></pre><p id="5699" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">标准化</strong></p><p id="d931" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">对于自动编码器，通常最好使用标准化数据(转换为高斯数据，均值为 0，方差为 1)。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="7b40" class="lu ld iq oa b gy of og l oh oi">scaler = StandardScaler().fit(df_train_0_x)<br/>df_train_0_x_rescaled = scaler.transform(df_train_0_x)<br/>df_valid_0_x_rescaled = scaler.transform(df_valid_0_x)<br/>df_valid_x_rescaled = scaler.transform(df_valid.drop(['y'], axis = 1))</span><span id="1610" class="lu ld iq oa b gy oj og l oh oi">df_test_0_x_rescaled = scaler.transform(df_test_0_x)<br/>df_test_x_rescaled = scaler.transform(df_test.drop(['y'], axis = 1))</span></pre><h2 id="87f9" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated">自动编码器分类器</h2><p id="6356" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated"><strong class="ki ir">初始化</strong></p><p id="0da4" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">首先，我们将初始化自动编码器架构。我们正在构建一个简单的自动编码器。应该探索更复杂的架构和其他配置。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="54b3" class="lu ld iq oa b gy of og l oh oi">nb_epoch = 200<br/>batch_size = 128<br/>input_dim = df_train_0_x_rescaled.shape[1] <em class="nw">#num of predictor variables, </em><br/>encoding_dim = 32<br/>hidden_dim = int(encoding_dim / 2)<br/>learning_rate = 1e-3<br/><br/>input_layer = Input(shape=(input_dim, ))<br/>encoder = Dense(encoding_dim, activation="relu", activity_regularizer=regularizers.l1(learning_rate))(input_layer)<br/>encoder = Dense(hidden_dim, activation="relu")(encoder)<br/>decoder = Dense(hidden_dim, activation="relu")(encoder)<br/>decoder = Dense(encoding_dim, activation="relu")(decoder)<br/>decoder = Dense(input_dim, activation="linear")(decoder)<br/>autoencoder = Model(inputs=input_layer, outputs=decoder)<br/>autoencoder.summary()</span></pre><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi on"><img src="../Images/928738fffedb151df950b74a21daf659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KB2jHzb9zxcNR7YbiGND9A.png"/></div></div></figure><p id="4023" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">培训</strong></p><p id="d462" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们将训练模型并将其保存在文件中。保存训练好的模型是为将来的分析节省时间的好方法。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="d0b7" class="lu ld iq oa b gy of og l oh oi">autoencoder.compile(metrics=['accuracy'],<br/>                    loss='mean_squared_error',<br/>                    optimizer='adam')</span><span id="2655" class="lu ld iq oa b gy oj og l oh oi">cp = ModelCheckpoint(filepath="autoencoder_classifier.h5",<br/>                               save_best_only=True,<br/>                               verbose=0)</span><span id="ee77" class="lu ld iq oa b gy oj og l oh oi">tb = TensorBoard(log_dir='./logs',<br/>                histogram_freq=0,<br/>                write_graph=True,<br/>                write_images=True)</span><span id="8b05" class="lu ld iq oa b gy oj og l oh oi">history = autoencoder.fit(df_train_0_x_rescaled, df_train_0_x_rescaled,<br/>                    epochs=nb_epoch,<br/>                    batch_size=batch_size,<br/>                    shuffle=True,<br/>                    validation_data=(df_valid_0_x_rescaled, df_valid_0_x_rescaled),<br/>                    verbose=1,<br/>                    callbacks=[cp, tb]).history</span></pre><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oo"><img src="../Images/bc767bd57b7ec5b45316f1adbf35004a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Goka-8kvztNMJCFPsoAmWg.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure 2. Loss for Autoencoder Training.</figcaption></figure><p id="ab1a" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir">分类</strong></p><p id="1112" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在下文中，我们展示了如何将自动编码器重构误差用于罕见事件分类。</p><p id="dfe2" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">如前所述，如果重建误差较高，我们会将其归类为断纸。我们需要确定这方面的门槛。</p><p id="7765" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们将使用验证集来确定阈值。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="41e6" class="lu ld iq oa b gy of og l oh oi">valid_x_predictions = autoencoder.predict(df_valid_x_rescaled)<br/>mse = np.mean(np.power(df_valid_x_rescaled - valid_x_predictions, 2), axis=1)<br/>error_df = pd.DataFrame({'Reconstruction_error': mse,<br/>                        'True_class': df_valid['y']})</span><span id="5d8b" class="lu ld iq oa b gy oj og l oh oi">precision_rt, recall_rt, threshold_rt = precision_recall_curve(error_df.True_class, error_df.Reconstruction_error)<br/>plt.plot(threshold_rt, precision_rt[1:], label="Precision",linewidth=5)<br/>plt.plot(threshold_rt, recall_rt[1:], label="Recall",linewidth=5)<br/>plt.title('Precision and recall for different threshold values')<br/>plt.xlabel('Threshold')<br/>plt.ylabel('Precision/Recall')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi op"><img src="../Images/1537d9377d050ed2f51dd2e6185c2195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cms_AGaJzcDXeqwqynfsFQ.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure 3. A threshold of 0.4 should provide a reasonable trade-off between precision and recall.</figcaption></figure><p id="673e" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">现在，我们将对测试数据进行分类。</p><blockquote class="oq or os"><p id="f4bf" class="kg kh nw ki b kj kk jr kl km kn ju ko ot kq kr ks ou ku kv kw ov ky kz la lb ij bi translated"><strong class="ki ir">我们不应该从测试数据中估计分类阈值。这将导致过度拟合。</strong></p></blockquote><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="96af" class="lu ld iq oa b gy of og l oh oi">test_x_predictions = autoencoder.predict(df_test_x_rescaled)<br/>mse = np.mean(np.power(df_test_x_rescaled - test_x_predictions, 2), axis=1)<br/>error_df_test = pd.DataFrame({'Reconstruction_error': mse,<br/>                        'True_class': df_test['y']})<br/>error_df_test = error_df_test.reset_index()</span><span id="5660" class="lu ld iq oa b gy oj og l oh oi">threshold_fixed = 0.4<br/>groups = error_df_test.groupby('True_class')</span><span id="a04e" class="lu ld iq oa b gy oj og l oh oi">fig, ax = plt.subplots()</span><span id="aaf9" class="lu ld iq oa b gy oj og l oh oi">for name, group in groups:<br/>    ax.plot(group.index, group.Reconstruction_error, marker='o', ms=3.5, linestyle='',<br/>            label= "Break" if name == 1 else "Normal")<br/>ax.hlines(threshold_fixed, ax.get_xlim()[0], ax.get_xlim()[1], colors="r", zorder=100, label='Threshold')<br/>ax.legend()<br/>plt.title("Reconstruction error for different classes")<br/>plt.ylabel("Reconstruction error")<br/>plt.xlabel("Data point index")<br/>plt.show();</span></pre><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ow"><img src="../Images/84a9d2fb1b73a3b2fac88ced2262a3ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhvOWxzkLKc5H1Fhx9EI3A.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure 4. Using threshold = 0.4 for classification. The orange and blue dots above the threshold line represents the True Positive and False Positive, respectively.</figcaption></figure><p id="8956" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">在图 4 中，阈值线上方的橙色和蓝色圆点分别代表真阳性和假阳性。如我们所见，我们有很多误报。为了看得更清楚，我们可以看到一个混淆矩阵。</p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="112c" class="lu ld iq oa b gy of og l oh oi">pred_y = [1 if e &gt; threshold_fixed else 0 for e in error_df.Reconstruction_error.values]</span><span id="7345" class="lu ld iq oa b gy oj og l oh oi">conf_matrix = confusion_matrix(error_df.True_class, pred_y)</span><span id="f5d6" class="lu ld iq oa b gy oj og l oh oi">plt.figure(figsize=(12, 12))<br/>sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt="d");<br/>plt.title("Confusion matrix")<br/>plt.ylabel('True class')<br/>plt.xlabel('Predicted class')<br/>plt.show()</span></pre><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi oo"><img src="../Images/a889591118d0df625262cd194c313a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K4l6QMaFOuPgBOtKTWIdeQ.png"/></div></div><figcaption class="ns nt gj gh gi nu nv bd b be z dk">Figure 5. Confusion Matrix on the test predictions.</figcaption></figure><p id="2435" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">我们可以预测 41 个中断实例中的 8 个。请注意，这些情况包括提前 2 或 4 分钟的预测。这在 20%左右，对于造纸行业来说是一个不错的召回率。假阳性率在 6%左右。对于一个磨坊来说，这并不理想，但也不可怕。</p><p id="c167" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">尽管如此，该模型还可以进一步改进，以较小的误报率提高召回率。我们将查看下面的 AUC，然后讨论下一步的改进方法。</p><p id="fc70" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><strong class="ki ir"> ROC 曲线和 AUC </strong></p><pre class="nh ni nj nk gt ob oa oc od aw oe bi"><span id="f77a" class="lu ld iq oa b gy of og l oh oi">false_pos_rate, true_pos_rate, thresholds = roc_curve(error_df.True_class, error_df.Reconstruction_error)<br/>roc_auc = auc(false_pos_rate, true_pos_rate,)</span><span id="cb66" class="lu ld iq oa b gy oj og l oh oi">plt.plot(false_pos_rate, true_pos_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)<br/>plt.plot([0,1],[0,1], linewidth=5)</span><span id="07ab" class="lu ld iq oa b gy oj og l oh oi">plt.xlim([-0.01, 1])<br/>plt.ylim([0, 1.01])<br/>plt.legend(loc='lower right')<br/>plt.title('Receiver operating characteristic curve (ROC)')<br/>plt.ylabel('True Positive Rate')<br/>plt.xlabel('False Positive Rate')<br/>plt.show()</span></pre><figure class="nh ni nj nk gt nl gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi ox"><img src="../Images/d9c65153b0bd66b7e1ce8863c8866c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XVt8bi0_31V4YuRnhvbwDw.png"/></div></div></figure><p id="4187" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">AUC 是 0.69。</p><h2 id="8ce2" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated"><a class="ae kf" href="https://github.com/cran2367/autoencoder_classifier" rel="noopener ugc nofollow" target="_blank"> Github 库</a></h2><p id="79d9" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">带注释的全部代码都呈现在<a class="ae kf" href="https://github.com/cran2367/autoencoder_classifier" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><div class="oy oz gp gr pa pb"><a href="https://github.com/cran2367/autoencoder_classifier/blob/master/autoencoder_classifier.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="pc ab fo"><div class="pd ab pe cl cj pf"><h2 class="bd ir gy z fp pg fr fs ph fu fw ip bi translated">cran 2367/自动编码器 _ 分类器</h2><div class="pi l"><h3 class="bd b gy z fp pg fr fs ph fu fw dk translated">稀有事件分类的自动编码器模型。通过创建……为 cran 2367/auto encoder _ classifier 开发做出贡献</h3></div><div class="pj l"><p class="bd b dl z fp pg fr fs ph fu fw dk translated">github.com</p></div></div><div class="pk l"><div class="pl l pm pn po pk pp nq pb"/></div></div></a></div><h1 id="d734" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">这里有哪些地方可以做得更好？</h1><h2 id="5d90" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated">自动编码器优化</h2><p id="bb47" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">自动编码器是 PCA 的非线性扩展。然而，上面开发的传统自动编码器不遵循 PCA 的原理。在<a class="ae kf" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第一部分</a>和<a class="ae kf" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6">第二部分</a>，解释并实现了自动编码器中应包含的 PCA 优化原则。</p><h2 id="fcba" class="lu ld iq bd le lv lw dn li lx ly dp lm kp lz ma lo kt mb mc lq kx md me ls mf bi translated">LSTM 自动编码器</h2><p id="4ff7" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">这里讨论的问题是一个(多元)时间序列。然而，在自动编码器模型中，我们没有考虑时间信息/模式。在下一篇<a class="ae kf" href="https://medium.com/@cran2367/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb" rel="noopener">文章</a>中，我们将探索 RNN 是否可行。我们将尝试一个 LSTM 自动编码器。</p><h1 id="6bdb" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="3a1b" class="pw-post-body-paragraph kg kh iq ki b kj mg jr kl km mh ju ko kp mi kr ks kt mj kv kw kx mk kz la lb ij bi translated">我们研究了来自造纸厂的极端罕见事件二进制标记数据，以构建自动编码器分类器。我们达到了合理的精确度。这里的目的是演示如何使用基本的自动编码器进行罕见事件分类。我们将进一步开发其他方法，包括一个<a class="ae kf" href="https://medium.com/@cran2367/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb" rel="noopener"> LSTM 自动编码器</a>，它可以提取时间特征以获得更好的准确性。</p><p id="37a6" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated">关于 LSTM 自动编码器的下一个帖子在这里，<a class="ae kf" href="https://medium.com/@cran2367/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb" rel="noopener"> LSTM 自动编码器罕见事件分类</a>。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="9766" class="lc ld iq bd le lf pq lh li lj pr ll lm jw ps jx lo jz pt ka lq kc pu kd ls lt bi translated">推荐后续阅读</h1><ul class=""><li id="4045" class="ms mt iq ki b kj mg km mh kp mu kt mv kx mw lb mx my mz na bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-i-1f01f821999b">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第一部分</a>。</li><li id="0e27" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6">构建正确的自动编码器——使用 PCA 原理进行调整和优化。第二部分</a>。</li><li id="4a88" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb"> LSTM 极端罕见事件分类自动编码器。</a></li></ul><h1 id="c14d" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">参考</h1><ol class=""><li id="ffc5" class="ms mt iq ki b kj mg km mh kp mu kt mv kx mw lb pv my mz na bi translated">Ranjan、m . Mustonen、k . pay nabar 和 k . pour AK(2018 年)。数据集:多元时间序列中的稀有事件分类。<a class="ae kf" href="https://arxiv.org/abs/1809.10717" rel="noopener ugc nofollow" target="_blank">T3【arXiv 预印本 arXiv:1809.10717T5】。</a></li><li id="4073" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb pv my mz na bi translated"><a class="ae kf" href="https://www.datascience.com/blog/fraud-detection-with-tensorflow" rel="noopener ugc nofollow" target="_blank">https://www . data science . com/blog/fraud-detection-with-tensor flow</a></li><li id="0a7f" class="ms mt iq ki b kj nb km nc kp nd kt ne kx nf lb pv my mz na bi translated">Github 回购:<a class="ae kf" href="https://github.com/cran2367/autoencoder_classifier" rel="noopener ugc nofollow" target="_blank">https://github.com/cran2367/autoencoder_classifier</a></li></ol><p id="14d5" class="pw-post-body-paragraph kg kh iq ki b kj kk jr kl km kn ju ko kp kq kr ks kt ku kv kw kx ky kz la lb ij bi translated"><em class="nw">免责声明:这篇文章的范围仅限于构建密集层自动编码器并将其用作罕见事件分类器的教程。通过网络调优，从业者有望获得更好的结果。这篇文章的目的是帮助数据科学家实现一个自动编码器。</em></p></div></div>    
</body>
</html>