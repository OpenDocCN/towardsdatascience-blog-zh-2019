<html>
<head>
<title>Visualizing Topic Models with Scatterpies and t-SNE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用散点图和 t-SNE 可视化主题模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/visualizing-topic-models-with-scatterpies-and-t-sne-f21f228f7b02?source=collection_archive---------12-----------------------#2019-11-04">https://towardsdatascience.com/visualizing-topic-models-with-scatterpies-and-t-sne-f21f228f7b02?source=collection_archive---------12-----------------------#2019-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/ee8d49eb7836d62e58e1ed1508d8f304.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MvI1_GSUZito7WPjtanPFw.png"/></div></div></figure><p id="91ff" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">不管是好是坏，我们的语言还没有进化成乔治·奥威尔 1984 年的新话版本，有人知道吗？).但是如果英语类似于新话，我们的计算机理解大量文本数据会容易得多。当今自然语言处理(NLP)的挑战来自于这样一个事实，即自然语言天生是模糊的，不幸的是不精确的。</p><p id="da79" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">文本数据与图像和视频等格式一起属于非结构化数据。根据<a class="ae kz" href="https://books.google.com/books?id=0-9lDwAAQBAJ&amp;pg=PA241&amp;lpg=PA241&amp;dq=dama+international+unstructured+data+Any+document,+file,+graphic,+image,+text,+report,+form,+video,+or+sound+recording+that+has+not+been+tagged+or+otherwise+structured+into+rows+and+columns+or+records&amp;source=bl&amp;ots=tjVlGGZcl0&amp;sig=ACfU3U1lQ4_8ZX6lLzG_YkHhI2KkIkI3vw&amp;hl=en&amp;sa=X&amp;ved=2ahUKEwiSraarm_zkAhVQqZ4KHfP1CHYQ6AEwAHoECAkQAQ#v=onepage&amp;q=dama%20international%20unstructured%20data%20Any%20document%2C%20file%2C%20graphic%2C%20image%2C%20text%2C%20report%2C%20form%2C%20video%2C%20or%20sound%20recording%20that%20has%20not%20been%20tagged%20or%20otherwise%20structured%20into%20rows%20and%20columns%20or%20records&amp;f=false" rel="noopener ugc nofollow" target="_blank"> Dama </a>的说法，“非结构化数据在技术上是指任何没有被标记或以其他方式组织成行、列或记录的文档、文件、图形、图像、文本、报告、表格、视频或录音。”“非结构化”这个标签有点不公平，因为通常还是有一些结构的。图像分解成用 RGB 或黑/白数值表示的像素行。文本在文档中分解成句子、段落和/或章节，文档的集合形成语料库。</p><p id="42ea" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于理解书面自然语言的计算机来说，它需要理解文本背后的符号结构。使用下面的一些自然语言处理技术可以让计算机对文本进行分类，并回答诸如主题是什么之类的问题。语气积极吗？主观？<a class="ae kz" href="https://twitter.com/SienaDuplan/status/717420808759476224" rel="noopener ugc nofollow" target="_blank">读起来有多容易？</a></p><p id="416d" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">下面是一些我发现对揭示语料库背后的符号结构有用的自然语言处理技术:</p><ul class=""><li id="c2a5" class="lb lc it kd b ke kf ki kj km ld kq le ku lf ky lg lh li lj bi translated">主题建模(主题、聚类)</li><li id="1da2" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">单词/短语频率(和“关键字搜索”)</li><li id="111b" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">文本可视化(<a class="ae kz" href="https://www.jasondavies.com/wordtree/" rel="noopener ugc nofollow" target="_blank">单词树</a>，<a class="ae kz" href="https://www.jasondavies.com/wordcloud/" rel="noopener ugc nofollow" target="_blank">单词云</a>均由@Jason Davies 提供)</li><li id="da47" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">嵌入(例如 Word2Vec)</li><li id="d986" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">情感分析(正面/负面、主观/客观、情感标签)</li><li id="1e2b" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">文本相似性(例如“余弦相似性”)</li><li id="63dd" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">TF-IDF(术语频率/逆文档频率)</li><li id="e0a9" class="lb lc it kd b ke lk ki ll km lm kq ln ku lo ky lg lh li lj bi translated">词性标注</li></ul><p id="7a65" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在这篇文章中，我将重点关注我用来理解文本的主要技术:主题建模，特别是使用<a class="ae kz" href="https://github.com/vi3k6i5/GuidedLDA" rel="noopener ugc nofollow" target="_blank">guide LDA</a>(一种增强的 LDA 模型，使用采样类似于半监督方法，而不是无监督方法)。<strong class="kd iu">我在训练一个主题模型后遇到的一个困难是显示它的结果。</strong>主题模型的输出是一个文档主题矩阵，形状为<em class="la"> D x T </em> — <em class="la"> D </em>行表示<em class="la"> D </em>文档，而<em class="la"> T </em>列表示<em class="la"> T </em>主题。单元格包含介于 0 和 1 之间的概率值，该值为每个文档分配属于每个主题的可能性。文档-主题矩阵中各行的总和应该总是等于 1。在最佳情况下，文档将很有可能被分类到单个主题中。对于更模糊的数据——每个文档可能谈论许多主题——模型应该在其讨论的主题中更均匀地分布概率。以下是 GuidedLDA 模型输出的文档主题矩阵的前几行示例:</p><figure class="lq lr ls lt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi lp"><img src="../Images/8f68b8b4abac3c0f5439d0e7b4cce5d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*moA-msQBO9mC2JnRUMi2_A.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Document 0 belongs to topic 0 with 71% probability and topic 1 with 29% probability. Document 1 belongs to topic 0 with almost 100% probability. And so on.</figcaption></figure><p id="a94b" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">像上面这样的文档主题矩阵很容易变得非常庞大。除非结果被用来链接回单个文档，否则从整体上分析文档-主题-分布可能会变得混乱，尤其是当一个文档可能属于几个主题时。这就是我想用散点图和饼图的组合来可视化矩阵本身的地方:请看散点图！</p><figure class="lq lr ls lt gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ly"><img src="../Images/b88bf8572dd086f0105a6c3d0cf4f809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FnsTVrgcH6MyFYkj0zvU9Q.png"/></div></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk">Each bubble is a document. Bubbles are colored by topic probability and sized by #1 topic probability (argmax of each row/document in the document-topic matrix). Where there is a pie chart, the document belongs to multiple topics with varying probabilities. The scatterpie chart shows which topics tend to co-occur and how topics overlap with each other. Individual bubbles (documents) are pulled in the direction of the “center of mass” of another topic they may also belong to. Note: to simplify the chart, you may need to filter out documents with #1 topic probability less than some threshold. In this example, I removed documents with #1 topic probability &lt; 0.3 to clean up the visualization.</figcaption></figure><p id="eb70" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">但是不要这么快——你可能首先想知道我们是如何将主题简化成一个易于可视化的二维空间的。为此，我使用了 t-分布式随机邻居嵌入(或 t-SNE)。从 GuidedLDA 获取文档-主题矩阵输出，用 Python 运行:</p><pre class="lq lr ls lt gt lz ma mb mc aw md bi"><span id="0f79" class="me mf it ma b gy mg mh l mi mj">from sklearn.manifold import TSNE</span><span id="a8c3" class="me mf it ma b gy mk mh l mi mj">tsne_model = TSNE(n_components=2, verbose=1, random_state=7, angle=.99, init=’pca’)</span><span id="9790" class="me mf it ma b gy mk mh l mi mj"># 13-D -&gt; 2-D<br/>tsne_lda = tsne_model.fit_transform(doc_topic) # doc_topic is document-topic matrix from LDA or GuidedLDA </span></pre><p id="d3ef" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">在将 t-SNE 数据的两个数组(使用<code class="fe ml mm mn ma b">tsne_lda[:,0]</code>和<code class="fe ml mm mn ma b">tsne_lda[:,1]</code>)连接到原始的文档-主题矩阵之后，我在矩阵中有两列可以用作散点图中的 X，Y 坐标。</p><p id="89b1" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">对于情节本身，我切换到 R 和 ggplot2 包。下面代码片段中的 dataframe <em class="la"> data </em>特定于我的示例，但是列名应该或多或少是不言自明的。<em class="la"> x_tsne </em>和<em class="la"> y_tsne </em>是 t-SNE 结果的前两个维度。<em class="la"> row_id </em>是每个文档的惟一值(就像整个文档主题表的主键)。<em class="la"> x_1_topic_probability </em>是文档-主题矩阵每一行中的#1 最大概率(即文档最有可能代表的主题)。<em class="la"> topic_names_list </em>是每个主题带有<em class="la"> T </em>标签的字符串列表。</p><pre class="lq lr ls lt gt lz ma mb mc aw md bi"><span id="5a8f" class="me mf it ma b gy mg mh l mi mj">library(ggplot2)<br/>library(scatterpie)</span><span id="4ac7" class="me mf it ma b gy mk mh l mi mj"># high resolution tiff image<br/>tiff(“scatterpie.png”, units=”in”, width=12, height=9, res=500)</span><span id="2763" class="me mf it ma b gy mk mh l mi mj"># plot<br/>p &lt;- ggplot() + <br/> geom_scatterpie(aes(x=x_tsne, y=y_tsne, group=row_id, r=x_1_topic_probability), data=data, cols=topic_names_list, color=NA, alpha=0.7) + <br/> coord_equal() + <br/> geom_label(...) + <br/> ggtitle(“Scatterpie Chart”) + <br/> xlab(“”) + ylab(“”) + labs(subtitle=”t-SNE Representation of Guided-LDA Topics Colored and Sized by Topic Probability”) +<br/> scale_fill_manual(values=colors) + <br/> theme_minimal() + <br/> theme(text = element_text(color=”white”),<br/> legend.position = “none”,<br/> panel.background = element_rect(fill = “gray17”, colour = “gray17”), <br/> plot.background = element_rect(fill = “gray17”),<br/> panel.grid.major = element_line(colour = “gray25”),<br/> panel.grid.minor = element_line(colour = “gray25”),<br/> axis.text = element_text(color=”white”))<br/></span><span id="f1ab" class="me mf it ma b gy mk mh l mi mj"># shut down graphics device<br/>dev.off()</span></pre><p id="6856" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">瞧，您已经有了构建主题模型输出的散点图表示的基本要素。将来，我想通过一个交互式的图(looking you，<a class="ae kz" href="https://d3js.org/" rel="noopener ugc nofollow" target="_blank"> d3.js </a>)来进一步发展，在这个图中，将鼠标悬停在一个气泡上可以显示该文档的文本以及关于其分类的更多信息。</p><p id="fca5" class="pw-post-body-paragraph kb kc it kd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky im bi translated">如果你对更酷的 t-SNE 例子感兴趣，我推荐你去看看劳伦斯·范·德·马腾的页面。</p><div class="lq lr ls lt gt ab cb"><figure class="mo ju mp mq mr ms mt paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/3ebe0966fd2f920060b7dcccc53c3544.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*jRId_vMTOCVA2z7eucDzYQ.png"/></div></figure><figure class="mo ju mu mq mr ms mt paragraph-image"><img src="../Images/f92b0e65b3b36942e1355830372577f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/1*-iM-ATFuGn59l64H1ALChw.gif"/></figure><figure class="mo ju mv mq mr ms mt paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><img src="../Images/ad9cbdd14450292c2e1c93ae855cbda3.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*4Q6Cd3Citrt40CRF7ZBa2g.jpeg"/></div><figcaption class="lu lv gj gh gi lw lx bd b be z dk mw di mx my">Left &amp; middle: 2D &amp; 3D t-SNE visualization of 6,000 digits from the <a class="ae kz" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">MNIST</a> dataset. Right: t-SNE visualization of image data. <a class="ae kz" href="https://lvdmaaten.github.io/tsne/" rel="noopener ugc nofollow" target="_blank">Source</a></figcaption></figure></div></div></div>    
</body>
</html>