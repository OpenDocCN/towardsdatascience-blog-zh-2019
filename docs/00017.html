<html>
<head>
<title>How does Facebook tune Apache Spark for Large-Scale Workloads?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书如何针对大规模工作负载调整 Apache Spark？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-does-facebook-tune-apache-spark-for-large-scale-workloads-3238ddda0830?source=collection_archive---------6-----------------------#2019-01-02">https://towardsdatascience.com/how-does-facebook-tune-apache-spark-for-large-scale-workloads-3238ddda0830?source=collection_archive---------6-----------------------#2019-01-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="8734" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我想开始祝你有一个美好的 2019 年，在我今年的第一篇文章中，我将分享由刘和来自的 Sital Kedia 在 Spark 峰会会议上介绍的针对大规模工作负载调整 Apache Spark 的会议摘要<a class="ae ko" href="https://databricks.com/session/tuning-apache-spark-for-large-scale-workloads" rel="noopener ugc nofollow" target="_blank">以及我的日常经验。</a></p><p id="6816" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当我们谈论 Spark 调优时，我们需要认识到每个应用和环境都是不同的，因此<strong class="js iu">我们不能假设这种配置对所有情况都是最好的</strong>。在这种情况下，大多数推荐的属性都与大型管道或以批处理模式处理大型数据集的作业相关。</p><p id="411d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">让我们开始定义我们可以从脸书收集的主题</p><ol class=""><li id="52ff" class="kp kq it js b jt ju jx jy kb kr kf ks kj kt kn ku kv kw kx bi translated">缩放火花驱动器</li><li id="0acd" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">缩放火花执行器</li><li id="0be0" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">扩展外部洗牌服务</li><li id="fdb8" class="kp kq it js b jt ky jx kz kb la kf lb kj lc kn ku kv kw kx bi translated">工具</li></ol><h1 id="b58e" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">1.缩放火花驱动器</h1><p id="7a15" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated"><strong class="js iu">动态执行人分配</strong></p><blockquote class="mg mh mi"><p id="bb8b" class="jq jr mj js b jt ju jv jw jx jy jz ka mk kc kd ke ml kg kh ki mm kk kl km kn im bi translated">是一个 Spark 特性，它允许动态地添加和删除 Spark 执行器，以匹配工作负载。[ <a class="ae ko" href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-dynamic-allocation.html" rel="noopener ugc nofollow" target="_blank">掌握 Apache Spark </a> ]</p></blockquote><p id="5c3a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如果您与其他团队共享集群资源，那么完全推荐启用此配置，这样您的 Spark 应用程序就只使用它最终将使用的资源。它可以根据工作量调整执行者的数量。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="4adb" class="mw le it ms b gy mx my l mz na">spark.dynamicAllocation.enable = true<br/>spark.dynamicAllocation.executorIdleTimeout = 2m<br/>spark.dynamicAllocation.minExecutors = 1<br/>spark.dynamicAllocation.maxExecutors = 2000</span></pre><p id="700e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这四个参数是自我描述的，也许第二个需要更多的细节。executorIDleTimeout 用于正确移除执行器。</p><p id="956b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">更好的获取失败处理</strong></p><p id="d1a6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">中止阶段之前允许的连续阶段尝试次数(默认为 4)。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="b4da" class="mw le it ms b gy mx my l mz na">spark.stage.maxConsecutiveAttempts = 10</span></pre><p id="e389" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">调整 RPC 服务器线程</strong></p><p id="db16" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">增加 RPC 服务器线程以修复内存不足(实际上我在<a class="ae ko" href="https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/configuration.html#networking" rel="noopener ugc nofollow" target="_blank"> spark 官方文档</a>中找不到更多细节，一个很好的解释是<a class="ae ko" href="https://github.com/jaceklaskowski/mastering-apache-spark-book/blob/master/spark-rpc-netty.adoc#settings" rel="noopener ugc nofollow" target="_blank">这里是</a>)</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="5812" class="mw le it ms b gy mx my l mz na">spark.rpc.io.serverTreads = 64</span></pre><h1 id="9ae5" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">2.缩放火花执行器</h1><p id="9062" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated">首先必须理解如何基于自 Spark 1.6 [ <a class="ae ko" href="https://0x0fff.com/spark-memory-management/" rel="noopener ugc nofollow" target="_blank"> Spark 内存管理</a> ]以来开发的统一内存管理来定义执行器内存的结构(图 1)</p><figure class="mn mo mp mq gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nb"><img src="../Images/1e042176bbcbdebdacdec0de0c5750c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rk2eUfEQYibpGuuqDB_mGQ.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Fig. 1 Executor memory layout</figcaption></figure><p id="4292" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">随机存储器</strong></p><p id="6bda" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">一小部分(堆空间— 300MB)用于执行和存储<a class="ae ko" href="http://Deep Dive: Memory Management in Apache Spark" rel="noopener ugc nofollow" target="_blank">【深入探讨:Apache Spark 中的内存管理】</a>。这个值越低，溢出和缓存数据回收就越频繁。此配置的目的是为内部元数据、用户数据结构和稀疏、异常大的记录的不精确大小估计留出内存(默认为 60%)。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0ec4" class="mw le it ms b gy mx my l mz na">spark.memory.fraction * (spark.executor.memory - 300 MB)</span></pre><p id="4416" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">用户记忆</strong></p><p id="9fee" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">是为 Spark 中的用户数据结构、内部元数据保留的，并且在记录稀疏和异常大的情况下，默认情况下，保护内存不足错误的发生。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c9db" class="mw le it ms b gy mx my l mz na">(1 - spark.memory.fraction) * (spark.executor.memory - 300 MB)</span></pre><p id="9b61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">保留记忆</strong></p><p id="e251" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是系统保留的内存。它的值是 300MB，这意味着这 300MB 的 RAM 不参与 Spark 内存区域大小的计算。它会储存火花内部物体。</p><p id="5301" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">记忆缓冲区</strong></p><p id="4377" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">要为每个执行器分配的堆外内存量(以兆字节为单位)。这是一个考虑到虚拟机开销、内部字符串、其他本机开销等因素的内存。<a class="ae ko" href="https://spark.apache.org/docs/2.2.0/running-on-yarn.html#spark-properties" rel="noopener ugc nofollow" target="_blank">【火花属性】</a></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="807f" class="mw le it ms b gy mx my l mz na">spark.yarn.executor.memoryOverhead = 0.1 * (spark.executor.memory)</span></pre><p id="e2b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">启用堆外内存</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="c648" class="mw le it ms b gy mx my l mz na">#Shuffle Memory </span><span id="f169" class="mw le it ms b gy nn my l mz na">spark.memory.offHeap.enable = true<br/>spark.memory.ofHeap.size = 3g</span><span id="c68f" class="mw le it ms b gy nn my l mz na">#User Memory</span><span id="33e5" class="mw le it ms b gy nn my l mz na">spark.executor.memory = 3g</span><span id="501e" class="mw le it ms b gy nn my l mz na">#Memory Buffer</span><span id="d955" class="mw le it ms b gy nn my l mz na">spark.yarn.executor.memoryOverhead = 0.1 * (spark.executor.memory + spark.memory.offHeap.size)</span></pre><p id="a56e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">垃圾收集调优</strong></p><blockquote class="mg mh mi"><p id="0b4a" class="jq jr mj js b jt ju jv jw jx jy jz ka mk kc kd ke ml kg kh ki mm kk kl km kn im bi translated">当您的程序存储的 rdd 有大量“变动”时，JVM 垃圾收集会是一个问题。(在只读取一次 RDD，然后在其上运行许多操作的程序中，这通常不是问题。)当 Java 需要驱逐旧对象为新对象腾出空间时，它将需要跟踪所有 Java 对象并找到未使用的对象。GCT </p></blockquote><p id="2767" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里的一个建议是使用 GC 而不是 G1GC</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="0461" class="mw le it ms b gy mx my l mz na">spark.executor.extraJavaOptions = -XX:ParallelGCThreads=4 -XX:+UseParallelGC</span></pre><p id="92c1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">调混文件缓冲</strong></p><p id="a46f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">磁盘访问比内存访问慢，因此我们可以通过缓冲读/写来分摊磁盘 I/O 成本。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9ded" class="mw le it ms b gy mx my l mz na">#Size of the in-memory buffer for each shuffle file output stream. #These buffers reduce the number of disk seeks and system calls made #in creating intermediate shuffle files. [<a class="ae ko" href="https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/configuration.html#shuffle-behavior" rel="noopener ugc nofollow" target="_blank">Shuffle behavior</a>]<br/>spark.shuffle.file.buffer = 1 MB</span><span id="f826" class="mw le it ms b gy nn my l mz na"><br/>spark.unsafe.sorter.spill.reader.buffer.size  = 1 MB</span></pre><p id="d5d8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">优化溢出文件合并</strong> [ <a class="ae ko" href="https://issues.apache.org/jira/browse/SPARK-20014" rel="noopener ugc nofollow" target="_blank"> Spark-20014 </a></p><p id="25ca" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">通过关闭到的传输并使用缓冲文件读/写来提高 io 吞吐量，从而使用 mergeSpillsWithFileStream 方法。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="3478" class="mw le it ms b gy mx my l mz na">spark.file.transferTo = false<br/>spark.shuffle.file.buffer = 1 MB<br/>spark.shuffle.unsafe.file.ouput.buffer = 5 MB</span></pre><p id="1511" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">调整压缩块大小</strong></p><p id="f597" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">默认压缩块为 32 kb，这对于大型数据集来说不是最佳选择。如果您转到<a class="ae ko" href="https://www.slideshare.net/databricks/tuning-apache-spark-for-largescale-workloads-gaoxiang-liu-and-sital-kedia" rel="noopener ugc nofollow" target="_blank">幻灯片</a>，您会发现通过增加块大小，随机播放/溢出文件大小减少了 20%。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="dee2" class="mw le it ms b gy mx my l mz na">#Block size used in LZ4 compression, in the case when LZ4 #compression codec is used. Lowering this block size will also lower #shuffle memory usage when LZ4 is used. [<a class="ae ko" href="http://Block size used in LZ4 compression, in the case when LZ4 compression codec is used. Lowering this block size will also lower shuffle memory usage when LZ4 is used." rel="noopener ugc nofollow" target="_blank">Compression and Serialization</a>]<br/>spark.io.compression.lz4.blockSize = 512KB</span><span id="e8be" class="mw le it ms b gy nn my l mz na">#Note that tha default compression code is LZ4 you could change #using<br/>spark.io.compression.codec</span></pre><h1 id="1acd" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">3.扩展外部洗牌服务</h1><p id="c4b7" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated"><strong class="js iu">在 Shuffle 服务器上缓存索引文件</strong></p><blockquote class="mg mh mi"><p id="eb6c" class="jq jr mj js b jt ju jv jw jx jy jz ka mk kc kd ke ml kg kh ki mm kk kl km kn im bi translated">问题是，对于每次 shuffle 提取，我们都要重新打开同一个索引文件并读取它。如果我们能够避免多次打开同一个文件并缓存数据，效率会更高。我们可以使用 LRU 缓存来保存索引文件信息。通过这种方式，我们还可以限制缓存中条目的数量，这样我们就不会无限地浪费内存。[ <a class="ae ko" href="https://issues.apache.org/jira/browse/SPARK-15074" rel="noopener ugc nofollow" target="_blank"> Spark-15074 </a></p></blockquote><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="bb57" class="mw le it ms b gy mx my l mz na">#Cache entries limited to the specified memory footprint.<br/>spark.shuffle.service.index.cache.size = 2048</span></pre><p id="ca3d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">可配置洗牌注册超时和重试</strong></p><p id="d7ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于更有可能发生节点故障的大型集群(例如，超过 50 个节点)，这是特别推荐的。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="ba10" class="mw le it ms b gy mx my l mz na">spark.shuffle.registration.timeout = 2m<br/>spark.shuffle.registration.maxAttempst = 5</span></pre><h1 id="e496" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">4.工具</h1><p id="4a22" class="pw-post-body-paragraph jq jr it js b jt mb jv jw jx mc jz ka kb md kd ke kf me kh ki kj mf kl km kn im bi translated"><strong class="js iu"> Spark UI 指标</strong></p><p id="3b01" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我认为这可能是下一篇文章的一部分(这次有实际的例子👩‍💻 👨‍💻)因为那里有很多调试、优化、调优的有用信息。</p><p id="ca59" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">首先，为了进行优化，您可以检查随机读取阻塞时间(任务等待随机数据从远程机器读取所花费的阻塞时间[<a class="ae ko" href="https://stackoverflow.com/questions/37468394/spark-shuffle-read-blocked-time" rel="noopener ugc nofollow" target="_blank">堆栈溢出</a>])</p><figure class="mn mo mp mq gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi no"><img src="../Images/05fafa1eaefd81364201f38cd9de0a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cyu1gMdlUsTCmbpp.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Fig 2. Example of a Spark UI Metric [<a class="ae ko" href="https://community.hortonworks.com/questions/67659/what-are-the-important-metrics-to-notice-for-each.html" rel="noopener ugc nofollow" target="_blank">Community Hortonworks</a>]</figcaption></figure><p id="9bdc" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">感谢阅读！下一篇文章再见。</p><p id="e740" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">PS 如果你有任何问题，或者想要澄清一些事情，你可以在<a class="ae ko" href="https://twitter.com/thony_ac77" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae ko" href="https://www.linkedin.com/in/antoniocachuan/" rel="noopener ugc nofollow" target="_blank"> LinkedIn 上找到我。</a>如果你想了解 Apache Arrow 和 Apache Spark，我有一篇文章<strong class="js iu"> </strong> <a class="ae ko" rel="noopener" target="_blank" href="/a-gentle-introduction-to-apache-arrow-with-apache-spark-and-pandas-bb19ffe0ddae"> <strong class="js iu">用一些例子对 Apache Arrow 和 Apache Spark 以及 Pandas </strong> </a> <strong class="js iu"> </strong>进行了简单的介绍，此外，今年出版了一本我认为很棒的书<a class="ae ko" href="https://amzn.to/2NQxTmZ" rel="noopener ugc nofollow" target="_blank"> Spark:权威指南</a>。</p></div></div>    
</body>
</html>