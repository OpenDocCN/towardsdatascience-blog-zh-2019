<html>
<head>
<title>It’s a No Brainer: An Introduction to Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这很容易:神经网络导论</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/its-a-no-brainer-an-introduction-to-neural-networks-10e2738c2d7e?source=collection_archive---------20-----------------------#2019-08-28">https://towardsdatascience.com/its-a-no-brainer-an-introduction-to-neural-networks-10e2738c2d7e?source=collection_archive---------20-----------------------#2019-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a5c4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经网络的温和介绍，现在与僵尸。</h2></div><p id="9280" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络是 1944 年首次提出的一种人工智能方法。神经网络大致模拟人脑，由大量简单的处理节点(称为神经元)组成，这些节点高度互联，并通过这些网络连接发送数据以估计目标变量。</p><p id="c6c2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我将讨论简单神经网络的结构和训练(特别是<a class="ae le" href="https://en.wikipedia.org/wiki/Multilayer_perceptron" rel="noopener ugc nofollow" target="_blank">多层感知器</a>，又名“香草神经网络”)，以及演示一个简单的神经网络。</p><p id="979e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lf">问题:为什么僵尸只和聪明的女人约会？</em></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/91480ac4dbd5a75b08e84040875d0760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*74fIfgv8M4CFF5wL.gif"/></div></figure><p id="5f77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">答案:他们只是喜欢有头脑的女人。</p><p id="8ed4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">神经网络如何工作</strong></p><p id="b557" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一般来说，神经网络模型由成千上万个密集连接的神经元(节点)组成。在大多数神经网络模型中，神经元被组织成层。这包括输入层和输出层，输入层包括所有提供的预测变量的神经元，<a class="ae le" href="https://www.techopedia.com/definition/33264/hidden-layer-neural-networks" rel="noopener ugc nofollow" target="_blank">隐藏层</a>。神经网络的隐藏层有效地将输入转换成<a class="ae le" href="https://stats.stackexchange.com/questions/63152/what-does-the-hidden-layer-in-a-neural-network-compute" rel="noopener ugc nofollow" target="_blank">输出层可以解释的东西</a>。输出图层返回类别标注(分类)或估计值(回归)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lo"><img src="../Images/db5ff0f8299fd1333499026baabaa8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EUS8cS3nuKlFkVc6"/></div></div></figure><p id="4c30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每个神经元处，所有输入值被加在一起，然后用<a class="ae le" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">激活函数</a>(例如<a class="ae le" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid 函数</a>)进行处理，这将确定神经元是否被“激活”。通常，在激活函数之前，偏差也将包括在该计算中。偏差类似于<a class="ae le" href="https://en.wikipedia.org/wiki/Regression_analysis" rel="noopener ugc nofollow" target="_blank">回归模型</a>中的<a class="ae le" href="https://en.wikipedia.org/wiki/Y-intercept" rel="noopener ugc nofollow" target="_blank">截距项</a>。</p><p id="1382" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后使用阈值来确定神经元是否会“触发”，这意味着激活所有到下一层的传出连接。神经网络可以是<a class="ae le" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈</a>，意味着所有数据只向前传递，或者是<a class="ae le" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归</a>，可以包括通过网络层的循环或回路。多层感知器只是前馈。</p><p id="7754" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经元之间的每个连接都被赋予一个(正的或负的)数字权重。对于每个连接，当一个神经元达到或超过给定阈值时，这个权重乘以神经元的数值，将被传递给下一层中连接的神经元。如果没有达到阈值，神经元将不会被激活，并且来自该神经元的数据将不会传递给下一层中的任何神经元。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lt"><img src="../Images/ae4e52759c30e0a5b6262833bb622fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QRDA--D-RT95Ni9E"/></div></div></figure><p id="cf6e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在分类模型中，输出层中具有最高值的神经元将确定每个记录的模型估计标签。对于回归模型，神经网络通过单个输出节点返回值。</p><p id="8b18" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果这个快速概述没有意义，或者如果你想知道更多关于数学的知识，3BLUE1BROWN 有一个非常棒的视频叫做<a class="ae le" href="https://www.youtube.com/watch?v=aircAruvnKk" rel="noopener ugc nofollow" target="_blank">，但是我推荐观看什么是神经网络</a>。</p><p id="4000" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如何训练神经网络</strong></p><p id="fd2f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多层感知器神经网络通常使用一种称为<a class="ae le" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>的方法进行训练，这涉及通过计算<a class="ae le" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">成本(损失)函数</a>的<a class="ae le" href="https://en.wikipedia.org/wiki/Gradient" rel="noopener ugc nofollow" target="_blank">梯度</a>来调整神经网络中神经元的权重。</p><p id="71e8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了开始训练神经网络，所有的初始权重和阈值都是随机生成的。然后，训练数据通过输入层输入，并通过模型，直到到达输出层。在输出层，计算成本函数以估计模型在估计已知目标变量中的表现。当网络自信地估计正确的值时，成本函数的输出被最小化，并且随着错误分类而增加。训练算法的目标是最小化成本函数值。</p><p id="fbb0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后调整神经网络中的权重和阈值，以最小化成本函数(这是微积分部分)，直到模型收敛于局部最小值。重复该过程，并且基于训练数据和成本函数继续调整权重和阈值，直到具有相同标签的所有数据产生相似的值。</p><p id="1638" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">3BLUE1BROWN 关于神经网络系列的第 2 部分和<a class="ae le" href="https://youtu.be/Ilg3gGewQ5U" rel="noopener ugc nofollow" target="_blank">第 3 部分</a>分别涵盖了训练和反向传播，如果您想了解更多，可能会对您有所帮助。还有一本关于神经网络和深度学习的优秀开源教科书，作者是迈克尔·尼尔森(Michael Nielsen)，这里有<a class="ae le" href="https://theclevermachine.wordpress.com/2014/09/11/a-gentle-introduction-to-artificial-neural-networks/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="ec25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">前馈神经网络的剖析</strong></p><p id="d15d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">R 包<a class="ae le" href="https://cran.r-project.org/web/packages/nnet/nnet.pdf" rel="noopener ugc nofollow" target="_blank"> nnet </a>可以生成一个简单的<a class="ae le" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">单隐层前馈神经网络</a>。前馈指的是数据可以在层间传递的方向。前馈模型只能向“下游”传递数据。单一隐藏层也是这个特定实现的一个重要方面。虽然用户可以调整隐藏层中包含的神经元数量，但隐藏层的数量不能增加。这些神经网络确实属于多层感知器的类别。</p><p id="4efd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具有单个隐藏项的经过训练的前馈神经网络的结构可能如下所示:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="gh gi lo"><img src="../Images/4d45c98e6d9f05bcec5571a9dfa9d138.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6OCTkvS8kGnJ0c4o"/></div></div></figure><p id="0ff0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">浅绿色的线(浅绿色)是负权重，深绿色的线(青苹果)是正权重。线条粗细用于描述分配给每个连接的相对权重。</p><p id="bb81" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">预测变量(如果你还没有猜到，这个神经网络是用臭名昭著的虹膜数据集训练的)包括在第一层中。</p><p id="7786" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图顶部描绘的偏置层(粉红色)将常数值应用于神经元(类似于回归模型中的截距项)。隐藏层的激活函数将是<a class="ae le" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid </a>，输出层的<a class="ae le" href="https://en.wikipedia.org/wiki/Activation_function" rel="noopener ugc nofollow" target="_blank">激活函数</a>将取决于目标场。<a class="ae le" href="https://en.wikipedia.org/wiki/Binary_classification" rel="noopener ugc nofollow" target="_blank">二元分类</a>将使用<a class="ae le" href="https://en.wikipedia.org/wiki/Logistic_function" rel="noopener ugc nofollow" target="_blank">逻辑</a>激活函数，<a class="ae le" href="https://en.wikipedia.org/wiki/Multiclass_classification" rel="noopener ugc nofollow" target="_blank">多项式分类</a>将使用<a class="ae le" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax 函数</a>，回归(具有连续目标变量的模型)将应用<a class="ae le" href="https://en.wikipedia.org/wiki/Linear_function" rel="noopener ugc nofollow" target="_blank">线性</a>激活函数。</p><p id="174f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用于训练模型的成本函数(使用反向传播方法)也将取决于目标变量。对于分类模型(目标变量是分类的)，成本函数是称为<a class="ae le" href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_minimization" rel="noopener ugc nofollow" target="_blank">交叉熵</a>的熵度量。对于回归模型(目标变量是连续的)，用于训练神经网络的成本函数是<a class="ae le" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank">残差平方和</a>。</p><p id="17c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">数据集中的每个记录(观测值)都通过这个网络传递。根据预测变量的值，每层中的不同神经元将被激活，从而对输出层中的记录进行估计。</p><p id="4049" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">希望，这已经打破了神经网络周围的坚硬外壳，让你能够消化好的，多肉的部分。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/620171d1b02b0df12faa25d43ab6c1e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*ffbPx6AWNtyQ36uB.gif"/></div></figure><p id="75cf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这篇文章最初发表在 Alteryx 数据科学博客上，可以在这里 找到<a class="ae le" href="https://community.alteryx.com/t5/Data-Science-Blog/It-s-a-No-Brainer-An-Introduction-to-Neural-Networks/ba-p/300479" rel="noopener ugc nofollow" target="_blank"> <em class="lf">。</em></a></p></div></div>    
</body>
</html>