<html>
<head>
<title>Support Vector Machines — Soft Margin Formulation and Kernel Trick</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——软间隔公式和核技巧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe?source=collection_archive---------0-----------------------#2019-04-30">https://towardsdatascience.com/support-vector-machines-soft-margin-formulation-and-kernel-trick-4c9729dc8efe?source=collection_archive---------0-----------------------#2019-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8fae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">学习一些使支持向量机成为强大的线性分类器的高级概念</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f9d5edde0617bb73a7ee7d071e7ebbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_3iYollNTlz0PVn5udCBQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">SVM’s soft margin formulation technique in action</figcaption></figure><h1 id="e6f0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="a3a0" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">支持向量机(SVM)是最流行的分类技术之一，旨在直接最小化误分类错误的数量。有许多可访问的资源可以理解支持向量机(SVM)如何工作的基础知识，然而，在几乎所有的现实世界应用中(其中数据是线性不可分的)，SVM 使用一些高级概念。</p><blockquote class="mk"><p id="3216" class="ml mm iq bd mn mo mp mq mr ms mt mi dk translated">这篇文章的目的是解释软边界公式的概念和支持向量机用来分类线性不可分数据的核心技巧。</p></blockquote><p id="b23a" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw mw ly lz ma mx mc md me my mg mh mi ij bi translated">如果你想先重温一下 SVM 的基础知识，我推荐你看看下面这篇文章。</p><div class="mz na gp gr nb nc"><a rel="noopener follow" target="_blank" href="/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47"><div class="nd ab fo"><div class="ne ab nf cl cj ng"><h2 class="bd ir gy z fp nh fr fs ni fu fw ip bi translated">支持向量机——机器学习算法简介</h2><div class="nj l"><h3 class="bd b gy z fp nh fr fs ni fu fw dk translated">从零开始的 SVM 模式</h3></div><div class="nk l"><p class="bd b dl z fp nh fr fs ni fu fw dk translated">towardsdatascience.com</p></div></div><div class="nl l"><div class="nm l nn no np nl nq kp nc"/></div></div></a></div><h1 id="af0e" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">线性<em class="nr">不可分性</em></h1><p id="d54f" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在我们继续讨论软余量和核心技巧的概念之前，让我们先确定它们的必要性。假设我们有一些数据，可以在 2D 空间中描述如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/c0c6f4c3744a676dd6d6a3413dcc8740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ei48ImEpZc2xHjZZTFP09Q.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 1: Data representation where the two classes are not linearly separable</figcaption></figure><p id="ef50" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">从图中可以明显看出，没有特定的线性判定边界可以很好地分离数据，即<em class="mj">数据是线性不可分的</em>。在更高维度的表现中，我们也可以有类似的情况。这可以归因于这样一个事实:通常，我们从数据中获得的特征不包含足够的信息<em class="mj">以便我们可以清楚地区分这两个类别。在许多现实世界的应用程序中通常都是这种情况。幸运的是，研究人员已经提出了能够处理这种情况的技术。让我们看看它们是什么以及它们是如何工作的。</em></p><h1 id="67a0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">软边界公式</h1><blockquote class="mk"><p id="28b4" class="ml mm iq bd mn mo mp mq mr ms mt mi dk translated">这个想法是基于一个简单的前提:<em class="nr">允许 SVM 犯一定数量的错误，并保持尽可能大的差距，以便其他点仍然可以正确分类</em>。只要修改 SVM 的目标，就可以做到这一点。</p></blockquote><h2 id="8090" class="ny kw iq bd kx nz oa dn lb ob oc dp lf lw od oe lh ma of og lj me oh oi ll oj bi translated">动机</h2><p id="b4e1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们简要回顾一下采用这种提法的动机。</p><ul class=""><li id="525c" class="ok ol iq lp b lq nt lt nu lw om ma on me oo mi op oq or os bi translated">如前所述，几乎所有现实世界的应用程序都有线性不可分的数据。</li><li id="072d" class="ok ol iq lp b lq ot lt ou lw ov ma ow me ox mi op oq or os bi translated">在数据<em class="mj">是</em>线性可分的极少数情况下，我们可能不想选择一个完美分离数据的决策边界来避免过度拟合。例如，考虑下图:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b6f60557ea510074512c9cf8a15d7e86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z9HQRL4rTdubV2a0hLtCjA.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 2: Which decision boundary is better? Red or Green?</figcaption></figure><p id="2df3" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">这里的<code class="fe oy oz pa pb b">red</code>决策边界完美地分隔了所有的训练点。然而，拥有如此少的余量的决策边界真的是一个好主意吗？你认为这样的决策边界会在看不见的数据上很好地推广吗？答案是:不。<code class="fe oy oz pa pb b">green</code>决策边界有更宽的边界，这将允许它对看不见的数据进行很好的概括。从这个意义上说，软余量公式也有助于避免过度拟合问题。</p><h2 id="ee79" class="ny kw iq bd kx nz pc dn lb ob pd dp lf lw pe oe lh ma pf og lj me pg oi ll oj bi translated">它是如何工作的(数学上)？</h2><p id="8aca" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们看看如何修改我们的目标，以达到预期的行为。在这种新环境下，我们的目标是最大限度地降低以下目标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/6c540d55643c82171de9bef26409ccd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IcfdeBQ8PQNzJFpMSi5EFw@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 1</figcaption></figure><p id="4441" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">这与第二任期的最初目标不同。这里，<code class="fe oy oz pa pb b"><strong class="lp ir">C</strong></code>是一个超参数，它决定了最大限度地提高利润和最小化错误之间的权衡。当<code class="fe oy oz pa pb b"><strong class="lp ir">C</strong></code>较小时，分类错误的重要性较低，重点更多地放在最大化裕度上，而当<code class="fe oy oz pa pb b"><strong class="lp ir">C</strong></code>较大时，重点更多地放在避免错误分类上，代价是保持裕度较小。</p><p id="44e8" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">在这一点上，我们应该注意到，然而，并不是所有的错误都是相同的。与距离较近的数据点相比，远离决策边界错误一侧的数据点应该招致更多的惩罚。让我们看看如何在下图的帮助下实现这一点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/f9d5edde0617bb73a7ee7d071e7ebbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M_3iYollNTlz0PVn5udCBQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 3: The penalty incurred by data points for being on the wrong side of the decision boundary</figcaption></figure><p id="b053" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">想法是:对于每个数据点<code class="fe oy oz pa pb b"><strong class="lp ir">x_i</strong></code>，我们引入一个松弛变量<code class="fe oy oz pa pb b"><strong class="lp ir">ξ_i</strong></code>。<code class="fe oy oz pa pb b"><strong class="lp ir">ξ_i</strong></code> <strong class="lp ir"> </strong>的值是<code class="fe oy oz pa pb b"><strong class="lp ir">x_i</strong></code>到<em class="mj">对应类的边距</em>的距离，如果<code class="fe oy oz pa pb b"><strong class="lp ir">x_i</strong></code>在边距的错误一侧，否则为零。因此，在错误一侧远离边缘的点将得到更多的惩罚。</p><p id="5f3c" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">有了这个想法，每个数据点<code class="fe oy oz pa pb b"><strong class="lp ir">x_i</strong></code> <strong class="lp ir"> </strong>需要满足以下约束:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b499462cfdaf2306eb4a3de2aed99ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*qE4Qi4p0rRjiBFXhYjxwIA@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 2</figcaption></figure><p id="a402" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">这里，不等式的左边可以被认为是分类的置信度。置信度得分≥ 1 表明分类器对该点进行了正确分类。但是，如果置信度得分&lt; 1, it means that classifier did not classify the point correctly and incurring a linear penalty of  【T11】 <strong class="lp ir">。</strong></p><p id="76dc" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">鉴于这些限制，我们的目标是最小化以下功能:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/0748ed1c854ba02d935527df8ed9085e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qvl4L0qJk_fVDp7kv541kw@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 3</figcaption></figure><p id="41d0" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">其中我们使用了<a class="ae pk" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">拉格朗日乘数</a>的概念来优化约束条件下的损失函数。让我们把它与 SVM 处理线性可分情况的目标相比较(如下所示)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/afe6edc78297c7e299aeccd5b2cedeeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gMwNaK5Nk-bC8nq-Z-OIUw@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 4</figcaption></figure><p id="74c8" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">我们看到，在修改后的目标中，只有<code class="fe oy oz pa pb b"><strong class="lp ir">ξ_i</strong></code> <strong class="lp ir"> </strong>项是额外的，其他都是相同的。</p><blockquote class="pm pn po"><p id="745e" class="ln lo mj lp b lq nt jr ls lt nu ju lv pp nv ly lz pq nw mc md pr nx mg mh mi ij bi translated">注意:在最终解决方案中，对应于最接近边缘和边缘错误侧的点(即具有非零的<code class="fe oy oz pa pb b"><strong class="lp ir">ξ_i</strong></code>)的<code class="fe oy oz pa pb b"><strong class="lp ir">λ_i</strong></code>将是非零的，因为它们在决策边界的定位中起关键作用，本质上使它们成为支持向量。</p></blockquote><h1 id="2e8c" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">内核技巧</h1><p id="529b" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在让我们探索使用“内核技巧”来解决线性不可分性问题的第二种解决方案。但首先，我们应该了解什么是核函数。</p><h2 id="4b31" class="ny kw iq bd kx nz pc dn lb ob pd dp lf lw pe oe lh ma pf og lj me pg oi ll oj bi translated">核函数</h2><p id="4ac1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">核函数是将两个向量(任何维度)作为输入并输出表示输入向量相似程度的分数的广义函数。你已经知道的一个简单的核函数是点积函数:如果点积小，我们得出向量不同的结论，如果点积大，我们得出向量更相似的结论。如果您有兴趣了解其他类型的内核函数，<a class="ae pk" href="https://en.wikipedia.org/wiki/Kernel_method#Popular_kernels" rel="noopener ugc nofollow" target="_blank">这个</a>将是一个很好的来源。</p><h2 id="3a4b" class="ny kw iq bd kx nz pc dn lb ob pd dp lf lw pe oe lh ma pf og lj me pg oi ll oj bi translated">“诡计”</h2><p id="c547" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们看看线性可分情况的目标函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ps"><img src="../Images/75bfe6cf2c440c773fc1b3cae596bca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZynYYDjqVnm6LcoUYJWbA@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 5</figcaption></figure><blockquote class="pm pn po"><p id="51f7" class="ln lo mj lp b lq nt jr ls lt nu ju lv pp nv ly lz pq nw mc md pr nx mg mh mi ij bi translated">这是<code class="fe oy oz pa pb b">equation 4</code>中物镜的修改形式。这里，我们代入了<code class="fe oy oz pa pb b"><strong class="lp ir">w</strong></code>和<code class="fe oy oz pa pb b"><strong class="lp ir">b</strong></code>的最佳值。这些最佳值可以通过对这些参数求微分<code class="fe oy oz pa pb b">equation 4</code>并使其等于 0 来计算。</p></blockquote><p id="fb76" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">我们可以从<code class="fe oy oz pa pb b">equation 5</code>中观察到，目标依赖于输入向量对的点积(<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">x_i . x_j</em></strong></code>)，它只不过是一个核函数。现在这里有一件好事:<em class="mj">我们不必局限于像点积</em>这样简单的内核函数。我们可以使用任何<a class="ae pk" href="https://en.wikipedia.org/wiki/Kernel_method#Popular_kernels" rel="noopener ugc nofollow" target="_blank">花哨的核函数</a>来代替点积，它具有在更高维度中测量相似性的能力(在这种情况下，它可能更精确；稍后将详细介绍)，而不会增加太多的计算成本。这基本上被称为<em class="mj">内核技巧</em>。</p><h2 id="9bc1" class="ny kw iq bd kx nz pc dn lb ob pd dp lf lw pe oe lh ma pf og lj me pg oi ll oj bi translated">它是如何工作的(数学上)？</h2><p id="77f3" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">核函数可以用数学方法写成如下形式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/23ebd6cee6eb14073a883d69ecb13c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*RVVEKTBZS6Jo8ymc3F_L3w@2x.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 6</figcaption></figure><p id="7de6" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">这里<code class="fe oy oz pa pb b"><strong class="lp ir">x</strong></code>和<code class="fe oy oz pa pb b"><strong class="lp ir">y</strong></code>是输入向量，<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">ϕ</em></strong></code>是变换函数，<code class="fe oy oz pa pb b">&lt; , &gt;</code>表示点积运算。在点积函数的情况下，<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">ϕ</em></strong></code> <strong class="lp ir"> <em class="mj"> </em> </strong>只是将输入向量映射到自身。</p><blockquote class="mk"><p id="92cc" class="ml mm iq bd mn mo mp mq mr ms mt mi dk translated">核函数本质上采用变换后的输入向量的点积。</p></blockquote><p id="2e2a" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw mw ly lz ma mx mc md me my mg mh mi ij bi translated">现在让我们考虑下面<code class="fe oy oz pa pb b">figure 4</code>中描述的情况。我们看到，在 2d 空间中没有可以完美分离数据点的线性决策边界。圆形(或二次型)决策边界可能可以完成这项工作，但是，线性分类器无法得出这些类型的决策边界。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pu"><img src="../Images/cbea3ba1caaedee4c1881436d1f0c1b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1A_qbWnfHXy8XCEs0GVrBQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 4: Points in 2D space are separable by a circular decision boundary.</figcaption></figure><p id="c3e9" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">在<code class="fe oy oz pa pb b">figure 4</code>中，每个点<code class="fe oy oz pa pb b">P</code>由 2D 空间中形状<code class="fe oy oz pa pb b">(x,y)</code>的特征表示。查看理想的决策边界，我们可以将点<code class="fe oy oz pa pb b">P</code>的转换函数<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">ϕ</em></strong></code>定义为<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">ϕ</em></strong>(P) = (x^2, y^2, √2xy)</code>(我们为什么要进行这样的转换一会儿就清楚了)。让我们看看对于两点<code class="fe oy oz pa pb b">P_1</code>和<code class="fe oy oz pa pb b">P_2</code>的这种类型的变换，核函数是什么样子的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pv"><img src="../Images/28c08110ee11be8db149839671d433b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WfC4vVmM78VArPGe6k8tOQ@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 7</figcaption></figure><p id="800d" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">如果我们观察内核函数的最终形式，它无非是<a class="ae pk" href="https://www.mathsisfun.com/algebra/circle-equations.html" rel="noopener ugc nofollow" target="_blank">一个圆</a>！这意味着我们改变了相似性的概念:我们不再通过点的接近程度(使用点积)来衡量相似性，而是根据点是否在一个圆内来衡量相似性。在这个意义上，定义这样的变换允许我们在 2D 空间中有一个<em class="mj">非线性决策边界</em>(它在原始 3D 空间中仍然是线性的)<em class="mj">。</em>要跟踪的内容可能很多，因此以下是我们所做决定的简要总结:</p><pre class="kg kh ki kj gt pw pb px py aw pz bi"><span id="70c9" class="ny kw iq pb b gy qa qb l qc qd"><strong class="pb ir">1</strong> - Each point P is represented by (<strong class="pb ir">x</strong>,<strong class="pb ir">y</strong>) coordinates in 2D space.</span><span id="d9d7" class="ny kw iq pb b gy qe qb l qc qd"><strong class="pb ir">2</strong> - We project the points to 3D space by transforming their coordinates to <!-- -->(<strong class="pb ir">x^2</strong>, <strong class="pb ir">y^2</strong>, <strong class="pb ir">√2xy</strong>)</span><span id="fac9" class="ny kw iq pb b gy qe qb l qc qd"><strong class="pb ir">3</strong> - <!-- -->Points which have high value of <strong class="pb ir">x</strong>.<strong class="pb ir">y</strong> <!-- -->would move upwards along the z-axis (in this case, mostly the red circles). <a class="ae pk" href="https://www.youtube.com/watch?v=3liCbRZPrZA" rel="noopener ugc nofollow" target="_blank">This video</a> provides a good visualization of the same.</span><span id="64b8" class="ny kw iq pb b gy qe qb l qc qd"><strong class="pb ir">4</strong> - We find a hyperplane in 3D space that would perfectly separate the classes.</span><span id="d31e" class="ny kw iq pb b gy qe qb l qc qd"><strong class="pb ir">5</strong> - The form of Kernel function indicates that this hyperplane would form a circle in 2D space, thus giving us a non-linear decision boundary.</span></pre><p id="a7e1" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">主要的要点是:</p><blockquote class="mk"><p id="062c" class="ml mm iq bd mn mo mp mq mr ms mt mi dk translated">通过将数据嵌入到更高维的特征空间中，我们可以继续使用线性分类器！</p></blockquote><p id="f76a" class="pw-post-body-paragraph ln lo iq lp b lq mu jr ls lt mv ju lv lw mw ly lz ma mx mc md me my mg mh mi ij bi translated">这里需要注意的是，这些变换可能会大幅增加特征空间，从而增加计算成本。有什么方法可以在不增加计算成本的情况下获得上述好处呢？原来是有的！</p><p id="bbc1" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">让我们尝试重写<code class="fe oy oz pa pb b">equation 7</code>中的内核函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qf"><img src="../Images/779d82ab306c9bf6455f6d15405551ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GaiU7CoXED7y4ka2qWsWFg@2x.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">equation 8</figcaption></figure><p id="e406" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">哇哦。因此，核函数的值(因此，3D 空间中的点之间的相似性)正好是 2D 空间中的点之间的点积的平方。很棒，对吧？！但是这是怎么发生的呢？</p><p id="9442" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">原因是我们明智地选择了我们的转换函数<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">ϕ</em></strong></code> <strong class="lp ir"> <em class="mj"> </em> </strong>。只要我们继续这样做，我们就可以绕过变换步骤，直接从 2D 空间中的点之间的相似性来计算核函数值。这反过来也会抑制计算成本。我们有许多流行的<a class="ae pk" href="https://en.wikipedia.org/wiki/Kernel_method#Popular_kernels" rel="noopener ugc nofollow" target="_blank">内核函数</a>，它们都有这种良好的特性，可以开箱即用(我们不需要搜索完美的<code class="fe oy oz pa pb b"><strong class="lp ir"><em class="mj">ϕ</em></strong></code>)。</p><h1 id="0cbf" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结束语</h1><p id="ae56" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">到此为止，我们已经完成了这篇文章。希望本文提供的细节能让您深入了解是什么让 SVM 成为一个强大的线性分类器。如果你有任何问题或建议，请在评论中告诉我。干杯！🥂</p></div><div class="ab cl qg qh hu qi" role="separator"><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql qm"/><span class="qj bw bk qk ql"/></div><div class="ij ik il im in"><p id="ff2b" class="pw-post-body-paragraph ln lo iq lp b lq nt jr ls lt nu ju lv lw nv ly lz ma nw mc md me nx mg mh mi ij bi translated">如果你喜欢这篇文章，并对我未来的努力感兴趣，可以考虑在 Twitter 上关注我:<a class="ae pk" href="https://twitter.com/rishabh_misra_" rel="noopener ugc nofollow" target="_blank">https://twitter.com/rishabh_misra_</a></p></div></div>    
</body>
</html>