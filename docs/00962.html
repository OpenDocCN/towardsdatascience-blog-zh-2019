<html>
<head>
<title>Reinforcement Learning Tutorial Part 2: Cloud Q-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习教程第 2 部分:云 Q 学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-tutorial-part-2-cloud-q-learning-fc101e1cdcf5?source=collection_archive---------8-----------------------#2019-02-13">https://towardsdatascience.com/reinforcement-learning-tutorial-part-2-cloud-q-learning-fc101e1cdcf5?source=collection_archive---------8-----------------------#2019-02-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/fb6b9a373e7218d6dde2d2232fa311ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILWsbK7sjK1EcC5RhP0TnA.jpeg"/></div></div></figure><p id="6827" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在第一部分中，我们用一个非常简单的地牢游戏研究了 Q-learning 背后的理论，这个游戏有两个策略:会计和赌徒。第二部分使用 Valohai 深度学习管理平台，将这些例子转化为 Python 代码，并在云中训练它们。</p><p id="59f9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">由于我们示例的简单性，我们不会故意使用任何像 TensorFlow 这样的库或像 OpenAI Gym 这样的模拟器。相反，我们将从头开始自己编写代码，以提供完整的画面。</p><p id="a087" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">所有的示例代码都可以在 https://github.com/valohai/qlearning-simple 的<a class="ae kw" href="https://github.com/valohai/qlearning-simple" rel="noopener ugc nofollow" target="_blank">找到</a></p><h1 id="4846" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">代码布局</h1><p id="d0cb" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">典型的强化学习代码库有三个部分:</p><ul class=""><li id="b464" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">模拟</li><li id="f434" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">代理人</li><li id="d1a0" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">管弦乐编曲</li></ul><p id="af92" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">模拟</strong>是代理人操作的环境。它根据其内部规则和代理执行的动作更新状态并分发奖励。</p><p id="e6d9" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> Agent </strong>是在模拟中操作、学习和执行策略的实体。它执行动作，接收带奖励的状态更新，并学习一种策略来最大化它们。</p><p id="ab7a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">编排</strong>是所有剩余的样板代码。解析参数，初始化环境，在模拟和代理之间传递数据，最后关闭商店。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/61a746dea4643b0133f3c3edc834fa23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6gFwplGxUTEzqSv4QL_lQ.png"/></div></div></figure><h1 id="5b2c" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">模拟</h1><p id="d739" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们将使用第 1 部分中介绍的地牢游戏。这些是规则:</p><ul class=""><li id="7767" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">地牢有 5 块瓷砖长</li><li id="7a10" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">可能的操作有向前和向后</li><li id="dfae" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">向前总是一步，除了在最后一个瓷砖碰到墙</li><li id="03af" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">后退总是带你回到起点</li><li id="412c" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">有时会有风把你的动作吹向相反的方向</li></ul><p id="2d01" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">代理未知:</p><ul class=""><li id="d1c2" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">进入最后一张牌会给你+10 奖励</li><li id="5c29" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">进入第一个方块给你+2 奖励</li><li id="6a5c" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">其他瓷砖没有奖励</li></ul><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h1 id="29d2" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">代理人</h1><p id="2169" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在教程的<a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第一部分，我们介绍了两种策略:会计和赌徒。会计只关心过去的业绩和保证的结果。赌徒更渴望探索风险更高的选项，以在未来获得更大的回报。也就是说，为了保持第一个版本的简单，引入了第三个策略:酒鬼。</a></p><p id="cc23" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">酒鬼是我们的基本策略，有一个非常简单的启发:无论如何都要随机行动。</p><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h1 id="bbc8" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">管弦乐编曲</h1><p id="7769" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">现在我们已经有了模拟和代理，我们需要一些样板代码将它们粘在一起。</p><p id="d00a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">地牢模拟是回合制的，所以流程很简单:</p><ul class=""><li id="a46f" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">向代理查询下一个动作，并将其传递给模拟</li><li id="b6cb" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">获取新的状态和奖励，并将其传递给代理</li><li id="4067" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">重复</li></ul><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><h1 id="b15f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">基础设施</h1><p id="d8bd" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">虽然有可能在本地运行这样的简单代码，但目标是提供一个如何正确编排更复杂项目的示例。</p><p id="9f1e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">机器学习不等于软件开发。仅仅将代码推送到 Git 存储库是不够的。为了完全的再现性，您需要对数据、超参数、执行环境、输出日志和输出数据进行版本控制。</p><p id="3c0a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在本地运行也会让你慢下来。虽然玩具示例通常会给人一种快速迭代的错觉，但为真实问题训练真实模型并不适合您的笔记本电脑。毕竟，它一次只能运行一个训练执行，而基于云的环境让你可以同时启动几十个训练执行，同时拥有快速的 GPU、轻松的比较、版本控制和对同事更大的透明度。</p><h1 id="1479" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">瓦罗海设置</h1><p id="8bfb" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">前往<a class="ae kw" href="https://valohai.com/" rel="noopener ugc nofollow" target="_blank">valohai.com</a>并创建一个账户。默认情况下，您的帐户上有价值 10 美元的信用，足以在云实例上运行以下培训。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/bbfaf28bae55106a3f36d2d396bb7937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kESmacKTdzOmSHB28S2Yew.gif"/></div></div></figure><p id="bdc5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">注册后，点击<em class="mw">跳过教程</em>或者打开右上角的<em class="mw">项目</em>下拉菜单，选择<em class="mw">创建项目</em>。</p><p id="623e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">给你的项目起一个有意义的名字，然后点击下面的蓝色<em class="mw">创建项目</em>按钮。</p><p id="defe" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在 https://github.com/valohai/qlearning-simple<a class="ae kw" href="https://github.com/valohai/qlearning-simple" rel="noopener ugc nofollow" target="_blank">的示例 git 存储库中，我们已经有了我们的代码和 valohai.yaml。</a></p><p id="e241" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们只需要告诉瓦罗海他们在哪里。点击上面截图中的<em class="mw">项目库设置</em>链接或导航到<em class="mw">设置&gt;库</em>选项卡。</p><p id="89bd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">将存储库 URL 粘贴到<em class="mw"> URL </em>字段，确保获取引用为“主”，然后点击<em class="mw">保存</em>。</p><h1 id="22a4" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">执行！</h1><p id="7a4b" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">一切就绪，现在是我们第一次执行的时候了。</p><p id="0122" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">点击 Valohai 项目<em class="mw">执行</em>选项卡中的蓝色<em class="mw">创建执行</em>按钮。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mv"><img src="../Images/92d1e41a22d3975ebe0bf3fe9da5c3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*r8hPE69oAEy2JF8lkRywpw.gif"/></div></div></figure><p id="ed33" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Git 存储库中的 valohai.yaml 文件为我们提供了所有正确的默认值。如果你想了解更多关于<em class="mw"> valohai.yaml </em>的信息，请查阅<a class="ae kw" href="https://docs.valohai.com/valohai-yaml.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><p id="ebb0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">点击底部的蓝色<em class="mw">创建执行</em>按钮。</p><p id="f8ea" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在 Valohai 中运行一个执行意味着大致会发生以下序列:</p><ul class=""><li id="8f43" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">启动一个新的服务器实例</li><li id="6ec7" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">您的代码和配置(valohai.yaml)是从 git 存储库中获取的</li><li id="4f37" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">下载训练数据</li><li id="3bf0" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">下载并实例化 Docker 映像</li><li id="c8f5" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">基于<em class="mw"> valohai.yaml </em> +您的参数执行脚本</li><li id="5ae9" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">所有标准输出都存储为 log + JSON 格式的元数据，并针对图表进行解析</li><li id="4737" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">为了以后的可再现性，整个环境是受版本控制的</li><li id="ac44" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">脚本完成后，服务器实例会自动关闭</li></ul><p id="2d20" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">只需点击一下鼠标，你就能得到如此多的东西，这难道不令人惊奇吗？</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/0a568f6eb27bfcdae963bd4a6660d7d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JlRMCAiJHKJ1vgxOVVEpUA.png"/></div></div></figure><p id="13de" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">如果您前往执行的<em class="mw">元数据</em>标签，您可以看到我们的酒鬼策略代理如何收集奖励。毫不奇怪，采取随机行动的策略会产生一条几乎有轻微抖动的直线，这告诉我们没有学习发生。</p><h1 id="6372" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">会计</h1><p id="dd03" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在运行了基线随机策略之后，是时候带回在教程的<a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第一部分中介绍的两个原始策略了:会计和赌徒。</a></p><p id="980f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">会计师有以下策略:</p><ul class=""><li id="21a8" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">总是选择基于会计的最有利可图的行动</li><li id="adae" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">如果所有选项都为零，则选择一个随机操作</li></ul><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="3ddd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">要执行会计，记得更改执行的代理参数，然后再次单击蓝色的<em class="mw">创建执行</em>。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi my"><img src="../Images/4a15198a4d5133cf2869dc42a69fbedd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*URP-0MC_FsZnc7xhQdsbmg.png"/></div></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mx"><img src="../Images/58250ac0600568eb42635adeaede5e64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVnijxpJCZ7t1oevQOaH7g.png"/></div></div></figure><p id="af98" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">查看<em class="mw">元数据</em>选项卡，会计在学习方面也好不到哪里去。曲率甚至更直，因为在最初的几次迭代后，它将学会总是选择向后，以获得一致的+2 奖励，而不是像随机的醉汉一样摇摆。</p><p id="da54" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">不过，总奖励大约是+4000。至少有所改善！</p><h1 id="9faf" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">赌徒</h1><p id="e39b" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">从我们的三个代理人，赌徒是唯一一个做真正的 Q 学习。还记得我们在<a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第一部分</a>中的算法吗:</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mz"><img src="../Images/a2a24c1ed0c9061628d758ae08110e6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-F8AfutP0a8gPWs_5BBLQ.png"/></div></div></figure><p id="4711" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">该战略如下:</p><ul class=""><li id="d284" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">默认情况下，从我们的 Q 表中选择最有利可图的行动</li><li id="46ee" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">有时赌博并选择随机行动</li><li id="e1a7" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">如果 Q 表显示两个选项都为零，选择一个随机动作</li><li id="9ef1" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">从 100%赌博(探索)开始，慢慢向 0%(剥削)移动</li></ul><figure class="mp mq mr ms gt jr"><div class="bz fp l di"><div class="mt mu l"/></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi na"><img src="../Images/1ee240d5045169ebc5e17852a9e0068a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fir2pEqd9snXGU0U1mUz5g.png"/></div></div></figure><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nb"><img src="../Images/6b27d7715d8fbcaa1a5d34d9f46022cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWbL9ZicDKjhyjkX1I6K5g.png"/></div></div></figure><p id="0760" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">最后，我们可以看到一些真正的 Q 学习正在发生！</p><p id="d114" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在以高探索率缓慢开始后，我们的 Q 表充满了正确的数据，与我们的其他代理相比，我们达到了几乎两倍的总回报。</p><h1 id="a492" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">并行执行</h1><p id="7ede" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">到目前为止，我们一次只运行一个执行，如果您想快速迭代您的模型探索，这并不能很好地扩展。在我们最后的努力中，我们将尝试运行 Valohai 任务，而不是一次性执行。在 Valohai 中，一个任务仅仅意味着执行一组具有不同超参数的执行。</p><p id="b125" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">切换到 Valohai 中的<em class="mw">任务</em>选项卡，然后点击蓝色<em class="mw">创建任务</em>按钮。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nc"><img src="../Images/05df701f1aca74f876d8f7f6fb20664e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tdBx4qwhyq-8QlE8"/></div></div></figure><p id="35c0" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们看看<em class="mw"> learning_rate </em>参数如何影响我们的性能。选择<em class="mw">倍数</em>，然后在不同行输入三个不同的选项(0.01，0.1，0.25)。最后，点击底部蓝色的<em class="mw">创建任务</em>开始执行。</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/98a0f270010ee62b0d1537bb5807c887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*WjFwDgl_JMc9IbSnguV-Cg.gif"/></div></div></figure><p id="f56c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">Valohai 现在将并行启动不是一个而是三个服务器实例。每个带有数字的方框代表一个具有不同 learning_rate 的运行实例。一旦颜色变成绿色，就意味着执行已经完成。您还可以在执行过程中实时查看元数据图表！</p><figure class="mp mq mr ms gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mo"><img src="../Images/f34d5fadeb469bc0208a8371313e7c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0An8nRkBmTleQ41TJb5xkQ.png"/></div></div></figure><p id="8f00" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">下面是<em class="mw"> learning_rate </em>三个不同值的奖励。看起来我们最初的 0.1 比 0.25 表现得好一点。使用 0.01 显然更糟，并且对于 10000 次迭代似乎根本学不到任何东西。</p><p id="2246" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在我们的下一部分中，我们将更深入地研究并行执行，以加速我们的迭代，并看看如何有效地使用 Valohai 来搜索那些最优的超参数。敬请期待！</p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><p id="480f" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-part-1-q-learning" rel="noopener ugc nofollow" target="_blank">第一部分:Q-Learning </a></p><p id="5725" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-cloud-q-learning" rel="noopener ugc nofollow" target="_blank">第二部分:云 Q 学习</a></p><p id="45ba" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-basic-deep-q-learning" rel="noopener ugc nofollow" target="_blank">第三部分:基础深度 Q 学习</a></p></div><div class="ab cl ne nf hu ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="ij ik il im in"><p id="8564" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><em class="mw">原载于</em><a class="ae kw" href="https://blog.valohai.com/reinforcement-learning-tutorial-cloud-q-learning" rel="noopener ugc nofollow" target="_blank"><em class="mw">blog.valohai.com</em></a><em class="mw">。</em></p></div></div>    
</body>
</html>