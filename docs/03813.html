<html>
<head>
<title>Your Friendly, Neighborhood Superintelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你友好的邻居的超级智慧</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/your-friendly-neighborhood-superintelligence-f905ff21dfa4?source=collection_archive---------16-----------------------#2019-06-16">https://towardsdatascience.com/your-friendly-neighborhood-superintelligence-f905ff21dfa4?source=collection_archive---------16-----------------------#2019-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/4669ee39716c61a577f63e595b11e3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVVNzU3Ep-1dOm8bO9AxrA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Martin, The Plains of Heaven</figcaption></figure><div class=""/><div class=""><h2 id="9722" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">“如果类人猿问他们是否应该进化成智人(并说)‘哦，如果我们变成人类，我们会有很多香蕉’会怎么样？”—尼克·博斯特罗姆</h2></div><p id="9d38" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们的一个老故事是恶魔召唤变坏了。他们总是这样。我们的祖先想警告我们:小心那些提供无限力量的生物。这些天来，我们依赖于预示未来恶魔到来的计算实体——强大的人类水平的人工智能。幸运的是，我们的圣贤们，比如尼克·博斯特罗姆，现在正在思考比恶魔故事中的安抚、猴子的聪明和更强的魔法束缚更好的控制方法。在回顾了当前的想法后，我将提出一个更安全地获得人工智能超能力的协议。</p><h2 id="9fdf" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">智力爆炸的困境。</h2><p id="a64d" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">我们看到<a class="ae mr" rel="noopener" target="_blank" href="/life-after-death-as-a-parrot-92e1e731c61e">指针在 AI 上移动</a>。许多，但不是所有的专家，部分受博斯特罗姆出版的书《T2 超级智慧》的激励，相信下面的场景。</p><p id="f03b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在我们实现一种“强大的人工智能”之后，这种脱节将会发生，这种人工智能是如此全面，就像一个真正聪明的人。人工智能将变得越来越聪明，因为它吸收了越来越多的世界知识，并从中筛选，发现更好的思维模式和创意组合。</p><p id="f50d" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在某个时候，人工智能会重新设计自己，然后新的自我会重新设计自己——冲洗，重复——期待已久的智能爆炸就会发生。在这一点上，我们还不如吹口哨走过墓地，因为我们将无法控制接下来会发生什么。</p><p id="8afb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">博斯特罗姆说，我们可能会创造一个超级人工智能，类似于反复无常、可能怀有恶意的上帝。有害的组合是自主和无限的权力，正如他们在魔法时代所说的那样，<em class="ms">没有咒语将他们与对我们无害且有益的价值观或目标捆绑在一起。</em></p><h2 id="2241" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">人工智能动物园。</h2><p id="1b78" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">博斯特罗姆按照不断增强的能力、精神自主性和对我们造成伤害的风险来想象人工智能的类型:</p><p id="edfb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><strong class="kz jj"> {工具→甲骨文→精灵→主权} </strong>。</p><p id="7c06" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一个<strong class="kz jj">工具</strong>是可控的，就像今天的人工智能应用一样，但没有独创性，只能解决我们可以描述的问题，甚至可能解决我们自己更慢。一个<strong class="kz jj">神谕</strong>只会为我们解答难题，它自己没有任何目标。一个<strong class="kz jj">精灵</strong>会接受请求，想出答案或解决方案，然后实施它们，但之后会停下来等待下一个请求。一个主权国家就像一个永不停息的精灵，能够创造自己的目标。</p><p id="5774" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">博斯特罗姆展示了任何较弱的类型是如何演变成较强的类型的。人类总是想要更有效的帮助，因此给予他们的机器仆人更多的自由和权力。与此同时，这些仆人变得更有能力规划、操纵他人，并让自己变得更聪明:发展所谓的超能力，将他们推向主权范围的一端。</p><h2 id="3373" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">目标的问题。</h2><p id="3429" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">博斯特罗姆的一个主要担忧是超级智能拥有错误的“最终目标”。在为最终目标服务时，不知疲倦而强大的人工智能可能会伤害它的人类创造者，无论是否有意伤害或关心所述伤害。</p><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mt"><img src="../Images/439469b84664317d79289f47205d4d94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xjP8lH8yJBa-h9C2jjmwQw.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Degas, Autumn Landscape</figcaption></figure><p id="7a02" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">博斯特罗姆的例子包括将地球上所有的原子(当然也包括我们自己的原子)变成回形针，或者将所有可用的物质都用到了 computronium 中来确定一些目标计算，比如数沙粒。</p><p id="b293" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">选择这些例子是为了显示超级人工智能的行为可能是多么任意。从人类的角度来看，这种疯狂的、强迫性的行为是愚蠢的，而不是聪明的。但是，如果人工智能缺乏我们认为理所当然的保护生命和环境的价值观，这种情况就可能发生。如果我们能够将这样的价值观传递给人工智能，我们能指望它会随着时间的推移保留这些价值观吗？博斯特罗姆和其他人认为会的。</p><h2 id="4d60" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">作为身份的目标完整性。</h2><p id="2138" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">想想我们生命中有多少时间是用来为最终目标服务的，比如爱情、权力、财富、家庭、友谊、推迟我们自己的死亡和消灭我们的敌人。这是我们时间的很大一部分，但不是全部。我们也渴望休闲、新奇的刺激和快乐。我们所有人都在做白日梦和幻想。极少数人甚至为了学习和创造而学习和创造。如果我们的动机是复杂的，为什么超级人工智能不会如此呢？</p><p id="97a5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">可以说，人类水平的人工智能将被激励在精神上探索它的世界，我们的世界，寻找项目，改变的事物，联系和提炼的想法。为了在动态环境中良好运行，它还应该做出最大化未来行动自由的选择。这反过来需要更广泛的探索性知识。</p><p id="07cc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">尽管如此，我们应该像博斯特罗姆那样假设，人工智能的特定目标可能会被狂热地追求。那么，很大程度上取决于我们对人工智能目标的某种控制。给定一个可以重写自己的编程代码的实体，如何做到这一点呢？</p><p id="f171" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">博斯特罗姆的答案是，人工智能的本质，因为它是由可变的软件组成的，事实上，是它的最终目标集。人类改变他们的价值观和目标，但是一个人被锁在一个身体/头脑中，这个身体/头脑以各种方式构成了它的身份和自我。我们首要目标是自我的生存。</p><p id="8d92" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">另一方面，一个计算机化的代理由可以交换、借用、复制、重新开始和自我修改的可变部分组成。因此，系统并不是真正由这些部分构成的；它的本质是它的最终目标，博斯特罗姆称之为目的论(目标导向)线程。它的动机是不改变这些目标，因为这是确保他们在未来得到满足的唯一方法。</p><h2 id="0a2f" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">学习价值观:人类连贯的外推意志。</h2><figure class="mu mv mw mx gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/c07c7733159019e48de6ef0e73081b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yirKjviL4vocF9y_iIfTZA.jpeg"/></div></div><figcaption class="jc jd gj gh gi je jf bd b be z dk">Raphael, The School of Athens</figcaption></figure><p id="a447" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在最终目标稳定性的概念中有明显的循环，我读过的人中没有人认为超级人工智能的价值观真的不会改变。此外，人类的价值观是复杂的、动态的，而且常常是不相容的，所以我们不能相信任何一组开发人员会选择价值观并安全地将它们编程到我们的 AI 中。到目前为止，提出的解决方案依赖于让系统在一个过程中学习价值观，希望使它们:(1)对人类广泛可取和有帮助，(2)不太可能造成大破坏，例如人工智能“用笑脸贴上地球未来的光锥”或在每个人的大脑中安装愉悦电极，以及(3)构成一个起点和方向，在价值观和目标演变时保留这些品质。</p><p id="0cf4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">反复出现的一个技巧是对初始人工智能(通常称为“种子人工智能”)进行编程，其主要目标是找出与人类兼容的值应该是什么。这为一个非常困难的问题应用了新的智能，这意味着对所述值的持续改进是系统本质的一部分。</p><p id="0da5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一篇早期的论文(“<a class="ae mr" href="https://intelligence.org/files/CEV.pdf" rel="noopener ugc nofollow" target="_blank">连贯的外推意志</a>”)，它机智、独到、精彩；机器智能研究所的 Eliezer Yudkowsky 称这种学习过程的目标为“连贯的外推意志”或 CEV。<em class="ms">意志</em>因为这是我们真正想要的，而不是我们通常所说的<em class="ms">T5】我们想要的，这被自我和社会欺骗所掩盖。Yudkowsky 说:“如果你找到一个能给你三个愿望的精灵瓶子…把精灵瓶子密封在一个上锁的保险箱里…除非精灵关注你的意志，而不仅仅是你的决定。”<em class="ms">外推</em>因为要搞清楚。因为它需要在我们无数的意识形态、政治、社会、宗教和道德体系背后找到共同点。</em></p><h2 id="b933" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">CEV 是否可能。</h2><p id="f50f" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">也许你会被一个 CEV 可能认可的人类行为规范吓到，你会发现这些规范是完全排斥的，然而这些规范却被相当一部分人类所接受。同样，对于其他人来说，你的想法是令人厌恶的。对于我们在这些问题上的分歧，假定的解决办法是 CEV 的“连贯”方面。也就是说，通过更好的思考和更广泛的知识，人工智能可以找到我们的道德共同点，并以我们足够多的人可以同意的方式表达它。我想到了“猪食”这个词，我想知道是否有哲学家已经在写一本关于为什么 CEV 永远行不通的书了。但是让我们更深入地看看。</p><p id="62a5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">Yudkowsky 把目标放在他(作为一个 AI 硬汉)所谓的诗意术语中:“(一个 CEV 是)我们的愿望是，如果我们知道得更多，思考得更快，更像我们希望的那样，在一起成长得更远；外推收敛而不是发散，我们的愿望一致而不是干涉；按照我们希望的那样推断，按照我们希望的那样解释。”</p><p id="8f7f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">因此，请想一想，我们实际上从未尝试过这样的事情:一位受人尊敬的杰出思想家考虑了我们所有不同的道德关切，并找到了所有国家和信仰中最受尊敬的人都能认同的文明原则。如果你在写剧本，这种尝试将会失败，直到人工智能给人类一些有巨大价值的礼物，一个古老问题的解决方案。这使得公众对人工智能的态度发生了翻天覆地的变化，它现在被视为恩人，而不是魔鬼。这个比喻暗示了高级人工智能项目开放和国际化的必要性。</p><p id="bd7e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">自从 Yudkowsky 的第一篇论文以来，CEV 的想法已经获得了关注，无论是在博斯特罗姆的重要著作中还是在其他地方(Steve Petersen，<a class="ae mr" href="https://core.ac.uk/download/pdf/187354645.pdf" rel="noopener ugc nofollow" target="_blank">super intelligence as super ethical</a>)。思想家们试图确定 CEV 过程的初始机制，如指定目标内容的方法、决策理论、知识理论及其现实世界基础，以及人工智能 CEV 发现的人类认可程序。对于 CEV 来说，如果没有一个好的伦理一致性理论，这些哲学技巧仍然是无用的。即，如何在各种“关于世界如何的信念和关于世界应该如何的愿望”之间找到一致性(Petersen，见上文)显然，一个强大的人工智能项目的成功和后果现在不仅取决于深奥的计算机科学，还取决于实用且非常重要的哲学。</p><h2 id="6064" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">思维主义、科学和超能力。</h2><p id="08d0" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">人们常说，智能爆炸会发生得如此之快，以至于我们没有时间以任何方式做出反应或引导它。这种信念被凯文·凯利嘲笑为依赖于“思维主义”，即智力本身可以导致进步的爆发。根据凯利的说法，思维主义忽略了一个已经证明的事实:新的力量需要关于自然的新知识，而你必须做科学——包括观察和实验——来获得新知识。科学需要时间和物质资源。一个新的人工智能，目标是让自己变得更好，或者解决任何困难的问题，它将如何进行科学研究？</p><p id="e5fa" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">博斯特罗姆认为人工智能有很多“超能力”可以被激发去发展。这些将是有用的，因为它们将服务于许多其他目标和项目。一个超级大国是技术研究，这也是科学研究的推动者和范例。</p><h2 id="8efe" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">避免什么:作为邦德反派的人工智能恶魔。</h2><p id="041f" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">假设我们决定通过严格限制人工智能对物理世界的访问来限制它，实际上让它依赖我们来执行(但可能不是设计)技术研究。这使得它只剩下另外四个(博斯特罗姆确定的)超级大国来开始进行新的研究:制定战略、社会操纵、系统黑客和经济生产率。一个物理上孤立的人工智能可以通过学习人类历史、心理学和政治方面的记录知识来自学如何操纵人。然后，它将使用先进的战略来间接获得对资源的控制，然后使用这些资源作为一个楔子来提高其他超级大国。</p><p id="dab1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">就像詹姆斯·邦德故事中的反派一样，人工智能可能会首先培养自己的真正信徒、理想主义者、愤世嫉俗者和反社会者。这些将有助于它获得和操作可能的秘密实验室和其他企业。它可以绕过一些繁琐的科学研究:不需要发表论文和同行评议，没有拨款申请和进展报告。但是仅仅这样还不足以让超级人工智能加快速度。它将需要开发自己的机器人，并可能找到一种方法来创造僵尸人类，以应对正常人的反对。</p><h2 id="f106" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">一个安全的甲骨文人工智能的愿景。</h2><p id="9254" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">假设一个人工智能可以与物理资源的直接访问充分隔离，并且它的通信只能发生在公共信道上。后一种情况将防止秘密操纵人，也将减少人类对人工智能偏袒特定派别的恐惧。此外，假设一个工具级的半人工智能(也锁定到公共专用频道)可用于分析公开通信，以寻找隐藏消息的证据，这些隐藏消息可能被种子人工智能用来操纵盟友或骗子释放它。这种人工智能的人类治理将保留能力和权力，以在出现问题时物理地关闭它。</p><h2 id="c1d1" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">它能为我们做什么。</h2><p id="81a1" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">这样的人工智能可能被认为是一个安全的先知和整个世界的顾问。它可以被允许以只读方式访问人类知识和艺术的很大一部分。它将开始了解我们的最佳技术:从现有知识中产生想法、外推推理、战略规划和决策理论，以帮助它回答对人类福祉至关重要的问题。</p><p id="bc7f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">也许对于一个先知来说，首要的任务应该是对有助于 CEV 的步骤提出建议，包括更好的理论，比如价值澄清和决策，以及一系列哲学问题。这些任务的结果，以及随之而来的 CEV 的第一稿，就像是人类/人工智能文明的结合体。它可以让我们以更安全、更缓慢、非爆炸性的方式起飞，达到博斯特罗姆和其他深层思想家所希望的超级智能。</p><p id="d0c1" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">在其开发过程中的某个时刻，甲骨文人工智能会给我们提供关于如何处理环境、政治、社会和经济问题的深思熟虑、解释清楚的意见。关于设定 R&amp;D 优先事项的建议将有助于我们避免其他各种生存风险，如气候、纳米技术、核战争和小行星。甲骨文将继续完善它或它的继任者人工智能可以保持的价值和动机框架，如果我们决定让它走出封闭的盒子，到野外去。</p><h2 id="dc16" class="lt lu ji bd lv lw lx dn ly lz ma dp mb lg mc md me lk mf mg mh lo mi mj mk ml bi translated">减缓爆炸。</h2><p id="853e" class="pw-post-body-paragraph kx ky ji kz b la mm kj lc ld mn km lf lg mo li lj lk mp lm ln lo mq lq lr ls im bi translated">公共 oracle 方法允许我们获得“人类级别”人工智能的更好思维和更广泛信息覆盖的早期优势。它试图将人类的派系斗争和利益争夺最小化。一个公开的甲骨文项目有很多障碍。首先，它必须是第一个成功的强人工智能。如果一个不太安全的项目首先发生，公共预言可能永远不会发生。博斯特罗姆提出了一个强有力的案例，即第一个人工通用智能如果没有足够的控制或良性的最终目标，将会爆炸成一个“单体”，一个几乎控制一切的非道德实体。不只是一个恶魔，而是一个不受约束的神。</p><p id="b4b0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">任何爆炸性的起飞对人类来说都是不祥之兆。获得较慢起飞的最佳方式是一个包括许多人的项目，他们必须在采取每一个重要步骤之前达成一致。可能只有当我们已经从其他一些生存挑战中幸存下来，比如濒临环境崩溃的时候，我们才会有这种合作的动机。</p><p id="2c63" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一个缓慢的项目有一个缺点:它可能会抢先到达终点。博斯特罗姆已经确定了(“<a class="ae mr" href="https://nickbostrom.com/papers/openness.pdf" rel="noopener ugc nofollow" target="_blank">人工智能开发中开放的战略含义</a>”)这里的另一个转折。一个缓慢的、包容的项目需要公开它的开发过程。开放可以增加竞争项目的可能性和竞争力。安全需要时间，所以最不关注安全的项目赢得了强人工智能的比赛，然后“啊哈！”。同样，一个单一的缓慢而包容的项目预示着最好的结果。</p><p id="0195" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一个安全友好的强人工智能的挑战可以是富有成效的，即使我们距离制造一个人工智能还有几十年的时间。它迫使我们寻找更好的关于如何成为一个成熟物种的想法；如何不总是搬起石头砸自己的脚？</p></div></div>    
</body>
</html>