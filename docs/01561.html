<html>
<head>
<title>Generating Synthetic Classification Data using Scikit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Scikit 生成合成分类数据</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/https-medium-com-faizanahemad-generating-synthetic-classification-data-using-scikit-1590c1632922?source=collection_archive---------2-----------------------#2019-03-13">https://towardsdatascience.com/https-medium-com-faizanahemad-generating-synthetic-classification-data-using-scikit-1590c1632922?source=collection_archive---------2-----------------------#2019-03-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/0427135753bea654dd1be7e85173e3a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*SnthcB0Q56TvM8HHf6o8gQ.jpeg"/></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Generating Synthetic Data</figcaption></figure><p id="5836" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是关于不平衡和噪声数据系列文章的第 1 部分。<a class="ae kw" rel="noopener" target="_blank" href="/selecting-the-right-metric-for-skewed-classification-problems-6e0a4a6167a7">关于偏斜分类指标的第 2 部分已经发布</a>。</p><h1 id="f564" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">为什么我们需要数据生成器？</h1><p id="f214" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">数据生成器帮助我们用不同的分布和概要文件创建数据来进行实验。如果您正在测试各种可用的算法，并且希望找到哪种算法在什么情况下有效，那么这些数据生成器可以帮助您生成特定于情况的数据，然后测试算法。</p><p id="3ad8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">例如，你想检查是否梯度推进树可以做得很好，只给 100 个数据点和 2 个特征？现在，您可以搜索 100 个数据点的数据集，也可以使用您自己正在处理的数据集。但是你怎么知道这个分类器是不是一个好的选择，假设你有这么少的数据，做交叉验证和测试仍然留下了过度拟合的机会？或者，您可以使用生成的数据，看看在这种情况下什么通常效果好，是升压算法还是线性模型。</p><p id="333b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">您需要生成数据的几个原因</p><ul class=""><li id="4f1c" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">你的模型能处理嘈杂的标签吗？</li><li id="40c0" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">当你的标签 99%是负面的，只有 1%是正面的，会发生什么？</li><li id="2371" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">如果你的模型能告诉你哪些功能是多余的？</li><li id="71cf" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">在模型提供特征重要性的情况下，模型如何处理冗余特征。</li><li id="55ab" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">移除多余的特征会提高模型的性能吗？</li><li id="e038" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">当冗余要素、噪声和不平衡同时出现在数据集中时，您的模型会如何表现？</li><li id="f73d" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">如果你有<em class="mo"> N </em>个数据点和<em class="mo"> M </em>个特征，N，M 的安全值是多少，这样你的模型才不会过拟合？</li></ul><p id="05e2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">找到一个真正的数据集来满足这种已知水平的标准组合将是非常困难的。因此，我们只考虑了生成器必须具备的几个能力，以给出真实世界数据集的良好近似。</p><h1 id="ff35" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">发电机能力</h1><p id="fec0" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">在寻找发电机时，我们寻找某些功能。我列出了我们在生成器中寻找的重要功能，并对它们进行了相应的分类。</p><h2 id="9653" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">支持不平衡的类</h2><p id="a9e4" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">很多时候，你会得到有巨大不平衡的分类数据。例如，欺诈检测具有不平衡性，使得大多数例子(99%)是非欺诈的。要检查分类器在不平衡情况下的表现，您需要能够生成多种类型的不平衡数据。</p><ul class=""><li id="0838" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">高斯分位数</li><li id="cc80" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">制作分类 API</li></ul><h2 id="25ae" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">支持生成噪声数据</h2><p id="e81e" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">即使类别标签有噪音，你的分类器能完成它的工作吗？如果一些欺诈例子被标记为非欺诈，一些非欺诈被标记为欺诈怎么办？你如何知道你选择的分类器在有噪声的情况下的行为？你如何选择一个健壮的分类器？</p><ul class=""><li id="7dc7" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">制作分类 API</li></ul><h2 id="404b" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">添加冗余/无用的功能</h2><p id="5945" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这些是你有用特征的线性组合。许多模型，如线性回归，对相关特征给出任意的特征系数。在树模型的情况下，它们混淆了特征的重要性，并且随机地和可互换地使用这些特征进行分割。移除相关特征通常会提高性能。</p><ul class=""><li id="c164" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">制作分类 API</li></ul><h1 id="aa05" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">例子</h1><p id="0257" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated"><a class="ae kw" href="https://github.com/faizanahemad/data-science/blob/master/exploration_projects/imbalance-noise-oversampling/Generating%20and%20Visualizing%20Classification%20Data%20using%20scikit.ipynb" rel="noopener ugc nofollow" target="_blank">用于此的笔记本在 Github 中。</a>助手功能在这个<a class="ae kw" href="https://github.com/faizanahemad/data-science/blob/master/exploration_projects/imbalance-noise-oversampling/lib.py" rel="noopener ugc nofollow" target="_blank">文件</a>中定义。</p><p id="d35c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">在这里，我们将介绍 scikit 中可用的 3 个非常好的数据生成器，并了解如何在各种情况下使用它们。</p><h2 id="7e30" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">高斯分位数</h2><p id="f74f" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated"><strong class="ka ir"> 2 级 2D </strong></p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="0fd5" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_gaussian_quantiles</span><span id="9e26" class="mp ky iq ng b gy no nl l nm nn"># Construct dataset<br/>X1, y1 = make_gaussian_quantiles(cov=3.,<br/>                                 n_samples=10000, n_features=2,<br/>                                 n_classes=2, random_state=1)</span><span id="f44d" class="mp ky iq ng b gy no nl l nm nn">X1 = pd.DataFrame(X1,columns=['x','y'])<br/>y1 = pd.Series(y1)<br/>visualize_2d(X1,y1)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/069bbadeda0e125411a82dad3ffa2c23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gFrmNfO8gopd6HXAtK0vvQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Gaussian Data</figcaption></figure><p id="8f17" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">多级 2D </strong></p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="75cb" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_gaussian_quantiles</span><span id="93ae" class="mp ky iq ng b gy no nl l nm nn"># Construct dataset<br/>X1, y1 = make_gaussian_quantiles(cov=3.,<br/>                                 n_samples=10000, n_features=2,<br/>                                 n_classes=3, random_state=1)</span><span id="cc05" class="mp ky iq ng b gy no nl l nm nn">X1 = pd.DataFrame(X1,columns=['x','y'])<br/>y1 = pd.Series(y1)<br/>visualize_2d(X1,y1)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nu"><img src="../Images/d2dc1e85ddc3dab1190f37e140459a0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f6q1idPVPuG6wcjtjTmHuQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">3 Class Gaussian</figcaption></figure><p id="9652" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2 级 3D </strong></p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="893b" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_gaussian_quantiles</span><span id="ce91" class="mp ky iq ng b gy no nl l nm nn"># Construct dataset<br/>X1, y1 = make_gaussian_quantiles(cov=1.,<br/>                                 n_samples=10000, n_features=3,<br/>                                 n_classes=2, random_state=1)</span><span id="f4ed" class="mp ky iq ng b gy no nl l nm nn">X1 = pd.DataFrame(X1,columns=['x','y','z'])<br/>y1 = pd.Series(y1)<br/>visualize_3d(X1,y1)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nv"><img src="../Images/ac84bbd8fd24595f3808b05a27bbcb39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ZmMYVhInzi_zMuXgZ7ifKg.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">3D Gaussian Data</figcaption></figure><p id="7f8a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">通过组合两个高斯函数得到更硬的边界</strong></p><p id="b733" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们创建两个中心位置不同的高斯。<code class="fe nw nx ny ng b">mean=(4,4)</code>在第二个高斯创建中，它以 x=4，y=4 为中心。接下来，我们反转第二个高斯，并将其数据点添加到第一个高斯的数据点。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="0df8" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_gaussian_quantiles</span><span id="3f06" class="mp ky iq ng b gy no nl l nm nn"># Construct dataset</span><span id="4a17" class="mp ky iq ng b gy no nl l nm nn"># Gaussian 1<br/>X1, y1 = make_gaussian_quantiles(cov=3.,<br/>                                 n_samples=10000, n_features=2,<br/>                                 n_classes=2, random_state=1)</span><span id="37ba" class="mp ky iq ng b gy no nl l nm nn">X1 = pd.DataFrame(X1,columns=['x','y'])<br/>y1 = pd.Series(y1)<br/></span><span id="d339" class="mp ky iq ng b gy no nl l nm nn"># Gaussian 2</span><span id="0cd2" class="mp ky iq ng b gy no nl l nm nn">X2, y2 = make_gaussian_quantiles(mean=(4, 4), cov=1,<br/>                                 n_samples=5000, n_features=2,<br/>                                 n_classes=2, random_state=1)</span><span id="eb4b" class="mp ky iq ng b gy no nl l nm nn">X2 = pd.DataFrame(X2,columns=['x','y'])<br/>y2 = pd.Series(y2)</span><span id="a8e2" class="mp ky iq ng b gy no nl l nm nn"># Combine the gaussians</span><span id="0de3" class="mp ky iq ng b gy no nl l nm nn">X1.shape<br/>X2.shape</span><span id="21ee" class="mp ky iq ng b gy no nl l nm nn">X = pd.DataFrame(np.concatenate((X1, X2)))<br/>y = pd.Series(np.concatenate((y1, - y2 + 1)))</span><span id="3271" class="mp ky iq ng b gy no nl l nm nn">X.shape</span><span id="e591" class="mp ky iq ng b gy no nl l nm nn">visualize_2d(X,y)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi nz"><img src="../Images/24677b9c1110cbde0fa7566cc5bc15b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ndVOic_BxSSGAlLS5Kq6eA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Combined Gaussians</figcaption></figure><h2 id="4a55" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">一滴</h2><p id="48d3" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">如果你想要更简单和容易分离的数据，Blobs 是个不错的选择。这些可以通过线性决策边界来分隔。这里我将展示一个 4 类 3D (3 特征斑点)的例子。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oa"><img src="../Images/b326b912cc8ed633a089187bca5386fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*sZl_uKr1UQlK6HOWwqgknA.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Blobs with 4 classes in 3D</figcaption></figure><p id="6da7" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">你可以注意到斑点是如何被简单的平面分开的。因此，这些数据点非常适合测试线性算法，如 LogisticRegression。</p><h2 id="324b" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">制作分类 API</h2><p id="6050" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">这是用于数据生成的最复杂的 scikit api，它带有所有的附加功能。它允许您拥有多种功能。还允许您向数据中添加噪声和不平衡。</p><p id="5fcd" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">一些更好的特性包括添加冗余特性，这些特性基本上是现有特性的线性组合。添加非信息特征来检查模型是否过度拟合这些无用特征。也增加了直接重复的特征。</p><p id="2b3e" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">此外，为了增加分类的复杂性，您可以拥有多个类簇，并减少类之间的间隔，以强制分类器具有复杂的非线性边界。</p><p id="3ca1" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我在下面提供了使用这个 API 的各种方法。</p><p id="1a37" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3 类 3D 简单案例</strong></p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="cfb0" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_classification<br/>X,y = make_classification(n_samples=10000, n_features=3, n_informative=3, <br/>                    n_redundant=0, n_repeated=0, n_classes=3, n_clusters_per_class=2,<br/>                          class_sep=1.5,<br/>                   flip_y=0,weights=[0.5,0.5,0.5])</span><span id="37a1" class="mp ky iq ng b gy no nl l nm nn">X = pd.DataFrame(X)<br/>y = pd.Series(y)</span><span id="fadc" class="mp ky iq ng b gy no nl l nm nn">visualize_3d(X,y)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ob"><img src="../Images/f5345a57019c178b17ec211f4caca99f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*JIYrubd6d8RCjm0B7Jr65A.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Simple case of Make Classification API</figcaption></figure><p id="b323" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 3 类 2D 带噪声</strong></p><p id="5781" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们将使用参数<code class="fe nw nx ny ng b">flip_y</code>来添加额外的噪声。这可以用来测试我们的分类器在添加噪声后是否工作良好。如果我们有真实世界的噪声数据(比如来自 IOT 的设备)，并且分类器不能很好地处理噪声，那么我们的准确性将会受到影响。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="4de5" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_classification</span><span id="3ee7" class="mp ky iq ng b gy no nl l nm nn"># Generate Clean data</span><span id="a248" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=10000, n_features=2, n_informative=2,n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1,class_sep=2,<strong class="ng ir">flip_y=0</strong>,weights=[0.5,0.5], random_state=17)</span><span id="a903" class="mp ky iq ng b gy no nl l nm nn">f, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,figsize=(20,8))<br/>sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax1);<br/>ax1.set_title("No Noise");</span><span id="5e2e" class="mp ky iq ng b gy no nl l nm nn"># Generate noisy Data</span><span id="121d" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=10000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1,class_sep=2,<strong class="ng ir">flip_y=0.2</strong>,weights=[0.5,0.5], random_state=17)</span><span id="6990" class="mp ky iq ng b gy no nl l nm nn">sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax2);<br/>ax2.set_title("With Noise");</span><span id="28d4" class="mp ky iq ng b gy no nl l nm nn">plt.show();</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oc"><img src="../Images/3f4e925187bfdd7fb0507319aaf33760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mJ7C4SdgKXtvnNdh-Zg4wA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Without and With Noise</figcaption></figure><p id="099c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir"> 2 级 2D 与不平衡</strong></p><p id="8961" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这里我们会有比正面例子多 9 倍的反面例子。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="7a6e" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_classification</span><span id="739e" class="mp ky iq ng b gy no nl l nm nn"># Generate Balanced Data</span><span id="4c30" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=1000, n_features=2, n_informative=2,n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=2,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="3a35" class="mp ky iq ng b gy no nl l nm nn">f, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,figsize=(20,8))<br/>sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax1);<br/>ax1.set_title("No Imbalance");</span><span id="37df" class="mp ky iq ng b gy no nl l nm nn"># Generate Imbalanced Data</span><span id="45aa" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=2,flip_y=0,weights=[0.9,0.1], random_state=17)</span><span id="bbeb" class="mp ky iq ng b gy no nl l nm nn">sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax2);<br/>ax2.set_title("Imbalance 9:1 :: Negative:Postive");</span><span id="5abd" class="mp ky iq ng b gy no nl l nm nn">plt.show();</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi od"><img src="../Images/90345800ed78ab5f6b0050181ceeba1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ioXZt7f-8PulZOK8RvFA_g.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Imbalance: Notice how the right side has low volume of class=1</figcaption></figure><p id="b6f2" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">使用冗余功能(3D) </strong></p><p id="818c" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这增加了冗余特征，这些冗余特征是其他有用特征的线性组合。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="9f9b" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_classification</span><span id="3a3b" class="mp ky iq ng b gy no nl l nm nn"># All unique features</span><span id="9d2f" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=10000, <strong class="ng ir">n_features=3, n_informative=3, n_redundant=0,</strong> n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=2,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="795d" class="mp ky iq ng b gy no nl l nm nn">visualize_3d(X,y,algorithm="pca")</span><span id="fbea" class="mp ky iq ng b gy no nl l nm nn"># 2 Useful features and 3rd feature as Linear Combination of first 2</span><span id="e470" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=10000, <strong class="ng ir">n_features=3, n_informative=2, n_redundant=1, </strong>n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=2,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="5c3c" class="mp ky iq ng b gy no nl l nm nn">visualize_3d(X,y,algorithm="pca")</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oe"><img src="../Images/096dbd98596dcf679b654df4213632b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*LGgz1uRmV4H3q4QhtZmanQ.gif"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Non Redundant features</figcaption></figure><p id="e4b8" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意，在存在冗余特征的情况下，第二张图似乎是由某个 3D 平面(而非完整的 3D 空间)中的数据点组成的。与第一张图相比，第一张图中的数据点是在所有 3 个维度上分布的云。</p><p id="f6ed" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">对于第二张图，我直觉地认为，如果我将我的坐标改变到数据点所在的 3D 平面，那么数据将仍然是可分离的，但是它的维度将减少到 2D，即，通过减少第二张图的维度，我将不会丢失任何信息。但是如果我减少第一个图的维度，数据将不再保持可分，因为所有 3 个特征都是非冗余的。让我们试试这个主意。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="a863" class="mp ky iq ng b gy nk nl l nm nn">X,y = make_classification(n_samples=1000, <strong class="ng ir">n_features=3, n_informative=3, n_redundant=0,</strong> n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=0.75,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="8215" class="mp ky iq ng b gy no nl l nm nn">visualize_2d(X,y,algorithm="pca")</span><span id="ac4b" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=1000, <strong class="ng ir">n_features=3, n_informative=2, n_redundant=1,</strong> n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=0.75,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="bb12" class="mp ky iq ng b gy no nl l nm nn">visualize_2d(X,y,algorithm="pca")</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi of"><img src="../Images/6405f6515dad59a3e1d992ab63fc853e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zBEzaKXztHPoongaK0kciQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Non Redundant — Can’t Separate in 2D</figcaption></figure><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi og"><img src="../Images/b39461c78162caf4e871223bc17143d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jz6hjvXEE6L-LHTkTE73lA.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Redundant 3rd Dim — Separable in 2D as well</figcaption></figure><p id="9318" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated"><strong class="ka ir">利用阶级分离</strong></p><p id="65f5" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">改变类分离会改变分类任务的难度。在低等级分离的情况下，数据点不再容易分离。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="3171" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_classification</span><span id="429b" class="mp ky iq ng b gy no nl l nm nn"># Low class Sep, Hard decision boundary</span><span id="8ef2" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=1000, n_features=2, n_informative=2,n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,<strong class="ng ir">class_sep=0.75</strong>,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="fd70" class="mp ky iq ng b gy no nl l nm nn">f, (ax1,ax2, ax3) = plt.subplots(nrows=1, ncols=3,figsize=(20,5))<br/>sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax1);<br/>ax1.set_title("Low class Sep, Hard decision boundary");</span><span id="e013" class="mp ky iq ng b gy no nl l nm nn"># Avg class Sep, Normal decision boundary</span><span id="1605" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=1000, n_features=2, n_informative=2,n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,<strong class="ng ir">class_sep=1.5</strong>,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="1896" class="mp ky iq ng b gy no nl l nm nn">sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax2);<br/>ax2.set_title("Avg class Sep, Normal decision boundary");</span><span id="de08" class="mp ky iq ng b gy no nl l nm nn"># Large class Sep, Easy decision boundary</span><span id="b5ba" class="mp ky iq ng b gy no nl l nm nn">X,y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,<strong class="ng ir">class_sep=3</strong>,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="5c62" class="mp ky iq ng b gy no nl l nm nn">sns.scatterplot(X[:,0],X[:,1],hue=y,ax=ax3);<br/>ax3.set_title("Large class Sep, Easy decision boundary");</span><span id="c94f" class="mp ky iq ng b gy no nl l nm nn">plt.show();</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oh"><img src="../Images/829d655d471f9c7dc72f910cbd181d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqnFZthvGxUMlFQjOh2MOw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">From Left to Right: Higher Class separation and easier decision boundaries</figcaption></figure><h1 id="6d4f" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">测试各种分类器以了解数据生成器的使用</h1><p id="8bc3" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们将生成两组数据，并展示如何测试二进制分类器的性能并检查其性能。我们的第一组将是具有容易分离性的标准 2 类数据。我们的第二组将是具有非线性边界和较小类别不平衡的 2 类数据。</p><h2 id="822a" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">要测试的假设</h2><p id="e579" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">我们想要检验的假设是，单独的逻辑回归不能学习非线性边界。梯度推进在学习非线性边界时最有效。</p><h2 id="bc6a" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">数据</h2><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="8cf7" class="mp ky iq ng b gy nk nl l nm nn">from sklearn.datasets import make_classification</span><span id="9f0d" class="mp ky iq ng b gy no nl l nm nn"># Easy decision boundary</span><span id="bfe8" class="mp ky iq ng b gy no nl l nm nn">X1,y1 = make_classification(n_samples=10000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=2,flip_y=0,weights=[0.5,0.5], random_state=17)</span><span id="dd5e" class="mp ky iq ng b gy no nl l nm nn">f, (ax1,ax2) = plt.subplots(nrows=1, ncols=2,figsize=(20,8))<br/>sns.scatterplot(X1[:,0],X1[:,1],hue=y1,ax=ax1);<br/>ax1.set_title("Easy decision boundary");</span><span id="facc" class="mp ky iq ng b gy no nl l nm nn"># Hard decision boundary</span><span id="68cd" class="mp ky iq ng b gy no nl l nm nn">X2,y2 = make_classification(n_samples=10000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=1,flip_y=0,weights=[0.7,0.3], random_state=17)</span><span id="716b" class="mp ky iq ng b gy no nl l nm nn">X2a,y2a = make_classification(n_samples=10000, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=2,class_sep=1.25,flip_y=0,weights=[0.8,0.2], random_state=93)</span><span id="fce0" class="mp ky iq ng b gy no nl l nm nn">X2 = np.concatenate((X2,X2a))<br/>y2 = np.concatenate((y2,y2a))</span><span id="9b3d" class="mp ky iq ng b gy no nl l nm nn">sns.scatterplot(X2[:,0],X2[:,1],hue=y2,ax=ax2);<br/>ax2.set_title("Hard decision boundary");</span><span id="ce89" class="mp ky iq ng b gy no nl l nm nn">X1,y1 = pd.DataFrame(X1),pd.Series(y1)<br/>X2,y2 = pd.DataFrame(X2),pd.Series(y2)</span></pre><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oi"><img src="../Images/b90d9bca790088e3347c39caab67f47e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phW5JFFKIh-4WW-DYONY4A.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Easy vs Hard Decision boundaries</figcaption></figure><p id="a696" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们将用这些测试 3 个算法，看看算法的表现如何</p><ul class=""><li id="2ec8" class="ma mb iq ka b kb kc kf kg kj mc kn md kr me kv mf mg mh mi bi translated">逻辑回归</li><li id="e042" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">多项式特征的逻辑回归</li><li id="56c8" class="ma mb iq ka b kb mj kf mk kj ml kn mm kr mn kv mf mg mh mi bi translated">XGBoost(梯度推进算法)</li></ul><h2 id="1602" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">简单决策边界的测试</h2><p id="f2fa" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">参考笔记本第 5 节的完整代码。</p><pre class="nb nc nd ne gt nf ng nh ni aw nj bi"><span id="a4bd" class="mp ky iq ng b gy nk nl l nm nn">f, (ax1,ax2,ax3) = plt.subplots(nrows=1, ncols=3,figsize=(20,6))<br/>lr_results = run_logistic_plain(X1,y1,ax1)</span><span id="0a59" class="mp ky iq ng b gy no nl l nm nn">lrp_results = run_logistic_polynomial_features(X1,y1,ax2)</span><span id="27aa" class="mp ky iq ng b gy no nl l nm nn">xgb_results = run_xgb(X1,y1,ax3)<br/>plt.show()</span></pre><p id="6d16" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">让我们绘制性能和决策边界结构。</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi oj"><img src="../Images/7cca938aea641dedae0b9afb5e3bc81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DhGNKj0lRWdVnPEv_qdAgw.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Decision Boundary : LR and XGB on Easy Dataset</figcaption></figure><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ok"><img src="../Images/e4373ddb30c2bf74ff5dbd0a483152bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPmHDbIJzmc0UQydUYDiKQ.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Train and Test Performances</figcaption></figure><h2 id="fce7" class="mp ky iq bd kz mq mr dn ld ms mt dp lh kj mu mv ll kn mw mx lp kr my mz lt na bi translated">硬决策边界测试</h2><p id="4c56" class="pw-post-body-paragraph jy jz iq ka b kb lv kd ke kf lw kh ki kj lx kl km kn ly kp kq kr lz kt ku kv ij bi translated">判别边界</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi ol"><img src="../Images/3488fb285674811101f44f84ec05b479.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FWYGsQ9TCVpVlNqNunEN9Q.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Decision Boundary for Hard dataset</figcaption></figure><p id="605b" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">表演</p><figure class="nb nc nd ne gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi om"><img src="../Images/d366b8d1659516ac7da470285511c16d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BwjXkEqExrx-MYhp6EXqig.png"/></div></div><figcaption class="ju jv gj gh gi jw jx bd b be z dk">Train and Test Performance for Non Linear Boundary</figcaption></figure><p id="52ac" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">请注意 XGBoost 如何以 0.916 的分数脱颖而出。这是因为梯度增强允许学习复杂的非线性边界。</p><p id="eef3" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我们能够检验我们的假设，并得出结论说它是正确的。由于生成数据很容易，我们节省了初始数据收集过程的时间，并且能够非常快速地测试我们的分类器。</p><h1 id="1c9a" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">其他资源</h1><div class="on oo gp gr op oq"><a href="http://www.blackarbs.com/blog/synthetic-data-generation-part-1-block-bootstrapping" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">合成数据生成(第 1 部分)-块引导</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">引言数据是定量研究的核心。问题是历史只有一条路。因此我们是…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">www.blackarbs.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe js oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">Scikit 数据集模块</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">sklearn.datasets 模块包括人工数据生成器以及多个真实数据集…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">scikit-learn.org</p></div></div><div class="oz l"><div class="pf l pb pc pd oz pe js oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://github.com/faizanahemad/data-science/blob/master/exploration_projects/imbalance-noise-oversampling/Generating%20and%20Visualizing%20Classification%20Data%20using%20scikit.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">此处使用的笔记本</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">本笔记本中的全部代码连同助手一起给出…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="pg l pb pc pd oz pe js oq"/></div></div></a></div><div class="on oo gp gr op oq"><a href="https://github.com/faizanahemad/data-science/blob/master/exploration_projects/imbalance-noise-oversampling/lib.py" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">助手文件</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">此项目中使用的帮助器函数…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="ph l pb pc pd oz pe js oq"/></div></div></a></div><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/synthetic-data-generation-a-must-have-skill-for-new-data-scientists-915896c0c1ae"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd ir gy z fp ov fr fs ow fu fw ip bi translated">合成数据生成—新数据科学家的必备技能</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">为自驱动数据科学项目和深潜生成合成数据的包和想法的简要概述…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pi l pb pc pd oz pe js oq"/></div></div></a></div><p id="a5a4" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">这是一系列文章中的第一篇，我计划在给定噪声和不平衡的情况下分析各种分类器的性能。<a class="ae kw" rel="noopener" target="_blank" href="/selecting-the-right-metric-for-skewed-classification-problems-6e0a4a6167a7">接下来的第二部分在这里。</a></p><p id="bb23" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">感谢阅读！！</p><p id="d00a" class="pw-post-body-paragraph jy jz iq ka b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv ij bi translated">我利用数据科学、人工智能、机器学习和深度学习来解决现实世界的问题。随时联系我<a class="ae kw" href="https://www.linkedin.com/in/faizan-ahemad-7851a07b/" rel="noopener ugc nofollow" target="_blank"> <strong class="ka ir"> LinkedIn </strong> </a> <strong class="ka ir">。</strong></p></div></div>    
</body>
</html>