<html>
<head>
<title>Time2Vec for Time Series features encoding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Time2Vec 用于时间序列特征编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e?source=collection_archive---------3-----------------------#2019-09-25">https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e?source=collection_archive---------3-----------------------#2019-09-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f6ac" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为您的机器学习模型学习一个有价值的时间表示</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/90872169c4af76371abc47ece224e8ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5SYd19JfkE1CqI1l"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@lucamicheli?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Luca Micheli</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="bc23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在每个涉及时间序列的机器学习问题中，时间是黄金信息。作为数据科学家，我们必须尽最大努力提取时间模式，让我们的数据自己说话。在预处理过程中，常见的程序有标准化(平稳性检查、自相关消除…)、创建和编码分类时间特征(日、周、月、季…)、人工特征工程(傅立叶变换…)。我们的努力并不总是有回报的，因为在模型化过程中，我们选择的模型可能无法正确地将时间本身视为一个特征。</p><p id="6ecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我试图再现论文'<a class="ae ky" href="https://arxiv.org/abs/1907.05321" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Time2Vec:学习时间的向量表示法</strong> </a> <strong class="lb iu"> ' 【T5，<strong class="lb iu"> </strong>中提出的方法，其最终目标是<em class="lv">开发一种通用的模型不可知的时间表示法，这种表示法可以潜在地用在任何架构中</em>(我在 Keras 中开发了一个神经网络，采用了这种解决方案)。作者不想为时间序列分析提出一个新的模型，相反，他们的目标是以向量嵌入的形式提供时间的表示，以便以更好的方式自动化特征工程过程和建模时间。</strong></p><h1 id="8d5f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">数据集</h1><p id="cabf" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">为了给出证据和整个解决方案的具体效用，我们需要一个足够的数据集。在我的例子中，我需要时间序列格式的数据，没有任何额外特征形式的冗余信息。这种情况在自回归问题中很典型，在这种情况下，我们只有一个时间序列，并将其作为预测未来的一个特征。在现实生活中这是一个常见的任务，所以不难找到一个数据集。我在<a class="ae ky" href="https://www.kaggle.com/lbronchal/venezia#Punta_Salute_1983_2015.zip" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上发现了一个不错的。它储存了威尼斯大量的历史水位。预测这些值是一项严肃的任务；每天游客都可以获得该城市不同地区海平面的详细而准确的报告。</p><p id="1ef5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我爱威尼斯，我的目标不是和他们竞争。我们也没有足够的信息来提供真正明显的表现(额外的回归因素，如温度、月相、天气条件等，提供了足够的推动)。在这里，我们不得不利用仅有的历史数据来预测下一个小时的水位。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f190df544ad1e828fb4cf996e23ba0f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*Gr8l-B6PtSEFxTsr4MI_hQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Example of hourly water level (cm) in a week</figcaption></figure><h1 id="bb59" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">时间 2 秒实现</h1><p id="4c75" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">从数学角度来说，实现 Time2Vec 非常容易:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/aac29e31627ab3511d858dea855fac96.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*E8ZiELOiGvNu_WCAHX8fRw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">from: <a class="ae ky" href="https://arxiv.org/pdf/1907.05321.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1907.05321.pdf</a></figcaption></figure><p id="b01d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<em class="lv"> k </em>是时间 2 维，<em class="lv">τ</em>是原始时间序列，<em class="lv"> F </em>是周期性激活函数，<em class="lv">ω</em>和<em class="lv">φ</em>是一组可学习的参数。在我的实验中，我将<em class="lv"> F </em>设置为一个 sin 函数，以使选定的算法能够捕捉数据中的周期性行为。同时，<em class="lv">线性项代表时间的进程，可用于捕捉依赖于时间的输入中的非周期性模式</em>。</p><p id="343a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种简单性使得这种时间向量表示很容易被不同的架构所使用。在我的例子中，我试图在修改简单 Keras 密集层的神经网络结构中转移这个概念。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="bfe5" class="na lx it mw b gy nb nc l nd ne">class T2V(Layer):<br/>    <br/>    def __init__(self, output_dim=None, **kwargs):<br/>        self.output_dim = output_dim<br/>        super(T2V, self).__init__(**kwargs)<br/>        <br/>    def build(self, input_shape):</span><span id="53c5" class="na lx it mw b gy nf nc l nd ne">        self.W = self.add_weight(name='W',<br/>                      shape=(input_shape[-1], self.output_dim),<br/>                      initializer='uniform',<br/>                      trainable=True)</span><span id="325c" class="na lx it mw b gy nf nc l nd ne">        self.P = self.add_weight(name='P',<br/>                      shape=(input_shape[1], self.output_dim),<br/>                      initializer='uniform',<br/>                      trainable=True)</span><span id="63d8" class="na lx it mw b gy nf nc l nd ne">        self.w = self.add_weight(name='w',<br/>                      shape=(input_shape[1], 1),<br/>                      initializer='uniform',<br/>                      trainable=True)</span><span id="3545" class="na lx it mw b gy nf nc l nd ne">        self.p = self.add_weight(name='p',<br/>                      shape=(input_shape[1], 1),<br/>                      initializer='uniform',<br/>                      trainable=True)</span><span id="1481" class="na lx it mw b gy nf nc l nd ne">        super(T2V, self).build(input_shape)<br/>        <br/>    def call(self, x):<br/>        <br/>        original = self.w * x + self.p<br/>        sin_trans = K.sin(K.dot(x, self.W) + self.P)<br/>        <br/>        return K.concatenate([sin_trans, original], -1)</span></pre><p id="f33b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该自定义层的输出维度是用户指定的隐藏维度<em class="lv"> (1 ≤ i ≤ k) </em>，即从网络学习的正弦曲线，加上输入的线性表示<em class="lv"> (i = 0)。有了这个工具，我们只需将它与其他层堆叠在一起，并在我们的案例研究中尝试它的威力。</em></p><h1 id="5bcc" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">模型</h1><p id="0d90" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">Time2Vec 是一个很好的时间表示吗？为了回答这个问题，我比较了在我们的预测任务中实现的性能，建立了两个不同的序列神经网络模型。第一个将我们定制的 Time2Vec 层作为输入，叠加在一个简单的 LSTM 层上。第二层仅由先前结构中使用的简单 LSTM 层组成。</p><pre class="kj kk kl km gt mv mw mx my aw mz bi"><span id="ff42" class="na lx it mw b gy nb nc l nd ne">def T2V_NN(param, dim):<br/>    <br/>    inp = Input(shape=(dim,1))<br/>    x = T2V(param['t2v_dim'], dim)(inp)<br/>    x = LSTM(param['unit'], activation=param['act'])(x)<br/>    x = Dense(1)(x)<br/>    <br/>    m = Model(inp, x)<br/>    m.compile(loss='mse', optimizer=Adam(lr=param['lr']))<br/>    <br/>    return m</span><span id="30f6" class="na lx it mw b gy nf nc l nd ne">def NN(param, dim):<br/>    <br/>    inp = Input(shape=(dim,1))<br/>    x = LSTM(param['unit'], activation=param['act'])(inp)<br/>    x = Dense(1)(x)<br/>    <br/>    m = Model(inp, x)<br/>    m.compile(loss='mse', optimizer=Adam(lr=param['lr']))<br/>    <br/>    return m</span></pre><p id="f9a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们执行拟合程序操作超参数优化。这是使用<a class="ae ky" href="https://github.com/cerlymarco/keras-hypetune" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">keras-hype tune</strong></a>完成的。该框架以非常直观的方式提供了神经网络结构的<strong class="lb iu">超参数优化</strong>。我们对一些参数组合进行标准网格搜索。所有涉及的两个训练程序都要这样做。</p><h1 id="dbe2" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="47d5" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我使用前 70%的数据作为训练集，剩下的 30%作为测试集。在拟合过程中，该系列还分为验证，以便进行超参数调整。最佳和优化的 T2V + LSTM 在测试中实现了大约 1.67 MAE，而简单和优化的 LSTM 实现了大约 2.02。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/ccc00f8f4aa5ca32e94a97d981a6fd0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hsZo6qUkZgwCixgZ4Gi8ag.png"/></div></div></figure><p id="a50f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这两个网络似乎都能很好地识别数据中的模式。使用 T2V 编码可以稍微提高性能。</p><h1 id="0e95" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="34cb" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在这篇文章中，我介绍了一种自动学习时间特征的方法。特别是，我复制了<a class="ae ky" href="https://arxiv.org/pdf/1907.05321.pdf" rel="noopener ugc nofollow" target="_blank"> Time2Vec </a>，一个时间的向量表示，使其适应神经网络架构。最后，我能够在一个真实的任务中展示这种表现的有效性。我想指出的是，正如论文作者所建议的，T2V 不是时间序列分析的新模型，而是一种简单的向量表示，可以很容易地导入到许多现有和未来的架构中，并提高它们的性能。</p><p id="ffb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你对题目感兴趣，我建议:</strong></p><ul class=""><li id="242b" class="nh ni it lb b lc ld lf lg li nj lm nk lq nl lu nm nn no np bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/word2vec-with-time-series-a-transfer-learning-approach-58017e7a019d"> <strong class="lb iu"> Word2Vec 带时间序列:一种迁移学习方法</strong> </a></li><li id="c48b" class="nh ni it lb b lc nq lf nr li ns lm nt lq nu lu nm nn no np bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/corr2vec-a-wavenet-architecture-for-feature-engineering-in-financial-market-94b4f8279ba6"> <strong class="lb iu"> Corr2Vec:金融市场特征工程的 WaveNet 架构</strong> </a></li></ul></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="715b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/cerlymarco/MEDIUM_NoteBook" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">查看我的 GITHUB 回购</strong> </a></p><p id="8162" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">保持联系:<a class="ae ky" href="https://www.linkedin.com/in/marco-cerliani-b0bba714b/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="cffa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献</strong></p><p id="29f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Time2Vec:学习时间的向量表示法<em class="lv">。赛义德·迈赫兰·卡泽米、里沙卜·戈埃尔、塞佩赫尔·埃格巴利、贾纳汉·拉曼南、贾斯普里特·萨霍塔、桑杰·塔库尔、斯特拉·吴、卡塔尔·史密斯、帕斯卡尔·普帕特、马库斯·布鲁贝克</em></p></div></div>    
</body>
</html>