<html>
<head>
<title>Mixture of Variational Autoencoders — a Fusion Between MoE and VAE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可变自动编码器的混合 MoE 和 VAE 的融合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675?source=collection_archive---------12-----------------------#2019-04-01">https://towardsdatascience.com/mixture-of-variational-autoencoders-a-fusion-between-moe-and-vae-22c0901a6675?source=collection_archive---------12-----------------------#2019-04-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="689d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种无监督的数字分类和生成方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/674e3916777b84f131b7bdac53faec25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNpl11cSC2c8duG60KxVZg.jpeg"/></div></div></figure><p id="fa9a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="http://anotherdatum.com/vae.html" rel="noopener ugc nofollow" target="_blank">变分自动编码器(VAE) </a>是试图学习输入空间形状的神经网络的典范。一旦经过训练，该模型可用于从输入空间生成新样本。</p><p id="5b62" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们有输入数据的标签，那么<a class="ae lq" href="http://anotherdatum.com/vae2.html" rel="noopener ugc nofollow" target="_blank">也可以在标签上设定生成过程</a>。在<a class="ae lq" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>的情况下，这意味着我们可以指定我们想要为哪个数字生成图像。</p><p id="96a5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们更进一步……我们能在完全不使用标签的情况下根据数字来决定生成过程吗？我们能用无监督的方法达到同样的结果吗？</p><p id="f794" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们想依赖标签，我们可以做一些简单得令人尴尬的事情。我们可以训练 10 个独立的 VAE 模型，每个模型都使用一位数的图像。</p><p id="d181" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那显然可以，但是你用的是标签。那是作弊！</p><p id="61db" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好吧，我们根本不用它们。让我们训练我们的 10 个模型，在把它传给合适的模型之前，用我们的眼睛看一下每张图像。</p><p id="c074" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">嘿，你又作弊了！虽然您不使用标签本身，但您确实会查看图像，以便将它们发送到适当的模型。</p><p id="d1a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">好吧……如果我们让另一个模特学习路线，而不是自己做路线，那根本不算作弊，不是吗？</p><p id="1f7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">对！:)</strong></p><p id="3a2f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以使用 11 个模块的架构，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lr"><img src="../Images/1b86741197ddb4a65606c105427569cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nF21sfG5hVR8y1XIUmBUng.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">A manager module routing an input to the appropriate expert module</figcaption></figure><p id="a365" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">但是经理如何决定将图像传递给哪个专家呢？我们可以训练它来预测图像的数字，但是我们不想使用标签！</p><p id="ef48" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">唷…我还以为你要作弊呢… </strong></p><p id="a2cf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">那么，我们如何在不使用标签的情况下培训经理呢？这让我想起了一种不同类型的模型——专家混合模型。让我绕个小圈子来解释一下 MoE 是如何工作的。我们需要它，因为它将是我们解决方案的关键组成部分。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="8b14" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">向非专家解释的专家组合</h1><p id="1138" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">MoE 是一个监督学习框架。你可以在 Coursera 和 YouTube 上找到 Geoffrey Hinton 的精彩解释。MoE 依赖于根据𝑥→𝑦映射对输入进行分段的可能性。看看这个简单的函数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/41558dfcf6c4656820b72b7018ff4e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4f0drTZI21X5486nhY70VQ.png"/></div></div></figure><p id="88c9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">地面实况被定义为𝑥的紫色抛物线</p><p id="0668" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">In complex datasets we might not know the split points. One (bad) solution is to segment the input space by clustering the 𝑥’s using K-means. In the two parabolas example, we’ll end up with 𝑥’’ as the split point between two clusters. Thus, when we’ll train the model on the 𝑥</p><p id="044f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">So how can we train a model that learns the split points while at the same time learns the mapping that defines the split points?</p><p id="2ff4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">MoE does so using an architecture of multiple subnetworks — one manager and multiple experts:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/95c2eee01d041a58295eae960a8085da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uoiv4WYTNo-vygAhKQkP3g.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">MoE architecture</figcaption></figure><p id="2a1f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">The manager maps the input into a soft decision over the experts, which is used in two contexts:</p><p id="31c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">First, the output of the network is a weighted average of the experts’ outputs, where the weights are the manager’s output.</p><p id="65d0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Second, the loss function is</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/648c3aa9eb6bb32826da669567e50584.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*hwlW0QswViEfZoNM_DR2xA.png"/></div></figure><p id="a80d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">𝑦¡ is the label, 𝑦¯¡ is the output of the i’th expert, 𝑝¡ is the i’th entry of the manager’s output. When you differentiate the loss, you get these results (I encourage you to watch the <a class="ae lq" href="https://www.youtube.com/watch?v=d_GVvIBlWtI" rel="noopener ugc nofollow" target="_blank">视频</a>了解更多详情):</p><ol class=""><li id="2795" class="nd ne it kw b kx ky la lb ld nf lh ng ll nh lp ni nj nk nl bi translated">经理为每个专家决定它对损失的贡献大小。换句话说，管理者选择哪些专家应该根据他们的误差来调整他们的权重。</li><li id="72e6" class="nd ne it kw b kx nm la nn ld no lh np ll nq lp ni nj nk nl bi translated">管理器以这样一种方式调整它输出的概率，使得答对的专家将比没答对的专家获得更高的概率。</li></ol><p id="a6c2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这个损失函数鼓励专家专攻不同种类的输入。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><h1 id="cba1" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">拼图的最后一块……是𝑥</h1><p id="1855" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">让我们回到我们的挑战！MoE 是一个监督学习的框架。当然，我们可以把𝑦换成𝑥，在无人监管的情况下，对不对？MoE 的强大之处在于，每位专家都专门研究输入空间的不同部分，并具有唯一的制图𝑥→𝑦.如果我们使用映射𝑥→𝑥，每个专家将专注于输入空间的不同部分，在输入本身中具有独特的模式。</p><p id="335e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们将使用 VAEs 作为专家。VAE 的部分损失是重建损失，其中 VAE 试图重建原始输入图像𝑥:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/75f9755a20e8db11a026ae98f16fbc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cffKfVkF3hkV0Km8kuMvoA.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">MoE architecture where the experts are implemented as VAE</figcaption></figure><p id="aaa7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这种架构的一个很酷的副产品是，管理器可以使用图像的输出向量对图像中的数字进行分类！</p><p id="f975" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在训练这个模型时，我们需要小心的一件事是，管理器可能很容易退化为输出一个常量向量——不管手头的输入是什么。这导致一个 VAE 专用于所有数字，而九个值不专用于任何数字。在教育部的论文中描述了一种减轻损失的方法，即在损失中增加一个平衡项。它鼓励管理者的输出在一批输入中达到平衡:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/df84f563920e602574aecb054b1f47a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*PFPoXa3dGDSp7ukuZk2_RQ.png"/></div></figure><p id="82c3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">说够了，训练时间到了！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/d28b642ccd883ed9e63ad8f28e9c2d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*61mYvHRN4b7a6J7Qa8iItw.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/d34c6a7e4679ae5326f82b201166b196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9fZrOWZxM6YfBt4LqsBWIg.png"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk">Images generated by the experts. Each column belongs to a different expert.</figcaption></figure><p id="b8d5" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在最后一张图中，我们看到了每个专家都学到了什么。在每个时期之后，我们使用专家从他们擅长的分布中生成图像。第 I 列包含第 I 个专家生成的图像。</p><p id="5195" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们可以看到，一些专家很容易地专注于一个位数，例如— 1。有些人被相似的数字搞糊涂了，比如专家既擅长 3 又擅长 5。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/674e3916777b84f131b7bdac53faec25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FNpl11cSC2c8duG60KxVZg.jpeg"/></div></div><figcaption class="ls lt gj gh gi lu lv bd b be z dk"><em class="nx">An expert specializing in 2</em></figcaption></figure><h1 id="412b" class="md me it bd mf mg ny mi mj mk nz mm mn jz oa ka mp kc ob kd mr kf oc kg mt mu bi translated">还有什么？</h1><p id="7eb0" class="pw-post-body-paragraph ku kv it kw b kx mv ju kz la mw jx lc ld mx lf lg lh my lj lk ll mz ln lo lp im bi translated">使用一个简单的模型，不需要太多的调整，我们得到了合理的结果。最理想的情况是，我们希望每个专家专门研究一个数字，从而通过管理器的输出实现完美的无监督分类。</p><p id="80e1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">另一个有趣的实验是把每个专家变成他们自己的 MoE！它将允许我们学习 vae 应该专门化的分级参数。例如，一些数字有多种绘制方式:7 可以有或没有删除线。这种变化的来源可以由层次结构中第二级的教育部来模拟。但是我会为以后的帖子留些东西…</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="c73b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="od">原文由我在</em><a class="ae lq" href="http://anotherdatum.com" rel="noopener ugc nofollow" target="_blank"><em class="od"/></a><em class="od">发表。</em></p></div></div>    
</body>
</html>