# 信用卡不应该是黑色的

> 原文：<https://towardsdatascience.com/credit-card-should-not-be-black-b810be66bbf8?source=collection_archive---------23----------------------->

![](img/630dbd18f79d08e464ca5dab2e62f8d7.png)

“File:Insert Credit Card (61471) — The Noun Project.svg” by Icons8 is marked under CC0 1.0\. To view the terms, visit [http://creativecommons.org/publicdomain/zero/1.0/deed.en](http://creativecommons.org/publicdomain/zero/1.0/deed.en)

信用卡不应该是黑色的。我们在西班牙经历了惨痛的教训，但我们最近发现了另一个黑卡不是好主意的例子。

一位非常知名的软件开发人员在本月早些时候发布了这条推文，他在其中声称，用于信用评分的全新 Apple Card 算法是性别歧视的。

当苹果联合创始人史蒂夫·沃兹尼亚克加入谈话时，事情变得更糟，他基本上声称自己也遭受了同样的问题。

苹果公司和高盛公司对这一病毒式的新闻做出了回应，声明**该算法是由第三方针对偏见**进行评估的，并且性别没有被用作该算法计算信贷限额的输入数据。

事实是**在美国，性别不能作为任何信贷决策**的一部分，正如美联储规定的[。](https://www.federalreserve.gov/econres/notes/feds-notes/gender-related-differences-in-credit-use-and-credit-scores-20180622.htm)

> *“《平等机会信贷法案》在很大程度上禁止在信贷承销、定价、报告和评分中使用人口统计信息，包括性别。”*

然而，性别不是一个算法变量并不会使情况变得更好。事实上，正如 [Wired 在本文](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/)中指出的，这让事情变得更糟:

> 这个解释有双重误导性。首先，算法完全有可能进行性别歧视，即使它们被编程为对该变量“视而不见”。另一方面，对性别这样重要的事情故意视而不见只会让公司更难发现、防止和扭转对这一变量的偏见。

正如凯茜·奥尼尔在一次关于苹果卡事件的采访中所指出的:

> 我们必须发展一种新的谈论方式，因为旧的谈论方式已经行不通了。旧的谈论方式是，“让我们不要明确地使用种族或性别。”当我们没有收集关于彼此和我们自己的大量数据时，这种方法就起作用了，而且它不是到处都可以得到的。但是现在我们可以根据各种不同的指标来推断种族和性别。对于那些对推断这类事情感兴趣的公司来说，在 wazoo 上有这类课程的代理。从统计学上来说，它们比随机猜测要好很多。仅仅说“我们不要明确使用种族和性别”已经不够了。

这就是为什么，正如我在上一篇短文([计算机说不](/computer-says-no-52a9c31cb8f2?source=friends_link&sk=520a61d2a467aeaf716de27fc7dee6b3)，**中介绍的，我们不仅需要算法的可解释性，还需要可追溯性和可审计性**，**将我们从可解释的人工智能转移到可追溯和透明的人工智能，**这也是为什么我们不应该让信用卡成为黑色的**，黑色的算法盒子。**

*如果你喜欢阅读这篇文章，请* [*考虑成为会员*](https://dpereirapaz.medium.com/membership) *以便在支持我和媒体上的其他作者的同时，获得每个故事的全部信息。*