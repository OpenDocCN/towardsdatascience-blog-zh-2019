<html>
<head>
<title>Generating High-Resolution Images Using Deep Autoregressive Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度自回归模型生成高分辨率图像</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-high-resolution-images-using-autoregressive-models-3683f9af0db4?source=collection_archive---------11-----------------------#2019-07-04">https://towardsdatascience.com/generating-high-resolution-images-using-autoregressive-models-3683f9af0db4?source=collection_archive---------11-----------------------#2019-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d2c8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">超越 GANs，捕捉真实数据分布的多样性</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2d4f56fd5ff055c7392e33dfca9dea00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YNMwl_o0WL3YRajL"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 1. High-resolution (256x256 pixels) 8-bit celebrity images generated using a deep autoregressive model trained on the CelebA-HQ dataset.</figcaption></figure><h1 id="b17a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="86e6" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">图 1 中的名人面孔并不存在。它们是使用深度自回归模型生成的，该模型是由谷歌 Deepmind 和谷歌大脑的研究人员在一篇题为“<a class="ae mj" href="https://arxiv.org/abs/1812.01608" rel="noopener ugc nofollow" target="_blank">用亚尺度像素网络和多维尺度放大生成高保真图像</a>”的论文中引入的。这项工作特别重要，因为它朝着生成高分辨率、高保真度的自然图像迈出了一大步，同时支持整个数据分布并保证模型的概化能力。不用担心这里介绍的不熟悉的术语。在这篇文章中，我将带你浏览这篇文章，并解释所有的术语。最近我也在<a class="ae mj" href="https://aisc.ai.science/" rel="noopener ugc nofollow" target="_blank"> AISC </a>展示并讨论了这项工作。你可以在<a class="ae mj" href="https://youtu.be/CCE5trqFanA" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上找到完整的演示。</p><p id="8e9d" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">在我们深入阅读本文之前，让我先概述一下我们的目标:</p><ol class=""><li id="8554" class="mp mq iq lp b lq mk lt ml lw mr ma ms me mt mi mu mv mw mx bi translated">文件概述:其目标和贡献；</li><li id="75e9" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">图像生成的深度生成模型与该领域最新趋势的比较:</li><li id="f36b" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">图像生成的深度自回归模型概述:它们是什么，为什么我们对它们感兴趣，以及以前方法的主要挑战；和</li><li id="f6e1" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">深入探究子尺度像素网络和多尺度放大:它们是什么，它们为什么工作，以及它们如何与以前的深度自回归模型进行比较。</li></ol><p id="ec63" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">本文的目标是构建一个深度自回归(AR)解码器，它可以无条件地生成高保真图像。保真度是指模型生成真实图像的程度，这项工作能够使用深度 AR 模型在高分辨率下生成高度逼真的图像。我们将讨论与使用深度 AR 模型生成高分辨率、高保真度图像相关的主要挑战，并且我们将解释在这项工作中引入的用于解决这些问题的解决方案。</p><p id="2077" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">这里将简单介绍的<a class="ae mj" href="https://arxiv.org/abs/1601.06759" rel="noopener ugc nofollow" target="_blank"> pixelRNN、pixelCNN </a>等深度 AR 模型就是成功利用深度 AR 模型进行图像生成的例子；但是，它们只能生成分辨率相对较低的图像(32x32 和 64x64 像素图像)。使用深度 AR 模型生成高分辨率图像具有挑战性，因为网络的大小随着图像大小线性增加，这是不可持续的。然而，该论文生成最大尺寸为 256x256 的图像，并且网络的尺寸不依赖于图像尺寸。他们使用在受欢迎的<a class="ae mj" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html" rel="noopener ugc nofollow" target="_blank"> CelebA-HQ 数据集</a>上训练的模型生成 256x256 张名人脸。他们还使用在著名的<a class="ae mj" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet 数据集</a>上训练的模型，生成各种类别的 32x32 到 256x256 图像。</p><h1 id="439c" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">图像的深层生成模型</h1><p id="20bd" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">生成模型旨在学习训练数据的经验分布，并通过对学习的分布进行采样来生成图像，在样本质量和样本多样性之间进行权衡。图像的深度生成模型可以分为三个主要类别:</p><ol class=""><li id="c067" class="mp mq iq lp b lq mk lt ml lw mr ma ms me mt mi mu mv mw mx bi translated">可变自动编码器(VAEs)，</li><li id="c671" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">生成对抗网络(GANs)，以及</li><li id="15fd" class="mp mq iq lp b lq my lt mz lw na ma nb me nc mi mu mv mw mx bi translated">自回归(AR)模型。</li></ol><p id="74be" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">变分推理模型(各种类型的变分自动编码器)逼近潜在空间，并通过对所学习的潜在空间进行采样来生成一组不同的图像；然而，生成的图像往往是模糊的。对抗图像生成模型(各种类型的生成对抗网络)生成清晰、高分辨率的图像；但是，不能保证这些模型以有意义的方式学习整个数据分布，并且生成的样本可能不会覆盖整个数据分布。最后，自回归模型捕捉整个数据分布，保证生成一组不同的样本；然而，AR 模型往往局限于低分辨率图像，因为内存和计算要求随着图像的大小而增长。这是本文试图解决的问题。</p><p id="89ff" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">比较这些模式时要考虑的其他要点是它们的训练过程和有效采样，即快速生成新样本的能力。变分推理和对抗模型在采样时相对有效；然而，这是自回归模型研究的一个开放领域，因为它们在采样过程中往往效率低下。AR 方法是具有稳定训练过程的简单模型，而对抗性方法通常在训练期间不稳定并且难以训练。</p><h1 id="62d2" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">图像深度生成建模的最新趋势</h1><p id="94d6" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">谷歌 Deepmind 的研究人员最近发表了许多突破性的研究论文，都专注于图像的深度生成模型(见图 2)。这些论文的中心主题是，可以生成高保真度和高分辨率的样本，而不会遭受 GAN 的缺点，如模式崩溃和缺乏多样性。这些结果非常重要，因为它们显示了在探索除 GANs 之外的方法中的价值，例如用于图像生成任务的变分推断方法和自回归方法。他们还引入了一个新的度量标准来衡量模型对数据分布的了解程度。这个想法是，如果一个模型已经学习了整个数据分布，那么它生成的样本可以成功地用于下游任务，如分类。他们表明，对抗方法比变分推断和自回归模型获得更低的分数，表明对抗方法不一定学习整个数据分布，尽管它们能够生成高保真图像。本文成功使用自回归模型生成高分辨率、高保真图像，同时保证学习整个数据分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/cca2b2d76790086778af1d3c3ab353b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6UING_wpxjnp3VmaGrq0Yw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 2. Recent papers published by researchers at Google Deepmind with a common theme.</figcaption></figure><h1 id="d3a4" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">自回归模型</h1><p id="f30e" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">AR 模型将图像视为一系列像素，并将其概率表示为所有像素的条件概率的乘积。如下式所示，每个像素强度的概率取决于所有先前生成的像素。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e10a7ea76c19557f47152043b3e03c19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*njiVZNax30l4Zps-W2f_UA.png"/></div></figure><p id="aa61" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">换句话说，为了生成图 3 中的像素<em class="nf"> xi </em>，我们需要之前生成的所有蓝色像素的亮度值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/96063825cec994cf4eba30d93a0616f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*VXbWNJRq4ZXmJ7F-nE33mA.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 3. Generating individual image pixels using an autoregressive model</figcaption></figure><p id="d9e7" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">对抗模型的挑战之一是没有内在的概括措施。在文献中已经提出了不同的度量，但是没有关于单个度量的共识，使得比较不同图像生成模型的性能具有挑战性。另一方面，AR 模型由于其目标函数而被迫捕捉整个数据分布。它们是基于似然的模型，通过最大似然估计来训练。这允许直接计算负对数似然(NLL)分数，这是对模型在保留数据上进行良好概括的能力的一种测量，并可用作测量生成样本的视觉保真度的代理。一个成功的生成模型能产生高保真的样本，并能很好地概括保留的数据。虽然对抗方法产生高保真图像，但不能保证捕捉到整个数据分布。</p><h1 id="3375" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">以前的尝试</h1><p id="0407" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在过去的几年中，人们已经做出了许多努力来使用深度 AR 模型来顺序预测图像中的像素。最著名的型号是<a class="ae mj" href="https://arxiv.org/abs/1601.06759" rel="noopener ugc nofollow" target="_blank"> PixelRNN 和 PixelCNN </a>。PixelRNN 使用 LSTM 层从先前生成的像素中捕获信息，而 PixelCNN 对先前生成的像素使用掩蔽卷积。最近的<a class="ae mj" href="https://arxiv.org/abs/1606.05328" rel="noopener ugc nofollow" target="_blank">门控 PixelCNN </a>架构通过对 LSTM 门使用掩蔽卷积，结合了 pixelRNN 和 PixelCNN 的思想，实现了与 PixelRNN 相当的性能，同时训练速度与 pixelCNN 一样快。最近，<a class="ae mj" href="https://arxiv.org/abs/1712.09763" rel="noopener ugc nofollow" target="_blank"> PixelSNAIL </a>架构结合了掩蔽卷积和自关注，利用卷积在有限上下文大小上的高带宽访问和自关注在无限大上下文上的访问来生成图像。本文中描述的基线解码器使用类似 PixelSNAIL 的架构。</p><p id="508f" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">尽管做出了这些努力，AR 模型仍然需要生成高分辨率的样本，并捕捉长程结构和语义一致性。此外，由于 AR 模型被迫支持整个数据分布，它们倾向于将容量用于与保真度无关的部分分布。</p><h1 id="62fe" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">亚比例像素网络</h1><p id="4266" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">本文中描述的论文旨在通过引入亚尺度像素网络(SPN)以及将大图像划分为一系列大小相等的切片的替代排序，来生成捕捉细节和全局一致性的高分辨率、高保真图像。如图 4 所示，从原始图像每隔<em class="nf"> n </em>个像素对图像切片进行二次采样。这种架构将模型大小与图像分辨率/大小分离，从而保持内存和计算需求不变。它还可以轻松地对大图像中许多像素之间的长程相关性进行压缩编码。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/0ab9c394e48d305f66220c19931f3617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dE-ACZoTnk_MtkGJy0zaRQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 4. Subsampling 4 image slices from the original image</figcaption></figure><p id="ed7a" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">SPN 是一种条件解码器，它将图像生成为一系列大小相等的子图像。这些子图像随后在尺寸和深度上增长，以在称为多维放大的过程中生成全分辨率、全深度图像。这个过程如图 5 所示，可以描述为引导模型首先关注分布中视觉上更显著的位，然后关注视觉上不太显著的位。该文件确定了两个视觉突出的子集:大小和深度。尺寸是指生成初始切片(子图像),然后通过以编码丰富空间结构的方式基于先前生成的切片一次生成一个切片来对原始图像进行上采样。深度是指用于分别生成每个 RGB 通道的最高和最低有效位的不同网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/9c703a76db3be16b17de2cd2df3911a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IDyZFE9Y_QV9VpCP00RaKQ.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 5. Three networks working together to generate the final full-size full-depth image</figcaption></figure><p id="f25a" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">因此，训练了 3 个具有相似架构的独立网络:</p><p id="66f1" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">(a)在小尺寸、低深度图像切片上训练的解码器；</p><p id="168b" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">(b)尺寸放大解码器，其生成以初始图像切片为条件的大尺寸、低深度图像，即小尺寸、低深度图像；和</p><p id="fbc5" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">(c)深度提升解码器，其以大尺寸、低深度图像为条件生成大尺寸、高深度图像。</p><p id="e18b" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">图 6 (i)示出了网络(a)和(b)一起工作以生成全尺寸、低深度图像，而图 6 (ii)示出了用于生成大尺寸、高深度图像的所有三个网络。最初的小尺寸、低深度图像切片(白色像素)是使用网络(a)生成的。然后，网络(b)用于通过生成蓝色像素来生成大尺寸、低深度的图像。最后，网络(c)用于生成紫色像素，该紫色像素代表大尺寸图像的每个 RGB 通道的剩余位。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/9c3042464da8041f0f6bf0ce190e2f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*rlUqaetNSjlANhMOVT0r7A.png"/></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 6. Distinct colors correspond to distinct neural networks used to generates various pixels of an image</figcaption></figure><p id="feb5" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">SPN 架构如图 7 所示。该网络具有编码器或切片嵌入器和屏蔽解码器。编码器将先前生成的沿深度维度连接的切片作为输入，并将先前切片的上下文编码在单个切片形状张量中。该嵌入被传递到解码器，该解码器使用掩蔽卷积和自关注层来生成图像切片。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/f460848073380c2e5381b1a2de417b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u9bb4enql0BOCM_1c9luMg.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 7. Subscale pixel network architecture</figcaption></figure><p id="c9c3" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">您可以在<a class="ae mj" href="https://youtu.be/CCE5trqFanA" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上观看完整的演示，了解 SPN 架构的详细解释以及围绕本文算法和结果的进一步讨论。</p><h1 id="9d24" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="941d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在这篇文章中，我介绍了一种新颖的深度 AR 架构，它能够学习复杂域的分布，如名人图像，并从所得的学习分布中生成高保真样本。生成的样本显示了前所未有的语义一致性和细节的准确性，即使在高分辨率下也是如此，这令人印象深刻。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/639018cb6bdbefe5ee3b80d5537358c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iUfz-NE6eAdSUYGZ"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Figure 8. High-resolution (128x128 pixels) 8-bit images generated using a deep autoregressive model trained on the ImageNet dataset.</figcaption></figure><p id="47e6" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">本文表明，学习复杂自然图像的分布并获得高样本分辨率和保真度是可能的。获得捕捉整个数据分布的准确数据表示对于生成各种高分辨率样本至关重要，本文介绍的想法是朝着这个方向迈出的重要一步。这对我们<a class="ae mj" href="https://looka.com/" rel="noopener ugc nofollow" target="_blank"> Looka </a>来说尤其重要，因为我们的目标是创造各种高质量的设计资产，这对创造伟大的设计至关重要。Looka 的使命是让每个人都可以获得伟大的设计，我们使用深度学习来创建与图形设计师在线合作的体验。特别是，我们正在使用深度生成模型，如本文中介绍的模型，来自动生成令人惊叹的设计资产，如独特的字体和符号。</p><p id="32ac" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我希望你喜欢这篇文章。如果您有进一步的问题或意见，请随时联系我们。</p></div><div class="ab cl nm nn hu no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="ij ik il im in"><p id="6aa8" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated"><em class="nf">我是</em> <a class="ae mj" href="https://looka.com/" rel="noopener ugc nofollow" target="_blank"> <em class="nf"> Looka </em> </a> <em class="nf">的一名数据科学家，在这里，我们利用深度学习让每个人都能接触到并喜欢伟大的设计。如果你有兴趣在 AI 和设计的交叉点工作，可以看看我们的</em> <a class="ae mj" href="https://looka.com/careers" rel="noopener ugc nofollow" target="_blank"> <em class="nf">招聘页面</em> </a> <em class="nf">。</em></p></div></div>    
</body>
</html>