<html>
<head>
<title>Deep Double Descent: when more data is a bad thing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度双重下降:当更多的数据是一件坏事</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538?source=collection_archive---------13-----------------------#2019-12-08">https://towardsdatascience.com/deep-double-descent-when-more-data-and-bigger-models-are-a-bad-thing-3a3f108d5538?source=collection_archive---------13-----------------------#2019-12-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f331" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解决经典统计学和现代 ML 建议之间的根本冲突</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/347296699e302a21a7f3e307de78f872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vk6WWWeKIn0xqsnl"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Franki Chamaki</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="58bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近偶然看到一篇<a class="ae ky" href="https://arxiv.org/abs/1912.02292" rel="noopener ugc nofollow" target="_blank">非常有意思的论文</a>写于 OpenAI，题目是深度双下降。该论文触及了训练机器学习系统和模型复杂性的本质。我希望在这篇文章中以一种可接近的方式总结本文中的观点，并推进对模型大小、数据量和正则化之间权衡的讨论。</p><h2 id="e1bb" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">问题是</h2><p id="b0b6" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">统计学习和现代最大似然理论之间存在着根本的冲突。经典统计学认为<em class="mt">太大的模型是不好的。</em>这是因为复杂的模型更容易过度拟合。事实上，经典统计学中经常应用的一个强有力的定理是<a class="ae ky" href="https://papers.nips.cc/paper/1925-occams-razor.pdf" rel="noopener ugc nofollow" target="_blank">奥卡姆剃刀</a>，其本质是说 t <em class="mt">最简单的解释通常是正确的</em>。</p><p id="322c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这可以用一个可视化来清楚地解释。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/c052f75da7bd50df8d48f653d44013d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*8e7IKtRiLOmG3y3wP2SLIQ.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">The green line is an example of a model overfitting the training data, while the black line is a simpler model that approximates the true distribution of the data.</figcaption></figure><p id="55ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管复杂性和可推广性之间存在明显的权衡，但你经常会看到现代 ML 理论认为<em class="mt">更大的模型更好。有趣的是，这种说法在很大程度上似乎奏效了。来自世界上一些顶级人工智能研究团队的研究，包括来自谷歌和微软的团队，表明更深层次的模型尚未饱和。事实上，通过实施仔细的正则化和早期停止，似乎通常情况下，将您的模型的性能提高几个点的最佳方法是简单地添加更多的层或收集更多的训练数据。</em></p><h2 id="b736" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">深度双重下降</h2><p id="c951" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">OpenAI 论文的焦点提供了经典统计学和现代 ML 理论之间的矛盾的实际调查。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/6828db1779bb75f47d20df857d279f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYdbuImoyIbfHNzjjzrtuQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Empirical evidence shows that the truth of how modern machine learning systems work is a mixture of both classical statistics and modern theory.</figcaption></figure><p id="aa89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度双下降是指性能提高，然后随着模型开始过度拟合而变得更差，最后随着模型大小、数据大小或训练时间的增加而进一步提高的现象。上图以图形方式说明了这种行为。</p><p id="5ab3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度双下降现象在模型的复杂性、数据量和训练时间方面有多种含义。</p><p id="4b7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">有时，更大的型号更糟糕</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/b9bf598e38b8af9f0892dda78b99ecb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qq2qHPGOocjCYj85ZQ_S3g.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Before the model hits the interpretation threshold, there is a bias-variance tradeoff. Afterwards, the current wisdom of “Larger models are better” is applicable</figcaption></figure><p id="e92a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在对 ResNet18 进行实验时，OpenAI 的研究人员发现了一个关于偏差和方差之间权衡的有趣笔记。在模型的复杂性超过<strong class="lb iu">插值阈值</strong>之前，或者模型刚好大到足以适合训练集的点之前，较大的模型具有较高的测试误差。然而，在模型的复杂性允许它适合整个训练集之后，具有更多数据的更大模型开始表现得更好。</p><p id="fc2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">似乎有一个复杂的区域，在那里模型更容易过度拟合，但是如果在模型中捕捉到足够的复杂性，模型越大越好。</p><p id="ad8f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">有时，样本越多越糟糕</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/e4ecc70aa9e74527c59f09c2784659bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dqzY_B58qELMhI0pshMTgQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">There’s a point where models with more data actually perform worse on the test set. Again, however, there is a point near the interpolation threshold at which this reverses.</figcaption></figure><p id="1338" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有趣的是，对于低于插值阈值的模型，似乎更多的训练数据实际上会在测试集上产生更差的性能。然而，随着模型变得更加复杂，这种权衡发生了逆转，现代智慧“数据越多越好”开始再次适用。</p><p id="bca3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个有效的假设是，不太复杂的模型可能无法捕获太大训练集中所需的一切，因此无法很好地推广到看不见的数据。然而，随着模型变得足够复杂，它能够克服这个限制。</p><p id="ee5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">有时，训练时间越长，过度适应就会消失</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi my"><img src="../Images/2ebe209eac134b09c136ba1be9541db4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7tSKDAbLf5nyy4swAyA-g.png"/></div></div></figure><p id="fe97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面关于被训练的时期数的图表中，训练和测试误差首先随着时期数的增加而急剧下降。最终，随着模型开始过度拟合，测试误差开始增加。最后，随着过度拟合奇迹般地被消除，测试误差再次减小。</p><p id="5e99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在论文中，研究人员称之为划时代的双重下降。他们还注意到，测试误差的峰值正好在插值阈值处。这里的直觉是，如果一个模型不是非常复杂，那么只有一个模型最适合训练数据。如果模型符合噪声数据，其性能将大幅下降。但是，如果模型复杂到足以通过插值阈值，则有几个模型适合训练集和测试集，并且随着训练时间的延长，可以近似这些模型中的一个。发生这种情况的原因是一个公开的研究问题，并且对训练深度神经网络的未来非常重要。</p><p id="1570" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你对这个研究问题感兴趣，看看启发了这篇文章的<a class="ae ky" href="https://arxiv.org/pdf/1912.02292.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>和<a class="ae ky" href="https://openai.com/blog/deep-double-descent/" rel="noopener ugc nofollow" target="_blank">相关总结</a>。</p></div></div>    
</body>
</html>