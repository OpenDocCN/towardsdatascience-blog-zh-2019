<html>
<head>
<title>Deep Reinforcement Learning: Build a Deep Q-network(DQN) to Play CartPole</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习:建立一个深度 Q 网络(DQN)来玩钢管舞</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998?source=collection_archive---------1-----------------------#2019-12-16">https://towardsdatascience.com/deep-reinforcement-learning-build-a-deep-q-network-dqn-to-play-cartpole-with-tensorflow-2-and-gym-8e105744b998?source=collection_archive---------1-----------------------#2019-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/55041dfce4f035bdc653e10fd48a3069.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I83aE2Gah4C7UeWE"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Photo by the author</figcaption></figure><p id="3755" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在本教程中，我将向您介绍如何训练一个深度 Q-net(DQN)模型来玩掷骰子游戏。我们会用 OpenAI 的健身房和 TensorFlow 2。本文假设您对强化学习和深度学习有所了解。</p><h1 id="b140" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">什么是强化学习和 DQN？</h1><p id="c09d" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">让我们从强化学习和 DQN 算法的快速复习开始。强化学习是机器学习的一个领域，它专注于训练<strong class="kh iu">代理</strong>在<strong class="kh iu">环境</strong>中在某些<strong class="kh iu">状态</strong>采取某些<strong class="kh iu">动作</strong>，以最大化<strong class="kh iu">回报</strong>。比方说我想做一个打扑克的机器人(代理)。该机器人将与其他机器人在有筹码和卡片的扑克桌上玩(环境)。这个机器人应该能够根据桌上的牌、手里的牌和其他机器人的下注(状态)来弃牌或下注(行动)。机器人想要最大化它赢得游戏的筹码数量(奖励)。</p><p id="851c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">DQN 是一种强化学习算法，其中建立了一个深度学习模型，以找到代理在每个状态下可以采取的行动。</p><h1 id="35c6" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">技术定义</h1><p id="1575" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">RL 的基本术语包括但不限于:<strong class="kh iu">当前状态<em class="mg">【s】</em></strong><strong class="kh iu">下一步状态<em class="mg">【s’】</em></strong><strong class="kh iu">动作<em class="mg">【a】</em></strong><strong class="kh iu">政策<em class="mg">【p】</em></strong><strong class="kh iu">奖励<em class="mg">【r】</em></strong>。<strong class="kh iu">状态-动作-值函数<em class="mg"> (Q(s，a)) </em> </strong> <em class="mg"> </em>是<strong class="kh iu"> </strong>一个主体从当前状态开始的期望总报酬，其输出称为<strong class="kh iu"> Q 值</strong>。正如我所说的，我们的目标是在状态<em class="mg">【s】</em>选择某个动作<em class="mg">【a】</em>，以最大化回报，或 Q 值。</p><p id="435d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">DQN 是深度学习和强化学习的结合。模型目标是逼近<em class="mg"> Q(s，a) </em>，并通过反向传播进行更新。假设<em class="mg"> Q(s，a) </em>的近似值为 y(hat ),损失函数为 L，我们有:</p><p id="a9c0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">预测:<strong class="kh iu"> <em class="mg"> y(hat) = f(s，θ) </em> </strong></p><p id="7962" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">损耗:<strong class="kh iu"> <em class="mg"> L(y，y(hat)) = L(Q(s，a)，f(s，θ)) </em> </strong></p><p id="8b3e" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在反向传播过程中，我们对损失函数求θ的偏导数，以找到使损失最小的θ值。因为这是监督学习，你可能想知道如何找到基本事实<em class="mg"> Q(s，a)。</em>答案是用贝尔曼方程。</p><p id="e7de" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">贝尔曼方程:<strong class="kh iu"> <em class="mg"> Q(s，a)= max(r+Q(s’，a)) </em> </strong></p><p id="ff63" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在哪里</p><p id="3389" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"><em class="mg">Q(s’，a)= f(s’，θ) </em> </strong>，如果<strong class="kh iu"> <em class="mg"> s </em> </strong>不是<strong class="kh iu">终端状态</strong>(最后一步的状态)</p><p id="b499" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh iu"> <em class="mg"> Q(s '，a) = 0 </em> </strong>，如果<strong class="kh iu"> <em class="mg"> s </em> </strong>则为终端状态</p><p id="7ed0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们可以看到，当<strong class="kh iu"> <em class="mg"> s </em> </strong>为终态时，<strong class="kh iu"> <em class="mg"> Q(s，a) = r. </em> </strong>因为我们是用模型预测<em class="mg"> f(s '，θ) </em>来逼近<em class="mg"> Q(s '，a)</em>的真实值，我们把这个叫做半梯度。正如你可能已经意识到的，使用半梯度的一个问题是，模型更新可能非常不稳定，因为每次模型更新时，真实的目标都会改变。解决方案是创建一个<strong class="kh iu">目标网络</strong>，其中<strong class="kh iu"> </strong>本质上是在特定时间步骤的训练模型的副本，因此目标模型更新不太频繁。</p><p id="f127" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">该模型的另一个问题是过度拟合。当我们在每局结束后更新模型时，我们已经潜在地玩了几百步，所以我们本质上是在做批量梯度下降。因为每一批总是包含一个完整游戏的步骤，模型可能无法很好地从中学习。为了解决这个问题，我们创建了一个<strong class="kh iu">体验重放缓冲区</strong>，它存储了几百个游戏的<em class="mg"> (s，s '，a，r) </em>值，每次都从中随机选择一批来更新模型。</p><h1 id="7934" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">什么是横竿？</h1><p id="8e13" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated"><a class="ae mh" href="https://github.com/openai/gym/wiki/CartPole-v0" rel="noopener ugc nofollow" target="_blank"> CartPole </a>是一种游戏，其中一根杆子通过一个未驱动的关节连接到一辆小车上，小车沿着无摩擦的轨道移动。钟摆开始直立，目标是通过增加和减少小车的速度来防止它倒下。单个状态由 4 个元素组成:小车位置、小车速度、磁极角度和磁极尖端的速度。</p><p id="9a62" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">要移动杆子有两个动作:向左或向右移动。每走一步(包括终止一步)，它获得+1 奖励。当杆倒下时，游戏结束，即当杆角度大于 12°，或手推车位置大于 2.4°(手推车的中心到达显示器的边缘)。新的健身房版本也有一个长度限制，当剧集长度超过 200 集时就会终止游戏。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi mi"><img src="../Images/fda796e18e106b91c2793284c2e3f367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hy2XFb6r_kWw3OT3jWwW6A.png"/></div></div></figure><h1 id="281e" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">履行</h1><p id="be77" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">完整的代码是<a class="ae mh" href="https://github.com/VXU1230/reinforcement_learning" rel="noopener ugc nofollow" target="_blank">这里是</a>。</p><h2 id="b3b7" class="mn le it bd lf mo mp dn lj mq mr dp ln kq ms mt lr ku mu mv lv ky mw mx lz my bi translated">1.构建 tf.keras 模型类</h2><p id="f27e" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">我们先在 TensorFlow 中实现深度学习神经网络模型<em class="mg"> f(s，θ) </em>。在 TF2，急切执行是默认模式，因此我们不再需要先创建操作，然后在会话中运行它们。另外，TF2 在<code class="fe mz na nb nc b">tf.function()</code>提供亲笔签名。有两种方法可以实例化一个模型。更简单的方法是通过链接 Keras 层来指定模型的正向传递，并从输入和输出创建模型。然而，为了训练更复杂和定制的模型，我们需要通过子类化 Keras 模型来构建模型类。更多详情，请见<a class="ae mh" href="https://www.tensorflow.org/alpha/guide/effective_tf2" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="a08f" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在 MyModel 类中，我们在<code class="fe mz na nb nc b">__init__</code>中定义了所有层，并在<code class="fe mz na nb nc b">call().</code>中实现了模型的向前传递。注意，输入形状是【批量大小，状态大小(本例中为 4)】,输出形状是【批量大小，动作数量(本例中为 2)】。本质上，我们向模型提供状态，并输出在每个状态下采取每个动作的值。<code class="fe mz na nb nc b">call() </code>的<code class="fe mz na nb nc b">@tf.function </code>注释支持自动签名和自动控制依赖。</p><h2 id="41c9" class="mn le it bd lf mo mp dn lj mq mr dp ln kq ms mt lr ku mu mv lv ky mw mx lz my bi translated">2.构建主 DQN 模型类</h2><p id="3dbb" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">主 DQN 类是创建、调用和更新深度 Q-net 模型的地方。我们刚刚建立的神经网络模型是深度 Q-net 模型的一部分。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="664d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在<code class="fe mz na nb nc b">__init__()</code>中，我们定义了梯度下降的动作数量、批量大小和优化器。折扣因子γ是一个介于 0 和 1 之间的值，在下一步乘以 Q 值，因为代理人更不关心遥远未来的回报，而不是眼前的回报。我们还将 MyModel 初始化为实例变量<code class="fe mz na nb nc b">self.mode</code>，并创建体验回放缓冲区<code class="fe mz na nb nc b">self.experience.</code>，代理不会开始学习，除非缓冲区的大小大于<code class="fe mz na nb nc b">self.min_experience</code>，一旦缓冲区达到最大大小<code class="fe mz na nb nc b">self.max_experience</code>，它将删除最早的值，为新值腾出空间。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="0e56" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">实例方法<code class="fe mz na nb nc b">predict()</code>接受单个状态或一批状态作为输入，运行<code class="fe mz na nb nc b">self.model</code>的正向传递并返回模型结果(动作的逻辑)。请注意，tf.keras 模型默认将输入识别为批处理，因此我们希望确保输入至少有 2 个维度，即使它是单个状态。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="15c4" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在<code class="fe mz na nb nc b">train()</code>中，我们首先随机选择一批<em class="mg"> (s，s’，a，r) </em>值，用布尔<em class="mg"> done </em>表示当前状态<em class="mg"> (s) </em>是否为终止状态。然后我们调用<code class="fe mz na nb nc b">predict()</code>来获得下一个状态的值。注意，我们在这里使用复制的目标网络来稳定这些值。接下来，我们从贝尔曼函数中得到地面真值。如前所述，如果 state <em class="mg"> (s) </em>是终端状态，target <em class="mg"> Q(s，a) </em>只是奖励<em class="mg"> (r)。</em>在<code class="fe mz na nb nc b">tf.GradientTape()</code>内，我们计算真实目标和预测的均方损失。因为我们没有使用内置的损失函数，所以我们需要使用<code class="fe mz na nb nc b">tf.one_hot()</code>手动屏蔽逻辑。一旦我们得到损失张量，我们就可以使用方便的 TensorFlow 内置 ops 来执行反向传播。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="b489" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">RL 中的另一个重要概念是<strong class="kh iu">ε贪婪</strong>。ε是随时间衰减的 0 到 1 之间的值。这个想法是为了平衡探索和开发。当模型在开始时不太精确时，我们希望通过选择随机动作来探索更多，所以我们选择更大的ε。随着我们从玩游戏中收集更多的数据，我们逐渐衰减ε来更多地利用模型。ε-greedy 的实现在<code class="fe mz na nb nc b">get_action()</code>。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="84fa" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在<code class="fe mz na nb nc b">add_experience()</code>和<code class="fe mz na nb nc b">copy_weights()</code>中，我们实现了前面提到的<strong class="kh iu">经验回放缓冲</strong>和<strong class="kh iu">目标网络</strong>技术。每次我们从玩游戏中收集新数据时，我们会将数据添加到缓冲区，同时确保它不会超过定义为<code class="fe mz na nb nc b">self.max_experiences</code>的限制。我们将创建 DQN 类的两个实例:一个训练网和一个目标网。当训练网络用于更新权重时，目标网络仅执行两个任务:在下一步骤<em class="mg">Q(s’，a) </em>预测值，以供训练网络在<code class="fe mz na nb nc b">train()</code>中更新，以及从训练网络复制权重。</p><h2 id="79e1" class="mn le it bd lf mo mp dn lj mq mr dp ln kq ms mt lr ku mu mv lv ky mw mx lz my bi translated">3.玩游戏</h2><p id="e150" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">终于可以玩游戏了！</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="c2a0" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们通过向<code class="fe mz na nb nc b">play_game()</code>函数传递 5 个参数来开始游戏:健身房预定义的横竿环境、训练网、目标网、epsilon 和重量复制的间隔步数。在函数内部，我们首先重置环境以获得初始状态。然后我们创建一个循环来玩这个游戏，直到它到达终端状态。在循环内，我们ε-greedy 选择一个动作，移动一步，将<em class="mg"> (s，s '，a，r) </em>和<em class="mg"> done </em>对添加到缓冲区，并训练模型。默认情况下，环境总是为每个时间步长提供+1 的奖励，但为了惩罚模型，我们在完成完整集之前，当奖励达到终止状态时，我们为奖励分配-200。<code class="fe mz na nb nc b">iter</code>记录我们在一场比赛中走了多少步，这样我们就可以在每一步<code class="fe mz na nb nc b">copy_step</code>将重量复制到目标网。游戏结束后，我们会返还全部奖励。</p><h2 id="484c" class="mn le it bd lf mo mp dn lj mq mr dp ln kq ms mt lr ku mu mv lv ky mw mx lz my bi translated">4.制作测试视频</h2><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="70d6" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">训练完模型后，我们想看看它在横竿游戏中的实际表现。为此，我们简单地将 CartPole 环境包装在<code class="fe mz na nb nc b">wrappers.Monitor</code>中，并定义一个保存视频的路径。我们通过充分利用模型来玩游戏，游戏结束后会保存一段视频。</p><h2 id="8785" class="mn le it bd lf mo mp dn lj mq mr dp ln kq ms mt lr ku mu mv lv ky mw mx lz my bi translated">5.定义超参数和 Tensorboard 日志，并把一切放在一起</h2><p id="f4a9" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">DQN 模型现在已经建立起来了，我们需要做的就是定义我们的超参数，为 Tensorboard 输出日志并训练模型。让我们看看这是如何在<code class="fe mz na nb nc b">main()</code>函数中实现的。</p><figure class="mj mk ml mm gt ju"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="5444" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们首先创建了体操侧身环境、训练网和目标网。然后，我们定义超参数和张量流摘要编写器。当前的超参数设置将在 15000 集后产生 200 的集奖励，这是当前 200 集长度内的最高奖励。然而，我们的模型很不稳定，进一步的超参数调整是必要的。</p><p id="ef4a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在 for 循环中，我们玩 50000 次游戏，随着玩游戏次数的增加而衰减ε。对于 Tensorboard 可视化，我们还跟踪每个游戏的奖励，以及窗口大小为 100 的运行平均奖励。最后我们通过调用<code class="fe mz na nb nc b">make_video()</code>做一个视频，关闭环境。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c642452d1b62ccffc56f8024d71258fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*pyASv5UtwRNcpTTBEA0pEw.gif"/></div></figure><p id="dc85" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">一旦测试完成，你应该可以在你指定的文件夹中看到这样的视频。</p><p id="5288" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">要启动 Tensorboard，只需输入<code class="fe mz na nb nc b">tensorboard --logdir <em class="mg">log_dir</em>(the path of your Tensorflow summary writer)</code>。</p><figure class="mj mk ml mm gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi ng"><img src="../Images/7b348d0e72b65a325ef0c93eca0b99b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yxLgwyY8ADo6bYyoB5YdeQ.png"/></div></div></figure><p id="960d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在您的终端(Mac)中，您将看到一个带有 Tensorflow 端口的本地主机 IP。点击它，您将能够在 Tensorboard 上查看您的奖励。</p><h1 id="be17" class="ld le it bd lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma bi translated">结论及对未来工作的建议</h1><p id="fe5e" class="pw-post-body-paragraph kf kg it kh b ki mb kk kl km mc ko kp kq md ks kt ku me kw kx ky mf la lb lc im bi translated">在本文中，我们了解到:</p><ol class=""><li id="abf3" class="nh ni it kh b ki kj km kn kq nj ku nk ky nl lc nm nn no np bi translated">强化学习和 DQN 算法:</li><li id="4d20" class="nh ni it kh b ki nq km nr kq ns ku nt ky nu lc nm nn no np bi translated">通过在 TF 2 中子类化 tf.keras.Model 来构建定制模型；</li><li id="f05e" class="nh ni it kh b ki nq km nr kq ns ku nt ky nu lc nm nn no np bi translated">用 tf 训练一个 tf.keras.Model。gradient . Tape()；</li><li id="f701" class="nh ni it kh b ki nq km nr kq ns ku nt ky nu lc nm nn no np bi translated">在包装器中创建视频。监测以检验 DQN 模型；</li><li id="cb3a" class="nh ni it kh b ki nq km nr kq ns ku nt ky nu lc nm nn no np bi translated">在 Tensorboard 上显示奖励。</li></ol><p id="705a" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">但值得一提的是，由于自举、采样以及值函数逼近带来的不稳定性，DQN 并不能保证收敛。对 DQN 来说，横竿也是一个相对难学的环境。事实上，学习经常失败，上面显示的结果是在多次尝试后取得的。我很幸运地在 50k 个纪元内实现了收敛，但我怀疑这种情况每次都会发生。为了复制它，我强烈建议你至少运行几百万个纪元。</p><p id="28dc" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如果想要实现快速学习，可以使用较小的初始ε值，或者以较快的速率衰减。我当前的 epsilon 是 0.99，这意味着在 epsilon 衰减到一个很小的值之前，策略会渲染很长时间的随机动作。然而，ε的小初始值可能由于缺乏探索而导致学习陷入次优。</p><p id="ca8b" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">另一个重要的事实是，DQN 是一种偏离策略的算法，只要满足<a class="ae mh" href="https://en.wikipedia.org/wiki/Stochastic_approximation" rel="noopener ugc nofollow" target="_blank"> Robbins-Monro 算法</a>并且所有状态和动作已经被访问了无限次，Q 值甚至可以收敛于随机策略。因此，Q 值可能已经收敛，并准备在许多时代后用于预测，即使回报看起来并不完美。</p><p id="c41d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了使策略收敛，一旦值收敛，我们需要确保策略接近贪婪(ε接近 0)，这可以通过播放足够多的剧集来实现。</p><p id="788c" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">为了进一步改进，我鼓励你探索更多的模型结构(更多的层，卷积层，其他激活函数等)。)和超参数设置(目标净复制步长、经验大小、ε衰减值等。).</p><p id="8e1d" class="pw-post-body-paragraph kf kg it kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我希望你能从这篇文章中得到乐趣。:)</p></div></div>    
</body>
</html>