<html>
<head>
<title>13 Solutions to Multi-Arm Bandit Problem for Non-Mathematicians</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非数学家多臂 Bandit 问题的 13 种解决方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc?source=collection_archive---------9-----------------------#2019-01-14">https://towardsdatascience.com/13-solutions-to-multi-arm-bandit-problem-for-non-mathematicians-1b88b4c0b3fc?source=collection_archive---------9-----------------------#2019-01-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e0d5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">非数学家(比如我)多臂 bandit 问题的简单解决方案。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/67e083baf681bd3b1244d23ff44f72e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*5l2Kug-fHCNz4QATsyviWw.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">GIF from this <a class="ae ky" href="https://giphy.com/gifs/upvote-machine-upvotegifs-4eQFLKTo1Tymc" rel="noopener ugc nofollow" target="_blank">website</a></figcaption></figure><p id="c4f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">简介</strong></p><p id="dea4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">问题很简单，我们有一个有 n 个手臂的老虎机。我们对哪只手臂可以拉的试验次数有限，我们也不知道哪只手臂会给我们最多的钱。</p><p id="24ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设概率分布不随时间变化(意味着这是一个平稳问题)…</p><blockquote class="lv lw lx"><p id="456b" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">我们应该拉哪只胳膊？我们应该拉过去给我们最多奖励的那只手臂，还是应该探索，希望得到更好的手臂？</p></blockquote><p id="28f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个问题有多种解决方案，通常，人们衡量后悔是为了给每个解决方案排名。(遗憾==简单地说，我们因为没有拉最佳臂而得到的惩罚量。).因此，为了减少遗憾，我们只需拉起最有可能给我们奖励的手臂。</p><p id="2a00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但我想看看额外的测量，具体来说，我还会考虑每个算法对每个臂的概率分布估计得有多好。(他们给我们奖励的概率)。在更小的范围内，我们只有 12 只手臂，在更大的范围内，我们有 1000 只手臂。</p><p id="3581" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，这篇文章的目的是为非数学家(比如我)提供每个解决方案的简单实现。因此，理论保证和证明没有讨论，但我提供了不同的链接，为人们谁希望更深入地研究这个问题。</p><p id="5fb9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是我们将要比较的方法列表…..</p><p id="6143" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="ly"> 1)矢量化<br/> 2)贪婪<br/>3)e-贪婪<br/> 4)衰变 e-贪婪<br/> 5)线性奖励不作为(追踪方法)<br/> 6)线性奖励惩罚(追踪方法)<br/> 7)置信上限<br/> 8)置信上限调谐<br/> 9)汤普森采样(贝塔分布)<br/> 10)汤普森采样(均匀分布)<br/> 11)神经网络<br/> 12)玻尔兹曼探索(Softmax)【12】</em></strong></p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="281b" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">1.矢量化</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="2f9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这个方法实际上不是一个解决方案。多臂土匪问题背后的想法是如何在勘探和开发之间取得最佳平衡。</p><p id="096f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们只是计算每只手臂能得到多少奖励，然后除以我们拉每只手臂的次数。(因此直接计算获得奖励的百分比。)</p><p id="eb91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我只是想展示一个简单的方法来估计概率分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/cbee8ae9afc77d3a170e4f823da8daeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1314/format:webp/1*cLheDqgTXdWx2VzV2XsqOg.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="742e" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">2.<strong class="ak"> <em class="nf">贪婪</em> </strong></h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="cae3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们的策略是拉给我们最大回报的手臂。但是，最开始的问题是，我们不知道每个手臂的概率是如何分布的。</p><p id="26e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我决定通过从均匀分布初始化我们的估计分布来给出一些“信息性的”先验。(我猜这可能与“最佳初始化方案”有点关系，但这不是我的意图，在全零之前初始化我们也可以。).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/7ee07175df0289dcae24375c444ed59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*-D21j_s2Z_FkdbnpCR65zg.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="5a24" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">3.贪婪的</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="2148" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似的概念，贪婪的方法，但有一些概率ε，我们探索和选择一个随机土匪，而不是选择手臂，我们认为有最高的概率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/ac8752393794d813318cb027ffc6b25e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*_gSyPpcUf2eBec86F1N3Ow.png"/></div></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="2067" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">4.腐败贪婪</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7b67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与 e-Greedy 方法的想法完全相同，但我们在这里慢慢降低ε。(我们探索的概率)。因此，随着时间的推移，这种方法将完全相同的贪婪方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/0aa218e9dd591ce7e5d94ee68e874bad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*y-RUiNt-WGUSAU2ZMVngBA.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="b2e9" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">5.线性奖励无为(追求法)</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="6ee7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">追踪方法背后的一般思想是保持对盗匪的明确策略，并使用经验方法更新它们。开始时，概率分布是统一初始化的(意味着每个手臂都有同等的机会被选中)，但随着时间的推移，分布会发生变化。</p><p id="9f99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为对于追踪方法，估计值被归一化为和为 1，所以为了公平比较，重新调整可能是合适的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/754acf3eff22b3942132e3806d7192d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*-DeaaUo7S5m2sdCCgRu3RQ.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="105a" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">6.线性奖励惩罚(追踪法)</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="7ed7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性奖励惩罚扩展了线性奖励不作为背后的思想，在某种程度上，如果我们没有得到奖励，我们也在惩罚手臂。</p><p id="deeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于线性奖励惩罚和不作为，我的朋友迈克尔写了一篇更详细的博文。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/55838ec8fd9216ff7ad691ae8a274caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*bJcgJ02R9pjmhls4UWYWYw.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="3b35" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">7.<strong class="ak"> <em class="nf">置信上限 1 </em> </strong></h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="9f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，UCB 遵循一个简单的原则，即“乐观面对不确定性”。理解这个概念的另一种方式是更喜欢拉得不太频繁的那只手臂，因此我们对那只手臂不太确定。(更多信息请阅读这篇<a class="ae ky" href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html" rel="noopener ugc nofollow" target="_blank">博文</a>或观看这段<a class="ae ky" href="https://www.youtube.com/watch?time_continue=24&amp;v=fIKkhoI1kF4" rel="noopener ugc nofollow" target="_blank">视频</a>。).</p><p id="77e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法很有吸引力，因为它有强有力的理论保证。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5a737e338bcf195c2ff079429f5b43ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*OsupE8gMurrrdVWv7lan1g.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="1182" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">8.置信上限 1 已调整</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="3479" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在他们提出 UCB1 的同一篇论文中，Auer，Cesa-Bianchi &amp; Fisher 提出了 UCB1-Tuned 算法。有趣的是，这个算法给出了更好的经验结果，但是作者自己不能给出理论上的后悔界限。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/d84489aaba5f2ba033477201921b0edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*ChkDb6WvGW3ttsBchMqMiQ.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="6943" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">9.<strong class="ak"> <em class="nf">【汤普森抽样(贝塔分布)</em> </strong></h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="4f29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法是一种贝叶斯推理方法，在这种方法中，我们在每条臂上都有一组先验分布，当我们观察到我们从每条臂获得了多少奖励时，我们就更新我们的后验分布。</p><p id="4b45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关此方法的更多信息，请点击<a class="ae ky" href="https://eigenfoo.xyz/bayesian-bandits/" rel="noopener ugc nofollow" target="_blank">此处</a>或<a class="ae ky" href="https://peterroelants.github.io/posts/multi-armed-bandit-implementation/" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/4d62b5f445a1f5caf7384b4319ac512a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*QbtSJgOcjPZrRyjF4u9n7A.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="5f3e" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">10.汤普森抽样(均匀分布)</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="228d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这种方法纯粹是为了娱乐。均匀分布不是在伯努利分布到<a class="ae ky" href="https://en.wikipedia.org/wiki/Bernoulli_distribution" rel="noopener ugc nofollow" target="_blank">之前的</a><a class="ae ky" href="https://en.wikipedia.org/wiki/Conjugate_prior" rel="noopener ugc nofollow" target="_blank">共轭分布。我只是想尝试一下这种方法，总的来说，与贝塔分布相比，均匀分布倾向于探索更多。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d5f5220cdb22a78b72f046570d81b110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*Xk6EuwlJqAaWaxYtOHG8ng.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="ccc9" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">11.神经网络</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="6cac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法也是为了娱乐目的，它没有任何理论上的保证。看了这篇<a class="ae ky" href="https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3" rel="noopener ugc nofollow" target="_blank">博文，很受启发。</a>不同的是，我使用了 KL 散度作为目标函数，并使用了 Adam 优化器。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/9a593fd7c1f0dd08ef1449b46a024026.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*spf8V5sURUK9GXiD9Eoqaw.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="63c8" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">12.<strong class="ak"> <em class="nf">【玻尔兹曼探索(Softmax) </em> </strong></h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="c7f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Softmax 方法以与其平均奖励成比例的概率挑选每只手臂。因此，如果一只手臂给予较大的奖励，它被选中的概率就较高。</p><p id="1694" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与这种方法相关的一个有趣的想法是，我们有一个控制探索程度的温度参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/aad3d4aa7e36977d4eb3375858e1c322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*Y1swiYSlqZyzMTsokAMh8g.png"/></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h2 id="466a" class="mj mk it bd ml mm mn dn mo mp mq dp mr li ms mt mu lm mv mw mx lq my mz na nb bi translated">13.渐变强盗</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nc nd l"/></div></figure><p id="f145" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度土匪使用梯度上升，以找到最佳的手臂拉。简而言之，我们有一个称为平均奖励的变量，它跟踪奖励的平均值，直到某个时间 t。如果我们拉的强盗给出的奖励高于平均值，我们就增加武器被选中的概率，反之亦然。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/8cba17133e6346921a667e7c660f123c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*n-7BnsU3h1d1m7ixjVYsWA.png"/></div></div></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="90c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">实验/讨论</strong></p><p id="29e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行两组实验<br/> 1)小:12 个强盗，20 集，每集 1000 次迭代<br/> 2)大:1000 个强盗，500 集，每集 1500 次迭代</p><blockquote class="lv lw lx"><p id="3249" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">小的</p></blockquote><div class="kj kk kl km gt ab cb"><figure class="no kn np nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/0225950e65fcaa96b54a5898be164355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*l9vRSmEDNLDMLd_L5sBZXg.png"/></div></figure><figure class="no kn np nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/36c843ff4bc9d66a138fcefb17334b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*V4XH2wUXjzVDQRDPTSu7fA.png"/></div></figure></div><p id="0e1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">左图</strong> →每种方法一段时间后的遗憾<br/> <strong class="lb iu">右图</strong> →每种方法一段时间后的累计奖励</p><p id="3bd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当方法的后悔在对数时间内增长时，它被认为是对 bandit 问题的一种解决方案。从上面的图中，我们可以看到 UCB1 优化后的结果是最佳的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/d2c9025c864c9dd323709489f467149b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*DCWi1FCp83UAXfBIYeYTAQ.png"/></div></div></figure><p id="2405" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当我们查看地面真实概率分布之间的相关矩阵时，我们可以看到 e-greedy 和 softmax 策略具有一对一的相关性。(向量方法不被认为是一种解决方案。).</p><blockquote class="lv lw lx"><p id="3762" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">大的</p></blockquote><div class="kj kk kl km gt ab cb"><figure class="no kn nv nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/287c78d233256594a66a8425c277f2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*YoACa5WoiGGxTo5YBsbslA.png"/></div></figure><figure class="no kn nw nq nr ns nt paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/bd1246b125cde486765c48278861d40c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*avmOWfiscOEEl7yMRPyZ4w.png"/></div></figure></div><p id="6ff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面可以看到一个非常令人惊讶的结果，我们可以马上注意到简单的方法，如贪婪和衰减贪婪方法能够胜过更先进的解决方案。(取样方法要花很多时间。)</p><p id="2976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看到神经网络方法表现良好，我感到非常惊讶。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/e6aff1545d93bc15f989de1df1d96b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*vvQaqXUfykQJwHUHVmLH2g.png"/></div></div></figure><p id="235c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是当涉及到估计 1000 个臂的概率分布时，我们可以看到 softmax 和 UCB1 方法给出了最精确的估计。</p><p id="f621" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论是在大型实验还是小型实验中，衰减贪婪方法似乎都是整体最优的解决方案。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="c8e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">交互代码</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/a2933e22bfd2987255d02030743c50be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pSq_BtcrjZPIJqSR57OC2g.png"/></div></div></figure><p id="0db5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要获取更小实验的代码，请点击<a class="ae ky" href="https://colab.research.google.com/drive/1k0CCyHZ2-KdLTLfUxom01mQiJwffHqy7" rel="noopener ugc nofollow" target="_blank">这里</a>或<a class="ae ky" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/reinforcement_learning/bandit/z%20bandit%20blog%20.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="7d6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要访问更大实验的代码，请点击<a class="ae ky" href="https://colab.research.google.com/drive/1cHAs8BKEoP5viaSjeHawOZmC8YdrkS-E" rel="noopener ugc nofollow" target="_blank">此处</a>或<a class="ae ky" href="https://github.com/JaeDukSeo/Daily-Neural-Network-Practice-2/blob/master/reinforcement_learning/bandit/z%20bandit%20blog%20large.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="add3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">最后的话</strong></p><p id="76c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个简单的问题仍在研究中，因此，存在许多更先进的解决方案，如需进一步阅读，请阅读这篇<a class="ae ky" href="http://banditalgs.com/" rel="noopener ugc nofollow" target="_blank">博文。</a></p><p id="9246" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这篇<a class="ae ky" href="https://www.cs.mcgill.ca/~vkules/bandits.pdf?fbclid=IwAR0zRTBvxp0Eam47JTABWDodxJtPcm8QtIID04RBfsaZATWFTkJeXMywQR4" rel="noopener ugc nofollow" target="_blank">论文</a>中还可以看出，在传统的多臂 bandit 问题中，简单的策略(如 e-greedy 方法)可以胜过更先进的方法，并且在现实生活的临床试验中也能给出有竞争力的结果。</p><p id="609a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然理论上的保证非常重要，但令人惊讶的是，观察到其他更简单的方法也能得到更好的结果。</p><p id="a91b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更多文章请访问我的<a class="ae ky" href="https://jaedukseo.me/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="52d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考</strong></p><ol class=""><li id="309b" class="ny nz it lb b lc ld lf lg li oa lm ob lq oc lu od oe of og bi translated">(2019).Www-anw.cs.umass.edu。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%202.pdf" rel="noopener ugc nofollow" target="_blank">http://www-anw . cs . umass . edu/~ barto/courses/cs 687/Chapter % 202 . pdf</a></li><li id="df16" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">翁，L. (2018)。多臂土匪问题及其解决方案。lilian Weng . github . io . 2019 年 1 月 13 日检索，来自<a class="ae ky" href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions . html</a></li><li id="cd30" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">用多臂 Bandit 进行强化学习。(2018).ITNEXT。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2" rel="noopener ugc nofollow" target="_blank">https://it next . io/reinforcement-learning-with-multi-arm-bandit-decf 442 e 02d 2</a></li><li id="a579" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">多臂 Bandit 强化学习(下)。(2018).ITNEXT。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://itnext.io/reinforcement-learning-with-multi-arm-bandit-part-2-831a43f22a47" rel="noopener ugc nofollow" target="_blank">https://it next . io/reinforcement-learning-with-multi-arm-bandit-part-2-831 a 43 f 22 a 47</a></li><li id="eab5" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">ankonzoid/LearningX。(2019).GitHub。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://github.com/ankonzoid/LearningX/blob/master/classical_RL/MAB/MAB.py" rel="noopener ugc nofollow" target="_blank">https://github . com/ankonzoid/learning x/blob/master/classic _ RL/MAB/mab . py</a></li><li id="484e" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">迈克尔·帕切科。(2019).Michaelpacheco.net。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2" rel="noopener ugc nofollow" target="_blank">https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-2</a></li><li id="6da7" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">迈克尔·帕切科。(2019).Michaelpacheco.net。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3" rel="noopener ugc nofollow" target="_blank">https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3</a></li><li id="5a22" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">多武装匪徒。(2013).数据折纸。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://dataorigami.net/blogs/napkin-folding/79031811-multi-armed-bandits" rel="noopener ugc nofollow" target="_blank">https://data origami . net/blogs/nappin-folding/79031811-multi-armed-土匪</a></li><li id="020e" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">多臂土匪实现。(2019).peterroelants . github . io . 2019 年 1 月 13 日检索，来自<a class="ae ky" href="https://peterroelants.github.io/posts/multi-armed-bandit-implementation/" rel="noopener ugc nofollow" target="_blank">https://peterroelants . github . io/posts/multi-armed-bandit-implementation/</a></li><li id="9d68" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">(2019).Cs.mcgill.ca 检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://www.cs.mcgill.ca/~vkules/bandits.pdf" rel="noopener ugc nofollow" target="_blank">https://www.cs.mcgill.ca/~vkules/bandits.pdf</a></li><li id="7ec2" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">用多臂 Bandit 进行强化学习。(2018).ITNEXT。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2" rel="noopener ugc nofollow" target="_blank">https://it next . io/reinforcement-learning-with-multi-arm-bandit-decf 442 e02d 2</a></li><li id="82de" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">ankonzoid/LearningX。(2019).GitHub。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://github.com/ankonzoid/LearningX/tree/master/classical_RL/MAB" rel="noopener ugc nofollow" target="_blank">https://github . com/ankonzoid/learning x/tree/master/classical _ RL/MAB</a></li><li id="b112" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">迈克尔·帕切科。(2019).Michaelpacheco.net。检索于 2019 年 1 月 13 日，来自 https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3<a class="ae ky" href="https://www.michaelpacheco.net/blog/RL-multi-armed-bandit-3" rel="noopener ugc nofollow" target="_blank"/></li><li id="5e9a" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">试用编辑器 3.6 版(2019)。W3schools.com。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://www.w3schools.com/html/tryit.asp?filename=tryhtml_lists_intro" rel="noopener ugc nofollow" target="_blank">https://www.w3schools.com/html/tryit.asp?filename = try html _ lists _ intro</a></li><li id="ac55" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">bgalbraith/土匪。(2019).GitHub。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://github.com/bgalbraith/bandits/blob/master/bandits/policy.py" rel="noopener ugc nofollow" target="_blank">https://github . com/bgalbraith/bottoms/blob/master/bottoms/policy . py</a></li><li id="56ad" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">(2019).Www-anw.cs.umass.edu。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="http://www-anw.cs.umass.edu/~barto/courses/cs687/Chapter%202.pdf" rel="noopener ugc nofollow" target="_blank">http://www-anw . cs . umass . edu/~ barto/courses/cs 687/Chapter % 202 . pdf</a></li><li id="f2b2" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">bgalbraith/土匪。(2019).GitHub。检索于 2019 年 1 月 13 日，来自<a class="ae ky" href="https://github.com/bgalbraith/bandits/blob/master/bandits/policy.py" rel="noopener ugc nofollow" target="_blank">https://github . com/bgalbraith/bottoms/blob/master/bottoms/policy . py</a></li><li id="a306" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">脱落神经网络(带 ReLU)。(2019).要点。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://gist.github.com/yusugomori/cf7bce19b8e16d57488a" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/yusugomori/cf7bce19b8e16d57488a</a></li><li id="69c6" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">NumPy . minimum—NumPy 1.15 版手册。(2019).Docs.scipy.org。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.minimum.html" rel="noopener ugc nofollow" target="_blank">https://docs . scipy . org/doc/numpy-1 . 15 . 1/reference/generated/numpy . minimum . html</a></li><li id="313d" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">matplotlib.pyplot，h .，&amp; P，B. (2011 年)。如何用 matplotlib.pyplot 改变图例大小.堆栈溢出？检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://stackoverflow.com/questions/7125009/how-to-change-legend-size-with-matplotlib-pyplot" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/7125009/how-to-change-legend-size-with-matplotlib-py plot</a></li><li id="5d95" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">seaborn . heat map-seaborn 0 . 9 . 0 文档。(2019).Seaborn.pydata.org。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://seaborn.pydata.org/generated/seaborn.heatmap.html" rel="noopener ugc nofollow" target="_blank">https://seaborn.pydata.org/generated/seaborn.heatmap.html</a></li><li id="a05d" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">error，h .，&amp; Cunningham，P. (2015)。热图 Seaborn fmt='d '错误。堆栈溢出。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://stackoverflow.com/questions/31087613/heat-map-seaborn-fmt-d-error" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/31087613/heat-map-seaborn-fmt-d-error</a></li><li id="276d" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">皮奥特博士和洛杉矶大学(2017 年)。pivot 中 seaborn 热图的数据顺序。堆栈溢出。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://stackoverflow.com/questions/43694368/data-order-in-seaborn-heatmap-from-pivot" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/43694368/data-order-in-seaborn-heat map-from-pivot</a></li><li id="6007" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">热图公司(2017 年)。自动调整 seaborn 热图中的字体大小。堆栈溢出。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://stackoverflow.com/questions/33104322/auto-adjust-font-size-in-seaborn-heatmap" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/33104322/auto-adjust-font-size-in-seaborn-heat map</a></li><li id="2370" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">(2019).Homes.di.unimi.it 于 2019 年 1 月 14 日检索，来自<a class="ae ky" href="https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf" rel="noopener ugc nofollow" target="_blank">https://homes.di.unimi.it/~cesabian/Pubblicazioni/ml-02.pdf</a></li><li id="a8ac" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">共轭先验。(2019).En.wikipedia.org。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="https://en.wikipedia.org/wiki/Conjugate_prior" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Conjugate_prior</a></li><li id="8a75" class="ny nz it lb b lc oh lf oi li oj lm ok lq ol lu od oe of og bi translated">强盗算法。(2018).强盗算法。检索于 2019 年 1 月 14 日，来自<a class="ae ky" href="http://banditalgs.com/" rel="noopener ugc nofollow" target="_blank">http://banditalgs.com/</a></li></ol></div></div>    
</body>
</html>