<html>
<head>
<title>Multi-Task Learning in Language Model for Text Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类语言模型中的多任务学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89?source=collection_archive---------20-----------------------#2019-03-27">https://towardsdatascience.com/multi-task-learning-in-language-model-for-text-classification-c3acc1fedd89?source=collection_archive---------20-----------------------#2019-03-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6da7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用于文本分类的通用语言模型微调</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/49ec643466c88ac8f2546dcbaeca2a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*B3I88m23HT4zoA_J"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Photo by <a class="ae ky" href="https://unsplash.com/@jeremythomasphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jeremy Thomas</a> on <a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="96ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Howard 和 Ruder 提出了一种新的方法，通过使用预训练嵌入、LM 微调和分类微调来实现任何 NLP 任务的鲁棒迁移学习。具有相同超参数但不同下降的样本 3 层 LSTM 架构展示了用于 6 个下游 NLPS 任务的超越和稳健的模型。他们将其命名为通用语言模型微调(ULMFiT)。</p><p id="80e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本故事将讨论关于<a class="ae ky" href="https://arxiv.org/pdf/1801.06146.pdf" rel="noopener ugc nofollow" target="_blank">文本分类的通用语言模型微调</a> (Howard 和 Ruder，2018)，并将涵盖以下内容:</p><ul class=""><li id="daad" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">体系结构</li><li id="b6b6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">实验</li><li id="fb96" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">履行</li></ul><h1 id="3f41" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">体系结构</h1><p id="533b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">如前所述，ULMFiT 有 3 个阶段。第一阶段是使用通用领域数据来建立 LM 预训练模型。第二阶段是对目标数据集 LM 进行微调，第三阶段是对目标数据集分类进行微调。语言模型(LM)是平均随机梯度下降权重下降长短期记忆(AWD-LSTM)。它不使用变压器，而是使用具有各种调谐压差超参数的常规 LSTM。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/4a2b7f5b788c291e44e351afa7ef0623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9n9yv4EalUn76yP1Yffhfw.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Flow of ULMFiT (Howard and Ruder, 2018)</figcaption></figure><h2 id="9811" class="nh mk it bd ml ni nj dn mp nk nl dp mt li nm nn mv lm no np mx lq nq nr mz ns bi translated">通用域 LM 预训练</h2><p id="4f8b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这是一个不受领域限制的问题，因此我们可以利用任何数据来训练模型。换句话说，数据量可能非常非常大。例如，它可以使用维基百科或 reddit 内容的全部内容。这样做的目的是获取一般特征来处理不同类型的下游问题。大量的实验表明迁移学习对自然语言处理有显著的改善。</p><h2 id="81e9" class="nh mk it bd ml ni nj dn mp nk nl dp mt li nm nn mv lm no np mx lq nq nr mz ns bi translated">目标任务 LM 微调</h2><p id="001e" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">有一个通用的向量，它可能不会直接在特定的问题上表现很好，因为它太一般了。所以微调是必须的动作。首先，模型将通过语言模型(LM)问题进行微调。理论上，它比一般领域的 LM 训练收敛得更快，因为它只需要学习目标任务的单源特性。使用了一些技巧来提高 ULMFiT 的性能。他们是<code class="fe nt nu nv nw b">discriminative fine-tuning</code>和<code class="fe nt nu nv nw b">slanted triangular learning rates</code>。D</p><p id="d1b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">区别性微调</em> </strong>提出对不同层使用不同的学习速率。从实验中，Howard 和 Ruder 发现只选择最后一层的学习速率。最后 2 层的学习率是最后一层/ 2.6，并且使用相同的公式来设置较低层的学习率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e15a492f74596c424c76f5476b80bb59.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*KIgkM9K1oI554VUQ-4DPNw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Stochastic Gradient Descent (SGD). η is learning rate while ∇θJ(θ) is the gradient of objective function. (Howard and Ruder, 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/11cb87c317315e30129470df968bc0f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*X5Q6dksiUIOrkyB-VxQ9yw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">SGD with discriminate fine-tuning. η l is learning rate of l-th layer. (Howard and Ruder, 2018)</figcaption></figure><p id="ed91" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">【斜三角形学习率(STLR) </em> </strong>是另一种使用动态学习率的方法，它在开始时线性增加，然后线性衰减，从而形成一个三角形。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/01a71fb0ba39ee81565784b050de4231.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*Wh1qEhtE38pElGDU0u1Ybg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">STLR Formula. T is number of training iteration. cut_frac is the fraction of increasing learning rate. cut is the iteration switching from increasing to decreasing. p is the fraction of the number of iterations which increase or decreased. (Howard and Ruder, 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/77ca57965abccc50e753b965a3b3b3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*QptmUluWXteT6oI5bD22rw.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Slanted triangular learning rate schedule used for ULMFiT (Howard and Ruder, 2018)</figcaption></figure><h2 id="615d" class="nh mk it bd ml ni nj dn mp nk nl dp mt li nm nn mv lm no np mx lq nq nr mz ns bi translated">目标任务分类器微调</h2><p id="73ee" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">该分类器层中的参数是从零开始学习的。由于强信号可以存在于任何地方，但不限于序列中的最后一个字，Howard 和 Ruder 建议通过最大轮询和平均轮询来连接所有层。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7aab1dddfc1c47ce8b741c57fa7fb139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*HIqFbbrZ48jetJVHHfbP6A.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Concatenate layers. hT is the hidden state at the last time step. (Howard and Ruder, 2018)</figcaption></figure><p id="293f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了将最后一个隐藏状态与最大池和平均轮询连接起来，在这个阶段还应用了一些技巧来提高性能。招数有<code class="fe nt nu nv nw b">gradual unfreezing</code>、<code class="fe nt nu nv nw b">BPTT for Text Classification (BPT3C)</code>和<code class="fe nt nu nv nw b">bidirectional language mode</code>。</p><p id="3602" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">逐步解冻</em> </strong>是在第一个历元中冻结除最后一层以外的所有层，只微调最后一层。在下一个时期，最后冻结层将被解冻，并微调所有未冻结层。在未来时代，越来越多的地层将被解冻。</p><p id="5a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">用于文本分类的 BPTT(BPT3C)</em></strong>如果输入序列很大，启用梯度传播。一个大的输入序列(假设是一个文档)将被分成固定长度的批。每一批的初始隐藏状态是前一批的最终状态。如前所述，最大池和平均轮询被跟踪和反向传播。</p><p id="977f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="nx">双向语言模型</em> </strong>利用正向和反向 LM 从输入中学习特征。</p><h1 id="4bfe" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">实验</h1><p id="9c15" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">从 IMDb，TREC-6 和 AG 数据集，你可以注意到，如果目标数据集很小，它获得了很大的好处。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/c27ba968acc83dc41416c16e98f8be24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WtE9ImbGtAWklok694LSiQ.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Validation errors for IMDb, TREC-6, and AG (Howard and Ruder, 2018)</figcaption></figure><p id="5a28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">霍华德和鲁德提出了许多提高性能的技巧。下图展示了技巧组合的结果。</p><ul class=""><li id="799a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">完整:微调完整模型</li><li id="341c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">discr:应用区别性微调。</li><li id="038e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">最后:仅微调最后一层</li><li id="359f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">解冻链:文本分类的 BPTT</li><li id="118f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Freez:逐渐解冻</li><li id="26e5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">stlr:倾斜三角形学习率</li><li id="85f8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">cos:积极的余弦退火计划</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/11b9082eb4f220dfa1b83b5057bedd14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*i-o-1aY0jKjKxyeVFxH7eg.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Comparison result among different tracks. (Howard and Ruder, 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/6c0a405495ea45320ee0417466883d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UB86AfClhytmQxKbSF2htA.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">IMDb Comparison Result (Howard and Ruder, 2018)</figcaption></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/5420309d2bf3830f90ea736b43519fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aCZCRyhq6zj1JEU1TTPx9A.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">AG, DBpedia, Yelp-bi and Yelp-full Comparison Result (Howard and Ruder, 2018)</figcaption></figure><h1 id="b4f5" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">履行</h1><p id="e5a6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">fast.ai 提供了一个来自<a class="ae ky" href="https://github.com/fastai/fastai/blob/master/examples/text.ipynb" rel="noopener ugc nofollow" target="_blank"> github </a>的样本代码。</p><h1 id="c77b" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">拿走</h1><ul class=""><li id="b0aa" class="lv lw it lb b lc nb lf nc li oh lm oi lq oj lu ma mb mc md bi translated">本文提出了几种新颖的微调技术。</li><li id="97e9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">展示了自然语言处理中迁移学习的能力。</li></ul><h1 id="94ec" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">关于我</h1><p id="49f6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我是湾区的数据科学家。专注于数据科学、人工智能，尤其是 NLP 和平台相关领域的最新发展。你可以在 LinkedIn<a class="ae ky" href="https://www.linkedin.com/in/edwardma1026" rel="noopener ugc nofollow" target="_blank">和我联系，或者在 Medium </a>或 Github 上关注我。</p><h1 id="1470" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考</h1><p id="2852" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">J.霍华德和史路德。<a class="ae ky" href="https://arxiv.org/pdf/1801.06146.pdf" rel="noopener ugc nofollow" target="_blank">文本分类通用语言模型微调</a>。2018</p></div></div>    
</body>
</html>