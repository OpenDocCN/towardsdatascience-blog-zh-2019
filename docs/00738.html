<html>
<head>
<title>Spectral clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">谱聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/spectral-clustering-82d3cff3d3b7?source=collection_archive---------5-----------------------#2019-02-04">https://towardsdatascience.com/spectral-clustering-82d3cff3d3b7?source=collection_archive---------5-----------------------#2019-02-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="733c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">其工作原理背后的直觉和数学！</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/3ce59998ef41165cbb2b62239aceb310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I5kqgnzaheUy7UxA3YkMpA.jpeg"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Photo by <a class="ae kz" href="https://unsplash.com/photos/UVzcwmngd2s?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Alexandre Chambon</a> on <a class="ae kz" href="https://unsplash.com/search/photos/semi-circles?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><h1 id="f759" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">什么是集群？</h1><p id="3092" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">聚类是一种广泛使用的无监督学习方法。分组是这样的，一个聚类中的点彼此相似，而与其他聚类中的点不太相似。因此，找到数据中的模式并为我们分组是由算法决定的，根据所使用的算法，我们可能会得到不同的聚类。</p><p id="e63a" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated"><strong class="lu iu">聚类有两种大致的方法:</strong> <br/> <strong class="lu iu"> 1 .紧密度</strong>-彼此靠近的点落在同一个聚类中，并且围绕聚类中心紧密。密切程度可以通过观察之间的距离来衡量。<strong class="lu iu">例如:K-Means 聚类</strong> <br/> <strong class="lu iu"> 2。连通性</strong> —相互连接或紧邻的点被放在同一个群集中。即使 2 个点之间的距离更小，如果它们不相连，它们也不会聚集在一起。<strong class="lu iu">谱聚类</strong>是一种遵循这种方法的技术。</p><p id="bdf8" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">这两者之间的差异可以通过下图很容易地显示出来:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi mt"><img src="../Images/a321c0c289ff8b646b9bdbfaf05e2e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1AkY1paiZYAUX6pAAZlPQ.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Figure 1</figcaption></figure><h1 id="0eaf" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated"><strong class="ak">谱聚类是如何工作的？</strong></h1><p id="1aca" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">在谱聚类中，数据点被视为图的节点。因此，聚类被视为一个图划分问题。然后，节点被映射到一个低维空间，该空间可以很容易地被分离以形成集群。需要注意的重要一点是，没有对集群的形状/形式进行假设。</p><h1 id="4726" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">谱聚类的步骤是什么？</h1><blockquote class="mu"><p id="1133" class="mv mw it bd mx my mz na nb nc nd mn dk translated">谱聚类包括 3 个步骤:<br/> 1 .计算相似度图<br/> 2。将数据投影到低维空间<br/> 3。创建集群</p></blockquote><h2 id="6e29" class="ne lb it bd lc nf ng dn lg nh ni dp lk mb nj nk lm mf nl nm lo mj nn no lq np bi translated"><strong class="ak">步骤 1——计算相似度图:</strong></h2><p id="dc88" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">我们先创建一个无向图 G = (V，E)，顶点集 V = { <em class="nq"> v1，v2，…，vn </em> } = 1，2，…，n 个数据中的观测值。这可以由邻接矩阵来表示，该矩阵以每个顶点之间的相似性作为其元素。为此，我们可以计算:</p><p id="a531" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated"><strong class="lu iu">1)ε-邻域图:</strong>这里我们连接所有成对距离小于ε的点。由于所有连接点之间的距离大致相同(最多为ε),对边进行加权不会将更多的数据信息合并到图表中。因此，ε-邻域图通常被认为是一个不加权的图。</p><p id="3864" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated"><strong class="lu iu"> 2) KNN 图:</strong>这里我们用 K 个最近邻连接顶点<em class="nq"> vi </em>和顶点<em class="nq"> vj </em>如果<em class="nq"> vj </em>在<em class="nq"> vi </em>的 K 个最近邻中。<br/>但是如果最近的邻居不是对称的，我们可能会有一个问题，即如果有一个顶点<em class="nq"> vi </em>以<em class="nq"> vj </em>作为最近的邻居，那么<em class="nq"> vi </em>不一定是<em class="nq"> vj </em>的最近邻居。因此，我们最终得到一个有向图，这是一个问题，因为我们不知道在这种情况下两点之间的相似性意味着什么。有两种方法可以使这个图没有方向。<br/>第一种方法是简单地忽略边的方向，即如果<em class="nq"> vi </em>是<em class="nq"> vj </em>的 k 个最近邻居之一，或者如果<em class="nq"> vj </em>是<em class="nq"> vi </em>的 k 个最近邻居之一，我们用一条无向边连接<em class="nq"> vi </em>和<em class="nq"> vj </em>。得到的图就是通常所说的 k 近邻图。<br/>第二个选择是连接顶点<em class="nq"> vi </em>和<em class="nq"> vj </em>，如果<em class="nq"> vi </em>都是<em class="nq"> vj </em>的 k 个最近邻，并且<em class="nq"> vj </em>是<em class="nq"> vi </em>的 k 个最近邻。得到的图称为互 k 近邻图。<br/>在这两种情况下，在连接适当的顶点后，我们通过相邻点的相似性对边进行加权。</p><p id="be9a" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated"><strong class="lu iu"> 3)全连通图</strong>:为了构建这个图，我们简单地将所有的点相互连接起来，我们通过相似度<em class="nq"> sij </em>对所有的边进行加权。该图应该模拟局部邻域关系，因此使用相似性函数，例如高斯相似性函数。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/88c8fbfbb15a677338f9f16a72235bac.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*El0lNEy5iCHUZQErxkalUA.png"/></div></figure><p id="df63" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">这里，参数σ控制邻域的宽度，类似于ε-邻域图中的参数ε。</p><p id="ecde" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">因此，当我们为这些图中的任何一个创建邻接矩阵时，当这些点靠近时，<em class="nq"> Aij </em> ~ 1，如果这些点远离，则<em class="nq"> Aij </em> → 0。<br/>考虑以下具有节点 1 至 4、权重(或相似度)<em class="nq"> wij </em>及其邻接矩阵的图:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ns"><img src="../Images/0934e91cb59a6ad6c8d8e48fe5c72cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uRAzUgu6WHJGhK7i35zwjw.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">L: Graph, R: n x n symmetric adjacency matrix</figcaption></figure><h2 id="b556" class="ne lb it bd lc nf nt dn lg nh nu dp lk mb nv nk lm mf nw nm lo mj nx no lq np bi translated">步骤 2 —将数据投影到低维空间:</h2><p id="1003" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated"><strong class="lu iu">正如我们在图 1 中看到的，同一聚类中的数据点也可能相距很远——甚至比不同聚类中的点更远</strong> <em class="nq">。</em>我们的目标是转换空间，这样当两个点靠近时，它们总是在同一个簇中，当它们远离时，它们在不同的簇中。我们需要把我们的观察投射到一个低维空间。为此，我们计算了<a class="ae kz" href="https://en.wikipedia.org/wiki/Laplacian_matrix" rel="noopener ugc nofollow" target="_blank">图拉普拉斯</a>，这只是图的另一种矩阵表示，可以用于发现图的有趣属性。这可以计算为:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ny"><img src="../Images/05f480596cb51001eda1bdc38808812e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1q3yRIHXrWqBcnYvI9jGAw.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Computing Graph Laplacian</figcaption></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi nz"><img src="../Images/6c587b8da48a4f0fda076018b0da8b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g9ZrAfm_F2s8LhcgBYqDiQ.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Graph Laplacian for our example above</figcaption></figure><p id="084e" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">计算图拉普拉斯 L 的全部目的是找到它的特征值和特征向量，以便将数据点嵌入到低维空间中。所以现在，我们可以继续寻找<a class="ae kz" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener ugc nofollow" target="_blank">特征值</a>。我们知道:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oa"><img src="../Images/e2a4f63fc5970d3e6e7c7795d3d138e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LQbIFXQLuVtX1k0k_Kd4Nw.png"/></div></div></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ob"><img src="../Images/0aa2d05f53e6c33a62f18b2357efe98a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y95LplU9iOY_G3u56LWl8g.png"/></div></div></figure><p id="c40c" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">让我们考虑一个数字的例子:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oc"><img src="../Images/df4eabea4932b72c7c029dd1f144edd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfO_oMIu2qCZk3Hh6p9xIg.png"/></div></div></figure><p id="af46" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">然后我们计算 l 的特征值和特征向量。</p><h2 id="e794" class="ne lb it bd lc nf nt dn lg nh nu dp lk mb nv nk lm mf nw nm lo mj nx no lq np bi translated">步骤 3 —创建集群:</h2><p id="8da5" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">对于这一步，我们使用对应于第二特征值的特征向量来给每个节点赋值。经计算，第二特征值为 0.189，相应的特征向量 v2 = [0.41，0.44，0.37，-0.4，-0.45，-0.37]。<br/>为了获得二分聚类(两个不同的聚类)，我们首先将 v2 的每个元素分配给节点，使得<em class="nq"> {node1:0.41，node2:0.44，… node6: -0.37} </em>。然后，我们分割节点，使得值为&gt; 0 的所有节点都在一个集群中，而所有其他节点都在另一个集群中。因此，在这种情况下，我们在一个集群中得到节点 1，2 &amp; 3，在第二个集群中得到 4，5 &amp; 6。</p><blockquote class="od oe of"><p id="677b" class="ls lt nq lu b lv mo ju lx ly mp jx ma og mq md me oh mr mh mi oi ms ml mm mn im bi translated">值得注意的是，第二个特征值表示图中节点的连接紧密程度。对于好的、干净的划分，第二特征值越低，聚类越好。</p></blockquote><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oj"><img src="../Images/1b3a50bb4691f5cb2ceaec69e754ca7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nErclOjO_vquplrYs_A0GA.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Eigenvector v2 gives us bipartite clustering.</figcaption></figure><h2 id="166d" class="ne lb it bd lc nf nt dn lg nh nu dp lk mb nv nk lm mf nw nm lo mj nx no lq np bi translated">对于<em class="ki"> k </em>簇，我们必须修改我们的拉普拉斯算子以使其规范化。</h2><p id="1ba8" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">因此我们得到:</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ok"><img src="../Images/c5322abdacb96019b55c089f0a07fd00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p3WUrtbn1uu1DBCEeIAzpQ.png"/></div></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Normalized Laplacian — Ng, Jordan, Weiss</figcaption></figure><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ol"><img src="../Images/fa3333d44830e1b4063c1c9ef2795d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BC51BhrA1K5p26qZDk1HJQ.png"/></div></div></figure><h1 id="39a0" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">谱聚类的优缺点</h1><h2 id="2478" class="ne lb it bd lc nf nt dn lg nh nu dp lk mb nv nk lm mf nw nm lo mj nx no lq np bi translated">优势:</h2><ol class=""><li id="0126" class="om on it lu b lv lw ly lz mb oo mf op mj oq mn or os ot ou bi translated">不对聚类的统计数据做出强有力的假设-聚类技术(如 K-Means 聚类)假设分配给聚类的点是关于聚类中心的球形。这是一个很强的假设，并不总是相关的。在这种情况下，谱聚类有助于创建更准确的聚类。</li><li id="2c41" class="om on it lu b lv ov ly ow mb ox mf oy mj oz mn or os ot ou bi translated">易于实现并给出良好的聚类结果。它可以正确地对实际上属于同一类但由于维数减少而比其他类中的观测值更远的观测值进行聚类。</li><li id="a954" class="om on it lu b lv ov ly ow mb ox mf oy mj oz mn or os ot ou bi translated">对于几千个元素的稀疏数据集来说相当快。</li></ol><h2 id="b484" class="ne lb it bd lc nf nt dn lg nh nu dp lk mb nv nk lm mf nw nm lo mj nx no lq np bi translated">缺点:</h2><ol class=""><li id="fdf4" class="om on it lu b lv lw ly lz mb oo mf op mj oq mn or os ot ou bi translated">在最后一步中使用 K-Means 聚类意味着聚类不总是相同的。它们可以根据初始质心的选择而变化。</li><li id="3e04" class="om on it lu b lv ov ly ow mb ox mf oy mj oz mn or os ot ou bi translated">对于大型数据集，计算成本很高—这是因为需要计算特征值和特征向量，然后我们必须对这些向量进行聚类。对于大型密集数据集，这可能会大大增加时间复杂度。</li></ol><p id="22e0" class="pw-post-body-paragraph ls lt it lu b lv mo ju lx ly mp jx ma mb mq md me mf mr mh mi mj ms ml mm mn im bi translated">在这篇博客中，我解释了谱聚类背后的数学原理。欢迎任何反馈或建议！同时，一定要看看我的其他博客。</p><h1 id="7111" class="la lb it bd lc ld le lf lg lh li lj lk jz ll ka lm kc ln kd lo kf lp kg lq lr bi translated">参考</h1><ol class=""><li id="6c3e" class="om on it lu b lv lw ly lz mb oo mf op mj oq mn or os ot ou bi translated"><a class="ae kz" href="https://calculatedcontent.com/2012/10/09/spectral-clustering/" rel="noopener ugc nofollow" target="_blank">https://calculated content . com/2012/10/09/spectral-clustering/</a></li><li id="d5ec" class="om on it lu b lv ov ly ow mb ox mf oy mj oz mn or os ot ou bi translated">【http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf T4】</li><li id="4a94" class="om on it lu b lv ov ly ow mb ox mf oy mj oz mn or os ot ou bi translated"><a class="ae kz" href="https://www.youtube.com/watch?v=zkgm0i77jQ8" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=zkgm0i77jQ8</a></li></ol></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><h1 id="6cc9" class="la lb it bd lc ld ph lf lg lh pi lj lk jz pj ka lm kc pk kd lo kf pl kg lq lr bi translated"><strong class="ak">关于我</strong></h1><p id="169d" class="pw-post-body-paragraph ls lt it lu b lv lw ju lx ly lz jx ma mb mc md me mf mg mh mi mj mk ml mm mn im bi translated">一名数据科学家，目前正在保护 AWS 客户免受欺诈。以前的工作是为金融领域的企业构建预测和推荐算法。<br/> <strong class="lu iu">领英:</strong><a class="ae kz" href="https://www.linkedin.com/in/neerja-doshi/" rel="noopener ugc nofollow" target="_blank"><strong class="lu iu">https://www.linkedin.com/in/neerja-doshi/</strong></a></p></div></div>    
</body>
</html>