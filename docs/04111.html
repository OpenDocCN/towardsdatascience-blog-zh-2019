<html>
<head>
<title>Building a Search Engine with BERT and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 BERT 和 TensorFlow 构建搜索引擎</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a?source=collection_archive---------2-----------------------#2019-06-28">https://towardsdatascience.com/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a?source=collection_archive---------2-----------------------#2019-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4664" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在这个实验中，我们使用一个预先训练好的 BERT 模型检查点来建立一个通用的文本特征提取器，并将其应用到最近邻搜索任务中。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2295d9ecf18d5d521d8bd859fbb73a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*amiNjYHGJDVreoF5HTp2Gg.png"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">T-SNE decomposition of BERT text representations (Reuters-21578 benchmark, 6 classes)</figcaption></figure><p id="f518" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于深度<a class="ae lu" href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf" rel="noopener ugc nofollow" target="_blank">神经概率语言模型</a>如<a class="ae lu" rel="noopener" target="_blank" href="/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">伯特</a>的特征提取器可以提取与大量下游 NLP 任务相关的特征。出于这个原因，它们有时被称为<a class="ae lu" href="https://en.wikipedia.org/wiki/Natural-language_understanding" rel="noopener ugc nofollow" target="_blank">自然语言理解</a> (NLU)模块。</p><p id="c306" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些特征也可以用于计算文本样本之间的相似性，这对于基于<a class="ae lu" href="https://en.wikipedia.org/wiki/Instance-based_learning" rel="noopener ugc nofollow" target="_blank">实例的学习</a>算法(例如<a class="ae lu" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank"> K-NN </a>)是有用的。为了说明这一点，我们将为文本构建一个最近邻搜索引擎，使用 BERT 进行特征提取。</p><p id="9f9d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个实验的计划是:</p><ol class=""><li id="655d" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">获取预训练的 BERT 模型检查点</li><li id="d8e1" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">提取为推理而优化的子图</li><li id="5712" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">用 tf 创建特征提取器。估计量</li><li id="ce9f" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">用 T-SNE 和嵌入式投影仪探索向量空间</li><li id="10de" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">实现最近邻搜索引擎</li><li id="03de" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">用数学加速搜索查询</li><li id="3fb3" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">示例:构建电影推荐系统</li></ol><h1 id="b701" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">问题和答案</h1><h2 id="999e" class="nb mk it bd ml nc nd dn mp ne nf dp mt lh ng nh mv ll ni nj mx lp nk nl mz nm bi translated">这本指南里有什么？</h2><p id="51b9" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">本指南包含两件事的实现:一个 BERT 文本特征提取器和一个最近邻搜索引擎。</p><h2 id="da30" class="nb mk it bd ml nc nd dn mp ne nf dp mt lh ng nh mv ll ni nj mx lp nk nl mz nm bi translated">这本指南是给谁的？</h2><p id="e18a" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">本指南对那些有兴趣使用 BERT 进行自然语言理解任务的研究人员应该是有用的。它也可以作为与 tf 接口的工作示例。估计器 API。</p><h2 id="4da7" class="nb mk it bd ml nc nd dn mp ne nf dp mt lh ng nh mv ll ni nj mx lp nk nl mz nm bi translated">需要什么？</h2><p id="509c" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">对于熟悉 TensorFlow 的读者来说，完成本指南大约需要 30 分钟。</p><h2 id="6299" class="nb mk it bd ml nc nd dn mp ne nf dp mt lh ng nh mv ll ni nj mx lp nk nl mz nm bi translated">给我看看代码。</h2><p id="765b" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">这个实验的代码可以在 Colab <a class="ae lu" href="https://colab.research.google.com/drive/1ra7zPFnB2nWtoAc0U5bLp0rWuPWb6vu4" rel="noopener ugc nofollow" target="_blank">这里</a>获得。另外，看看我为我的 BERT 实验建立的<a class="ae lu" href="https://github.com/gaphex/bert_experimental" rel="noopener ugc nofollow" target="_blank">库</a>:它包含额外的东西！</p><p id="2bfb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们开始吧。</p><h1 id="ef6f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">步骤 1:获得预训练模型</h1><p id="4d5d" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">我们从预先训练的 BERT 检查点开始。出于演示的目的，我将使用谷歌工程师预先训练的无外壳<a class="ae lu" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">英文模型</a>。要训练一个不同语言的模型，请查看我的<a class="ae lu" href="https://medium.com/p/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379?source=email-e6b20e934e90--writer.postDistributed&amp;sk=51c46354668b0fc4255fe8bb7e1e3035" rel="noopener">其他指南</a>。</p><p id="80bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了配置和优化用于推理的图表，我们将利用令人敬畏的<a class="ae lu" href="https://github.com/hanxiao/bert-as-service" rel="noopener ugc nofollow" target="_blank"> bert-as-a-service </a>存储库。这个存储库允许通过 TCP 为远程客户端提供 BERT 模型。</p><p id="8d58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在多主机环境中，拥有远程 BERT-server 是非常有益的。然而，在实验的这一部分，我们将着重于创建一个本地<br/>(进程内)特征提取器。如果希望避免由客户端-服务器架构引入的额外延迟和潜在故障模式，这是有用的。</p><p id="bdc4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们下载模型并安装软件包。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="e2c3" class="nb mk it nt b gy nx ny l nz oa">!wget <a class="ae lu" href="https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip</a><br/>!unzip wwm_uncased_L-24_H-1024_A-16.zip<br/>!pip install bert-serving-server --no-deps</span></pre><h1 id="62cf" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">步骤 2:优化推理图</h1><p id="a304" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">通常，要修改模型图，我们必须做一些低级的张量流编程。然而，由于 bert-as-a-service，我们可以使用简单的 CLI 界面来配置推理图。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="f0fd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有几个参数需要注意。</p><p id="f156" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于每个文本样本，BERT 编码层输出一个形状张量[ <em class="od"> sequence_len </em>，<em class="od"> encoder_dim </em> ]，每个标记一个向量。如果我们要获得一个固定的表示，我们需要应用某种类型的池。</p><p id="c6c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> POOL_STRAT </strong>参数定义应用于编码器层号<strong class="la iu"> POOL_LAYER </strong>的池策略。默认值'<em class="od"> REDUCE_MEAN </em>'对序列中所有记号的向量进行平均。当模型没有微调时，这种策略最适合大多数句子级任务。另一个选项是<em class="od"> NONE </em>，在这种情况下，根本不应用池。这对于单词级的任务很有用，例如命名实体识别或词性标注。关于这些选项的详细讨论，请查看晓寒的博客文章。</p><p id="a29f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">SEQ _ 莱恩</strong>影响模型处理的序列的最大长度。较小的值将几乎线性地提高模型推理速度。</p><p id="efc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">运行上述命令会将模型图和权重放入一个<a class="ae lu" href="https://www.tensorflow.org/guide/extend/model_files" rel="noopener ugc nofollow" target="_blank"> GraphDef </a>对象，该对象将在<strong class="la iu"> GRAPH_OUT </strong>序列化为一个<em class="od"> pbtxt </em>文件。该文件通常比预训练模型小，因为训练所需的节点和变量将被移除。这产生了一个非常可移植的解决方案:例如，英语模型在序列化后只需要 380 MB。</p><h1 id="2b64" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">步骤 3:创建特征提取器</h1><p id="fd7e" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">现在，我们将使用串行化图来构建一个使用<a class="ae lu" href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator" rel="noopener ugc nofollow" target="_blank"> tf 的特征提取器。估计器</a> API。我们将需要定义两件事情:<strong class="la iu">输入 _fn </strong>和<strong class="la iu">模型 _fn </strong></p><p id="c36b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> input_fn </strong>管理将数据放入模型。这包括执行整个文本预处理管道，并为 BERT 准备一个<strong class="la iu"> feed_dict </strong>。</p><p id="5938" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，每个文本样本被转换成一个<strong class="la iu"> tf。示例</strong>实例<strong class="la iu"> </strong>包含输入名称中列出的必要特性。bert_tokenizer 对象包含了单词表并执行文本预处理。之后，在一个<strong class="la iu"> feed_dict </strong>中，示例按特征名重新分组。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="93f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">tf。估算器有一个有趣的特性，每次调用 predict 函数时，它都会重新构建和初始化整个计算图。因此，为了避免开销，我们将向 predict 函数传递一个生成器，该生成器将在一个永无止境的循环中向模型提供特征。哈哈。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="5182" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> model_fn </strong>包含模型的规格。在我们的例子中，它是从我们在上一步中保存的<em class="od"> pbtxt </em>文件中加载的。这些特征通过<strong class="la iu">输入映射</strong>明确映射到相应的输入节点。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="1230" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们几乎拥有了进行推理所需的一切。我们开始吧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3939" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于我们使用了生成器，对<strong class="la iu"> bert_vectorizer </strong>的连续调用不会触发模型重建。</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="4770" class="nb mk it nt b gy nx ny l nz oa">&gt;&gt;&gt; bert_vectorizer = build_vectorizer(estimator, build_input_fn)<br/>&gt;&gt;&gt; bert_vectorizer(64*['sample text']).shape<br/>(64, 768)</span></pre><p id="9d97" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上述特征提取器的独立版本可以在<a class="ae lu" href="https://github.com/gaphex/bert_experimental" rel="noopener ugc nofollow" target="_blank">库</a>中找到。</p><h1 id="33a9" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">第四步:用投影仪探索向量空间</h1><p id="38ca" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">现在是展示的时候了！</p><p id="26fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用矢量器，我们将为来自 Reuters-21578 基准语料库的文章生成嵌入。<br/>为了在 3D 中可视化和探索嵌入向量空间，我们将使用一种叫做<a class="ae lu" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank"> T-SNE </a>的降维技术。</p><p id="3e34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们首先获得文章嵌入。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="6f0c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<a class="ae lu" href="https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/gaphex/7262af1e151957b1e7c638f4922dfe57/raw/3b946229fc58cbefbca2a642502cf51d4f8e81c5/reuters_proj_config.json" rel="noopener ugc nofollow" target="_blank">嵌入投影仪</a>上可以获得生成嵌入的交互式可视化。</p><p id="818d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从链接中你可以自己运行 T-SNE 或者使用右下角的书签加载一个检查点(加载只在 Chrome 上有效)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol oc l"/></div></figure><p id="e46c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用生成的特征构建监督模型非常简单:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><h1 id="9c48" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated"><strong class="ak">第五步:构建搜索引擎</strong></h1><p id="b3c3" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">现在，假设我们有一个 50k 文本样本的知识库，我们需要根据这些数据快速回答查询。我们如何从文本数据库中检索与查询最相似的样本？答案是<a class="ae lu" href="https://en.wikipedia.org/wiki/Nearest_neighbor_search" rel="noopener ugc nofollow" target="_blank">最近邻搜索</a>。</p><p id="badd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">形式上，我们要解决的搜索问题定义如下:<br/>给定向量空间<strong class="la iu"> M </strong>中的一组点<strong class="la iu"> S </strong>，以及一个查询点<strong class="la iu">Q</strong><strong class="la iu">∈</strong><strong class="la iu"><em class="od">M</em></strong>，找出<strong class="la iu"> <em class="od"> S </em> </strong>到<strong class="la iu"> <em class="od"> Q </em> </strong>中最近的点。在向量空间中有多种方法来定义“最近的”，我们将使用<a class="ae lu" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>。</p><p id="1e44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，要构建文本搜索引擎，我们将遵循以下步骤:</p><ol class=""><li id="3bf6" class="lv lw it la b lb lc le lf lh lx ll ly lp lz lt ma mb mc md bi translated">向量化知识库中的所有样本——这给出了<strong class="la iu"> S </strong></li><li id="9017" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">向量化查询——这给出了<strong class="la iu"> Q </strong></li><li id="6e6f" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">计算<strong class="la iu"> Q </strong>和<strong class="la iu"> S </strong>之间的欧几里德距离<strong class="la iu"> D </strong></li><li id="4e84" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">按升序排序<strong class="la iu">D</strong>——提供最相似样本的索引</li><li id="7ff6" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated">从知识库中检索所述样本的标签</li></ol><p id="27b5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了让这个简单的实现变得更令人兴奋，我们将在纯张量流中实现它。</p><p id="177f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们为<strong class="la iu"> Q </strong>和<strong class="la iu"> S </strong>创建占位符</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="155a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">定义欧几里德距离计算</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="321b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，得到最相似的样本指数</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="bb89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们已经建立了一个基本的检索算法，问题是:<br/>我们能让它运行得更快吗？通过一点点数学知识，我们可以做到。</p><h1 id="2060" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">第六步:用数学加速搜索</h1><p id="e18b" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">对于一对向量<strong class="la iu"> p </strong>和<strong class="la iu"> q，</strong>，欧几里德距离定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/94a8d0f5f3b6d849e147c0d0292bd93c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9oQzTBGJ1qr1jfzciLGbdA.png"/></div></div></figure><p id="5dba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这正是我们在第四步中的计算方法。</p><p id="9316" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，由于<strong class="la iu"> p </strong>和<strong class="la iu"> q </strong>是向量，我们可以扩展并重写它:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/13709346c26f58963c8542ae91a77df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQl-cYKkjtI_YDayjY8RyA.png"/></div></div></figure><p id="25b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中⟨…⟩表示内积。</p><p id="b90a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在张量流中，这可以写成如下形式:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="0a29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">顺便说一下，上面公式中的<strong class="la iu"> PP </strong>和<strong class="la iu"> QQ </strong>实际上是各自向量的平方<a class="ae lu" href="https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm" rel="noopener ugc nofollow" target="_blank"> L2 范数</a>。如果两个向量都是 L2 归一化的，那么<strong class="la iu"> PP </strong> = <strong class="la iu"> QQ </strong> = 1。这给出了内积和欧几里德距离之间一个有趣的关系:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/0ebe271c8bba62a33e6f37476cce18a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6n49V1YYpD2bGOErcid2YQ.png"/></div></div></figure><p id="8aed" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，进行 L2 归一化丢弃了关于矢量幅度的信息，这在许多情况下是不期望的。</p><p id="fbb2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，我们可能注意到，只要知识库不变，<strong class="la iu"> PP，</strong>其平方向量范数也保持不变。因此，我们可以只做一次，然后使用预计算的结果进一步加速距离计算，而不是每次都重新计算。</p><p id="a145" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们把它们放在一起。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="b5a7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，您可以将这种实现用于任何矢量器模型，而不仅仅是 BERT。它在最近邻检索方面非常有效，能够在双核 Colab CPU 上每秒处理几十个请求。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/ef3b4f122df6cc2424d4614a939d6d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GsOkDVQkQKEnyso0XucLyA.png"/></div></div></figure><h1 id="6f92" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">例子:电影推荐系统</h1><p id="3f63" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">对于这个例子，我们将使用来自 IMDB 的电影摘要数据集。使用 NLU 和检索器模块，我们将构建一个电影推荐系统，推荐具有相似情节特征的电影。</p><p id="22bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，让我们下载并准备好<a class="ae lu" href="http://www.cs.cmu.edu/~ark/personas/" rel="noopener ugc nofollow" target="_blank"> IMDB 数据集</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="51d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用伯特·NLU 模块矢量化电影情节:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="1e1d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，使用 L2Retriever，找到与查询电影情节向量最相似的电影，并将其返回给用户。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="a9b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们去看看吧！</p><pre class="kj kk kl km gt ns nt nu nv aw nw bi"><span id="820a" class="nb mk it nt b gy nx ny l nz oa">&gt;&gt;&gt; recommend = buildMovieRecommender(names, X_vect)<br/>&gt;&gt;&gt; recommend("The Matrix")<br/>Impostor <br/>Immortel <br/>Saturn 3 <br/>Terminator Salvation <br/>The Terminator <br/>Logan's Run <br/>Genesis II <br/>Tron: Legacy <br/>Blade Runner</span></pre></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h1 id="7959" class="mj mk it bd ml mm oq mo mp mq or ms mt jz os ka mv kc ot kd mx kf ou kg mz na bi translated">结论</h1><p id="468e" class="pw-post-body-paragraph ky kz it la b lb nn ju ld le no jx lg lh np lj lk ll nq ln lo lp nr lr ls lt im bi translated">在本指南中，我们构建了一个通用的 BERT 特征提取器。用从 BERT 中提取的特征建立的模型在分类和检索任务上表现充分。虽然可以通过微调来进一步提高它们的性能，但是所描述的文本特征提取方法为下游 NLP 解决方案提供了可靠的无监督基线。</p><h2 id="3c39" class="nb mk it bd ml nc nd dn mp ne nf dp mt lh ng nh mv ll ni nj mx lp nk nl mz nm bi translated">本系列中的其他指南</h2><ol class=""><li id="9d00" class="lv lw it la b lb nn le no lh ov ll ow lp ox lt ma mb mc md bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/pre-training-bert-from-scratch-with-cloud-tpu-6e2f71028379">用云 TPU 从头开始预训练伯特</a></li><li id="f87d" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/building-a-search-engine-with-bert-and-tensorflow-c6fdc0186c8a">用 BERT 和 Tensorflow 搭建搜索引擎</a>【你在这里】</li><li id="bee0" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/fine-tuning-bert-with-keras-and-tf-module-ed24ea91cff2">用 Keras 和 tf 微调 BERT。模块</a></li><li id="4a84" class="lv lw it la b lb me le mf lh mg ll mh lp mi lt ma mb mc md bi translated"><a class="ae lu" rel="noopener" target="_blank" href="/improving-sentence-embeddings-with-bert-and-representation-learning-dfba6b444f6b">使用 BERT 和表示学习改进句子嵌入</a></li></ol></div></div>    
</body>
</html>