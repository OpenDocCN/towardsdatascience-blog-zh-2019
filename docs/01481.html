<html>
<head>
<title>It’s Only Natural: An Excessively Deep Dive Into Natural Gradient Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">这很自然:过度深入自然梯度优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb?source=collection_archive---------1-----------------------#2019-03-09">https://towardsdatascience.com/its-only-natural-an-excessively-deep-dive-into-natural-gradient-optimization-75d464b89dbb?source=collection_archive---------1-----------------------#2019-03-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="49b6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我要讲一个故事:一个你以前几乎肯定听过的故事，但与你习惯的侧重点不同。</p><p id="c147" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于一阶近似，所有现代深度学习模型都是使用梯度下降来训练的。在梯度下降的每一步，您的参数值从某个起点开始，您将它们向损失减少最大的方向移动。你可以通过对你的损失对你的整个参数向量求导来做到这一点，也就是所谓的雅可比矩阵。然而，这只是你损失的一阶导数，它不能告诉你任何关于曲率的东西，或者说，你的一阶导数变化有多快。由于您可能处于一阶导数的局部近似值可能不会在离估计点很远的地方推广的区域(例如，在一个巨大的山丘之前的一个向下的曲线)，您通常希望小心谨慎，不要迈出太大的一步。因此，为了谨慎起见，我们用一个步长α来控制我们的前进速度，如下式所示。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi kl"><img src="../Images/b01446b4a2150aea39db04eb5f8f7e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*vI4WYTgmaWnivfz1DT0y6A.png"/></div></figure><p id="7988" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个步长做了一些有趣的事情:它限制了你要在梯度方向上更新每个参数的距离，并且是以一个固定的量。在该算法的最简单版本中，我们取一个标量α，假设它是 0.1，然后乘以相对于损耗的梯度。请记住，我们的梯度实际上是一个向量，即模型中每个参数的损失梯度，因此当我们将它乘以一个标量时，我们将沿着每个参数轴的更新乘以<em class="kt">，以欧几里德参数距离</em>表示相同的固定量。并且，在梯度下降的最基本版本中，我们在学习过程中的某些点使用相同的步长。</p><p id="c655" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是…这真的有意义吗？具有小学习率的前提是我们知道梯度的单个局部估计可能仅在该估计周围的小局部区域中有效。但是，参数可以在不同的尺度上存在，并且可以对你学习到的条件分布产生不同程度的影响。而且，这种影响程度会随着训练的进行而波动。从这个角度来看，在欧几里德参数空间中用一个固定的全局半径来定义一个安全气泡似乎不是一件特别明智或有意义的事情。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi ku"><img src="../Images/17b48f8241d5f488edfa79d5686ac940.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*jeMxJLZz-o5xniDMKqcTAg.jpeg"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">Image Credit: Science Magazine, because I don’t have the software to make a cool image like this, and damnit the subtleties of gradient calculation techniques are not an easy thing to visualize</figcaption></figure><p id="5b2a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自然梯度的支持者含蓄地提出了一个反建议，即我们不应该根据参数空间中的<em class="kt">距离来定义我们的安全窗口，而应该根据分布空间</em>中的<em class="kt">距离来定义它。因此，不要说“我将遵循我的当前梯度，前提是将参数向量保持在当前向量的ε距离内”，而是说“我将遵循我的当前梯度，前提是将我的模型预测的分布保持在它之前预测的分布的ε距离内”。这里的概念是分布之间的距离对于任何缩放、移位或一般的重新参数化都是不变的。例如，可以使用方差参数或比例参数(1/方差)来参数化相同的高斯；如果查看参数空间，两个分布之间的距离会有所不同，这取决于它们是使用方差参数化还是使用比例参数化。但是如果你在原始概率空间中定义一个距离，它将是一致的。</em></p><p id="756c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这篇文章的其余部分将试图对一种称为自然梯度学习的方法建立更强、更直观的理解，这是一种概念上优雅的想法，旨在纠正参数空间中缩放的任意性。我将深入研究它是如何工作的，如何在组成它的不同数学思想之间建立桥梁，并最终讨论它是否以及在哪里真正有用。但是，首先:计算分布之间的距离意味着什么？</p><h1 id="692d" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">许可给 KL</h1><p id="6c2b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">KL 散度，或者更恰当地说，Kullback-Leibler 散度，在技术上不是<em class="kt"/>分布之间的距离度量(数学学家对被称为度量或适当距离的东西很挑剔)，但它非常接近这个概念。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/8f9fd910d70d52a5c004b84224b35b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*lMXoMVnzFZVW5CtcDZERew.png"/></div></figure><p id="e555" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从数学上讲，它是通过计算对数概率(即概率值的原始差值)与从一个分布或另一个分布中采样的 x 值之比的期望值而得到的。事实上，期望是在一个分布或另一个分布上得到的，这使得它成为一个不对称的度量，其中 KL(P||Q)！= KL(Q||P)。但是，在许多其他方面，KL 散度映射到我们对概率距离应该是什么样子的概念:它直接测量如何定义概率密度函数，也就是说，在定义分布的一系列点上的密度值的差异。这有一个非常实际的方面，当分布对一系列 X 的“在这个 X 的概率是多少”的问题有更远的答案时，它们被认为是更不同的。</p><p id="4943" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在自然梯度的背景下，KL 散度被用作测量我们的模型预测的输出分布变化的方法。如果我们正在解决一个多向分类问题，那么我们的模型的输出将是一个 softmax，它可以被视为一个多项式分布，每个类有不同的概率。当我们讨论由我们当前参数值定义的条件概率函数时，这就是我们讨论的概率分布。如果我们使用 KL 散度作为缩放梯度步长的方式，这意味着对于给定的输入特征集，如果两个参数配置会导致预测的类别分布在 KL 散度方面非常不同，则我们会看到这两个参数配置在该空间中“相距较远”。</p><h1 id="4033" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">费希尔的事</h1><p id="5616" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">到目前为止，我们已经讨论了为什么在参数空间中缩放我们的更新步骤的距离是不令人满意的任意性，并建议了一个不那么任意的替代方案:缩放我们的步骤，使得在 KL 偏离我们的模型先前预测的类分布方面，最大仅前进一定的距离。对我来说，理解自然梯度最困难的部分是下一部分:KL 散度和 Fisher 信息矩阵之间的联系。</p><p id="1ab2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从故事的结尾开始，自然渐变是这样实现的:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi md"><img src="../Images/9b01c82f1e80a3ea1c6d5aad839f6545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*1YdL1eZim3--C9zHFNkY_g.png"/></div><figcaption class="kv kw gj gh gi kx ky bd b be z dk">“Natural Gradient is defined as…”</figcaption></figure><p id="2db8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等号上的<em class="kt"> def </em>意味着右边的是左边符号的定义。右边的术语由两部分组成。首先，是损失函数相对于参数的梯度(这与更普通的梯度下降步骤中使用的梯度相同)。“自然”位来自第二部分:对数概率函数的平方梯度在 z 上的期望值。我们将被称为费希尔信息矩阵的整个对象乘以损失梯度的倒数。</p><p id="8fed" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">p-theta(z)项是我们模型定义的条件概率分布，也就是说:一个神经网络末端的 softmax。我们在看所有 p-θ项的梯度，因为我们关心，我们预测的类概率，会随着参数的变化而变化。预测概率的变化越大，我们的更新前和更新后预测分布之间的 KL 差异就越大。</p><p id="95aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自然梯度优化令人困惑的部分原因是，当你阅读或思考它时，有两个不同的梯度对象你必须理解和争论，这意味着不同的事情。作为一个题外话，这不可避免地陷入了困境，尤其是在讨论可能性的时候，而且也没有必要掌握整体直觉；如果你不想看所有血淋淋的细节，请随意跳到下一部分。</p><h2 id="413d" class="me la iq bd lb mf mg dn lf mh mi dp lj jy mj mk ln kc ml mm lr kg mn mo lv mp bi translated">相对于损失的梯度</h2><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/5632e549d9274221ddf4b23e73f67ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*8HWxP8SpeAE-wT8Bpf6hRQ.png"/></div></figure><p id="1691" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常，您的分类损失是一个交叉熵函数，但更广泛地说，它是一个将模型的预测概率分布和真实目标值作为输入的函数，并且当您的分布离目标值越远，它的值就越高。这个对象的梯度是梯度下降学习的核心面包和黄油；它表示如果您将每个参数移动一个单位，您的损失将会改变的量。</p><h2 id="4d17" class="me la iq bd lb mf mg dn lf mh mi dp lj jy mj mk ln kc ml mm lr kg mn mo lv mp bi translated">对数似然的梯度</h2><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/2b9c6cd3b52ec2bf17a6d33f9c999f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*XEuAR_Ju3Qg24BXQvEsFdA.png"/></div></figure><p id="08d7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对我来说，这无疑是学习自然渐变最令人困惑的部分。因为，如果你读过费希尔信息矩阵，你会得到很多链接，解释它与模型的对数似然的梯度有关。我以前对似然函数的理解是，它代表了你的模型认为某组数据的可能性有多大；特别是，您需要目标值来计算它，因为您的目标是当您根据输入要素对其进行调整时，计算您的模型分配给真实目标的概率。在讨论似然性的大多数情况下，例如非常常见的最大似然性技术，您关心对数似然性的梯度，因为您的似然性越高，您的模型分配从真实分布中采样的值的概率就越高，我们都越高兴。实际上，这类似于计算 p(class|x)梯度的期望值，期望值内的概率来自数据中的实际类别分布。</p><p id="5dc8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是，您也可以用另一种方式来评估可能性，而不是计算您相对于真实目标值的可能性(在这种情况下，您会期望有一个非零梯度，因为有可能推动您的参数来增加真实目标的可能性)，您可以使用从您的条件分布本身中提取的概率来计算您的期望<strong class="jp ir">。也就是说，如果您的网络产生了一个 softmax，而不是根据给定观测值的数据中的真实类来获取概率为 0/1 的 logp(z)的期望值，而是使用该类的模型估计概率作为其在期望值中的权重。这将导致总体预期梯度为 0，因为我们将模型的当前信念作为基础事实，但我们仍然可以获得梯度(即梯度平方)的<em class="kt">方差的估计，这是我们的费希尔矩阵中需要的，以(隐式)计算预测类空间中的 KL 散度。</em></strong></p><h1 id="edb7" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">那么…有帮助吗？</h1><p id="df08" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">这篇文章花了很多时间讨论力学:这个被称为自然梯度估计器的东西到底是什么，以及关于它如何以及为什么工作的更好的直觉。但是我觉得如果我不回答这个问题，我就是失职:这个东西真的提供价值吗？</p><p id="c954" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">简单的回答是:实际上来说，它并没有提供足够令人信服的价值来普遍用于大多数深度学习应用。有证据表明，自然梯度导致收敛发生在更少的步骤中，但是，正如我稍后将讨论的，这是一个有点复杂的比较。自然梯度的思想是优雅的，并且满足了那些对参数空间中任意缩放更新步骤感到沮丧的人。但是，除了优雅之外，我不清楚它是否提供了无法通过更多启发式方法提供的价值。</p><p id="8c00" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">据我所知，自然梯度提供了两个关键的价值来源:</p><ol class=""><li id="3d28" class="ms mt iq jp b jq jr ju jv jy mu kc mv kg mw kk mx my mz na bi translated">它提供了关于<strong class="jp ir">曲率</strong>的信息</li><li id="3782" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk mx my mz na bi translated">它提供了一种方法让<strong class="jp ir">直接控制</strong>你的模型<strong class="jp ir">在预测分布空间</strong>的移动，与你的模型在损失空间的移动是分开的</li></ol><h2 id="3340" class="me la iq bd lb mf mg dn lf mh mi dp lj jy mj mk ln kc ml mm lr kg mn mo lv mp bi translated">弯曲</h2><p id="d854" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">现代梯度下降的一大奇迹是它是用一阶方法完成的。一阶方法是只计算你想要更新的参数的导数，而不计算二阶导数的方法。有了一阶导数，你所知道的就是曲线在特定点的切线(多维版本)。你不知道切线变化有多快:二阶导数，或者更确切地说，你的函数在任何给定方向上的曲率水平。了解曲率是一件很有用的事情，因为在一个高曲率的区域，从一点到另一点的梯度变化很大，你可能要小心谨慎地迈出一大步，以免你攀登陡峭山峰的本地信号误导你跳下悬崖。我喜欢用一种更具启发性而非严格性的方式来思考这个问题，如果你在一个点与点之间的梯度变化很大(也就是说:方差很大)的区域，那么你对梯度的小批估计在某种意义上更不确定。相比之下，如果梯度在给定点几乎没有变化，那么在采取下一步时就不需要那么小心了。二阶导数信息非常有用，因为它可以让您根据曲率的级别来调整步长。</p><p id="4b96" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">自然梯度实际上是，机械地，将你的参数更新除以梯度的二阶导数。相对于给定的参数方向，梯度变化越大，费希尔信息矩阵中的值就越高，并且该方向上的更新步长就越低。这里讨论的梯度是你的批次中的点的经验似然的梯度。这和关于损失的梯度不是一回事。但是，直觉上，可能性的巨大变化不对应损失函数的巨大变化是很罕见的。因此，通过捕捉关于对数似然导数空间在给定点的曲率的信息，自然梯度也为我们提供了真实、潜在损失空间中曲率的良好信号。有一个很强的论点，当自然梯度被证明可以加速收敛(至少在所需梯度步骤的数量方面)，这就是好处的来源。</p><p id="8f82" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而。请注意，我说过自然梯度显示出加速收敛<strong class="jp ir">的梯度步骤</strong>。这种精度来自于这样一个事实，即自然梯度的每一步都需要更长的时间，因为它需要计算费希尔信息矩阵，记住，它是存在于 n_parameters 空间中的一个量。事实上，这种戏剧性的减速类似于通过计算真实损失函数的二阶导数而导致的减速。虽然可能是这样，但我没有看到任何地方说计算自然梯度费希尔矩阵比计算关于潜在损失的二阶导数更快。以此为假设，与对损失本身进行直接二阶优化的方法(也可能同样昂贵)相比，很难看出自然梯度提供了什么边际价值。</p><p id="6d2c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在理论预测仅一阶方法会失败的情况下，现代神经网络能够成功的很多原因是，深度学习实践者已经找到了一系列聪明的技巧，从本质上根据经验近似分析二阶导数矩阵中包含的信息。</p><ul class=""><li id="0f39" class="ms mt iq jp b jq jr ju jv jy mu kc mv kg mw kk ng my mz na bi translated"><strong class="jp ir">动量</strong>作为一种优化策略，通过保持过去梯度值的运行指数加权平均值，并将任何给定的梯度更新偏向过去的移动平均值来工作。这有助于解决处于梯度值变化很大的空间中的问题:如果你不断地得到矛盾的梯度更新，它们会以这样或那样的方式平均化为没有强烈的意见，类似于减慢你的学习速度。相比之下，如果你反复得到同一个方向的梯度估计，这是一个低曲率区域的指示，并表明一个更大的步骤的方法，动量跟随。</li><li id="fc70" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk ng my mz na bi translated"><strong class="jp ir"> RMSProp </strong>，有趣的是，它是 Geoff hint on mid-Coursera-Coursera course 发明的，是对之前存在的 Adagrad 算法的温和修改。RMSProp 的工作原理是对过去的平方梯度值进行指数加权移动平均，换句话说，就是梯度的过去方差，然后用该值除以更新步长。这可以粗略地认为是对梯度二阶导数的经验估计。</li><li id="383e" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk ng my mz na bi translated"><strong class="jp ir"> Adam </strong>(自适应矩估计)，本质上结合了这两种方法，估计梯度的运行均值和运行方差。这是当今最常见、最默认使用的优化策略之一，主要是因为它可以消除一阶梯度信号的噪声</li></ul><p id="ef02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">有趣的是，所有这些方法都值得一提的是，除了根据函数曲率来缩放更新步骤之外，它们还根据特定方向上的曲率量来不同地缩放不同的更新方向。这就涉及到我们之前讨论过的一些问题，关于如何用相同的量缩放所有参数可能不是一件明智的事情。你甚至可以从距离的角度来考虑这个问题:如果一个方向的曲率很高，那么在相同量的欧几里德参数空间中的一个步长将会使我们在梯度值的预期变化方面移动得更远。</p><p id="08e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因此，虽然在定义参数更新的一致方向方面，这没有自然梯度那么优雅，但它确实检查了大多数相同的框:在曲率不同的方向和时间点，以及理论上给定大小的参数步长具有不同程度的实际影响的地方，调整更新步长的能力。</p><h2 id="2ef9" class="me la iq bd lb mf mg dn lf mh mi dp lj jy mj mk ln kc ml mm lr kg mn mo lv mp bi translated">直接分配控制</h2><p id="2b44" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">好的，那么，最后一节提出:如果我们的目标是使用对数似然的解析曲率估计作为损失的曲率估计的替代，为什么我们不做后者，或者近似后者，因为两种解析 N 计算似乎都非常耗时。但是，如果你真的关心预测的阶级分布的变化，而不仅仅是作为损失变化的代表，那该怎么办呢？那样的情况会是什么样的呢？</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/e545e56418bdd100abd91af1a865bf41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dtG0VF_pAzx_aPCQgNFGQ.png"/></div></div></figure><p id="42d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种情况的一个例子大概不是巧合，是自然梯度方法当前使用的主要领域之一:<a class="ae nm" href="https://arxiv.org/abs/1502.05477" rel="noopener ugc nofollow" target="_blank">信任区域策略优化</a>在强化学习领域。TRPO 的基本直觉包含在灾难性失败或灾难性崩溃的想法中。在策略梯度设置中，您在模型末尾预测的分布是在操作上的分布，取决于某些输入状态。此外，如果您正在学习 on-policy，您的下一轮训练的数据是从您的模型的当前预测策略中收集的，则有可能将您的策略更新为一种制度，在这种制度下，您无法再收集有趣的数据来了解您的出路(例如，在一个圆圈中旋转的策略，这不太可能让您获得有用的奖励信号来学习)。这就是一项政策经历灾难性崩溃的含义。为了避免这种情况，我们希望谨慎行事，不要进行会显著改变我们政策的梯度更新(根据我们在给定场景中采取不同行动的概率)。如果我们在让我们预测的概率改变多少方面保持谨慎和渐进，这就限制了我们突然跳到一个不可行的制度的能力。</p><p id="db9d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">那么，这是自然梯度的一个更强的例子:在这里，<strong class="jp ir">我们真正关心控制的是在一个新的参数配置下，不同行为的预测概率会改变多少</strong>。我们关心它本身，而不仅仅是作为损失函数的代表。</p><h1 id="ceea" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">开放式问题</h1><p id="0f29" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我喜欢通过让你知道我对一个主题还有哪些困惑来结束这些帖子，因为，尽管解释者帖子的框架暗示了完全理解的崇高立场，但这并不完全是现实。和往常一样，如果你注意到我做错了什么，请评论/DM，我会努力改正的！</p><ul class=""><li id="1bd8" class="ms mt iq jp b jq jr ju jv jy mu kc mv kg mw kk ng my mz na bi translated">我从未最终发现计算对数似然费希尔矩阵是否比仅仅计算损失函数的海森更有效(如果是，这将是自然梯度作为获得损失表面曲率信息的更廉价方式的一个论据)</li><li id="efee" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk ng my mz na bi translated">相对而言，我并不完全相信，当我们对对数概率的 z 取一个期望值时，这个期望值就是我们的模型预测的概率(期望值必须相对于某个概率集来定义)。</li></ul><h1 id="0b58" class="kz la iq bd lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw bi translated">参考</h1><ul class=""><li id="41af" class="ms mt iq jp b jq lx ju ly jy nn kc no kg np kk ng my mz na bi translated"><a class="ae nm" href="https://arxiv.org/pdf/1301.3584.pdf" rel="noopener ugc nofollow" target="_blank">“再访深层网络的自然梯度”</a></li><li id="4877" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk ng my mz na bi translated"><a class="ae nm" href="https://arxiv.org/pdf/1507.00210.pdf" rel="noopener ugc nofollow" target="_blank">《自然神经网络》</a></li><li id="291b" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk ng my mz na bi translated"><a class="ae nm" href="http://- &quot;It's in general hard to choose a consistent learning rate...because the the gradient scales  vary in size for the different directions&quot; (interesting, this is just discussing gradient magnitude, not curvature)" rel="noopener ugc nofollow" target="_blank">《为什么自然渐变》</a></li><li id="554a" class="ms mt iq jp b jq nb ju nc jy nd kc ne kg nf kk ng my mz na bi translated">Sebastian Ruder 的<a class="ae nm" href="http://ruder.io/optimizing-gradient-descent/" rel="noopener ugc nofollow" target="_blank">关于自适应梯度方法的惊人帖子</a></li></ul></div></div>    
</body>
</html>