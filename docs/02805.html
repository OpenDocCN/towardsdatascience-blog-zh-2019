<html>
<head>
<title>Forward propagation in neural networks — Simplified math and code version</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的前向传播—简化的数学和代码版本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250?source=collection_archive---------3-----------------------#2019-05-07">https://towardsdatascience.com/forward-propagation-in-neural-networks-simplified-math-and-code-version-bbcfef6f9250?source=collection_archive---------3-----------------------#2019-05-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="e976" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们都知道从最近十年<strong class="js iu"><em class="ko"/></strong>深度学习已经成为最广泛接受的新兴技术之一。这是由于其功能的代表性。</p><p id="a0e3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">根据<strong class="js iu"> <em class="ko">通用逼近定理</em> </strong>，一个经过良好引导和工程化的<em class="ko">深度神经网络</em>可以逼近变量之间任意复杂和连续的关系。其实<em class="ko">深度学习</em>成功的背后还有其他几个原因。我不打算在这里讨论这些可能的原因。</p><p id="ffe7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这篇文章的目的是用更简单的方式解释<strong class="js iu"> <em class="ko">正向传播</em> </strong>(学习阶段的核心过程之一)。</p><p id="06b1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">学习算法/模型在<em class="ko">正向传播</em>和<em class="ko">反向传播的帮助下找出参数(权重和偏差)。</em></p><h1 id="41ee" class="kp kq it bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">正向传播</h1><p id="e0d3" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">顾名思义，输入数据是通过网络正向输入的。每个隐藏层接受输入数据，按照激活函数对其进行处理，并传递给后续层。</p><h2 id="fe0d" class="ls kq it bd kr lt lu dn kv lv lw dp kz kb lx ly ld kf lz ma lh kj mb mc ll md bi translated">为什么选择前馈网络？</h2><p id="3835" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">为了产生一些输出，输入数据应该只向前馈送。在生成输出的过程中，数据不应反向流动，否则会形成一个循环，永远无法生成输出。这种网络结构被称为<strong class="js iu"> <em class="ko">前馈网络</em> </strong>。<em class="ko">前馈网络</em>有助于<em class="ko">正向传播</em>。</p><p id="a2be" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在隐藏层或输出层中的每个神经元处，处理分两步进行:</p><ol class=""><li id="b38d" class="me mf it js b jt ju jx jy kb mg kf mh kj mi kn mj mk ml mm bi translated"><strong class="js iu">预激活:</strong>是输入的<em class="ko">加权和，即权重</em> w.r.t 对可用输入的<em class="ko">线性变换。基于这个<em class="ko">聚集总和</em>和<em class="ko">激活函数</em>，神经元决定是否进一步传递该信息。</em></li><li id="8c9e" class="me mf it js b jt mn jx mo kb mp kf mq kj mr kn mj mk ml mm bi translated"><strong class="js iu">激活:</strong>计算出的输入加权和传递给激活函数。激活函数是给网络增加非线性的数学函数。有四种常用和流行的激活函数— sigmoid、双曲正切(tanh)、ReLU 和 Softmax。</li></ol><p id="821e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们借助一个例子来理解<em class="ko">正向传播</em>。考虑一个<em class="ko">非线性</em>可分离数据，其形式为遵循漩涡模式的两个数据点的卫星。这个生成的数据有两个不同的类。</p><p id="f8b9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">可以使用<code class="fe ms mt mu mv b">sklearn.datasets</code>模块的<code class="fe ms mt mu mv b">make_moons()</code>功能生成数据。要生成的样本总数和关于月亮形状的噪声可以使用函数参数进行调整。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="b37c" class="ls kq it mv b gy ne nf l ng nh">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import matplotlib.colors<br/>from sklearn.datasets import make_moons</span><span id="5b56" class="ls kq it mv b gy ni nf l ng nh">np.random.seed(0)</span><span id="0616" class="ls kq it mv b gy ni nf l ng nh">data, labels = make_moons(n_samples=200,noise = 0.04,random_state=0)<br/>print(data.shape, labels.shape)</span><span id="529e" class="ls kq it mv b gy ni nf l ng nh">color_map = matplotlib.colors.LinearSegmentedColormap.from_list("", ["red","yellow"])<br/>plt.scatter(data[:,0], data[:,1], c=labels, cmap=my_cmap)<br/>plt.show()</span></pre><figure class="mw mx my mz gt nk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/78fc3ae770dfdadb127e06f90b63fae2.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*F50x2COgQ8LySWV0LtdFLw.png"/></div><figcaption class="nn no gj gh gi np nq bd b be z dk">dataset visualization</figcaption></figure><p id="7ed7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，200 个样本用于生成数据，它有两个类，以红色和绿色显示。</p><p id="bf33" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，让我们看看神经网络结构来预测这个<em class="ko">二元分类</em>问题的类。这里，我将使用一个具有两个神经元的<em class="ko">隐藏层</em>，一个具有单个神经元的<em class="ko">输出层</em>和<em class="ko"> sigmoid 激活</em>功能。</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/49889b35afdf83a65fd0b6b3520ad662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*tp73P0isrrfpj8RG-5aH6w.png"/></div></figure><p id="dff6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在<em class="ko">正向传播</em>期间，在隐藏和输出层<em class="ko">的每个节点发生预激活</em>和<em class="ko">激活</em>。例如在隐藏层的第一个节点，首先计算<strong class="js iu"> a1 </strong> ( <em class="ko">预激活</em>)，然后计算<strong class="js iu"> h1 </strong> ( <em class="ko">激活</em>)。</p><p id="8664" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> a1 </strong>是输入的加权和。这里，权重是随机生成的。</p><p id="0ffa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">a1</strong>= w1 * x1+w2 * x2+B1 = 1.76 * 0.88+0.40 *(-0.49)+0 = 1.37 近似值<strong class="js iu"> h1 </strong>是应用于 a1 的激活函数值。</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f5ab6b9da1d490cb1569fec852e1b791.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*WrkgXLQSjHpzmR_H3xsnCQ.png"/></div></figure><p id="ab8e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">类似地</p><p id="0931" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">a2</strong>= w3 * x1+w4 * x2+B2 = 0.97 * 0.88+2.24 *(0.49)+0 =-2.29 近似值和</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d6ef5dee853f49ce9c70af699b1e8fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/format:webp/1*46xma79g8Gdew_LbT6x2aw.png"/></div></figure><p id="654b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">对于第一个隐藏层之后的任何层，输入是从前一层输出的。</p><p id="4214" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">a3</strong>= w5 * h1+w6 * H2+B3 = 1.86 * 0.8+(-0.97)* 0.44+0 = 1.1 近似值</p><p id="a2aa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">和</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/5865604dd9416eaa6f5c53b035a6a295.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*lCVQROFldjILndKg-pKHxw.png"/></div></figure><p id="b39d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">所以有 74%的几率第一次观察属于第一类。像这样对所有其他的观察预测输出可以计算出来。</p><p id="5228" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下图显示了第一次观测的数据从输入图层到输出图层的转换。</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi nv"><img src="../Images/3b6b8db2cfd7a15112bdf7acdcf871ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ts5LSdtkfSsMYS7M0X84Tw.gif"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">Data transformation from the input layer to the output layer</figcaption></figure><p id="1123" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们看看上面的神经网络在 Jupyter 笔记本中的实现。事实上，在构建 Tensorflow、Keras、PyTorch 等深度神经网络框架的同时。被使用。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="11a1" class="ls kq it mv b gy ne nf l ng nh">from sklearn.model_selection import train_test_split</span><span id="1128" class="ls kq it mv b gy ni nf l ng nh">#Splitting the data into training and testing data<br/>X_train, X_val, Y_train, Y_val = train_test_split(data, labels, stratify=labels, random_state=0)<br/>print(X_train.shape, X_val.shape)<br/></span></pre><p id="d8ec" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，按照默认的 75:25 的分割比，150 个观察值用于训练目的，50 个用于测试目的。</p><p id="2de3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在让我们为<em class="ko">正向传播</em>定义一个类，其中权重被随机初始化。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="81d1" class="ls kq it mv b gy ne nf l ng nh">class FeedForwardNetwork:<br/> <br/> def __init__(self):<br/>     np.random.seed(0)<br/>     self.w1 = np.random.randn()<br/>     self.w2 = np.random.randn()<br/>     self.w3 = np.random.randn()<br/>     self.w4 = np.random.randn()<br/>     self.w5 = np.random.randn()<br/>     self.w6 = np.random.randn()<br/>     self.b1 = 0<br/>     self.b2 = 0<br/>     self.b3 = 0<br/> <br/> def sigmoid(self, x):<br/>     return 1.0/(1.0 + np.exp(-x))<br/> <br/> def forward_pass(self, x):<br/>     self.x1, self.x2 = x<br/>     self.a1 = self.w1*self.x1 + self.w2*self.x2 + self.b1<br/>     self.h1 = self.sigmoid(self.a1)<br/>     self.a2 = self.w3*self.x1 + self.w4*self.x2 + self.b2<br/>     self.h2 = self.sigmoid(self.a2)<br/>     self.a3 = self.w5*self.h1 + self.w6*self.h2 + self.b3<br/>     self.h3 = self.sigmoid(self.a3)<br/>     forward_matrix = np.array([[0,0,0,0,self.h3,0,0,0], <br/>                      [0,0,(self.w5*self.h1),        (self.w6*self.h2),self.b3,self.a3,0,0],<br/>                      [0,0,0,self.h1,0,0,0,self.h2],<br/>                      [(self.w1*self.x1), (self.w2*self.x2),         self.b1, self.a1,(self.w3*self.x1),(self.w4*self.x2), self.b2,  self.a2]])<br/>     forward_matrices.append(forward_matrix)<br/>     return self.h3</span></pre><p id="660c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这里，<code class="fe ms mt mu mv b">forward_pass()</code>函数计算给定输入观测值的输出值。<code class="fe ms mt mu mv b">forward_matrix </code>是一个 2d 数组，用于存储每次观察的 a1、h1、a2、h2、a3、h3 等的值。使用它的原因只是为了用 GIF 图像来可视化这些值的转换。<code class="fe ms mt mu mv b">forward_matrix </code>的条目如下所示</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi oa"><img src="../Images/68b931ca2997e1f70d6769f0636673b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*v_s725vl3FTGyCLmORU3HA.png"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">forward_matrix</figcaption></figure><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="a4b2" class="ls kq it mv b gy ne nf l ng nh">forward_matrices = []<br/>ffn = FeedForwardNetwork()<br/>for x in X_train:<br/>   ffn.forward_pass(x)</span></pre><p id="c637" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe ms mt mu mv b">forward_matrices </code>是所有观察值的列表<code class="fe ms mt mu mv b">forward_matrix</code>。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="fa02" class="ls kq it mv b gy ne nf l ng nh">import seaborn as sns<br/>import imageio<br/>from IPython.display import HTML</span><span id="8c99" class="ls kq it mv b gy ni nf l ng nh">def plot_heat_map(observation):<br/>    fig = plt.figure(figsize=(10, 1))<br/>    sns.heatmap(forward_matrices[observation], annot=True,     cmap=my_cmap, vmin=-3, vmax=3)<br/>    plt.title(“Observation “+str(observation))</span><span id="705c" class="ls kq it mv b gy ni nf l ng nh">    fig.canvas.draw()<br/>    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype=’uint8')<br/>    image = image.reshape(fig.canvas.get_width_height()[::-1] +             (3,))</span><span id="b0c4" class="ls kq it mv b gy ni nf l ng nh">    return image</span><span id="9abc" class="ls kq it mv b gy ni nf l ng nh">imageio.mimsave(‘./forwardpropagation_viz.gif’, [plot_heat_map(i) for i in range(0,len(forward_matrices),len(forward_matrices)//15)], fps=1)</span></pre><p id="0402" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><code class="fe ms mt mu mv b">plot_heat_map()</code>功能创建一个热图，以可视化每次观察的<code class="fe ms mt mu mv b">forward_matrix </code>值。这些热图存储在<code class="fe ms mt mu mv b">forwardpropagation_viz.gif</code>图像中。这里，总共为 15 个不同的观察创建了 15 个不同的热图。</p><figure class="mw mx my mz gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nw nx di ny bf nz"><div class="gh gi ob"><img src="../Images/7fba045077d59a2380522752a9476cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Y9Mt8EJnPaowXQfcleWSGg.gif"/></div></div><figcaption class="nn no gj gh gi np nq bd b be z dk">forward propagation for 15 different observations</figcaption></figure><h2 id="f203" class="ls kq it bd kr lt lu dn kv lv lw dp kz kb lx ly ld kf lz ma lh kj mb mc ll md bi translated">代码优化</h2><p id="0a0f" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">而不是使用不同的变量，如 w1、w2…w6、a1、a2、h1、h2 等。单独地，矢量化矩阵可以分别用于权重、预激活(a)和激活(h)。矢量化能够更高效、更快速地执行代码。它的语法也很容易理解和学习。</p><pre class="mw mx my mz gt na mv nb nc aw nd bi"><span id="fede" class="ls kq it mv b gy ne nf l ng nh">class FeedForwardNetwork_Vectorised:<br/> <br/> def __init__(self):<br/>    np.random.seed(0)<br/>    self.W1 = np.random.randn(2,2)<br/>    self.W2 = np.random.randn(2,1)<br/>    self.B1 = np.zeros((1,2))<br/>    self.B2 = np.zeros((1,1))<br/> <br/> def sigmoid(self, X):<br/>    return 1.0/(1.0 + np.exp(-X))<br/> <br/> <br/> def forward_pass(self,X):<br/>    self.A1 = np.matmul(X,self.W1) + self.B1 <br/>    self.H1 = self.sigmoid(self.A1) <br/>    self.A2 = np.matmul(self.H1, self.W2) + self.B2<br/>    self.H2 = self.sigmoid(self.A2) <br/>    return self.H2</span><span id="0da7" class="ls kq it mv b gy ni nf l ng nh">ffn_v = FeedForwardNetwork_Vectorised()<br/>ffn_v.forward_pass(X_train)</span></pre><h2 id="575c" class="ls kq it bd kr lt lu dn kv lv lw dp kz kb lx ly ld kf lz ma lh kj mb mc ll md bi translated"><strong class="ak">结论</strong></h2><p id="e34f" class="pw-post-body-paragraph jq jr it js b jt ln jv jw jx lo jz ka kb lp kd ke kf lq kh ki kj lr kl km kn im bi translated">这是从我的角度讲的<strong class="js iu"> <em class="ko">正向传播</em> </strong>，我希望我能够解释<em class="ko">正向传播</em>中涉及的直觉和步骤。如果你有兴趣学习或探索更多关于神经网络的知识，请参考我的其他关于神经网络的博客文章。链接如下</p><p id="7e5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae oc" href="https://www.analyticsvidhya.com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2020/10/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python/</a></p><p id="bd72" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae oc" rel="noopener" target="_blank" href="/why-better-weight-initialization-is-important-in-neural-networks-ff9acf01026d">为什么更好的权重初始化在神经网络中很重要？</a></p><p id="ecf3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae oc" rel="noopener" target="_blank" href="/analyzing-different-types-of-activation-functions-in-neural-networks-which-one-to-prefer-e11649256209">分析神经网络中不同类型的激活函数——选择哪一种？</a></p><p id="6e30" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae oc" rel="noopener" target="_blank" href="/why-gradient-descent-isnt-enough-a-comprehensive-introduction-to-optimization-algorithms-in-59670fd5c096">为什么梯度下降还不够:神经网络优化算法综合介绍</a></p></div></div>    
</body>
</html>