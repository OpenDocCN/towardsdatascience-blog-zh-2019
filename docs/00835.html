<html>
<head>
<title>Policy Based Reinforcement Learning, the Easy Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于策略的强化学习，简单的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083?source=collection_archive---------6-----------------------#2019-02-08">https://towardsdatascience.com/policy-based-reinforcement-learning-the-easy-way-8de9a3356083?source=collection_archive---------6-----------------------#2019-02-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="81b3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">逐步理解强化学习中基于策略的方法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a9cd90726996ab74973262d927b9899a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Lli8QtxraCbmaA8s"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@jmrthms?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Jomar</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="f3ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更新 1 </strong>:学习和练习强化学习的最好方式是去 http://rl-lab.com<a class="ae kv" href="http://rl-lab.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="7086" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更新 2 </strong>:如果你是这个主题的新手，从<a class="ae kv" rel="noopener" target="_blank" href="/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182">开发人员强化学习政策</a>文章开始可能更容易。</p><h2 id="07e0" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">介绍</h2><p id="36be" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">假设你在一个新城镇，既没有地图也没有 GPS，你需要到达市中心。你可以试着评估你相对于目的地的当前位置，以及你所采取的每个方向的有效性(价值)。你可以认为这是计算价值函数。或者你可以问一个当地人，他会告诉你直走，当你看到一个喷泉时，你向左走，一直走到市中心。他给了你一个可以遵循的政策。<br/>自然，在这种情况下，遵循给定的策略比自己计算价值函数要简单得多。</p><p id="b8f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在另一个例子中，假设您正在管理库存，并且您决定当每件商品的数量低于某个限制时，您发出一个购买订单来补充您的库存。这是一个比研究客户的活动、他们的购买习惯和偏好要简单得多的策略，以便预测对你的股票的影响…</p><p id="ff29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">毫无疑问，值函数将导致确定策略，如前几篇文章中所见，但是还有其他方法可以学习使用参数选择操作的策略，而无需咨询值函数(这不太正确，因为需要值函数来提高准确性)。</p><p id="349d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，主要思想是能够确定在状态<strong class="ky ir">(<em class="mq">【s】</em>)</strong>下采取哪种行动，以使回报最大化。</p><p id="0252" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实现这一目标的方法是微调𝜽指出的参数向量，以便为𝜋.政策选择最佳行动<br/>策略记为𝜋(a|s，𝜽) = Pr{At = a | St = s，𝜽t = 𝜽}，这意味着策略𝜋是在状态<strong class="ky ir"><em class="mq"/></strong>时采取行动<strong class="ky ir"> <em class="mq"> a </em> </strong>的概率，参数为𝜽.</p><h2 id="03e5" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">优势</h2><ul class=""><li id="8f00" class="mr ms iq ky b kz ml lc mm lf mt lj mu ln mv lr mw mx my mz bi translated">更好的收敛特性</li><li id="9955" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">在高维或连续动作空间有效<br/>当空间很大时，内存的使用和计算消耗增长很快。基于策略的 RL 避免了这一点，因为目标是学习一组远小于空间计数的参数。</li><li id="561b" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">可以学习随机策略<br/>随机策略比确定性策略更好，尤其是在两人游戏中，如果一个玩家确定性地行动，另一个玩家会制定对策以获胜。</li></ul><h2 id="eedd" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">不足之处</h2><ul class=""><li id="d975" class="mr ms iq ky b kz ml lc mm lf mt lj mu ln mv lr mw mx my mz bi translated">通常收敛于局部最优而不是全局最优</li><li id="a76f" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated">评估策略通常效率低下且差异大<br/>基于策略的 RL 差异很大，但有一些技术可以降低这种差异。</li></ul><h2 id="511d" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">随机政策</h2><p id="e609" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">首先需要注意的是，随机并不意味着在所有状态下都是随机的，但在某些有意义的状态下，它可能是随机的。<br/>通常报酬最大化会导致确定性政策。但是在某些情况下，确定性策略并不适合这个问题，例如在任何两个玩家的游戏中，确定性的参与意味着另一个玩家将能够采取反制措施以便一直获胜。例如，在石头剪刀布游戏中，如果我们每次都玩确定性的相同形状，那么其他玩家可以很容易地对抗我们的策略并赢得每场游戏。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/157fad54b28788a4a36911d93780a557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dAtXhlQMYLuYrLP0"/></div></div></figure><p id="9ef6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以在这个博弈中，最优策略是随机的，比确定性策略好。</p><h2 id="a383" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">蓝图</h2><p id="a395" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在深入研究数学和算法的细节之前，了解一下如何进行是很有用的，这是一种蓝图:</p><ol class=""><li id="90e0" class="mr ms iq ky b kz la lc ld lf ng lj nh ln ni lr nj mx my mz bi translated">找出一个可以用来评估政策有效性的目标函数。换句话说，告诉我们政策的效果有多好。</li><li id="e260" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr nj mx my mz bi translated">定义策略。<br/>我们的意思是列出一些可以在学习过程中使用的有用政策。</li><li id="a922" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr nj mx my mz bi translated">幼稚的算法。<br/>提出一种直接利用策略来学习参数的算法。</li><li id="5fb9" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr nj mx my mz bi translated">改进的算法<br/>寻找改进目标函数的算法，以最大化策略的有效性。</li></ol><h1 id="7705" class="nk lt iq bd lu nl nm nn lx no np nq ma jw nr jx md jz ns ka mg kc nt kd mj nu bi translated">蓝图的第 1 部分:寻找目标函数</h1><p id="c487" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">请记住，在上面的蓝图中，我们谈到了寻找一个目标函数来评估政策的有效性。在这一节中，我们将定义目标函数及其一些有用的推导。<br/>(关于政策梯度的更多细节可以在文章<a class="ae kv" href="https://medium.com/@zsalloum/policy-gradient-step-by-step-ac34b629fd55" rel="noopener">政策梯度一步一步</a>中找到)。</p><h2 id="e21a" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">目标函数</h2><p id="ec2e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">当谈到函数的最大化时，一个突出的方法是梯度。</p><p id="f443" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们如何根据𝜽最大化回报呢？一种方法是找到一个目标函数 J(𝜽)这样</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/db178bbabb40ca909867da456f9231b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*6EzBfaj1qRIkMlefHHkHew.png"/></div></figure><p id="1344" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，V𝜋𝜽是策略𝜋𝜽的值函数，<strong class="ky ir"> <em class="mq"> s0 </em> </strong>是起始状态。</p><p id="722b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，J(𝜽最大化意味着 V𝜋𝜽(s).最大化由此可见</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/83ae25646aedbba1bf1a3deadfdfa162.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*nMgXq87b28TYRK_A5yBLwA.png"/></div></figure><p id="ef16" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据政策梯度定理</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/6b2c0c1e7f6ce3c510bb18ee56aeb321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t5NaBEL942hh2ymqhc6Wrw.png"/></div></div></figure><p id="61f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中𝝻(s)是𝜋下的分布(意为遵循政策𝜋时处于状态<strong class="ky ir"> <em class="mq"> s </em> </strong>的概率)，q(s，a)是𝜋下的动作值函数，∇𝜋(a|s，𝜽)是𝜋给定<strong class="ky ir"> <em class="mq"> s </em> </strong>和𝜽.的梯度<br/>最后𝝰的意思是成比例的。</p><p id="104b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以这个定理说∇J(𝜽)正比于<strong class="ky ir"> <em class="mq"> q </em> </strong>函数的和乘以我们可能处于的状态下所有行为的策略梯度。然而我们不知道𝜋(a|s、𝜽)，我们怎么能找到它的梯度呢？</p><p id="0832" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">事实证明，如以下演示所示，这是可能的:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/07973105e78059f970d2a549d7623b0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Y4ANmESS6usU_Ozj9ST9g.png"/></div></div></figure><p id="2feb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">提醒:<strong class="ky ir"> ∫ dx/x = Log(x) </strong>意思是<strong class="ky ir"> dx/x = (Log(x))' = ∇Log(x) </strong></p><p id="f710" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> ∇Log 𝜋𝜃(s,a) </strong>被称为得分函数。<br/>注意，政策的梯度可以表示为预期。如果你问自己为什么？查看维基百科关于<a class="ae kv" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">期望值</a>的文章。</p><h2 id="c980" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">参数更新</h2><p id="0d2e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">由于这是一个梯度方法，参数的更新(我们正试图优化)将按通常的方式进行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/097dcccb8aec9a486c67bec2abe7f61e.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*Aoj2jjEYY77KsXDbYGZhjA.png"/></div></figure><h1 id="0962" class="nk lt iq bd lu nl nm nn lx no np nq ma jw nr jx md jz ns ka mg kc nt kd mj nu bi translated">蓝图的第 2 部分:定义策略</h1><p id="5313" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">本节介绍了几种标准梯度策略，如 Softmax 和 Guassian。我们在 RL 算法中使用这些策略来学习参数𝜽.<br/>实际上，每当在 RL 算法中我们看到对<strong class="ky ir"> ∇Log 𝜋𝜃(s,a) </strong>的引用时，我们就插入所选策略的公式。</p><h2 id="0e17" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">Softmax 策略</h2><p id="2eb9" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">softmax 策略由一个将输出转换为概率分布的 softmax 函数组成。这意味着它影响每个可能动作的概率。</p><p id="599a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Softmax 主要用于离散动作的情况:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/379fa79dcf21b44cd0958a868e815893.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*tmz_nlcdNyCN0LXr123EqA.png"/></div></figure><p id="329b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/21a35919416c468b767ee9121928952d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*eGHXH5ULTr515PEiin_Xbw.png"/></div></figure><p id="bc90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/91180d51d2405586657da39693e470c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*Ntw9uqEaWIhlTrmiCa84fw.png"/></div></figure><p id="fc94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里可以查看推导<a class="ae kv" href="https://math.stackexchange.com/questions/2013050/log-of-softmax-function-derivative" rel="noopener ugc nofollow" target="_blank">的完整演示。</a></p><h2 id="09b4" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">高斯策略</h2><p id="3d26" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">高斯策略用于连续动作空间的情况，例如当你驾驶一辆汽车时，你转动方向盘或踩下油门踏板，这些都是连续动作，因为这些都不是你做的少数动作，因为你可以(理论上)决定旋转角度或气体流量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/df82640156385cdbc34b21ecad2bc942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*HxRxNNYIDDrql-jAHwkLtQ.png"/></div></figure><p id="90dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">推导变成了</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f0b38afefe186434a8b3ebbee10c2372.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*tq1bXOKyQ1BjxAU_0pp33A.png"/></div></figure><h1 id="76eb" class="nk lt iq bd lu nl nm nn lx no np nq ma jw nr jx md jz ns ka mg kc nt kd mj nu bi translated">蓝图的第 3 部分:朴素算法</h1><p id="7dba" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">本节将给出一些算法，这些算法将考虑策略和它们的目标函数，以便学习给出最佳代理行为的参数。</p><h2 id="b347" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">强化(蒙特卡罗策略梯度)</h2><p id="a455" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">该算法使用蒙特卡罗根据策略𝜋𝜃创建剧集，然后对于每个剧集，它迭代该剧集的状态并计算总回报 G(t)。它使用 G(t)和∇Log 𝜋𝜃(s,a(可以是 Softmax 策略或其他)来学习参数𝜃.</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/e368e661dca10d6f3d8aaaeb3fcce657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAmPwZdNUDcMcuAtr-Xrfw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">from Sutton Barto book: Introduction to Reinforcement Learning</figcaption></figure><h1 id="3e7b" class="nk lt iq bd lu nl nm nn lx no np nq ma jw nr jx md jz ns ka mg kc nt kd mj nu bi translated">蓝图的第 4 部分:改进的算法</h1><p id="9ef5" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们已经说过基于策略的 RL 具有很高的方差。然而，有几个算法可以帮助减少这种差异，其中一些是加强基线和演员的批评。</p><h2 id="2f81" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">用基线算法增强</h2><p id="0411" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">基线的概念是从 G(t)中减去称为基线的 b(s ),目的是减少结果的大范围变化。<br/>假设 b(s)不依赖于动作 a，可以证明<strong class="ky ir"> ∇J( </strong> 𝜽)的方程仍然成立。</p><p id="4e91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以现在的问题是如何选择 b(s)？</p><p id="1cb0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基线的一个选择是计算状态值的估计值,( St，w ),其中 w 是通过蒙特卡罗等方法学习的参数向量。<br/>所以 b(s)=③(St，w)</p><p id="28a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用基线增强算法变成</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/896b2d500f4fd298a62b3cf937462e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0D_viRaWScI_vNyscg3eKA.png"/></div></div></figure><h2 id="101c" class="ls lt iq bd lu lv lw dn lx ly lz dp ma lf mb mc md lj me mf mg ln mh mi mj mk bi translated">演员评论家算法</h2><p id="1514" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">(详细解释见<a class="ae kv" rel="noopener" target="_blank" href="/introduction-to-actor-critic-7642bdb2b3d2">演员评论家简介</a>篇)<br/>演员评论家算法使用 TD 来计算用作评论家的价值函数。批评家是一个状态值函数。评估事情的进展是很有用的。在每个行动之后，批评家计算新的状态来确定是否有任何改进。这个评估就是 TD 误差:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6041ab9976d214eef1fe985f9ff719d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/0*sEsytkfpYbJ8uQD4.png"/></div></figure><p id="487d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，δ(t)用于调整参数𝜽和<strong class="ky ir"> w </strong>。<br/>简而言之，𝜽和<strong class="ky ir"> w </strong>都以修正该误差的方式进行调整。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/b27f2a58ee261fc0f0722a6f904422ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vy2RySGkQv1nECpLUxIrAw.png"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">from Sutton Barto book: Introduction to Reinforcement Learning</figcaption></figure><h1 id="396c" class="nk lt iq bd lu nl nm nn lx no np nq ma jw nr jx md jz ns ka mg kc nt kd mj nu bi translated">相关文章</h1><ul class=""><li id="6bc8" class="mr ms iq ky b kz ml lc mm lf mt lj mu ln mv lr mw mx my mz bi translated"><a class="ae kv" href="https://medium.com/@zsalloum/revisiting-policy-in-reinforcement-learning-for-developers-43cd2b713182" rel="noopener">开发人员强化学习政策</a></li><li id="775f" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/policy-gradient-step-by-step-ac34b629fd55">政策梯度逐步推进</a></li><li id="c4c1" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/function-approximation-in-reinforcement-learning-85a4864d566">强化学习中的函数逼近</a></li><li id="489b" class="mr ms iq ky b kz na lc nb lf nc lj nd ln ne lr mw mx my mz bi translated"><a class="ae kv" rel="noopener" target="_blank" href="/introduction-to-actor-critic-7642bdb2b3d2">强化学习中的演员评论家介绍</a></li></ul></div></div>    
</body>
</html>