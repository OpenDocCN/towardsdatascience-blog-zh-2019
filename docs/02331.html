<html>
<head>
<title>Processing a Slowly Changing Dimension Type 2 Using PySpark in AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 AWS 中使用 PySpark 处理渐变维度类型 2</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/processing-a-slowly-changing-dimension-type-2-using-pyspark-in-aws-9f5013a36902?source=collection_archive---------1-----------------------#2019-04-17">https://towardsdatascience.com/processing-a-slowly-changing-dimension-type-2-using-pyspark-in-aws-9f5013a36902?source=collection_archive---------1-----------------------#2019-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="6c61" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">随着新技术的出现，使得数据处理变得像闪电一样快，云生态系统允许灵活性、成本节约、安全性和便利性，似乎有一些数据建模哲学不太经常使用。其中一种方法是星型模式数据架构。</p><p id="44e8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">简而言之(<em class="ko">我假设阅读本文的人熟悉概念</em>)，星型模式以多维方式存储数据，以便通过提供更快更简单的查询性能来提供更好的用户体验。在维度模型中，数据驻留在事实表或维度表中。事实表保存动作的度量和相关维度的键，维度包含所述动作的属性。维度可以是静态的(例如时间维度)，也可以保存历史(也称为渐变维度类型 2，也称为 SCD2)。值得注意的是，星型模式是分析系统，这意味着它们不<em class="ko">通常</em>直接从用户应用程序中消费数据。相反，数据存储在高度规范化的事务系统中，星型模式利用一些提取过程从该系统中获取数据。</p><p id="24f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">下面是一个销售计划的基本星型模式的示例，它有一个事实表和三个维度(dim_customer_scd 是一个 SCD2)。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi kp"><img src="../Images/1aaf0b156c36c852ba16b71f7730e7c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*Zch3wJsUA-AnI_lfSuWXzg.png"/></div></figure><p id="3abf" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">星型模式通常在 RDBMS 层处理，这里允许更新和删除(这里不讨论),但是当事实和 SCD2 表包含数百万或数十亿行时，处理时间可能是个问题。在这个时代，我们经常会想到 Spark 或 Hadoop 这样的分布式集群框架。人们可以整天开玩笑和假设哪个是首选框架，但这不是本次讨论的主题，我将解释如何在 AWS 环境中使用 Spark 作为框架和 PySpark 作为脚本语言来处理 SCD2，并使用大量 SparkSQL。</p></div><div class="ab cl kx ky hx kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="im in io ip iq"><p id="ccf2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最基本的，SCD2 的目的是保存变更的历史。如果客户更改了他们的姓氏或地址，SCD2 将允许用户将订单链接回客户及其在订单时所处状态的属性。如果我不保留那段历史，就很难做到。本质上，我可以找到客户在任何时间点的真实状态。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi le"><img src="../Images/3bd316c3c5eed99e2cef7b915bbf462a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gzcVbxrEFBE33-IZQ1DYvA.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">dim_customer_scd (SCD2)</figcaption></figure><p id="a800" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据集非常窄，由 12 列组成。我可以把这些列分成 3 个小组。</p><ul class=""><li id="4233" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated"><strong class="js iu">按键</strong> : customer_dim_key</li><li id="b596" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated"><strong class="js iu">非维度属性</strong>:名，姓，中间名，地址，城市，州，邮政编码，客户号</li><li id="2763" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated"><strong class="js iu">行元数据</strong>:生效开始日期，生效结束日期，当前日期</li></ul><p id="ee5b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">key</strong>通常是自动创建的，没有商业价值。它们只是用作其他表的外键，并为行提供唯一性。有时可以使用自然键，但是在星型模式中非常少见。</p><p id="647f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">非维度属性</strong>描述了数据集主题的不同方面，在所用的例子中是一个客户。这些属性可能具有业务意义，并在报告和分析中被进一步用于下游。我将把这些简单地称为“属性”。</p><p id="18ea" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu">行元数据</strong>列特定于 SCD2s，描述记录的状态。eff_start_date 存储记录生效的日期。eff_end_date 存储记录到期的日期(<em class="ko">注意:未到期的记录通常有一个遥远的日期，如‘9999–12–31’)</em>；is_current 表示记录是否是最新的。is_current 对于生效日期来说是多余的，但是有些人喜欢这个标志，以避免在他们的查询或脚本中使用日期逻辑。</p><p id="7e9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我在这个数据集中有四行。如果我查看<strong class="js iu"> customer_number </strong>，我可以看到所有的行都由三个不同的客户组成:John、Susan 和 William。John 和 William(分别在第 1 行和第 4 行)在表中只有一个条目，这意味着他们的数据在插入后没有更改过。</p><p id="663e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">另一方面，Susan 有两条记录(第 2 行和第 3 行)。第 2 行是历史记录，用<strong class="js iu"> is_current = false </strong>表示，而第 3 行是 Susan 自<strong class="js iu"> is_current = true </strong>以来的最新信息。第 2 行“姓氏”列中的值为“Jones ”,第 3 行包含“Harris ”,而所有其他属性字段保持不变。</p><p id="9f21" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">当 Susan 的姓是 Jones 时，表中只有三行，当她的姓改变时，会添加第四行。在表格中实现这一点的实际逻辑是什么？在高层次上:</p><ol class=""><li id="2503" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn mb lt lu lv bi translated">发现源系统中的姓氏发生了变化。</li><li id="7150" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn mb lt lu lv bi translated">记录已过期；eff_end_date 更新为昨天的日期，is_current 设置为 false。</li><li id="e3d8" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn mb lt lu lv bi translated">将插入一条新记录，其中包含新的姓氏、今天的生效开始日期和将来的生效结束日期，并且 is_current 设置为 true。</li></ol><p id="14c9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="ko">注意:对于 eff_start_date 和/或 eff_end_date 字段，使用逻辑业务日期通常更好，但是为了讨论的目的，让它保持简单</em></p><p id="7163" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在 RDBMS 中，很容易实现这一点。然而，由于 Spark 中更新功能的各种限制，我不得不采用不同的方式。</p></div><div class="ab cl kx ky hx kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="im in io ip iq"><p id="c97c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">是时候讨论细节了。</p><h2 id="764c" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 1:创建 Spark 会话</h2><p id="fe27" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">我可以开始我们的 Spark 会议，并为我们在 S3 的目标路径创建一个变量:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="cadb" class="mc md it nb b gy nf ng l nh ni">from pyspark.sql import SparkSession</span><span id="7c61" class="mc md it nb b gy nj ng l nh ni">spark = SparkSession.builder.appName("scd2_demo").getOrCreate()</span><span id="9813" class="mc md it nb b gy nj ng l nh ni">v_s3_path = "s3://mybucket/dim_customer_scd"</span></pre><h2 id="b7b7" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 2:创建 SCD2 数据集(用于演示目的)</h2><p id="2edc" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">您可以使用下面的脚本来生成当前的 SCD2，将其写入 Parquet，创建一个临时表，并查看结果(我<em class="ko">将始终使用这个模式来帮助描述</em>):</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="f473" class="mc md it nb b gy nf ng l nh ni"># ############## generate current_scd2 dataset ############## #</span><span id="5806" class="mc md it nb b gy nj ng l nh ni">hd_current_scd2 = """<br/> SELECT   BIGINT(1) AS customer_dim_key,<br/>          STRING('John') AS first_name,<br/>          STRING('Smith') AS last_name,<br/>          STRING('G') AS middle_initial,<br/>          STRING('123 Main Street') AS address,<br/>          STRING('Springville') AS city,<br/>          STRING('VT') AS state,<br/>          STRING('01234-5678') AS zip_code,<br/>          BIGINT(289374) AS customer_number,<br/>          DATE('2014-01-01') AS eff_start_date,<br/>          DATE('9999-12-31') AS eff_end_date,<br/>          BOOLEAN(1) AS is_current<br/> UNION<br/> SELECT   BIGINT(2) AS customer_dim_key,<br/>          STRING('Susan') AS first_name,<br/>          STRING('Jones') AS last_name,<br/>          STRING('L') AS middle_initial,<br/>          STRING('987 Central Avenue') AS address,<br/>          STRING('Central City') AS city,<br/>          STRING('MO') AS state,<br/>          STRING('49257-2657') AS zip_code,<br/>          BIGINT(862447) AS customer_number,<br/>          DATE('2015-03-23') AS eff_start_date,<br/>          DATE('2018-11-17') AS eff_end_date,<br/>          BOOLEAN(0) AS is_current<br/> UNION<br/> SELECT   BIGINT(3) AS customer_dim_key,<br/>          STRING('Susan') AS first_name,<br/>          STRING('Harris') AS last_name,<br/>          STRING('L') AS middle_initial,<br/>          STRING('987 Central Avenue') AS address,<br/>          STRING('Central City') AS city,<br/>          STRING('MO') AS state,<br/>          STRING('49257-2657') AS zip_code,<br/>          BIGINT(862447) AS customer_number,<br/>          DATE('2018-11-18') AS eff_start_date,<br/>          DATE('9999-12-31') AS eff_end_date,<br/>          BOOLEAN(1) AS is_current<br/> UNION<br/> SELECT   BIGINT(4) AS customer_dim_key,<br/>          STRING('William') AS first_name,<br/>          STRING('Chase') AS last_name,<br/>          STRING('X') AS middle_initial,<br/>          STRING('57895 Sharp Way') AS address,<br/>          STRING('Oldtown') AS city,<br/>          STRING('CA') AS state,<br/>          STRING('98554-1285') AS zip_code,<br/>          BIGINT(31568) AS customer_number,<br/>          DATE('2018-12-07') AS eff_start_date,<br/>          DATE('9999-12-31') AS eff_end_date,<br/>          BOOLEAN(1) AS is_current<br/>"""</span><span id="14c5" class="mc md it nb b gy nj ng l nh ni">df_current_scd2 = spark.sql(hd_current_scd2)</span><span id="b8f5" class="mc md it nb b gy nj ng l nh ni">df_current_scd2.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/current_scd2/")</span><span id="63f1" class="mc md it nb b gy nj ng l nh ni">df_current_scd2.createOrReplaceTempView("current_scd2")</span><span id="5872" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="bcdc" class="mc md it nb b gy nj ng l nh ni">df_current_scd2 = spark.read.parquet(v_s3_path + "/current_scd2/*").orderBy("customer_dim_key")</span><span id="c05c" class="mc md it nb b gy nj ng l nh ni">df_current_scd2.show(10, False)</span></pre><h2 id="8310" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 3:从源系统创建客户数据集(用于演示目的)</h2><p id="ad2c" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">您可以使用下面的脚本来生成您的源数据，我将用它来修改我们的 SCD2:</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="5284" class="mc md it nb b gy nf ng l nh ni"># ############## generate customer_data dataset ############## #</span><span id="5e81" class="mc md it nb b gy nj ng l nh ni">hd_customer_data = """<br/> SELECT   BIGINT(289374) AS customer_number,<br/>          STRING('John') AS first_name,<br/>          STRING('Smith') AS last_name,<br/>          STRING('G') AS middle_initial,<br/>          STRING('456 Derry Court') AS address,<br/>          STRING('Springville') AS city,<br/>          STRING('VT') AS state,<br/>          STRING('01234-5678') AS zip_code<br/> UNION<br/> SELECT   BIGINT(932574) AS customer_number,<br/>          STRING('Lisa') AS first_name,<br/>          STRING('Cohen') AS last_name,<br/>          STRING('S') AS middle_initial,<br/>          STRING('69846 Mason Road') AS address,<br/>          STRING('Atlanta') AS city,<br/>          STRING('GA') AS state,<br/>          STRING('26584-3591') AS zip_code<br/> UNION<br/> SELECT   BIGINT(862447) AS customer_number,<br/>          STRING('Susan') AS first_name,<br/>          STRING('Harris') AS last_name,<br/>          STRING('L') AS middle_initial,<br/>          STRING('987 Central Avenue') AS address,<br/>          STRING('Central City') AS city,<br/>          STRING('MO') AS state,<br/>          STRING('49257-2657') AS zip_code<br/> UNION<br/> SELECT   BIGINT(31568) AS customer_number,<br/>          STRING('William') AS first_name,<br/>          STRING('Chase') AS last_name,<br/>          STRING('X') AS middle_initial,<br/>          STRING('57895 Sharp Way') AS address,<br/>          STRING('Oldtown') AS city,<br/>          STRING('CA') AS state,<br/>          STRING('98554-1285') AS zip_code<br/>"""</span><span id="4f23" class="mc md it nb b gy nj ng l nh ni">df_customer_data= spark.sql(hd_customer_data)</span><span id="53a9" class="mc md it nb b gy nj ng l nh ni">df_customer_data.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/customer_data/")</span><span id="90f1" class="mc md it nb b gy nj ng l nh ni">df_customer_data.createOrReplaceTempView("customer_data")</span><span id="5747" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="596c" class="mc md it nb b gy nj ng l nh ni">df_customer_data= spark.read.parquet(v_s3_path + "/customer_data/*").orderBy("customer_number")</span><span id="55da" class="mc md it nb b gy nj ng l nh ni">df_customer_data.show(10, False)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi le"><img src="../Images/3bd316c3c5eed99e2cef7b915bbf462a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gzcVbxrEFBE33-IZQ1DYvA.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Current State of the SCD2</figcaption></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nk"><img src="../Images/9d157054879a309e81fe9663153d206b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_40-hRciC4qqE40V-wLlw.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Customer_Data from Source System</figcaption></figure><h2 id="9d80" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">第四步:手动查找变更(<em class="nl">只是为了主题</em>)</h2><p id="0d35" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">请记住，来自源系统的数据馈入到我们的 SCD2 中，因此我需要比较这两个数据集，以确定是否有任何差异。经过我们的手动调查，我看到:</p><ul class=""><li id="101f" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">约翰·史密斯改变了他们的地址</li><li id="d572" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">丽莎·科恩是一位新顾客</li><li id="4480" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">威廉·蔡斯和苏珊·哈里斯的属性保持不变</li></ul><p id="cbac" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">现在，我需要编写一些逻辑来完成以下所有任务:</p><ul class=""><li id="a81d" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">为 John Smith 创建新条目</li><li id="e42e" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">使约翰·史密斯的当前条目过期</li><li id="244b" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">保持威廉·蔡斯和苏珊·哈里斯的记录不变</li><li id="3a56" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">为我们的新客户 Lisa Cohen 添加一个条目</li></ul><h2 id="36f9" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 5:为现有客户创建新的当前记录</h2><p id="335e" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">为了从逻辑上捕捉这种地址变化，我需要比较当前的 SCD2 和源数据(<em class="ko">，就像我在</em>上面手动做的那样)以及标志变化。我还需要注意我们的行元数据字段，以确保我使用适当的日期终止和开始记录。</p><p id="fe4c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">幸运的是，我可以在一个 SQL 块中完成这项工作，并将结果写到 S3 的一个文件中:</p><ul class=""><li id="59e3" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">将 customer_data 数据集连接到 customer_number 上的当前 SCD2 数据集和当前记录</li><li id="9355" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">检查 WHERE 子句中的差异</li><li id="dbf6" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">从源数据集中选择所有属性</li><li id="29c2" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">从当前 SCD2 数据集中选择 customer _ dim _ key(<em class="ko">用于步骤 6 </em>)</li><li id="5748" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将生效日期设置为今天</li><li id="9df1" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将生效日期设置为将来</li><li id="cee8" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将 is_current 设置为 1</li></ul><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="3fe6" class="mc md it nb b gy nf ng l nh ni"># ############## create new current recs dataaset ############## #</span><span id="1a49" class="mc md it nb b gy nj ng l nh ni">hd_new_curr_recs = """<br/> SELECT   t.customer_dim_key,<br/>          s.customer_number,<br/>          s.first_name,<br/>          s.last_name,<br/>          s.middle_initial,<br/>          s.address,<br/>          s.city,<br/>          s.state,<br/>          s.zip_code,<br/>          DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'PDT'))<br/>              AS eff_start_date,<br/>          DATE('9999-12-31') AS eff_end_date,<br/>          BOOLEAN(1) AS is_current<br/> FROM     customer_data s<br/>          INNER JOIN current_scd2 t<br/>              ON t.customer_number = s.customer_number<br/>              AND t.is_current = True<br/> WHERE    NVL(s.first_name, '') &lt;&gt; NVL(t.first_name, '')<br/>          OR NVL(s.last_name, '') &lt;&gt; NVL(t.last_name, '')<br/>          OR NVL(s.middle_initial, '') &lt;&gt; NVL(t.middle_initial, '')<br/>          OR NVL(s.address, '') &lt;&gt; NVL(t.address, '')<br/>          OR NVL(s.city, '') &lt;&gt; NVL(t.city, '')<br/>          OR NVL(s.state, '') &lt;&gt; NVL(t.state, '')<br/>          OR NVL(s.zip_code, '') &lt;&gt; NVL(t.zip_code, '')<br/>"""</span><span id="e401" class="mc md it nb b gy nj ng l nh ni">df_new_curr_recs = spark.sql(hd_new_curr_recs)</span><span id="08e5" class="mc md it nb b gy nj ng l nh ni">df_new_curr_recs.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/new_curr_recs/")</span><span id="d465" class="mc md it nb b gy nj ng l nh ni">df_new_curr_recs.createOrReplaceTempView("new_curr_recs")</span><span id="1921" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="5930" class="mc md it nb b gy nj ng l nh ni">df_new_curr_recs = spark.read.parquet(v_s3_path + "/new_curr_recs/*").orderBy("customer_number")</span><span id="7948" class="mc md it nb b gy nj ng l nh ni">df_new_curr_recs.show(10, False)</span></pre><p id="2423" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上面的逻辑遍历所有记录，找到我手动观察到的一个变化。正如您在下面看到的，为 John Smith 用他的新地址创建了一个新记录，行元数据显示这个记录今天生效，将来过期。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nm"><img src="../Images/1d75489c680e9547bea429d02ad01d47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xV3vQQcDdLov8gOEzvjnIg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">New Record for John Smith</figcaption></figure><h2 id="e2e8" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 6:查找要过期的以前的当前记录</h2><p id="c894" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">现在我有了一个已经存在的客户的新的当前记录，我需要终止以前的当前记录。我在之前的数据集中包含了之前的当前记录的 customer_dim_key，所以我将其分离出来供将来使用。</p><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="0799" class="mc md it nb b gy nf ng l nh ni"># ########### isolate keys of records to be modified ########### #</span><span id="3fa9" class="mc md it nb b gy nj ng l nh ni">df_modfied_keys = df_new_curr_recs.select("customer_dim_key")</span><span id="8964" class="mc md it nb b gy nj ng l nh ni">df_modfied_keys.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/modfied_keys/")</span><span id="2a92" class="mc md it nb b gy nj ng l nh ni">df_modfied_keys.createOrReplaceTempView("modfied_keys")</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/276582c2e5311ba49283489aaf0f985e.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*73gaypn4FzbAapytMoWfDg.png"/></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Records to be Modified</figcaption></figure><h2 id="e609" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 7:终止以前的当前记录</h2><p id="d3ed" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">现在我可以让之前的记录过期，同时再次注意我们的行元数据字段并正确地修改它们。回想一下，我不能更新记录，所以我必须创建它的一个新实例。</p><p id="47f9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在这段代码中，我将:</p><ul class=""><li id="bbf1" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">将当前 SCD2 数据集连接到 customer_dim_key 上的 modified_keys 数据集</li><li id="660f" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">通过仔细检查 is_current = 1，确保当前记录即将到期</li><li id="2866" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">从当前 SCD2 数据集中选择所有属性和 eff_start_date</li><li id="22e8" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将生效日期设置为昨天</li><li id="a3e0" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将 is_current 设置为 0</li></ul><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="35ee" class="mc md it nb b gy nf ng l nh ni"># ############## create new hist recs dataaset ############## #</span><span id="b909" class="mc md it nb b gy nj ng l nh ni">hd_new_hist_recs = """<br/> SELECT   t.customer_dim_key,<br/>          t.customer_number,<br/>          t.first_name,<br/>          t.last_name,<br/>          t.middle_initial,<br/>          t.address,<br/>          t.city,<br/>          t.state,<br/>          t.zip_code,<br/>          t.eff_start_date,<br/>          DATE_SUB(<br/>              DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'PDT')), 1<br/>          ) AS eff_end_date,<br/>          BOOLEAN(0) AS is_current<br/> FROM     current_scd2 t<br/>          INNER JOIN modfied_keys k<br/>              ON k.customer_dim_key = t.customer_dim_key<br/> WHERE    t.is_current = True<br/>"""</span><span id="fe97" class="mc md it nb b gy nj ng l nh ni">df_new_hist_recs = spark.sql(hd_new_hist_recs)</span><span id="6f3d" class="mc md it nb b gy nj ng l nh ni">df_new_hist_recs.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/new_hist_recs/")</span><span id="3578" class="mc md it nb b gy nj ng l nh ni">df_new_hist_recs.createOrReplaceTempView("new_hist_recs")</span><span id="fb6a" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="d3c0" class="mc md it nb b gy nj ng l nh ni">df_new_hist_recs = spark.read.parquet(v_s3_path + "/new_hist_recs/*").orderBy("customer_number")</span><span id="b24a" class="mc md it nb b gy nj ng l nh ni">df_new_hist_recs.show(10, False)</span></pre><p id="74d2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">上述逻辑使记录正确过期，并写入其自己的数据集:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi no"><img src="../Images/6b5f929469ffaeec9bf382a9b6e9aa3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAASXjUKDgLK1KFqLdaGVg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Expired Record for John Smith</figcaption></figure><h2 id="ba1d" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 8:隔离未受影响的记录</h2><p id="ef71" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">威廉·蔡斯和苏珊·哈里斯的记录没有变化，它们需要保留在目标数据集中，所以下一步是将它们放入自己的数据集中。</p><p id="0fbb" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">该逻辑将:</p><ul class=""><li id="e74a" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">将 modified_keys 数据集左连接到 customer_dim_key 上的当前 SC2 数据集</li><li id="4cc6" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">查看不在 modified_keys 数据集中的记录</li><li id="d126" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">保持所有属性和行元数据不变</li></ul><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="c0bd" class="mc md it nb b gy nf ng l nh ni"># ############## create unaffected recs dataset ############## #</span><span id="6234" class="mc md it nb b gy nj ng l nh ni">hd_unaffected_recs = """<br/> SELECT   s.customer_dim_key,<br/>          s.customer_number,<br/>          s.first_name,<br/>          s.last_name,<br/>          s.middle_initial,<br/>          s.address,<br/>          s.city,<br/>          s.state,<br/>          s.zip_code,<br/>          s.eff_start_date,<br/>          s.eff_end_date,<br/>          s.is_current<br/> FROM     current_scd2 s<br/>          LEFT OUTER JOIN modfied_keys k<br/>              ON k.customer_dim_key = s.customer_dim_key<br/> WHERE    k.customer_dim_key IS NULL<br/>"""</span><span id="21bb" class="mc md it nb b gy nj ng l nh ni">df_unaffected_recs = spark.sql(hd_unaffected_recs)</span><span id="ee63" class="mc md it nb b gy nj ng l nh ni">df_unaffected_recs.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/unaffected_recs/")</span><span id="6259" class="mc md it nb b gy nj ng l nh ni">df_unaffected_recs.createOrReplaceTempView("unaffected_recs")</span><span id="8114" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="c24b" class="mc md it nb b gy nj ng l nh ni">df_unaffected_recs = spark.read.parquet(v_s3_path + "/unaffected_recs/*").orderBy("customer_number")</span><span id="7be6" class="mc md it nb b gy nj ng l nh ni">df_unaffected_recs.show(10, False)</span></pre><p id="ada0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">未受影响的记录实际上是孤立的:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi np"><img src="../Images/a1012465eae826b9dc4aecae1a6096d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*onIOCFgm2Pwj1exklywhWA.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">Unaffected Recs</figcaption></figure><h2 id="1f7f" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 9:为新客户创建记录</h2><p id="6ca1" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">Lisa Cohen 是一位新客户，因此还不存在于我们的 SCD2 中。</p><p id="202e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">以下逻辑:</p><ul class=""><li id="8bdf" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">Left 将当前 SCD2 数据集连接到 customer_number 上的 customer_data 数据集</li><li id="aa85" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">查看不在当前 SCD2 数据集中的记录</li><li id="b5c0" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">使用源中的所有属性</li><li id="7b1e" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将生效日期设置为今天</li><li id="2873" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将生效结束日期设置为将来</li><li id="1229" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将 is_current 设置为 1</li></ul><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="71f8" class="mc md it nb b gy nf ng l nh ni"># ############## create new recs dataset ############## #</span><span id="7956" class="mc md it nb b gy nj ng l nh ni">hd_new_cust = """<br/> SELECT   s.customer_number,<br/>          s.first_name,<br/>          s.last_name,<br/>          s.middle_initial,<br/>          s.address,<br/>          s.city,<br/>          s.state,<br/>          s.zip_code,<br/>          DATE(FROM_UTC_TIMESTAMP(CURRENT_TIMESTAMP, 'PDT')) <br/>              AS eff_start_date,<br/>          DATE('9999-12-31') AS eff_end_date,<br/>          BOOLEAN(1) AS is_current<br/> FROM     customer_data s<br/>          LEFT OUTER JOIN current_scd2 t<br/>              ON t.customer_number = s.customer_number<br/> WHERE    t.customer_number IS NULL<br/>"""</span><span id="bdfa" class="mc md it nb b gy nj ng l nh ni">df_new_cust = spark.sql(hd_new_cust)</span><span id="a9a6" class="mc md it nb b gy nj ng l nh ni">df_new_cust.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/new_cust/")</span><span id="6312" class="mc md it nb b gy nj ng l nh ni">df_new_cust.createOrReplaceTempView("new_cust")</span><span id="81e0" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="a6a0" class="mc md it nb b gy nj ng l nh ni">df_new_cust = spark.read.parquet(v_s3_path + "/new_cust/*").orderBy("customer_number")</span><span id="511b" class="mc md it nb b gy nj ng l nh ni">df_new_cust.show(10, False)</span></pre><p id="7fba" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这是结果。新客户的预期格式是:</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nq"><img src="../Images/98300a93c683e4d8021b8f0bf7830b0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aed33Syu7FBQlE7YQF3ZkQ.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">New Customer Record</figcaption></figure><h2 id="b7a7" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">步骤 10:合并新 SCD2 的数据集</h2><p id="67f0" class="pw-post-body-paragraph jq jr it js b jt mv jv jw jx mw jz ka kb mx kd ke kf my kh ki kj mz kl km kn im bi translated">我已经创建了创建新的 SCD2 迭代所需的四个数据集:</p><ul class=""><li id="a032" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">现有客户的新当前记录的数据集("<em class="ko"> new_curr_recs </em>")</li><li id="abec" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">现有客户的过期前当前记录的数据集("<em class="ko"> new_hist_recs </em>")</li><li id="c595" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">未被修改的记录数据集("<em class="ko">未受影响 _ 记录</em>")</li><li id="b39c" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">以前未见过的新客户的数据集("<em class="ko"> new_cust </em>")</li></ul><p id="7886" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">剩下的就是将它们融合在一起，形成最终产品。在最后这段代码中，我将:</p><ul class=""><li id="ca8b" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">找出最大客户尺寸关键值</li><li id="1bcf" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">联合两个没有 customer_dim_key 的数据集:new_cust 和 new_curr_recs</li><li id="601b" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">要创建新的 customer_dim_key，请使用 ROW_NUMBER()函数并添加最大 customer_dim_key 值(以保持连续性和唯一性)</li><li id="7a2d" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">将先前联合的数据集联合到 unaffected _ recs 和 new_hist_recs</li></ul><pre class="kq kr ks kt gt na nb nc nd aw ne bi"><span id="7751" class="mc md it nb b gy nf ng l nh ni"># ############## create new scd2 dataset ############## #</span><span id="c80d" class="mc md it nb b gy nj ng l nh ni">v_max_key = spark.sql(<br/>    "SELECT STRING(MAX(customer_dim_key)) FROM current_scd2"<br/>).collect()[0][0]</span><span id="50a0" class="mc md it nb b gy nj ng l nh ni">hd_new_scd2 = """<br/> WITH a_cte<br/> AS   (<br/>        SELECT     x.first_name, x.last_name,<br/>                   x.middle_initial, x.address,<br/>                   x.city, x.state, x.zip_code,<br/>                   x.customer_number, x.eff_start_date,<br/>                   x.eff_end_date, x.is_current<br/>        FROM       new_cust x<br/>        UNION ALL<br/>        SELECT     y.first_name, y.last_name,<br/>                   y.middle_initial, y.address,<br/>                   y.city, y.state, y.zip_code,<br/>                   y.customer_number, y.eff_start_date,<br/>                   y.eff_end_date, y.is_current<br/>        FROM       new_curr_recs y<br/>      )<br/>  ,   b_cte<br/>  AS  (<br/>        SELECT  ROW_NUMBER() OVER(ORDER BY a.eff_start_date)<br/>                    + BIGINT('{v_max_key}') AS customer_dim_key,<br/>                a.first_name, a.last_name,<br/>                a.middle_initial, a.address,<br/>                a.city, a.state, a.zip_code,<br/>                a.customer_number, a.eff_start_date,<br/>                a.eff_end_date, a.is_current<br/>        FROM    a_cte a<br/>      )<br/>  SELECT  customer_dim_key, first_name, last_name,<br/>          middle_initial, address,<br/>          city, state, zip_code,<br/>          customer_number, eff_start_date,<br/>          eff_end_date, is_current<br/>  FROM    b_cte<br/>  UNION ALL<br/>  SELECT  customer_dim_key, first_name,  last_name,<br/>          middle_initial, address,<br/>          city, state, zip_code,<br/>          customer_number, eff_start_date,<br/>          eff_end_date, is_current<br/>  FROM    unaffected_recs<br/>  UNION ALL<br/>  SELECT  customer_dim_key, first_name,  last_name,<br/>          middle_initial, address,<br/>          city, state, zip_code,<br/>          customer_number, eff_start_date,<br/>          eff_end_date, is_current<br/>  FROM    new_hist_recs<br/>"""</span><span id="d533" class="mc md it nb b gy nj ng l nh ni">df_new_scd2 = spark.sql(hd_new_scd2.replace("{v_max_key}", v_max_key))</span><span id="e271" class="mc md it nb b gy nj ng l nh ni"># ############## review dataset ############## #</span><span id="ab6c" class="mc md it nb b gy nj ng l nh ni">df_new_scd2.coalesce(1).write.mode("overwrite").parquet(v_s3_path + "/new_scd2/")</span><span id="24e3" class="mc md it nb b gy nj ng l nh ni">df_new_scd2 = spark.read.parquet(v_s3_path + "/new_scd2/*").orderBy("customer_dim_key")</span><span id="2676" class="mc md it nb b gy nj ng l nh ni">df_new_scd2.show(10, False)</span></pre><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="gh gi nr"><img src="../Images/29d4f61d544db950345f5001e9283ae5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcaO4uZNoe7cQ1zHX_eoSg.png"/></div></div><figcaption class="lj lk gj gh gi ll lm bd b be z dk">New SCD2</figcaption></figure><p id="988e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在新版本的 SCD2 中，一切都如预期的那样:</p><ul class=""><li id="3735" class="ln lo it js b jt ju jx jy kb lp kf lq kj lr kn ls lt lu lv bi translated">威廉·蔡斯和苏珊·哈里斯的记录没有改变</li><li id="1e8e" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">约翰·史密斯有一个旧地址在 2019 年 4 月 14 日到期的记录</li><li id="cfcc" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">约翰·史密斯有一个新记录，新地址于 2019 年 4 月 15 日生效</li><li id="2438" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">丽莎·科恩的新纪录于 2019 年 4 月 15 日生效</li><li id="24f0" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn ls lt lu lv bi translated">数据集上一次迭代中存在的记录保留了它们的 customer_dim_keys，因此星型模式中的事实表不需要重新映射</li></ul><p id="a4ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">新的 SCD2 存放在 S3，可以随心所欲地使用。</p><h2 id="d14b" class="mc md it bd me mf mg dn mh mi mj dp mk kb ml mm mn kf mo mp mq kj mr ms mt mu bi translated">一些注意事项:</h2><ol class=""><li id="3d04" class="ln lo it js b jt mv jx mw kb ns kf nt kj nu kn mb lt lu lv bi translated">表演很精彩。在我的生产环境中，源表有 382 列和大约 700 万条记录，SCD2 有 81 列和大约 1 . 1 亿条记录。处理这些数据平均需要大约 10 分钟。在标准 RDBMS 中，大约 180 分钟即可完成。处理时间缩短了 94%。</li><li id="6d52" class="ln lo it js b jt lw jx lx kb ly kf lz kj ma kn mb lt lu lv bi translated">我在这次讨论中使用的 S3 路径并不真正适用于现实生活中的商业场景，但在这个教学场景中非常适用。</li></ol></div><div class="ab cl kx ky hx kz" role="separator"><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc ld"/><span class="la bw bk lb lc"/></div><div class="im in io ip iq"><p id="fa12" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我很难在网上找到任何关于使用 Spark 处理 SCD2s 和 star 图式的信息。坦白地说，这很令人沮丧，我很高兴自己弄明白了这一点。此外，我惊喜地发现，不需要火箭科学学位就能做到这一点。我希望您可能遇到的任何挫折都是短暂的，并且这个讨论可以帮助您进行大数据、星型模式脚本工作。</p></div></div>    
</body>
</html>