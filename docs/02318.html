<html>
<head>
<title>Calculating the Semantic Brand Score with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 计算语义品牌得分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/calculating-the-semantic-brand-score-with-python-3f94fb8372a6?source=collection_archive---------16-----------------------#2019-04-16">https://towardsdatascience.com/calculating-the-semantic-brand-score-with-python-3f94fb8372a6?source=collection_archive---------16-----------------------#2019-04-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3670" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">大数据时代的品牌智能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f11ffab0082d4f639641944e2b71b869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KmektU_kJtMCadpWUnSA-w.jpeg"/></div></div></figure><p id="fb8c" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">语义品牌得分</strong> ( <strong class="kw iu"> SBS </strong>)是一个新颖的指标，旨在评估一个或多个品牌在不同背景下的重要性，只要有可能分析<strong class="kw iu">文本数据</strong>，甚至<strong class="kw iu">大数据</strong>。</p><p id="d631" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">相对于一些传统措施的优势在于，SBS 不依赖于对小样本消费者进行的调查。这一衡量标准可以根据任何文本来源进行计算，例如报纸文章、电子邮件、推文、在线论坛、博客和社交媒体上的帖子。这个想法是通过对<strong class="kw iu">大文本数据</strong>的分析来捕捉洞察和<strong class="kw iu">诚实信号</strong> 。消费者或其他品牌利益相关者的自发表达可以从他们通常出现的地方收集——例如，如果研究博物馆品牌的重要性，可以从旅游论坛收集。这样做的好处是减少了因使用问卷而产生的偏见，因为受访者知道他们正在被观察。SBS 还可以适应不同的语言，并研究特定单词或一组单词的重要性，不一定是“品牌”。</p><p id="d306" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">“品牌”可以是一个政治家的名字，或者是代表一个概念的一组词(例如，“创新”的概念或者一个公司的核心价值)。该指标用于评估一个新品牌取代一个旧品牌时发生的转变动态[1]。语义品牌得分也有助于将品牌的重要性与其竞争对手的重要性联系起来，或者分析单个品牌的重要性时间趋势。在一些应用中，分数被证明对于预测目的是有用的；例如，已经发现政治候选人在网络媒体上的品牌重要性与选举结果之间存在联系[4]，或者博物馆品牌的重要性与游客数量的趋势之间存在联系[6]。使用 SBS 的出版物的更新列表是<a class="ae lq" href="https://semanticbrandscore.com/articles/sbsarticles.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">，可在此处</strong> </a>获得。</p><h2 id="5e88" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">品牌重要性的三个维度</h2><p id="031f" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">SBS 衡量<strong class="kw iu">品牌重要性</strong>，这是品牌资产的基础[1]。事实上，这一指标部分受到了众所周知的品牌资产概念化以及品牌形象和品牌意识结构的启发(参见凯勒的<a class="ae lq" href="https://journals.sagepub.com/doi/abs/10.1177/002224299305700101?journalCode=jmxa" rel="noopener ugc nofollow" target="_blank">工作</a>)【2】。</p><p id="0735" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">品牌重要性从三个维度来衡量:<em class="mp">流行度</em>、<em class="mp">多样性</em>和<em class="mp">连通性</em>。<strong class="kw iu">流行度</strong>衡量品牌名称的使用频率，即品牌被直接提及的次数。<strong class="kw iu">多样性</strong>衡量与品牌相关的词语的多样性。<strong class="kw iu">连接性</strong>代表品牌在其他单词或词组(有时被视为话语主题)之间架起连接桥梁的能力。</p><p id="52c8" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">关于 SBS 的更多信息可以在<a class="ae lq" href="https://semanticbrandscore.com/" rel="noopener ugc nofollow" target="_blank">这个网站</a>【5】，在<a class="ae lq" href="https://en.wikipedia.org/wiki/Semantic_Brand_Score" rel="noopener ugc nofollow" target="_blank">维基百科</a>，或者阅读<a class="ae lq" href="https://www.sciencedirect.com/science/article/pii/S0148296318301541" rel="noopener ugc nofollow" target="_blank">这篇论文</a>【1】中找到。在本文中，我不会在这个指标上花太多时间，因为我的重点是描述使用 Python 3 计算它的主要步骤。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="518f" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">数据收集和文本预处理</h2><p id="8ed9" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">语义品牌得分的计算需要结合文本挖掘和社会网络分析的方法和工具。图 1 说明了主要的初步步骤，包括数据收集、文本预处理和构建单词共现网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/ea56e61fc3bfcae489e0b03ea6e73ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k_6ixQSNVGo3c7xhjDi5zg.jpeg"/></div></div></figure><p id="a04a" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">对于本入门教程，我们可以假设相关的文本数据已经收集并组织在一个文本文件中，其中每一行都是一个不同的文档。我将把两个想象的品牌<em class="mp">(‘BrandA’</em>和【T20’‘BrandB’)插入随机的英文文本中。</p><h2 id="0526" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">使用 Python 3 计算语义品牌得分</h2><p id="8437" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">此演示的更新<strong class="kw iu"> GitHub 库</strong>可在<a class="ae lq" href="https://github.com/iandreafc/semanticbrandscore-demo" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">此处</strong> </a>获得。在那里，您将找到一个 Python 笔记本，以及示例文件。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="5043" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu"># Read text documents from an example CSV file</strong><br/>import csv<br/>readfile = csv.reader(open("AliceWonderland.csv", 'rt',  encoding="utf8"), delimiter = "|", quoting=csv.QUOTE_NONE)</span><span id="c804" class="lr ls it mz b gy nh ne l nf ng">texts = [line[0] for line in readfile]</span><span id="bae8" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#I imported 4 Chapters of Alice in Wonderland</strong><br/>print(len(texts))<br/>print(texts[0][:200])</span></pre><p id="0b79" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我将文本文件作为文本文档列表(<em class="mp"> texts </em>)导入 Python，现在对其进行处理以删除标点符号、停用词和特殊字符。单词被小写并拆分成记号，从而获得一个新的<em class="mp"> texts </em>变量，这是一个列表的列表。更复杂的文本操作<strong class="kw iu">预处理</strong>总是可能的(比如移除 html 标签或' # ')，为此我推荐阅读 Python 中自然语言处理的众多教程之一。停用词列表取自 NLTK 包。最后，单词词缀通过滚雪球式词干去除。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="e094" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu">##Import re, string and nltk, and download stop-words</strong><br/>import re<br/>import nltk<br/>import string<br/>from nltk.stem.snowball import SnowballStemmer</span><span id="69af" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Define stopwords</strong><br/>#nltk.download("stopwords")<br/>stopw = nltk.corpus.stopwords.words('english')</span><span id="e8e3" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Define brands (lowercase)</strong><br/>brands = ['alice', 'rabbit']</span><span id="74de" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu"># texts is a list of strings, one for each document analyzed.</strong></span><span id="36d2" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Convert to lowercase</strong><br/>texts = [t.lower() for t in texts]<br/><strong class="mz iu">#Remove words that start with HTTP</strong><br/>texts = [re.sub(r"http\S+", " ", t) for t in texts]<br/><strong class="mz iu">#Remove words that start with WWW</strong><br/>texts = [re.sub(r"www\S+", " ", t) for t in texts]<br/><strong class="mz iu">#Remove punctuation</strong><br/>regex = re.compile('[%s]' % re.escape(string.punctuation))<br/>texts = [regex.sub(' ', t) for t in texts]<br/><strong class="mz iu">#Remove words made of single letters</strong><br/>texts = [re.sub(r'\b\w{1}\b', ' ', t) for t in texts]<br/><strong class="mz iu">#Remove stopwords</strong><br/>pattern = re.compile(r'\b(' + r'|'.join(stopw) + r')\b\s*')<br/>texts = [pattern.sub(' ', t) for t in texts]<br/><strong class="mz iu">#Remove additional whitespaces</strong><br/>texts = [re.sub(' +',' ',t) for t in texts]</span><span id="1f51" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Tokenize text documents (becomes a list of lists)</strong><br/>texts = [t.split() for t in texts]</span><span id="492a" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu"># Snowball Stemming</strong><br/>stemmer = SnowballStemmer("english")<br/>texts = [[stemmer.stem(w) if w not in brands else w for w in t] for t in texts]<br/>texts[0][:6]</span></pre><p id="a183" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在文本预处理过程中，我们应该注意不要丢失有用的信息。表情符号:-)，由标点符号组成，如果我们计算情感，它会非常重要。</p><p id="22df" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们现在可以继续计算<strong class="kw iu">流行度</strong>，它计算每个品牌名称的出现频率——<strong class="kw iu">随后标准化</strong>，考虑文本中所有单词的得分。我在这里选择的标准化是减去均值，除以标准差。其他方法也是可能的[1]。这一步对于比较考虑不同时间框架或文件集(例如，4 月和 5 月 Twitter 上的品牌重要性)的衡量标准非常重要。在合计流行度、多样性和连接性以获得语义品牌得分之前，绝对得分的标准化是必要的。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="27c1" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu">#PREVALENCE</strong><br/><strong class="mz iu">#Import Counter and Numpy</strong><br/>from collections import Counter<br/>import numpy as np</span><span id="ac82" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Create a dictionary with frequency counts for each word</strong><br/>countPR = Counter()<br/>for t in texts:<br/>    countPR.update(Counter(t))</span><span id="eb84" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Calculate average score and standard deviation</strong><br/>avgPR = np.mean(list(countPR.values()))<br/>stdPR = np.std(list(countPR.values()))</span><span id="b37d" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Calculate standardized Prevalence for each brand</strong><br/>PREVALENCE = {}<br/>for brand in brands:<br/>    PR_brand = (countPR[brand] - avgPR) / stdPR<br/>    PREVALENCE[brand] = PR_brand<br/>    print("Prevalence", brand, PR_brand)</span></pre><p id="1a38" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">下一步也是最重要的一步是将文本(标记列表的列表)转换成一个社交网络，其中<strong class="kw iu">节点是单词</strong>和<strong class="kw iu">链接根据每对单词之间的共现次数</strong>进行加权。在这个步骤中，我们必须定义一个<strong class="kw iu">共现范围</strong>，即共现单词之间的最大距离(这里设置为 7)。此外，我们可能想要移除表示可忽略的同现的链接，例如那些权重= 1 的链接。如果这些不是品牌，有时去除隔离物也是有用的。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="b9d7" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu">#Import Networkx</strong><br/>import networkx as nx</span><span id="57e2" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Choose a co-occurrence range</strong><br/>co_range = 7</span><span id="1fbe" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Create an undirected Network Graph</strong><br/>G = nx.Graph()</span><span id="8b05" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Each word is a network node</strong><br/>nodes = set([item for sublist in texts for item in sublist])<br/>G.add_nodes_from(nodes)</span><span id="ead7" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Add links based on co-occurrences</strong><br/>for doc in texts:<br/>    w_list = []<br/>    length= len(doc)<br/>    for k, w in enumerate(doc):<br/>        <strong class="mz iu">#Define range, based on document length</strong><br/>        if (k+co_range) &gt;= length:<br/>            superior = length<br/>        else:<br/>            superior = k+co_range+1<br/>        <strong class="mz iu">#Create the list of co-occurring words</strong><br/>        if k &lt; length-1:<br/>            for i in range(k+1,superior):<br/>                linked_word = doc[i].split()<br/>                w_list = w_list + linked_word<br/>        <strong class="mz iu">#If the list is not empty, create the network links</strong><br/>        if w_list:    <br/>            for p in w_list:<br/>                if G.has_edge(w,p):<br/>                    G[w][p]['weight'] += 1<br/>                else:<br/>                    G.add_edge(w, p, weight=1)<br/>        w_list = []</span><span id="fff8" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Remove negligible co-occurrences based on a filter</strong><br/>link_filter = 2<br/><strong class="mz iu">#Create a new Graph which has only links above<br/>#the minimum co-occurrence threshold</strong><br/>G_filtered = nx.Graph() <br/>G_filtered.add_nodes_from(G)<br/>for u,v,data in G.edges(data=True):<br/>    if data['weight'] &gt;= link_filter:<br/>        G_filtered.add_edge(u, v, weight=data['weight'])</span><span id="b5f7" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Optional removal of isolates</strong><br/>isolates = set(nx.isolates(G_filtered))<br/>isolates -= set(brands)<br/>G_filtered.remove_nodes_from(isolates)</span><span id="f75f" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Check the resulting graph (for small test graphs)</strong><br/>#G_filtered.nodes()<br/>#G_filtered.edges(data = True)<br/>print("Filtered Network\nNo. of Nodes:", G_filtered.number_of_nodes(), "No. of Edges:", G_filtered.number_of_edges())</span></pre><p id="207b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">确定了共现网络之后，我们现在可以计算<strong class="kw iu">多样性</strong>和<strong class="kw iu">连通性</strong>，它们是品牌节点的<strong class="kw iu">独特性中心性</strong>(之前我们使用度)和加权介数中心性。我们对这些值进行标准化，就像我们对患病率进行标准化一样。关于<strong class="kw iu">显著性中心性</strong>的更多信息在<strong class="kw iu"/>【7】和<a class="ae lq" rel="noopener" target="_blank" href="/distinctiveness-centrality-56c1e6762328?source=friends_link&amp;sk=4c92a370a95cc75a78d833bf10ceba7b"> <strong class="kw iu">这篇文章</strong> </a>中给出。你还需要安装 Python<a class="ae lq" href="https://github.com/iandreafc/distinctiveness" rel="noopener ugc nofollow" target="_blank"><strong class="kw iu"><em class="mp">distinct ns</em>包</strong> </a> <strong class="kw iu">。</strong></p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="5b27" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu">#INSTALL AND IMPORT THE DISTINCTIVENESS PACKAGE</strong><br/>#pip install -U distinctiveness<br/>from distinctiveness.dc import distinctiveness</span><span id="463b" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#DIVERSITY</strong><br/><strong class="mz iu">#Calculate Distinctiveness Centrality</strong><br/>DC = distinctiveness(G_filtered, normalize = False, alpha = 1)<br/>DIVERSITY_sequence=DC["D2"]</span><span id="204e" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Calculate average score and standard deviation</strong><br/>avgDI = np.mean(list(DIVERSITY_sequence.values()))<br/>stdDI = np.std(list(DIVERSITY_sequence.values()))</span><span id="d985" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Calculate standardized Diversity for each brand</strong><br/>DIVERSITY = {}<br/>for brand in brands:<br/>    DI_brand = (DIVERSITY_sequence[brand] - avgDI) / stdDI<br/>    DIVERSITY[brand] = DI_brand<br/>    print("Diversity", brand, DI_brand)</span></pre><p id="f384" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果我们将连通性计算为加权中间中心性，我们首先必须定义<strong class="kw iu">逆权重</strong>，因为权重被 Networkx 视为距离(这与我们的情况相反)。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="c19c" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu">#Define inverse weights </strong><br/>for u,v,data in G_filtered.edges(data=True):<br/>    if 'weight' in data and data['weight'] != 0:<br/>        data['inverse'] = 1/data['weight']<br/>    else:<br/>        data['inverse'] = 1</span><span id="0475" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#CONNECTIVITY</strong><br/>CONNECTIVITY_sequence=nx.betweenness_centrality(G_filtered, normalized=False, weight ='inverse')<br/><strong class="mz iu">#Calculate average score and standard deviation</strong><br/>avgCO = np.mean(list(CONNECTIVITY_sequence.values()))<br/>stdCO = np.std(list(CONNECTIVITY_sequence.values()))<br/><strong class="mz iu">#Calculate standardized Prevalence for each brand</strong><br/>CONNECTIVITY = {}<br/>for brand in brands:<br/>    CO_brand = (CONNECTIVITY_sequence[brand] - avgCO) / stdCO<br/>    CONNECTIVITY[brand] = CO_brand<br/>    print("Connectivity", brand, CO_brand)</span></pre><p id="4525" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">每个品牌的<strong class="kw iu">语义品牌得分</strong>最终通过对流行度、多样性和连接性的标准化值求和得到。不同的方法也是可能的，例如取非标准化系数的几何平均值。</p><pre class="kj kk kl km gt my mz na nb aw nc bi"><span id="338f" class="lr ls it mz b gy nd ne l nf ng"><strong class="mz iu">#Obtain the Semantic Brand Score of each brand</strong><br/>SBS = {}<br/>for brand in brands:<br/>    SBS[brand] = PREVALENCE[brand] + DIVERSITY[brand] + CONNECTIVITY[brand]<br/>    print("SBS", brand, SBS[brand])</span><span id="279d" class="lr ls it mz b gy nh ne l nf ng"><strong class="mz iu">#Generate a final pandas data frame with all results</strong><br/>import pandas as pd</span><span id="a7b3" class="lr ls it mz b gy nh ne l nf ng">PREVALENCE = pd.DataFrame.from_dict(PREVALENCE, orient="index", columns = ["PREVALENCE"])<br/>DIVERSITY = pd.DataFrame.from_dict(DIVERSITY, orient="index", columns = ["DIVERSITY"])<br/>CONNECTIVITY = pd.DataFrame.from_dict(CONNECTIVITY, orient="index", columns = ["CONNECTIVITY"])<br/>SBS = pd.DataFrame.from_dict(SBS, orient="index", columns = ["SBS"])</span><span id="ddb5" class="lr ls it mz b gy nh ne l nf ng">SBS = pd.concat([PREVALENCE, DIVERSITY, CONNECTIVITY, SBS], axis=1, sort=False)<br/>SBS</span></pre><h2 id="6a2f" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">分析演示</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/da6564ee1a4cc383126b542f0db01b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9cH1CxmOSM75-bS21x051Q.jpeg"/></div></div></figure><p id="8152" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><a class="ae lq" href="https://semanticbrandscore.com/demographs/graphs.html" rel="noopener ugc nofollow" target="_blank">这个链接</a>指向一个简短的<strong class="kw iu">演示</strong>，一旦计算出 SBS，就可以进行分析[8]。为了推断独特的和共享的品牌特征，词共现网络可以另外用于研究文本品牌关联。品牌情感的计算也可以补充分析。</p><h2 id="dd23" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">结论</h2><p id="7a03" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">本文简要介绍了语义品牌得分，并提供了使用 Python 3 简化计算的简短教程。在学习基础知识的同时，我们应该记住，有许多选择可以做出，并且会影响结果。例如，可以选择不同的加权方案或标准化方法，将 3 个维度合并为一个分数。应特别注意选择合适的词共现范围。此外，可以使用不同的技术来修剪那些假定代表可忽略的同现的链接。</p><p id="0598" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最后，如果计算是在大数据上进行的，最终的代码会复杂得多。像介数中心性这样的度量在大型图上有很高的计算复杂度。<a class="ae lq" href="https://graph-tool.skewed.de/" rel="noopener ugc nofollow" target="_blank"> Graph-Tool </a>是一个对我帮助很大的库，因为它的性能明显高于 Networkx。<strong class="kw iu"> </strong>在某些情况下，处理初始数据集可以降低复杂性。例如，对于在线新闻，人们可以选择只分析标题和第一段，而不是全部内容。</p><p id="c5e3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">作为一名自学的 Python 程序员，我将感谢您对这个指标及其有效计算的任何评论或建议。随时<a class="ae lq" href="https://andreafc.com" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">联系我</strong> </a>。</p><p id="09a2" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">您也可以查看 GitHub 资源库中的笔记本来了解这个演示(<a class="ae lq" href="https://github.com/iandreafc/semanticbrandscore-demo" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">此处</strong> </a>)。</p><h2 id="e8b8" class="lr ls it bd lt lu lv dn lw lx ly dp lz ld ma mb mc lh md me mf ll mg mh mi mj bi translated">参考</h2><p id="ce8b" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">[1] Fronzetti Colladon，A. (2018 年)。语义品牌得分。<em class="mp">商业研究杂志</em>，<em class="mp"> 88 </em>，150–160。<a class="ae lq" href="https://doi.org/10.1016/j.jbusres.2018.03.026" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.jbusres.2018.03.026</a></p><p id="8f75" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[2]凯勒，K. L. (1993 年)。概念化，测量和管理基于顾客的品牌资产。<em class="mp">市场营销杂志</em>，<em class="mp"> 57 </em> (1)，1–22。</p><p id="0120" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[3] <a class="ae lq" href="https://en.wikipedia.org/wiki/Semantic_Brand_Score" rel="noopener ugc nofollow" target="_blank">维基百科上的语义品牌评分</a>页面。</p><p id="d3a7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[4] Fronzetti Colladon，A. (2020 年)。通过研究在线新闻中的品牌重要性预测选举结果。<em class="mp">国际预测杂志</em>，<em class="mp"> 36 </em> (2)，414–427。【https://doi.org/10.1016/j.ijforecast.2019.05.013 T4】</p><p id="09bf" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[5]<a class="ae lq" href="https://semanticbrandscore.com/" rel="noopener ugc nofollow" target="_blank">Semanticbrandscore.com</a>，公制网站，有更新的链接和信息</p><p id="deed" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[6] Fronzetti Colladon，a .，Grippa，f .，&amp; Innarella，R. (2020 年)。研究线上品牌重要性与博物馆访客的关联性:语意品牌评分的应用。<em class="mp">旅游管理透视</em>，<em class="mp"> 33 </em>，100588。<a class="ae lq" href="https://doi.org/10.1016/j.tmp.2019.100588" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.tmp.2019.100588</a></p><p id="3dcc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[7]弗伦泽蒂·科拉顿和纳尔迪，M. (2020 年)。社会网络中的独特性中心性。<em class="mp"> PLoS ONE </em>，<em class="mp"> 15 </em> (5)，e0233276。<a class="ae lq" href="https://doi.org/10.1371/journal.pone.0233276" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1371/journal.pone.0233276</a></p><p id="6f19" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">[8]弗伦泽蒂·科拉顿和格里帕(2020 年)。品牌情报分析。在 A. Przegalinska，F. Grippa 和 P. A. Gloor(编辑)，<em class="mp">协作的数字化转型</em>(第 125–141 页)。瑞士斯普林格自然基金会。<a class="ae lq" href="https://doi.org/10.1007/978-3-030-48993-9_10" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-3-030-48993-9_10</a></p></div></div>    
</body>
</html>