<html>
<head>
<title>Beginner’s Guide to BERT for Multi-classification Task</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多分类任务的 BERT 初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/beginners-guide-to-bert-for-multi-classification-task-92f5445c2d7c?source=collection_archive---------3-----------------------#2019-06-11">https://towardsdatascience.com/beginners-guide-to-bert-for-multi-classification-task-92f5445c2d7c?source=collection_archive---------3-----------------------#2019-06-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/cabd8e8b7794512f82bd78eb8948820a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NhUXAvxbE1NKFLeYa82stw.jpeg"/></div></div><figcaption class="kb kc gj gh gi kd ke bd b be z dk">Original Photo by <a class="ae kf" href="https://unsplash.com/@davidpisnoy?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">David Pisnoy</a> on <a class="ae kf" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>. It was later modified to include some inspiring quotes.</figcaption></figure><p id="12ed" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本文的目的是提供如何使用 BERT 进行多分类任务的分步指南。BERT(<strong class="ki iu">B</strong>I directional<strong class="ki iu">E</strong>n coder<strong class="ki iu">R</strong>presentations from<strong class="ki iu">T</strong>transformers)，是 Google 提出的一种新的预训练语言表示方法，旨在解决广泛的自然语言处理任务。该模型基于无监督的深度双向系统，并在 2018 年首次向公众发布时成功实现了最先进的结果。如果你想了解更多，可以在下面的<a class="ae kf" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">链接</a>中找到学术论文。</p><p id="437e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本教程有 5 个部分:</p><ol class=""><li id="4a38" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">设置和安装</li><li id="bc0d" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">数据集准备</li><li id="a1d4" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">培训模式</li><li id="37b7" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">预言；预测；预告</li><li id="2bed" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">结论</li></ol></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="4c9f" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">[第 1 节]设置和安装</h1><p id="aad7" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在本教程中，我将使用 Ubuntu 18.04 搭配单个 GeForce RTX 2080 Ti。就我个人而言，我不建议在没有 GPU 的情况下进行训练，因为基础模型太大了，训练时间非常长。</p><h2 id="301a" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">虚拟环境</h2><p id="db53" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">建议建立一个虚拟环境。如果你是第一次使用 Ubuntu，打开终端，将目录切换到你想要的位置。它将是您环境的根文件夹。运行以下命令安装 pip:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1326" class="nc ma it nt b gy nx ny l nz oa">sudo apt-get install python3-pip</span></pre><p id="938f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后，运行以下命令安装 virtualenv 模块:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7c7e" class="nc ma it nt b gy nx ny l nz oa">sudo pip3 install virtualenv</span></pre><p id="150c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您现在可以创建自己的虚拟环境(用您喜欢的任何名称替换 bertenv):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="4361" class="nc ma it nt b gy nx ny l nz oa">virtualenv bertenv</span></pre><p id="e8c1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你喜欢不使用 virtualenv 模块，还有一种方法可以创建虚拟环境，只需使用 python3</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="6792" class="nc ma it nt b gy nx ny l nz oa">python3 -m venv bertenv</span></pre><p id="e77a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您应该已经创建了一个<strong class="ki iu"> bertenv </strong>文件夹。查看以下<a class="ae kf" href="https://gist.github.com/Geoyi/d9fab4f609e9f75941946be45000632b" rel="noopener ugc nofollow" target="_blank">链接</a>了解更多信息。您可以使用以下命令激活虚拟环境:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1487" class="nc ma it nt b gy nx ny l nz oa">source bertenv/bin/activate</span></pre><h2 id="6391" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">开源代码库</h2><p id="8a4e" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">从下面的<a class="ae kf" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">链接</a>克隆存储库。完成后，解压缩 zip 文件并将其放入您选择的目录中。你应该有一个<strong class="ki iu"> bert-master </strong>文件夹。我把它放在虚拟环境文件夹旁边。因此，在根目录中，我有以下子文件夹:</p><ol class=""><li id="7bdd" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">贝尔坦夫</li><li id="2c76" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">伯特-马斯特</li></ol><h2 id="c0af" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">Python 模块</h2><p id="cd30" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">BERT 只需要 tensorflow 模块。您必须安装等于或高于 1.11.0 的版本。确保您安装了 CPU 版本或 GPU 版本，但不是两者都安装。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d9ba" class="nc ma it nt b gy nx ny l nz oa">tensorflow &gt;= 1.11.0   # CPU Version of TensorFlow.                       tensorflow-gpu  &gt;= 1.11.0  # GPU version of TensorFlow.</span></pre><p id="b21d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可以通过 pip 或者位于<strong class="ki iu"> bert-master </strong>文件夹中的<a class="ae kf" href="https://github.com/google-research/bert/blob/master/requirements.txt" rel="noopener ugc nofollow" target="_blank"> <em class="ob"> requirement.txt </em> </a>文件来安装。</p><h2 id="357c" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">伯特模型</h2><p id="bb9b" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">我们将需要一个微调过程的基础模型。我将在本教程中使用<a class="ae kf" href="https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> BERT-Base，Cased </strong> </a> ( 12 层，768-hidden，12-heads，110M 参数)。如果你想试试<strong class="ki iu"> BERT-Large </strong> ( 24 层，1024 隐藏，16 头，340M 参数)，确保你有足够的内存。12GB 的 GPU 不足以运行 BERT-Large。我个人会推荐你用 64GB 的 GPU 做 BERT-Large。在撰写本文时，BERT 背后的团队还发布了其他模型，如中文、多语言和全词屏蔽。请通过以下<a class="ae kf" href="https://github.com/google-research/bert#pre-trained-models" rel="noopener ugc nofollow" target="_blank">链接</a>查看。下载完文件后，解压缩该文件，您应该得到以下文件:</p><ol class=""><li id="89ef" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">三个 ckpt 文件</li><li id="4309" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">vocab.txt</li><li id="916d" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">伯特配置文件</li></ol><p id="21eb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将它们放在<strong class="ki iu">模型</strong>文件夹中，并将其移动到<strong class="ki iu"> bert-master </strong>文件夹中。请转到下一节数据集准备。</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="a450" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">[第 2 节]数据集准备</h1><p id="9e48" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">对于 BERT 来说，数据准备是非常复杂的，因为官方的 github 链接并没有包含太多需要什么样的数据。首先，有 4 个类可用于序列分类任务:</p><ol class=""><li id="e844" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">Xnli(跨语言 nli)</li><li id="aaac" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">多体裁自然语言推理</li><li id="b0c4" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">微软研究释义语料库</li><li id="005c" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">语言可接受性语料库</li></ol><p id="3898" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">所有的类都是基于数据处理器类(参见第 177 行的<a class="ae kf" href="https://github.com/google-research/bert/blob/master/run_classifier.py" rel="noopener ugc nofollow" target="_blank"><em class="ob">run _ classifier . py</em></a>文件)，该类用于将数据提取到以下内容:</p><ol class=""><li id="0cd8" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu"> guid </strong>:示例的唯一 id。</li><li id="2efb" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> text_a </strong>:字符串数据。第一个序列的未标记文本。对于单序列任务，只能指定此序列。</li><li id="812b" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> text_b </strong>:(可选)字符串数据。第二个序列的未标记文本。必须为序列对任务指定 Only。</li><li id="0a88" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">标签</strong>:字符串数据。示例的标签。这应该为训练和评估示例指定，而不是为测试示例指定。</li></ol><p id="35f0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">换句话说，我们可以修改我们的数据集来模拟 4 个类之一的模式和格式，或者编写我们自己的类来扩展 DataProcessor 类以读取我们的数据。在本教程中，我将把数据集转换成可乐格式，因为它是所有格式中最简单的。其他数据集的例子可以在下面的<a class="ae kf" href="https://gluebenchmark.com/tasks" rel="noopener ugc nofollow" target="_blank">链接</a>(胶水版)中找到。</p><p id="18f3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在 BERT 的原始版本中，<a class="ae kf" href="https://github.com/google-research/bert/blob/master/run_classifier.py" rel="noopener ugc nofollow" target="_blank"><em class="ob">run _ classifier . py</em></a>基于从三个 tsv 文件中读取输入:</p><ol class=""><li id="4938" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">train.tsv(无标题)</li><li id="1aa4" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">dev.tsv(评估，无标题)</li><li id="d861" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">test.tsv(头是必需的)</li></ol><p id="a04f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="ob"> train.tsv </em>和<em class="ob"> dev.tsv </em>，你应该有以下格式(无表头):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="8215" class="nc ma it nt b gy nx ny l nz oa">a550d    1    a    To clarify, I didn't delete these pages.<br/>kcd12    0    a    Dear god this site is horrible.<br/>7379b    1    a    I think this is not appropriate.<br/>cccfd    2    a    The title is fine as it is.</span></pre><ol class=""><li id="2f93" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu">列 1 </strong>:示例的 guid。它可以是任何唯一的标识符。</li><li id="72b2" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">第 2 列</strong>:示例的标签。它是基于字符串的，可以是文本形式，而不仅仅是数字。为了简单起见，我在这里只使用数字。</li><li id="44ea" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">第 3 列</strong>:第二个序列的未分词文本。必须为序列对任务指定 Only。因为我们正在进行单序列任务，所以这只是一个一次性的专栏。对于所有的行，我们都用“a”来填充它。</li><li id="cfea" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">第 4 列</strong>:第一个序列的未分词文本。用示例的文本填充它。</li></ol><p id="f1ac" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于<em class="ob"> test.tsv </em>，你应该有如下格式(需要表头):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="69ca" class="nc ma it nt b gy nx ny l nz oa">guid     text<br/>casd4    I am not going to buy this useless stuff.<br/>3ndf9    I wanna be the very best, like no one ever was</span></pre><h2 id="36eb" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">将其他来源的数据转换成所需的格式</h2><p id="b789" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">如果您有不同于上面给出的格式的数据，您可以使用 pandas 模块和 sklearn 模块轻松地转换它。通过 pip 安装模块(确保虚拟环境已激活):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="a06c" class="nc ma it nt b gy nx ny l nz oa">pip install pandas</span></pre><p id="007a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您打算使用 train_test_split，也要安装 sklearn:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="e9ab" class="nc ma it nt b gy nx ny l nz oa">pip install sklearn</span></pre><p id="da6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，如果我们有以下 csv 格式的训练数据集:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="6530" class="nc ma it nt b gy nx ny l nz oa">id,text,label<br/>sadcc,This is not what I want.,1<br/>cj1ne,He seriously have no idea what it is all about,0<br/>123nj,I don't think that we have any right to judge others,2</span></pre><p id="a1d0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以使用以下代码轻松加载数据集并将其转换为相应的格式(相应地修改路径):</p><h2 id="60cc" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">从 csv 文件创建数据帧</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="cda7" class="nc ma it nt b gy nx ny l nz oa">import pandas as pd</span><span id="9315" class="nc ma it nt b gy oc ny l nz oa">df_train = pd.read_csv('dataset/train.csv')</span></pre><h2 id="cf27" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">从现有数据框架创建新的数据框架</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="4233" class="nc ma it nt b gy nx ny l nz oa">df_bert = pd.DataFrame({'guid': df_train['id'],<br/>    'label': df_train['label'],<br/>    <strong class="nt iu">'alpha': ['a']*df_train.shape[0]</strong>,<br/>    'text': df_train['text']})</span></pre><p id="3ddf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">粗体突出显示的部分意味着我们将根据 df_train 数据帧中的行数用字符串 a 填充列 alpha。shape[0]指的是行数，而 shape[1]指的是列数。</p><h2 id="4c18" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">输出 tsv 文件</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7de4" class="nc ma it nt b gy nx ny l nz oa">df_bert_train.to_csv('dataset/train.tsv', <strong class="nt iu">sep='\t'</strong>, index=False, header=False)</span></pre><p id="d03b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不要对 to_csv 函数调用感到惊讶，因为除了分隔符之外，tsv 和 csv 具有相似的格式。换句话说，我们只需要提供正确的制表符分隔符，它就会变成一个 tsv 文件(以粗体突出显示)。</p><p id="3dcf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是创建所有必需文件的完整工作代码片段(相应地修改路径)。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="5884" class="nc ma it nt b gy nx ny l nz oa">import pandas as pd<br/>from sklearn.model_selection import train_test_split</span><span id="9a76" class="nc ma it nt b gy oc ny l nz oa">#read source data from csv file<br/>df_train = pd.read_csv('dataset/train.csv')<br/>df_test = pd.read_csv('dataset/test.csv')</span><span id="8cf3" class="nc ma it nt b gy oc ny l nz oa">#create a new dataframe for train, dev data<br/>df_bert = pd.DataFrame({'guid': df_train['id'],<br/>    'label': df_train['label'],<br/>    'alpha': ['a']*df_train.shape[0],<br/>    'text': df_train['text']})</span><span id="4388" class="nc ma it nt b gy oc ny l nz oa">#split into test, dev<br/>df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)</span><span id="75fe" class="nc ma it nt b gy oc ny l nz oa">#create new dataframe for test data<br/>df_bert_test = pd.DataFrame({'guid': df_test['id'],<br/>    'text': df_test['text']})</span><span id="0f9d" class="nc ma it nt b gy oc ny l nz oa">#output tsv file, no header for train and dev<br/>df_bert_train.to_csv('dataset/train.tsv', sep='\t', index=False, header=False)<br/>df_bert_dev.to_csv('dataset/dev.tsv', sep='\t', index=False, header=False)<br/>df_bert_test.to_csv('dataset/test.tsv', sep='\t', index=False, header=True)</span></pre><p id="aa7a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦你有了所有需要的文件，将<strong class="ki iu">数据集</strong>文件夹移动到<strong class="ki iu"> bert-master </strong>文件夹。让我们继续下一部分来微调您的模型。</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="78d5" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">[第三节]培训模式</h1><p id="b4a0" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">微调 BERT 模型最简单的方法是通过命令行(终端)运行 run_classifier.py。在此之前，我们需要根据我们的标签修改 python 文件。最初的版本是使用 0 和 1 作为标签的二进制分类。如果使用不同的标签进行多分类或二分类，需要更改<strong class="ki iu"> ColaProcessor </strong>类的<em class="ob"> get_labels() </em>函数(第 354 行，如果使用其他<strong class="ki iu">数据处理器</strong>类，请相应修改):</p><h2 id="66d6" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">原始代码</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="aa2c" class="nc ma it nt b gy nx ny l nz oa">def get_labels(self):<br/>    return ["0", "1"]</span></pre><h2 id="8e26" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">5 标签多分类任务</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="d2ab" class="nc ma it nt b gy nx ny l nz oa">def get_labels(self):<br/>    return ["0", "1", "2", "3", "4"]</span></pre><h2 id="74dd" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">使用不同标签的二元分类</h2><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="a202" class="nc ma it nt b gy nx ny l nz oa">def get_labels(self):<br/>    return ["POSITIVE", "NEGATIVE"]</span></pre><p id="3faf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您遇到如下任何键错误，这意味着您的 get_labels 函数与您的数据集不匹配:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="7892" class="nc ma it nt b gy nx ny l nz oa">label_id = label_map[example.label]<br/>KeyError: '2'`</span></pre><p id="625c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在已经准备好接受训练了。如果您使用的是 NVIDIA GPU，您可以在终端中键入以下内容来检查状态和 CUDA 版本。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="8e5c" class="nc ma it nt b gy nx ny l nz oa">nvidia-smi</span></pre><p id="1fb5" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更改目录指向<strong class="ki iu"> bert-master </strong>文件夹，确保<strong class="ki iu">数据集</strong>文件夹和<strong class="ki iu"> bert-master </strong>文件夹中的所需文件。建议通过命令行运行培训，而不是使用 jupyter notebook，原因如下:</p><ol class=""><li id="8eed" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">官方代码使用 2 个单位缩进，不同于笔记本默认的 4 个单位缩进。</li><li id="be08" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">内存问题和配置用于训练的 GPU 的额外代码。</li></ol><h2 id="1395" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">因素</h2><p id="10d5" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在此之前，让我们探索一下可以针对培训流程进行微调的参数:</p><ol class=""><li id="bbed" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu"> data_dir </strong>:包含 train.tsv、dev.tsv、test.tsv 的输入目录。</li><li id="7942" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> bert_config_file </strong>:预先训练好的 bert 模型对应的 config json 文件。这指定了模型架构。</li><li id="488a" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">任务名称</strong>:训练任务的名称。4 个选项可用(xnli、mrpc、mnli、cola)。</li><li id="df69" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> vocab_file </strong>:训练 BERT 模型的词汇文件。</li><li id="6b18" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> output_dir </strong>:模型检查点将被写入的输出目录。</li><li id="4afe" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> init_checkpoint </strong>:初始检查点(通常来自预训练的 BERT 模型)。</li><li id="535b" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> do_lower_case </strong>:输入的文本是否小写。对于无大小写应为 True，对于有大小写应为 False。</li><li id="82a4" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> max_seq_length </strong>:分词后最大总输入序列长度。长于此长度的序列将被截断，短于此长度的序列将被填充。默认值为 128。</li><li id="5074" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> do_train </strong>:是否在 train.tsv 上运行训练。</li><li id="593f" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> do_eval </strong>:是否在 dev.tsv 上运行评估</li><li id="4bd7" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> do_predict </strong>:是否在 test.tsv 上以推理模式运行模型</li><li id="2ea1" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> train_batch_size </strong>:训练的总批量。默认值为 32。</li><li id="70a9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> eval_batch_size </strong>:评估的总批量。默认值为 8</li><li id="0b2e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> predict_batch_size </strong>:测试和预测的总批量。默认值为 8。</li><li id="229b" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">learning _ rate</strong>:Adam 的初始学习率。默认为 5e-5。</li><li id="b564" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> num_train_epochs </strong>:要执行的训练总次数。默认值为 3.0。</li><li id="e9c2" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">热身 _ 比例</strong>:从 0 到 1 进行线性学习率热身的训练比例。默认值为 0.1 表示 10%。</li><li id="6af4" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> save_checkpoints_steps </strong>:保存模型检查点的步数间隔。默认值为 1000。</li><li id="77c5" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> iterations_per_loop </strong>:每次估算器调用的步数间隔。默认值为 1000。</li><li id="3065" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">使用 _tpu </strong>:是否使用 tpu。</li><li id="ed0f" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">tpu _ 姓名</strong>:云 TPU 用于训练。</li><li id="788e" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> tpu 区</strong>:TPU 云所在的 GCE 区。</li><li id="4740" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> gcp_project </strong>:启用云 TPU 的项目的项目名称</li><li id="7cf6" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">主</strong> : TensorFlow 主 URL。</li><li id="34f6" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">数量 _ TPU _ 核心数</strong>:仅当<strong class="ki iu">使用 _tpu </strong>为真时使用。要使用的 TPU 核心总数。</li></ol><p id="adb2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不要被参数的数量所淹没，因为我们不会指定每个参数。</p><h2 id="4398" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">培训说明</h2><p id="a587" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">从<a class="ae kf" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">官方文档</a>中，它建议通过以下命令行调用将路径导出为变量(如果您使用的是 Windows 操作系统，请将导出替换为设置):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="16b2" class="nc ma it nt b gy nx ny l nz oa">export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12</span></pre><p id="1d9a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本教程中，我将不导出路径，因为您仍然需要在命令行中指定它。只要确保你正确地组织了你的文件夹，你就可以开始了。要指定 GPU，您需要在 python 调用之前键入它(示例如下，不要运行它):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="6ed2" class="nc ma it nt b gy nx ny l nz oa">CUDA_VISIBLE_DEVICES=0 python script.py</span></pre><p id="714c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">0 是指 GPU 的顺序。使用以下命令检查它:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="0a20" class="nc ma it nt b gy nx ny l nz oa">nvidia-smi</span></pre><p id="48ff" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<strong class="ki iu"> bert-master </strong>文件夹中，创建一个输出文件夹。我就把它叫做<strong class="ki iu"> bert_output </strong>。确保在<strong class="ki iu"> bert-master </strong>文件夹中有以下文件夹和文件:</p><ol class=""><li id="255b" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated"><strong class="ki iu">数据集</strong>文件夹(包含 train.tsv，dev.tsv，test.tsv)</li><li id="feb8" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu">型号</strong>文件夹(包含 ckpt，vocab.txt，bert_config.json)</li><li id="0afb" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><strong class="ki iu"> bert_output </strong>文件夹(空)</li></ol><h2 id="a9b5" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">通过命令行培训</h2><p id="5380" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">确保终端指向 bert-master 目录，并且虚拟环境已激活。根据您的偏好修改参数并运行它。我做了以下更改:</p><ol class=""><li id="fc6d" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">将<strong class="ki iu"> train_batch_size </strong>减少到 2:如果你有足够的内存，可以随意增加。这影响了训练时间。越高，训练时间越短。</li><li id="eeac" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">将<strong class="ki iu">的保存 _ 检查点 _ 步骤</strong>增加到 10000。我不想有这么多检查点，因为每个检查点模型都是原始大小的 3 倍。放心，这个脚本一次只保留 5 个模型。旧型号将被自动删除。强烈建议将其保持为 1000(默认值)。</li><li id="7ce9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">将<strong class="ki iu">最大序列长度</strong>减少到 64。由于我的数据集 99%的长度不超过 64，将其设置得更高是多余的。根据数据集对此进行相应的修改。默认值为 128。(序列长度是指词块标记化后的字符长度，请考虑这一点)。</li></ol><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="b7ae" class="nc ma it nt b gy nx ny l nz oa">CUDA_VISIBLE_DEVICES=0 python run_classifier.py --task_name=cola --do_train=true --do_eval=true --data_dir=./dataset --vocab_file=./model/vocab.txt --bert_config_file=./model/bert_config.json --init_checkpoint=./model/bert_model.ckpt --max_seq_length=64 --train_batch_size=2 --learning_rate=2e-5 --num_train_epochs=3.0 --output_dir=./bert_output/ --do_lower_case=False --save_checkpoints_steps 10000</span></pre><p id="7bbf" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练应该已经开始，显示跑步步数/秒。检查 bert_output 文件夹，您应该注意到以下内容:</p><ol class=""><li id="65ea" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">三个 ckpt 文件(模型)</li><li id="9659" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">tf _ 记录</li><li id="5d50" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">检查点和事件文件(临时文件，可以在培训后安全地忽略和删除)</li><li id="ad61" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated">graph.pbtxt</li></ol><p id="246b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能需要几个小时到几天的时间，具体取决于您的数据集和配置。</p><h2 id="64b1" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">培训完成</h2><p id="2cca" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">培训完成后，您应该有一个<em class="ob"> eval_results.txt </em>来指示您的模型的性能。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="a2af" class="nc ma it nt b gy nx ny l nz oa">eval_accuracy = 0.96741855<br/>eval_loss = 0.17597112<br/>global_step = 236962<br/>loss = 0.17553209</span></pre><p id="4150" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">确定模型的最高步骤数。如果您对此不确定，请在文本编辑器中打开检查点，您应该会看到以下内容:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="9252" class="nc ma it nt b gy nx ny l nz oa">model_checkpoint_path: "model.ckpt-236962"<br/>all_model_checkpoint_paths: "model.ckpt-198000"<br/>all_model_checkpoint_paths: "model.ckpt-208000"<br/>all_model_checkpoint_paths: "model.ckpt-218000"<br/>all_model_checkpoint_paths: "model.ckpt-228000"<br/>all_model_checkpoint_paths: "model.ckpt-236962"</span></pre><p id="cadb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这种情况下，最高步骤是 236962。我们现在可以用这个模型来预测结果。</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="814a" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">[第四节]预测</h1><p id="689b" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">为了进行预测，我们将使用相同的<em class="ob"> run_classifier.py </em>。这一次，我们需要将<strong class="ki iu"> do_predict </strong>指定为 True，并将<strong class="ki iu"> init_checkpoint </strong>设置为我们拥有的最新模型 model.ckpt-236962(根据您拥有的最高步骤进行相应修改)。但是，您需要确保<strong class="ki iu"> max_seq_length </strong>与您用于训练的相同。</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="bb25" class="nc ma it nt b gy nx ny l nz oa">CUDA_VISIBLE_DEVICES=0 python run_classifier.py --task_name=cola --do_predict=true --data_dir=./dataset --vocab_file=./model/vocab.txt --bert_config_file=./model/bert_config.json --init_checkpoint=./bert_output/model.ckpt-236962 --max_seq_length=64 --output_dir=./bert_output/</span></pre><p id="f08a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦流程完成，您应该在<strong class="ki iu"> bert_output </strong>文件夹中有一个 test_results.tsv(取决于您为<strong class="ki iu"> output_dir </strong>指定的内容)。如果使用文本编辑器打开它，您应该会看到以下输出:</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="1cf4" class="nc ma it nt b gy nx ny l nz oa">1.4509245e-05 1.2467547e-05 0.99994636<br/>1.4016414e-05 0.99992466 1.5453812e-05<br/>1.1929651e-05 0.99995375 6.324972e-06<br/>3.1922486e-05 0.9999423 5.038059e-06<br/>1.9996814e-05 0.99989235 7.255715e-06<br/>4.146e-05 0.9999349 5.270801e-06</span></pre><p id="f544" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">列的数量取决于标签的数量。每一列按照您为 get_labels()函数指定的顺序表示每个标签。该值代表预测的可能性。例如，模型预测第一个示例属于第 3 类，因为它具有最高的概率。</p><h2 id="2244" class="nc ma it bd mb nd ne dn mf nf ng dp mj kr nh ni mn kv nj nk mr kz nl nm mv nn bi translated">将结果映射到相应的类</h2><p id="45df" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">如果您想映射结果来计算精确度，可以使用以下代码(相应地修改):</p><pre class="no np nq nr gt ns nt nu nv aw nw bi"><span id="6ae8" class="nc ma it nt b gy nx ny l nz oa">import pandas as pd</span><span id="09ca" class="nc ma it nt b gy oc ny l nz oa">#read the original test data for the text and id<br/>df_test = pd.read_csv('dataset/test.tsv', sep='\t')</span><span id="9e66" class="nc ma it nt b gy oc ny l nz oa"><br/>#read the results data for the probabilities<br/>df_result = pd.read_csv('bert_output/test_results.tsv', sep='\t', header=None)</span><span id="cb24" class="nc ma it nt b gy oc ny l nz oa">#create a new dataframe<br/>df_map_result = pd.DataFrame({'guid': df_test['guid'],<br/>    'text': df_test['text'],<br/>    'label': df_result.idxmax(axis=1)})</span><span id="8394" class="nc ma it nt b gy oc ny l nz oa">#view sample rows of the newly created dataframe<br/>df_map_result.sample(10)</span></pre><p id="c061" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> idxmax </strong>是一个函数，用于返回请求轴上第一次出现的最大值的索引。不包括 NA/null 值。在这种情况下，我们传递 1 作为轴来表示列而不是行。</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="193f" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">[第五节]结论</h1><p id="c28f" class="pw-post-body-paragraph kg kh it ki b kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz nb lb lc ld im bi translated">在本教程中，我们已经学会了针对多分类任务微调 BERT。供您参考，BERT 可以用于其他自然语言处理任务，而不仅仅是分类。就我个人而言，我也测试了基于 BERT 的中文情感分析，结果出乎意料的好。请记住，非拉丁语言(如中文和朝鲜语)是字符标记化，而不是单词标记化。请随意在不同种类的数据集上尝试其他模型。感谢您的阅读，祝您有美好的一天。下篇再见！</p></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="91f3" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">伯特相关的故事</h1><ol class=""><li id="908f" class="le lf it ki b kj mx kn my kr od kv oe kz of ld lj lk ll lm bi translated"><a class="ae kf" rel="noopener" target="_blank" href="/3-ways-to-optimize-and-export-bert-model-for-online-serving-8f49d774a501">优化和导出在线服务 BERT 模型的 3 种方法</a></li></ol></div><div class="ab cl ls lt hx lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="im in io ip iq"><h1 id="9366" class="lz ma it bd mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw bi translated">参考</h1><ol class=""><li id="2c03" class="le lf it ki b kj mx kn my kr od kv oe kz of ld lj lk ll lm bi translated">【https://arxiv.org/abs/1810.04805 T4】</li><li id="07a5" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/bert</a></li><li id="5bbf" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://gist.github.com/Geoyi/d9fab4f609e9f75941946be45000632b" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/geo yi/d 9 fab 4 f 609 e 9 f 75941946 be 45000632 b</a></li><li id="0cb9" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://gluebenchmark.com/tasks" rel="noopener ugc nofollow" target="_blank">https://gluebenchmark.com/tasks</a></li><li id="cb09" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://github.com/google-research/bert/blob/master/run_classifier.py" rel="noopener ugc nofollow" target="_blank">https://github . com/Google-research/Bert/blob/master/run _ classifier . py</a></li><li id="ecc6" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://mc.ai/a-guide-to-simple-text-classification-with-bert/" rel="noopener ugc nofollow" target="_blank">https://MC . ai/a-guide-to-simple-text-class ification-with-Bert/</a></li><li id="4545" class="le lf it ki b kj ln kn lo kr lp kv lq kz lr ld lj lk ll lm bi translated"><a class="ae kf" href="https://appliedmachinelearning.blog/2019/03/04/state-of-the-art-text-classification-using-bert-model-predict-the-happiness-hackerearth-challenge/" rel="noopener ugc nofollow" target="_blank">https://appliedmachinehlearning . blog/2019/03/04/stat-of-art-text-classification-using-Bert-model-predict-the-happy-hackere earth-challenge/</a></li></ol></div></div>    
</body>
</html>