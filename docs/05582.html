<html>
<head>
<title>AI for Industrial Process Control</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">工业过程控制的人工智能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-for-industrial-process-control-ee774267094b?source=collection_archive---------13-----------------------#2019-08-16">https://towardsdatascience.com/ai-for-industrial-process-control-ee774267094b?source=collection_archive---------13-----------------------#2019-08-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9ba9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用强化学习来调整过程炉</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b1a4f424fc90c2916fad91d976184432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oKh4cMArrQfvYFwihH8Egw.jpeg"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 1. Reflow Oven</figcaption></figure><p id="0cba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">确定工业过程的最佳控制设置可能很困难。例如，控件可以交互，调整一个设置需要重新调整其他设置。此外，控件与其效果之间的关系可能非常复杂。这种复杂性对于优化工艺来说是具有挑战性的。本文探索了一种用于控制工业传送带式炉的强化学习解决方案。</p><h2 id="5a10" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">介绍</h2><p id="be90" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这类设备的一个例子是用于将电子元件焊接到电路板上的回流焊炉(图 1 和图 2)。烤箱有一个传送带，将产品传送通过多个加热区。该过程根据确保可靠焊接连接所需的精确温度-时间曲线加热产品。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/67845af2e69446da050d6e4605045995.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/1*GrZapDG47cDlM_b8G8H7Kw.gif"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 2. Product Exiting Oven</figcaption></figure><p id="1860" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文讨论的回流焊炉有八个加热区，每个加热区都有一个控制装置，用于设置加热区加热器的温度。当产品通过烤箱时，传感器会记录大约 300 点的温度。每个点的温度由从加热器传递到产品的热量决定。</p><h2 id="c534" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">强化学习解决方案</h2><p id="1764" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">操作员通常采取以下步骤来学习加热器设置:</p><ul class=""><li id="5a28" class="mt mu it la b lb lc le lf lh mv ll mw lp mx lt my mz na nb bi translated">让产品通过烤箱一次</li><li id="886b" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">根据传感器读数观察温度-时间曲线</li><li id="81b7" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">调整加热器设置(希望)改善轮廓</li><li id="df2f" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">等待烤箱稳定到新的设置</li><li id="fb0e" class="mt mu it la b lb nc le nd lh ne ll nf lp ng lt my mz na nb bi translated">重复此程序，直到传感器读数的曲线可接受地接近所需的曲线</li></ul><p id="da7c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">强化学习系统用两阶段过程代替了操作员步骤。在第一阶段，智能代理学习烤箱的动态，并创建一个在各种烤箱条件下更新加热器设置的策略。</p><p id="9838" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第二阶段，代理按照学习到的策略寻找最佳加热器设置。这些设置将在实际产品曲线和所需的温度-时间曲线之间产生最接近的匹配。图 3 显示了代理按照策略寻找最佳设置。红色轨迹是所需的温度-时间曲线，蓝色轨迹是代理发现最佳加热器设置时的实际曲线。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/0c23334f4e9f86f399465af7dc9d089f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*8BeL_6N_8XRTALZl5_kRVg.gif"/></div></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 3. red: desired profile — blue: actual profile</figcaption></figure><h2 id="8e31" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">代理人</h2><p id="8cba" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">由于通过烘箱需要相当长的时间(&gt; 300 秒)并稳定烘箱(许多分钟)，因此使用烘箱模拟器来大大加快该过程。模拟器模拟烤箱对产品的加热作用。</p><p id="0302" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在第一阶段的每个步骤中，强化学习代理将八个加热器的设置传递给模拟器。模拟运行后，模拟器返回产品温度读数(大约 300 个读数，间隔 1 秒)。</p><p id="3aa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">代理使用选择的读数来确定系统的状态。它还通过比较返回的读数和期望的温度-时间曲线之间的差异来计算当前运行的回报。如果当前运行的差异小于先前运行的差异，则奖励为正；否则为负。奖励用于更新策略。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d75de506ff05f0bf20f64264a4ecf053.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pmZYkrx_5-mA7b0yl_4fCA.png"/></div><figcaption class="ku kv gj gh gi kw kx bd b be z dk">Figure 4. Reinforcement Learning System</figcaption></figure><p id="1db4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在重复该过程数千次之后，代理将已经学习了在各种烤箱条件下更新加热器设置的广泛策略。在第二阶段，代理遵循学习到的策略来寻找最佳加热器设置，该设置将在实际产品曲线和期望的温度-时间曲线之间产生最接近的匹配。</p><h2 id="cf75" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">更深的潜水</h2><p id="1e2c" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">项目中使用的强化学习系统使用双 Deep-Q 模型，该模型包含两个神经网络和经验重放。在第一阶段过程之后，其中一个神经网络保存代理在第二阶段使用的学习策略。要了解更多细节，请查看本文末尾引用的论文。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="4c1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[1]: van Hasselt，h .，Guez，a .，Silver，D. <strong class="la iu">采用双 Q 学习的深度强化学习。</strong> arXiv 预印本<a class="ae nq" href="https://arxiv.org/abs/1509.06461v3" rel="noopener ugc nofollow" target="_blank"> arXiv:1509.06461 </a>，2015。</p><p id="34f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]: Mnih 等，<strong class="la iu">通过深度强化学习实现人级控制。</strong>自然，518(7540):529–533，2015。[ <a class="ae nq" href="https://deepmind.com/research/publications/human-level-control-through-deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> Deepmind </a></p></div></div>    
</body>
</html>