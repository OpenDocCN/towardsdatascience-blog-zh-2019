<html>
<head>
<title>The state of 3D object detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">三维物体检测的现状</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-state-of-3d-object-detection-f65a385f67a8?source=collection_archive---------7-----------------------#2019-05-09">https://towardsdatascience.com/the-state-of-3d-object-detection-f65a385f67a8?source=collection_archive---------7-----------------------#2019-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3f30" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基于 KITTI 排行榜的技术现状综述</h2></div><h1 id="e0e2" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">前言</strong></h1><blockquote class="la lb lc"><p id="d32d" class="ld le lf lg b lh li ju lj lk ll jx lm ln lo lp lq lr ls lt lu lv lw lx ly lz im bi translated"><strong class="lg iu">此贴更新</strong> <a class="ae ma" href="https://medium.com/ike-blog/perception-for-automated-trucking-c8a8c12e1015" rel="noopener"> <strong class="lg iu">此处</strong> </a> <strong class="lg iu">。</strong></p></blockquote><h1 id="af8a" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">简介</strong></h1><p id="82fd" class="pw-post-body-paragraph ld le it lg b lh mb ju lj lk mc jx lm md me lp lq mf mg lt lu mh mi lx ly lz im bi translated">3D 对象检测是自动驾驶的一个基本挑战。<a class="ae ma" href="http://www.cvlibs.net/datasets/kitti/" rel="noopener ugc nofollow" target="_blank"> KITTI vision 基准</a>提供了一个标准化数据集，用于训练和评估不同 3D 物体检测器的性能。在这里，我使用来自 KITTI 的数据来总结和强调 3D 检测策略中的权衡。这些策略通常可以分为使用<strong class="lg iu">激光雷达</strong>的策略和使用<strong class="lg iu">激光雷达+图像(RGB) </strong>的策略。我分别分析这几类。</p><h1 id="bf03" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">激光雷达</strong></h1><p id="1476" class="pw-post-body-paragraph ld le it lg b lh mb ju lj lk mc jx lm md me lp lq mf mg lt lu mh mi lx ly lz im bi translated"><a class="ae ma" href="http://cs231n.github.io/convolutional-networks/" rel="noopener ugc nofollow" target="_blank"> CNN 机械</a>对 2D 天体的探测和分类已经<a class="ae ma" href="http://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank">成熟</a>。但是，用于自动驾驶的 3D 物体检测提出了至少两个独特的挑战:</p><ul class=""><li id="eadf" class="mj mk it lg b lh li lk ll md ml mf mm mh mn lz mo mp mq mr bi translated">与 RGB 图像不同，激光雷达点云是 3D 和非结构化的。</li><li id="3168" class="mj mk it lg b lh ms lk mt md mu mf mv mh mw lz mo mp mq mr bi translated">自动驾驶的 3D 检测必须快速(&lt; ~100ms).</li></ul><p id="0396" class="pw-post-body-paragraph ld le it lg b lh li ju lj lk ll jx lm md lo lp lq mf ls lt lu mh lw lx ly lz im bi translated">Several 3D detection methods have tackled the first problem by discretizing the LIDAR point cloud into a 3D voxel grid and <a class="ae ma" href="https://arxiv.org/pdf/1611.08069.pdf" rel="noopener ugc nofollow" target="_blank">应用 3D 卷积</a>)。然而，3D 卷积比 2D 卷积具有更大的计算成本和更高的等待时间。或者，点云可以被投影到自上而下鸟瞰视图(<strong class="lg iu"> BEV </strong>)或激光雷达的本地范围视图(<strong class="lg iu"> RV </strong>)中的 2D 图像。优点是投影图像可以用更快的 2D 卷积有效地处理，产生更低的等待时间。</p><p id="f764" class="pw-post-body-paragraph ld le it lg b lh li ju lj lk ll jx lm md lo lp lq mf ls lt lu mh lw lx ly lz im bi translated">我从<a class="ae ma" href="http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=bev" rel="noopener ugc nofollow" target="_blank"> KITTI BEV </a>排行榜中挑选了一些方法，以强调在<strong class="lg iu"> RV </strong>、<strong class="lg iu"> BEV </strong>和操作体素特征的方法之间的一些权衡。该图显示了报告的推断延迟(毫秒)与车辆<a class="ae ma" href="https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener"> AP </a>的关系:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi mx"><img src="../Images/1a997f79ba74bddc769e164a68633a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YtBWthQWmq5bqOytEl51NQ.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Detector (LIDAR only) latency vs vehicle AP</figcaption></figure><p id="76ef" class="pw-post-body-paragraph ld le it lg b lh li ju lj lk ll jx lm md lo lp lq mf ls lt lu mh lw lx ly lz im bi translated">结果中的关键要点:</p><ul class=""><li id="f923" class="mj mk it lg b lh li lk ll md ml mf mm mh mn lz mo mp mq mr bi translated"><strong class="lg iu"> <em class="lf"> BEV </em> </strong> <em class="lf">投影保留了物体大小随距离的变化，为学习提供了强有力的先验。</em>Z 轴被视为 2D 卷积的特征通道。手工制作的 Z 轴宁滨(例如<a class="ae ma" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Yang_PIXOR_Real-Time_3D_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank"> PIXOR </a>)可以使用<a class="ae ma" href="https://arxiv.org/pdf/1612.00593.pdf" rel="noopener ugc nofollow" target="_blank"> PointNet </a>进行改进，以将 Z 轴整合到学习的特征中(例如<a class="ae ma" href="https://arxiv.org/pdf/1812.05784.pdf" rel="noopener ugc nofollow" target="_blank"> PointPillars </a>)。此外，地面高度可用于展平 Z 轴上的点(如<a class="ae ma" href="http://proceedings.mlr.press/v87/yang18b/yang18b.pdf" rel="noopener ugc nofollow" target="_blank"> HDNet </a>)，减轻道路坡度引起的平移变化的影响。</li><li id="4d9f" class="mj mk it lg b lh ms lk mt md mu mf mv mh mw lz mo mp mq mr bi translated"><strong class="lg iu"> <em class="lf"> BEV </em> </strong> <em class="lf">用</em> <strong class="lg iu"> <em class="lf"> </em> </strong> <em class="lf">的博学(PointNet)特性来巩固 Z 轴实现强劲性能</em>。<a class="ae ma" href="https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf" rel="noopener ugc nofollow" target="_blank">第二个</a>用体素特征编码层和稀疏卷积实现；新版<a class="ae ma" href="https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf" rel="noopener ugc nofollow" target="_blank">秒</a>(1.5 版)报告了更好的 AP (86.6%)和低延迟(40ms)。<a class="ae ma" href="https://arxiv.org/pdf/1812.05784.pdf" rel="noopener ugc nofollow" target="_blank"> PointPillars </a>在 Z 轴柱子上应用简化的点网，产生一个 2D BEV 图像，该图像被输入到 2D CNN。</li><li id="0844" class="mj mk it lg b lh ms lk mt md mu mf mv mh mw lz mo mp mq mr bi translated"><strong class="lg iu"> <em class="lf"> RV </em> </strong> <em class="lf">投影受遮挡和物体大小随距离变化的影响。</em> <strong class="lg iu"> RV </strong>探测器(如<a class="ae ma" href="https://arxiv.org/pdf/1903.08701.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet </a>)在 KITTI 的~7.5k 帧训练数据集上性能落后于<strong class="lg iu"> BEV </strong>探测器。但是，<a class="ae ma" href="https://arxiv.org/pdf/1903.08701.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet </a>在 1.2M 帧 ATG4D 数据集上的性能与<strong class="lg iu"> BEV </strong>探测器<strong class="lg iu"> </strong>(例如<a class="ae ma" href="http://proceedings.mlr.press/v87/yang18b/yang18b.pdf" rel="noopener ugc nofollow" target="_blank"> HDNet </a>)不相上下。</li><li id="81d1" class="mj mk it lg b lh ms lk mt md mu mf mv mh mw lz mo mp mq mr bi translated"><strong class="lg iu"> <em class="lf"> RV </em> </strong> <em class="lf">投影具有低延迟</em>(例如<a class="ae ma" href="https://arxiv.org/pdf/1903.08701.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet </a>)，可能是由于相对于较稀疏的<strong class="lg iu"> BEV </strong>的密集<strong class="lg iu"> RV </strong>表示。<a class="ae ma" href="https://arxiv.org/pdf/1711.06396.pdf" rel="noopener ugc nofollow" target="_blank">体素网</a>开创了体素特征的使用，但由于 3D 卷积而遭受高延迟。较新的方法(例如<a class="ae ma" href="https://pdfs.semanticscholar.org/5125/a16039cabc6320c908a4764f32596e018ad3.pdf" rel="noopener ugc nofollow" target="_blank">第二</a>)可以使用相同的体素特征编码层，但是使用稀疏卷积来避免 3D 卷积以减少等待时间。</li></ul><h1 id="5922" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">激光雷达+ RGB </strong></h1><p id="0c77" class="pw-post-body-paragraph ld le it lg b lh mb ju lj lk mc jx lm md me lp lq mf mg lt lu mh mi lx ly lz im bi translated">激光雷达+ RGB 融合提高了 3D 检测性能，特别是对于较小的对象(例如，行人)或长距离(&gt; ~ 50 米-70 米)，其中激光雷达数据通常很稀疏。下面总结了几种融合方法。<strong class="lg iu">基于提议</strong>的方法生成 RGB(例如<a class="ae ma" href="https://arxiv.org/pdf/1711.08488.pdf" rel="noopener ugc nofollow" target="_blank"> F-Pointnet </a>或 BEV(例如<a class="ae ma" href="https://arxiv.org/abs/1611.07759" rel="noopener ugc nofollow" target="_blank"> MV3D </a>)的对象提议。<strong class="lg iu">密集融合</strong>方法将激光雷达和 RGB 特征直接融合到一个公共投影中，通常具有不同的分辨率。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi nn"><img src="../Images/91fc7e20c19fddc5cee8c55457cd3da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N5ilVL6YmjtIHCr-SghsgQ.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">General approaches for LIDAR+RGB fusion. Images are adapted from MV3D (Chen et. at. 2016), F-Pointnet (Qi et. al. 2017), ContFuse (Liang et. al. 2018), and LaserNet (Meyer et. al. 2018).</figcaption></figure><p id="c816" class="pw-post-body-paragraph ld le it lg b lh li ju lj lk ll jx lm md lo lp lq mf ls lt lu mh lw lx ly lz im bi translated">该图显示了报告的推断潜伏期(ms)与车辆 AP 的关系:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi no"><img src="../Images/54d5075bc62d9935a75d495289b82ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11IfMVEO1yFrI5sz5NAH6A.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Detector (LIDAR+RGB fusion labeled) latency vs vehicle AP</figcaption></figure><p id="5d57" class="pw-post-body-paragraph ld le it lg b lh li ju lj lk ll jx lm md lo lp lq mf ls lt lu mh lw lx ly lz im bi translated">结果中的关键要点:</p><ul class=""><li id="f5de" class="mj mk it lg b lh li lk ll md ml mf mm mh mn lz mo mp mq mr bi translated"><strong class="lg iu"><em class="lf">RV</em></strong><em class="lf"/><strong class="lg iu"><em class="lf">密集融合</em> </strong> <em class="lf">在所有方法中延迟最低，基于</em> <strong class="lg iu"> <em class="lf">提议</em> </strong> <em class="lf">的方法一般比</em> <strong class="lg iu"> <em class="lf">密集融合</em> </strong> <em class="lf">延迟高。</em> <strong class="lg iu"> RV </strong> <strong class="lg iu">密集融合</strong>(例如<a class="ae ma" href="https://arxiv.org/pdf/1904.11466.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet++ </a>)速度快因为 RGB 和 LIDAR 特征都在<strong class="lg iu"> RV 中。</strong>激光雷达特征可以直接投影到影像中进行融合。相比之下，<a class="ae ma" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">密集融合</a>不<strong class="lg iu">密集融合</strong> <strong class="lg iu">。<strong class="lg iu"> </strong>它从 RGB 特征生成一个<strong class="lg iu"> BEV </strong>特征图，并与激光雷达<strong class="lg iu"> BEV </strong>特征图融合。这很有挑战性，因为在<strong class="lg iu"> RV </strong> RGB 图像中并不是所有<strong class="lg iu"> BEV </strong>中的像素都是可见的。几个步骤可以解决这个问题。对于一个未观察到的示例<strong class="lg iu"> BEV </strong>像素，提取 K 个附近的激光雷达点。计算每个点和目标<strong class="lg iu"> BEV </strong>像素之间的偏移。这些点被投影到<strong class="lg iu"> RV </strong>以检索相应的 RGB 特征。偏移和 RGB 特征被馈送到<a class="ae ma" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Deep_Parametric_Continuous_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">连续卷积</a>，其在 RGB 特征之间进行插值，以在目标<strong class="lg iu"> BEV </strong>像素处生成未观察到的特征。这是为所有的<strong class="lg iu"> BEV </strong>像素完成的，生成 RGB 特征的密集插值<strong class="lg iu"> BEV </strong>图。</strong></li><li id="ebf6" class="mj mk it lg b lh ms lk mt md mu mf mv mh mw lz mo mp mq mr bi translated"><em class="lf">融合方法通常在激光雷达稀疏的较长距离和小物体上具有最大的性能增益。</em>激光雷达+ RGB 特征融合(<a class="ae ma" href="https://arxiv.org/pdf/1904.11466.pdf" rel="noopener ugc nofollow" target="_blank">激光网络++ </a>)相对于激光雷达(<a class="ae ma" href="https://arxiv.org/pdf/1903.08701.pdf" rel="noopener ugc nofollow" target="_blank">激光网络</a>)的 AP 改进在车辆上是适度的(在 0-70 米处大约 1%的 AP)，但在较小的类别上更显著，特别是在较长的范围内(在 50-70 米处大约 9%的自行车 AP)。<a class="ae ma" href="https://arxiv.org/pdf/1904.11466.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet++ </a>在 ATG4D 上表现强劲，但其 KITTI 性能未见报道。</li></ul><h1 id="b4f6" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated"><strong class="ak">总结</strong></h1><p id="a341" class="pw-post-body-paragraph ld le it lg b lh mb ju lj lk mc jx lm md me lp lq mf mg lt lu mh mi lx ly lz im bi translated">在<strong class="lg iu"> BEV </strong>和<strong class="lg iu"> RV </strong>投影之间存在权衡。<strong class="lg iu"> BEV </strong>保留度量空间，保持对象大小与范围一致。相比之下，<strong class="lg iu"> RV </strong>在范围和遮挡方面存在尺度变化。因此，<strong class="lg iu"> BEV </strong>检测器(如<a class="ae ma" href="https://arxiv.org/pdf/1812.05784.pdf" rel="noopener ugc nofollow" target="_blank"> PointPillars </a>)在小数据集(如约 7.5k 帧的 KITTI)上实现了优于<strong class="lg iu"> RV </strong>(如<a class="ae ma" href="https://arxiv.org/pdf/1903.08701.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet </a>)的性能，具有相似的延迟(如<a class="ae ma" href="https://arxiv.org/pdf/1812.05784.pdf" rel="noopener ugc nofollow" target="_blank"> PointPillars </a>为 16 毫秒，而<a class="ae ma" href="https://arxiv.org/pdf/1903.08701.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet </a>为 12 毫秒)。然而，<strong class="lg iu"> RV </strong>的性能在更大的(例如，1.2M 帧 ATG4D)数据集上与<strong class="lg iu"> BEV </strong>不相上下。尽管有这个缺点，<strong class="lg iu">密集特征融合</strong>在<strong class="lg iu"> RV </strong>中比<strong class="lg iu"> BEV 中更快。</strong> <a class="ae ma" href="https://arxiv.org/pdf/1904.11466.pdf" rel="noopener ugc nofollow" target="_blank"> LaserNet++ </a>报告了令人印象深刻的延迟(38 毫秒)和比<strong class="lg iu">密集 BEV fusion </strong>探测器更好的性能(例如<a class="ae ma" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ming_Liang_Deep_Continuous_Fusion_ECCV_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">cont fuse</a>60 毫秒)。下图总结了这些权衡。新的激光雷达+ RGB 融合架构可以找到在投影之间移动的方法，利用每个投影的优势。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/f368eb4933876cd65d7856d65a0adc7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYUa1qJsG8Hsp6sh4L9X8w.png"/></div></div><figcaption class="nj nk gj gh gi nl nm bd b be z dk">Trade-offs between RV and BEV projections</figcaption></figure></div></div>    
</body>
</html>