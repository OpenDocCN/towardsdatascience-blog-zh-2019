<html>
<head>
<title>Basic Ensemble Learning (Random Forest, AdaBoost, Gradient Boosting)- Step by Step Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基本集成学习(随机森林，AdaBoost，梯度推进)-一步一步解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725?source=collection_archive---------0-----------------------#2019-01-02">https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725?source=collection_archive---------0-----------------------#2019-01-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b4dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">前三种基于树的集成学习算法的逐步解释。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2e9556585c50d0e06c926ddf210a9381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ScOjCds7YEEQisf5"/></div></div><figcaption class="kr ks gj gh gi kt ku bd b be z dk">Photo by <a class="ae kv" href="https://unsplash.com/@kazuend?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">kazuend</a> on <a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="b7ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们都这样做。在我们做出任何重大决定之前，我们会询问人们的意见，比如我们的朋友、家人，甚至我们的狗/猫，以防止我们有偏见😕或者不理智😍。</p><p id="a081" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型也是这样。单个模型遭受偏差或方差是很常见的，<strong class="ky ir">这就是为什么我们需要<em class="ls"> </em>集成学习</strong>。</p><p id="52c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi lt translated"><span class="l lu lv lw bm lx ly lz ma mb di"> E </span> nsemble 学习，一般来说，是一种基于许多不同模型进行预测的模型。通过组合单独的模型，集合模型趋向于更加灵活🤸‍♀️(偏差较小)和 data-sensitive🧘‍♀️(方差较小)。</p><p id="47e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">两种最流行的集成方法是<strong class="ky ir">打包</strong>和<strong class="ky ir">助推</strong>。</p><ul class=""><li id="0119" class="mc md iq ky b kz la lc ld lf me lj mf ln mg lr mh mi mj mk bi translated"><strong class="ky ir">装袋:</strong>并行训练一堆个体模型。每个模型由数据的随机子集训练</li><li id="87d8" class="mc md iq ky b kz ml lc mm lf mn lj mo ln mp lr mh mi mj mk bi translated"><strong class="ky ir"> Boosting: </strong>按顺序训练一堆个体模型。每个单独的模型都从前一个模型所犯的错误中学习。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/d883ce502bd7f1f551a640c4a18775aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bUySDOFp1SdzJXWmWJsXRQ.png"/></div></div></figure><p id="20fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了对什么是集成学习的基本了解，让我们来种一些“树”🎄。</p><p id="df0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下内容将逐步解释随机森林、AdaBoost 和梯度增强，以及它们在 Python Sklearn 中的实现。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="690c" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">随机森林</h1><p id="de34" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi lt translated">andom forest 是一个集合模型，使用 bagging 作为集合方法，决策树作为个体模型。</p><p id="654e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们仔细看看<strong class="ky ir">这个魔法🔮的随机性</strong>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/b66524bb8d512a104a0f327e453fbdd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jXkT3mj1mCqMaX5SqU1wNw.png"/></div></div></figure><p id="094b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 1: <strong class="ky ir">从训练集中选择 n 个(例如 1000 个)随机子集</strong></p><p id="d78e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步:<strong class="ky ir">训练 n 个(例如 1000 个)决策树</strong></p><ul class=""><li id="b220" class="mc md iq ky b kz la lc ld lf me lj mf ln mg lr mh mi mj mk bi translated">一个随机子集用于训练一个决策树</li><li id="5072" class="mc md iq ky b kz ml lc mm lf mn lj mo ln mp lr mh mi mj mk bi translated">每个决策树的最佳分割基于随机的特征子集(例如，总共 10 个特征，从 10 个特征中随机选择 5 个进行分割)</li></ul><p id="1432" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 3: <strong class="ky ir">每棵单独的树独立地预测</strong>测试集中的记录/候选者。</p><p id="94e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第四步:<strong class="ky ir">进行最终预测</strong></p><p id="6c5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于测试集中的每个候选人，随机森林使用具有<strong class="ky ir">多数票</strong>的类(例如猫或狗)作为该候选人的最终预测。</p><p id="d1ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，我们的 1000 棵树就是这里的议会。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="3acc" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">自适应增强</h1><p id="dbc5" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">AdaBoost 是一个增强集成模型，特别适合决策树。Boosting 模型的关键是从以前的错误中学习，例如错误分类数据点。</p><p id="9bac" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi lt translated">通过增加错误分类数据点的权重，daBoost 从错误中学习。</p><p id="6808" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们举例说明<strong class="ky ir">AdaBoost 如何适应</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/762d39feb17e806fb995e8a0a4ecbdf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMvOaN1uNcaI9dvwH7McKw.png"/></div></div></figure><p id="cc4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 0: <strong class="ky ir">初始化数据点的权重</strong>。如果训练集有 100 个数据点，那么每个点的初始权重应该是 1/100 = 0.01。</p><p id="43ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一步:<strong class="ky ir">训练</strong>决策树</p><p id="6b33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二步:<strong class="ky ir">计算决策树的加权错误率(e) </strong>。<strong class="ky ir">加权误差率(e) </strong>是指所有预测中有多少是错误的，你可以根据数据点的权重来区别对待错误的预测。<strong class="ky ir">权重</strong>、<strong class="ky ir">越高，在(e)的计算过程中，相应的误差将被加权</strong>。</p><p id="1505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 3: <strong class="ky ir">计算该决策树在集合中的权重</strong></p><p id="dc91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这棵树的权重=学习率* log( (1 — e) / e)</p><ul class=""><li id="6224" class="mc md iq ky b kz la lc ld lf me lj mf ln mg lr mh mi mj mk bi translated">树的加权错误率越高，😫在随后的投票中，给予该树的决策权越少</li><li id="bd62" class="mc md iq ky b kz ml lc mm lf mn lj mo ln mp lr mh mi mj mk bi translated">树的加权错误率越低，😃在以后的投票中，该树将被给予更高的决策权</li></ul><p id="c5cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 4: <strong class="ky ir">更新错误分类点的权重</strong></p><p id="5c76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个数据点的权重=</p><ul class=""><li id="feb5" class="mc md iq ky b kz la lc ld lf me lj mf ln mg lr mh mi mj mk bi translated">如果模型得到的数据点正确，重量保持不变</li><li id="ef42" class="mc md iq ky b kz ml lc mm lf mn lj mo ln mp lr mh mi mj mk bi translated">如果模型得到的这个数据点是错误的，那么这个点的新权重=旧权重* np.exp(这棵树的权重)</li></ul><p id="e305" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:树的权重越高(该树执行的越准确)，该树的误分类数据点将获得越多的提升(重要性)。在所有错误分类的点被更新之后，数据点的权重被归一化。</p><p id="c252" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第五步:<strong class="ky ir">重复</strong>第一步(直到达到我们设定的训练树的数量)</p><p id="e622" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第六步:<strong class="ky ir">进行最终预测</strong></p><p id="913d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AdaBoost 通过将(每棵树的)权重相加乘以(每棵树的)预测来进行新的预测。显然，权重越高的树对最终决策的影响力越大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/1fb03086f3f91a9cf86f0e79c062d2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MwIT3Gu-dhICzEou7he3OQ.png"/></div></div></figure></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="dc87" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">梯度推进</h1><p id="9e20" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">梯度推进是另一种推进模式。记住，推进模型的关键是从以前的错误中学习。</p><p id="ccd7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi lt translated">radient Boosting 直接从错误——残差中学习，而不是更新数据点的权重。</p><p id="c031" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面举例说明<strong class="ky ir">梯度提升是如何学习的。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/d724317f613ebc7108863b294b31d709.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yXP45jD1OqOnv-1RoshxMA.png"/></div></div></figure><p id="efbe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 1: T <strong class="ky ir">雨</strong>决策树</p><p id="c701" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">步骤 2: <strong class="ky ir">应用</strong>刚刚训练好的决策树进行预测</p><p id="36a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三步:<strong class="ky ir">计算</strong>该决策树的残差，将残差保存为新的 y</p><p id="27af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第四步:<strong class="ky ir">重复</strong>第一步(直到达到我们设定的训练树数)</p><p id="5bfd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第五步:<strong class="ky ir">进行最终预测</strong></p><p id="8d69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">梯度推进通过简单地将(所有树的)预测相加来进行新的预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/478e188bfd1c03a40ebf374b129e3b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r3VlUhPBPfHP9zDU97GJjQ.png"/></div></div></figure></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><h1 id="aa65" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated"><strong class="ak">Python sk learn 中的实现</strong></h1><p id="f22b" class="pw-post-body-paragraph kw kx iq ky b kz nq jr lb lc nr ju le lf ns lh li lj nt ll lm ln nu lp lq lr ij bi translated">下面是 Python Sklearn 中上述三种方法的简单实现。</p><pre class="kg kh ki kj gt oa ob oc od aw oe bi"><span id="cb6c" class="of mz iq ob b gy og oh l oi oj"><strong class="ob ir"># Load Library<br/></strong>from sklearn.datasets import make_moons<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier</span><span id="9b76" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"># Step1: Create data set</strong><br/>X, y = make_moons(n_samples=10000, noise=.5, random_state=0)</span><span id="687e" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"># Step2: Split the training test set</strong><br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><span id="1b09" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"># Step 3: Fit a Decision Tree model as comparison</strong><br/>clf = DecisionTreeClassifier()<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)<br/>accuracy_score(y_test, y_pred)</span><span id="24ee" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir">OUTPUT: 0.756</strong></span><span id="ae3c" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"># Step 4: Fit a Random Forest model, " compared to "Decision Tree model, accuracy go up by 5%<br/></strong>clf = RandomForestClassifier(n_estimators=100, max_features="auto",random_state=0)<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)<br/>accuracy_score(y_test, y_pred)</span><span id="39de" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir">OUTPUT: 0.797</strong></span><span id="6f02" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"># Step 5: Fit a AdaBoost model, " compared to "Decision Tree model, accuracy go up by 10%</strong><br/>clf = AdaBoostClassifier(n_estimators=100)<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)<br/>accuracy_score(y_test, y_pred)</span><span id="86b0" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir">OUTPUT:0.833</strong></span><span id="2c6e" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"># Step 6: Fit a Gradient Boosting model, " compared to "Decision Tree model, accuracy go up by 10%<br/></strong>clf = GradientBoostingClassifier(n_estimators=100)<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)<br/>accuracy_score(y_test, y_pred)</span><span id="59e0" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir">OUTPUT:0.834</strong></span><span id="4d3a" class="of mz iq ob b gy ok oh l oi oj"><strong class="ob ir"><em class="ls">Note: Parameter - n_estimators stands for how many tree we want to grow</em></strong></span></pre></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="5c7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总的来说，集成学习是非常强大的，不仅可以用于分类问题，也可以用于回归。在这篇博客中，我只使用决策树作为集成方法中的个体模型，但是其他的个体模型(线性模型，SVM 等。)也可以应用在打包或增强集合中，以获得更好的性能。</p><p id="a3c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个博客的代码也可以在我的 GitHub 链接中找到。</p><p id="1330" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请在下面留下任何评论、问题或建议。谢谢大家！</p></div></div>    
</body>
</html>